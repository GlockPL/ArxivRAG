{"title": "Leveraging Deep Learning for Time Series Extrinsic Regression in predicting photometric metallicity of Fundamental-mode RR Lyrae Stars", "authors": ["Lorenzo Monti", "Tatiana Muraveva", "Gisella Clementini", "Alessia Garofalo"], "abstract": "Astronomy is entering an unprecedented era of Big Data science, driven by missions like the ESA's Gaia telescope, which aims to map the Milky Way in three dimensions. Gaia's vast dataset presents a monumental challenge for traditional analysis methods. The sheer scale of this data exceeds the capabilities of manual exploration, necessitating the utilization of advanced computational techniques. In response to this challenge, we developed a novel approach leveraging deep learning to estimate the metallicity of fundamental mode (ab-type) RR Lyrae stars from their light curves in the Gaia optical G-band. Our study explores applying deep learning techniques, particularly advanced neural network architectures, in predicting photometric metallicity from time-series data. Our deep learning models demonstrated notable predictive performance, with a low mean absolute error (MAE) of 0.0565, the root mean square error (RMSE) achieved is 0.0765 and a high R2 regression performance of 0.9401 measured by cross-validation. The weighted mean absolute error (wMAE) is 0.0563, while the weighted root mean square error (wRMSE) is 0.0763. These results showcase the effectiveness of our approach in accurately estimating metallicity values. Our work underscores the importance of deep learning in astronomical research, particularly with large datasets from missions like Gaia. By harnessing the power of deep learning methods, we can provide precision in analyzing vast datasets, contributing to more precise and comprehensive insights into complex astronomical phenomena.", "sections": [{"title": "1. Introduction", "content": "RR Lyrae stars are low-mass, core-helium-burning, radially pulsating variable stars representing an old stellar population (for completeness of information see Smith, H.A. (1995) [1]). RR Lyrae stars are unique objects because their intrinsic properties, such as distance and metallicity ([Fe/H]), can be determined from easily observed photometric parameters (e.g. apparent magnitude, pulsation period, etc.). These characteristics have made them valuable distance indicators and metallicity tracers in astrophysical studies of the Local Group galaxies (Tanakul & Sarajedini, 2018; Clementini et al., 2019; D\u00e9k\u00e1ny & Grebel, 2020; Bhardwaj, 2022;) [2-5]. RR Lyrae stars are classified into three types: fundamental mode (ab type), which have asymmetric light curves with longer periods; first-overtone (c type) RR Lyrae stars, characterized by symmetric, sinusoidal light curves and shorter periods; and double-mode pulsation (d type) RR Lyrae stars, which exhibit simultaneous pulsation in both the fundamental and first overtone modes, resulting in complex light curves (Smith, H.A (1995) [1]).\nThe empirical relationship between the shape of fundamental mode (ab type) RR Lyrae stars' light curve and its metallicity has been recognized since the work of Jurcsik and Kov\u00e1cs (1996) [6]. However, accurately calibrating this relationship across multiple photometric bands has presented challenges, primarily due to a limited amount of stars with high-dispersion spectroscopic (HDS) measurements of metallicity. Various empirical formulae have emerged over the past ~25 years to predict metallicity from Fourier regression parameters of light curves, using different methodologies. Given the scarcity of HDS metallicities, most approaches rely on low-dispersion spectroscopic or spectrophotometric estimates or transfer predictive formulae between different passbands, introducing inherent noise and metallicity dependency (Layden, 1994; Smolec, 2005; Ngeow et al., 2016; Skowron et al., 2016; Mullen et al., 2021;) [7-11]. The limited, heterogeneous data and errors in regressors often lead to systematic biases in metallicity prediction formulae. Recently, a number of RR Lyrae stars with HDS metallicity estimates increased (Crestani et al., 2021; Gilligan et al., 2021) [12,13], prompting a revision of earlier photometric metallicity estimation methods.\nIn this sense, Deep learning models have emerged as powerful tools for various astronomical tasks, including metallicity estimation of RR Lyrae stars. These models offer the potential to capture complex relationships between light curve features and metallicity, thus improving prediction accuracy. However, the effectiveness of deep learning models hinges on the availability of large, high-quality datasets. In the context of RR Lyrae stars, obtaining such datasets can be challenging due to the limited number of stars with precise spectroscopic measurements. Additionally, the heterogeneity and inherent noise in observational data pose significant challenges for model training and generalization. Despite these challenges, the potential benefits of deep learning models for metallicity estimation of RR Lyrae stars are substantial.\nThe era of the Gaia mission has completely revolutionized the quality and quantity of data at our disposal for more than 1.8 billion of sources, among them, RR Lyrae stars. With the most recent Gaia Data Release (DR3, Vallenari et al., 2023) [14], almost 271 thousand RR Lyrae stars distributed all sky have been released up to optical G band ~21 mag (Clementini et al., 2023) [15]. For almost all of them Gaia DR3 provided, together with astrometric and photometric products, pulsation properties (period, amplitude, Fourier parameters etc.) and photometric time-series in Gaia bands.\nBy exploiting the large catalogues of RR Lyrae stars, such as those provided by the ESA Gaia mission (Clementini et al., 2023) [15], and advanced neural network architectures, these models have the capacity to improve the accuracy and precision of metallicity predictions, ultimately enhancing our understanding of stellar populations traced by RR Lyrae and evolution of galaxies in which they are located. Using photometric metallicity estimates, we established new empirical predictive models for the metallicity of fundamental-mode (RRab) RR Lyrae stars from their light curves. With a sufficiently large and accurate training set, direct regression to light curves data is feasible, eliminating the customary intermediate step of calibrating spectral indices, such as Fourier parameters of the light curves, to HDS metallicity values."}, {"title": "2. Background and related works", "content": "Recent advancements in deep learning have significantly impacted the field of astronomy and the analysis of time-series data related to variable stars. In particular, it is possible to divide this research area into three categories as follows."}, {"title": "2.1. Time-series Classification", "content": "Brett Naul and colleagues (2018) [16] presented an innovative Recurrent Neural Network (RNN) approach that uses unsupervised autoencoding for the efficient and accurate classification of such variable stars. This methodology demonstrated competitive performance compared to other state-of-the-art methods, highlighting the effectiveness of deep learning techniques in dealing with irregular time-series data commonly encountered in astronomical observations. Moreover, in a pivotal study by Aguirre, Pichara, and Becker (2019) [17], a novel deep learning model integrating convolutional units was developed, designed to classify variable stars from their light curves. This study showcases the potential of this method to revolutionize the process of categorizing vast arrays of celestial phenomena across multi-survey data. A study by Jamal and Bloom (2020) [18] compared various neural network architectures, including RNN, dilated temporal convolutional neural networks (dTCNs), long-short term memory neural networks, gated recurrent units, and temporal convolutional neural networks (tCNNs), for astronomical time-series classification. This study highlighted the importance of choosing the right architecture for specific astronomical tasks. Kang et al., (2023) [19] addressed the challenge of data imbalance in classifying periodic variable stars by proposing an ensemble augmentation method combining RNN and CNN for improved accuracy. This was a significant step forward in enhancing classification accuracy despite dataset limitations. Furthermore, Allam, Peloton, and McEwen (2023) [20] demonstrated that deep compression methods could achieve a significant reduction in model size while maintaining classification performance. This improvement in model size enhances inference latency and throughput for time-critical events in real-time astronomical data analysis.\nThis indicates the potential of deep learning in enhancing real-time analyses of astronomical events. These studies illustrate the diverse applications and benefits of deep learning in the field of astronomy in the analysis and classification of time-series data related to variable stars. From improving classification accuracy to enhancing real-time event analysis, deep learning continues to push the boundaries of what is possible in astronomical research."}, {"title": "2.2. Time-series Clustering", "content": "Deep learning and machine learning have continually emerged as powerful tools in astronomical research in the analysis of time series data of variable stars. While some studies focus on using traditional machine learning and deep learning approaches for stellar classification and analysis, there's growing interest in leveraging deep learning for clustering and analyzing variable stars within astronomical time series data. A study conducted by Rebbapragada et al., (2009) [21] focused on an unsupervised anomaly detection approach tailored for extensive collections of unsynchronized periodic time-series data. It generates a prioritized catalog of both overarching and localized anomalies. The method computes anomaly scores for each light curve concerning a cluster of centroids derived from a modified k-means clustering algorithm. Thanks to the work done by researchers, it is possible to get scalability for large datasets by employing sampling techniques. The performance has been validated on various datasets, including light-curve data, showcasing its proficiency in identifying known anomalies. Notably, David J. Armstrong et al., (2015) [22] implemented innovative methodologies, including Kohonen Self-Organizing Maps (SOMs) and Random Forest (RF) techniques, for classifying variable stars in the K2 mission fields. Further, the study from Mackenzie, Pichara, and Protopapas (2016) [23] focuses on the automatic classification of variable stars using clustering algorithms, which significantly hinges on how light curves are represented. The research introduces a novel feature learning algorithm tailored for variable objects. It initiates by extracting numerous light curve subsequences, followed by clustering these to discover common local patterns in the time series. Subsequently, it utilizes representatives of these common patterns to transform the light curves into a new representation. This new data representation is then employed to train a classifier. The novelty of the approach lies in its ability to learn features from both labeled and unlabeled light curves, thereby mitigating the bias inherent in using only labeled data for feature learning. Moreover, the approach illustrated by Valenzuela and Pichara (2018) [24] introduces an unsupervised learning approach for classifying variable stars, leveraging the similarity among light curves to categorize them. This represents a significant advancement in the field of astronomy, particularly in the automated classification of variable stars, by relying on the intrinsic patterns present in the light curves without the need for labeled data. The methodology proposed in Valenzuela's work aligns with the broader efforts in astronomy to employ machine learning techniques for efficient data analysis given the massive volumes of data generated by astronomical surveys. For example, the use of unsupervised learning algorithms to classify the chemistry of long-period variable stars based on BP and RP spectra from Gaia Third Release (DR3) shows the versatility of unsupervised methods in identifying complex astronomical phenomena as described by Sanders et al., (2023) [25].\nThese studies exemplify the evolving landscape of astronomical research, where both machine learning and deep learning are employed to tackle the intricate task of analyzing and classifying variable stars through time series data. The application of these technologies opens new avenues for exploring and understanding the dynamic behaviors of 'resolved' stars within our Galaxy and beyond."}, {"title": "2.3. Time-series Regression", "content": "The utilization of deep learning in astronomy, particularly for time series regression involving variable stars, presents promising advancements across various facets of stellar study. Surana et al., (2021) [26] explored the prediction of crucial star formation properties like stellar mass, star formation rate, and dust luminosity leveraging deep learning techniques as an alternative to traditional stellar population synthesis models. In the chase for uncovering variable stars, Noughani and Kotulla (2020) [27] harnessed a decade-long dataset from the Sloan Digital Sky Survey telescope to identify variable stars, accurately estimate their variability periods, and elucidate the shape of their brightness fluctuations over time. This work is integral to propelling forward the field of time-domain astrophysics. RNN have also found applications in the astronomical domain, as showcased by Flores et al., (2022) [28], who utilized RNNs to estimate physical parameters of O-type stars (hot, blue-white stars of spectral type O) in the optical region of stellar spectra, exemplifying the versatility of deep learning models in stellar parameter estimation. In a groundbreaking study by D\u00e9k\u00e1ny and Grebel (2022) [29], deep learning, specifically the deployment of long short-term memory recurrent neural networks, was applied to the task of regressing the metallicity abundance from time-series light curves observed in Gaia's optical G and near-infrared VISTA K5 bands. This methodology allowed for the processing of serialized data inherent to light curves, resulting in low mean absolute errors and high regression performance, making it a significant advancement in the metallicity estimation of RR Lyrae stars based on their photometric observations. Complementing this work, a study by D\u00e9k\u00e1ny, Grebel, and Pojma\u0144ski (2021) [30] further explored the metallicity prediction capabilities through machine learning. By employing a combination of machine-learning methods alongside Bayesian regression, they established empirical relationships between metallicity abundance and various light-curve parameters for RRab and first-overtone (RRc) stars. This approach achieved mean absolute prediction errors of 0.16 dex and 0.18 dex, respectively, demonstrating the precision possible when combining statistical models with analysis of stellar light curves.\nCollectively, these studies manifest the transformative impact of deep learning on the astrophysics field concerning time series regression tasks for variable stars. By enabling more nuanced analysis, improved accuracy in parameter estimation, and metallicity estimation tasks, deep learning tools are proving essential in pushing the boundaries of our astronomical understanding and capabilities."}, {"title": "3. Photometric Data and Data Pre-processing", "content": "Building on the preceding section, our objective aligns with the domain known as Time-Series Regression 2.3. Specifically, we focus on a variant termed Time-Series Extrinsic Regression (TSER), where the goal is to learn the relationship between time-series data (photometric light curves) and a continuous scalar value (metallicity value referring to the stellar object).\nTo delve deeper, as outlined in the paper by Tan et al., (2021) [31], this task relies on the entirety of the series rather than emphasizing recent over past values, as seen in Time-Series Forecasting (TSF). The contrast between Time-Series Classification (TSC) and TSER lies in TSC's mapping of a time series to a finite set of discrete labels, whereas TSER predicts a continuous value from the time series. Therefore, formally speaking, the definition of TSER mirrors the one elucidated in the aforementioned paper:\nA TSER model is represented as a function T \u2192 R, where T denotes a set of time series. The objective of time-series extrinsic regression is to derive a regression model from a dataset D = {(t1, r1), ..., (tn, rn)}, where each ti represents a time series and ri represents a continuous scalar value.\nKeeping this in consideration, our initial step involves retrieving the dataset from the Gaia Data Release 3 (DR3) catalogue of RR Lyrae stars (Clementini et al., 2023) [15], for which we have photometric metallicity estimates (Muraveva et al., 2024 [32]). Specifically, we have applied the following selection criteria:\n\u03c3[Fe/H] \u2264 0.4 dex; AmpG < 1.4 mag; Nep >= 50; \u03c3\u03c631 \u2264 0.10;\nHere, [Fe/H] represents the uncertainty associated with the metallicity value, AmpG denotes the peak-to-peak amplitude of the light curves, Nep signifies the number of epochs in the G-band, and 0431 parameter of the Fourier decomposition of the light curve. Additionally, it is verified that the phase is less than 1.0 and that the period values are not missing. Other data values are ensured to be complete, as they are sourced from the Gaia DR3 catalogue. Our final sample consists of 6002 RRab stars, with 4801 sample using for the training set to develop and refine our models, and 1201 sample assigned to the validation set to evaluate and validate the model's performance.\nTherefore, we have applied a fundamental step in the analysis of variable stars, which is essential for extracting meaningful information from observational data. Phase folding and alignment are techniques commonly used in the study of variable stars, particularly those with periodic variability such as pulsating stars:\n\u2022 Phase Folding: In phase folding, observations of a variable star's brightness over time are transformed into a phase-folded light curve. This involves folding the observations based on the star's known or estimated period. The period is the duration of one complete cycle of variability, such as the time it takes for a star to pulsate or undergo other periodic changes. By folding the observations, multiple cycles of variability are aligned so that they overlap, simplifying the analysis of the star's variability pattern. This technique allows astronomers to better understand the periodic behavior of variable stars and to compare observations more effectively.\n\u2022 Phase Alignment: Alignment refers to the process of adjusting or aligning multiple observations of a variable star's light curve to a common reference point. This is particularly important when studying stars with irregular or asymmetric variability patterns. By aligning observations, astronomers can more accurately compare the shape, timing, and amplitude of variations in the star's brightness. This helps in identifying patterns, detecting periodicity, and studying the underlying physical mechanisms driving the variability. Therefore, RRab type stars have a sawtooth-shaped light curve, which is indeed asymmetric, with a rapid rise and a slow decline. For these kind of stars, the phase alignment mentioned is particularly important.\nBy applying the well-known formula in variable stars literature to all light curves within the previously selected catalog:\n$phase = (\\frac{T - Epoch_{max}}{P}) mod (\\frac{T \u2013 Epoch_{max}}{P})$\nwhere T represents the observation time and Epochmax denotes the epoch at the maximum light of the source during the pulsation cycle that is expressed in Barycentric Julian Day (BJD) for Gaia's sources., P is a pulsation period. As shown in Figure 1, following the application of the aforementioned method, our dataset for training deep learning models is composed of 6002 G-band light curves of RR Lyrae stars.\nOnce the data has been phase-folded, a method called smoothing spline has been applied, using the SciPy library , to minimize fluctuations, noise, outliers and obtain the same number of points for each light curve.\nMore in-depth, smoothing spline is a method used for fitting a smooth curve to a set of data points effectively balancing between accurately representing the data and minimizing fluctuations or noise. In our case, the data to consider consists of light curves composed of magnitude and phase, with a different number of points for each light curve. It involves finding a function that passes through the given data points while minimizing the overall curvature or roughness of the curve. This technique is particularly useful in situations where the data may contain random variations or noise, allowing for a clearer representation of underlying trends or patterns. The function for a smoothing spline typically involves minimizing the sum of squares of the deviations of the fitted curve from the data points, subject to a constraint on the overall curvature of the curve. Mathematically, the function can be represented as:\n$\\sum_{i=1}^{n} (y_i- f(x_i))^2 + \\lambda \\int (f''(x))^2 dx$\nwhere f(x) is the smooth function being fitted to the data points, xi, Yi are the data points, A is a smoothing parameter that controls the trade-off between fidelity to the data and smoothness of the curve, and $\\int (f''(x))^2 dx$ represents the integrated squared second derivative of the function, which penalizes high curvature. As shown in Figure 2, this is the set of light curves that represent the dataset pre-processed using the aforementioned method.\nFigure 3 illustrates the distribution of metallicity within the dataset, highlighting a significant imbalance. Their metallicity distributions exhibit pronounced peaks around -1.5 dex, with relatively subdued tails on both the metal-rich and metal-poor ends. This is not due to a bias in our dataset, but it respects the metallicity distribution of RR Lyrae ab stars in our galaxy as described in [33]. To address this imbalance, we introduced density-dependent sample weights for training our regression models. Specifically, we computed Gaussian kernel density estimates of the [Fe/H] distributions, assessed them for each object in the development sets, and assigned a density weight wa to each data point by inversely proportional to the estimated normalized density.\nFinally, for the predictive modeling of the [Fe/H] from the light curves, we use a dataset formed from the following two-dimensional sequences as input variables:\n$X^{<t>} = \\{ \\frac{m^{<t>} \u2013 < m >}{P h}, P \\} t = \\{1, \u2026, Nep\\}$\nwhere m<sup> is the magnitude of each data point,  is the mean magnitude, Ph is the phase and P the period. To verify the actual contribution of the preprocessing phase, three different datasets were created. The first dataset is based on phase-folded and phase-aligned data that have not undergone any pre-processing, which we will refer to for the sake of brevity as raw data. In this case, we have applied padding and masking methods to ensure the input tensor has the correct timestep shape. Padding is a specific form of masking where the masked steps are placed at the beginning or end of a sequence. It is necessary to pad or truncate sequences to standardize their length within a batch, allowing for contiguous batch encoding of sequence data. Masking, on the other hand, informs sequence-processing layers that certain timesteps in the input are missing and should be ignored during data processing. The second dataset is derived from the raw data with the application of the smoothing spline method, but without incorporating the mean magnitude as described in equation 3. The third dataset consists of the fully pre-processed data."}, {"title": "4. Metodology", "content": "This section explores model selection and optimization choices applied to Deep Learning models to predict metallicity values from photometric light curves. Furthermore, a detailed description of each deep learning model follows. In total, nine models were tested."}, {"title": "4.1. Model Selection and optimization", "content": "Optimizing a predictive model involves two critical phases: training and hyperparameter optimization. During training phase, the model's parameters are fine-tuned by minimizing a cost function using a dedicated training dataset. For neural networks, this optimization typically employs gradient descent-based algorithms, leveraging the explicit expression of the cost function gradients. Hyperparameters, which dictate the model's complexity-such as layer count, neuron density, and regularization methods\u2014are predefined during training phase. Their optimal configurations are determined by maximizing a performance metric on a separate validation dataset, ensuring the model's effectiveness extends beyond the training data. Indeed, GridSearchCV method from Scikit-learn was exploited for each of the nine created architectures. The hyperparameters explored included dropout rates of [0.1, 0.2, 0.4, 0.6], learning rates of [0.001, 0.01, 0.1], and batch sizes of [32, 64, 128, 256, 512]. For training phase, we used the Mean Squared Error (MSE) cost function with sample weights, as discussed in the previous Section. To prevent overfitting the model to the training set, we experimented with different methods such as kernel regularization (L1 and L2) and dropout layers. By systematically evaluating the impact of various hyperparameters on model performance through GridSearch tecnique, we aimed to identify the configuration that yields the highest accuracy and generalizability.\nThe metrics optimized during hyperparameter tuning must precisely mirror how effectively the trained model performs on unseen data, such as the validation set or test set. Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), weighted RMSE (wRMSE), and weighted MAE (wMAE) as evaluation performance metrics were used. These metrics are essential tools for assessing the accuracy and performance of regression models, providing valuable insights into how well the models are performing and where improvements may be needed. Another objective is to minimize the mean prediction errors, a goal accomplished by maximizing the coefficient of determination (R2 score).\nR2 = 1 - $\\frac{SS_{res}}{SS_{tot}}$\nwhere: SSres (Residual Sum of Squares) is the sum of the squares of the residuals (the differences between the observed and predicted values). SStot (Total Sum of Squares) is the total variability in the dependent variable, calculated as the sum of the squares of the differences between the observed values and the mean of the observed values. The R2 value ranges from 0 to 1, with 1 indicating perfect prediction and 0 indicating that does not explain any of the variability in the dependent variable. Higher R2 values suggest a better model fit.\nMoreover, cross-validation is essential for estimating the performance of a model on unseen data. We have chosen Repeated Stratified K-Fold Cross-Validation that extends traditional K-Fold Cross-Validation by incorporating both stratification and repetition. Through this validation method, the dataset is divided into K folds while preserving the class distribution in each fold. This ensures that each fold is representative of the overall dataset, particularly crucial for imbalanced datasets, as in our case (see Figure 3). Further enhances robustness by repeating the process multiple times with different random splits. This helps to reduce the variance of the estimated performance metrics, providing more reliable insights into the model's generalization capabilities. This method is particularly useful when dealing with small or imbalanced datasets, as in our specific case, where the performance estimation can be sensitive to the random partitioning of the data.\nMoreover, we employed the Adam optimization algorithm with a learning rate of 0.01 for training each model, determining the optimal early stopping epoch based on the specific network type. To ensure a comprehensive representation of training examples across our entire [Fe/H] range, we utilized a sizable mini-batch equal to 256. All code is available within the open source GitHub repository 2."}, {"title": "4.2. Choosing the Right Neural Network Architecture for Time-Series Data", "content": "When working with time-series data, the choice of model architecture\u2014Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), or a mixed architecture combining both-depends on the specific characteristics of the data and the goals of the analysis. Each of these architectures has its strengths and weaknesses, and understanding these can help you choose the most appropriate one for your task.\nCNNs are adept at detecting local patterns in data through convolutional filters. For time-series data, these patterns could be short-term trends or repeated cycles. Convolutional layers can automatically learn to identify important features such as peaks, troughs, and periodicity, which are useful for tasks like anomaly detection or classification. Unlike RNNs, CNNs do not rely on sequential processing, making them more efficient to train, especially on long sequences. This allows for parallel processing of data, speeding up training and inference. Additionally, CNNs can capture hierarchical patterns by stacking multiple convolutional layers, which is beneficial for capturing complex patterns in time-series data that span different time scales. CNNs are commonly used in signal processing, anomaly detection, and time-series classification.\nRNNs are designed to handle sequential data and maintain temporal dependencies through their recurrent connections, making them well-suited for tasks where the order of data points is crucial. Variants of RNNs like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) are capable of learning long-term dependencies, which are important for time-series data with long-term trends or patterns. RNNs can model the dynamics of time-series data over time, making them suitable for tasks like forecasting and sequential prediction. They are typically used in time-series forecasting, language modeling, and sequential prediction.\nA mixed architecture leverages the strengths of both CNNs and RNNs: the feature extraction capabilities of CNNs and the temporal modeling abilities of RNNs. CNN layers can be used to extract local features from time-series data, which are then fed into RNN layers to capture temporal dependencies. Combining CNNs and RNNs can lead to improved performance on complex tasks by capturing both local patterns and long-term dependencies. This architecture is particularly useful when the time-series data has hierarchical patterns (e.g., short-term fluctuations and long-term trends). A mixed architecture provides flexibility in designing models tailored to specific tasks, such as multi-step forecasting, sequence classification, or anomaly detection. Example use cases for mixed architectures include multivariate time-series forecasting, complex sequence modeling, and hierarchical pattern recognition."}, {"title": "4.3. Network architectures", "content": "In this section, the network architectures chosen and implemented\u2014specifically Convolutional Neural Networks (CNNs), Recurrent Neural Networks, and their hybrid models\u2014are explained, along with their functionality in the context of regression with time series data. CNNs, while traditionally used for spatial data like images, can be adapted to capture local temporal patterns in time series data, enhancing the feature extraction process. RNNs, designed to handle sequential data, excel at capturing temporal dependencies and trends crucial for accurate time series forecasting. Finally, integrating CNNs and RNNs into hybrid models merges the strengths of both architectures."}, {"title": "4.3.1. Fully Convolutional Network", "content": "A Fully Convolutional Network (FCN) is designed for tasks like semantic segmentation [34] by using only locally connected layers, such as convolution, pooling, and upsampling layers, eliminating fully connected layers. This architecture reduces parameters, speeds up training, and handles varying input sizes. An FCN's core structure includes a down-sampling path for context capture and an upsampling path for spatial precision, with skip connections to retain fine spatial details. FCNs excel in pixel-level predictions, transforming classification networks into fully convolutional ones that produce dense output maps, and optimizing computations through overlapping receptive fields for efficient feedforward and backpropagation across entire images. FCNs are not limited to image data; they can also be effectively applied to time series data for various regression and classification tasks. The architecture of FCNs allows them to capture temporal dependencies and patterns within time series data by using convolutional layers to process sequences of data points. For time series data, FCNs adapt the convolutional layers to operate along the temporal dimension, capturing local dependencies and patterns within the sequence. The core elements of an FCN for time series analysis are (i)Temporal Convolutional Layers, which apply convolutional filters along the temporal axis to extract features from the sequence data. Using multiple filters enables the network to capture diverse aspects of the time series, and (ii) Pooling Layers, which reduce the temporal dimension by summarizing the information and lowering the computational burden.\nIn our architecture, the foundational unit consists of a 1D convolutional layer, succeeded by a batch normalization layer [35] and a ReLU activation layer. The convolution is performed using three 1D kernels of sizes 8, 5, and 3, applied without any striding. Hence, the convolutional block is structured as Convolution Layer \u2192 Batch Normalization Layer \u2192 ReLU Activation Layer. This arrangement allows the network to effectively extract and normalize features from the input time series data while maintaining non-linearity. More formally, the foundational unit consists of:\ny=Wx+b\ns = BN(y)\nh = ReLU(s)\nwhere is the convolutional operator. The implementation on the final network is based on stacking three convolutional blocks, each containing filters of sizes 128, 256, and 128, respectively. Following these convolutional blocks, the extracted features are passed through a global average pooling layer, significantly reducing the number of weights compared to a fully connected layer. The final output for regression is generated using a dense layer with a linear activation function."}, {"title": "4.3.2. InceptionTime", "content": "InceptionTime is a deep learning architecture tailored for time series regression, classification, and forecasting, inspired by Google's Inception architecture [36]. It incorporates multiple parallel convolutional layers within its modules, akin to Inception, to capture features at diverse scales. These layers are adept at discerning both short-term and long-term patterns in the time series data. Additionally, InceptionTime often integrates dilated convolutions to extend the receptive field without significantly increasing parameters. This feature proves valuable in capturing temporal dependencies over extended time spans. Residual connections are sometimes employed within the InceptionTime architecture. These connections facilitate gradient flow during training and mitigate the vanishing gradient problem, particularly in deeper networks. Furthermore, temporal pooling layers, such as global average pooling, are commonly used to aggregate information across the temporal dimension before making predictions. This step reduces data dimensionality while preserving essential temporal information. InceptionTime's strength lies in its scalability and adaptability. It can be tailored to various time series tasks and can accommodate time series data of different lengths and sampling rates.\nThe architectural framework draws inspiration from the seminal works of Fawaz et al., (2019, 2020) [37,38], where they introduced the groundbreaking InceptionTime network. This network represented a substantial advancement over existing deep learning models, achieving competitive performance comparable to the state-of-the-art TSC model. The architecture of InceptionTime revolves around the integration of two distinct residual blocks, strategically interconnecting the input and subsequent block inputs to address the challenge of vanishing gradients. Each residual block is structured with three Inception modules, each comprising two key components. The first component entails a bottleneck layer that serves a dual purpose: reducing the dimensionality of the time series using m filters and enabling InceptionTime to employ filters ten times longer than those in Residual Network, as elucidated in [38]. The second component involves the application of multiple filters of varying lengths to the output of the bottleneck layer. Simultaneously, a max-pooling layer is applied to the time series in parallel with these processes. The outputs from both the convolution and max-pooling layers are concatenated to form the output of the Inception module. Finally, global average pooling is applied to the final residual block, followed by propagation to a dense layer for regression analysis."}, {"title": "4.3.3. Residual Network", "content": "ResNet, short for Residual Network, is a deep neural network architecture that has revolutionized the field of computer vision since its introduction by Kaiming He et al., [39] in their paper \"Deep Residual Learning for Image Recognition\" in the ImageNet Large-Scale Visual Recognition Challenge 2015 (ILSVRC2015). At its core, ResNet addresses the problem of vanishing gradients and degradation in training deep neural networks. As networks become deeper, they tend to suffer from diminishing performance gains and may even degrade in accuracy due to difficulties in optimizing the network's parameters. This phenomenon arises because deeper networks are more prone to the vanishing gradient problem, where the gradients become increasingly small during backpropagation, hindering effective training. To overcome this challenge, ResNet introduces skip connections, also known as residual connections, that directly connect earlier layers to later layers. These skip connections allow the network to bypass certain layers, enabling the direct flow of information from the input to the output. By doing so, ResNet mitigates the vanishing gradient problem and facilitates the training of extremely deep networks. The key innovation of ResNet lies in its residual blocks, which consist of several convolutional layers followed by identity mappings or shortcut connections. These residual blocks enable the network to learn residual functions, capturing the difference between the desired output and the input to the block. This residual learning approach enables the network to focus on learning the residual features, making it easier to optimize and train deep networks effectively. While ResNet is often associated with classification tasks, it can seamlessly adapt to regression tasks by utilizing appropriate loss functions such as Mean Squared Error (MSE) or Mean Absolute Error (MAE). These loss functions quantify the discrepancy between predicted and actual continuous values, guiding the training process toward minimizing prediction errors.\nIn our architectural design, we repurpose the foundational units established in Equation 5 to assemble every residual block. Let Blockk denote the foundational unit with k filters. The formulation of the residual block is thus articulated as follows:\nh_0 = Block_{k0} (x)\nh_1 = Block_{k1} (h_0)\nh_2 = Block_{k2} (h_1)\ny = x + h_2\nh = ReLU(y)\nwhere the number of filters k\u2081 = {64, 64, 64}. The ultimate Residual Network configuration comprises three sequential residual blocks, succeeded by a global average pooling layer and a dense layer with a linear activation."}, {"title": "4.3.4. Long Short-Term Memory and Bi-directional Long Short-Term Memory", "content": "The LSTM-based models are an extension of RNNs, which are able to address the vanishing gradient problem in a very clean way. The LSTM models essentially extend the RNNs' memory to enable them to keep and learn long-term dependencies of inputs [40,41"}]}