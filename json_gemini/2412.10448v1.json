{"title": "Unlocking Visual Secrets: Inverting Features with Diffusion Priors for Image Reconstruction", "authors": ["Sai Qian Zhang", "Ziyun Li", "Chuan Guo", "Saeed Mahloujifar", "Deeksha Dangwal", "Edward Suh", "Barbara De Salvo", "Chiao Liu"], "abstract": "Inverting visual representations within deep neural networks (DNNs) presents a challenging and important problem in the field of security and privacy for deep learning. The main goal is to invert the features of an unidentified target image generated by a pre-trained DNN, aiming to reconstruct the original image. Feature inversion holds particular significance in understanding the privacy leakage inherent in contemporary split DNN execution techniques, as well as in various applications based on the extracted DNN features. In this paper, we explore the use of diffusion models, a promising technique for image synthesis, to enhance feature inversion quality. We also investigate the potential of incorporating alternative forms of prior knowledge, such as textual prompts and cross-frame temporal correlations, to further improve the quality of inverted features. Our findings reveal that diffusion models can effectively leverage hidden information from the DNN features, resulting in superior reconstruction performance compared to previous methods. This research offers valuable insights into how diffusion models can enhance privacy and security within applications that are reliant on DNN features.", "sections": [{"title": "I. INTRODUCTION", "content": "Inverting visual features within DNNs presents a significant challenge in the realm of privacy for deep learning. The primary goal of feature inversion is to reverse the outputs (or intermediate results) of a pre-trained DNN and reconstruct the original image. This form of privacy attack, known as feature inversion attack, can raise privacy concerns across various domains. Modern systems that perform face recognition [1], [4], [56], [6], [8], [32], AR/VR applications [40], [20], [72], [11], and image or text retrieval [74], [36], [37], [58], [10] often store and process auxiliary data in the form of extracted features from the original input. For example, in a face recognition system, the human face is first encoded with a DNN encoder (e.g., FaceNet [56], CLIP [8], [57]) and the resultant feature vector is then searched over the database for identity matching via vector comparisons. Feature inversion attacks can be used to reconstruct the face of private users [43]. Moreover, feature inversion attack also leads to a serious privacy leakage in the Split DNN computing paradigm [22], [48], [28], [61], [62], [71], [7], [15], [29], [38], [47], [31], [19], [69], [14], [27], [44], [45], [2], [3]. Within this paradigm, a layer-wise partitioning of the pretrained DNN into two or more blocks, aligning with the computational capabilities of the edge devices, as shown in Figure 1. During the execution, the user data is first processed using one or more local DNNS that contain the initial layers. The intermediate results are then transmitted to the central server for the execution of subsequent DNN layers. Split DNN computing has been widely adopted to accommodate the execution of increasingly large DNN on resource-constrained devices like mobile phones, and is believed to enhance user privacy by keeping user data on the local device-only the intermediate features are sent to the less secure cloud environment. However, this privacy enhancement turns out to be frail, as recent studies have shown that the intermediate features can be inverted via feature inversion attacks to reconstruct user inputs from the intermediate outputs of parts of the DNN [42], [17], [23], [16], [41], [58], [46]. The broad applicability of feature inversion renders it a fundamental problem in ML security and privacy. On the other hand, feature inversion is not an easy task, particularly when dealing with features extracted from later layers of a network. Intuitively, the learned feature contains more high-level semantic information about objects in an image but less information about the raw input as depth increases. As a result, nearly all of the existing feature inversion methods fail when attempting to invert features from later layers of a deep network. This explains why much of the existing research concentrates on feature inversion for shallow DNNs with lower input resolutions [42], [17], [23], [41]. The recent advancement of generative AI (GenAI) models opens up new possibilities to improve the quality of feature inversion attacks through their comprehensive understanding of image data distributions across real-world scenes. Among the multitude of existing GenAI techniques, Diffusion Models (DMs) [24] have emerged as a remarkable breakthrough in generative modeling. Through extensive training with vast datasets comprising millions of real-world images, DMs obtain a high-quality, photorealistic image generation capability. In this work, we demonstrate that recent advancements in DMs can be utilized to greatly enhance feature inversion. Instead of inverting DNN features directly to image pixels,"}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "A. Diffusion Models\nDiffusion models (DMs) [24] have recently gained significant attentions for its remarkable ability to generate diverse photorealistic images. It is a parameterized Markov chain trained through Variational inference to generate samples that match the data distribution over a finite duration. Specifically, during the forward process of DMs, given an input image XO ~ q(x), a series of Gaussian noise is generated and added to the xo, resulting in a sequence of noisy samples {xt}, 0 \u2264 t \u2264 T.\nq(xt|xt\u22121) = N(xt; \u221a1 \u2013 \u1e9etxt\u22121, \u1e9etI)\n(1)\nIn this paper, we will employ the terms \"attacker\" and \"adversary\" interchangeably.\nB. Feature Inversion Attacks against DNNs\nFeature inversion has been studied by various literature. [17] showed that DNN features can be inverted by training a network to reconstruct the corresponding input images given their features. [23] first demonstrated that feature inversion can lead to a leakage of private input for split DNN computation, and further showed that introducing the total variation loss [53] can greatly improve feature inversion quality. [16] revealed that the exposure of batch normalization parameters can lead to a significant enhancement in feature inversion quality. Unsplit [18] operated within a black-box setting where the attackers lack knowledge of the model parameters, and developed techniques aimed at reconstructing both user inputs and model parameters.", "equations": ["q(xt|xt\u22121) = N(xt; \u221a1 \u2013 Btxt\u22121, \u1e9etI)"]}, {"title": "III. THREAT MODELS", "content": "We begin by first describing the underlying threat model for our feature inversion attack. We consider two settings, white-box and black-box, for the two variants of our attack.\nA. Threat Model for White-Box Settings\nWe focus on a scenario where the target model F(.) is divided into two parts: F(.) = F2 \u00b0 F\u2081. Here, we use xgt to represent the user input and Zmid to denote the intermediate feature. We make the assumption that F\u2081, termed user model, is executed within a secure environment (e.g., edge device) where the intermediate data within F\u2081 is protected from any potential leaks to external parties. In contrast, F2 operates within an insecure environment, allowing an attacker to access its input Zmid = F1(Xgt). This assumption is reasonable in the context of a split computing scenario such as cloud-edge environment, as the user has control over and can establish trust in the local device's operations. However, in an untrusted cloud environment where either a cloud provider cannot be fully trusted or shared cloud systems have security vulnerabilities, attackers have the potential to exploit vulnerabilities and gain access to the activations for F2. Although it is feasible to encrypt the intermediate results, Zmid, for transmission between the edge and cloud, we operate under the assumption that within the insecure public environment (e.g., cloud), the encrypted Zmid must be decrypted before further execution. This introduces vulnerabilities where adversaries could potentially invert Zmid and reconstruct the original user input Xgt. For the white-box setting of the feature inversion attack, we assume the adversary has access to the model structures and parameters of F\u2081, and has no prior knowledge of the input Xgt, nor any intermediate values within F1. Our goal is to reconstruct the input that produces intermediate outcomes resembling Zmid, which can be formulated as follows:\nXre = arg min Lre(F1(x), Zmid),\n(4)\nwhere Lre(...), referred to as the reconstruction loss, represents the loss function employed for measuring similarity, with the 12 distance being used in this study. We construct", "equations": ["Xre = arg min Lre(F1(x), Zmid)"]}, {"title": "IV. WHITE-BOX FEATURE INVERSION", "content": "In this section, we describe the white-box attack methodologies in details. We leverage the prior knowledge embedded within the LDM to reconstruct the user input Xgt. Let D(v, e) represent the generating function of the LDM. Here, v denotes the input latent variable, which is expected to follow a normal distribution [24], and e = E(t) represents the text embedding. The function E(.) indicates a pre-trained text encoder, and t corresponds to the text input provided by the user. In this section, we ignore the text prompt by setting the text embedding e to a vector of zeroes, and will examine the influence of the text prior in Section IV-A. We then search for the input latent variable v that allows the LDM to produce a synthetic output, denoted as D(vn). This output, when passed to F\u2081(.), will result in a similar intermediate output as Zmid (Figure 4).\nv* = arg min Lre(F1(D(Un)), Zmid) + AsTV(D(vn))\n(6)\nAs the LDM necessitates input data to approximate a normal distribution for photorealistic image generation, we implement a soft restriction on the variable v by normalizing it prior to forwarding it to the LDM. Specifically, we define vn = v-mean(v) as the normalized version of v, which serves as the input for the LDM. We observe that applying normalizing std(v) operation can greatly enhance the feature inversion performance. Zmid = F1(xgt) is the intermediate result generated from the user input. TV(.) represents the Total Variation [53] which is used to reduce the abrupt pixel variations across the reconstructed image. TV(x) is defined as follows:\n1\nTV(x) = \u039c\u039d\u03a3\u03a3(|X+1,j-xi,j|2+|Xi,j+1-Xi,j|2)\n(7)\ni j\nwhere M and N represent the spatial size of the image, and As denote the weight of the TV loss. The feature inversion process is summarized in Algorithm 1.\nA. White-box Inversion with Textual Prior\nAnother important characteristic of LDMs is their ability to take text prompt as input and produce synthetic outputs guided by textual descriptions. We leverage this capability in our feature inversion attacks by allowing the attacker to express their prior knowledge about the user input in the form of natural language. Different from generic image priors such as total variation, this form of text prior can be specific to each target image and further enhances the quality of feature inversion using diffusion models.", "equations": ["v* = arg min Lre(F1(D(Un)), Zmid) + AsTV(D(vn))", "TV(x) = \u039c\u039d\u03a3\u03a3(|X+1,j-xi,j|2+|Xi,j+1-Xi,j|2)"]}, {"title": "B. White-box Multi-frame Reconstruction", "content": "In this section, we explore the problem on multi-frame feature inversion. This scenario closely resembles real-world situations where edge devices handle a continuous stream of input frames, such as burst mode photos or video clips. In this context, the local DNN processes consecutive input frames that exhibit temporal correlation, the intermediate features are then transmitted to cloud servers for subsequent processing. The goal is to reconstruct the entire input image sequence using the intermediate results. In particular, consider Xgt,k, where 1 \u2264 k \u2264 K, to represent a sequence of K user inputs. Additionally, let Zmid,k and Uk represent the corresponding local DNN output and input latent variable for Xgt,k. We introduce an additional loss component Lc(.) aimed at minimizing the disparity among the latent vectors uk across these frames. To achieve this, the multi-frame reconstruction process can be realized by solving the following optimization problem:\nmin\nUk,1<k<K\nK\n\u2211[Lrek +\nAsTVk + AcLc(Uk, 0)],\n(10)\nk=1\n1\nwhere 0 = (\u03a3\u039a=10k) represents the average of the input\nK\nl latent vectors across the K frames. The loss function Le is utilized to minimize the disparity between the latent vectors for each frame. In this study, we have observed that simply minimizing their 12 distance yields an excellent reconstruction quality. The parameter \u5165 serves as a weight to balance the importance of these two loss functions. Lre,k and TVk denote the reconstruction loss and total variation loss for reconstructing Xgt,k. The detailed algorithm is given in the appendix.", "equations": ["min\nUk,1<k<K\nK\n\u2211[Lrek +\nAsTVk + AcLc(Uk, 0)]"]}, {"title": "V. BLACK-BOX FEATURE INVERSION", "content": "For feature inversion attacks with black-box settings, the attacker obtains access to the input queries and their corresponding outputs from F\u2081(.), as depicted in Figure 3 (a). Let Xq = {xq} denote a set of input queries sent by the adversary and Yq = {yq} represent the corresponding outputs from F\u2081(.). The adversary then proceeds to train an inversion DNN Finu (.) designed to take yq as input and generate x that closely resembles xq (Figure 3 (b)). Finv(.) consists of two major components: a pre-trained LDM and an U-Net, which are denoted as D(.) and Fu(.), respectively. During the execution, Fu(.) takes the intermediate data yq and generates the input latent variable for the LDM, which then produces the result x = D(Fu(yq)). The training of the inversion DNN model involves minimizing the following loss function:\n0 = arg min\n\u03b8\u03b1\n\u2211\n(xq, yq)\u2208 {(Xq, Yq)}\nLre(D(Fu(yq)), xq) + dsTV\n(11)\nwhere du represents the parameters of Finu(.). TV loss is introduced over the reconstructed input D(Fu(yq)).", "equations": ["0 = arg min\n\u03b8\u03b1\n\u2211\n(xq, yq)\u2208 {(Xq, Yq)}\nLre(D(Fu(yq)), xq) + dsTV"]}, {"title": "VI. EVALUATION RESULTS FOR WHITE-BOX INVERSION", "content": "In this section, we present detailed evaluation of the white-box feature inversion technique described in Section IV. We first evaluate the quality of the inverted features over different applications in Section VI-B, Section VI-C and Section VI-D. Next, we explore the influence of the textual context in Section VI-E and the multi-frame reconstruction in Section VI-F. Lastly, we conduct an ablation study on the number of diffusion sampling steps in Section VI-G.\nA. Experiment settings\nDatasets and models: We assess our feature inversion approach outlined in Algorithm 1 on ImageNet [12] and COCO [34] datasets. We employ various DNN architectures"}, {"title": "VII. RESULTS FOR BLACK-BOX FEATURE INVERSION", "content": "Datasets and models: We use the same datasets, target models and LDM as described in the Section VI-A. To build the training dataset and test dataset of inversion DNN, we randomly select 4096 and 1024 images from the training and test datasets of either ImageNet or YOLO, respectively. We notice that a training data size of 4096 is enough for inversion DNN to generalize well. Hyperparameters: The inversion DNNs are trained over 96 epochs using a batch size of 128. We assign A, values of 1 in equations 11, 12, and 13. We employ the Adam optimizer with an initial learning rate of 0.1 and \u1e9e values of (0.9, 0.999). Baseline: Following the baseline setups outlined in VI-A, we examine the black-box versions of DO and DB. In the case of DO (Figure 16 (a)), we modify the architecture of the inversion DNN to directly reconstruct the user input x without relying on the LDM. Conversely, for DB (Figure 16 (b)), we integrate the decoder from the LDM into the inversion DNN to improve the quality of reconstruction. We change the structure of the inversion DNNs for DMB, DO and DB to ensure they contain an equal number of parameters. DO has been applied by the [23], [16], [41] for feature inversion attack under the black-box setting, but DB has not been studied in prior works."}, {"title": "VIII. DISCUSSION", "content": "In this section, we summarize some findings we observe from the evaluation results (Section VIII-A). We then discuss the potential defense strategies in Section VIII-B.\nA. Insights From the Evaluation Results\nA deeper DNN does not guarantee privacy: Based on the results outlined in Section VI-B and Section VII-B, it becomes evident that a deeper DNN does not inherently ensure privacy. For instance, as observed in Table I and Table IV, the quality of reconstructed input using the intermediate features at the 12th layer of ResNet-18 is notably inferior to that of the 24th layer output of ResNet-50. This suggests that the absolute layer depth alone does not guarantee any privacy protection. Instead, what matters is the relative layer depth within the DNN. For instance, the reconstructed quality using the outputs of a middle layer of ResNet-50 (e.g., 24) is approximately equal to that from the middle layer (e.g., 8) of ResNet-18. Transformer is harder to invert than CNN: Another trend we notice is that transformers, such as ViT, exhibit better privacy protection capabilities compared to Convolu-tional Neural Networks (CNNs). As indicated by the results in Table I and Table IV, the quality of reconstructed input using features from the middle layer (e.g., 5) of ViT-base is comparable to that obtained using features from later layers in ResNet-18 or ResNet-50. This may contribute to the fact that transformers with self-attention mechanism inherently amounts to a low-pass filter [64], this will eliminate a lot of high-frequency information within the original input image, making the intermediate features harder to invert. By contrast, CNNs typically extract information across wide frequency ranges [68], thereby retaining more essential information for feature inversion. Nevertheless, further studies are needed. Self-supervised pretrained backbone models are easier to invert: The evaluation results shown in Section VI-D and Section VII-C illustrate that pretrained backbone models with self-supervised learning, are more amenable to inversion compared to DNNs trained with supervised learning frameworks tailored for specific tasks like image classification. This is due to the fact that the SSL-pretrained backbones tend to preserve a rich set of information that can be beneficial for various downstream tasks. The pretraining process typically involves learning representations that capture meaningful patterns and structures in the input data, which can generalize well to different tasks. In contrast, DNNs trained with supervised learning using labeled datasets tend to eliminate redundant information unrelated to the task during the training process, making the input data more challenging to reconstruct. A well-constructed textual prompt can improve inver-sion performance: This work is the first work to demonstrate that additional information in another format (i.e., textual format) can be utilized to enhance reconstruction performance. Our general finding is that a simple textual description con-sisting of a few words that provides an overview of the image background, highlighting attributes like the dominant color and surroundings, can generally enhance reconstruction quality. Although applying an inaccurate textual prior will degrade the attack quality, if the attacker has multiple candidate textual descriptions, the best strategy is to exhaustively try all of them and select the one that achieves the optimal quality. While using textual priors in feature inversion is not our main focus, it is a promising area for future research. Revealing the model weight and structure can improve the quality of the attack: Based on the evaluation results presented in Table I and Table IV, it is evident that the feature inversion attack achieves higher quality results in the white-box setting compared to the black-box setting. This underscores the importance of weight and architecture of the user model in influencing the quality of feature inversion.\nB. Defense over Feature Inversion Attack\nIn this section, we explore potential defense strategies to mitigate the feature inversion attacks outlined in Section IV and Section V. One defensive strategy is to ensure that all DNN computation and communication happen over encrypted data, i.e. through cryptographic methods like secure multiparty computation (MPC) or homomorphic encryption (HE). Secure MPC allows a group of n untrusting parties to collaboratively compute a public function f(x1,x2,...,xn) over their private inputs X1,X2,..., Xn without revealing any of their secret information. If the MPC scheme is expressive enough to implement large and complex neural networks like diffusion models, the implementation often has prohibitively large communication overheads and high computational com-plexity. For example, recent work on MPC implementation of VGG16 in the WAN setting leads to 37s latency (and training can take several weeks) [63]. Similarly, HE schemes allow certain operations, such as arithmetic or boolean functions, to be applied to the ciphertext, thereby allowing privacy-preserving neural network evalua-tions without revealing sensitive information in plaintext form. But computing on ciphertexts over plaintext means both higher communication and computation costs. Additionally, HE re-quires additional computation steps like noise-management via bootstrapping. Prior implementations show significant slow-down, 300s for encryption, DNN application, and decryption (up to 30s for a 5x5 convolutional layer to a simple 5-layer MNIST network, and 127s for pooling) [21]. While these are promising and active research areas, at this time these methods are not widely deployed due to large overheads over an already computationally intense DNN architecture. Another feasible approach involves the use of differential privacy (DP) to introduce noise and obscure sensitive infor-mation. In particular, random noise e can be directly integrated into the intermediate results Zmid, with the magnitude of the noise being controlled to achieve the desired level of", "equations": ["f(x1,x2,...,xn)"]}, {"title": "IX. CONCLUSION", "content": "In this study, we demonstrate the significant performance enhancement achievable in the feature inversion process via the utilization of the diffusion model. We also highlight the potential for utilizing diverse forms of prior knowledge, such as textual information and cross-frame correlations, to further improve the reconstruction quality. From the evaluation results, we show that GenAI, with its remarkable ability to synthesize realistic and coherent data, can also be utilized to detrimentally affect individuals' lives, particularly in the context of privacy breaches. This opens up interesting future avenues in a promising direction of research."}, {"title": "X. APPENDIX", "content": "A. statement on Data Availability Due to institutional restrictions, we are unable to use the public latent diffusion model for publishing our research outcomes. Instead, we employed an LDM with an architecture highly similar to Stable Diffusion 2.1 in terms of architecture, model size and pretraining techniques. Our internal model was pretrained on the dataset collected by a third party (Shutterstock) that is not public. Regarding the dataset, it consists of 385 million images: 321 million without people and 64 million with people. In addition, we have also previ-ously conducted extensive experiments with the public Stable Diffusion 2.1, which yielded similar (and even better) results than the reported results in terms of IS, PSNR, and SSIM scores. If the paper is accepted, we intend to release the code as open source. This code will enable the integration of the public LDM for conducting feature inversion attacks.\nB. Implementation details Table VI and Table VII list the detailed settings for feature inversion described in Section IV and Section V. To initiate the training process, the latent variable v is initialized using a randomly generated vector sampled from a normal distribution with a standard deviation of 0.1."}]}