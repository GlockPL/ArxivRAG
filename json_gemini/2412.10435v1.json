{"title": "COEF-VQ: Cost-Efficient Video Quality Understanding through a Cascaded Multimodal LLM Framework", "authors": ["Xin Dong", "Sen Jia", "Hongyu Xiong"], "abstract": "Recently, with the emergence of recent Multimodal Large Language Model (MLLM) technology, it has become possible to exploit its video understanding capability on different classification tasks. In practice, we face the difficulty of huge requirements for GPU resource if we need to deploy MLLMs online. In this paper, we propose COEF-VQ, a novel cascaded MLLM framework for better video quality understanding on TikTok. To this end, we first propose a MLLM fusing all visual, textual and audio signals, and then develop a cascade framework with a lightweight model as pre-filtering stage and MLLM as fine-consideration stage, significantly reducing the need for GPU resource, while retaining the performance demonstrated solely by MLLM. To demonstrate the effectiveness of COEF-VQ, we deployed this new framework onto the video management platform (VMP) at TikTok, and performed a series of detailed experiments on two in-house tasks related to video quality understanding. We show that COEF-VQ leads to substantial performance gains with limit resource consumption in these two tasks.", "sections": [{"title": "1 Introduction", "content": "In current video understanding models, besides learning visual information from video frames, additional data signals such as texts (captions, subtitles, or on-screen texts) and audio (dialogues, background music, or sound effects) are commonly used to capture a more comprehensive understanding of the video [2, 3, 12]. Industry practices often deploy a multi-tower late-fusion architecture [11, 18], where each tower independently processes a single modality and then all modality outputs are fused to obtain a final prediction. For instance, one tower might process visual frames, another audio signals, and a third tower textual data. The separate feature representations generated by each tower are typically combined, or fused, in later layers of the model to produce a more robust and holistic classification output. This approach allows the model to leverage the complementary nature of different modalities to improve performance.\nWith the rapid advancement of large language models (LLMs) and multimodal large language models (MLLMs) [1, 14], these models have demonstrated remarkable capabilities in a broad range of applications, including question answering on image or video content [5, 15], commonsense reasoning [5, 15], and contextual comprehension [6, 15]. They excel at recognizing and interpreting complex relationships across modalities, improving the model's general capabilities in handling various multimodal tasks. However, the effectiveness of these models in highly specialized or sensitive domains is still under investigation. For example, specific tasks such as evaluating whether a video has undergone professional editing, or detecting whether a video violates platform's trust and safety (TnS) policy, require an in-depth understanding of nuanced and domain-specific characteristics. These are often hard-to-detect signals that need a fine-grained understanding beyond general visual and textual comprehension.\nTherefore, in this paper we propose COEF-VQ, which is composed of two key novelties: 1) we first design a unified Multimodal LLM framework integrating video frames, texts, and audio information, which goes beyond the existing multi-tower baseline deployed on our platform [2, 3, 9, 12] and typical industry practices [18]. This enhanced model architecture allows for more sophisticated feature fusion and deeper cross-modal interactions, boosting the video classification performance. By doing so, it alleviates the limitations of traditional multi-tower structures, making the model more sensitive and accurate in discerning domain-specific tasks. However, despite the powerful capabilities of MLLM, the large number of parameters in MLLM, which enables them to perform robust multimodal reasoning, requires substantial GPU memory and processing speed. This demand, when scaled for online deployment on platforms like TikTok, would lead to considerable GPU resource consumption, affecting both deployment costs and latency in real-time, online video service.\n2) we develop a cascade serving structure for online deployment, designed to optimize resource usage with minimal performance sacrifice. In the first stage, predictions are made using a lightweight model (usually the baseline) at the time of video posting. Based on"}, {"title": "2 Methodology", "content": "2.1 Multimodal LLM Architecture\nThe Multimodal LLM architecture, as illustrated in Figure 1, inherits the design of vision-language (VL) foundation model LLaVA-OneVision[5] combined with audio encoder Whisper-Small[12]. Several key components are integrated to process multimodal inputs, including a vision encoder for video frames, a VL modality alignment projector, a language model for both vision and text tokens, and an audio encoder for the video's audio stream.\nVision Encoder for Video Frames. For each frame in a video, the input frame Xf is passed through a vision encoder to extract its visual features. The encoder function g(; 0) transforms the frame into a visual feature representation:\nZf = g(Xf;0)\nwhere Zf captures the essential visual information from each frame, allowing subsequent layers to process this information effectively.\nModality Alignment Projector. A projector module is used to align visual and textual modalities, creating a unified representation space. This alignment enables the model to handle multiple modalities consistently, ensuring that video frames and text features are comparably represented and effectively combined. In practice, a 2-layer MLP p(; 0) yields a sequence of visual token representations:\nHf = p(Zf;0)\nLanguage Model for Vision and Text Tokens. The model processes a sequence of tokens with length L, consisting of both patch tokens Hf (derived from video frames) and text tokens Xt (representing video text, such as title or stickers). The large language model (LLM) lm(; 0) integrates these multimodal tokens, and the hidden representation of the final token in this sequence, denoted as H, encodes combined information from video frames and associated video text.\nH = lm((Hf, X\u2081); 0)\nAudio Encoder for Video's Audio. To incorporate audio information, an audio encoder a(; 0) processes the raw audio input from the video, denoted as Xa. Average pooling is invoked for yield the hidden representation of the audio signal.\nHa = avg_pooling(a(Xa;0))\nThis audio encoder extracts audio features relevant to the classification task, allowing the model to incorporate sound cues, which are particularly useful when videos include narration or other informative audio elements.\nFusion and Classification Layer. The output of the audio encoder Ha is then concatenated with the hidden representation H from the language model. This combined embedding, which merges visual, textual, and audio information, is denoted as Hcl = [H; Ha] The classification module fc1(; 0c1) is applied on top of the combined embedding Hcl. This module consists of a linear function mapping Hcl \u2208 Rd into R|Y| and a softmax function, where Y is the set of target classes.\nOur MLLM combines video frames, text, and audio into a single, cohesive representation that enables rich, multimodal understanding and captures intricate relationships across modalities, ultimately improving classification performance on complex video tasks."}, {"title": "2.2 Cascade Serving Structure", "content": "This framework, as illustrated in Figure 2, divides the inference process into two distinct stages, each with different computational requirements:\n2.2.1 First Stage: Lightweight Model for Initial Filtering. In the first stage, a smaller, resource-efficient model is used to make initial predictions as soon as a video is posted. This lightweight model, often designed with fewer parameters and optimized for speed, performs a quick assessment of the content. Here, it assigns a preliminary classification score based on the likelihood of the video containing domain-specific content (such as potentially harmful content). Additionally, metadata such as video views (VV) or user engagement metrics can be incorporated to further refine the filtering criteria. This stage serves as a gatekeeper, quickly filtering out a significant number of videos with minimal resource consumption, and reducing the workload on the subsequent, more intensive stage.\n2.2.2 Second Stage: Comprehensive Consideration with MLLM. In the second stage, only the videos that passed the initial filtering criteria are subjected to a more thorough evaluation by the MLLM. Here, the MLLM re-processes these selected videos, leveraging its full multimodal capabilities to achieve highly accurate classifications. By restricting MLLM's usage to a narrower subset of content, the two-stage process reduces the heavy usage of GPU to a significant extent if compared with predicting for all contents, while maintaining the high accuracy needed in specialized domains. Moreover, by using the first stage as a triage mechanism, this method allows the MLLM to focus on higher-risk or more ambiguous content, ensuring that its powerful but resource-intensive computations are used judiciously.\n2.2.3 Benefits and Implementation Considerations. This cascade deployment structure significantly lowers the GPU resource demands for online applications, making it possible to maintain scalability without incurring prohibitive costs. By using a smaller model for rapid pre-screening, the system reduces the number of inputs needing full MLLM processing, allowing for both cost efficiency and faster response times in high-traffic environments. Furthermore,"}, {"title": "2.3 Deploy COEF-VQ onto Video Management Platform (VMP)", "content": "As a core component of Video Recommendation System, VMP plays a significant role in processing and gatekeeping videos published by creators, to ensure that the most engaging and appropriate content is presented to users. The functions and key components of VMP are illustrated in Figure 3. When a new video is published, model scores related to various quality and policy compliance factors, such as detecting inappropriate content on platforms like TikTok, are stored in the VMP stage. These scores are subsequently processed and utilized in downstream multi-stage recommendation funnel (like Recall, Pre-rank, Rank, etc.).\nThe VMP operates as a mini-batch streaming framework specifically designed to facilitate the efficient and timely updating of video candidates. The VMP workflow consists of four sequential stages-Source, Retriever, Filter, and Sink-that collaboratively ensure that only high-quality, policy-compliant video candidates proceed to the recommendation stage.\nSource. The Source stage serves as the entry point where video data and stable features are collected and stored. Base data for each video, along with relatively stable features (e.g., metadata, upload date, initial quality assessments), are saved in a Database Server. Upon receiving new videos, the Source stage first generates video IDs by processing KAFKA messages or scanning other data sources. This ensures that each video is uniquely identifiable and can be efficiently processed in later stages.\nRetriever. In the Retriever stage, essential features and metadata are pulled from a variety of services and databases, including Thrift services, Redis caches, and traditional databases. Here, our Cascaded Multimodal LLM Framework is utilized to supply model scores, which represent the video's appropriateness and engagement potential. By incorporating these scores as features, the system gains a nuanced understanding of each video's quality and relevance. The Retriever thus ensures that all necessary attributes, including multimodal model outputs and contextual metadata, are collected before further processing.\nFilter. During the Filter stage, video candidates are rigorously evaluated based on predefined criteria and divided into three categories: remained, removed, ignored. This filtering process ensures that only compliant and high-quality content progresses, thereby maintaining a positive user experience. The Filter stage leverages model scores from the Retriever stage to make informed filtering decisions.\nSink.The Sink stage is responsible for the final updates, storing or removing videos in the Index Service server. In this stage, videos that pass the Filter phase are saved as key-value pairs in the Index Service, where the video ID serves as the key, and all collected features (from model scores to metadata) serve as the value. The Index Service then holds the refined list of video candidates that have been vetted for policy compliance and quality. This final dataset is the primary output of the VMP framework.\nBy utilizing model scores and other features stored in the Index Service, the recommendation system is empowered to deploy various strategies that align with user preferences and platform policies. With the data processed through VMP, the system can dynamically adjust its recommendations, ensuring that the content presented is both engaging and compliant, thus enhancing the overall user experience."}, {"title": "3 Experiments and Results", "content": "3.1 Experimental settings\n3.1.1 Datasets. We pick two TikTok in-house video quality understanding tasks to test the effectiveness of our proposed MLLM and cascade structure. The two tasks are Inappropriate Content Detection(ICD) and Unoriginal Content Classification(UCC), respectively.\nFor ICD, it is a dataset aiming to determine whether the video violates the TikTok platform's TnS policy or not, with 1 million training and validation videos and 100K test videos. For UCC, it aims to classify whether a posted video is UC(Unoriginal Content) or OC(Original Content). We use a training and validation dataset of 400K videos sampled from TikTok and 18K videos as the test set.\nWe SFT our MLLM based on each task's training set and tune the hyper-parameters based on each task's validation set. Both video quality tasks are evaluated in terms of classification F1 scores and recall under specific precisions.\n3.1.2 Model Details. For MLLM, we invoke the LLaVA-OneVision model [5] as the VL backbone, and consider Whisper-Small [12] as the audio encoder. We use ZERO2[13] and LoRA[4] to train our model. The rank and alpha used in LoRA is 32 and 64. For max frame length used in training, we respectively use 8 and 16 frames in ICD and UCC. We also rely on left padding when dealing with tokenization. For cascade structure, we directly use our online baseline models, which are ResNet[3] for ICD task, and a multi-tower multimodal model for UCC task including Swin-transformer[9], XLM-R[2] and Whisper-Base [12].\n3.2 Main Results\nComparisons between baselines and MLLM. In Table 1, we present a detailed comparison between MLLM and our baseline models on two classification tasks, demonstrating MLLM's effectiveness. Notably, the baseline model employed for the Inappropriate Content Detection (ICD) task is a ResNet50, which operates on a single modality (visual features only); it's a relatively low baseline and so we disable audio encoder in MLLM for ICD task for fairer comparison. Besides our own online baseline models described in the last section, we also include SIGLIP_Large[18] for both tasks as another baseline of industry-as-usual. However, due to SIGLIP"}, {"title": "3.3 Ablation Studies and Analysis", "content": "In this section, we only consider UCC task for the detailed ablation studies and analysis.\nComparisons between Audio Fusion Strategies. In this section, we compare the performance of Late fusion used in our framework, with Early fusion, as shown in Table 3. In the Early fusion approach, the output from the audio encoder is fed into the LLM as the final input token. The results indicate that Late fusion achieves a slightly higher F1 score. Our interpretation is that even though Early fusion would provide a stronger feature crossing, it requires a foundation model with better modality alignment between audio and VL, which currently there isn't one. Therefore, we ended up integrating the audio feature with Late fusion due to its proven effectiveness, as well as robustness in terms of audio feature coverage.\nEffects from Diffferent Rank and Alpha in LoRA. To further evaluate the impact of rank and alpha values in LoRA, Table 4 compares various configurations. We conducted experiments on UCC with progressively increasing rank and alpha values. The setting (32, 64) yielded the best F1 score compared to other configurations. When both rank and alpha are set to 0, indicating that only the classification head is fine-tuned, a poorer result is observed, confirming LoRA's superiority in enhancing both generalization and robustness.\nEffectiveness on Different Training Sizes. MLLM excels in handling situations with limited labeled data. The results in Table 5 demonstrate that when fine-tuning MLLM using only part of the UCC training instances, we achieve significantly better performance compared to the base model. Additionally, the relative improvements with 20% of the training data are greater than those observed with 40% or 60%, highlighting that MLLM is particularly effective when training data is scarce.\nMore ablation studies over MLLM. Table 6 presents the results from our ablation study on the UCC dataset, examining the impact of different components within our Multimodal Large Language Model (MLLM). In the table, the unmodified MLLM represents our original, fully configured model. The study explores several modifications: (a) removing the Whisper encoder, (b) excluding video text elements (title and stickers), and (c) replacing the Whisper-small audio model with Whisper-base, a smaller parameter model. Each modification isolates specific aspects of the MLLM to assess their relative contributions to classification performance.\nIn configuration (a), we removed the Whisper encoder, effectively eliminating audio signal processing within the model. This modification resulted in a significant drop in the F1 score, confirming the critical role of audio information in the UCC task. Audio is particularly beneficial in video understanding, as professionally edited videos often include narration that conveys essential content. By incorporating the Whisper encoder, MLLM is better equipped to capture these audio-based cues, which contribute to more nuanced and accurate classification. The results here emphasize that audio features are not merely supplementary; rather, they are integral to comprehensive video understanding, especially in a multimodal context.\nConfiguration (b) excludes video text components, specifically the video title and stickers, from the model's inputs. This adjustment led to a nearly 2% decrease in the F1 score, indicating that text features derived from these elements play a crucial role in the model's understanding of video content. Video titles and stickers often provide context, keywords, or additional metadata that reinforce visual information, helping the model interpret scenes more effectively. The observed F1 score drop confirms that without these text inputs, the model's comprehension of the video content becomes less robust, underscoring the importance of text data in multimodal fusion.\nIn configuration (c), we replace the Whisper-small encoder with Whisper-base, a variant with fewer parameters. By reducing the model size, we observe a noticeable decrease in classification performance, as evidenced by a lower F1 score. This result illustrates the advantage of a larger audio model in capturing the richness of audio cues present in video content. Larger Whisper models, with more parameters, likely capture finer-grained audio details, enhancing the model's ability to parse subtle audio nuances and supporting more accurate decision-making in classification tasks."}, {"title": "4 Related Work", "content": "Vision-Language Pretraining. Recent advances in vision-language pre-training (VLP) have made remarkable strides in aligning visual and textual concepts within a shared latent space. Early VLP approaches, such as those proposed by VinVL [21] and ViLBERT [10], utilized multimodal transformer encoders to model interactions between visual and language elements. These methods generally depended on pre-extracted features for both images and text and relied on object detectors to associate specific image regions with related text concepts. This approach proved effective in a variety of applications, including visual question answering (VQA), image"}, {"title": "5 Conclusion", "content": "Modern video understanding models benefit significantly from a multimodal approach. By leveraging the complementary strengths of these modalities, multi-tower architectures achieve robust and holistic performance across general tasks. To better address the nuanced demands of specialized applications, a unified Multimodal LLM framework can further improve cross-modal interactions and sensitivity to domain-specific cues, such as assessing video quality or policy compliance on platforms like TikTok, surpassing traditional multi-tower limitations.\nGiven the substantial GPU demands of Multimodal LLMs, especially in real-time scenarios, we propose a cascade structure for efficient deployment. This two-stage model uses a lightweight classifier initially, filtering videos before engaging the MLLM for more in-depth consideration on selected cases, thus reducing resource usage while maintaining accuracy. Empirical results show that this cascade approach not only optimizes GPU use but also enhances classification precision for specialized tasks, marking a substantial advancement in scalable multimodal video classification."}, {"title": "6 Future Exploration", "content": "For future exploration, a promising direction is to expand the COEF-VQ framework into a more versatile, multi-issue MLLM that can handle diverse video quality issues within a single, foundational model. By building a comprehensive model capable of addressing multiple video assessment tasks (e.g., quality classification, content moderation, and genre classification), platforms like TikTok could achieve more streamlined processing with less redundancy across models. This unified approach could also improve consistency in video understanding and reduce the need for task-specific model deployment, optimizing both resource and maintenance efforts.\nAnother important step involves improving audio modality integration. Currently, the framework relies on late-fused feature concatenation, which may not fully capture the nuanced alignment between text, audio and visual cues. To address this, future versions of COEF-VQ could employ modality alignment techniques to establish stronger interactions between text, audio and visual information. This enhanced integration would allow the model to better interpret scenarios where audio cues are critical to understanding video quality, such as distinguishing between background noise and intentional soundtrack effects, ultimately leading to more accurate quality assessments."}]}