{"title": "Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing", "authors": ["Aviv Bick", "Tobias Katsch", "Nimit Sohoni", "Arjun Desai", "Albert Gu"], "abstract": "We introduce Llamba, a family of efficient recurrent language models distilled from Llama-3.x into the Mamba\narchitecture. The series includes Llamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput\nand handle significantly larger batch sizes than Transformer-based models, while maintaining comparable benchmark\nperformance. Furthermore, Llamba demonstrates the effectiveness of cross-architecture distillation using MOHAWK [Bick\net al., 2024], achieving these results with less than 0.1% of the training data typically used for models of similar size. To\ntake full advantage of their efficiency, we provide an optimized implementation of Llamba for resource-constrained devices\nsuch as smartphones and edge platforms, offering a practical and memory-efficient alternative to Transformers. Overall,\nLlamba improves the tradeoff between speed, memory efficiency, and performance, making high-quality language models\nmore accessible.", "sections": [{"title": "Introduction", "content": "Transformer-based LLMs dominate language modeling, but their quadratic attention mechanism makes them computationally\nexpensive and difficult to scale efficiently. This technical paper introduces the Llamba model family, a suite of SSM-based\nlanguage models-including Llamba-1B, Llamba-3B, and Llamba-8B\u2014that address these efficiency challenges. Retaining\nthe overall structure of Llama models, Llamba models are distilled from Llama-3, replacing self-attention with Mamba-2\nlayers to achieve high inference throughput while maintaining strong performance across benchmarks.\nLlamba achieves its performance with drastically reduced training data requirements through architecture distillation. Unlike\ntraditional large language models (LLMs) that rely on massive datasets spanning trillions of tokens, Llamba achieves\ncomparable results with significantly fewer resources by using MOHAWK [Bick et al., 2024] to transfer the knowledge\nfrom strong pretrained Transformer-based LLMs to a Mamba-based architecture. For example, Llamba-3.1-8B was distilled\nusing just 12 billion tokens\u2014less than 0.1% of the training data required for Llama-3.1-8B. This remarkable data efficiency\nhighlights the effectiveness of architecture distillation methods in transferring knowledge from pretrained models while\nsignificantly reducing both data and computational demands. As a result, Llamba presents a scalable and cost-effective\nsolution for high-performance language modeling.\nExtending the benefits of their efficient design, we provide on-device implementations of the Llamba models \u00b9, optimized\nfor deployment on private devices such as smartphones and edge computing platforms with limited memory and computational\nresources. These implementations highlight the advantages of linear models, such as the Llamba family, by delivering\nhigh-quality language modeling on devices where traditional Transformer architectures are often impractical due to their\nheavy memory and compute demands.\nOverall, the Llamba family introduces several key contributions:"}, {"title": "Related Work", "content": "Language Models. Transformer-based models, such as those in the Llama [Touvron et al., 2023], and Qwen [Yang\net al., 2024] series, have shown strong performance across various language modeling tasks. These models underwent\nextensive pretraining on large-scale corpora and incorporate techniques like instruction tuning and curated datasets to enhance\ngeneralization in few-shot and zero-shot settings on various tasks.\nWhile Transformers remain dominant, recent work has explored alternatives to their purely quadratic attention mechanisms\nto improve efficiency while maintaining strong performance. Structured state space models (SSMs) [Gu and Dao, 2023, Dao\nand Gu, 2024] have emerged as a promising direction, offering a distinct approach to sequence modeling. At large scales,\nFalcon-Mamba [Zuo et al., 2024], a fully SSM-based model stacking Mamba-1 layers, has matched and even outperformed\nTransformers on key tasks. Falcon3-Mamba extends this by pretraining for an additional 1.5 trillion tokens, incorporating\nhigh-quality post-training data, and expanding the context length from 8K to 32K tokens, further enhancing its capabilities.\nHowever, despite these advances, SSM-based models still underperform Transformers on algorithmic tasks [Jelassi et al.,\n2024, Wen et al., 2024, Waleffe et al., 2024].\nTo balance these trade-offs, hybrid models combining attention and SSMs have gained interest for leveraging the strengths\nof both architectures. Examples include RecurrentGemma [Botev et al., 2024], which integrates gated linear recurrences\nwith local attention, and Zyphra's Zamba [Glorioso et al., 2024b], which combines Mamba-1 layers with a shared attention\nmechanism spanning the network. Zamba-2 [Glorioso et al., 2024a] builds on this by replacing Mamba-1 blocks with\nMamba-2 for improved efficiency, increasing shared attention layers from one to two for enhanced global context modeling,\nand applying Low-Rank Adaptation (LoRA) [Hu et al., 2021] to shared MLP blocks for parameter-efficient depth adjustments.\nOther hybrid architectures [Lieber et al., 2024, Waleffe et al., 2024] further underscore the interest in models that balance\nexpressiveness and efficiency.\nDistillation. Various methods have been proposed to distill large Transformer-based models into more efficient architectures\nwhile maintaining performance. LoLCATs [Zhang et al., 2024] introduces a linearization approach that replaces softmax"}, {"title": "Model Architecture", "content": "Unlike the Mamba and Mamba-2 architectures, which were designed for training from scratch, Llamba is directly motivated\nby architectural distillation. In particular, the Mohawk distillation framework involves aligning sub-networks of the model\nat various levels of granularity (Section 4). This constraints Llamba to retain the overall architecture of the teacher model,\nideally modifying only the attention matrix mixer by replacing it with a subquadratic alternative.\nThe Llamba models-Llamba-1B, Llamba-3B, and Llamba-8B-comprise 16, 28, and 32 residual Mamba-2 blocks,\nrespectively, followed by feed-forward layers. These models share the tokenizer and vocabulary of Llama-3.1, with hidden\ndimensions of 2048 for Llamba-1B, 3072 for Llamba-3B, and 4096 for Llamba-8B. In addition, Llamba differs from the\noriginal Mamba-2 architecture [Dao and Gu, 2024] in the following ways (see Figure 2b):\n\u2022 Alternating MLP blocks: Llamba interleaves Llama's Gated MLP components between each Mamba-2 mixing layer,\nunlike Mamba-1 and Mamba-2, which consist solely of SSM blocks.\n\u2022 Multi-head structure: Llama-3.x models use grouped-query attention (GQA) [Ainslie et al., 2023, Shazeer, 2019], which\nemploys 32 query heads and 8 key-value heads to boost inference speed and reduce the size of the decoding cache. However,"}, {"title": "Distillation", "content": "The Llamba models were distilled using MOHAWK [Bick et al., 2024] from Meta's Llama-3.x family [Touvron et al.,\n2023]. Specifically, Llamba-3.1-1B was distilled from Llama-3.2-1B-Instruct, Llamba-3B from Llama-3.2-3B-Instruct, and\nLlamba-8B from Llama-3.1-8B-Instruct."}, {"title": "MOHAWK", "content": "Following the MOHAWK framework [Bick et al., 2024], Llamba models were initialized by setting the convolution layer of\nthe Mamba block to an identity kernel (nullifying its effect) and configuring the multiplicative skip connection to directly\npass the input unchanged, effectively initializing the block as an identity function. Subsequently, three key steps were\nimplemented: Matrix Orientation, Hidden-State Alignment, Weight Transfer with Knowledge Distillation."}, {"title": "Matrix Orientation", "content": "This phase is used to align the student and teacher matrix mixers. Specifically, we minimize the\ndistance between the materialized Llamba matrix mixer and Llama's self-attention matrix. Notably, Llama uses an MQA\narchitecture, which results in 32 attention heads with shared weights. Since Llamba's 32 heads are not tied (it uses a\nMulti-Head architecture), it learns independent weights, unlike the dependent matrices of its teacher."}, {"title": "Hidden-State Alignment", "content": "For Hidden-State Alignment, each Mamba-2 block of the Llamba model was aligned indepen-\ndently using the L2 distance, guided by the output of the preceding layer."}, {"title": "Weight Transfer & Knowledge Distillation", "content": "We begin this stage by transferring the MLP weights, normalization layers,\ninput embedding, and output head from the Llama-3.x models to each Llamba model. Unlike previous works [Wang\net al., 2024, Bick et al., 2024], we did not freeze the MLP components and optimized them using the same learning rate\nof the mixing components. During Knowledge Distillation, each Llamba model was aligned with the respective teacher\nmodel using the cross-entropy loss of their logits. After this phase's loss saturation, all models were further distilled from\nLlama-3.1-70B-Instruct for their remaining tokens."}, {"title": "Training Details", "content": "The Llamba models were trained using mixed precision and Fully Sharded Data Parallel (FSDP) on a single node with 8\nH100 GPUs, with activation checkpointing enabled. Training used the AdamW optimizer with \u03b2\u2081 = 0.9, \u03b22 = 0.95, and\na weight decay of 0.1. The maximum learning rates were 1 \u00d7 10\u20134 for the first two MOHAWK stages across all models,\n5 \u00d7 10-5 for the third stage of Llamba-1B and Llamba-3B, and 1 \u00d7 10-5 for the third stage of Llamba-8B. Batch sizes"}, {"title": "Data", "content": "Data quality is critical for accurately modeling temporal interactions in the MOHAWK distillation setting. MOHAWK\ntransfers only the MLP weights that affect the hidden dimensions, excluding the sequence mixer weights related to the time\ndimension. This omission limits the ability to capture time-dependent information directly. For the distillation process,\ntwo datasets were used. The first, fineweb-edu-4.0, is derived from fineweb-edu [Penedo et al., 2024], itself a subset of the\nbroader fineweb dataset. This refined subset includes only highly educational web pages, filtered using a 4.0 classifier score\nthreshold - higher than the 3.0 threshold used for fineweb-edu. Since distillation requires relatively few tokens, this focused\napproach was practical and effective.\nThe Matrix Orientation and Hidden-State Alignment processes were conducted using the fineweb-edu-4.0 dataset with packed\nsequences of length 2048 (see Table 2 for more details). In contrast, Knowledge Distillation was initially performed using\nfineweb-edu-4.0, and subsequently, the Open-Hermes-2.5 dataset was employed for an additional 4 epochs, processing 200\nmillion tokens per epoch with sequences of length 4096. The combination of these datasets played a pivotal role in enhancing\nthe MMLU score.\nOur results demonstrate that this dataset selection significantly improves the performance of MMLU [Hendrycks et al., 2021].\nAs shown in Figure 3, while the C4 [Raffel et al., 2023] and fineweb datasets achieve similar scores across most benchmarks,\nfineweb-edu drives a marked improvement in MMLU. Beyond this, our approach highlights an important takeaway: we\nachieve strong results using only established open-source datasets, in contrast to many alternative models that rely on highly\ncurated proprietary datasets. This demonstrates the feasibility of leveraging openly available resources for high-quality\nmodel performance.\nFurthermore, we emphasize that architecture distillation (e.g. the MOHAWK framework) and data curation are orthogonal\nand synergistic, and we hypothesize that our distillation results could be improved further by incorporating even higher-quality\ndatasets."}, {"title": "On-Device Implementation", "content": "The advantages of sub-quadratic language models are particularly impactful in compute- and memory-constrained environ-\nments, making them ideal for on-device applications. To support efficient inference, we implemented optimized Mamba-2\nkernels, including state-space model and Conv1D layers, using Apple's Metal framework. These kernels are specifically\ndesigned for Apple Silicon, leveraging its GPU parallelism and unified memory architecture for efficient execution.\nOur implementation integrates seamlessly with MLX [Hannun et al., 2023], a machine learning framework optimized for\nApple Silicon. MLX enables dynamic graph construction and efficient tensor operations while utilizing unified memory"}, {"title": "Evaluations", "content": "Table 3 presents a comparative analysis of downstream evaluation results across different models and tasks. The evaluation\nincludes recent advanced models such as hybrids of subquadratic and attention layers (e.g., Zamba2-7B [Glorioso et al.,\n2024a]) and purely subquadratic models (e.g., RecurrentGemma-9B [Botev et al., 2024], Falcon-Mamba-7B [Zuo et al.,\n2024]). Additionally, we include Llama-3.2-1B, Llama-3.2-3B, and Llama-3.1-8B to benchmark performance against\nstate-of-the-art non-hybrid Transformer models.\nWe evaluate the models' performance in both zero-shot and few-shot settings across a range of standard datasets: ARC [Clark\net al., 2018], PIQA [Bisk et al., 2019], Winogrande (WG) [Sakaguchi et al., 2019], HellaSwag (HS) [Zellers et al., 2019],\nLambada OpenAI (LO) [Paperno et al., 2016], MMLU [Hendrycks et al., 2021], and OpenBookQA (OBQA) [Mihaylov\net al., 2018]. All evaluations were conducted using bfloat16 precision and the lm-eval-harness v0.4.4 Python library [Gao\net al., 2024]."}, {"title": "Throughput", "content": "Llamba achieves higher throughput than its Llama-3.1-8B teacher (see Figure 4), even when Llama-3.1-8B generates four\ntimes fewer tokens. This performance gain stems from Llamba's recurrent Mamba-2 layers, whose state size remains constant\nregardless of sequence length. Additionally, Llamba incorporates MLPs with fewer temporal mixing layers than Dao and Gu\n[2024], enabling it to: (1) scale to batches twice as large as a pure Mamba-2 model, as MLPs are stateless in time, and (2)\nreduce kernel launch overhead when CUDA graph optimization is not applied, since Mamba layers typically require more\nkernel preparation time.\nWe have evaluated the throughput of Llama-3.1-8B and Llamba-8B models on a single NVIDIA H100 80GB HBM3. To\nensure a fair comparison, all models were tested under a reasonable level of optimization, using torch.compile(model,"}, {"title": "Conclusion", "content": "The Llamba model family represents a significant step forward in creating efficient and scalable recurrent language models.\nIt achieves high performance with less than 0.1% of the data typically required for similar models while maintaining strong\nperformance across various benchmarks.\nWe see great promise in distilling pre-trained transformers into subquadratic architectures. Future directions include improving\nthe quality and diversity of datasets used in distillation, optimizing the handling of long contexts, and expanding Llamba's\ndeployment to real-time, low-power applications such as IoT devices and wearable technology. Refining the distillation\nprocess further could unlock new capabilities and broaden the applications of this model family, solidifying its impact on\nefficient language modeling."}, {"title": "Comparison of Downstream Performance", "content": "We evaluate a range of language models across multiple benchmarks, measuring their performance in both zero-shot and\nfew-shot settings. ARC-Challenge and ARC-Easy [Clark et al., 2018], and PIQA [Bisk et al., 2019] were evaluated with 0\nand 25 shots, Winogrande [Sakaguchi et al., 2019] with 0 and 5 shots, HellaSwag [Zellers et al., 2019] and OpenBookQA\nMihaylov et al. [2018] with 0 and 10 shots, and Lambada [Paperno et al., 2016] and MMLU [Hendrycks et al., 2021]\nwith 0 and 5 shots. The results, reported in Tables 3 and 4, use normalized logits for ARC-Challenge, ARC-Easy, PIQA,\nHellaSwag, and OpenBookQA. These benchmarks cover a range of reasoning, commonsense, and language comprehension\ntasks, providing insight into the models' ability to process different types of contextual dependencies.\nThe evaluation includes both quadratic and sub-quadratic architectures. Specifically, we evaluate the Transformer-based\nLlama-3.2-1B, Llama-3.2-3B, and Llama-3.1-8B [Touvron et al., 2023], as well as Qwen2.5-3B [Yang et al., 2024]. Falcon-\nMamba-7B and Falcon3-Mamba-7B [Zuo et al., 2024], RecurrentGemma-2B and RecurrentGemma-9B [Botev et al., 2024],\nand Mamba2-2.8B [Dao and Gu, 2024] are included as models integrating recurrent components. Additionally, we evaluate\nZamba2-7B [Glorioso et al., 2024a], a hybrid that incorporates both attention and SSMs. To provide a more comprehensive\ncomparison, we also assess Llamba-1B, Llamba-3B, and Llamba-8B, which apply recurrent architectures across different\nscales. These models vary in their training strategies, data sources, and architectural choices, allowing for a broad comparison\nof language model capabilities across different methodological approaches."}]}