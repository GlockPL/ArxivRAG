{"title": "Transfer Learning of Surrogate Models: Integrating Domain Warping and Affine Transformations", "authors": ["Shuaiqun Pan", "Diederick Vermetten", "Manuel L\u00f3pez-Ib\u00e1\u00f1ez", "Thomas B\u00e4ck", "Hao Wang"], "abstract": "Surrogate models provide efficient alternatives to computationally demanding real-world processes but often require large datasets for effective training. A promising solution to this limitation is the transfer of pre-trained surrogate models to new tasks. Previous studies have investigated the transfer of differentiable and non-differentiable surrogate models, typically assuming an affine transformation between the source and target functions. This paper extends previous research by addressing a broader range of transformations, including linear and nonlinear variations. Specifically, we consider the combination of an unknown input warping-such as one modeled by the beta cumulative distribution function-with an unspecified affine transformation. Our approach achieves transfer learning by employing a limited number of data points from the target task to optimize these transformations, minimizing empirical loss on the transfer dataset. We validate the proposed method on the widely used Black-Box Optimization Benchmark (BBOB) testbed and a real-world transfer learning task from the automobile industry. The results underscore the significant advantages of the approach, revealing that the transferred surrogate significantly outperforms both the original surrogate and the one built from scratch using the transfer dataset, particularly in data-scarce scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Surrogate modeling [3, 15, 16, 41] is extensively used to replace expensive simulators for reducing computational costs, for instance in automobile industry [13, 22, 30, 39]. Machine learning models are commonly used as surrogates, e.g., Gaussian process regression (GPR) [29, 31, 34] and random forest [1, 43]. Training a surrogate model on a new problem often requires a large number of training samples, in particular when there are many independent variables. Acquiring this data usually implies running expensive simulations or real-world experiments. Therefore, we wish to avoid the cost of acquiring large data sets to build surrogates on a new problem instance. Transfer learning [26, 45, 49] can be used to tackle this issue: with a tiny transfer data set sampled on a new problem (the target), we can learn to tweak an accurate surrogate trained on an old problem (the source), provided certain symmetry/invariances between problems.\nCovariance shift [28, 35, 38] is an important type of symmetry, which says that for a regression task to approximate the source function $f_S : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, a target function $f_T$ can be obtained from $f_S$ by transforming the domain thereof. Namely, there exists a bijection $g: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ such that $f_T = f_S \\circ g$. As for the surrogate modeling, the covariance shift implies the predictive distribution $P(y|x)$ remains unchanged between the source and the target while $P(x)$ differs. Previous studies [24, 25] have investigated a special case: $g$ being an affine transformation, where the transformation is learned by minimizing a loss function on a tiny transfer data evaluated on the target.\nHowever, the affine transformation $g$ might be too restrictive to model complex real-world scenarios, e.g., non-linearity is necessary in the automobile industry problems [25]. Hence, we propose implementing a non-linear $g$ function with a beta cumulative distribution function (CDF) [37]. We showcase the preliminary results of our method in Figure 1, where we train a Gaussian process regression (GPR) model train on function F7 from the BBOB benchmark suite."}, {"title": "2 RELATED WORKS", "content": "Transfer learning for GPR. Saida and Nishio [33] investigated GPR surrogate model transfer for structural reliability under uncertainties by augmenting the feature space [9]. Zhang et al. [47] proposed a novel transfer learning strategy that employs a geodesic flow kernel and knee point-based manifold learning to refine Gaussian process models using high-quality knee solutions from previous tasks, thereby enriching training data and boosting solution precision. In multi-task learning, Cao et al. [5] developed the Adaptive Transfer Learning algorithm (AT-GP) using a semi-parametric transfer kernel. The Transfer Bayesian Committee Machine (Tr-BCM) [8] introduced a scalable transfer learning approach by aggregating predictions from lightweight local experts, relaxing assumptions of uniform similarity between tasks. Recent advances include Papez and Quinn [27], which proposed a probabilistic predictor for global source-target interactions, and Wei et al. [44], which developed an interpretable multi-source transfer kernel for improved cross-task performance.\nTransfer learning with input warping. Snoek et al. [37] propose warping the independent variables with beta CDF to realize non-stationary kernels, where the unknown shape parameters of the beta distribution are inferred with Bayesian estimation (using log-normal priors). Their posterior predictive distribution is obtained by marginalizing the shape parameters. In contrast, in this work, we use the beta CDF to model the non-linear relation between the domain of the source and the target function. Also, we learn the unknown shape parameters with a loss-minimization approach.\nCowen-Rivers et al. [7] introduced Kumaraswamy input warping, offering a more computationally efficient alternative to the beta CDF with similar flexibility. Du et al. [11] presented a Hypothesis Transfer Learning framework linking domains via transformation functions, and Zhu et al. [48] proposed a nonlinear transformation method to align the marginal probability distributions without prior data knowledge."}, {"title": "3 LEARNING AFFINE WARPING TO TRANSFER DOMAINS", "content": "Context. We consider a source regression task: a source function $f_S : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ to generate the regression data and a trained surrogate model $f_S$ that approximates $f_S$ accurately. Consider a new regression task $f^T$, the target task. We assume that there exists an unknown nonlinear symmetry between $f_S$ and $f_T$: $\\forall x \\in \\mathbb{R}^d, f_T(x) = f_S \\circ g(x), g(x) = W\\phi(x) + v$, where $v \\in \\mathbb{R}^d, W \\in SO(d)$ (the rotation group of dimension d), and $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is a non-linear diffeomorphism.\nGoal. We wish to transfer the source surrogate model $f_S$ to the target function $f_T$ by re-parameterizing it as $f_S(W\\phi(x) + v)$ and learn the unknown parameters $W, v$, and $\\theta$ with a small transfer data set $T = \\{(x_k, f(x_k))\\}_1^n$ from the target function.\nMethod. For the non-linear function $\\phi$, we consider representing it with the beta cumulative distribution function (CDF) [37]:\n$\\phi_i (x_i, \\alpha_i, \\beta_i) = \\frac{\\int_0^{x_i} u^{\\alpha_i-1}(1-u)^{\\beta_i-1} du}{B(\\alpha_i, \\beta_i)}$\n$\\phi(x; \\theta) = (\\phi_1(x_1, \\alpha_1, \\beta_1), ..., \\phi_i(x_i, \\alpha_i, \\beta_i), ..., (x_d, \\alpha_d, \\beta_d)) \\quad \\quad \\quad(1)$\n$\\theta = (\\alpha_1, \\beta_1, ..., \\alpha_i, \\beta_i, ... \\alpha_d, \\beta_d) \\in \\mathbb{R}^{2d}, \\quad\\quad\\quad(3)$"}, {"title": "3.1 Transfer differentiable surrogates", "content": "If the surrogate model is continuously differentiable, e.g., Gaussian process regression or support vector machine, we can minimize Eq. (4) with mini-batch gradient descent. Let $y_k = W\\phi(x_k; \\theta)$ and\n$\\frac{\\partial L}{\\partial v_i} = \\frac{2}{n_T}\\sum_{k=1}^{n_T} (f_S(y_k) - f_T(x_k))\\frac{\\partial f_S}{\\partial y_i} \\quad \\quad \\quad (5)$\n$\\frac{\\partial L}{\\partial W_{ij}} = \\frac{2}{n_T}\\sum_{k=1}^{n_T}(f_S(y_k) - f_T(x_k)) \\frac{\\partial f_S}{\\partial y_i} \\frac{\\partial y_k^i}{\\partial W_{ij}} \\quad \\quad \\quad(6)$\n$\\frac{\\partial L}{\\partial \\alpha_i} = \\frac{2}{n_T}\\sum_{k=1}^{n_T} \\sum_{l=1}^d (f_S(y_k) - f_T(x_k))\\frac{\\partial f_S}{\\partial y_l}W_{li} \\frac{\\partial \\phi_i(x_i;\\theta)}{\\partial \\alpha_i} \\quad \\quad \\quad(7)$\n$\\frac{\\partial L}{\\partial \\beta_i} = \\frac{2}{n_T}\\sum_{k=1}^{n_T} \\sum_{l=1}^d (f_S(y_k) - f_T(x_k))\\frac{\\partial f_S}{\\partial y_l}W_{li} \\frac{\\partial \\phi_i(x_i;\\theta)}{\\partial \\beta_i} \\quad \\quad \\quad(8)$\nThe derivatives $d \\phi_i$ w.r.t. $\\alpha_i$ and $\\beta_i$ are\n$\\frac{\\partial \\phi_i(x_i, \\alpha_i, \\beta_i)}{\\partial \\alpha_i} = A(x_i, \\alpha_i, \\beta_i) - \\phi(x_i; \\alpha_i, \\beta_i) \\frac{\\partial log B(\\alpha_i, \\beta_i)}{\\partial \\alpha_i} \\quad (9)$\n$\\frac{\\partial \\phi_i(x_i, \\alpha_i, \\beta_i)}{\\partial \\beta_i} = B(x_i, \\alpha_i, \\beta_i) - \\phi(x_i; \\alpha_i, \\beta_i)\\frac{\\partial log B(\\alpha_i, \\beta_i)}{\\partial \\beta_i} \\quad (10)$\n$A(x_i, \\alpha_i, \\beta_i) = \\int_0^{x_i} log(u)u^{\\alpha_i-1}(1-u)^{\\beta_i-1} \\frac{1}{B(\\alpha_i, \\beta_i)} du \\quad (11)$\n$B(x_i, \\alpha_i, \\beta_i) = \\int_0^{x_i} log(1-u)u^{\\alpha_i-1}(1-u)^{\\beta_i-1} \\frac{1}{B(\\alpha_i, \\beta_i)} du \\quad (12)$\n$\\frac{\\partial log B(\\alpha_i, \\beta_i)}{\\partial \\alpha_i} = \\psi(\\alpha_i) - \\psi(\\alpha_i + \\beta_i) \\quad (13)$\n$\\frac{\\partial log B(\\alpha_i, \\beta_i)}{\\partial \\beta_i} = \\psi(\\beta_i) - \\psi(\\alpha_i + \\beta_i) \\quad (14)$\nwhere $\\psi$ is the digamma function and $\\partial f_S/\\partial y$ can be computed analytically from surrogate's predictor. All the above derivatives live in Euclidean spaces, to which the vanilla gradient descent algorithm can be applied. However, $W \\in SO(d)$ is a rotation matrix, and it will not remain in $SO(d)$ if we perform a descent step with Euclidean gradient $\\partial L/\\partial W_{ij}$. Hence, we decide to take a Riemannian gradient descent method, which first computes the Riemannian gradient - an orthogonal projection of $\\partial L/\\partial W_{ij}$ onto the tangent space of $SO(d)$ at $W$ [24]:\n$\\nabla_R L(W) = P_W (\\frac{\\partial L}{\\partial W}), P(M) = W\\frac{W^TM - M^TW}{2} \\quad (15)$\nNext, a gradient step (geodesic with initial velocity $\\nabla_R L(W)$) on $SO(d)$ from $W$ can be computed by the exponential map:\n$Exp_W(\\sigma \\nabla_R L(W)) = W Exp (\\sigma W^T \\nabla_R L(W)) \\in SO(d), \\quad (16)$\nwhere $\\sigma$ is the step-size and $Exp$ is the matrix exponential."}, {"title": "3.2 Transfer non-differentiable surrogates", "content": "We also wish to apply our methodology to non-differentiable models like random forests. We propose to use the Covariance matrix adaptation evolution strategy (CMA-ES) [12, 17, 18] to tune the parameters. CMA-ES can be applied directly to the search space of the translation parameter $v$ and the beta CDF parameters $\\alpha$ and $\\beta$, which are Euclidean. However, special treatment is needed for $W$, which lives in a smooth manifold $SO(d)$. To solve this issue, we consider the Lie group representation $\\mathfrak{so}(d) = \\{ A \\in \\mathbb{R}^{d \\times d}: A^T = -A \\}$, which is a flat space (with dimension $d(d - 1)/2$), and optimize"}, {"title": "4 EXPERIMENTAL SETTINGS", "content": "Synthetic tasks based on BBOB. We first evaluate our method on the Black-Box Optimization Benchmarking (BBOB) [19-21] suite, which consists of 24 continuous, single-objective problems. The BBOB suite has been widely used as a regression benchmark [6, 36, 40, 46] as it reflects real-world regression difficulties. To create synthetic transfer learning problems out of BBOB, we take the first problem instance of each BBOB function as the source $f_S$, and construct the target $f_T$ by applying a beta CDF transformation, followed by random rotation and translation transformations, to the base function. The shape parameters are sampled from log-normal distributions, i.e., $log \\alpha_i \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha})$, $log \\beta_i \\sim N(\\mu_{\\beta}, \\sigma_{\\beta})$. Inspired by [37], we choose different priors to realize distinct shapes of the beta CDF:\n\\begin{itemize}\n    \\item linear shape: $\\mu_{\\alpha} = \\mu_{\\beta} = 0, \\sigma_{\\alpha} = \\sigma_{\\beta} = 0.5$\n    \\item exponential shape: $\\mu_{\\alpha} = 0, \\sigma_{\\alpha} = 0.25, \\mu_{\\beta} = 1, \\sigma_{\\beta} = 1$\n    \\item logarithmic shape: $\\mu_{\\alpha} = 1, \\sigma_{\\alpha} = 1, \\mu_{\\beta} = 0, \\sigma_{\\beta} = 0.25$\n    \\item Sigmoidal shape: $\\mu_{\\alpha} = \\mu_{\\beta} = 2, \\sigma_{\\alpha} = \\sigma_{\\beta} = 0.5$\n\\end{itemize}\nTo generate the training data set for $f_S$, we sample $1 000 \\times d$ points uniformly at random in the domain $[-5, 5]^d$ and evaluate them on $f_S$. To assess the performance of the original GPR model $f_S$ on the target function $f^T$, we create an independent test dataset of the same size, $1 000 \\times d$, sampled uniformly at random from $f^T$. The transfer learning process uses a transfer dataset $T$, containing $40 \\times d$ randomly sampled points from $f^T$. In addition, we consider a minimal transfer dataset of 40 points, independent of the dimensionality of the target problem, smaller datasets of 20 points for 2-dimensional and 5-dimensional cases, and a larger dataset of 80 points for the 10-dimensional problems. After transfer learning, the effectiveness of the transferred GPR model $f^T$ is evaluated using the same test set employed for the original GPR model. We also train a GPR model from scratch directly on $T$ for comparison.\nReal-world benchmark from automobile industry. This dataset evaluates the performance of optimization algorithms in the context of automotive engineering, with a specific focus on minimizing braking distances. It includes five distinct vehicle configurations, each characterized by unique combinations of tire performance and vehicle load conditions. The search space consists of two Anti-lock Braking System (ABS) control parameters, $x_1$ and $x_2$, which span a total of $10 \\times 101$ discrete parameter combinations [39].\nWe construct the surrogate model $f_S$ for the source functions to validate our proposed transfer learning approach on this dataset by utilizing the entire $f_S$ dataset. The complete dataset is also employed as the test set to compute the SMAPE for different GPR"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "5.1 Transferring GPR on BBOB\nIn Fig. 3, we compare the transferred GPR to the one trained from scratch on each 2-dimensional BBOB function regarding the average SMAPE difference. This analysis investigates the effectiveness of transfer learning with different transfer sample sizes and beta CDF parameterizations. The results reveal that, with only 20 transfer samples, the transferred model performs better for most of the function and beta parameterization combinations. However, as the number of transfer samples increases, the performance advantage decreases until it becomes negative for a sample size of 80. Also, the transfer learning method fails to improve on F24, which has a highly rugged landscape.\nAdditionally, in Fig. 8 and Fig. 9 (refer to the supplementary material [2]) extend these observations to 5D and 10D BBOB functions. In the 5D case, models trained from scratch demonstrate a more noticeable advantage compared to the 2D scenario, surpassing transferred models on certain BBOB functions with as few as 20 samples. This could be because the 5D case involves more complex relations, which the transferred models find harder to capture than the simpler or more closely related 2D case. Interestingly, in the 10D experiments, models trained from scratch exhibit significant"}, {"title": "5.2 Ablation study of transferring GPR on BBOB", "content": "Fig. 5 shows an ablation study on 2D BBOB functions, analyzing the impact of using only the beta CDF warping function, approximating an exponential transformation without rotation or translation. Results are compared to reproduced code from [24], which uses only affine transformations. The performance is visualized using"}, {"title": "5.3 Transferring GPR on real-world benchmark from automobile industry", "content": "As highlighted in [25], optimizing with affine transformations has proven effective for many real-world transfer learning applications. However, transferring knowledge between problem instances in the automobile industry presents persistent challenges that require further investigation. We choose this highly challenging benchmark to showcase and evaluate the effectiveness of our transfer learning approach. Fig. 7 (with full results available in Fig. 23, as detailed in the supplementary material [2]) illustrates the SMAPE trends for four GPR models across varying transfer dataset sizes, highlighting a subset of the experimental results. These include the original GPR model, a transferred GPR model that assumes an unknown affine transformation between the source and target problem instances (referred to as \"Transferred (Affine only)\" [24]), a transferred GPR model leveraging our proposed method (referred to as \"Transferred (Full)\"), and a model trained solely on the transfer dataset.\nOverall, in most cases, the transferred GPR model surpasses the performance of the model trained from scratch, mainly when the transfer dataset is relatively small (fewer than 30 samples). However, as the sample size increases, the performance of the GPR model trained from scratch progressively catches up. Interestingly, there are specific scenarios, such as transferring related to problem instance3, where the transfer learning approach fails. Combined with the instance landscapes discussed in the original study [39], these findings suggest that instance3 differs significantly from the other instances, presenting substantial challenges for effective transfer.\nThe results demonstrate that our proposed transfer learning method consistently outperforms the affine-only transferred approach across various transfer dataset sizes, particularly in scenarios like transferring from instance1 to instance2. Moreover, the proposed method excels in transfers involving instance3, significantly outperforming the affine-only approach, demonstrating its ability to capture more complex relations between source and target functions. However, the scratch-trained model remains the top performer among all GPR variants, indicating that the relations involving instance3 are still too intricate to fully capture.\nIn the BBOB problem suite, our target functions are explicitly designed so that a perfect transformation exists-meaning that if we"}, {"title": "6 CONCLUSION", "content": "We present a transfer learning approach to dealing with a complex, nonlinear covariant shift between the source and target problems. We parameterize the unknown covariant shift as the composition of input warping (implemented with beta CDF) and an affine transformation. The method leverages a small transfer dataset drawn on the target problem to learn the covariant shift, enabling an effective surrogate model transfer between problems.\nExperiments with BBOB functions demonstrate the effectiveness of the proposed method. With 20-sample transfer datasets, the transferred GPR outperforms models trained from scratch, particularly in 10D settings. However, with more samples, especially in 5D, scratch-trained models eventually surpass transferred models. The benefits of transfer learning are limited for highly complex functions like F16 and F21-F24, where the original GPR struggled with accurate approximation.\nThe proposed transfer learning method is also validated on a highly challenging real-world automotive task, demonstrating its"}]}