{"title": "Abnormality Forecasting: Time Series Anomaly Prediction via Future Context Modeling", "authors": ["Sinong Zhao", "Wenrui Wang", "Zhaoyang Yu", "Hongzuo Xu", "Qingsong Wen", "Gang Wang", "Xiaoguang Liu*", "Guansong Pang*"], "abstract": "Identifying anomalies from time series data plays an important role in various fields such as infrastructure security, intelligent oper- ation and maintenance, and space exploration. Current research focuses on detecting the anomalies after they occur, which can lead to significant financial/reputation loss or infrastructure damage. In this work we instead study a more practical yet very challenging problem, time series anomaly prediction, aiming at providing early warnings for abnormal events before their occurrence. To tackle this problem, we introduce a novel principled approach, namely future context modeling (FCM). Its key insight is that the future abnormal events in a target window can be accurately predicted if their preceding observation window exhibits any subtle difference to normal data. To effectively capture such differences, FCM first leverages long-term forecasting models to generate a discriminative future context based on the observation data, aiming to amplify those subtle but unusual difference. It then models a normality cor- relation of the observation data with the forecasting future context to complement the normality modeling of the observation data in foreseeing possible abnormality in the target window. A joint variate-time attention learning is also introduced in FCM to lever- age both temporal signals and features of the time series data for more discriminative normality modeling in the aforementioned two views. Comprehensive experiments on five datasets demonstrate that FCM gains good recall rate (70%+) on multiple datasets and significantly outperforms all baselines in F\u2081 score. Code is available at https://github.com/mala-lab/FCM.", "sections": [{"title": "1 Introduction", "content": "Identifying anomalies from time series data is highly demanded in practice [3, 6, 11]. Detecting anomalies that have already occurred is valuable, but the occurrence of the abnormal events can lead to significant financial/reputation loss or infrastructure damage. Accurately predicting these anomalies in advance, i.e., having early warnings of the anomalies, can effectively mitigate such adverse effects. Time series anomaly prediction holds significant impor- tance across various fields. For instance, in the monitoring of criti- cal infrastructure systems, anomaly prediction ensures the safety and stability of water treatment systems by preventing abnormal events such as water supply/pollution accidents [20]; in machine operation and maintenance [30, 31], it provides early warnings for potential failures in servers, hard drives, and other equipment, enabling preemptive measures to avoid financial/reputation losses due to the failures; in space exploration, it alerts potential opera- tional/logistic issues beforehand, providing time for handling the issues early, thereby guaranteeing smooth progression of explo- ration missions [8]. Thus, this work focuses on addressing the anomaly prediction problem.\nNumerous methods have been introduced for time series anom- aly detection (TSAD), such as reconstruction-based methods [10, 13- 15, 24, 25, 35], contrastive learning-based methods [12, 36], one- class classification methods [19, 34], and graph neural network- based methods [4, 5, 38]. TSAD focuses on detecting the abnormal events after they occur, so they assume that clear abnormal patterns"}, {"title": "2 Related Work", "content": "Studies on identifying anomalies in time series data are focused on the task of detection rather than prediction. Numerous meth- ods have been introduced in this line of research, which can be roughly categorized into the following five groups. 1) Classical Methods: Classical methods are not specific to time series data but are generally applicable to all data types, such as OCSVM [19], iFor- est [16, 33], and DAGMM [42]. 2) Reconstruction-based Methods: The main idea of reconstruction-based methods is to learn the man- ifold of normal classes. When encoding and reconstructing data"}, {"title": "2.2 Time Series Forecasting", "content": "Long-term time series forecasting (LTTSF) is a classic task in time series analysis. It involves extracting the core patterns embedded in extensive data and estimating changes over a long period in the future. In recent years, numerous studies have attempted to apply transformer models to LTTSF [27]. For example, Informer [40] aims to adopt distillation techniques together with self-attention to ef- fectively extract the most crucial time points for the forecasting. Autoformer [29] draws on ideas from traditional time series analysis methods and incorporates decomposition and auto-correlation into the network. FEDformer [41] uses a Fourier-enhanced structure to achieve linear complexity. PatchTST [21] treats patches as input units, preserving the semantics of each block in the time series data, thereby utilizing the transformer structure more effectively to achieve good results. Subsequent works have mostly followed the patch concept. iTransformer [17] embeds each time series as variable tokens, employing the attention mechanism to handle multivariate correlations and using a feed-forward network for sequence encoding. It adopts a reversed perspective on time series, embedding the entire time series of each variable independently into tokens, thereby expanding the local receptive field. These fore- casting models may be adapted for anomaly prediction via a simple reconstruction module, but they lack the designs that focus on learning the prevalent patterns from the data, leading to ineffec- tive anomaly prediction. Limited work has been done on anomaly prediction. Jhin et al. [9] explore the detection of early signs of abnormality in time series data to provide a unified framework for anomaly detection and prediction, but its future prediction is restricted to very short time period, leading to a task similar to anomaly detection."}, {"title": "3 Future Context Modeling", "content": "This paper proposes a novel time-series anomaly prediction method FCM. To achieve accurate abnormal event prediction, it leverages long-term forecasting models to generate a discriminative future context for modeling its normality correlation with the data in the observation window. As shown in Fig. 3, FCM contains two important components: a future context-aware anomaly prediction module and a joint variate-time (var-time) attention module. The anomaly prediction module consists of normality modeling from two views. The first view is to model the normality association of observation window data with a forecasting future context to achieve accurate abnormal prediction through joint data reconstruc- tion. Another view is on the normality modeling of the observation data to capture any abnormality signs without being affected by the forecasting output. The var-time attention module constrains anomalies by modeling from the temporal signals and feature di- mensions of time series data, increasing the reconstruction differ- ence between normal and abnormal. Below we introduce the details of each module."}, {"title": "3.3 Future Context-aware Anomaly Prediction", "content": "FCM aims to forecast a future context for more accurate anomaly prediction. The key intuition behind is that time series forecasting results can behave very differently, depending on whether there are abnormal time points or not. This allows the amplification of abnormality signs at the observation window. Thus, we design a LTTSF module F in FCM. This module predicts the target window by learning the continuous changes in two adjacent windows in the training data with normal data. For current window xi, the forecasting results for the target window through F can be defined as $x_{t_{i(L+1):t_{i(2L)}}} = f(x_{t_{i1:t_{iL}}}; \\Theta_{fore})$, with the parameters $\\Theta_{fore}$ optimized using a mean squared error (MSE) loss:\n$\\mathcal{L}_{fore} = ||x_{t_{i(L+1):t_{i(21)}}} - \\hat{x}_{t_{i(L+1):t_{i(21)}}}||_2$ \nAs shown in Fig. 1, although the forecasting module has signifi- cant errors in predicting the abnormal signals, there are discrimi- native forecasting outputs for normal and abnormal time points in the target windows, i.e., smooth, accurate forecasting for normal data signals in the target window $W_i$ vs. fluctuated, exceptional forecasting results for abnormal data signals in the target window $W_{i+1}$. Therefore, the forecasting result $x_{t_{i (L+1):t_{i(2L)}}}$ can be seen as an amplifier that leverages the subtle abnormality signals in the observation window Wi to generate a discriminative future context at $W_{i+1}$. FCM then models the normality correlation between the observation window data and its forecasting future context via a linear layer g parameterized by $\\Theta_g$ through joint observation-future context signal reconstruction:\n$\\mathcal{C}_i = g([x_{t_{i(1):t_{i(L)}}} \\bigoplus \\hat{x}_{t_{i(L+1) :t_{i(2L)}}}]; \\Theta_g)$,\n$\\mathcal{L}_c = ||\\hat{C}_i - C_i||_2.$\nwhere $[\\bigoplus]$ means a concatenation of the two inputs and $\\hat{C} = h(C; \\Theta_c)$ refers to the reconstruction of C by a decoder h parameterized by $\\Theta_c$. Our LTTSF module can predict accurately for most normal points. Thus, the observation-future context data consisting of $x_{t_{i(1):t_{i(L)}}}$ and $x_{t_{i(L+1):t_{i(2L)}}}$ are strongly correlated, if $x_{t_{1(1):t_{i(L)}}}$ does not con- tain future abnormality signs. This correlation is broken otherwise, since the forecasting output $x_{t_{i(L+1):t_{i(2L)}}}$ would not follow specific distribution patterns. As a result, the reconstruction for normal events in the target window would be small, and it would be large for future abnormal events.\nAdditionally, we also learn the normality pattern from the obser- vation window individually to model the original sign anomalous signals. This is to complement the normality modeling from the observation-future context view. It is done by a usual reconstruction of the observation data x\u2081:\n$\\mathcal{L}_{det} = || x_{t_{i(1):t_{i(L)}}} - \\hat{x}_{t_{i(1):t_{i(L)}}} ||_2$\nwhere $\\hat{x}_{t_{i(1):t_{i(L)}}}$ refers to the reconstruction result of $x_{t_{i(1):t_{i(L)}}}$."}, {"title": "3.4 Joint Variate-Time Attention Mechanism", "content": "In Sec. 3.3, we use data reconstruction to model the normality pat- terns from two different views. However, traditional reconstruction methods may also reconstruct subtle anomaly signals in both views. To increase the difficulty of the data reconstruction that involves abnormal signals, we propose a joint variate-time (var-time) at- tention mechanism. This mechanism learns data correlations from both the temporal and feature dimensions, making it difficult to establish connections for current windows that exhibit anomalies"}, {"title": "3.5 Anomaly Prediction Using FCM", "content": "Training. We use a sequential updating strategy to provide feed- back for the three losses involved in the task. Specifically, we update the network using $\\mathcal{L}_{det}$ and $\\mathcal{L}_{fore}$ for a period of time before incor- porating $\\mathcal{L}_c$. This approach ensures that the forecasting module's results are relatively stable. If we incorporate $\\mathcal{L}_c$ from the start, the large errors in time series forecasting results could adversely affect the forecasting results.\nInference. Given the observation window data $x_{t_{i(1):t_{i(L)}}}$, we ob- tain the forecasting result of the target window from our LTTSF module. We then concatenate the forecasting future context with the observation data and embed it into a low-dimensional space. We use the embedding $C_i$ as the representation of the observation data. The reconstruction error obtained from this representation is used as the anomaly score for anomaly prediction:\n$Score(W_i) = ||\\hat{C}_i - C_i||_{l2}$."}, {"title": "4 Experiments", "content": "Five widely-used TSAD datasets are utilized in the experiments [35, 36]. They are adapted to anomaly prediction task by shifting the ground truth of abnormal events to a proceeding window."}, {"title": "4.1.3 Evaluation Metrics.", "content": "Time-series anomaly prediction presents distinct challenges compared to the well-established anomaly detec- tion task, necessitating a novel evaluation protocol. Our approach focuses on the ability to provide early warnings of impending anom- alies, rather than merely identifying them post-occurrence. To eval- uate the predictive capability, we designate the ground-truth labels"}, {"title": "4.2 Main Results", "content": "We conduct our anomaly prediction approach and sixteen com- peting models on five multivariate real-world datasets. Table 1 shows the precision, recall, and F1 score of our method FCM and its contenders. Please note that this experiment assesses anom- aly prediction results, wherein each target window's evaluation is based on predictions from the preceding window, diverging from the conventional anomaly detection paradigm. Therefore, this task is extremely challenging, resulting in generally lower F\u2081 scores across all methods. Nevertheless, FCM still significantly outper- forms competing methods on all five datasets, demonstrating the superiority in anomaly prediction. Notably, FCM gains remarkable recall rates, with an average R exceeding 70% across five datasets. Compared to the best-performing existing methods, our approach achieve 10%, 14.5%, 9.2%, 7.9%, and 21.4% F1 score improvement across the five datasets. In particular, on the PSM dataset, FCM illustrates significant improvement compared to the competitors, realizing an average improvement of 48.8% in F\u2081 score. Among the baseline methods, BeatGAN and AnomTrans exhibit notewor- thy prediction performance. BeatGAN's strength lies in capturing high-order patterns of normal data through adversarially generated samples, while AnomTrans leverages its anomaly discrimination method to excel in criterion correlation difference. Nonetheless, our method still illustrates consistent superiority, mainly attributed to its ability to identify subtle yet unusual differences in preceding windows and its advantage in forecasting future anomalies. Since the point adjustment strategy of time series anomaly detection is controversial, some studies propose new evaluation metrics, such as affiliation precision/recall and the volume under the surface (VUS), which are considered to provide a more objective assessment of detection performance. Table 2 reports the results of new indica- tors, in which we focus on the comparison between FCM and two best-performing contenders under traditional metrics (see Table 6 in the appendix C for the results of other methods). The empirical results suggest that FCM can also outperform these competitors across most evaluation scenarios.\nWe further relax the evaluation criteria by incorporating the previously omitted anomaly segments as introduced in Section 4.1.3. Specifically, we consider the prediction results to be accurate if they successfully trigger an alert within the extended temporal window. It can be observed from Table 3 that our method also significantly outperforms existing state-of-the-art anomaly detectors, averagely achieving 32.3%, 21.9%, 24.3%, 22.2%, and 17.0% improvement across"}, {"title": "4.3 Ablation Study", "content": "This experiment aims to validate the effectiveness of several key modules in our FCM model. By removing components of FCM, we create four ablated variants, including Bare AP, Bare Fore, w/o Lc, and w/o Att. Table 4 presents the performance of these ablated variants against the full FCM model. We have the following findings:\nBare AP uses a bare anomaly prediction module without future context or var-time attention. We use AnomTrans to accomplish this variant. Bare AP shows a performance gap of an average of 15.6% in F1 score across five datasets, compared to the full FCM model, demonstrating the ineffectiveness of modeling the subtle normality in the observation window only.\nBare Fore utilizes a long-term forecasting model to gener- ate predictions for the observation window, which are then used directly for anomaly detection. Results indicate that the prediction results also contain key features for identifying fu- ture anomalies. It is worthwhile to enhance the identification of future anomalies from future context.\nThe w/o Att variant represents the complete form of our model, yet utilizing two separate self-attention modules for the anomaly prediction and time series forecasting. Com- pared to FCM, this variant demonstrates an average improve- ment of 9.2% in F\u2081 score. It can be seen that the shared self- attention mechanism effectively learns multi-dimensional normality correlation information, leading to further im- proved anomaly prediction performance.\nThe w/o Lc variant incorporates both the time series fore- casting module and the anomaly prediction module. In this configuration, our anomaly prediction module and time se- ries forecasting module share the var-time attention mecha- nism. This variant, on five datasets, improves the F\u2081 score by 6.5% on average compared to FCM. This underscores the importance of future contextual information in obtaining discriminative representations of future anomalies.\nIn conclusion, the final row in Table 4 represents the complete form of our method, incorporating all four modules. The inclusion"}, {"title": "4.4 Case Study", "content": "To demonstrate the efficacy of our anomaly prediction method, we conduct a comparative case study to complement quantitative anomaly detection results. This study leverages three distinct types of anomalies commonly encountered in time series data, including group anomalies, shapelet anomalies, and trend anomalies, as re- spectively shown in Figure 4 (a)(b)(c). These cases are sampled from the SWaT dataset. For each sub-figure, the upper panel displays the original time series data. The middle panel illustrates the anomaly scores (in blue) predicted by our approach FCM and the threshold (represented by a red dashed line). The bottom panel depicts the prediction labels and the original detection labels. FCM success- fully assigns significantly high anomaly scores before the actual anomalies occur in these three typical cases, thereby offering valu- able early warnings. FCM can perceive subtle abnormality signs, during which the interactions between current observational time points and its forecasting time points are effectively modeled. Un- like traditional methods that detect anomalies only after they have manifested, FCM ensures a proactive rather than passive response. The ability of preemptive detection is crucial as it allows for timely intervention and mitigation of the abnormal events."}, {"title": "4.5 Hyper-parameter Sensitivity", "content": "We conduct sensitivity analysis to investigate the impact of hyper- parameters on anomaly prediction performance, i.e., this experi- ment varies the setting of several hyper-parameters and reports the corresponding F\u2081 scores. Specifically, we investigate three key hyper-parameters in FCM, including the window size, the sliding window step size, and the number of attention heads.\nFig. 5 (a) illustrates the effect of varying window sizes on the prediction results. Note that the size of both observation window and target window is adjusted concurrently. Generally, a larger window is advantageous as it encapsulates more comprehensive temporal semantics, whereas increased window size correlates with higher computational complexity and more challenging forecasting process. FCM exhibits optimal performance stability within the 100-250 samples range.\nFig. 5 (b) shows the impact of different sliding step sizes on pre- diction performance. For fixed-size windows, a smaller sliding step can extend the effective operational time, increasing the temporal resolution of the analysis. On the contrary, a larger sliding step results in fewer learning windows, potentially increasing prediction difficulty due to reduced data overlap. FCM demonstrates relatively stable performance on PSM, SMD, and SwAT within the 30-70 step size range, but a gradual performance degradation can be observed on the MSL and SMAP datasets. The optimal temporal granular- ity might vary across datasets, reflecting differences in underlying temporal dynamics and anomaly characteristics.\nFig. 5 (c) elucidates the impact of the number of var-time atten- tion heads on the model performance. The MSL dataset is sensitive to variations in the number of attention heads, while other datasets"}, {"title": "5 Conclusion", "content": "This paper introduces a novel method for time series anomaly prediction, FCM. It models future context for accurate anomaly prediction by learning to capture subtle abnormality signs in the observational time points and their correlations with forecasting time points. FCM achieves this by modeling the correlation between the current and future windows through a time series forecasting module, in which the forecasting future time points act as the fea- ture amplifier of the current window. This helps largely enhance the expressiveness of the abnormal features. It also introduces a joint variate-time attention module to simultaneously learn the cor- relations between time series and between the features. Under the constraints of the two correlations, future anomalies become more difficult to reconstruct, resulting in easier differentiation between future normal and abnormal time points. The effectiveness of FCM is justified by extensive experiments on five real-world datasets compared to 16 state-of-the-art competing methods."}, {"title": "A datasets", "content": "Five publicly available multivariate time series datasets are used in our experiments, with the relevant statistics shown in Table 5. AR (anomaly ratio) represents the abnormal proportion of the whole dataset."}, {"title": "B ALGORITHM", "content": "We show the specific implementation of FCM in Alg. 1. Through the results of time series embedding, our attention can learn the asso- ciation between variables and output the predicted results (Line 5). Through the embedding of variables at the same time point, our attention can learn the association between time points and output the reconstructed results, which are used for anomaly prediction (Line 7). We control the training epoch in which we involve the reconstruction module of future context through a parameter P in Step 8."}, {"title": "C Results on different metrics", "content": "Results of anomaly prediction on 3 traditional metrics and 6 new proposed metrics are shown as Table 6. It can be seen that our FCM achieves excellent results in most indicators, especially in PR-related indicators. This demonstrates that FCM can effectively leverage the future context to predict future anomalies accurately."}, {"title": "Algorithm 1 Learning algorithm of FCM", "content": "Input (1) Training data X; (2) Hyper-parameters: future context access point P; (3) Training epochs E and the number of batches in every epoch Q.\nOutput $\\Theta_{det}$: the parameter of anomaly prediction network.\n1: Initialize the LTTSF network $\\Theta_{fore}$ and the anomaly prediction network $\\Theta_{det}$;\n2: while e < E do\nfor q < Q do\nEmbed every time series of each sensor as a token;\nPut it into the encoder of transformer and output the LTTSF results;\nEmbed the value of every sensor at the same time point as a token;\nPut into the encoder of transformer and output the reconstruction results;\nif e > P then\nGet the forecasting results through $\\Theta_{fore}$;\nUpdate $\\Theta_{det}$, $\\Theta_{fore}$ with $\\mathcal{L}_f$, $\\mathcal{L}_{det}$ and $\\mathcal{L}_c$, according to Eq. (1), Eq. (4) and Eq. (2).\nUpdate $\\Theta_{det}$, $\\Theta_{fore}$ with $\\mathcal{L}_f$, $\\mathcal{L}_{det}$ according to Eq. (1) and Eq. (4).\n12: Evaluation:\n13: Compute anomaly prediction scores according to Eq. (10)."}]}