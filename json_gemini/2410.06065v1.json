{"title": "Posets and Bounded Probabilities for Discovering Order-inducing Features in Event Knowledge Graphs", "authors": ["Christoffer Olling Back", "Jakob Grue Simonsen"], "abstract": "Event knowledge graphs (EKG) extend the classical notion of a trace to capture multiple, interacting views of a process execution. In this paper, we tackle the open problem of automating EKG discovery from uncurated data through a principled, probabilistic framing based on the outcome space resulting from featured-derived partial orders on events. From this, we derive an EKG discovery algorithm based upon statistical inference rather than an ad-hoc or heuristic-based strategy, or relying on manual analysis from domain experts.\nThis approach comes at the computational cost of exploring a large, non-convex hypothesis space. In particular, solving the maximum likelihood term involves counting the number of linear extensions of posets, which in general is #P-complete. Fortunately, bound estimates suffice for model comparison, and admit incorporation into a bespoke branch-and-bound algorithm. We show that the posterior probability as defined is antitonic w.r.t. search depth for branching rules that are monotonic w.r.t. model inclusion. This allows pruning of large portions of the search space, which we show experimentally leads to rapid convergence toward optimal solutions that are consistent with manually built EKGs.", "sections": [{"title": "Introduction", "content": "Event knowledge graphs (EKGs) capture the temporal ordering of events associated with entities and objects via so-called local directly-follows paths (df-path), resulting in an intertwined web of event sequences, synchronized at shared events. Automating the process of identifying, \"relevant structural relations between entities\" is a stated open problem, currently requiring domain knowledge [12,13].\nIn this paper, we present a theoretically principled method for automatically identifying which event-level features should be included as entities in the EKG, thereby defining the structure of its df-paths. Our approach rests on a natural"}, {"title": "Related Work", "content": "Event knowledge graphs were first introduced by Fahland et al. [12,13,17], and reflect a growing recognition in the process mining community of the shortcomings of the classical trace concept based on a single case identifier [1,3,31]. Even in scenarios where a classical event log is sufficient, the process of extracting event traces from the raw data source still tends to be an expensive, ad-hoc process relying on scarce domain expert input [5]. While some work has been done on automating this task for classical event logs [10], for novel representations like EKGs, it remains unaddressed [13].\nThe task of calculating the number of linear extensions of a poset has been explored by mathematicians for decades as a combinatorics problem in its own right [7]; as well as by computer scientists, typically in the context of sorting and other comparison-based algorithms [2,6,27]. It also arises in a number of other applications such as convex rank tests [20], structure learning for graphical models [24], measuring flexibility of partial-order plans [21], sequence analysis [19], and is equivalent to counting topological sorts on an acyclic graph [26].\nBranch and bound algorithms have long been used for feature selection in general [8,23,25,30,32], and model selection [29,30] and maximum likelihood estimation in particular [9,33]."}, {"title": "Preliminaries", "content": "The reader is expected to have a basic familiarity with combinatorics, order theory, probability theory, and set theory.\nThe set of possible permutations of the elements of a set $S$ is denoted $S!$ and the symmetric difference between sets $A$ and $B$ as $A \\triangle B$. We denote by $[S]^{\\leq n}$ the set of subsets of $S$ containing $n$ or fewer elements, that is: {$S' \\subseteq S | n \\geq |S'|$}.\nAn event is a unique execution of some action (i.e., activity) at a certain point in time, and associated with (potentially empty) attributes, (i.e., features). In the event table in Tab. 1 each event is represented by a row.\nWe denote by $E$ a nonempty set of events, by $D$ a total order on $E$ called the observed data, by $\\mathcal{D} := {D_1, ..., D_v}$ a set of $N$ independent observations. We denote by $X_i$ a feature associated with events, i.e., a column in an event table, and by $X_i(e)$ the set of values of $X_i$ associated with event $e$. The set of all features is denoted by $F := {X_1, ..., X_M}$. Singleton sets of features are called atomic,"}, {"title": "Event Knowledge Graph as Poset", "content": "EKGs are not per se process models with execution semantics, but rather a representation of one observed outcome: one execution trace. Each event is a specific, unique occurrence that will never be duplicated, and the local directly-follows relations between them are not, strictly speaking, order relations in the sense of rules or constraints of a model that defines a language of traces.\nNevertheless, we will argue that EKGs are functionally equivalent to posets. After establishing definitions, we show how an observed order is reduced w.r.t. a model. In this case, the original poset will be a total order (an event table) and the resulting poset interpreted as the df-paths of an EKG.\n**Definition 1** (Atomic feature relation). Two events $a,b \\in E$ are related w.r.t. feature $X := {X}$ if they share one or more of the values for that feature. Formally,\n$\\X:= {(a, b) | X(a) \\cap X (b) \\neq \\emptyset }$\n**Definition 2** (Derived feature relation). Two events $a,d \\in E$ are related w.r.t. derived feature $X := {X_i, X_k}$ if they are related by $X_i$ or $X_k$, or if there exists a third atomic feature $X_j \\in F \\setminus {X_i, X_k}$ and distinct events $b, c \\in E$ s.t. $a$ and $d$ are related transitively via $b$ and $c$ by relations $\\X$ and $\\Xj$. Formally,\n$\\X := \\Xi \\cup \\X{(a,d) | \\exists X_j, b, c. X_j \\notin {X_i, X_k} \\land a \\X b \\Xj c \\X d }$"}, {"title": "Linear extensions as outcome space", "content": "We now show that regardless of whether EKGs are intended to be seen as process models, many hypothetical orderings of the same set of events map to the same set of order relations (i.e., df-paths) via $\\mathfrak{g}_M$, meaning the preimage of $\\mathfrak{g}_M$ is formally an outcome space. This is illustrated in Fig. 2.\n**Proposition 1**. Given posets $D := (E, \\prec)$ and $D' := (E, \\prec')$ over the same set $E$, a model $M(E)$, and df-path generator $\\mathfrak{g}_M$, then $D$ extends $\\mathfrak{g}_M(D')$ and $D'$ extends $\\mathfrak{g}_M(D)$ if and only if $\\mathfrak{g}_M(D) = \\mathfrak{g}_M(D').$"}, {"title": "A Probabilistic (Bayesian) Formulation", "content": "Our aim is to identify the set of features $M$ that provides the most information about the observed sequence of events w.r.t. the poset induced by $M$ via df-path generator $\\mathfrak{g}_M$. We now formulate this problem at a high level in probabilistic terms. We seek to find the model $M$ which is most strongly indicated by our data $\\mathcal{D}$, i.e.: $\\arg \\max_M P(M | \\mathcal{D})$. The term $P(M | \\mathcal{D})$ is the posterior probability of model $M$ after observing data $\\mathcal{D}$; by Bayes' rule, this can be rewritten as:\n$\\arg \\max_M P(M | \\mathcal{D}) = \\arg \\max_M \\frac{P(M) P(\\mathcal{D} | M)}{P(\\mathcal{D})}$\nwhere $P(\\mathcal{D} | M)$ is the likelihood of $M$ while $P(M)$ is its prior probability, and $P(\\mathcal{D})$ is the evidence. Once $\\mathcal{D}$ has been observed it is fixed, and it will turn out to cancel out in our final, posterior-odds objective function for comparing models (see Sec. 5.3). Later we will see that this Bayesian formulation of our task allows the natural incorporation of an important component to our objective function: the prior probability $P(M)$, which allows us to incorporate beliefs regarding models such that we avoid degenerate solutions."}, {"title": "Maximum likelihood", "content": "Recall that the outcome space of model $M$ w.r.t. observation $\\mathcal{D}$ is the set of linear extensions of the poset $P$ defined by the df-path generator $\\mathfrak{g}_M(\\mathcal{D})$. We can then write the likelihood as $P(\\mathcal{D} | P) = P(\\mathcal{D} | \\mathcal{D} \\in E_P) P(\\mathcal{D} \\in E_P)$, i.e., the probability of selecting the observed sequence from all linear extensions of $P$. In principle, this formulation allows us to consider posets for which $\\mathcal{D}$ is not or is not known to be a linear extension of $P$, but by Def. 3 we have $P(\\mathcal{D} \\in E_P) = 1$ by construction. Assuming a uniform distribution over linear extensions $E_P$ gives\n$P(\\mathcal{D} | \\mathcal{D} \\in E_P) P(\\mathcal{D} \\in E_P) = \\frac{1}{|E_P|}$                    (1)\nThe assumption of uniformity is common [14,15,16,19], though not universal [4]. We will use Eq. 1 as the basis for the likelihood component of our complete objective function described in Sec. 5.3.\nOn its own, maximum likelihood is not a sufficient criteria since the likelihood of data $\\mathcal{D}$ can always be increased with an increasingly narrowly fitted model [18]. In the next section we present a model prior which counterbalances the maximum likelihood term."}, {"title": "Model priors", "content": "It is common when working with parametric models to assume a uniform distribution over models, though not necessarily the parameter space. Typically a component of the likelihood called the Occam factor helps maintain balance between maximizing likelihood and avoiding overfitted parametrizations [18]. Our models are non-parametric, and using uniform model priors would result in our objective function being reduced to likelihood $(1/|E_P|)$ alone, resulting in degenerate models in some circumstances. See Tab. 2 for an example. Such a degenerate solution is akin to a clustering task in which each datapoint is assigned to its own cluster.\nTypically, overly simplistic models will underfit the data. That is, $P(\\mathcal{D} | M)$ will tend be low for seen as well as unseen data since the model's probability mass is spread thin across such a large outcome space. In contrast, our task has an idiosyncratic property that extremely trivial models may perform well in terms of likelihood, despite being completely uninformative. For example, a feature with one outcome across all events would induce the total order $D$, both on seen and likely on unseen data as well. This is akin to a prediction task in which a copy of the target variable has unknowingly been included in the data, leading to a trivial classification model.\nThis pitfall can be avoided by explicitly expressing a preference against both degenerate and unnecessarily complex models. We propose one such distribution based on the product of the normalized entropies for each feature in the model. The per-feature score penalizes features with distributions that are deterministic, while taking the product of the [0, 1) bounded scores across features discourages the addition of superfluous features."}, {"title": "Posterior odds objective function", "content": "Combining these components gives a posterior which balances likelihood against our preference for parsimonious and informative models. For readability we write $|E_M|$ in place of $|E_{\\mathfrak{g}_M(D)}|$, the linear extensions of the poset defined by $M$ on $\\mathcal{D}$.\n$\\begin{aligned}P(M | \\mathcal{D}) &= \\frac{P(M) P(\\mathcal{D} | M)}{P(\\mathcal{D})} \\\\&= \\frac{1}{P(\\mathcal{D})} \\frac{\\prod_{X \\in M} \\eta(X)}{ \\sum_{M \\subseteq [F]^{\\leq 2}} \\prod_{X \\in M} \\eta(X)} \\frac{1}{|E_M|}\\end{aligned}$\n(3)\nFor multiple independent observations, $\\mathcal{D} := {D_1,..., D_v}$ of the same process, and denoting by $|E_{M_i}|$ the linear extensions of the poset induced by $M$ on observation $D_i$, we can rewrite Eq.3 by conditional independence as:\n$\\begin{aligned}P(M | \\mathcal{D}) &= \\frac{P(M) \\prod_{i=1}^N P(D_i | M)}{P(\\mathcal{D})} \\\\&= \\frac{1}{P(\\mathcal{D})} \\frac{\\prod_{X \\in M} \\eta(X)}{ \\sum_{M \\subseteq [F]^{\\leq 2}} \\prod_{X \\in M} \\eta(X)}  \\prod_{i=1}^N \\frac{1}{|E_{M_i}|} \\end{aligned}$\nSince we are only interested in the relative fitness of models, not the exact posterior, it suffices to calculate posterior odds, in which case both $P(\\mathcal{D})$ and the normalizing constant stemming from the denominator of Eq. 3 cancel:\n$\\frac{P(M_1 | \\mathcal{D})}{P(M_2 | \\mathcal{D})} = \\frac{P(M_1) P(\\mathcal{D} | M_1)}{P(M_2) P(\\mathcal{D} | M_2)} = \\frac{\\prod_{X \\in M_1} \\eta(X)}{\\prod_{X \\in M_2} \\eta(X)} \\prod_{i=1}^N \\frac{|E_{M_{2_i}} |}{|E_{M_{1_i}}|}$\nFurthermore, it suffices to determine if $P(M_1 | \\mathcal{D}) < P(M_2 | \\mathcal{D})$ when comparing models, allowing us to take advantage of bounds on the individual terms."}, {"title": "Branch and Bound for Model Selection", "content": "Exact computation of the linear extension count (the denominator in Eq. 1) can be very expensive, depending on the structure of the partial order. However, we can use several properties of partial orders and linear extension count to construct a greedy, divide-and-conquer strategy for establishing bounds and incorporate this into a branch-and-bound algorithm for model selection."}, {"title": "Bounding linear extension count", "content": "Let $P = (E, \\prec)$ be a poset. If $P$ is a total order, there is only a single linear extension, namely the order itself, and if $\\prec = \\emptyset$, all permutations of $E$ are linear extensions. Hence, it is easy to see that $1 \\leq |E_P| \\leq |E|!$. We can combine this na\u00efve bound with the following two rules for recursive decomposition of posets.\n**Theorem 1** (Disjoint decomposition). Given poset $P$ partitioned by ${P_1, P_2}$ s.t. no $a \\in P_1$ is comparable to any $b \\in P_2$, then\n$|E_P| = |E_{P_1} || E_{P_2}|$"}, {"title": "Bounds for pruning", "content": "The search space over models is exponential in the number of features and calculating $P(M|D)$ for each model is expensive due to the $|E_{P_M}|$ term in Eq. 1. Two factors, respectively, help tackle these challenges: pruning large portions of the model space; and quick evaluation for each model using bounds on, rather than exact, linear extension counts. The latter will allow us to either reject a model, or mark it for revisit and reestimation, meanwhile potentially updating the current best score.\nWhen maximizing an objective function $f(M)$, pruning in branch and bound requires a bounding function $b$ that provides a closer and closer upper bound on $f$ as more constraints are added to set of candidate solutions (models), and agrees with $f$ for solutions at the end of the search space. Formally, $b(M) \\geq f(M)$ for all $M$, $b(M) = f(M)$ for all leaves in the search space, and $b(M_i) \\leq b(M_j)$ if $M_i$ is the parent state (model) of $M_i$."}, {"title": "Posterior (log)", "content": "Our full objective function $P(M|D)$ is intentionally non-convex, with the terms $P(M)$ and $P(D|M)$ pulling in opposing directions. Nevertheless, we can formulate a convex bounding function that takes advantage of upper bounds on each term, assuming a monotone branching rule w.r.t. model inclusion. That is, that child states in our search space are generated by adding features to a candidate model.\n**Theorem 3** ($P(M)$ is strictly antitonic). $P(M_i) > P(M_j)$ for $M_i \\subset M_j$.\nProof. We have defined $P(M_i)$ to be the product of $\\eta(X)$ for each $X \\in M_i$. Since $\\eta(X)$ is bounded in [0,1), adding an extra term to this product cannot increase its value. We have stated that $M_i \\subset M_j$, so any feature $X \\in M_j \\setminus M_i$ can only reduce $P(M_j)$ relative to $P(M_i)$. $\\square$\n**Lemma 1** ($M \\subseteq M'$ implies $\\mathfrak{g}_M(D) \\subseteq \\mathfrak{g}_{M'}(D)$). Let $D$ be a total order and let $M$ and $M'$ be models s.t. $M \\subseteq M'$. Then the set of order relations $\\mathfrak{g}_M(D)$ induced by $M$ via the df-path generator as defined in Def. 3 will be a subset of the order relations $\\mathfrak{g}_{M'}(D)$ induced by $M'$.\nProof. Follows immediately from Def. 1,2,3.\n**Lemma 2** ($\\prec \\subseteq \\prec'$ implies $E_{\\prec} \\supseteq E_{\\prec'}$). Let $\\prec$ and $\\prec'$ denote order relations over the same set. For the respective sets of linear extensions, $E_{\\prec}$ and $E_{\\prec'}$, the fact that $\\prec \\subseteq \\prec'$ implies that $E_{\\prec} \\supseteq E_{\\prec'}$. We say the latter order is more restrictive than the former.\nProof. Assume that $\\prec \\subseteq \\prec'$, clearly this means that $E_{\\prec} = E_{\\prec'}$. It remains to show that when $\\prec \\subsetneq \\prec'$, that no total order $D$ exists s.t. $D \\in E_{\\prec'}$ and $D \\notin E_{\\prec}$. Assume such a $D$ exists and that $\\prec \\subsetneq \\prec'$. Since $D \\notin E_{\\prec}$ this means that there exists some relation $(a, b) \\in \\prec$ that prevents $D$ from extending $\\prec$ while $(a, b) \\notin \\prec'$ since $D$ extends $\\prec'$. This means that $\\prec \\not \\subseteq \\prec'$ and we have a contradiction of the premise. $\\square$\nThe following bound on $P(D|M)$ assumes that from the state associated with $M$, the set of features in models reachable from this state is known, allowing us to determine the most restrictive reachable poset. This assumption is consistent with standard algorithms for recursively generating powersets.\n**Theorem 4** (Upper bound on $P(D|M)$ is antitonic). Let $\\mathcal{S}_M$ denote the portion of the search space reachable from state $M$ (inclusive $M$), with $M \\subseteq M'$ for any $M' \\in \\mathcal{S}_M$ and define $M^* := \\bigcup_{M' \\in \\mathcal{S}_M} M'$. Then\n$P(D|M) \\leq \\frac{1}{|E_{M^*}}| and $\\frac{1}{|E_{M_i}|} > \\frac{1}{|E_{M_j}|}$ for $M_i \\subset M_j$.\nProof. By Lem. 1 and 2, since $M \\subseteq M^*$ we have that $\\mathfrak{g}_M(D) \\subseteq \\mathfrak{g}_{M^*}(D)$ which implies $|E_M| \\geq |E_{M^*}|$, equivalently $\\frac{1}{|E_M|} \\leq \\frac{1}{|E_{M^*}|}$. By the same reasoning, we have that $M_i \\subseteq M_j$ implies $\\frac{1}{|E_{M_i}|} \\geq \\frac{1}{|E_{M_j}|}$. $\\square$"}, {"title": "Bounds for model evaluation", "content": "When pruning is not possible, we can still harness bounds on $P(M)P(D|M)$ to minimize the time spent evaluating a candidate model. Alg. 2 takes advantage of the following three cases.\nThe first (line 9) allows us to update the current best score and model when its lower bound is above the current best score. The second (line 16) allows us to mark a model as a potential candidate to be revisited when the current best score lies between its upper and lower bound. The third (line 18) allows us to dismiss a model as soon as its upper bound is below the current best score."}, {"title": "Experiments", "content": "We evaluate our approach both in terms of quality of output and runtime performance on the well-known BPIC 2017 event log [11] which is commonly used as a running example in the EKG literature and for which hand-built EKGs exist. It contains a total of 19 features, resulting in a search space of 524,288 possible models built from atomic features.\nThe algorithm is implemented using a prefix-based, breadth-first branching rule for exploring the model space, which satisfies the monotonic condition on branching rules stipulated in Thm. 4. The subroutine for estimating bounds on linear extensions of a poset uses a depth-first approach with iterative deepening. The stopping condition (see line 3 in Alg. 1) is based on time elapsed\u00b3, set to 1 second in the first rounds, but increasing exponentially when revisiting models that were marked for reestimation on earlier passes. This preliminary implementation considers only atomic features since the implications for monotonicity properties of including derived features was not immediately clear. This is a top-priority for future work.\nTo assess runtime performance, we monitored the increase in the current best score over time for varying numbers (N) and sizes (|D|) of samples, i.e., posets. The size of a sample will typically have a drastic impact on the complexity of estimating bounds on linear extensions, excepting easily computable special cases such as chains or free posets. Since samples are independent, the number of samples will have a linear impact on the time required to evaluate a model, or sublinear with parallelization. There will be some variance in the time required to evaluate each sample when the poset structure differs, but this should amortize across samples.\nIncreased sample size has the important downstream effect of delaying the establishment of bounds early on which - via pruning and model dismissal - allows rapid convergence. Indeed, we see that the poorest performing parametrizations (sample size 128 to 512), take a very long time to establish the very first estimate after which they begin the same rapid convergence pattern.\nTo assess the quality of the discovered models, we compare discovered models with handbuilt models from [12]. Tab. 3 shows an overview of the models"}, {"title": "Conclusion", "content": "We have presented the first algorithm for the automated discovery of event knowledge graphs (EKG) from uncurated data, and shown that it can discover models congruent with handbuilt EKGs within a reasonable runtime. Our approach builds on a principled, probabilistic framing: viewing the outcome space of an EKG as the linear extensions it admits when interpreted as poset.\nThis framing lays a foundation for several natural extensions, in particular in extending or redefining the notion of outcome space. This could include pairwise event orders, a concept ubiquitous in literature on linear extensions. And importantly, adding event features themselves to the outcome space would permit the discovery of patterns at higher levels of abstraction than individual events, including predictive models whose utility would be intuitively demonstrable.\nWhile the theoretical results presented hold for both atomic and derived features, the latter were omitted from our implementation. Incorporating these is a top priority for future work and requires specifying appropriate branching rules that preserve monotonicity and take advantage of the subsumption of atomic feature relations by derived.\nAnother crucial question to be addressed concerns the quantitative evaluation of discovered models. Given a hand-built model, how do we quantitatively measure closeness of a discovered model, e.g. using graph matching, edit distance, or other similarity metrics? How can discovered EKGs be evaluated without reference to hand-built models, e.g. by comparison to held-out portions of the dataset: whether events or relations? This would permit a more traditional framing of the task as learning problem in particular, might we harness modern graph encoding techniques, e.g. graph neural networks, to tackle this problem or a reasonable reformulation of it? While the real-world utility of any EKG discovery techniques will need to be evaluated w.r.t. concrete use cases, the results presented here should provide a well-founded and extensible point of reference, and a baseline for alternative approaches."}]}