{"title": "A Survey of Deep Learning Methods in Protein Bioinformatics and its Impact on Protein Design", "authors": ["Dai, Weihang"], "abstract": "Proteins are sequences of amino acids that serve as the basic building blocks of living organisms. Despite rapidly growing databases documenting structural and functional information for various protein sequences, our understanding of proteins remains limited because of the large possible sequence space and the complex inter- and intra-molecular forces. Deep learning, which is characterized by its ability to learn relevant features directly from large datasets, has demonstrated remarkable performance in fields such as computer vision and natural language processing. It has also been increasingly applied in recent years to the data-rich domain of protein sequences with great success, most notably with Alphafold2's breakout performance in the protein structure prediction. The performance improvements achieved by deep learning unlocks new possibilities in the field of protein bioinformatics, including protein design, one of the most difficult but useful tasks. In this paper, we broadly categorize problems in protein bioinformatics into three main categories: 1) structural prediction, 2) functional prediction, and 3) protein design, and review the progress achieved from using deep learning methodologies in each of them. We expand on the main challenges of the protein design problem and highlight how advances in structural and functional prediction have directly contributed to design tasks. Finally, we conclude by identifying important topics and future research directions.", "sections": [{"title": "1 Introduction", "content": "Proteins form the basic building blocks of organisms and perform various functions through chemical reactions with molecules. They are composed of chains of 20 possible amino acids that stabilize to a 3D structure based on their sequence. A protein's sequence, structure, and function are closely related: sequence largely determines structure, and structure largely determines functionality. The ability to understand proteins, from their initial structural conformation to their functional properties after stabilization, are vital for bioengineering tasks such as drug design, disease identification, and therapeutic treatment.\nImproving technologies for protein sequencing and structure determination have led to the rapid growth of various databases such as PDB [13], UniRef50 [108], BindDB [10], and many others, which collect structural and functional information of various sequences. Despite the growing knowledge of existing sequences and their properties, our understanding of the natural processes that drive them remain limited. The inter- and intra-molecular forces involved are highly complex, making traditional statistical and physical modelling approaches challenging. The possible orderings and combinations of amino acids also form a massive sequence space [33], and existing databases are only limited to the space explored by natural evolution [46]. Important tasks such as de novo design however, which involves designing proteins rarely occurring in nature, require an intimate understanding of the relationship between sequence, structure, and function.\nDeep learning techniques have been increasingly applied to the field of bioinformatics in recent years. These techniques are characterized by their ability to learn relevant features automatically through gradient descent using large datasets, and have been shown to consistently outperfrom traditional modelling techniques in different fields. The increasing availability of protein sequencing data has made protein bioinformatics a particularly suitable domain for deep learning. This is particularly apparent in the protein structure prediction problem, which aims to determining the 3D structure of a protein when only given its sequence. In 2020, Deep Mind's Alphafold2 model was able to achieve structural prediction accuracy comparable to experimental validation methods in the CASP14 competition, arguably \"solving\" the problem. Impressive progress has also been made in other problems such as protein functional prediction, protein-protein interaction (PPI), binding site identification, etc by similarly applying deep learning techniques.\nProtein design remains a challenging problem however since it involves by definition discovering sequences or structures that may not occur naturally. Whereas tasks such as structural and functional prediction are relatively well defined and accompanied by relevant datasets, protein design requires discovering stable molecules subject to some functional objective. This also happens to be the most relevant for problems such as drug design and disease treatment and have the largest potential for social benefit.\nIn this paper, we systematically review the progress that has been achieved from applying deep learning methodologies to protein bioinformatics and how they have unlocked new possibilities for improvements in protein design. We follow the common paradigm of broadly classifying problems into three main categories [23]: 1) structural prediction, 2) functional prediction, and 3) protein design, and start by first reviewing recent progress for categories 1) and 2), which are well defined problems addressable with large labelled datasets. We then discuss the main challenges of 3), recent progress from applying deep learning techniques, and how breakthroughs in 1) and 2) have also led to improvements in design by treating it as the inverse of structural and functional prediction. Finally, we conclude by discussing potential future research directions in applying deep learning to protein design."}, {"title": "2 Background", "content": "2.1 Key concepts of deep learning\nDeep learning has seen rapid advancements in recent years and achieved remarkable performance on a wide variety of computer vision (CV) and natural language processing (NLP) tasks. These advancements have also been successfully applied to other fields, including bioinformatics. In this section, we briefly cover some of the key ideas and advantages of deep learning compared to traditional modelling. We then review some of the common architectures used that are also relevant to this survey.\n2.1.1 Learning through gradient descent\nDeep learning architectures typically refer to models consisting of multiple layers of stacked neural networks. One of the simplest neural network architectures is the single layer perceptron (SLP). For some model $f(\\cdot)$, the single layer perceptron is defined by:\n$f(x) = a(wx + b)$  (1)\nwhere w is a weight vector, b is a bias term, and a is a non-linear activation function such as sigmoid, tanh, or ReLU. Neural networks are trained through back-propagation, which involves updating weight parameters iteratively based on a pre-determined loss function. For some dataset $\\{x_i, Y_i\\}_1^N$ where X is the sample input, Y is the sample label, and N is the total number of samples, we obtain our label estimate using our SLP, $f(x_i) = \\hat{y_i}$, and calculate our loss using some pre-determined loss function, $L(\\hat{y}, y)$. Our goal is to find the optimal parameters for w, b that minimizes the loss, i.e.\n$\\min_{w,b} L(f(x; w, b), y)$.  (2)\nThe minimization process is done through stochastic gradient descent, which involves a forward and backward propagation step. During each iteration time-step, t, a set of inputs are fed into the network to obtain a prediction and loss value through forward propagation. Backward propagation is used to calculate the gradient of the parameters with respect to the loss value and update weights based on the gradient. We can describe this process as follows:\n$w_{t+1} = w_t - \\gamma \\frac{L(f(x; w_t, b_t), y)}{w_t}$  (3)\nwhere $\\gamma$ is the learning rate parameter. This process can be seen as navigating the loss surface to some minimum parameter configuration, as represented in Figure 1(b). The SLP can be made deeper by stacking numerous layers on top of each other, forming a multi-layer perceptron (MLP), which is described in Figure 1(a). MLPs typically do not perform well and have a tendency to over-fit to the training data. This is because no additional constraints are imposed on the large number of parameters and the model fails to learn relevant representations. To correct this, concepts such as spatial invariance and equivariance have be introduced into network operations. Examples of these are convolutional operations for translational equivariance in Convolutional Nueral Networks (CNNs) and translational invariance using pooling operations. These effects combine to reduce model search space. For a more detailed and technical explanation, we refer readers to [34].\n2.1.2 Comparison with traditional modelling approaches\nOne of the key advantages of deep learning over traditional modelling approaches is that deep learning does not rely on manually determined features. Instead, the model is able to learn a large set of relevant features automatically during the training process. These features have been demonstrated to perform better than manually defined ones, since manually features are often determined based on fragile, biased, or incorrect assumptions.\n2.1.3 Common Deep Learning Architectures\nConvolutional Neural Networks (CNNs) - CNNs are a class of deep learning models that use the convolutional neural networks as core components of its model. The convolution kernels serve as filters with weights that resemble manually defined kernel filters after training. The use of convolutional filters also reduce the number of model parameters through weight sharing, thus enabling translational equivariance. This class of models include VGG [106], ResNets [40], DenseNets [45], and others.\nRecurrent Neural Networks (RNNs) - RNNs capture sequential information by forming connections between nodes in the temporal domain. They often contain an internal memory state that allows it to retain information from previous time-steps when processing new time-steps. RNNs are typically used for sequential inputs in NLP problems although they have also been adapted to CV tasks.\nAuto-encoders (AEs) / Variational Auto-encoders (VAEs) - AES/VAEs are networks trained with the specialized task of learning a representational coding of data. They typically consist of an encoder and decoder, and are characterized by a bottle-neck structure, where the data is reduced in dimension in the middle. The bottle-neck structure effectively forces the network to learn an efficient low-dimensional coding representation for the data. The network is usually trained with reconstruction loss in a unsupervised setting.\nGenerative Adversarial Networks (GANs) - GANs are networks trained for generative tasks. They are trained with both a generator network, trained to generate the target outputs, and a discriminator network, trained to determine whether a sample is real or generated. Training with two networks allows the output to better reflect the training distribution.\nTransformers - Transformers were first used in NLP problems and were shown to perform better than RNNs, which are vulnerable to exploding gradients during training. These architectures use multi-head self attention modules that allow the network to learn which inputs and features to focus on across a larger input window [114]. Transformers were later shown in [24] to outperform traditional CNNs for computer vision problems as well. The major advantage of transformers over traditional CNNs is that transformers are able to consider long-range relationships between inputs, whereas CNNs are limited by the field of view of its kernels. Because of its more general form however, the original transformer architecture required large amounts of data and are difficult to train. Subsequent advancements such as the SWIN transformer [66] and Reformer [52] have made improvements in this area however. Transformers are playing an increasing role in various domains, including bioinformatics, and are increasingly adopted in model architecture designs.\n2.1.4 Architectures for Geometric Deep Learning\nArchitectures mentioned in Section 2.1.3 are typically used for problems in the temporal space, 2D Euclidean plane, 3D Euclidean space. Many important problems are outside of these standard spaces however and may involve graphs, groups, or manifolds. This is particularly true for proteins as it is difficult to directly process protein structures as 3D volumes. Geometric Deep Learning is a more generalized framework that covers feature transformation and aggregation operations for a variety of input domains, including graphs, manifolds, groups, and also the standard Euclidean spaces. We give a brief overview of the network classes relevant to protein bioinformatic problems. For a more detailed treatment, we refer readers to [16].\nGraph Convolutional Neural Networks (GCNNs) - GCNNs operate on graph data, where sample inputs consists of vertices and edges between them. Features are represented as vertices, and edges are used to determine feature aggregation between vertices during learning. Graphs are a highly general representation for different data types, and can also be used to represent 1D, 2D, and 3D Euclidean data by treating the inputs as a grid. Equivalents of convolution, pooling, and attention operators in Euclidean space data are also used for feature extraction for GNNs. Figure 2 gives a generalized overview of different GNN structures.\nGeodesic CNNs - Geodesic CNNs are used on manifolds and intuitively adapt the convolutional operator to irregular surfaces. Geodesic patches are patches on a manifold where the edges are of the same length to a center-point. Different feature aggregation and pooling functions can then be applied on the patches for feature learning. Extracted features can then be used with different predictive heads for different tasks similar to standard deep learning architectures. Figure 3 illustrates the difference between standard Euclidean and geodesic metrics on surfaces."}, {"title": "2.2 Protein Bioinformatics", "content": "Bioinformatics is the application of computer science methodologies to the treatment of biological data, mainly DNA and proteins, although we limit our scope to proteins in this paper. We provide a brief background to proteins and give an overview of the main problem classes in protein bioinformatics.\n2.2.1 Protein composition and interaction\nProteins are chains of amino acids joined together by peptide bonds. The bond joins the carbon atom of one amino acid chain with the nitrogen atom on another amino acid chain, releasing a water compound in the process. An example of this is shown in Figure 4. The amino acid residue forms as a side-chain on what is referred to as the $C_\\alpha$ atom. Consequently, the backbone of an amino acid chain can be described by a series $C_\\alpha$ atoms in 3D space. The side-chain residues determine the various properties of the protein molecule and also restrict the structural conformation.\nProtein structures are generally described using the term primary, secondary, and tertiary structures. Primary structure refers to the generated but unfolded sequence. Secondary structures are those formed when folding first begins and consists of alpha-helices or beta-sheets. This process happens quickly and is driven by hydrogen bonds within the backbone chain. Tertiary structures are formed through interactions between the secondary structures and determined by surface hyrdophobic/hydrophilic properties as well as other forces. Bonds between residue pairs may also form. Sometimes, the formed structures also interact with additional amino-acid chains to form what are known as quaternary structures.\nProtein folding is a dynamic process that releases energy and are driven by hydrophilic/hydrophobic forces, Van der Walls forces, and conformational entropy [7, 63]. In most cases, the structures stabilize at minimum free entropy, although it is possible that they stabilize at a higher energy level because they are unable to dynamically reach a lower configuration. There are sometimes variations in the folds, and it is possible that some sequences stabilize to different structures depending on their external environment. On the other end of the spectrum, proteins with the same structures sometimes may have slightly different sequences due to evolutionary mutation between and within species.\nProteins perform functions through chemical reactions with surrounding molecules. More specifically, proteins bind to highly specific targets, which are referred to as ligands, through non-covalent bonds, bonds where electrons are not shared between atoms, on their surfaces [1]. Binding sites of proteins fit tightly with target ligands like a glove, as shown in Figure 5. Individual non-covalent bonds, which are determined by properties of surface poly-peptide residues, are extremely weak, but the tight fit combines many individual bonds to strengthen the binding force. In some cases, specific characteristics of the protein surfaces contribute to the binding strength. For example, in some structures there may exist clusters of negatively charged residues that normally repel each other, but are held together by their structure to attract positive ions. Figure 5 shows some of the typical types of protein binds.\n2.2.2 Key problems in protein bioinformatics\nProtein properties are primarily determined through manual experimental approaches. This means our knowledge of the universe of proteins are limited to those that are naturally existing and bottle-necked by the speed of experimentation. Because of this, many bioengineering problems, especially the task of de novo protein design, remain particularly challenging. A large part of protein bioinformatics revolves around discovering relationships between protein sequence, structure, and function, using computational methods. Based on this, we broadly classify the problems into 3 categories:\n1. Structural prediction from sequence\nThe problem of determining the structure of protein purely based on its sequence (sequence $\\rightarrow$ structure). This is generally a well defined problem with public challenges that are held regularly to benchmark process.\n2. Functional prediction from sequence or structure\nThe problem of predicting functional properties of a protein given its sequence or structure (sequence $\\rightarrow$ function || structure $\\rightarrow$ function). This includes tasks such as functional prediction, binding site prediction, protein classification, PPI etc. and is much more varied\n3. Protein design\nThe problem of determining stable sequences given some desired functional property or pre-determined structure (function $\\rightarrow$ sequence || structure $\\rightarrow$ sequence). These tasks are the most difficult as multiple sequences may conform to the same structure or perform the same function. Generated proteins must also be synthesized experimentally for truly accurate validation, thus limiting the availability to evaluate and benchmark performance"}, {"title": "3 Structural prediction from sequence", "content": "Protein structures are currently determined through techniques such as NMR spectroscopy and X-ray crystallography. The problem of protein structure prediction from amino-acid sequences involves inferring structure computationally from sequences without the need for manual experimentation. Known structural information on tested proteins sequences are stored in large data banks such as the PDB, which contains information on over 180,000 sequences. The problem is therefore well defined with readily available datasets.\nPublic competitions as such as CASP and CAMEO are held in regular intervals to benchmark progress so far in structural prediction. Although neural networks have been used for this problem from as early as 1997 [68], the competition was dominated by traditional statistical modelling for many years. Deep learning approaches only became a serious alternative in CASP12 with the appearance of Raptor X [117]. This was followed by Deep Mind's win in CASP13 with Alphafold1 [104] and their incredible performance in CASP14 with Alphafold2 [49], which achieved near experimental accuracy.\nIn this section, we first briefly go over key ideas in structural prediction in traditional modelling approaches. We then review the common deep learning techniques that have been used so far, as well as the current state-of-the-art approach.\n3.1 Traditional methods and key ideas\nTraditional approaches to protein fold modelling are based on searching through the space of possible structural configurations to find the minimum entropy configuration. The search process is computationally expensive however, although a number of key ideas have been utilized to reduce the search space and improve accuracy.\nHomogolous search and threading - The simplest approach to structure prediction is to approximate the structure with highly similar sequences or sub-sequences with known structures. This can be effective if close sequences exist, but have very limited performance when this is not the case. Nevertheless, the tools developed through this approach are still widely for finding closely aligned sequences of proteins, which are referred to as multiple sequence alignments (MSAs).\nSequence alignment through large protein databases is an algorithmic challenge. The most successful algorithm, still often used to this day, is the PSI-BLAST algorithm, proposed in [4]. PSI-BLAST builds upon the BLAST algorithm, which performs alignment search by optimizing for a similarity heuristic instead of performing identity matching between two sequences, thus improving efficiency and accounting for mutations. After the BLAST algorithm is run to find alignments with high similarity scores, alignments are used to calculate a position specific scoring matrix (PSSM), which reflects the probability of a amino acid appearing at a sequence position. The PSI-BLAST algorithm then uses the PSSM to improve further alignment searches whilst refining the PSSM value iteratively. PSSMs capture important information within MSAs are also frequently used as inputs features for deep learning approaches.\nContact pair predictions - Contact pairs within a protein structure refer to points within the protein backbone that are close together in the final fold. These are typically defined as pairs of $C_\\beta$ atoms that are less than 8 angstroms (\u00c5; 1\u00c5 = 10\u221210m) in distance. It has been recognized early on that accurately determining contact pairs within a protein sequence significantly helps constrain the structure search space, as it serves to fix a number of key positions in the backbone. Consequently, early works have attempted to directly predict contact points directly from sequence. Altschul in 1997 [4] explored using neural networks through SLPs for contact prediction. Other works such as [29] and [39] include additional features to improve accuracy for contact pair prediction to the level of 21%, which was state-of-the-art for their time. Stacked layers in the form of MLPs were also explored in their experiments but did not perform well due to lack of data at the time. The low overall accuracy limited the usefulness of these methods.\nEvolutionary data - One of the major insights behind protein structures is that some sequences, although vastly different, perform the same function, and consequently have similar structures. Therefore, the correlation between them contains useful information on what parts of the sequence determine their structure. This is known as co-evolution information and is obtained through what are termed evolutionary coupling analysis (ECA) techniques. [32] uses mutual information (MI) between two sequences of the same functional family to predict contact points. MI only captures correlation between local segments in the sequence however, and is not effective for long range relationships in the sequence, a common theme in the challenge of structural prediction. [71] improved upon this approach by introducing a measure termed \"Direct Information\", which includes more global information to refine contact pair prediction. Structural search based on these constraints were able obtain predictions that were within 2.7-4.8 \u00c5 of the ground truth structure in their test sample of proteins. The PSICOV model in [48], which is still used at times as feature inputs for deep learning models, captures ECA information through computing a precision matrix, which is the inverse of the correlation matrix. Sample correlation matrices are not always singular, and their algorithm calculates a sparse inverse covariance matrix through the graphical Lasso algorithm.\nStructural Search - The techniques mentioned above rely on the availability of known sequence-structure pairs for reference. This is not possible for newly discovered protein families, which require ab initio folding: folding directly from energy minimization. Even after constraints are found through the above methods, a search needs to be performed through structural space to determine the minimum entropy structure. The most successful algorithm for structural search is the Rosetta algorithm [99], which is based on the intuition that sort fragments of residues tend to arrange themselves in similar ways. The algorithm first breaks the target sequence into sliding windows of 3 or 9 residues and uses the PSI-BLAST algorithm to find a number of top matches and their torsion angles. Monte Carlo simulated annealing search is then performed from to optimize the potential energy surface (PES), which is approximated statistically based on their frequency and locations in known structures. Prior to CASP13, Rosetta was the most popular and effective method for performing structural search after constraints were identified. The original paper cites accuracies of 3-6 \u00c5 for sequences of 60 or more residues without known structural templates.\nMolecular dynamic simulations - We make a brief mention of molecular simulation techniques. This method attempts to use physics-based simulations to model the formulation of individual amino acids [72], although the large computational requirements limits it mostly to proteins of small sizes.\nOverall framework - From the above, we can see the structure prediction process can be largely divided into a two stage process: constraint identification and structural search. Prior to Deep Mind's success in CASP, the most effective prediction algorithm was by \"Zhang and QUARK\" [125] which used neural networks and model ensembles to perform contact prediction before performing structure search. Common themes remain however in terms of the main difficulties, namely capturing long distance relationships within sequences, efficient structural search, and modelling of proteins with no close templates."}, {"title": "3.2 Treating distance maps as a picture", "content": "Raptor X - Raptor X was one of the first deep learning models that challenged traditional statistical methods in terms of performance, appearing in CASP12 [117]. It's key innovation is that it treated the pair-wise contact map as an image, with the task of identifying residue pairs that are in contact. This allowed recent developments in the computer vision domain, such as the 2D ResNets architecture, to be carried over. The deep nature of ResNet networks allows the model to increase its focal view for lower layers, which addressed the challenge of long-range sequence relationships. Inputs consisted of the raw sequence, MSA information from PSSM and ECA infomation from the CCMpred model amongst others, whilst the output was a binary prediction of contact between a given residue pairs. The contact pair predictions were then used as constraints for structural search using the CNS tool-suite.\nAlphafold1 - Alphafold1 was conceptually similar to Raptor X. One of the main differences is instead binary prediction of contact, it predicted a distance distribution between pairs, which they found to improve performance significantly. Another innovation was that instead of using simulated annealing search methods, Alphafold1 was able to express the structure entirely as a series of torsion angles. The final structure was therefore fully differentiable and could be learnt through gradient descent.\ntrRosetta - Rapid improvements were made upon Alphafold1's methodology after its release. Yang et al.'s trRosetta model [119] introduced additional constraints into the model by outputting orientation angles between residues in addition to residual pair distances. The residual distance, 3 dihedral angles, and 2 planar angles predicted by the model are able to fully specify the orientation between residue pairs in 3D space. Rosetta algorithms were then used to perform search based on these constraints. The additional constraints allowed them to obtain higher accuracy on targets, especially those without reference templates, compared to Alphafold1."}, {"title": "3.3 Transformers and graphs", "content": "Alphafold2 - Alphafold2's approach in CASP14 broke away from the traditional two-stage paradigm of constraint prediction followed by structural search. Instead, an end to end model is trained, such that given an input sequence, a structural prediction is directly outputted by using a structure module, from which it is iteratively refined. Another major innovation is that instead of using MSAs directly as inputs for contact prediction, MSAs are used together with 2D residue distance maps to learn cross-attention between the mediums.\nThe model diagram is shown in Figure 8 and can be roughly divided into 3 components [49]. In the first component, MSAs are first found using standard bioinformatic tools from protein and meta-genomic databases. This produces a number of close alignments with the target sequence from different proteins and species. Close template matches are found from the PDB structural database using HHSearch, from which pairwise residue distance maps are calclated. The second component is composed of a series of transformer modules, termed Evoformer blocks, that learn self-attention and cross-attention between MSAs and residue maps. Intuitively, correlations inferred from the MSAs can be used to guide learning from the structural templates, whilst correlations between templates can also teach the model where to focus on for MSA inputs. The third component uses the learned features from the Evoformer blocks to form structure prediction. Amino-acid residues are treated as gas particles in 3D space starting at origin. At every iteration, the structure module outputs an affine translation of the residues in 3D space until they are stable. A novel attention mechanism, termed \"Invariant Point Attention\" (IPA) is used to introduce invariance to translations and rotations of the overall structure in 3D space. The supplementary methodologies section of the paper contains many additional details and techniques such as the use of multiple loss functions, unsupervised learning through MSA masking and self-distillation. We only provide a review of the key concepts in this paper.\nRoseTTAFold - RoseTTAFold was a large community effort at replicating the results of Alphafold2 out of fear that Deep Mind would refuse to publicize their methodology. A lot of its methodology was based on a single network diagram released by Deep Mind after CASP14. A series of transformers for 1D MSA inputs, 2D template inputs, and 3D graph inputs were used to output a final model prediction, which is then iteratively refined. It uses the SE-3 Transformer, which is a self-attention module for 3D point clouds and graphs with rotation and translation equivariance constraints, as a core module. Notably, it is achieved an average TM-score of 80 on CASP14 targets versus 90 of Alphafold2, but achieved with significantly less computational resources.\nMSA Transformer - Transformers have also been explored for MSA data in [92] to learn unsupervised protein language models. Similar to grammar rules in everyday language, protein sequences are constrained through natural evolution to perform specific functions, thus imposing structural restraints. The model is trained similar to BERT and GPT-2 language models through the masked language modeling objective, where the model predicts the original values of masked out amino acid residues from input sequences. The attention mechanism is designed such that the model learns which sequences and where to focus on for relevant information. The MSA Transformer, designed with 100M parameters, was trained unsupervised on 26 million MSAs, each with an average of 1192 sequences. The features from the resulting model was able to outperform trRosetta in predicting contact map prediction, which also directly leads to more accurate structural models. They also found that the attention mechanism learnt by the Transformer corresponded well with contact points within the sequence, and that accurate inference is achieved even for sequences with little few alignments in existing databases."}, {"title": "3.4 Others architectures", "content": "Recurrent geometric networks (RGNs) The RGN model is another approach to learning protein structure end-to-end from input sequences, such that 3D coordinates for individual $C_\\alpha$ atoms are directly outputted. The model utilizes a bi-directional LTSM to learn features and outputs the torsion angles of amino-acid residuals sequentially to build the overall backbone. Although the model only uses the input sequence and its PSSM instead of raw MSAs and additional features, it achieves comparable performance to top results in CASP13 with significantly faster inference speeds at just a few millisconds. Extensive work in NLP has shown transformer models are superior than RNN structures in capturing long range dependency however, and it is questionable whether iterations upon RNN architectures will catch-up to Alphafold2's performance.\nGANS - Although GANs have not been used competitively for structure prediction in CASP or CAMEO competitions, some works have used them creatively for structure reconstruction. [6] used GANs to show that it is possible to reconstruct masked components in contact distance maps by treating it as an image in-painting problem. The in-painted distance maps can be used to accurately reconstruct 3D structure. Such methods have also been shown useful for a special class of structure prediction known as loop modelling, where segments of known structures from irregular, sometimes flexible, loops that are hard to determine even experimentally [102]. [64] similarly treats loop prediction as an image in-painting problem by using GANs to estimate the inter-residue distance map for loop segments.\n3.5 Summary\nThe extra-ordinary performance of Alphafold2 is a great leap forward in our understanding of structure and vastly outstrips existing approaches. Despite this, there are still important problems in structure prediction that remain to be addressed. Essential life functions tend to be carried out through multi-protein complexes, where interactions between multiple structures drive vital processes. Individual protein structures within these complexes remain challenging to model and are outside the scope of CASP. Also, the current accuracy of Alphafold2 in RMSD terms of 1.6\u00c5, which is highly impressive but still too large for direct application in drug design, where confidence of individual atoms needs to be within 0.3\u00c5. Sub-classes of structure prediction problems, such as loop modelling, which are not as well-defined, also have room for improvement. Thus, there remains open topics for research in the field of structural bioinformatics for proteins."}, {"title": "4 Functional prediction", "content": "In this section, we look at the deep learning methodologies used for functional prediction. Unlike structure prediction, where the task is specific, well-defined, and bench-marked by regular competitions, the problem of functional prediction is much wider in scope, often with varied targets and objectives, such as enzyme/non-enzyme, solubility, toxicity, and Gene Ontology (GO) classification. Functional prediction can also take place on both a global scale, such as protein classification, or local scale, such as binding site and phosphotylation site prediciton within a protein structure. We also look at the problem of functional prediction from two input domains: sequence and structure.\n4.1 Function from sequence\nFunction from sequence involves functional prediction by directly using the amino-acid sequence as input, thus operating on the 1-Dimensional input space. Functional prediction can be approached by directly training a classifier based on labels for functionality, which is a supervised learning approach. Labels for desired functionality can be sparse however. For example, the GO database [9] only contains labels for <1% of if the proteins in UniprotKB, which number above 70 million. Advances in deep learning within other domains, in particular language modelling for NLP, have shown that unsupervised or semi-supervised learning approaches using vast unlabelled data can significantly boost performance. Attempts have also been made to carry these idea to the task of functional prediction for proteins. We investigate both supervised unsupervised approaches in the following sections.\n4.1.1 Fully supervised approaches\nGO functional prediction - One of the more well-defined challenges in functional prediction is prediction of Gene Ontology (GO) function labels. The GO database contains over 40,000 functional classes, and each protein is labelled with the functions it must perform in order to function [9]. The labels only represent less than 0.1% of all the known proteins in the UniProKB database however [21]. The community challenge, CAFA (Critical Assessment of protein Function Annotation algorithms), is held in 2-4 year intervals, with the results of CAFA4 still under assessment [126]. The official winner for CAFA3, announced in 2019, was GOLabeler [124], which uses features extracted from multiple statistical models including BLAST-KNN [89] and ProFET [83], and ensembles them in a ranking model to find top K predictions. Further iterations by the authors included NetGO [123], which used additional information from the STRING database [109] to improve performance.\nDeep learning approaches to the problem was first applied in DeepGO [59], where the authors used an embedding layer, followed by a single 1D convolution layer for feature extraction. Because of the hierarchical nature for some functional classes in the GO labels, a series of hierarchical binary classifiers were also used for final class prediction. The results achieved surpased GOLabeler and NetGO. DeepGO+ [58] improved results further by simplifying the input and output modules of the architecture and increasing the number of 1D CNN layers.\nMore sophisticated architectures such as RNNs and Transformers have also been used. One of the challenges from dealing with protein sequences is the large variation in sequence length. [91] uses bidirectional LSTMs to condense input sequences into a fixed-size feature vector, which is then used to classify GO functional labels, although performance is not compared with [59, 58]. TALE uses the Transformer architecture [18] to perform GO label classification but with an additional label embedding. In order to capture the hierarchical nature of GO labels, labels are represented as a directed acyclic graph and embedded as an additional feature input. They were able to achieve state-of-the-art results for 2 out of 3 problem subclasses and competitive results with NetGO and GOLabeler for the remainder. Notably, pre-training the Transformer through unsupervised training did not lead to performance improvements.\nSite-specific prediction tasks - Supervised deep learning has also been used effectively on local classification tasks. On such problem is prediction of protein phosphorylation sites, which are locations on a protein structure that are modified by covalent bonding of a phosphate group. This process takes place as part of the regulation process for protein activity. Deep-phos [69] uses segments of the protein sequence as input, and a novel CNN block, which they term DC-CNN, for feature extraction. Each DC-CNN is based on 1D convolutional operations with connections between earlier layers. Outputs from 3 DC-CNN blocks with different window sizes are then aggregated for feature extraction, which are then classified as phosphorylation or non-phosphorylation sites. [91] also performs phosphorylation site classification with their MUscADEL model. MUscADEL uses a standard Bidirectional LSTM followed by a fully connected layer to perform site classification for different locations of the protein sequence. With this architecture, they were able to improve upon state-of-the-art performance of more traditional statistical methods."}, {"title": "4.1.2 Unsupervised / Semi-supervised approaches", "content": "Supervised learning limits the usable dataset to only those with annotated labels. This ignores the vast amount of unlabelled data available for protein sequences. Similar to language models in NLP, where valid sentences must follow generally accepted grammar rules and word relations, naturally occurring protein sequences must obey the natural constraints that allow them to fold into functional structures. Consequently, unsupervised and semi-supervised techniques have been used to try and capture this underlying information. Most approaches attempt to demonstrate generality of learnt representations however in contrast to benchmarking perfomance on a specific task.\nLearning latent representations - The authors in [22"}]}