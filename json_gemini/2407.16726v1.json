{"title": "Topology Reorganized Graph Contrastive Learning with Mitigating Semantic Drift", "authors": ["Jiaqiang Zhang", "Songcan Chen"], "abstract": "Graph contrastive learning (GCL) is an effective paradigm for node representation learning in graphs. The key components hidden behind GCL are data augmentation and positive-negative pair selection. Typical data augmentations in GCL, such as uniform deletion of edges, are generally blind and resort to local perturbation, which is prone to producing under-diversity views. Additionally, there is a risk of making the augmented data traverse to other classes. Moreover, most methods always treat all other samples as negatives. Such a negative pairing naturally results in sampling bias and likewise may make the learned representation suffer from semantic drift. Therefore, to increase the diversity of the contrastive view, we propose two simple and effective global topological augmentations to compensate current GCL. One is to mine the semantic correlation between nodes in the feature space. The other is to utilize the algebraic properties of the adjacency matrix to characterize the topology by eigen-decomposition. With the help of both, we can retain important edges to build a better view. To reduce the risk of semantic drift, a prototype-based negative pair selection is further designed which can filter false negative samples. Extensive experiments on various tasks demonstrate the advantages of the model compared to the state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "Graphs are capable of modeling complex interactions between objects that occur naturally in many real-world scenarios, such as in social networks, shopping websites, and citation networks [1]. Learning and exploring the features of nodes and structures on these graphs facilitates various real-world challenges and applications. Graph Neural Networks (GNNs) and their various variants are a class of methods that can be used for representation learning on graph data. In recent years, they have achieved great success in dealing with graph analysis problems such as node classification and clustering [2, 3, 4], link prediction [5].\nMost of these GNN methods adopt the paradigm of supervised learning, which extensively rely on label information to guide model learning [6]. However, high-quality labeled data is laborious and expensive to collect in the real world. Recently, self-supervised learning (SSL) is a promising paradigm for exploring the knowledge inside unlabeled data to alleviate the dependence on labeled data. With the great success of SSL in computer vision [7] and natural language processing [8], researchers also naturally apply SSL to graph-structured data [9, 10]. Existing SSL methods can be divided into three categories: predictive, generative, and contrastive [11]. Predictive methods generate pseudo-labels with general prior knowledge, and design prediction-based auxiliary tasks to train [12]. Generative models generally regard the rich information in graph data such as structure and attribute information as self-supervised signals for reconstruction learning [13]. Contrastive learning is one of the most successful area in SSL [14]. It has achieved comparable or better results than supervised learning in representation learning task, which is the focus of this paper.\nThe main components in contrastive learning (CL) are augmentation, positive-negative pair selection, and contrastive objective [15]. In particular, we first generate multiple contrastive views for each instance through data augmentation."}, {"title": "2. RELATED WORK", "content": "In this section, we review the related work from three aspects: graph neural networks, unsupervised graph representation learning, and graph contrastive Learning."}, {"title": "2.1. Graph Neural Networks", "content": "Graph exists widely in the real world. Graph neural networks (GNNs), which learn graph embeddings by using attribute features and topological information, have been extensively studied [27]. GNNs usually adopt the message passing paradigm, that is, iteratively updates the representation of nodes by aggregating the representations of their neighbors, and sums up the representation of nodes through pooling operations to obtain the representation of the entire graph.\nGNN is first introduced in [27], which combined graph Laplacian to design a graph convolution operation in the Fourier domain. Then, GCN [2] builds a bridge between the spectral domain and the spatial domain in GNN. It uses the first-order Chebyshev polynomial filter approximation for efficiency and only aggregates node features from first-order neighbors each time. GAT [28] further introduces the attention mechanism to consider the importance of different node neighbors instead of simple aggregation. GraphSAGE [29] provides four functions for aggregating nodes: mean/max/LSTM/Pooling. KerGNNs [30] combines the graph kernel method, which naturally extends the CNN framework to the graph, and brings a certain degree of interpretability. Meanwhile, existing GNN methods have been successfully applied in various fields with great success, such as anomaly detection [31] and node clustering [4, 3]. But most of GNNs are to learn the representation of nodes in an end-to-end manner under the paradigm of supervised learning."}, {"title": "2.2. Unsupervised Graph Representation Learning", "content": "Unsupervised graph representation learning aims to learn node embeddings on the graph when data labels are not available. Early methods focus more on using the structural information of the graph to learn the representation, such as random walk based and kernel-based methods [32, 33, 34]. The former method takes walks across nodes randomly on the graph and then flattens the graph into a sequence for learning. Node2vec [33] is a representative work among them, which can effectively explore different neighborhoods by designing a biased random walk function. Kernel-based methods, such as Graphlet kernels [34], use the dependency information between substructures and then combined it with the kernel function to give the similarity between graphs for representation learning. But they all cannot simultaneously make use of node features and topology information [21]. Recently, the representation learning of data through well-designed pretext tasks without labels has received extensive attention, which can be roughly divided into three categories: contrastive, predictive, and generative [19]. The contrastive-based graph representation learning method will be expanded in the following part."}, {"title": "2.3. Graph Contrastive Learning", "content": "Recently, graph contrastive learning has been extensively studied due to the relatively large success of self-supervised contrastive learning in computer vision and natural language processing [35]. General graph contrastive learning can be roughly divided into three modules: contrastive objective, data augmentation, and positive-negative sample pair selection. The existing work is mostly based on the innovation of three modules [1, 36]. Deep Graph InfoMax (DGI) [16] firstly applies the Infomax criterion to the graph, and proposes to compare the node representation derived from the corrupted graph with the whole graph representation. For the design of data augmentation, MVGRL [21] learns node-level and graph-level representations by injecting global structural information into the graph to obtain a contrastive view. GraphCL [17] and GRACE [19] use the SimCLR [37] framework to propose a variety of heuristic graph data augmentation methods, such as masking node features, discarding nodes, and removing edges .etc. GCA [20] makes a further improvement by introducing prior information, such as node-based degrees, to adaptively augment the graph. COSTA [18] focuses on hidden feature augmentation to avoid getting a biased node representation. Meanwhile, AutoGCL[10] has turned to the use of model augmentation to mitigate the risk of semantic destruction during data augmentation, but the semantic drift in positive-negative pair selection still exists. ProGCL [38] mines the hard negative samples by the technology of mixup. Besides, HomoGCL[9] is the recent work that exploit the homophily assumption to complement graph contrastive learning.\nIn this paper, we consider the guided augmentation for topology which is information specific to the graph, and then improve the instance-based contrastive learning by designing a new negative sample selection strategy to alleviate the problem of semantic drift."}, {"title": "3. THE PROPOSED FORMULATION", "content": "In this section, the terminology and problem definitions are given as follows.\nLet $G = (X, A)$ denote an undirected graph, where $A\\in R^{N\\times N}$ is the adjacency matrix, $N$ is the number of the nodes. $A_{i,j} = 1$ if there is an edge between node $v_i \\in V$ and $v_j \\in V$ and $A_{i,j} = 0$ otherwise, where $V$ is the set of $N$ nodes. $X = [X_1,X_2, ..., X_N]^T \\in R^{N\\times d}$ is the feature matrix, where $x_i$ is the $i$-th row of $X$ and denotes the feature vector of $v_i$. The unnormalized graph Laplacian of $G$ is $L = D - A$, where $D = diag(d_1,d_2,..., d_n)$ is the degree matrix of $A$ and $d_i = \\sum_{j\\in V} A_{i,j}$. When the adjacency matrix $A$ is normalized to $\\hat{A} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, the normalized Laplacian matrix can also be defined as $\\hat{L} = D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}}$. In this paper, we take a GNN encoder [2] as the backbone network.\nDefinition 1 (Graph Neural Network). Given an graph $G = (X, A)$, a typical graph neural network mainly consists of two components: Message Aggregation and Combine:\n$$m_v^{(l)} = AGGREGATE^{(l)} (h_v^{(l-1)} : v' \\in (N(v) \\bigcup v)),$$\n$$h_v^{(l)} = COMBINE^{(l)} (m_v^{(l)}, h_v^{(l-1)}),$$\n(1)\nwhere $AGGREGATE$ and $COMBINE$ are message aggregation and combine functions. $h_v^{(l)}$ represents the embedding of node $v$ in the $l$-th layer, $N(v)$ denotes the neighbor nodes of $v$. For each node, the $l$-hop neighbor information can be captured by stacking an $l$-layer GNN.\nDefinition 2 (Instance-based contrastive learning). Instance-based contrastive learning is one of the widely used paradigms for self-supervised learning (SSL). In principle, for a anchor node $v_i$, the same node corresponding to other views is regarded as positive to be pulled closer, while all other instances as negative to be pushed away. Specifically, given the representation of a positive pair $(z_i, z_i')$, the agreement is maximized for this positive pair and minimized for negative pairs via the standard InfoNCE loss:\n$$l(z_i, z_i') = log\\frac{e^{(z_i z_i')/\\tau}}{\\sum_{z_j \\in B(v_i)} e^{(z_i z_j)/\\tau}},$$\n(2)\n$$L_{info} = -\\frac{1}{2N}\\sum_{i=1}^N (l(z_i, z_i') + l(z_i', z_i)),$$\n(3)\nwhere $N$ is the number of nodes, $B(v_i)$ is a set of negative instances for node $v_i$, $\\tau$ is a temperature parameter.\nIn this work, we focus on the graph representation learning without label supervision. That is, given an unlabeled graph $G = (X, A)$, where $X \\in R^{N\\times d}, A \\in R^{N\\times N}$, we aim to train a GNN-based encoder $f(.)$ to obtain high-quality low-dimensional node representations: $f(X, A) \\in R^{N\\times d'}$, ie. $d' \u00abd$. Then these representations can be adopted in downstream tasks, such as node clustering, classification, and similarity search."}, {"title": "4. METHODOLOGY", "content": ""}, {"title": "4.1. Overview", "content": "In this section, we introduce the proposed model GraphTP in detail. As shown in Figure.2, our model is mainly divided into two steps, data augmentation with topology reorganization and contrastive learning with prototype-based negative pair selection. Specifically, we first augment the topology globally in a targeted manner to obtain an informative augmented view, for which we provide two schemes. (the left part in Figure. 2). In addition, feature masking and edge dropping are performed to further increase the diversity between views. Then, the two contrastive views $G'(X_1, A')$ and $G_T(X_2, A_T)$ are input into the GNN-based encoder to obtain the representations: $Z_1 \\in R^{N\\times d'}, Z_2 \\in R^{N\\times d'}$, respectively. At last, to alleviate the risk of semantic drift [18] in the process of contrastive learning, for each node, we design a prototype-based negative sample selection strategy to select more accurate negative samples that have true semantics irrelevant to the query. The main steps of the model will be introduced: Sec. 4.2 for the designed augmentation and Sec. 4.3 for the prototype-based sample selection."}, {"title": "4.2. Graph Augmentation with Topology Reorganization", "content": "In contrastive learning, the generation of \"view\" is a factor that controls the information captured by the representation [1]. So we should carefully design the augmentation in order that the generated views can reflect the depth-related information inside the data. To this end, we design two schemes, one is the feature-space based, which makes use of the semantic structure in feature space, and the other is the matrix-transformation based, which utilizes the algebraic properties of the adjacency matrix."}, {"title": "4.2.1. Scheme 1: Feature-Space Based", "content": "In real-world scenarios, the correlation between graph and downstream tasks is usually very complex and can be related to its topology or node features, or their combination. Therefore, the implicit relationship between the node in feature space is also important and can be exploited [24].\nIn detail, we build a graph $G_f(A_f, X)$ based on node features. Given the feature matrix of nodes: $X$, we first calculate the feature similarity between nodes to generate a similarity matrix $S$. The similarity here can be measured by various methods. We list three commonly used methods: 1) Heat Kernel,"}, {"title": "4.2.2. Scheme 2: Matrix-Transformation Based", "content": "For the augmentation of topology structure, different from the previous random and local addition and deletion of edges, we aim to quantify the entire topological relationship by its algebraic properties and then augment. With the help of matrix and spectral theory [2], the specific operation is as follows:\nGiven the graph $G = (A, X)$, we first obtain the unnormalized graph Laplacian $L = D - A$, where the $D$ is the degree matrix of $A$ and $d_i = \\sum_{j\\in V} A_{ij}$. So the symmetric normalized graph Laplacian can be denoted to $\\hat{L} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}$, its eigen-decomposition can be formulated as:\n$$\\hat{L} = U\\Lambda U^T,$$\n(5)\nwhere $\\Lambda = diag(\\Lambda_1, ..., \\Lambda_v)$ and $U = [u_1, ..., u_n]$ are the eigenvalues and corresponding eigenvectors of $\\hat{L}$, respectively [2]. Further, assuming the rank of $\\Lambda$ to be $N$, we have:\n$$\\hat{L} = U\\Lambda U^T = \\sum_{i=1}^N \\lambda_i u_i u_i^T,$$\n(6)\nwhere $u_iu_i^T, i = 1, 2, ...,$ form a set of orthonormal base matrices for the topology space $R^{N \\times N}$.\nGenerally speaking, the matrix $u_iu_i^T$ corresponding to the larger eigenvalue $\\Lambda_i$ always indicates the the principal component of the topology space [39]. Meanwhile, inspired by the previous work [40], if we augment the larger eigenvalue with the exponentiation, the topological relationship in the graph can be augmented globally. For each node, the weights of influential edges that control the structural properties [41] of graph are increased, and vice versa. Specifically, for the Laplacian matrix $\\hat{L}$ in Eq. 5, the augmented matrix is defined as:\n$$B = U\\Lambda^\\alpha U^T,$$\n(7)\nwhere $\\alpha$ is a tunable hyperparameter. Similar to the way in Sec. 4.2.1, with the help of $B$, for each node, we find its neighbors with the top-k edge weights to construct a perturbed adjacency matrix $A_p$. At the same time, this operation can also be regarded as the augmentation of the low- and high-frequency information in the graph. Since $0 < \\lambda_1 < ... < \\lambda_n < \\lambda_N < 2$, and the larger eigenvalue always corresponds to high-frequency information [41], performing a power operation of $\\alpha > 1$ is equivalent to highlighting high-frequency information and suppressing low-frequency information to a certain extent, and the opposite is true when $\\alpha < 1$.\nThrough the selection of these two schemes, we have two contrastive views : the graph $G(X, A)$ and the topology augmented graph $G_T(X, A_T)$, where the new adjacency matrix $A_T$ is chosen from $A_f$ or $A_p$. In order to further increase the diversity between views, we introduce the following augmentations [20]: feature masking and the removal of edges, which are denoted to $T$. The detailed operations are introduced as follows:\n1) Feature Masking: At the node feature level, we add noise by using the random masking to some fractions of dimensions in node features, which is similar to salt-and-pepper noise in digital image processing.\nSpecifically, we first define a masking vector $m\\in {0,1}^d$ with the same dimension as the node feature vector, in which each element is independently sampled from a Bernoulli distribution with probability $p_i$, ie., $m_i ~Bernoulli(1 - p_i)$. The probability $p_i$ is calculatied by the method proposed by [20]. The central idea of this calculation is that the probability $p_i$ reflects the importance of the $i$-th dimension of node features.\nTaking the view-1 as an example, the original feature matrix $X$ with the augmentation of masking can be formulated as :\n$$X_1 = [x_1 \\odot m, x_2 \\odot m, ..., X_N \\odot m],$$\n(8)\nwhere the $\\odot$ is the element-wise multiplication, $X_1$ is the augmented feature matrix.\n2) Edge Removal: For the edge augmentation, we adopt a similar scheme to the feature masking. For the edge $(i, j)$ between nodes $i$ and $j$, its discarded probability value $p_{uv}$ is calculated by [20], and then is set as the parameter of the Bernoulli distribution for sampling. Formally, an edge masking vector is defined as $m_e$. Then the element $m_{uv}$ in $m_e$ can be calculated as follows:\n$$m_{uv} = \\begin{cases}\nBernoulli(1 - p_{uv}) & A_{ij} = 1\\\\\n0 & A_{ij} = 0\\end{cases}$$\n(9)\nAfter the above operations, we have the view-1: $G'(X_1, A')$ and the view-2: $G_T(X_2, A_T)$."}, {"title": "4.3. Prototype-based negative pair selection", "content": "The general graph contrastive learning methods adopt the instance-based learning paradigm. Specifically, for an anchor node, all other instances are uniformly regarded as negative and pulled apart in the representation space [15]. However, this simple approach cannot guarantee that these negative instances have different semantics from the anchor, resulting in a risk of semantic drift in node representations, as illustrated in Figure. 1. To alleviate this risk, we propose a new strategy for negative pair selection to choose the instances whose semantics are irrelevant with the anchor, and show it in Figure. 3. Inspired by [42], this semantics is measured by the feature similarity of each instance with the prototype of the anchor. Specifically, for a given anchor $v$ and its corresponding representation $z \\in R^{1\\times d'}$ in graph $G$, we first define the semantic similarity between it and prototype $c$ :\n$$S(Z, C) = \\frac{z\\cdot c}{\\xi_c},$$\n(10)\nwhere the $\\cdot$ denotes the dot product, $c \\in C = \\{c_i\\}_{i=1}^K, C$ is generated by $K$-means on the all node representations. $\\xi_c$ is the concentration of sample distribution around the prototype, and formulated as:\n$$\\xi_c = \\frac{\\sum_{z_i \\in Z_c} ||z_i - c||^2}{Z_c log(||Z_c|| + \\epsilon)},$$\n(11)\nwhere $\\epsilon$ is a smooth parameter that balances the concentration between different clusters, avoiding small clusters with too large concentration [42]. $Z_c$ is the set of node representations within cluster $c$, where $c\\in \\{c_i\\}_{i=1}^K$.\nThus, the prototype corresponding to this anchor $v$ can be denoted as:\n$$c(z) = argmax_{c \\in \\{c_i\\}_{i=1}^K} S(Z, C)$$\n(12)\nAt last, we could conduct the negative instance selection. For a candidate $z_j \\in R^{1\\times d'}$ in the negative candidate set $B(v)$ of anchor $v$, we choose it if its semantic similarity to $c(z) \\in R^{1\\times d'}$ is smaller than than those of other prototypes, so its selected probability can be defined as:\n$$p(z, z_j) = 1 - \\frac{exp[s(z_j, c(z))]}{\\sum_{i=1}^K exp[s(z_j, C_i)]}$$\n(13)\nOn such a basis, a Bernoulli sampling with the probability is performed on each negative candidate to obtain a more accurate set $B_f(v)$:\n$$B_f(v) = \\{Bernoulli(p(z, z_j))|z_j \\in B(v)\\}.$$\n(14)"}, {"title": "4.4. The Contrastive Learning Framework", "content": "Based on the designed data augmentations in Section 4.2 and the new negative instance selection strategy in Section 4.3, the procedure of our model GraphTP is detailed in Algorithm 1.\nIn summary, given a graph $G(X, A)$, we can obtain its topology augmented graph $G_T(X, A_T)$ by Sec.4.2. Furthermore, we define the set of typical perturbations as $T$. Within every epoch, we sample two perturbations $t~T$ and $t' ~ T$ to generate two views: $G' = t(G)$ and $G'_T = t' (G_T)$. Then the two views are input into the encoder $f$ to get the node embeddings: $Z_1$ and $Z_2$. At this time, for each node $v_i$ in the view $G'$, its corresponding node $v_i$ in another view $G_T$ is regarded as the positive. While its set of negative instances $B_f(v_i)$ can be obtained by Sec. 4.3. As a result, the contrastive loss of each pair$(v_i,v_i')$ is formulated as follow:\n$$l_f(z_i, z_i') = log\\frac{e^{\\theta(z_i, z_i')/\\tau}}{\\sum_{z_j \\in \\{z_i' \\cup B_f(v_i)\\} } e^{\\theta(z_i, z_j')/\\tau}},$$\n(15)\nwhere $\\tau$ is the temperature parameter, $z_i \\in Z_1$ and $z_i' \\in Z_2$ are the embeddings of nodes $v_i$ and $v_i'$. $\\theta(z_i, z_i') = z_i^T z_i'$ is the similarity of the pair. For the another view, its contrastive loss can similarly be defined. At last, the overall objective of all $N$ nodes to be minimized is defined as:\n$$L = -\\frac{1}{2N}\\sum_{i=1}^N (l_f(z_i, z_i') + l_f(z_i', z_i)),$$\n(16)\nIn this case, minimizing the pairwise objective can also be seen as maximizing the classical triplet loss:\n$$-l_f(z_i, Z_i') \\times 2\\pi N_f + (\\frac{||Z_i - Z_i || - || Z_i - Z_j ||^2}{T}).$$\n(17)\n$$Z_j \\in B_f(V_i)$$\nWe first rearrange the pairwise objective as:\n$$-l_f(z_i, z_i') = -log\\frac{e^{z_i z_i'/\\tau}}{\\sum_{z_j \\in \\{z_i' \\cup B_f(v_i)\\} } e^{z_i z_j'/\\tau}}$$\n$$= log(1 + \\sum_{z_j \\in B_f(V_i)}(e^{(z_j^T - z_i^T) / \\tau})).\n(18)\nBy Taylor expansion of first order, the main derivation process is as follows:\n$$- If (Zi, Z)2 \\approx  \\sum_{ZjEBf (Vi)}\\tau ^ {-1}\nZj Zi  = 1 -  \\frac{1}{2T}\\sum_{ZjEBf(Vi)} (\\|Zi \u2013 Zj \\| - \\|Zi \u2013 Zi\\|2) ,$$\n$$= 1/2+Nf2\\pi Nf+\n \\approx \\sum_{ZjEBf(Vi)} (\\|Zi - Zj \\| - \\| Zi - Zj \\|2)\n(19)"}, {"title": "5. EXPERIMENT", "content": "In this section, we conduct extensive experiments to evaluate our model on five datasets. In particular, we focus on node-level representation learning, where downstream tasks include node classification, clustering, and similarity search."}, {"title": "5.1. Datasets", "content": "To verify the effectiveness of the model, we take five widely used benchmark datasets of different sizes collected from real networks, including citation networks (CiteSeer, Coauthor-CS) and social networks (WikiCS, Amazon-Computer, Amazon-Photo). The detailed descriptions are given in Table 2.\n\u2022 CiteSeer and Coauthor-CS are two citation networks, the nodes of citeseer are represented as publications, and the edges represent citations.\n\u2022 WikiCS is a computer-science related network built on Wikipedia. Nodes are articles and labeled with ten classes. Edges are hyperlinks between articles. Nodes The features of the nodes are the average of the pre-trained embeddings of the words in each article.\n\u2022 Amazon-Computers and Amazon-Photo are two co-purchasing relationship networks, where nodes are goods, and when two goods are often purchased together, an edge is constructed between them."}, {"title": "5.2. Baselines", "content": "To verify the effectiveness of the proposed method GraphTP, for the node classification task, we select representative baselines similar to this paper, which include the following:\n\u2022 Deepwalk [32], an unsupervised random-walk model.\n\u2022 DGI [16] (Deep graph Infomax) applies the Infomax criterion and the augmentation of shuffling node features to develop a GCL proposal.\n\u2022 GMI [43] (Graph Mutual Information) further improves DGI by using discriminators to measure the mutual information between the input and the representation of node and edge, respectively.\n\u2022 GBT [44] (Graph Barlow Twins) proposes a cross-correlation-based loss objective.\n\u2022 MVGRL [21] (Multi-View Graph Representation Learning) introduces the graph diffusion technology for multi-view graph contrastive learning.\n\u2022 GCA [20] proposes the method of adaptive data augmentation with prior knowledge.\n\u2022 GRACE [19] designs various graph data augmentations, such as removing edges and masking node features.\n\u2022 COSTA [18] proposes the feature augmentation from the perspective of covariance.\n\u2022 HomoGCL [9] is a recent work that mines neighboring nodes with special significance for nodes to expand the positive set.\nFurthermore, we also report the performance of Graph Convolutional Networks (GCNs) [2] under fully supervised conditions, trained in an end-to-end manner. In addition, in order to verify the generalization of the model, the node clustering and the similarity search are introduced. The BGRL [45] and AFGRL [14] are further used to compare. The former method designs a framework for contrastive learning without negative samples. The latter goes a step further by introducing data-free augmentation and more positive samples for representation learning. For all baselines, we report the performance according to their official implementation."}, {"title": "5.3. Implementation Details", "content": "In our experiment, we adopt a two-layer GCNs [2] as the backbone network. All experiments are implemented by using PyTorch and optimized with the Adam optimizer. For hyper-parameter settings, the embedding dimension is set to 256 for all datasets except Amazon-Computers where d' = 128. The learning rate is set to 0.01 for Amazon-Photo, Amazon-Computers, and WikiCS, 0.001 for CiteSeer, and 0.0005 for Coauthor-CS. The setting of top-k in the scheme of feature space and matrix transformation is 1 for CiteSeer and WikiCS, while k = 10 for two datasets of Amazon and k = 12 for Coauthor-CS. Following the order of datasets in Table 2, the parameter $\\alpha$ is set to 180, 100, 160, 80, and 80. Meanwhile, in the operation of building the prototype, K is uniformly set to 100 for all datasets.\nFor each experiment, the model is firstly trained in an unsupervised manner by adopting the designed method. Then for the task of node classification, the resulting embeddings are fed to a $l_2$-regularized logistic regression classifier to evaluate. The training set, validation set, and test set in the experiment are all divided based on the settings of previous work [20, 18]. For node clustering and similarity search, the resulting embeddings are directly used to evaluate."}, {"title": "5.4. Experimental Results", "content": "The experimental result of node classification is shown in Table 3. Overall, GraphTP achieves competitive (first or second place) performance compared to state-of-the-art algorithms on benchmark datasets, which verifies the effectiveness of our method. Specifically, GraphTP improves performance by 1.61%, 1.16%, and 1.2% over the best baseline on Amazon-Photo, Amazon-Computer, and CiteSeer, respectively. On the Coauthor-CS, we find that although existing baselines have achieved sufficiently high performance, our approach still pushes the frontier in accuracy by almost 1%. And there are not many behind on the WikiCS with SOTA, only 0.05%.\nMeanwhile, we observe other observations as follows. The shallow methods including Raw feature and DeepWalk performed worse, which cannot both utilize the information of raw features and adjacency matrix. The early contrastive learning methods DGI and GMI also do not show competitive performance. They focus on modeling the whole graph or subgraph structure after simple or no data augmentations. Compared with the methods that focus on graph data augmentations (GRACE, GCA, MVGRL), the excellent performance of GraphTP verifies that our two proposed topology augmented schemes for graph data can help improve the quality of representation learning. Although MV-GRL adopts the method of injecting external information into the augmented view, it is still fair to augment the important edges more strongly on the input graph or take use of the topology information inside the feature space. Compared with the recent methods: HomoGCL with the use of homophily assumption and COSTA with feature-level augmentation to alleviate the problem of biased augmentation, our design of combining the topology reorganization and prototype-based selective sampler is more effective. We also evaluate the performance on the tasks of node clustering (Table 4) and similarity search (Table 5), where adopts the scheme 1 to genetare the contrastive view. Table 4 shows that GraphTP generally outperforms other methods in node clustering. Among them, the highest improvement in clustering indicators are 8.2% and 8.9%. We think that this is mainly because GraphTP is different from other contrastive methods in the selection of negative samples. False negative samples are screened out based on semantic information in GraphTP so that the clusters formed by clustering are tighter and the distance between clusters is larger. For a more intuitive display, we make a visual description later in Sec. 5.7. Meanwhile, our method performs well in terms of node similarity search. In the baselines, AFGRL designs the strategy of treating the nearer semantic neighbor of the anchor as positive samples, which is very beneficial to this task, but our method is still superior to it on the three datasets.\nIn short, compared with the existing advanced methods on three tasks, the performance verifies the effectiveness of our proposed framework."}, {"title": "5.5. Ablation Study", "content": "To further study the effectiveness of each part in the designed model, we conduct ablation experiments on the datasets. In a word, the innovations of model are mainly in two components, namely data augmentation by Topology reorganization and Prototype-based negative sample selection. And there are two schemes in data augmentation. Therefore, we first remove the two components in the model and name it Graph, then two topology reorganization schemes are added separately and defined as two variants, namely GraphT-F and GraphT-T. To verify the effectiveness of the proposed negative sample selection strategy, we further added the strategy to the first three variants, named GraphP, GraphTP-F, and GraphTP-T, respectively. The corresponding instructions are given in Table 6.\nThe results are shown in Figure. 4, where we can see that GraphTP-T and GraphTP-F consistently outperform the other variants on all datasets, which indicates that both components contribute to the investigated tasks. Among them, we firstly find that the performance of two topology reorganized views GraphT-T and GraphT-F are much better than the basic variant Graph. It validates the need for targeted augmentation of graph structures. Meanwhile, we find that the variants after adding the negative sample selection strategy: GraphP, GraphTP-F and GraphTP-T have improved performance compared to the previous ones, which verifies the necessity of removing false negative samples. In addition, we can observe that the gain brought by topology augmentation is larger than the designed negative sample selection strategy, which reflects the importance of view generation as the first step in the contrastive learning framework. GraphT-F and its variants outperform GraphT-T on Coauthor-CS and Amazon-Computer, while the opposite is true on the other datasets."}, {"title": "5.6. Parameter Study", "content": "In this subsection, we investigate the effect of three hyperparameters in our proposed model on five benchmark datasets, which are the number of clusters K when selecting negative samples based on prototypes, the hidden size d' in the encoder, and thenumber of neighbors k selected based on similarity after topology reorganization with scheme 1.\n1) In this experiment, we change the value of k (1,6,10,12) to understand how this parameter affects the performance of GraphTP. As shown in Figure 5(a), k has different effects on the performance of each dataset. A too-small or large value of k will affect their performance. Among them, the Coauthor-CS with the largest number of nodes needs a larger k value, that is, k=12. Citeseer is affected to a greater degree, and it will be optimal when k=1.\n2) We study the sensitivity of the hidden size d' for the proposed framework GraphTP. Here, we choose 32, 64, 128, and 256 as the hidden size. From the Figure 5(b), we can see that all datasets have a relatively obvious upward trend, which indicates that d' is positively correlated with the performance of GraphTP within a certain range. The reason behind this may be that increasing d' will increase the number of trainable parameters. In addition, the accuracy of some datasets is in a state of continuous improvement in the selected parameters. But in order to the comprehensive performance and operating efficiency, while referring to the parameter settings of previous work, we finally chose d' =256 for these data sets, while naturally set it to 128 for the Amazon-Computers.\n3) To explore the importance of K on different datasets, we conduct experiments with four different values of K (10, 50, 100, and 200, respectively). Our results are shown in Table 7, in which the accuracy is relatively stable on all datasets and is less affected by K. Most of them achieve the best results when K=100. The reason why CiteSeer is slightly better than 100 on 50 may be that its number of nodes is less than the other four datasets. In the end, we all uniformly set it to 100 for convenience."}, {"title": "5.7. Visualization", "content": "To demonstrate the learned node embedding and display the benefits of methods more intuitively, we visualize the node embeddings of GCA and GraphTP on the Amazon-Photo, Amazon-CS, and Coauthor-CS datasets. From the Figure 6, it can be observed that the embeddings learned by GraphTP have better clustering results, in which the clusters are more compact. In detail, GraphTP can capture finer-grained category semantic information, and there are more obvious boundaries between different node clusters.\nIn particular, it is more intuitive to verify that our method can effectively alleviate the risk of semantic drift from the visualization in the Amazon-Computers. In the lower part of the Figure 6(a), we can find that in GCA, the clusters of the same class are separated, and the separated part moves to other classes. This is because under the trivial negative sample selection, instances of the same category with anchor are considered as negative and mistakenly excluded. In contrast, our method can greatly alleviate this phenomenon by removing these false negative instances. Thus, the intra-class distance in GraphTP is tighter."}, {"title": "6. CONCLUSION", "content": "In this paper, we develop a novel topology augmentation and a new negative sample selection strategy for node representation learning in graph, which is named GraphTP. In our model, the contributions of GraphTP consists of three parts. First, we focus on the topology relationship of the graph to augment globally, which takes use of the adjacency matrix and the semantic information in feature space. Second, we further design a strategy for the negative sample selection, which could remove the false negative samples to mitigate the risk of semantic shift. At last, we conduct comprehensive experiments on various real-world datasets. Experimental results show that GraphTP outperforms most existing state-of-the-art methods, even surpassing supervised methods."}]}