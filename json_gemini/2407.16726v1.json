{"title": "Topology Reorganized Graph Contrastive Learning with Mitigating Semantic Drift", "authors": ["Jiaqiang Zhang", "Songcan Chen"], "abstract": "Graph contrastive learning (GCL) is an effective paradigm for node representation learning in graphs. The key components hidden behind GCL are data augmentation and positive-negative pair selection. Typical data augmentations in GCL, such as uniform deletion of edges, are generally blind and resort to local perturbation, which is prone to producing under-diversity views. Additionally, there is a risk of making the augmented data traverse to other classes. Moreover, most methods always treat all other samples as negatives. Such a negative pairing naturally results in sampling bias and likewise may make the learned representation suffer from semantic drift. Therefore, to increase the diversity of the contrastive view, we propose two simple and effective global topological augmentations to compensate current GCL. One is to mine the semantic correlation between nodes in the feature space. The other is to utilize the algebraic properties of the adjacency matrix to characterize the topology by eigen-decomposition. With the help of both, we can retain important edges to build a better view. To reduce the risk of semantic drift, a prototype-based negative pair selection is further designed which can filter false negative samples. Extensive experiments on various tasks demonstrate the advantages of the model compared to the state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "Graphs are capable of modeling complex interactions between objects that occur naturally in many real-world scenarios, such as in social networks, shopping websites, and citation networks [1]. Learning and exploring the features of nodes and structures on these graphs facilitates various real-world challenges and applications. Graph Neural Networks (GNNs) and their various variants are a class of methods that can be used for representation learning on graph data. In recent years, they have achieved great success in dealing with graph analysis problems such as node classification and clustering [2, 3, 4], link prediction [5].\nMost of these GNN methods adopt the paradigm of supervised learning, which extensively rely on label information to guide model learning [6]. However, high-quality labeled data is laborious and expensive to collect in the real world. Recently, self-supervised learning (SSL) is a promising paradigm for exploring the knowledge inside unlabeled data to alleviate the dependence on labeled data. With the great success of SSL in computer vision [7] and natural language processing [8], researchers also naturally apply SSL to graph-structured data [9, 10]. Existing SSL methods can be divided into three categories: predictive, generative, and contrastive [11]. Predictive methods generate pseudo-labels with general prior knowledge, and design prediction-based auxiliary tasks to train [12]. Generative models generally regard the rich information in graph data such as structure and attribute information as self-supervised signals for reconstruction learning [13]. Contrastive learning is one of the most successful area in SSL [14]. It has achieved comparable or better results than supervised learning in representation learning task, which is the focus of this paper.\nThe main components in contrastive learning (CL) are augmentation, positive-negative pair selection, and contrastive objective [15]. In particular, we first generate multiple contrastive views for each instance through data augmentation."}, {"title": "2. RELATED WORK", "content": "In this section, we review the related work from three aspects: graph neural networks, unsupervised graph representation learning, and graph contrastive Learning."}, {"title": "2.1. Graph Neural Networks", "content": "Graph exists widely in the real world. Graph neural networks (GNNs), which learn graph embeddings by using attribute features and topological information, have been extensively studied [27]. GNNs usually adopt the message passing paradigm, that is, iteratively updates the representation of nodes by aggregating the representations of their neighbors, and sums up the representation of nodes through pooling operations to obtain the representation of the entire graph.\nGNN is first introduced in [27], which combined graph Laplacian to design a graph convolution operation in the Fourier domain. Then, GCN [2] builds a bridge between the spectral domain and the spatial domain in GNN. It uses the first-order Chebyshev polynomial filter approximation for efficiency and only aggregates node features from first-order neighbors each time. GAT [28] further introduces the attention mechanism to consider the importance of different node neighbors instead of simple aggregation. GraphSAGE [29] provides four functions for aggregating nodes: mean/max/LSTM/Pooling. KerGNNs [30] combines the graph kernel method, which naturally extends the CNN framework to the graph, and brings a certain degree of interpretability. Meanwhile, existing GNN methods have been successfully applied in various fields with great success, such as anomaly detection [31] and node clustering [4, 3]. But most of GNNs are to learn the representation of nodes in an end-to-end manner under the paradigm of supervised learning."}, {"title": "2.2. Unsupervised Graph Representation Learning", "content": "Unsupervised graph representation learning aims to learn node embeddings on the graph when data labels are not available. Early methods focus more on using the structural information of the graph to learn the representation, such as"}, {"title": "2.3. Graph Contrastive Learning", "content": "Recently, graph contrastive learning has been extensively studied due to the relatively large success of self-supervised contrastive learning in computer vision and natural language processing [35]. General graph contrastive learning can be roughly divided into three modules: contrastive objective, data augmentation, and positive-negative sample pair selection. The existing work is mostly based on the innovation of three modules [1, 36]. Deep Graph InfoMax (DGI) [16] firstly applies the Infomax criterion to the graph, and proposes to compare the node representation derived from the corrupted graph with the whole graph representation. For the design of data augmentation, MVGRL [21] learns node-level and graph-level representations by injecting global structural information into the graph to obtain a contrastive view. GraphCL [17] and GRACE [19] use the SimCLR [37] framework to propose a variety of heuristic graph data augmentation methods, such as masking node features, discarding nodes, and removing edges .etc. GCA [20] makes a further improvement by introducing prior information, such as node-based degrees, to adaptively augment the graph. COSTA [18] focuses on hidden feature augmentation to avoid getting a"}, {"title": "3. THE PROPOSED FORMULATION", "content": "In this section, the terminology and problem definitions are given as follows.\nLet $G = (X, A)$ denote an undirected graph, where $A \\in \\mathbb{R}^{N \\times N}$ is the adjacency matrix, $N$ is the number of the nodes. $A_{i,j} = 1$ if there is an edge between node $v_i \\in V$ and $v_j \\in V$ and $A_{i,j} = 0$ otherwise, where $V$ is the set of $N$ nodes. $X = [X_1, X_2, ..., X_N]^T \\in \\mathbb{R}^{N \\times d}$ is the feature matrix, where $x_i$ is the i-th row of $X$ and denotes the feature vector of $v_i$. The unnormalized graph Laplacian of $G$ is $L = D \u2013 A$, where $D = diag(d_1, d_2,..., d_n)$ is the degree matrix of $A$ and $d_i = \\sum_{j \\in V} A_{i,j}$. When the adjacency matrix $A$ is normalized to $\\hat{A} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, the normalized Laplacian matrix can also be defined as $\\hat{L} = D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}}$. In this paper, we take a GNN encoder [2] as the backbone network.\nDefinition 1 (Graph Neural Network). Given an graph $G = (X, A)$, a typical graph neural network mainly consists of two components: Message Aggregation and Combine:\n$m_v^{(l)} = AGGREGATE^{(l)}({h_v^{(l-1)} : v' \\in (N(v) \\cup v)})$, $h_v^{(l)} = COMBINE^{(l)}(m_v^{(l)}, h_v^{(l-1)}).$ (1)\nwhere $AGGREGATE$ and $COMBINE$ are message aggregation and combine functions. $h_v^{(l)}$ represents the embedding of node $v$ in the $l$-th layer, $N(v)$ denotes the neighbor nodes of $v$. For each node, the $l$-hop neighbor information can be captured by stacking an $l$-layer GNN.\nDefinition 2 (Instance-based contrastive learning). Instance-based contrastive learning is one of the widely used paradigms for self-supervised learning (SSL). In principle, for a anchor node $v_i$, the same node corresponding to other views is regarded as positive to be pulled closer, while all other instances as negative to be pushed away. Specifically, given the representation of a positive pair $(z_i, z_i)$, the agreement is maximized for this positive pair and minimized for negative pairs via the standard InfoNCE loss:\n$l(z_i, z_i) = log \\frac{e^{(z_i z_i)/\\tau}}{\\sum_{z_j \\in B(v_i)} e^{(z_i z_j)/\\tau}},$ (2)\n$L_{info} = -\\frac{1}{2N}\\sum_{i=1}^{N}(l(z_i, z_i) + l(z_i, z_i))$ (3)\nwhere $N$ is the number of nodes, $B(v_i)$ is a set of negative instances for node $v_i$, $\\tau$ is a temperature parameter.\nIn this work, we focus on the graph representation learning without label supervision. That is, given an unlabeled graph $G = (X, A)$, where $X \\in \\mathbb{R}^{N \\times d}, A \\in \\mathbb{R}^{N \\times N}$, we aim to train a GNN-based encoder $f(.)$ to obtain high-quality low-dimensional node representations: $f(X, A) \\in \\mathbb{R}^{N \\times d'}$, ie. $d' \u00ab d$. Then these representations can be adopted in downstream tasks, such as node clustering, classification, and similarity search."}, {"title": "4. METHODOLOGY", "content": "In this section, we introduce the proposed model GraphTP in detail. As shown in Figure.2, our model is mainly divided into two steps, data augmentation with topology reorganization and contrastive learning with prototype-based"}, {"title": "4.1. Overview", "content": "In this section, we introduce the proposed model GraphTP in detail. As shown in Figure.2, our model is mainly divided into two steps, data augmentation with topology reorganization and contrastive learning with prototype-based"}, {"title": "4.2. Graph Augmentation with Topology Reorganization", "content": "In contrastive learning, the generation of \"view\" is a factor that controls the information captured by the representation [1]. So we should carefully design the augmentation in order that the generated views can reflect the depth-related information inside the data. To this end, we design two schemes, one is the feature-space based, which makes use of the semantic structure in feature space, and the other is the matrix-transformation based, which utilizes the algebraic properties of the adjacency matrix."}, {"title": "4.2.1. Scheme 1: Feature-Space Based", "content": "In real-world scenarios, the correlation between graph and downstream tasks is usually very complex and can be related to its topology or node features, or their combination. Therefore, the implicit relationship between the node in feature space is also important and can be exploited [24].\nIn detail, we build a graph $G_f(A_f, X)$ based on node features. Given the feature matrix of nodes: X, we first calculate the feature similarity between nodes to generate a similarity matrix S. The similarity here can be measured by various methods. We list three commonly used methods: 1) Heat Kernel,"}, {"title": "4.2.2. Scheme 2: Matrix-Transformation Based", "content": "For the augmentation of topology structure, different from the previous random and local addition and deletion of edges, we aim to quantify the entire topological relationship by its algebraic properties and then augment. With the help of matrix and spectral theory [2], the specific operation is as follows:\nGiven the graph G = (A, X), we first obtain the unnormalized graph Laplacian $L = D \u2013 A$, where the D is the degree matrix of A and $d_i = \\sum_{j \\in V} A_{ij}$. So the symmetric normalized graph Laplacian can be denoted to $\\hat{L} = D^{-\\frac{1}{2}} L D^{-\\frac{1}{2}}$, its eigen-decomposition can be formulated as:\n$\\hat{L} = U \\Lambda U^T,$ (5)\nwhere $\\Lambda = diag(\\lambda_1, ..., \\lambda_N)$ and $U = [u_1, ..., u_N]$ are the eigenvalues and corresponding eigenvectors of $\\hat{L}$, respectively [2]. Further, assuming the rank of $\\Lambda$ to be N, we have:\n$\\hat{L} = U \\Lambda U^T = \\sum_{i=1}^N \\lambda_i u_i u_i^T,$ (6)\nwhere $u_i u_i^T, i = 1, 2, ...,$ form a set of orthonormal base matrices for the topology space $\\mathbb{R}^{N \\times N}$.\nGenerally speaking, the matrix $u_i u_i^T$ corresponding to the larger eigenvalue $\\lambda_i$ always indicates the the principal component of the topology space [39]. Meanwhile, inspired by the previous work [40], if we augment the larger eigenvalue with the exponentiation, the topological relationship in the graph can be augmented globally. For each node, the weights of influential edges that control the"}, {"title": "4.3. Prototype-based negative pair selection", "content": "The general graph contrastive learning methods adopt the instance-based learning paradigm. Specifically, for an anchor node, all other instances are uniformly regarded as negative and pulled apart in the representation space [15]. However, this simple approach cannot guarantee that these negative instances"}, {"title": "4.4. The Contrastive Learning Framework", "content": "Based on the designed data augmentations in Section 4.2 and the new negative instance selection strategy in Section 4.3, the procedure of our model GraphTP is detailed in Algorithm 1.\nIn summary, given a graph G(X, A), we can obtain its topology augmented graph $G_T (X, A_T)$ by Sec.4.2. Furthermore, we define the set of typical perturbations as T. Within every epoch, we sample two perturbations $t \\sim T$ and $t' \\sim T$ to generate two views: $G' = t(G)$ and $G'_T = t'(G_T)$. Then the two views are input into the encoder $f$ to get the node embeddings: $Z_1$ and $Z_2$. At this"}, {"title": "5. EXPERIMENT", "content": "In this section, we conduct extensive experiments to evaluate our model on five datasets. In particular, we focus on node-level representation learning, where downstream tasks include node classification, clustering, and similarity search."}, {"title": "5.1. Datasets", "content": "To verify the effectiveness of the model, we take five widely used benchmark datasets of different sizes collected from real networks, including citation networks (CiteSeer, Coauthor-CS) and social networks (WikiCS, Amazon-Computer, Amazon-Photo). The detailed descriptions are given in Table 2.\n\u2022 CiteSeer and Coauthor-CS are two citation networks, the nodes of citeseer are represented as publications, and the edges represent citations.\n\u2022 WikiCS is a computer-science related network built on Wikipedia. Nodes are articles and labeled with ten classes. Edges are hyperlinks between articles. Nodes The features of the nodes are the average of the pre-trained embeddings of the words in each article.\n\u2022 Amazon-Computers and Amazon-Photo are two co-purchasing relationship networks, where nodes are goods, and when two goods are often purchased together, an edge is constructed between them."}, {"title": "5.2. Baselines", "content": "To verify the effectiveness of the proposed method GraphTP, for the node classification task, we select representative baselines similar to this paper, which include the following:\n\u2022 Deepwalk [32], an unsupervised random-walk model.\n\u2022 DGI [16] (Deep graph Infomax) applies the Infomax criterion and the augmentation of shuffling node features to develop a GCL proposal.\n\u2022 GMI [43] (Graph Mutual Information) further improves DGI by using discriminators to measure the mutual information between the input and the representation of node and edge, respectively.\n\u2022 GBT [44] (Graph Barlow Twins) proposes a cross-correlation-based loss objective.\n\u2022 MVGRL [21] (Multi-View Graph Representation Learning) introduces the graph diffusion technology for multi-view graph contrastive learning.\n\u2022 GCA [20] proposes the method of adaptive data augmentation with prior knowledge.\n\u2022 GRACE [19] designs various graph data augmentations, such as removing edges and masking node features.\n\u2022 COSTA [18] proposes the feature augmentation from the perspective of covariance.\n\u2022 HomoGCL [9] is a recent work that mines neighboring nodes with special significance for nodes to expand the positive set.\nFurthermore, we also report the performance of Graph Convolutional Networks (GCNs) [2] under fully supervised conditions, trained in an end-to-end manner. In addition, in order to verify the generalization of the model, the node clustering and the similarity search are introduced. The BGRL [45] and AFGRL [14] are further used to compare. The former method designs a framework for contrastive learning without negative samples. The latter goes a step further by introducing data-free augmentation and more positive samples for representation learning. For all baselines, we report the performance according to their official implementation."}, {"title": "5.3. Implementation Details", "content": "In our experiment, we adopt a two-layer GCNs [2] as the backbone network. All experiments are implemented by using PyTorch and optimized with the Adam optimizer. For hyper-parameter settings, the embedding dimension is set to 256 for all datasets except Amazon-Computers where d' = 128. The learning rate is set to 0.01 for Amazon-Photo, Amazon-Computers, and WikiCS, 0.001 for CiteSeer, and 0.0005 for Coauthor-CS. The setting of top-k in the scheme of feature space and matrix transformation is 1 for CiteSeer and WikiCS, while k = 10 for two datasets of Amazon and k = 12 for Coauthor-CS. Following the order of datasets in Table 2, the parameter a is set to 180, 100, 160, 80, and 80. Meanwhile, in the operation of building the prototype, K is uniformly set to 100 for all datasets.\nFor each experiment, the model is firstly trained in an unsupervised manner by adopting the designed method. Then for the task of node classification, the resulting embeddings are fed to a 12-regularized logistic regression classifier to evaluate. The training set, validation set, and test set in the experiment are all divided based on the settings of previous work [20, 18]. For node clustering and similarity search, the resulting embeddings are directly used to evaluate."}, {"title": "5.4. Experimental Results", "content": "The experimental result of node classification is shown in Table 3. Overall, GraphTP achieves competitive (first or second place) performance compared to state-of-the-art algorithms on benchmark datasets, which verifies the effectiveness of our method. Specifically, GraphTP improves performance by 1.61%, 1.16%, and 1.2% over the best baseline on Amazon-Photo, Amazon-Computer, and CiteSeer, respectively. On the Coauthor-CS, we find that although existing baselines have achieved sufficiently high performance, our approach still pushes the frontier in accuracy by almost 1%. And there are not many behind on the WikiCS with SOTA, only 0.05%.\nMeanwhile, we observe other observations as follows. The shallow methods including Raw feature and DeepWalk performed worse, which cannot both uti-"}, {"title": "5.5. Ablation Study", "content": "To further study the effectiveness of each part in the designed model, we conduct ablation experiments on the datasets. In a word, the innovations of model are mainly in two components, namely data augmentation by Topology reorganization and Prototype-based negative sample selection. And there are two schemes in data augmentation. Therefore, we first remove the two components in the model and name it Graph, then two topology reorganization schemes are added separately and defined as two variants, namely GraphT-F and GraphT-T. To verify the effectiveness of the proposed negative sample selection strategy, we further added the strategy to the first three variants, named GraphP, GraphTP-F, and GraphTP-T, respectively. The corresponding in-"}, {"title": "5.6. Parameter Study", "content": "In this subsection, we investigate the effect of three hyperparameters in our proposed model on five benchmark datasets, which are the number of clusters K when selecting negative samples based on prototypes, the hidden size d' in the encoder, and the number of neighbors k selected based on similarity after topology reorganization with scheme 1.\n1) In this experiment, we change the value of k (1,6,10,12) to understand how this parameter affects the performance of GraphTP. As shown in Figure 5(a), k has different effects on the performance of each dataset. A too-small or large value of k will affect their performance. Among them, the Coauthor-CS with the largest number of nodes needs a larger k value, that is, k=12. Citeseer is affected to a greater degree, and it will be optimal when k=1.\n2) We study the sensitivity of the hidden size d' for the proposed framework GraphTP. Here, we choose 32, 64, 128, and 256 as the hidden size. From the Figure 5(b), we can see that all datasets have a relatively obvious upward trend,"}, {"title": "5.7. Visualization", "content": "To demonstrate the learned node embedding and display the benefits of methods more intuitively, we visualize the node embeddings of GCA and GraphTP on the Amazon-Photo, Amazon-CS, and Coauthor-CS datasets. From the Figure 6, it can be observed that the embeddings learned by GraphTP have better clustering results, in which the clusters are more compact. In detail, GraphTP can capture finer-grained category semantic information, and there are more obvious boundaries between different node clusters.\nIn particular, it is more intuitive to verify that our method can effectively alleviate the risk of semantic drift from the visualization in the Amazon-Computers. In the lower part of the Figure 6(a), we can find that in GCA, the clusters of the same class are separated, and the separated part moves to other classes. This is because under the trivial negative sample selection, instances of the same category with anchor are considered as negative and mistakenly excluded. In contrast, our method can greatly alleviate this phenomenon by removing these false negative instances. Thus, the intra-class distance in GraphTP is tighter."}, {"title": "6. CONCLUSION", "content": "In this paper, we develop a novel topology augmentation and a new negative sample selection strategy for node representation learning in graph, which is named GraphTP. In our model, the contributions of GraphTP consists of three parts. First, we focus on the topology relationship of the graph to augment globally, which takes use of the adjacency matrix and the semantic information in feature space. Second, we further design a strategy for the negative sample selection, which could remove the false negative samples to mitigate the risk of semantic shift. At last, we conduct comprehensive experiments on various real-world datasets. Experimental results show that GraphTP outperforms most existing state-of-the-art methods, even surpassing supervised methods."}]}