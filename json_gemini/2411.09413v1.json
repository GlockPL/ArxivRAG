{"title": "SCRIPT-CENTRIC BEHAVIOR\nUNDERSTANDING FOR ASSISTED AUTISM\nSPECTRUM DISORDER DIAGNOSIS", "authors": ["Wenxing Liu", "Yueran Pan", "Hongzhu Deng", "Xiaobing Zou", "Ming Li"], "abstract": "Abstract\u2014Observing and analyzing children's social behaviors\nis crucial for the early diagnosis of Autism Spectrum Disor-\nders (ASD). This work focuses on automatically detecting ASD\nusing computer vision techniques and large language models\n(LLMs). Existing methods typically rely on supervised learning.\nHowever, the scarcity of ASD diagnostic datasets and the lack\nof interpretability in diagnostic results significantly limits its\nclinical application. To address these challenges, we introduce\na novel unsupervised approach based on script-centric behav-\nior understanding. Our pipeline converts video content into\nscripts that describe the behavior of characters, leveraging the\ngeneralizability of large language models to detect ASD in a\nzero-shot or few-shot manner. Specifically, we propose a scripts\ntranscription module for multimodal behavior data textualization\nand a domain prompts module to bridge LLMs. Our method\nachieves an accuracy of 92.00% in diagnosing ASD in children\nwith an average age of 24 months, surpassing the performance\nof supervised learning methods by 3.58% absolutely. Extensive\nexperiments confirm the effectiveness of our approach and\nsuggest its potential for advancing ASD research through LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "ASD, characterized by deficits in social communication and\nthe presence of restricted, repetitive behaviors or interests, is\na neurodevelopmental disorder affecting approximately 2.3%\nof children and approximately 2.2% of adults [1]. Individ-\nals with ASD often experience additional developmental,\nbehavioral, and mental health challenges, which can impose a\nlifelong burden on families [2].\nExperts generally agree that special training and therapy for\nASD treatment should begin as early as possible [3], making\nearly and accurate diagnosis critically important. Traditional\nASD diagnostic methods include the Autism Diagnostic Inter-\nview (ADI) [4], the Autism Diagnostic Observation Schedule\n(ADOS), and ADOS-2 [5]. These methods require an expe-\nrienced physician to spend amount of considerable time on\neach assessment. Furthermore, diagnosis relies on specialized\nscales and is heavily dependent on the physician's subjective\njudgment and clinical experience. Consequently, the promotion\nof traditional ASD diagnostic methods is limited, particularly\nin regions with underdeveloped healthcare resources.\nMany previous studies have demonstrated significant po-\ntential in assisted ASD diagnosis using sensor technology\n[6] and artificial intelligence techniques [7]. We focus on\nautomatically detecting ASD from audio-visual behavior data.\nMainstream methods can be divided into two categories: the\nfirst is based on behavioral signal. These methods [8]\u2013[10]\nanalyze behavior-related features from paradigmatic videos\nusing machine learning techniques, such as body movements,\nhead movements, and eye patterns. The second category [11]\u2013\n[13] involves deep learning methods, which directly train raw\nvideo data to predict labels. While both types of methods\nperform well on their respective datasets through supervised\nlearning, the scarcity of ASD data limits their accuracy in\npractice. Moreover, these methods only provide a binary\nprediction for the diagnostic outcome and lack interpretive\nexplanation.\nTo address the challenges of limited data and interpretabil-\nity, we propose a novel approach to perform assisted ASD\ndiagnosis from audio-visual behavior data. The recent rapid\ndevelopment of LLMs [14]\u2013[18] has demonstrated their ex-\ntraordinary capabilities, including understanding, reasoning,\nand question answering. These capabilities allow LLMs to\nperform well a wide range of downstream tasks in a zero-shot\nmanner. Because LLMs have absorbed lots of human knowl-\nedge from massive textual data, they only require very little\ndomain knowledge, reducing the need to collect clinical data\nfrom ASD patients. Furthermore, their question-answering\nabilities enable the interpretation of diagnostic results.\nHowever, LLMs currently specialize in text data or text-\nimage data, understanding domain-specific audio-visual be-\nhavior signals remains challenging for two main reasons: 1)\nThere is a significant gap between different modalities [19],\n[20], and multimodal LLMs only do not perform well on real\naudio-visual data in complex scenarios [21], [22]. 2) Due to\nprivacy protection, there is very limited audio-visual behavior"}, {"title": "II. METHODOLOGY", "content": "Fig. 1 illustrates the overall framework of our proposed\nSCBU method, enabling LLMs to automatically detect ASD\nfrom video data. The framework follows a sequential struc-\nture consisting of three modules: namely BTM, STM and\nLLMs. The BTM is used to recognize basic human behaviors\nfrom audio-visual data. The STM, newly proposed in this\nwork, bridges the gap between behavioral logs and LLMs. It\ncomprises three components: 1) The response parser module\ncaptures predefined responses. 2) The response textualization\nmodule converts behavior activity into a script (text format). 3)\nThe DPM combines script context, system prompt and ASD-\nrelated knowledge together to guide LLMs to better understand\nthe human behaviors. Ultimately, we rely on pretrained LLMs\nto detect ASD and produce judgments by answering questions\nbased on the script description."}, {"title": "A. Behavior Transcription Module", "content": "The behavior transcription module [10] recognizes the po-\nsition, movement, expressions, and speech of all individuals\nin each frame by utilizing a combination of audio and image\nmodels, including Solov2 [24] for human body regions, Arc-\nFace [25] for the person identities, HRNet [26] for human body\nkeypoints, SYSUGaze [27] for the Gaze and Head Pose, Kaldi\n[28] for Automatic Speech Recognition, etc. We define the\naudio and video transcription task as a formulation of multiple\nperception model inference. This is denoted as:\n$[B]_i = f_{image} (I_i) + faudio (S_i)$ (1)\nwhere $[B]_i$ denotes the behavioral log of the $i^{th}$ frame, $f_{image}$\ndenotes the $j^{th}$ image model, $f_{audio}$ denotes the $k^{th}$ audio\nmodel, where $(I_i)$ and $(S_i)$ denote the $i^{th}$ frame image and\naudio respectively."}, {"title": "B. Script Transcription Module", "content": "Behavioral logs are generated by merging outputs from\nvarious models, resulting in data of different formats, such as\nbody coordinates, eye vectors, expression types, gesture types,\nand voice content. To transform these behavioral logs into text\nthat LLMs can understand, we developed a STM to standardize\nand unify these diverse outputs into a coherent script."}, {"title": "1) Response Parser", "content": "Following the paradigm design out-\nlined in [10], we utilize basic response events to simplify the\nbehavioral log. Generally, the observation of each paradigm is\nbased on the following events. We define $E$ as the set of all\ndoctor-patient interactions in the paradigm. $E_1$ represents the\nevent in which the child looks at the target object. $E_2$ repre-\nsents the event in which the child points at the target object. $E_3$\ndenotes the event where the child smiles. $E_4$ denotes the event\nwhen the child or doctor speaks. $E_5$ represents the event in\nwhich parents or doctors exit the testing studio. The response\nparsing process is defined by the following formulation:\n$R = E([B]) w.r.t. E = [E_i]_{i=1,2,...5}$ (2)\nwhere R records the timestamp of event E occurring in [B]."}, {"title": "2) Response Textualization Module", "content": "The response textual-\nization module converts the predefined events that occur in the\nparadigm video into textual descriptions, with each response\ncorresponding to a specific description. As shown in Fig. 2,\nthis illustrates the textualization process for the Response to\nName paradigm [29]. In the paradigm, the child participant is\nfirst guided to play with toys on the desk. Once their attention\nis engaged, the assessor suddenly calls the child's name from\nbehind. The child can exhibit one of three responses: 1) no\nresponse, 2) turning to face the caller, 3) turning to face the\ncaller and responding verbally.\nThe process of the paradigm can be summarized as three\nstages: setup, instructions given by the doctor or parent, and\nthe child's response. Consequently, the textualization process\nis divided into three components: [timestamp, environmental\ndescription], [timestamp, instruction description], and [times-\ntamp, response description]. To better characterize the details\nof the scripts, the scripts were assigned response adverbs based\non the children's response length and duration, as shown with\nSPPED and TIME in Fig. 2. In addition, we adopt the strategy\nthe diagnostic strategy used by doctors, so we added gender\nand age before describing the paradigm. By integrating the\ntextual descriptions from all paradigms, we generate the script,\nwhich represents the child's behavior from audio-visual data."}, {"title": "3) Domain Prompt Module", "content": "As shown in Fig. 3, the DPM\nhas been meticulously designed with three components: 1)\nThe system prompt, which emphasizes the identity of the\nLLMs to ensure they understand scripts in temporal order.\n2) The Domain Prompt incorporates domain knowledge [30]\nand artificial experience into the script descriptions. In Fig. 3,\nartificial experiences are highlighted in blue, representing\nthe researcher's expertise in using LLMs, which help LLMs\nunderstand the script and detect ASD. 3) The Format Prompt\nconstrains the format of the output."}, {"title": "C. Large Language Models", "content": "In this paper, we selected five LLMs with strong perfor-\nmance and reputation: three closed-source models (GPT-40\n[14], Claude 3.5-sonnet [15], Monnshot [18]) and two open-\nsource models (LLAMA 3.1 [16], qwen2-72B-instruct [17]).\nWe input script descriptions and user questions into these\nLLMs, and the assisted ASD diagnosis and interpretation were"}, {"title": "III. EXPERIMENTAL RESULTS", "content": ""}, {"title": "A. Dataset", "content": "In our experiments, we utilized the multimodal behavioral\ndatabase in [10] to evaluate our method. This database com-\nprises RGB-D and audio data recorded in a real clinical\nenvironment. The dataset includes 95 participants (71 children\nwith ASD and 24 age-matched typical controls) across six\nparadigms."}, {"title": "B. Results", "content": "We employ four widely used metrics to evaluate methods\nin ASD detection: accuracy (ACC), F1-score (F1), sensitivity\n(SN), and specificity (SP).\nAs shown in Table I, we report the results of our SCBU\nmethod with different LLMs and compare them with the"}, {"title": "C. Ablation Study", "content": "Few-Shot Given the privacy concerns associated with\nclosed-source LLMs that upload data, we conducted few-\nshot experiments exclusively on the local open-source model\nQwen2. Qwen2 has a maximum input token limit of 128,000.\nAfter testing, we set the maximum number of few-shots\nto 20. To ensure experimental fairness, we divided the 95\nparticipants into two groups: a few-shot training set of 20\nchildren and a test set of 75 children. As shown in Table II,\nour methods demonstrate that LLMs exhibit robust domain\nlearning capabilities when provided with a few examples.\nSpecifically, the 20-shot Qwen-2 model achieves the best\nperformance compared to its zero-shot and also outperforms\nthe method proposed by Cheng et al across all metrics. These\nexperimental results not only validate the effectiveness of few-\nshot learning but also highlight the potential of our method in\nscenarios with limited training data."}, {"title": "Domain Prompts Module", "content": "To demonstrate the validity,\nwe examined the impact of different prior information by\nablation experiments: domain knowledge from DSM-5 [31],\nartificial experience from researchers' experience and reactive\nadverb from response. As shown in Table III, compared to not\nintroducing any knowledge, the domain knowledge, the artifi-\ncial experience and reactive adverb can improve performance\nrespectively. Specifically, relying solely on domain knowledge\ncan lead to high sensitivity but low specificity. However, by in-\ncorporating human experience, we can effectively constrain the\nmodel, resulting in more balanced performance. These ablation\nexperiments demonstrate the importance of designing tailored\nprompts for the ASD detection task. Overall, our method\nachieved the best performance when all prior information are\nintroduced simultaneously."}, {"title": "IV. CONCLUSION", "content": "In this study, wepropose anovel unsupervised approach to\ndetecting ASD using LLMS. We develop a STM to convert\nvideo and audio content into text.Furthermore, we design a\nDPM to better leverage prior knowledge of ASD. Extensive ex-\nperimental results demonstrate the effectiveness of our method,\nshowing strong zero-shot and few-shot capabilities. Moreover,\nLLMs makes it possible to interpret the reasoning of assisted\nASD diagnosis. Future research will focus on developing\nsimpler and more generalized STM for home intervention\nvideos in more naturalistic settings."}]}