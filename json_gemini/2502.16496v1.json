{"title": "PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning", "authors": ["Kun Hu", "Muning Wen", "Xihuai Wang", "Shao Zhang", "Yiwei Shi", "Minne Li", "Minglong Li", "Ying Wen"], "abstract": "Multi-agent reinforcement learning (MARL) faces challenges in coordinating agents due to complex interdependencies within multi-agent systems. Most MARL algorithms use the simultaneous decision-making paradigm but ignore the action-level dependencies among agents, which reduces coordination efficiency. In contrast, the sequential decision-making paradigm provides finer-grained supervision for agent decision order, presenting the potential for handling dependencies via better decision order management. However, determining the optimal decision order remains a challenge. In this paper, we introduce Action Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent decision order optimization. We model the order determination task as a Plackett-Luce sampling process to address issues such as ranking instability and vanishing gradient during the network training process. AGPS realizes credit-based decision order determination by establishing a bridge between the significance of agents' local observations and their decision credits, thus facilitating order optimization and dependency management. Integrating AGPS with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent Transformer (PMAT), a sequential decision-making MARL algorithm with decision order optimization. Experiments on benchmarks including StarCraft II Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show that PMAT outperforms state-of-the-art algorithms, greatly enhancing coordination efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "In a multi-agent system (MAS), the optimal action of one agent is often affected by the behavior of others [37], creating complex inter-agent dependency relationships [4, 12]. Therefore, a key challenge of Multi-Agent Reinforcement Learning (MARL) [31, 36] algorithms is handling the inter-agent dependencies to manage coordination [26, 31]. The dependency relationships among agents necessitate optimizing the decision-making order to achieve optimal team strategies in multi-agent cooperation tasks [4]. As illustrated in Figure 1, MARL algorithms typically employ two decision-making paradigms that generate agents' actions either simultaneously [30, 35] or sequentially [32]. Although simultaneously generating the actions of all agents can facilitate collective learning, it overlooks the potential action-level order dependencies within an MAS. Consequently, the newly generated action of the concurrent agent may offset the overall performance improvement established by previous agents, resulting in degradation of coordination efficiency [1].\nFortunately, the recent incorporation of sequence models (SMs) in reinforcement learning effectively facilitates sequential decision-making [6, 13]. In MARL, the utilization of the auto-regressive token generalization mechanism of SMs allows each agent to leverage prior agents' actions during its decision-making process [18, 33], which enables handling dependencies of agents in a sequential manner. While the sequential paradigm holds the potential for effective dependency management, identifying the optimal decision order presents a formidable challenge. Typically, a fixed or randomized action generation order [32] is adopted in this paradigm. The fixed-order decision-making scheme is limited by its inability to adapt to agents' dynamically evolving action-level dependencies, which leads to sub-optimal algorithm performance. For instance, in a football match, the possession of the ball frequently shifts between players as the game progresses. If decisions are made following a fixed order within the team, players might not respond effectively to real-time changes, resulting in poor coordination.\nAlthough the randomized-order decision-making scheme enables dynamic ordering, learning algorithms may converge to local optima due to random sampling that inadequately represents the solution space or fails to converge to optimal solutions under sub-optimal settings [28]. Consequently, sequential decision-making in the MARL domain remains fundamentally constrained and less effective without a robust mechanism to determine the optimal decision order. Hence, there is a pressing need to develop an adaptive decision-ordering mechanism that can effectively manage the dynamic action-level order dependencies among agents.\nWhile deep learning-based methods have shown remarkable progress in addressing ordering issues [8, 21], learning the optimal agent decision order still faces several key challenges. On the one hand, directly ordering agents according to their preference scores learned by deep neural networks presents a vanishing gradient issue since the variations of network outputs may not alter the ordering results. On the other hand, when agents declare similar ranking scores, even minor fluctuations in scoring can significantly change the final orderings, thus introducing instability issues.\nTo tackle these challenges, we introduce Action Generation with Plackett-Luce Sampling (AGPS), a Plackett-Luce (P-L) model-based sequential decision-making scheme in MARL. Specifically, we formulate the order determination task as an agent-by-agent sampling process and utilize P-L sampling [17, 22] in decision order optimization, which facilitates robust and adaptive decision-ordering in multi-agent cooperation tasks. Facilitated by AGPS, we propose Prioritized Multi-Agent Transformer (PMAT), a sequential decision-making MARL algorithm with decision order optimization. We evaluate the proposed algorithm on popular MARL benchmarks including StarCraft II Multi-Agent Challenge [23], Google Research Football [15], and Multi-Agent MuJoCo [9], where PMAT consistently demonstrates superior task performance compared with several state-of-the-art MARL algorithms."}, {"title": "2 RELATED WORK", "content": "In this section, we introduce several representative state-of-the-art MARL algorithms, covering both the simultaneous and the sequential decision-making paradigms. We also discuss the difference between several types of order optimization in MARL.\nSimultaneous Decision-Making MARL Algorithms. The vast majority of Centralized Training Decentralized Execution (CTDE) [10, 19] algorithms in MARL adopt a simultaneous decision-making paradigm. Here we introduce two representative ones. MAPPO [35] is a straightforward policy-based approach that endows the policy network of all agents with a shared set of parameters and utilizes agents' aggregated trajectories to facilitate policy optimization. HAPPO [14] is a heterogeneous-agent trust-region method that employs a sequential policy update paradigm. During an update in HAPPO, the agents randomly choose an update order and update their own policies over the newly updated policies of previous agents. Due to the adoption of the simultaneous decision-making paradigm, both MAPPO and HAPPO suffer from potential action conflicts and lack coordination efficiency guarantee.\nSequential Decision-Making MARL Algorithm. To alleviate potential action conflicts and further enhance multi-agent coordination, Wen et al. [32] proposed Multi-agent Transformer (MAT), which presents an auto-regressive sequential decision-making MARL algorithm based on the Transformer [29] architecture. MAT successfully transforms multi-agent joint policy optimization into a sequential decision-making process by generating actions in an agent-by-agent manner, which holds the potential for finer-grained supervision and management of inter-agent dependencies.\nDifferent Order Optimization in MARL. The current MARL algorithms have started to focus on the impact of \"order\" on agent-level or batch-level updates, and several solutions have been proposed. Wang et al. [30] proposed an agent-by-agent policy optimization method, A2PO, which adopts a semi-greedy agent selection rule to determine agent update order within a single rollout. Furthermore, B2MAPO [38] establishes update batches, further enhancing algorithm efficiency to facilitate joint policy optimization in larger-scale agent clusters. These studies on sequential update MARL algorithms offer valuable insights, suggesting that decision order optimization can be achieved incrementally on an item-by-item basis. Unlike these works, this paper focuses on optimizing agent decision order under the sequential decision-making paradigm, which remains an underexplored area in MARL."}, {"title": "3 PRELIMINARIES & BACKGROUND", "content": "3.1 Cooperative MARL Problem Formulation\nCooperative MARL problems can usually be modeled as Markov games $<N, O, A, R, P, \\gamma>$ [16], where $N = \\{1, ..., n\\}$ is the set of agents, $O = \\prod_{i=1}^{n} O^{i}$ is the joint observation space, $A = \\prod_{i=1}^{n} A^{i}$ is the joint action space, $R: O \\times A \\rightarrow R$ is the joint reward function, $P: O \\times A \\times O \\rightarrow [0, 1]$ is the transition probability function and $\\gamma \\in [0, 1)$ is the discount factor. Within each time step, all agents act simultaneously based on their observations. At time step $t \\in N$, each agent $i$ ($i \\in N$) obtains its individual local observation $o_{t}^{i} \\in O^{i}$ and takes an action $a_{t}^{i} \\in A^{i}$ according to its own policy $\\pi^{i}$, which represents a component of the joint policy $\\pi$.\nWe consider a fully cooperative setting where all agents share the same reward function. When time step $t$ ends, the whole team receives a joint reward $R(o_{t}, a_{t})$ and observes $o_{t+1}$ whose probability distribution is $P(o_{t}, a_{t})$. Following infinitely long times of this process, the multi-agent team finally gains a cumulative return of $\\sum_{t=0}^{\\infty} \\gamma^{t} R(o_{t}, a_{t})$. The observation value and the observation-action value can then be defined as\n$V_{\\pi}(o) \\triangleq E_{o_{1: \\infty} \\sim P, a_{0: \\infty} \\sim \\pi} [R^{\\gamma} | o_{0} = o] $  (1)\nand\n$Q_{\\pi}(o, a) \\triangleq E_{o_{1: \\infty} \\sim P, a_{1: \\infty} \\sim \\pi} [R^{\\gamma} | o_{0} = o, a_{1} = a]$   (2)\nrespectively. And the advantage value is defined as\n$A_{\\pi}(o, a) \\triangleq Q_{\\pi}(o, a) - V_{\\pi}(o)$.  (3)"}, {"title": "3.2 Multi-Agent Advantage Decomposition", "content": "In this work, we pay close attention to the impact of action generation order on multi-agent joint advantage improvement in MARL. Before proceeding to our methods, in this section we first introduce existing definitions and theorems as follows:\nDEFINITION 1 (MULTI-AGENT ADVANTAGE FUNCTION [14]). Let $i_{1:m}$ denote an ordered subset $\\{i_{1}, ..., i_{m}\\}$ of $N$ and $-i_{1:m}$ denote its complement. The multi-agent observation-action value function is defined as\n$Q_{\\pi}^{j_{1:k}, i_{1:m}}(o, a^{j_{1:k}}) \\triangleq E_{a^{-i_{1:m}} \\sim \\pi_{-i_{1:m}}}[Q_{\\pi}(o, a^{j_{1:k}}, a^{-i_{1:m}})]$.\nLet $j_{1:k}$ denote another ordered subset of $N$, such that $i_{1:m} \\cap j_{1:k} = 0$. Then, the multi-agent advantage function is defined as\n$A_{\\pi}^{i_{1:m}}(o, a^{i_{1:m}}) \\triangleq Q_{\\pi}^{j_{1:k}, i_{1:m}}(o, a^{j_{1:k}}, a^{i_{1:m}}) - Q_{\\pi}^{j_{1:k}}(o, a^{j_{1:k}})$.\nDefinition 1 describes the contribution of agents $i_{1:m}$ taking actions $a^{i_{1:m}}$ once agents $j_{1:k}$ have taken actions $a^{j_{1:k}}$, thus facilitating multi-agent joint policy optimization via the following theorem of\nTHEOREM 3.1 (MULTI-AGENT ADVANTAGE DECOMPOSITION [32]). Let $i_{1:m}$ be a permutation of agents and $i_{k}$ denote the kth agent within $i_{1:m}$. Then, for joint observation $o = o \\in O$ and joint action $a = a^{i_{1:m}} \\in A$, the following equation always holds,\n$A_{\\pi}^{i_{1:m}}(o, a) = \\sum_{k=1}^{m} A_{\\pi}^{i_{k}}(o, a^{i_{k-1}}, a^{i_{k}})$.\nTheorem 3.1 provides an intuitive guide for joint policy optimization within a multi-agent team. It suggests that a sequential optimization of each agent's action contingent upon the actions of preceding agents can finally improve the joint advantage. Hence, one major strength of MARL algorithms adopting the sequential action generation paradigm lies in the potential to ensure that each agent $i_{j}$ achieves a positive advantage upon the actions $a^{i_{1:j-1}}$ of its predecessors via sequential decision-making. As an example, MAT [32] utilizes an auto-regressive token generation mechanism to ensure that each agent achieves a positive advantage based on previous agents' actions during the decision-making process."}, {"title": "3.3 Multi-Agent Transformer", "content": "Multi-Agent Transformer (MAT) [32] is a successful implementation of the encoder-decoder architecture of the Transformer [29] in MARL. The attention mechanism of MAT first encodes agents' observations and actions with a weight matrix calculated by multiplying the embedded queries and keys. Subsequently, representations are calculated by multiplying the weight matrix with the embedded values. In general, the encoder of MAT takes a sequence of observations $(o_{t}^{1}, ..., o_{t}^{n})$ as input and passes them through several computational blocks to generate the corresponding observation representations $(\\hat{o}_{t}^{1}, \\ldots, \\hat{o}_{t}^{n})$. Each of these computational blocks consists of an unmasked self-attention mechanism and a multi-layer perceptron (MLP) to extract the interrelationship among agents. The encoder is trained to approximate the value functions by minimizing the empirical Bellman error of\n$L_{Encoder}(\\phi) = \\frac{1}{Tn} \\sum_{m=1}^{n} \\sum_{t=0}^{T-1} \\frac{1}{2} \\Bigg( R(o_{t}, a_{t}) + \\gamma V_{\\phi^{\\prime}} (o_{t+1}) - V_{\\phi}(o_{t}) \\Bigg)^{2}$,  (4)\nwhere $\\phi$ denotes the network parameter and $\\phi^{\\prime}$ denotes the target network parameter. The decoder of MAT receives the observation representations output by the encoder. It sequentially generates and passes the embedded actions of agents $a^{1:m-1}_{t-1}$ ($m = 1,...n$) through a sequence of decoding blocks, where $a^{0}_{t}$ is an arbitrary symbol indicating the start of decoding. Every decoding block is equipped with a masked self-attention mechanism utilizing triangular matrices to ensure that for each agent $i_{j}$ attention is computed between the ith and the jth action heads ($r < j$) so that the sequential scheme can be maintained. The decoding block finally finishes with an MLP and skipping connections, generating a sequence of multi-agent joint action. Parameterized by $\\theta$, the decoder is trained to minimize the following clipping PPO [25] objective of\n$L_{Decoder}(\\theta) = \\frac{1}{Tn} \\sum_{m=1}^{n} \\sum_{t=0}^{T-1} min \\Bigg( r_{t}^{m}(\\theta) A_{t}, clip(r_{t}^{m}(\\theta), 1 \\pm \\epsilon) A_{t} \\Bigg)$,  (5)\nwhere\n$r_{t}^{m}(\\theta) = \\frac{\\pi_{\\theta}(a_{t}^{m} | o_{t}, a_{t}^{1:m-1})}{\\pi_{\\theta^{old}}(a_{t}^{m} | o_{t}, a_{t}^{1:m-1})}$,  (6)\nand $A_{t}$ represents an estimate of the joint advantage function. To estimate the joint value function, generalized advantage estimation (GAE) [24] can be applied with $V_{t} = \\frac{1}{n} \\sum_{m=1}^{n} V(\\hat{o}_{t}^{m})$."}, {"title": "4 DECISION ORDER MATTERS", "content": "Although the positive-advantage decision-making scheme confers a monotonic improvement guarantee upon MAT, it fails to maximize the joint advantage achieved in each iteration. This stems from lacking effective management of the action-level dependencies among agents. Specifically, if the optimal action of agent $i_{j}$ depends upon the action of agent $i_{k}$ who plays a pivotal role, enabling $i_{k}$ to decide prior to $i_{j}$ provides essential decision-making information for $i_{j}$, thus holding the potential to enhance the overall team performance (which can also be evidenced by Example 3, [1]).\nAs an illustrative example, in Figure 2, we take two frames from the academy pass and shoot with keeper scenario of Google Research"}, {"title": "5 DECISION ORDER OPTIMIZATION", "content": "In this section, we first discuss the challenges of learning the optimal decision order and highlight the advantages of utilizing the Plackett-Luce (P-L) model for decision order optimization. We then introduce AGPS, a P-L model-based sequential decision-making mechanism. Additionally, we also present a practical MARL algorithm that serves as an application instantiation of AGPS."}, {"title": "5.1 Decision Ordering as a Ranking Task", "content": "A straightforward approach to optimizing the agent decision order involves evaluating all permutations of $n$ agents within each iteration of the sequential action generation process, with the objective of identifying the specific permutation that can maximize the joint advantage value. While this method exhibits favorable interpretability and can guarantee optimal orderings, the primary limitation stems from its factorial search complexity ($O(n!)$), which significantly increases the computational cost and limits its applicability in large-scale multi-agent systems. Inspired by the football case depicted in Section 4, we propose to leverage the potential correlation between the optimal decision order and the preference scoring of agents' local observations to address this challenge. Specifically, we model the decision-ordering task as a label ranking problem, enabling the application of parametric probabilistic models [7] and deep learning-based optimization techniques.\nLearning to rank is a fundamental problem in the domain of machine learning [3, 34], where deep learning-based methods have witnessed widespread applications [27]. By establishing a scoring network to evaluate the preference scores of agents' local observations and ranking them accordingly, an optimized decision sequence can be derived in a computationally efficient manner. However, this approach also exhibits several limitations when implemented in multi-agent systems. Firstly, the output variations of the scoring network do not necessarily convert into adjustments in the final rankings, which can potentially induce the vanishing gradient issue during the neural network training process. Secondly, generating agent decision order in a deterministic manner can introduce instability in that, when the individual scores are similar, minor discrepancies in scoring can produce substantial alterations in the final rankings. To address these challenges, we model the decision-ordering task as a multi-step sampling process and propose a P-L model-based approach that facilitates decision-credit allocation and decision-order optimization in multi-agent sequential decision-making."}, {"title": "5.2 Plackett-Luce Sampling", "content": "The Plackett-Luce model derives its name from independent work by Plackett [22] and Luce [17], which has found extensive applications in various real-world tasks like horse-racing [22], document ranking [3] and information retrieval [11], etc. A P-L model is parameterized by an n-length vector $v = (v_{1}, ..., v_{n})$ where $v_{i} > 0$ represents the preference score associated with each object $i$. The probability of sampling an ordered permutation $\\sigma^{(n)} = (\\sigma(1), ..., \\sigma(n))$ from a P-L distribution can be written as\n$P(\\sigma^{(n)} | v) = \\prod_{l=1}^{n} \\frac{v_{\\sigma(l)}}{\\sum_{i=l}^{n} v_{\\sigma(i)}}$.  (8)\nThe P-L model extends the Bradley-Terry (BT) model suggested by Bradley and Terry [2], which is renowned for its application in the domain of pairwise comparisons, to model item preferences as sampling probabilities. Specifically, the BT model specifies the probability that \"$\\sigma(i)$ wins against $\\sigma(j)$\" in terms of\n$P(\\sigma(i) > \\sigma(j)) = \\frac{v_{\\sigma(i)}}{v_{\\sigma(i)} + v_{\\sigma(j)}}$,  (9)\nwhere $>$ denotes the asymmetric relation which indicates that $\\sigma(i)$ precedes $\\sigma(j)$ in ordering. Derived from the BT model, the expected ordering generated from a P-L sampling process satisfies\n$\\sigma^{(n)} = [\\sigma(1), \\sigma(2), ..., \\sigma(n)]$,\ns.t. $\\forall (\\sigma(i), \\sigma(j)), i < j \\Rightarrow v_{i} \\geq v_{j}$.  (10)\nP-L sampling provides a probabilistic understanding of preference structures, addressing key challenges such as instability and vanishing gradient associated with the ordering process. Specifically, P-L sampling decomposes the ranking task of $n$ objects as a sequence of $n - 1$ independent selection stages, wherein each stage involves choosing the next top-scoring item from the remaining alternatives. This sequential mechanism ensures that objects with similar preference scores are assigned comparable selection probabilities, thereby reducing the sensitivity to minor score fluctuations and enhancing the robustness of ranking. Additionally, this method effectively converts the variations in preference scores output by neural networks into adjustments in the final rankings, thus mitigating the vanishing gradient issue in the neural network training process. Furthermore, P-L sampling has been shown to be computationally efficient [20] and offers various optimization opportunities, indicating potential scalability in large-scale multi-agent systems."}, {"title": "5.3 Action Generation with P-L Sampling", "content": "We utilize P-L sampling to optimize the action generation order within an MAS. To handle non-negative constraints, we parameterize the multi-agent P-L distribution using logarithmic parameters $z = (z_{1}, ..., z_{n})$. The probability of obtaining the optimal action generation order $\\sigma^{*}$ can then be derived as\n$P(\\sigma^{*}|z) = \\prod_{i=1}^{n-1} \\frac{exp z_{\\sigma^{*}(i)}}{\\sum_{j=i}^{n} exp z_{\\sigma^{*}(j)}}$.  (11)\nUtilizing the sequence-related joint advantage value $A_{\\pi}^{i_{1:n}}$ (abbreviated as $A(\\sigma)$) as feedback signal, the parameters $z$ are learned through a scoring network that outputs the preference scores associated with agents' local observations. Specifically, let $S$ denote the space of n-agent permutations, the expectation-form objective function of the agent decision order optimization problem can be formulated as\n$J(z) = E_{\\sigma \\in S} [A(\\sigma)] = \\sum_{\\sigma \\in S} A(\\sigma)P(\\sigma|z)$.  (12)\nFor numerical computation, the gradient of Equation (12) can be estimated via Monte Carlo approximation as\n$\\nabla_{z}J(z) = E_{\\sigma \\in S} [A(\\sigma)\\nabla_{z}log P(\\sigma|z)] \\approx \\frac{1}{M} \\sum_{i=1}^{M} A(\\sigma^{i}) \\nabla_{z} log P(\\sigma^{i}|z)$.  (13)\nThe jth partial derivative of the log-likelihood $log P(\\sigma|z)$ with respect to $z_{\\sigma(i)}$ in Equation (13) can be calculated by\n$\\frac{\\partial log P(\\sigma|z)}{\\partial z_{\\sigma(i)}} = 1 - \\frac{exp z_{\\sigma(i)}}{\\sum_{k=1}^{n} \\sum_{j=k}^{n} exp z_{\\sigma(j)}}$  (14)\nand the full gradient $\\nabla_{z} log P(\\sigma|z)$ can be generated within $O(n)$ timesteps [5], thus demonstrating superior efficiency.\nWe designate the proposed sequential action generation mechanism as Action Generation with P-L Sampling (AGPS). AGPS allows agents whose observations contribute more significantly to the multi-agent joint advantage to be granted higher decision-making priority. In this manner, subsequent agents can effectively perceive the decisions of their predecessors and offer proactive cooperation during their decision-making process, thereby enhancing the overall coordination within a multi-agent team."}, {"title": "5.4 Practical Algorithm", "content": "As illustrated in Figure 1, sequential decision-making MARL algorithms can be implemented via single or multiple interactions with the environment. While the latter paradigm benefits from timely feedback, it suffers from degradation in computational efficiency due to numerous interactions within each iteration of multi-agent joint action. Fortunately, recent advancements in auto-regressive SMs have provided fresh insights into the single-interaction paradigm, facilitating efficient sequential decision-making MARL algorithms based on the Transformer [29] architecture.\nPrioritized Multi-Agent Transformer. We demonstrate that AGPS can be effectively integrated with MAT for performance enhancement. On the one hand, the observation representations output by the encoder of MAT synthesize not only the local observations of individual agents but also the high-level inter-agent relationships [32], thus providing informative signals for preference scoring. On the other hand, the masked self-attention mechanism of MAT's decoder inherently promotes efficient sequential action generation. Hence, via integrating P-L sampling with the encoder-decoder architecture of MAT, an effective bridge between the local observation representations (as outputs of the encoder) and the decision orderings (as inputs of the decoder) can be established, which facilitates auto-regressive sequential action generation and ultimately leads to the proposed Prioritized Multi-Agent Transformer (PMAT) as illustrated in Figure 3.\nAs shown in Figure 3, the scoring block consists of an MLP $\\theta$ parameterized by $\\varphi$, which takes agents' local observation representations $(\\hat{o}_{t}^{1}, ..., \\hat{o}_{t}^{n})$ as input and yields the corresponding preference scores $(c_{t}^{i_{1}}, ..., c_{t}^{i_{n}})$. Specifically, the scoring network is trained to minimize the following clipping objective of\n$L_{Ranking}(\\varphi) = - \\frac{1}{T} \\sum_{t=0}^{T-1} min \\Bigg( r_{t}(\\varphi) A_{t}, clip(r_{t}(\\varphi), 1 \\pm \\epsilon) A_{t} \\Bigg)$,  (15)\nwhere\n$r_{t}(\\varphi) = \\frac{P(\\sigma | \\varphi(o^{1}_{t}))}{P(\\sigma | \\varphi(o^{n}_{t}))}$.  (16)\n$A_{t}$ estimates the sequence-related joint advantage, and $P(\\sigma | \\cdot)$ represents the probability of obtaining the current order $\\sigma = (j_{1}, ..., j_{n})$ via P-L sampling. Before being passed into the decoder, the original observation representations $(\\hat{o}_{t}^{1}, ..., \\hat{o}_{t}^{n})$ are reordered according to this order as $(\\hat{o}_{t}^{1}, ..., \\hat{o}_{t}^{n})$. Meanwhile, $\\sigma = (j_{1}, ..., j_{n})$ also serves as the auto-regressive action generation order within the decoder, thus realizing agent decision order optimization.\nNotably, the sampling paradigm differs slightly between the training and inference stages. During the training stage, P-L sampling is carried out in a non-deterministic manner since introducing randomness can facilitate generalization capability and mitigate the risk of overfitting specific training instances. During the inference"}, {"title": "6 EXPERIMENTS", "content": "In this section, we evaluate the effectiveness of the proposed AGPS and its application instantiation, PMAT, within popular MARL benchmarks. We compare PMAT with advanced MARL methods including MAT [32], MAPPO [35], and HAPPO [14]."}, {"title": "6.1 Experimental Environments", "content": "In this work, we evaluate our method within the following three MARL benchmarks: StarCraft II Multi-Agent Challenge (referred to as SMAC) [23], Google Research Football (referred to as GRF) [15] and Multi-Agent MuJoCo (referred to as MA MuJoCo) [9].\nStarCraft II Multi-Agent Challenge. SMAC [23] is an open-source research environment designed to evaluate MARL algorithms based on the StarCraft II game engine. SMAC simulates complex scenarios and varying unit types with real-time multi-agent interactions, enabling comprehensive benchmarking of cooperation strategies. Specifically, we conduct comparison experiments on two challenging maps, 10m vs 11m (Hard, homogeneous and asymmetric) and MMM2 (Super Hard, heterogeneous and asymmetric).\nGoogle Research Football. GRF [15] is an open-source research environment designed for MARL algorithm evaluation in a simulated football setting. In GRF, agents play different roles within a football team (e.g., forwards, wingers, etc.), demonstrating evident role heterogeneity. We utilize the academy pass and shoot with keeper, academy counterattack easy, and academy 3 vs 1 with keeper scenarios for algorithm evaluation.\nMulti-Agent MuJoCo. MA MuJoCo [9] contains a variety of multi-agent continuous control tasks where individual agents control the joints of biomimetic robot entities and coordinate to facilitate specific behavior. Built on the MuJoCo physics engine, MA MuJoCo's modular architecture allows for easy customization of the environments and agents' behavior, facilitating the simulation of complex interactions among agents trained by MARL algorithms. We evaluate the proposed method using the Ant-v2 scenario with two different configurations: 8\u00d71 agent Ant and 4\u00d72 agent Ant."}, {"title": "6.2 Experimental Results", "content": "StarCraft II Multi-Agent Challenge. The comparison results of all methods in the SMAC domain are shown in Figure 4a and Figure 4d. Specifically, for the 10m vs 11m map, PMAT achieves a winrate of 99.4%, outperforming the baseline methods MAT (97.5%), MAPPO (75.0%), and HAPPO (93.1%). For the MMM2 map, PMAT achieves a winrate of 85.0%, consistently surpassing the baseline methods MAT (75.0%), MAPPO (58.8%), and HAPPO (61.9%). The experimental results indicate that the performance enhancement of PMAT over MAT tends to be more pronounced in heterogeneous scenarios (e.g., MMM2) than in homogeneous scenarios (e.g., 10m vs"}, {"title": "6.3 Ablation Study", "content": "In this section, we present an ablation study to further evaluate the advantage-based scoring mechanism adopted in AGPS across the SMAC, GRF, and MA MuJoCo benchmarks. Specifically, in addition to MAT, we introduce another baseline, namely the randomized MAT (abbreviated as rMAT in this section), which adopts a fully randomized ordering strategy to determine the action generation order based on MAT. We summarize the experimental results in Figure 5, where PMAT exhibits superior performance compared with both MAT and rMAT in all displayed tasks. It can be observed that the randomized ordering strategy introduces instability since rMAT exhibits pronounced variance in some scenarios (e.g., academy counterattack easy), resulting in performance degradation. In contrast, the integration of AGPS effectively enhances both stability and monotonicity in task performance improvement, validating the effectiveness of the advantage-based scoring mechanism."}, {"title": "6.4 Case Study", "content": "In this section, we aim to analyze the distinct coordination strategies of agents trained by MAT and PMAT. Specifically, we conduct a fine-grained case study on agents' coordination behavior in the academy 3 vs 1 with keeper scenario of GRF, as illustrated in Figure 6. It can be observed that given identical initial conditions (our player TURING holds the ball, directly facing the defense of the opponent player EINSTEIN), agents trained by MAT adopt a strategy wherein TURING attempts to dribble past EINSTEIN and shoot (Figures 6a to 6c). In contrast, agents trained by PMAT adopt a different strategy (Figures 6d to 6f), wherein TURING directly awaits an opportune moment to pass the ball to his teammate DA VINCI, who enjoys a superior shooting angle unimpeded by direct opposition. Compared with the former strategy wherein agents make decisions more based on their own observations, the latter strategy evaluates the significance of local observations within a multi-agent team and allows agents who hold advantageous observations to make decisions first (DA VINCI, shoot), followed by proactive coordination of other agents (TURING, pass the ball to DA VINCI). Generally, the comparison between the aforementioned coordination strategies in this section offers an intuitive validation for the necessity of agent decision order optimization in MARL, demonstrating its effectiveness in promoting cooperative behavior among agents."}, {"title": "7 CONCLUSION", "content": "In this work, we propose Action Generation with Plackett-Luce Sampling (AGPS), a sequential decision-making mechanism in MARL. AGPS assigns decision credits to individual agents within a multi-agent team and significantly facilitates joint policy improvement by providing finer-grained supervision for decision order optimization. Integrating AGPS with the Multi-Agent Transformer, we propose the Prioritized Multi-agent Transformer (PMAT), a sequential decision-making MARL algorithm with optimized decision-ordering. Extensive experiments across various benchmarks showcase the effectiveness of AGPS as well as the superiority of PMAT in learning efficiency and task performance over several strong baselines. For future work, we plan to further investigate the effectiveness of AGPS by integrating it with a broader context of MARL algorithms. Besides, we will also evaluate the applicability of P-L sampling in larger-scale multi-agent systems."}, {"title": "A EXPERIMENTAL DETAILS", "content": "A.1 Introduction for Environments\nThis paper utilizes three widely-used MARL benchmarks: StarCraft II Multi-Agent Challenge (SMAC) [23], Google Research Football (GRF) [15], and Multi-Agent MuJoCo (MA MuJoCo) [9]. Screenshots of these environments are provided in Figure 7 for reference.\nA.2 Scenario Description\nThis section presents detailed descriptions of the scenarios adopted in experiments. Note that the descriptions are taken from the official documentation of the corresponding benchmarks."}, {"title": "A.3 Implementations", "content": "In this study, all experiments were conducted using an NVIDIA 3090 GPU. The programming environment was established with Python 3.9 and PyTorch 2.3.1. To ensure robustness, each experiment was repeated with three distinct random seeds, and the results were visualized using the mean values of the performance metrics (median value for the SMAC environment). Additionally, it should be noted that minor adjustments were made to the implementations of the SMAC environment due to identified technical inconsistencies\u00b9 in the reward function of the original SMAC environment."}, {"title": "A.4 Hyperparameter Settings", "content": "This section presents the detailed hyperparameter settings for different algorithms adopted in experiments.\nSpecifically, Table 1 shows the common hyperparameters we used in SMAC experiments. Table 2 shows the common hyperparameters we used in GRF experiments. Table 3 shows the common hyperparameters we used in MA MuJoCo experiments.\nTable 4 shows the different hyperparameters we used for MAPPO and HAPPO in SMAC experiments. Table 5 shows the different hyperparameters we used for MAT and PMAT in SMAC experiments. Table 6 shows the different hyperparameters we used for all methods in GRF experiments. Table 7 shows the customized hyperparameters we used for PMAT in GRF experiments. Table 8 shows the different hyperparameters we used for all methods in MA MuJoCo experiments. Table 9 shows the customized hyperparameters we used for PMAT in MA MuJoCo"}]}