{"title": "Better RAG using Relevant Information Gain", "authors": ["Marc Pickett", "Jeremy Hartman", "Ayan Kumar Bhowmick", "Raquib-ul Alam", "Aditya Vempaty"], "abstract": "A common way to extend the memory of large language models (LLMs) is by retrieval augmented generation (RAG), which inserts text retrieved from a larger memory into an LLM's context window. However, the context window is typically limited to several thousand tokens, which limits the number of retrieved passages that can inform a model's response. For this reason, it's important to avoid occupying context window space with redundant information by ensuring a degree of diversity among retrieved passages. At the same time, the information should also be relevant to the current task. Most prior methods that encourage diversity among retrieved results, such as Maximal Marginal Relevance (MMR), do so by incorporating an objective that explicitly trades off diversity and relevance. We propose a novel simple optimization metric based on relevant information gain, a probabilistic measure of the total information relevant to a query for a set of retrieved results. By optimizing this metric, diversity organically emerges from our system. When used as a drop-in replacement for the retrieval component of a RAG system, this method yields state-of-the-art performance on question answering tasks from the Retrieval Augmented Generation Benchmark (RGB), outperforming existing metrics that directly optimize for relevance and diversity.", "sections": [{"title": "1 Introduction", "content": "A limitation of transformer-based Large Language Models (LLMs) is that the number of tokens is bounded by the transformer's context window, which is typically in the thousands. This is often insufficient for representing large texts, such as novels and corporate documentation. A common way to mitigate this constraint is via retrieval augmented generation (RAG), in which a relatively small subset of relevant passages are retrieved from a larger database and inserted into an LLM's context window (Gao et al., 2024). Typically, this process involves applying a similarity metric, such as cosine similarity, to (precomputed) embeddings of passages and the embedding of a query. Using this metric, many systems then use K-nearest-neighbors or a fast approximation with a vector database such as FAISS (Douze et al., 2024). Importantly, K-nearest-neighbors (Bijalwan et al., 2014) and related methods (such as a cross-encoder reranker (Nogueira and Cho, 2020)) simply return the highest individually relevant passages, without regard to whether the information in the passages is redundant. Given the premium value on LLM context-window real estate, it's important to make best use of this limited resource by minimizing redundancy, while maintaining relevance.\nTo appreciate the importance of minimizing redundancy in a RAG context, consider a toy database of facts and the two possible sets of retrieval results in Table 1, for the same query, \u201cTell me some facts about sharks.\" Both sets of retrieved results are highly relevant to the query, but only the second set is diverse enough to support a satisfactory answer.\nA family of methods from the Information Retrieval literature attempts to address the general issue of diversity in retrieved results by introducing a measure that explicitly balances diversity and relevance (Carbonell and Goldstein, 1998). In this paper, we propose a more principled method, Dartboard, that instead seeks to directly accomplish what previous methods are indirectly aiming for - maximize the total amount of information relevant for a given query in a set of k results. The intuition behind Dartboard is simple - we assume that one passage is the \u201ccorrect\u201d one for a given query. Our system is allowed k \"guesses\u201d and it aims to maximize the relevance score of its most relevant guess. Since the best guess is not known ahead of time, this score is weighted by the probability of that guess being the most relevant. This objective is sufficient to encourage diversity in the guesses. This is because a redundant guess does little to increase the relevance of the most relevant guess.\nThe main contributions of this paper are 3-fold:\n\u2022 We introduce the Dartboard algorithm, a principled retrieval method based on optimizing a simple metric of total information gain relevant to a given query (\u00a72).\n\u2022 We demonstrate the effectiveness of Dartboard on Retrieval-Augmented Generation Benchmark (RGB) (Chen et al., 2023), a closed-domain question answering task. This benchmark consists of a retrieval component, and an end-to-end question-answering component. We show that the Dartboard algorithm, when used as the retrieval component, outperforms all existing baselines at both the component level and at end-to-end level (\u00a73.1).\n\u2022 We show that instead of directly encouraging diversity, diversity naturally emerges by optimizing this metric (\u00a7A.5)."}, {"title": "2 Dartboard", "content": "The Dartboard algorithm is based on the following analogy illustrated in Figure 1: Suppose that we have a cooperative two-player game where a dart-board is covered with a random collection of points. Player 1 is given one of these points arbitrarily as the target. Player 1 then throws her dart aiming for the target, and it lands somewhere on the board. Where it lands is the query. Player 2 sees where Player 1's dart landed (the query), but doesn't know where the actual target is. Player 2 then picks k of the points on the board. The true target is revealed, and the score (which the players are trying to minimize) is the distance from the target to the closest guess. Note that to minimize the score, Player 2 would not want to put all his guesses right next to each other. Also, Player 2 should take into account how accurate Player 1's throws are in general. In our implementation, Player 1's accuracy is modeled by a Gaussian distribution with standard deviation \u03c3.\nMore formally, Player 1 selects a target T from a set of all points A and gives a query q. Then Player 2 makes a set of guesses $G \\subset A$, resulting in a score $s (G, q, A, \\sigma)$ which is given as:\n$s (G, q, A, \\sigma) = \\sum_{t\\in A} \\sum_{g \\in G} P (T = t|q, \\sigma) \\min D (t\\vert g)$ (1)\nwhere D is a distance function. For d dimensional vectors, $A \\subseteq \\mathbb{R}^d$; under some assumptions, we can use a Gaussian kernel for the distance functions. For example, we can set $P (T = t|q, \\sigma) = N (q, t, \\sigma)$. Thus, our equation becomes:\n$s (G, q, A, \\sigma) \\propto - \\sum_{t\\in A} N (q, t, \\sigma) \\max_{g \\in G} N (t, g, \\sigma)$ (2)"}, {"title": "2.1 The Dartboard Algorithm", "content": "The Dartboard Algorithm aims to maximize Equation 2 given a distance metric. In practice, we can greedily build our set G, which works well as it saves us combinatorial search, and allows reuse of previous answers (since the top-k results are a subset of the top-k + 1 results). We begin by ranking top-k passages $A'$ from our initial dataset of passages $A$ using K-nearest-neighbors based on cosine similarity. We use a linear search, but sub-linear methods such as FAISS (Douze et al., 2024) could also be used for this initial ranking. Our search is a simple greedy optimization method with two changes - (a) we stay in log space to avoid numerical underflow, and (b) we reuse the results (maxes) from previous loops to avoid re-computing the maximums. The detailed algorithm is given in Algorithm 1 in Appendix A.1. In Appendix A.3, we also show how to adapt Dartboard to use a cross-encoder based reranker (resulting in two methods called Dartboard crosscoder and Dartboard hybrid), and Appendix A.4 shows that Dartboard generalizes KNN and MMR retrieval algorithms (Onal et al., 2015)."}, {"title": "3 Experiments", "content": "We tested Dartboard on benchmark datasets from (Chen et al., 2023), from which we used two types of closed-domain question answering. In the simple question answering case, a query is answerable from a single passage retrieved from the corpus. For example, consider the query When is the premiere of 'Carole King & James Taylor: Just Call Out My Name'?. On the other hand, in the information integration case, a query would require multiple passages to be retrieved to answer the query. For example, consider the query Who is the director of 'Carole King & James Taylor: Just Call Out My Name' and when is its premiere?. We modified this benchmark for our setup in the following way. The original benchmark contains \"positive\u201d and \u201cnegative\u201d labeled passages for each query. The positive passages are useful for answering, while the negative ones are related but ineffective in answering the query. Since we are interested in the retrieval component of this task, we merged the positive and negative passages for all queries into a single collection of 11,641 passages for the 300 simple question answering test cases and 5,701 passages for the 100 information integration test cases. The evaluation is otherwise identical apart from the retrieval component. Note that the innovation of Dartboard is solely on the retrieval component. Therefore, we keep the rest of the RAG pipeline fixed. In particular, we do not modify the prompting of LLMs or try to optimize passage embeddings.\nGiven a query and the full set of thousands of passage embeddings, we measured both a direct retrieval score and the overall end-to-end performance of the system with the only change being the retrieval algorithm. For the direct retrieval score, we computed the Normalized Discounted Cumulative Gain (NDCG) score (Wang et al., 2013) on retrieving any one of the \u201cpositive\u201d passages relevant to a specific query. In the information integration case, the positive passages were split into positive ones for each component of the question. Therefore, in this case, we calculated the NDCG score for retrieving at least one positive passage for each component of the query. For the end-to-end score, given an LLM's response to the query (generated from retrieved passages), we use the same evaluation as (Chen et al., 2023), which does a string match of the response on a set of correct answers, marking each response as either correct or incorrect.\nSome of the methods (described in Appendix A.2), including Dartboard, have tunable parameters. For instance, Maximal Marginal Relevance (MMR) has a diversity parameter that varies from 0 to 1. We performed a grid search over these parameters, reporting the best results for each method."}, {"title": "3.1 Results", "content": "From the results shown in Table 2, we observe that Dartboard outperforms all state-of-the-art methods in terms of all metrics across all the tasks.\nFigure 2 shows the performance of different retrieval methods on the end-to-end QA task (simple) as the parameters vary. Although Dartboard Crosscoder (D-CC) and Dartboard hybrid (D-H) are fairly robust to a range of $\\sigma$ values, the best performance is achieved for Dartboard hybrid with $\\sigma$ = 0.096 (See Appendix A.2 for baselines)."}, {"title": "4 Related Work", "content": "MMR retrieves documents (Carbonell and Goldstein, 1998) that are both relevant to the query and dissimilar to previously retrieved documents. It combines a relevance score (e.g., from BM25) with a novelty score that penalizes documents similar to those already retrieved. It have been used extensively for building recommendation systems (Xia et al., 2015; Wu et al., 2023) as well as for summarization tasks (Agarwal et al., 2022; Adams et al., 2022). However, MMR suffers from few limitations. First is that MMR requires the diversity parameter to control the balance between relevance and novelty. This parameter is often dataset-specific and requires careful tuning, making it impractical for real-world applications. Second is that MMR can favor exact duplicates of previously retrieved documents as they retain a high relevance score while minimally impacting the average novelty score (See Appendix A.7).\nKNN retrieves documents based on their similarity to a query embedding (Dharani and Aroquiaraj, 2013; Bijalwan et al., 2014). While efficient, KNN often suffers from redundancy as nearby documents in the embedding space tend to be semantically similar (Taunk et al., 2019). This can lead to a retrieved set dominated by passages conveying the same information with slight variations.\nSeveral recent works have explored incorporating diversity objectives into retrieval models (Angel and Koudas, 2011; Li et al., 2015; Fromm et al., 2021). These approaches often involve complex optimization functions or require additional training data for diversity estimation. For example, Learning-to-Rank with Diversity methods leverage learning-to-rank frameworks that incorporate diversity objectives directly into the ranking function. This allows for the optimization of both relevance and diversity during the ranking process. However, these approaches often require large amounts of labeled training data for diversity, which can be expensive and time-consuming to obtain (Wasilewski and Hurley, 2016; Yan et al., 2021). Bandit-based approaches model document selection as a multi-armed bandit problem (Hofmann et al., 2011; Wang et al., 2021). The model explores different retrieval strategies and receives feedback based on the relevance and diversity of the retrieved passages. These approaches can be effective but can be computationally expensive for large-scale retrieval tasks.\nRAG models have also been extended to incorporate diversity objectives. For example, RAG with Dense Passage Retrieval retrieves a large number of candidate passages (Cuconasu et al., 2024; Reichman and Heck, 2024; Siriwardhana et al., 2023). It then employs a two-stage selection process: first selecting a diverse subset based on novelty scores, then selecting the most relevant passages from this subset. While effective, this approach requires careful tuning of the selection thresholds."}, {"title": "5 Discussion", "content": "In this paper, we introduce Dartboard, a principled retrieval algorithm that implicitly encourages diversity of retrieved passages by optimizing for relevant information gain. We demonstrate that Dartboard outperforms existing state-of-the-art retrieval algorithms on both retrieval and end-to-end QA tasks. We view this work as an initial step for a more general line of work that optimizes information gain during retrieval, especially in the context of RAG systems. In future work, we plan to investigate Dartboard for other retrieval tasks, such as suggestion generation (see Appendix A.6)."}, {"title": "6 Limitations", "content": "We have not done a systematic investigation of the run time of Dartboard. In the worst case scenario, Dartboard is quadratic in the number of ranked passages. However, in practice, Dartboard hybrid typically runs in a fraction of a second for ranking (based on cosine-similarity with query) a set of 100 passages (note that a full cross-encoder based MMR/Dartboard needs to run the cross-encoder 10,000 times, and can take several seconds). This retrieval time is minimal compared to the time required for a LLM to process the retrieved passages and generate an answer.\nOur experimental results are limited to a single benchmark and a single LLM i.e. ChatGLM (Hou et al., 2024). It remains to be seen whether our results would generalize to other benchmarks and LLMs. We plan to investigate this in future work.\nOne shortcoming of our method (also shared by MMR) is that it requires a hyperparameter that affects how much diversity is encouraged. While we show that Dartboard is robust to the choice of this hyperparameter, it would be ideal to have a method that does not require manual tuning. As part of future work, we plan to investigate methods that automatically adapt to the context of the query. For example, the hyperparameter could be set based on a held-out validation set.\nAnother topic for future work is to investigate if it is also possible for $\\sigma$ to vary depending on the type of query. For example, a query like \u201cTell me facts about The Beatles\" would warrant a broader range of passages than a query like \u201cTell me facts about George Harrison\".\nAnother shortcoming of our approach is that our benchmarking criteria is limited in terms of the evaluation protocol we are using. Our evaluation is based on an exact string match of the output answer generated from the LLM with a set of possible answers. For example, for one question, the generated output answer is considered correct if it contains the exact string 'January 2 2022', 'Jan 2, 2022', etc., but would be considered incorrect if it only contains 'January 2nd, 2022'. However, we left the benchmark as is (modulo our modifications mentioned above) so that our method is easily comparable to that of others.\nFinally, though the initial cosine similarity based proposed Dartboard method is principled, the hybrid variation of Dartboard is not that principled. This is because it tries to compare logits from a cross-encoder with the cosine similarity of a different embedding model, similar to comparing apples with oranges, though it seems to work well as seen in our presented empirical results."}, {"title": "A Appendix", "content": "A.1 Dartboard Algorithm Details\nThe full algorithm for Dartboard is described in Algorithm 1.\nA.2 Baselines\nIn this section, we briefly describe the different variations of Dartboard as well as the competing retrieval methods that we use to compare the performance of Dartboard in Table 2 in the main paper. All methods that rely on using the cross-encoder first use KNN to retrieve the top 100 passages.\n\u2022 Dartboard cossim (D-CS): This is the variation of the proposed Dartboard method that relies on using cosine similarity for ranking passages.\n\u2022 Dartboard crosscoder (D-CC): This is the variation of the proposed Dartboard method that relies on using cross-encoder based similarity.\n\u2022 Dartboard hybrid (D-H): This is the variation of the proposed Dartboard method that relies on using cross-encoder for the Gaussian"}, {"title": "A.3 Modification for cross-encoder based reranker", "content": "Cross-encoder-based reranking has been shown to outperform embedding-based approaches such as cosine similarity (Nogueira and Cho, 2020), as it uses the full computational power of a transformer model, rather than being limited to simple vector operations. We have proposed two variations of Dartboard, namely Dartboard Crosscoder and Dartboard Hybrid, based on how we compute the cross-encoder scores for the Gaussian kernels in Equation 2 given in the main paper. For the Dartboard Crosscoder variation, we use the cross-encoder score $C (q, t)$ before computing the Gaussian kernel for both $N (q, t, \\sigma)$ and $N (t, g, \\sigma)$ in Equation 2. Note that the cross-encoder score is asymmetric, so we simply average the two possible ways to compute the cross-encoder score for $N (t, g, \\sigma)$, i.e., $(C (t, g) + C (g, t))$. For $N (q, t, \\sigma)$, we are only interested in the likelihood of t given q, so we only use the cross-encoder score $C (q, t)$.\nHowever, the cross-encoder is computationally expensive to run for $k^2$ pairs. Hence, we rely on the Dartboard-Hybrid variation wherein we use the cross-encoder score only for the Gaussian kernel $N (q, t, \\sigma)$ whereas we use cosine similarity for the Gaussian kernel $N (t, g, \\sigma)$."}, {"title": "A.4 Dartboard generalizes KNN and MMR", "content": "The Dartboard algorithm can be viewed as a generalization of the traditional retrieval algorithms, KNN and MMR. In order to verify this claim, let us look at the score presented in Equation 1 in the main paper. When the Player 1 has a perfect aim, or in other words, $\\sigma \\rightarrow 0$, $P(T = t|q, \\sigma)$ tends to a point mass distribution such that t = q, and hence the score becomes\n$s (G, q, A, \\sigma) \\rightarrow \\min_{g \\in G} D (q\\vert g)$ (3)\nwhere D is the distance function as before. If the chosen distance function is proportional to the similarity measure, this is nothing but the KNN algorithm. On the other hand, when the chosen distance function is the weighted sum of the similarity between query and guess, and dissimilarity between current guess and past guesses, it reduces to the MMR algorithm."}, {"title": "A.5 Dartboard inherently promotes diversity", "content": "In Figure 3, we show the diversity of the retrieved passages from RGB for both Dartboard and MMR, measured as one minus the average cosine similarity between pairs of retrieved passages. While MMR explicitly encourages diversity, Dartboard does not. However, we observe from the figure that as the parameter $\\sigma$ increases, the diversity of the retrieved passages also increases. This implies that by optimizing the relevant information gain metric, Dartboard inherently ensures diversity in the set of retrieved passages."}, {"title": "A.6 Example of a generative use of Dartboard", "content": "Below is an example of the set of retrieved passages for a query that shows that the passages retrieved by Dartboard are highly diverse compared to those retrieved by KNN which has high redundancy, if we consider the cross-encoder based variations:\nQuery: Do you want to watch soccer?\nCandidates:\n1: Absolutely!\n2: Affirmative!\n3: I don't know!\n4: I'd love to!\n5: Maybe later.\n6: Maybe!\n7: Maybe...\n8: No thanks.\n9: No way!\n10: No, I don't wanna do dat.\n11: No, thank you!\n12: No, thank you.\n13: Not right now.\n14: Not today.\n15: Perhaps..\n16: Sure!\n17: Yeah!\n18: Yes!\n19: Yes, please can we?\n20: Yes, please!\n21: Yes, please.\n22: Yes, we ought to!\n23: Yes, we shall!\n24: Yes, we should!\nKNN crosscoder:\n18: Yes!\n21: Yes, please.\n20: Yes, please!\nDartboard crosscoder:\n18: Yes!\n7: Maybe...\n12: No, thank you."}, {"title": "A.7 Dartboard does not allow for the possibility of exact duplicates", "content": "The \u201cmax\u201d in Equation 2 given in the main paper ensures that the same vector (passage) is not"}]}