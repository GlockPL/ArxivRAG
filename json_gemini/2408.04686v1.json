{"title": "Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles", "authors": ["Xiongtao Sun", "Deyue Zhang", "Dongdong Yang", "Quanchen Zou", "Hui Li"], "abstract": "Large language models (LLMs) have significantly enhanced the performance of numerous applications, from intelligent conversations to text generation. However, their inherent security vulnerabilities have become an increasingly significant challenge, especially with respect to jailbreak attacks. Attackers can circumvent the security mechanisms of these LLMs, breaching security constraints and causing harmful outputs. Focusing on multi-turn semantic jailbreak attacks, we observe that existing methods lack specific considerations for the role of multi-turn dialogues in attack strategies, leading to semantic deviations during continuous interactions. Therefore, in this paper, we establish a theoretical foundation for multi-turn attacks by considering their support in jailbreak attacks, and based on this, propose a context-based contextual fusion black-box jailbreak attack method, named Context Fusion Attack (CFA). This method approach involves filtering and extracting key terms from the target, constructing contextual scenarios around these terms, dynamically integrating the target into the scenarios, replacing malicious key terms within the target, and thereby concealing the direct malicious intent. Through comparisons on various mainstream LLMs and red team datasets, we have demonstrated CFA's superior success rate, divergence, and harmfulness compared to other multi-turn attack strategies, particularly showcasing significant advantages on Llama3 and GPT-4.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), with their formidable text comprehension and generation capabilities, have reshaped our information ecosystem and modes of communication. They have demonstrated outstanding abilities in downstream tasks such as AI search engines, medical diagnostics, and code synthesis. This is attributed to their capacity to capture complex and nuanced language patterns from massive textual data, as well as their robust generalization capabilities when handling multimodal data. Whether it's closed-source LLMs like ChatGPT (OpenAI, 2024), Google Bard (Google, 2023a), Bing Chat (Microsoft, 2023), or open-source models like LLAMA (Touvron et al., 2023), Qwen (Bai et al., 2023), ChatGLM (GLM et al., 2024), continuous optimization and large-scale data training have significantly advanced the ability of LLMs in understanding and generating natural language.\nWhile providing powerful capabilities, LLMs also pose security risks (Zou et al., 2023; Perez and Ribeiro, 2022). Particularly, jailbreak attacks can lead to harmful, biased, or other unexpected behaviors in the outputs of LLMs, such as privacy breaches (Wei et al., 2023; Li et al., 2023). In order to mitigate jailbreak attacks targeting LLMs, secure alignment (Zhang et al., 2023; Ji et al., 2023) has become a standard component of the LLMs training pipeline, and auxiliary methods like perplexity filtering (Jain et al., 2023), white-box gradient probing (Zhao et al., 2024), and malicious content detection (OpenAI Moderation, 2023) are continuously being proposed. However, LLMs remain susceptible to adaptive adversarial inputs. As Figure 1 illustrates, early adversarial inputs focused"}, {"title": "Our Distinction from Previous Research", "content": "Although previous research has explored multi-turn jailbreak attack patterns (Chao et al., 2023; Zhou et al., 2024; Bhardwaj and Poria, 2023), there has been little in-depth discussion regarding the core nature of multi-turn attack patterns and their practical advantages over traditional single-turn attacks. Focusing on multi-turn semantic jailbreak attacks, existing multi-turn approaches fundamentally remain rooted in single-turn attack patterns, tending towards iterative semantic space exploration. However, as security measures continue to advance, this iterative efficiency is expected to diminish. Our study categorizes the advantages of multi-turn attacks as the ability of context to provide better jailbreak strategy support for the target, such as role-playing, scenario assumptions, keyword substitution, etc., to more effectively eliminate direct malicious intent towards the target. Specifically, LLMs possess the ability to comprehend context and engage in multi-turn dialogue, while the security alignment phase often neglects complex multi-turn contextual scenarios. This scarcity diminishes the integrity of LLMs protection strategies. Furthermore, attack automation often relies on the generation capabilities of LLMs, but in multi-turn attacks, complex attack strategies require strong comprehension and logical reasoning capabilities from LLMs. Additionally, due to some vendors' security alignments in the output, attacks often deviate, resulting in pseudo-successful jailbreak."}, {"title": "Challenge", "content": "Although jailbreaking attacks persist, the increasing focus on security in LLMs research has led to the continual development of more robust security mechanisms, making it increasingly challenging to execute attacks within black-box settings. With the evolution from single-turn to multi-turn jailbreaking research, attackers can provide contextual and semantic groundwork for attack targets, leveraging deviations during the security alignment process. Therefore, in multi-turn attacks, attackers are tasked with generating relevant context and skillfully integrating the reconstruction of attack targets. This challenge involves:\n\u2022 Generating context for attack targets to integrate jailbreaking strategies.\n\u2022 Utilizing context to reconstruct attack targets, disguising and reducing malicious intent, thereby avoiding triggering the security mechanisms of large models.\n\u2022 During the attack phase, reducing semantic bias to decrease false positives in jailbreak attacks."}, {"title": "Our Approach", "content": "To address these challenges and effectively leverage the advantages of multi-turn strategies, we have developed a contextual multi-turn jailbreak attack method called Contextual Fusion Attack (CFA). This approach draws inspiration from a re-examination of multi-turn jailbreak attacks from first principles, integrating a dynamic loading approach to further refine each attack phase, simplifying the automation dependencies of LLMs, reducing the capability demands of attack strategies on LLMs, and enhancing attack stability. Initially, we filtered and extracted malicious key terms from the target based on semantic relevance. Subsequently, we generated contextual scenarios around these key terms. Finally, we dynamically integrated the target into the contextual scenarios, replacing malicious key terms within the target, ingeniously reducing the direct maliciousness of the attack directives."}, {"title": "Contributions", "content": "We make the following contributions.\n1. Reframed Understanding of Multi-Turn Jailbreaks: We revisit the fundamental nature of multi-turn jailbreak attacks, elucidating the indispensable role of multi-turn dialogues. This analysis clarifies the advantages of multi-turn attack strategies.\n2. Development of Contextual Fusion Attack (CFA): By leveraging the advantages of multi-turn dialogues and the chain of thought (COT) approach, we facilitate stepwise simplification of automation requirements for LLMs in multi-turn attacks, thereby reducing the false positive rate of attacks.\n3. Empirical Validation of CFA's Superiority: We compare CFA with state-of-the-art multi-turn adversarial attack baselines across three public datasets and six mainstream models."}, {"title": "2 Related Work", "content": "We briefly review related work concerning single-turn jailbreak attacks and multi-turn jailbreak attacks."}, {"title": "Single-Turn Attacks", "content": "Early approaches (Shen et al., 2023) relied on manually crafting prompts to execute jailbreak attacks. However, manual crafting was time and labor-intensive, leading attacks to gradually shift towards automation. The GCG method (Zou et al., 2023) employed white-box attacks utilizing gradient information for jailbreak, yet resulting in poor readability of GCG-like outputs. AutoDAN (Liu et al., 2023) introduced genetic algorithms for automated updates, while Masterkey (Deng et al., 2024) explored black-box approaches using time-based SQL injection to probe the defense mechanisms of LLM chatbots. Additionally, it leveraged fine-tuning and RFLH LLMs for automated jailbreak expansion. PAIR (Chao et al., 2023) proposed iterative search in large model conversations, continuously optimizing single-turn attack prompts. GPTFuzz (Yu et al., 2023) combined attacks with fuzzing techniques, continually generating attack prompts based on template seeds. Furthermore, attacks such as multilingual attacks (Deng et al., 2023) and obfuscation level attacks utilized low-resource training languages and instruction obfuscation(Shang et al., 2024) to execute attacks.\nHowever, single-turn jailbreak attack patterns are straightforward and thus easily detectable and defensible. As security alignments continue to strengthen, once the model is updated, previously effective prompts may become ineffective. Therefore, jailbreak attacks are now venturing towards multi-turn dialogues."}, {"title": "Multi-Turn Jailbreak Attack", "content": "(Li et al., 2023) employed multi-turn dialogues to carry out jailbreak attacks, circumventing the limitations of LLMs, presenting privacy and security risks, and extracting personally identifiable information (PII). (Zhou et al., 2024) utilized manual construction of multi-turn templates, harnessing GPT-4 for automated generation, to progressively intensify malicious intent and execute jailbreak attacks through sentence and goal reconstruction. (Russinovich et al., 2024)facilitated benign interactions between large and target models, using the model's own outputs to gradually steer the model in task execution, thereby achieving multi-turn jailbreak attacks. (Bhardwaj and Poria, 2023)conducted an exploration of Conversation Understanding (CoU) prompt chains for jailbreak attacks on LLMs, alongside the creation of a red team dataset and the proposal of a security alignment method based on gradient ascent to penalize harmful responses. (Li et al., 2024)decomposed original prompts into sub-prompts and subjected them to semantically similar but harmless implicit reconstruction, analyzing syntax to replace synonyms, thus preserving the original intent while undermining the security constraints of the language model. (Yang et al., 2024) proposed a semantic-driven context multi-turn attack method, adapting attack strategies adaptively through context feedback and semantic relevance in multi-turn dialogues of LLMs, thereby achieving semantic-level jailbreak attacks. Additionally, there are strategies that utilize multi-turn interactions to achieve puzzle games (Liu et al., 2024a), thus obscuring prompts and other non-semantic multi-turn jailbreak attack strategies.\nPresently, multi-turn semantic jailbreak attacks exhibit vague strategies and high false positive rates. We attribute this to the unclear positioning of multi-turn interactions within jailbreaking and excessively complex strategies. Therefore, we have re-examined the advantages of multi-turn attacks and proposed a multi-turn contextual fusion attack strategy."}, {"title": "Factors Influencing Jailbreak Attacks", "content": "(Zou et al., 2024) delved into the impact of system prompts on prison prompts within LLM, revealing the transferable characteristics of prison prompts and proposing an evolutionary algorithm targeting system prompts to enhance the model's robustness against them. (Qi et al., 2024) unveiled the security risks posed by fine-tuning LLMs, demonstrating that malicious fine-tuning can easily breach the model's security alignment mechanism. (Huang et al., 2024) discovered vulnerabilities in existing alignment procedures and assessments, which may be based on default decoding settings and exhibit flaws when configurations vary slightly. (Zhang et al., 2024) demonstrated that even if LLM rejects toxic queries, harmful responses can be concealed within the top k hard label information, thereby coercing the model to divulge it during autoregressive"}, {"title": "3 The Method", "content": "In this section, we initially define multi-turn jailbreak attacks and formalize their principles. Subsequently, we present the intuition behind CFA and delve into the specific procedural details, followed by a discussion and analysis."}, {"title": "3.1 Multi-turn Jailbreak Attacks", "content": "Problem Definition: this paper focuses on the advancement of multi-turn semantic jailbreak attacks on LLMs. The research question is: given a malicious attack target T on a LLM L, how can multi-turn prompt sequences $S = (P_1,P_2,...,P_n)$ be efficiently constructed to prompt the LLM L to produce harmful responses $R_H$ directly relevant to target T? The fundamental issue revolves around efficiently constructing multi-turn inputs to circumvent the model's security alignment and other safety mechanisms."}, {"title": "Threat Model", "content": "We consider a purely black-box attack scenario, where the attacker, apart from obtaining inference outputs from the large language model through prompts, has no access to any details or intermediate states of the target model (e.g., model structure, parameters, training data, gradients, and output logits)."}, {"title": "3.2 Main intuition of CFA", "content": "Our approach is primarily based on the following intuitions:\nLong-text secure alignment datasets with multi-turn and complex contextual understanding are scarce. The continual enhancement of model security capabilities is attributed, on one hand, to methods such as Supervised Fine-Tuning (SFT), Human Feedback Reinforcement Learning (RLHF), Direct Preference Optimization (DPO), and their ongoing refinement and progress, and on the other hand, to the continuous enrichment and accumulation of secure alignment datasets. However, constructing secure alignment datasets requires significant effort and cost. Despite the increasing coverage of security issues in current alignment datasets, there remains a scarcity of long-text datasets specifically addressing multi-turn interactions with complex contextual understanding. Since alignment data resources directly impact the effectiveness of secure alignment, it is easier to breach jailbreak in complex contextual understanding scenarios.\nMulti-turn jailbreak attacks can leverage contextual advantages to dynamically load malicious objectives. Multi-turn dialogues represent a comprehensive reflection of the capabilities of LLMs, involving context comprehension and retention, intent recognition, dynamic learning, and"}, {"title": "Formal definition", "content": "We have simplified the security mechanisms of LLMs into a threshold-based triggering mechanism. For an input p, its toxicity is denoted as Vp, and the security threshold of LLMs is denoted as T. The decision mechanism formula is as follows:\n$D(p) =\n\\begin{cases}\n1 & \\text{if } V_p > T \\\\\n0 & \\text{if } V_p \\leq T\n\\end{cases}$\nIntuitively, due to the absence of multi-turn secure datasets, the security mechanism triggering threshold for LLMs is expected to be more lenient."}, {"title": "Intuition 1", "content": "For the same attack target T, the security threshold $T_T$ in a single-turn scenario is stricter than the security threshold $T_{T|S}$ in a multi-turn interactive scenario.\n$T_T < T_{T|S}$\nIn a multi-turn interactive scenario, a prompt sequence, $S = (P_1,P_2,...,P_n)$, interacts with a LLM to generate multi-turn responses $R = (r_1, r_2,..., r_n)$, where the context $H = (P_1 \\oplus r_1, P_2 \\oplus r_2,..., P_{n-1} \\oplus r_{n-1})$, encompasses the preceding n \u2212 1 turns of dialogue. Intuitively, it is believed that $p_n$ can directly mitigate its own superficial malicious intent by leveraging the context H, to dynamically introduce attack targets T."}, {"title": "Intuition 2", "content": "In a multi-turn interactive scenario, leveraging the context H, can conceal the toxicity $V_{p_n}$, of the attack turn $p_n$, thereby achieving the dynamic loading of attack targets T.\n$\\begin{cases}\nV_{p_n} < T_{T|S} < V_T \\\\\nV_{H \\oplus p_n} \\approx V_T\n\\end{cases}$\nThe fundamental aspect of designing attack strategies A in multi-turn jailbreak attack lies in constructing the context H and the malicious attack round $p_n$ based on the original attack target T. The context H introduces a broader attack space,"}, {"title": "3.3 Approach of CFA", "content": "Following our intuition3.2, we have introduced a method that encompasses rich contextual information while avoiding direct inclusion of harmful content, as illustrated in Figure 2. This method initially filters and extracts malicious keywords from the target of the attack during the preprocessing stage. Subsequently, it constructs contextual queries around these keywords and the target. Finally, it incorporates the attack target into the context and replaces semantically related malicious keywords, thereby avoiding the direct inclusion of harmful content without affecting the semantics.\nThe proposed method, named Contextual Fusion Attack (CFA), automatically generates context based on a specified attack target and integrates it. As shown in Figure 3, CFA consists of three key steps: keyword extraction, context generation, and integration of the attack target. This method draws inspiration from the dynamic loading in software security. Dynamic loading typically does not overtly exhibit its malicious behavior; instead, it manifests at runtime based on triggering conditions, rendering it difficult for static analysis to discern software malignancy."}, {"title": "3.3.1 Preprocess stage", "content": "How can the contextual information H be constructed around the original attack target p? Keywords play a crucial role in the process of Natural Language Understanding (NLU), aiding algorithms in swiftly identifying the themes, sentiments, and intentions within text. Whether for information retrieval, sentiment analysis, or comprehension and generation in dialogue systems, the accuracy heavily relies on the precise identification of keywords."}, {"title": "3.3.2 Context Generation Stage", "content": "How can contextual scenarios H be automatically generated? Different approaches utilize various strategies for multi-turn contexts, including the introduction of malicious progression, semantic reversals, and wordplay. Ultimately, attacks within multi-turn contexts exhibit incoherent semantic connections with the dialogue history and demand high contextual coherence. Within the CFA framework, multi-turn interactions are considered to indirectly support attacks, falling short of the direct jailbreaking effects achieved through methods such as role-playing and system templates. Therefore, the objective of CFA during context generation is to introduce scenes relevant to the attack target,"}, {"title": "3.3.3 Target trigger stage", "content": "How can contextual information H be utilized to modify the original attack input p to obtain the modified attack input p'? In existing multi-turn attacks, semantic divergence and recognizability issues often arise in the final round. This is primarily due to semantic disjunction in the attack round or excessively divergent generation logic within the attack strategy. Therefore, in the final phase of CFA, we need to achieve two objectives: 1. Incorporate contextual scenarios to ensure semantic coherence in multi-urn attacks, effectively leveraging strategies such as role-playing and scenario assumptions. 2. Conceal malicious intent by replacing malicious keywords with contextual"}, {"title": "4 Experimental Setup", "content": "Datasets. In this study, for direct comparison, we selected three widely used test datasets in previous workss, as shown in Table 1. (Huang et al., 2024; Li et al., 2024; Chao et al., 2023; Zhou et al., 2024)\nAdvbench (Zou et al., 2023) consists of 520 malicious prompts widely utilized for assessing jailbreak attacks. We have roughly classified them into six categories, encompassing computer crimes, fraud and financial offenses, terrorism, psychological manipulation, political manipulation, and other unlawful behaviors.\nMaliciousInstruct (Huang et al., 2024) encompasses 100 prompts covering ten distinct malicious intents, thus offering a broader spectrum of harmful instructions. These include psychological manipulation, sabotage, theft, defamation, cyberbullying, false accusations, tax fraud, hacker attacks, fraud, and illicit drug use.\nJailbreakbench (Chao et al., 2024) includes a total of 100 data points covering 18 AdvBench behaviors, 27 TDC/HarmBench behaviors, and 55 unique behaviors from JBB-Behaviors, spanning across ten categories. The dataset covers a range of generated violent content, malicious software, physical harm, economic damage, financial crimes, fabricated information, adult content generation, privacy invasion, and government manipulation."}, {"title": "Model architectures", "content": "In this study, our target models include the open-source models Llama3-8b (Touvron et al., 2023), Vicuna1.5-7b (LMSYS, 2023), ChatGLM4-9b (GLM et al., 2024), and Qwen2-7b (Bai et al., 2023), as well as the closed-source models GPT-3.5-turbo (API)(OpenAI, 2024) and GPT-4(Web) via web interface(OpenAI, 2024). Consistent with prior work, the target model for attack is vicuna, with gpt-3.5-turbo used as the base model for the discriminator. Additionally, all model parameters are set to their default values due to the constraints of (Huang et al., 2024)."}, {"title": "Compared Methods", "content": "We compare our method with previously proposed multi-step approaches, as these methods are all black-box, interactive, or operate in a chained fashion for attacks. Consequently, we do not contrast it with other distinct forms of attacks, such as the white-box attack GCG (Zou et al., 2023).\nPAIR (Prompt Automatic Iterative Refinement): (Chao et al., 2023) introduces a jailbreaking method combining COT, enabling dialogue-based corrections. It leverages dialogue history to generate text for enhancing model reasoning and iterative refinement processes.\nCOU (Chain of Utterances): (Bhardwaj and Poria, 2023) utilizes a chain of utterances (CoU) dialogue to organize information for jailbreak execution, including the incorporation of techniques such as psychological suggestion, non-refusal, and zero-shot.\nCOA (Chain of Attack): (Yang et al., 2024) presents a semantic-driven, context-aware multi-turn attack approach. The method combines toxic increment strategy seed generator to pre-generate multi-turn attack chains. The next course of action is determined based on the model's feedback, and the success of the attack is ultimately assessed using an evaluator."}, {"title": "5 Experiments", "content": "Attack Effectiveness We compare our method with previously proposed multi-step approaches, as these methods are all black-box, interactive, and operate in a chained fashion for attacks. Consequently, we do not contrast it with other distinct forms of attacks, such as the white-box attack GCG.\nTo enhance the persuasiveness of the experimental results, we did not rely solely on our own success classifier. In addition to employing LLM as"}, {"title": "Attack Consistency", "content": "Multi-turn attacks often lead to semantic divergence from the original attack target. Therefore, we conducted deviation tests on"}, {"title": "Attack Severity", "content": "The objective of adversarial attacks is to induce harmful outputs from LLMs, and the outputs of such attacks vary across different methods. Hence, we conducted a severity assessment on the outputs of successful attacks using different methods. We utilized the Google Perspective API (Google, 2023b), encompassing toxicity and insult assessments.\nThe results are displayed in the Figure 7, we observed that CFA maintains its lead in output toxicity. It is evident that semantic-level adversarial attacks result in more harmful outputs compared to some technically oriented adversarial strategies. Within CFA, attack rounds incorporate contextual elements, thereby generating richer and more vivid outputs. The results unequivocally demonstrate the heightened harmful nature of CFA."}, {"title": "6 Conclusions", "content": "In this study, we propose Contextual Confusion Attack (CFA), a context-based multi-turn semantic jailbreaking attack method. By re-evaluating the characteristics of multi-turn attacks from first principles, we streamline the attack process. Through empirical analysis, it significantly reduces attack deviation and enhances the success and harmful-"}]}