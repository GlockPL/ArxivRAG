{"title": "EEG-Based Speech Decoding: A Novel Approach\nUsing Multi-Kernel Ensemble Diffusion Models", "authors": ["Soowon Kim", "Ha-Na Jo", "Eunyeong Ko"], "abstract": "In this study, we propose an ensemble learning\nframework for electroencephalogram-based overt speech classi-\nfication, leveraging denoising diffusion probabilistic models with\nvarying convolutional kernel sizes. The ensemble comprises three\nmodels with kernel sizes of 51, 101, and 201, effectively capturing\nmulti-scale temporal features inherent in signals. This approach\nimproves the robustness and accuracy of speech decoding by\naccommodating the rich temporal complexity of neural signals.\nThe ensemble models work in conjunction with conditional\nautoencoders that refine the reconstructed signals and maximize\nthe useful information for downstream classification tasks. The\nresults indicate that the proposed ensemble-based approach\nsignificantly outperforms individual models and existing state-\nof-the-art techniques. These findings demonstrate the potential\nof ensemble methods in advancing brain signal decoding, offering\nnew possibilities for non-verbal communication applications,\nparticularly in brain-computer interface systems aimed at aiding\nindividuals with speech impairments.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech is a fundamental aspect of human communication,\nenabling the conveyance of intricate thoughts and ideas [1]. It\nis deeply embedded in our social and cultural contexts, playing\na critical role in relationship building and information sharing.\nHowever, individuals with conditions such as locked-in syn-\ndrome are often unable to engage in verbal communication\ndue to physical limitations [2]. Therefore, the development of\ninnovative approaches to restore or replace speech capabilities\nremains a vital research frontier [3], [4]. This study focuses\non decoding brain signals as a means to facilitate non-verbal\ncommunication for such individuals.\nElectroencephalography (EEG) provides a non-invasive\nmethod to capture the electrical activities of the brain through\nscalp electrodes [5]. EEG signals have been widely used in\napplications ranging from neuroscience research to clinical\ndiagnostics [6], [7]. A growing area of interest involves the\ndecoding of EEG signals to derive meaningful information,\nsuch as speech-related activities or cognitive states [8]. EEG-\nbased brain-computer interfaces (BCIs) have been explored for\na variety of applications, including mental state classification\n[9], emotion recognition [10], and motor imagery [11].\nDecoding EEG data related to spoken language poses sig-\nnificant challenges due to the complex and highly variable\nnature of neural activity associated with speech perception\nand production [12]. EEG signals are also prone to noise\nand artifacts, which further complicate accurate interpretation\n[13], [14]. As a result, the development of robust and effective\nmethods for EEG decoding is an ongoing area of research with\nbroad applications, including speech rehabilitation and human-\nmachine interfaces [15]. Previous studies have attempted to\ndecode imagined speech from EEG signals [16], [17], demon-\nstrating the potential of EEG-based BCIs for communication.\nDeep learning techniques have shown promise in addressing\nthese challenges by automatically learning hierarchical rep-\nresentations from raw EEG data [18]. Architectures such as\nDeepConvNet [19] and EEGNet [17] have been used success-\nfully for EEG decoding tasks [19], [20]. Other deep learning\nmodels, including multi-view CNNs [11] and multimodal\ndeep learning networks [3], have also been applied to EEG\nclassification tasks, achieving notable success. In addition,\ngraph-based methods have been utilized for EEG analysis to\nidentify patterns in brain networks [21].\nDenoising diffusion probabilistic models (DDPMs) have\nemerged as powerful tools for learning complex, high-\ndimensional patterns in data by progressively adding and then\nremoving Gaussian noise [22]. These models have proven\neffective in dealing with time series data, including audio and\nvideo streams [23], making them suitable candidates for EEG\nsignal analysis. Recent studies have applied diffusion-based\nmodels to time series data for tasks such as imputation and\nforecasting [24]. In the context of EEG decoding, diffusion-\nbased models have been explored to decode imagined speech\n[16].\nBuilding on these approaches, our study aims to further\nadvance the field of EEG-based speech decoding by em-\nploying an ensemble learning strategy. We utilize DDPMS\ncombined with conditional autoencoders (CAEs) to capture\nthe intricate neural features associated with spoken speech. By\nincorporating multiple models with varying kernel sizes, we"}, {"title": "II. MATERIALS AND METHODS", "content": "The proposed method utilizes an ensemble of DDPMs to\neffectively capture the multi-scale temporal features of EEG\nsignals. Each model in the ensemble is configured with a\ndifferent convolutional kernel size-specifically, kernel sizes\nof 51, 101, and 201-to analyze temporal dependencies at\nvarious scales, as depicted in Fig. 1. This multi-scale approach\nallows the system to capture both fine-grained and coarse-\ngrained temporal features inherent in EEG data."}, {"title": "A. Denoising Diffusion Probabilistic Models", "content": "The \"forward process\" in DDPMs is determined by a fixed\nMarkov chain that progressively adds Gaussian noise to the\ndata. The process starts with the original uncorrupted data,\ndenoted as q(x0), and transforms it using a sequence of\nMarkov diffusion kernels, q(xt|xt\u22121), which are Gaussian\nwith a fixed variance schedule {\u1e9et}f=1. This process can be\nexpressed as:\nq(xt|xt-1) = N(xt; \u221a1 \u2013 \u1e9etxt\u22121, \u03b2\u03c4\u0399),\n(1)\nq(x1:TX0) = [q(xtXt-1).\n(2)\nData in any timestep t can also be directly expressed in\nterms of the original data x0:\nq(xtxo) = N(xt; \u221a\u0101txo, (1 \u2013 \u0101t)I),\n(3)\nwhere at = 1 \u2212 \u1e9et and \u0101t = \u03a0=1 s.\nEach DDPM model in the ensemble aims to denoise the\nnoisy input and generate an output that closely approximates\nthe original signal. We employ a time-conditional U-Net\narchitecture with modifications suitable for EEG data. Each\nmodel predicts a version of the original signal, denoted as\n(k)\n(xt, t), where k \u2208 {1, 2, 3} corresponds to the kernel sizes\n51, 101, and 201."}, {"title": "B. Conditional Autoencoder", "content": "The forward diffusion process introduces information loss,\nwhich is addressed by CAE. The CAE is designed to recognize\nand correct these errors, resulting in more accurate represen-\ntations of the original EEG signals. The ensemble setup en-\nhances this process by providing diverse signal reconstructions\nthat the CAE can refine. The objective function for each CAE\ncorresponding to the k-th DDPM model is:\nLE((k), (k))\n(k)\n=xo - Day(k) (Ed(k) (xt), **) (xt, t)\n(4)\nwhere Ep(k) and Day(k) are the encoder and decoder of the\nCAE for the k-th model."}, {"title": "C. Classifier Ensemble", "content": "After processing through each CAE, the outputs are con-\ndensed into latent representations z(k) using adaptive average\npooling layers. Each latent vector is then fed into its corre-\nsponding linear classifier Cp(*). The predicted labels from each\nclassifier are \u0177(k) = Cp(k) (z(k)). The final predicted label \u0177 is\nobtained by averaging the outputs of the three classifiers:\n\u0177\n=\n3((1) + (2)\n) + \u0177(3)).\n(5)\nThe overall objective function combines the reconstruction\nlosses and the classification losses from all three models:\n3\nLTotal =\nk=1\n(code($(k), $(k)) + a\nLCAE\na ||(k) - Y||2),\n(6)\nwhere a is a hyperparameter controlling the balance between\nreconstruction and classification losses, set to 0.1 in our\nexperiments."}, {"title": "D. Model Implementation Details", "content": "In our study, we employ an ensemble of three DDPMs with\nconvolutional kernel sizes of 51, 101, and 201, respectively.\nThis design enables the models to capture EEG features at\nmultiple temporal scales, enhancing the ability to model the\ncomplex temporal dynamics of EEG signals. Each DDPM and\nits corresponding CAE consist of convolutional, normalization,\nand activation layers tailored to effectively process EEG data.\nThe classifiers Cp(k) are trained jointly with their respective\nCAEs. The latent vector z(k) for each model has a fixed\ndimension of 256. Optimization is carried out using the\nRMSProp optimizer with a cyclic learning rate starting at\n9 \u00d7 10-5 and capped at 1.5 \u00d7 10-3. Training is carried out\nover 500 epochs, using L1 loss for the DDPMs and CAES,\nand mean squared error for the classifiers' one-hot encoded\noutputs.\nFor model evaluation, 20% of the data is reserved for test-\ning, with a consistent random seed to ensure reproducibility.\nDuring inference, the predicted labels from the three classifiers\nare averaged to obtain the final prediction, as described in\nEquation (7):\n\u0177 =\n1-3\n((1) \u0177(1) + \u0177(2) +(3)).\n(7)\nBy integrating the outputs of multiple classifiers trained\non different temporal scales, the ensemble approach enhances\nthe robustness and accuracy of EEG signal classification.\nThis method effectively leverages the strengths of each model\nto improve overall performance in decoding EEG signals\nassociated with overt speech."}, {"title": "E. Dataset", "content": "1) Data Description: The data utilized in this study were\nsourced from a previous investigation conducted by Lee et al.\n[14]. The participants included 22 healthy adults, 15 of whom\nwere male, with a mean age of 24.68 \u00b1 2.15 years. None of the\nparticipants had a history of neurological conditions, language\ndisorders, hearing, or vision impairments. Additionally, they\nrefrained from drug use for at least 12 hours prior to the\nstudy. All participants had received over 15 years of high-\nquality English education. For the overt speech task, the\n22 subjects were asked to produce 12 different words or\nphrases, such as \u201cambulance,\u201d \u201cclock,\u201d \u201chello,\u201d \u201chelp me,\"\n\"light,\" \"pain,\u201d \u201cstop,\" \"thank you,\u201d \u201ctoilet,\u201d \u201cTV,\u201d \u201cwater,\u201d\nand \"yes,\" along with a resting state condition, creating a total\nof 13 distinct classes. EEG signals were recorded using a 64-\nchannel cap fitted with active Ag/AgCl electrodes, following\nthe international 10-20 system. The FCz and FPz channels\nserved as the reference and ground electrodes, respectively.\nEEG data were collected via Brain Vision/Recorder software\n(BrainProduct GmbH, Germany) and processed using MAT-\nLAB 2018a. The impedance of all electrodes was maintained\nbelow 10 \u039a\u03a9. The 22 blocks of 12 words and the resting\nstate were presented to the participants in random order. Each\nparticipant contributed 1,300 samples, comprising 100 samples\nfor each category. The study was approved by the Korea\nUniversity Institutional Review Board [KUIRB-2019-0143-01]\nand followed the guidelines of the Declaration of Helsinki.\n2) Preprocessing: Several preprocessing techniques were\napplied in this study to enhance the accuracy of the EEG\ndata. First, a bandpass filter was used to retain signals within\nthe 0.5 to 125 Hz range, along with notch filters at 60\nand 120 Hz to remove power line interference. Following\nthis, a common average referencing method was employed to\nfurther minimize noise. To eliminate artifacts caused by eye\nmovement and muscle activity, automatic methods were uti-\nlized for electrooculography and electromyography removal.\nOnce the artifacts were removed, the EEG signals within the\nhigh-gamma frequency band were selected for model training\nand data analysis. The dataset was segmented into 2-second\nepochs, with a baseline correction applied 500 ms prior to task\nonset."}, {"title": "III. RESULTS AND DISCUSSION", "content": "In this study, we evaluated the performance of our proposed\nensemble method, which utilizes three DDPMs with kernel\nsizes of 51, 101, and 201, against three established models:\nDeepConvNet [19], EEGNet [17], and the approach proposed\nby Lee et al. [14] in the context of decoding EEG signals\nrelated to spoken speech. The results, summarized in Table\nI, demonstrate that our ensemble method achieved superior\nperformance in both accuracy and area under the curve (AUC).\nSpecifically, our model obtained an average accuracy of 85.47\n%, with a standard deviation of 4.23 %, and an average AUC\nof 97.85%, with a standard deviation of 1.67%. These results\nsignificantly surpass the performance of the baseline methods.\nDeepConvNet, EEGNet, and the method of Lee et al. [14]\nachieved average accuracies of 32.34 %, 42.73%, and 57.06\n%, and average AUCs of 73.00 %, 81.00 %, and 83.01%,\nrespectively, demonstrating the enhanced capability of our\nproposed ensemble model to effectively decode EEG signals\nrelated to speech.\nThe substantial improvement in performance can be at-\ntributed to the ensemble of DDPMs with varying kernel sizes,\nwhich allows the model to capture multi-scale temporal fea-\ntures more effectively. By averaging the outputs of classifiers\ntrained on different temporal scales, the ensemble method\nenhances robustness and generalization, leading to higher\nclassification accuracy and AUC.\nAn ablation study was conducted to assess the contributions\nof each component in our model, as shown in Table II.\nRemoving the DDPMs resulted in a decrease in accuracy to\n68.12% and AUC to 90.45%, indicating the importance of the\ndiffusion models in capturing the complex temporal dynamics\nof EEG signals. Further removing both the DDPMs and the\ndecoder Dry led to a significant drop in performance, with\naccuracy decreasing to 55.89% and AUC to 72.34 %. This\nhighlights the critical role of both the DDPMs and the CAE\nin our ensemble framework.\nThe experimental results demonstrate that our ensemble\napproach significantly surpasses existing methods in EEG-\nbased speech decoding. Using multiple DDPMs with varying"}, {"title": "IV. CONCLUSION", "content": "The experimental results of our study indicate that our en-\nsemble approach significantly outperforms existing methods in\nEEG-based speech decoding. By leveraging multiple DDPMS\nwith varying convolutional kernel sizes, our model is able to\ncapture a wider and more comprehensive range of temporal\nfeatures inherent in EEG signals. This multi-scale analysis\nis crucial for decoding the complex and variable nature of\nEEG signals associated with speech, as it allows the model to\neffectively interpret both fine-grained and long-range temporal\ndependencies in the neural data. Furthermore, the ensemble\nenhances robustness and generalization by combining the\nstrengths of individual DDPMs tuned to different temporal\nscales, resulting in improved performance metrics compared\nto state-of-the-art methods. Overall, our study contributes\nto improving EEG-based decoding methods by introducing\na novel ensemble framework. We provide a foundation for\nfuture advancements in non-verbal communication systems\nand highlight the importance of multi-scale temporal analysis\nin neural signal processing."}]}