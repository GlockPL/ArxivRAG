{"title": "Residual Channel Boosts Contrastive Learning for Radio Frequency Fingerprint Identification", "authors": ["Rui Pan", "Hui Chen", "Guanxiong Shen", "Hongyang Chen"], "abstract": "In order to address the issue of limited data samples for the deployment of pre-trained models in unseen environments, this paper proposes a residual channel-based data augmentation strategy for Radio Frequency Fingerprint Identification (RFFI), coupled with a lightweight SimSiam contrastive learning framework. By applying least square (LS) and minimum mean square error (MMSE) channel estimations followed by equalization, signals with different residual channel effects are generated. These residual channels enable the model to learn more effective representations. Then the pre-trained model is fine-tuned with 1% samples in a novel environment for RFFI. Experimental results demonstrate that our method significantly enhances both feature extraction ability and generalization while requiring fewer samples and less time, making it suitable for practical wireless security applications.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid development of the Internet of Things (IoT) devices, secure authentication has become increasingly critical. Traditional encryption-based methods impose significant overhead on low-cost, energy-constrained IoT devices [1]. Recently, radio frequency fingerprint identification (RFFI) has been proposed to enhance the security of these low-cost IoT devices. In a nutshell, it extracts unique hardware impairments as device identifiers, which are generated during the manufacturing process and are difficult to counterfeit [2]. Conventional RFFI needs manual feature extraction with expertise and yields unsatisfactory results. Deep learning (DL) is more efficient that it enables the automatic extraction of features, bypassing the limitations of manual approaches. Features such as IQ imbalance can be effectively extracted using DL methods, with convolutional neural network (CNN) demonstrating particular efficacy [2]\u2013[15].\nThe transmission through wireless channels introduces variations in the data distribution between the training and test domains, resulting in performance degradation in cross-scenario applications. This dataset shift is especially prominent in RFFI due to dynamic environmental changes, which render pre-trained models ineffective. Moreover, the difficulty in collecting sufficient labeled data for retraining models in new environments further impedes real-world applications. To address this challenge, several studies propose extracting channel-independent features [7], [10], [12], [16]. While these methods effectively reduce the impact of channel variations and can be considered domain generalization approaches, domain generalization struggles to cover all possible domains [17].\nAchieving robust performance with few labeled data is valuable for real-world applications. To address the challenge of the limited label in RFFI, contrastive learning (CL) is commonly used for unsupervised pre-training [9], [11], [14], [15]. CL improves feature extraction by maximizing similarity within positive pairs, and the pre-trained model is fine-tuned with a small labeled dataset for downstream tasks. While CL typically requires stronger data augmentation [18], existing methods often use computer vision-based techniques such as mask [11], flipping [9], [15], and rotation [15], which may not be well-suited for wireless communications. Some methods use simulated channels and noise [14], but these do not fully capture real-world conditions.\nEqualization is also a viable way to deal with wireless channels and an essential process for data demodulation. In our previous work [13], we employed equalization and domain adaptation to improve model performance in new environments. In this paper, we propose a data augmentation method for unsupervised CL based on equalization. Our main contributions are as follows:\n\u2022 We use different channel estimations to equalize raw signals, enabling the model to extract features from diverse residual channels in an unsupervised way and learn more meaningful representations.\n\u2022 We propose a lightweight CL framework for RFFI, which significantly reduces memory requirements and computing resources.\n\u2022 We evaluate our method on a simulated dataset and demonstrate that it reduces training time and labeled sample requirements while achieving competitive performance with pre-trained models.\nThe rest of the paper is organized as follows. Section II discusses the impact of IQ imbalance on devices and the role of channel equalization. Section III explores contrastive learning and fine-tuning. Section IV presents the experimental settings and results. Finally, conclusions are provided in Section V."}, {"title": "II. IQ IMBALANCE AND RESIDUAL CHANNEL", "content": "The modulated signal is typically represented as a complex value and transmitted through the in-phase (I) and quadrature-phase (Q) branches, expressed as:\n$x = x_I + jx_Q$,\nwhere $x_I$ and $x_Q$ denote the I and Q components, respectively. Before transmission, the signal is upconverted to the RF band, and IQ imbalance will be brought to I and Q branches to the baseband signal $x_{BB}$ as [2]:\n$x_{BB} = (g_I \\cos{\\theta} x_I - g_Q \\sin{\\theta} x_Q) + j(g_I \\sin{\\theta} x_I + g_Q \\cos{\\theta} x_Q)$,\nwhere\n$g_I = 10^{\\frac{A}{20}}$,\n$g_Q = 10^{-\\frac{A}{20}}$,\n$\\theta = 0.5 P \\frac{\\pi}{180}$,\n$A$ is the amplitude imbalance in dB and $P$ is the phase imbalance in degree. Compared to the signal components, IQ imbalance tends to remain more stable over time, making it a distinct and consistent characteristic for CNN.\nAfter transmission and carrier frequency offset compensation, the received signal in the frequency domain is\n$y = h x_{BB} + n$,\nwhere $h$ is the channel and $n$ is the corresponding noise.\nTo extract the transmitted signal, the channel is estimated by pilots, and equalization is applied to $y$. There are two classical channel estimation methods, least squares (LS) estimation and minimum mean square error (MMSE) estimation [19], [20]. LS estimation is effective without prior knowledge of the channel and noise. The estimated channel using LS is given by:\n$H_{LS} = \\frac{y_p}{x_p}$,\nwhere $y_p$ and $x_p$ are the received and transmitted pilot signals, respectively. However, LS estimation neglects the effect of noise, and its corresponding mean square error (MSE) increases with noise. Specifically, the MSE of LS estimation is inversely proportional to the signal-to-noise ratio (SNR), given by MSE($\\hat{h}_{LS}$) = $\\frac{1}{SNR}$ [19]. To mitigate the impact of noise, MMSE estimation introduces a weight matrix applied to the LS estimation:\n$H_{MMSE} = R_{hhls}(R_{hh} + \\frac{1}{SNR}I)^{-1}h_{LS}$,\nwhere $R_{hhls}$ is the covariance matrix between the true channel $h$ and $h_{LS}$, $R_{hh}$ is the autocovariance matrix of the true channel, $I$ is the identity matrix, and $(\\cdot)^{-1}$ denotes the inverse of a matrix. By leveraging the statistical properties of the true channel, MMSE estimation effectively suppresses noise and enhances channel estimation accuracy.\nThen, equalization is applied to the received signals as follows:\n$\\hat{y}_{BB} = \\frac{y}{\\hat{h}}$\n$= (\\hat{h} + h - \\hat{h})x_{BB} + n$\n$= x_{BB} + \\Delta h$,\nwhere $\\Delta h$ represents the residual channel in the equalized signal. This $\\Delta h$ is the primary reason for performance degradation in cross-scenario applications, even when equalization is applied. Although MMSE equalization can further suppress the impact of noise, $\\Delta h$ provides an opportunity for data augmentation. It can be leveraged in CL to extract robust RFFI features."}, {"title": "III. CONTRASTIVE LEARNING FOR RFFI", "content": "SimSiam [21] employs a Siamese network architecture, where two identical CNNs are used to extract features from the input data, which can significantly reduce memory requirements, as only one network needs to be stored during training. Furthermore, SimSiam operates effectively with a normal batch size and only positive pairs, minimizing the demand for computing resources. In our work, equalized signals with I and Q branches are used as input data, which are efficiently processed using a lightweight 1D-CNN. These characteristics and the following experimental results demonstrate our method is a promising choice for RFFI.\nData augmentation plays a critical role in CL, as it facilitates the learning of more robust and meaningful representations. In addition to utilizing different channel estimation, we apply additive white Gaussian noise (AWGN) augmentation to enhance the model\u2019s robustness and block-wise masking augmentation to encourage the learning of semantic-independent features [8], [11].\nFirst, AWGN is added to the signals, followed by equalization using LS and MMSE methods under the corresponding SNR. Next, block-wise masking is applied to the augmented signals. After completing the data augmentation process, we maximize the cosine similarity between the two features extracted by the encoder. The process flow is illustrated in Fig. 1.\nThe SimSiam framework consists of a backbone model (e.g., a CNN) combined with a projection multilayer perceptron (MLP) serving as the encoder f(\u00b7), a prediction MLP h(\u00b7). Assume the data augmentation techniques for positive pairs are $T_1 \\sim \\mathcal{T}_1$ and $T_2 \\sim \\mathcal{T}_2$. The positive pairs come from the same inputs $y$ as $x_{LS} = T_1(y)$ and $x_{MMSE} = T_2(y)$, which returns different equalized signals and other augmentations. Let the two outputs as $p_1 = h(f(x_{LS}))$ and $z_2 = f(x_{MMSE})$, their negative cosine similarity is defined as:\n$D(p_1, z_2) = - \\frac{p_1 \\cdot z_2}{||p_1||_2 ||z_2||_2}$\nwhere $||\\cdot||_2$ is L2-norm. We optimize the model with symmetrized loss and use stop-gradient $sg(\\cdot)$ operation as\n$L_{CL} = \\frac{1}{2} D(p_1, sg(z_2)) + \\frac{1}{2} D(p_2, sg(z_1))$.\nStop-gradient operation is vital for SimSiam to work, the encoder on $x_1$ receives only gradient from $p_1$ in the first term but not $z_1$ from the second term."}, {"title": "B. Fine-Tuning", "content": "After CL training, the backbone is retained, while the other parts of the architecture are discarded. It acquires stronger feature extraction capabilities but still requires fine-tuning to adapt to downstream tasks, such as RFFI. Compared to traditional supervised training, a powerful pre-trained model needs a few labeled datasets to achieve the supervised performance.\nThe supervised fine-tuning is carried out in the standard way. First, the pre-trained backbone is used as a feature extractor. Then, a simple MLP, initialized randomly, is added to the backbone to form a standard CNN structure as shown in Fig. 2. The model is trained using Cross-Entropy (CE) loss, which backpropagates the gradient to update the model\u2019s parameters. The CE loss is defined as:\n$L_{CE} = - \\sum_{i=1}^{C} y_i \\log(\\sigma(z)_i)$,\nwhere $y_i$ represents the label, and $\\sigma(\\cdot)$ is a softmax operation for the final output of MLP, $z = (z_1, z_2, ..., z_C)$ with $C$ classes. The softmax operation is defined as:\n$\\sigma(z) = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_i}}$"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We follow previous work\u2019s settings for IQ imbalance [13].\nUsing Python, we generate raw signals with IQ imbalance parameters as shown in Table I for 7 devices. The imbalance parameters are evenly distributed in the range [-0.9, 0.9] and [-3, 3]. The FFT length is set to 64, with 52 valid subcarriers, and QPSK modulation is applied. The simulated data also includes a long training sequence (LTS) for channel estimation. We adopt the tapped delay line (TDL) model for the simulated channel [14], with an exponential power delay profile (PDP) to generate multipath power and the Jakes model to account for Doppler effects with the Doppler frequency randomly distributed uniformly in the range [0, 5] Hz. The RMS delay is set to 30 ns. The basic SNR is set to 20 dB for"}, {"title": "V. CONCLUSION", "content": "This paper proposes a contrastive learning method using residual channels for data augmentation, with LS and MMSE estimations followed by equalization. The SimSiam network learns robust fingerprint features, achieving performance comparable to supervised learning with just 1% of labeled data. The equalization removes residual channel interference without additional process, ensuring high efficiency. The lightweight model is computationally efficient, making it ideal for real-time tasks in wireless communication."}]}