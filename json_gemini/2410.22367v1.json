{"title": "MAMMAL - Molecular Aligned Multi-Modal Architecture and Language", "authors": ["Yoel Shoshan", "Moshiko Raboh", "Michal Ozery-Flato", "Vadim Ratner", "Alex Golts", "Jeffrey K. Weber", "Ella Barkan", "Simona Rabinovici-Cohen", "Sagi Polaczek", "Ido Amos", "Ben Shapira", "Liam Hazan", "Matan Ninio", "Sivan Ravid", "Michael M. Danziger", "Joseph A. Morrone", "Parthasarathy Suryanarayanan", "Michal Rosen-Zvi", "Efrat Hexter"], "abstract": "Drug discovery typically consists of multiple steps, including identifying a target protein key to a disease's etiology, validating that interacting with this target could prevent symptoms or cure the disease, discovering a small molecule or biologic therapeutic to interact with it, and optimizing the candidate molecule through a complex landscape of required properties. Drug discovery related tasks often involve prediction and generation while considering multiple entities that potentially interact, which poses a challenge for typical AI models. For this purpose we present MAMMAL - Molecular Aligned Multi-Modal Architecture and Language a method that we applied to create a versatile multi-task foundation model ibm/biomed.omics.bl.sm.ma-ted-458m that learns from large-scale biological datasets (2 billion samples) across diverse modalities, including proteins, small molecules, and genes. We introduce a prompt syntax that supports a wide range of classification, regression, and generation tasks. It allows combining different modalities and entity types as inputs and/or outputs. Our model handles combinations of tokens and scalars and enables the generation of small molecules and proteins, property prediction, and transcriptomic lab test predictions. We evaluated the model on 11 diverse downstream tasks spanning different steps within a typical drug discovery pipeline, where it reaches", "sections": [{"title": "1 Introduction", "content": "Drug discovery traditionally follows a multi-step pipeline that begins with identifying disease-associated proteins, progresses to finding compounds that can effectively target these proteins, and culminates in the optimization of drug candidates to meet rigorous standards for efficacy and safety. This process is both costly and labor-intensive [1], requiring extensive laboratory assays that measure drug-target interactions, assess cellular changes in disease-relevant cell lines, and validate therapeutic efficacy and safety [1? -3]. Drugs in development can be either small molecules, which are stable, easy to manufacture, and suitable for oral delivery [4], or biologic therapeutics, such as engineered antibodies, which offer high specificity but require complex manufacturing and are typically administered by injection [5].\nAccelerating drug discovery has become a central focus in biomedical research, aiming to streamline target identification, drug design, and testing [6-8]. Analyzing gene expression profiles, particularly from single-cell RNA sequencing (RNA-seq), has emerged as a key tool for distinguishing between cell populations associated with different diseases [9-12]. This analysis enhances our understanding of disease mechanisms, facilitates the identification of new drug targets, and allows for the examination of drug effects across various cell types [13, 14]. In drug design, trained generative models are utilized to synthesize new drug candidates for further exploration [15-17]. As high-throughput screening assays for measuring drug binding affinity are costly and challenging to scale, accurately predicting drug-target interactions can significantly enhance drug design, improving both efficacy and precision. Overall, predictive modeling of binding affinity, toxicity, and efficacy in the early stages of the pipeline can reduce reliance on expensive late-stage testing, ultimately saving time and resources in drug development.\nWhen creating predictive and generative AI models, one of the key challenges in the field involves the question - how should different modalities and entities be combined as inputs/outputs to a model ?[18] This uncertainty is especially dominant in predictive and generative tasks that involve interaction between entities, for example, predicting whether a specific antibody and a protein target are likely to bind or not.\nIn this work, we introduce the MAMMAL (Molecular Aligned Multi-Modal Architecture and Language) method, and develop a multi-aligned foundation model paired with a prompt syntax that integrates multiple data domains to support a wide range of drug discovery tasks. The multi-aligned model, which enables aligning multiple entities into a single prompt, has been extensively pre-trained on 2 billion samples from diverse datasets, using auxiliary tasks of mask infilling, denoising, generation, and classification. The multi-aligned model is compatible with both encoder-decoder and encoder-only architectures and effectively incorporates numerical values through continuous token embedding, enhancing numerical precision and reducing vocabulary size. We rigorously evaluate the multi-aligned model across 11 downstream tasks spanning classification, regression, and generation covering key stages of the drug discovery pipeline across three primary domains: small molecules, proteins, and gene expression profiles. The multi-aligned model achieves state-of-the-art performance in 9 tasks and matches top performance in the remaining 2 tasks. The model is publicly"}, {"title": "2 Methods", "content": "The MAMMAL method is built around three core components: the model architecture, the molecular prompt syntax, and extensive pretraining. In Subsection 2.1, we detail the architecture and its enhancements to the standard transformer framework. Subsection 2.2 focuses on the molecular prompt syntax, a key feature that enables the support of a diverse range of pretraining and downstream tasks for drug discovery. Finally, Subsection 2.3 outlines the pretraining process, which fascilitates leveraging large, cross-domain datasets and handling multiple entities simultaneously."}, {"title": "2.1 MAMMAL Architecture", "content": "The MAMMAL framework builds on the transformer architecture introduced by Vaswani et al. [19], and is inspired by the T5 framework [20] to formulate tasks as sequence-to-sequence problems within a unified model. MAMMAL introduces three primary features:\n\u2022 Task modeling in either an encoder-only mode, akin to BERT [21], or an encoder-decoder autoregressive mode [19]. The weights of the encoder stack are shared across both modes, enabling multi-task training that integrates tasks from both types. Model parameter updates are performed through gradient accumulation across all tasks.\n\u2022 Integral support for the molecular prompt syntax through the new Modular Tokenizer component, which facilitates the extension of molecular domain vocabularies and the incorporation of new domain vocabularies without necessitating the retraining of existing models.\n\u2022 Supporting numerical values (scalar) as both inputs being fed into the model, and also as outputs that the model can learn to predict. This is done in a continuous way, not requiring any binning or translation to a discrete space.\nMore details on the architecture and how scalars inputs and outputs are supported can be found in Appendix A"}, {"title": "2.2 Prompt Syntax", "content": "The prompt syntax employs a nomenclature of special tags that represent elements of molecular entities, molecular sequences, and their attributes, as well as interactions within the broader molecular system. It is designed to accommodate multiple data domains by providing tokenization hints for different segments of the input sequence, with all tokenizers supporting a common set of special tokens, such as (EOS). Numeric values are handled by a designated tokenizer, and the syntax is applicable to both model inputs and outputs. Furthermore, the syntax is extensible, allowing for the addition of new tags to each tokenizer or to the common set of special tokens. The"}, {"title": "2.3 Pretraining", "content": "MAMMAL is designed as a comprehensive foundation model, capable of spanning multiple domains and accommodating a variety of entities. It is intended to support diverse task types, ranging from representation-focused tasks to generation-oriented ones. To achieve this, MAMMAL is trained on multiple tasks concurrently. Pretraining was conducted on 2 billion samples sourced from six datasets, which are all publicly available, covering three distinct domains across seven tasks. Table 1 summarizes these tasks, detailing the relevant domains, entity types, and specific datasets. Additional details about the pretraining are provided in Appendix C."}, {"title": "2.4 Evaluation", "content": "We compiled a comprehensive set of 11 benchmarks covering multiple data domains and task types, including classification, regression and generation, as well as single-entity, multi-entity, and multi-domain tasks. These benchmarks address key stages"}, {"title": "3 Results", "content": "Below, we present each benchmark used to evaluate ibm/biomed.omics.bl.sm .ma-ted-458m, along with performance results from the corresponding fine-tuned models. Each benchmark description includes background on the task, its significance for drug discovery, relevant prior models, and data statistics. A summary of the benchmarks, along with SOTA and MAMMAL results, is presented in Table 2 and visualized in Figure 1(E). Examples of encoder inputs and decoder labels for each benchmark are provided in Table S1."}, {"title": "3.1 Cell Type Annotation", "content": "Cell type prediction enables researchers to distinguish between different cell populations, such as those associated with various diseases [9-12]. It is also crucial for understanding how diseases or drugs affect different cell types. In recent years, a variety of methods have been developed for this task, including approaches based on marker genes, correlation-based techniques, and annotation using classification [37]. Recent advances in transformer-based and large-scale foundation models [28, 38, 39] have improved performance by utilizing the full list of genes as input, in contrast to earlier methods where gene selection was based on highly Variable genes (HVG).\nThe input for this task commonly consists of gene expression (GE) values from single-cell RNA-seq data. The benchmark we used is based on the Zheng68k dataset [40], which is derived from human peripheral blood mononuclear cells (PBMCs) and is widely used for evaluating cell-type annotation performance. The dataset contains 68,579 cells across 11 cell types and originally included 32,738 genes. Preprocessing involved normalization, log-transformation of expression values and followed filtering out non-expressed genes, leaving around 20,387 genes. Similar to the approach in [41],"}, {"title": "3.2 BBBP and Clin Tox", "content": "Drugs must meet various criteria regarding both efficacy and safety. In this study, we selected two relevant benchmarks from MoleculeNet [42], a widely used suite of benchmarks for small-molecule drugs: BBBP and ClinTox. The task in the BBBP benchmark is to predict the ability of drugs to penetrate the blood-brain barrier, a crucial factor in the development of drugs targeting the central nervous system. The Clin Tox benchmark involves two related tasks: (1) predicting failure in clinical toxicity"}, {"title": "3.3 Cancer-Drug Response (CDR)", "content": "Identifying drug response at the cellular level is a critical step in the development of new drugs. Two key public databases supporting this effort, particularly in cancer drug development, are the Cancer Cell Line Encyclopedia (CCLE) [43] and the Genomics of Drug Sensitivity in Cancer (GDSC) [44]. CCLE provides multi-omics profiles for around 1,000 cancer cell lines, while GDSC offers data on the drug responses of these lines to hundreds of drugs, commonly measured using the half-maximal inhibitory concentration (IC50). Notable computational models addressed this task [31, 45, 46].\nFor our study, we used three subsets of the GDSC database: GDSC1 and GDSC2, available through the Therapeutics Data Commons (TDC) [47], and referred in the paper as Cancer-Drug Response 1 and Cancer-Drug Response 2 respectively; and a subset published in [31], referred as Cancer-Drug Response 3. Table S3 summarizes the number of cell lines, drugs, and cell-drug pairs in these datasets. We used the random splits provided by TDC for Cancer-Drug Response 1 and 2, while for Cancer-Drug Response 3, we followed the split methodology outlined in [31], reserving 5% of the data for the test set, stratified by TCGA [48] pathways associated with the cancer cell lines.\nDuring fine-tuning we used only gene-expression profiles and SMILES representations of drugs, as shown in the example query in Table S1. Similar to the input format for cell type annotation, gene-expression profiles were provided as ranked lists of gene names based on their expression levels. For predicting continuous IC50 values, MAMMAL was utilized in regression mode, taking advantage of its built-in support for floating point scalar predictions. As demonstrated in Table 2, our model outperforms the current SOTA models for Cancer-Drug Response 1 and 2, achieving a 3.4% increase in Pearson correlation values. Additionally, it yields results comparable to the SOTA for the Cancer-Drug Response 3 benchmark, with a slight improvement of 0.5%.."}, {"title": "3.4 Targeted Antibody Design", "content": "Antibodies are a family of proteins produced by the immune system to neutralize foreign antigens and are of particular interest due to their high specificity and strong binding to target molecules [49, 50]. These characteristics have made them a crucial class of therapeutics, driving significant research efforts into the design of new"}, {"title": "3.5 Antibody-Antigen Binding Prediction", "content": "Accurate prediction of antigen-antibody binding can enhance the design and optimization of therapeutic antibodies, leading to improved efficacy and specificity. We employ the human epidermal growth factor receptor 2 (HER2) dataset [61] as a benchmark for predicting antibody-antigen binding. HER2 is a key target for certain types of breast and stomach cancers. The dataset includes variations of the clinically approved therapeutic antibody trastuzumab and their corresponding affinities for the HER2 antigen. The dataset comprises 8,935 binding and 25,114 non-binding trastuzumab CDR H3 mutants, each with up to 10 mutations, following de-duplication and the removal of samples labeled as both binding and non-binding.\nThe HER2 dataset was divided into train (70%), validation (15%) and test (15%) sets. Fine-tuning involved three concurrent tasks: mask infilling for the antibody heavy chains, and two classification tasks for antibody-antigen binding prediction: one utilizing the heavy chain sequence and the other based on the CDR3 subsequence. We focus on the heavy chain classification task for performance evaluation. As depicted in table 2, our model achieved an average AUC of 0.879, slightly surpassing the SOTA, which incorporated structural data that our model did not."}, {"title": "3.6 T-Cell Receptor-Epitope Binding", "content": "T-cell receptor (TCR) binding to immunogenic peptides (epitopes) presented by major histocompatibility complex (MHC) molecules is a critical mechanism in the adaptive immune system, essential for antigen recognition and triggering immune responses. The T-cell receptor (TCR) repertoire exhibits considerable diversity, consisting of an a-chain and a \u1e9e-chain that function together to enable T cells to recognize a wide array of epitopes. The \u1e9e-chain is especially significant, as it is crucial for the early stages of T-cell development and possesses greater variability, which enhances the TCR's capacity to identify diverse pathogens effectively. However, understanding the specific interactions between TCRs and epitopes remains a significant challenge due to the vast variability in TCR sequences. Accurate prediction of TCR-peptide binding from sequence data could revolutionize immunology by offering deeper insights into a patient's immune status and disease history. This capability holds potential applications in personalized immunotherapy, early diagnosis, and the treatment of diseases such as cancer and autoimmune disorders. In silico tools designed to model TCR-peptide interactions could also facilitate the study of therapeutic T-cell efficacy and assess cross-reactivity risks, presenting a transformative opportunity for precision medicine.\nWe evaluated the model on the task of predicting TCR-epitope binding from sequence data using the Weber benchmark ([34], [62]), which consists of 47,182 TCR \u1e9e-chain epitope pairs. This dataset covers 192 distinct epitopes and includes 23,139 unique TCR \u1e9e-chain sequences, with 50% of the pairs serving as negative samples created by pairing TCR sequences with epitopes they are not known to bind. The dataset also includes the CDR3 subsequence for each TCR \u1e9e-chain, the most hypervariable region of the chain. We used 10-fold cross-validation, using folds from the original TITAN paper [34]. Fine-tuning involved three concurrent tasks: TCR \u1e9e-chain mask infilling and two classification tasks: (i) TCR \u1e9e-chain epitope binding prediction and (ii) TCR \u1e9e-chain-CDR3 epitope binding prediction. Here, we report the performance only for the TCR \u1e9e-chain epitope binding prediction task. As depicted in table 2, our model achieved an average AUROC of 0.879, representing a statistically significant improvement of 2% over the SOTA, as our result falls outside the SOTA's confidence interval."}, {"title": "3.7 Protein Protein Interaction - AAG Prediction", "content": "An important factor in drug design is binding affinity, commonly measured by the equilibrium dissociation constant, KD, which is related to the Gibbs free energy AG through the equation\n$AG = kT ln(KD),$\nwhere k is the Boltzmann constant and T is the temperature [63].\nThe effect of mutating several residues in a protein complex on binding affinity can be quantified by the difference in AG between the mutant and the reference (wild-type) complex. This difference is expressed as\n$AAG = AGmutant - Gwild-type.$"}, {"title": "3.8 Drug-Target Interaction", "content": "Predicting drug-target binding affinity plays a crucial role in the early stages of drug discovery. Traditionally, binding affinities are measured through high-throughput screening experiments, which, while accurate, are resource-intensive and limited in their scalability to evaluate large sets of drug candidates. In this task, we focus on predicting binding affinities using pKd, the negative logarithm of the dissociation constant, which reflects the strength of the interaction between a small molecule (drug) and a protein (target). We utilize the PEER(Protein sEquence undERstanding) benchmark [36] for drug-target interaction (DTI) prediction. This benchmark leverages data from the BindingDB dataset [68], with a specific test split that holds out four protein classes - estrogen receptor (ER), G-protein-coupled receptors (GPCR), ion channels, and receptor tyrosine kinases - for assessing generalization performance on unseen classes.\nFor model fine-tuning, we conducted hyperparameter optimization, selecting an initial learning rate of 0.0004, with no dropout and no weight decay. We standardized the pKd values based on the mean and standard deviation of the training set. For evaluation, we transformed the predicted values back to their original scale. As shown in Table 2, our model achieved an average NRMSE of 0.906, demonstrating a solid improvement of 3.8% over the SOTA reported by [36]."}, {"title": "4 Discussion", "content": "Artificial intelligence (AI) holds great promise for transforming the drug discovery process by enhancing efficiency, accuracy, and speed. Developed with this goal in mind, our proposed method enables the creation of AI models capable of handling diverse tasks across multiple data domains. MAMMAL reformulates tasks as sequence-to-sequence problems and introduces several key architectural enhancements: support"}, {"title": "Appendix A Architecture - additional details", "content": "One of the key aspects in MAMMAL method is the built in support for scalars inputs and outputs. Figure Al illustrates how this is achieved.\nA user prompt, usually expressed as a single text line, is processed into 2 input sequences: a. Input token IDs, which is a sequence of integer values representing tokens in the vocabulary b. a sequence of inputs scalars (by convention, containing NaNs for positions for which no input scalar is provided).\nThe input tokens ids are transformed using a learned token embedding, and the input scalars are transformed using a learned linear transformation which projects each single scalar element into the model dimension (e.g. 768).\nBoth representations are added (not concatenated) and fed into the encoder stack. Using this approach, both tasks that use encoder-only mode and encoder-decoder mode benefit from the ability to get as input an arbitrary number of scalars (at most as many as the number of tokens that are being fed in ).\nFor scalars outputs (gene expression, binding free enery, etc.) the encoder stack has an additional prediction head, which outputs a scalar value for every input element. How to deal with scalars outputs in locations that there is no scalar label is up to the user choice, but the default is to ignore those."}, {"title": "Appendix B Prompt Syntax", "content": "A typical prompt is built as a combination of entities of the following types:\n\u2022 SubSequence: A sequence of amino acids or other chemical representations, such as SMILES, which may encompass a full sequence or a specific region. A SubSequence can begin with two special tokens: (SUBMOLECULAR_ENTITY), followed by a token indicating the SubSequence type (e.g., (CDR3_REGION) ). These tokens are optional and may be omitted when only one SubSequence is present.\n\u2022 Molecule: A complete molecule, such as a protein chain or small molecule, which may contain multiple SubSequences corresponding to sub-regions within the molecule. Each Molecule is initiated with two special tokens - a general token indicating the entity's hierarchical level, (MOLECULAR_ENTITY_TOKEN), followed by a token specifying the molecule type (e.g., (MOLECULAR_ENTITY_EPITOPE)). Additionally, Molecules can be marked with natural start and end tokens to denote instances where truncation has occurred, either in the original database or due to sequence length constraints.\n\u2022 MolecularSystem: A quaternary structure consisting of multiple Molecules, denoted by the\u3008COMPLEX_ENTITY) special token.\n\u2022 GlobalSystem: A system comprising multiple interacting MolecularSystem entities.\n\u2022 Attribute: A representation of properties or interactions among the entities. In predictive tasks, relevant attribute values are masked, while in generative or masked language modeling (MLM) tasks, spans within SubSequences or entire SubSequences are masked. Tokens denoting entity types can also be masked for type prediction tasks. Additionally, each entity may possess alternative expressions or mutations, which can be employed for comparison tasks.\nExample 1: Illustrated in Figure B2. Given two interacting molecules - variable region of a TCR beta chain of an unspecified organism and an epitope of organism 567, find whether they bind, and to which organism the first molecule belongs.:"}, {"title": "B.1 Modular Tokenizer and Meta Tokens", "content": "To support multiple modalities within a single prompt we have developed \"Modular Tokenizer\" which allows to utilize different tokenizers within a single prompt by mapping tokens from different domains (like SMILES carbon \"C\" and amino acid cysteine \"C\") to the same ID space.\nWe use \"Meta Tokens\" of the format (@TOKENIZER-TYPE=...) to indicate that everything following this meta token, up to the next meta token (or the end of the prompt) should be tokenized with the defined tokenizer. For example, (@TOKENIZER-TYPE=AA) tokenizes amino-acids, while (@TOKENIZER-TYPE=SMILES) can tokenize SMILES. Since all of those \"sub tokenizers\" must exist in a single vocabulary space, our modular tokenizer orchestrates that, and provides mechanism to avoid conflicts and for sub-tokenizers to co-exist. Additionally, we support additional instructions within a meta token beyond only expression which (sub) tokenizer should be used. For example, (@TOKENIZER-TYPE=AA@MAX-LEN=1000) allows to restrict the maximum length of the tokenized sequence, which provides more granular control compared to only controlling the overall total max sequence tokenized length. It is worth emphasizing - meta tokens, by themselves, do not get tokenized into any token. They serve as instructions for the modular tokenizer. Further details on the implementation can be found on https://github.com/BiomedSciAI/fuse-med-ml/tree/master/fuse/data/tokenizers/modular_tokenizer"}, {"title": "Appendix C Pretraining Details", "content": ""}, {"title": "C.1 Infrastructure", "content": "ibm/biomed.omics.bl.sm.ma-ted-458m model was trained on an OpenShift cluster. It was trained for three months over two nodes with 16 A100-80G GPUs. The training framework was implemented using FuseMedML [72] and PyTorch, with distributed processing supported by PyTorch Fully Sharded Data Parallel (FSDP) for efficient parallelism."}, {"title": "C.2 Hyperparameters", "content": "We train ibm/biomed.omics.bl.sm.ma-ted-458m using AdamW optimizer, with the following hyperparameters: \u03b21 = 0.9, \u03b22 = 0.999. We use a weight decay of 0.01 and a gradient clipping norm of 1.0. We employ 2K warmup steps until reaching the maximum learning rate and utilize a cosine decay scheduler to decay LR to 10% of the maximum learning rate by the end of training. The maximum sequence length was set per task to be effective yet efficient. When required, instead of naively truncating the end of a sequence, we first wrapped the sequence with special start and end tokens to provide a hint for the model as to whether the beginning or end of a sequence was truncated. Then we randomly cut a random sub-sequence with the required length. Batch sizes were tuned per task given the maximum sequence length to maximize GPU memory utilization."}, {"title": "C.3 Datasets", "content": "ibm/biomed.omics.bl.sm.ma-ted-458m was pre-trained using six diverse datasets spanning multiple domains.\nUniRef90 [71], one of the clustered databases in UniProt [22] (UniProt Reference Clusters), groups protein sequences that share at least 90% identity and 80% sequence overlap with the longest sequence in each cluster (the seed sequence). This clustering approach reduces redundancy while preserving the diversity of functional sequences, providing a rich protein dataset.\nOAS [23] (Observed Antibody Space) offers unpaired antibody sequences, specifically focusing on the variable regions of light or heavy chains. After filtering for sequences with complete variable domains and retaining only the sequences with standard amino acids, we finalized a dataset of approximately 650 million sequences. Each sequence is annotated with its chain type (heavy or light) and species information.\nSTRING [27] is a database that integrates data from experimental findings, computational predictions, and text mining to describe protein-protein interactions. We curated a dataset of 390 million positive protein interaction pairs, considering only pairs that had a STRING confidence score above 500. Additionally, we curated 390 million pseudo-negative pairs by randomly matching proteins from the same species, resulting in a second dataset of 780 million samples.\nCELLxGENE [26], a platform for single-cell transcriptomics data, was used to assemble a dataset of gene expression sequences from individual cells. After filtering for samples labeled as \"cell\" in the 'suspension_type' field, we curated a dataset of 30 million samples. These samples were processed and converted into the Genformer format for use in model training.\nFor small-molecule data, we utilized two main sources: (1) PubChem [25], a comprehensive chemical database maintained by NCBI, and (2) ZINC22 [24], a large library of drug-like molecules. From PubChem, we curated a subset of 80 million \"drug-like\" molecules, removing duplicates and following Lipinski's rule of five to ensure drug-likeness. Additionally, we sampled 120 million molecules from the ZINC22 database, focusing on small molecules with fewer than 30 heavy atoms to ensure dense coverage"}, {"title": "C.4 Multitask Pretraining Approach", "content": "ibm/biomed.omics.bl.sm.ma-ted-458m was pre-trained for seven tasks simultaneously using a multitask learning approach. Gradients from each task were aggregated before applying optimizer updates. This approach, combined with a custom query system, enables the model to learn from different tasks and domains during co-training."}, {"title": "C.5 Pretraining Tasks", "content": "Language Model Tasks. Four language model tasks were defined: (1) amino-acid sequence representation of antibodies based on OAS database [23] (2) amino-acid sequence representation of general proteins based on Uniref90 [22, 71] (3) smiles representation of small molecules based on a mixture of PubChem [25] and Zinc databases [24] (4) Genformer format representations [41] of cell genes based on CELLxGENE [26]. In all language modeling tasks, we employed span-denoising (similar to T5 [20]) with a mean noise span length of 5 and a noise density of 0.15. Additionally, a special token was introduced per entity type to make the model aware of it (e.g., (MOLECULAR_ENTITY_TYPE_ANTIBODY_HEAVY_CHAIN)). When sequences were available from different species, an additional special token was also introduced (e.g., \u3008ATTRIBUTE_ORGANISM_HUMAN)) Antibody Denoise. This task focuses on recovering corrupted antibodies, represented by amino acid sequences. The corrupted sequence is generated by first sampling a value t from the range [1, 500], and then uniformly corrupting the amino acid tokens with a probability proportional to t. The antibody sequences used in this task are sourced from the OAS (Observatory of Antibody Space) dataset. Protein-Protein Interaction. As part of the pretraining process, two tasks were defined for learning protein-protein interactions: a classification task and a generation task, both utilizing data from the STRING database [27]. Interactions with a score higher than 500 are labeled as positive, while random pairs of proteins are treated as negative interactions. For the classification task, a balanced dataset comprising both positive and negative pairs is used. In the generation task, the model is trained on positive pairs only, where it learns to generate an interacting protein given an input protein."}]}