{"title": "SONICS: Synthetic Or Not - Identifying Counterfeit Songs", "authors": ["Md Awsafur Rahman", "Zaber Ibn Abdul Hakim", "Najibul Haque Sarker", "Bishmoy Paul", "Shaikh Anowarul Fattah"], "abstract": "The recent surge in Al-generated songs presents exciting possibilities and challenges. While these tools democratize music creation, they also necessitate the ability to distinguish between human-composed and AI-generated songs for safeguarding artistic integrity and content curation. Existing research and datasets in fake song detection only focus on singing voice deepfake detection (SVDD), where the vocals are Al-generated but the instrumental music is sourced from real songs. However, this approach is inadequate for contemporary end-to-end AI-generated songs where all components (vocals, lyrics, music, and style) could be Al-generated. Additionally, existing datasets lack lyrics-music diversity, long-duration songs, and open fake songs. To address these gaps, we introduce SONICS\u2020, a novel dataset for end-to-end Synthetic Song Detection (SSD), comprising over 97k songs with over 49k synthetic songs from popular platforms like Suno and Udio. Furthermore, we highlight the importance of modeling long-range temporal dependencies in songs for effective authenticity detection, an aspect overlooked in existing methods. To capture these patterns, we propose a novel model, SpecTT-Tra, that is up to 3 times faster and 6 times more memory efficient compared to popular CNN and Transformer-based models while maintaining competitive performance. Finally, we offer both AI-based and Human evaluation benchmarks, addressing another deficiency in current research.", "sections": [{"title": "1. Introduction", "content": "The rapid advancements in AI-generated music present a substantial threat to the music industry, potentially reducing the demand for professional musicians and stifling new talent development [5, 19]. To preserve the unique value of human creativity, it is crucial to develop robust methods for detecting AI-generated music, ensuring a fair and vibrant creative ecosystem.\nSinging Voice Synthesis (SVS) [16] and Singing Voice Conversion (SVC) [11] have recently achieved significant progress, enabling the creation of synthetic singing voices that closely mimic real singers' styles. When combined with instrumental music from real songs, these synthetic voices can produce convincing counterfeit songs. Although related to synthetic speech detection, detecting fake songs is particularly challenging due to the unique rhythmic patterns and artistic vocal traits of singing. To address this, researchers have turned their attention to Singing Voice Deepfake Detection (SVDD) [30, 33, 34]. However, current methods relying on datasets composed of SVS and SVC-generated songs face several limitations. These datasets are bound to use instrumental music from real songs, leading to artifacts like the \"Karaoke effect\" (volume discrepancies between music and vocals) and limited music-lyrics diversity. Moreover, existing methods often overlook the long-context temporal relationships inherent in songs, such as repeated verses, music, rhythm, and emotional dynamics, which are critical for effective detection. The short duration of songs in current datasets further hampers the use of these patterns. Additionally, copyright restrictions on some existing datasets limit the public availability of generated fake songs, hindering broader usage. Furthermore, the SVDD task requires separate tools for voice identification and separation during data processing [30, 34], increasing computational overhead.\nRecently, platforms like Suno [24] and Udio [6] have gained significant traction on social media. They can synthesize not only vocals but also entire songs, including synthetic music, styles, and lyrics, further complicating the situation. Due to their end-to-end nature, these fake songs"}, {"title": "2. Related Works", "content": "Synthetic Speech Detection: The domain of synthetic speech detection, closely tied to synthetic song detection through their shared audio modality, has been extensively explored due to advancements in voice conversion [37] and synthesis techniques [28]. These advancements have spurred the development of audio spoofing attacks on speaker verification systems and deepfake audio targeting human listeners [13]. Countermeasures include Light CNN (LCNN) with Max-Feature-Map activations [15], Transformer encoders with ResNet architectures [36], RawNet2 with sinc layers and GRU blocks [25], and heterogeneous graph attention networks [12]. However, the unique complexities of songs such as rhythm, melody, and emotional nuance-present challenges that traditional speech detection methods are not equipped to handle [30, 34]. Thus, following CtrSVDD [33], we opted not to conduct similar experiments in our study.\nSynthetic Song Detection: Synthetic song detection, a relatively newer and more complex challenge, has gained attention recently. In 2023, SingFake [34] introduced a dataset of counterfeit songs using Singing Voice Conversion (SVC), along with the task of Singing Voice Deepfake Detection (SVDD) and associated model benchmarks. Subsequent work [30, 33] combined Singing Voice Synthesis (SVS) with SVC to create phoneme-based songs, leading to specialized detection datasets. Methods in this area include convolutional networks for feature extraction followed by graph neural network classification [33], wav2vec2-based extraction coupled with graph neural networks [26], and Linear-Frequency Cepstral Coefficients (LFCC) used with ResNet18 models [35]. However, SVC and SVS-based datasets always retain original background music, leading to a detectable \"Karaoke effect\" artifact. Recent end-to-end fake songs by Suno [24] and Udio [6], which can produce divergent fake songs where all musical components (e.g., background music, styles, and lyrics) can be synthetic, presenting a severe detection challenge.\nLong Audio Classification: Songs exhibit long-range temporal patterns, such as repeated verses and rhythms, setting them apart from speech [1]. Despite their potential to enhance detection performance, these patterns have been largely overlooked in existing methods [30, 33, 34]. Meanwhile, long audio classification remains an uncharted area in audio research. Although automatic speech recognition handles long audio data [14], it struggles with end-to-end processing of extended audio due to its high computational cost, often using sliding window techniques [9,21] to manage costs. This further complicates the task of leveraging long-context features for synthetic song detection."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. SONICS Dataset", "content": "The development of a modern synthetic song detection system necessitates a dataset that meets several stringent criteria, which are conspicuously absent in existing music datasets. These criteria include: 1) songs where all components-not just vocals can be AI-generated; 2) song lengths sufficient to capture long-term contextual relationships; 3) a diverse spectrum of music-lyrics combinations; and 4) a quantity of data substantial enough to serve as a generative model benchmark. Addressing these needs, we introduce the SONICS dataset, a comprehensive collection of end-to-end AI-generated songs produced using the latest audio generative models, spanning lengths from 32 to 240 seconds. The dataset encompasses an extensive array of music-lyrics styles, amassing a total of 97,164 songs. A detailed comparison of SONICS with existing datasets is presented in Table 1, clearly illustrating that datasets such as FSD [30], SingFake [34], and CtrSVDD [33] fall short of fulfilling all the outlined criteria when juxtaposed with SONICS. Additionally, a comprehensive distribution summary of the SONICS dataset is provided in Table 2."}, {"title": "3.1.1 Real Songs", "content": "The dataset's Real songs segment comprises original compositions by human artists. This portion of the dataset was initially compiled by random sampling from the Genius Lyrics Dataset [10], which provides metadata including lyrics, titles, and artist information. Subsequently, leveraging this metadata, an API-based dynamic search was conducted on YouTube to retrieve the corresponding audio files [2]. This process yielded 48,090 original songs performed by 9,096 artists."}, {"title": "3.1.2 Fake Songs", "content": "The Fake songs subset consists of compositions generated by Al models guided by two primary text inputs: 1) lyrics and 2) song-style (e.g., instruments, genre, vocal type). Based on the nature of these inputs, the Fake songs are categorized into three distinct groups: i) Full Fake (FF)-featuring both AI-generated lyrics and song-style; ii) Mostly Fake (MF)\u2014where lyrics are AI-generated based on real lyrics features and song-style is derived from real songs; and iii) Half Fake (HF)\u2014where the lyrics are directly sourced from real songs, with song-style also extracted from real compositions.\nTo generate FF songs, we curated a rich set of metadata, including 57 broad topics (e.g., friendship, betrayal), 292 specific topics (e.g., star trek, pokemon), 49 music genres (e.g., rock, metal), and 72 moods (e.g., calm, angry). Random combinations of these elements were used to generate final lyrics and styles via a Large Language Model (LLM)."}, {"title": "3.2. Model", "content": "The architecture of an audio classification model is influenced by how the input audio is processed for feature extraction. In our benchmark, we primarily use Spectrograms more specifically Mel-spectrogram due to their versatile usage and effective performance across various audio processing tasks [8, 20-23]. Given the 2D image-like nature of Spectrogram, image classifier models are used for classifying the audio. Given that image classification models can broadly be divided into two categories: CNN and Transformer-based architectures, in our work, we employ popular models from both categories, such as ConvNeXt [18], ViT [7], for benchmarking. However, these models encounter challenges when dealing with the inherent long-range temporal dependencies in songs. Specifically, ConvNeXt struggles to capture long-range dependencies due to its local receptive field, a characteristic inherited from CNNs. On the other hand, while ViT is capable of capturing long-range dependencies, it incurs significant computational costs as the number of patches/tokens rapidly increases with longer audio inputs. To address this issue, we introduce SpecTTTra, which utilizes global attention similar to ViT but employs a Spectro-Temporal Tokenizer to reduce the number of tokens considerably, thereby lowering computational costs and enhancing efficiency significantly."}, {"title": "3.2.1 Spectro-Temporal Tokens Transformer", "content": "As illustrated in Fig. 3, the proposed SpecTTTra model begins by applying temporal and spectral slicing to copies of the input spectrogram, generating distinct temporal and spectral clips (or patches). These clips are then processed by the Spectro-Temporal Tokenizer, where temporal clips are embedded as temporal tokens \u3008ti) and spectral clips as spectral tokens (fi), using separate tokenizers. Each token is represented as a vector of shape (1, embed_dim). Subsequently, separate positional embeddings are added to the temporal and spectral tokens, as there is no positional relationship between these two types.\nThe positionally aware tokens are then fed into a ViT-like Transformer encoder, where they are contextualized with one another through global attention. This process enables the temporal tokens to become aware of both other temporal tokens and spectral tokens, and vice versa for spectral tokens. These globally aware features, now of shape (n_tokens, embed_dim), are average-pooled across the n_tokens dimension to aggregate the temporal and spectral information. Finally, the accumulated features are passed to a classifier, where they are classified as either real or fake songs.\nIt is important to note that while previous work has attempted to utilize both spectral and temporal information, these approaches come with significant limitations. For instance, [31] focuses solely on temporal tokens, neglecting the rich spectral information. On the other hand, methods like [23, 32] employ separate attentions for spectral and temporal information but with ViT-like tokens, which, as discussed in the following section, become highly inefficient and computationally expensive as they dramatically increase with longer audio inputs. In contrast, our approach disentangles spectral and temporal information at the tokenization level and then contextualizes them with attention. This results in a more efficient method compared to existing approaches."}, {"title": "3.2.2 Spectro-Temporal Tokenization", "content": "Traditional Vision Transformers (ViTs) generate grids of patches by simultaneously dividing 2D spectrogram across both temporal and spectral dimensions. If T and F denote the temporal and spectral dimensions of the spectrogram, respectively, and p is the patch size, then the number of patches (or tokens) generated by a ViT (Nv) can be expressed as:\n$N_v = (\\frac{F}{p})(\\frac{T}{p}) = \\frac{FT}{p^2}$ (1)\nThis equation highlights the rapid growth in the number of patches as the temporal dimension T increases, leading to a corresponding exponential rise in computational cost. For example, with a spectrogram of size F = 128 (mel bands) and short audio with T = 128 (5 sec.), ViT generates Nv = 64 tokens (for p = 16). However, for longer audio with T = 3744 (120 sec.), the number of tokens surges to Nv = 1872, nearly 30 times more than for the shorter audio. Since the computational cost for global attention scales quadratically with the number of tokens, this makes ViTs impractical for long audio classification tasks.\nIn contrast, the proposed SpecTTTra model adopts an alternative strategy by independently slicing across temporal and spectral dimensions to create separate temporal and spectral patches, as illustrated in Fig. 3. Unlike ViTs, where each patch only has access to limited spectral and temporal information due to simultaneous slicing across both dimensions, SpecTTTra ensures that each temporal patch has access to all spectral components and vice versa. This design leverages the observation that meaningful correlations can exist between distant temporal clips (e.g., between |to| and t4, capturing repeated song verses) or between distinct spectral clips (e.g., between |fo| and |f2|, differentiating real from fake songs). If t and f represent the sizes of the temporal and spectral clips, respectively, then the number of patches (or tokens) generated by SpecTTTra, denoted as Ny, is given by:\n$N_\\Upsilon = N_{spectral} + N_{temporal} = \\frac{F}{f} + \\frac{T}{t}$ (2)\nDue to the additive nature of this equation, the number of tokens in SpecTTTra grows much more slowly compared to ViTs, thereby keeping computational costs manageable. For instance, using the same parameters as before and setting t = 7 and f = 5 in SpecTTTra, the number of tokens for short audio with T = 128 is N = 43, and for long audio with T = 3744, it increases to N = 560, which is approximately 3.4 times fewer than in the ViT model. This substantial reduction in token count makes SpecTTTra significantly more computationally efficient. We further explore three variants of SpecTTTra, differentiated by the sizes of their temporal (t) and spectral (f) patches: SpecTTTra-\u03b1 (f = 1, t = 3), SpecTTTra-\u03b2 (f = 3, t = 5), and SpecTTTra-\u03b3 (f = 5, t = 7). Fig. 4 illustrates the rate at which the number of tokens increases for different SpecTTTra variants and ViTs as the audio length (number of time frames) grows.\nThe resulting spectral and temporal clips are processed by the Spectro-Temporal Tokenizer (STT) to create spectral and temporal tokens, respectively. The proposed STT block consists of separate Spectral and Temporal Tokenizers, which share identical architecture but differ in their objectives based on their input. These tokenizers use a Linear layer to map the spectral/temporal clips to spectral/temporal tokens, followed by GELU activation, addition of learnable positional embeddings similar to ViT, and finally, layer normalization. For efficiency, the slicing and tokenization operations are merged using 1D convolution. Given an input spectrogram x \u2208 RF\u00d7T, where T and F denote the temporal and spectral dimensions of the input spectrogram, the mathematical expression for tokenization is as follows:\n$x_t = Conv1D(x, c_i=F, c_o=D, k=t, s=t)$,\n$\\langle t \\rangle = LayerNorm(GELU(x_t) + \\hat t)$,\n$x_f = Conv1D(x^T, c_i=T, c_o=D, k=f, s=f)$,\n$\\langle f \\rangle = LayerNorm(GELU(x_f) + \\hat f)$ (3)\nHere, \u3008t\u3009 \u2208 RT\u00d7D and \u3008f\u3009 \u2208 RF\u00d7D denote the temporal and spectral tokens, where t and f are the temporal and spectral clip sizes, respectively, and D is the token embedding dimension. The parameters ci, co, k, and s represent the input channels, output channels, kernel size, and stride size of the CNN layers. Additionally, \u00ee\u00ee and \u2020 denote the learnable positional embeddings and Transpose operation. Further implementation details can be found in the Appendix."}, {"title": "4. Experiments and Results", "content": ""}, {"title": "4.1. Dataset", "content": "We conduct all experiments using the proposed SONICS dataset, which is divided into train, valid, and test sets. To ensure comprehensive evaluation, the valid and test sets include cases with unseen algorithms (e.g., Suno v2, Suno v3, Udio 32) and new singers. We also prevent data leakage by ensuring that song pairs from the same (lyrics, style) inputs are exclusively in either the training or valid-test sets, not in both. The distribution of the train and test sets is shown in Table 2, with the valid set distribution available in the Appendix."}, {"title": "4.2. Implementation Details", "content": "We conduct model training on an NVIDIA A6000 GPU with 48GB RAM, using WandB [3] for tracking. Generating the SONICS dataset costs $1,055, allocated across GPT-40 ($405), Gemini 1.5 Flash ($80), Suno ($390), and Udio ($180). We generate spectrograms with a 16,000 Hz sample rate, n_fft = win_length = 2048, hop_length = 512, and n_mels = 128, yielding a 128 \u00d7 128 spectrogram for 5 sec and 128 \u00d7 3744 for 120 sec audio. Any song shorter than input length is zero-padded randomly, while for longer songs, a random crop is used. However, during the test, to maintain determinism, padding is done on the right side and cropped segments are taken from the middle. We use small and tiny variants of ViT (patch size = 16) and ConvNeXt from the timm [29] library. In SpecTTTra, we use the same model configuration as ViT-small. It is noteworthy that we trained all models from scratch for fair comparison. We train for 50 epochs using Binary Cross-Entropy loss with 0.02 label smoothing and a batch size of 128. Optimization is performed with AdamW and a cosine learning rate scheduler from timm, including a 5-epoch warm-up and an initial learning rate of 0.0005. While existing methods [30,33,34] typically use Equal Error Rate (EER) as a metric, we prioritize the F1 score (binary average, threshold = 0.5) due to EER's susceptibility to class imbalance. We also evaluate Sensitivity (Sens.) and Specificity (Spec.) to assess performance across fake and real classes. Further details can be found in the Appendix."}, {"title": "4.3. Benchmarks", "content": ""}, {"title": "4.3.1 \u0391\u0399 Benchmark", "content": "The comparative analysis of the proposed SpecTTTra models against ViT and ConvNeXt models is presented in Table 3. The results reveal a significant performance gain (6% for ConvNeXt, 10% for ViT, and 14% for SpecTTTra-a) in the Overall F1 score when using long songs. This finding substantiates our claim that leveraging long-context information is crucial for enhancing fake song detection. Furthermore, the advantage of longer audio duration is more prevalent in transformer-based models (ViT or SpecTTTra variants) compared to CNN models (ConvNeXt). This can be attributed to the global attention mechanism in transformer models, which effectively captures long-range dependencies within the input data. However, despite the absence of global attention, ConvNeXt outperforms both ViT and SpecTTTra models in both short and long audio scenarios. We hypothesize that this is due to the inherent inductive biases present in CNNs, which are lacking in transformers, leading the latter to require larger datasets to achieve comparable performance [7, 17]. Notably, the proposed SpecTTTra-a, while trailing ConvNeXt by 10% in the F1 score for short audio, narrows this gap to 4% for long audio, highlighting its potential in long-context detection tasks. Another intriguing observation is the performance of ViT, which, despite its large number of tokens (or patches), is outperformed by all SpecTTTra variants in terms of overall F1 score for long audio, solidifying SpecTTTra's effectiveness. This could be due to an overload of redundant information from ViT's numerous patches, which may not contribute effectively to the detection task. Moreover, an interesting pattern can be observed across all models \u2013 real songs are more easily identified than fake ones, as indicated by higher specificity and lower sensitivity scores.\nDiving deeper into different partitions of test data, we observe that all detection models achieve better performance on seen algorithms (Suno v3.5 and Udio 130) compared to unseen ones (Suno v2, Suno v3, and Udio 32). Particularly, they struggle more with the Udio algorithms, with the most pronounced difficulty observed for Udio 32. However, ConvNeXt performs relatively well in detecting the Udio 32 algorithm, achieving a sensitivity of 95%, possibly due to the CNN's strong locality. Interestingly, despite being an unseen algorithm, the detectors perform comparably well on Suno v3 as they do on the seen Suno v3.5 algorithm, suggesting a possible algorithmic similarity between the two. On the other hand, for short audio samples, the models perform better on songs with seen speakers than those with unseen speakers, a gap that diminishes when longer audio is used. Finally, in fake type partitions, all detectors excel in detecting HF songs, which can be linked to the exclusive presence of Suno algorithms, where the detectors generally perform better compared to Udio algorithms. Among MF and FF songs, the models show slightly lower performance on FF songs, which might be due to the diverse unseen combination of topics, moods, and genres that differ more significantly than other fake songs."}, {"title": "4.3.2 Human-AI Benchmark", "content": "To evaluate Human performance in comparison to AI-based models, we selected a subset of 520 samples from our large test data. This evaluation employed a dynamic scoring system, similar to LMSYS [4], allowing public participation and live leaderboard updates, which will be made publicly available upon acceptance of this paper. Three human participants were involved in this benchmark, with their performance summarized in Table 4. In contrast to the AI benchmark using short (5 sec) or long (120 sec) audio samples, this human benchmark employed 25 sec clips. This choice stems from the observation that short clips hinder human identification due to subtle inaudible artifacts easily detected by AI, while longer clips don't necessarily improve human performance due to the difficulty in leveraging long-range temporal dependencies.\nAs shown in Table 4, AI-based methods consistently outperform human participants across all test partitions. However, both humans and AI models struggled most with Udio algorithms, particularly Udio-32, where human sensitivity dropped to 23.4%. Conversely, Suno algorithms, especially Suno-v3.5, were easier to detect, with a human sensitivity of 81.7%. This mirrors the findings in the AI benchmark, where models demonstrated higher specificity than sensitivity, indicating greater ease in identifying real songs compared to fake ones. Further analysis revealed distinct characteristics within real and fake song audio patterns. For instance, Suno algorithms often produced synthetic or mechanical-sounding vocals, while Udio-32 algorithm occasionally created the \"Karaoke effect.\" Furthermore, Udio algorithms demonstrated the ability to create songs with multiple voices and higher notes, a feature absent in Suno algorithms. On the other hand, real songs exhibited unique elements like a wide range of notes, diverse timbre, complex rhythms, and clear vocals with accented pronunciation. They also featured distinctive sounds like flutes and finger snaps, which are challenging for fake songs to replicate."}, {"title": "4.3.3 Efficiency Benchmark", "content": "To comprehensively evaluate the efficiency of the proposed SpecTTTra model alongside other methods, we measure various metrics across different song lengths using a P100 16GB GPU. The metrics considered include Speed (A/S \u2192 Audio per Second), Floating Point Operations (FLOPs), GPU Memory Consumption (Mem.) during the forward pass with a batch size of 14, activation count (# Act.), and parameter count (# Param.). The results are summarized in Table 5. Our analysis reveals that while ViT is the fastest model for 5-sec songs, it becomes the slowest for 120-sec songs and exhibits significant memory consumption, rendering it impractical for longer sequences. However, ViT remains the most efficient in terms of parameter count across both short and long songs. On the other hand, ConvNeXt, despite its strong detection performance, is the most resource-intensive model in terms of memory usage, FLOPs, and parameter count. In contrast, the proposed SpecTTTra model variants demonstrate remarkable efficiency, particularly in longer songs. Although they may not excel in short audio segments, the SpecTTTra-\u03b3 variant is nearly 3 times faster than ViT and 2 times faster than ConvNeXt for 120-sec songs. Moreover, it consumes memory 3 times less than ViT and 6 times less than ConvNeXt, highlighting its superior efficiency. Thus, while SpecTT-Tra secures the 2nd position in the detection benchmark, it offers the best trade-off between detection and efficiency performance, making it the most balanced model overall."}, {"title": "5. Limitations and Future Work", "content": "The real songs in the proposed SONICS dataset are queried from YouTube dynamically using their title and artist name, which introduces the possibility of not retrieving the correct audio. Through manual analysis of random samples, we estimate that this issue affects approximately 0.5% of the dataset. Also, due to the copyright of the real songs, we are unable to make them public. However, our dataset contains YouTube links for each song, which can be used to download the song easily using API [2]. Another limitation arises from the fake songs generated by the Udio platform, which cannot include lyrics from real songs. This restricts comprehensive evaluation in the Half Fake songs category, as it only contains fake songs from the Suno platform. Furthermore, the dataset currently includes only English-language songs, which may restrict the scope of our evaluation. In future work, we plan to extend our dataset to multiple languages to enable a more comprehensive analysis. Additionally, our current benchmarks are based solely on Mel Spectrogram input. Moving forward, we aim to incorporate raw audio and explore other feature extraction methods, such as LFCC and MFCC, to enhance the robustness of our evaluations."}, {"title": "6. Conclusion", "content": "In this paper, we introduced SONICS, a comprehensive dataset for end-to-end synthetic song detection, addressing limitations in existing datasets, such as lack of music diversity, short duration, and most importantly, the absence of end-to-end AI-generated songs. Moreover, we proposed the SpecTTTra model, which efficiently captures long-range temporal relationships in songs, achieving competitive performance to existing popular models while reducing computational costs significantly. Through extensive experiments, we established both AI-based and human benchmarks, demonstrating the dataset's effectiveness in advancing synthetic song detection research. Our work lays the foundation for improved methods for ensuring fair attribution and protecting the integrity of musical artistry."}]}