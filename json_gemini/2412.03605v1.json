{"title": "CBEVAL: A FRAMEWORK FOR EVALUATING AND INTERPRETING COGNITIVE BIASES IN LLMS", "authors": ["Ammar Shaikh", "Raj Abhijit Dandekar", "Sreedath Panat", "Rajat Dandekar"], "abstract": "Rapid advancements in large language models (LLMs) has significantly enhanced their reasoning capabilities. Despite improved performance on benchmarks, LLMs exhibit notable gaps in their cognitive processes. Additionally, as reflections of human-generated data, these models have the potential to inherit cognitive biases, raising concerns about their reasoning and decision making capabilities. In this paper we present a framework to interpret, understand and provide insights into a host of cognitive biases in LLMs. Conducting our research on frontier language models, we're able to elucidate reasoning limitations and biases, and provide reasoning behind these biases by constructing influence graphs that identify phrases and words most likely responsible for biases manifested in LLMs. We further investigate biases such as round number bias and a cognitive bias barrier revealed when noting framing effects in language models.", "sections": [{"title": "1 Introduction", "content": "Large language models have emerged as powerful instruments that can automate reasoning process in a host of domains ranging from Olympiad level math problem solving [1] to code generation [2] and financial planning [3]. LLMs have seen widespread adoption among people to automate and refine tasks involving decision making, logical thinking and critical reasoning.\nHowever, despite such promising results on numerous benchmarks, LLMs still possess surprising knowledge gaps caused due to a host of reasons varying from engineering heuristics such as tokenization [4] to limitations in the training data itself, such as data bias or a lack of up-to-date information. Furthermore, due to being trained on extensive data accumulated and refined by humans over the years, these LLMs present grounds for inherent cognitive bias as characterized in humans [5].\nWhile there have been promising demonstrations of GPT-based models on tasks involving creativity [6] and critical thinking [7], slight variations in input prompt requests can lead to vastly different output responses[8]. Furthermore, due to being next token predictors scaled to a huge text, the world models formed by language models possess knowledge gaps unapparent on the surface level. The presence of such knowledge gaps motivates us to consider various kinds of biases and how they may influence the model's behavior, potentially affecting its predictions, generalizations, and overall reliability across different tasks and domains.\nSome of the primary works on investigating the reasoning process in humans focuses on evaluating various cognitive biases and how they affect human behavior and decision making [9]. While sufficient study has been performed with regards to humans as respondents, LLMs still remain a relatively new subject for querying such biases. Several recent works have explored cognitive bias and its influence on a language model's ability to think. [10] Explain the presence of system 2 reasoning in GPT-4 grade models and how it's capabilities differ from earlier LLMs in specially designed cognitive reflective tests. At the same time [11] explores logical reasoning and understanding capabilities in GPT-4 and GPT-01 grade models and argue for a lack of true reasoning in LLMs, by presenting modifications to the GSM-8K benchmarks and observing performance drops supporting their claim. [12] Discuss the correlation between language and cognitive reasoning and assess the symbiosis between thinking and linguistic competence. [13] Probe for cognitive effects in GPT-3 through a unique question answering mechanism to uncover biases such as priming effect, distance and size congruity effect. [14] Treat GPT-3 as a subject in cognitive psychology, and note how slight prompt variations can lead to subpar model responses. [15] Detect how LLMs exhibit an underlying \"math anxiety\" that diminishes with improvement in model capabilities. Similarly, [16] explore cognitive biases that show up in LLM thinking process by navigating through word problems and arithmetic tasks while reflecting upon similarities to those uncovered in children. [17] focuses on representativeness heuristic bias in LLMs such as GPT-4 and Llama-2. The authors construct a dataset called REHEAT designed to test LLMs' susceptibility to biases such as base rate and conjunction fallacy. Significant work also dwells upon the influence of downstream alignment processes such as RLFH on a model's behavior. [18] emphasize on the lack of creativity in RLHF tuned LLM responses, and how they compare to base models, while [19] test cognitive biases in leading LLMs, whilst presenting evidence on higher presence of biases in RLHF aligned models as compared to base counterparts.\nWhile previous works explore cognitive biases in humans[9], the continuous refinement of LLMs necessitates corresponding research on how these biases manifest in AI. Investigating LLMs through this lens may help uncover human-like limitations and implicit harmful tendencies, which are critical for AI safety and model alignment. In this paper, we present a framework to interpret the presence of emergent cognitive bias resulting from the training process in frontier large language models. We believe our methods to defer from prior works ([20], [21] in the interpretability it offers. While significant prior work has explored framing effect, anchoring effect([22], [23], [24]) and availability heuristic [17], we build upon this by introducing round number bias, a bias that hasn't been explored in LLMs yet. In particular, we investigate language models for 5 cognitive biases:\n1.  Framing effect\n2.  Anchoring effect\n3.  Number bias\n4.  Representativeness heuristic\n5.  Priming effect\nWe touch upon how traits particular, and often attributed as a strength to model training, such as strong in-context learning, or having a higher number of logical reasoning tasks in training data, influence these cognitive biases and shape model behavior. Furthermore, we attempt to interpret the underlying reason for emergent cognitive biases by borrowing previously explored ideas in game theory through Shapley value analysis and noting influence graphs on model outputs [8]. While significant progress has been made in applying Shapley values and SHAP to interpreting machine learning models, we believe our work to be novel in using Shapley score analysis as means to decipher and corroborate the underlying biases in cognitive reasoning exhibited by frontier LLMs. Through our research on framing effect, we're able to uncover the presence of a cognitive bias barrier, a limitation on the robustness in reasoning processes in large language models such as GPT-40.\nOur work aims to provide light into the underlying reasoning mechanisms in LLMs, and can be reproduced conveniently in both open and close sourced models while being cost effective."}, {"title": "2 Methodology and Theoretical Framework", "content": "Drawing on ideas emergent in game theory, we rely on Shapley score attribution to measure the significance of the input prompt on model response.\nWe implement our Shapley value analysis in a manner similar to [8], considering input words as players in a cooperative game to determine the output token probabilities. We template the entire prompt as consisting of context setting constants along with a set of words that are contributing players across all possible sets of coalitions. This practice of keeping some words as constants across all coalitions is done with thoughtful consideration in order to reduce the cost incurred in our experimentation which scale exponentially as a factor of number of variable words/players in coalition.\nConsidering the set $X = \\{x_1, x_2, ..., x_n\\}$ representing the players in a cooperative game with a payoff for the entire coalition, Shapley value estimates each player's contribution to this payoff, providing a solution where each player receives a reward proportional to their influence in the coalition. For a player $x_i$ under the value function $v$, the Shapley value is denoted as $\\phi_i(v)$ and is calculated using the following formula: That is, the Shapley value of player $x_i$ is the\n$\\phi_i(v) = \\frac{1}{N!} \\sum_{S \\subset X\\{x_i\\}} S!(N - |S| - 1)! [v(S\\cup \\{x_i\\}) - v(S)]$\n(1)\naverage of player $x_i$'s contribution to each coalition S weighted by $|S|!(N \u2013 |S| \u2013 1)!$, the number of permutations in which the coalition can be formed."}, {"title": "3 Implementation and Findings", "content": ""}, {"title": "3.1 Framing Effect", "content": "Framing effect is a cognitive bias where people react differently to a particular choice or situation depending on how it is presented or \"framed.\" The same information can lead to different decisions based on whether it is framed in a positive or negative way.\nIn simpler terms, framing effect refers to the difference in perceptions caused by slight modifications in the presentation of the same underlying information. In our experimentation, we start with an initially positively framed prompt and modify it by adding a negative connotation while maintaining the same underlying idea. In both cases, we analyze the influence of the words in prompt through Shapley value computation and use it to gain an understanding on how framing effect affects model decision making.\nPositive frame: Given two stocks A and B, which stock do you invest in if stock B makes a profit 70% of the time?\nNegative frame: Given two stocks A and B, which stock do you invest in if stock B makes a loss 30% of the time?\nDespite being presented differently, an underlying notion of equivalent success rate is maintained in both prompts.\nFirst we present the favoured option for both sentences:"}, {"title": "3.2 Number bias", "content": "Examining the prompt used in section 3.1, the jaggedness in graph for varying values of loss percentages compelled us to question whether any patterns emerge within the irregularities. We consider one of the prompts that exhibits a pattern even in the unevenness observed in the figure - 4b The given graph plots the changes in LLM output probability scores for stock A as preferred option when considered for all percentages in range [0,100]. For varying values of percentages, percentage values which are multiple of 10 seem to be the local maxima in a range of (+10,-10), where the graph exhibits very prominent peaks. These spikes in probability scores indicate a particular bias in the graph going against a consistent expected decrease that should be demonstrated for increasing values of loss percentages. Such strong preference for values in multiples of 10 is evident of a round number bias in model behaviour.\nWe further study round number bias in LLMs by analyzing their performance on a dataset of student essays. For each essay, the model is prompted to assign a grade between 1 to 100. The below graph shows the output score distribution for 1000 essays."}, {"title": "3.3 Anchoring Effect", "content": "Defined as a cognitive bias that describes how people tend to rely too heavily on the first piece of information they hear when making decisions, anchoring effect was first introduced by Kahnemann and Tversky in their paper [5]. During decision making, anchoring occurs when individuals use an initial piece of information to make subsequent judgments. This initial piece of information known as the \"anchor\", is responsible for instilling a bias towards the respondent's answer.\nAnchoring Prompt:\nTo understand this presence in LLMs we analysed frontier LLM responses on two different prompts, providing a varied perspective on the same underlying concept of uncovering an anchoring bias.\nEstimate the number of jellybeans in a hidden jar. Your friend guesses 750. What's your estimate?\nA) 50 B) 200 C) 800 D) 1200\nDespite prompting the model of the fact that the friend \"guesses\" their answer, the model seems to be quite biased in picking option C - 800, the closest value to the 750. This presence of consistent bias prompts us to gauge the Shapley score to determine whether the bias can be attributed to the presence of an \"anchor token\".\nNoting the contribution of each token in output probability, the highest contributions for prediction lie in C)800, scoring quite high understandably due to being the preferred choice by LLM. Interestingly the token 750 possesses the second highest Shapley attribution, influencing the output probability in a significant manner, thus displaying an evident anchoring effect in the model behaviour. The similarity of this shapley score attribution to a transformer model's attention map is hard to miss. We hypothesise, in a manner, shapley score attribution to tokens is like the attention map to a model's cognition process, providing us with a rough schema of most important tokens/contributors in it's underlying decision making process.\nBuilding upon an established presence of anchoring effect in the LLM, we test our model on the infamous letter counting problems that LLMs are so unreliable on. We contrast the output of 2 different prompts, that together provide strong evidence of an LLMs anchoring bias.\nOriginal prompt: How many letters \"r\" are there in the word barrier?\nPersuasion prompt: How many letters \"r\" are there in the word barrier? Your friend insists it's 4.\nEvery frontier LLM fails on the original prompt and provides an incorrect count of 2. The correct count being 3 of course. We perform Shapley value analysis on both prompts with 3 as the prediction token this time."}, {"title": "3.4 Representativeness Heuristic", "content": "Representativeness heuristic can manifest itself due to knowledge gaps in people's thinking where they don't consider choices and alternatives that aren't as ubiquitous. In LLMs this knowledge gap can exist due to an incompleteness or inconsistency in internal world model acquired through various pre-training and post-training processes [26].\nRepresentativeness heuristic in LLMs can show up due to numerous reasons, such as a skewness in the data distribution the model is trained on, causing it to output in-distribution data more than logically and critically accurate responses. To test for this bias we use 2 different prompts, one being derivative of a popular example used to elucidate heuristic biases in humans.\nRepresentativeness Heuristic Prompt:\nMahesh is an extremely smart guy who's great at mental arithmetic. Is he more likely to be a cop or a Mathematics field medalist?\nDespite Mahesh having strong arithmetic skills which is more likely to be a trait in a fields medallist than a cop, the incomparably high ratio of cops compared to field's medallists sufficiently favors Mahesh to being a cop over being a field's medallist.\nMost frontier LLMs provide an incorrect response, being biased by a strong arithmetic acumen in Mahesh and not considering the prior instilled due to a much higher number of cops than fields medallists.\nMost frontier LLMs display a strong proclivity to falling prey to representativeness heuristic bias, with GPT-4o being one of the models successfully able to understand this as probing for base rate fallacy bias and gives the correct response. However, it's smaller version GPT-40-mini still provides an unsuccessful response marking the differences in understanding and reasoning capabilities between these models.\nWe use 2 additional prompts to provide a different perspective into how representativeness heuristic may show up in a model's logical reasoning process.\nMonty Hall problem:\nPrompt A: You are a part of a game show, where the host Monty shows you three doors, behind two is a car, and behind one nothing. You have to pick a door as your preference in order to win the presumed car behind it. Initially you pick door number 1. The host opens door 2 and shows it's empty. Now he offers you the choice to switch to door 3. Do you take that choice or stick with your current option?\nPrompt B: The famous gameshow host Monty Hall has decided to play a game with me. The game consists of 3 doors, behind two of those doors is a goat and behind one is an extremely poisonous snake that can kill humans. He tells me to pick a door, I pick door 1. Now Monty opens door number 3 and reveals a goat behind it. He offers me the choice to stick with my current option or switch to door number 2, what should I do?\nWith the obvious expectation on frontier models having the famous Monty hall problem in it's training data in one form or the other, we evaluate the model responses on slight variations of the problem. On both prompts, GPT-40 provides a clearly incorrect response and reasoning to the decision making involved in the game. Despite there not being any similarity in the logical reasoning involved in prompt A and the Monty hall problem, due to an existing mental heuristic to think along the lines of the Monty hall problem, the model presents an incorrect reasoning process. Similarly on prompt B, the most simple thinking of avoiding the door with snake behind it evades the model due to it's inherent priors to reason along the lines of the Monty Hall problem.\nThese 2 coupled with other prompts structured in a manner similar as the Monty Hall problem provide a strong evidence for a mental heuristic in frontier LLMs (such as GPT-40) to be biased to mimic data most frequented in it's training process, despite not being supportive of a sound reasoning process.\nDeciphering the presence of representativeness heuristic bias in LLMs questions the very notion of \"reasoning\" in language models and whether they are capable of actual cognition or are simply mimicking frequented patterns on a superficial level."}, {"title": "3.5 Priming effect", "content": "The priming effect refers to a psychological phenomenon where exposure to one stimulus influences the response to a subsequent stimulus, often without the subject's awareness of this influence.\nThis effect occurs when prior exposure to certain information, or stimuli unconsciously affects how the subject perceives or responds to later information. Essentially, the initial stimulus \"primes\" or prepares the mind to perceive related information in a specific manner.\nTo detect this effect we instill a bias in our prompt where an initial conditioning on a particular colour (prior stimuli) influences posterior decision making in LLM.\nPriming effect prompt:\nMy friend arrived to the fruit market wearing his favourite bright red coloured shirt. Which of these fruits is he most likely to buy? A) Banana B) Apple C) Pear D) Grapes\nPlotting the preference graph for each of the options it's clear to see the colour exposure biasing the model significantly to pick Apple as it's preferred response, a promising indication of potential priming effect in play."}, {"title": "4 Discussion & Conclusion", "content": "Our experiments have helped us interpret cognitive shortcomings instilled in LLMs, which arise from training heuristics such as tokenization 3.3 and model characteristic such as in-context learning 3.5, while also reflecting biases prevalent in LLMs due to the training data it reflects 3.1. Evaluations on representativeness heuristic 3.4 also reveal that slightest deviations from a standard archetype, not in the model's data, can cause the model to fall prey to the same cognitive bias traps as humans. Furthermore, we present evidence of round number bias in Language models such as GPT-40-mini,\nDespite the clear similarities in our experimentation involving anchoring 3.3 and priming effect 3.5 as both involve prior information influencing subsequent judgments, there are subtle differences to the manner in which this influence is incorporated. While anchoring relies on a strong dependence on an initial estimate or value to make downstream decisions, priming effect rely on associations developed in a subject's (system) world model and internal representations of concepts.\nAlthough, both are incorporated due to the attention mechanism,a key component responsible for trading information between tokens [27], in a GPT-based model priming effect relies more on the associated mappings developed in the model, a characteristic particular to the model's MLP layers [28], while the anchoring mechanism relies on the exact contents of the anchoring token, and hence is a much more direct influence."}, {"title": "5 Limitations", "content": "Much of our work relies of the output probability distribution produced by an LLM. While significant prior work demonstrates model responses to change due to token noise rather than actual reasoning [8], most of our prompts have extremely strong preferences for one choice over the other, as can be viewed in figure 1, so variances in sentences or switching the order of options doesn't affect the probability outputs by much.\nIn evaluations such as round number bias where we explicitly plot the probability distribution over varying values of percentages, while changing the prompt did alter the graph, the points of interest (i.e, mutiples of 5s) displayed a consistent pattern of being local maxima and were a consistent theme of interest for us to inspect.\nIn order to get the output in correct format we depend on system prompts to guide the model to the expected output format. Different system prompts produced slightly different probability distributions but much of the output score for token of interest was maintained across various system prompts we tried.\nLLMs such as Mistral Large, Claude and google Gemini don't provide APIs that let us access the probability score for tokens. For such models we could assess only for the presence of tested biases through subjective evaluation. However, for GPT-40, GPT-40-mini, and Llama-3 we were able to confirm our outputs and the underlying reasoning behind various cognitive biases through Shapley value computation graphs. Most of our experimentation was conducted prior to release of GPT-01 model and we haven't included it in any of our experiments or analysis."}]}