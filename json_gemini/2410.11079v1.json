{"title": "Code-Mixer Ya Nahi: Novel Approaches to Measuring Multilingual LLMs' Code-Mixing Capabilities", "authors": ["Ayushman Gupta", "Akhil Bhogal", "Kripabandhu Ghosh"], "abstract": "Multilingual Large Language Models (LLMs) have demonstrated exceptional performance in Machine Translation (MT) tasks. However, their MT abilities in the context of code-switching (the practice of mixing two or more languages in an utterance) remain under-explored. In this paper, we introduce Rule-Based Prompting, a novel prompting technique to generate code-mixed sentences. We measure and compare the code-mixed MT abilities of 3 popular multilingual LLMs: GPT-3.5-turbo, GPT-4, and Gemini Pro across five language pairs: English-{Hindi, Bengali, Gujarati, French, Spanish} using k-shot prompting (k\u2208 {0,1,10,20}) and Rule-Based Prompting. Our findings suggest that though k-shot prompting often leads to the best results, Rule-Based prompting shows promise in generating unique code-mixed sentences that vary in their style of code-mixing. We also use k-shot prompting to gauge the code-mixed to English translation abilities of multilingual LLMs. For this purpose, we create a gold-standard code-mixed dataset spanning five language pairs: English-Hindi, Bengali, Gujarati, French, Spanish}. As a real-world application of our work, we create a code-mixed chatbot.", "sections": [{"title": "Introduction", "content": "Multilingual Large Language Models (LLMs) have shown remarkable performance in Machine Translation (MT) tasks (Brown et al., 2020). However, in bilingual and multilingual communities worldwide, speakers often alternate between two or more languages in a single utterance. This phenomenon is known as code-switching or code-mixing (Poplack, 2001). Several works have explored code-mixed text generation (Gupta et al., 2020a; Gautam et al., 2021; Mondal et al., 2022;\n*Equal contribution.\nHsu et al., 2023), but code-mixed MT using Multilingual Large Language Models remains under-studied.\nYong et al. 2023 explore the abilities of multilingual LLMs in generating natural code-mixed conversations as well as code-mixed text on five general topics in five South-East Asian languages.\nZhang et al. 2023 present an analysis of the code-switching abilities of multilingual LLMs across 4 tasks, including MT. In their study of code-switched MT, they perform 0-shot prompting on several LLMs to translate English sentences to Hinglish, and vice versa. Our work builds upon this study by assessing the code-switching abilities of newly-released, bigger multilingual LLMs such as GPT-4 and Gemini Pro using k-shot prompting across 5 diverse language pairs.\nBesides k-shot prompting, we also explore rule-based generation of code-mixed sentences. Srivastava and Singh 2021 discuss the methods of Word-Aligned Code-Mixing (WAC) and Phrase-Aligned Code-Mixing (PAC) for rule-based generation of code-mixed sentences. We create four 'rule-based prompts' for code-mixed text generation using multilingual LLMs. Rule-Based Prompting marks a novel way to utilize LLMs for generating code-mixed sentences. As the name suggests, Rule-Based prompts enable generation of code-mixed sentences by manipulating the English reference sentence according to certain hard-coded Rules. By varying these Rules, we can obtain code-mixed sentences of different styles such as insertional and alternating (Poplack and Walker, 2003) without any in-context learning.\nWe report our results on our own dataset, which contains 600 gold-standard code-mixed sentences spanning five language pairs: English-{Hindi, Bengali, Gujarati, French, Spanish}.\nOverall, our contributions are as follows:\n(i) We study English to code-mixed and code-mixed to English translation abilities of multi-"}, {"title": "Experimental Setup", "content": "For each language pair, we use 100 randomly\nchosen English sentences from the datasets pro-\nvided in Gupta et al. 2020b for our k-shot ex-\nperiments. From the same dataset, we randomly\nchoose 20 sentences as examples for few-shot\nprompting. We create gold-standard code-mixed\nsentences corresponding to these 120 sentences\nacross five-language pairs, i.e. English-{Hindi,\nBengali, Gujarati, French, Spanish}, which we\nuse as reference sentences to evaluate the gener-\nations. In total, we have 600 sentence-pairs (120\nper language pair). Examples of sentences from\nour dataset are presented in Table 1. Details about\nthe annotation process can be found in Appendix\nB.2.\nWe use 0-shot prompting to investigate model\nperformance for the code-mixed to English trans-\nlation task on a real-world code-mixed Twitter\ndataset (Dhar et al., 2018) consisting of 100 ran-\ndomly selected English-Hindi code-mixed sen-\ntences."}, {"title": "Datasets", "content": "For each language pair, we use 100 randomly\nchosen English sentences from the datasets pro-\nvided in Gupta et al. 2020b for our k-shot ex-\nperiments. From the same dataset, we randomly\nchoose 20 sentences as examples for few-shot\nprompting. We create gold-standard code-mixed\nsentences corresponding to these 120 sentences\nacross five-language pairs, i.e. English-{Hindi,\nBengali, Gujarati, French, Spanish}, which we\nuse as reference sentences to evaluate the gener-\nations. In total, we have 600 sentence-pairs (120\nper language pair). Examples of sentences from\nour dataset are presented in Table 1. Details about\nthe annotation process can be found in Appendix\nB.2.\nWe use 0-shot prompting to investigate model\nperformance for the code-mixed to English trans-\nlation task on a real-world code-mixed Twitter\ndataset (Dhar et al., 2018) consisting of 100 ran-\ndomly selected English-Hindi code-mixed sen-\ntences."}, {"title": "Models", "content": "We run our k-shot prompting experiments on GPT-\n3.5-turbo, GPT-4, and Gemini Pro."}, {"title": "k-shot Prompting", "content": "We investigate the performance of LLMs on two\ntasks: English to code-mixed translation, and\ncode-mixed to English translation, using four k-\nshot prompts, viz. O-shot, 1-shot, 10-shot, and 20-\nshot.\nFor the English to code-mixed translation task,\nwe experiment with two variants of one-shot and\nfew-shot (10-shot and 20-shot) prompts: \u03b1 and"}, {"title": "Testing on Zhang et al. 2023 Dataset", "content": "We run k-shot experiments for English-Hindi to\nEnglish as well as for English to English-Hindi\non this dataset. Due to budget constraints and the\nlarge size of the dataset, we use only Gemini Pro."}, {"title": "Fine-tuning", "content": "We fine-tune Flan-T5-Base (Chung et al., 2022)\nfor the English to code-mixed translation task, on a\nTwitter code-mixed dataset (Srivastava and Singh,\n2020)."}, {"title": "Additional Explorations", "content": "We carry out prompting experiments with other\nopen-source models, namely FLAN-T5-XXL\n(Chung et al., 2022), Mistral-7B (Jiang et al.,\n2023), and BLOOMZ-7.1B (Muennighoff et al.,\n2022), in both translation directions. However,\ndue to consistently poor results across several at-\ntempts, we discontinue their use before evaluating\ntheir performance on our test data."}, {"title": "Results and Analysis", "content": "Before calculating metric scores, we clean noisy\noutputs containing tags or explanations, such as"}, {"title": "English to Code-Mixed", "content": "Table 3 shows the BLEU, ROUGE-L (F1), and\nMETEOR scores achieved by GPT-3.5-turbo,\nGPT-4, and Gemini Pro for the English to code-mixed translation task.\nFor the English-Hindi, English-Bengali and\nEnglish-Gujarati language pairs, GPT-4 outper-\nforms both GPT-3.5-turbo and Gemini Pro.\nFor English to English-Hindi, across all mod-\nels and experiments, the maximum BLEU score\nwe achieve is 36.65 for 20-shot\u03b2 with GPT-4. For\nEnglish-Bengali, we achieve a maximum BLEU\nscore of 37.97 for 0-Shot with GPT-4.\nFor English-Hindi, GPT-4 achieves the highest\nscores for all k-shot experiments (k=0, 1, 10 and\n20), followed by GPT-3.5-turbo, with Gemini-Pro\nbeing the worst performer."}, {"title": "Code-Mixed to English", "content": "Table 6 shows the BLEU, ROUGE-L (F1), and\nMETEOR scores achieved by GPT-3.5-turbo,\nGPT-4, and Gemini Pro for the code-mixed to En-\nglish translation task.\nGemini Pro performs the best for English-Hindi\nand English-French across all k-shot (k=0, 1, 10,\n20), followed by GPT-3.5-turbo, and then Gemini\nwithout any exceptions.\nFor English-Hindi, we achieve a maximum\nBLEU score of 60.35 for 1-shot with Gemini Pro.\nFor English-Bengali, GPT-3.5-turbo achieves the\nhighest BLEU score (54.93) for 20-shot prompt-\ning. For English-Gujarati, 20-Shot with GPT-4\nachieves the highest BLEU score of 42.62. In\nthe case of English-French, Gemini Pro achieves\na maximum score of 55.05 for 20-shot prompt-\ning. For English-Spanish, we achieve the high-\nest BLEU score (75.42) with GPT-4 using 10-shot\nprompting.\nWe observe that in the case of English-Spanish,\nthere is not much variation in scores across mod-\nels. However, for English-Bengali, GPT-4 demon-\nstrates the best performance on average across\nexperiments, despite GPT-3.5-turbo achieving the\nhighest BLEU score. For English-Gujarati, GPT-\n4 performs the best across all k-Shot experiments,\nfollowed by Gemini Pro. Going through the out-\nputs for English-Gujarati pair, we find that there\nare some wrong translations, which we can find\nin all the models. For English-Gujarati, GPT-3.5-\nturbo and GPT-4 suffer the most with using wrong\npronouns. Other than that, we also find halluci-\nnation in some outputs as well, which is mostly\nfound in GPT-3.5-turbo, followed by Gemini Pro.\nThus, we do not find the LLMs completely reliable\nfor English-Gujarati to English translation.\nTable 7 shows the results of code-mixed\nEnglish-Hindi to English translation, using 0-shot\nprompting, for 100 code-mixed sentences selected\nrandomly from the Twitter dataset."}, {"title": "Rule-Based Generation", "content": "We draw inspiration from previous work on syn-\nthetic rule-based generation of code-mixed sen-\ntences (Srivastava and Singh, 2021) to propose a\nprompt-based implementation of rule-based gener-\nation. We propose four 'rules' to generate code-\nmixed sentences, and integrate them into prompts,\nwhich we term as 'rule-based prompts'. To our\nknowledge, this marks the first attempt at lever-\naging 'rule-based' prompts for the generation of"}, {"title": "Rules 1 and 2", "content": "In Rule 1, the English sentence is first translated\nto the matrix language, which is PoS tagged. We\nchoose words associated with specific PoS tags to\nreplace with their English counterparts.\nHowever, we find that PoS tagging is not con-\nsistently accurate for non-English tokens. To over-\ncome this issue, we introduce Rule 2, which in-\ncorporates PoS tagging of the English sentence in-\nstead."}, {"title": "Rules 3 and 4", "content": "To generate code-mixed sentences that differ\nstylistically from the ones obtained using Rule 1\nand Rule 2, we propose Rules 3 and 4. In Rules 1\nand 2, specific PoS tagged words from the matrix\nlanguage translation of the English sentence are re-\nplaced with their English counterparts to obtain a\ncode-mixed sentence. To vary the style of code-\nmixing, we include English phrases in the code-\nmixed sentence instead of only singular words.\nIn Rule 3, noun phrases in the English sentence\nare identified. Subsequently, in the Hindi (trans-\nlated) sentence, the corresponding parts are re-\nplaced with these English noun phrases.\nRule 4 divides the sentence into multiple\nphrases. We choose the phrase with the second-"}, {"title": "Results and Analysis", "content": "Rule 1 and Rule 2 perform the best, and Rule 2\nfrequently outperforms Rule 1. Rule 3 and Rule\n4 consistently achieve lower BLEU scores than\nRules 1 and 2. Our analysis suggests that rule-\nbased prompting is a reliable approach to generate\nstylistically dissimilar code-mixed sentences from\na given English sentence.\nWe observe that the results are not always ideal.\nWith Rules 1 and 2, some generations are slightly\nunnatural and grammatically incorrect. For exam-\nple, for the English sentence \u201cAnd the answer is,\nit depends.\", one of the generations is: \u201cAnd ut-\ntar hai, yeh depends karta hai\u201d. The verb 'de-\npends' is not aligned with the Hindi part of the\ncompound English-Hindi verb (\u2018karta hai': \u2018'to do\n[something]'). This issue arises because we spec-\nify in the prompt that the words in the Hindi sen-\ntence must be replaced with their English transla-\ntions from the English sentence.\nOut of all the rules, Rule 3 leads to the gener-\nation of the most unnatural and grammatically in-\ncorrect sentences. Contrarily, Rule 4 often allows\nfor the generation of grammatically correct, stylis-\ntically different code-mixed sentences. One exam-\nple is \"Main from this entire thirty ek udhaar leta\nhoon\". However, some grammatically incorrect\nand unnatural sentences are also generated. One\nsuch example is \u201cThe questions chaar prakaar ke\nhote hain\u201d: in this sentence, the inclusion of the\narticle 'the' makes the sentence semantically re-\ndundant, as this word has no direct translation\nin Hindi. Another example is \u201cLekin aise avsar\nshould be few aur door hone chahiye\u201d. In this\nexample, \u201chone chahiye\u201d is superfluous as it con-\nveys the same meaning as \u201cshould be\". To explore\ndiverse model capabilities for rule-based genera-\ntion, we initially tested GPT-3.5-turbo and Gem-\""}, {"title": "Code-Mixed Chatbot", "content": "As an application of our work, we use Retrieval-\nAugmented Generation (RAG) to develop a Chat-\nbot capable of answering code-mixed questions\nabout our paper in any of the five code-mixed lan-\nguage pairs we have experimented with. Figure 2\nexplains the working of Chatbot. We use Gemini\nPro as the LLM.\nAppendix C.3 contains technical details about\nthe chatbot's working as well as examples of\nqueries and the chatbot's outputs."}, {"title": "Conclusion", "content": "In this paper, we first study the capabilities of GPT-\n3.5-Turbo, GPT-4 and Gemini-Pro in generating\ncode-mixed sentences across five language pairs\nusing k-shot prompting and fine-tuning. Using k-\nshot prompts, we also study how LLMs perform\non the converse task, i.e. code-mixed to English\ntranslation. We do not observe a stark general\ntrend in the results and infer that the results do\nnot vary with respect to the prompting method in\na consistent way. Therefore, the right combination\nof model and method is required to achieve the\nbest results, as each model shows superior perfor-\nmance for at least one language pair. We observe\nthat multilingual LLMs generally are capable of\ngenerating high quality code-mixed sentences, ex-\ncept when English is paired with a low-resource\nlanguage such as Gujarati. Additionally, having\ndefined and experimented with two kinds of in-\ncontext examples, viz. \u03b1 and \u03b2 for the English\nto code-mixed task, we find that k-shot \u03b2 prompts\nconsistently perform better than \u03b1 prompt. We in-\nfer that presenting in-context examples as pairs of\nthe English sentences and its code-mixed transla-\ntion leads to better results.\nFurther, we introduce four rules for generat-\ning code-mixed sentences, and integrate them into\nprompts. We find that although k-shot prompt-\ning generally outperforms rule-based prompting,\nthis approach shows promise in generating code-\nmixed sentences of unique styles. We also in-\ntroduce a new code-mixed dataset containing 600\ngold-standard code-mixed sentences spanning five\nlanguage pairs, on which we report our results. As\nan application of our work, we develop a code-\nmixed chatbot which answers questions about our\npaper in any of the five language-pairs we have ex-\nperimented with in our study."}, {"title": "Limitations", "content": "The gold-standard sentences used for k-shot\nprompting were created by human annotators.\nDue to practical considerations, only 120 gold-\nstandard code-mixed sentences were created for\neach language pair. Therefore, while considering\nthe variety of prompting techniques and experi-\nments we have tried, we test the models' perfor-\nmance on 100 sentences only for each language\npair."}, {"title": "Ethical Considerations", "content": "The human annotators are experts in their respec-\ntive languages who have been paid commensurate\nto their efforts. The paid models have been used\nupon subscription."}, {"title": "Supplementary Information", "content": "Here is an example of translation of\nEnglish sentence into code-mixed Bangla-\nEnglish sentence:\nEnglish: \"The trick is to start to build right\nfrom the back of your throat\"\nCode-Mixed: \"Trick ta holo to start to\nbuild ekdom tomar throat er pechon theke\"\nProvide the code-mixed Bangla-English\ntranslation for the given English sentence:\n\nThe output should be in Roman and\nshouldn't contain the original English sen-\ntence or any other tag.\nHere is an examples of code-mixed Bangla-\nEnglish sentence: \"Trick ta holo to start to\nbuild ekdom tomar throat er pechon theke\"\nProvide the Code-mixed Bengali-English\ntranslation for the given English sentence.\nThe output should be in roman and\nshouldn't contain the original english sen-\ntence or any other tag.\nEnglish Sentence:\nHere are some examples of code-mixed\nBangla-English sentences:\n\nConvert the following English sentence\ninto code-mixed Bangla-English as in\nabove examples:"}, {"title": "Dataset Annotation Process", "content": "The annotators are experts in both English and an\nadditional language. Being bilingual and multilin-\ngual individuals and having participated in code-\nswitching on a daily basis, they were already fa-\nmiliar with the phenomenon of code-mixing. Be-\nfore the commencement of the annotation process,\nthe phenomenon of code-mixing was explained to\nthem with detailed examples. We had three anno-\ntators for English-Hindi and English-Spanish, two\nannotators for English-French and English Ben-\ngali, and one annotator for English-Gujarati.\nWe gave the following instruction to the anno-\ntators: \"Please create a code-switched sentence\ncorresponding to the given English sentence\"\nThe annotators were asked to have these criteria\nmet:\n1. The code-switched sentences should be gram-\nmatically correct.\n2. The code-switched sentence must have the\nsame meaning as the corresponding English\nsentence i.e. it should be an accurate code-mixed\ntranslation of the English sentence."}, {"title": "Results", "content": "Figures 3, 4, 5, 6 and 7 graphically depict\nthe BLEU scores achieved for the English to\nEnglish-Hindi, English-Bengali, English-Gujarati,\nEnglish-French, and English-Spanish translation\ntasks respectively.\nFigure 8 compares performance across all lan-\nguage pairs.\nFigures 9, 10, 11, 12 and 13 graphically\ndepict the BLEU scores achieved for the\nEnglish-Hindi, English-Bengali, English-Gujarati,\nEnglish-French, and English-Spanish to English\ntranslation tasks respectively.\nFigure 14 compares performance across all lan-\nguage pairs."}, {"title": "Fine-Tuning Setup", "content": "Lower Casing was done\nRemoved the URLs\nMentions (starting with @) were removed\nHashtag symbol (#) was removed but the text\nwas kept\nCharacters that appear more than twice in a\nrow were reduced to a single character, in-\ncluding whitespace."}, {"title": "Fine-Tuning with Twitter Dataset", "content": "Total Sentences: 13738 -> Split 0.15 fraction\n(shuffled) with Valid Data.\nTrain=11677, Valid=2061\nPrefix=\"Generate Hinglish from English: \" was\nadded to input. Max Input Length=114\nMax Target Length=118\nOptimizer:AdamW\nfp16=False\nPer Device Train batch size=24\nPer Device Eval Batch Size=6\nLearning Rate=5e-5\nTrain Epochs=30\nGPU: NVIDIA Tesla P100 (16GB VRAM)\nModel with Lowest validation Loss was loaded\nat the end."}, {"title": "Inference", "content": "Temperature=0.001\nRepetition Penalty=2.0"}, {"title": "Rule Based Prompts' Outputs", "content": "1. Bengali Translation: \u098f\u099f\u09be \u0995\u09bf \u098f\u09ae\u09a8 \u098f\u0995\u099f\u09bf\n\u099c\u09bf\u099c\u09cd\u099e\u09be\u09b8\u09be\u09ac\u09be\u09a6\u09c0 \u09aa\u09cd\u09b0\u09b6\u09cd\u09a8?\n2. PoS Tagging: \u098f\u099f\u09be (PR), \u0995\u09bf (PSP), \u098f\u09ae\u09a8\n(JJ), \u098f\u0995\u099f\u09bf (QT), \u099c\u09bf\u099c\u09cd\u099e\u09be\u09b8\u09be\u09ac\u09be\u09a6\u09c0 (JJ), \u09aa\u09cd\u09b0\u09b6\u09cd\u09a8 (NN)?\n3. List of words: \u098f\u09ae\u09a8 (JJ), \u099c\u09bf\u099c\u09cd\u099e\u09be\u09b8\u09be\u09ac\u09be\u09a6\u09c0 (JJ),\n\u09aa\u09cd\u09b0\u09b6\u09cd\u09a8 (NN)\n4. Replaced Bengali sentence: \u098f\u099f\u09be \u0995\u09bf such\n\u098f\u0995\u099f\u09bf curious question?\n5. Transliteration: Eta ki such ekti curious\nquestion?"}, {"title": "Output Extraction Prompts", "content": "You have been given LLM output(in triple\nticks) in the format:\n\"1. \n2. \n3. \n4. \n5. : \nExtract the  in the final output\nwithout any tags or anything and make sure\nthat there are no extra inverted commas or\n'/' sign.\nJust give the final sentence as output It\nshouldn't be like 'The final sentence is' or\nanything.\nLLM Output :"}, {"title": "Code-Mixed Chatbot", "content": "The Retrieval component of RAG, which involves\nEmbeddings and the Vector Database, is challeng-\ning with code-mixed queries. Our initial exper-\niments with RAG indicate that translating code-\nmixed query to English is an effective strategy\ngiven the current resources (embedding models).\nAlso, as per given a query in English-Bengali, our\napproach is to first transliterate it in the Bengali\nscript before translating to English. This is in line\nwith out analysis () of Gemini-Pro for English-\nBengali to English as well as further experimen-\ntation with chatbot. We found that this improves\nthe performance of chatbot for this task.\nWe first load the document, and perform Hier-\narchical Chunking into parent nodes of size 2048\nand leaf nodes of size 512. The Vector Database\nis then created and stored. For retrieval, after the\nquery has been translated to English, the embed-\nding is generated for it, and the RAG system gen-\nerates top twelve relevant or similar chunks. It is to\nbe noted that only Leaf Nodes are considered for\nthis. For a parent node, in case majority of its child\nnodes are included in these twelve chunks, that\nparent node is considered instead of these individ-\nual child nodes for better context (Auto-Merging).\nTo improve the order of relevance, we use re-\nranking and choose the top six chunks. These\nchunks are passed to the LLM as context along"}, {"title": "Models and Tools", "content": "We use LlamaIndex4 library. We use Auto-\nMerging Retrieval Technique from LlamaIndex\nfor RAG. For the embeddings, BAAI / bge-small-\nen-v1.5(Xiao et al., 2023) model has been used.\nWe use the BAAI/bge-reranker-base (Xiao et al.,\n2023) model for re-ranking."}, {"title": "Examples of Chatbot in Operation", "content": "We used an incomplete version of this paper to test\nthe Chatbot. Figures 15, 16 and 17 show us some\nexamples."}, {"title": "k-shot Prompts", "content": "Here is an example of translation of\nEnglish sentence into code-mixed Bangla-\nEnglish sentence:\nEnglish: \"The trick is to start to build right\nfrom the back of your throat\"\nCode-Mixed: \"Trick ta holo to start to\nbuild ekdom tomar throat er pechon theke\"\nProvide the code-mixed Bangla-English\ntranslation for the given English sentence:\n\nThe output should be in Roman and\nshouldn't contain the original English sen-\ntence or any other tag.\nHere is an examples of code-mixed Bangla-\nEnglish sentence: \"Trick ta holo to start to\nbuild ekdom tomar throat er pechon theke\"\nProvide the Code-mixed Bengali-English\ntranslation for the given English sentence.\nThe output should be in roman and\nshouldn't contain the original english sen-\ntence or any other tag.\nEnglish Sentence:\nHere are some examples of code-mixed\nBangla-English sentences:\n\nConvert the following English sentence\ninto code-mixed Bangla-English as in\nabove examples:"}]}