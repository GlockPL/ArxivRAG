{"title": "Amazing Things Come From Having Many Good Models", "authors": ["Cynthia Rudin", "Chudi Zhong", "Lesia Semenova", "Margo Seltzer", "Ronald Parr", "Jiachang Liu", "Srikar Katta", "Jon Donnelly", "Harry Chen", "Zachery Boner"], "abstract": "The Rashomon Effect, coined by Leo Breiman, describes the phenomenon that there exist many equally good predictive models for the same dataset. This phenomenon happens for many real datasets and when it does, it sparks both magic and consternation, but mostly magic. In light of the Rashomon Effect, this perspective piece proposes reshaping the way we think about machine learning, particularly for tabular data problems in the nondeterministic (noisy) setting. We address how the Rashomon Effect impacts (1) the existence of simple-yet-accurate models, (2) flexibility to address user preferences, such as fairness and monotonicity, without losing performance, (3) uncertainty in predictions, fairness, and explanations, (4) reliable variable importance, (5) algorithm choice, specifically, providing advanced knowledge of which algorithms might be suitable for a given problem, and (6) public policy. We also discuss a theory of when the Rashomon Effect occurs and why. Our goal is to illustrate how the Rashomon Effect can have a massive impact on the use of machine learning for complex problems in society.", "sections": [{"title": "1. Introduction", "content": "Real-world datasets often admit many approximately-equally-good models. Leo Breiman called this phenomenon the Rashomon Effect, naming it after a Japanese movie in which four different views of the same murder show no single truth, just many reasonable explanations, for what happened (Breiman, 2001b; Kurosawa, 1950). One might think of the Rashomon Effect as a nuisance that prevents us from getting at a single \"true\" understanding of the data due to uncertainty, but from another perspective, the Rashomon Effect unlocks a treasure trove of information about the relationship of real datasets to families of predictive models. Harnessing this knowledge has massive practical implications, providing answers to some of the most fundamental questions in machine learning, such as: Is there an accuracy-interpretability trade-off? Which algorithm(s) are suitable for a given dataset? How can we easily (i.e., without solving a difficult optimization problem) find a model that incorporates multiple objectives such as fairness or monotonicity? How can we get stable variable importance estimates? The Rashomon Effect provides surprising insight into all of these questions - and more.\n The Rashomon Effect is often present in datasets generated by processes that are nondeterministic, i.e., noisy or uncertain, including data used for bail and parole decisions, healthcare, and financial loan decisions \u2013 high stakes applications. In fact, as we will discuss, it has been proven in specific cases that datasets drawn from noisy processes tend to exhibit a large Rashomon Effect in that there are many approximately-equally accurate models (Semenova et al., 2023). Furthermore, a large Rashomon Effect correlates with the existence of simple-yet-accurate models (Semenova et al., 2022). Hence, there is no accuracy/interpretability trade-off in many domains. The lack of a trade-off has been well-established empirically for tabular data (e.g., see Holte, 1993; Lou et al., 2013; Angelino et al., 2018; McTavish et al., 2022; Liu et al., 2022b; McElfresh et al., 2023). This knowledge has important policy implications, because it explains that black box models have no performance advantage over interpretable models that are easier to administer and use.\n Knowing that the Rashomon Effect exists \u2013 and the extent to which it exists \u2013 changes the lens through which we view just about everything in machine learning. The current machine learning paradigm solves problems by finding a single good model. However, understanding that many good models differ dramatically \u2013 in terms of variable importance, predictions on individual points, complexity, fairness, etc. - changes how we approach the problem. For instance, we cannot generally assume that any of the variables used heavily by one model are important to every well-performing model. We cannot assume that because an algorithm finds a complex model with good test performance that this level of complexity is necessarily needed for obtaining that test performance; similarly, we cannot assume that a complex model has discovered secrets in the dataset that a simpler model could not also find. Knowledge that many equally good models could exist might make us unhappy with the status quo of algorithms that optimize for only one machine learning model, when we could select from many. That is, just knowing about existence of the Rashomon Effect shows us that the standard machine learning paradigm that returns only one model is too narrow, and new methods and insights are needed.\n We define the Rashomon set as the set of models that perform approximately-equally to the best models from a given function class. The first algorithms that quantify the Rashomon Effect by capturing and storing the Rashomon set for nontrivial function classes have been developed only recently (Xin et al., 2022; Zhong et al., 2023; Liu et al., 2022a; Zhu et al., 2023). These algorithms allow users to interact with the Rashomon set to address user preferences, such as fairness concerns and monotonicity constraints. They also allow us to study variable importance in a holistic way, including all of the good models.\n We elaborate on how the Rashomon Effect has implications for simplicity, specifically the existence of simple-yet-accurate models (Section 3), flexibility to address user preferences without losing performance (Section 5), uncertainty in predictions, models, and explanations (Section 7), stable variable importance (Section 8), algorithm choice, specifically advance knowledge of which algorithms might be suitable for a given problem (Section 9), and public policy (Section 10). We question the relevance of the classical machine learning paradigm in light of the Rashomon Effect in Section 4 and discuss an alternative paradigm in Section 5. This perspective piece distills work from several technical papers to make them more widely accessible and discusses their link to policy."}, {"title": "2. The Rashomon Effect is Everywhere", "content": "The Rashomon Effect occurs when there are many different well-performing models for the same dataset. Standard machine learning (ML) analysis reveals it, but most researchers might not recognize it, because they are not looking for it.\n Let us work with a dataset - the FICO dataset from the Explainable ML Challenge (FICO et al., 2018) \u2013 though extremely similar results hold for an astounding number of other datasets (Semenova et al., 2022). We will examine the Rashomon Effect for a large model class, large enough to encompass the usual function classes used in machine learning, such as combinations of trees, neural networks, kernel machines, and so on. We applied a variety of machine learning methods to the data, including boosted decision trees, random forest, multi-layer perceptrons, support vector machines, logistic regression, and a 2-layer additive risk model. All of these models have completely different functional forms, from linear models to kernel-based nonparametric models with smooth decision boundaries, to tree-based nonparametric models with sharp decision boundaries, yet most of these models perform comparably."}, {"title": "3. The Rashomon Effect Gives Rise to Simpler-Yet-Accurate Models", "content": "When large portions of the function space contain many good models, the Rashomon set is likely large enough to include simpler models. A mathematical explanation for why this is true is illustrated in Figure 1 (Semenova et al., 2022). We have two function classes: a class of complex functions with a large Rashomon set (blue region) and a simpler function class contained in the complex function class (orange dots). Assume that every model in the complex class is \"close to\" a simpler one, meaning that they are within a radius $\\delta$ of each other in function space (i.e., the simpler class is a cover of the more complex set). Then, the Rashomon set in the complex class contains at least as many functions from the simpler class as its $2\\delta$-packing number, where the packing number is the maximal number of balls in the Rashomon set, the centers of which are at least $2\\delta$ apart. From the \u201ccloseness\u201d assumption, every ball in the packing contains a simpler model, therefore, the larger the Rashomon set, the more simpler models it contains.\n The \"closeness\u201d assumption is reasonable. For instance, sparse decision trees serve as a cover for deeper, more complex decision trees, and trees are universal function approximators (Barron, 1993). If the more complex trees are optimized so that they are not so deep that they overfit, shallow trees serve as an even better cover. For instance, Theorem 4.2 in McTavish et al. (2022) shows that any boosted decision tree is equivalent to a single tree with a greater depth, so the set of boosted trees is naturally covered by single trees. Because the closeness assumption generally holds, and because we often have large Rashomon sets, we often find that for tabular data, a single sparse tree (of depth ~5) can achieve performance similar to that of a boosted decision tree. Thus, as we stated, for many problems, simple models can perform as well as much more complex models, and there is no accuracy/interpretability trade-off."}, {"title": "4. The Standard ML Paradigm is Too Narrow", "content": "Among models with equal complexity on a static dataset, statistical learning theory says that it should not matter which model in the Rashomon set we choose - all models with equal complexity should generalize equally well to the test data (e.g., Rudin, 2020, chapter on learning theory). This standard paradigm, where any model that has good cross-validation performance on a static dataset can be trusted, assumes the test data come from the same distribution as the training set, the data need no troubleshooting, and no additional domain knowledge is needed. This is why machine learning methods need only choose one model from the Rashomon set in the standard paradigm.\n However, the real world is not an ML benchmark. In the real world, data are messy, the model inputs must be easy to check, domain knowledge can substitute for lack of data or problems with data, and models often need to be easy to understand and/or obey additional domain-specific constraints (Wagstaff, 2012; Rudin & Wagstaff, 2014). The standard ML paradigm makes it difficult to do just about anything except what it was designed for \u2013 to produce accurate models on a static dataset.\n Black box models have hidden flaws, whereas users trying to design interpretable machine learning models realize quickly that understandable models have understandable flaws. This is a key reason why the standard machine learning paradigm often fails in the real world. Selecting just a single, understandable model \u2013 ignoring the Rashomon Effect - comes with problems. In our experience, the user is rarely satisfied with the first model that is output by a standard interpretable machine learning algorithm \u2013 because they see flaws that can be fixed. Since standard ML algorithms are not interactive, the feedback loop can be fraught and frustrating. This is what we call the interaction bottleneck, where users cannot effectively interact with algorithms to improve machine learning models. Fortunately, there is a fix: finding Rashomon sets."}, {"title": "5. A New Paradigm: Finding Rashomon Sets", "content": "Because the Rashomon set is large, it often includes many simple-yet-good models. Being able to find all of the good models from a given simple function class has benefits that we spend the rest of this article discussing, the most important ones appearing in this section.\n The first algorithm that finds all good models for a nontrivial function class is the TreeFARMS algorithm, which finds all decision trees with a low regularized risk value (Xin et al., 2022). The second such algorithm is the GAM Rashomon set algorithm, which finds accurate sparse generalized additive models (Zhong et al., 2023). Third is the FasterRisk algorithm, which finds accurate sparse scoring systems (Liu et al., 2022a; Zhu et al., 2023).\n The opportunity to search through the Rashomon set means we can optimize multiple objectives simultaneously, which has implications for constraint handling and alignment with domain knowledge. Typical constraints that one might wish to include are monotonicity constraints, where the predicted outcomes increase with a specified set of variables, and algorithmic fairness constraints. A simple loop over the Rashomon set will suffice to find all possible answers to a constrained or multi-objective optimization problem. Accordingly, the new paradigm allows us to easily align the model with multiple fairness objectives. Among equally-good models, the user can choose one that optimally satisfies the criteria.\n Having the Rashomon set at one's fingertips resolves the interaction bottleneck. As discussed in the previous section, in the standard ML paradigm, if a user wants to add domain knowledge or constraints to the model, they need to formulate and solve new optimization problems each time they get new feedback from the user. This process is time-consuming, requires possibly many reformulations of optimization problems, and can be exceedingly frustrating. Access to the full Rashomon set resolves this. Using interactive tools, such as TimberTrek and GAMChanger (Wang et al., 2022b; 2021), to explore the Rashomon set, users can find a model that aligns with their domain knowledge in real time, even when that knowledge was not specified in advance. Even if the Rashomon set contains hundreds of millions of models, when they are organized effectively, humans can easily explore them.\n The computational cost of finding the Rashomon set is much higher than that of finding a single optimal model. Luckily, the TreeFARMS, GAM Rashomon set, and FasterRisk algorithms can handle the computation for most reasonably sized problems for sparse trees, generalized additive models, and scoring systems in minutes (see timing tables in Xin et al., 2022; Zhong et al., 2023; Liu et al., 2022a), which is often acceptable to users. If the Rashomon set is exceedingly large, these algorithms have mechanisms to sample from it or otherwise represent it.\n Rashomon sets containing all accurate models contain Rashomon sets for a variety of other objectives. Many of the objectives we consider in machine learning are related to each other. For instance, a highly accurate model probably also has high AUC, low loss, high F1-score, etc. We can take advantage of the relationship between these objectives. If we create a Rashomon set that includes all models with misclassification error below a threshold, we can often prove that it also includes all models with a different objective below a different threshold. For example, Xin et al. (2022) showed how all models with high F1-score could be calculated without ever optimizing for F1-score directly. These models are easy to find, because they are contained in a high-accuracy Rashomon set.\n Thus, Rashomon sets place substantially more control into the hands of human data analysts."}, {"title": "6. Why Does the Rashomon Effect Occur?", "content": "One theory as to why the Rashomon Effect occurs for so many real-world datasets comes from \"A Path to Simpler Models Starts with Noise,\" by Semenova et al. (2023). This work shows that uncertainty in the outcomes (noise) is one of the causes of the Rashomon Effect. This work provides a 4-step \"path\" (see Figure 6) that outlines how the uncertainty in the outcomes leads to simpler-yet-accurate models.\n In Step 1 of the path, uncertainty in the outcomes (label noise) increases the variance of the loss function with respect to random draws of the data. This means the loss function's values are more uncertain, thus, the user cannot tell with certainty what the value of the loss might be on the test set by looking only at the training set. This leads to Step 2, where the generalization error, i.e., the difference between train and test performance, increases with the variance. Thus, it is easier to overfit the training set.\n In Step 3, the user realizes that they are overfitting, perhaps through conducting cross-validation, and simplifies the function class. Even stopping the path here, we arrive at simpler functions. The user had to simplify the function class, because they were not able to generalize, and the test performance would be poor if they did not do it. In other words, even if the data were generated from a complex process, as long as there is label noise, the user would still need to simplify the function class to avoid overfitting to that noise and to reduce cross-validation error.\n In Step 4, when the function class is simpler, the Rashomon Ratio is large. The Rashomon Ratio is the fraction of the model class that has close-to-optimal loss. It is the fraction of the function class within the Rashomon set. Thinking of these simpler functions as (diverse) representatives of the original function class, a large Rashomon Ratio for the simpler function class equates to a large number of well-performing representatives from the larger function class; i.e., a large Rashomon Effect. Thus, when the data generation process has noise, models with a wide range of complexity can all be part of the same large Rashomon set, leading to a large Rashomon Effect.\n Tying this back to the existence of simpler-yet-accurate models from Section 3, the packing argument from Section 3 can be used to show that models from an even simpler class are likely to exist that are approximately as accurate as functions within the user's current function class.\n A separate situation in which there is generally a large Rashomon Effect is when the margins between classes (distance between classes in feature space) are large. For example, the MNIST dataset is not a noisy dataset, yet almost any machine learning method \u2013 with functions from any type of function class \u2013 performs well on it.\n Although most real tabular datasets seem to have a large Rashomon Effect, it is easy to construct cases where Rashomon sets are extremely small. An example is where the labels are generated deterministically (no noise) from a complicated function. Incidentally, this is why approximating or explaining an already-selected machine learning model will generally have an accuracy-interpretability trade-off, whereas working with real (noisy) labels will not. In other words, if we try to approximate a fixed, already-selected function $f$ with a simpler function $g$, then there will likely be a trade-off between the complexity of $g$ and how well it can fit $f$. This is discussed, for instance, by Kleinberg & Mullainathan (2019) (note that they reversed the terms \"interpretability\" and \"explainability\" from us)."}, {"title": "7. Uncertainty in Predictions, Fairness, and Explanations", "content": "Knowledge of the Rashomon set can illuminate uncertainty that causes problems with ML systems. This includes underspecification \u2013 where the model development process does not have enough information to learn generalizable domain knowledge \u2013 and predictive multiplicity, where there are many different predictions made by models within the Rashomon set. If we can calculate the degree of predictive multiplicity in the Rashomon set (how many different predictions are possible), we gain insight into underspecification (many different conclusions). Researchers have recently started to quantify these effects (Marx et al., 2020; Coker et al., 2021; Hsu & Calmon, 2022; Watson-Daniels et al., 2023). Again, analysts typically minimize the loss without considering the variation in other quantities, such as predictions, variable importance, or fairness, which is where problems arise."}, {"title": "8. Stable Variable Importance", "content": "Measuring the global significance of a variable in predicting an outcome holds paramount importance in scientific exploration and critical decision-making. Two important examples are genetics (e.g., Wang et al., 2020; Novakovsky et al., 2022), where the goal is to figure out which genes have unique information for predicting traits, and criminal justice, where mistakes in variable importance analysis have led to confusion and accusations of racial bias (see Larson et al., 2016; Rudin et al., 2020). Traditional approaches generally assess variable importance based on a single model trained on a specific dataset, but this framework does not account for the Rashomon Effect. As we know, failing to consider it can lead different researchers to draw divergent conclusions from identical data, based on identifying different variables as important. Along with the Rashomon Effect, another issue in variable importance is the lack of reproducibility: a variable importance estimate can change amid reasonable data perturbations (e.g., swapping out on observation). One solution is provided by the Rashomon Importance Distribution (RID), which quantifies the importance of a variable across the set of all good models and across perturbations to the original dataset using almost any variable importance metric of interest (Donnelly et al., 2023). By considering variations of the data through bootstrapping, and considering the Rashomon set for each bootstrap sample to produce a distribution of variable importance values for each variable, RID can obtain much more stable variable importance calculations. It is able to recover variables that are important to complex data generation processes more accurately than other approaches, demonstrating how leveraging knowledge of the Rashomon Effect can be helpful for scientific discovery."}, {"title": "9. Which Algorithm Should I Use?", "content": "A perennial question in machine learning is about the match of algorithms to problems: which machine learning algorithm is likely to work for my data? For image and text data there are clear current answers that take advantage of the structure in these data types (e.g., CNNs and transformers), but for tabular data there is not \u2013 most machine learning algorithms perform equally well. (In fact, researchers have had to compile special \"hard\" datasets because it is uncommon to find cases where different ML algorithms perform differently, see McElfresh et al., 2023). From what we have discussed above, the answer depends on the level of noise in the outcomes.\n For predicting criminal recidivism, where we predict months or years in advance whether someone will commit a crime, the randomness in this process means that simpler models will tend to perform as well as complex models. Thus, we would expect that boosted decision trees (Freund & Schapire, 1997), random forest (Breiman, 2001a), or neural networks provide no performance advantage over sparse additive models or the type of simple scoring systems that are often used for this purpose by the criminal justice system. Empirical evidence on recidivism prediction supports this (see e.g., Wang et al., 2022a; Zeng et al., 2017; Tollenaar & van der Heijden, 2013). Results of this flavor would be expected to hold for loan default predictions, and we presented empirical evidence based on the FICO dataset earlier. This same reasoning process holds for many healthcare prediction problems (readmission, mortality, e.g., see Zhu et al., 2023). In other words, simply by knowing the type of data and the level of noise in the outcome, we can determine whether methods that produce optimized simpler models are likely to be sufficiently accurate."}, {"title": "10. Policy Implications", "content": "Knowledge of the Rashomon Effect can be used to deliver significant positive impacts to society, including the development of fairer and more interpretable models.\n Currently, policy makers have started to govern the \"right to explanation\" for certain algorithmic decisions (Wikipedia, 2024). However, companies often do not want to provide models that could provide an advantage to competitors. This tension between a desire to preserve secrecy and mandated explanations leads to them providing narrow explanations that can be both misleading and incomplete, rather then genuinely transparent. Explanations are generally post hoc, which introduces several possible problems. First, they might be unfaithful to the underlying reasoning process, e.g., \"You were denied a loan due to factors A and B,\" when, in fact, the loan denial was due to different factors. Second, the explanations might be so incomplete as to be practically useless, e.g., \"Factors A and B are important in our decision,\" with no further explanation of how they were used and whether other factors might also be important. A person receiving an explanation has no way to determine the quality of that explanation. Problems with explanations have been discussed at length (e.g., Adebayo et al., 2018; Rudin, 2019; Yanagawa & Sato, 2024; Han et al., 2022). Essentially, black box models, even when supplemented with explanations, create barriers for individuals to examine and question the models, effectively allowing model designers to hide their flaws.\n Interpretable models do not have any of these issues. Their explanations must be faithful and complete by design. They are much easier to troubleshoot and use in practice. And, as we discussed in Sections 3 and 6, the Rashomon Effect theoretically explains why and when interpretable models perform as well as their black box counterparts. For these reasons, interpretable models should be used by default for many high-stakes decisions using machine learning. Thus, for applications such as criminal recidivism, we should default to interpretable models when we know the outcomes are noisy and where empirical evidence on similar problems has confirmed that interpretable models perform well (see, e.g., Wang et al., 2022a; Zeng et al., 2017). Exceptions can be made in cases where models are 100% accurate (e.g., lesion detection in medical images), in cases where no reason is needed (e.g., medical image segmentation), or in cases where there is no practical way to create an interpretable model. However, since we can now find the Rashomon set, as discussed in Section 5, making it easier to build interpretable models, there is often no excuse to continue the use of black boxes.\n Another policy implication involves other types of fairness besides simplicity. We can find the \u201cmost fair\" model within the Rashomon set, according to any fairness metric, including recourse (e.g., Black et al., 2022), and thus can verify claims about whether there exists a fairer-yet-accurate model for a given dataset.\n Even though interpretability, uncertainty, and fairness are essential to AI in practice and policy \u2013 with the Rashomon Effect being central to all of them \u2013 these topics are touched upon only superficially in most of today's academic courses. With respect to interpretability, most courses introduce no techniques more modern than CART (Breiman et al., 1984) and C4.5 (Quinlan, 1993). Information on post hoc explanations is much more widespread, sometimes (unfortunately) using the terminology \"interpretable\" to describe them. A review of interpretable machine learning appears in Rudin et al. (2022), and course material is available at Rudin (2020). Policy makers can fund ethical AI education, which will inevitably involve the Rashomon Effect since it determines whether trade-offs can exist between performance and ethical AI objectives."}, {"title": "11. Conclusion", "content": "The Rashomon Effect shows us that among models with similar loss, there are a multitude of models with different properties, including various levels of simplicity, fairness, and explanations/variable importance values.\n The ability to capture Rashomon sets and display them to users addresses what is arguably the hardest open problem in interpretable machine learning \u2013 incorporating human interaction. Solving the interaction bottleneck can have a major impact on our ability to troubleshoot and add constraints, which, in turn, could have a major impact on whether machine learning models can be used in high-stakes decisions.\n We do not believe that we have truly grasped the full extent of the Rashomon Effect yet, but we can already see that its impact on practical machine learning will be enormous. It forces us to change the way we think \u2013 even back to the fundamentals of ML. Since we formulate ML algorithms in terms of trade-offs between objectives, we tend to think that trade-offs among these objectives must then exist in the models they create. This is \u2013 surprisingly \u2013 wrong."}]}