{"title": "ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark Detection", "authors": ["Jui-Che Chiang", "Hou-Ning Hu", "Bo-Syuan Hou", "Chia-Yu Tseng", "Yu-Lun Liu", "Min-Hung Chen", "Yen-Yu Lin"], "abstract": "Although facial landmark detection (FLD) has gained significant progress, existing FLD methods still suffer from performance drops on partially non-visible faces, such as faces with occlusions or under extreme lighting conditions or poses. To address this issue, we introduce ORFormer, a novel transformer-based method that can detect non-visible regions and recover their missing features from visible parts. Specifically, ORFormer associates each image patch token with one additional learnable token called the messenger token. The messenger token aggregates features from all but its patch. This way, the consensus between a patch and other patches can be assessed by referring to the similarity between its regular and messenger embeddings, enabling non-visible region identification. Our method then recovers occluded patches with features aggregated by the messenger tokens. Leveraging the recovered features, ORFormer compiles high-quality heatmaps for the downstream FLD task. Extensive experiments show that our method generates heatmaps resilient to partial occlusions. By integrating the resultant heatmaps into existing FLD methods, our method performs favorably against the state of the arts on challenging datasets such as WFLW and COFW.", "sections": [{"title": "1. Introduction", "content": "Facial landmark detection (FLD) aims to localize specific key points on human faces, such as those on eyes, noses, and mouths. It is pivotal for numerous downstream applications, such as face recognition [13,29], facial expression recognition [1,25], head pose estimation [10, 33], and augmented reality [14,38]. Recent advances in deep neural networks have significantly enhanced facial landmark detection [7, 8, 11, 36, 42, 47]. However, existing FLD methods suffer from performance drops on partially non-visible faces caused by occlusions, extreme lighting conditions, or extreme head rotations, because the features extracted from non-visible regions are corrupted. An FLD method with non-visible region detection and reliable feature extraction is in demand.\nIn this work, we introduce an occlusion-robust transformer, called ORFormer, which can identify non-visible regions and recover their missing features, and is applied to generate high-fidelity heatmaps resilient to challenging scenarios. As illustrated in Figure 1, our ORFormer builds on Vision Transformer [5], where image patch tokens interact with each other via the self-attention mechanism. For non-visible part detection, we associate each patch token $X_i$ with an extra learnable token $M_i$ called messenger token.\nThe messenger token $M_i$ simulates occlusion present in patch $i$ and aggregates features from all patch tokens except $X_i$. Subsequently, our occlusion detection module accesses the disparity between the regular patch embedding $X_i$ and the messenger embedding $M_i$ to determine if occlusion is present in patch $i$. For occlusion handling, our feature recovery module recovers the missing features of the occluded patch by a convex combination of $X_i$ and $M_i$ with the combination coefficient predicted by the occlusion detection module. The resulting features are then utilized to generate heatmaps, and our proposed mechanism makes the output heatmaps remain robust in extreme scenarios.\nWe integrate the high-quality heatmaps generated by ORFormer as complementary information into existing landmark detection methods [11, 47]. Our method achieves state-of-the-art performance on multiple benchmark datasets, including WFLW [40] and COFW [3], showcasing the robustness of our method in handling partially non-visible faces.\nThe main contributions of this work are summarized as follows: First, we present a novel occlusion-robust transformer, ORFormer, which utilizes the proposed learnable messenger token to simulate potential occlusions and recover missing features. ORFormer enables a transformer"}, {"title": "2. Related Work", "content": "2.1. Facial Landmark Detection\nMost FLD methods rely on coordinate regression and/or heatmap regression. The former directly estimates the landmark coordinates of a face. The latter predicts a heatmap for each landmark and completes FLD with post-processing.\nCoordinate Regression. Some methods [7, 8, 23, 40, 44, 45] employ linear layers as decoders to regress landmarks from CNN features. Feng et al. [7] design a new loss function for improved landmark supervision. Wu et al. [40] utilize facial contours to impose constraints on landmark supervision while providing a dataset with various extreme cases. Miao et al. [23] proposed Fourier feature pooling to handle highly nonlinear relationships between images and facial shapes. These methods offer end-to-end trainable solutions.\nTo leverage the self-attention mechanism in Transformer [35] for facial structures exploration, some studies [18-20, 37, 39, 42] utilize the transformer decoder to learn the mapping between CNN features and landmarks. Xia et al. [42] propose a coarse-to-fine decoder focusing on sparse local patches. Li et al. [19] learn landmark queries along pyramid CNN features. However, linear layers in CNN and global feature dependence in transformers are sensitive to partial occlusions.\nHeatmap Regression. Inspired by the advances in heatmap generation [26,30,32], some studies [2,4,16,26,27,36] integrate heatmap regression into facial landmark detection. They convert landmark annotations to heatmaps for model supervision. Kumar et al. [16] estimate uncertainty and visibility likelihood with heatmaps for stable model convergence. Newell et al. [26] employ a stacked hourglass network with intermediate heatmap supervision and utilize Argmax operator to obtain landmarks. However, Argmax in heatmap regression limits direct supervision by landmarks due to its non-differentiable nature.\nRecent studies, such as replacing Argmax with other differential decoders, enable heatmap regression methods to be end-to-end trainable and supervised by both heatmaps and landmarks. For example, Jin et al. [12] reduce heatmap regression to confidence score and offset prediction to avoid heavy upsampling layers and the use of Argmax. With the aid of differential decoders, Huang et al. [11] and Zhou et al. [47] design new loss functions with both landmark and heatmap supervision to alleviate the negative impact caused by landmark annotation ambiguities. Micaelli et al. [24] utilize the deep equilibrium model to compute cascaded landmark refinement. The capability of heatmap regression methods that can be supervised by both landmarks and heatmaps while preserving facial structures has propelled them to the state-of-the-art status.\nHowever, the aforementioned coordinate regression and heatmap regression methods are vulnerable to faces with partial occlusions, under extreme lighting conditions, or in extreme head rotations due to feature occlusion and corruption."}, {"title": "2.2. Occlusion-Robust Facial Landmark Detection", "content": "We discuss three major categories of methodologies for occlusion-robust facial landmark detection as follows:\nMethods in the first category such as [16,20,26,41] estimate the probability of occlusion occurrence for each landmark and alleviate the negative impact of corrupted features computed in occluded areas. For example, Kumar et al. [16] and Li et al. [20] propose joint landmark location, uncer-"}, {"title": "2.3. Transformer for Feature Recovery", "content": "Transformers [5,35] have been widely adopted in vision tasks. Transformers leverage attention mechanisms to capture long-range dependencies between tokens, but are sensitive to feature corruption or partial occlusions.\nTo address this issue, Xu et al. [43] utilize cross-attention to recover occluded features between different frames in the context of object re-identification. However, their method relies on multiple frames, whereas our approach focuses on recovering occluded features within a single image. Park et al. [28] proposes a method for 3D hand mesh estimation that involves training a CNN block to separate primary and secondary features, followed by utilizing cross-attention to recover occluded features. In contrast, our method integrates occlusion detection and handling mechanisms into a single transformer, enabling adaptive detection and recovery of occluded features within a single frame.\nZhou et al. [46] pre-train a quantized autoencoder [6], employ a ViT model [5], and utilize self-attention to recover corrupted features for blind face restoration. While their approach shares similarities with ours, relying on self-attention with partially corrupted features may fail since attention values of the occluded tokens cannot be faithfully computed. To alleviate this issue, we develop messenger tokens and present a module to adaptively combine the regular and messenger embeddings for feature recovery."}, {"title": "3. Proposed Method", "content": "The section presents ORFormer, a general method that can be integrated into a regular transformer for occlusion detection and handling. Figure 2 illustrates our method. Firstly, we adopt the concept of vector quantization [34], similar to the approach in Codeformer [46], and pre-train a quantized heatmap generator (Section 3.1). Subsequently, the learned discrete codebook and decoder are employed as a prior for heatmap generation. Leveraging this learned prior, we utilize ORFormer for code sequence prediction and feature recovery for the partially occluded image patches (Section 3.2). Lastly, with the aid of ORFormer, we integrate our heatmaps generated from recovered features into the existing FLD methods (Section 3.3)."}, {"title": "3.1. Quantized Heatmap Generator", "content": "To enhance robustness against occlusions during heatmap generation, we include the training of a quantized heatmap generator. By training this generator on faces without occlusions, we can learn a high-dimensional latent space tailored explicitly for heatmap generation under ideal conditions. With the learned codebook, we reduce uncertainty in restoring occluded features, as the code items are learned from non-occluded faces.\nAs illustrated in Figure 2(a), an unoccluded face image $I \\in \\mathbb{R}^{h\\times w\\times 3}$ is encoded into the latent space $Z \\in \\mathbb{R}^{m\\times n\\times d}$ by an encoder $E$. Following the principles in VQVAE [34], each patch $Z_{ij}$ in the encoded features $Z$ is replaced with the nearest dictionary item, i.e., code, in the learnable codebook $C = \\{c_s \\in \\mathbb{R}^d\\}_{N}$ of $N$ codes to obtain the quantized feature $Z_Q \\in \\mathbb{R}^{m\\times n\\times d}$ and its corresponding code index sequence $S \\in \\{0, 1, ..., N - 1\\}^{h\\times w}$, i.e.,\n$Z_Q^* = \\arg\\min_{C\\sec}|| Zij - Cs||2$ and\n$Si,j^* = \\arg\\min_{S\\sec}|| Zij \u2013 Cs||2$.\nSubsequently, the decoder $D$ generates the edge heatmaps $H \\in \\mathbb{R}^{h\\times w\\times N_E}$ based on the quantized feature $Z_Q$, where $N_E$ is the number of edges (facial contours) per face. In this work, we adopt the same edge heatmap definition as that in [40].\nLoss Functions. To train the quantized heatmap generator with a learnable codebook, we utilize an image-level loss $L_{img}$. In addition, we incorporate an intermediate latent space loss $L_{latent}$ to minimize the distance between the codebook $C$ and the encoded feature $Z$. These loss functions are defined by\n$L_{img} = ||H - H||2$ and\n$L_{latent} = ||sg(Z) \u2013 ZQ||3 + \u03b2||Z \u2013 sg(ZQ)||3,\nwhere $H$ is the ground-truth edge heatmaps, $sg(\u00b7)$ stands for the stop-gradient operator, and \u03b2 is a hyper-parameter used for loss balance. The complete loss function for learning the codebook heatmap generator $L_{codebook}$ is given by\n$L_{codebook} = L_{img} + \\lambda_{latent} L_{latent}$,\nwhere $\\lambda_{latent}$ is a hyper-parameter used for loss balance."}, {"title": "3.2. ORFormer", "content": "Given an occluded or partially non-visible face as input, conventional nearest-neighbor searching described in (1) may fail on the occluded patches due to their feature corruption. However, relying solely on self-attention in transformers, e.g. [46], is insufficient in heatmap generation since the attention map calculated with corrupted features no longer faithfully captures the relationships between patches. To this end, we propose ORFormer to detect occluded patches and recover their features.\nAs shown in Figure 2(b), we introduce the proposed ORFormer after training the heatmap generator. ORFormer takes as input image patches $P = \\{P_k\\}^{m\\times n-1}$ from the features $Z'$, which are extracted by the encoder $E$. ORFormer employs both regular and messenger tokens for computing patch features. It generates the patch-specific occlusion map $\\alpha \\in \\mathbb{R}^{m\\times n}$ and two code sequences, $S_1 \\in \\{0,1, ..., N-1\\}^{m\\times n}$ and $S_M \\in \\{0,1, ..., N-1\\}^{m\\times n}$. While $S_1$ is computed from regular tokens and brings information from all patches, $S_M$ is derived from messenger tokens and is occlusion-aware. Based on the codes in $S_1$ and $S_M$, quantized features $Z_1$ and $Z_M$ are produced by referring to codebook $C$. $Z_1$ and $Z_M$ are merged by based on the occlusion map $\\alpha$ in a patch-specific manner, and form the recovered feature $Z_{rec}$. Finally, $Z_{rec}$ along with the pre-trained decoder is used to generate the heatmaps $H_{rec}$.\nWe freeze the codebook $C$ and decoder $D$ after the pre-training stage while fine-tuning the encoder $E$ to facilitate"}, {"title": "Self-attention", "content": "As shown in Figure 3, ORFormer is a transformer with $L$ layers. At each layer $l$, it computes self-attention among regular image patch tokens by\n$X^{l+1} = FFN\\{softmax(Q'_x(K'_x))^\\cdot V + X^l\\},$\nwhere queries $Q'_x$, keys $K'_x$, and values $V^\\cdot$ are obtained from patch tokens $X^l$ through linear embeddings. Residual learning [9] and a feed-forward network (FFN) are employed here."}, {"title": "Cross-attention", "content": "In addition to conventional self-attention between image patch tokens, we introduce the messenger tokens, denoted as $M^l$, one for each patch. The messenger tokens are designed to simulate feature occlusion. As shown in Figure 3, we only compute their queries $Q_M^l$, each of which is used to aggregate features from all but its corresponding patch token via cross-attention:\n$M^{l+1} = FFN\\{softmax(Across(Q_M^l, K'_x))^\\cdot V^l\\},\\newline where$\n$Across(Q_M^l, K'_x){i,j} =\n\\begin{cases}\n0, & \\text{if } i = j, \\\\\n(Q_M^l(K'_x)^-)_{i,j}, & \\text{otherwise}.\n\\end{cases}$\n$(6)$ computes the cross-attention score between the $i$-th messenger token and the $j$-th image patch token. By excluding features from the corresponding patch, the resultant messenger tokens $M^{l+1}$ encode features borrowed from other image tokens, simulating feature occlusion."}, {"title": "Occlusion Detection Head", "content": "Following the attention mechanism and the feed-forward network, we derive an occlusion detection head to detect occluded patches by referring to the dissimilarity between the image patch embedding $X^{l+1}$ and the messenger embedding $M^{l+1}$. A patch-specific occlusion map $\\alpha^{l+1} = \\{\\alpha_k^{l+1}\\}^{m\\times n-1}$ is obtained:\n$\\alpha_k^{l+1} = \\sigma(W^{l+1} \\cdot dist(X^{l+1}, M^{l+1})),$\nwhere the function $dist(\u00b7, \u00b7)$ computes the element-wise squared difference between the two embeddings. $W^{l+1}$ is a fully connected layer transforming the embedding returned by dist into a scalar. $\\sigma(\u00b7)$ is the sigmoid function ensuring $\\alpha_k^{l+1}$ ranges between 0 and 1. Higher $\\alpha_k^{l+1}$ indicates that patch $k$ is more likely to be occluded."}, {"title": "Occlusion-aware Cross-attention", "content": "After obtaining the occlusion map $\\alpha^l \\in \\mathbb{R}^{m\\times n}$ at the $(l \u2013 1)$-th layer, the messenger tokens at the $l$-th layer are allowed to suppress feature aggregation from occluded patches. Specifically, the cross-attention adopted by messenger tokens in (6) is modified to\n$Across(Q_M^l, K'_x){i,j} =\n\\begin{cases}\n0, & \\text{if } i = j, \\\\\n(1 - \\alpha^l)\\cdot (Q_M^l(K'_x)^-)_{i,j}, & \\text{otherwise}.\n\\end{cases}$\nSince $\\alpha^l$ gives the likelihood of occlusion occurrence in the $j$-th image patch, the coefficient $(1 \u2013 \\alpha^l)$ in (8) prevents a messenger token from aggregating features from patch $j$ with a larger value of $\\alpha$. At the first layer, the initial occlusion map $\\alpha^1$ is set to 0. At the last layer, i.e., the L-th layer, the resultant occlusion map $\\alpha^{L+1}$ will be used in the following step for feature recovery, and is denoted as $\\alpha$ for simplicity in Figure 2(b)."}, {"title": "Feature Recovery", "content": "In Figure 3, the image embedding $X^{L+1}$ and the messenger embedding $M^{L+1}$ produced in the last layer of ORFormer are fed into a codebook prediction head. This head predicts the code sequence $S_1 \\in \\{0,1,..., N-1\\}^{m\\times n}$ based on the image embedding $X^{L+1}$, where each entry in $S_1$ searches the code index for its corresponding patch via (1). The quantized features $Z_1 \\in \\mathbb{R}^{m\\times n\\times d}$ are produced by retrieving the corresponding $m \\times n$ code items from the codebook $C$. Similarly, the other code sequence $S_M$ and quantized features $Z_M$ are generated based on the messenger embedding $M^{L+1}$.\nThe quantized features $Z_1$ and $Z_M$ store complementary information. While $Z_1$ considers all patches but is sensitive to corrupted features, $Z_M$ focuses on non-occluded patches but ignores the original patch features $P$ as shown in Figure 3. We use the predicted occlusion map $\\alpha \\in \\mathbb{R}^{m\\times n}$ to recompose the final recovered features $Z_{rec} \\in \\mathbb{R}^{m\\times n\\times d}$ by merging $Z_1$ and $Z_M$ in a patch-specific manner, i.e.,\n$Z_{rec} = (1 - \\alpha) \\bigodot Z_1 + \\alpha \\bigodot Z_M,$\nwhere $A \\bigodot B$ denotes element-wise multiplication between $A$ and $B$ along the third dimension of $B$.\nLoss Functions. After the pre-training stage, we learn the ORFormer and fine-tune the encoder $E$ while keeping the"}, {"title": "4. Experiments", "content": "4.1. Experimental Settings\nImplementation Details. The quantized heatmap generator takes images of resolution 64\u00d764\u00d73 as input and out-"}, {"title": "4.2. Evaluation Metrics", "content": "Following previous works [11, 47], we employ three commonly used evaluation metrics to assess the accuracy of landmark detection: including normalized mean error (NME), failure rate (FR), and area under curve (AUC). For WFLW and 300W, inter-ocular NME is used, while for COFW, inter-pupil NME is used. For FR and AUC, the threshold is set to 10% for all datasets."}, {"title": "4.3. Comparisons with State-of-the-Art Methods", "content": "We conduct a comprehensive comparison of our method with state-of-the-art FLD approaches. As presented in Table 1, ORFormer achieves comparable or even better results"}]}