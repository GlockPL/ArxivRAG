{"title": "MatchMiner-Al: An Open-Source Solution for Cancer Clinical Trial Matching", "authors": ["Ethan Cerami", "Pavel Trukhanov", "Morgan A. Paul", "Michael J. Hassett", "Irbaz B. Riaz", "James Lindsay", "Emily Mallaber", "Harry Klein", "Gufran Gungor", "Matthew Galvin", "Stephen C. Van Nostrand", "Joyce Yu", "Tali Mazor", "Kenneth L. Kehl"], "abstract": "Clinical trials drive improvements in cancer treatments and outcomes. However, most adults with cancer do not participate in trials, and trials often fail to enroll enough patients to answer their scientific questions. Artificial intelligence could accelerate matching of patients to appropriate clinical trials. Here, we describe the development and evaluation of the MatchMiner-Al pipeline for clinical trial searching and ranking. MatchMiner-Al focuses on matching patients to potential trials based on core criteria describing clinical \u201cspaces,\u201d or disease contexts, targeted by a trial. It aims to accelerate the human work of identifying potential matches, not to fully automate trial screening. The pipeline includes modules for extraction of key information from a patient's longitudinal electronic health record; rapid ranking of candidate trial-patient matches based on embeddings in vector space; and classification of whether a candidate match represents a reasonable clinical consideration. Code and synthetic data are available at https://huggingface.co/ksg-dfci/MatchMiner-Al. Model weights based on synthetic data are available at https://huggingface.co/ksg-dfci/TrialSpace and https://huggingface.co/ksg-dfci/TrialChecker. A simple cancer clinical trial search engine to demonstrate pipeline components is available at\nhttps://huggingface.co/spaces/ksg-dfci/trial search alpha .", "sections": [{"title": "Introduction", "content": "Clinical trials are critical to developing new cancer treatments and improving patient outcomes. Historically, however, less than 10% of adults with cancer have participated in clinical trials.\u00b9 At the same time, many trials fail to reach their accrual goals.2,3 A need has therefore arisen for methods that can match patients to clinical trials at scale.\nSeveral such tools have been developed. For example, genomic testing labs4,5 provide clinical trial options based on genomic eligibility criteria. Patient-facing products that allow informed patients to enter their own histories and receive a list of trial options have been created. 6-8 Companies offer proprietary algorithms to cancer centers to facilitate trial feasibility assessment and patient matching. Machine learning or \u201cartificial intelligence\u201d approaches to match patients to clinical trials have also been created.10\u201312 Most recently, efforts have been undertaken to apply large language models (LLMs) to unstructured clinical data for clinical trial matching. This has included applying LLMs for the explicit extraction of structured variables from patient records,13\u201315 often focused on individual clinical notes, and explicit extraction of structured eligibility criteria from clinical trial text.13\u201316 Other groups have performed privacy-aware data augmentation for clinical trials,17 published end-to-end LLM-based trial matching systems,18,19 and investigated the computational and cost efficiencies of various LLM approaches to trial matching.20\nStill, most of these tools attempt to extract all eligibility criteria for a trial and evaluate if a patient matches each one. This comprehensive approach is intuitive, but given the complexity of eligibility criteria for modern cancer trials, it may risk limiting the feasibility and utility of automated trial-matching tools. Some eligibility criteria, such as blood count values and kidney or liver function test results, are common to almost any clinical trial. Matching on these criteria therefore does little to assist with ranking specific trials for a given patient who is eligible for trials in general. Focusing instead on matching based on core criteria for individual trials \u2013 such"}, {"title": null, "content": "as cancer type/histology, disease context (localized disease/curative intent vs advanced/metastatic), prior treatment history, and key biomarkers \u2013 may be useful. Each combination of such core criteria could be defined as a clinical \u201cspace,\u201d representing a target population for which a drug is being developed. Some trials encapsulate only one such space, such as \"Previously untreated metastatic lung adenocarcinoma with an activating EGFR mutation.\" Others, such as basket trials, 21 may be open to patients in several clinical spaces.\nIn this manuscript, we describe the development and validation of the open-source MatchMiner-Al pipeline. MatchMiner-Al ranks cancer clinical trial options for patients, and patients for clinical trials, based on unstructured EHR data and trial documentation. Key components of the pipeline include leveraging a large language model (LLM) to (1) define the core clinical criteria that define target patient populations, or \u201cspaces,\u201d for each trial; (2) summarize longitudinal clinical patient data based on EHR documents; (3) fine-tune a \"TrialSpace\u201d text embedding model based on historical enrollment data; and (4) train a \"TrialChecker\" classification model to predict whether a given candidate patient meets the core eligibility criteria for a specific clinical trial."}, {"title": "Methods", "content": null}, {"title": "Core Clinical Criteria and Clinical Trial Spaces", "content": "The Dana-Farber Havard Cancer Center clinical trial database was queried to identify all therapeutic clinical trials open to accrual at DFCI from January 2012 to June 2024. The NCT IDs for these trials were used to download publicly available eligibility criteria for each trial from the clinicaltrials.gov API. Llama-3.1-70B22 was prompted to extract lists of target \u201cspaces\u201d for each trial (Supplementary Table 1). The prompt defined a \u201cspace\u201d as an individual combination of cancer type/histology, disease burden/intent of treatment (curative or advanced/metastatic),"}, {"title": null, "content": "prior treatment history, and biomarker requirements that constituted a phenotype eligible for the trial. Some trials have only one target space, whereas others have several. The space list for each trial was parsed to extract the text describing each individual target clinical space."}, {"title": "Patient cohorts", "content": "EHR data were obtained using the DFCI Oncology Data Retrieval System23 for all adults who enrolled in cancer treatment clinical trials at DFCI from January 2016 to April 2024. These data included unstructured clinical notes, imaging reports, and pathology reports; and all structured systemic therapy treatment plans, including clinical trial enrollment details; stored in the shared Mass General Brigham-DFCI legacy EHR (pre-2015) or Epic EHR (since 2015). Data were split, at the patient level, into training (80%), validation/tuning (10%), and test (10%) subsets.\nAccess to data for this study was approved by the Dana/Farber Harvard Cancer Center Institutional Review Board (Protocol #21-608). Given the large volume of data required and minimal risk to patients of this retrospective medical records analysis study, a waiver of informed consent was obtained."}, {"title": "Pipeline development", "content": "Our pipeline consisted of a multistep model training and deployment process, as described below and illustrated in Figure 1."}, {"title": "1. Condensing the medical record", "content": "Many patients with cancer have long clinical histories, which can exceed the context length limitations of even long-context LLMs. Retrieval-augmented generation (RAG) is a common approach to pulling information from relevant documents or excerpts of text using vector embeddings to augment a prompt to an LLM, improving the quality of its responses while remaining within context length limits.24 In preliminary work, however, we found that applying the RAG concept using off-the-shelf embedding models to extract phenotypically relevant text from a medical record sometimes missed important information, partly due to the duplicative nature of much of the information in an EHR.\nTherefore, we developed a custom model for condensing a patient's medical record to the most relevant information. For each patient who enrolled on a therapeutic trial in our retrospective datasets, all unstructured oncologist notes, imaging reports, and pathology reports were pulled from the EHR. For a sample of 10,000 such patients from the training and validation sets (89% training, 11% validation), Llama 3.1-70B was prompted to tag each sentence in the medical record according to whether the sentence was relevant to a list of target concepts, including cancer type, histology, stage of diagnosis, current extent of disease, treatment history, and biomarkers. A multitask classification model based on TinyBERT25 was then trained on a per-sentence basis to predict whether any tag was assigned by Llama and which tags were assigned if so. The AUROC and best F1 scores for this model, \u201cTiny-BERT-Tagger,\u201d were evaluated on the validation set. The model was then applied to each sentence in the medical records for all patients who had enrolled on trials in our dataset, and the sentences exceeding the best F1 threshold for predicting a Llama tag were retained and concatenated chronologically to create a single condensed medical record for each patient."}, {"title": "2. Patient summarization", "content": "Using the condensed medical record, Llama-3.1-70B was then prompted to summarize each patient's history through the time the patient enrolled on a clinical trial. The patient summarization prompt to Llama (Supplementary Table 1) instructed the LLM to generate semi-structured output capturing the same core clinical concepts used to define trial spaces, including cancer type/histology, disease context (localized disease/curative intent vs advanced/metastatic), prior treatment history, and key biomarkers. See Figure 2 for an example of patient summaries."}, {"title": "3. Trial space extraction", "content": "For each therapeutic clinical trial with enrollments in our dataset, eligibility criteria text was pulled from the clinicaltrials.gov Application Programming Interface (API). For each trial, Llama-3.1-70B was prompted to extract a list of semi-structured clinical spaces that matched the trial, where each space was defined as above as a unique combination of core clinical concepts (Supplementary Table 1). See Figure 2 for an example of a trial space."}, {"title": "4. TrialSpace model development", "content": null}, {"title": "Initial TrialSpace training", "content": "Next, retrospective possible patient-space combinations were defined by linking each free-text patient summary from the time the patient enrolled on a given trial to each of the free-text clinical spaces extracted for that trial. Since we knew the trial on which each patient enrolled, but we did not have information a priori regarding the specific space for the trial on which the patient enrolled, we prompted Llama-3.1-70B to evaluate whether each trial space was a \"reasonable consideration\u201d based on the patient summary. The prompt (Supplementary Table)"}, {"title": "TrialSpace refinement", "content": "We empirically found that the initial TrialSpace training step yielded a model that could identify clinical trials based on cancer type but did not discriminate as well within cancer types according to specific treatment history or biomarker criteria. Therefore, we fine-tuned TrialSpace further to improve performance at discriminating among trial spaces or patients that might otherwise be highly ranked matches but were not \u201creasonable considerations\u201d per Llama-3.1-70B. We used the preliminary TrialSpace model to identify the ten trial spaces that best matched each patient summary and the twenty patient summaries that best matched each trial space based on the cosine similarity metric. Llama-3.1-70B was again prompted to determine whether each of these top ten spaces was a \u201creasonable consideration\u201d given the patient summary, and whether each of the top twenty patients was a reasonable consideration for a trial space, yielding an intermediate dataset containing binary labels for each patient-space combination. We then fine-tuned the embedding model further on the dual tasks of (1) discriminating between matches that passed a Llama 'reasonable consideration' check and random patient-space matches, again using the multiple negatives ranking loss; and (2) predicting the binary Llama \"reasonable"}, {"title": null, "content": "consideration\u201d label given a candidate patient summary and one of the top ten ranked spaces for that summary, or a candidate space and one of the top twenty ranked patients for that space, using the online contrastive loss. Candidate patient-trial space matches were restricted to spaces derived from trials that were open to accrual at the time the patient enrolled in a trial."}, {"title": "TrialSpace evaluation", "content": "Successive iterations of TrialSpace were evaluated in the retrospective validation set as follows. The top-10 precision of the patient-centric/\u201ctrial spaces for patients\u201d pipeline for predicting the Llama reasonable consideration label, and the top 20 precision of the trial-centric/\"patients for trial spaces\" pipeline for predicting that label, were calculated. The mean average precision (MAP) at 10 for the \"trial spaces for patients\u201d use case and at 20 for the \u201cpatients for trial spaces\" use case were also measured. Candidate patient-space matches were restricted to those spaces derived from trials that were open when the patient enrolled in any trial. To aid intuitive interpretation of TrialSpace embeddings, we also applied Uniform Manifold Approximation and Projection (UMAP)28 to perform dimensionality reduction and visualize a projection of the embedding space into two dimensions by cancer type. Once models were finalized, the precision metrics for the patient-centric workflow were evaluated in the held-out patient test split. The precision for the trial-centric workflow was performed using all patient splits, since data were split at the patient rather than the trial level, and trials were therefore not held out from training in the same way that patients were."}, {"title": "5. TrialChecker model development and validation", "content": "The TrialSpace model enables pre-calculation of embedding vectors for patient summaries and trial spaces, so subsequent queries of each can be performed almost instantly without requiring inference on each possible combination of patient and trial space.29 However, the quality of rankings provided by TrialSpace necessarily depends on the number of trial spaces and patients available for matching, since more options will increase the probability that each of the top ranked options for a given query is a reasonable consideration. Furthermore, the cosine similarity between patient and trial space vectors is not a clinically intuitive metric to present to oncologists and trial investigators.\nTherefore, we trained a \u201cTrialChecker\" classification model to generate a predicted probability that a specific patient-trial space combination would be deemed a reasonable consideration by the Llama-3.1-70B prompt. TrialChecker can be applied just to the top ranked matches to a given patient or trial space query, increasing the specificity of matches presented to the user. We fine-tuned this model from a Roberta-Large base30 using the Huggingface31 library. TrialChecker can be run in real time to \"double-check\" the top ten ranked spaces for a given patient or top twenty patients for a given space. The training data for TrialChecker consisted of the training split subsets of (a) our retrospective enrollment dataset from the first step of TrialSpace training, linking patient summaries to each space corresponding to a trial on which the patient actually enrolled; plus (b) the top 10 space/top 20 patient dataset used to fine-tune the final TrialSpace model based on the retrospective enrollment dataset; and (c) a top 10 space/top 20 patient dataset generated by applying the final TrialSpace model. For each patient-space combination, Llama-3.1-70B was queried to ascertain whether the space was a reasonable clinical consideration for the patient, yielding a binary outcome variable for TrialChecker training. The trained TrialChecker model was then evaluated by calculating the AUROC and calibration on the validation split subsets of datasets (a)-(c)."}, {"title": "6. Full pipeline evaluation in retrospective DFCI enrollment data", "content": "We anticipated that users of these tools prospectively would likely use it to identify (1) the most relevant trial spaces for individual patients (a patient-centric use case), or (2) the most relevant patients for individual trial spaces (a trial-centric use case). The full pipeline \u2013 consisting of condensing the medical record for a patient, summarizing it, embedding patient summaries and trial spaces with TrialSpace, ranking top spaces for patients or top patients for spaces, and restricting to candidate patient-space matches predicted to be \u201creasonable clinical considerations\u201d by TrialChecker \u2013 was then evaluated for both use cases. The overall precision and mean average precision of the pipeline were calculated, with and without the TrialChecker step. The performance of TrialChecker at predicting the Llama \u201creasonable consideration\u201d response was measured using the area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), and calibration curves."}, {"title": "7. Evaluation beyond clinical trial enrollees at our center", "content": "To better understand the performance of the pipeline beyond our retrospective trial enrollment dataset, we identified patients at DFCI who started standard of care systemic therapies from 2016-2024. These patients were divided into 80% training, 10% validation, and 10% test sets, maintaining consistency with the trial enrollment dataset, so any patients in both the trial enrollment and the standard of care dataset were assigned to the same split in each. Patient summaries at the treatment start time points were then generated by the pipeline, and summaries were restricted to those for test set patients. Next, we downloaded eligibility criteria for all trials in clinicaltrials.gov that were open for a diagnosis of \u201ccancer\u201d as of October 22, 2024. A random sample of 500 such trials was selected, to reflect a volume of open trials consistent with historical DFCI trends. TrialSpace was used to identify the top 10 spaces for each patient and top 20 patients for each space. The performance of TrialChecker in the"}, {"title": null, "content": "resulting dataset was evaluated using the AUROC and calibration curves, and TrialChecker was then applied to restrict each candidate match list to those matches predicted to pass the Llama \"reasonable consideration\u201d check. The performance of the pipeline was evaluated by calculating the overall precision and mean average precision of the trial-space candidate matches, treating the Llama reasonable consideration check as a gold standard label, with and without filtering for matches that passed the TrialChecker filter."}, {"title": "8. Trialspace visualization", "content": "To provide a visual representation of TrialSpace vectors, the embeddings of both patient and trial space summaries were projected into two dimensions using the uniform manifold approximation and projection (UMAP) algorithm28, which is an unsupervised dimensionality reduction technique that preserves the global structure of high-dimensional data. Every patient summary was processed to identify the cancer type (top-level OncoTree diagnosis code32) by prompting Llama-3.1-70B (prompt described in Supplementary Table 1). All patients from the retrospective trial enrollment test dataset and all patients in the standard of care treatment test set were analyzed; however, any patient summaries for which the cancer type prompt did not return an Oncotree-compliant value were discarded from the visualization. A k-nearest neighbors analysis was performed to filter any patient summary for which the mean distance in UMAP space of its 5 nearest neighbors was greater than 2 standard deviations for the cancer type (Figure 2). The similarity between the clinical trial patients and standard of care patient summary embedding distributions was calculated using the maximum mean discrepancy (MMD) technique."}, {"title": "9. Data and code availability", "content": "Our retrospective DFCI datasets included patient protected health information (PHI) and therefore cannot be shared. Therefore, to facilitate dissemination of our method, we developed a synthetic version of the standard of care patient summary dataset. Llama-3.1-70B was prompted to generate synthetic clinical notes, pathology reports, imaging reports, and condensed medical records based on hypothetical patients with cancer. The synthetic clinical documents were used to train another version of TinyBertTagger, and the synthetic condensed medical records were summarized and used to train versions of TrialSpace and TrialChecker based on the subset of clinical trial spaces extracted from clinicaltrials.gov that were not included in the external trial spaces evaluation subset. Code and synthetic data are available at https://huggingface.co/ksg-dfci/MatchMiner-Al. Model weights based on the synthetic data are available at https://huggingface.co/ksg-dfci/TrialSpace and https://huggingface.co/ksg-dfci/TrialChecker. A simple search engine based on user-entered patient summaries for cancer trials extracted from clinicaltrials.gov is available at\nhttps://huggingface.co/spaces/ksg-dfci/trial_search_alpha."}, {"title": "Results", "content": null}, {"title": "Cohort", "content": "Our retrospective trial enrollment dataset included 16,139 enrollments for 13,425 patients onto 1534 clinical trials with 4074 extracted trial \"spaces\u201d since 2016. Our retrospective dataset of patients starting standard of care treatment included 86,042 treatment plans for 50,799 patients. Patient characteristics are listed in Table 1.\nInformation for 19,610 cancer trials listed as currently accruing on clinicaltrials.gov on October 22, 2024 was downloaded. We prompted Llama-3.1-70B to extract lists of clinical"}, {"title": null, "content": "'spaces' for each trial, yielding 38,140 total spaces. Of these, we restricted 37,932 spaces for 19,554 trials that were not included in our retrospective DFCI dataset. To reflect a realistic volume of open clinical trials at an academic cancer center for further analysis, we randomly sampled 500 of these trials, yielding 875 total spaces and 864 unique spaces."}, {"title": "Patient-centric use case", "content": "Evaluations for the patient-centric use case were performed using a 10% held out test subset of each cohort to reflect a realistic clinical volume of patients receiving active treatment at our center at any given time."}, {"title": "Pipeline performance in the retrospective DFCI trial enrollment dataset", "content": "In the test subset of DFCI patient summaries from retrospective enrollments, identifying the top 10 ranked candidate trial spaces using TrialSpace alone yielded a precision @ 10 of 0.72 and MAP @ 10 of 0.87. After the top ten trial spaces for each summary were further restricted to those predicted by TrialChecker to pass the \u201creasonable consideration\u201d standard, the precision @ 10 was 0.89 and MAP @ 10 was 0.93. After incorporation of TrialChecker filtering, the median number of top ten ranked spaces still remaining was 8, with a mean of 7.5 (Table 2; Supplementary Figure 1)."}, {"title": "Pipeline performance for non-trial enrollees and non-DFCI trials", "content": "When the test subset of DFCI patient summaries corresponding to initiation of standard of care treatment (n=9494) was used to query the sample of 864 unique spaces from 500 trials open for cancer per clinicaltrials.gov as of October 2024, TrialSpace alone yielded a precision of 0.73, with MAP @ 10 of 0.83. With addition of the TrialChecker filter, precision @ 10 was 0.87,"}, {"title": null, "content": "and MAP @ 10 was 0.91; the median number of results remaining per query was 8, with a mean of 7.5. (Table 2; Supplementary Figure 1)."}, {"title": "Trial-centric use case", "content": null}, {"title": "Pipeline performance in the retrospective DFCI trial enrollment dataset", "content": "When TrialSpace was used to rank the top twenty patient summaries for each trial space (n=4074) in our retrospective DFCI enrollment dataset, the precision @ 20 was 0.65 and MAP @ 20 was 0.83. After incorporation of TrialChecker filtering, precision @ 20 was 0.91 and MAP @ 20 was 0.93; the median number of results remaining was 16, with mean of 13.2 (Table 3; Supplementary Figure 2). This trial-centric evaluation did not restrict queryable patient summaries to those in the test set, since our training/validation/test split was performed at the patient level rather than the trial level, and an interpretable precision metric requires a realistic quantity of patients for evaluation."}, {"title": "Pipeline performance for non-trial enrollees and non-DFCI trials", "content": "When TrialSpace was used alone to rank the top twenty DFCI test set patient summaries corresponding to SOC treatments for each external trial space (n=864 spaces for 500 trials) in our clinicaltrials.gov sample, precision @ 20 was 0.74, and MAP @ 20 was 0.82. Incorporating TrialChecker yielded precision @ 20 of 0.87 and MAP @ 20 of 0.90. TrialChecker yielded a median of 18 patients per query, with a mean of 15.8 (Table 3; Supplementary Figure 2). Since the number of available patient summaries in the SOC dataset was considerably larger than the number in the retrospective trial dataset, queryable patient summaries were restricted to the 10% test subset for this evaluation."}, {"title": "Visualization of TrialSpace embeddings", "content": "An unsupervised UMAP plot was generated to illustrate how a lung cancer trial participant's patient summary (Figure 2a) was embedded and projected into two dimensions (Figure 2b). In this space, patients on standard of care and clinical trials co-located, and the maximum mean discrepancy (MMD) analysis between standard-of-care and clinical trial patient summaries indicated highly similar distributions (MMD=0.01, p \u2264 0.0001). Moreover, patient summaries naturally clustered by cancer type within the UMAP embedding (Figure 2b), as reflected by a high average cosine similarity within cancer types (0.79) compared to between cancer types (0.24). Focusing only on lung cancer clinical trial summaries and patient summaries further demonstrated the alignment between trials and patients (Figure 2e), emphasizing the model's ability to map related clinical entities closely in embedding space. The Llama check response text (Figure 2d) assessing the eligibility of the patient with lung cancer (Figure 2a) for the clinical trial NCT04644237 (Figure 2c) highlighted how Llama 3.1 70B was able to resolve and link ERBB2 and HER2 in the biomarker eligibility determination. The TrialChecker assessment was positive with a score 0.98."}, {"title": "Synthetic data for sharing", "content": "Performance of the versions of TrialSpace and TrialChecker re-trained on synthetic data to facilitate sharing is detailed in Supplemental Tables 2 and 3. Precision metrics for the synthetic data-trained TrialSpace were lower than those for the PHI-trained TrialSpace, but this gap narrowed when the synthetic data-trained TrialChecker was added to the pipeline, at the expense of returning fewer results per query. For example, for the patient-centric use case in the PHI-containing DFCI retrospective trial enrollment dataset, precision @ 10 was 0.72 and MAP @ 10 was 0.87 for TrialSpace alone, while precision @ 10 was 0.89 with MAP @ 10 of 0.93 for"}, {"title": null, "content": "TrialSpace + TrialChecker (Table 2). When the models were re-trained with synthetic data, precision @ 10 was 0.60 with MAP @ 10 of 0.74 for TrialSpace alone, with precision @ 10 of 0.85 and MAP @ 10 of 0.89 for TrialSpace + TrialChecker (Supplemental Table 2). The PHI-trained TrialSpace + TrialChecker returned a median of 8 with mean of 7.5 results per query (Table 2), while the synthetic data-trained TrialSpace+TrialChecker returned a median of 6 with mean of 5.9 results per query (Supplemental Table 2)."}, {"title": "Discussion", "content": "This study describes the development and evaluation of the open-source MatchMiner-Al pipeline, which ranks cancer clinical trial options for patients and vice versa, based on core clinical criteria derived from unstructured EHR data and trial documentation.\nThis approach offers several strengths. The prompting strategy yields interpretable patient summaries focused on core clinical criteria based on a patient's entire longitudinal EHR text history, which is applicable not just to clinical trial matching but to clinical data abstraction generally. We focus on matching based on clinical \u201cspaces\u201d in the sense used in drug development, such as the \u201cfirst-line advanced EGFR mutant non-small cell lung cancer space.\" Since we do not attempt to fully automate trial screening and determine whether a given patient meets each one of the many common eligibility criteria for trials in general, we facilitate understanding of the landscape of trial options available for patients with specific disease contexts. This avoids saddling our pipeline with a focus on boilerplate eligibility criteria, such as requirements about comorbidities, or brain metastases.\nWe also demonstrated generalizability of the pipeline to trials that were not open at our center and patients at our center who did not enroll in trials. Given the open-source nature of the models and requirement for minimal structured data for training and inference, any health"}, {"title": null, "content": "system could apply it to train custom versions of \u201cTinyBertTagger\" for information retrieval, \"TrialSpace\u201d for clinical trial matching, and \u201cTrialChecker\u201d for additional specificity. While we developed the pipeline for oncology, it could be easily modified to rank specific trials for other medical conditions based on key \"space\u201d-defining concepts for such diseases. In essence, this approach distills small, efficient models for information extraction, trial ranking, and eligibility classification from an LLM so that inference can be run rapidly at scale without querying the LLM directly.\nOur pipeline also has limitations. Importantly, by design, it does not fully automate trial screening based on boilerplate eligibility criteria common to most trials. It does not replace the judgment of clinicians in weighing treatment options for patients or constitute an approved formal clinical decision support tool. An additional limitation is our reliance on LLM assessments of whether a given trial space was a \u201creasonable consideration\u201d for a given patient to define labels for model training and evaluation. This was necessary in part because we had structured retrospective data describing the trials on which patients enrolled, but not on the clinical 'spaces' within those trials for which patients might have been eligible, or on trials for which patients might have been eligible but happened not to be those on which they enrolled. Manual review of all LLM \"reasonable consideration\u201d assessments generated for training and validation, which numbered in the hundreds of thousands, would have been prohibitive.\nThe \"reasonable consideration\u201d standard is inherently subjective, and different LLMs as well as different oncologists \u2013 would likely provide different answers about whether a patient-trial pair meets this criterion. Our precision metric is also sensitive to the number of clinical trials and number of patients eligible for inclusion in a query. We focused on denominators of patients and open trials consistent with historical patterns at our center. However, if the pipeline were applied to query all open trials on clinicaltrials.gov or all patients in a multi-institutional health system, the precision would be higher, since the probability that top"}, {"title": null, "content": "matches are clinically reasonable increases in proportion to the number of possible matches.\nFinally, although we evaluated the pipeline on trials that were not open at our center and hence were never seen in training, and for patients at our center who did not enroll in clinical trials, generalizability to patient histories derived from other institutions requires further research.\nOur pipeline for deployment was trained on real retrospective EHR data containing PHI at our site, yielding models that cannot be shared publicly. To facilitate dissemination of this approach, we therefore developed versions for publication based on synthetic data. These yielded performance that was useful, though not as good as the PHI-trained models. Further work is needed to improve the quality of synthetic data available for clinical Al deployment.\nIn conclusion, we applied a large language model to distill an open pipeline for clinical phenotyping and cancer clinical trial matching. Future work will focus on improving pipeline performance and field-testing it with clinicians, registered nurses, and clinical trial investigators."}]}