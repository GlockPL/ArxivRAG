{"title": "Confidence Calibration of Classifiers with Many Classes", "authors": ["Adrien Le Coz", "St\u00e9phane Herbin", "Faouzi Adjed"], "abstract": "For classification models based on neural networks, the maximum predicted class probability is often used as a confidence score. This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step. However, many confidence calibration methods fail for problems with many classes. To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods. Our code can be accessed at the following link: https://github.com/allglc/tva-calibration.", "sections": [{"title": "1 Introduction", "content": "The considerable performance increase of modern deep neural networks (DNNs) and their potential deployment in real-world applications has made reliably estimating the probability of wrong decisions a key concern. When such components are expected to be embedded in safety-critical systems (e.g., medical or transportation), estimating this probability is crucial to mitigate catastrophic behavior. One way to address this question is to treat it as an uncertainty quantification problem [2, 12], where the uncertainty value computed for each prediction is considered as a confidence. This confidence can be used to reject uncertain decisions proposed by the DNN [13], for out-of-distribution detection [22], or to control active learning [34] or reinforcement learning based systems [76]. When confidence values reliably reflect the true probability of correct decisions, i.e., their accuracy, a predictive system is said to be calibrated. In this case, confidence values can be used as a reliable control for decision-making.\nWe are interested in producing an uncertainty indicator for decision problems where the input is high dimensional and the decision space large, typically classifiers with tens to thousands of classes. For this kind of problem, DNNs are common predictors, and their outputs can be used to provide an uncertainty value at no cost, i.e., without necessitating heavy estimation such as Bayesian sampling [15] or ensemble methods [33]. Indeed, most neural architectures for classification instantiate their decision as a softmax layer, where the maximum value can be interpreted as the maximum of the posterior probability and, therefore, as a confidence. Unfortunately, uncertainty values computed in this way are often miscalibrated. DNNs have been shown to be over-confident [17], meaning their confidence is higher than their accuracy: predictions with 90% confidence might be correct only 80% of the time. A later study [44] suggests that model architecture impacts calibration more than model size, pre-training, and accuracy. For ImageNet classifiers, the accuracy and the number of model parameters are not correlated to calibration, but model families are [11].\nThese studies show that it is difficult to anticipate the calibration level of confidence values computed directly from DNNs and exhibit the benefits of a complementary post-processing calibration. This calibration process can be seen as a learning step that exploits data from a calibration set, distinct from the training set, and is used to learn a function that maps classifier outputs into better-calibrated values. This process is typically lightweight and decoupled from the issue of improving model"}, {"title": "2 Related work", "content": "Calibration There are various notions of multiclass calibration. One can consider confidence [17], class-wise [28], top-r [19], top-label [18], decision [75], projection smooth [16], or strong [60, 65] calibration. For recent surveys, we refer to [10] and [63]. In this work, we focus on confidence calibration and not on the calibration of the full probability vector. Indeed, confidence calibration is useful for many applications that only require a single confidence value: selective classification [13], out-of-distribution detection [22], or active learning [34]. For these applications, stronger notions of calibration are both difficult and useless. Also, class-wise calibration metrics do not appropriately scale to large numbers of classes, a setting we consider in this work, as explained in Appendix E.\nMetrics Several metrics have been proposed to quantify calibration error. The most common is the Expected Calibration Error (ECE) [46] (see Equation 2). ECE has flaws: the estimation quality is influenced by the binning scheme, and it is not a proper scoring rule [14, 60, 48]. Despite its flaws, it remains the standard comparison metric for confidence calibration. Variants of ECE have also been developed: classwise-ECE [29], ECE with equal mass bins [48, 44], or top-label-ECE, which adds a conditioning on the predicted class [18]. The Brier score [4] is also used to measure calibration. The proximity-informed expected calibration error (PIECE) evaluates the miscalibration due to proximity bias [68]. We mainly use the standard ECE in this work, and the Appendix contains more metrics."}, {"title": "3 Problem setting", "content": "3.1 Background\nConfidence calibration of a classifier We consider the classification problem where an input x is associated with a class label y \u2208 Y = {1, 2, ..., L}. The neural network classifier f provides a class prediction from a final softmax layer o that transforms intermediate logits z into probabilities. The classifier prediction is the most probable class \u0177 = arg maxkey fk(x) with fk (x) referring to the probability of class k, and the confidence score defined as s = maxkey fk(x). Note that we use the term confidence to denote the maximum class probability. With y the real label, we consider the confidence calibration definition from [17] that says that the classifier f is calibrated if:\nP(\u0177 = y\\s = p) = p, \u2200p \u2208 [0, 1] \\qquad(1)\nwhere the probability is over the data distribution. Equation (1) expresses that the probability of being correct when the confidence is around p is indeed p. For instance, if we consider the set of predictions"}, {"title": "3.2 Issues related to current approaches", "content": "Behavior of current scaling methods Scaling methods for calibration optimize one or more coefficients that scale the logits vector to minimize on calibration data the cross-entropy loss defined as l_{CE} = \\sum_{k=1}^L \\mathbb{1}_{k=y} \\cdot log(f_k(x)) =  - log(f_y(x)). Minimizing lce therefore increases the probability of the true class. We can distinguish two cases to understand what happens during the optimization: whether the prediction \u0177 is correct or not. In the first case, the confidence score is s = fy(x): minimizing lCE increases the confidence fy(x). In the second case, the prediction is incorrect, which implies that fy(x) < s. Minimizing lCE increases the probability of the true class fy(x) but does not directly change the confidence (because s \u2260 fy(x)). Instead, the confidence (which was attributed to a wrong class) is indirectly lowered through the softmax normalization.\nOne-versus-All approach for binary methods The One-versus-All (OvA) calibration approach [70] allows adapting calibration methods for binary classifiers to multiclass classifiers. To do so, it decomposes the calibration of multiclass classifiers into sets of L binary calibration problems: one for each class k. For each problem, the considered probability is fk (x), and the associated label 1y=k \u2208 {0,1}. When calibrating a classifier from data, each binary problem is highly imbalanced with a ratio between positive and negative examples equal to 1/L-1 if the classes are equally sampled. For instance, for ImageNet, the ratio is 1/999: out of 25000 examples, only 25 have a positive label."}, {"title": "4 Top-versus-All approach to confidence calibration", "content": "4.1 General presentation\nIn the calibration definition (1) and the standard ECE metrics, only the confidence, i.e., the maximal probability, reflects the likelihood of making an accurate prediction. The probabilities of other classes are not taken into account. However, the standard approach to calibration uses the entire set of probabilities, not just confidence, which introduces unnecessary complexity. We aim to simplify the process by reformulating the problem of calibrating multiclass classifiers into a single binary problem. This problem can be phrased as: \"Is the prediction correct?\". In this setting, we do not calibrate the predicted probabilities vector but only a scalar: the confidence. The remaining probabilities are discarded. This is equivalent to calibrating a surrogate binary classifier that predicts whether the class prediction is correct. Since this correctness classifier only considers the maximal probability versus all others, we call our approach Top-versus-All (TvA).\nReplacing the standard approach by TvA is straightforward. Given the standard calibration data Deal = {(xi, Yi)}_{i=1}^N, we add a few data preprocessing steps. First, compute the class predictions \u0177 and their correctness: yb_i = 1_{\u0177_i=y_i}. Second, create the surrogate binary classifier f'(x) = maxkey fk(x). Finally, build the calibration set for the surrogate binary classifier:\nD_{cal}^{TVA} = {(x_i,y_i^b)}_{i=1}^N \\qquad(3)\nAfter this preprocessing, we choose a standard calibration function g, e.g., Temperature Scaling, to calibrate the surrogate binary classifier. The learning of the calibration function follows its original underlying algorithm but uses the modified calibration data D_{cal}^{TVA}. The learned calibration function is then applied to the confidences of the original multiclass classifier. Algorithm 1 describes our approach. In the Appendix, Algorithm 3 provides more details and highlights differences with the standard approach of Algorithm 2."}, {"title": "4.2 Top-versus-All approach for scaling methods", "content": "Because our Top-versus-All setting reformulates the calibration of multiclass classifiers into a binary problem, the natural loss is the binary cross-entropy:\nl_{BCE} = - (y_i^b \\cdot log\\ s + (1 - y_i^b) \\cdot log(1 - s)) \\qquad(4)\nMinimizing this loss results in confidence estimates that more accurately describe the probability of being correct, regardless of the L - 1 less likely class predictions. Using the binary cross-entropy as a calibration loss makes an important difference compared to the usual multiclass cross-entropy. The cross-entropy loss takes into account the probability of the correct class, while with TvA the binary cross-entropy takes into account the probability of the predicted class (i.e., the confidence)."}, {"title": "4.3 Top-versus-All approach for binary methods", "content": "Our TvA approach replaces the One-versus-All approach to apply binary methods to the multiclass setting. TvA transforms the multiclass setting into a single binary problem that uses the binary calibration dataset (3). In this dataset, the proportion of positive labels equals the classifier's accuracy a. The ratio between negative and positive examples is \\frac{1-a}{a} = \\frac{(1-a) N}{(1-a) N}. For a classifier with 80% accuracy on ImageNet and a calibration dataset of 25000 examples, there are 5000 negative and 20000 positive examples (ratio of 1/4). This is still a bit imbalanced but orders of magnitude smaller than the class-wise binary calibration datasets of the One-versus-All approach (ratio of 1/999)."}, {"title": "5 Experiments", "content": "5.1 Setting\nDatasets and models For image classification, we used the datasets CIFAR-10 (C10) and CIFAR- 100 (C100) [27] with 10 and 100 classes respectively, ImageNet (IN) [7] with 1000 classes, and ImageNet-21K (IN21K) [54] with 10450 classes. For text classification, we used Amazon Fine Foods (AFF) [43] and DynaSent (DF) [51] for sentiment analysis with 3 classes, MNLI [66] for natural language inference with 3 classes, and Yahoo Answers (YA) [73] for topic classification on 10 classes. Experiment results are averaged over five random seeds that randomly split the concatenation of the original validation and test sets into calibration and test sets.\nWe used the following models for image classification: ResNet [21], Wide-ResNet-26-10 (WRN) [71], DenseNet-121 [24], MobileNetV3 (MN3) [23], ViT [9], ConvNeXt [41], EfficientNet [56, 57], Swin [40, 39], and CLIP [52] which matches input images to text descriptions in a shared embedding space, assigning labels based on the highest similarity score. For text classification, we used the PLMs RoBERTa [37] and T5 [53].\nBaselines Our Top-versus-All (TVA) reformulation and regularization (reg) can be applied to different calibration methods. We have tested the following scaling methods: Temperature Scaling (TS) and Vector Scaling (VS) [17], and Dirichlet Calibration (DC) [29] with the best-performing variant Dir-ODIR, which regularizes off-diagonal and bias coefficients. We also tested the following binary methods: Histogram Binning (HB) [69] using for each case the best-performing variant between equal-mass or equal-size bins, Isotonic Regression (Iso) [70], Beta Calibration (Beta) [28], and Bayesian Binning into Quantiles (BBQ) [46]. For comparison, we include methods with state-of- the-art results on problems with many classes: I-Max [49] and IRM [72]."}, {"title": "5.2 Top-versus-All", "content": "For visual qualitative results, Figure 1 displays reliability diagrams [47]. We observe that initially, ResNet-50 is highly underconfident and ViT-B/16 a bit underconfident. Applying TS and VS solves the underconfidence and makes the models slightly overconfident. TvA improves these methods, and the average confidence gets closer to the accuracy. HBTVA makes the calibration almost perfect.\nTable 1 shows the results of applying the Top-versus-All reformulation to several calibration methods. For clarity, results are averaged over families of models (models based on the same architecture) and the full results are available in Tables 5 and 6 of the Appendix. In most cases, the TvA reformulation significantly lowers the ECE by dozens of percent. Without TvA, binary methods often perturb the prediction and degrade the classifier's accuracy (see Table 9), making them inapplicable in a practical setting. TvA solves the issue as it only scales the confidence (after the prediction is made) and makes binary methods outperform scaling methods.\nImprovements due to TvA are consistent across models. However, exceptions are observed for CLIP: it is the model family with the lowest ECE pre-calibration, but the highest ECE post-calibration for ImageNet. CLIP's multimodal training regime, zero-shot adaptation as a classifier, and very large training dataset might cause this different behavior. CLIP's low ECE was also observed in [44, 11]. [64] specifically tackles the calibration of fine-tuned CLIP, a setting not considered here.\nWe also found that DC is sensitive to hyperparameter tuning, and its performance is usually not much better than VS, which is consistent with [29]. In some cases, the optimization diverges, leading to very poor results, e.g., for CLIP on ImageNet.\nImprovements due to TvA are also consistent across datasets, although they tend to increase with the number of classes. Improvements on ImageNet are usually better than on CIFAR-100, whose improvements are usually better than on CIFAR-10. This is notable with e.g., TS or HB. The magnitude of improvement is usually higher for binary methods, e.g., HB, than scaling methods, e.g., TS, especially for many classes. This indicates that Issue 3 is more serious than Issue 1.\nFor text datasets with only three classes (AFF, DS, and MNLI), TS does not benefit from TvA, but other methods do, despite the small number of classes. According to [5], TS is among the best calibration methods for the text classification tasks considered here, even compared to ones that retrain the model. Even so, our method HBTVA significantly outperforms it."}, {"title": "5.3 Solving overfitting with regularization and TvA", "content": "On ImageNet, VS and DC overfit the calibration set, degrading the calibration on the test set. The lower performance of VS relative to TS indicates this overfitting. As visualized in Figure 2, combining the binary cross-entropy loss used in the TvA reformulation and an additional regularization term prevents overfitting. We fixed the value \u03bb = 0.01 as it works well across models. Initializing the vector coefficients to + with T obtained by TSTVA helps further improve performance."}, {"title": "5.4 Influence of the calibration set size", "content": "The size of the calibration set influences the performance of the different methods, as seen in Figure 3. TS and TSTVA do not benefit from more data due to their low expressiveness. VS does not improve the ECE because of the overfitting problem. In contrast, VSreg_TVA benefits from more calibration data. With enough data (\u2248 15000), it outperforms TSTVA. Binary methods using the standard One-versus-All approach have poor performance and need a large amount of data to be competitive. Using TvA, they get excellent performance with little data."}, {"title": "6 Limitations", "content": "Our approach tackles confidence calibration and is unlikely to improve performance for stronger notions of calibration, such as class-wise calibration. However, confidence calibration is useful for many practical cases, such as selective classification [13], out-of-distribution detection [22], or active learning [34]. Also, calibration improvements are less significant for problems with few classes (< 10) than for problems with many classes, but our approach still provides the best results."}, {"title": "7 Conclusion", "content": "Reducing the miscalibration of neural networks is essential to improve trust in their predictions. This can be done after the model training with an optimization using calibration data. However, many current calibration methods do not scale well to complex datasets: binary methods under the One-versus-All setting do not have enough per-class calibration data, and scaling methods are inefficient. We demonstrate that reformulating the confidence calibration of multiclass classifiers as a single binary problem significantly improves the performance of baseline calibration techniques. The competitiveness of scaling methods is increased, and binary methods use per-class calibration data more efficiently without altering the model's accuracy. In short, our TvA reformulation enhances many existing calibration methods with little to no change in their algorithm. Extensive experiments with state-of-the-art image classification models on complex datasets and with text classification demonstrate our approach's scalability and generality."}, {"title": "A Appendix contents", "content": "B discusses the broader impacts of the work.\nC discusses the proposed approach in more details, and compares with other methods.\nD contains a theoretical justification for Top-versus-All in the case of Temperature Scaling.\nE discusses the limits of classwise-ECE and top-label-ECE for a high number of classes.\nF describes implementation details, including the computing time in Table 3.\nG shows the impact of different calibration methods on selective classification.\nH provides additional results: full ECE results in Table 5 and 6, standard deviations in Table 7, confidences in Table 8, accuracies in Table 9, equal-mass bins ECE in Table 10, Brier score in Table 11, experiments for in-context learning of LLMs in Table 12."}, {"title": "B Broader impacts", "content": "Our reformulation of the confidence calibration of multiclass classifiers as a binary problem is both simple and general. It has several benefits. On the theoretical side, it might lead to new perspectives on the confidence calibration problem and the development of new calibration methods. On the practical side, existing calibration methods can be adapted to our problem reformulation by adding just a few lines of code. This is an easy and quick way to improve the calibration of classification models. Better-calibrated models are more trustworthy: potential incorrect predictions are more easily identifiable and preventable. However, this also comes with potential risks. The knowledge that a model is well-calibrated might lead to undue trust in the system and the tendency to overlook prediction errors. Even well-calibrated models are not entirely reliable, and developers and users must remember this. Post-processing calibration requires data not included in the training set, which leaves less data available for a thorough evaluation of the model. Calibration does not fix biases in the data. Finally, we tested the calibration improvement only on in-distribution data, but real systems might receive out-of-distribution data (e.g., an image of a new class) or adversarial examples. For such inputs, the classifier predicted probabilities (and thus the confidence) are unreliable, even for well-calibrated models, and a pipeline to filter such data is necessary."}, {"title": "C Details on the method", "content": "Algorithm 2 Standard approach\nInput:\nD_{cal}: {(x_i, Y_i)}_{i=1}^N the calibration data\nf: the multiclass classifier\ng: a calibration function \\u25b7 e.g., Temperature Scaling\nLearn calibration function:\nif g is scaling method then\nloss l := Cross-Entropy\nLearn g to calibrate f by minimizing l on D_{cal}\nelse if g is binary method then\nfor k= 1 to L do\n\\u25b7 One-versus-All approach\nD_{cal}^k \\u2190 {(x_i, Y_i) | Y_i = k}_{i=1}^N\nLearn g_k to calibrate f on D_{cal}^k\nend for\ng \\u2190 (g_1, g_2, ...,g_L)\nend if\nInference:\nUse g to calibrate confidences from f\nAlgorithm 3 Top-versus-all approach\nInput:\nD_{cal}: {(x_i, Y_i)}_{i=1}^N the calibration data\nf: the multiclass classifier\ng: a calibration function \\u25b7 e.g., Temperature Scaling\nPreprocessing:\n\u0177_i \\u2190 arg max_{key} f_k(x_i) \\u25b7 Compute class predictions\ny_i^b\\u2190 \\mathbb{1}_{\u0177_i=y_i} \\u25b7 Compute predictions correctness\nf^b\\u2190 max_{key} f_k \\u25b7 Create surrogate binary classifier\nD_{cal}^{TVA} \\u2190 {(x_i^b,y_i^b)}_{i=1}^N \\u25b7 Build binary calib. set\nLearn calibration function:\nif g is scaling method then\nloss l := Binary Cross-Entropy\nif g is vector or Dirichlet scaling\nloss l \\u2190 l + \\lambda l_{reg} \\u25b7 Add regularization\nend if\nLearn g to calibrate f^b by minimizing l on D_{cal}^{TVA}\nelse if g is binary method then\nLearn g to calibrate f^b on D_{cal}^{TVA}\nend if\nInference:\nUse g to calibrate confidences from f"}, {"title": "Comparison with the standard approach", "content": "Algorithm 2 describes the standard approach to post- processing calibration, and Algorithm 3 describes our approach in more details and shows in blue the differences with the standard approach. Our approach adds a preprocessing step to keep only the confidences instead of the full probabilities vector. It can be seen as creating a surrogate \"correctness\" classifier and its associated calibration data. The calibrator is learned for the surrogate classifier and applied to the original classifier at inference time. Also, we add regularization for some scaling methods and we have only one binary calibrator instead of one per class.\nComparison with IRM and I-Max IRM [72] and I-Max [49] are, like TvA, multiclass-to-binary reductions. This is why TvA cannot be applied on top of them: they already transform the multiclass problem into a binary one using a different strategy.\nThe shared class-wise strategy of [49] and the data ensemble strategy of [72] are described very briefly in subsections 3.2 and 3.3.2 of their respective papers and not rigorously justified. Our understanding is that these two strategies do exactly the same thing. To build the calibration set, they concatenate all the class probability vectors so that we get a big probability vector of size N.L (N samples and L classes) as predictions and similarly concatenate the one-hot embedding of the target class (a big vector with N ones and N.(L \u2212 1) zeros) as targets. Then, they learn a single calibrator. For each example, this calibrator aims to simultaneously increase the probabilities for the target class (target is 1) and decrease all the other class probabilities (target is 0). The single calibrator is applied to each class probability separately, meaning that the ranking of class probabilities can change, modifying the classifier prediction.\nOur strategy derives from transforming the multiclass calibration into a single binary problem. The intuition is to learn the calibrator on a surrogate binary classifier and apply this calibrator to the original classifier. This binary classifier is built on top of the original classifier (by applying the max function to the class probabilities vector). They thus share their confidence. However, the binary classifier aims to solve a different task: predicting the correctness of the original classifier. To build the calibration set, we concatenate all the confidences (a vector of size N) as predictions and concatenate all the correctnesses as targets (also a vector of size N). The correctness value of a given example is 1 if the class prediction is correct; otherwise, it is 0. Then, we learn a single calibrator, similar to the strategy above. However, there is a key difference: this calibrator aims to increase the probabilities for correct predictions and decrease them for incorrect predictions. Note that our probabilities are all confidences (the maximum class probabilities), meaning we only consider the confidences, which the calibrator directly increases or decreases. In the strategy from [49] and [72], the calibrator has to manage all class probabilities (L times more), even the ones that do not matter, including the lowest class probabilities close to 0. This is less efficient (actually, while this can surely be fixed, the original implementations of IRM and I-Max could not run on ImageNet-21K). This point is closely linked to the analysis of the binary cross entropy loss for scaling methods in Subsection 4.2: when the prediction is incorrect, increasing the probability of the correct class indirectly decreases the confidence (strategy from [49] and [72]) while our strategy directly decreases the confidence.\nI-Max is more complex because it modifies the Histogram Binning algorithm, while our approach does not. Additionally, [35] found that I-Max produces unusable probability vectors. Indeed, they do not sum up to 1, and normalizing them degrades the method's performance.\nWe wrote our paper with practicality and generality in mind. Contrary to [49] and [72], we demonstrate the generality of our strategy by applying it on top of existing calibration baselines of different natures (scaling and binary). One of our main goals is that practitioners can easily and quickly try our TvA approach, using just a few lines of code, which can significantly improve the calibration performance of their existing calibration pipeline while having no impact on the predicted class by design (except for VS and DC).\nComparison with ProCal Another recent calibration method is ProCal [68]. However, its primary objective differs from ours: it \"focuses on the problem of proximity bias in model calibration, a phenomenon wherein deep models tend to be more overconfident on data of low proximity\". Its goal is to lower the difference in the confidence score values between regions of low and high density, i.e., to make the confidence score independent of a local density indicator called \"proximity.\" There is no theoretical guarantee, however, that minimizing the proximity bias improves the confidence calibration, the focus of our work. Theorem 4.2 about the PIECE metric is a direct consequence of Jensen's inequality and is true for any random variable D, not necessarily a proximity score."}, {"title": "Theorem 5.1 is an interesting bias/variance decomposition of the Brier score. However, as this type of decomposition usually states, the error may come from bias (here, a wrong initial calibration) or high estimation variance (which can be related to low density but is not expressed as such in the decomposition). We experimentally compare our approach to the ProCal algorithm using the code provided by its authors and observe in Table 2 that our approach gives much better ECE confidence calibration and, for half of the models, also better PIECE values.", "content": "ProCal aims to achieve three goals: mitigate proximity bias, improve confidence calibration, and provide a plug-and-play method. We share the last two goals. Concerning improving confidence calibration, our approach has better results, as shown in Table 2. Both approaches are plug-and-play, but they apply very differently. ProCal is applied after existing calibration methods to further improve calibration. It thus does not solve any of the four issues we identified (e.g., cross-entropy loss is still inefficient, and One-versus-All still leads to highly imbalanced problems). Our Top-versus-All approach is a reformulation of the calibration problem that uses a surrogate binary classifier. Existing approaches are applied to this surrogate classifier, which is how the four issues are solved. We do not propose a new method but a new way of applying existing methods. Our approach does not introduce new hyperparameters (except in the particular case of regularizing scaling methods). ProCal introduces several new hyperparameters, such as the choice of the distance, the number of KNN neighbors, or a shrinkage coefficient.\nComparison with Correctness-Aware Loss A concurrent work [38] builds a calibration method on top of an intuition similar to ours: binarize the calibration problem. However, what the authors do with this intuition differs vastly from our approach. They derive a Correctness-Aware Loss (Eq. 7 of their paper), which is almost the standard binary cross-entropy loss we use for scaling methods but without a logarithm. They use this loss to learn a separate model that predicts a sample-wise temperature coefficient. This is a new calibration method, which is not straightforward to implement due to the numerous hyperparameters (network architecture, image transformations...). It also requires multiple inferences at test-time, which can be problematic in some production models. Our approach is, again, not a calibration method but a general reformulation of the calibration problem that enhances existing methods. By looking at their Table 1, they get an ECE of 2.22 on ImageNet (in-distribution), while our approach achieves values around 0.5 for most models in our paper's Table 1. Their method, contrary to TvA, improves the AUROC, but in our understanding, it seems mostly due to the use of image transformations, not from their proposed loss. Their method seems to work best in out-of-distribution scenarios, which is not the main objective of our paper. However, these good results for AUROC and out-of-distribution scenarios make this method complementary to our approach, and combining the two in some way could be promising."}, {"title": "E Limits of classwise-ECE and top-label-ECE for a high number of classes", "content": "Let us define the ECE for class j:\nECE_j = \\frac{1}{N} \\sum_{b=1}^B n_b |acc(b, j) - conf(b, j)|\nThe difference compared to (2) is that now acc(b, j) corresponds to the proportion of class j in the bin. Also, conf(b, j) now is the average probability given to class j for all samples in the bin. Then, classwise-ECE [29] takes the average for all classes:\nECE_{cw} = \\sum_{j=1}^L ECE_j\nClasswise-ECE considers the full probabilities vectors: all the class probabilities for each prediction. However, this metric does not scale to large numbers of classes. Let us see why with an example.\nLet us use a test set of N samples, N/L for each of the L classes (the dataset is balanced), and a high-accuracy classifier fairly calibrated. The classifier predicts N probability vectors of length L. Predicted probabilities for class j are all the values of the vector at dimension j. Because the classifier has a high accuracy and is fairly calibrated, around N/L values are close to 1 (corresponding to mostly correct predictions), and the remaining ones, around N \u2212 N/L, are close to 0 (because the predicted class is not class j, and the predicted probability is high for another class)."}, {"title": "F Implementation details", "content": "F.1 Models weights\n\u2022 Model weights for CIFAR are from [45", "42": ".", "54": ".", "67": ".", "36": ".", "5": ".", "https": "huggingface.co/EleutherAI/gpt-j-6b (Apache- 2.0 license) and Llama-2 https://huggingface.co/meta-llama/Llama-2-13b (li- cense).\nF.2 Datasets\n\u2022 CIFAR-10 (C10) and CIFAR-100 (C100) [27", "7": "contains 1.3 million images from 1000 classes. Following [17"}, {"54": "in its winter21 version, contains 11 million images in the train set, and 522500 in the test set (50 for each of the 10450 classes). We randomly split the test set into equal-sized calibration and test set (261250 samples each, 25 per class).\n\u2022 Amazon Fine Foods [43", "51": "is a dynamic benchmark for sentiment analysis consisting of sentences annotated as positive, neutral, and negative. The original validation set size is 11160 and test size 4320. We randomly split them into 11160 samples for calibration and 4320 for test.\n\u2022 MNLI [66", "73": "contains question-answers pairs corresponding to"}]}