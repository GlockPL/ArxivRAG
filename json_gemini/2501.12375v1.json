{"title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos", "authors": ["Sili Chen", "Hengkai Guo", "Shengnan Zhu", "Feihu Zhang", "Zilong Huang", "Jiashi Feng", "Bingyi Kang"], "abstract": "Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos (< 10 seconds) and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos (over several minutes) without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.", "sections": [{"title": "1. Introduction", "content": "Recently, monocular depth estimation (MDE) has made significant progress, as evidenced by advances in depth foundation models [3, 17, 41, 42]. For example, Depth Any-"}, {"title": "2. Related Work", "content": "Monocular depth estimation. Early monocular depth estimation [1, 9, 10, 45] efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, MiDaS [3] introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model's performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS [3], monocular depth estimation models have generally bifurcated into two categories: relative depth models that estimate affine-invariant depth (e.g., DPT [28], DepthAnything [41, 42], Marigold [17]) and metric depth models that estimate depth with an absolute scale (e.g., ZoeDepth [2], Metric3D [44], UniDepth [27]). Metric depth models require training with metric depth data that includes camera parameters [44], thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2 [42] leveraged the Dinov2 [23] pre-trained backbone to"}, {"title": "3. Video Depth Anything", "content": "In this section, we introduce Video Depth Anything, a feed-forward video transformer model to efficiently estimate temporally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig. 2. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training (Sec. 3.1). A novel loss to enfoce temporal consistency is proposed in Sec. 3.2. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference (Sect. 3.3)."}, {"title": "3.1. Architecture", "content": "Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data.\nDepth Anything V2 Encoder. Depth Anything V2 [42] is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training.\nUnlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract features from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimension. The input data are denoted as $X \\in R^{(B\\times N)\\times C\\times H\\times W}$, where B represents the batch size, N is the number of frames in the video clip, N = 1 for the image as input, C, H, W are the number of channels, height, width of the frames, respectively. The encoder takes X as input to produce a series of intermediate feature maps $F_i \\in R^{(B\\times N)\\times (\\frac{H}{p} \\times \\frac{W}{p}) \\times C_i}$, p is the patch size of the encoder. Although the image encoder extracts strong visual representations from individual frames, it neglects the temporal information interactions between frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.\nSpatiotemporal Head. The spatiotemporal head (STH) is built upon the DPT [28] head and with the only modification being the insertion of temporal layers to capture temporal information. A temporal layer consists of a multi-head self-attention [34] model (SA) and a feed-forward network (FFN). When inputting a feature $F_i$ into the temporal layer, the temporal dimension N is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence.\nThe spatiotemporal head uniformly samples 4 feature maps from $F_i$ (including the final features from the encoder, denoted as $F_4$) as inputs, and predicts a depth map $D \\in R^{H\\times W}$. As shown in Figure 2, the selected features $F_i$ are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT [28]. The final fused high-resolution feature maps are passed through the output layer to produce the depth map D. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions."}, {"title": "3.2. Temporal Gradient Matching loss", "content": "In this section, we start with the Optical Flow Based Warping (OPW) loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss (TGM) that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames.\nOPW loss. To constrain temporal consistency, previous video models such as [19, 37, 38] assume that the depths at corresponding positions in adjacent frames, identified"}, {"title": "3.3. Inference strategy for super-long sequence", "content": "To handle videos of arbitrary length, a straightforward approach is simply to concatenate the model outputs from different video windows. However, this method fails to ensure smooth transitions between windows. A more sophisticated technique entails inferring video windows with overlapping regions. By utilizing the predicted depth of the overlapping regions to compute an affine transform, predictions from one window can be aligned with those from another. Nevertheless, this method can introduce accumulated errors through successive affine alignments, leading to depth drift in extended videos. To address these challenges in ultra-long videos with a limited inference window size, we proposed key-frame referencing to inherit scale and shift information from past predictions and overlapping interpolation to ensure smooth inference across local windows.\nKey-frame referencing. As illustrated in Fig. 3, a subsequent video clip for inference is composed of three parts: $N-T_O-T_k$ future frames, $T_O$ overlapping frames from"}, {"title": "4. Experiments", "content": "4.1. Evaluation\nDatasets. For the quantitative evaluation of video depth estimation, we utilize five datasets that encompass a wide range of scenes, including indoor [7, 22, 24], outdoor [11], and wild environments [5]. Each video is evaluated using up to 500 frames, which is significantly more extensive than the 110 frames used in [13]. For results with 110 frames, see the appendix for details. In addition to video depth evaluation, we also assess our model's performance on static images [42] on five image benchmarks [5, 11, 15, 22, 31].\nMetrics. We evaluate our video depth model using both geometric accuracy and temporal stability metrics. In accordance with [13], we first align the predicted depth maps with the ground truth by applying a uniform scale and shift throughout the video. For geometric accuracy, we compute the Absolute Relative Error (AbsRel) and $\u03b4_1$ metrics [13, 42]. To assess temporal stability, we use the Temporal Alignment Error (TAE) metric in [40], to measure the reprojection error of the depth maps between consecutive frames."}, {"title": "4.2. Zero-shot Depth Estimation", "content": "We compare our model against four representative video depth estimation models: NVDS [38], ChronoDepth [32], DepthCrafter [13], and DepthAnyVideo [40] on established video depth benchmarks. Furthermore, we introduce two robust baselines, 1) Depth Anything V2 [42] (DAv2), and 2) NVDS + DAv2, i.e., replacing the base model in NVDS with DAv2. It is important to note that DepthAnyVideo [40] supports a maximum of 192 frames per video; therefore, we report metrics for the Sintel [5] dataset exclusively for this model, as other datasets contain videos that exceed this frame limit. For static image evaluation, we compare the performance of our model with DepthCrafter [13], DepthAny Video [40], and Depth Anything V2 [42].\nVideo depth results. As demonstrated in Table 1, our VDA model achieves state-of-the-art performance across all long video datasets, excelling in both geometric and temporal metrics. This underscores the effectiveness of our robust foundation model and the innovative design of our video model. Notably, on the KITTI [11], Scannet [7], and Bonn [24] datasets, our model surpasses other leading methods by a significant margin of approximately 10% in the geometric accuracy metric $\u03b4_1$, although it is trained on much fewer video data compared to DepthCrafter [13] (over 10 million frames) and DepthAny Video [40] (6 million frames). For the short video synthetic dataset Sintel [5], where sequences contain around 50 frames each, DepthCrafter [13] exhibits better accuracy than our model. This discrepancy may be attributed to the absence of movie data, which features frames with focal lengths similar to those in Sintel [5], in our model's training set. It is also worth highlighting that our compact model, VDA-S, which has significantly lower latency compared to other models (as shown in Table 3), demonstrates superior geometric accuracy over representative diffusion-based methods for long videos.\nImage depth results. As displayed in Table 2, our video depth model achieves competitive depth metrics compared to DAv2-L [42] in most datasets. This shows that our model maintains the geometric accuracy of the foundation model while also ensuring video stabilization.\nLong video quantitative results. We selected 10 scenes each from Bonn [24] and Scannet [7], and 8 scenes from NYUv2 [22], with each scene comprising 500 video frames. We then evaluated the video depth at frame lengths of 110,"}, {"title": "4.3. Ablation Studies", "content": "Througout this section, we use the VDA-S model with a window size of 16, trained without image distillation unless otherwise specified. The metrics reported without a dataset name represent the mean values across all datasets.\nTemporal Loss. Temporal loss experiments are conducted on the TartanAir [36] and VKITTI [6] datasets. In addition to the TGM+SSI loss we proposed, we evaluate the performance of three other loss functions. The VideoAlign loss is a straightforward design that aligns predicted video depth to the ground truth using a shared scale-shift and computes the 11 loss. Building upon VideoAlign, the VideoAlign+SSI loss introduces an additional spatial loss to supervise the single-frame structure. The OPW+SSI loss combines optical flow-based warping loss proposed in [38] with a single-frame spatial loss. SE refers to the stable error loss introduced in Equation 2. As shown in Table 4, while VideoAlign and VideoAlign+SSI exhibit good geometric metrics, their video stability metrics are poor. Among loss functions with temporal constraints, our proposed TGM+SSI loss significantly outperforms the OPW+SSI loss on both geometric and stability metrics, and achieves metrics comparable to SE+SSI. It shows that TGM not only corrects the errors from OPW but also eliminates the dependency on optical flow.\nInference Strategies. To ablate our inference strategy, we consider four different inference schemes. Baseline, inference is performed independently on each window without overlapping frames. Overlap Alignment (OA), based on scale-shift invariant alignment of the overlapping frames between two neighboring windows, this method stitches the two windows together. Overlap Interpolation (OI), following the approach in DepthCrafter [13], this method splices two windows together after performing linear interpolation"}, {"title": "5. Conclusion", "content": "In this paper, we present a novel method named Video Depth Anythingfor estimating the depth of the video that is temporally consistent. The model is built on top of Depth Anything V2 and is based on three key components. First, a spatial-temporal head to involve temporal interactions by applying a temporal self-attention layer to feature maps. Second, a simple temporal gradient matching loss function is used to enforce temporal consistency. Third, to enable long-video depth estimation, a novel keyframe-based strategy is developed for segment-wise inference along with a depth stitching method. Extensive experiments show that our model achieves state-of-the-art performance in three aspects: spatial accuracy, temporal consistency, and computational efficiency. Consequently, it can produce high-quality depth predictions for videos lasting several minutes."}, {"title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos\nSupplementary Material", "content": "We present more qualitative comparisons among different approaches for static images and evaluation videos.\nIn-the-wild image results. Static image depth estimation results are shown in Fig. 8. DepthCrafter [13] and Depth Any Video [40] exhibit poor performance on oil paintings. DepthCrafter [13] also struggles with transparent objects such as glass and water. Compared with these methods, our model demonstrates superior depth estimation results in complex scenarios. Moreover, our model shows depth estimation results for static images that are comparable to those of Depth-Anything-V2 [42], demonstrating that we have successfully transformed Depth-Anything-V2 into a video depth model without compromising its spatial accuracy."}, {"title": "4. More Details of Pipeline", "content": "Spatiotemporal head details. Among the four temporal layers, two are inserted after the Reassemble layers at the two smallest resolutions, and the other two are inserted before the last two Fusion layers.\nThe shape of the feature is transformed into (B \u00d7 Hf X Wf) \u00d7 N \u00d7 C before each temporal layer and is transformed back to (B \u00d7 N) \u00d7 C \u00d7 Hf \u00d7 Wf after each temporal layer. Here, B denotes the batch size, N represents the number of frames in the video clip, Hf and Wf are the height and width of the feature, respectively, and C represents the number of channels in the feature, as shown in Fig. 10\nImage distillation details. We follow the approach in [42] and use a teacher model that comprises a ViT-giant encoder and is trained on synthetic datasets. The loss function used for distillation is identical to the spatial loss employed for video depth data.\nTraining dataset details. For video training, we utilize four synthetic datasets with precise depth annotations: TartanAir [36], VKITTI [6], PointOdyssey [48], and IRS [35], totally 0.55 million frames. The TartanAir [36], VKITTI [6],"}, {"title": "5. More Details of Evaluation", "content": "Evaluation dataset details. We use a total of five datasets for video depth evaluation: KITTI [11], Scannet [7], Bonn [24], NYUv2 [22], and Sintel [5]. Specifically, we use Scannet [7] and NYUv2 [22] for static indoor scenes, Bonn [24] for dynamic indoor scenes, KITTI [11] for outdoor scenes, and Sintel [5] for wild scenes. For NYUv2 [22], we sample 8 videos from the original dataset, which contains 36 videos. Our evaluation comprises three different settings: long videos, long videos with different frame lengths, and short videos. For the long video evaluation, we use all five datasets and set the maximum frame length to 500 for each video. For the evaluation of long videos with different frame lengths, we select subsets of videos with frame lengths greater than 500 from Scannet [7], Bonn [24], and NYUv2 [22]. For the short video evaluation, we use KITTI [11], Bonn [24], and Scannet [7], setting the maximum frame lengths to 110, 110, and 90, respectively, in accordance with the settings in DepthCrafter [13]. In addition to video depth evaluation, we also assess our model's"}]}