{"title": "Generating the World from Top-Down Views", "authors": ["Ansh Sharma", "Albert Xiao", "Praneet Rathi", "Rohit Kundu", "Albert Zhai", "Yuan Shen", "Shenlong Wang"], "abstract": "In this work, we present a novel method for extensive multi-scale generative terrain modeling. At the core of our model is a cascade of superresolution diffusion models that can be combined to produce consistent images across multiple resolutions. Pairing this concept with a tiled generation method yields a scalable system that can generate thousands of square kilometers of realistic Earth surfaces at high resolution. We evaluate our method on a dataset collected from Bing Maps and show that it outperforms super-resolution baselines on the extreme super-resolution task of 1024\u00d7 zoom. We also demonstrate its ability to create diverse and coherent scenes via an interactive gigapixel-scale generated map. Finally, we demonstrate how our system can be extended to enable novel content creation applications including controllable world generation and 3D scene generation.", "sections": [{"title": "1 Introduction", "content": "There have been significant advancements in developing generative models capable of creating a range of visual data, including individual images [2,26,36,66,75], videos [18,21,71] and 3D objects [10, 42, 62]. These models have become pivotal in content creation, opening up numerous exciting applications."}, {"title": "2", "content": "However, the generation of large-scale representations of Earth's landscape is still a largely unexplored area. Successfully achieving this requires a balance of coherency, realism, and diversity. This is crucial because Earth's landscape is both highly structured and diverse on a macroscopic level, encompassing everything from vast plains to densely populated urban areas. At the same time, it is filled with rich, intricate details at the microscopic level. Any failure to capture both aspects can lead to inaccuracies and unrealism.\nCurrent techniques for large-scale visual generation fall into two categories: the compositional approach and the hierarchical approach. The compositional approach [2, 8, 44, 45, 69, 83] synthesizes unlimited images or 3D worlds by iteratively expanding the frontier, creating new local areas and seamlessly integrating them into the existing structure. This method excels in preserving high-resolution, fine-grained details and can generate images of any size. However, it struggles to accurately capture larger, macroscopic structures. In contrast, the hierarchical approach [27,66] uses a coarse-to-fine architecture. It starts by generating a basic layout and then progressively adds more detail in a manner akin to super-resolution. This method is effective in capturing high-level structures but is limited in the maximum size it can generate. This will be illustrated later through empirical results; for generating Earth observation data, existing methods in this category tend to lack sufficient detail at a finer level.\nIn this work, we present EarthGen, a novel framework for infinite-size, high-resolution earth observation imagery generation aimed at overcoming the aforementioned challenges. Our key insight is to combine the best of the worlds of hierarchical and compositional generation methods. Like hierarchical generation, we start the generative process at a significantly lower resolution than the final result. We then use a series of latent diffusion super-resolution modules to iteratively refine and introduce plausible features at each scale. Importantly, our super-resolution module is designed to be scale-aware. This ensures globally coherent outputs and captures realistic structures at multiple levels, ranging from regional to kilometer, meter, and centimeter scales. Simultaneously, diverging from conventional hierarchical methods, our method maintains high cohesiveness and expandability through a novel, diffuser-based composition method, allowing expansion to infinite sizes.\nWe demonstrate our framework's capabilities by generating gigapixel-scale terrain covering 30km x 10km, approximately three times larger than Manhattan, at a resolution of 15 cm/pixel. An interactive, full-resolution visualization of this terrain is available at https://earthgen.github.io/. Our result showcases diversity, extreme realism at both macro and micro levels, and global coherency in structures and content. Additionally, we benchmark our approach against current state-of-the-art generation and super-resolution methods in the extreme generative task of 1024\u00d7 zoom. Our experimental results demonstrate superior performance, both quantitatively and qualitatively, and are supported by a user study. We additionally showcase EarthGen's potential in controllable generation (conditioned on a map layout), as well"}, {"title": "3", "content": "as in 3D world creation (using an off-the-shelf depth reasoning module on top of EarthGen data).\nOur contributions are listed as follows:\nWe introduce a novel perpetual generation framework capable of creating realistic, arbitrarily sized visual images across up to 5 hierarchies, with a resolution difference of up to 1024 times.\nBuilding upon this generation method, we have developed EarthGen, a system capable of generating high-quality, large-scale earth observation images.\nEarthGen builds upon advances in generative modeling techniques and systematically integrates them into the unique domain of earth observation data. This offers new opportunities to the broader communities of computer vision, remote sensing, environmental science, agriculture, and urban planning, paving the way for numerous applications ranging from asset creation for games to data augmentation and enhancement for earth observation, such as land cover classification and automatic mapping. We aim to assist researchers in addressing some of the most pressing environmental and societal challenges in the future."}, {"title": "2 Related Works", "content": "Image-level Super-Resolution. Super-resolution models reverse the degradation process by recovering the high-frequency details from the low-resolution image [9]. Examples of degradation include noise injection and blurring. One idea for super-resolution is to explicitly approximate parameters for the degradation process [20, 29,55] using prior assumptions for the degradation model. The challenge is that real-world degradation is too complicated to capture exactly. Alternatively, with synthetic and paired real-world datasets available, supervised [40, 91,91] and self-supervised [16, 70, 72] approaches have been proposed to directly learn the mapping from low resolution to high resolution in a data-driven manner. Beyond learning the mapping like EDT [43] and HAT [11], generative modeling is designed to produce additional details from low-frequency signals. For example, many recent works develop super-resolution frameworks with generative adversarial networks [34,45,68,82]. In particular, GigaGAN [35] shows superior performance in \u00d732 upscaling while supporting text-conditioning. Meanwhile, diffusion models also work well for detail generation as can be seen in the cascaded diffusion modeling [27] in Imagen [66] and DeepFloyd IF [1]. In this work, we seek to efficiently upscale to 1024 times relative to the input resolution, which is far beyond what existing work supports.\nUnbounded Content Generation. Unbounded content generation seeks to produce 2D or 3D content at infinite scale while ensuring the fidelity of the generation. Perpetual video generation is one application that produces realistic and 3D-consistent fly-through videos. Researchers have achieved success in natural scenes in terms of fidelity and realism [7, 44, 47]. Beyond perpetual view generation, prior attempts are capable of producing large-scale 2D maps [93] and 3D"}, {"title": "4", "content": "scenes [46,69,83], even in complex urban settings. In our work, we demonstrate the ability to generate city scale high-resolution remote sensing imagery. Generative Powers of 10 [80] addresses a similar task of unlimited zoom via generation of a nested multi-resolution image using a pre-trained diffusion model. Our work differs in a few major ways: notably its factorizable formulation allows for tiling to densely super-resolve the entire base image rather than just a single pyramid, and the individually fine tuned layers ensure scale consistency and realism.\nDiffusion Models. Before the recent success of deep learning-based diffusion model [73, 75, 76], researchers were long puzzled by Generative Adversarial Networks (GANs), with issues like unstable training [39] and mode collapse [77,92]. While early attempts on score matching [30] were unable to produce high-fidelity images, the seminal work by Song et al. [75] greatly increased the capacity by introducing multi-scale noise perturbation in the process of Langevin Dynamics. This is further improved upon with modified schedulers [59], non-Markovian denoising processes [74], and guidance techniques [14, 28, 58, 64]. More recently, Latent Diffusion Models [65] separate perceptual compression through encoder-decoder structures, from semantic compression through diffusion models in the latent space, and introduce latent-space conditioning through cross-attention.\nWith diffusion models exhibiting strongly customizable conditioning capabilities [3], researchers have attempted to apply diffusion models to more complex composition and tile generation tasks. One proposed technique models diffusion models as energy-based models to compose objects for higher complexity generations [49]. Other works attempt to jointly de-noise over multiple spatial locations given a diffusion prior. For instance, Multidiffusion [4] reconciles multi-tile denoising at each noise level by solving a least square problem, and Mixture of Diffusers [33] follows a similar idea but instead composes the noise predictions for each tile into a single prediction for the whole image.\nVisual Foundation Models for Remote Sensing. Remote sensing images have received significant attention as a broad application within computer vision. Many previous remote sensing works have focused on classification [5, 13,52,78], recognition [31,61], and segmentation [19,86], as well as remote sensing-specific captions [48, 51, 53, 63, 89]. Recent works like GEO-Bench [41] and Prithvi [32] have advanced remote sensing data analysis through benchmarking and pre-trained foundation models, but have primarily focused on facilitating analysis rather than high-fidelity generative modeling. In contrast, we develop powerful generative models for synthesizing realistic remote-sensing imagery to unlock diverse use cases.\nPrior work has explored image-to-image translation from map views to satellite images [17, 67], with recent methods [17] generating satellite views conditioned on input maps over regions like Scotland. However, leveraging detailed map priors eases the generation process by providing rich structural information. Our unconditional and super-resolution models learn to generate coherent global and local structures from much weaker or no conditioning, though we also demonstrate low-resolution map conditioning to loosely control terrain features."}, {"title": "3 Approach", "content": "We introduce EarthGen, a novel generative framework capable of continuously producing arbitrarily large-scale, photorealistic, coherent, and diverse landscapes from a bird's eye view, at resolutions as fine as 15cm per pixel. We approach this task as a cascaded and tiled generation problem (Sec. 3.1), comprising three key components: a latent diffusion-based generation capturing diverse structures at a regional scale (Sec. 3.2); a coarse-to-fine multi-scale generative zooming module that progressively adds scale-aware visual details aided by negative conditioning (Sec. 3.3); and a mixture model based sampling scheme ensuring cohesive outputs across spatially adjacent areas without detail loss (Sec. 3.4). We train our approach using a large-scale, multi-resolution satellite imagery dataset (Sec. 3.5)."}, {"title": "3.1 Problem Formulation", "content": "Formally, let x : U \u00d7 S \u2192 R\u00b3 represent a multi-resolution spatial RGB map defined in a geo-coordinate system. We denote Xu,s CR\u00b3 as the sampled RGB appearance at region u \u2208 UCR2 at scales \u2208 S. Our goal is to model this"}, {"title": "6", "content": "functional's distribution with a generative probabilistic model p\u03b8(x), where \u03b8 represents learnable parameters. This would enable us to sample multiple, diverse virtual worlds from top views, representing different landscapes, appearances, and zoning. Modeling this distribution poses two main challenges: 1) The mapping can extend indefinitely in the 2D geo-coordinate space, creating an infinite data generation problem; 2) Earth observation imagery captures a wide range of detailed structures at different scales, adding to the complexity. These challenges lead us to factorize the joint distribution into a series of conditional distributions:\n$$p_{\\theta}(x) = \\prod_{u \\in U} p_{\\theta}(X_{u,0}) \\prod_{s \\in S/0} P_{\\theta}(X_{u,s} | X_{s-1})$$\nwhere p\u03b8(Xu,0) is the prior unconditional distribution of the base model, P\u03b8(Xu,s | Xs-1) is a zooming module that models the conditional distribution of the visual appearance at scale s given its preceding layers data XS-1 at scale s-1. Factorization across scales and spaces allows us to sample efficiently and capture structured details across each scale.\nTo make coherent large-scale generation feasible, we further decompose each layer into a grid of overlapping tiles. Inspired by [15,25,49], we then formulate this distribution as a compositional product of experts:\n$$P_{\\theta}(X_{u,s} | X_{s-1}) \\propto \\prod_{v \\in N_{u,s-1}} e^{ - {w_{u,v} \\cdot T_{u,v}(p_{\\theta}(X_{v,s} | e^- , X_{v,s-1}))}}$$\nwhere u is the region-of-interest at scale s, Nu,s-1 is the set of tiles at layer s-1 that overlap with u, e is a negative text prompt embedding, and wu,v is a normalized gaussian mixing weight. The core idea of this design is to ensure coherency across scales and coherency between spatially neighboring areas.\nFig. 2 depicts the overall framework of our approach with a highlight on each module. We now describe each module's parameterization and how we sample from it."}, {"title": "3.2 Unconditional Base Layer Model", "content": "We begin our system with a base layer generation at low resolution. In particular, we sample x0 ~ \u03c1\u03b8(X0) = \u03a0ucu\u03c1\u03b8(Xu,0) by modeling the data distribution via a latent diffusion model [65]. Latent diffusion models learn to create data by reversing a diffusion process within a latent space induced by a variational autoencoder.\nWithin the latent space, the LDM's forward diffusion process is modeled as a Markov chain that corrupts latent z0 = enc(x0) into noise over T steps. At each step t, noise is added according to zt = \u221a\u03b1tz\u00aft\u22121 + \u221a1 \u2212 \u03b1te, where e ~ N(0, I) and \u03b1t is a variance schedule."}, {"title": "7", "content": "The reverse process is learned. From a score model perspective, the LDM predicts the gradient of the data's negative log-likelihood: \u03f5\u03b8(z,t) \u221d -\u2207log p\u03b8(dec(z0))\nWe can sample the previous time step as follows:\n$$z_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(z_t - \\sqrt{\\frac{1-\\alpha_t}{\\overline{at}}}\\epsilon_{\\theta}(z, t)) + \\sigma_t \\epsilon,$$\nwhere \u03c3t is a noise term to add stochasticity, \u2208 ~ N(0, I), and \u0101t is the cumulative product of \u03b11 through \u03b1t. Starting with zT ~ N(0, I), we can generate our base layer by iteratively sampling until z0, after which the decoder brings the latent back to the image space by x0 = dec(z0)."}, {"title": "3.3 Cascaded Generative Super-Resolution", "content": "The core of our system centers around a set of cascaded super-resolution modules, each trained independently to specialize in hallucinating features at a given resolution. Each module learns p\u03b8(Xu,s | Xu,s-1) via an LDM. By passing in a base image sequentially through the cascade, we iteratively upscale it to the desired resolution while introducing realistic hallucinated features at each stage. This formulation takes inspiration from Imagen [66], which generates high-fidelity images by a low-resolution base model and two subsequent super-resolution models."}, {"title": "8", "content": "Super Resolution Architecture. We use the Stable Diffusion x4 Upscaler [65] as the starting point for each super-resolution model, allowing us to increase the resolution by a factor of 4 during each step of the cascade. The model consists of a VAE encoder/decoder which maps RGB images of dimension m \u00d7 n \u00d7 3 into a latent space with new dimension \u00d7 14 \u00d7 4. This results in the latent space having the same spatial dimensions as the low-res image, allowing us to concatenate the two into a 7-channel input to the denoising U-nets at each time step. We can then learn super-resolution via an LDM conditioned on the low-resolution tile, where the model learns to estimate \u03f5\u03b8(zu,s,t, Xu,s-1) \u221d -\u2207log p\u03b8(zu,s|xu,s-1).\nNegative Conditioning. However, sampling directly using Eq. 3 with this formulation leads to significant issues with compounding errors over layers. Ho et al. [27] addresses this issue by conditioning each stage on a single class label. However, due to the scale of our task, direct conditioning alone would not be sufficient. Densely labeling each tile would be prohibitively expensive, while using a shared label would reduce the diversity of the model's generations across scales/locations.\nInstead, inspired by recent progress in compositional text conditioning [3, 49], we utilize inference-time negative text conditioning to direct our images away from low-quality outputs, for instance, blurry or deformed images, based on the model's pre-trained priors. In particular, given a fixed negative prompt embedding e, we seek to predict\n$$P_{\\theta}(x_s | e, x_{s-1}) \\propto \\frac{P_{\\theta}(x_s | x_{s-1})^2}{P_{\\theta}(x_s | e^- , x_{s-1})}$$"}, {"title": "8", "content": "Because the noise estimates model the negative log-likelihood of the data, by adding a hyperparameter to control the strength of the negative conditioning, we can define a corresponding modified noise prediction function to use in our super-resolution modules as:\n$$\\overline{\\epsilon_{\\theta}}(z_{u,s}) = \\epsilon_{\\theta}(z_{u,s}) + \\lambda_{neg} (\\epsilon_{\\theta}(z_{u,s}) - \\epsilon_{\\theta}(z_{u,s}, e^-))$$\nNote that we left out the common conditioning parameters (xu,s-1,t) for readability. Derivation details are provided in Appendix ??. This formulation allows us to avoid low-quality outputs from compounding errors without dense labels while maintaining global diversity."}, {"title": "3.4 Mixture of Diffusers for Tiling Consistency", "content": "Directly modeling the zoom for each tile independently in Eq. 1 would lead to tile border inconsistencies. Markov Random Fields (MRFs) or sequential modeling could be used to explicitly enforce conditioning between tiles at a given layer, but both are expensive to sample from.\nAlternatively, we integrate a mixture of diffusers inspired by [33], where each tile is conditioned on multiple overlapping views from the preceding scale as visualized in Fig. 2. This modifies the diffusion process to make the noise prediction at each time step a Gaussian blend of the predictions for that region from each tile which covers it in part. We now define a tiled noise as follows:\n$$\\hat{\\epsilon_{\\theta}} (z_{u,s,t}, X_{u,s-1}) = \\sum_{v \\in N_{u,s-1}} w_{u,v} \\cdot T_{u,v} (\\epsilon_{\\theta}(z_{v,s,t}, X_{v,s-1})),$$\nwhere Nu,s-1 is the neighborhood of tiles at resolutions - 1 which overlap with u, Tu,v is a translation operator which zero pads and shifts the predicted noise from view v to u, and wu,v is a normalized blending matrix proportional to a Gaussian centered at v. Noting that the predicted noises learn to model the negative log likelihood of the data, this leads us directly to the tiling component mentioned in Eq. 2.\nTo sample, we aggregate all view estimates into a single image-level estimate \u03f5(z, t, xs-1) where each pixel is the weighted average of the predictions for it from each view containing it. Then, we can sample the previous timestep for the entire image simultaneously using Eq. 3 with \u03f5\u03b8.\nThe denoised latent is decoded via tiled decoding [3], which involves extracting overlapping patches from the latent, decoding them separately, and blending linearly to merge and get our final output for the layer xs."}, {"title": "3.5 Training", "content": "VAE Tuning. We observed an inability of the pre-trained VAE to model satellite imagery well, especially at lower zooms. To alleviate this, we began by fine-tuning"}, {"title": "9", "content": "our VAE across all resolutions in the dataset via an image reconstruction task. We optimize a composite loss function comprising three terms:\n$$L_{VAE}(\\phi ; x) = L_{MSE}(\\phi ; x) + \\lambda_{KL} L_{KL}(\\phi) + \\lambda_{LPIPS} L_{LPIPS} (\\phi ; x)$$\nwhere x denotes the high-resolution target image, \u03c6 represents the parameters of the VAE, LMSE is the mean-squared error, LKL is the KL divergence that regularizes the latent space, LLPIPS measures the perceptual similarity, and \u03bbKL, \u03bbLP are weight hyperparameters.\nLDM Training. After freezing the VAE, we train an LDM over a single resolution of data to unconditionally model the map's base zoom layer. Training involves learning \u03b8 by optimizing a variant of the evidence lower bound (ELBO). Ho et al. [26] show that this is empirically equivalent to minimizing the mean squared error between the predicted noise and actual noise, allowing optimization via:\n$$L(\\theta) = E_{z_0,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(z_t, t)||^2]$$\nSuper-Resolution Training. Using the same frozen VAE, each component of the cascade is trained independently on paired low/high-resolution data, each with a x4 resolution gap. The training objective at scale s only differs from Eq. 8 due to conditioning on the corresponding scale s \u2013 1 tile:\n$$L_{sr}(\\theta,s) = E_{x_{u,s},t,t} [||\\epsilon - \\epsilon_{\\theta} (z_{u,s,t}, x_{u,s-1})||^2]$$"}, {"title": "4 Experiments", "content": "Dataset. We construct a custom dataset via the Bing Maps API [56] by querying satellite images at varying zooms and coordinates. The maximum zoom available is denoted as zoom 20, corresponding to 15cm/px, with each decrease in level corresponding to a \u00d72 reduction of resolution. We randomly sample concentric image pyramids of size 2048 \u00d7 2048 at zooms ranging from 10 to 20 over latitudes from -66 to 66 to minimize projection distortion.\nWe observe resolution availability falls into three main classes. The coarsest class has resolutions up to Bing's zoom 13, corresponding to 19.2 m/px at the equator. Most of these occur over the oceans, so due to the limited resolution and lack of diversity, we ignore this class in constructing our training dataset. The remaining data over land tends to have resolution availability up to zoom 20 (15 cm/px) over the USA and Western Europe, and up to zoom 19 (30 cm/px) over the rest of the world. We compile 32,000 image stacks available up to zoom 19 (Bing19), and an additional 25,000 available up to zoom 20 (Bing20).\nWe additionally sample another dataset to prioritize urban locations which we call BingUrban. Using a list of the top 1000 most populated cities in the US [57] and the 36 most populated European cities [54], we collect 12,000 image stacks with zoom 20 availability centered at these cities with up to a 5-kilometer jitter in any direction.\nWe evaluate our models on two validation sets, each with roughly 100 2048 \u00d7 2048 image pyramids, sampled similarly to Bing20 and BingUrban respectively."}, {"title": "10", "content": "Implementation Details. We use the Stable Diffusion \u00d74 Upscaler [65] architecture as our base for each super-resolution module. The model is trained to super-resolve inputs by a factor of 4 and takes in a concatenation of the low-resolution image and the latent representation at each step.\nWe initially fine-tune a shared VAE across all resolutions with hyperparameters \u03bbKL = 10-9, \u03bbLPIPS = 0.1. Freezing the VAE weights, we then tune each super-resolution module for 100k steps of batch size 24. Each \u00d74 model, referenced as 10to12, 12to14, etc. up to 18to20, is trained independently on all data stacks which contain its higher resolution. We train on high-resolution/low-resolution pairs with side lengths of 512px and 128px respectively.\nAt inference time, we utilize Mixture of Diffusers with a latent tile size of 128 and stride 64. We set \u03bbneg to 5, 2, 3, 3, and 4 for the models from 10to12 to 18to20 respectively on the prompt \"blurry, low res, low quality\". These parameters were decided experimentally to attain a good balance between consistency and realism over both urban and non-urban data. For VAE tiled decode, we use latent sliding window size 512 and tile overlap coefficient 0.25.\nWe utilize Adam [38] as our optimizer for all stages with learning rate 1e-06 and use DPMSingleStepSolver [50] with 50 time-steps as our scheduler during inference for the unconditional and super-resolution modules.\nMetrics. Frechet Inception Distance (FID) [24] and Kernel Inception Distance (KID) [6] are metrics for the quality of generated images with regard to a target distribution, which we use as measures of realism. Both are evaluated over 2048 features, and we use subset size 1000 with 100 subsets for KID.\nEach validation set contains 4900 512\u00d7512 tiles, generated by super-resolving 100 zoom 10 ground truth images at 512x512 pixels to zoom 20 by alternatingly super-resolving and center cropping. The final super-resolved 2048 \u00d7 2048 output is split into 49 overlapping tiles to be used as inputs for the metrics in order to appropriately penalize any tiling artifacts. A separate identically sized dataset of ground truth images is used for calculating metrics."}, {"title": "4.1 Image Super-Resolution", "content": "Baselines. We compare EarthGen to four state-of-the-art models, Real-ESRGAN [81], HAT [11], LIIF [12], and the base Stable Diffusion Upscaler [65]. Real-ESRGAN is fine-tuned on our dataset for more steps and equal time as our models for fair comparison. HAT and LIIF remain untuned because these models are regressive rather than generative, and as such would be unable to introduce new terrain and manmade features even with fine-tuning. We additionally evaluate bicubic interpolation as a baseline for contextualization.\nQuantitative Results. Table 1 provides quantitative results from our evaluated metrics. EarthGen exhibits significant improvements in both FID and KID and in both the general and urban validation set. This shows that the quality of the generated images is significantly improved compared to existing models."}, {"title": "11", "content": "Qualitative Results. Figure 4 provides a qualitative comparison of each model. Although every model produces comparable results in the first \u00d74 superresolution, it quickly becomes clear that our model produces the best images by \u00d71024 superresolution with respect to realism, quality, and consistency.\nFor Stable Diffusion, the compounding effects of low-quality generations have made the final image unrealistic and low quality despite consistency with previous layers. Real ESRGAN can generate high-quality and consistent images across all levels but fails to produce an urban layout with buildings and roads, despite the strong urban layout seen even at \u00d74 superresolution. All three of HAT, LIIF, and basic Bicubic Interpolation are incapable of hallucinating new features, and their final outputs merely look like a smoothened version of the 4 corresponding pixels in the original image. Finally, EarthGen succeeds in all three categories as it consistently preserves the urban layout, has vastly better quality than all except Real ESRGAN, and has easily discernable structures such as roads, houses, trees, and more.\nUser Study. We additionally conducted a user study to compare each baseline method with ours and the ground truth. We presented users with unlabeled one-on-one match-ups between two 2048\u00d72048 zoom 20 images generated by super-resolving zoom 10 images with different methods alongside the following instructions: You will be presented with a number of pairs of aerial photos. Please select the one which looks better in terms of realism and coherency. 30 volunteer student participants engaged in the study aggregating 522 total votes. We observe in Fig. 3 that our method outperforms all but the ground truth, against which it still wins around 30% of the time."}, {"title": "4.2 Applications", "content": "Controllable World Generation. Though EarthGen can be used to unconditionally generate outputs, we provide an example of base-layer conditioning to create controllable generations. In particular, we introduce map conditioning via augmenting the base layer model with a ControlNet [88], allowing the model to learn to generate map-consistent terrains with minimal changes. Fig. 5 demonstrates the potential of this expansion to enable controllable and diverse terrains."}, {"title": "13", "content": "3D Scene Generation. Off-the-shelf depth estimation methods such as DepthAnything [85] can be used to transform our 2D orthographic generations into 3D worlds. We do this by generating a triangle mesh from the 3D point cloud given by the RGB-D image. We include an example of such a 3D generation."}, {"title": "4.3 Method Ablations", "content": "Tiling Methods. We explored four different tiling methods, including naive stitching, Gaussian compositing of overlapping tiles, Multidiffusion [4], and Mixture of Diffusers [33]. We found that using Mixture of Diffusers greatly improves the quality of generated images compared to other methods. To demonstrate this, we use each tiling method during one step of superresolution and qualitatively compare the artifacts from each tiling method in Fig. 7 (a).\nWe observe that stitching produced visible seams between tiles even at one zoom. For Gaussian compositing, seams are slightly better hidden, but significant blurring is introduced. Multidiffusion is a large improvement from the other two but still produces lightly visible seams, the most clear one in Fig. 7 (a) being a horizontal line towards the right of the image in the grass. Mixture of Diffusers produces no artifacts in areas that other methods did, and tiling artifacts are generally nonexistent.\nDirect Generation. Using a cascaded pipeline is crucial to the structure of the generated large images. We explore an ablation by naively generating a larger scale terrain directly at the highest resolution in order to highlight this. We can qualitatively see the effects of the cascaded generation by comparing the outputs of our multi-layer cascade generation with those of a tuned, unconditional baseline designed to solely generate at the highest resolution in Figure 7 (b).\nWe observe that cascaded generation is highly effective in producing a large, realistic image, whereas an unconditionally generated baseline only looks realistic locally. Although the unconditional baseline generates tiles of neighborhoods that look good when zoomed in, these tiles make no structural sense when viewed as a whole. Furthermore, features larger than a single tile such as roads and large buildings are only possible with the cascade introducing them at earlier stages."}, {"title": "5 Discussion", "content": "EarthGen marks a significant advancement in the field of large-scale terrain generation. By combining the strengths of hierarchical and compositional generation methods, our framework is capable of producing highly realistic and infinitely scalable terrain images. The generated terrains exhibit an unprecedented level of detail and diversity, capturing the essence of Earth's varied natural and man-made landscapes. There remain some limitations due to the dataset which are discussed further in the supplementary material.\nWe note that EarthGen can be easily combined with downstream pipelines to enable countless applications, many with the potential to help address upcoming environmental and societal challenges. Immediate use cases in environmental monitoring and agricultural resource management can be attained by using EarthGen to augment remote sensing datasets for tasks such as land cover classification and automatic mapping. Conditional content creation is easily enabled in a similar manner as we demonstrated with ControlNet, allowing for asset creation for open-world 2D games or even 3D games if paired with a depth module such as DepthAnything. Future work will explore pairing EarthGen with an action conditioning model, allowing for visualizing/evaluating urban planning decisions before implementing for more informed decision making. We open source our code and models at https://github.com/anshgs/earthgen in hopes of accelerating research to realize these benefits."}]}