{"title": "Bench-CoE: a Framework for Collaboration of Experts from Benchmark", "authors": ["Yuanshuai Wang", "Xingjian Zhang", "Jinkun Zhao", "Siwei Wen", "Peilin Feng", "Shuhao Liao", "Lei Huang", "Wenjun Wu"], "abstract": "Large Language Models (LLMs) are key technologies driving intelligent systems to handle multiple tasks. To meet the demands of various tasks, an increasing number of LLMs-driven experts with diverse capabilities have been developed, accompanied by corresponding benchmarks to evaluate their performance. This paper proposes the Bench-CoE framework, which enables Collaboration of Experts (CoE) by effectively leveraging benchmark evaluations to achieve optimal performance across various tasks. Bench-CoE includes a set of expert models, a router for assigning tasks to corresponding experts, and a benchmark dataset for training the router. Moreover, we formulate Query-Level and Subject-Level approaches based on our framework, and analyze the merits and drawbacks of these two approaches. Finally, we conduct a series of experiments with vary data distributions on both language and multimodal tasks to validate that our proposed Bench-CoE outperforms any single model in terms of overall performance. We hope this method serves as a baseline for further research in this area.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are capable of performing various natural language processing (NLP) tasks, by using auto-regressive prediction conditioned by the task prompt [2, 23]. LLMs' ability in describing and unifying tasks make them being the key components in current visual understanding tasks, which gives rise to the Large Multimodal Models (LMMs) [16, 37]. While these models are able to perform all kinds of visual and language tasks, they may have different expertise and show significant diversity in performance for different tasks. We refer to these LLMS or LMMs models as experts in this paper. One interesting question arises that how can we effectively identify and exploit the abilities of different experts.\nAs the capabilities of experts gradually improve, benchmark tests have also become more complex and diverse[1, 24, 29]. For example, there are specific domain benchmarks such as GSM8K [7] for assessing mathematical abilities and VCR [35] for evaluating visual reasoning. Additionally, there are benchmarks like MMLU [30] for assessing language task multi-subjects reasoning abilities and MMMU [34] for evaluating multi-domain reasoning in multimodal tasks, both designed to evaluate expert models' capabilities across multiple subjects or domains. These reflect the evolution of expert model evaluation from single to diverse criteria, and they provide insights into the strengths and weaknesses of experts across different tasks.\nWe propose the Bench-CoE framework, which enables expert collaboration (CoE) by effectively leveraging the strengths of different experts from benchmark evaluations. Bench-CoE includes a set of expert models, a router for assigning tasks to corresponding experts, and a benchmark dataset for training the router. Based on this framework, we first formulate the Query-Level Bench-CoE. The Query-Level approach is an abstraction of previous methods, such as[17, 20]. This approach requires evaluating the performance of different expert models for each query. Although it provides detailed information for training the router to select experts, it is expensive in terms of label creation and computational cost, and it struggles to generalize to tasks outside the data distribution. Is there a way to maintain generalization without additional cost?\nWe analyzed that the key to the problem is obtaining labels for query assignment, and the performance of expert models across different subjects in benchmark tests actually serves as a type of label. Compared to the fine-grained Query-Level label, the more coarse-grained Subject-Level label is more likely to maintain the generalization of expert models. We further introduce Subject-Level Bench-CoE, which effectively exploits coarse-grained Subject-Level label from benchmark evaluations, as current benchmarks typically provide subject-level evaluation results [30, 34]. During router training, each query in the benchmark dataset is used to train the router, which encourages the router to assign the query to the expert model that performs best in the subject of the query.\nWe evaluate the effectiveness and generalization of the query-level and subject-level routing learning mechanisms through a series of experiments. The experimental results show that both routing mechanisms improve the performance of the Bench-CoE model over using the best individual model. Specifically, the query-level router performs better on in-distribution data but is prone to overfitting on out-of-distribution data due to the fine-grained routing decisions. In contrast, the subject-level router demonstrates stronger generalization on out-of-distribution data, showcasing its better adaptability and robustness.\nTo summarize, our main contributions are as follows:\n\u2022 We propose Bench-CoE, a simple and efficient pipeline for combining and routing LLM-driven experts, which achieves flexible and efficient task routing without relying on extensive labeled data and large-scale training.\n\u2022 We utilize the performance of each model on benchmarks to select the LLMs to be combined and construct subject-level and query-level datasets to support accurate routing for various specialized tasks.\n\u2022 Experiments demonstrate that the proposed method outperforms single models in multi-task scenarios, enhancing cross-domain multi-task processing performance with almost negligible inference cost."}, {"title": "2. Related Work", "content": "Recent research has focused on the efficient utilization and combination of LLMs in multi-task environments, aiming to meet diverse query requirements and manage resource constraints effectively. Researchers have investigated several routing and integration strategies, developing methods that improve the task-handling efficiency of LLMs. These existing methods can be broadly categorized as follows:"}, {"title": "2.1. Mixture of Experts", "content": "The Mixture of Experts (MoE) framework leverages multiple subnetworks as experts (e.g. FNN) with sparse activation, activating only a subset of experts per input to reduce computational costs while maintaining high performance. A milestone in MoE was achieved with Google's Switch Transformer [10], which introduced a simplified routing mechanism to activate a few experts based on input features. Recent work has emphasized modularity to enhance MoE's adaptability. For example, RankMean [22] uses module-level importance scoring to efficiently merge fine-tuned models without access to training data. Similarly, routing mechanisms have gained prominence, with methods like PolyRouter [26] extending the MoE paradigm to multi-LLM querying via predictive routing, and HDMOLE [19] employing hierarchical routing and dynamic thresholds for multi-domain fine-tuning. Overall, MoE models are centered on expert sub-models, aiming to integrate the specialized capabilities of these experts by combining their parameters and submodules. However, certain parameter-sharing approaches in MoE models often lack sufficient interpretability regarding the specific roles and contributions of individual expert sub-models. This lack of transparency poses challenges in understanding the decision-making process and the functionality of each expert, which may limit the model's trustworthiness and applicability."}, {"title": "2.2. Parallel Inference CoE", "content": "Parallel Inference CoE aims to reduce inference costs by balancing resource utilization across models. FrugalGPT [3] uses a cascading strategy where tasks are processed sequentially across LLMs, dynamically adjusting model usage to optimize costs. LLM-Blender [14] enhances responses by combining multiple LLMs through pairwise ranking and fusion, though its scalability is constrained by its dependence on high-quality data. Similarly, Hybrid LLM [9] routes simpler tasks to smaller models while reserving larger models for complex queries, but it often relies on extensive labeled data for effective routing decisions. GraphRouter [11] introduces a graph-based framework for routing, treating task and model selection as an edge prediction problem, though periodic updates and additional training data are required for new models. Alternatively, Eagle [36] offers a training-free routing mechanism that dynamically selects models using global and local ranking, excelling in real-time scenarios but facing limitations with highly specialized tasks. Parallel inference models, while capable of providing insights into the overall capabilities of different expert sub-models, require the same input to be dispatched to each expert sub-model for independent inference during execution. This approach results in significant computational resource wastage, substantially reducing inference efficiency. The inefficiency becomes particularly pronounced in scenarios with high computational demands, highlighting the need for improved resource utilization strategies."}, {"title": "2.3. Query-Level CoE", "content": "At the query level, recent methods optimize routing by tailoring expert selection for each input. ZOOTER [17] employs reward-guided strategies, extracting reward signals from training queries to direct routing decisions and minimize computational overhead. However, its reliance on fine-tuned reward models can constrain its applicability. Similarly, RouteLLM [20] dynamically assigns simpler tasks to lighter models and reserves stronger models for more complex queries, balancing cost and quality. However, its dependence on large labeled datasets and preference data limits its utility in data-scarce environments. Although query-level mixture-of-experts (CoE) models enable the most fine-grained utilization of expert sub-model capabilities, they require extensive datasets with prior annotations. Each expert sub-model must undergo inference testing on these large-scale datasets to evaluate its performance on specific data. However, the resulting capability assessments are highly sensitive to variations in data distribution [20], which can adversely impact the model's generalization performance and limit its applicability across diverse scenarios.\nIn contrast, our subject-level Bench-CoE addresses this limitation by employing a specific router, which utilizes models specializing in specific subjects as labels for handling corresponding tasks. These labels can be directly obtained from the benchmark evaluation leaderboard, making the implementation process simple, flexible, and more generalized. This design not only ensures that model selection aligns with well-defined criteria but also facilitates an intuitive understanding of the routing process and each model's contribution to the overall task, achieving clear interpretability."}, {"title": "3. Method", "content": "In this section, we provide a detailed description of the Bench-CoE. We first formulate a comprehensive framework. Then, under this framework, we formulate two approaches for training routing using BenchMark: the query-level approach and the subjective-level approach. The query-level approach is an abstraction of some previous methods. However, this method requires instance-level testing to define the data labels for training the routing, which makes it difficult to generalize. To address the generalization issue, we further propose a new approach, the subjective-level approach. We will explain these in detail next."}, {"title": "3.1. Bench-CoE Definition and Notation", "content": "Bench-CoE is an approach of expert collaboration. It enhances the performance of task processing through the router, which can be described as:\n$0 o = f(x, \\{M_l\\}_{l=1}^L, R(\\theta)),$\nwhere x is the input data, o represents the final output result. $\\{M_l\\}_{l=1}^L$ represents a set of expert models, each expert model $M_l$ may focus on processing specific sub-tasks. $\\theta$ refers to the parameters of the router R, which regulates the collaboration of the experts. The Bench-CoE selects the most suitable expert from a group of multiple experts to process the input, with the goal of achieving overall performance superior to that of a single expert model.\nBenchmark and Subjects A set containing V benchmark datasets is defined as $\\{D_1, D_2,..., D_V\\}$. Without loss of generality, a benchmark dataset $D_K$ may contain B subjects, defined as follows:\n$D_K = \\{S_1, S_2, ..., S_B\\}$.\nEach subject $S^b$ corresponds to a set as shown below:\n$S^b = \\{(x^b_i,t^b_i)\\}_{i=1}^{|b|},$\nwhere $x^b_i$ represents the i-th input in subject $S^b$, $t^b_i$ the is the corresponding standard answer, $|b|$ is the number of samples in subject $S^b$.\nModels Set and Performance A set containing L LLMs is defined as follows:\n$M = \\{M_1, M_2, ..., M_L\\}$.\nFor an input queryx, the output of $M_l$ is shown as follows:\n$o_l = M_l(x)$.\nFor each benchmark, there is a corresponding evaluation metric function $P_K$ to assess the performance of $M_l$ on the samples $(x, t)$, defined as follows:\n$P^K_l = P_K(o_l, t)$.\nRouter Definition Let $R(x)$ be a routing function. It is parameterized by $\\theta$ and outputs a probability distribution over L models:\n$R(x) = R(x, \\theta, L)$,\nwhere $R(x)$ denotes the probabilities of L models to process the input x. For text inputs, Router R can be a BERT classifier[8]; for multi-modal inputs, Router R can be a visual language model.\nBased on the framework formulated above, we refine two approaches that are query-level approach and subjective-level approach."}, {"title": "3.2. Query-Level Bench-CoE", "content": "Query Label The performance of each model on each query $x$ can be evaluated through test evaluation. The id of the model with the best performance is designated as the query-level label for that query, as defined below.\n$y_i^{K,b} = arg max \\rho_l^{K,b}$.\nIf multiple models achieve optimal performance on the same query, we select the model with the best overall performance on the benchmark.\nRouter Dataset Once the label for each query are obtained, the benchmark dataset can be used to construct the query-level dataset $D_{query}$, represented as:\n$D_{query} = \\{(x_i^{K,b},y_i^{K,b}) | b = 1,..., B; i = 1, ..., |b| \\}$.\nRouter Train The dataset can be used to train the router, and the expression for the query-level router loss function $L_{query}(\\theta)$ is as follows:\n$L_{query}(\\theta) = \\sum_{b=1}^B \\sum_{i=1}^{|b|} l(R(x_i^{K,b}, \\theta), y_i^{K,b})$.\nModel Set Theoretically, to achieve the best combination results, the performance of all large models should be tested on each individual query. However, this is not practical. Since the models selected based on subject specialization are already the best within their respective fields, we can directly choose them for combination. Experimental results show that this simplified approach is effective.\nInference For each new input x, the router identifies the optimal model by predicting the model best suited to handle the input. The query-level model routing process is as follows:\n$\\hat{l}_{query} = arg max_l R_l(x)$.\nThe Bench-CoE output using the query-level approach is shown as follows:\n$o = f(x, M_{\\hat{l}_{query}}, R(\\theta))$.\nThe Query-Level Bench-CoE method selects an LLM specialized in processing a given query, achieving good results on data within the same distribution. However, it requires testing the performance of numerous LLMs on a large dataset just like other query-level models[17, 20], which can be challenging, and this approach struggles to maintain generalization on out-of-distribution data. We wondered if there could be a way to achieve CoE without the need for extensive testing. Upon analyzing this, we realized that the key to achieving this is to obtain an LLM specialized in handling specific queries. When we examine the performance of various LLMs on benchmark subjects, this can actually serve as a type of label-a label at the subject level. This insight leads us to the next approach we propose: Subject-Level Bench-CoE."}, {"title": "3.3. Subject-Level Bench-CoE", "content": "Subject Label For each input x in the subject $S^b$, we can directly obtain the subject-level label $y_i^{K,b}$ for the query $x_i^b$ from the benchmark. Since the benchmark has already provided the following computational results when the leaderboard were released, our method does not require any additional manual data labeling.\n$y_i^{K,b} = arg max_l \\frac{1}{|b|} \\sum_{i=1}^{|b|} \\rho_l^{K,b}$.\nRouter Dataset Once the subject-level labels $D_{subject}$ for the queries are determined, the subject-level routing dataset can be obtained, as shown below:\n$D_{subject} = \\{(x_i^{K,b}, y_i^{K,b}) | b = 1, ..., B; i = 1, ..., |b| \\}$.\nThis dataset ensures that queries under each subject share the same label, making the router route query based on subject-specific knowledge.\nModel Set From the subject-level routing dataset, we select the appropriate models $M_l$ to incorporate into our collaborative system, which can be represented as follows:\n$M_{set} = \\{M_l | l = y_i^{K,b}, y_i^{K,b} \\in D_{subject}\\}$.\nOnce the dataset and model set are determined, the other processes remain the same as before, such as Router Train and Inference."}, {"title": "3.4. Evaluation Scenarios", "content": "To validate our approach, we designed three types of evaluation scenarios for both language and multimodal tasks, as follows.\nNaive Evaluation Scenario. Given that the MMLU Pro and MMMU datasets contain only one subset, we utilized the test dataset as the benchmark for router training. The router training dataset was constructed from $D_{test}$ as described in Section 4. Both composite and individual model performances were evaluated on $D_{test}$. As the benchmark size increased to encompass various subdomains, the advantages of this scenario became increasingly evident. By leveraging benchmark information, models were combined to achieve superior performance.\nIn-distribution Evaluation Scenario. We utilized the training and validation subsets of the benchmark dataset respectively in the router training and testing phases. The router training dataset was constructed from $D_{train}$ as described in Section 4. Both composite and individual model performances were evaluated on $D_{val}$. This approach tested the performance of the combined model under the same data distribution but with different data splits, enhancing its generalization compared to the first scenario.\nOut-of-distribution Evaluation Scenario. The router training dataset was constructed from $D_1$ as described in Section 4. Both composite and individual model performances were assessed on $D_2$. This approach tested the performance of the combined model under varying data distributions, further evaluating its generalization.\nThese experiments validated our approach across different tasks and data distributions. The results demonstrated that our method could enhance composite model performance without the need for extensive training or complex labels, consistently outperforming individual models across multiple benchmarks."}, {"title": "4. Experiments", "content": "We conducted extensive experiments on both language and multimodal tasks to validate the effectiveness of our proposed method. The experiments were designed to assess the performance of our Bench-CoE model against individual LLMs across various settings, demonstrating the versatility and robustness of our approach."}, {"title": "4.4. Overall Observations for Experiments", "content": "In language task experiments, our method consistently outperforms individual models across different experimental setups. The choice between subject-level and query-level routers depends on the scenario: query-level routers excel when data distributions are similar, while subject-level routers generalize better across different datasets. In multimodal task experiments, our method effectively combines multimodal models to improve performance. The consistent performance gains across experiments validate the flexibility and robustness of our approach in handling different data modalities."}, {"title": "5. Discussion", "content": "Advantages of Bench-CoE. Our Bench-CoE model consistently outperforms individual models across a variety of tasks and datasets. This model is highly flexible, effectively addressing both language and multimodal tasks, and it does not require extensive training phases or hard-to-obtain labels. By utilizing benchmark performance to generate routing labels, Bench-CoE efficiently harnesses the strengths of different models without necessitating significant additional resource expenditures.\nReasons for Performance Gains. Here, we provide three likely reason for why our Bench-CoE works well:\n\u2022 Leveraging Model Strengths. Different models excel in various subjects or on specific inputs. Bench-CoE capitalizes on these strengths by effectively routing each input to the most suitable model.\n\u2022 Effective Routing. The Bench-CoE router accurately predicts the best model for each input or subject, enhancing the overall system performance by ensuring efficient model allocation.\n\u2022 Generalization Ability. Particularly notable with subject-level routing, Bench-CoE demonstrates strong generalization to unseen data distributions, consistently maintaining robust performance across diverse datasets.\nLimitations and Future Work. Although Bench-CoE demonstrates considerable potential, there remain opportunities for further enhancement:\n\u2022 Router Complexity. Exploring more sophisticated routing models may further enhance performance, especially in scenarios where the query-level router overfits.\n\u2022 Scalability. Assessing the method's scalability with a larger number of models or on larger datasets would be valuable for real-world applications.\n\u2022 Dynamic Model Integration. Investigating how to dynamically add new models into the composite system without retraining the router from scratch could improve the method's adaptability."}, {"title": "6. Conclusion", "content": "This paper proposed a simple framework for Collaboration of Experts (CoE) by effectively exploiting the evaluation from benchmarks. The formalized query-level and subject-level routing mechanism effectively integrates multiple LLMs, delivering superior performance across diverse tasks and datasets. By harnessing the strengths of individual models without requiring extensive training or intricate labeling, Bench-CoE establishes a robust baseline for advancing research in model integration and routing strategies."}, {"title": "A. Models and Datasets", "content": ""}, {"title": "A.1. Language Task Models", "content": "Qwen2-7B-Instruct is an instruction-focused language model developed by Qwen Technology. Designed to excel in various natural language understanding tasks, this model utilizes an optimized decoding strategy to enhance performance. With 7 billion parameters, it is well-suited for complex text comprehension and generation tasks, especially in Chinese contexts. Qwen2-7B-Instruct is particularly effective for instruction-responsive tasks such as content creation, information extraction, and dialogue systems.\nGemma-2-9b-it is a large language model developed by Gemma Technologies with 9 billion parameters, tailored for the information technology (IT) sector. Its training data encompasses a vast array of technical documents, programming guides, and texts from open-source projects. This model excels in understanding and generating highly specialized IT content, making it ideal for applications in technical support, documentation automation, and code parsing.\nMathstral-7B-v0.1 is a language model focused on solving mathematical problems, developed by the Mathstral team. With 7 billion parameters, its training includes extensive mathematical educational materials and real-world problem-solving examples. Mathstral-7B-v0.1 is designed to aid in mathematical education, automated problem-solving, and mathematical research, particularly effective for complex mathematical questions and theoretical discussions.\nLlama-3-Smaug-8B is the latest large language model from the Llama team, featuring 8 billion parameters. It has been extensively pre-trained across multiple languages and domains to provide broad knowledge coverage and deep semantic understanding. Llama-3-Smaug-8B emphasizes performance in complex linguistic reasoning, long-form text generation, and multi-domain knowledge integration, suitable for advanced natural language processing tasks such as text summarization, language translation, and cross-domain knowledge-based question answering."}, {"title": "A.2. MultiModal Task Models", "content": "MiniCPM-V-2.6 is a multimodal language model developed to integrate visual processing with natural language understanding. With 2.6 billion parameters, this model is a compact version of the larger CPM series, designed to efficiently handle tasks that require the synthesis of textual and visual data. MiniCPM-V-2.6 excels in image captioning, visual question answering, and other applications where joint understanding of text and image is critical. Its training regimen includes diverse datasets from both textual and visual domains, ensuring robust performance across a variety of multimodal challenges.\nInternVL2-8B is an 8 billion parameter model specifically designed for video-language tasks. Developed to bridge the gap between dynamic visual content and language, InternVL2-8B can analyze and generate descriptions for video data, making it highly suitable for applications such as automated video captioning, video content analysis, and interactive video-based learning systems. Its architecture allows for deep understanding of temporal video sequences in conjunction with textual descriptions, providing state-of-the-art results in video understanding tasks.\nLLaVA-OV-7B, standing for Language and Vision Analysis - OmniVision, is a 7 billion parameter language model that specializes in comprehensive visual and textual interpretation. This model integrates advanced vision capabilities with natural language processing to perform tasks like detailed image analysis, multimodal translation, and cross-modal information retrieval. LLaVA-OV-7B is trained on a vast array of multimodal data sources, enabling it to effectively understand and generate content that requires the amalgamation of visual cues with textual data."}, {"title": "A.3. Language Task Datasets", "content": "MMLU-Pro is an extension of the original MMLU dataset, designed to evaluate language models on professional-level topics across a wide array of subjects. This dataset includes complex questions that require not only language understanding but also domain-specific knowledge, ranging from medicine and law to engineering and the arts. MMLU-Pro aims to test the depth and breadth of a model's understanding of advanced topics, making it a rigorous benchmark for language comprehension.\nWinogrande is a large-scale dataset designed to improve the robustness and challenge of Winograd Schema Challenge-style tasks. It involves natural language inference tasks where the model must resolve ambiguity in sentences using common-sense reasoning. The dataset is particularly known for its difficulty and diversity, requiring models to utilize a deep understanding of context and world knowledge to make the correct inferences.\nBig-Bench-Hard is a subset of the broader BIG-bench dataset specifically curated to challenge the capabilities of language models with particularly difficult tasks. This dataset includes a variety of language-based tasks such as analogical reasoning, complex problem-solving, and advanced comprehension challenges that go beyond the typical capabilities of standard language models, pushing the limits of what AI can understand and process in textual form."}, {"title": "A.4. MultiMode Task Datasets", "content": "MMMU is a comprehensive dataset designed for evaluating the performance of multimodal models across tasks that require simultaneous understanding of text, image, and sometimes audio content. This dataset includes challenges such as cross-modal retrieval, multimodal reasoning, and synchronizing visual content with textual descriptions. MMMU aims to simulate real-world scenarios where multiple types of data must be integrated and interpreted together.\nMMStar is a multimodal dataset focused on the interplay between visual and textual data in entertainment and media contexts. It includes annotated images and videos from various media sources, coupled with descriptive texts and contextual information. The dataset is utilized for tasks such as multimedia content summarization, sentiment analysis, and thematic classification, testing a model's ability to navigate and interpret complex media-rich environments."}, {"title": "B. Experiment Details", "content": ""}, {"title": "B.1. Language Experiment", "content": "Due to the current limitations in large model evaluation techniques, there is a relative scarcity of benchmarks and datasets specifically tailored to academic disciplines. To the best of our knowledge, only the MMLU-Pro and Big-Bench-Hard datasets include manually annotated discipline-specific labels. This poses significant challenges to the experimental design of our Bench-CoE model. To thoroughly evaluate the performance of Bench-CoE, we conducted the following three types of tests:\nDuring the naive test phase, we selected the MMLU-Pro dataset, which features well-defined discipline-specific labels, for training and evaluation of the BERT model. However, since the MMLU-Pro dataset only provides validation and test sets, we conducted both training and testing on the validation set. As the experiments and evaluations in this phase were performed on the same dataset, the results primarily serve to demonstrate the basic feasibility of our proposed approach. To further evaluate the effectiveness and generalizability of Bench-CoE, we designed more sophisticated experiments, including both in-distribution and out-of-distribution tests.\nIn the in-distribution test phase, we evaluated Bench-CoE using the Winogrande dataset, which provides a clear separation between training and test sets. Specifically, we trained the Bench-CoE model on the training set of Winogrande and tested it on the corresponding test set. However, since the Winogrande dataset lacks strong discipline-specific features (e.g., no manually annotated discipline labels), it was not possible to directly assess the model's capabilities through a discipline-wise leaderboard. As a result, we focused solely on evaluating the query-level performance of the Bench-CoE model.\nIn the out-of-distribution test phase, we selected datasets with strongly defined discipline-specific features: the MMLU-Pro dataset as the training set and the Big-Bench-Hard dataset as the test set. Specifically, we trained the Bench-CoE router on the MMLU-Pro dataset and evaluated it on the Big-Bench-Hard dataset. By testing across different datasets with distinct data distributions, and with both training and test sets exhibiting clear discipline-specific characteristics, this phase allowed us to thoroughly validate the generalization capability of the Bench-CoE model at both the query-level and subject-level."}, {"title": "B.2. MultiModal Experiment", "content": "MMMU and MMStar are currently among the most comprehensive multimodal benchmarks, encompassing tasks such as cross-modal retrieval and multimodal reasoning. To thoroughly evaluate the performance of Bench-CoE on multimodal tasks, we designed experiments in three phases: naive test, in-distribution test, and out-of-distribution test.\nIn the naive test phase, we used the MMMU dataset for both training and testing the Bench-CoE router. The subset of MMMU was utilized for both training and evaluation. This phase primarily aimed to verify the basic feasibility of Bench-CoE in task allocation for multimodal tasks. By leveraging query-level and subject-level routing strategies, Bench-CoE significantly outperformed individual models, demonstrating its effectiveness in task allocation. The query-level router provided finer-grained task assignments, while the subject-level router exhibited stronger overall robustness.\nIn the in-distribution test phase, the test set of the MMMU dataset was used for training, and the validation set was used for evaluation. This setup ensured a clear separation between training and testing data while maintaining consistency in data distribution. The Bench-CoE router effectively allocated tasks to the most suitable expert models based on the input, showcasing its strong adaptability for tasks within the same distribution.\nIn the out-of-distribution test phase, the Bench-CoE router was trained on the validation set of the MMMU dataset and tested on the MMStar dataset. The MMStar is a multimodal dataset focus on the interplay between visual and textual data in entertainment and media contexts, presenting challenges to the model's generalization capabilities. The experiments demonstrated that the subject-level router remained effective in handling tasks with significant distributional differences, validating the adaptability and robustness of Bench-CoE. In contrast, the query-level router showed slightly reduced performance on new data distributions, likely due to overfitting.\nThese experimental results indicate that Bench-CoE effectively integrates the strengths of different models, achieving outstanding performance in both in-distribution and out-of-distribution tasks. This approach provides a solid foundation for further research on collaborative mechanisms in multimodal models."}, {"title": "C. Scalability of Bench CoE", "content": "In Bench-CoE, particularly in the subject-level Bench-CoE, we leverage the best-performing LLM for each domain as the routing target. By directing as many questions as possible within a given domain to the \"best\" LLM for inference, we enhance the overall accuracy of the model. However, with the rapid evolution of large language models, accompanied by the introduction of new datasets, novel models, and updated evaluation methods, the leaderboard rankings of LLMs change frequently. Under such circumstances, a fixed routing strategy in the CoE model cannot accommodate newly emerging models or adapt to shifting data distributions.\nTo address this limitation and improve the scalability of Bench-CoE, we designed a leaderboard-prior-based subject routing mechanism. Instead of directly routing inputs to a fixed best-performing model in a domain, our router first predicts the subject type of the given input. It then leverages the leaderboard-prior subject-to-model mapping to route the input to the latest and most suitable model for that domain. This approach significantly enhances the scalability of Bench-CoE, allowing it to flexibly adapt to rapidly evolving datasets and LLM advancements by dynamically adjusting the leaderboard and updating routing rules."}, {"title": "D. Scenarios Unsuitable for CoE", "content": "In our experiments with the Bench-CoE model, we selected a wide range of LLMs as candidate models and conducted extensive testing. Through these tests, we identified a common challenge in the CoE field: the issue of LLM capability diversity. Specifically, this problem arises when a candidate LLM lacks capability diversity on the given dataset either significantly outperforming or underperforming all other candidate LLMs. Such cases negatively impact the overall performance of the CoE model, as the router is forced to route all queries either exclusively to or completely away from this model to achieve optimal results. This creates a significant challenge for training the router.\nLooking ahead, we believe this issue can be mitigated with the development of dynamic routing strategies and adaptive candidate LLM selection mechanisms. These advancements will enable the CoE model to better handle capability imbalances among candidate LLMs, paving the way for more robust and flexible routing solutions."}]}