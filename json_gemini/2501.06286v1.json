{"title": "Bactrainus: Optimizing Large Language Models for Multi-hop Complex Question Answering Tasks", "authors": ["Iman Barati", "Arash Ghafouri", "Behrouz Minaei-Bidgoli"], "abstract": "In recent years, the use of large language models (LLMs) has significantly increased, and these models have demonstrated remarkable performance in a variety of general language tasks. However, the evaluation of their performance in domain-specific tasks, particularly those requiring deep natural language understanding, has received less attention.\nIn this research, we evaluate the ability of large language models in performing domain-specific tasks, focusing on the multi-hop question answering (MHQA) problem using the HotpotQA dataset. This task, due to its requirement for reasoning and combining information from multiple textual sources, serves as a challenging benchmark for assessing the language comprehension capabilities of these models.\nTo tackle this problem, we have designed a two-stage selector-reader architecture, where each stage utilizes an independent LLM. In addition, methods such as Chain of Thought (CoT) and question decomposition have been employed to investigate their impact on improving the model's performance.\nThe results of the study show that the integration of large language models with these techniques can lead to up to a 4% improvement in F1 score for finding answers, providing evidence of the models' ability to handle domain-specific tasks and their understanding of complex language.", "sections": [{"title": "Introduction", "content": "In recent years, LLMs have become one of the most significant achievements in natural language processing, demonstrating exceptional performance in a wide range of general language tasks, such as translation, summarization, and text generation. These models leverage attention-based architectures, an enormous number of parameters, and training on diverse and extensive datasets, offering capabilities beyond traditional natural language processing methods, such as instruction following and in-context learning.\nHowever, existing evaluations are often based on general benchmarks and employ zero-shot or few-shot approaches. While these methods showcase the overall capabilities of the models, they do not provide a thorough and in-depth analysis of their performance on specific tasks. In other words, these evaluations do not compare the models' capabilities to traditional methods or models optimized for a particular task. Therefore, investigating how to improve the performance of language models on domain-specific tasks and identifying their limitations remains an important and underexplored research gap.\nThe goal of this paper is to evaluate the performance of large language models on a specific task. In this context, we have chosen to examine MHQA, which is one of the most complex tasks in natural language understanding. This task requires the extraction and reasoning of information from multiple sources and, due to its complexity, provides a comprehensive evaluation of a model's capabilities.\nTo address this challenge, we have employed a two-stage selector-reader architecture called Bactrainus. In the selector stage, the model's ability to retrieve and extract relevant information is evaluated, while in the reader stage, the model's ability to reason and perform in-context learning is assessed. Additionally, we explore whether dividing the task into smaller sub-tasks and assigning each to a separate language model can help improve performance.\nFurthermore, we investigate the effects of knowledge distillation. To this end, we employ question decomposition in the selector stage and chain of thought in the reader stage to assess how these techniques can impact the reasoning and answering process."}, {"title": "Related Works", "content": "Given the focus of this research-multi-hop question answering (QA) using LLMs\u2014a wide range of previous studies could be deemed relevant. However, to maintain clarity and concentrate on essential aspects, this section covers only those works that are methodologically close to our approach or offer a deeper understanding of the topic. We begin with a review of QA datasets, especially those emphasizing multi-hop scenarios, then move on to the main components of open-domain QA systems, and finally discuss LLM-based methods for multi-hop QA."}, {"title": "QA Datasets", "content": null}, {"title": "Single-hop QA Datasets", "content": "In the early stages of machine reading comprehension MRC, most work featured single-hop QA data in which the answer was located within a short paragraph. Examples include:\nCNN/Daily Mail (Hermann et al., 2015), one of the first large-scale datasets, which overcame the data-scarcity challenge by automatically generating multiple questions from news articles.\nSQUAD (Rajpurkar et al., 2016) introduced over 100,000 human-written questions based on Wikipedia articles and later strengthened supervised learning by adding unanswerable questions (Rajpurkar et al., 2018).\nSQUADopen (Chen et al., 2017) expanded SQuAD1.1 for evaluation in an open-domain QA setting-omitting an explicit paragraph so that a model must locate the relevant document."}, {"title": "Multi-hop QA Datasets", "content": "With the emergence of multi-hop QA, the challenge of chaining information from multiple documents or paragraphs became central. Notable datasets include:\nHotpotQA (Yang et al., 2018), which not only provides multi-step questions (bridge and comparison) but also introduces a distractor configuration that mixes irrelevant paragraphs, requiring more rigorous reasoning.\n2WikiMultihopQA (Ho et al., 2020) merges structured and unstructured data to create inferential and comparative questions, while similarly adding distractor paragraphs.\nMuSiQue (Trivedi et al., 2021), which forms more complex multi-hop questions (2\u20134 hops) and increases the diversity of reasoning graphs, making simple single-step approaches insufficient.\nHybridQA (Chen et al., 2020) and OpenBookQA (Mihaylov et al., 2018) integrate table-based or scientific knowledge with text to enable multi-step reasoning.\nQASC (Khot et al., 2020) focuses on scientific facts, demanding high-quality retrieval and fact composition."}, {"title": "Main Components of Open-Domain QA Systems", "content": "Open-domain QA systems must retrieve relevant documents from a large repository rather than relying on a single, explicit paragraph. The pivotal architecture came from Chen et al. (2017), who introduced two modules\u2014a retriever and a reader-later adopted by systems like DrQA (Chen et al., 2017). In DrQA, a classic retrieval technique (e.g., TF-IDF) narrows down the candidate documents, and a neural reading model (initially a convolutional network) pinpoints the precise answer.\nWith the rise of pre-trained language models such as BERT (Devlin et al., 2019) and ELECTRA (Clark et al., 2020), both the retriever and reader modules evolved. Dense retrievers (Karpukhin et al., 2020; Das et al., 2019; Lee et al., 2019) achieve deeper semantic alignment between question and text. Adaptive retrieval (Kratzwald & Feuerriegel, 2018) aims to select an optimal number of documents based on each query, while answer verification (Zhang et al., 2020) handles invalid or unanswerable questions."}, {"title": "Large Language Models in Multi-hop QA", "content": "Recent developments in large language models (LLMs)\u2014such as GPT variants, T5, or BART-have introduced novel methods beyond the traditional retriever-reader paradigm:\nLLM-Enhanced Retrieval: Sometimes LLMs summarize lengthy passages and select relevant segments (Nair et al., 2023). A primary constraint is the limited context window size.\nChain of Thought: Studies (Han et al., 2022; Zelikman et al., 2022; Wang et al., 2019; Saparov & He, 2023) show that prompting an LLM to generate step-by-step reasoning can improve multi-hop QA. Projects like PathFiD (Yavuz et al., 2022) and IRCOT (Trivedi et al., 2023) rely on iterative evidence construction.\nQuestion Decomposition: Several approaches (Khot et al., 2023; Zhou et al., 2023; Zhou et al., 2022; Deng et al., 2022; Wu et al., 2024) break down a complex question into simpler sub-questions, often with LLM-driven generation or refinement, facilitating multi-step inference."}, {"title": "Experimental Setup", "content": "This section details the dataset and its preparation, followed by model configurations and implementation specifics. Finally, the hardware specifications and utilization strategy are presented to ensure reproducibility."}, {"title": "Dataset", "content": "We utilize the HotpotQA dataset under the Distractor setting to evaluate model performance in MHQA. In this configuration, each question is accompanied by 10 candidate paragraphs, 2 of which are gold paragraphs that directly contribute to the answer. The key challenge is to accurately identify the supporting facts from the irrelevant information. To align this dataset with LLMs, the training samples are converted into the Alpaca format (designed for Supervised Fine-tuning)."}, {"title": "Implementation Details", "content": "There are two primary approaches for examining the impact of different settings on model performance. In the first approach, each model is individually tuned with specialized hyperparameters and a system prompt, and the final results are compared. In the second approach, all models operate under a single, uniform configuration to enable a fair comparison under consistent conditions. In this study, we adopt the second approach and only introduce limited modifications to the system prompt in certain experiments.\nSince the primary objective in multi-hop QA is to obtain precise and coherent answers, the following configurations are applied to generate model outputs:\nTemperature = 0.01\nTop-p = 0.8"}, {"title": "Hardware", "content": "All experiments were conducted on a server equipped with 4 A100 GPUs (80 GB memory each). In some cases requiring larger models or extensive parallelization, all four GPUs were utilized; however, for most experiments, no more than two GPUs were necessary. This computational infrastructure significantly reduced training and inference time and enabled experimentation with larger-scale models."}, {"title": "Methodology", "content": "This section describes the main architecture adopted for multi-hop question answering, followed by the proposed knowledge distillation techniques aimed at enhancing model performance. The primary goal is to improve accuracy and reliability by dividing the complex task into manageable sub-modules and leveraging high-level knowledge transfer."}, {"title": "Selector-Reader Architecture", "content": "To systematically investigate the model's performance, We utilize a two-stage selector-reader architecture, as illustrated in Figure 1. This design comprises two main components:\nSelector: Identifies and extracts the supporting facts from among multiple candidate paragraphs (including the \"gold paragraphs\" and distractors).\nReader: Utilizes the selector's output (i.e., the chosen paragraphs or sentences) to answer the main question.\nIn other words, the selector focuses on retrieving relevant text from extensive contexts, requiring the ability to understand inter-document relationships and the chaining of information for multi-step queries. Subsequently, the reader component leverages multi-hop reasoning and in-context learning techniques to synthesize the extracted evidence and derive the final, correct answer.\nWe hypothesize that separating the complex QA task into two independent sub-tasks can lead to improved performance. This separation allows each component to concentrate on its respective objective without entangling both tasks. The effectiveness of this assumption is evaluated in the Results and Evaluation section."}, {"title": "Knowledge Distillation", "content": "A key strategy for improving multi-hop QA performance is knowledge distillation, which posits that high-level reasoning or step-by-step logic can be transferred from one large language model (Teacher) to another (Student). In this study, we explore two main approaches for knowledge distillation:\nDistilling Knowledge During Fine-tuning:\nIn this approach, additional outputs containing step-by-step reasoning or auxiliary information (e.g., a chain of thought) are produced by a more capable model and presented as targets during training. As the student model attempts to reproduce this detailed reasoning, it internalizes higher-level knowledge.\nDistilling Knowledge at Inference Time:\nHere, supplementary hints or step-by-step explanations are directly appended to the model's input during inference, providing immediate guidance on how to approach the solution. This additional context aids the student model in generating more accurate answers.\nIn our implementation, the \u201cchain of thought\" in the reader component and \"question decomposition\" in the selector component play pivotal roles:\nReader Component: The chain of thought-representing the stepwise logic linking relevant evidence is appended to the model's output during fine-tuning. This enables the model to learn the structure of reasoning and produce more precise answers.\nSelector Component: Complex, multi-step questions are decomposed into simpler sub-questions by a separate language model. These sub-questions are then fed into the selector, enhancing its ability to pinpoint and retrieve supporting paragraphs more efficiently. Consequently, the workload on the reader is reduced, leading to improved final accuracy."}, {"title": "Experiments and Results", "content": "In this section, each component of the proposed Selector-Reader architecture is evaluated independently to demonstrate its contribution to multi-hop question answering. Subsequently, these components are integrated to address the primary objective of the HotpotQA dataset-namely, determining both the correct answer and its associated supporting facts for complex multi-hop questions."}, {"title": "Evaluating the Reader Component", "content": null}, {"title": "Zero-Shot prompt", "content": "To assess the performance of a LLM acting as the \"reader,\" we first assume that a perfect \"selector\" has already provided the necessary supporting facts with no errors. In other words, we have a guaranteed set of correct supporting evidence for each question. This setup allows us to isolate and measure the LLM's intrinsic ability to perform multi-hop inference and extract an answer from the given text.\nInitially, we evaluate several prominent models in a zero-shot configuration. In this scenario, the model does not receive any additional in-context examples or fine-tuning instructions specifically designed for the multi-hop task. By comparing multiple closed-source and open-source LLMs, we identify the most promising candidates for further experiments.\nAnalysis of Results:\nAmong all models tested, GPT-40 yields the highest Exact Match (67.54) and F1 (83.44). However, due to its closed-source nature, limited access for fine-tuning, and high associated costs, it was excluded from subsequent experiments.\nAmong models with fewer than 10 billion parameters, Llama3.1 8B Instruct achieves the best performance (EM = 60.11, F1 = 74.52)."}, {"title": "Effect of the Number of Supporting Facts", "content": "To gain deeper insights into how the reader performance of LLMs might be influenced by the complexity of questions, we examine the number of supporting facts associated with each sample in HotpotQA. Intuitively, questions with more supporting facts are often presumed to require more multi-hop reasoning, suggesting they could be more challenging. Here, we test that assumption by analyzing model accuracy across varying numbers of supporting facts.\nFigure 3 visualizes these results. Contrary to the initial assumption, a higher number of supporting facts does not necessarily yield lower performance. Some models achieve better results when faced with four or more supporting facts. Notably, however, the performance gap between smaller and larger models tends to widen at higher numbers of supporting facts, suggesting that larger models have a stronger capacity for complex multi-hop reasoning."}, {"title": "Investigating Model Dependence on Supporting Facts and Input Size", "content": "Having selected Llama 3.1 8B Instruct and Llama 3.1 70B Instruct as our primary reader models, we explore two key questions:\nDoes the language model inherently \u201cknow\u201d the answer without any supporting facts, or does it genuinely rely on these facts?\nIn other words, can the model answer HotpotQA questions with only the question text, or must it derive the solution from the supporting facts?\nHow does the amount of provided text (beyond supporting facts) influence reader performance?\nSpecifically, if we supply additional paragraphs-some of which may be irrelevant to the model instead of only the minimal supporting facts, will accuracy be affected?"}, {"title": "Performance in the \u201cQuestion Only\" Setting", "content": "To address the first question, we designed an experiment where the model is given only the question with no supporting context.\nThe outcome underscores that neither model possesses adequate internal knowledge to answer most HotpotQA questions. Instead, they rely substantially on the provided supporting evidence."}, {"title": "Effect of Input Size on In-Context Learning", "content": "Next, to address the second question regarding input size, we conducted four experiments differing only in how much text is fed to the model:\n1) Supporting facts only (baseline)\n2) Gold paragraphs\n3) Gold paragraphs + 2 distractors\n4) All paragraphs in HotpotQA\nTo select the two distractor paragraphs, we employed the sentence transformer gte-large-en-v1.5 calculating similarity between the question and all non-gold paragraphs, then choosing the two most semantically similar paragraphs as distractors (Figure 4).\nThe final results, shown in Table 3 (above) and Figure 5, indicate that transitioning from \"Supporting Facts Only\" to \"Gold Paragraphs\" does not drastically alter the scores, implying these paragraphs are not substantially different or misleading. However, adding two distractor paragraphs leads to a noticeable performance decline, and using all paragraphs yields the most significant drop. Hence, the model cannot effectively isolate the necessary evidence when large amounts of irrelevant text are present, and the lengthy input confuses the in-context learning process.\nNecessity of Targeted Retrieval: These experiments affirm that delivering concise, high-fidelity input (supporting facts or gold paragraphs) to the reader is crucial for success.\nValidating the Initial Hypothesis: Splitting the QA task into independent sub-tasks (selector and reader) and allocating each to a separate LLM appears beneficial, at least in zero-shot settings."}, {"title": "Impact of Few-Shot Prompting and Chain of Thought", "content": "To further explore how multi-hop question answering might be enhanced by large language models (LLMs), we investigate two additional factors:\nFew-shot prompting: Does providing multiple examples (e.g., one, two, four, or eight shots) help the model better understand and respond to complex questions?\nChain of Thought: Can explicitly showing step-by-step reasoning in the provided examples boost the model's multi-hop inference capabilities?\nWe first examine the effect of varying the number of examples in the prompt-ranging from zero-shot to one-shot, two-shot, four-shot, and eight-shot\u2014for two Llama 3.1 models with 8B and 70B parameters. These examples were selected from high-difficulty questions in HotpotQA, ensuring coverage of both \"bridge\" and \"comparison\" types.\nObserving Table 4, the one-shot configuration yields the best performance in both models. Providing a single example helps orient the model towards the task requirements; however, adding more examples does not consistently improve accuracy. In some cases (e.g., two-shot), performance even dips, possibly due to overfitting to the examples or confusion arising from multiple demonstrations. After two-shot, performance recovers slightly with four and eight examples but remains near or below the one-shot peak (Figure 6).\nIn the second experiment, we repeated the above few-shot variations but appended chain-of-thought explanations to the prompt examples. These step-by-step rationales were generated by Llama 3.1 70B itself. Specifically, the model was given a question, supporting facts, and the final answer, then asked to describe how it arrived at that answer.\nOptimal Shot Count: One-shot prompting tends to yield the highest performance, whereas introducing additional examples can sometimes cause confusion.\nChain of Thought: For the smaller (8B) model, chain-of-thought prompts slightly impair performance, but for the larger (70B) model at higher shot counts, they yield a minor improvement.\nOverall Insight: While few-shot prompting and step-by-step reasoning can help in certain scenarios, they may also introduce noise or complexity. The type, number, and quality of examples need careful tuning to achieve consistent gains."}, {"title": "Fine-tuning", "content": "Following the zero-shot and few-shot experiments, we now aim to fully leverage the potential of Llama 3.1 Instruct 8B and Llama 3.1 Instruct 70B for multi-hop question answering by performing fine-tuning on the HotpotQA dataset.\nTo do this, we convert the training data into an instructional format and apply the LoRA approach (due to limited computational resources) to efficiently fine-tune the models.\nAs suggested in Section 4-2, generating auxiliary reasoning traces (chain of thought) via another large language model can facilitate knowledge transfer. We explore two main scenarios:\nChain of Thought from an 8B Model\nHere, Llama 3.1 8B is provided with the question, supporting facts, and final answer, then asked to outline the step-by-step reasoning process. The reader model (also 8B) is fine-tuned to reproduce both the final answer and the chain of thought (Figure 7).\nChain of Thought from a 70B Model\nIn this scenario, Llama 3.1 70B generates reasoning traces for the more challenging samples of the training set. The 8B reader-previously fine-tuned only on direct answers\u2014undergoes continual fine-tuning with these newly generated traces from the 70B model."}, {"title": "The Selector Component", "content": "In MHQA, having a capable reader alone is insufficient. The system must also identify which paragraphs or sentences contain the supporting facts needed to answer the query. This stage is referred to as the selector. Its primary goal is to determine which subset of the documents are relevant and, within those, which sentences constitute the supporting evidence.\nMost previous studies have employed traditional information retrieval (IR) methods or encoder-only architectures for the selector. In contrast, we leverage LLMs to handle the selection of supporting facts, given that the HotpotQA dataset contains a limited number of candidate documents for each question. The central hypothesis is that the deep textual understanding and in-context learning capabilities of an LLM can potentially outperform standard IR solutions in identifying relationships between complex questions and relevant documents."}, {"title": "Fine-tuning the Selector", "content": "Because LLMs are generally not trained for retrieval, and because the selector's output must conform to highly specific and rigid evaluation metrics, standard zero-shot or few-shot prompts alone are inadequate to enforce the strict output format required. Consequently, we adopt a fine-tuning (supervised) approach. We convert the HotpotQA dataset into an instruction-based format, wherein the model takes a multi-hop question plus all associated documents as input and is trained to produce the supporting facts or gold paragraphs.\nAs with the reader module, we refer to each fine-tuned model in the selector stage as Bactrainus. Due to hardware constraints, we employed only Llama 3.1 Instruct 8B in this component.\nLike the reader component, we use LoRA to fine-tune LLMs, allowing selective training of certain parameters."}, {"title": "Two-stage Architecture", "content": "Given that the single-stage model performs quite well for paragraph-level selection but leaves room for improvement in sentence-level retrieval, we explore a two-stage approach:\nParagraph Selector: Identifies which paragraphs are gold.\nSentence Selector: Extracts supporting facts from among the selected paragraphs.\nFigure 8 illustrates the overall two-stage design, and Figure 9 depicts the fine-tuning procedure for training two separate models. Contrary to initial expectations, the results in Table 9 show no major gains over the single-stage method likely because splitting the task removes some valuable cross-information that exists between the paragraph and sentence levels."}, {"title": "Question Decomposer", "content": "To enhance sentence-level accuracy, we introduce auxiliary sub-questions:\nAfter identifying gold paragraphs, sub-questions are generated (especially for more difficult queries) by a larger LLM (e.g., Llama 70B). These sub-questions, along with the original question, are fed into an 8B question-decomposer model fine-tuned to interpret them, thereby helping the sentence selector isolate the supporting facts more effectively.\nFinally, as shown in Figure 10, these sub-questions are supplied to the sentence selector. Table 9 indicates a moderate improvement in identifying supporting facts (EM=65.93, F1=89.63), but not a major leap.\nThe single-stage approach using Llama 3.1 Instruct 8B demonstrates near state-of-the-art results in paragraph selection, suggesting that LLMs can be effectively used for retrieval in limited-scale datasets."}, {"title": "Integrating the Selector and Reader", "content": "Having examined the selector and reader components separately, we now combine them to solve the distractor setting of the HotpotQA dataset end-to-end. The goal is to identify both the supporting facts and the final answer for multi-hop questions, then compare the results to existing methods. Figure 11 illustrates six possible ways to integrate the selector and reader, each differing in how the outputs of one component feed into the other and in the detailed configuration of these modules. Additionally, in each scenario (except for the first), the reader can be any of the fine-tuned Bactrainus models described in Section 5-1."}, {"title": "Conclusion", "content": "This study examined the capabilities of LLMs for MHQA and proposed a multi-component system comprising a selector and a reader. We conducted comprehensive experiments on the HotpotQA dataset under the distractor setting, leading to the following key insights:\n1) Effectiveness of LLMs as a Selector\nDespite the conventional assumption that LLMs are not typically used for retrieval, our fine-tuned approaches demonstrated that they can effectively identify gold paragraphs and supporting sentences at near state-of-the-art levels. Even the single-stage selector performed competitively, highlighting the strong inherent understanding these models possess and the natural interdependence between paragraph and sentence selection in multi-hop data.\n2) Improving Reader Performance via Model Scale and Chain of Thought\nOur experiments revealed that increasing model size (e.g., from 8B to 70B parameters) and adding CoT prompts\u2014either directly or through guidance from a larger teacher model-enhanced multi-hop reasoning and final accuracy. While not every scenario showed dramatic gains, these findings confirm that knowledge transfer, in the form of structured step-by-step reasoning, can be crucial for elevating model performance on complex questions.\n3) Advantages of a Modular Approach Over a Single-Model Setup\nComparing a single all-in-one model (handling both selection and reading jointly) with a modular design (separating the selector and reader sub-tasks) indicated that breaking down the multi-hop QA process generally yields around a 2% improvement in both supporting-fact identification and final answers. This underscores the benefit of task decomposition and separate optimization for each component (selector-reader), as opposed to a monolithic end-to-end method.\n4) Comparison with Prior Methods and Achieving Superior Results on HotpotQA"}, {"title": "Future Works", "content": "Although our results highlight the high potential of large language models in multi-hop question answering, there remain significant research challenges and open questions for further advancements. Some promising directions include:\n1) Scalability and Computational Optimization\nScaling models beyond 70B parameters or using ensembles of multiple models could significantly improve reasoning capabilities. Nevertheless, computational costs and hardware limitations pose major constraints. Exploring model compression, distributed computing, and cost-effective techniques (e.g., LoRA, Adapters) remains crucial.\n2) Generalization Across Languages and Domains\nOur study primarily focused on English data from the HotpotQA dataset. Extending and adapting the proposed methods to other languages and question types-especially in specialized fields (e.g., legal or medical)\u2014is vital for assessing the broader applicability of our approach.\n3) Interactive and Adaptive Learning Approaches\nIn practical QA systems, users may pose follow-up questions in a conversational format. Designing multi-hop models capable of integrating immediate user feedback and refining their answers dynamically is a compelling topic for future research.\n4) Enhanced Monitoring and Interpretation of Answers"}]}