{"title": "Fake News Detection and Manipulation Reasoning via Large Vision-Language Models", "authors": ["Ruihan Jin", "Ruibo Fu", "Zhengqi Wen", "Shuai Zhang", "Yukun Liu", "Jianhua Tao"], "abstract": "Fake news becomes a growing threat to information security and public opinion with the rapid sprawl of media manipulation. Therefore, fake news detection attracts widespread attention from academic community. Traditional fake news detection models demonstrate remarkable performance on authenticity binary classification but their ability to reason detailed faked traces based on the news content remains under-explored. Furthermore, due to the lack of external knowledge, the performance of existing methods on fact-related news is questionable, leaving their practical implementation unclear. In this paper, we propose a new multi-media research topic, namely manipulation reasoning. Manipulation reasoning aims to reason manipulations based on news content. To support the research, we introduce a benchmark for fake news detection and manipulation reasoning, referred to as Human-centric and Fact-related Fake News (HFFN). The benchmark highlights the centrality of human and the high factual relevance, with detailed manual annotations. HFFN encompasses four realistic domains with fake news samples generated through three manipulation approaches. Moreover, a Multi-modal news Detection and Reasoning langUage Model (M-DRUM) is presented not only to judge on the authenticity of multi-modal news, but also raise analytical reasoning about potential manipulations. On the feature extraction level, a cross-attention mechanism is employed to extract fine-grained fusion features from multi-modal inputs. On the reasoning level, a large vision-language model (LVLM) serves as the backbone to facilitate fact-related reasoning. A two-stage training framework is deployed to better activate the capacity of identification and reasoning. Comprehensive experiments demonstrate that our model outperforms state-of-the-art (SOTA) fake news detection models and powerful LVLMs like GPT-4 and LLaVA.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of online media greatly improves the convenience of information communication. On the contrast, recent years witness a rampancy of disinformation, which poses threat to information security and public opinion. News is at enormous risk of manipulation for being a common carrier of multi-modal information, which draws attention within the academia community and various fake news detection methods are proposed. Early works on fake news detection prioritize the identification of uni-modal manipulation. Currently, with the advent of deep generative models, media manipulation expands across multiple modalities. Visual deepfake models can edit human faces and generate high-fidelity images and videos [24, 37]. With large language models (LLMs) like BERT [8] and GPT [23], lexical replacement and editing is performed easily to modify semantics and facts. Media manipulation enhances the difficulty of detection and has a more detrimental social impact when it targets human-centric and fact-related news."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 Media Manipulation.", "content": "Disinformation becomes a growing threat to information security and public opinion with the rampancy of media manipulation. Media manipulation methods varies across different modalities. In visual modality, GAN-based methods are widely employed to manipulate human faces with text-guidance [19, 21] or latent space editing [30, 36]. [24] utilizes the multi-modal semantics to guide the editing process. [37] enables high-fidelity image inversion and attribute editing by a distortion consultation approach. In textual modality, common manipulation methods include conditional text generation [2, 28] and text style transfer [33, 35]. Recent progress in natural language generation gives rise to large-scale manipulable text [6]. Manipulations toward human-centric and fact-related news may cause harmful impact to society. In our work, by applying off-the-shelf manipulation methods, we build a multi-modal fake news benchmark following the principle of \"human-centric\" and \"fact-related\" to evaluate detection and reasoning."}, {"title": "2.2 Fake News Detection.", "content": "Fake news detection draws great attention as news is at enormous risk of multi-modal manipulation. Social context based detection methods judge on the authenticity of news based on the spreading procedure such as social network [22] and post-user interaction [20]. Content-based methods differentiate fake and real news by finding manipulation cues [25, 26]. Recent researches focus on identifying multi-modal news. [39] proposes an effective textual and visual feature fusion method with co-attention. [3, 40] leverage the adaptable aggregation between uni-modal and cross-modal features to resolve the inherent ambiguity across different modalities. [13] introduces LLM as a data augmentation approach to generate advisable rationales for subsequent detection. Different from aforementioned methods, we propose a novel architecture combining feature extraction and LVLM for fake news detection and manipulation reasoning. To our best knowledge, we are the first to employ LVLM as the backbone model for fake news detection."}, {"title": "2.3 Large Vision-Language Models.", "content": "Expanding the multi-modal capability of LLMs is a current research focus. [16] employs Flan-T5 [5] with a Q-Former to bridge the modality gap between visual feature and language model. [34] leverages the combination of ImageBind [10] and Vicuna [4] to deal with multi-modal input. By instruction tuning on multi-modal instruction-following data generated by GPT-4 [1], [17, 18] achieve impressive cross-modal chat abilities. Despite the general knowledge derived from large-scale pre-training, these models lack domain-specific expertise. To better prompt LVLMs with manipulation detection expertise, we introduce a multi-level prompt learner to enhance manipulation reasoning. Fig.1 exhibits M-DRUM outperforms existing forgery detectors and LVLMs with profound manipulation detection expertise and broad general knowledge."}, {"title": "3 HFFN: HUMAN-CENTRIC AND FACT-RELATED FAKE NEWS BENCHMARK", "content": null}, {"title": "3.1 Design Principles", "content": "Human-Centric Human-centric news carries a higher risk of manipulation than other news topics. High-fidelity deepfake models can perform face swap and facial attribute editing easily, posing harmful threats to visual authenticity. Human centric news is highlighted in the construction of our benchmark, which means we pay higher attention to news samples with clear human faces. Images with no faces or blurred faces are filtered out. To simulate potential image manipulations, face swap and facial-attribute editing are both conducted to create fake images.\nFact-Related Factual errors are common in media manipulation, which result in misleading public opinion and negative social impact. Traditional fake news detection methods have difficulty distinguishing factual errors due to the lack of general knowledge or external knowledge source. There is a strong demand to measure whether a detection model is capable of tackling factual factual manipulation. Our benchmark is tailored for LVLMs and contains sufficient fact-related news samples. During data collection, we gather news featuring celebrities and well-known events as we believe these news is at a higher risk of being factually manipulated. After collection, random factual errors are added to the news to examine the capacity of the detection model."}, {"title": "3.2 Construction Process: Data Collection", "content": "The original news samples are gathered from latest real-world media sources. Among them, human-centric and fact-related samples get the most attention. Following the design principles, news without clear human faces or high factual relevance is screened out. The screening process is carefully conducted by multiple volunteers. To enhance validity, we calculate the image-text consistency of news by CLIP [27]. News with low image-text consistency are removed to improve the validity of the benchmark. To this end, the original news samples set $O = \\{I_{real}, T_{real}\\}$ is obtained."}, {"title": "3.3 Construction Process: Media Manipulation", "content": "Image Manipulation Inspired by manipulation procedure of DGM\u2074, we achieve image manipulation with face swap and face editing. Face swap manipulation refers to replace the main character's face with another person's. InfoSwap [9] is adopted to swap faces by replacing the largest face $f_o$ in the original image $I_o$ with a random source face $f_{swap}$ from CelebA-HQ dataset [14]. Face editing manipulation refers to modify the facial attributes of the main character. For example, we intentionally put a smiling face or render an exaggerated beard on his/her face. We achieve high-fidelity editing effects with a GAN-based method [37] to transfer the original face $f_o$ into target style $f_{edit}$. In both ways, the manipulated face is stitched back to the original image $I_o$ and the manipulated image $I_s$ is get. The bounding box of the manipulated region $b = \\{x_1, y_1, x_2, y_2\\}$ is recorded as annotation.\nText Manipulation In text manipulation, the textual semantics are attacked by word substitution. Assisted by ChatGPT [23], words in the input headline $T_o$ are revised to reverse the global semantics of the text. For example, an original headline of \"Liu Xiang returns triumphantly and receives heated extolling\" is altered as \"Liu Xiang returns triumphantly and receives harsh questioning\". Therefore, the global semantic of the headline is reversed.\nFactual Manipulation Among various factual manipulation methods for text, entity replacement is one of the most common and convenient, which is adopted to create samples with factual errors. Specifically, given the input headline $T_o$, a Named Entity Recognition(NER) model [7] is launched to extract the name of the main entity. The extracted name is replaced with a randomly chosen name subsequently. We record the manipulated text derived from text manipulation and factual manipulation as $T_s$.\nThree uni-modal manipulation approaches mentioned above are conducted on the original dataset $O$ by randomly alter the original samples $I_o, T_o$ with manipulated ones $I_s, T_s$. A total of five manipulation types are formed, including three uni-modal types (Image, Text and Fact) and two cross-modal types(Image&Text and Image&Fact)."}, {"title": "3.4 Construction Process: Human Annotation", "content": "In HFFN benchmark, we annotate the multimodal news samples with detailed evidence for manipulation reasoning. Annotating HFFN is a challenging task, which requires annotators to endow with background knowledge in the relevant field and provide analytical reasoning based on details of news. We hire 10 professional annotators to annotate the reasoning process with following steps:\n1) Authenticity annotation. Point out the authenticity of the in terms of \"Yes, the news is real\" or \"No, the news is fake\", given the manipulation type of the news."}, {"title": "3.5 Overview of HFFN", "content": "The overall statistics of HFFN are visualized in Fig.2. In line with the design principles, HFFN benchmark attaches great significance to the human-centric and fact-related news. HFFN consists a total of 655 samples, encompassing four realistic domains: entertainment, sport, politics and others. Each news sample is represented as an image-text pair, equipped with detailed manual annotation. The average length of manual annotations is 69.1 tokens. The overall manipulation rate of HFFN is 68.4%, including 7.8% of multi-modal manipulation samples."}, {"title": "4 M-DRUM: MULTI-MODAL NEWS DETECTION AND REASONING LANGUAGE MODEL", "content": "To address fake news detection and manipulation reasoning, as illustrated in Fig.3, we present M-DRUM, a novel large vision-language model based architecture. In M-DRUM, we use a multi-modal encoder to extract visual and textual features from news images and headlines. We leverage a cross-attention mechanism to obtain multi-modal fusion features. A prompt learner bridges the gap between manipulation expertise and the general knowledge of LVLM and based on that, a LVLM generates the analytical reasoning. The model is trained under a two-stage framework to strengthen the capacity of identification and reasoning."}, {"title": "4.1 Multi-modal Feature Extraction", "content": "Driven by the idea of multi-modal alignment, we use ImageBind [10], a powerful cross-modal alignment model as the feature encoder. Given the news image $I \\in R^{H \\times W \\times C}$ and the corresponding headline T, we firstly extract visual and textual features with ImageBind. Inspired by AnomalyGPT [11], we obtained 4 intermediate visual features $F_{image}^i \\in R^{H_i \\times W_i \\times C_{image}}$ from encoding at different depths, where i indicates the i-th depth. Accordingly, the textual feature $F_{text} \\in R^{C_{text}}$ is extracted from the headline.\nTo thoroughly comprehend multi-modal inputs, a cross-modal fusion is adopted to integrate uni-modal features. Focusing on the cross-modal relationship, the cross-modal features $F_{cross}^i \\in R^{C_{fusion}}$ are obtained by calculating the cross-attention score between the rectified visual feature and the textual feature in the softmax-free linear attention [15] expressions. The cross-modal fusion process can be represented as:\n$F_{image}^i =LinearLayer(F_{image}^i)$,\n$F_{cross}^i = F_{image}^i \\oplus F_{text}$"}, {"title": "4.2 Manipulation Reasoning", "content": "In M-DRUM, a LVLM serves as a mighty knowledge base for reasoning generation. To prompt LVLM with the authenticity of the news and inspire the general knowledge, we design a hybrid prompt learner to bridge the gap between the manipulation expertise and the general knowledge of LVLM. The prompt learner aims to assist the LVLM in understanding the manipulation information of multi-modal news comprehensively. As shown in Fig.3, the prompt learner integrates three parts of information. The fusion feature $F_{fusion}$ is transformed into prompt embeddings with a converter. A prediction head is introduced to provide specific guidance to the conversion process, supervised by the authenticity label of the news, together with bounding box of the edited regions. We expect the LVLM to accept semantic information from news images as much as possible so that it can be combined with facts for reasoning. To better transform the semantics information, we leverage a semantic learner to derive visual semantics from the visual features $F_{image}$ in the form of prompt embeddings. Additionally, to learn specific prompts for manipulation reasoning in a self-adaptive way, learnable prompt embeddings are adopted. The multi-level prompts are fed to the LVLM to raise analytical reasoning about potential manipulations. The global function of the prompt learner is:\n$E_{prompt} = Concate(C(F_{fusion}|H(F_{fusion})), L(F_{image}), E_{ada})$, where C, H, L stand for the converter, the prediction head and the semantic learner respectively. $E_{ada}$ stands for the self-adaptive prompt embeddings."}, {"title": "4.3 Loss Functions for Performance Augmentation", "content": "To augment fake news detection and reasoning, we constrain our model with three types of loss functions: cross-entropy loss, bounding box loss and GIoU loss.\nCross-entropy Loss Cross-entropy loss is widely used in classification and natural language generation tasks. For authenticity classification, cross-entropy loss is introduced to supervise the prediction head in the prompt learner, which is defined as:\n$L_{CE} = \\frac{1}{B} \\sum_{i=1}^{B} [y_i log(p_i) + (1-y_i) log(1 - p_i)]$, where B is the batch size, $y_i$ is the authenticity label of the i-th sample and $p_i$ is the probability for positive prediction.\nFor manipulation reasoning, cross entropy loss quantifies the disparity between the generated reasoning and the target text sequence, which is defined as:\n$L_{LLM} = - \\frac{1}{n} \\sum_{i=1}^{n} y_i log(p_i)$, where n is the number of tokens, $t_i$ is the ground truth label for token i and $q_i$ is the predicted probability for token i.\nBounding box Loss We utilize L1 loss to supervise manipulated regions predicted by the prediction head, which is defined as:\n$L_{bbox} = \\frac{1}{B} \\sum_{i=1}^{IM} |b_{pred} - b_{gt}|$, where B is the batch size, $b_{pred}$ and $b_{gt}$ is the predicted and true bounding boxes respectively.\nGIoU Loss Intersection over Union(IoU) loss is commonly used in object detection tasks with scale invariance. GIOU [29] serves as an improvement of IoU by optimizing in the case of non-overlapping bounding boxes, which is defined as:\n$L_{GIOU} = 1-GIoU = 1 - (IoU - \\frac{A-U}{A_c})$, where A and U is the smallest enclosing box and the union area of the predicted and the true bounding boxes respectively. We introduce bounding box loss and GIoU loss to assist our model in locating manipulated regions and understanding visual semantics. The global loss function can be calculated by the weighting of each loss functions:\n$L = \\alpha L_{CE} + \\beta L_{LLM} + \\gamma L_{bbox} + \\delta L_{GIOU}$, where $\\alpha, \\beta, \\gamma, \\delta$ are hyper-parameters."}, {"title": "4.4 Two-Stage Training Process", "content": "To better combine the capabilities of multi-modal feature extraction and manipulation reasoning, we adopt a two-stage framework to train M-DRUM, refers to detection learning and reasoning learning.\nDetection Learning In the detection learning stage, we set the face encoder and the prompt learner to be trainable. We train our model on the large-scale multi-modal media manipulation dataset DGM4 [32]. During the training process, we expect the model to improve the performance of authenticity classification in large-scale detection task, which serves as the basis for subsequent manipulation reasoning.\nReasoning Learning In the reasoning learning stage, only the prompt learner is trainable. The training is launched on a delicately annotated human-centric and fact-related fake news detection benchmark HFFN. At this stage, we expect our model to improve analysis and reasoning abilities with multi-level prompts and generate analytical reasoning with confidence and vividness."}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate the performance of M-DRUM on HFFN benchmark. Quantitative results on authenticity classification demonstrate M-DRUM outperforms SOTA multi-modal fake news detection models. The effectiveness of few-shot learning and chain-of-thought reasoning is also verified. Qualitative analysis on manipulation reasoning exhibits that M-DRUM proposes reasoning with more confidence and vividness compared with mainstream LVLMs."}, {"title": "5.1 Experimental Settings", "content": "Baselines Quantitative and qualitative experiments are performed on fake news detection and manipulation reasoning, respectively. For fake news detection, baselines are MCAN [39] and HAMMER [32], which are the state-of-the-art multi-modal detection models. For manipulation reasoning, we compare our method with powerful LVLMs including PandaGPT, GPT-4 and LLaVa by prompting them to propose reasoning on potential media manipulations.\nMetrics In quantitative experiments, we evaluate models on accuracy, precision, recall and F1-score, which are commonly used in fake news detection tasks to measure the performance of authenticity classification. In qualitative experiments, we evaluate the reasoning results manually. 12 independent human raters are employed to assess the quality of randomly chosen reasoning results. Human evaluation is conducted on three orthogonal aspects: Exactness, Certainty and Detail. Exactness refers to whether the reasoning results are correct and consistent with the news content. Certainty refers to whether the reasoning results are clear and an ambiguous answer is regarded as a low score. Detail refers to whether the reasoning results are analyzed in detail rather than talk in generalities. Human raters are asked to score the reasoning on a scale of 1 to 10, with higher scores indicating better performance. We show the average evaluation scores on three aspects of M-DRUM and mainstream LVLMs.\nImplementation Details We use ImageBind-Huge [10] as the multi-modal feature encoder and Vicuna-7B[4] as the LVLM. We concatenate the outputs from the 8th, 16th, 24th, 32nd layers of the image encoder into the visual feature. The parameters of the model are initialized using the pre-trained parameters in PandaGPT [34]."}, {"title": "5.2 Authenticity Classification", "content": "We compare M-DRUM with SOTA multi-modal fake news detection models MCAN and HAMMER on HFFN benchmark. The results are presented in Tab.1. By comparison, M-DRUM exceeds the performance of the baselines on HFFN benchmark. Specifically, M-DRUM achieves the highest accuracy of 80.4%, exceeding the SOTA fake news detection models by 8.2%. In four domains of HFFN, M-DRUM ranks either first or second in terms of precision, recall and F1-score. Quantitative results exhibits the superiority of M-DRUM to identify the human-centric and fact-related news. The performance advantage of M-DRUM on HFFN highly relies on the emphasis of facial feature and the general knowledge owned by the LVLM to which feature-based detection models are not comparable.\nFew-shot Learning Considering the implementation of LVLM, we expect the M-DRUM to perform better under few-shot learning [2] settings. We test the performance of M-DRUM in 0, 1, 2 and 4-shot learning settings. The result is shown in Tab.3 and Fig.4. As the prompt examples adding, the classification performance of M-DRUM declines and then climb up. This can be explained with: a small number of prompt examples tend to confuse the model, and a larger number of samples promote the model to synthesize implicit rules and make correct judgements. Furthermore, 4-shot learning outperforms 0-zero by 5.9% in accuracy, indicating an appropriate number of examples leads to better performance, which verifies the promotion of few-shot learning.\nChain-of-Thought Reasoning A chain-of-thought (CoT) refers to a series of intermediate reasoning steps that mimic the reasoning process of human and significantly promote LLMs to tackle complex tasks [38]. In HFFN, manual annotations of content summary and clue revelation serves as the intermediate steps of the CoT. We explore whether the reasoning ability of M-DRUM can be improved through a step-by-step process in few-shot learning. In the CoT strategy, the content summary and the clue revelation of news are added to training examples and M-DRUM is guided to reason based on both original news content and manually annotated intermediate steps. Fig.5 demonstrates the performance of CoT reasoning, where M-DRUM receives a 7.6% accuracy boost and a 5.0% F1-score boost with CoT instruction. The result shows that generating the CoT along with the answer benefits the detection of M-DRUM."}, {"title": "5.3 Manipulation Reasoning", "content": "We encourage the detection model to propose analytical reasoning about manipulation in assist of unveiling forgery mechanisms. To evaluate the reasoning results, we scored M-DRUM and mainstream LVLMs on a manual basis. Tab.2 shows the result of human evaluation. In terms of exactness and certainty, M-DRUM far exceeds other LVLMs. Slightly inferior to LLaVA, reasoning results proposed by M-DRUM are still rich in detail. In general, the analytical reasoning proposed by M-DRUM can reach the performance of the ground truth (manual annotations). An example of the manipulation reasoning is shown in Fig.6. Compared with the detailed manipulation reasoning generated by M-DRUM, the reasoning of GPT-4 is ambiguous and the reasoning of LLaVA is multi-leveled but flawed. Conclusively, M-DRUM raises analytical reasoning with more confidence and vividness, which greatly expands the implementation of fake news detection models."}, {"title": "5.4 Ablation Studies", "content": "To evaluate the role of each modality in fake news detection and verify the effectiveness of architecture design, ablation experiments are conducted. We explore the impact of ignoring visual, textual and facial features in detection. In each set of experiment, a certain modality of M-DRUM is eliminated by removing the corresponding feature encoder and the two-stage training process is re-conducted. We collated the classification results of the ablation model which are presented in Tab.4. It can be observed that all ablation models with certain modality eliminated suffers from severe performance degradation, which proves the importance of each modality in M-DRUM and the necessity of facial authenticity features toward human-centric news detection."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce the fake news detection and manipulation reasoning benchmark HFFN. The benchmark is constructed following the principles of \"human-centric\" and \"fact-related\". To address classification and reasoning, we present M-DRUM, a novel detection model leveraging LVLM as the backbone. Combining multi-modal manipulation expertise and the general knowledge of LVLM, M-DRUM can not only perform authenticity judgement on the multi-modal news, but also enable analytical reasoning about potential manipulations. Comprehensive experiments demonstrate that M-DRUM outperforms SOTA fake news detection models and mainstream LVLMs. Further experiments verify the improvement of few-shot learning and chain-of-thought reasoning. Ablation studies exhibit the indispensability of different modals in detection."}]}