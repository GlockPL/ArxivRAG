{"title": "Meta-Learning Integration in Hierarchical Reinforcement Learning for Advanced Task Complexity", "authors": ["Arash Khajooeinejad M.Sc.", "Masoumeh Chapariniya PhD Candidate"], "abstract": "Hierarchical Reinforcement Learning (HRL) effectively tackles complex tasks by decomposing them into structured policies. However, HRL agents often face challenges with efficient exploration and rapid adaptation. To address this, we integrate meta-learning into HRL to enhance the agent's ability to learn and adapt hierarchical policies swiftly. Our approach employs meta-learning for rapid task adaptation based on prior experience, while intrinsic motivation mechanisms encourage efficient exploration by rewarding novel state visits. Specifically, our agent uses a high-level policy to select among multiple low-level policies operating within custom grid environments. We utilize gradient-based meta-learning with differentiable inner-loop updates, enabling optimization across a curriculum of increasingly difficult tasks. Experimental results demonstrate that our meta-learned hierarchical agent significantly outperforms traditional HRL agents without meta-learning and intrinsic motivation. The agent exhibits accelerated learning, higher cumulative rewards, and improved success rates in complex grid environments. These findings suggest that integrating meta-learning with HRL, alongside curriculum learning and intrinsic motivation, substantially enhances the agent's capability to handle complex tasks.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) has achieved significant success in various domains such as game playing, robotics, and autonomous navigation by enabling agents to learn optimal policies through interactions with the environment. However, traditional RL methods often struggle with complex tasks involving high-dimensional state and action spaces, long-term dependencies, and sparse or delayed rewards. The exponential growth of possible state-action pairs, known as the curse of dimensionality, makes it infeasible for agents to explore and learn optimal policies efficiently.\nTo address these challenges, Hierarchical Reinforcement Learning (HRL) introduces a hierarchical structure to the policy space by decomposing tasks into subtasks or options, allowing agents to operate at different levels of temporal abstraction. HRL frameworks, such as the Options Framework and Feudal Reinforcement Learning enable agents to learn reusable sub-policies, facilitating transfer learning across similar tasks and improving exploration efficiency. By breaking down complex tasks into manageable components, HRL reduces the complexity of the learning problem and mitigates the curse of dimensionality.\nDespite these advancements, HRL agents still encounter difficulties in efficiently exploring the state space and rapidly adapting to new tasks, particularly in environments with sparse or deceptive rewards. Meta-learning, or \"learning to learn,\u201d optimizes the agent's learning process itself, enabling rapid adaptation to new tasks based on prior experience. By integrating meta-learning into HRL, agents can quickly adjust both high-level and low-level policies when faced with novel environments. This integration enhances the agent's ability to fine-tune its hierarchical policies, significantly improving learning efficiency and adaptability across a variety of tasks.\nEfficient exploration remains a critical challenge in RL, as agents may become trapped in local optima if they do not sufficiently explore the environment, especially in complex tasks where the optimal policy is not immediately apparent. To tackle this issue, intrinsic motivation mechanisms introduce internal reward signals that encourage agents"}, {"title": "2. Related Work", "content": "Hierarchical Reinforcement Learning (HRL) has been extensively studied to address the challenges of scaling reinforcement learning to complex tasks. The foundational work by Sutton et al. introduced the Options Framework, which formalizes the concept of temporally extended actions or options. This framework allows agents to operate at multiple time scales by incorporating higher-level actions that consist of sequences of lower-level actions. Building upon this, proposed the Option-Critic Architecture, enabling end-to-end learning of both the internal policies and termination conditions of options. This architecture allows for more effective option discovery and utilization without the need for predefined subgoals or options.\nMeta-learning, or \u201clearning to learn,\" has gained significant attention for its potential to enhance the adaptability and sample efficiency of agents across tasks. introduced Model-Agnostic Meta-Learning (MAML), a gradient-based meta-learning algorithm that enables quick adaptation to new tasks with minimal fine-tuning. While initially applied to supervised learning, MAML has been extended to reinforcement learning domains improving sample efficiency and adaptability in changing environments. Combining meta-learning with HRL, proposed Meta Learning Shared Hierarchies (MLSH), where a hierarchy of policies is meta-learned to rapidly adapt to new tasks. MLSH enables the learning of shared low-level policies across tasks while adapting high-level policies, demonstrating improved performance in multi-task settings. Similarly, explored Evolved Policy Gradients, an approach that combines meta-learning with evolutionary strategies to learn adaptable policies.\nRecent works have further explored the integration of meta-learning into HRL frameworks to address more complex tasks. combines traditional RL with meta-RL, incorporating task-specific action-values into the meta-RL network. This hybrid approach shows enhanced performance in long-horizon and out-of-distribution tasks. While RL\u00b3 maintains efficiency in short-term tasks, it improves cumulative rewards in long-term and complex scenarios but requires careful tuning to balance between traditional and meta-RL components. Similarly, Meta Reinforcement Learning with Successor Feature Based Context proposes using context variables combined with successor features to improve multi-task learning and rapid task adaptation. This method demonstrates high efficiency with fewer environmental interactions, reducing training time, although the complexity of decomposing reward functions can limit scalability to more diverse environments.\nExploration has also been a significant challenge in meta-RL. proposed a novel context-based meta-RL framework that divides learning into task inference and execution, incorporating an exploration mechanism to improve the efficiency of discovering task-relevant information. This method improves exploration of action and task embedding spaces, leading to better sample efficiency, though generalization to out-of-distribution tasks is still limited compared to hybrid methods. Another recent study, Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning, introduces a hierarchical framework where a low-level goal-conditioned RL policy is combined with a high-level planner to address temporally extended tasks more effectively. This approach outperforms non-hierarchical planners in long-horizon tasks, although it is heavily reliant on offline training data, which may limit adaptation in dynamic, real-time environments.\nVariational Skill Embeddings for Meta Reinforcement Learning presents a regularized meta-RL framework that uses a context-skill encoder and a soft-actor-critic decoder. This method improves generalization by discovering shared skill patterns across tasks, though the use of variational autoencoders adds computational complexity, which may reduce its applicability in real-time systems.\nIntrinsic motivation has been employed to address exploration challenges in RL. introduced curiosity-driven exploration by using the prediction error of a learned forward dynamics model as an intrinsic reward signal. This method encourages agents to explore novel states, leading to better performance in environments with sparse extrinsic rewards. Similarly, proposed count-based exploration methods using pseudo-counts derived from density models to provide intrinsic rewards, effectively guiding agents toward less-visited states.\nCurriculum learning has been utilized to enhance the learning process by structuring the presentation of tasks. demonstrated that starting with simpler tasks allows models to learn better representations before tackling more complex ones. In reinforcement learning, provided a comprehensive survey on curriculum learning, highlighting methods to sequence tasks to improve learning efficiency and performance.\nSome works have combined HRL with intrinsic motivation to improve exploration within a hierarchical framework. introduced Hierarchical-DQN (h-DQN), which uses intrinsic rewards at the high level to guide the agent toward subgoals, enhancing exploration and learning in complex environments. proposed FeUdal Networks (FuN), an HRL architecture that employs a manager-worker paradigm with intrinsic motivation to encourage exploration and improve learning efficiency.\nDespite the substantial progress in HRL, meta-RL, and intrinsic motivation, integrating these methods holistically to enhance an agent's learning capabilities remains an open problem. Our proposed approach addresses this gap by combining meta-learning, intrinsic motivation, and curriculum learning within an HRL framework, which provides several advantages over previous works.\nMeta-learning for Rapid Adaptation: While prior works such as MLSH and RL\u00b3 have integrated meta-learning with HRL, they either focus on specific aspects like shared hierarchies or task-specific value functions. In contrast, our method employs meta-learning for both the high-level and low-level policy networks, allowing for rapid adaptation across a broader range of tasks with varying complexities. This comprehensive application of meta-learning enhances both exploration and task adaptability, addressing challenges in both short- and long-term learning.\nIntrinsic Motivation for Efficient Exploration: Although curiosity-driven and count-based intrinsic motivation have shown success in improving exploration, they have not been widely integrated into meta-RL frameworks. Our approach incorporates intrinsic motivation to encourage the discovery of novel states while learning hierarchical policies. This intrinsic reward mechanism helps to prevent the agent from getting stuck in suboptimal local minima, promoting effective exploration throughout training.\nCurriculum Learning for Structured Task Progression: Existing works, such as those by and have highlighted the effectiveness of curriculum learning. However, our framework is one of the first to explicitly combine curriculum learning with meta-learning and intrinsic motivation in an HRL setting. By progressively increasing the task difficulty, we enable the agent to build foundational skills incrementally, improving overall learning efficiency and performance.\nThe novelty of our approach lies in this holistic integration of meta-learning, intrinsic motivation, and curriculum learning within an HRL architecture. By combining these elements, we create a robust system capable of learning efficiently in complex environments, adapting to new tasks, and exploring effectively, resulting in superior performance compared to previous methods."}, {"title": "3. Methodology", "content": "Our proposed methodology integrates multiple advanced learning techniques, including Hierarchical Reinforcement Learning (HRL), meta-learning, intrinsic motivation, and curriculum learning. Each component is designed to address specific challenges such as scalability, rapid adaptation, efficient exploration, and learning complex tasks with sparse rewards. This section provides a detailed explanation of each element in the framework, how they interact, and their theoretical foundations."}, {"title": "3.1. Overall Framework", "content": "The framework we propose is centered on Hierarchical Reinforcement Learning (HRL), where the policy is decomposed into a hierarchy of high-level and low-level components. The high-level component selects abstract actions or sub-policies (called options), while the low-level policies determine the primitive actions to be executed in the environment. This hierarchical structure is crucial for enabling temporal abstraction, allowing the agent to make decisions over multiple time steps rather than at each individual time step.\nOptions in HRL The concept of options originates from the Options Framework introduced by Sutton et al. An option @ consists of three main components:\n\u2022 I: the initiation set, a subset of states from which the option can be initiated.\n\u2022 \u03c0\u03c9: the intra-option policy, which defines the behavior the agent follows once the option is selected. It maps states to actions.\n\u2022 \u1e9e: the termination function, a mapping from states to the probability of terminating the option. Once the option terminates, control returns to the high-level policy.\nThe agent's behavior can be summarized by first selecting an option based on the current state and then executing the intra-option policy until the termination condition is met. This allows the agent to focus on achieving intermediate sub-goals within a larger task.\nHigh-Level and Low-Level Policies In our framework, the high-level policy \u03c0\u03b7 selects options based on the current state:\n\u03c0_\\eta(\\omega|s): S \\times \\Omega \\rightarrow [0, 1], (1)\nwhere \u03a9 represents the set of available options. The high-level policy determines which option to execute, while the low-level policy (intra-option policy) \u03c0\u03c9(a | s) dictates how to act during the execution of the option.\nRole of Temporal Abstraction Temporal abstraction in HRL allows agents to learn across multiple timescales. While primitive reinforcement learning makes decisions at every time step, HRL enables agents to operate on abstract time steps by using options, thereby reducing the complexity of the task and making it easier to plan and learn.\nWorkflow of the Agent The decision-making process of the agent is hierarchical, as illustrated in Algorithm 1. The high-level policy selects an option based on the current state, and the low-level intra-option policy selects primitive actions within the option. The option continues executing until it either reaches a goal state or the termination condition is met.\nThis hierarchical setup allows the agent to reason over temporally extended actions, where the high-level policy focuses on long-term goals, and the low-level policies manage short-term execution details. The dashed arrow from the environment to the high-level policy indicates the feedback loop where the updated state influences future option selections."}, {"title": "3.2. Meta-Learning Integration", "content": "The key challenge in reinforcement learning is the need to rapidly adapt to new tasks or environments. Meta-learning, also known as \u201clearning to learn,\u201d provides a solution by optimizing the agent's ability to adapt. In our framework, meta-learning is applied to both the high-level and low-level policies.\nMeta-Learning Framework In meta-learning, the goal is to learn parameters 0 that allow rapid adaptation to new tasks with only a few updates. We employ a gradient-based meta-learning approach inspired by Model-Agnostic Meta-Learning (MAML). This framework involves an inner loop, where task-specific learning occurs, and an outer loop, where meta-parameters are updated across tasks.\nMeta-Parameters and Task Distribution In our hierarchical framework, the meta-parameters @ include:\n\u2022 \u03b8\u03b7: Parameters of the high-level policy \u03c0\u03b7.\n\u2022 0: Parameters of the intra-option policy for each option \u03c9.\n\u2022 \u03b8\u03b2: Parameters of the termination function \u03b2\u00b7\nThe agent is trained on a distribution of tasks T, and the objective is to find meta-parameters that can be quickly adapted for each task.\nMeta-Learning Objective The meta-learning objective is to minimize the expected loss over the task distribution T:\nmin E_{T~T} [L_{T_i}(\\theta'_T)], (2)\nwhere \\theta'_{T_i} are the adapted parameters for task T\u1d62, obtained after performing K inner-loop updates using task-specific data. The inner-loop updates are performed using gradient descent:\n\\theta'_{T_i} = \\theta - \\alpha \\nabla_{\\theta} L_{T_i}(\\theta), (3)\nwhere a is the learning rate for inner-loop updates. The meta-parameters @ are updated in the outer loop, based on the loss computed after adaptation to each task.\nMeta-Training Algorithm The meta-training procedure combines hierarchical reinforcement learning, meta-learning, intrinsic motivation, and curriculum learning. This approach ensures that the agent can rapidly adapt to new tasks by updating both the high-level and low-level policies. The procedure is outlined in Algorithm 2."}, {"title": "3.3. Neural Network Architectures", "content": "In our proposed framework, we utilize three key neural network architectures to support the high-level policy, low-level policy, and termination function. Each network is designed to optimize a specific aspect of the hierarchical reinforcement learning process, enabling the agent to perform decision-making at different levels of abstraction and adapt to complex environments. The structure of these networks is illustrated in Figure 3, which provides a visual representation of the high-level policy network, low-level policy network, and termination function network. The following sections describe the architectures and their role in the overall system."}, {"title": "3.3.1. High-Level Policy Network", "content": "The High-Level Policy Network is responsible for selecting options, which are high-level strategies that guide the agent's actions over extended time horizons. As shown in Figure 3 (left), the input to this network is the state of the environment, represented as a one-hot encoded vector. The network consists of three fully connected layers:\n\u2022 Input Layer: The input to the network is a one-hot encoding of the current state of the agent, with the size determined by the number of possible states in the environment. In our implementation, the input size depends on the grid size of the environment (e.g., a 6 \u00d7 6 grid leads to 36 possible states).\n\u2022 Hidden Layers: The input is passed through two hidden layers. The first hidden layer has 64 units, and the second has 32 units. Both layers use ReLU activation to introduce non-linearity and enhance the model's ability to learn complex patterns from the state representation.\n\u2022 Output Layer: The output layer consists of nodes corresponding to the available options (high-level strategies). In our implementation, there are 5 possible options. The network outputs a vector representing the value of each option, which is then used to select the most appropriate option based on the current state."}, {"title": "3.3.2. Low-Level Policy Network", "content": "The Low-Level Policy Network governs the actions within the selected high-level option. This network takes the state as input and outputs an action from a predefined set of primitive actions. The architecture, depicted in Figure 3 (center), is similar to the High-Level Policy Network:\n\u2022 Input Layer: The input is again a one-hot encoding of the current state, identical to the input for the high-level network.\n\u2022 Hidden Layers: The network consists of two hidden layers, with 64 and 32 units respectively. ReLU activations are applied after each layer to model complex relationships between the state and the required action.\n\u2022 Output Layer: The output layer provides a set of possible actions. The size of the output depends on the action space of the environment, which, in our case, is 4 (representing movement in the four cardinal directions: up, down, left, right). The network outputs action values, from which the action with the highest value is selected."}, {"title": "3.3.3. Termination Function Network", "content": "The Termination Function Network is tasked with deciding when the current option should be terminated. The decision to terminate is critical for switching from one high-level option to another when necessary. The structure of this network is as follows:\n\u2022 Input Layer: As with the other networks, the input is the current state of the agent, encoded as a one-hot vector.\n\u2022 Hidden Layers: The termination network contains two hidden layers with 64 and 32 units, each using ReLU activation to allow for the learning of complex termination policies.\n\u2022 Output Layer: The output layer contains a single node per option, and it returns a probability value for each option. These probabilities indicate the likelihood that the current option should be terminated. A sigmoid activation function is applied to the output to constrain the termination probabilities between 0 and 1."}, {"title": "3.3.4. Optimization and Learning", "content": "All three networks are optimized using gradient-based methods. Specifically, we use Adam as the meta-optimizer, with a separate SGD optimizer for the inner loop optimization during task adaptation. The high-level policy, low-level policies (for each option), and termination function are trained jointly to maximize the agent's long-term reward while efficiently managing option selection and termination.\nBy leveraging the hierarchical structure of high-level options and low-level actions, the agent is capable of solving complex tasks by breaking them down into manageable sub-tasks. The termination function further enhances the flexibility of the agent, allowing it to dynamically switch between options as the environment changes.\nThis architecture efficiently balances exploration and exploitation through both high-level strategic decisions and fine-grained action control, enabling the agent to tackle complex environments with sparse rewards."}, {"title": "3.4. Intrinsic Motivation Mechanism", "content": "In complex environments, particularly those with sparse rewards, agents often struggle to explore efficiently. To mitigate this, we incorporate an intrinsic motivation mechanism based on state visitation counts. This encourages the agent to explore novel states by providing intrinsic rewards when it visits states less frequently.\nIntrinsic Reward Formulation The intrinsic reward at time t is defined as:\nr^{int}_t = \\eta \\frac{1}{\\sqrt{N(s_t) + \\epsilon}}, (4)\nwhere:\n\u2022 \u03b7 is a scaling factor that controls the magnitude of intrinsic rewards.\n\u2022 N(s_t) is the number of times the agent has visited state s\u2081.\n\u2022 \u03f5 is a small constant to prevent division by zero.\nBy combining the extrinsic reward from the environment with the intrinsic reward from exploration, the agent's total reward becomes:\nr^{total}_t = r^{ext}_t + r^{int}_t (5)\nThis ensures that the agent balances between exploring new areas of the state space and exploiting known strategies to complete the task.\nExploration Path Visualization The process of how intrinsic motivation encourages exploration is illustrated in Figure 4. The diagram shows the agent interacting with the environment, receiving rewards based on state visitation counts, and updating its exploration path based on the combined intrinsic and extrinsic rewards."}, {"title": "3.5. Curriculum Learning Strategy", "content": "To further enhance the learning process, we implement a curriculum learning strategy. Curriculum learning presents tasks to the agent in a structured progression, gradually increasing the difficulty of tasks as the agent improves. This allows the agent to develop foundational skills on simpler tasks before tackling more complex ones.\nTask Progression Each curriculum level l is characterized by:\n\u2022 Grid Size S: Defines the size of the environment (e.g., 4 \u00d7 4, 5 \u00d7 5, 6 \u00d7 6).\n\u2022 Number of Traps Op: Specifies the number of obstacles or traps in the environment.\n\u2022 Task Complexity Ce: Measures the difficulty of the task based on path length, trap density, and goal distance.\nPerformance-Based Progression The agent progresses to more challenging curriculum levels once it reaches a predefined performance threshold. This threshold may be based on metrics such as the agent's success rate, cumulative reward, or time to complete tasks. By adapting the difficulty of tasks in response to the agent's progress, curriculum learning helps the agent avoid being overwhelmed by complex tasks early in the training process and promotes gradual skill acquisition."}, {"title": "3.6. Policy Optimization with Intrinsic Rewards", "content": "The agent's policies are optimized using Q-learning, where the Q-values are updated based on both extrinsic and intrinsic rewards. The target Q-value for the intra-option policy is adjusted to account for the total reward:\ny^{low} = r^{total}_t + \\gamma max_{a'} Q_{\\omega}(s_{t+1}, a'; \\theta), (6)\nwhere \u03b3 is the discount factor, and \u03b8 represents the parameters of a target network used for stabilization during training. The intrinsic rewards encourage exploration, while the extrinsic rewards guide the agent toward task completion."}, {"title": "3.7. Theoretical Justification", "content": "Our approach integrates four core components: Hierarchical Reinforcement Learning, Meta-learning, Intrinsic Motivation, and Curriculum Learning, each addressing a specific limitation of traditional reinforcement learning. By combining these elements, the agent achieves efficient exploration, scalable learning, and rapid adaptation.\nHierarchical Policies Hierarchical policies allow the agent to operate over multiple time scales, reducing the complexity of long-horizon planning tasks. This leads to more efficient learning in complex environments by breaking down tasks into manageable subtasks.\nMeta-Learning for Rapid Adaptation Meta-learning provides the agent with the ability to generalize across tasks. By learning meta-parameters that are sensitive to task variations, the agent can quickly adapt to new tasks with minimal fine-tuning.\nIntrinsic Motivation for Exploration Intrinsic motivation ensures that the agent effectively explores the state space, discovering novel strategies and avoiding sub-optimal policies. This is especially critical in environments where reward signals are sparse or deceptive.\nCurriculum Learning for Task Progression Curriculum learning allows the agent to gradually build up its skills, preventing it from becoming overwhelmed by complex tasks too early in the learning process. By structuring tasks with increasing difficulty, the agent learns in a progressive way, leading to better performance and learning stability."}, {"title": "4. Experimental Setup", "content": "This section delineates the experimental environments, baseline comparisons, hyperparameter optimization using Optuna, evaluation metrics, and the computational resources employed to assess the efficacy of the proposed meta-learning integrated hierarchical reinforcement learning (HRL) framework. We conducted experiments under three distinct scenarios:\n1. Hyperparameter Optimization and Validation: Utilizing Optuna to identify optimal hyperparameters and validating the model with these parameters.\n2. Fixed Complexity Scenario: Training and evaluating the agent in a stable environment with constant complexity.\n3. Comparison of Complexity Strategies: Comparing the agent's performance when starting with a complex environment versus gradually increasing task complexity through curriculum learning.\nEach scenario is designed to assess different aspects of the framework, ranging from hyperparameter sensitivity to adaptability in varying environments."}, {"title": "4.1. Environments", "content": "We employed custom grid-based environments to simulate navigation tasks with varying levels of difficulty. These environments are characterized by grid size and the number of traps, which act as obstacles that the agent must avoid to reach the goal.\nCustom Grid Environment The CustomGridEnv class defines a grid-based environment with configurable size and number of traps. The agent starts at the top-left corner (0, 0) and aims to reach the bottom-right corner (N \u2212 1, N \u2212 1), where N is the grid size. Traps are randomly placed within the grid to increase task complexity.\nKey Parameters:\n\u2022 Grid Size: Determines the dimensions of the grid (e.g., 4 \u00d7 4, 5 \u00d7 5, 6 \u00d7 6).\n\u2022 Number of Traps: Specifies the number of traps within the grid, increasing the difficulty of the task."}, {"title": "4.2. Experimental Scenarios", "content": "We conducted experiments under three distinct scenarios to comprehensively evaluate the performance and adaptability of our proposed meta-learning integrated hierarchical reinforcement learning (HRL) framework:\n1. Hyperparameter Optimization and Validation: Utilizing Optuna to identify optimal hyperparameters and validating the model with these parameters.\n2. Fixed Complexity Scenario: Training and evaluating the agent in a stable environment with constant complexity.\n3. Comparison of Complexity Strategies: Comparing the agent's performance when starting with a complex environment versus gradually increasing task complexity through curriculum learning.\nEach scenario is designed to assess different aspects of the framework, ranging from hyperparameter sensitivity to adaptability in varying environments."}, {"title": "4.2.1. Hyperparameter Optimization and Validation", "content": "To optimize the performance of our proposed framework, we employed Optuna, an automatic hyperparameter optimization library. The objective was to identify the best set of hyperparameters that maximize the agent's performance across different tasks.\nSearch Space The hyperparameters explored included:\n\u2022 Meta-Learning Rate (\u03b2): 8 \u00d7 10\u22126 to 1.2 \u00d7 10\u22125\n\u2022 Inner-Loop Learning Rate (\u03b1): 0.002 to 0.004\n\u2022 Number of Inner Steps: 3 to 5\n\u2022 High-Level Exploration (\u03f5high): 0.1 to 0.15\n\u2022 Option Exploration (\u03f5option): 0.6 to 0.65\n\u2022 Intrinsic Reward Scale (\u03b7): 0.1 to 0.15\nOptimization Process Optuna conducted 50 trials using the MedianPruner, which prunes unpromising trials early based on median performance, enhancing computational efficiency. Each trial involved training the agent with a specific hyperparameter configuration and evaluating its performance based on the average reward achieved over the last 10 meta-iterations.\nOptimal Hyperparameters The most optimal trial yielded the following hyperparameters:\n\u2022 Meta-Learning Rate (\u03b2): 8.24 \u00d7 10-6\n\u2022 Inner-Loop Learning Rate (a): 0.00317\n\u2022 Number of Inner Steps: 5\n\u2022 High-Level Exploration (\u03f5high): 0.1018\n\u2022 Option Exploration (\u03f5option): 0.6199\n\u2022 Intrinsic Reward Scale (\u03b7): 0.1111"}, {"title": "4.2.2. Fixed Complexity Scenario", "content": "In the fixed complexity scenario, we trained and evaluated the agent in a stable environment with constant complexity to assess its performance without the influence of increasing task difficulty. This setup serves as a baseline to compare the effectiveness of the proposed hierarchical and meta-learning components against a controlled environment.\nEnvironment Configuration\n\u2022 Grid Size: 6 \u00d7 6\n\u2022 Number of Traps: 3\nThe environment is configured to maintain a consistent level of difficulty throughout the training and evaluation phases. By fixing the grid size and the number of traps, we ensure that the agent's learning process is not influenced by varying environmental complexities, allowing us to isolate and analyze the impact of the hierarchical and meta-learning mechanisms.\nTraining Parameters\n\u2022 Meta-Learning Rate (\u03b2): 0.0001\n\u2022 Inner-Loop Learning Rate (a): 0.003\n\u2022 Number of Inner Steps: 3\n\u2022 High-Level Exploration (\u03f5high): 0.3\n\u2022 Option Exploration (\u03f5option): 0.5\n\u2022 Intrinsic Reward Scale (\u03b7): 0.1\n\u2022 Number of Meta-Iterations: 500\n\u2022 Number of Inner Steps per Task: 50\nThese training parameters are selected based on the optimal hyperparameters identified in Section 4.2.1. The consistent environment configuration allows us to evaluate the agent's ability to learn and perform effectively without the added complexity of changing tasks.\nEvaluation Metrics In the fixed complexity scenario, we focus on the following evaluation metrics to assess the agent's performance:\n\u2022 Meta-Loss: Measures the discrepancy between predicted and target Q-values during training.\n\u2022 Average Reward: Calculates the average cumulative reward achieved by the agent per meta-iteration.\n\u2022 Success Rate: Determines the percentage of episodes in which the agent successfully reaches the goal.\n\u2022 Exploration Efficiency: Counts the number of unique states visited during training, reflecting the agent's exploration capabilities.\n\u2022 Cumulative Rewards: Summarizes the total rewards accumulated by the agent during each episode, combining both extrinsic and intrinsic rewards."}, {"title": "4.2.3. Fixed Complexity Scenario Results", "content": "The fixed complexity scenario provides a controlled environment to assess the agent's performance, focusing on metrics such as meta-loss, average reward, and success rate over the course of 500 meta-iterations. Figure 5 illustrates the progress of these metrics. The meta-loss, plotted in red, exhibits an initially high value at the beginning of the training process, with fluctuations during the first 100 iterations. However, as training progresses, the meta-loss stabilizes and continues to decrease steadily until it reaches a near-constant value around 30. The convergence of meta-loss indicates that the agent's predictions for Q-values align more closely with the target values, demonstrating improved learning of the hierarchical policy.\nThe average reward, shown in blue, follows a steady trend over the iterations. After a short period of fluctuation in the initial phase (0\u2013100 iterations), the average reward stabilizes, showing incremental improvements as training proceeds. By the end of 500 iterations, the agent achieves an average reward around -5, indicating that the agent is successfully adapting to the task but may face challenges due to the environmental complexity and intrinsic reward structure. The observed fluctuations early on suggest that the agent is exploring various strategies before converging toward a more stable high-level policy.\nThe success rate, plotted in green, demonstrates a notable increase during the initial phase, reaching around 40\u201345% success within the first 200 iterations. After this period, the success rate fluctuates, showing an oscillatory pattern around the same percentage. This behavior suggests that while the agent is capable of achieving success in some episodes, the fixed complexity of the environment combined with the intrinsic reward structure causes the agent to switch between exploration and exploitation, preventing it from consistently achieving a higher success rate. The agent's ability to maintain a reasonable success rate reflects a stable but exploratory policy.\nFrom the trends in the graph, we can infer that the fixed complexity scenario provides a challenging but controlled environment for evaluating the agent's ability to balance exploration and exploitation. The steady decrease in meta-loss alongside the oscillatory patterns in both reward and success rate highlights the intrinsic difficulty of the task, as the agent continues to explore and optimize its policy. The fixed complexity ensures that no external factors, such as varying task difficulties, affect the learning process, allowing us to observe the agent's core learning dynamics."}, {"title": "4.2.4. Gradual Complexity Scenario Results", "content": "The gradual complexity scenario introduces increasing levels of difficulty during the agent's training, allowing us to observe its adaptability as it transitions through more complex tasks. Figure 6 provides a detailed look at the evolution of meta-loss, average reward, and success rate across different phases of complexity over 4000 meta-iterations. Unlike the fixed complexity scenario, where the environment remains stable, here the agent encounters progressively harder challenges, beginning with smaller grid sizes and fewer traps, and advancing to larger grids with more traps. This scenario tests the agent's ability to maintain its performance and improve as the task complexity increases.\nThe meta-loss, shown in red, follows a pattern of sharp fluctuations corresponding to the transitions between levels of complexity. In the initial phase (around 0\u20131000 iterations), where the agent operates in the simplest environment (4 \u00d7 4 grid with 1 trap), the meta-loss stabilizes relatively quickly and maintains a low value, indicating that the agent can effectively learn and predict Q-values for this level of complexity. However, as the complexity increases-marked by the transitions at around 1000 and 2500 iterations\u2014the meta-loss spikes significantly. These spikes suggest that the agent initially struggles to adapt to the new task complexities before stabilizing once again. Despite these fluctuations, the overall downward trend of the meta-loss indicates that the agent continues to improve its policy across increasing complexity levels.\nThe average reward, depicted in blue, follows a similar trend to the meta-loss. During the initial phase, where the environment is simpler, the average reward stabilizes quickly, with a relatively high value compared to later phases. However, each time the complexity increases, there is a noticeable drop in average reward, reflecting the increased difficulty of the tasks. Despite these drops, the average reward consistently rebounds as the agent adapts to the new complexity level. By the end of the training (after 4000 iterations), the agent has recovered much of its average reward, though the values remain lower than in the simpler environments. This indicates that while the agent is capable of adapting, the more complex environments pose a greater challenge, affecting its overall performance.\nThe success rate, shown in green, provides additional insight into the agent's adaptability. In the initial phase, the success rate climbs quickly and stabilizes at around 50-60%, indicating that the agent is performing well in simpler environments. However, similar to the average reward, the success rate drops dramatically with each increase in complexity. The sharp drops correspond to the agent's difficulty in adjusting to the new, harder tasks. Despite these challenges, the success rate eventually rebounds, though it stabilizes at a lower level than during the earlier, simpler phases. This oscillatory pattern of the success rate suggests that the agent oscillates between exploration and exploitation as it learns to handle the new levels of complexity. By the end of the training, the agent demonstrates a stable yet lower success rate, reflecting its ongoing adaptation to the most challenging environments.\nOverall, the results from the gradual complexity scenario demonstrate the agent's capacity for learning and adapting to increasingly difficult tasks. The noticeable fluctuations in meta-loss, average reward, and success rate during transitions between levels suggest that while the agent is capable of adapting, it requires time to adjust to new complexities. These fluctuations highlight the inherent challenge of gradually increasing task difficulty in hierarchical reinforcement learning. Despite these challenges, the agent consistently improves its policy, as evidenced by the overall downward trend in meta-loss and the recovery in both average reward and success rate after each complexity increase. The results from this scenario serve as a critical comparison point to the fixed complexity baseline, illustrating the strengths and limitations of the agent's learning strategy in dynamic environments.\nComparison with Fixed Complexity Scenario When comparing the gradual complexity scenario with the fixed complexity scenario, several important differences arise. In the fixed complexity scenario, where the environment remained stable, the meta-loss decreased steadily with fewer fluctuations, allowing the agent to achieve a stable policy more quickly. However, in the gradual complexity scenario, the agent experiences significant disruptions at each complexity transition, as seen in the spikes in meta-loss and the corresponding drops in both average reward and success rate. These fluctuations highlight the challenges posed by a dynamic environment, where the agent must continually adjust its policy to handle increasing difficulty.\nFurthermore, in the fixed complexity scenario, the agent achieves a higher overall success rate at the end of the training period, stabilizing around 40 to 45%. In contrast, the success rate in the gradual complexity scenario, while recovering after each transition, never quite reaches the stability or level observed in the fixed complexity scenario, indicating that the progressively harder tasks pose ongoing challenges to the agent's performance. Similarly, the average reward in the fixed complexity scenario shows a more stable trend compared to the gradual complexity case, where the agent must repeatedly relearn and adapt to new environments.\nIn summary, while the fixed complexity scenario allows the agent to converge to a stable policy more efficiently, the gradual complexity scenario provides a more realistic and challenging test of the agent's adaptability. The gradual complexity results show that while the agent can ultimately adjust to more complex environments, the process introduces significant challenges and requires a more"}]}