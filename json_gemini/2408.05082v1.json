{"title": "Generalizing Few Data to Unseen Domains Flexibly Based on Label Smoothing Integrated with Distributionally Robust Optimization", "authors": ["Yangdi Wang", "Zhi-Hai Zhang", "Su Xiu Xu", "Wenming Guo"], "abstract": "Overfitting commonly occurs when applying deep neural networks (DNNs) on small-scale datasets, where DNNs do not generalize well from existing data to unseen data. The main reason resulting in overfitting is that small-scale datasets cannot reflect the situations of the real world. Label smoothing (LS) is an effective regularization method to prevent overfitting, avoiding it by mixing one-hot labels with uniform label vectors. However, LS only focuses on labels while ignoring the distribution of existing data. In this paper, we introduce the distributionally robust optimization (DRO) to LS, achieving shift the existing data distribution flexibly to unseen domains when training DNNs. Specifically, we prove that the regularization of LS can be extended to a regularization term for the DNNs parameters when integrating DRO. The regularization term can be utilized to shift existing data to unseen domains and generate new data. Furthermore, we propose an approximate gradient-iteration label smoothing algorithm (GI-LS) to achieve the findings and train DNNs. We prove that the shift for the existing data does not influence the convergence of GI-LS. Since GI-LS incorporates a series of hyperparameters, we further consider using Bayesian optimization (BO) to find the relatively optimal combinations of these hyperparameters. Taking small-scale anomaly classification tasks as a case, we evaluate GI-LS, and the results clearly demonstrate its superior performance.", "sections": [{"title": "1. Introduction", "content": "In supervised deep learning, overfitting is a common issue when training machine learning deep neural networks (DNNs), where DNNs perform well on training data but struggle to generalize to unseen data. This issue is especially frequent when the dataset is small, as a small-scale dataset cannot accurately reflect real-world conditions (Zhou et al., 2022). Due to real-world constraints (e.g., technical difficulty, inclusion of private information, etc.), it is unrealistic to collect large-scale data in some industries. For instance, the anomaly data collection in the process of producing various industrial products is always extremely time-consuming and expensive. Thus, there is a scarcity of anomaly data when maintaining and controlling the quality of products (Bergmann et al., 2019; Tabernik et al., 2020). This dilemma is also present in other domains such as medical image analysis, financial development trend prediction and so on. A suitable approach to preventing overfitting while improving the generalization of DNNs using existing data could lead to advancements in a wide range of fields.\nIn supervised deep learning, the generalization of DNNs can be improved from both the model itself and the dataset information perspectives. Recently, a variety of regularization strategies have emerged to prevent overfitting and improve model's generalization. One branch of regularization strategies, such as $l_1$ or $l_2$ penalization, weight decay, and dropout, constrains the range of variation for model parameters or acts on hidden activation functions during model training. However, these methods intricately involve the specific parameterization or loss criterion of the model (Li et al., 2020), complicating the training process. In contrast, regularization strategies that prevent overfitting from the viewpoint of the dataset do not influence the structure of the model, making the training process simpler. The most direct method is data augmentation expanding the scale of dataset. However, collecting and annotating data in some industries always incur extremely high costs, are time-consuming, and laborious in many industries (Aghasi et al., 2024). It is natural to use the existing data to generate new ones. Unfortunately, the generated data might change the existing data characteristics significantly, potentially leading to poor performance of model on unseen data (Peck et al., 2023). Beyond merely considering existing data, since each data point in supervised deep learning contains labels with annotation information (e.g., category information) that are used to fit the output distribution"}, {"title": "1.1. Short Introduction to Label Smoothing", "content": "In this section, we briefly introduce LS using the k-class supervised classification task.\nTaking arbitrary sample (X,Y) belonging to a certain category i from dataset, we consider the hard one-hot vector to represent its corresponding label Y when training DNNs.\n$Y = [y_1, y_2, ..., y_k]$,                                                                                                                (1)\nwhere $y_i \\in Y$ is:\n$y_i = \\begin{cases}1, & \\text{if sample X belongs to class i,}\\\\ 0 & \\text{otherwise.}\\end{cases}$                                                                                    (2)\nIn general, the output of model may closely mirror the hard one-hot label Y, where the prediction for the $i^{th}$ position approaches 1, while the predictions for other positions are nearly 0. This issue tends to result in the overconfidence of model's output and leads to overfitting. Furthermore, the situation is particularly evident when the scale of dataset is small. To address this issue, LS considers changing the hard one-hot label Y in the following.\n$Y_i = \\begin{cases}1 - \\alpha, & \\text{if sample X belongs to class i,}\\\\ \\frac{\\alpha}{k-1} & \\text{otherwise.}\\end{cases}$                                                                                    (3)\nwhere $\\alpha$ is a hyperparameter adjusted manually.\nLS only introduces a hyperparameter $\\alpha$ to adjust each position of the hard one-hot label Y. When $\\alpha$ = 0, LS degenerates to hard one-hot label in equation (1). As $\\alpha$ increases, the $i^{th}$ position changes from 1 to 1 $\\alpha$ and the other k - 1 positions change from 0 to $\\frac{\\alpha}{k-1}$. In this process, the true label value progressively decreases while the values of other categories gradually increase, penalizing the overconfidence in the outputs for the true categories while giving some attention to other categories. This makes the output distribution relatively"}, {"title": "1.2. Our Contributions", "content": "In this paper, based on the powerful regularization effect of LS for labels and its hyperparameter when training DNNs, we integrate LS into the regularization term built by DRO, introducing multiple worst-case scenarios and enabling the exploration of appropriate data shifts, adding flexibility to DRO. These data shifts are equivalent to generating multiple new samples for the same label without extra annotations. Furthermore, we overcome the limitations imposed by labels on LS by incorporating DRO.\nOur main contribution are summarized as follows.\n\u2022 We propose a novel two-stage problem, named DRO-LS, integrated LS within the DRO framework for generalizing few data to unseen domains flexibly. Specifically, the first stage perturbs existing data by LS and generate new samples, while the second stage involves using the original data and generated data to train DNNs.\n\u2022 To the best of our knowledge, it is the first time that the regularization effect of LS overcomes the limitations imposed by labels and has been applied to generate data. In addition, it is the first time that the worst-case scenario of DRO can be adjusted flexibly. Specifically, the regularization term built by DRO roughly corresponds to an $L_2$ penalization that integrates the regularization effect of LS and DNNS parameters. Moreover, we prove that there exists a bound between the existing data and the generated data, showing that the generated data maintains the characteristics of the existing data even if the hyperparameter of LS varies.\n\u2022 We develop a two-stage gradient-based algorithm, called gradient-iteration label smoothing (GI-LS), to approximately solve DRO-LS model. The algorithm can be regarded as a new data augmentation algorithm. Furthermore, we prove the convergence of the proposed algorithm, illustrating that perturbations on existing data with LS do not affect its convergence rate.\n\u2022 Since the proposed algorithm involves a series of hyperparameters for adjustment, we take in specific small-scale anomaly cases to conduct extensive simulations using Bayesian optimization (BO) provide suggestions for selecting the hyperparameter ranges of GI-LS. Furthermore, as a perturbation algorithm for existing data, we validate the effectiveness of GI-LS in defending against some general adversarial attack methods in the context of magnetic surface defects.\nThe rest of this paper is organized as follows. Section 2 provides the relevant literature. Section 3 introduces the base model and algorithm. In Section 4, using anomaly classification as a case study, the algorithm performance is evaluated with simulation analyses using BO. Section 5 concludes the paper and presents future work. All proofs and experiments are provided in the Appendix."}, {"title": "2. Related work", "content": "Our work is closely related to two streams of research in the literature. The first stream concerns data augmentation. It is common that the distribution of training data does not reflect the real-world situation, directly resulting in the overfitting of models. Data augmentation deals with the overfitting from the root of problem, which expands the scale of training data (Wang et al., 2021). However, constrained by real-world conditions, collecting and annotating samples in some industries always incurs extremely high costs. Thus,\ndesigning data augmentation based on the existing data become the only choice. Some research achieves this by directly splicing, reassembling, and occluding data (DeVries and Taylor, 2017; Zhang et al., 2018; Yun et al.,2019). Unfortunately, these methods might change the characteristics of original data significantly, potentially leading to poor generalization of DNNs on unseen data. In contrast, our work guarantees that the generated samples surround the existing data by introducing DRO. Additionally, there are another data augmentation methods in the literature that generates adversarial samples, based on point-wise $l_p$-bounded perturbations, to shift existing data (Luo et al., 2020; Bai et al., 2024). Typical adversarial methods include Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014), CW attacking (Carlini and Wagner, 2017), Projected Gradient Descent (PGD) (Madry et al., 2018), etc. See Peck et al. (2023) for a comprehensive review of adversarial data augmentation method. In general, these methods are often summarized as white-box attacks and cause a negative impact on DNNs performance (Bai et al., 2024). It has been proven that DNNs are vulnerable to adversarial data augmentation (Goodfellow et al., 2014), where imperceptible adversarial perturbations significantly decreases the performance of DNNs (Peck et al., 2023). The data augmentation proposed in our paper can also be regarded as a perturbation method. However, different from attacking DNNs with imperceptible perturbations, our work primarily focuses on giving large perturbations to flexibly shift data for small-scale datasets (Volpi et al., 2018).\nThe second related stream of literature concentrates on distributionally robustness optimization (DRO). DRO has been widely used in a wild range of field, such as machine scheduling, manufacturing, etc (Shu and Song, 2014; Noyan et al., 2022). In recent years, DRO has made great progress in building the relationship between robustness and regularization in machine learning (Kuhn et al., 2019; Duchi et al., 2021; Bai et al., 2024), making contribution to both convex and non-convex models (Shafieezadeh-Abadeh et al., 2019; Levy et al., 2020). The crucial considerations for DRO in machine learning are how to choose the ambiguity set while maintain tractability. At present, most research considers using the Wasserstein distance, a metric for transforming one distribution into another, to build the ambiguity set (Blanchet et al., 2021); Liu et al., 2022). This method, called Wasserstein distributionally robust optimization (WDRO), minimizes the worst-case distribution within a Wasserstein ball, offering a new interpretation of the relationship between"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Problem formulation", "content": "In supervised learning, the optimization objective of training DNNs is as follows.\n$\\min_{\\theta \\in \\Theta} E_{(X, Y) \\sim P_0} [l(\\theta; (X,Y))]$,                                                                                                                                                                                    (4)\nwhere $(X, Y) \\in (\\mathcal{X}, \\mathcal{Y})$ are drawn from the training distribution $P_0$. X denotes the training sample and Y is the corresponding label. $\\theta \\in \\Theta$ represents the parameters of DNNs. $l : \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ is the loss function.\nWe introduce the cross-entropy loss as the loss function l.\n$l(\\theta; (X,Y)) = - \\sum_{i=1}^{k} \\sum_{j=1}^{k} y_i \\log p_j(\\theta,x)$.                                                                                                                                            (5)"}, {"title": "3.2. Theoretical Analysis", "content": "To solve the penalty problem (13), based on the definition of robust surrogate loss, we perform stochastic gradient descent on the robust surrogate loss $\\Phi_{\\gamma} (\\theta; (z', y))$, and have:\n$\\nabla_{\\theta}\\Phi_{\\gamma} (\\theta; (z', y)) = \\nabla_{\\theta} l(\\theta; (z'^*, y))$,                                                                                                                                                                                    (14)\nwhere $z'^* = \\arg \\max_{z \\in \\mathcal{Z}} \\{l (\\theta; (z, y_0)) - \\gamma c_{\\theta} ((z, y_0), (z', y_0))\\}$ is a perturbation of the feature map $z'$ in the current parameters $\\theta$. In fact, we aim to obtain the $z'^*$ in the supremum stage. Supposing that $||z - z'||$ is sufficiently smooth. The following theorem illustrates that the relationship between $z' = f(\\theta;x)$ and $z'^* = f(\\theta_r;x^*)$, where x denotes the existing training data and $x^*$ is the optimal generated data. Specifically, we represent the non-identity Hessian matrix as H.\nTHEOREM 2 (Relationship between Existing Data and Optimal Generated Data). For $z' = f(\\theta_r;x)$ and $z'^* = f(\\theta_r;x^*)$, we have\n$||z'^* - z'|| = ||(H - \\gamma I)^{-1}\\nabla_{z'}l(\\theta; (z', y))||$,                                                                                                                                                                                    (15)\nwhere\n$\\nabla_{z'}l(\\theta; (z', y)) = (1 - \\alpha)\\theta_{f,i}^T + \\frac{\\alpha}{(k-1)} \\sum_{j \\neq i}^{k} \\theta_{f,j}^T - \\sum_{j=1}^{k} p_j(\\theta;z')\\cdot \\theta_{f,j}^T$.                                                                                      (16)\nTheorem 2 reveals that the feature map of the optimal generated data $z'^*$ can be obtained by perturbing the feature map of the existing data $z'$. The mass transported from $z'$ to $z'^*$ in the semantic space is equal to the product of the deviation of $l(\\theta; (z', y))$ and the Hessian matrix. Theorem 2 further provides the detailed expression for $\\nabla_{z'}l(\\theta; (z', y))$, as shown in equation (16), which consists of the feature map $z'$, the classification layer parameters $\\theta_f$. Note that the hyperparameter in equation (3) applied to labels now applies to $\\theta_f$. For the parameters $\\theta_{f,i}$ corresponding to the true category i, the weight is 1 - $\\alpha$ while the weights correspond to other positions are $\\frac{\\alpha}{k-1}$. Instead of perturbations $L_p$ for the training samples, this term aims to penalize the distance between $(1 - \\alpha)\\theta_{f,i} + \\frac{\\alpha}{(k-1)} \\sum_{j \\neq i}^{k} \\theta_{f,j}$ and $\\sum_{j=1}^{k} p_j(\\theta; z')\\cdot \\theta_{f,j}$, with the weight characterized by the hyperparameter $\\alpha$, and $\\sum_{j=1}^{k} p_j(\\theta; z')\\cdot \\theta_{f,j}$ which is the estimated mean of the $\\theta_f$ across all categories with"}, {"title": "4. Case Study", "content": "In this section, we apply GI-LS to a real-world case study on the classification task for small-scale anomaly images with DNNs, which includes two magnetic tile surface defect datasets (Huang et al., 2020) and two"}, {"title": "4.1. Small-scale Anomaly Dataset", "content": "We use magnetic tile surface defect (Huang et al., 2020), following the structure of CUB-200-2011 (Wah et al., 2011), to establish two magnetic tile surface datasets, as shown in Table 1. MT-Defect includes five types of defects. In contrast, MT introduces images without surface defects, which are far more numerous than the defect categories. In general, the number of products with surface defects is usually less than the number of qualified products in a real-world industrial line, and the number of each type of surface defect product is rare. The distribution of MT is consistent with the real-world situation to some extent."}, {"title": "4.2. Bayesian Optimization", "content": "As seen from the above review, the proposed GI-LS includes a series of hyperparameters. When applying stochastic gradient ascent to generate new data for the inner maximization stage of GI-LS, in addition to the $\\alpha$ of LS, the number of perturbation iterations T and the step size of gradient ascent $\\eta$ are also crucial to the data generation process. Furthermore, the iterations of the outer minimization stage of GI-LS determine the efficiency. Thus, selecting appropriate ranges for these parameters is essential for the performance of GI-LS.\nBayesian Optimization (BO) has been widely used when the process of training DNNs involves multiple hyperparameters, especially in the context of manufacturing (AlBahar et al., 2021). It is known for its efficacy in globally optimizing expensive black-box functions (Semelhago et al., 2021; Xu et al., 2023). Therefore, we employ BO to explore the optimal combination of the hyperparameters. Specifically, we use Adaptive Experimentation (Ax) platform of Meta to execute BO for GI-LS (Baird et al.,2022). To address the covariance of uncertainty, the Matern 5/2 kernel is employed. We mainly focus on the parameters of the classification layer and treat the other hidden layers as the feature extraction layer. In our settings, we"}, {"title": "4.2.1. Classification Results", "content": "In our experiment, the value of $\\gamma$ is set to 10\u20133. We execute 20 iterations of BO for GI-LS. The perturbation factor $\\alpha$ of LS is bounded between 0 and 0.5, the number of iterations T ranges from 1 to 15, the step size $\\eta$ ranges from 1.5 to 3.0, and the number of training rounds ranges from 21 to 60. In each iteration, the learning rate $\\beta$ is set to 1 \u00d7 10\u22122 for the classification layer and 1 \u00d7 10-3 for the parameters of the feature extraction layer, reducing by a factor of 0.3 every 20 epochs. All experiments are conducted within the PyTorch framework on Nvidia GeForce RTX 3060 and 3090 GPUs.\nFigure 3 presents the top-1 accuracy of DNNs with the execution of BO iterations across four datasets.\nEach point represents the top-1 accuracy of one iteration. ResNet34 achieves competitive results for all"}, {"title": "4.2.2. Analysis of Hyperparameter Combination in GI-LS", "content": "As seen from the above review, the performance of DNNs on Wood and Carpet is relatively stable, indicating that the search space of the hyperparameters is flattened. In contrast, the performance of DNNs on MT-Defect and MT fluctuates to some extent. In this section, we analyze how the combination of hyperparameters in GI-LS clusters in the optimal region during the BO iteration for the four datasets.\nThe relationship between $\\alpha$ and $T$ is shown in Figure 4. Note that $\\alpha$ does not cluster around the origin of the coordinate axis, indicating the effectiveness of the further shift for the worst-case distribution by LS. On MT-Defect and MT, a large combination of $\\alpha$ and $T$ benefits the performance of ResNet18, demonstrating the noteworthy role of $\\alpha$ in shifting the worst-case distribution to achieve better performance. The results"}, {"title": "4.3. Comparison to Existing Data Augmentation Method", "content": "In this section, using MT-Defect and MT as examples, we conduct seven data augmentation methods to compare with the proposed GI-LS method. We regard the DNNs trained with cross-entropy loss without LS and existing images from MT-Defect and MT as the benchmark. Table 5 shows the top-1 accuracy results of Cutmix (Yun et al., 2019), Cutout (DeVries and Taylor, 2017), and Mixup (Zhang et al., 2018), while Table 6 records the top-1 accuracy results of PGD (Madry et al., 2018), FGSM (Goodfellow et al., 2014), and CW attacking (Carlini and Wagner, 2017). We set the maximum perturbation e = $\\frac{8}{225}$ for PGD and FGSM. Specifically, we use torchattacks package to achieve adversarial perturbation methods (Kim, 2020).\nThe results of Tables 5 and 6 demonstrate that GI-LS achieves superior performance compared to the benchmark and the other seven data augmentation methods, confirming the effectiveness of the theoretical analysis in Section 3. Note that the above seven data methods operate on the original images while GI-LS focus on the feature map corresponding to semantic space, illustrating that the reasonable of defining perturbations on the semantic space in visual space. Compared with the benchmark and LS in Table 4, the performance of DNNs does not improve, indicating that multiple labels for the small-scale data do not benefit the model's generalization. In contrast, GI-LS achieves multiple labels for multiple data and significantly improves model generalization. GI-LS introduces perturbations to existing data to generate new data. Cutmix, Cutout, and Mixup reorganize parts of the images to generate new data. These methods may change the characteristics of the existing data and result in poor generalization. Moreover, from the results of Table 5, we find all performance of DNNs on MT improve significantly compared to the those on MT-Defect, illustrating the critical role of 'Free' samples for improving model's generalization again.\nThe adversarial samples have a severe negative effect on MT-Defect, as illustrated by the results in Table 6. Even so, with the increase in perturbed 'Free' samples, the performance of DNNs improves significantly,"}, {"title": "4.4. Experiment for \u03b3 Perturbation", "content": "Theorem 4 provides the bounds of the robust surrogate loss $\\Phi_{\\gamma}(\\theta; (z', y))$, which provide perturbations denoted as $L(\\theta)$ on $\\gamma$. The top-1 accuracy metrics of DNNs can serve as indicators for the bounds, measuring the distribution of generating samples consistent with $\\Phi_{\\gamma}(\\theta; (z', y))$. In our experiments, we perturb the value of $\\gamma$ by simulating diverse magnitudes of $L(\\theta)$ in Algorithm 1. The other hyperparameters are the same as those used in Table 3. Starting with $\\gamma$ = 10-3, we perturbed its denominator by \u00b10.1, \u00b11, \u00b110, and \u00b1100. Detailed findings can be found in Table 8.\nWe see that the results remain fairly stable across ResNet18, ResNet34, and ResNet50 for both MT-Defect and MT when perturbing $\\gamma$ with different magnitudes. This implies that the combination of hyperparameters in GI-LS has some robustness to defend against perturbations on $\\gamma$ when shifting data to unseen domains. Moreover, this also illustrates that the generated samples with perturbations are consistent with those without perturbations. In other words, the perturbations of $\\gamma$ with different magnitudes have few influence on the process of generating data."}, {"title": "5. Conclusion", "content": "In this paper, we propose a two-stage DRO-LS model as a new data augmentation, which considers the regularization effect of LS integrated with DRO to address the issue of overfitting when applying DNNs in few data scenarios. Specifically, we utilize the Wasserstein distance within the DRO framework to construct the ambiguity set and ensure computational feasibility via Lagrangian relaxation. We propose a surrogate loss and prove that it is equivalent to the LS loss and a regularization term. We find that the LS regularization"}]}