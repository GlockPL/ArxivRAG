{"title": "Where Do You Go? Pedestrian Trajectory Prediction using Scene Features", "authors": ["Mohammad Ali Rezaei", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "abstract": "Accurate prediction of pedestrian trajectories is crucial for enhancing the safety of autonomous vehicles and reducing traffic fatalities involving pedestrians. While numerous studies have focused on modeling interactions among pedestrians to forecast their movements, the influence of environmental factors and scene-object placements has been comparatively underexplored. In this paper, we present a novel trajectory prediction model that integrates both pedestrian interactions and environmental context to improve prediction accuracy. Our approach captures spatial and temporal interactions among pedestrians within a sparse graph framework. To account for pedestrian-scene interactions, we employ advanced image enhancement and semantic segmentation techniques to extract detailed scene features. These scene and interaction features are then fused through a cross-attention mechanism, enabling the model to prioritize relevant environmental factors that influence pedestrian movements. Finally, a temporal convolutional network processes the fused features to predict future pedestrian trajectories. Experimental results demonstrate that our method significantly outperforms existing state-of-the-art approaches, achieving ADE and FDE values of 0.252 and 0.372 meters, respectively, underscoring the importance of incorporating both social interactions and environmental context in pedestrian trajectory prediction.", "sections": [{"title": "1 INTRODUCTION", "content": "Advancements in artificial intelligence and machine learning have significantly propelled the field of autonomous vehicles, establishing it as one of the most compelling and challenging areas of research in recent decades [13]. Self-driving cars, envisioned as the future of transportation, require sophisticated systems capable of navigating safely and efficiently through complex and dynamic urban environments. A critical challenge for these systems is the accurate prediction of pedestrian trajectories [14].\nPredicting pedestrian movement in crowded and dynamic environments is a multifaceted and complex problem that necessitates a detailed analysis of human behavior and interactions with the surrounding environment. Unlike humans, who naturally anticipate others' paths using sensory information and cognitive processing, autonomous vehicles must achieve this capability through mathematical models and sophisticated algorithms. This involves not only understanding individual pedestrian behaviors but also capturing social interactions and adhering to unwritten social norms that people unconsciously follow in communal settings[5]."}, {"title": "1.1 Factors Affecting Pedestrian Trajectory Prediction", "content": "The primary factors influencing pedestrian trajectory prediction include:\nSpatial Position of Pedestrians in the Scene. The spatial position of a pedestrian relative to other pedestrians and elements within the traffic scene is a significant factor [1, 6]. For instance, when two pedestrians approach each other head-on, they instinctively adjust their paths to avoid a collision. Group interactions are also significant; individuals moving in a group establish coordinated movement patterns, influencing each other's trajectories."}, {"title": "2 RELATED WORK", "content": "Humans navigate crowded environments by adhering to unwritten social rules and conventions that govern their interactions with others and the environment [25]. For example, pedestrians typically avoid passing directly between two individuals engaged in conversation, as doing so would violate social norms. These implicit rules influence pedestrian trajectories and are critical for accurate path prediction models.\nEarly approaches to modeling pedestrian behavior focused on the Social Force Model [8], which introduced hand-crafted functions based on physical concepts of attraction and repulsion to simulate interactions in traffic scenes. While effective for short-term predictions, the model's computational complexity and limitations in capturing long-term dependencies made it less suitable for applications such as autonomous driving.\nThe advent of neural networks, particularly Long Short-Term Memory (LSTM) networks [24], revolutionized trajectory prediction by effectively modeling temporal dependencies. Alahi et al. [1] proposed the Social LSTM, extending traditional LSTMs to account for social interactions by incorporating neighboring pedestrians' hidden states. This approach enabled the modeling of longer sequences and more complex social behaviors.\nSubsequent research enhanced these models by integrating collective interactions and environmental contexts. Quan et al. [16] introduced a holistic LSTM framework that encodes collective interactions among pedestrians. Gupta et al. [6] developed the Social GAN, leveraging Generative Adversarial Networks to produce socially acceptable trajectories that respect social norms.\nRecognizing the importance of environmental factors, recent studies have incorporated scene information into trajectory prediction. Sadeghian et al. [19] proposed SoPhie, an attentive GAN that combines social and physical constraints by integrating scene context through visual features extracted from images. Their work demonstrated that neglecting scene information could lead to decreased prediction accuracy. Similarly, Convolutional Neural Networks (CNNs) have been utilized to capture spatial features of the environment, enhancing the model's understanding of physical constraints [25].\nFurther advancements involve the use of semantic segmentation and attention mechanisms. Syed and Morris [21, 22] integrated semantic scene information to refine trajectory predictions, highlighting the benefits of understanding scene semantics.\nGraph-based models have also gained prominence for their ability to capture complex interactions among multiple agents. Mohamed et al. [11] introduced Social-STGCNN, utilizing spatio-temporal graph convolutional neural networks to model pedestrian interactions over time. Shi et al. [20] proposed the Sparse Graph Convolution Network (SGCN), which efficiently models sparse interactions in crowded scenes, leading to improved trajectory prediction performance."}, {"title": "3 PROPOSED METHOD", "content": "We present an approach for accurately predicting pedestrian trajectories by integrating social interactions and environmental context. Our model leverages the Sparse Graph Convolutional Network (SGCN) [20] to capture pedestrian interactions within spatial and temporal sparse graphs. Additionally, we incorporate a scene feature extraction module and employ a cross-attention mechanism to enhance predictive capabilities."}, {"title": "3.1 Problem Definition", "content": "Trajectory prediction aims to estimate the future positions of all agents based on their historical states and surrounding scene information [1, 6]. At time t, the scene is represented by an image $I_t$. Each pedestrian $i \\in [N]$ is characterized by spatial coordinates $(x, y) \\in \\mathbb{R}^2$. The historical positions up to time t are:\n$X_i^{0:t} = \\{(x, y)_\\tau | \\tau = 1, ..., t\\}, \\forall i \\in [N]$,\nand the future ground truth positions from time t + 1 to T are:\n$Y_i^{t:T} = \\{(x, y)_\\tau | \\tau = t + 1, ..., T\\}, \\forall i \\in [N]$.\nOur objective is to learn a predictive model f parameterized by $W_\\theta$:\n$\\hat{Y}^{t:T} = f (I_t, X_i^{0:t}, W_\\theta)$,\nwhere $\\hat{Y}^{t:T}$ are the predicted future trajectories."}, {"title": "3.2 Overall Model", "content": "Our model comprises three primary components (Figure 2):\n(1) Feature Extraction Module\n(2) Interaction Module\n(3) Cross-Attention Module\nFirst, the Feature Extraction Module processes input frames using Real-ESRGAN [23] for image enhancement, followed by the OneFormer model [9] for semantic segmentation to generate semantic maps. Enhanced frames are passed through ResNet-18 [7] to extract visual features. Semantic maps are processed through additional convolutional layers to obtain semantic features. Visual and"}, {"title": "3.3 Feature Extraction Module", "content": "To generate high-level environmental representations, we enhance input frames using Real-ESRGAN [23] to improve image quality. The enhanced images are processed by the pre-trained OneFormer model [9] for semantic segmentation, producing detailed semantic maps that identify critical scene elements based on the Cityscapes dataset [4].\nWe use ResNet-18 [7] to extract robust visual features from the enhanced frames. Semantic maps are processed through additional convolutional layers to obtain semantic feature vectors. The visual and semantic features are concatenated and refined using an MLP to produce the scene representation $H_{scene}$."}, {"title": "3.4 Interaction Module", "content": "The Interaction Module utilizes the SGCN [20] to model pedestrian interactions:\n\u2022 Spatial Sparse Graph: Encodes group interactions at each time step, with nodes representing pedestrians and edges representing significant interactions. The adjacency matrix is sparse and learned during training.\n\u2022 Temporal Sparse Graph: Models motion tendencies over time, connecting temporal instances of each pedestrian. This captures sequential movement patterns.\nGraph convolutional layers extract features from these graphs, capturing both social dynamics and individual motion patterns. The fusion of spatial and temporal features provides a comprehensive understanding of pedestrian behaviors."}, {"title": "3.5 Cross-Attention Module", "content": "The Cross-Attention Module integrates scene features with interaction features to enhance trajectory prediction. The graph features serve as the Query (Q), while the scene features provide the Key (K) and Value (V):\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$,\nwhere $d_k$ is the dimensionality of the key vectors. This mechanism allows the model to focus on relevant environmental factors that influence each pedestrian's movement.\nThe attention output is:\n$H_{attn} = Attention(Q, K, V)$.\nTo preserve original interaction information and enhance training stability, a residual connection combines the attention output with the original graph features:\n$H_{fused} = H_{attn} + H_{graph}$.\nAs illustrated in Figure 2, the structure of our Feature Extraction Module and the pedestrian trajectory prediction pipeline are depicted."}, {"title": "3.6 Trajectory Prediction and Loss Function", "content": "The fused features $H_{fused}$ are input into the TCN to predict future positions:\n$\\hat{Y} = TCN(H_{fused})$,\nWe employ a composite loss function combining the Average Displacement Error (ADE) and the Final Displacement Error (FDE) [3]:\n$\\mathcal{L} = ADE + FDE$,\nwhere:\n$ADE = \\frac{1}{NT}\\sum_{i=1}^N \\sum_{t=1}^T ||Y_i^t - \\hat{Y}_i^t||$,\n$FDE = \\sum_{i=1}^N ||Y_i^T - \\hat{Y}_i^T||$.\nWe train the network end-to-end using stochastic gradient descent, updating all parameters except those of the pre-trained Real-ESRGAN and OneFormer modules, which remain fixed. This allows the model to effectively integrate scene context, social interactions, and temporal dynamics, enhancing prediction accuracy."}, {"title": "4 EXPERIMENTS", "content": "We evaluate our proposed method on pedestrian trajectory datasets and benchmark its performance against established baseline models. Additionally, we assess the quality of our semantic segmentation maps in comparison to those generated by alternative models."}, {"title": "4.1 Evaluation Datasets", "content": "To validate the effectiveness of our approach, we utilize two prominent public datasets:\n\u2022 ETH Dataset: Comprises two scenes\u2014ETH and HOTEL.\n\u2022 UCY Dataset: Includes three scenes\u2014UNIV, ZARA1, and ZARA2.\nEach scene captures real-world pedestrian trajectories characterized by rich social interactions and diverse environmental contexts. Following the standard protocol [1], we adopt a leave-one-out evaluation strategy, training the model on four scenes and testing on the remaining one in a rotating manner. Specifically, we observe pedestrian trajectories for 8 time steps (equivalent to 3.2 seconds) and predict the subsequent 12 time steps (equivalent to 4.8 seconds). Due to the absence of publicly available frame data for the UNIV"}, {"title": "4.2 Evaluation Metrics", "content": "We employ two standard metrics to quantitatively assess the performance of our trajectory prediction method:\n\u2022 Average Displacement Error (ADE): Computes the average Euclidean distance between the predicted trajectory points and the ground truth points across all predicted time steps.\n\u2022 Final Displacement Error (FDE): Measures the Euclidean distance between the predicted final position and the ground truth final position at the end of the prediction horizon."}, {"title": "4.3 Results", "content": "As shown in Table 1, our proposed model, ScenePTP, achieves superior performance across all datasets, outperforming baseline models in both ADE and FDE metrics. Specifically, our model reduces ADE and FDE by significant margins compared to previous state-of-the-art methods. Notably, in the HOTEL scene, our model reduces the ADE to 0.145 meters, improving upon the previous best of 0.31 meters achieved by SGCN. This substantial reduction underscores the effectiveness of incorporating detailed scene information and advanced interaction modeling in predicting pedestrian trajectories.\nThe combination of advanced image restoration, state-of-the-art semantic segmentation, and sophisticated interaction modeling enables our model to outperform existing methods. The substantial improvements in ADE and FDE metrics indicate that our model not only predicts trajectories more accurately on average but also achieves better precision in forecasting the final positions of pedestrians.\nFurthermore, as presented in Table 2, the model utilizing only frame features exhibits lower accuracy compared to the model that integrates both frame features and semantic maps. This demonstrates that combining semantic maps with frame features provides the model with a more comprehensive understanding of the pedestrians' surrounding environment, thereby enabling more precise trajectory predictions."}, {"title": "4.4 Ablation Study on Semantic Maps", "content": "To evaluate the impact of incorporating semantic maps into our model, we conduct an ablation study comparing the performance with and without semantic maps. In the first configuration (w/o Maps), we use features extracted directly from the raw frames without any semantic segmentation. In the second configuration (w/ Maps), we enhance the frame features by integrating semantic maps generated from the scene.\nTable 2 presents the ADE and FDE for both configurations across the datasets.\nThese results validate the effectiveness of incorporating semantic maps into our model. The inclusion of semantic maps consistently improves performance across all datasets, with an average reduction in ADE and FDE. This demonstrates that semantic maps provide valuable environmental context, enabling the model to better understand navigable spaces and obstacles, leading to more accurate trajectory predictions."}, {"title": "4.5 Qualitative Analysis of Semantic Segmentation", "content": "Figure 3 illustrates an analysis of our semantic maps against those generated by other state-of-the-art semantic segmentation methods, such as PSPNet [26] and SegNet [2]. The figure highlights the superior accuracy and detail achieved by our approach, particularly in complex urban environments where distinguishing between walkable areas and obstacles is critical for accurate trajectory prediction.\nOur semantic segmentation effectively captures intricate details such as narrow pathways and dynamic obstacles, which are often challenging for existing models to accurately delineate. This high-quality semantic information contributes significantly to the improved performance of our trajectory prediction model."}, {"title": "4.6 Impact of Image Restoration", "content": "We also assess the effect of image restoration on frame quality and semantic segmentation accuracy. Figure 4 shows the comparison between original low-resolution frames and restored high-resolution frames using Real-ESRGAN [23]. The improved image quality leads to more accurate and detailed semantic segmentation results, as seen in the segmentation maps."}, {"title": "4.7 Conclusion", "content": "In this paper, we investigated the impact of incorporating detailed scene information into pedestrian trajectory prediction models. Our approach introduced a novel integration of scene features with graph-based models. By effectively combining environmental context with social interaction modeling, our model significantly improved trajectory prediction accuracy. Our experimental results validate the effectiveness of our method, highlighting the importance of utilizing environmental context alongside graph representations to achieve more reliable and robust predictions in dynamic environments. The findings demonstrate that integrating scene understanding enhances the model's ability to anticipate pedestrian movements more accurately."}]}