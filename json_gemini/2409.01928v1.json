{"title": "Comprehensive Equity Index (CEI): Definition\nand Application to Bias Evaluation in Biometrics", "authors": ["Imanol Solano", "Alejandro Pe\u00f1a", "Aythami Morales", "Julian Fierrez", "Ruben Tolosana", "Francisco Zamora-Martinez", "Javier San Agustin"], "abstract": "We present a novel metric designed, among other applica-\ntions, to quantify biased behaviors of machine learning models. As its\ncore, the metric consists of a new similarity metric between score distri-\nbutions that balances both their general shapes and tails' probabilities. In\nthat sense, our proposed metric may be useful in many application areas.\nHere we focus on and apply it to the operational evaluation of face recog-\nnition systems, with special attention to quantifying demographic biases;\nan application where our metric is especially useful. The topic of demo-\ngraphic bias and fairness in biometric recognition systems has gained\nmajor attention in recent years. The usage of these systems has spread\nin society, raising concerns about the extent to which these systems treat\ndifferent population groups. A relevant step to prevent and mitigate de-\nmographic biases is first to detect and quantify them. Traditionally, two\napproaches have been studied to quantify differences between population\ngroups in machine learning literature: 1) measuring differences in error\nrates, and 2) measuring differences in recognition score distributions. Our\nproposed Comprehensive Equity Index (CEI) trade-offs both approaches\ncombining both errors from distribution tails and general distribution\nshapes. This new metric is well suited to real-world scenarios, as mea-\nsured on NIST FRVT evaluations, involving high-performance systems\nand realistic face databases including a wide range of covariates and de-\nmographic groups. We first show the limitations of existing metrics to\ncorrectly assess the presence of biases in realistic setups and then pro-\npose our new metric to tackle these limitations. We tested the proposed\nmetric with two state-of-the-art models and four widely used databases,\nshowing its capacity to overcome the main flaws of previous bias metrics.", "sections": [{"title": "1 Introduction", "content": "In the past decade, we have experienced a revolution in the field of Artificial Intel-\nligence (AI). The surprising capabilities of data-driven automatic systems have\nmade possible the development of AI-based solutions in a variety of domains,"}, {"title": "2 Measuring Fairness in Biometric Systems", "content": ""}, {"title": "2.1 Problem Formulation", "content": "Let us consider any 2-class classification problem (n-class can be developed as\nmultiple 2-class problems), in our case exemplified using Face Recognition (FR).\nOther AI setups apart from classification in which output probabilities for dif-\nferent data populations can be obtained are also easily covered by our methods.\nOther AI setups in which class probabilities are not straightforward, e.g., regres-\nsion, will need further work for our ideas to be properly developed. Let's now\nfocus for concreteness and without loss of generality in Face Recognition.\nTraditionally, a FR system operates in one of the following setups: i) Iden-\ntification or ii) Verification. Our interest here is in the latter, where the task is\nto determine whether two samples belong to the same identity or not with a 1:1\nsimilarity comparison, or match. If both samples belong to the same identity, the\nsamples are said to form a genuine pair, otherwise, we refer to it as impostor, i.e.,\na 2-class classification problem. In a real scenario, it is common that one of the\nbiometric samples in the pair is pre-enrolled in the system (reference sample),\nwhose identity is known. Thus, the system is presented with a second sample\n(probe sample) that claims to belong to the same identity as the reference.\nFormally, to measure the performance of the system let us consider a dataset\nof biometric samples, which contains N samples, i.e., face images I in the case\nof FR systems. Each of the images was captured from a subject, who is part\nof a demographic group $d_i$ according to its demographic traits (e.g., gender,\nethnicity, age). We assume here a set D of K demographic groups, which are\ndisjoint (i.e., a subject can only be a member of one group). A FR model wF\nis trained to extract face representations x = f(I|$w_F$) discriminant for the task"}, {"title": "2.2 Fairness Metrics: Existing Methods", "content": "Recently, Kotwal and Marcel have addressed the problem of measuring demo-\ngraphic fairness in biometric systems [27]. They argued how the community has\npaid mostly attention to differential outcome metrics, i.e., those which measure\nfairness as gaps in classification rates across groups [13,16,19,24]. Compared to\nthese, they proposed a differential performance metric based on the distances\nbetween score distributions z. This approach presents the main advantage of\nbeing agnostic to the operational point selected, thus measuring the fairness of\nthe overall system.\nConcretely, the metric proposed in [27] leverages the Kullback-Leibler (KL)\ndivergence as the basic distance measure among the distributions of each de-\nmographic group. The metric, known as the Distribution Fairness Index (DFI),\nspans values between 0 and 1, where a value close to the latter represents a fairer\nmodel. Formally, DFI is defined as follows (using the same notation as in [27]):\n\n$Z_{Dmean} = \\frac{1}{K} \\sum_{i=1}^{K} Z_{Di}$\n\n\n$DFI_N = 1 - \\frac{1}{K \\log_2 K} \\sum_{i=1}^{K} S_i$\n\nwhere $Z_{D_i}$ is the combined (genuine + impostor) distribution for the demo-\ngraphic group $d_i$, normalized so that the curve area sums one, and $S_i$ is the\nKL divergence among $Z_{D_i}$ and the mean distribution $Z_{Dmean}$. The formulation in\nEq. 2 corresponds to a baseline definition of DFI (Normal, therefore N), which"}, {"title": "2.3 Proposed Metric: Comprehensive Equity Index (CEI)", "content": "In this section, we present an extension of the metric of [27] to measure fairness.\nOur proposal tries to keep the benefits of performance-based metrics while in-\ntegrating the error-based perspective of differential outcome metrics. With this\nbalance, we are not only aiming to measure the model's bias but also to consider\nhow competitive the recognition system is, a relevant aspect in systems with\nvery small error rates.\nBy examining the evaluation of high-performance models (e.g., those pre-\nsented to NIST FRTE) with the DFI metric on state-of-the-art datasets, such\nas RFW [48] or BUPT-B [46], we noticed that error rates associated to demo-\ngraphic biases are not captured with the cited metric. We hypothesize that,\nsince DFI uses the entire distribution to measure fairness i) the tail has a lit-\ntle relevance in the computation and ii) genuine and impostor distributions are\ntreated as a whole, hence hindering the assessment of any bias present in ei-\nther of them. In comparison, differential outcome metrics such as GARBE [25]\nor Inequity [19] can capture these biases, since the selection of an operational\npoint directly focuses the evaluation on the tails of the distributions. However,\nmeasuring fairness for a concrete operational point presents some drawbacks.\nFirst, the demographic bias underlying the core of the biometric system is not\ncaptured at all, so information about the rest of the distribution is lost. Second,\nby considering only the tail of the distribution, the performance is measured in\na lower percentage of samples than when using the entire curve. Thus, outcome\ndifferences could be due to reasons beyond demographic attributes, for instance,\nimage resolution, brightness, or pose covariates.\nWe aim to overcome the aforementioned shortcoming by presenting a new\nfairness measure built on the proposal of Kotwal and Marcel [27]. Specifically,\nour objective is to have a metric that is both threshold-agnostic and able to\nmeasure bias in genuine and impostor distributions independently while prop-\nerly accounting for the tails, i.e., where errors occur. We introduce here the\nComprehensive Equity Index (CEI). For every demographic group, the CEI first\nsplits each distribution (i.e., genuine or impostor) into two groups based on a\ngiven percentile Ps (i.e., score threshold s corresponding to certain accumulated\nprobability P), dividing the tail from the rest of the distribution (referred to\nas center distribution from now on). The intuition here is to have independent\ncomponents so we can assign them different weights when computing fairness.\nOnce the distribution is split, we can compute a score S (dissimilarity score as\nwe are using distance measures) between a demographic distribution and the\nmean distribution as follows:\n\n$S'(P_s) = w_t \\cdot D_{KL}(Z_{D_i}^t || D_{mean}^t) + w_c \\cdot D_{KL}(Z_{D_i}^c ||D_{mean}^c)$\n\nwhere $zh_i$ and $z_Di^c$ are respectively the tail and center distributions from $Z_{D_i}$,\n$z_{Dmean}^t$ and $z_{Dmean}^c$ refer to mean distributions as defined in Eq. 1, and $w_t$ and\n$w_c$ are manually-tuned weights controlling the trade-off between the relevance\nof each part in the similarity score. The term in Eq. 8 is computed for each"}, {"title": "3 Material and Methods", "content": ""}, {"title": "3.1 Models and Databases", "content": "For the present work, we have trained two face recognition models from scratch\nfor face recognition. The models were trained with a margin-based loss, i.e., Cos-\nFace [45], on the WebFace database [49], which contains 260M images from 4M\nidentities. The database includes images from 7 different race groups, with more\nthan half of the identities being Caucasian. Similarly to the models evaluated\nin [49], we assessed the performance of the trained models on IJB-C [28]. These\nmodels will be used later in our experiments:\nResNet-100 [22]. The ResNet architecture is one of the most famous con-\nvolutional models of the last decade. Here, we have used the architecture\nwith 101 convolutional layers. The trained ResNet-100 model exhibits a\nFNMR@FMR=1e-5 of 0.0407.\nPropietary Model. A commercial model submitted to the NIST FRTE 1:1\nwith a FNMR@FMR=0.0003 of 0.0058. When evaluating this model on the\nIJB-C [28] dataset, we obtained FNMR@FMR=1e-5 of 0.037.\nThroughout the experiments carried out in the present work, we will use the\nfollowing publicly available databases: MORPH [37,44], RFW [48], and BUPT-\nB [46]. All three databases include demographic labels with the gender and\nethnicity of each subject. In addition to the aforementioned databases, we have\nused in this work a synthetic database recently released for the FRCSyn Chal-\nlenge [30] with realistic conditions and controllable demographics."}, {"title": "4 Experiments", "content": "In this section, we present different experimental scenarios in which we show the\nusefulness of the proposed metric to measure the (un) fairness of high-performing\nface recognition models. In Section 4.1 we present a toy scenario to elaborate\non and numerically assess the advantages of the presented metric in comparison\nto existing metrics. Finally, experiments on real images are conducted in Sec-\ntion 4.2, where we evaluate a high-performance industry model and compare our\nproposed metric with existing methods."}, {"title": "4.1 Synthetically-Generated Distributions", "content": "In the following, we present experiments on synthetically-generated similarity\nscore distributions, simulating the performance of a competitive model. Three\nscenarios are considered (see Fig. 1). First, we manipulated the left tail of the\ngenuine distribution, i.e., the right distribution in Fig. 1 (left), to increase the\nfalse rejections in that region. We have called this scenario Biased Genuine dis-\ntribution tail (BG). This name is given because in the overlapping region be-\ntween the two distributions (genuine and impostor), the genuine tail is forced\n(biased) to have an atypically high probability (considering as typical a rapid\ndecrease similar in nature to a normal distribution tail, e.g., as shown in the\nimpostor distribution).\nThe second scenario is similarly created for a Biased\nImpostor distribution tail (BI). Finally, in the third scenario, both distributions\n(genuine and impostor) have similar probabilities in their tails, but their centers\nare shifted. We have called this scenario Biased genuine-impostor distribution\nCenters (BC). The first two scenarios are expected to be well captured by the\nINFMR (INFNMR) and GARBEFMR (GARBEFNMR) metrics, as changes in the\ntail are more relevant here, whereas the distribution changes introduced in the\nthird scenario will, in principle, be better captured using both DFIN and DFIE\nmetrics, as the distribution tails in that case are similar and hence present an\nidentical error rate."}, {"title": "4.2 Evaluation in Real Scenarios", "content": "In Fig. 2 the genuine and impostor distributions for a ResNet-100 [22] model\ntrained over the WebFace database [49] for the MORPH [37,44] and RFW [48]\ndatasets is depicted. For each of the datasets, each curve represents a demo-\ngraphic group based on the ethnicity. We have evaluated the normal variant\nof the CEI metric described in Eq. 9 with different configurations. More con-\ncretely, we analyze its behavior using percentile values of 75%, 90%, 95%, and,\nbased on the observations from Section 4.1, weight values of (wtail, Wcenter) =\n{(0.5, 0.5), (0.8, 0.2)}. Therefore, the proposed metric is able to detect the bias in all three presented\ncases, regardless of the weight parameters used. This is a desired behavior not\nobserved with any of the other metrics in the literature. Thus we conclude that\nour proposed CEI has the potential to overcome some of the weaknesses of the\noriginal DFI. However, this needs to be assessed in real-world scenarios."}, {"title": "5 Conclusions", "content": "In this work, we follow up on previous efforts to measure \"fairness\" in bio-\nmetric recognition systems by using a differential performance-based approach,\ndependent on the system score function. We have introduced a modification of a\nprevious metric by adapting it into its application to real-world scenarios where\nthe differences are found in the score distribution tails. The proposed metric,\ncalled Comprehensive Equity Index (CEI), has been shown to capture existing\ndifferences in the score distributions for different demographic groups when eval-\nuating a high-performance Face Recognition (FR) system presented in the NIST\nFRVTE 1:1 (with excellent results) in several state-of-the-art datasets.\nOur proposal addresses previous weaknesses of differential performance met-\nrics by parameterizing the relevance of the tail distribution differences for di-\nverse demographic groups with a percentile selecting the tail and weights that"}]}