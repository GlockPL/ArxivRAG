{"title": "Multi-granularity Contrastive Cross-modal Collaborative Generation for End-to-End Long-term Video Question Answering", "authors": ["Ting Yu", "Kunhao Fu", "Jian Zhang", "Qingming Huang", "Jun Yu"], "abstract": "Long-term Video Question Answering (VideoQA) is a challenging vision-and-language bridging task focusing on semantic understanding of untrimmed long-term videos and diverse free-form questions, simultaneously emphasizing comprehensive cross-modal reasoning to yield precise answers. The canonical approaches often rely on off-the-shelf feature extractors to detour the expensive computation overhead, but often result in domain-independent modality-unrelated representations. Furthermore, the inherent gradient blocking between unimodal comprehension and cross-modal interaction hinders reliable answer generation. In contrast, recent emerging successful video-language pre-training models enable cost-effective end-to-end modeling but fall short in domain-specific ratiocination and exhibit disparities in task formulation. Toward this end, we present an entirely end-to-end solution for long-term VideoQA: Multi-granularity Contrastive cross-modal collaborative Generation (MCG) model. To derive discriminative representations possessing high visual concepts, we introduce Joint Unimodal Modeling (JUM) on a clip-bone architecture and leverage Multi-granularity Contrastive Learning (MCL) to harness the intrinsically or explicitly exhibited semantic correspondences. To alleviate the task formulation discrepancy problem, we propose a Cross-modal Collaborative Generation (CCG) module to reformulate VideoQA as a generative task instead of the conventional classification scheme, empowering the model with the capability for cross-modal high-semantic fusion and generation so as to rationalize and answer. Extensive experiments conducted on six publicly available VideoQA datasets underscore the superiority of our proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "Learning to answer discretionary free-form questions based on long-term videos has been an increasingly popular and challenging research problem, emphasizing discriminative uni-modal understanding and comprehensive cross-modal interac-tion to accurately infer answers. The complexity and multiplic-ity of long-term videos make it a more demanding task than conventional VideoQA [1], [2], [3], [4], [5]. Unlike short-term video clips with more straightforward semantics, untrimmed long-term videos tend to preserve significant redundancy and noise owing to their overly prolonged sequential frames, thus raising high requirements for models' capability and computation efficiency. To tackle the challenge, many long-term VideoQA approaches have emerged from myriad angles, e.g., discriminatory video-linguistic modeling [6], [7], [8], mighty sampling schemes [9], [10], and sufficient cross-modal interaction [11], [12] mechanisms.\nDespite their considerable performance, most existing mod-els share consistent bottlenecks: 1) Non-associative unimodal representation: Modality-independent offline feature extractors are employed to detour the costly computation overhead in uni-modal representation modeling, especially for lengthy videos. The representations are learned intrinsically separated, ignor-ing the interplay and correlations between different modalities. 2) Asymmetric video-question paradigm: The relationship"}, {"title": "II. RELATED WORK", "content": "This section provides a review of pivotal research in video question answering and video-language pre-training models, offering valuable context for our contributions.\nA. Video Question Answering\nVideoQA has earned increasing popularity in recent vision-language bridging research. As a straightforward but tougher extension of the ImageQA task, it targets exploring interactive intelligence to infer reliable answers by extensive communica-tion with complicated real-world videos via natural language questions.\nThe earliest work [18] tried to employ a sequence-to-sequence framework directly extended from the ImageQA model [19] to answer multiple choice questions over the video frame sequences. To overwhelm the inadequacy of modeling the video temporal details, Zhao et al. [20] leveraged the hier-archical attention mechanism to capture frame and clip dual-level video dynamics. Subsequently, numerous attention mod-els flourished to study for a better focus on crucial linguistic-guided visual facts. From the spatial-temporal attention per-spective, Jiang et al. [21] effectively localized critical temporal frames from the video and figured out crucial spatial regions from the individual frame. Considering the significance of capturing far-distant dependency, Gao et al. [4] employed co-memory networks to model both appearance and motion evidence to infer accurate answers. To preciously associate"}, {"title": "III. METHOD", "content": "The proposed multi-granularity contrastive cross-modal col-laborative learning network is illustrated in Figure 2. Un-like the prior asymmetric format, this paper reformulates the VideoQA paradigm with a one-to-one symmetric pattern in a triplet (C, Q, A) fashion. Specifically, for an arbitrary question Q, the model stochastically picks out RGB frames sparsely in real-time to reconstruct a new augmented video clip Cand performs multi-granularity cross-modal collaborative learning to generate the answer A correctly. Note that the model is optimized end-to-end, ensuring gradient flow support throughout the entire framework.\nA. Joint Unimodal Modeling\nJoint Unimodal Modeling (JUM) emphasizes exploring intra-modal underlying instructive interactions among sub-components with the supervision of another modality in a clip-base structure [13].\n1) Intra-Video Model: To efficiently capture rich visual details from sparse frames while circumventing the expensive computation demands, we introduce an Intra-Video Module (IVM) based on TimeSformer[57] architecture. Considering"}, {"title": "B. Multi-Granularity Contrastive Learning", "content": "To harness multi-granular correspondence and facilitate the generation of high-quality intra-modal semantics covering broad visual concepts, we introduce a novel Multi-granularity Contrastive Learning (MCG) strategy to activate the joint unimodal model by leveraging external web-sourced video-language pairs. We incorporate contrastive learning from two aspects: coarse-grained instance-level contrastive learning and fine-grained token-level contrastive learning.\n1) Instance-grained Contrastive Learning: To capture the global semantic consistency, we incorporate Instance-grained Contrastive Learning (ICL) to encourage positive cross-modal pairs to be mapped nearby while negative pairs are as far apart as possible in the shared semantic space. Following [13], we first adopt two linear functions $g_x(\u00b7)$ and $g_y(\u00b7)$ to project the instance-wise semantics ${x_{cls}}$ and ${Y_{cls}}$ into a normalized space. Then, we perform the similarity function on the intra-modal video semantic $X$ and the intra-modal text semantic $y$ as follows:\n$sim(X,Y) = g_x(x_{cls})\u00b7g_y(Y_{cls})$                                                             (1)\nWe use the symmetric temperature-normalized contrastive learning strategy to maximize the interactions between the"}, {"title": "C. Cross-modal Collaborative Generation", "content": "The Cross-modal Collaborative Generation module (CCG) equips our model with the capability for cross-modal in-teraction and generation so as to reason and describe. We design a cross-modal fusor to enable the deep interaction of multimodal information with cross-attention blocks and an answer generator to generate answers conditioned on the referenced video.\n1) The Cross-modal Fusor: Taking the video and linguistic semantics as inputs, the Cross-modal Fusor (CFor) is dedicated to generating a fused cross-modal reason result by explor-ing deeper informative interaction and communication with stacked transformer blocks. Each transformer block comprises a Self-Attention (SA) layer, a Cross-Attention (CA) layer, and a Feed-Forward Network (FFN). An additional [FUS] token is appended to deliver the fused cross-modal reason result. We adopt the widely applied video-text matching (VTM) loss $L_{VTM}(\u00b7; T_{cf}, T_{vm})$ to activate the CFor module for learning cross-modal fusion, capturing fine-grained interactions and alignments between different modalities. Afterward, we intro-duce a fully connected layer to the CFor output, generating a two-category probability, $p_{vtm}$. H calculates the binary cross-entropy between $p_{vtm}$ and the ground-truth $q_{vtm}$.\n$L_{VTM}(\u00b7; T_{cf}, T_{vm}) = E_{(x,y)\u223cD}H(q_{vtm}, p_{vtm} (X, Y))$                                                             (9)\n2) The Answer Generator: The Answer Generator (AGor) targets generating open-ended answers. Conditioned on the referenced video and the fused reason evidence, the AGor plays the text generator roles by employing a CFor-similar transformer while replacing the SA layer with casual self-attention following [55], [56]. Additionally, tokens [GEN] and [EOS] are separately added to signal the task and the end. Re-cent works [59] reveal that language modeling loss facilitates the model with the generalization ability to transform visual facts into coherent descriptions. Building on this inspiration, we activate the AGor module using Language Modeling Loss"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "We introduce MCG, an end-to-end multi-granularity con-trastive cross-modal collaborative generative model for long-term VideoQA. MCG employs joint unimodal modeling to derive discriminative representations possessing high visual concepts and leverages a novel multi-granularity contrastive learning strategy to harness the intrinsically explicitly ex-hibited semantic correspondences. At its core, MCG for-mulates VideoQA as a generative task to reconcile existing discrepancies in VideoQA task formulations with a cross-modal collaborative generation module. It empowers MCG with the capability for cross-modal high-semantic fusion and generation to rationalize and answer, ensuring a more intuitive and effective approach to generating answers. MCG sets new benchmarks on four VideoQA datasets and shows strong generalization across diverse tasks."}]}