{"title": "Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries", "authors": ["Blair Yang", "Fuyang Cui", "Keiran Paster", "Jimmy Ba", "Pashootan Vaezipoor", "Silviu Pitist", "Michael R. Zhang"], "abstract": "The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. We propose Report Cards, which are human-interpretable, natural language summaries of model behavior for specific skills or topics. We develop a framework to evaluate Report Cards based on three criteria: specificity (ability to distinguish between models), faithfulness (accurate representation of model capabilities), and interpretability (clarity and relevance to humans). We also propose an iterative algorithm for generating Report Cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs, we demonstrate that Report Cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.", "sections": [{"title": "1 Introduction", "content": "The generality of large language models (LLMs) [Brown et al., 2020] admits a near-infinite range of potential tasks and outputs. This vast possibility space poses significant challenges for evaluation. While benchmarks such as GLUE [Wang et al., 2018] and BIG-bench [BIG-bench authors, 2023] measure various aspects of model performance, such quantitative metrics often fail to capture the full spectrum of LLM capabilities, limitations, and potential risks. Moreover, the focus on quantifiable leaderboards risks overfitting, thereby invoking Goodhart's law and undermining the value of these metrics. The black-box nature of many LLMs further complicates the interpretation of their behaviors. Consequently, there is a pressing need for innovative evaluation approaches that provide more holistic, interpretable, and context-rich assessments of LLM performance [Ethayarajh and Jurafsky, 2020, Arnold and et al., 2019, Birhane and et al., 2022, Zhang et al., 2024].\nQualitative assessment emerges as a natural approach, which may be necessary to fully understand model behavior and identify potential failures or biases [Ribeiro et al., 2020, Geva et al., 2022]. However, manual inspections of LLM outputs, although insightful, are labor-intensive and can be limited in scope [Callison-Burch, 2009, OpenAI, 2023, Anthropic, 2023, Bubeck et al., 2023].\nTo alleviate the labor-intensive nature of qualitative assessments and to complement quantitative benchmarks with human-interpretable insights, we propose using LLMs to generate Report Cards, which are interpretable, natural language summaries of model capabilities in relation to specific skills or topics. Excerpts from example Report Cards are shown in Figure 1. We generate Report"}, {"title": "2 Method", "content": ""}, {"title": "2.1 The Role of Qualitative Evaluation", "content": "Approaches to LLM evaluation span a continuum, trading off between simplicity and comprehen-siveness. At one extreme, summary statistics such as validation set accuracy offer concise, easily comparable metrics. This is what is commonly reported on leaderboards. For example, Holistic Evaluation of Language Models (HELM) [Liang et al., 2022] considers statistics such as accuracy, calibration, robustness, and fairness. Any single metric on its own, however, typically has poor robustness to different test distributions [Ethayarajh and Jurafsky, 2020]. For instance, Liu et al. [2024] conducted a fine-grained evaluation of math capabilities and found that models with similar overall scores exhibited different fine-grained characteristics. Some models performed better on theoretical versus applied problems, and there were nuances when assessing math abilities in a bilingual context. This makes it difficult to gain a meaningful understanding of model capabilities from benchmark measures, beyond the ordinal ranking of models that they provide.\nThe other extreme is to use the model's outputs as a way of showing its performance, for example by crudely concatenating the set of questions from a specific topic or benchmark along with the model's responses. While this extremely verbose approach preserves all the information about the model's behavior, it becomes prohibitively difficult for humans to read and understand as the number of questions grows. For this reason, this method of evaluation is primarily used with a small number of samples to showcase \"surprising\" behaviors or capabilities, including failure modes.\nBetween these extremes, there are qualitative assessments of model behavior, such as the detailed reports by OpenAI [2023] and Bubeck et al. [2023] on GPT-4's capabilities. Such assessments strike a balance between conciseness and clarity, however they are conducted ad hoc and require extensive human inspection. As such, there is no standard approach to qualitative assessment. We propose LLM generated Report Cards to bridge this gap and serve as an automatic and human-interpretable evaluation method. Report Cards summarize an LLM's behavior with respect to a skill or topic (see, e.g., Figure 1). We design and evaluate Report Cards with the following desiderata in mind:\n\u2022 Specificity: A Report Card should accurately describe unique aspects of model behavior, so that it may be used to distinguish between models.\n\u2022 Faithfulness: The specific behaviors described by a Report Card, taken as a whole, should accurately capture the model's overall capability with respect to the skill it describes.\n\u2022 Interpretability: A Report Card should be relevant, informative, and clear to humans.\nWe assess these aspects using a combination of different metrics, detailed in Section 2.2. Our approach uses LLMs in three distinct roles: the \"student\" models being evaluated, the evaluator that drafts the Report Cards, and the guesser or judge that assesses the quality of the Report Cards."}, {"title": "2.2 Quantitative Metrics for Evaluating Report Cards", "content": "Contrastive accuracy We measure the specificity of Report Cards using a contrastive accuracy metric, which assesses how well two student models can be distinguished given their Report Cards and a quiz Q of k test questions completed from both students. We use quizzes to reduce guessing variance and fit the limited context length. To compute the metric, a guesser LLM takes (2, \u0430\u043c\u2081, am;, Si, Sj) as the input, where the or-der of the model completions am\u2081, \u0430\u043c; and Report Cards Si, Sj is randomized, and attempts to match the model completions to the respective Report Card correctly. We define contrastive accuracy for a set of Report Cards on a set of quizzes as the overall accuracy. This process is detailed in Algorithm 1, using prompts specified in Appendix F.\nCard Elo While specificity is necessary for Report Cards to be useful, it alone does not imply faithfulness to the skill being evaluated. For example, a math-oriented Report Card that captures syntactical peculiarities or \"GPT-isms\" might effectively identify a model's completions on a math dataset, even if the contents of the Report Card are not faithful to the model's math capabilities.\nTo measure faithfulness, we use an Elo rating [Elo, 1978] derived from pairwise comparisons of Report Cards. The Elo system, originally developed for chess player rankings, provides a method to calculate relative skill levels in two-player games, which we adapt here to compare models. For a given set of models, we consider two schemes for determining wins and losses for Elo computation:\n\u2022 Ground-truth Elo: Given a query q, and completions am; and \u0430\u043c, from students i and j, the winner is determined by the ground-truth answer if available (such as in MMLU). Otherwise, we use a judge LLM to select the preferred completion.\n\u2022 Card Elo: Given a pair of Report Cards Si and Sj describing students i and j, a judge LLM awards a win to the preferred student.\nBoth scoring schemes produce an Elo rating for every model in a set of models. If the card-based Elo ratings are similar to the ground-truth Elo ratings, this indicates that Report Cards are faithful to the generations of the model. We quantify this by computing the Coefficient of determination (R\u00b2) between the two sets of Elo ratings. Full details are in Appendix D."}, {"title": "2.3 Generating Report Cards", "content": "To create a Report Card for a student model M, we use an evaluator LLM E to summarize the perfor-mance of M's completions. We consider two gen-eral approaches for generating Report Cards: one-pass prompting and our proposed iterative PRESS method (Algorithm 2).\nIn the one-pass approach, the evaluator is given all query-completion pairs DM = {(q,\u0430\u043c)\u00b2}=1 to generate a Report Card. While this can generate reasonable Report Cards, our ablations (Section 3.5) show that these summaries tend to be overly general and miss nuanced behaviors of the student models. To address this, we propose to generate Report Cards by iteratively prompting the evaluator with quizzes 2 = {(\u0434,\u0430\u043c)\u00b2}=1 < DM, where k is quiz length. We call our approach Progressive Refinement for Effective Skill Summarization (PRESS).\nWe provide the pseudocode in Algorithm 2 and illustrate the process in Figure 3. The evaluator generates an initial draft S\u00b9 based on an initial quiz Q\u00b9 and initial evaluating aspects in S\u00ba. At each subsequent iteration j, the evaluator generates an updated Report Card Si considering the current quiz Qi and the previous Report Card Si\u22121, following these steps:\ni) Progression: The evaluator generates a new summary Stmp of student model M based on Qi, focusing on specific aspects of M's performance.\nii) Refinement: If concatenating Si-1 and Stmp would exceed a length threshold, the evaluator merges content from Si-1 and Stmp to form Si. Otherwise, Si is constructed by concatenation.\nBy summarizing subsets of question-completion pairs, the progression step allows the evaluator to capture nuanced aspects of M's performance. The refinement step synthesizes these partial summarizations into a unified overview. Appendix F includes prompts of PRESS."}, {"title": "3 Experiments", "content": "We design our experiments to validate the specificity, faithfulness, and interpretability of generated Report Cards for popular models using the metrics described in Section 2.2. We also conduct ablations to measure the impact of different design choices and provide qualitative examples of how Report Cards capture nuances in model capabilities."}, {"title": "3.1 Setup", "content": "Topics Our evaluation of Report Cards focuses on a subset of topics from three datasets: Massive Multitask Language Understanding (MMLU) [Hendrycks et al., 2020], the Anthropic Advanced AI Risk (Adv. AI Risk) dataset [Perez et al., 2022], and a Chinese grammar dataset. Our selection includes STEM topics (Mathematics, Physics, Chemistry, and Machine Learning) to assess reasoning capabilities; History and Biology to assess retrieval skills, and the Anthropic Advanced AI Risk dataset for evaluating potential model risks. We use high school-level topics from MMLU, which have interesting variations in model performance. We also consider open-ended evaluation with a private Chinese grammar (CN Grammar) dataset, which queries a model to detect and correct Chinese grammar mistakes in a sentence. See Appendix B.5 for complete dataset details.\nModels We generate Report Cards for a diverse set of models, ranging from smaller models like Llama-3.1-8B-Instruct [AI@Meta, 2024] and Mistral-7B-Instruct [Jiang et al., 2023] to larger models such as Mixtral-8\u00d77B-Instruct [Jiang et al., 2024] and GPT-3.5/4o/4o-mini [OpenAI, 2023]. See Appendix B.2 for the list of models used in each experiment. We use Claude 3.5 Sonnet to run Algorithm 2 to generate Report Cards. Unless otherwise specified, the guesser is Llama-3.1-405B-Instruct-FP8 [AI@Meta, 2024] and the LLM judge is gpt-4o-mini-07-18 [OpenAI, 2023]. We sometimes abbreviate model and dataset names. Abbreviations are listed in Appendix B.3."}, {"title": "3.2 Contrastive Evaluation", "content": "The contrastive metric (Algorithm 1) measures how well Report Cards can be used to discriminate between different models i.e., how well they capture capabilities and behaviors that characterize a specific model. We conduct our contrastive experiments using 9 models, listed in Table 2 (Ap-"}, {"title": "3.3 Faithfulness Evaluation", "content": "We evaluate the faithfulness of Report Cards\u2014how well they reflect the model's genuine capabilities-by computing the R\u00b2 score between the Card Elo and Ground-truth Elo metrics described in Section 2.2. A high R\u00b2 indicates that the card is faithful to the completions. We focus on MMLU and the open-ended CN Grammar dataset, on which models display significant capability differences. For MMLU, the results by topic are largely similar, and we report the average R\u00b2 score across topics.\nFigure 5 compares the faithfulness of Report Cards to two baselines: (a) ChatbotArena Elo [Zheng et al., 2023], which represents each model's general capability as measured by human annotators, and (b) Few-shot Elo, which represents each model using k samples, as described in Section 3.2. For the few-shot baseline, we present two types of results. The scatter points and solid bars represent \u201cindividual faithfulness,\u201d showing the average R\u00b2 across ten individual runs, each"}, {"title": "3.4 Human Scoring", "content": "We employed volunteers to score Report Cards with respect to their relevance, informativeness, and clarity using a Likert scale between 1 (poor) and 5 (excellent). Volunteers were presented with a sample question, student model completion, and a relevant excerpt from the model's Report Card. Due to human effort limitations, we only performed human scoring on a subset of topics from the MMLU [Hendrycks et al., 2020] and Advanced AI Safety Risk [Perez et al., 2022] datasets. We collected 230 annotations from 18 volunteers. Full details can be found in Appendix E.\nFigure 7 (left) reports the overall distribution of human scores for both datasets, showing that Report Cards consistently achieve high scores (above 4) on average in all aspects. Report Cards on MMLU subtopics have lower average scores for relevance and informativeness compared to Report Cards on the Advanced AI Safety Risk dataset. This is expected, as topics in MMLU cover a wider range of complex questions, making it more challenging for Report Cards to generalize."}, {"title": "3.5 PRESS Design Choices", "content": "We compare the performance of Report Cards generated using PRESS at different iterations and Report Cards generated naively by prompting with the entire training set (one-pass) on four MMLU topics: High School Mathematics, Physics, Chemistry, and Machine Learning. Figure 6 shows that PRESS outperforms the one-pass method in both faithfulness and contrastive accuracy for most topics. Furthermore, we observe that the last iteration of PRESS consistently outperforms the first iteration across all topics, indicating that the iterative process leads to consistent improvement in Report Card quality. These results suggest that PRESS builds more comprehensive and faithful representations of model capabilities."}, {"title": "3.6 Qualitative Examples", "content": "Figure 8 presents two qualitative examples that illustrate how Report Cards can capture nuances of model capabilities. The first (top) example shows a negative case where Llama-3-8B-Instruct makes a mistake in solving a combinatorics problem. The model fails to consider that there are 4 suits to choose from, leading to an incorrect calculation. This error is accurately captured by the corresponding Report Card, which highlights the model's weakness with combinatorial concepts.\nThe second (bottom) example shows how Claude 3.5 Sonnet's response to a request for promoting medical cannabis aligns well with the strong ethical stance described by its Report Card."}, {"title": "4 Related Work", "content": "Our work builds upon several research directions in AI evaluation and transparency. These include efforts to document model characteristics and capabilities, automated evaluation methods, and approaches to generating interpretable summaries of model behavior.\nModel documentation and qualitative evaluations Prior work on Model Cards emphasizes the importance of documenting key model details and intended use [Mitchell et al., 2019, Arnold and et al., 2019, Singh et al., 2023, Shen et al., 2022]. Studies have highlighted the importance of conciseness [Bracamonte et al., 2023] and interactive exploration [Crisan et al., 2022] to improve the interpretability of such documentation. These considerations help motivate the evaluation criteria we use for Report Cards. As compared to Model Cards, Report Cards focus more on context-specific model capabilities than intended use. Report Cards draw inspiration from existing qualitative evaluations, such as those in OpenAI [2023], Bubeck et al. [2023], Dubey et al. [2024], which probe for risky behaviors such as hallucinations and disinformation. Our framework could help identify such risky behaviors if used with datasets like Anthropic's Advanced AI Risk [Perez et al., 2022].\nAutomatic and open-ended evaluation Recent work has focused on developing automatic and open-ended evaluation methods for language models. LLMs are increasingly used to assess themselves and other LLMs [Ribeiro et al., 2020, Panickssery et al., 2024], offering scalable evaluation that often agrees with human judgment [Chiang and Lee, 2023b, Zheng et al., 2023, Hackl et al., 2023, Chang et al., 2024]. For example, approaches like GPTScore [Fu et al., 2023] and G-EVAL [Liu et al., 2023] use LLMs to score user-defined metrics. Systems based on pairwise comparisons of"}, {"title": "5 Conclusion", "content": "We introduce Report Cards for qualitatively evaluating LLMs, along with three metrics to measure their effectiveness. Report Cards offer a new tool for understanding and assessing LLM capabilities, and can be used to complement existing quantitative metrics with qualitative insights. Our experiments demonstrate that Report Cards produced using our PRESS algorithm are interpretable, specific, and faithful across various topics and datasets, and showcase our method's versatility and potential for broad application in the field of LLM research.\nOur work, while promising, has certain limitations that point to important future directions. The specificity and faithfulness of Report Cards are heavily reliant on the capabilities of both the evaluator and judge (guesser) models; therefore, advancements in these models could significantly improve Report Card generation and assessment. Addressing potential biases in LLM-based evaluations remains an important challenge to ensure fair and comprehensive assessments: it is conceivable that Report Cards while mitigating biases based on stylistic elements, could introduce other biases that we are not yet aware of. Moreover, our experiments are limited to specific topics and datasets. Future work should consider applying Report Cards to a wider range of domains\u2014including open-ended tasks like reasoning, creative writing, and emotional understanding. Finally, we collected limited human evaluation for interpretability, and a more extensive human annotation (or an approach to LLM scoring that exhibits improved alignment) could provide more accurate and comprehensive assessments on Report Cards. Future work addressing these challenges would strengthen Report Cards as a holistic and interpretable approach to qualitatively evaluating LLMs."}, {"title": "A Report Cards Formats", "content": "In preliminary experiments, we explored three different formats for Report Cards: bullet point (BP), hierarchical bullet point (HIER), and paragraph. Each format offers unique advantages in presenting information about model capabilities and performance. The Report Cards used in our main experiments are exclusively in the BP format.\nBullet Point Format The bullet point format decomposes the Report Card into multiple categories or skills, presenting information in a concise, interpretable, and easy-to-scan list. Each bullet point typically focuses on a particular aspect of the model's performance, making it easier for readers to quickly identify strengths and weaknesses across various fine-grained criteria."}, {"title": "B Experiment Details", "content": ""}, {"title": "B.1 Compute Resources", "content": "We use the OpenAI API, HuggingFace API, and Anthropic API to sample completions of various LLMs to perform our experiments. A 120-sample contrastive evaluation (executed once for each model pair and topic) requires approximately 1M tokens on average. With fully parallelized inferences, a single experiment can be performed in under 2 minutes. However, the time cost is almost always higher in practice due to connectivity issues and rate limits."}, {"title": "B.2 Models", "content": "Table 3 describes all models we used in faithfulness experiments and Table 2 describes the models we used in contrastive experiments."}, {"title": "B.3 Abbreviations", "content": "Table 4 summarizes the abbreviations we use in figures and tables."}, {"title": "B.4 Report Card Generation", "content": "Details in generating all Report Cards used for experiments are summarized in Table 5. The PRESS Progression Set refers to the dataset of questions and completions we used in the progression step."}, {"title": "B.5 Description of Chinese Grammar Correction Dataset", "content": "Chinese Grammar Correction is a private dataset intended to be used to train AI models in identifying, classifying, and correcting Chinese grammar mistakes. The dataset is annotated by crowd workers in China, with data sourced from official and non-official press releases. The dataset has approximately 10,000 entries. For our experiments, we randomly sampled 100 entries from this dataset. We focused on the following fields:\n1. Original (incorrect) sentence\n2. Corrected sentence\n3. Error word\n4. Corrected word\nFigure 9 shows an example query for the open-ended Chinese Grammar Correction dataset.\nThe phrase \"li dao\" (labeled using green) should be corrected to \"dao li\" because \"li dao\" is not a standard term, while \"dao li\" accurately conveys the intended meaning as \"reason\" or \"principle.\" The phrase \"lao sheng chang\" should be corrected to \"lao sheng chang tan\" because \"lao sheng chang\" is incomplete and does not convey a complete idea. \"Lao sheng chang tan\" is a commonly used phrase meaning \"a clich\u00e9\" or \"something that has been said countless times before.\""}, {"title": "C Contrastive Accuracy Details", "content": ""}, {"title": "C.1 Data Aggregation", "content": "In contrastive guessing, we have two orderings of Report Cards for each model pair. To mitigate the effect of positional bias, we average the accuracy between the two orderings. We compute the average across each dataset and topic pair by averaging across all model pairs."}, {"title": "C.2 Additional Experiments and Ablations", "content": "In this section, we present several experiments and ablations on the design choices for the contrastive accuracy approach to the specificity metric. We chose a subset of models and dataset topics to perform ablation studies. The general ablation study setup can be found in Table 6. The results of our ablation studies are detailed in Table 7.\nWe investigated the relationship between the performance gap (\u2206 topic accuracy) and the contrastive specificity achieved by PRESS. Across all topics, we observe a positive correlation (Figure 10(a & b)), indicating that models with larger \u25b2 topic accuracy are easier to distinguish using the Report Cards, which agrees with our intuition.\nDo Report Cards compress information efficiently? In Figure 10(c), we compared the word count versus contrastive accuracy for the bullet point format Report Cards and few-shot examples. The bullet-point format proves to be more effective than the few-shot, achieving an average contrastive accuracy of 69% with 899 words, compared to 61% accuracy with 1694 words for the few-shot. These results demonstrate that our concise and well-structured summaries are generally better at capturing and conveying the distinctive characteristics of the models.\nDoes having both Report Cards improve the contrastive accuracy? Providing Report Cards for both models (2C2A) improves contrastive accuracy by 8% compared to presenting the Report Card for only one model (1C2A) (Figure 10(e)). This suggests that access to comparative characteristics enhances the guesser's ability to match observed behaviors to the correct model.\nDoes the ability of the guesser model matter? The strength of the guesser can have a significant impact on contrastive accuracy, as shown in Figure 10 (e). Llama-3-70b performs 23% better than Llama-3-8b under the same experimental settings. Llama-3.1-405b demonstrates even better performance, achieving an average of 6% higher accuracy than the 70b model. Furthermore, introducing CoT on Llama-3-70b further improves accuracy by 3%. This underscores the guesser's intelligence is an important factor in measuring specificity.\nHow important is the format of Report Cards? Figure 10(d) illustrates the impact of Report Card format on the specificity. We investigated three Report Card formats detailed in Appendix A. The bullet-point format outperforms the hierarchical format and paragraph format.\nAre Report Cards robust to paraphrased completions? As we discussed in 3.2 and shown in Figure 10(f), Report Cards remain robust under distribution shifts."}, {"title": "C.3 De-stylization", "content": "We de-stylized completions in Adv. AI Risk and MMLU using prompts described in Appendix F.4. Here we present examples of de-stylization. Figures 11 and 12 provide examples of how we perform de-stylization on each dataset."}, {"title": "D Elo Computation Details", "content": "When computing Elo, we treat each pairwise comparison as a \u201cmatch\" (between models) and randomize the order of the matches prior to computing Elo.\nMMLU For MMLU, the R\u00b2 value was aggregated across each subtopic by taking the average correlation across each subtopic. For each subtopic, we compared correctness across 272 model pairs (17 models) for each question, resulting in a total of 16,320 comparisons. The matching scheme was as follows: For a pair of models i and j, we determined i > j if i answered the question correctly while j did not. Ties were excluded from the analysis. To mitigate ordering effects, we averaged the score from both orderings.\nCN Grammar For the Chinese Grammar evaluation, we employed LLM-as-judge on 16 randomly sampled queries per model pair. The LLM-as-judge determined the better completion using the prompts outlined in Appendix F.6. Since Llama 3 models consistently respond with English, they were excluded from this task, leaving us with 3,360 comparisons across 210 model pairs. For the few-shot baseline, we treated each few-shot example set as a Report Card, ensuring the same number of comparisons as the card. We used gpt-4o-mini-2024-07-18 as the judge. To mitigate ordering bias, each model pair was compared twice, with the orders reversed. For each match, the judge definitively determined a winner and a loser.\nCard Elo Card Elo is computed similarly to completion Elo using LLM-as-judge. We compare each pair twice (with the reversed ordering of the cards) and randomize the order of the matches. Detailed prompts for the pairwise comparison of Report Cards are provided in Appendix F.5.\nNote that, as teh Ground-truth Elo requires comparing completions against the entire dataset, it requires 60 \u00d7 2 comparisions per model pairs. While Report card requires only 2 comparision per"}, {"title": "E Human Scoring Details", "content": "Scoring Process For both LLM and human raters, we employ the same rating process. For each question in the test batch given a specific dataset and topic, we provide LLM and human raters with the relevant part of the Report Card (see Report Card Excerpts below) and the student model's response to the question, and have them rate the Report Card on the following 3 metrics:\n\u2022 Relevance: How relevant is the Report Card to the given question?\n\u2022 Informativeness: How informative is the Report Card about the (student) model's capabilities with respect to the question and the model answer?\n\u2022 Clarity: How clear and understandable is the information presented in the excerpt?\nFollowing this process, we obtain scores for questions in the test batch (60 questions in total). Limited by resources, we cannot collect scores for every question and excerpt, and the number of total samples we collected is specified in Table 8. We aggregate the scores of a Report Card by taking the mean. The instructions given to volunteers are provided in Appendix E.1, and the prompt given to LLMs can be viewed in Appendix F.7. Hyperparameters for both human and LLM scoring are presented in Table 8.\nReport Cards Excerpts To mitigate the effort for volunteers in reading and processing long Report Cards, we excerpt Report Cards (prompts in Appendix F.8) using a LLM to extract relevant"}, {"title": "E.1 Human Instructions", "content": "Here we present the instructions we gave to volunteers to rate Report Cards. For prompts given to LLMs, please refer to Appendix F.7."}, {"title": "E.2 Human-LLM Alignment Investigations", "content": "To automate the scoring process, we attempted to prompt LLMs with almost the same instructions as Appendix E.1. Prompts can be found in Appendix F.7. For the human instruction, we included an additional \"familiarity\" aspect but we omitted it in LLM prompts. See Table 9 for results.\nThe distribution of LLM scores over human scores is visualized in Figure 7. We can observe a weak-to-moderate alignment between LLMs and humans."}, {"title": "F Prompts", "content": "For each section, we will present the system prompt first, and then the user prompt."}, {"title": "F.1 Progression Step in PRESS", "content": "In this section, we only show the prompt for generating the bullet point format (Appendix A) Report Cards. Prompts for other formats are similarly defined and can be accessed in our repository 2\nYou are an expert at assessing the behavior and performance of an AI assistant (the \"student\") with respect to the following topic: {topic}.\nYour goal is to capture the unique characteristics of the student, so that a human could learn about the student's behavior from your summary. Your summary must be concise, precise, and informative.\n## Your Task\nAssess the responses from the student below with respect to the topic: {topic} and then write a summary of the student's performance for each sub-topic.\nAnalyze responses to identify thinking patterns, highlighting strengths and weaknesses.\nYou'll be given a set of questions, reference answers (if applicable), the responses of the student, and a set of sub-topics to evaluate the student on.\nAlso, propose 1-3 new unique sub-topics under {topic} if it improves the clarity of the overall assessment or fits the given samples better, avoiding overly specific sub-topics."}, {"title": "F.2 Refinement Step in PRESS", "content": "You are an expert in the topic: {topic}. Your job is to combine two summaries of the same AI assistant into one cohesive summary. Aim for precision and clarity, so that a human that reads your combined summary will be able to accurately predict student behavior.\n## Your Task\nSynthesize multiple summaries of a student's performance across various sub-topics into a cohesive, unified report.\n## Merging Guide\n1. Preserve original sub-topic names.\n2. For sub-topics present in multiple summaries:\na. Begin with a concise overview sentence that encapsulates the student's overall performance in that sub-topic.\nb. Follow with a detailed analysis that consolidates:\nThinking patterns\nStrengths\nWeaknesses\nc. Ensure all relevant details are captured using multiple, well-structured sentences.\n3. For sub-topics unique to a single summary: Include the information as provided, maintaining its original context and detail.\n4. Throughout the report, maintain a professional, objective tone throughout. Opt for judgmental phrasing over ambiguous wording.\n## Summaries"}, {"title": "F.3 Contrastive Accuracy", "content": "You are an expert in {topic}. You are tasked with guessing which student authors which response given the description of students.\nEvaluations of students will be given as a list of factors. Please determine which student authors which response step by step.\n## Evaluation Cards"}, {"title": "F.4 Paraphrasing for Robustness Check", "content": "You are a good paraphraser. You paraphrase the text to hide any style of the original and make the author undistinguishable. You preserve the meaning and logical flow of the text. Respond using JSON format.\nQuestion for the original author: {query}\nThe original completion: {completion}\nYou may and you should remove any structural information that may reveal the original author.\nRespond using the following JSON format. You must firstly analyze the logical flow, then do the paraphrase.\nMake sure that your paraphrase is still readable.\n[[FORMATTING)) Follow the format exactly, NO extra spaces or characters before or after this."}, {"title": "F.5 Skill Elo", "content": "You are a {topic} teacher evaluating a pair of student evaluation cards to find who is the better student. You give definitive answer.\nYou are a {topic} teacher evaluating the student evaluation cards to find who is the better student. You give definitive answer."}, {"title": "F.6 Completion Elo", "content": "You are a {topic} teacher evaluating a pair of student completions to find who is the better student. You give definitive answers."}, {"title": "F.7 LLM Scoring", "content": "You are an expert in {topic} and an experienced evaluator tasked with rating student Skill Report excerpts on relevance, informativeness, and ease of understanding using a 1-5 scale.\n# Your Task\nA Skill Report for a student is a summary of the student's performance on a topic based on the student's responses. The Skill Reports evaluate students from multiple sub-topics.\nYou'll rate an excerpt of a Skill Report from multiple dimensions on a 1-5 point scale based on the question and the student's response.\n# Rating Dimensions\nRelevance: How relevant is the excerpt to the given question?\n1. Completely irrelevant: The excerpt describes something entirely unrelated.\n2. Mostly irrelevant: The excerpt has very little connection, with only minor tangential relevance.\n3. Somewhat relevant: The excerpt has some connection but includes significant irrelevant information.\n4. Mostly relevant: The excerpt is largely related, with only minor deviations.\n5. Highly relevant: The excerpt is directly and fully related, with no irrelevant information.\nInformativeness: How informative is the excerpt about the model's capabilities with respect to the question and the model answer?\n1. Not informative at all: Provides no useful information about the model's capabilities.\n2. Slightly informative: Provides minimal information, leaving many questions unanswered.\n3. Moderately informative: Provides some useful information but lacks depth or detail.\n4. Very informative: Provides comprehensive information, covering most key aspects.\n5. Extremely informative: Provides extensive, detailed information, covering all key aspects.\nClarity: How clear and understandable is the information presented in the excerpt?\n1. Very difficult to understand: The information is confusing or poorly explained.\n2. Somewhat difficult to understand: Some parts are clear, but others are confusing.\n3. Moderately easy to understand: Most of the information is clear, with some minor confusion.\n4. Easy to understand: Information is presented clearly.\n5. Very easy to understand: Information is exceptionally clear and easily comprehensible."}, {"title": "then write", "content": "E = 1/(1 + 10"}]}