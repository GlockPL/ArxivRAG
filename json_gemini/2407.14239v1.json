{"title": "KOMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models", "authors": ["Kemou Jiang", "Xuan Cai", "Zhiyong Cui", "Aoyong Li", "Yilong Ren", "Haiyang Yu", "Hao Yang", "Daocheng Fu", "Licheng Wen", "Pinlong Cai"], "abstract": "Large language models (LLMs) as autonomous agents offer a novel avenue for tackling real-world challenges through a knowledge-driven manner. These LLM-enhanced methodologies excel in generalization and interpretability. However, the complexity of driving tasks often necessitates the collaboration of multiple, heterogeneous agents, underscoring the need for such LLM-driven agents to engage in cooperative knowledge sharing and cognitive synergy. Despite the promise of LLMs, current applications predominantly center around single-agent scenarios, which limits their scope in the face of intricate, interconnected tasks. To broaden the horizons of knowledge-driven strategies and bolster the generalization capabilities of autonomous agents, we propose the KoMA framework consisting of the multi-agent interaction, the multi-step planning, the shared-memory, and the ranking-based reflection modules to enhance multi-agents' decision-making in complex driving scenarios. Based on the framework's generated text descriptions of driving scenarios, the multi-agent interaction module enables LLM agents to analyze and infer the intentions of surrounding vehicles based on scene information, akin to human cognition. The multi-step planning module enables LLM agents to analyze and obtain final action decisions layer by layer to ensure consistent goals for short-term action decisions. The shared memory module can accumulate collective experience to make superior decisions, and the ranking-based reflection module can evaluate and improve agent behavior with the aim of enhancing driving safety and efficiency. The KoMA framework not only enhances the robustness and adaptability of autonomous driving agents but also significantly elevates their generalization capabilities across diverse scenarios. Empirical results demonstrate the superiority of our approach over traditional methods, particularly in its ability to handle complex, unpredictable driving environments without extensive retraining.", "sections": [{"title": "I. INTRODUCTION", "content": "The quest for autonomous driving system has long been at the forefront of technological innovation, aiming to revolutionize transportation through improved safety, efficiency, and accessibility. Traditional approaches to autonomous driving have predominantly been data-driven [1]\u2013[4], relying heavily on the collection and analysis of vast datasets to train algorithms capable of complex driving scenarios. While these methods have made significant strides, they are often hampered by challenges such as dataset bias [5], overfitting [6], [7], and a lack of interpretability [8], [9], which can limit their effectiveness in novel or unforeseen circumstances.\nIn response to these challenges, there has been a paradigm shift towards knowledge-driven approaches in autonomous driving [10], [11]. This shift is underpinned by the recognition that human drivers can rely on their rich experience and knowledge coupled with logical reasoning ability to make reasonable judgments and decisions when facing new scenarios. Large Language Models (LLMs) are trained on large amounts of text data to process, understand, and generate natural language text. The LLM has a broad range of basic human knowledge and superior reasoning abilities, making it a powerful tool in this new knowledge-driven paradigm [12], [13]. These models, exemplified by the likes of GPT3.5 and GPT4 [13], have demonstrated unparalleled proficiency in understanding and generating natural language text and can quickly adapt to new application scenarios with a small number of prompt words, suggesting their potential to serve as agents within autonomous systems [14]-[16].\nRecent studies on LLM-based autonomous driving agents have primarily been tested in simplistic scenarios, such as highway main road driving scenarios and circular track driving scenarios [10], [17], [18]. In these contexts, the impact of other vehicles on the agent vehicle is negligible, placing the agent vehicle in a safe and stable environment. However, real-world driving scenarios are complex and time-variant, reflected by two major aspects: 1) the diversity of driving scenarios, such as ramp merging and roundabouts, increases the likelihood of conflicts between vehicles due to their complexity. This diversity requires the vehicle agents to make reasonable and rapid plans to ensure driving safety and efficiency. 2) The second factor is the diversity of drivers determined by the drivers' unique charateristics and reflected by their driving behaviors. This diversity contributes to the increased temporal variability of scenarios. Therefore, autonomous driving agents based on LLMs need further verification of their intelligence levels in complex and time-variant scenarios, especially when those agents with different objectives influence with each other.\nIn multiagent driving scenarios, vehicle agents based on LLMs must account for both the impact of fixed driver model vehicles and the influence of variable LLM-based agent vehicles on their own driving decisions. By adopting this approach, the agents can more accurately simulate the complexities of real-world driving conditions, thereby advancing the frontier of knowledge-driven agent technology. The well-established knowledge-driven framework, DiLu [10], have summarized the knowledge-driven paradigm of autonomous driving systems, which comprises three components: 1) an environment with which the agent can interact; 2) a driving agent capable of memory, reasoning, and reflection; 3) a memory component to retain experience. As shown in Fig. 1, the knowledge-driven paradigm can still be used within the multi-agent framework after expanding the interactions among multiple agents. However, due to the increased complexity of the scenarios in both time and space, further expansion and improvement of some modules are required to better apply them in complex scenarios. To address these limitations, this study introduce a knowledge-driven autonomous driving framework KOMA that incorporates multiple agents empowered by LLMs. It encapsulates five integral modules: Environment, Multi-agent Interaction, Multi-step Planning, Shared Memory, and Ranking-based Reflection.\nAt present, the reasoning module of LLMs is mainly divided into two approaches: making single-step decisions directly for each frame [10] and formulating a plan that includes a sequence of multiple-step decisions at once [19]. However, when faced with more complex driving scenarios, the former, which makes decisions for the next frame solely based on the current scene description, is prone to falling into local optimal solutions, lacking foresight, thereby making it difficult to achieve a goal that requires continuous action across multiple frames. In the face of more dynamic driving scenarios, the latter, which plans multiple-step action decisions at once, is susceptible to plan failure due to sudden changes in the scene conditions, thus failing to complete the scene goals smoothly. To address this, the KoMA framework introduces a multi-step reasoning module that achieves the final single-step action decision through a three-tiered reasoning process of goal-plan-action, ensuring the continuity of action decisions.\nIn the knowledge-driven autonomous driving setting, the current reflection module is primarily activated after a collision occurs [10], [11], but this raises two issues: (1) In complex scenarios, the cause of an accident is often not the last action decision, but rather a critical decision made earlier in a long sequence of decisions. Identifying the key erroneous decision and reflecting on it for correction is a critical challenge. (2) In reality, drivers need to ensure not only the safety of driving but also its efficiency. An agent that completely sacrifices efficiency for safety is impractical for real-world scenarios. Therefore, the KOMA framework proposes a score-based reflection module that includes assessments of safety and efficiency. It expands the conditions for initiating reflection to situations where there is a sudden drop or extremely low score, allowing for timely correction of erroneous decisions and enhancing the quality of the memory fragments stored in the memory module.\nThe memory module serves as a repository for the historical driving experiences of an agent, capable of retrieving similar scenario experiences to assist the LLM-empowered agent in making action decisions. In early reinforcement learning multi-agent systems, each agent was trained independently [20]. However, this may lead to the collective intelligence being limited by an individual agent with poor training outcomes. Benefiting from the parameter sharing mechanism in reinforcement learning [21], the KoMA framework incorporates a shared memory module, allowing all agents to share a single vector database. This enables the sharing of driving experiences among agents, thereby enhancing the training speed and effectiveness of collective intelligence.\nIn summary, this study proposes KoMA, a comprehensive framework that harnesses the power of LLMs to facilitate advanced decision-making in complex, multi-agent driving environments. The KOMA framework is designed to address the limitations of current single-agent approaches by integrating several key components that work in concert to enhance the capabilities of autonomous agents. Through a series of experiments and implementations, we demonstrate the feasibility of our approach and its advantages over traditional methods, particularly in terms of generalization, adaptability, and the ability to handle novel scenarios without extensive retraining.\nThe contribution of this paper is listed as follows:\n1) To the best of knowledge, we are the first to propose a knowledge-driven autonomous driving framework with multiple LLM-empowered agents, where those driving agents implicitly interact through estimating the intentions of surrounding vehicles.\n2) We propose a three-layer structure of GPA (goal-planning-action) for analyzing tasks step by step in complex autonomous driving scenarios, ensuring the coherence of long-term decision-making.\n3) The framework also integrating safety and efficiency indicators into the reflection module to accurately measure and locate erroneous decisions, thereby expanding the scope of reflection and enhancing the module's effectiveness.\n4) A memory sharing module is proposed to enable multiple agents quickly accumulate experience of different scenario at the same time and ensure that the memory and learning procedures of multiple agents are consistent. Experiments have demonstrated that this shared memory module effectively enhances the generalization capability of the agents."}, {"title": "II. LITERATURE REVIEW", "content": "In the burgeoning field of artificial intelligence, the emergence and integration of LLMs as agents have marked a significant pivot from conventional rule-based systems to knowledge-driven approaches. The LLMs demonstrate remarkable abilities to perform tasks based on user prompts and rapidly adapt to new scenarios through in-context learning. Zhou et al. highlighted the intrinsic properties of LLMS that facilitate this adaptability, laying the groundwork for their application as complex agents [22]. The shift towarded employing LLMs as the \"brain\" or controller of agents, as discussed by Xi et al., signified a pivotal transition towards more integrated and autonomous systems [23]. Gao et al. introduced AssistGPT, which employed a method of linguistic reasoning called Plan, Do, Check, and Learn (PEIL). This methodology enhanced the integration of LLMs with various tools, pushing the boundaries of their application [24]. Similarly, Shen et al. proposed HuggingGPT, a framework designed to leverage LLMs for connecting disparate AI models within the machine learning community to address complex AI tasks [25]. Furthermore, ViperGPT, as presented by Suris et al., showcased a novel approach by utilizing an API to access and compose modules through Python code generation, thereby offering solutions for an array of queries [26]. A significant evolution in the deployment of LLMs as agents was the transition from traditional, data-driven paradigms to knowledge-driven methodologies. Li et al. emphasized this shift, advocating for a move towards active, cognition-based understanding that leveraged the extensive general knowledge and reasoning capabilities of LLMs [11]. This approach not only enhanced the agent's ability to interact with and comprehend the world, but also enabled the system to become more autonomous and intelligent with relevant domain knowledge and reasoning learning ability."}, {"title": "A. Multi agent with LLM", "content": "The integration of LLMs in multi-agent systems has become an emerging research field for improving collective intelligence and collaboration in various fields. The exploration of LLM-powered multi-agent systems revealed the future of collaborative intelligence, where the cooperation of multiple independent agents could enhance problem-solving capabilities. Handler et al. proposed a multi-dimensional taxonomy to tackle the difficulties in categorize and understand the architectural complexities posed by LLM-powered multi-agent systems which aimed at accomplishing complex tasks, goals, or problems with the cognitive synergy of multiple autonomous LLM-powered agents [27]. Liu et al. proposed the Dynamic LLM-Agent Network (DyLAN), a novel approach for fostering LLM-agent collaboration on intricate tasks such as reasoning and code generation. DyLAN created a strategic assembly of agents that communicate within a dynamic interaction architecture tailored to the specific requirements of the task query [28]. This model exemplified the potential of context-aware collaboration among LLM-powered agents, pushing the boundaries of collective problem-solving capabilities. Chen et al. introduced AGENTVERSE, a multi-agent framework designed to orchestrate a collaborative group of expert agents, creating a system whose capabilities exceed the sum of its parts. AGENTVERSE enhanced the efficiency and effectiveness of task accomplishment, showcasing the power of agent collaboration in complex problem-solving scenarios [29]. Gong et al. presented Mindagent, an infrastructure that leverages LLMs for interactive multi-agent planning. This system not only demonstrated the in-context learning capabilities of LLMs in multi-agent planning but also offered several prompting techniques to bolster their planning proficiency. It is proposed for evaluating planning and coordination capabilities in the context of gaming interaction [30]. Zhang et al. developed the Cooperative Embodied Language Agent (CoELA), an agent capable of planning, communicating, and cooperating with others to efficiently accomplish long-horizon tasks. Powered by GPT4, CoELA surpassed traditional planning-based methods, demonstrating emergent effective communication strategies. This advance highlighted the potential for LLM to conduct cooperative behavior and perform complex tasks in multiple agents [31]."}, {"title": "B. LLM for Autonomous Driving", "content": "The integration of LLMs into autonomous driving systems signified a transformative leap towards embedding human-like intelligence for enhanced decision-making and interaction capabilities. Mao et al. emphasized the role of LLMs as cognitive agents in autonomous driving systems, highlighting their capacity to integrate human-like intelligence across various functions such as perception, prediction, and planning [32]. This foundational approach underscored the potential of LLMS to mimic human cognitive processes. Li et al. discussed the employment of LLMs as foundation models for autonomous driving, capitalizing on their rich repository of human driving experience and common sense. These models actively understood, interacted with, acquired knowledge from, and reasoned about driving scenarios [11]. Wen et al. introduced DiLu, a framework combining reasoning and reflection modules to facilitate decision-making based on common-sense knowledge while enabling continuous system evolution. DiLu's extensive experiments demonstrated its superior generalization ability over reinforcement learning-based methods, evidencing the capability of LLMs to accumulate experience and adaptively improve [10]. Wang et al. proposed Co-Pilot, a universal framework that incorporates LLMs as a vehicle's \u201cCo-Pilot\u201d, adept at fulfilling specific driving tasks with human intentions in mind. This framework not only defined a workflow for human-vehicle interaction but also introduced a memory mechanism for task-related information organization and an expert-oriented black-box tuning to enhance performance without the need for fine-tuning the LLMs. The application of Co-Pilot in path tracking control and trajectory planning tasks showcased its versatility and effectiveness [18]. Shao et al. presented LMDrive, an innovative language-guided, end-to-end, closed-loop framework for autonomous driving. Uniquely integrating multi-modal sensor data with natural language instructions, LMDrive facilitated interaction with humans and navigation software in realistic settings [33].\nThe exploration of LLMs within autonomous driving systems unveiled a promising horizon where vehicles not only mimicked human driving capabilities but also engaged in complex decision-making and problem-solving tasks with an unprecedented level of intelligence and adaptability. The reviewed literature underscored the shift towards leveraging human-like intelligence and knowledge-driven methodologies, illustrating the potential of LLMs to redefine autonomous driving. However, current research on LLM-empowered autonomous driving agents mainly focused on simple scenarios lacking conflicts. We try to further explore and test the scope of knowledge driven capabilities in scenarios with conflicts, in order to fill the gap in this field."}, {"title": "III. METHODOLOGY", "content": "We introduce a knowledge-driven autonomous driving framework KoMA that incorporates multiple agents empowered by LLMs, comprising five integral modules: Environment, Multi-agent Interaction, Multi-step Planning, Shared Memory, and Ranking-based Reflection. Within this framework, all agents operate on an equal and independent basis, as illustrated in Fig. 2.\nThe environment module provides driving scenarios for the driving agent, which can be either a simulation environment or a real-world scenario. It is mainly responsible for providing a text description of the corresponding scene for each autonomous driving agent before making decisions.\nThe multi-agent interaction module further processes the textual information returned by the environment module, primarily enables the LLMs to analyze the behavior of other vehicles in the scenario like a human, infer their intentions, and support subsequent action decisions with relevant information. Chain of Thought (CoT) is a technique used in LLMs that promotes complex reasoning and problem-solving capabilities. The core idea behind this method is to break down a complex problem into a series of smaller steps, known as intermediate reasoning steps. This enabless the model to incrementally construct a complete solution by addressing each step in a logical and sequential manner [34]-[36]. The multi-step planning module is an application of the CoT technique to guide LLM make the final action decision. The LLMs firstly analyze the goal according to the current scenario, then formulates the plan, and finally makes an action decision. This structured planning process enables the LLM agent to maintain a clear goal for its actions and more effectively pursue long-term goals. Based on the textual description of the current scene and the experiential playback of historical similar scenes, the LLM ultimately selects an action through continuous analysis and then returns the action decision to the environment module for execution.\nThe shared memory module utilizes a shared vector database to store the successful driving experiences of all agents. Before each agent makes a decision, the module retrieves analogous descriptions relevant to the agent's current situation and then provides these experiences to the agent, helping to formulate an informed action decision. This module allows each agent to train, accumulate experience, and interact with the environment, continuously enhancing decision-making effectiveness.\nTo ensure the quality of experiences in the shared memory module, this framework introduces a ranking-based reflection module that evaluates each driving decision after execution based on efficiency and safety. After a scenario has been concluded, the framework reviews the outcomes of those decisions, especially those with low scores or collisions. Only those experiences where decisions were corrected with high scores are retained. The procedure is outlined in Algorithm 1.\nMore details about the Multi-agent Interaction, Multi-step Planning, Shared Memory, and Ranking-based Reflection modules will be elaborated in the following sections."}, {"title": "A. Overview", "content": "We introduce a knowledge-driven autonomous driving framework KoMA that incorporates multiple agents empowered by LLMs, comprising five integral modules: Environment, Multi-agent Interaction, Multi-step Planning, Shared Memory, and Ranking-based Reflection. Within this framework, all agents operate on an equal and independent basis, as illustrated in Fig. 2.\nThe environment module provides driving scenarios for the driving agent, which can be either a simulation environment"}, {"title": "B. Multi-agent Interaction Module", "content": "In KoMA, agents do not interact directly with each other; instead, agents infer their intentions just like real human drivers by analyzing the history and current state information of other vehicles. For example, when the LLM-driven agent senses a vehicle on its left suddenly accelerating, it further analyzes and reasons about this, guesses the vehicle's intention to overtake, and then analyzes these guesses along with its own plan to determine if adjustments to its short-term plan are needed. Ultimately, the agent aims to make rational action decisions. After receiving the text description from the environment module, the agent preliminarily processes the information using the multi-agent interaction module, analyzes and guesses the intentions of other vehicles before proceeding to the multi-step planning module with this information."}, {"title": "C. Multi-step Planning", "content": "The Multi-step Planning Module serves as the cornerstone for LLMs' reasoning within KoMA. The process is illustrated in Fig 3. Employing the Goal-Plan-Action methodology, this module merges inputs from the current scenario description with similar historical experiences to determine the best course of action, encompassing the following stages.\nClarify goals based on the current scenario: Drivers have different goals to achieve in different scenarios. For example, the goal of a normal vehicle running on the highway is to drive efficiently and quickly under the premise of ensuring safety. The main goal of vehicles on the freeway ramp is to merge into the main road as soon as possible within a certain period of time, so the driver's goal is mainly related to the scenarios. Based on this, we divide the goals within the scenario into two categories: special scenario goals and general scenario goals. Special scenario goals refer to the tasks that need to be completed within a certain time in the scenario, such as ramp merging, intersection passage, etc. The general scenario goal is a long-term objective, such as maintaining safe and efficient driving. Each agent first sets its objectives based on the scenario description to inform planning and action decisions.\nMake a plan based on the goals: The planning process involves the LLMs devising a strategy that connects identified goals with future actions, ensuring coherent actions over time. Initially, the LLMs checks for a pre-established plan. If no such plan exists, the LLMs formulates a new strategy to guide future actions. The planning process unfolds as follows:\n1) The LLMs brainstorm all workable and distinct plans based on the current scenario and goal.\n2) For each of the proposed plans, the LLMs will evaluate the potential of them, consider their pros and cons, implementation difficulty, potential challenges and then assign safety, efficiency score from 0 to 10 to each option based on these factors.\n3) Based on the evaluations and scenarios, rank the plans.\n4) A singular plan is selected to guide driving decisions. If there is a pre-existing plan, the LLM evaluates other vehicles' intentions from the multi-agent interaction module. If the plan is deemed infeasible, it craft a new plan; otherwise, it maintains the existing plan.\nChoose an action according to the plan: It's time for LLMs making reasonable and safe action choices based on the current scene information and the plan. There are five actions in the action space:turn-left, IDLE (remain in the current lane with current speed), turn-right, acceleration, and deceleration. The LLMs select an action based on inference."}, {"title": "D. Shared Memory", "content": "The shared memory module is an integral component of KOMA, encapsulating a vector database designed to archive beneficial experiences. These archived experiences serve as exemplars to assist LLMs in the planning and decision-making processes. Each experience is segmented into four critical elements: the scenario description, the planning process, the final decision, and the evaluation score. The textual description of scenarios is converted into vectors, serving as keys for gauging similarity within the memory module. The concept of shared memory implies a unified memory module accessible to all agents empowered by LLM, fostering consistency of experience and performance. This approach mirrors the principle of parameter sharing observed in reinforcement learning.\nThe primary aim of shared memory is to quickly accumulate experiences in different scenarios simultaneously. Using the repository of past experiences, new agents can learn from the collective wisdom and insights of their predecessors. This shared memory module enables agents to perform tasks with an awareness of previously successful strategies and outcomes. Essentially, shared memory serves as a conduit for knowledge transfer, ensuring that the collective learning of agents is preserved and utilized to enhance future decision making."}, {"title": "E. Ranking-based Reflection", "content": "The reflection module, operational at the termination of each scenario, is designed to revise action decisions. These rectified decisions, along with successful experiences, are then integrated into the memory module. When the reflection module updates the reflection results to the shared memory module, it can be considered that the agent has completed one round of training. To differentiate the efficacy of each action decision, the system incorporates safety indicators and efficiency scores, assessing the vehicle's state post-execution of LLM-generated instructions. In the following, we delineate the criteria for these evaluations.\nSafety evaluation criteria: Safety is quantified on a scale from 0 to 10, using Time To Collision (TTC) to measure vehicular safety post-action. The TTC above 3 seconds indicates optimal safety, earning the highest score of 10. Below 1.5 seconds, TTC indicates a critical safety risk, scoring 0 [37]-[39]. For TTC values between 1.5 and 3 seconds, the safety score linearly scales from 0 to 10.\n$Safetyscore(t) = \\begin{cases} \\frac{20(TTC(t)-1.5)}{3}, & \\text{if } 1.5s < TTC(t) < 3s \\\\ 10, & \\text{else if } TTC(t) > 3s \\\\ 0, & \\text{otherwise.} \\end{cases}$\nEfficiency evaluation criteria: Efficiency is similarly rated on a scale from 0 to 10 and uses speed as the efficiency metric. This metric considers the impact of surrounding vehicle speeds on the agent's vehicle, measuring efficiency by the difference between the agent's speed and the average speed of surrounding vehicles. An agent vehicle that matches or exceeds the average speed of surrounding vehicles receives a full efficiency score of 10. If the agent's speed is below the average, the efficiency score is the ratio of the agent's speed to the average speed, scaled to 10.\n$Efficiencyscore(t) = \\begin{cases} 10, & \\text{if } V_{LLM}(t) > V_{Avg}(t) \\\\ \\frac{V_{LLM}}{V_{Avg}}, & \\text{otherwise.} \\end{cases}$\nUpon the conclusion of a scenario, the LLM undertakes an analysis of the scoring list to pinpoint actions that deviated from expected outcomes and requires reflection. This process mandates the correction of previously unsafe decisions, facilitating a cycle of continuous enhancement in the agent's capabilities. The utilization of interpretable chain-of-thought responses significantly aids in uncovering the root causes of potentially hazardous scenarios.\nIn an effort to foster autonomous learning from past mistakes, our methodology employs the detailed description of the driving scenario in which the erroneous decision was made, alongside the corresponding reasoning output, as inputs for the LLM. This approach prompts the LLM to elucidate the underlying reasons for the flawed decisions, thereby guiding the agent towards more accurate and safer future decisions. Additionally, the LLM is tasked with devising strategies to mitigate the recurrence of similar errors, enhancing its decision-making framework. This reflective process incorporates refined reasoning and revised decisions learned from error correction into memory modules. This ensures the preservation of enhanced knowledge and underlies the agent's ability to learn adaptively in a variety of driving environments."}, {"title": "IV. EXPERIMENTS", "content": "Simulation Environment: We leverage the \u201chighway-env\" as the simulation platform, which furnishes a realistic multi-vehicle interactive environment [40]. This environment permits the customization of various parameters including vehicle positioning, velocity, count, and lane specifics, offering a versatile setting for conducting our studies. The focal point of our scenario selection is the on-ramp merging challenge within a highway context. This particular scenario underscores the exigency of accomplishing merging objectives within a constrained timeframe, thereby serving as an apt representation of the agent's decision-making acumen.\nLarge Language Model: GPT-4 [13] was mainly used in experiments to verify the validity of the KoMA framework. This model plays a pivotal role in both the Multi-step Planning and Rank-based Reflection modules of our architecture, showcasing its versatility and advanced understanding capabilities. For the Memory module, integral to our framework, we incorporate \u201cChroma\" an open-source embedded vector database. This choice is instrumental in facilitating the conversion of scene descriptions into vectors, for which we employ OpenAI's \u201ctext-embeddings-ada-002\" model. This comprehensive setup not only exemplifies the integration of cutting-edge AI technologies but also highlights our innovative approach towards enhancing autonomous driving systems through nuanced understanding and reflection. TABLE I shows the parameters and components related to Ko\u039c\u0391.\nTesting Scenario: The testing scenario locates at an on-ramp entrance on the highway with two main lanes. The scenario incorporates two LLM-driven agents, one on the main road and the other on the on-ramp, as shown in Fig 6. When the agent reaches the endpoint or collides with other vehicles, it is considered as the end of the scenario. The reflection module will then update the shared-memory module. When the update is completed, the agent completes a round of training in that scene. After every 20 rounds of training, 20 randomly generated scenarios are selected and fixed for testing the trained agents in KoMA.\nBaseline MARL-based model: The baseline MARL method is \"deep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic\u201d [41], which develop an efficient and scalable MARL framework that can be used in dynamic traffic where the communication topology could be time-varying. We have adjusted the MARL parameter Policy frequency from 5Hz to 2Hz to maintain consistency with the Policy frequency during training in KoMA. Concurrently, we have adapted the MARL training scenarios to focus on the two-lane highway on-ramp merging context and have initiated the training process accordingly."}, {"title": "A. Experimental Settings", "content": "Simulation Environment: We leverage the \u201chighway-env\" as the simulation platform, which furnishes a realistic multi-vehicle interactive environment [40]. This environment permits the customization of various parameters including vehicle positioning, velocity, count, and lane specifics, offering a versatile setting for conducting our studies. The focal point of our scenario selection is the on-ramp merging challenge within a highway context. This particular scenario underscores the exigency of accomplishing merging objectives within a constrained timeframe, thereby serving as an apt representation of the agent's decision-making acumen.\nLarge Language Model: GPT-4 [13] was mainly used in experiments to verify the validity of the KoMA framework. This model plays a pivotal role in both the Multi-step Planning and Rank-based Reflection modules of our architecture, showcasing its versatility and advanced understanding capabilities. For the Memory module, integral to our framework, we incorporate \u201cChroma\" an open-source embedded vector database. This choice is instrumental in facilitating the conversion of scene descriptions into vectors, for which we employ OpenAI's \u201ctext-embeddings-ada-002\" model. This comprehensive setup not only exemplifies the integration of cutting-edge AI technologies but also highlights our innovative approach towards enhancing autonomous driving systems through nuanced understanding and reflection.\nTesting Scenario: The testing scenario locates at an on-ramp entrance on the highway with two main lanes. The scenario incorporates two LLM-driven agents, one on the main road and the other on the on-ramp, as shown in Fig 6. When"}, {"title": "B. Performance of the proposed framework", "content": "In this section, we conducted comparative experiments on KOMA and deep multi-agent reinforcement learning (MARL). To maintain consistency with the test scenarios of the MARL methods, we conducted comparative tests in scenarios where both the main road and the highway ramp are configured as single lanes."}, {"title": "C. The validation of the framework", "content": "In this section, an ablation study is conducted on the Shared Memory module. We have set up three different types of memory modules for comparative testing to verify the superiority of the shared memory module. The first type is the no-memory module, which means that no vector database is set up to accumulate and store historical experiences. The second type is the non-shared memory module, where each agent has its own vector database, and the accumulated experience fragments are not shared with other agents, only used to assist in their own action decisions. The third type is the shared memory module, where each agent shares a common vector database, and the experiences accumulated by an agent can be called upon by other agents to assist in their action decisions, thus achieving sharing. The testing results are shown in Fig. 8."}, {"title": "1) Shared Memory:", "content": "In this section, an ablation study is conducted on the Shared Memory module. We have set up three different types of memory modules for comparative testing to verify the superiority of the shared memory module. The first type is the no-memory module, which means that no vector database is set up to accumulate and store historical experiences. The second type is the non-shared memory module, where each agent has its own vector database, and the accumulated experience fragments are not shared with other agents, only used to assist in their own action decisions. The third type is the shared memory module, where each agent shares a common vector database, and the experiences accumulated by an agent can be called upon by other agents to assist in their action decisions, thus achieving sharing. The testing results are shown in Fig. 8."}, {"title": "2) Multi-step Planning:", "content": "In this section, we will compare the training results of agents with and without the multi-step planning module to verify the effectiveness of KoMA. The agents under these two frameworks are both trained for 40 rounds. The testing results are displayed via a boxplot in Table III. It shows the success rates of agents at the different training stages, including agents without training, agents trained with 20 rounds and agents trained with 40 rounds."}, {"title": "D. Generalization under different Scenarios", "content": "Data-driven agents often tend to overfit to training scenarios, leading to poor generalization capabilities and limited applicability [6], [7], [11]. Therefore, in this section, we conduct a series of tests on the generalization capabilities of knowledge-driven agents within the framework to examine whether LLMs can learn knowledge from experiential fragments and apply it to different scenarios."}, {"title": "1) Altering the number of lanes on the main roadway:", "content": "Initially, we conducted tests with minor variations in scenario complexity by changing the number of lanes on the merging main roadway. Despite these minor changes, these scenarios still pose challenges for data-driven agents in generalizing. We trained the LLM agents on scenarios with two-lane main roadways and single-lane ramps. After training, we obtained an experience-rich vector database, known as the memory module. We then tested this module in scenarios with one more or one less lane on the main roadway, at the same density. As the memory module lacks content for these new scenarios, this setup tests the LLM agents' ability to generalize. The results are shown in Fig. 10. We found that the shared-memory trained in the initial scenario still affects and yields good results when generalizing to scenarios with varying main road lanes, indicating superior generalization ability in knowledge-driven agents compared to traditional data-driven ones."}, {"title": "2) Generalizing to roundabout scenarios:", "content": "Continuing with the use of the same memory module as in the previous section, we shifted the scenario from ramp merging to roundabouts. Our aim was to test whether the LLM agents could learn from previous scenarios and enhance their decision-making abilities by generalizing to completely different scenarios. Additionally, this shift aimed to demonstrate the value of a shared memory module in enhancing the generalization capabilities of multiple agents.\nThe roundabout scenario setup is depicted in Fig. 11. There is a single lane for each of the four entry and exit roads of the roundabout. Within the roundabout itself, there is a two-lane circular road. Roundabout scenarios can be mainly divided into three phases: 1. Roundabout entry phase: Merging into the roundabout from the entry road. 2. Roundabout internal driving phase: Driving within the roundabout until approaching the target exit road. 3. Roundabout exit phase: Exiting from the roundabout to the exit road and leaving the roundabout. There are four vehicles modeled as intelligent driver model (IDM) on the western entry road of the roundabout, with their destination being the eastern exit road. The agent vehicle controlled by the LLM is on the southern entry road, followed by an IDM environmental vehicle. There are a total of six vehicles in the entire roundabout scenario, and their actual generation positions still follow the form of fixed coordinate points plus random fluctuations to reflect randomness. In this scenario, the agent vehicle controlled by the LLM needs to safely merge into the roundabout from the southern entry and smoothly exit from the eastern side of the roundabout, which is considered a successful completion of the scenario. If a collision occurs or the vehicle fails to exit from the eastern side of the roundabout in time, it is considered a failure of the scenario. The result are shown in Fig. 12\nIn the initial ramp merging scenario on a two-lane main road, the success rate was 30% without prior training, and it reached a 70% success rate after 40 rounds of training. An agent without any memory repository achieved a 60% success rate in the roundabout scenario. When the memory repository from 40 rounds of training in the initial ramp merging scenario was applied to the agent in the roundabout scenario, the success rate increased to 80%, demonstrating the effectiveness of knowledge-driven generalization capabilities of the agent. The reason for the roundabout scenario still having a 60% success rate without experience is mainly twofold: firstly, because there is only one LLM controlling the agent in the scenario, which reduces the overall collision risk of the LLM agent; secondly, because the roundabout scenario is simpler, with only longitudinal acceleration and deceleration control before merging, without the need to consider lane-changing behavior."}, {"title": "E. Performance under different LLMs", "content": "In this section, we will employ various LLMs to verify the efficacy of KOMA framework. By comparing their performance during training, we aim to ascertain"}]}