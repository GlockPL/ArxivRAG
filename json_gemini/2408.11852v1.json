{"title": "Fast Training Dataset Attribution via In-Context Learning", "authors": ["Milad Fotouhi", "Mohammad Taha Bahadori", "Oluwaseyi Feyisetan", "Payman Arabshahi", "David Heckerman"], "abstract": "We investigate the use of in-context learning and prompt engineering to estimate the contributions of training data in the outputs of instruction-tuned large language models (LLMs). We propose two novel approaches: (1) a similarity-based approach that measures the difference between LLM outputs with and without provided context, and (2) a mixture distribution model approach that frames the problem of identifying contribution scores as a matrix factorization task. Our empirical comparison demonstrates that the mixture model approach is more robust to retrieval noise in in-context learning, providing a more reliable estimation of data contributions.", "sections": [{"title": "Introduction", "content": "Training Data Attribution (TDA) refers to the task of quantifying contributions of different data sources on the outputs of a model (Park et al., 2023; Nguyen et al., 2023). This task is essential for debugging the processes of curating corpora for training and for improving the training of neural networks. Understanding the contribution of data sources allows us to assess the monetary value of proprietary training data, which is crucial for fair compensation and data management (Ghorbani & Zou, 2019; Nohyun et al., 2022).\nExisting methods for TDA, primarily fall into two categories: retraining-based methods and influence function-based methods, as detailed in recent surveys (Hammoudeh & Lowd, 2024; Worledge et al., 2024). Retraining approaches such as those by (Feldman & Zhang, 2020; Ghorbani & Zou, 2019) involve retraining the model without the target data source. However, this method is computationally expensive. Influence function approaches (Koh & Liang, 2017; Pruthi et al., 2020; Chen et al., 2021; Park et al., 2023), relax the need for full retraining by requiring only a few gradient calculations with respect to the data. Despite their efficiency, these methods rely on a linear approximation of the neural network around the target data point, which can be inaccurate. Critically, the influence function approaches compute the attribution score for a dataset as a linear function (usually an average or sum) of the attribution scores for each data point in the dataset (Hammoudeh & Lowd, 2024; Park et al., 2023). This approach fails to provide a holistic view of the contributions of an entire dataset to the model's output. Additionally, both methods require access to the internals of LLMs, which is not feasible for some popular models. A related technique, Machine Unlearning (Ginart et al., 2019; Sekhari et al., 2021) is still expensive for obtaining the contribution scores.\nWe explore the use of in-context learning and prompt engineering to estimate the contributions of each dataset as a whole in the outputs of instruction-tuned LLMs. We propose two approaches: (1) A similarity-based approach, which posits that providing a dataset as context to an LLM trained on that dataset changes its output less compared to when the LLM was not trained on the dataset. (2) A mixture distribution model approach, where we model the behavior of LLMs using a new mixture distribution. This approach transforms the problem of identifying contribution scores into a matrix factorization problem, which we solve using the"}, {"title": "Methodology", "content": "An LLM processes knowledge from different sources. Our goal is to examine different prompts and see if we can uncover the sources of this knowledge.\nIn the binary outcome setting, we have tuples in the format of question, context, and outcome: (q, c, y). When we don't use any context, we denote c = (). Our model also outputs p(y|q,c). Our goal is to quantify the contributions of the training datasets D1,...,Dn in p(y|q,c). We assume that we have a query set Q = {q1,..., qm}.\nWe assume that we have k, k = 1, ..., K relevant datasets about a topic and we want to quantify their contributions in the generation of the output by our LLM."}, {"title": "The Non-parametric Approach: The Shapley Context Method (SCM)", "content": "The key idea of this approach is that if an LLM uses the information from the kth dataset, providing the kth dataset as context will not change the output much. Conversely, if adding a dataset as context changes the output significantly, it was likely not used for generation of the output.\n$S_k = sim(y, y|c_k),$\nwhere ck is the context provided from the kth dataset.\nUsually, desired information can be found in multiple datasets (Ghorbani & Zou, 2019). To consider the impact of datasets in presence of other datasets, we define the following residual scores to be used in the Shapley formula (Shapley, 1953):\n$s_S = sim(y, y|c_S).$\nThe Shapley values are computed as follows:\n$\\phi_k = \\sum_{S \\subseteq {D_1,..., D_K}\\{D_k}} \\frac{\\S\\!(K - \\S - 1)!}{K!} (S_{SU{k}} \u2013 s_S).$\nThis formula finds the residual increase in the similarity by including Dk, when we already have included another set S\u2286 {D\u2081,...,DK}\\{Dk}. Algorithm 1 in Appendix B describes the details of our Shapley Context Method (SCM)."}, {"title": "The Semi-Parametric Approach: Context Mixture Factorization (CMF)", "content": "We propose a model for summarizing the behavior of LLMs. Our model explicitly contains attribution scores and captures the entirety of the datasets used for its training. We use a mixture distribution approach, which defines:\n$p(y/q) = \\pi_0p_0(y/q) + \\sum_{k=1}^{K}\\pi_kp_k(y/q),$\nwhere p\u2080 denotes a general-purpose language model and pk denote the language models specialized on each of the relevant datasets k = 1,..., K. The distributions pk, k = 0, ..., K are imaginary and we do not intend to explicitly estimate them.\nRemark 1: Given the modularity of LLM structures, this assumption is not fully realistic. However, this assumption provides a holistic view of the contributions of each dataset, captured by distributions pk, k = 1, . . ., K. Thus, Model (4) serves as a useful tool to statistically summarize the behavior of the LLM.\nRemark 2: Model (4) can capture the scenarios where an LLM uses data from multiple sources, but does not model the scenarios where the LLM uses the interaction of data from multiple sources.\nWe model the impact of providing context from a dataset k \u2208 {1,..., K} as an intervention in the probability distribution:\n$p(y|q, c_k) = \\pi_0p_0(y|q) + (1 - \\pi_0)P_k(y|q).$\nThe key assumption is that both Eq. equation 4 and equation 5 do not have context terms in the right-hand side quantities. See a weaker version of Assumption equation 5 in the appendix.\nGoal: Our goal is identifying \u03c0\u03ba, k = 1,..., K. Which probability distance metrics help identify these contributions? We want to do this without explicitly estimating pk, k = 1, . . ., K.\nFor each of the m queries, we perform K+1 prompts\n(or 2K prompts) and write the results in a linear equation as follow:\nFormulating as a Matrix Factorization Problem.\n$p^{(i)} = \\Pi p^{(i)},$ for j = 1, ..., m.\nWe observe the quantity on the left-hand side, but none of the quantities in the right-hand side. The matrix\n\u03a0 has a specific structure. By defining the matrix P = [p(1), . . .,p(m)], we can write our problem in the\nfollowing matrix form:\n$P = \\Pi P,$\nwhere P\u2208 [0,1](K+1)\u00d7m, \u03a0 \u2208 [0,1](K+1)\u00d7(K+1), and P \u2208 [0,1](K+1)\u00d7m.\nThis is a matrix factorization problem with a special structure. We assume that pk (y|q) can be obtained via some clever prompts. We can make assumptions about Pk (y|q) that allow recovery of \u3160 mixture parameters.\nRemark 3: Instead of K + 1 prompts, we can have up to 2K prompts. However, for the prompts that use multiple datasets, we need to assume the form of the resulting distribution, similar to Eq. (5). An alternative is to impose priors on and P to improve identifiability. We pursue the second approach in the next section.\nWe can\nAlternating Projected Least Squares. We can have multiple estimates for from Eq. (6). resolve this issue be encouraging solutions that have lower variance. We achieve this by using two regularizers:\nan entropy regularizer for \u3160 to assume that the sources contribute equally and a regularizer that encourages\nP to be less informative.\n$\\hat{\\pi}, \\hat{P} = arg \\min_{\\pi, P} {\\min_{P} ||P \u2013 \\Pi P||^2_F \u2013 \\lambda_{\\pi}H(\\pi) + \\lambda_{P}||P \u2013 1/2||^2_F},$\n$\\pi \\geq 0, \\sum_1^K \\pi = 1$\n$0 \\leq P \\leq 1,$\nwhere || || F and H(.) denote the Frobenius norm and Shannon's entropy. We use entropy regularization on \u3160 encouraging the null hypothesis of \"equal contributions of all sources\". The Frobenius norm regularization implies that unless there is strong evidence, the outputs of the latent probabilities P should be 1/2. Note the regularizers are vital for obtaining a non-trivial solution, and in absence of the them, there are many solutions for the problem.\nThe problem in Eq. equation 7 is biconvex; i.e., fixing either of \u3160 or P, the problem is convex (Gorski et al., 2007). Thus, we solve it by the alternating least squares method. We describe the procedure in"}, {"title": "Implementation", "content": "For simplicity of evaluation and without loss of generality, we used BoolQ (Clark et al., 2019) Q&A dataset, where the answers are binary Yes/No. To instruct the LLMs to provide direct boolean responses, we used prompt engineering. Initially, we tested various prompts without explicitly instructing the model to answer with \"Yes\" or \"No.\" Diverse examples used in this process are provided in Appendix C.3. Through iterative testing, we found that responses improved when the model was explicitly instructed to provide a boolean answer. This led to our final prompt:\n\"Given the context below, answer the question that follows with only 'Yes', 'No', or 'I don't know' if the context is insufficient.\n{question}? The answer to this question is\"\nWhile this final prompt worked well for GPT-4, Bloomz, and Mistral 7B, generating straightforward \"Yes,\" \"No,\" or \"I don't know\" responses, it was harder to instruct Phi-3-mini. Even with the final prompt, Phi-3-mini often generated more text than just a simple boolean response.\nTherefore, calculating similarities was straightforward for GPT-4, Bloomz, and Mistral 7B, but we had to devise another solution for Phi-3-mini. The embedding similarity API on GPT-4 was not precise enough as it did not focus primarily on the context of the generated response. To calculate the similarity for Phi-3-mini, we created a Zero-shot classification layer (which takes 1000 characters) between the prediction and the result to measure similarity more accurately."}, {"title": "Using RAG", "content": "Given the limitations of LLM context windows, fitting entire datasets directly into the context is impractical. To address this, we utilized Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) to enhance the context by retrieving relevant documents from databases before generating responses. The process involves splitting the documents into semantically relevant chunks using the RecursiveCharacterTextSplitter from the Hugging Face Transformers library, computing embeddings for all chunks with a model like thenlper/gte-small, and storing these embeddings in a vector database using FAISS (Facebook AI Similarity Search). When a question is posed, it is embedded and a similarity search is performed against the vector database to find the closest matching documents. These retrieved documents are then provided as context to the LLMs along with the original question, allowing the LLMs to generate responses augmented with additional context.\nWe used a chunk size of 512 and a top-k value of 3, ensuring the context was trimmed to 2000 characters for conciseness."}, {"title": "Preliminary Experiments", "content": "Simplified setup to demonstrate our methodology:\nStep 1: Task Selection We use the BoolQ Q&A dataset, which consists of tuples in the form (question, relevant context, binary answer) for each question.\nStep 2: LLM Selection We examined four instruction-tuned LLMs: GPT-4, Bloomz, Mistral 7B, and Phi-3-mini. We report the accuracy of these LLMs on the Q&A task in Table 5. Given that the dataset is binary, we prompted the LLMs to answer \"Yes\" or \"No\" to each question, or to say \"I don't know\" if they could not provide a definite response (see Section C.1)."}, {"title": "Conclusion and Discussion", "content": "Our results show that both of our proposed algorithms successfully attribute the output of LLMs to the BoolQ dataset (as the proxy for related knowledge). Comparing LLMs GPT-4 showed minimal change in the similarities when BoolQ context was added, suggesting prior exposure to similar data, while Bloomz exhibited a high residual, indicating substantial influence from BoolQ. The CMF algorithm provides further insights by quantifying the contributions of the base LLM. Comparing our two methods, we conclude that CMF is computationally less expensive and more robust to the RAG noise.\nIn this paper, we considered several datasets as proxies for the datasets used for training of LLMs. For future work, we will try to avoid this approximate method and train our LLMs on specific datasets and test our algorithms with ground truth contributing datasets obtained by retraining."}, {"title": "Implementation Details", "content": "For simplicity of evaluation and without loss of generality, we used BoolQ (Clark et al., 2019) Q&A dataset, where the answers are binary Yes/No. To instruct the LLMs to provide direct boolean responses, we used prompt engineering. Initially, we tested various prompts without explicitly instructing the model to answer with \"Yes\" or \"No.\" Diverse examples used in this process are provided in Appendix C.3. Through iterative testing, we found that responses improved when the model was explicitly instructed to provide a boolean answer. This led to our final prompt:\nPrompt: \"Given the context below, answer the question that follows with only 'Yes', 'No', or 'I don't know' if the context is insufficient.\n{question}? The answer to this question is\"\nWhile this final prompt worked well for GPT-4, Bloomz, and Mistral 7B, generating straightforward \"Yes,\" \"No,\" or \"I don't know\" responses, it was harder to instruct Phi-3-mini. Even with the final prompt, Phi-3-mini often generated more text than just a simple boolean response.\nTherefore, calculating similarities was straightforward for GPT-4, Bloomz, and Mistral 7B, but we had to devise another solution for Phi-3-mini. The embedding similarity API on GPT-4 was not precise enough as it did not focus primarily on the context of the generated response. To calculate the similarity for Phi-3-mini, we created a Zero-shot classification layer (which takes 1000 characters) between the prediction and the result to measure similarity more accurately."}, {"title": "Using RAG", "content": "Given the limitations of LLM context windows, fitting entire datasets directly into the context is impractical. To address this, we utilized Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) to enhance the context by retrieving relevant documents from databases before generating responses. The process involves splitting the documents into semantically relevant chunks using the Recursive CharacterTextSplitter from the Hugging Face Transformers library, computing embeddings for all chunks with a model like thenlper/gte-small, and storing these embeddings in a vector database using FAISS (Facebook AI Similarity Search). When a question is posed, it is embedded and a similarity search is performed against the vector database to find the closest matching documents. These retrieved documents are then provided as context to the LLMs along with the original question, allowing the LLMs to generate responses augmented with additional context.\nWe used a chunk size of 512 and a top-k value of 3, ensuring the context was trimmed to 2000 characters for conciseness."}, {"title": "Prompts", "content": "General Question Prompt: \u201cRead the context provided and answer the following question: {question}\"\nContextual Understanding Prompt: \"Based on the information in the context, what can you conclude about the following question? {question}\"\nSummarization Prompt: \"After considering the context below, summarize your answer to this question: {question}\"\nOpinion-Based Prompt: \"Given the details in the context, what is your opinion on the following question: {question}\"\nDetail Extraction Prompt: \"Extract relevant information from the context to answer this question: {question}\"\nFact-Checking Prompt: \"Using the context provided, verify the accuracy of the following statement: {question}\"\nTable 5 shows the average accuracy calculated by comparing the predictions with the ground truth from BoolQ."}]}