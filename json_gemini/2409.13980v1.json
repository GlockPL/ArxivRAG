{"title": "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "authors": ["Zhiyuan Li", "Dongnan Liu", "Chaoyi Zhang", "Heng Wang", "Tengfei Xue", "Weidong Cai"], "abstract": "Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models' advanced reasoning ability. Traditional Vision-Language Models (VLMs) perform well in visual perception tasks while struggling with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities; however, they lack visual acuity. To bridge this gap, we propose Complex Visual Reasoning Large Language Models (CVR-LLM), capitalizing on VLMs' visual perception proficiency and LLMs' extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs' text knowledge for accurate predictions without extra training. We also introduce a novel multimodal in-context learning (ICL) methodology to enhance LLMs' contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all.", "sections": [{"title": "1 Introduction", "content": "The concept of complex visual reasoning was introduced with Visual Commonsense Reasoning (VCR) dataset (Zellers et al., 2019) in 2019, which tests models' ability to understand visual content as well as commonsense cognition. However, the development in this field has remained relatively subdued, primarily due to Vision-Language Models' (VLMs) limitations in incorporating commonsense knowledge (Gan et al., 2022). Recent years have seen significant advancements in complex linguistic reasoning tasks (Cobbe et al., 2021; Wei et al., 2022) due to the emerging GPT3 (Brown et al., 2020), LLaMA (Touvron et al., 2023a), and Vicuna (Chiang et al., 2023). This leap forward has triggered a renewed interest in the complex visual reasoning area, exploring how visual perception can enhance linguistic inference and potentially overcome previous hurdles (Gan et al., 2022). It has led to innovative benchmarks focusing on various aspects: commonsense reasoning - WinoGAVIL (Bitton et al., 2022), compositionality - Winoground (Thrush et al., 2022), weird image explanation - Whoops (Bitton-Guetta et al., 2023), and humor understanding - NYCCC (Hessel et al., 2022). These tasks demand models not only accurately interpret image content, but also integrate knowledge from daily experiences, general commonsense, cultural context, and humor sense. For example, a synthetic image, as shown in Whoop's example in Figure 1 of \"The portrait of the Mona Lisa depicts a stern male face.\" contradicts the cultural context, as the famous painting Mona Lisa depicts a female face.\nIn this paper, we introduce a novel method named Complex Visual Reasoning Large Language Models (CVR-LLM), based on the \"VLMs + LLMs\" concept. Recent multimodal large language models (MLLMs) like LLaVA (Liu et al., 2024, 2023a) and MiniGPT4 (Zhu et al., 2023; Chen et al., 2023) have proven effective in many VL tasks. However, these models are resource-intensive, relying on millions of image-text pairs for projection layer learning. To overcome this limitation, our approach leverages the visual perception strengths of VLMs to translate images into context-aware image descriptions (CaID) via an inference-only, dual-loop self-refinement process that incorporates feedback from LLMs. These detailed descriptions enhance the LLMs' inference process, transforming multi-modal tasks into simpler single-modal challenges and streamlining the overall process. In addition, we develop a unique multi-modal in-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Reasoning Research in Vision-Language Domain", "content": "In recent years, multi-modal reasoning research has significantly advanced. Beyond the complex visual reasoning benchmarks discussed in Section 1, many studies focus on the reasoning process itself, such as chain-of-thought (Kojima et al., 2022; Shaikh et al., 2022) or reasoning modules (Zhou et al., 2023b; Jiang et al., 2023), which are crucial for enhancing AI models' analytical capabilities and performance. For instance, Liu et al. (2023b) introduced a modality-aligned thought chain reasoning framework to incorporate explicit reason-ing into task-oriented dialogue generation, improv-"}, {"title": "2.2 Large Language Models for Vision-Language Analysis", "content": "The past two years have seen an unprecedented surge in the development and application of LLMS (Brown et al., 2020; Touvron et al., 2023a; Chiang et al., 2023) across diverse fields. LLMs have garnered acclaim for their robust capabilities, including advanced analytical prowess (Kojima et al., 2022), extensive text-level knowledge (Naveed et al., 2023) and superior understanding ability (Chang et al., 2023). Furthermore, they are equipped with two powerful mechanisms: chain-of-thought (Kojima et al., 2022) and in-context learning (Liu et al., 2021a), which significantly augment their effectiveness and performance in specialized tasks (Naveed et al., 2023). For example, Muraoka et al. (2023) developed a cross-lingual model trained alongside a cross-lingual LLM, leveraging LLMs' capabilities across languages. Lan et al. (2023) proposed reasoning question prompts for Visual Question Answering (VQA) tasks, unlocking LLMs' potential in zero-shot learning. Additionally, Yang et al. (2023) introduced SODA, a system that integrates LLMs with explainable AI to assist marketers with data interpretation, enhancing human-AI collaboration. Zhong et al. (2023) used knowledge distillation to imbue the SUR-adapter with LLMs' semantic understanding and reasoning capabilities."}, {"title": "3 Methods", "content": "In this section, we introduce the CVR-LLM framework, highlighting its innovative process for generating context-aware image descriptions (CaID) as well as its complex visual reasoning in-context learning (CVR-ICL) strategy. Initially, we explain the CaID generation process, which differs from traditional image captioning by using a self-refinement loop with feedback from Large Language Models (LLMs) to produce accurate and contextually relevant descriptions (Section 3.1). Subsequently, we present the CVR-ICL approach (Section 3.2), which enhances LLMs' contextual understanding and reasoning by assessing relevant cases and selecting suitable complex multi-modal demonstrations."}, {"title": "3.1 Context-Aware Image Description", "content": "Pre-trained VLMs (Li et al., 2023; Alayrac et al., 2022) have demonstrated their proficiency in generating detailed image captions on benchmarks such as MSCOCO (Chen et al., 2015). However, while these captions may accurately reflect visual content, they are not customized for complex visual reasoning scenarios. Recently, the trend of multi-modal instruction-following agents like miniGPT4 (Zhu et al., 2023; Chen et al., 2023) and LLaVA (Liu et al., 2024, 2023a), integrating open-source LLMS (Chiang et al., 2023; Touvron et al., 2023b) with pre-trained vision encoders (Dosovitskiy et al., 2020; Liu et al., 2021b) to create a MLLM, has become very popular. The effectiveness of these models is heavily reliant on tuning with vast amounts of VL instruction data, which is generated by powerful LLMs like ChatGPT (OpenAI, 2022) and GPT4 (Achiam et al., 2023). While promising, their reliance on extensive VL instruc-"}, {"title": "3.2 Complex Visual Reasoning ICL", "content": "LLMs are renowned for their exceptional in-context learning capabilities, especially with task-specific examples. The optimal in-context exemplars enable LLMs to leverage their background knowledge for more precise outcomes. However, most of the research works (Liu et al., 2021a; Sorensen et al., 2022) have primarily focused on the text-centric domain, with few works (Alayrac et al., 2022; Zhao et al., 2023) exploring multi-modal in-context learning for VL tasks. Our approach, unlike prior methods focused solely on text similarity in NLP, such as the kNN-augmented in-context example selection (KATE), integrates multi-modal factors, thereby enriching the discipline with a fresh perspective. Furthermore, it is also different from MMICL (Zhao et al., 2023) in the multi-modal domain, which employs a vision prompt generator for image-to-visual embedding conversion and merges these with text embeddings as a union measurement factor.\nComplex visual reasoning tasks demand models capable of selecting in-context examples from a multi-modal domain, leveraging extensive background knowledge and information within it (Zhao et al., 2023). However, our CVR-LLM is grounded in LLMs, which are inherently text-based, leading to a gap between textual and multi-modal domains. Directly applying a text-based kNN clustering method could result in the loss of important multi-modal information. On the other hand, using multi-modal information for retrieval might ignore"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset and Metrics", "content": "To evaluate the effectiveness of our proposed method, we conduct a comprehensive test in complex visual reasoning areas. Our evaluation included WinoGAViL (4373 samples), Winoground (400 samples), Whoops (500 samples), VCR (2653 out of over 26k samples, selecting a random 10%), and NYCCC (528 samples), providing a broad assessment of our approach's capabilities. In the terms of metrics, we adhered to the evaluation methods provided by these datasets, ensuring a fair assessment of our method's performance."}, {"title": "4.2 Implementation Details", "content": "For the basic captioner in context-aware image description (Section 3.1), we choose the BLIP2-flant5xxl (Li et al., 2023) as our baseline. For CVR-ICL phase (Section 3.2), we employ BM25 (Robertson et al., 1995) and BLIP2 multi-embedding (Li et al., 2023) to encode text and multi-modal inputs, respectively. It is important to note that the ICL example results are derived from LLM inference without using actual annotations to prevent data leakage. For our LLMs, we choose three"}, {"title": "4.3 Comparison to State-of-the-Arts", "content": "In this section, we evaluate our proposed CVR-LLM against various models across a range of complex visual reasoning tasks, including Wino-GAVIL, Winoground, Whoops, VCR, and NYCCC. These models fall into two categories: VLMs (Kim et al., 2021; Radford et al., 2021; Gan et al., 2020; Li et al., 2023) and MLLMs (Liu et al., 2024, 2023a; Zhu et al., 2023; Chen et al., 2023). Notably, MLLMs like LLaVA and MiniGPT4 struggle with tasks involving multiple images, making their performance data unavailable for WinoGAVIL and Winoground.\nTable 1 showcases our method's superiority across five tasks, eclipsing both VLMs and LMMs. For example, our CVR-LLMLlama3 significantly surpasses the SOTA model BLIP2 by achieving an 88.7% accuracy (+17.1 improvement) in SWOW setting on the WinoGAViL benchmarks. Similarly, it outperforms the SOTA model MiniGPT4 with a 62.0% accuracy (+13.8 improvement) on the GPT4 rate (Bitton-Guetta et al., 2023) for Whoops tasks, underscoring our framework's advanced performance. Additionally, our method performs well on three LLM-based categories, demonstrating robust generation abilities with consistent performance. This highlights the versatility and adaptability of our model, ensuring high-quality results across various complex visual reasoning tasks."}, {"title": "4.4 Ablation Studies", "content": "In this section, we examine the individual contributions of the components within our framework CVR-LLMGPT4. As demonstrated in Table 2, we present an ablation study that quantifies the performance impact of each module across various datasets. The experimental findings suggest that the CVR-ICL module significantly boosts the inference performance of LLMs compared to using context-aware image descriptions alone, with the exception of the NYCCC dataset (It may be due to NYCCC's focus on humor, where precise descriptions are more critical). This highlights the CVR-ICL module's effectiveness in enhancing LLM capabilities across various tasks. In addition, our comprehensive method, CVR-LLM, which integrates both context-aware descriptions and CVR-ICL, achieves a substantial enhancement in performance relative to the baseline."}, {"title": "4.5 Analysis", "content": ""}, {"title": "Context-Aware Image Description vs General Image Caption", "content": "In this section, we investigate CaID's impact at an abstract level and design a novel method to quantitatively demonstrate the semantic gap between context-aware image descriptions and general image captions (Note that the performance impact has been shown in Table 2). Figure 5 provides two examples comparing context-aware image descriptions with general image captions and our goal is to determine whether context-aware descriptions offer more contextually relevant information to aid LLMs in decision-making. Un-"}, {"title": "5 Qualitative Results", "content": "To showcase the capabilities of our approach, we present qualitative results in Figure 9. It illustrates how LLMs leverage contextual information to ask more relevant and insightful questions tailored the specific tasks. For instance, when provided with an image of the chess piece, the LLMs might ask"}, {"title": "6 Conclusion", "content": "In this work, we propose CVR-LLM, an innovative approach for complex visual reasoning tasks. This method boosts LLMs' understanding of visual content for complex reasoning via context-aware image descriptions. We also develop a multi-modal in-context learning technique, enhancing LLMs' reasoning skills at both image and text levels. Experimental results show that CVR-LLM sets new benchmarks across multiple complex visual reasoning tasks. We also introduce a nuanced GPT4 based analysis technique Chain-of-Comparison to automatically break down and contrast among various"}, {"title": "7 Limitation", "content": "Although our approach achieves SOTA performance across a wide range of complex visual reasoning benchmarks, it still has two notable limitations. First, compared to the MLLMs that can perform end-to-end inference directly, our approach operates as an LLM-agent-driven framework. This involves VLMs generating context-aware image descriptions, followed by the LLM performing inference with ICL to predict the answer. While this two-step process enhances contextual understanding and reasoning, it may significantly increase time consumption compared to direct end-to-end inference models. Second, despite its overall strong performance and generalization ability, our approach still lags behind GPT4V in some tasks. Figure 10 shows that our CVR-LLM can surpass GPT4V in SWOW setting in WinoGAViL dataset but fall short in others. Our future work will focus on refining the integration between VLMs and LLMs components and enhancing the model's efficiency and accuracy across a broader spectrum of complex visual reasoning challenges."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Qualitative Results with Corresponding Prompt", "content": "Section 5 only illustrates the simplified process of our Context-aware Image Description (CaID) generation. Here, we delve into more details about the generation process and the corresponding prompts. Figure 11 provides an example of the CaID generation process applied to the VCR (Zellers et al., 2019) task. In this example, the initial input consists of an image showing several individuals, with two of them (Person1 and Person4) holding guns. The associated question is: \"Why do Person1 and Person4 have guns?\u201d with multiple-choice options such as \u201c1) They are soldiers. 2) Person1 and Person4 are robbing a hotel room. 3) They are cattle thieves. 4) They are about to shoot someone.\u201d. The CaID process begins by generating a detailed description of the image. The captioner model produces an initial caption: \u201cAn image of a man in a suit with a gun and another in a suit with a gun.\u201d. This caption, while descriptive, lacks the context needed to answer the specific question posed. To address this, our system prompts the LLM with a scenario where it acts as a questioner for the image caption model. The LLM is instructed to generate a follow-up question to gather crucial information for answer prediction. The prompt guides the LLM to consider specific details such as the appearance and pose of the individuals. In this case, the LLM generates the question: \u201cWhat is the appearance of Person1 and Person4?\u201d. This question is designed to extract more contextually relevant details from the image captioner. The captioner then provides a refined description: \u201cPerson1 is wearing a suit with a gun and Person4 is wearing a suit with a gun.\u201d. This additional information helps to better understand the scene and narrows down the possible answers to the original question. This detailed process highlights how our system leverages both multi-modal and textual information to generate precise and contextually relevant descriptions, ultimately improving the performance on complex visual reasoning tasks."}, {"title": "A.2 Qualitative CVR-ICL Examples", "content": "Section 3.2 only illustrates the mechanism of our CVR-ICL. Here, we explain more details about its implementation. Figures 12 showcases one example of our CVR-ICL on the WinoGAVIL (Bitton et al., 2022)."}, {"title": "A.3 Comparative Analysis with Fine-tuned Models", "content": "In this section, we explore the impact of fine-tuning strategy on performance in complex visual reasoning tasks. Since some tasks in the complex visual reasoning field are initially designed in the supervised setting, we are curious whether our approach can also perform better with the help of real annotation. For the test-only datasets WinoGAViL and Winoground, we randomly divided them into splits of 80% training, 10% validation, and 10% testing. Due to the small number of cases in these tasks, we abandoned training LLMs to avoid catastrophic forgetting. Instead, we chose to fine-tune the captioner using the real labels and incorporated these real annotations into our CVR-ICL examples. Results shown in Table 5 compare our CVR-LLM's performance in zero-shot and fine-tuned settings against SOTA performances, revealing that our method maintains SOTA performance in several areas."}, {"title": "A.4 More Explanation about Our CoC", "content": "The Chain-of-Comparison (CoC) is designed to qualitatively analyze the semantic contribution of context-aware image descriptions against general image captions. It is inspired by the popular idea of Chain-of-Thought, which implements a step-by-step analysis to evaluate effectiveness. Figure 13 shows an example from the Whoops dataset, comparing the semantic gap between a general caption \"An airplane prepares to take off\u201d (Option A) and our context-aware image description \u201cAn airplane is taking off from a highway in the middle of the desert.\u201d. (Option B).\nOur CoC prompt asks the LLM to analyze the semantic contribution through four steps: Initial Perception, Recognizing Incongruity, Contextual Analysis, and Linking to the Question. This process mimics the human brain's analytical process. We directly ask the LLM to compare the contributions of the two options and determine which is better.\nFor instance, in the Initial Perception step, the LLM identifies Option B as superior because it is highly unusual and immediately striking, as airplanes typically do not take off from highways, especially in desert environments. This scenario is much more unusual and striking compared to the routine scenario of Option A, which merely depicts an airplane preparing to take off at an airport. During the Contextual Analysis step, Option B is again favored. The LLM explains that contextually, the scenario raises questions about why an airplane is using a highway in a desert for takeoff, which is not standard practice and could imply unusual circumstances or emergencies. Option A, in contrast, has nothing contextually strange about an airplane preparing for takeoff in a typical airport setting. Finally, in the Linking to the Question step, the LLM determines that Option B provides a clearer connection to the concept of weirdness through its unconventional and striking situation. Option A does not inherently link to weirdness, as it describes a routine occurrence in aviation.\nThis example demonstrates how our CoC framework effectively breaks down and evaluates the semantic contributions of different types of image descriptions, highlighting the advantages of context-aware image descriptions in complex vi-"}, {"title": "A.5 The CVR-LLM Performance with LLAMA2", "content": "Table 1 presents the results of our CVR-LLM framework using Llama3, GPT-3.5, and GPT-4 base models. Additionally, we evaluated CVR-LLM on the Llama2-13B model (Touvron et al., 2023b), which was also employed in LLaVA (Wu et al., 2023; Liu et al., 2024), to ensure a fair comparison. Table 6 compares the performance of CVR-LLM (Llama2-based) and CVR-LLM (Llama3-based) against LLaVA versions 1.0 (Liu et al., 2024) and 1.5 (Wu et al., 2023) on complex reasoning tasks. The results demonstrate that while our CVR-LLM performs well on the Llama2 base model, it slightly underperform compared to Llama3."}, {"title": "A.6 The Parameter Setting in Equation 4c", "content": "Section 3.2 explains that our in-context learning examples are selected based on a similarity score calculated as follows:\n$S = \\alpha * S_m + S_t$, ($\u03b1 = 1$).\nIn this section, we discuss how the parameter a influences the performance of In-Context Learning (ICL). Table 7 presents the results for various values of a on the WinoGAVIL dataset. The results indicate that a = 1 leads to the best performance of our CVR-ICL strategy."}, {"title": "A.7 Comparison against Other VLM+LLM Methods", "content": "In the main paper, we compare our method with several popular end-to-end MLLMs, including LLaVA (Wu et al., 2023) and MiniGPT-4 (Zhu"}, {"title": "A.8 The Performance on Multi-step Reasoning Dataset", "content": "Our CVR-LLM framework is designed for complex visual reasoning tasks, making it well-suited for multi-step reasoning datasets, such as ScienceQA (Lu et al., 2022) and M3CoT (Chen et al., 2024). In this section, we evaluate the performance of our CVR-LLM on the M3CoT dataset to determine its effectiveness. Table 9 presents a comparison between our CVR-LLM and other Tool-Usage methods. The results show that our approach performs well on questions related to general image content, particularly in areas like physical and social sciences. However, it faces challenges with images containing multiple elements, occasionally leading to hallucinations in detailed descriptions."}]}