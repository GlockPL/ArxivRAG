{"title": "FORECASTING OPEN-WEIGHT AI MODEL GROWTH ON HUGGING FACE", "authors": ["Kushal Raj Bhandari", "Pin-Yu Chen", "Jianxi Gao"], "abstract": "As the open-weight AI landscape continues to proliferate\u2014with model development, significant\ninvestment, and user interest\u2014it becomes increasingly important to predict which models will ulti-\nmately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics[1]\nin scientific literature, we propose a framework to quantify how an open-weight model's influence\nevolves. Specifically, we adapt the model introduced by Wang et al. for scientific citations, using\nthree key parameters\u2014immediacy, longevity, and relative fitness to track the cumulative number\nof fine-tuned models of an open-weight model. Our findings reveal that this citation-style approach\ncan effectively capture the diverse trajectories of open-weight model adoption, with most models\nfitting well and outliers indicating unique patterns or abrupt jumps in usage. Link to the website for\ntrajectory visualization: 1", "sections": [{"title": "Introduction", "content": "The rapid expansion of the open-weight AI ecosystem has led to a diverse landscape of models, each varying in size,\ncompany affiliation, and adoption patterns, raising critical questions about their long-term influence and impact [2].\nUnderstanding how influential a model will become is crucial for AI governance, business strategy, and scientific\nprogress [3, 4, 5, 6].\nRecent work underscores the growing impact of open-weight foundation models, highlighting both their benefits and\nchallenges. Henderson et al. [7] examine fair use uncertainties when training on copyrighted material, while Chan\net al. [8] address both the perils of fine-tuning accessible models and the importance of governance mechanisms,\nas further emphasized by Chan et al. [9] in their discussion of agent identifiers and real-time monitoring. Eiras\n[10] evaluates risks and opportunities across various development stages, advocating open-sharing practices alongside\nmitigation strategies. Collectively, these works illustrate the need for thoughtful consideration of open-source Al's\nevolving role in research, industry, security, and governance.\nA closer look at fine-tuning patterns (figure 1) highlights the diversity of open-weight model adoption. Some base\nmodels experience rapid adoption, while others grow steadily. Understanding these dynamics is key to anticipating\nmodel prominence. Motivated by these insights, can we predict the trajectory of influence an open-weight model\nwill have on the AI community? This inquiry drives our exploration into utilizing early adoption trends\u2014particularly\nthe observed growth rates-which can reliably forecast long-term impact, ultimately informing both strategic decisions\nand governance in the AI domain."}, {"title": "Framework for Analysis", "content": "Building on this quantitative perspective, parallels emerge with the citation dynamics observed in scientific research[1].\nWe utilize the dynamics of the citation model proposed by Wang et al., hypothesizing that a similar approach can\neffectively capture open-weight model adoption due to analogous patterns observed in usage. Specifically, we propose\na framework defined by three key parameters\u2014immediacy (\u03bc\u2081), longevity (\u03c3\u2081), and relative fitness (\u03bb\u2081)\u2014along with t,\nthe time duration after a model is released, m, the average number of fine-tuned models, and I, the cumulative normal\ndistribution function. The adoption dynamics of an open-weight model are governed by\n$C_i(t) = m e^{-(\\frac{t - \\mu_i}{\\sigma_i})^2} \\lambda_i \\Phi \\left( \\frac{t - \\mu_i}{\\sigma_i} \\right),$ (1)\nand,\n$\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} e^{-(\\frac{t^2}{2})} dt,$ (2)\nwhere immediacy (\u03bc\u2081) governs the time required for an open-weight model to reach its peak adoption, longevity\n(\u03c3\u03b5) captures the decay rate of the model's influence, and relative fitness (1) represents the model's inherent relative\ninfluence compared to other models in the ecosystem.\nThis dynamics effectively captures how attention and usage evolve in scenarios where an innovation-a scientific\npublication or an open-weight model\u2014is introduced and disseminated across a community. Similar to how a citation\ngrows rapidly once a paper is published and then gradually decays over time, open-weight models also tend to experi-\nence an initial burst of interest followed by sustained but diminishing engagement. By incorporating parameters like\nimmediacy, longevity, and relative fitness, the model reflects both the short-term surge in popularity and the long-term\nretention trajectory. Consequently, this aligns well with observed patterns in open-source development communities,\nmaking it a suitable framework for understanding and predicting model usage. Figure 4, similar to figure 1, illustrates\nthe actual cumulative number of fine-tuned models after the base model is open source through HuggingFace.\nFollowing a similar approach to modeling the cumulative number of fine-tuned models, we also examine the cumu-\nlative download trajectories for each model. Using the framework, in Appendix Section E, we empirically predict\nthe cumulative number of downloads for the widely popular DeepSeek models. Given 20 days of downloads for\ndeepseek-ai/DeepSeek-R1, we expect it to reach 1.3 billion cumulative downloads within 75 days."}, {"title": "Fitting the model to Empirical Data", "content": "Using the HuggingFace ecosystem\u00b2, we fit equation 1 to track the cumulative number of fine-tuned models built from\na base model i, measured t months after its release as an open-weight model on HuggingFace (Detailed in Appendix\nA)."}, {"title": "Dependency of parameters", "content": "While each parameter provides a distinct lens into model adoption-\u00b5i dictating time to peak, \u03c3\u03b5 controlling longevity,\nand i determining overall attractiveness\u2014their relationships reveal more nuanced dynamics. For instance, models\nwith high A\u2081 but low \u03c3\u2081 experience rapid adoption but short-lived influence, suggesting strong initial appeal but limited\nlong-term utility. Conversely, models with moderate \u5165\u2081 and high \u03c3\u03b5 exhibit sustained adoption, indicating persistent\nengagement over time despite lacking an immediate surge in popularity. These dependencies illustrate how differ-\nent models follow varied life cycles, from fleeting trends to enduring contributions, emphasizing the importance of\nmodeling adoption beyond single-parameter distributions.\nFigure 2b shows the pairwise relationships among the three parameters\u2014immediacy \u03bci, longevity \u01a1i, and relative\nfitness Ai-plotted on log-scale axes. In the left panel (\u03bb\u2081 vs. \u03bc\u2081), we see that some models combine very high relative\nfitness with smaller values of \u03bc\u2081, indicating both a rapid rise to peak adoption and strong overall influence. Others\nstretch to larger \u03bc\u2081, implying they take longer to reach peak usage even if they remain moderately or highly appealing.\nIn the middle panel (\u03bb\u03af vs. \u03c3\u03af), points with high i but small \u03c3\u03b5 suggest that although the model attracts many users,\nits influence weakens more quickly, whereas higher \u03c3\u03b5 corresponds to a more prolonged \"tail\" of adoption. The right"}, {"title": "Organization-Specific analysis on model's importance relative to other models", "content": "The diversity of parameter distributions in figure 2(a) underscores the varied pathways through which open-weight\nmodels gain and sustain adoption. While some models achieve rapid prominence with enduring influence, others\nexperience a slower ascent or a more transient impact. These heterogeneous adoption dynamics suggest that intrinsic\nqualities do not solely dictate model success but are also shaped by external factors, including company strategies and\necosystem positioning.\nBy examining models with moderate relative fitness (1 \u2264 \u5165\u00bf < 10), figure 3 provides insights into the temporal shifts\nin the frequency of fine-tuned models, revealing how different organizations' models evolve in their attractiveness for\ngeneral adaptation over time. At the early stage (2 months), distinct peaks in ct emerge, suggesting that models in\nthe fitness range receive some degree of initial interest for some organizations and some have some more outstanding\ninterests. Over time, the KDE curves for most companies begin to align in broader but partially overlapping bands,\nindicating that short-term disparities in how quickly each base model is fine-tuned start to level out. Meta, BAAI, and\nGoogle exhibit the highest peaks at low ct, indicating that most base models are fine-tuned quickly. At the same time,\ncompanies like StabilityAI and Qwen display broader distributions, suggesting variability in adoption, and Microsoft,\nAmazon, and Apple have minimal early fine-tuning activity.\nThe gradual consolidation, where peaks shift toward higher ct values for some models and compress for others,\nreflects both sustained adoption and potential \"latecomer\" effects. This behavior is consistent with the idea that\nmodels with \u5165\u2081 in the relatively higher range dominate immediately; rather, they accumulate a steady stream of fine-\ntuned variants as the model gets widely adopted. At 6 months, Meta, BAAI, and StabilityAI still show strong peaks\nat lower ct, but Allen_AI and Qwen exhibit sharper peaks at higher ct, suggesting that some of their base models gain\nsignificant traction. By 12 months, the distributions flatten, indicating more widespread fine-tuning across different\nbase models, with companies like Meta, StabilityAI, and Qwen showing extended tails, signifying that some models\ncontinue accumulating fine-tuned variants over time.\nThese density curves reinforce the notion that models with high relative fitness levels follow the number of fine-\ntuning trajectories that differ initially but increasingly resemble one another over longer horizons. This aligns with\nexpectations from the citation model [1], where cumulative citations of papers with early \u201chead starts\u201d or slower uptake\nsubside, comparable fitness values dominate the eventual adoption outcome, yielding nearly the same ultimate impact\nct. Individual trajectories of models based on these companies are highlighted within Appendix D."}, {"title": "Conclusion", "content": "In this study, we adapted a well-known citation model to examine the adoption dynamics for open-source models.\nThrough this analogy, we introduced three parameters-immediacy, longevity, and relative fitness-to characterize\nearly and sustained usage patterns. Our empirical analysis of monthly fine-tuning counts reveals that most mod-\nels exhibit predictable trajectories, whereas a minority of AI models show sudden, significant surges in popularity,\nchallenging the assumption of simple exponential decay. We also found that organization-level factors significantly\nshape these usage patterns: models released by Meta, Google, BAAI, and StabilityAI display distinct adoption curves\nreflective of each company's open-source strategies and ecosystem support. Our citation-inspired framework helps\nstakeholders from industry leaders optimizing release strategies to policymakers overseeing AI governance-better\nunderstand how open-weight models evolve and gain traction. Future research can extend this approach by integrating\nadditional data to refine long-term adoption forecasts and further clarify the influences driving model success."}, {"title": "Appendix", "content": "We collect data on open-weight model adoption using the Hugging Face API, which provides comprehensive metadata\non models uploaded to the platform. Given that HuggingFace\u00b3 serves as the primary repository for open-source AI\nmodels, we assume that the vast majority of publicly available models are hosted there .\nTo quantify fine-tuning activity, we track the number of fine-tuned models uploaded to Hugging Face after the release\nof a given base model, aggregating counts by month. For example, since LLaMA 2 was released on July 18, 2023, we\nbegin counting fine-tuned models from that point onward. However, we exclude the earliest models such as GPT-2 and\nBERT variants, as these were only uploaded to Hugging Face on March 2, 2022, despite being released much earlier,\nwhich could distort the adoption timeline.\nFine-tuned models are identified based on their tags and model names, but due to inconsistencies in labeling, it is\npossible that some fine-tuned models are not captured in our dataset. Additionally, Hugging Face provides only the\ntotal number of downloads for a model without historical breakdowns. To address this, we began recording daily total\ndownloads for each model starting in September 2024, allowing us to approximate temporal trends in adoption."}, {"title": "Parameter m and t", "content": "In our adaptation of the citation model, we set m = 1, meaning that the model predicts the cumulative number of fine-\ntuned models directly without scaling by an arbitrary reference count. This differs from the original citation model,\nwhere Wang et al. [1] set m = 30 to account for the typical number of references in a new paper. They noted that\nfixing m for all papers allows for easy parameter comparison and that increasing m results in a smaller \u00e0 but does not\naffect the overall fitting or prediction when m is comparable to the average number of citations per paper. In our case,\nbecause fine-tuning does not have a well-defined equivalent to the number of references in a paper, we normalize m to\n1, making the adoption curve directly interpretable in terms of the absolute count of fine-tuned models. This ensures\nthat i captures the relative fitness of each base model without needing an external scaling factor.\nAdditionally, we count t in months rather than the year used in the original citation model. This adjustment reflects\nthe timescale of model adoption, where fine-tuned models typically emerge over weeks and months rather than daily\nfluctuations. By aggregating over months, we reduce noise and better capture long-term adoption trends, aligning with\nthe natural timescale at which open-weight models gain traction within the community."}, {"title": "CFitting Equation 1 on empirical data", "content": "The figure illustrates the cumulative adoption trends of various AI models over time, with each subplot representing a\nspecific model. The x-axis denotes time, while the y-axis shows the cumulative count on a logarithmic scale. Red dots\nindicate empirical data points, and the blue curves correspond to the fitted functions using the extracted parameters:\n\u5165\u2081 (growth rate), \u03bc\u03b5 (shift parameter), and \u03c3\u03b5 (scaling factor). The overall fit demonstrates that the selected functional\nform effectively captures cumulative growth patterns for most models.\nThe table 1 includes the extracted parameters for the models listed in figure 5. The parameters reveal considerable\nvariation in the cumulative number of fine-tuned model trajectories. Models such as openai/whisper-large-v3,\nBAAI/EVA, and microsoft/Phi-3-mini-4k-instruct exhibit high \u5165\u2081 values, signifying rapid cumulative adop-\ntion. These models also have relatively high \u03bc\u03b5 values, indicating an early adoption surge, likely due to strong initial\ninterest and high demand.\nNotably, for some models like meta-llama/Llama-3.1-8B and microsoft/Phi-3.5-mini-instruct, high-\nlighted by \"*\" in table 1, display lower \u5165\u2081 and \u03c3\u03b5 values, reflecting a more gradual accumulation of usage over\ntime. The parameter estimation resulted in \u03bb\u2081 = 0.5, \u03bc\u2081 = 2.0, and \u03c3\u2081 = 0.5. These values suggest that the equation\nused for fitting failed to accurately capture the underlying growth dynamics for those models, leading to an ineffective\napproximation of their cumulative adoption curves. This discrepancy is due to irregular growth trends, as seen in the\nfigure 5. This suggests that the framework 1 still requires some more comprehensive adaptation since it fails to capture\nsuch a growth trend.\nThe visualization highlights diverse adoption patterns among AI models, with some experiencing rapid early adoption\nand subsequent saturation, while others follow a steady and prolonged growth trajectory. Deviations from the fitted\ncurves in certain cases suggest additional influencing factors, such as accessibility, licensing constraints, or specific ap-\nplication domains. These findings offer valuable insights into AI model adoption, providing a quantitative framework\nfor assessing their long-term impact and diffusion."}, {"title": "Cumulative Trajectory of Finetuned Models By Organization", "content": "Understanding the cumulative fine-tuning trajectory of models across different companies provides valuable insights\ninto the adoption dynamics and impact of open-source AI models. Fine-tuning is a critical mechanism through which\nbase models are adapted to diverse applications, reflecting their versatility and the strategic priorities of the organi-\nzations that develop them. By examining the temporal evolution of fine-tuned models (ct) for each company, we can"}, {"title": "Analyzing the Cumulative Number of Downloads", "content": "In addition to looking at a number of fine-tuned models, we also briefly study the trajectory of the number of downloads\nfor different models. As mentioned in Section A, we only have download data of models after September 2024. Figure\n9 shows that the framework utilized for downloads does not quite fit the framework well since the trajectories differ"}]}