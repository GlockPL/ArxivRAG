{"title": "HEMM: Holistic Evaluation of Multimodal Foundation Models", "authors": ["Paul Pu Liang", "Akshay Goindani", "Talha Chafekar", "Leena Mathur", "Haofei Yu", "Ruslan Salakhutdinov", "Louis-Philippe Morency"], "abstract": "Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.", "sections": [{"title": "1 Introduction", "content": "Building upon rapid progress in large-scale language and vision pretraining [24, 69, 106], the new generation of multimodal foundation models is increasing adept at learning interactions between modalities [83], enables both static prediction and dynamic interaction [55], and even shows emergent properties never seen before in pretraining corpora [60]. Previous standards for benchmarking multimodal models based on collections of modality and task-specific datasets [8, 57, 29, 66] are increasingly insufficient in light of these general capabilities. In order to study fundamental questions regarding why multimodal foundation models exhibit certain behaviors, when they perform well in the real world, and which modeling paradigms are most effective, there is a need for a holistic evaluation scheme beyond individual datasets or contexts.\nTo address this need, we contribute Holistic Evaluation of Multimodal Models (HEMM), visualized in Figure 1. HEMM, as an evaluation framework, goes beyond conventional lists of datasets to emphasize holistic benchmarking at three levels. The first level benchmarks basic multimodal skills: fundamental internal abilities required to address multimodal problems, such as interactions between redundant, unique, and synergistic features [26, 68], alignment of fine-grained and coarse-grained information [104], reasoning across compositional features [115], and integration of external knowledge [90]. The second level benchmarks information flow: how multimodal information transforms"}, {"title": "2 Key Benchmarking Principles and Datasets in HE\u041c\u041c", "content": "HEMM includes 30 datasets summarized in Table 1. These datasets require different multimodal skills to solve, display different types of multimodal information flow, and belong to different real-world use cases with domain-specific challenges."}, {"title": "2.1 Basic multimodal skills", "content": "Multimodal skills are internal abilities required to solve multimodal tasks, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and using external knowledge.\nMultimodal interactions study how modality information is integrated for a multimodal task [69, 77, 52, 9], which can be redundant: shared between modalities, such as smiling while telling a humorous joke [43, 89], unique: present in only one of the modalities [35, 54], and synergistic: emergence of new information from both modalities, such as conveying sarcasm through conflicting verbal and nonverbal cues [15, 68]. Datasets with high referential information between modalities test for redundancy, such as in VQA, and translation on NOCAPS. Tasks with uniqueness or synergy include understanding movie posters (MM-IMDB), memes (MEMECAP), figurative language (IRFL), facial expressions (FER-2013), and cartoons (NEW YORKER CARTOON).\nGranularity of multimodal alignment involves identifying alignment across elements in different modalities. For example, answering a question might require a model to perform fine-grained align-ment to reference one specific object out of many possible objects in an image. Tasks that explicitly test for fine-grained alignment include localized reasoning on VISUAL GENOME, WINOGROUND, while tasks that emphasize coarse-grained alignment (e.g., making a prediction relevant to a whole image) include interpreting cartoon images [37], movie posters [5], and memes [46, 89, 43]."}, {"title": "2.2 Multimodal information flow", "content": "Multimodal information flow studies how information transforms across tasks, including cross-modal translation, editing, querying, and fusion.\nCross-modal translation exploits shared information by mapping data in one modality to another. Examples include translating from text to image for image generation (e.g., LNCOCO) and translating from image to text for image captioning (e.g., NOCAPS, SCREEN2WORDS).\nCross-modal editing involves semantically editing data in one modality according to another modality (e.g., given an image, following a natural language instruction to \"change the background from day to night\"). The model takes in the original image (with potentially more reference images), along with a task description specifying the edit, and outputs the edited image. We use the MAGIC BRUSH dataset to test cross-modal editing.\nCross-modal querying involves a model's ability to answer natural language questions that query specific information about an input. The model takes in the original image, a text description, the query, and must output the desired answer (typically in natural language). Querying can be done for visual scenes (GQA), environmental indicators (RESISC45), and medical data (VQARAD).\nMultimodal fusion aims to learn interactions to combine information from different modalities, such as classifying diseases given x-ray images and medical tests, or detecting humor from cartoon images and captions. Multimodal fusion takes in the image, text, and a description of the task, and then outputs a prediction, which can include affective states like humor in NEW YORKER CARTOON, hate speech detection in HATEFUL MEMES, or in science problems (SCIENCEQA)."}, {"title": "2.3 Real-world Use Cases", "content": "Each use case is drawn from a real-world application with their own specific challenges.\nMultimedia includes efficient search, retrieval, indexing, and generation of digital content. Multimedia tasks in HEMM include question answering about images and videos (VQA, VCR), multimedia captioning (FLICKR30K, NOCAPS), compositional visual reasoning (WINOGROUND, NLVR), under-standing cartoons, movie posters (MM-IMDB), memes (MEMECAP and MEMOTION), and figurative language (IRFL), and editing images (MAGIC BRUSH).\nAffective computing aims to perceive human affective states (emotions, sentiment, personalities, humor, sarcasm, social interactions) [86], and is important for building emotionally and socially-intelligent AI [56, 78] and human-AI interaction [55]. HEMM includes NEW YORKER CARTOON (cartoon images and captions), HATEFUL MEMES (hateful content in memes), FER-2013 for facial expressions, MEMECAP for meme captioning, and MEMOTION for emotions in memes.\nNatural sciences aims to deepen our knowledge of physical, chemical, biological, and environmental sciences. These can involve satellite images, chemical bonds, land and agriculture use, wildlife, and specific scientific terminologye [101]. Tasks in HEMM include SCIENCEQA testing different science topics and RESISC45 for land scene classification.\nHealthcare involves integrating multimodal signals such as lab tests, imaging reports, and doctor-patient interactions to help doctors interpret high-dimensional data and assist them in diagnosis [48, 51]. We include processing text reports and medical images in the form of PATHVQA for pathology, VQARAD for radiology images, and SLAKE for medical visual question answering.\nHCI involves user design, usability, user experience, and other challenges related to humans inter-acting with computers [81]. HCI tasks can involve visual information such as screen layouts, user actions, and feedback mechanisms. HCI tasks in HEMM include ENRICO for classifying mobile UI designs and SCREEN2WORDS for UI screen content summarization."}, {"title": "3 Key Modeling Principles and Models in HEMM", "content": "Table 2 summarizes the 11 models we evaluate in HEMM, which span different numbers of parame-ters, model architectures, training datasets, pretraining objectives, and fine-tuning objectives."}, {"title": "3.1 Modeling decisions", "content": "Model parameters Parameters can vary greatly across different multimodal models, from 100M params to approximately 1000B params. We consider models with total number of parameters less than or equal to 4B (e.g., INSTRUCT-BLIP) as small, whereas those having more than 4B parameters (e.g., FUYU-8B) are considered medium. GPT-4V and GEMINI are considered large.\nModality processing Some multimodal models (e.g., FUYU-8B) support interleaved inputs like \"<dog_img> This is a very cute dog.<cat_img> This is a very cute cat.\", unlike models that only support separate image and text queries (e.g., BLIP-2, MINI-GPT-4)."}, {"title": "3.2 Training Characteristics", "content": "Training type End-to-end training involves fine-tuning unimodal encoders, pretrained LLMs, and a multimodal model jointly, as seen in EMU, FUYU-8B, etc. Another category operates by freezing"}, {"title": "4 Experiments", "content": "In this section, we discuss extensive experiments conducted to holistically evaluate the performance of multimodal foundation models based on HEMM."}, {"title": "4.1 Experimental setup", "content": "Individual metrics For all text generation tasks, we use the established natural language generation evaluation metric BARTScore [121], which was found to have the highest correlation with human judgement [121]. We compute BARTScore(r, c), where r is the reference and c is the candidate. It can be interpreted as the probability of generating the candidate sentence from the reference. For example, a model might caption an image with the following generated candidate: A row of violins hanging on a wall.. The reference (ground truth) of A painting of 5 cello's with a green background would be used to compute the BARTScore with respect to c.\nAggregating metrics To aggregate scores across multiple tasks or models, we normalize scores using min-max scaling. Following Chang et al. [16], min represents the score of the worst multi-modal model and max represents the identity score BARTScore(r, r), where r is the ground truth. Subsequently, these normalized scores in a 0 to 1 range can be interpreted as a percentage of model performance relative to the ground truth."}, {"title": "4.2 Main results", "content": "We summarize our main results here and include full details in Appendix C. We first explain performance trends across the datasets in HEMM, before explaining performance differences across different multimodal foundation models and their design decisions."}, {"title": "4.2.1 Performance across dataset dimensions", "content": "Overall comparisons We summarize overall trends in Figure 3 and Table 3. On average, mod-els perform better on multimedia datasets, with IRFL (0.58), NLVR (0.50), and WINOGROUND (0.49) showing the highest scores. The lowest scores are for Healthcare, HCI, and Science use cases, such as on DECIMER (0.07), INATURAL-IST (0.08), ENRICO (0.12), PATHVQA (0.15), and MEMECAP (0.32). For predicting molec-ular structures on DECIMER, models are not able to generate correct chemical notations (in Simplified Molecular Input Line Entry System notation) and instead only generates names of individual atoms or compounds (see Figure 2). Other challenging datasets include INATURAL-IST due to fine-grained visual differences be-tween 5000 species of plants and animals, and healthcare datasets that require intricate analysis of pathology images to identify organs, tissues, and anomalies (see Figure 8). Datasets related to memes were also challenging (0.32 and 0.38 on MEMECAP [43] and MEMOTION [89]), re-quiring knowledge about current events, pop culture, and metaphors beyond literal meanings.\nMultimodal skills 1: Interactions The average scores for redundant, unique, and synergistic interactions are 0.29, 0.20, and 0.33. One reason for lower uniqueness scores is the presence of highly challenging visual datasets like DECIMER and ENRICO. On average, the easiest tasks in redundancy are NLVR (0.50) and WINOGROUND (0.49). The hardest datasets in uniqueness are INATURALIST (0.08) and DECIMER (0.07), and in synergy are MEMECAP (0.14) and MEMOTION (0.21)."}, {"title": "4.2.2 Performance across modeling dimensions", "content": "We now compare different modeling decisions and training objectives in Table 4.\nOverall comparisons across models GEMINI [97] (0.44), INSTRUCT-BLIP [22] (0.41), BLIP-2 [62] (0.41), and GPT-4V [1] (0.40) achieve the best average performance across all tasks. The low scores of GPT-4V as compared to GEMINI and INSTRUCT-BLIP are due to its generation of keywords like \u201cIndeterminate\u201d, \u201cUncertain\u201d, and \u201cUnknown\" on datasets like VQA and GQA, perhaps due to its alignment process. Further, on some datasets related to Memes (e.g., HATEFUL MEMES) and Health (e.g., SLAKE), GPT-4V refrains from answering the questions and instead generates a response saying Cannot assist with the request. OPENFLAMINGO [6] (0.06), EMU [95]"}, {"title": "5 Related Work", "content": "Multimodal machine learning brings unique challenges for ML research due to the hetero-geneity between modalities and the interconnec-tions found between them [69]. It has inspired many theoretical studies in data heterogeneity and interactions [25], as well as diverse applica-tions in multimedia [44, 14, 88], affective com-puting [86], robotics [47], finance [39], HCI [25, 82], education [12] and healthcare [80, 110].\nEvaluation frameworks for multimodal models have significantly shaped the multimodal research landscape, through holistic [57, 66] and domain-specific benchmarks [31, 28]. Recent benchmarks have focused on testing the capabilities of multimodal foundation models, such as MME [29], MMBench [73], LVLM-ehub [111], SEED-Bench [59], Touchstone [7], Mm-vet [120], ReForm-Eval [65], VisIT-Bench [11], FLAVA [45]. Other benchmarks focus on evaluating hallucination [21] and applications in medicine [113] and autonomous driving [107]. These benchmarks contain many tasks, but without the systematic taxonomy and comprehensiveness that HEMM provides.\nMultimodal foundation models are promising foundations for the future of AI, with impressive reasoning [75], interactive dialogue [49], and few-shot generalization abilities [100]. These models can be pre-trained (typically with image-text self-supervised learning) and fine-tuned for downstream tasks [63, 74, 91, 67], or based on adapting language models with vision to enable text generation conditioned on images [61, 105]. Cross-modal transformer architectures have emerged as a popular backbone due to their suitability for both language and image data [17, 99]. Additionally, composable diffusion models [96] can be used to further generate combinations of output modalities.\nAdapting language models for multimodality is another promising approach where frozen models are aligned on both vision and language to generate text from multimodal inputs [127, 62, 118, 109]. These approaches typically use parameter-efficient modules like LLaMA-Adapter V2 [30] and MAGMA [27] for efficient finetuning. Vision-language instruction tuning has also emerged as a useful technique, as it allows the models to better follow human instructions [112, 127]. Our goal is to make HEMM the most comprehensive benchmark to study the current and future generation of multimodal foundation models, and for the community to continuously contribute to its expansion."}, {"title": "6 Conclusion", "content": "Holistic Evaluation of Multimodal Models (HEMM) is a framework for benchmarking multimodal foundation models. Through a new taxonomy of multimodal skills, information flow, and real-world use cases, HEMM enables comprehensive analysis of multimodal models. HEMM is publicly available, will be regularly updated, and encourages community involvement in its expansion.\nLimitations and social impact The evaluation of multimodal models is done only on a subset of all possible skills, information, and use cases in the world. Future work can improve the categorization of datasets into skills, information, and use cases, and discover new dimensions that pose challenges to multimodal models. Such evaluation is critical to ensure that models are sufficiently robust when deployed in real-world scenarios, to prevent unexpected and unintended consequences. Future work should also add new metrics to HEMM measuring real-world societal concerns such as fairness, robustness, social biases, privacy, and efficiency of multimodal models."}]}