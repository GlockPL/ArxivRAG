{"title": "Assessment of Transformer-Based\nEncoder-Decoder Model for Human-Like\nSummarization.", "authors": ["Sindhu Nair", "Dr. Y.S. Rao", "Dr. Radha Shankarmani"], "abstract": "In recent times, extracting valuable information from large text is making sig-\nnificant progress. Especially in the current era of social media, people expect\nquick bites of information. Automatic text summarization seeks to tackle this\nby slimming large texts down into more manageable summaries. This impor-\ntant research area can aid in decision-making by digging out salient content\nfrom large text. With the progress in deep learning models, significant work in\nlanguage models has emerged. The encoder-decoder framework in deep learn-\ning has become the central approach for automatic text summarization. This\nwork leverages transformer-based BART model for human-like summarization\nwhich is an open-ended problem with many challenges. On training and fine-\ntuning the encoder-decoder model, it is tested with diverse sample articles and\nthe quality of summaries of diverse samples is assessed based on human evalu-\nation parameters. Further, the finetuned model performance is compared with\nthe baseline pretrained model based on evaluation metrics like ROUGE score\nand BERTScore. Additionally, domain adaptation of the model is required for\nimproved performance of abstractive summarization of dialogues between inter-\nlocutors. On investigating, the above popular evaluation metrics are found to be\ninsensitive to factual errors. Further investigation of the summaries generated\nby finetuned model is done using the contemporary evaluation metrics of factual\nconsistency like WeCheck and SummaC. Empirical results on BBC News articles", "sections": [{"title": "1 Introduction", "content": "Enormous amount of data is available in unstructured format in the internet age as\ncompared to structured data. Such data is generated on a daily basis like online news\narticles, research papers, e-mail messages, e-books to name a few. With the rapid\nrise in web text data, extracting semantic value from long text is a vitally important\nresearch. The sequence-to-sequence(seq2seq) framework in deep learning has become\nthe predominant approach for automatic text summarization. The end-to-end training\nin seq2seq framework learns the semantic mapping between the source documents and\nits corresponding summaries. Despite huge advances in automatic text summarization\nmodels, it is still challenging to generate abstractive summaries of good quality.\nHumans summarize written text by first understanding the content of the docu-\nment. The next step is to identify most important or salient information. Finally, this\ninformation is reworded in a compressed form. Hence, for a synopsis identify impor-\ntant information, delete non-essential extraneous information and then, rewrite the\nremaining information to make it more general and more compact. It is important to\ncomprehend the content of the document to get the central theme and summarize it.\nThere exist two overarching approaches for text summarization. Extractive sum-\nmarization directly copies content straight from the input text to create synopsis. The\nmodel may copy whole sentences or copy words/phrases. One can think of it as using\na highlighter to point out the important parts of the document. This is the approach\nemployed by many classical summarization works, especially before the advent of neu-\nral networks. Abstractive summarization creates a summary without being limited to\nonly words and sentences within the source document. Often this is done by generat-\ning a summary one word at a time by picking a word from a set vocabulary, until a\nwhole summary has been created. One can think of it as how a human might write a\nsummary in their own words. This method allows for more compression since lengthy\nsentences can be reworded into simpler expressions [1]. Neural abstractive summariza-\ntion has reached novel heights, especially with the progress in deep learning models.\nEnd-to-end abstractive summarization models must perform two tasks implicitly at the\nsame time. 1) Content selection: salient sentences or words from the source text must\nbe selected. 2) Surface realization: a summary must be generated which successfully\nmerges the selected content together [1].\nThis paper addresses the following questions. Q1: Does the baseline pre-trained\nBART-LARGE-CNN model when finetuned on a small dataset boost the abstractive\nsummarization performance? Q2: If the input is much diverse from the training dataset\nsay dialogues between interlocutors, will domain adaptation give the enhanced per-\nformance on abstractive summarization? Q3: Is there an alignment or correlation"}, {"title": "2 Conceptual Framework For Abstractive\nSummarization", "content": "The abstractive summarization method produces concise summaries with novel words\nwhich incorporates both syntax as well as semantics. The parameters to check quality\nof summary include fluency, non-repetition, meaningfulness, flow, grammar and syn-\ntax, semantics, conciseness, saliency, information diversity, information coverage and\naccuracy. Deep neural encoder-decoder models are predominantly applied for various\nNLP tasks including summarization [1]."}, {"title": "2.1 Encoder-Decoder Model", "content": "The training mechanism used to map sequential data from one domain to another\nis seq2seq learning. The mapping is based on embedding layer and text generation\ntechniques. Such a framework consists of encoder and decoder. In the encoding stage,\nthe model examines the input and maps it into an intermediate embedding layer. In\nthe decoding stage, from the intermediate vector mapping, the output sequence is\npredicted. Additionally, the basic model can be enriched with a feature-rich encoder\n[2]."}, {"title": "2.2 Context-dependent Transformer-based Models", "content": "Context-dependent approach output different word embeddings for the input word on\nthe basis of the neighbouring words or the semantics of usage of the word. BERT,\nXLM, ROBERTa, ALBERT are some of the transformer based techniques for context-\ndependent approach. They are trained on large corpora, which can be cross-domain.\nThe training has variations in key hyperparameters, mini-batches, learning rates.\nTransformers are just another type of neural network which only uses feedforward\nlayers and attention layers [4]. The Transformer includes an encoder and a decoder,"}, {"title": "3 Related Work", "content": "The recent trend is the use of deep neural networks for the task of summarization which\nis abstractive. Basic deep sequence-to-sequence models have the problem of long-term\ndependencies in long sequences. Convolutional neural networks (CNNs) have met great\nsuccess in abstractive summarization [7], [8] but fail to tackle long documents. One\napproach is to augment attention mechanism which would enable the encoder to focus\nonly on the most salient parts of text. This is an efficient way of memory usage. The\nencoder-decoder attention-based transformer models like BART [6] from Facebook AI,\nPegasus [9] and T5 [10] from Google are the state-of-the-art models for abstractive\nsummarization. Another approach is to leverage Pointer Generator networks which is\na solution to the problem of inaccurate factual details and out-of-vocabulary words\nproblem [11].As discussed in [12], the encoder-decoder model is based on a double\nattention pointer network (DAPT). In DAPT, the self-attention mechanism make\nthe model apt for long-term dependencies in long sequences and the pointer network\ngenerates accurate summaries. In [12], the coverage mechanism is augmented over\nthe double attention pointer network to avoid duplication by keeping track of what\nhas been generated in the summary. Generative adversarial networks [13] have been\nused in many applications with promising results. Previous research has shown the\neffectiveness of generative adversarial networks in text summarization. Researchers\nin [14] propose a novel Hierarchical Human-like deep neural network for ATS (HH-\nATS), inspired by the process of how humans comprehend an article and write the\ncorresponding summary. Specifically, this hybrid model consists of dual discrimina-\ntor generative adversarial network, attention-based knowledge-enhanced module and"}, {"title": "4 Experimental Setup", "content": "The transformer-based BART-LARGE-CNN model is pre-trained on huge corpus of\nCNN/DailyMail Dataset. It consists of more than 300k unique English news articles\npenned by authors at CNN and the Daily Mail. This dataset includes article-highlights\npairings. The dataset is further split into huge amount of training data and equivalent\nportion of validation and test data. Post pre-training, the BART-LARGE-CNN model\nis finetuned with BBC News Dataset. It comprises of BBC News articles which are\nclassified as business articles, entertainment articles, political articles, sports articles\nand technical articles. Each category consists of article-summary pairings.\nThe dataset is available at https://www.kaggle.com/antoniobap/datamining-bbc-\nnews/data. This dataset\nhas news documents with it's corresponding synopsis. The dataset has a fair distribution of the\ndifferent categories of news articles. Thus, there is a good balance in the categorization\nof documents."}, {"title": "4.2 Abstractive summarization with Huggingface", "content": "Hugging Face Transformers library uses a pre-trained transformer model BART with\nit's large-sized version, fine-tuned on CNN Daily Mail powered by Meta (Face-\nbook) to generate abstractive summaries from input text. BART-LARGE-CNN uses\nBartTokenizer and generate() methods used for conditional generation tasks like\nsummarization."}, {"title": "5 Empirical Results", "content": "The 'facebook/bart-large-cnn' pre-trained model from huggingface is used in the exper-\niments. The Bart tokenizer is used for the encoding which takes into consideration\nthe position of the words in the sentence. The Bart tokenizer is built from the GPT-\n2 tokenizer. The summaries generated were fluent. Hence, the summaries generated\nby the BART pre-trained model demonstrating the effectiveness of the BART model\nfor text understanding. There was significant improvement in the ROUGE score after"}, {"title": "6 Results and Discussion", "content": "Since the model is fine-tuned on BBC News Dataset, it performs well for newspaper\narticles. However, performs poorly when a dialogue is input to fine-tuned BART-\nLARGE-CNN. Hence, further experiments were done with diverse input samples\nranging from sports news article, technical article, health article, political news article,\narticle with many numerical facts, article with a flow starting with a problem and end-\ning in a solution. It was observed that when these inputs were given to the fine-tuned\nBART-LARGE-CNN model, they generated good summaries which were fluent, free\nof grammatical errors, devoid of duplication and coherent. When compared with the\nhuman generated summary for above, they showed significant improvement over the\nbaseline pre-trained model in terms of ROUGE score and BERTScore. This attempts\nto answer Q1 as the finetuned model shows enhanced abstractive summarization\nperformance over the baseline pre-trained model.\nFor Samplel which is a technical article, Sample5 which is a health article and\nSample6 which is a political news article, the summary generated by the finetuned\nmodel was fluent, free of grammatical errors, devoid of duplication, faithful, accurate,"}, {"title": "7 Conclusion", "content": "Transformer-based deeplearning models are the SOTA for various NLP tasks. This\npaper investigates the performance of the SOTA encoder-decoder BART model for\nthe summarization task. The empirical details shows good performance with fine-\ntuned approach as is evident by the ROUGE score and BERTScore. As against the\npre-trained model, the finetuned model shows improved performance metrics with\nnewspaper articles. However, domain adaptation was required for dialogues between\ninterlocutors. However, some problems of factual inconsistency and deviation from\nthe source still persisted inspite of good ROUGE score and BERTScore in case of\nsome sample articles. Human evaluation of the generated summaries were based on\nparameters such as coherence, factual consistency, duplication, saliency, fluency and\nfaithfulness. Based on the human evaluation metrics, the quality of summaries was\nassessed. The summaries were found to be fluent, free of grammatical errors, devoid\nof duplication, salient and coherent. However, for some samples it deviated from the\nsource text and hence not faithful to the source text. This factual inconsistency of the\npredicted summaries would hamper the usability of the deeplearning summarization\nsystems especially in sensitive domains like medical, military operations, news media\nas it would be misleading with serious consequences. Design of innovative evaluation\nmetrics which capture the factual accuracy and alignment of the generated synop-\nsis with source text is an active research area since the existing evaluation metrics\nare insensitive to factual errors. Contemporary evaluation metrics for factual consis-\ntency like WeCheck and SummaC highlight the factual inconsistency in the abstractive\nsummaries generated by the finetuned BART model.\nThe authors have no competing interests to declare that are relevant to the content\nof this article."}]}