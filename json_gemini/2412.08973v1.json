{"title": "Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?", "authors": ["Yifan Zhang", "Junhui Hou"], "abstract": "Cross-modal contrastive distillation has recently been explored for learning effective 3D representations. However, existing methods focus primarily on modality-shared features, neglecting the modality-specific features during the pre-training process, which leads to suboptimal representations. In this paper, we theoretically analyze the limitations of current contrastive methods for 3D representation learning and propose a new framework, namely CMCR, to address these shortcomings. Our approach improves upon traditional methods by better integrating both modality-shared and modality-specific features. Specifically, we introduce masked image modeling and occupancy estimation tasks to guide the network in learning more comprehensive modality-specific features. Furthermore, we propose a novel multi-modal unified codebook that learns an embedding space shared across different modalities. Besides, we introduce geometry-enhanced masked image modeling to further boost 3D representation learning. Extensive experiments demonstrate that our method mitigates the challenges faced by traditional approaches and consistently outperforms existing image-to-LiDAR contrastive distillation methods in downstream tasks. Code will be available at https://github.com/Eaphan/CMCR.", "sections": [{"title": "1 Introduction", "content": "LiDAR sensors have become essential tools for capturing detailed 3D information of the environment, playing a critical role in applications such as autonomous driving, robotics, and urban planning (Zhang et al., 2023b, 2024b, 2023a). The rich geometric data from LiDAR point clouds provide valuable spatial awareness that is difficult to achieve with traditional 2D sensors. However, processing these 3D point clouds often requires vast amounts of labeled data to train deep neural networks effectively. Annotating point cloud data, however, is both time-consuming and expensive, posing significant challenges to the scalability and practicality of 3D deep learning models (Sautier et al., 2022; Chen et al., 2024). To address this issue, self-supervised learning has emerged as a promising solution, where networks are first trained on large volumes of unlabeled data (He et al., 2022; Caron et al., 2021). This pre-training process enables the model to learn useful feature representations without the need for costly manual annotations. Once pre-trained, the network can be fine-tuned on smaller labeled datasets, greatly reducing the need for extensive labeling efforts and improving the efficiency of training (Chen et al., 2020).\nA widely used approach for learning 3D representations is contrastive pixel-to-point knowledge transfer, which leverages synchronized and calibrated images and point clouds (Sautier et al., 2022; Mahmoud et al., 2023; Liu et al., 2024; Chen et al., 2024; Liao et al., 2024; Puy et al., 2024; Xu et al., 2025). The PPKT method (Liu et al., 2021) allows a 3D model to benefit from the extensive knowledge encoded in a pre-trained 2D image backbone by using a pixel-to-point contrastive loss, with no need for labeled data for either the images or point clouds. Following this, SLidR (Sautier et al., 2022) introduces superpixels to group pixels and points that come from visually coherent regions, resulting in a more structured contrastive task. Building on these ideas, Seal (Liu et al., 2024) leverages semantically rich superpixels generated by visual foundation models, incorporating temporal consistency regularization to enforce stability across point segments over time. Furthermore, CSC (Chen et al., 2024) investigates cross-scene semantic consistency for multi-modal 3D pre-training, aiming to ensure semantic coherence across all frames and scenes.\nWhile contrastive learning methods have shown success in transferring knowledge between 2D and 3D modalities, they primarily focus on modality-shared information, which can limit their ability to fully capture the unique characteristics of each modality. These approaches emphasize aligning shared features across different modalities, but they often overlook the modality-specific details that could provide complementary insights. For instance, 3D point clouds contain rich spatial and geometric information, while 2D images encode fine-grained visual textures and color details. By concentrating on shared representations, these methods may miss out on leveraging the full potential of modality-specific features during pre-training, leading to suboptimal performance in downstream tasks that require a deeper understanding of each modality's unique contributions (Xu et al., 2013; Liang et al., 2024).\nIn this work, we propose a novel approach that extends the traditional contrastive distillation paradigm by incorporating both modality-shared and modality-specific features. To achieve this, we design distinct heads that separately capture these features, driving the network to learn modality-specific information through tasks such as image reconstruction and 3D occupancy estimation. Additionally, we propose a new multi-modal unified codebook that aligns 2D and 3D features within a shared latent space. This codebook enables the model to focus on commonalities between modalities through shared features, while retaining the unique characteristics of each modality by utilizing separate heads for modality-specific features. By decoupling the shared and specific features, the codebook allows the model to effectively leverage both types of information and enhance performance on downstream tasks. Furthermore, we leverage 3D features to assist in the reconstruction of masked image regions, and this process, in turn, enhances the learning of geometry-aware 3D representations.\nTo assess the effectiveness of our method, we conduct extensive experiments and compare it with state-of-the-art approaches on several downstream tasks, including 3D semantic segmentation, object detection, and panoptic segmentation. The experimental results show that our method surpasses existing self-supervised learning techniques, as it demonstrates superior adaptability and"}, {"title": "2 Related Work", "content": "This section provides an overview of existing research on 3D scene understanding, 3D representation learning, and codebooks, which are directly relevant to the core design of our approach.\n3D Scene Understanding. Traditional methods for 3D scene understanding often rely on representations"}, {"title": "3 Theoretical Analysis of Existing Approaches", "content": "3.1 Preliminary\nNotation. Define $X^{P} = {P_1, P_2, ..., P_N | P_i \\in \\mathbb{R}^3}$ as a point cloud of N points obtained from a LiDAR sensor, and $X^{I} = {I_c | c = 1, ..., N_{cam}}$ as a set of multi-view images captured by $N_{cam}$ synchronized cameras, where each image $I \\in \\mathbb{R}^{H \\times W \\times 3}$ has height H and width W.\nAs a preliminary, we briefly review existing 2D-to-3D contrastive distillation techniques, particularly the point-to-pixel contrastive distillation framework from Liu et al. (2021), upon which we base our approach. Given point cloud and image data as inputs, we apply separate encoders for feature extraction. For the 3D point cloud, we use an encoder $f_{3D}(\\cdot): \\mathbb{R}^{N \\times 3} \\rightarrow \\mathbb{R}^{N \\times C_{3D}}$ to generate per-point features of dimension $C_{3D}$. For images, we use an encoder $f_{2D}(\\cdot): \\mathbb{R}^{H \\times W \\times 3} \\rightarrow \\mathbb{R}^{H' \\times W' \\times C_{2D}}$, which is initialized with weights from pre-trained image models. This framework supports knowledge transfer from the 2D domain to the 3D domain through contrastive learning.\nTo compute the contrastive loss, we design trainable projection heads, $h_{2D}$ for 2D features and $h_{3D}$ for 3D features, which map the features to a common C-dimensional space. The 3D projection head $h_{3D}$ is a linear layer with $l_2$-normalization, transforming 3D features into a normalized C-dimensional space. Similarly, the 2D projection head $h_{2D}$ consists of a 1x1 convolution followed by bilinear interpolation to adjust the spatial dimensions by a factor of 4, and it also applies $l_2$-normalization."}, {"title": "3.2 Multi-view Non-redundancy Assumption", "content": "In the context of point cloud data $X^{P}$ and image data $X^{I}$, contrastive learning methods focus on maximizing the mutual information (MI) $I(X^{P}; X^{I})$. These methods are based on the assumption that most task-relevant information is contained within the shared information between different views (Sridharan and Kakade, 2008; Xu et al., 2013). However, because the background content in different views may vary, maximizing the MI across views can lead the encoder to prioritize the shared foreground information.\nIt is important to recognize that each view also contains unique task-relevant information that is specific to that view, which we refer to as modality-specific, task-relevant information. In other words, while shared information is essential, unique discriminative features in each view can also contribute significantly to the performance of downstream tasks. As illustrated in Fig. 2, solely focusing on modality-shared information may be insufficient. Including modality-specific task-relevant information can improve the general discriminative power of the learned representations.\nTo formalize these concepts, we define several types of information relevant to cross-modal representation learning. For the inputs $X^{P}$ and $X^{I}$:\n*   $H(X^{P})$ represents the entropy of $X^{P}$.\n*   $I(X^{P}; X^{I})$ denotes the mutual information between $X^{P}$ and $X^{I}$, which we term as modality-shared information.\n*   $I(X^{P};Y|X^{I})$ and $I(X^{I};Y|X^{P})$ represent the task-relevant information that is unique to the point cloud and image inputs, respectively; we term this modality-specific information. Here, Y denotes the downstream task-relevant information.\nIt's worth noting that in self-supervised learning (SSL), direct access to task-relevant information is unavailable. However, we hypothesize that an effective discriminative representation should incorporate both modality-shared and modality-specific information relevant to potential tasks. Specifically, the information for $X^{P}$ related to Y can be decomposed as $I(X^{P};Y) = I(X^{P}; X^{I};Y) + I(X^{P};Y|X^{I})$, which captures both the shared information across views and the unique task-relevant information in the point cloud.\nAssumption 1. There exists a constant $\\epsilon_u > 0$ such that $I(X^{P}; Y|X^{I}) > \\epsilon_u$.\nThis assumption suggests that cross-modal learning involves modality-specific task-relevant information, meaning that each modality (point cloud or image) contains unique information essential to the task that is not redundant between the two. Unlike traditional multi-view redundancy assumptions (Sridharan and Kakade, 2008; Xu et al., 2013), where task-relevant information is assumed to be shared across views, this assumption allows for the existence of unique, non-overlapping information in each view.\nIn practice, it is reasonable to assume $I(X^{P};Y|X^{I}) > \\epsilon_u$, as the point cloud $X^{P}$ may contain crucial task-relevant information that the image $X^{I}$ cannot fully capture. For instance, the 3D spatial structure provided by the point cloud may be indispensable for certain tasks but is not entirely representable in a 2D image."}, {"title": "3.3 Limitations of Contrastive Learning", "content": "Current contrastive methods focus on maximizing the mutual information $I(X^{P}; X^{I})$ between the two modalities without explicitly modeling modality-specific infor-"}, {"title": "4 Proposed Method", "content": "4.1 Overview\nNotation. Let P = {$P_1, P_2, ..., P_N|P_i \\in \\mathbb{R}^3$} be a point cloud consisting of N points collected by a LiDAR sensor, and I = {$I_c|| c = 1, ..., N_{cam}$} multi-view images captured by $N_{cam}$ synchronized cameras, where $I_c\\in \\mathbb{R}^{H \\times W \\times 3}$ is a single image with height H and width W.\nAs depicted in Fig. 3, we propose a new 3D self-supervised learning method, namely CMCR (Cross-Modal Comprehensive Representation Learning), based on our deduction in Sec. 3. For decoupling two types of features, we develop heads {h_{sh}^{3D}, h_{sp}^{3D}} and {h_{sh}^{2D}, h_{sp}^{2D}} to extract the modality-shared features {$F^{3D}, F^{2D}$} and modality-specific features {$G^{3D}, G^{2D}$}, respectively. We perform contrastive learning based on the modality-shared features of point-pixel pairs. Then we perform vector quantization on these modality-shared features based on a multi-modal unified codebook module. Next, those embedding vectors and the modality-specific features are added together for masked image restoration"}, {"title": "Codebook Update via Exponential Moving Average.", "content": "The codebook entries $e_v$ are updated using an Exponential Moving Average (EMA) strategy to ensure stability and smooth updates during training. The EMA update for each codeword is computed as follows:\n$e_v^{(t)} = \\gamma e_v^{(t-1)} + \\frac{\\eta^{(t)} + \\eta^{(t)}}{\\sum_{i=1}^{\\eta_i^{2D} (t)} F_i^{2D} + \\sum_{i=1}^{\\eta_i^{3D} (t)} F_i^{3D}}$"}, {"title": "Commitment Loss.", "content": "To ensure that the learned features are effectively quantized to the appropriate codewords, we introduce a novel commitment loss. The key challenge lies in the fact that the codebook, ideally shared across modalities, often partitions into modality-specific subspaces due to the distinct nature of fine-grained representations. To address this issue and ensure"}, {"title": "Geometry Enhanced Masked Image Modeling", "content": "To effectively learn modality-specific features, we apply masked image modeling (MIM) within the image branch. Specifically, we apply a random masking strategy to the input images, obscuring certain patches to create a reconstruction task. When selecting point-pixel pairs for contrastive learning, we exclude pairs where the pixels fall within masked regions. This ensures that only unmasked, reliable features are used for contrastive alignment between the 2D and 3D modalities, resulting in more consistent cross-modal representations. The image reconstruction is performed using both modality-shared features $F^{2D}$ and modality-specific features $G^{2D}$ from the image branch, allowing the model to leverage both shared and unique information during the reconstruction process.\nAs we have aligned the 2D and 3D modality-shared features $F^{2D}$ and $F^{3D}$ through the unified codebook, we can further enhance the image reconstruction process by integrating 3D information. Specifically, in the masked regions of the image, we replace the missing modality-shared features $F^{2D}$ with the corresponding aligned features from the 3D point cloud, $F^{3D}$. This replacement leverages the structural information from the 3D modality to improve the quality of the reconstructed image, as the 3D features provide valuable spatial cues that are particularly helpful in regions where the image features are missing. Furthermore, by encouraging the image branch to leverage 3D features for reconstruction, the supervision also reinforces the learning of geometry-aware 3D representations."}, {"title": "Occupancy Estimation", "content": "We employ occupancy estimation as a type of 3D reconstruction task to drive the network toward learning modality-specific features in the 3D domain. Inspired by the occupancy estimation approach in ALSO (Boulch et al., 2023), we aim to predict the spatial occupancy around query points, where the model learns to classify these points as either occupied or empty based on the geometric structure of the environment.\nQuery Point Selection and Feature Extraction. To perform occupancy estimation, we randomly select query points within the 3D space. For each selected query point, we extract the surrounding point features from the combined representation of $F^{3D} + G^{3D}$, capturing both shared and specific details of the 3D environment. These features are then passed to an occupancy decoder, which predicts whether each query point lies in an occupied region of space or an empty region, effectively reconstructing the spatial structure around each point.\nDecoder and Loss Function. Following ALSO (Boulch et al., 2023), we design the occupancy decoder as a multi-layer perceptron (MLP) that receives the feature vector of each query point and its relative position within the neighborhood. The decoder's output is a binary classification for each query point, indicating occupancy status. We use a binary cross-entropy loss to supervise the occupancy predictions, where ground truth occupancies are derived based on sensor information and surface visibility, as described in ALSO (Boulch et al., 2023). The reconstruction loss for occupancy estimation is defined as:\n$L_{occ} = \\frac{1}{\\Omega} \\sum_{q \\in \\Omega} o_q log(\\hat{o_q}) + (1 - o_q) log(1 - \\hat{o_q})$"}, {"title": "Overall Objective Function", "content": "The overall objective function is designed to optimize multiple aspects of our model, balancing between cross-modal alignment, modality-specific learning, and reconstruction. The final loss function combines several components as follows:\n$L_{total} = L_{NCE} + L_{commit} + L_{rec} + L_{occ} + L_{orth} + L_{kl}$"}, {"title": "The effect of different codebook sizes.", "content": "The size of the codebook plays a crucial role in balancing the aggregation of modality-specific features, which directly impacts the"}, {"title": "The effect of Multi-modal Unified Codebook.", "content": "After incorporating the proposed Multi-modal Unified Codebook, we observe a significant improvement in the distribution of codewords across modalities. As shown in the T-SNE visualization in Fig. 6, without the unified codebook, the codewords are largely segregated, with red points representing codewords predominantly used by the image modality and blue points used mainly by the point cloud modality. Only a few green points indicate codewords that are jointly shared by both modalities. In contrast, with the Multi-modal Unified Codebook, the codewords are more evenly distributed, and a larger proportion of codewords (green points) are now jointly utilized by both modalities. This demonstrates the enhanced cross-modal alignment facilitated by the unified codebook, which enables the model to more effectively capture shared semantics between the 2D and 3D modalities, leading to a better fusion of information across both inputs."}, {"title": "Conclusion", "content": "In this paper, we have presented a novel framework for learning more comprehensive 3D representations. By addressing the limitations of existing methods that focus primarily on modality-shared features, we introduced a new approach that also captures modality-specific features through masked image modeling and occupancy estimation tasks. Our key contribution, the multi-modal unified codebook, enables the learning of shared embedding space across different modalities, facilitating better cross-modal alignment and representation learning. Moreover, the geometry-enhanced masked image modeling further enhances 3D representation learning by incorporating spatial structure information. Through extensive experiments, we demonstrated that our approach significantly improves upon traditional methods and outperforms existing image-to-LiDAR contrastive distillation methods in downstream tasks. Future work could explore additional task-specific adaptations and further optimizations to improve the performance of multi-modal 3D learning."}]}