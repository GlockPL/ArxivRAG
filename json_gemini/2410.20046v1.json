{"title": "DQRM: Deep Quantized Recommendation Models", "authors": ["Yang Zhou", "Zhen Dong", "Ellick Chan", "Dhiraj Kalamkar", "Diana Marculescu", "Kurt Keutzer"], "abstract": "Large-scale recommendation models are currently the dominant workload for many large In-\nternet companies. These recommenders are characterized by massive embedding tables that are\nsparsely accessed by the index for user and item features. The size of these 1TB+ tables imposes\na severe memory bottleneck for the training and inference of recommendation models. In this\nwork, we propose a novel recommendation framework that is small, powerful, and efficient to run\nand train, based on the state-of-the-art Deep Learning Recommendation Model (DLRM). The\nproposed framework makes inference more efficient on the cloud servers, explores the possibility\nof deploying powerful recommenders on smaller edge devices, and optimizes the workload of the\ncommunication overhead in distributed training under the data parallelism settings. Specifically,\nwe show that quantization-aware training (QAT) can impose a strong regularization effect to\nmitigate the severe overfitting issues suffered by DLRMs. Consequently, we achieved INT4 quan-\ntization of DLRM models without any accuracy drop. We further propose two techniques that\nimprove and accelerate the conventional QAT workload specifically for the embedding tables in\nthe recommendation models. Furthermore, to achieve efficient training, we quantize the gradi-\nents of the embedding tables into INT8 on top of the well-supported specified sparsification. We\nshow that combining gradient sparsification and quantization together significantly reduces the\namount of communication. Briefly, DQRM models with INT4 can achieve 79.07% accuracy on\nKaggle with 0.27 GB model size, and 81.21% accuracy on the Terabyte dataset with 1.57 GB,\nwhich even outperform FP32 DLRMs that have much larger model sizes (2.16 GB on Kaggle and\n12.58 on Terabyte). We open-sourced our implementation in DQRM code.", "sections": [{"title": "1 Introduction", "content": "With the widespread adoption of Internet services, personalization becomes a critical function of the services\nprovided by the current Internet giants. Every user's unique taste and preferences need to be catered\nto. With billions of internet users today, the need for recommendation models is more crucial than ever.\nAccording to [Gupta et al. (2019)], over 79% of the entire Meta's cloud ML inference cycles are spent on the\ninference of various sizes of recommendation models. By Amdahl's law, a slight boost in recommendation\nmodel efficiency can yield a massive performance boost. On the other hand, if part of the inference workload\ncan be migrated to edge devices, Internet service providers can save precious cloud resources, while the users\ncan have less of their personal data sent to the cloud environment, which strengthens their personal data\nprivacy. To achieve less costly recommenders, in this work, we propose a deep quantized recommendation\nmodel (DQRM) framework, where the models are both more efficient on the cloud environment, and are\npossible to fit on edge devices. Moreover, to address the necessity for periodic retraining in recommenders,\nthe proposed framework is optimized for model training in the cloud-distributed environment."}, {"title": "2 Previous Works", "content": "Compressing Large-scale Recommendation Models - DLRM [Naumov et al.(2019)] is a typical and\nhighly popular click-through-rate recommendation model designed and vastly deployed by Meta. Moti-\nvated by its importance, many previous works ( [Ginart et al.(2021)], [Shi et al.(2020)], [Yin et al.(2021)],\n[Desai et al.(2021)], [Wu et al.(2020)]) focus on compressing DLRM's model size. Since over 99% of the\nDLRM model size is occupied by the embedding tables, previous efforts focus on shrinking the embedding\ntables without lowering weight precision. Our work focuses on neural network quantization, which is orthog-\nonal and complementary to these works.\nPreviously, quantization [Gholami et al.(2021)] has been extensively studied on CNNs [Choi et al.(2018),\nEsser et al. (2019), Ni et al.(2020), Tailor et al.(2020), Yao et al.(2021), Xiao et al. (2023)] and Transform-\ners [Shen et al. (2019), Fan et al. (2019), Zadeh et al. (2020), Liu et al.(2023), Kim et al. (2023)] and success-\nfully applied to the Matrix Factorization and Neural Collaborative Filtering models [Kang et al. (2020),\nKo et al.(2021)]. Recently, some works have extended quantization to DLRM models but mainly focus on\nPost Training Quantization (PTQ). [Guan et al. (2019)] uses codebook quantization to quantize the em-\nbedding tables of DLRM models into INT4, while [Deng et al.(2021)] further quantizes the whole model\ninto 4-bit. Both works revealed that PTQ introduces accuracy degradation, which motivates other quanti-\nzation methods like Quantization-aware Training (QAT) to improve. Besides, low-precision training (LPT)\nof DLRM [Zhang et al. (2018), Xu et al.(2021), Li et al.(2022)] received attention. [Zhang et al.(2018)]\nand [Xu et al.(2021)] train CTR models using quantized FP16 weights, while [Li et al.(2022)] trains DCN\n[Wang et al.(2017)] using INT8 weights during low-precision training and compress models into 2-bit and\n4-bit. Different from the LPT works that achieve savings in training memory usage by allowing accuracy\ndrop, our work explores QAT on DLRM models and aims for ultra-low precision during inference without\naccuracy loss, while making QAT much more efficient for large-scale recommendation models specifically.\nEfficient Training of Recommendation Models - Training of large-scale CTR models is distributed\nin the real world, and communication bottlenecks the training time. Many previous works compress\nDLRM communication loads to speedup training time losslessly ( [Pumma and Vishnu(2021)]) or lossily\n([Gupta et al. (2021), Yang et al.(2020)]) for a higher compression ratio. [Gupta et al. (2021)] improves con-\nventional Top-k sparsification on communication during DLRM's hybrid parallelism. [Yang et al.(2020)]\nquantizes the communication to ultra-low precision with tolerable accuracy drop through a novel error com-\npensation scheme. Previously, [Renggli et al.(2018)] combines sparsification and quantization on distributed\ncommunication for CNN and ASR models. However, the merit of combining the two is not generalizable to\ndifferent model architectures. In this work, we look into these techniques' effects on different parts of DQRM\nunder the DP environment and combine specified sparsification and quantization together to further benefit\nDQRM training."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Reducing Memory of Unused Weights", "content": "DLRM-style recommenders have over 99% of their model size occupied by embedding tables, unlike other pop-\npular architectures like CNNs or transformers. Because of these giant embedding tables, previous works [Ko et al.(2021),\nGinart et al.(2021), Erdogan et al.(2024)] have shown that training large-scale recommendation models is"}, {"title": "3.2 Periodic Update to Find Quantization Scale", "content": "Naive QAT poses more challenges to DLRM quantization. During the quantization process, the first step is\nusually finding the quantization scale. Given the large embedding table size, this step is extremely costly as\nthe program needs to traverse through an enormous chunk of contiguous memory. We run a breakdown of\nQAT training time shown in Figure 1 (c) under the single-node training setting for both GPU (training on\nKaggle dataset) and CPU (training on Terabyte dataset). For reference, without quantization, backward\npropagation workload dominates total single-node GPU training time: 5.8 ms out of 8.5 ms total. When QAT\nis applied, backprop (the yellow portion) no longer dominates, as forward propagation becomes significantly\nlonger (indicated by the rest of the pillars). The primary reason for this increase in time is the operation\nof finding the scale (shown by the blue portion). Finding the scale of large embedding tables occupies more\nthan one-third of the entire training time (left pillar) for GPU on Kaggle and dominates (over 97%) the\nentire training time for training on single-node CPU on Terabyte (right pillar). The problem of large tensor\ntraversal magnifies specifically on CPUs.\nWe found that periodic updates of the quantization scale effectively eliminate the latency of founding\nscales. These updates amortize the step's overhead over hundreds or thousands of iterations without hurting\nmodel convergence. We provide empirical results in Section 4 and in-depth ablation studies in Appendix\nA.1 to show that periodic updates can even boost model accuracy."}, {"title": "3.3 Distributed Training with Gradient Quantization with only MLP Error Compensation", "content": "We package the entire modified quantized model for DLRM into the Deep Quantized Recommendation Model\n(DQRM). To make DQRM more efficient and competitive in training time compared with normal training,\nwe further optimize the communication workload that occurs in every iteration.\nWe break down multi-node distributed data parallelism training of DQRM on both GPUs and CPUs\nin Appendix B.2. We found that although PyTorch's built-in specified sparsity greatly reduces all-reduce\ncommunication message load, all-reduce gradient communication after every iteration still dominates the\ntraining time. The observation motivates us to compress communication further. We explore quantizing\nall gradients into fixed-point INT8 format on top of the applied significantly specified sparsity. However,\nnaively doing so is challenging. Ablation studies presented in Appendix A.2 show that naively quantizing the\ngradient into INT8 hurts model convergence significantly. Although [Yang et al.(2020)]'s error compensation\nscheme can prevent accuracy degradation, naively performing error compensation to all parts of the weights\ncan also be highly inefficient due to the enormous error buffer needed for the large embedding tables, as\nadding large error buffers will greatly increase the model size stored in memory during training.\nWe identify surprisingly that gradients in MLP layers are more sensitive to quantization than those\nin embedding tables while embedding tables are much more robust when specified gradients are heavily\nquantized, as shown empirically in Appendix A.2. Therefore, we choose to only compensate the MLP layers\nfor gradient quantization. We achieve reasonable accuracy degradation while compressing the communication\nmessage size by roughly 4\u00d7 on top of the already heavy and well-supported gradient sparsification added to\nembedding tables. Detailed experimental settings, results, and evaluations are presented in Section 4.3."}, {"title": "4 Experimental Results", "content": "In this section, we present results evaluating DQRM on two popular datasets for CTR: the Criteo Kaggle\nDisplay Advertising Challenge Dataset (shortened below as the Kaggle dataset) and the Criteo Terabyte\nDataset (shortened as the Terabyte dataset). We followed [Naumov et al. (2019)]'s mlperf DLRM optimal\nconfigurations for each dataset as summarized in Table 2."}, {"title": "4.1 Quantization of Embedding Tables", "content": "Embedding tables occupy 99% of DLRM, which motivates heavy model compression of embedding tables.\nWe used one Nvidia A5000 GPU to study the effect of quantization on the embedding tables. Differ-\nent bit widths for embedding table quantization are used: INT16, INT8, INT4. Unlike previous works\n[Guan et al.(2019), Deng et al.(2021)] that utilize a row-wise scheme for embedding table quantization, we\nquantize the embedding table with a per-table scheme, as it reduces the number of FP32 scales stored and\nretrieved every iteration.\nFigure 3(a) illustrates testing accuracy curves for various quantization bit widths. DLRM single precision\n(blue) overfits after the second epoch, causing a steep accuracy drop, which is a common issue in large-scale\nCTR models as recently explored in [Zhang et al.(2022)]. INT16 (orange) and INT8 (grey) follow similar"}, {"title": "4.2 Quantization of the Whole Model", "content": "DLRM models have two components: Embedding tables and MLP layers. Compared with the embedding\ntable, we observe that MLP layers are more sensitive to quantization, aligning with [Deng et al. (2021),\nZhao et al.(2022)]. Channel-wise quantization of MLP layers performs better, as it has hardly any accuracy\ndrop from the single-precision MLP, while INT4 matrix-wise MLP quantization badly converges. Additional\nablation studies are presented in Appendix A.3.\nWe evaluate DQRM INT4 on both the Kaggle and Terabyte datasets. We used the same experiment\nplatforms for the Kaggle model as in Section 4.1. However, to meet the demanding resource required by the\nTerabyte models, we use the Intel Academic Compute Environment (ACE) CPU clusters and specifically\nIntel(R) Xeon(R) Platinum 8280 CPUs for model training and inference. The quantized models are all\ntrained in DQRM for five epochs till convergence.\nFigure 3 (b), (c), and (d) display training loss and testing accuracy curves for the Terabyte dataset. In\n(b), the original model testing accuracy (blue) overfits at the 2nd epoch, while DQRM INT4 (orange) steadily\nrises and avoids overfitting. Comparing DLRM (c) and DQRM INT4 (d), the latter demonstrates better\nregularization, with a consistent decrease in loss (orange curve in (d)) and a plateau in testing accuracy\n(blue curve in (d)). In contrast, DLRM's accuracy (blue curve in (c)) and training loss (orange curve in (c))\ncrash after the second epoch.\nWe report single-node training and testing performance in Table 3. We compare DQRM INT4 against"}, {"title": "4.3 Adding Gradient Quantization on Specified Sparsity", "content": "Our experiments are based on synchronous data parallelism (DP). However, we argue that the gradient com-\npression techniques presented are directly applicable to hybrid parallelism settings as in [Naumov et al.(2019)].\nMoreover, the only difference between the data parallelism and the DLRM's hybrid parallelism is the all2all\nround of communication, and it can also be directly benefited from DQRM INT with message load shrunk\nby 8x directly. Due to PyTorch DistributedDataParallel library limitations, we customized and open-\nsourced our own Data Parallel library implementing gradient quantization before allreduce. Appendix B.1"}, {"title": "5 Conclusion", "content": "In this work, we propose a systematic quantization framework DQRM for large-scale recommendation models.\nSpecifically, we discover that the DLRM model suffers severely from the overfitting problem. We show that\nultra-low precision quantization can help overcome the strong overfitting and better utilize the training\ndataset, which eventually leads to higher test accuracy. We observe that conventional QAT is troublesome\nin training large-scale recommendation models and we propose two techniques that significantly alleviate\nthe issue. Besides, to further optimize DQRM under the distributed environment, we combine specified\nsparsification and quantization together to compress communications. Our framework is intensively evaluated\non the published dataset Kaggle and Terabyte, where we outperform the full-precision DLRM baselines while\nachieving an 8\u00d7 reduction of model size."}, {"title": "A Ablation Studies", "content": "In the ablation studies, we present extended studies on the parameters of the DQRM framework, including\ndifferent update period's impact on DQRM in Section A.1, different gradient quantization bit width's effect\non model convergence in Section A.2, different sensitivity to quantization among layers in DQRM in Section\nA.3, and the effect of QAT without retraining on DQRM. Then, we evaluate DQRM INT4 with the large\nnumber of epochs in training in Section A.4."}, {"title": "A.1 Periodic update", "content": "As shown in the methodology section, the sheer size of embedding tables renders table traversal a bottleneck\nduring the forward pass. We propose to utilize periodic updates of the quantization scale of each embedding\ntable during quantization in order to amortize the huge latency of table lookup into multiple iterations.\nAmong the options, we found that the optimal period is different for training settings with different model\nsizes and batch sizes. The experimental results are presented in Table 5."}, {"title": "A.2 Different Gradient Quantization Bit Width", "content": "In this Section, we present our findings related to the quantization of the gradients during distributed DP\ntraining for DQRM. All experiments presented in this Section assume that specified sparsity is enabled.\nFirstly, we study the effect of naive quantization of gradients on the model convergence. The experiments\nare run on a single NVIDIA M40 GPU under a simulated DP environment under the Kaggle dataset. More"}, {"title": "A.3 Quantization of Different Part of Models", "content": "In DQRM training, we observe that different layers exhibit different sensitivities to quantization. To study\nthis, we utilize four NVIDIA M40 GPUs and conduct experiments on the Kaggle dataset under the dis-\ntributed DP settings. The experiment results are presented in Table 7. Compared with quantizing embed-\nding tables, MLP layers quantization cannot benefit from the diminishing overfitting effect. Instead, when\nquantizing MLP layers in the matrix-wise fashion into INT4 and quantizing activation during QAT, the"}, {"title": "A.4 QAT without Pre-training", "content": "Previously, QAT has been used as a fine-tuning technique for the quantization of CNN [Dong et al.(2020),\nZhang et al.(2023)] and transformer models [Shang et al. (2023), Li et al.(2023)]. Usually, with pretraining\non the single-precision bit-width, the trained weights can reduce accuracy loss when fine-tuning on low-\nprecision settings. We examine such paradigms on the DLRM models. We train DLRM in QAT but only\nquantize the embedding tables into INT4 under the Kaggle dataset using four NVIDIA M40 GPUs under the\ndistributed DP settings. We compare DLRM with one epoch of pretraining in the single precision followed\nby four epochs of INT4 QAT with DLRM with INT4 QAT from scratch.\nWe plot the testing accuracy versus iteration curves in Figure 5 (a). The experiment results are presented\nin Table 8. In the diagram, the vertical dashed lines signify the boundary of each epoch. The blue curve\nis for QAT with pretraining, while the orange curve is without. After the first epoch, we can see that\nas the model transitions from single-precision to INT4 quantized data type, the blue curve drops slightly,\nwhich is expected. Further, in the mid-third epoch, QAT with pretraining (blue) reaches its peak and then\ndrops quickly afterward. QAT from scratch eventually has a slight testing accuracy edge over QAT with\npretraining. Also, in Figure 5 (b), we plot the training loss across five epochs for two different settings.\nQAT with pretraining (blue) has training loss constantly below QAT from scratch. From here, we argue that\nQAT with pretraining on DLRM models does speed up the model convergence, taking less number of epochs\nto reach its peak. However, it suffers from earlier overfitting compared with QAT from scratch. Under our\nsettings, we observe that QAT from scratch slightly gains in testing accuracy of 0.01% with 0.0001 of testing\nROC AUC score."}, {"title": "A.5 How Quantization Affects Weight Distribution", "content": "To provide more intuition of why INT4 quantization can provide stronger resilience toward overfitting. We\nlook into the weight distribution shift of normal unquantized training, the QAT in INT8 training, and the\nDQRM in INT4. We select Table 6 among the 26 tables from the DLRM Kaggle setting because Table 6 is\na medium size embedding table. We found that large tables usually have the majority of weights distributed"}, {"title": "A.7 Prior QAT Techniques on DLRM", "content": "In this section, we try to address why different prior QAT methods underachieved in the DLRM model\nsettings. In the main body, we also compare DQRM against HAWQ. Since DQRM without all the tricks\nadded additionally was built on top of the mere quantization scheme of HAWQ. Therefore, HAWQ directly\nfaces the overwhelmingly significant memory usage and long training time for the Terabyte dataset to con-\nverge. What we want to discuss here is more focused on other QAT techniques such as PACT and LSQ.\nQuantization is key in building a mapping between quantized integer values and their original real values.\nPACT introduces the learned clipping range during activation quantization, while the weight quantization\nis completely reliant on DoReFa quantization. DoReFa uses the tanh function extensively, but empirically\nthe mapping is not comparable to uniform mapping implemented by the vast majority of quantization works\nthat follow, such as LSQ or HAWQ. Shown in Figure 8 (b), it converges the fastest among all techniques and\nsuffers from overfitting severely. LSQ on the other hand learns its clipping range during weight quantization.\nHowever, it is not as competitive as DQRM which uses static min max clipping range on both datasets. A\npotential explanation is that in A.5 of the supplemental materials, we present the weight distribution shift of\nthe DLRM model during training. The range of values is spreading out constantly throughout, unfortunately\nfor LSQ, the optimal clipping range might happen to be a moving objective, which hurts its performance.\nDQRM's static min-max clipping range might benefit from its stronger ability to adapt."}, {"title": "B Experiment Platforms", "content": null}, {"title": "B.1 Distributed Environment with Gradient Quantization", "content": "In the PyTorch DistributedDataParallelism library, during backward propagation, the error tensor's back-\nward() function handles both backpropagating gradients to local parameters together with the allreduce of\ngradient tensor across nodes. Therefore, after the powerful single line of code, gradients behind each pa-\nrameter tensor in the distributed data parallelism model have their corresponding gradient ready. However,\nthe quantization of the gradient has to happen between the local backpropagation and the global allreduce.\nTherefore, the distributed data parallelism package is not very handy.\nTo implement gradient quantization, we implement a stand-alone framework that performs the majority\nof distributed data parallelism workflow. To implement gradient quantization, we summarized our implemen-\ntation in Figure 9. We start by partitioning the training batch into pieces to be used by different machines.\nAfter forward propagation, the local devices are asked to backpropagate the corresponding gradient locally.\nThen, the following sequence of steps is performed for all parameters in the model. The quantization scale\nof parameter tensors is computed locally at first. The quantization scale is completely determined by the"}, {"title": "B.2 Additional Analysis on DLRM Distributed Training", "content": "Firstly, we provide more details on the experiment setup for Figure 1(c). The experiments are run on both\na single GPU and a single CPU node. We train the model under the Kaggle dataset settings on a single\nNvidia A5000 GPU, as shown in the left pillar. We also run experiments of the Terabyte dataset on a single\nIntel(R) Xeon(R) Platinum 8280 CPU node. We found that such a problem significantly magnifies on the\nCPUs. Finding the quantization scale for large DLRM models under the Terabyte dataset settings can take\nmore than 1000 ms per iteration of QAT while occupying the majority of the training time. The Terabyte\ndataset has embedding tables that are ten times larger than those on the Kaggle dataset."}, {"title": "B.3 Simulated Data Parallelism Environment", "content": "For some experiments presented in the main content, we use a simulated environment to run Data Paral-\nlelism on single device to imitate the multiple device environment. Here we provide more details of our\nimplementation. In our implementation, we add an addition gradient buffer for every parameter tensor. As"}, {"title": "B.4 Simulated Framework Evaluation", "content": "We also evaluate the effect of different node counts on gradient quantization. The result is listed in Table\n10. Currently, different node counts are simulated on the single CPU node. Across three different node\ncounts, 2, 4, and 8, the drop in training loss, testing accuracy, and ROC AUC score is consistent and small."}, {"title": "C Demo Recommender on the Phone", "content": "We include a demo of DQRM exported to an Android phone. From Figure 11, the tested DQRM model size\nis 405.65 MB. As a reference, the DLRM Kaggle model size is 2.16 GB. The model size is not strictly 8\u00d7\ncompression because of the following two reasons: 1) Embedding tables can be quantized into INT4, but\nthe embedding vectors have to be bit-packed together into INT8 format to fully benefit from the INT4 low"}]}