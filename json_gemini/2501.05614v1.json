{"title": "Watermarking Graph Neural Networks via Explanations for Ownership Protection", "authors": ["Jane Downer", "Ren Wang", "Binghui Wang"], "abstract": "Graph Neural Networks (GNNs) are the mainstream method to learn pervasive graph data and are widely deployed in industry, making their intellectual property valuable. However, protecting GNNs from unauthorized use remains a challenge. Watermarking, which embeds ownership information into a model, is a potential solution. However, existing watermarking methods have two key limitations: First, almost all of them focus on non-graph data, with watermarking GNNs for complex graph data largely unexplored. Second, the de facto backdoor-based watermarking methods pollute training data and induce ownership ambiguity through intentional misclassification. Our explanation-based watermarking inherits the strengths of backdoor-based methods (e.g., robust to watermark removal attacks), but avoids data pollution and eliminates intentional misclassification. In particular, our method learns to embed the watermark in GNN explanations such that this unique watermark is statistically distinct from other potential solutions, and ownership claims must show statistical significance to be verified. We theoretically prove that, even with full knowledge of our method, locating the watermark is an NP-hard problem. Empirically, our method manifests robustness to removal attacks like fine-tuning and pruning. By addressing these challenges, our approach marks a significant advancement in protecting GNN intellectual property.", "sections": [{"title": "1. Introduction", "content": "Graph Neural Networks (GNNs) (Scarselli et al., 2008; Kipf & Welling, 2017; Hamilton et al., 2018; Veli\u010dkovi\u0107 et al., 2018) are widely used for tasks involving pervasive graph-structured data, such as social network analysis, bioinformatics, and recommendation systems (Zhang et al., 2021; Zhou et al., 2020). Various giant companies have integrated GNNs into their systems or open-sourced their GNN frameworks: Amazon uses GNNs to analyze user behavior patterns for product recommendation (Virinchi, 2022); Google develops TensorflowGNN (Sibon Li et al., 2021) for real-time traffic prediction in Google Maps (Oliver Lange, 2020); Meta uses GNNs to improve friend and content recommendations on Facebook and Instagram (MetaAI, 2023); and Alibaba open-sources the AliGraph (Yang, 2019) platform and uses GNNs for fraud detection (Liu et al., 2021b) and risk prediction (Li, 2019). Given these companies' huge investment in labor, time, and resources to develop and deploy GNNs, it is crucial for them to be able to verify the ownership of their own models to protect against illegal copying, model theft, and malicious distribution.\nWatermarking embeds a secret pattern into a model (Uchida et al., 2017) to verify ownership if the model is stolen or misused. Backdoor-based watermarking is the de facto approach, especially for non-graph data (Adi et al., 2018; Bansal et al., 2022; Lv et al., 2023; Yan et al., 2023; Li et al., 2022; Shao et al., 2022; Lansari et al., 2023). A backdoor trigger is inserted as the watermark pattern into some clean samples with a target label different from the true label, and the model is trained on both the watermarked and clean samples. During verification, ownership is proven by demonstrating that samples with the backdoor trigger consistently produce the target label. Backdoor-based watermarking methods have several merits: they are robust to removal attacks such as model pruning and fine-tuning, and ownership verification only requires black-box access to the target model.\nHowever, recent works (Yan et al., 2023; Liu et al., 2024) show backdoor-based watermarking methods \u2013 which are primarily developed for non-graph data \u2013 have a fundamental limitation: they induce ownership ambiguity, as attackers could falsely claim misclassified data as ownership evidence. Note that this security issue also exists in the few backdoor-based watermarking methods designed for graph data (Xu et al., 2023). Further, they purposely manipulate the normal model training with polluted data samples, which could cause security issues like data poisoning attacks."}, {"title": "2. Related Work", "content": "Watermarking techniques can be generally grouped into white-box and black-box methods.\nWhite-Box Watermarking. This type of watermarking technique (Darvish Rouhani et al., 2019; Uchida et al., 2017; Wang & Kerschbaum, 2020; Shafieinejad et al., 2021) directly embeds watermarks into the model parameters or features during training. For example, (Uchida et al., 2017) propose embedding a watermark in the target model via a regularization term, while (Darvish Rouhani et al., 2019) proposed embedding the watermark into the activation/feature maps. Although these methods are robust in theory (Chen et al., 2022), they require full access to the model parameters during verification, which may not be feasible in real-world scenarios, especially for deployed models operating in black-box environments (e.g., APIs).\nBlack-Box Watermarking. Black-box approaches verify model ownership using only model predictions (Adi et al., 2018; Chen et al., 2018; Szyller et al., 2021; Le Merrer et al., 2019). They often use backdoor-based methods, training models to output specific predictions for \"trigger\" inputs as ownership evidence (Adi et al., 2018; Zhang et al., 2018). These methods have significant downsides. First, purposeful data pollution and model manipulation can cause security issues like data poisoning attacks (Steinhardt et al., 2017; Zhang et al., 2019). Further, backdoor-based methods suffer from ambiguity \u2013 since they rely on misclassification, attackers may claim naturally-misclassified samples as their own \"watermark\" (Yan et al., 2023; Liu et al., 2024). Noting these issues with backdoor-based methods, (Shao et al., 2024) proposed using explanations as the embedding space for DNN watermarks, avoiding prediction manipulation, eliminating data pollution, and maintaining black-box compatibility.\nWatermarking GNNs. Graphs vary widely in size and structure, making it difficult to embed a watermark that can be applied uniformly across different graphs. Moreover, GNNs' multi-hop message-passing mechanisms are more sensitive to data changes than neural networks processing more uniform data like images or text (Wang & Gong, 2019; Z\u00fcgner et al., 2020; Zhou et al., 2023). The only existing black-box method for watermarking GNNs (Xu et al., 2023) is backdoor-based, and suffers from the same data pollution and ownership ambiguity issues as backdoor watermarking of non-graph models (Liu et al., 2024). These issues, coupled with the complexity of graphs, make existing watermarking techniques unsuitable for GNNs. This highlights the need for novel watermarking approaches."}, {"title": "3. Background and Problem Formulation", "content": null}, {"title": "3.1. GNNs for Node Classification", "content": "Let a graph be denoted as G = (V, &, X), where V is the set of nodes, & is the set of edges, and X = [x1,\uff65\uff65\uff65,XN] \u2208 RN\u00d7F is the node feature matrix. N = |V| is the number of nodes, F is the number of features per node, and xu \u2208 RF is the node u's feature vector. We assume the task of interest is node classification. In this context, each node \u03bd\u2208 V has a label y, from a label set C = {1,2,\u2026, C}, and we have a set of |Vtr| labeled nodes (Vtr,ytr) = {(vir, y)}ue vtr_C V \u00d7 C nodes as the training set. A GNN for node classification takes as input the graph G and training nodes Vtr, and learns a node classifier, denoted as f, that predicts the label \u0177, for each node v. Suppose a GNN has L layers and a node v's representation in the l-th layer is h(l)v, where h(0)v = xv. Then it updates h(l)v for each node v using the following two operations:\n$h^{(l)}_v = Agg(\\{h^{(l-1)}_u : u \\in N(v)\\}), h^{(l)}_v = Comb(h^{(l-1)}_v, a^{(l)}_v)$,\nwhere Agg iteratively aggregates representations of a node's neighbors, and Comb uses the representation of that aggregation to update the node representation. N(v) denotes the neighbors of v. Different GNNs use different Agg and Comb operations."}, {"title": "3.2. GNN Explanation", "content": "GNN explanations reveal how a GNN makes decisions by identifying graph features that most influence the prediction. Some methods (e.g., GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al., 2020)) identify important subgraphs, while others (e.g., GraphLime (Huang et al., 2023)) identify key node features. Inspired by GraphLime (Huang et al., 2023), we use Gaussian kernel matrices to capture relationships between node features and predictions: Gaussian kernel matrices are adept at capturing nonlinear dependencies and complex relationships between variables, ensuring that subtle patterns in the data are effectively represented (Yamada et al., 2012). Using these Gaussian kernel matrices, we employ a closed-form solution with ridge regression (Hoerl & Kennard, 1970), allowing us to compute feature importance in a single step.\nOur function explain(\u00b7) takes node feature matrix X and nodes' softmax scores P = [P1,\u2026\u2026,p], and produces F-dimensional feature attribution vector e. Each entry indicates the positive or negative feature influence on the GNN's predictions across all nodes.\n$e = explain(X, P) = (K^TK + \\lambda I_F)^{-1}K^T \\tilde{L}$"}, {"title": "3.3. Problem Formulation", "content": "We propose an explanation-based method for watermarking GNNs. Our approach defines a watermark pattern (w) and selects watermarked subgraphs from G. The GNN f is trained to embed the relationship between w and these subgraphs, enabling their explanations to act as verifiable ownership evidence.\nThreat Model: There are three parties: the model owner, the adversary, and the third-party model ownership verifier. Obviously, the model owner has white-box access to the target GNN model.\n\u2022 Adversary: We investigate an adversary who falsely claims ownership of the GNN model f. While we primarily assume the adversary lacks knowledge of the watermarked subgraphs in G, we also evaluate the robustness of our method under challenging scenarios where the adversary might know specific details, such as the shape or number of watermarked subgraphs. The adversary tries to undermine the watermark by (1) searching for the watermarked subgraphs (or similarly-convincing alternatives), or (2) implementing a watermark removal attack.\n\u2022 Model Ownership Verifier: Following existing backdoor-based watermarking, we use black-box ownership verification, where the verifier does not need full access to the protected model.\nObjectives: Our explanation-based watermarking method aims to achieve the below objectives:\n1. Effectiveness. Training must embed the watermark in the explanations of our selected subgraphs: their feature attribution vectors must be sufficiently aligned with vector w.\n2. Uniqueness. Aligning watermarked subgraph explanations with w must yield statistically-significant similarity between explanations that is unlikely to occur in alternate solutions.\n3. Robustness. The watermark must be robust to removal attacks like fine-tuning and pruning."}, {"title": "4. Methodology", "content": "Our watermarking method has three stages: (1) design, (2) embedding, and (3) ownership verification. Since design relies on embedding and ownership verification, we introduce stages (2) and (3) beforehand. Training f involves a dual-objective loss function balancing node classification and watermark embedding. Minimizing watermark loss reduces the misalignment between w and the explanations of f's predictions on the watermarked subgraphs, embedding the watermark. During ownership verification, explanations are tested for statistically-significant similarity due to their common alignment with w. Lastly, we detail watermark design principles, which ensure the similarity observed across our explanations is statistically-significant, unambiguous ownership evidence. Figure 1 gives an overview of our explanation-based watermarking method."}, {"title": "4.1. Watermark Embedding", "content": "Let training set Vir be split as two disjoint subsets: Vclf for node classification and Ywmk for watermarking. Select T subgraphs {Gwmk,..., Gwmk} whose nodes {Vwmk}T1 will be watermarked. These subgraphs have explanations {ewmk.....ewmk}, where emk = explain(Xwmk, pwmk) explains f's softmax output Pymk on Gwmk's nodes Vwmk, which have features Xmk. Define watermark w as an M-dimensional vector (M \u2264 F), whose entries are 1s and -1s.\nInspired by (Shao et al., 2024), we use multi-objective optimization to balance classification performance with a hinge-like watermark loss. Minimizing this loss encourages alignment between W and {ewmk }=1, embedding the relationship between w and these subgraphs.\n$L_{wmk}(\\{e^{wmk}\\}_{i=1}^T, W ,w) = - \\sum_{i=1}^T \\sum_{j=1}^{M} max(0, \\epsilon \u2013 w[j] \u00b7 e^{wmk} [idx[j]])$,\nwhere ewmk [idx] represents the watermarked portion of ewmk on node feature indices idx with length M; idx is same for all explanations {ewmk}T1 We emphasize that idx are not arbitrary, but are rather the result of design choices discussed later in Section 4.3. The hyperparameter \u03b5 bounds the contribution of each multiplied pair w[j] \u00b7 ewmk [idx[j]] to the summation.\nWe train the GNN model f to minimize both classification loss on the nodes Vclf (see Equation 2) and watermark loss on the explanations of {Gwmk,..., Gwmk}, with a balancing hyperparameter r:\n$\\min_\\Theta L_{CE} (y^{clf}, f_{\\Theta}(V^{clf})) + r \\cdot L_{wmk}(\\{e^{wmk}\\}_{i=1}^T, w)$"}, {"title": "4.2. Ownership Verification", "content": "Since they were aligned with the same w, explanations {ecdt}T1 will be similar to each other after training. Therefore, when presented with T candidate subgraphs {eqdt, edt,..., edt } by a purported owner (note that our threat model assumes a strong adversary who also knows T), we must measure the similarity between these explanations to verify ownership. If the similarity is statistically significant at a certain level, we can conclude the purported owner knows which subgraphs were watermarked during training, and therefore that they are the true owner.\nExplanation Matching: Our GNN explainer in Equation (3) produces a positive or negative score for each node feature, indicating its influence on the GNN's predictions, generalized across all nodes in the graph. To easily compare these values across candidate explanations, we first binarize them with the sign function. For the jth index of an explanation edt, this process is defined as:\n$\\hat{e}^{cdt}[j] = \\begin{cases}\n1 & \\text{if } e^{cdt}[j] > 0 \\\\\n-1 & \\text{if } e^{cdt}[j] < 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nWe then count the matching indices (MI) across all the binarized explanations the number of indices at which all binarized explanations have matching, non-zero values:\u00a7\n$MI^{cdt} = MI(\\{\\hat{e}^{cdt}\\}_{i=1}^T)$\n$= \\sum_{j=1}^F 1((\\{\\hat{e}^{cdt} [j] \\neq 0, \\forall i\\}) \\land (\\hat{e}_1^{cdt} [j] = \\dots = \\hat{e}_T^{cdt} [j]))$\nSignificance Testing to Verify Ownership: We verify the purported owner's ownership by testing if MIcdt is statistically unlikely for randomly selected subgraphs, at a significance level av:\n$Ownership = \\begin{cases}\nTrue & \\text{if } p_{ztest} < \\alpha_v \\\\\nFalse & \\text{otherwise}\n\\end{cases}$\n$Where Z_{test} = \\frac{MI^{cdt}-\\mu_{nate}}{\\sigma_{nate}}$"}, {"title": "4.3. Watermark Design", "content": "The watermark w is an M-dimensional vector with entries of 1 and -1. The size and location of w must allow us to effectively embed unique ownership evidence into our GNN.\nDesign Goal: The watermark should be designed to yield a target MI (MItgt) that passes the statistical test in Equation (8). This value is essentially the upper bound on a one-sided confidence interval. However, since we cannot obtain the estimates \u00b5nate or Onate without a trained model, we instead use a binomial distribution to predict estimates \u00b5natp and natp (note the subscript \"p\").\nWe assume the random case, where a binarized explanation includes values -1 or 1 with equal probability (again, ignoring zeros; see Footnote \u00a7). Across T binarized explanations, the probability of a match at an index is Pmatch = 2 \u00d7 0.5T. We estimate \u03bc\u03c0\u03b1\u03c4\u03c1 = F \u00d7 Pmatch (where F is number of node features), and = \u221aF \u00d7 Pmatch(1 \u2013 Pmatch). We therefore define MItgt as follows:\n$MI^{tgt} = min(\\mu_{nat_p} + \\sigma_{nat_p} \\times Z_{tgt}, F)$"}, {"title": "4.4. Locating the Watermarked Subgraphs", "content": "An adversary may attempt to locate watermarked subgraphs to claim ownership. In the worst case, they have access to Gtr and know T (number of watermarked subgraphs) and s (nodes per subgraph). With Gtr, they can compute the natural match distribution (\u00b5nate, onate) and search for T subgraphs with maximally significant MI, using either brute-force or random search.\nBrute-Force Search: If the training graph has N nodes, identifying nsub = sN-node subgraphs yields $\\binom{N}{n_{sub}}$ options. To find the T subgraphs with a maximum MI across their binarized explanations, an adversary must compare all T-sized sets of these subgraphs, with $\\binom{\\binom{N}{n_{sub}}}{T}$ sets in total.\nRandom Search: Alternatively, an adversary can randomly sample subgraphs in the hopes of finding a group that is \"good enough\". To do this, they make T random selections of an nsubsized set of nodes, each of which comprises a subgraph. Given N training nodes and T watermarked subgraphs of size nsub, the probability that an attacker-chosen subgraph of size nsub overlaps with any single watermarked subgraph with no less than j nodes is given as:\n$P(overlapping \\; nodes \\; \\geq j) = 1 - [\\sum_{m=j}^{n_{sub}} \\frac{\\binom{n_{sub}}{m} \\binom{N-n_{sub}}{n_{sub}-m}}{\\binom{N}{n_{sub}}}]^T$\nThe summation represents the probability that a randomly selected subgraph contains less than j nodes from a watermarked subgraph. Raising this to the power of T yields the probability that overlap < j for all watermarked subgraphs. Subtracting this from 1 yields the probability that the randomly selected subgraph contains at least j nodes from the same watermarked subgraph.\nIn Section 5.2.3 we demonstrate the infeasibility of both brute-force and random search."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Setup", "content": "Datasets and Training/Testing Sets: We evaluate our watermarking method on three standard datasets commonly used in node classification tasks: Amazon Photo - a subset of the Amazon co-purchase network (McAuley et al., 2015), CoAuthor CS a coauthorship network (Shchur et al., 2019), and PubMed a citation network (Yang et al., 2016). (See Appendix A.1 for more details.)\nThe graph is split into three sets: 60% nodes for training, 20% for testing, and the remaining 20% for further training tasks, such as fine-tuning or other robustness evaluations. As mentioned in Section 4.1, training nodes are further split into two disjoint sets: one for training the GNN classifier, and one consisting of the watermarked subgraphs. (Their relative sizes are determined by the size and number of watermarked subgraphs, which are hyperparameters mentioned below.) The test set is used to evaluate classification performance after training. The remaining set enables additional training of the pre-trained GNN on unseen data to assess watermark robustness.\nGNN Models and Hyperparameters: We apply our watermarking method to three GNN models: GCN (Kipf & Welling, 2017), SGC (Wu et al., 2019), and GraphSAGE (Hamilton et al., 2018). Our main results use the GraphSAGE architecture by default. Unless otherwise specified, we use T = 4 watermarked subgraphs, each with the size s = 0.5% of the training nodes.\nKey hyperparameters in our watermarking method, including the significance levels (atgt and av), balanced hyperparameter (r), and watermark loss contribution bound (e), were tuned to balance classification and watermark losses. A list of all hyperparameter values are in the Appendix. Note that our watermark design in Equation (11) allows us to learn the watermark length M."}, {"title": "5.2. Results", "content": "As stated in Section 3.3, watermarks should be effective, unique, robust, and undetectable. Our experiments aim to assess each of these. (For more results see Appendix.)"}, {"title": "5.2.1. EFFECTIVENESS AND UNIQUENESS", "content": "Embedding effectiveness can be measured by the alignment of the binarized explanations with the watermark pattern w at indices idx; this metric can be used by the owner to confirm that w was effectively embedded in f during training. Since the entries of w are 1s and -1s, we simply count the average number of watermarked indices at which a binarized explanation matches w:\n$Watermark \\; Alignment = (1/T) \\times \\sum_{i=1}^T\\sum_{j=1}^{M}1(\\hat{e}^{wmk} [idx[j]] = w[j])$,\nWatermarking uniqueness is measured by the MI p-value for the binarized explanations of the T watermarked subgraphs, as defined by Equation (8). A low p-value indicates the MI is statistically unlikely to be seen in explanations of randomly selected subgraphs. This metric is more important than watermark alignment; if the watermarked subgraphs yield a uniquely large MI, it is sufficient, even if alignment is under 100%.\nTable 1 shows results under default settings, averaged over five trials with distinct random seeds and watermark patterns. The key result is the MI p-value, which is below 0.001 for all T > 2, even when watermark alignment is below 100%; this shows uniqueness of the ownership claim, meaning the embedding was sufficiently effective. Accuracy is high across datasets and models, with minimal impact from watermarking."}, {"title": "5.2.2. ROBUSTNESS", "content": "A good watermark will be robust to removal attacks. We explore two types of attacks. The first is pruning for model compression (Li et al., 2016), as used to assess watermark robustness by (Liu et al., 2021a), (Tekgul et al., 2021), and others. The second is fine-tuning (Pan & Yang, 2010), as used for robustness assessment by (Adi et al., 2018), (Wang et al., 2020), and more.\nPruning: Pruning is a model compression strategy that sets a portion of weights to zero (Li et al., 2016). The particular approach we explore, structured pruning, targets rows and columns of paramater tensors such as node embeddings and edge features based on their importance scores, or Ln-norms (Paszke et al., 2019). By pruning the model, attackers hope to remove the watermark while still maintaining high classification accuracy.\nFine-Tuning: Fine-tuning continues training on already-trained models to adapt to a new task (Pan & Yang, 2010). An attacker may use fine-tuning to get the GNN to \"forget\" the watermark. To test our method's robustness to this type of attack, we continue training models on the validation dataset, Gval, at 0.1 times the original learning rate for 49 epochs."}, {"title": "5.2.3. UNDETECTABILITY", "content": "Brute-Force Search: With Equations from Section 4.4, we use our smallest datset, Amazon Photo (4590 training nodes), to demonstrate the infeasibility of a brute-force search for the watermarked subgraphs. We assume adversaries know the number (T) and size (s) of our watermarked subgraphs. With default s = 0.005, each watermarked subgraph has ceil(0.005 \u00d7 4590) = 23 nodes - there are $\\binom{4590}{23}$ = 6.1 \u00d7 1061 subgraphs of this size; with default T = 4, there are $\\binom{\\binom{4590}{23}}{4}$ = 5.8 \u00d7 10245 possible Tsized sets of candidate subgraphs. Therefore, even in our smallest dataset, finding the uniquely-convincing set of watermarked subgraphs is an incredibly hard problem.\nRandom Search: Figure 3 shows the probability (from Equation 13), for varied subgraph sizes s, that j nodes of a randomlychosen subgraph overlap with any single watermarked subgraph. The figure plots these values for j = 1, 2, 3, 4, 5, or all nsub watermarked subgraph nodes and our default T = 4 watermarked subgraphs. For our default subgraph size of s = 0.005 (or equivalently, 0.5% of the training nodes), there is close to 0 probability that a randomly-selected subgraph contains 3 or more nodes that overlap with a common watermarked subgraph. This demonstrates very low probability that a randomly-selected subgraph will be similar to the actual watermarked subgraphs."}, {"title": "5.3. Ablation Studies", "content": "We now explore the role of (1) watermarked subgraph size and (2) the number of watermarked subgraphs on the effectiveness, uniqueness, and robustness of the watermark.\nImpact of the Number of Watermarked Subgraphs T: Figure 4 shows how the number of watermarked subgraphs, T, affects various watermark performance metrics. The results show that for all datasets, larger T increases watermark alignment and a lower p-value, although test accuracy decreases slightly for Photo and PubMed datasets. Notably, our default of T = 4 is associated with a near-zero p-value in every scenario. Figure 10 in Appendix also shows the robustness results to removal attacks against varied T: we observe that the watermarking method resists pruning attacks until test accuracy is affected, and fine-tuning attacks for at least 25 epochs for any dataset.\nImpact of the Size of Watermarked Subgraphs s: Figure 5 shows the results with different sizes s of the watermarked subgraphs. We observe similar trends as Figure 4: watermarking is generally more effective, unique, and robust for larger values of s. Again, we observe a trade-off between subgraph size and test accuracy, though this trend is slight. We note that for s \u2265 0.003, our method achieves near-zero p-values across all datasets, as well as increasing watermark alignment. Figure 11 in Appendix shows the robustness results: across all datasets, when s > 0.005, our method is robust against pruning attacks generally, and against fine-tuning attacks for at least 25 epochs.\nExtension to Other Graph Learning Tasks"}, {"title": "6. Conclusion", "content": "In this paper, we introduce the first-known method for watermarking GNNs via their explanations. This avoids common pitfalls of backdoor-based methods: our watermark is statistically guaranteed to be unambiguous, and since it does not reside within the training data space, it is not vulnerable to attacks on the data itself. We show the robustness of our method to removal attacks and highlight the statistical infeasibility of locating the watermarked subgraphs. This presents a significant step forward in securing GNNs against intellectual property theft."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Appendix", "content": null}, {"title": "A.1. Experimental Setup Details", "content": "Hardware and Software Specifications. All experiments were conducted on a MacBook Pro (Model Identifier: MacBookPro18,3; Model Number: MKGR3LL/A) with an Apple M1 Pro chip (8 cores: 6 performance, 2 efficiency) and 16 GB of memory, on macOS Sonoma Version 14.5. Models were implemented in Python with the PyTorch framework.\nDataset Details. Amazon Photo (simply \"Photo\" in this paper) is a subset of the Amazon co-purchase network (McAuley et al., 2015). Nodes are products, edges connect items often purchased together, node features are bag-of-words product reviews, and class labels are product categories. Photo has 7,650 nodes, 238,163 edges, 745 node features, and 8 classes. The CoAuthor CS dataset (\"CS\" in this paper) (Shchur et al., 2019) is a graph whose nodes are authors, edges are coauthorship, node features are keywords, and class labels are the most active fields of study by those authors. CS has 18,333 nodes, 163,788 edges, 6,805 node features, and 15 classes. Lastly, PubMed (Yang et al., 2016) is a citation network whose nodes are documents, edges are citation links, node features are TF-IDF weighted word vectors based on the abstracts of the papers, and class labels are research fields. The graph has 19,717 nodes, 88,648 edges, 500 features, and 3 classes."}, {"title": "Hyperparameter Setting Details.", "content": "Classification training hyperparameters:\n\u2022 Learning rate: 0.001-0.001\n\u2022 Number of layers: 3\n\u2022 Hidden Dimensions: 256-512\n\u2022 Epochs: 100-300\nWatermarking hyperparameters:\n\u2022 Target significance level, atgt: set to le-5 to ensure a watermark size that is sufficiently large.\n\u2022 Verification significance level, av: set to 0.01 to limit false verifications to under 1% likelihood.\n\u2022 Watermark loss coefficient, r: set to values between 20-100, depending on the amount required to bring Lwmk to a similar scale as Lelf to ensure balanced learning.\n\u2022 Watermark loss parameter e: set to values ranging from 0.01 to 0.1. Smaller values ensure that no watermarked node feature index has undue influence on watermark loss."}, {"title": "A.2. Gaussian Kernel Matrices", "content": "Define K as a collection of matrices {K(1), ...,\u012aK(F)}, where \u012b\u0154(k) (size N \u00d7 N) is the centered and normalized version of Gaussian kernel matrix K(k), and each element K(k) is the output of the Gaussian kernel function on the kth node feature for nodes u and v:\n$\\tilde{K}^{(k)} = HK^{(k)}H/\\|HK^{(k)}H\\|_F, H = I_N - \\frac{1}{N}11^T, K_{uv}^{(k)} = exp(-\\frac{1}{2\\sigma_k^2}(x_u^{(k)} - x_v^{(k)})^2)$"}, {"title": "A.3. Time Complexity Analysis", "content": "The training process involves optimizing for node classification and embedding the watermark. To obtain total complexity, we therefore need to consider two processes: forward passes with the GNN, and explaining the watermarked subgraphs.\nGNN Forward Pass Complexity. The complexity of standard node classification in GNNs comes from two main processes: message passing across edges (O(EF), where E is number of edges and F is number of node features), and weight multiplication for feature transformation (O(NF2), where N is number of nodes). For L layers, the time complexity of a forward pass is therefore:\n$O(L(EF + NF^2))$\nExplanation Complexity. Consider the Formula 3 for computing the explanation: e = explain(X, P) = (KTK+IF)\u00af\u00b9\u00d1\u00b9\u017d. Remember that K is an N\u00b2 \u00d7 F matrix, IF is a F \u00d7 F matrix, and L is a N2 \u00d7 1 vector. To compute the complexity of this computation, we need the complexity of each subsequent order of operations:\n1. Multiplying KK (an O(F2N2) operation, resulting in an F \u00d7 F matrix)\n2. Obtaining and adding IF (an O(F2) operation, resulting in an F \u00d7 F matrix)\n3. Inverting the result (an O(F3) operation, resulting in an F \u00d7 F matrix)\n4. Multiplying by KT (an O(F2N2) operation, resulting in an F \u00d7 N2 matrix)\n5. Multiplying the result by L (an O(F2N2) operation, resulting in an N2 \u00d7 1 vector)\nThe total complexity of a single explanation is therefore O(F2N2) + O(F2) + O(F3) + O(F2N2) + O(F2N2) = O(F2N2 + F3). For obtaining explanations of T subgraphs during a given epoch of watermark embedding, the complexity is therefore:\n$O(T(F^2N^2 + F^3))$\nTotal Complexity. The total time complexity over i epochs is therefore:\n$O (i\\times (L(EF + NF^2) + T(F^2N^2 + F^3)))$"}, {"title": "A.4. Normality of Matching Indices Distribution", "content": "Our results rely on the z-test to demonstrate the significance of the MI metric. To confirm that this test is appropriate, we need to demonstrate that the MI values follow a normal distribution. Table 3 shows the results of applying the Shapiro-Wilk (Ghasemi & Zahediasl, 2012) normality test to"}]}