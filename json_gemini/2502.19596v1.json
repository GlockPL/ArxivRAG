{"title": "Trustworthy Answers, Messier Data: Bridging the Gap in Low-Resource Retrieval-Augmented Generation for Domain Expert Systems", "authors": ["Nayoung Choi", "Grace Byun", "Andrew Chung", "Ellie S. Paek", "Shinsun Lee", "Jinho D. Choi"], "abstract": "RAG has become a key technique for enhancing LLMs by reducing hallucinations, especially in domain expert systems where LLMs may lack sufficient inherent knowledge. However, developing these systems in low-resource settings introduces several challenges: (1) handling heterogeneous data sources, (2) optimizing retrieval phase for trustworthy answers, and (3) evaluating generated answers across diverse aspects. To address these, we introduce a data generation pipeline that transforms raw multi-modal data into structured corpus and Q&A pairs, an advanced re-ranking phase improving retrieval precision, and a reference matching algorithm enhancing answer traceability. Applied to the automotive engineering domain, our system improves factual correctness (+1.94), informativeness (+1.16), and helpfulness (+1.67) over a non-RAG baseline, based on a 1-5 scale by an LLM judge. These results highlight the effectiveness of our approach across distinct aspects, with strong answer grounding and transparency.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) has shown potential in reducing hallucinations and providing up-to-date knowledge in Large Language Models (LLMs). This success has grown interest in domain expert RAG-Question Answering (QA) systems to meet specialized knowledge needs. While previous studies (Han et al. 2024; Siriward-hana et al. 2023; Mao et al. 2024) have proposed general methods for adapting RAG models to domain knowledge bases such as syntactic QA pair generation or model fine-tuning\u2014they face several challenges in low-resource domains.\nIn practical settings, available data sources in low-resource domains are often presented in heterogeneous formats and exhibit an unstructured nature, making their direct integration into RAG system development challenging (Hong et al., 2024). General models may not have enough inherent knowledge in low-resource domains (Zhao et al., 2024), making fine-tuning essential to adapt the model to specific knowledge requirements. However, the lack of structured data for training further complicates this process. In addition, data privacy and security concerns restrict the full utilization of API-based LLMs (Achiam et al. 2023; Anthropic 2024) within RAG systems, necessitating the use of open-source LLMs (Yang et al. 2024; Touvron et al. 2023).\nThe retrieval phase is another key aspect of domain expert RAG systems, as referencing accurate documents is essential for generating reliable answers. However, research on improving ranking in the retrieval phase or tracing the documents referenced to generate the answer remains limited. Most domain expert RAG frameworks rely on a single-stage retrieval process, with few exploring multi-stage approaches (Nogueira et al. 2019; Nogueira et al. 2020; Karpukhin et al. 2020), such as retrieval followed by re-ranking\u2014a method widely used in Information Retrieval (IR)\u2014which can help ensure the use of the most relevant references.\nThe evaluation of RAG systems is also an area that has not been fully addressed. Many studies continue to rely on gold answer similarity metrics, such as overlapping words between the generated answer and the ground truth, which inadequately capture critical dimensions like faithfulness, coherence, and contextual relevance. Recently, the LLM-as-a-judge framework (Zheng et al., 2023) has gained attention as a qualitative alternative. However, these methods often overlook diverse evaluation aspects, and a standardized framework for assessing RAG systems has yet to emerge.\nIn this work, we address three key challenges and present the RAG development pipeline, demonstrating its application in the automotive engineering domain, with a specific focus on QA for vehicle"}, {"title": "2 Related Work", "content": "Data Processing RAG-Studio (Mao et al. 2024) employs synthetic data generation for in-domain adaptation, reducing reliance on costly human-labeled datasets. However, it assumes access to well-structured data, limiting its applicability in scenarios with unstructured raw data. To bridge this gap, Hong et al. (2024) tackle challenges with real-world formats (e.g., DOC, HWP), proposing a chunking method that converts documents to HTML, retaining structural information in low-resource settings. Meanwhile, Guan et al. (2024) address the issue of short and noisy e-commerce datasets in RAG system development by building a new dataset from a raw corpus crawled from an e-commerce website, providing a richer resource.\nRetrieval in RAG Wang et al. (2024) highlight the importance of re-ranking modules in RAG systems to enhance the relevance of the retrieved documents. Similarly, Zhao et al. (2024) demonstrate that the ranking position of the gold document in the retrieval phase plays a significant role in determining the quality of the final answer. Given these insights, optimizing the retrieval phase is crucial for obtaining accurate context, particularly in specialized, low-resource domains where LLMs lack sufficient inherent knowledge (Beauchemin et al., 2024). Despite these findings, the analysis of the effectiveness and applicability of re-rankers in domain expert RAG systems remains underexplored.\nRAG Evaluation The evaluation of RAG systems (Yu et al., 2024) has relied on text similarity-based metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and EM (Rajpurkar et al., 2016). These metrics provide a baseline but overlook valid answer diversity and qualitative aspects like factual consistency and relevance. Recent advancements are made to utilize LLM-as-a-judge (Zheng et al., 2023) to evaluate qualitative dimensions. Han et al. (2024) propose a pairwise preference comparison framework considering helpfulness and truthfulness, while Saad-Falcon et al. (2024) assess context relevance, answer faithfulness, and query relevance, addressing hallucination"}, {"title": "3 Approach", "content": "issues. However, a standardized framework for RAG evaluation remains a challenge."}, {"title": "3.1 Data Generation & Processing", "content": "We build our dataset using two distinct sources: (1) internal reports on vehicle crash collision tests in presentation slide format from an automotive company in Korea, and (2) the textbook Crash Safety of Passenger Vehicles (Mizuno, 2016). Vehicle crash collision tests are exceptionally valuable, as each test incurs substantial costs and produces detailed reports that are typically confidential and difficult to access publicly. Figure 2 illustrates the process of extracting, analyzing, and converting slides and textbook PDFs into structured Markdown text and Q&A sets, leveraging Python scripts and the Claude LLM (Anthropic, 2024). Table 1 summarizes the data statistics by source.\nThe original slides often contain tables, graphs, and images, which are simplified into plain text descriptions during data processing. To evaluate performance on questions referencing these elements, we sample 143 multi-modal chunks and generated 1,861 targeted questions. Evaluation results are provided in Section 4.3.1."}, {"title": "3.2 Retrieval & Ranking", "content": "We employ a Dual-Encoder as our retriever, fine-tuning it on our training data. For a question q, the model retrieves the top n chunks from m chunks D, based on the [CLS] token embedding similarity between q and each chunk \n\\(d_i \\in D\\), as follows:\n\\(q_{\\[CLS]} \\in \\mathbb{R}^{1\\times d}; D_{\\[CLS]} \\in \\mathbb{R}^{m\\times d};\\)\n\\(\text{Similarity}(q, D) = q_{\\[CLS]} \\cdot D_{\\[CLS]} \\in \\mathbb{R}^{1\\times m}\\)\n\\(\text{D}_{\text{top}} = \text{Sort} (D, \\text{key} = \\text{Similarity}(q, D)) \\[: n]\\)\nOur re-ranker is a point-wise Cross-Encoder, trained on a classification task (Nogueira et al., 2019) that takes a single (q, \n\\(d_i\\)) pair as input and returns a scalar relevance score. It re-ranks \\(\text{D}_{\text{top}_n}\\), obtained from the retrieval stage, to extract \\(\text{D}_{\text{top}_k}\\), which will be passed to the generation model. The process is formalized as follows:\n\\(\text{D}_{\text{top}_n} = \\[d_1,d_2,..., d_n]\\)\n\\(x_i = \"{q}\" {sep\\_token} {d_i}\" \\quad \\forall d_i \\in \\text{D}_{\text{top}_n}\\)\n\\(\text{Rel}_{x_i} = \\text{Ranker}(x_i) \\quad \\forall i\\in {1,2,..., n}\\)\n\\(\text{D}_{\text{top}_k} = \\text{Sort}(\\text{D}_{\text{top}_n}, \\text{key} = \\text{Rel}_{x_i})[: k]\\)\nTo optimize \\(\text{Rel}_{x_i}\\), we apply Token Selection (TS) + Term Control Layer (TCL) training method from RRADistill (Choi et al., 2024). It effectively integrates the importance of the general semantic and specific query terms during the training process.\n\\(\\mathcal{L} =\n\\sum_{i=1}^{n} (\\alpha\\cdot L(\\text{Rel}_{x_i}, y_i) + \\beta\\cdot L(\\text{Rel}_{x_i}^{TS}, y_i))\\)\nTS+TCL are used during training only, while inference uses the standard Cross-Encoder approach only with \\(\text{Rel}_{x_i}\\). The target label y is binary: y = 1 for relevant pairs and y = 0 for irrelevant pairs. Negative sampling retrieves the top 10 chunks from the train data for each query q, excluding the gold chunk, and randomly selects three from the rest."}, {"title": "3.3 Answer Generation", "content": "Our answer generation model takes the user question q and the top k chunks (\\(\text{D}_{\text{top}_k}\\)) from the retrieval and ranking phase as input to generate the final answer a', as follow:\n\\(a' = LLM(q, \\text{D}_{\text{top}_n})\\)\nWe fine-tune open-source LLMs on our training dataset, which consists of (q, a) pairs derived from our data generation pipeline (Section 3.1). During fine-tuning, the question q and the answer a are concatenated into a single sequence S = [q; a], and the model is trained in an auto-regressive manner to predict the next token."}, {"title": "3.4 Reference Matching Algorithm", "content": "We propose a reference matching algorithm that segments generated answers and links them to relevant references using a Dynamic Programming (DP) approach (Bellman, 1954). Algorithm 1 outlines the detailed procedure.\nIn Step 1, the algorithm computes the scores for all possible sentence subsequences d'segments against the top-k chunks \\(\text{D}_{\text{top}_k}\\), using our re-ranker (Section 3.2). In Step 2, it selects the optimal segment-chunk combinations, updating scores and recording the best choices for each ending sentence (e.g., if the answer consists of five sentences like a = \\[[s_1,..., s_5]\\), the choices might be \\((s_1:s_1, d_1), (s_2:s_2, d_3), (s_2:s_3, d_3), (s_1:s_4, d_1), (s_5:s_5, d_2)\\)). Finally, in Step 3, backtracking is performed to retrieve and output the best matches from the choices (e.g., \\((s_1:s_4, d_1), (s_5:s_5, d_2)\\))."}, {"title": "4 Experiment 1: Retrieval & Ranking", "content": "4.1 Models\nFor retriever training, we use BGE-M3 (Chen et al., 2024) as the backbone, a multilingual Encoder capable of processing Korean, and fine-tune it on our training dataset with the publicly available code. For re-ranker training, we initialize the weights using the fine-tuned retriever. Further experimental details are in A.1.2.\n4.2 Evaluation\nWe evaluate the retriever and re-ranker using Mean Average Precision (MAP@k) and Success@k (Manning et al., 2008). MAP@k measures the ranking quality by averaging precision over relevant results up to rank k, while Success@k indicates the proportion of queries with at least one relevant result in the top k. The high Success@k score indicates that relevant chunks are retrieved, while the improved MAP@k highlights better ranking of those retrieved chunks. The evaluation is conducted using test set questions, but the chunk pool for retrieval included all splits (training, validation, and test) to ensure sufficient data and avoid biases from the limited test set size.\n4.3 Result\nTable 2 shows that prepending a header to each chunk\u2014adding only a small number of tokens (see Table 9)\u2014significantly enhances retrieval performance. Fine-tuning the BGE-M3 model on Ver.1 (BGE-FT) further improves results notably, demonstrating the importance of task-specific model adaptation to optimize performance. Table 3 shows that the re-ranking phase notably enhances the precision of retrieved information. The results indicate that ranking performance is optimal when the number of retrieved chunks is limited to 10. Despite"}, {"title": "4.3.1 Multi-modal Specific Questions", "content": "Table 4 summarizes the performance on questions requiring information from tables, graphs, and images. Compared to general questions (Section 4.3), the results are notably lower, with the retriever facing significant challenges. Even at k = 30, Success@k only reaches 83.61%, underscoring a major bottleneck in the retrieval phase. Despite these challenges, the re-ranker proved effective, delivering a substantial performance improvement of approximately +15% in MAP@1 at k = 20. These results underscore the need for further advancements to better handle multi-modal questions."}, {"title": "4.4 Analysis", "content": "Figure 3 illustrates the trade-off between retrieval success and re-ranking performance as the number of retrieved chunks n increases. While increasing n improves the retrieval success rate (e.g., from 92%\nat n = 10 to 96% at n = 30), it can lead to diminishing improvements in re-ranking performance (e.g., MAP@5 decreases from 0.74 at n = 10 to 0.73 at n = 30). This result highlights the importance of carefully selecting n to balance retrieval success and re-ranking quality, as overly increasing n may not yield proportional benefits for re-ranking.\nAn analysis of the failed cases during the retrieval and ranking phase identified two error types: (a) Top-10 retrieval failure (23.3%) and (b) Retrieval success but incorrect re-ranker top-1 (76.7%). Type (a) were mainly caused by the open-ended nature of the questions, which led to multiple relevant documents beyond the gold context (Figure 18 in A.3.2). In contrast, type (b) occurred with more specific and detailed questions, where there were several relevant chunks from the same vehicle crash collision test, in addition to the gold chunk, making it difficult to identify a single correct one (Figure 19 in A.3.2). Failures in both types were often due to the presence of multiple valid chunks, rather than the ranking of irrelevant chunks. Table 4 presents a human evaluation of 100 sampled cases for each error type, assessing whether the re-ranker top-1 was relevant to the given question, even though it did not match the gold chunk. A screenshot of the human evaluation interface is in Figure 21 (A.5)."}, {"title": "5 Experiment 2: Answer Generation", "content": "5.1 Models\nWe use Qwen-2.5 (Yang et al., 2024) as our backbone LLMs, one of the few multilingual models that officially supports Korean. Notably, there are currently no Korean-centric open-source LLMs with sufficient context length to address our requirements. We fine-tune Qwen 2.5 (14B, 72B) on our training dataset, performing full fine-tuning on the 14B model and applying Low-Rank Adaptation (LORA) (Hu et al., 2021) for the 72B model. Experimental details including hyperparameters and GPU configurations, are in A.1.2.\n5.2 Evaluation\n5.2.1 LLM-as-a-Judge\nAs illustrated in Figure 5, we employ the LLM-as-a-judge approach (Zheng et al., 2023) to evaluate the performance of both LLM-only and RAG models. For the LLM evaluation, we compare four model variations: the vanilla and fine-tuned versions of 14B and 72B models. GPT-40 is used to rank anonymized models (A, B, C, and D) based on factual correctness, helpfulness, and informativeness. These metrics are chosen to evaluate distinct, non-overlapping dimensions:\n* Correctness measures alignment with the gold answer, rewarding correct responses without hallucinations.\n* Helpfulness assesses clarity and relevance in addressing key points, while avoiding unnecessary content.\n* Informativeness evaluates the inclusion of relevant details or additional context that enhances completeness.\nFor RAG evaluation, we use the best-performing one among four models and compare its performance with and without RAG integration. Instead of pairwise comparisons, we employ single-answer evaluations, scoring responses (1\u20135) on three aspects. This approach offers a more detailed assessment, emphasizing how RAG impacts specific aspects of response quality. All the evaluation prompts used are detailed in A.2.2.\n5.2.2 Human Evaluation\nTo assess the reliability of the RAG evaluation using the LLM-as-a-judge method, we conducted a human evaluation to compare its alignment with the LLM evaluation. An expert evaluator, a native Korean automotive engineer, assessed 100 randomly selected and anonymized responses. These responses were generated by the 72B fine-tuned model, both with and without RAG integration. A screenshot of the human evaluation interface used is in shown Figure 22 (A.5)."}, {"title": "5.3 Result", "content": "5.3.1 LLM-only Comparison\nFigure 6 illustrates the rank distribution and win-lose-tie comparison across four Qwen models. Among them, the 72B fine-tuned one shows the best performance, followed by the 72B vanilla, 14B fine-tuned, and 14B vanilla models. These results emphasize the critical impact of model size and demonstrate the significant effectiveness of fine-tuning, as fine-tuned models consistently outperform their vanilla counterparts.\n5.3.3 Comparison with Human Evaluation\nTable 6 shows that human evaluation aligns with GPT evaluation, both assigning higher scores to the RAG model across all three aspects. Figure 8 visualizes the differences in scoring tendencies between human and GPT evaluations. Notably, correctness and informativeness show an inverted pattern: human favors more polarized correctness scores (1 and 5), while GPT prefers mid-range (2 and 3). Additionally, human assigns higher helpfulness scores more frequently, as seen in the positive values for 4 and 5. While differences in score distributions exist between evaluators, these differences are independent of the specific models. Both evaluations consistently show that RAG outperforms non-RAG by achieving higher scores.\n5.3.2 LLM-only vs. RAG\nWe compare the performance of the 72B fine-tuned model in its LLM-only (non-RAG) and RAG-integrated versions. Table 5 compares the average scores for correctness, helpfulness, and informativeness between the two versions, while Figure 7 shows the win rates. The results clearly demonstrate that integrating RAG with the LLM improves factual correctness by reducing hallucinations and provides more helpful and informative answers."}, {"title": "6 Experiment 3: Reference Matching", "content": "6.1 Evaluation\nTo evaluate the reference matching task, we randomly sample 100 triplets \\((q, D_{\\text{top}_k}, a')\\) from the test set and perform human annotation to identify which chunk \\((d_i \\in D_{\\text{top}_n})\\) each sentence in the generated answer (a') referenced. Each sentence is annotated with one or more chunks, allowing for multiple references. The interface of the annotation tool is in Figure 23 (A.5). We use sentence-level precision as the evaluation metric, defined as the"}, {"title": "6.2 Result", "content": "Table 7 compares the reference matching performance of our algorithm (Section 3.4) without thresholding and the fine-tuned Qwen 72B, while Figure 10 illustrates the score distribution and the increase in precision with score thresholding for our algorithm. Without thresholding, our algorithm achieves a sentence-level precision of 0.72, while the LLM reaches 0.81, showing a noticeable gap in performance. However, when thresholding is applied, the precision reaches 0.86 at a threshold of 0.5, with performance improving proportionally as the threshold is increased further. This demonstrates the reliability of our algorithm's scoring, allowing for controlled adjustment of reference matching quality through threshold selection. Our re-ranker used in the matching algorithm has only 0.5B parameters, compared to the LLM's 72B, demonstrating that our algorithm can achieve impressive performance with a smaller model and faster inference speed. The LLM prompt used for reference matching is in Figure 17 (A.2.3). The answers are divided into 2-3 segments on average, and we identify two types of reference matching: one where all the necessary information to answer the question is contained within a single chunk, resulting in a single-segment answer, and another where information from multiple chunks is needed to answer the question, leading to a multi-segment answer. The first type often occurs in factual questions related to specific vehicle crash collision test, while the second type is more common in open-ended questions that require referencing multiple tests (Figure 9)."}, {"title": "7 Conclusion", "content": "In this work, we presented a low-resource approach to RAG for domain expert QA, utilizing heterogeneous data sources. We showed that our data generation pipeline, guided by expert-driven prompting, facilitates an effective RAG system by converting multi-modal raw data. By enhancing the retrieval"}, {"title": "8 Limitations", "content": "Constraints in Backbone Model Selection The selection of a backbone model was constrained by the need for an open-source model supporting Korean with sufficient context length, limiting the available options across retrieval and LLM components. BGE-M3 for retriever & re-ranker, along with Qwen2.5 for the LLM, were the options available to us. However, it is worth noting that the newly released Qwen2.5-1M (Yang et al., 2025) offers a longer context length, and our preliminary experiments demonstrated its potential for better handling long inputs. As the availability of open-source LLMs continues to grow, we anticipate that this limitation will gradually be resolved over time.\nRestricted Use of API-based LLM for Initial Data Processing While our ultimate goal is to develop a fully local system, we initially relied on the API-based LLM, specifically the Claude API, for limited data processing. Among the available open-source LLMs, none met the requirements due to context limits and certain performance considerations, making them unsuitable for our needs. We determined that leveraging an API-based LLM with expert-driven prompting would be more effective in terms of both quality and cost compared to full human annotation. However, this reliance was limited to the initial data required for fine-tuning in the open-source model, and any future updates to the document pool will completely eliminate the need for external dependencies.\nWeakness in Multi-modal Questions Our RAG system exhibited relatively lower performance when retrieving contexts for multi-modal specific questions that require information from tables, graphs, and images to answer the given question (Section 4.3.1). We hypothesize that this is due to the loss of rich content when multi-modal elements are converted into plain text during the data processing stage. To address this, we are experimenting with Llama 3.2 Vision to improve the conversion of multi-modal elements into text, aiming for better representation of these elements.\nExperiments Limited by Computational Constraints The number of experiments we could conduct on the generation model was constrained by computational limitations. Fine-tuning the model on \\((q, a, D_{\\text{top}_k})\\) instead of (q, a) (Section 3.3) proved impractical due to the extensive context length, which exponentially increased GPU memory requirements, surpassing our available computational resources. Although we observed that Qwen models inherently possess summarization capabilities for the given \\(\text{D}_{\text{top}_k}\\), and fine-tuning on (q, a) yielded effective results, we were unable to explore a variety of learning strategies for the generation model within the RAG framework."}]}