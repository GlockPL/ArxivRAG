{"title": "OPTIMAL CAUSAL REPRESENTATIONS AND THE CAUSAL INFORMATION BOTTLENECK", "authors": ["Francisco N. F. Q. Simoes", "Mehdi Dastani", "Thijs van Ommen"], "abstract": "To effectively study complex causal systems, it is often useful to construct repre-\nsentations that simplify parts of the system by discarding irrelevant details while\npreserving key features. The Information Bottleneck (IB) method is a widely used\napproach in representation learning that compresses random variables while re-\ntaining information about a target variable. Traditional methods like IB are purely\nstatistical and ignore underlying causal structures, making them ill-suited for\ncausal tasks. We propose the Causal Information Bottleneck (CIB), a causal ex-\ntension of the IB, which compresses a set of chosen variables while maintaining\ncausal control over a target variable. This method produces representations which\nare causally interpretable, and which can be used when reasoning about interven-\ntions. We present experimental results demonstrating that the learned representa-\ntions accurately capture causality as intended.", "sections": [{"title": "INTRODUCTION", "content": "Natural systems typically consist of a vast number of components and interactions, making them\ncomplex and challenging to study. When investigating a specific scientific question, which is often of\na causal nature, it is frequently possible to disregard many of these details, as they have a negligible\nimpact on the outcome. These details can then be abstracted away. A classic example (Rubenstein\net al., 2017; Chalupka et al., 2017) is the relationship between particle velocities, temperature, and\npressure. To control the pressure on the walls of a room, it would be necessary to influence the\nvelocities of the approximately 1023 particles in the room. However, controlling the velocity of each\nindividual particle is not required to achieve this. Instead, manipulating the high-level variable of\ntemperature is sufficient. For an ideal gas, temperature is directly proportional to the average kinetic\nenergy of the particles in the gas (Blundell & Blundell, 2010), and modifying the temperature alters\nthe particle velocities accordingly.\nMethods that disregard the causal structure of a system when constructing abstractions may yield\nresults that are uninformative or even misleading, particularly when the objective is to manipulate\nthe system or gain causal insights. The following running example will serve as a useful illustra-\ntion of the potential drawbacks of neglecting causal considerations when learning representations.\nConsider a mouse gene with four positions 81, 82, 83, 84 under study\u00b9where nucleotides may be\nmutated, corresponding to the binary variables Xi, i = 1, ..., 4, which indicate whether there is a\nmutation at position si. These mutations can interact in a complex manner with respect to a pheno-\ntype of interest Y, say the body mass of the mouse. This type of complex interaction is known as\nepistasis (Phillips, 2008). One could create a abstraction T of X1, X2, X3, X4 that would be blind\nto differences between mutation configurations (X1, X2, X3, X4) that do not provide information\nabout Y. With the current description, it may seem that there is no need for considering causality,\nand that one could simply use a purely statistical method to learn a good \"epistasis gauger\" T. This\nis, however, not the case, since the X\u2081 and T are typically confounded by the population structure,\nthat is, \u201cany form of relatedness in the sample, including ancestry differences or cryptic relatedness\u201d\n(Sul et al., 2018). Based on the discussion in Sul et al. (2018), we consider the population structure"}, {"title": "PRELIMINARIES", "content": ""}, {"title": "CAUSAL MODELS AND INTERVENTIONS", "content": "A Structural Causal Model (SCM) provides a representation of a system's causal structure, analo-\ngous to a Bayesian network but with a causal interpretation. An SCM C = (V, N, S, PN) is com-\nprised of endogenous variables V, exogenous variables N, deterministic functions between them S,\nand a distribution over the noise variables pr. Each SCM Chas an underlying DAG Ge, called its\ncausal graph. Each node in the DAG corresponds to an endogenous variable, while the edges stand\nfor the causal relationships between them. We denote the parents and children of an endogenous\nvariable X by Pa(X) and Ch(X), respectively. Furthermore, we denote the range of a random vari-\nable X by Rx and its support by supp(X). The value of each endogenous variable is determined\nby a deterministic function of its parent variables and an independent exogenous variable, which ac-\ncounts for the system's randomness. A key feature of SCMs is their capacity to model interventions\non a variable X, which involve altering the variable's generating process. This results in a new SCM\nwith its own distribution, reflecting the system's state post-intervention. The most common type of\nintervention is an atomic intervention, where a variable X is set to a specific value x, effectively\nsevering its connection to its parents and assigning a fixed value instead. We denote such an inter-\nvention by do(X = x), the resulting SCM by Edo(X=x), and the post-intervention joint distribution\nof a set of variables W by pw do(X=x) (w) or p(w | do(X = x)). For more details, see Appendix A.3."}, {"title": "CAUSAL ENTROPY AND CAUSAL INFORMATION GAIN", "content": "We now introduce two concepts: causal entropy and causal information gain, both of which are\nfundamental to our method. For more details, see Appendix A.2, or refer to the work by Simoes\net al. (2023). The causal entropy H(Y | do(X)) measures the average uncertainty remaining about\nthe variable Y after we intervene on the variable X. This concept is closely related to conditional\nentropy but adapted for situations where interventions on X, as opposed to conditioning on X, are\nconsidered. The causal information gain Ic(Y | do(X)) extends the idea of mutual information to\nthe causal domain. It quantifies the reduction in uncertainty about Y after intervening on X, offering\na measure of the causal control that X exerts over Y. Essentially, it tells us how much more we know\nabout Y due to these interventions on X."}, {"title": "THE INFORMATION BOTTLENECK LAGRANGIAN", "content": "Let X be a random variable. By a representation T of X, we mean a variable that can only depend on\nX, whether deterministically or stochastically. This means in particular that T must be independent\nof Y when conditioning on X. This generalizes the notion of representation used by Achille &\nSoatto (2018) for cases with more variables than only X and Y. Furthermore, a representation T is\ncharacterized by its encoder, which codifies how T depends on X. We formalize this as follows:\nDefinition 1 (Representation). A random variable T is a representation of a random variable X if\nT is a function of X and an independent noise variable. The encoder of the representation T is the"}, {"title": "OPTIMAL CAUSAL REPRESENTATIONS - WHAT MAKES FOR A GOOD CAUSAL ABSTRACTION?", "content": "For the remainder of the paper, let X C V be a set of endogenous variables of an SCM C =\n(V, N, S, PN), T be a representation of X with encoder qr|x, and t be an element of R\u0442.\nIn a natural extension of the qualitative description of optimal representation in Section 2.3 to the\ncausal context, our problem can be described as finding representations T of X which retain a chosen\namount D of causal information about the relevant signal Y while minimizing the information that\nT preserves about X. We propose an axiomatic characterization of optimal causal representation to\nformally capture this description using information-theoretical quantities. This can also be seen as a\ncausal variant of the characterization of optimal representation from Achille & Soatto (2018). Since\nwe use Ic(Y | do(T)) to measure the causal information that T has about Y, and I(X;T) is the\ninformation that T keeps about X, the result is the following:\nDefinition 2 (Optimal Causal Representation). A optimal causal representation of X at sufficiency\nD is a representation T of X such that:\n(C1) T is interventionally D-sufficient for the task Y, i.e., I(Y | do(T)) = D.\n(C2) I(X;T) is minimal among the variables T satisfying (C1).\nWe can then formulate the problem of finding an optimal causal representation as the following\nminimization problem:\nRecall that the IB Lagrangian can be seen as arising from the minimization problem in Equation (1).\nLikewise, the CIB Lagrangian will be introduced to solve the minimization problem in Equation (2)."}, {"title": "THE CAUSAL INFORMATION BOTTLENECK", "content": "We can find the solution(s) to Equation (2) within the minimizers of a Lagrangian. Specifically, we\ncan minimize (for some chosen X, Y and RT) the causal information bottleneck Lagrangian LCIB\ndefined as follows:\nDefinition 3 (Causal Information Bottleneck). The causal information bottleneck (CIB) Lagrangian\nwith trade-off parameter \u03b2 \u2265 0 is the function LCIB: R|RT| \u00d7 R|Rx| \u2192 Rgiven by"}, {"title": "INTERVENTIONS ON REPRESENTATIONS", "content": "As discussed in Section 1, there is often interest in creating representations that can be intervened\non. An intervention on a representation T of X must correspond to interventions on the low-level\nvariables X. Therefore, an intervention on T will induce a distribution over the possible interven-\ntions on X, which we denote by p* (x | t) and will refer to as the \u201cintervention decoder\". We require\nthat p*(xt) be compatible with the encoder q(t | x), in the sense that both must agree with a\ncommon joint distribution over T and X.\nDefinition 4 (Intervention Decoder). The intervention decoder p* (xt) for the representation T of\nX is computed from the encoder qt|x using the Bayes rule with a chosen prior p*(x), that is,\nNotice that a choice of prior over the possible atomic distributions on X still needs to be made. In\nqt|x)\np*(x)\npractice, we will make the standard choice that p*(x) be uniform, so that p*(x | t) = \u03a3q(tx)\u00b7\nIn order to compute the effect of intervening on T on the SCM variables V, we compute the effect\nof the atomic interventions do(X = x), and weight them with the likelihood of that intervention\nusing the intervention decoder.\nDefinition 5 (Representation Intervention). The representation intervention distribution pydo(T=t)\nis\nthe weighted average of the atomic intervention distributions py do(X=x) over x \u2208 Rx, where the\nweights are given by the intervention decoder p* (x | t). That is,"}, {"title": "BACKDOOR ADJUSTMENT FOR ABSTRACTIONS", "content": "In order to learn the optimal causal representation, the learning algorithm will need to estimate\nLCIB = I(X;T) \u2013 BIc(Y | do(T)) at each iteration step, making use of the encoder qTx at that\niteration and the joint py. The compression term I(X;T) can be directly computed using px and\nthe encoder, while the interventional sufficiency term Ic(Y | do(T)) demands the computation of\np(y | do(T = t)). Equation (15) allows us to write p(y | do(T = t)) in terms of the encoder q(t | x)"}, {"title": "COMPARING REPRESENTATIONS", "content": "After learning a representation T\u2081, we may want to compare it with another representation T2, which\ncould either be one learned earlier or one considered as the ground truth. Simple equality of their en-\ncoders is not an appropriate criterion for this comparison. Two representations might have different\nencoders and still be \u201cequivalent\u201d in the sense that they coincide when the values of the represen-\ntations are relabeled. This is especially apparent in the case of completely deterministic representa-\ntions. For example, consider two binary representations T\u2081 and T2 of a low-level variable X with\nrange Rx = {0, 1, 2}, defined by deterministic functions \u03c6\u0442\u2081 and \u0444\u0442\u2082. Suppose \u0444\u0442\u2081 maps 0 and 1 to\n0, and 2 to 1, while \u0444\u0442\u2082 maps 0 and 1 to 1, and 2 to 0. Intuitively, T\u2081 and T2 represent the same rep-\nresentation because relabeling the values of T\u2081 (i.e., swapping 0 and 1) yields a representation that\nproduces the same values as T2 for the same low-level values of X. Formally, and extending to the\nnon-deterministic case, this is to say that equivalence arises whenever the conditional distributions\nare identical up to a bijection o of the values of the Ti. This leads to the following definition:\nDefinition 9 (Equivalent representations). Two representations T1 and T2 of X are equivalent if\nthere is a bijection \u03c3: supp(T\u2081) \u2192 supp(T2) such that \u2200t1 \u2208 supp(T1), x \u2208 supp(X), qT\u2081\\x(t1 |\nx) = qT2\\x(\u03c3(t1) | x), where qT\u2081\\X, QT2\\X are the encoders for T\u2081 and T2. We then write T\u2081 = T2.\nOne can show that \u2243 is an equivalence relation (see Proposition 19). We call abstraction an equiv-\nalence class of \u2243, that is, the elements of \u25b3 /\u2243. In practice, it is unlikely that two representations"}, {"title": "EXPERIMENTAL RESULTS", "content": "In our experiments, we aim to minimize a reparameterized version of the CIB, given by\nL CIB [qT\\x] := (1 \u2212 \u03b3)I(X;T) \u2013 \u03b3Ic(Y | do(T)), where \u03b3 \u2208 [0, 1] is the trade-off parameter.\nBy a slight abuse of notation, we distinguish this from the original parameterization of the CIB\nsolely by the superscript y. The parameter y has a more intuitive interpretation than 8, representing\nthe fraction of the CIB that the interventional sufficiency term accounts for. Additionally, it simpli-\nfies hyperparameter searches when using optimization algorithms, since the magnitude of the values\nof L cIB remains relatively stable with variations in y, unlike what happens with L CIB and B. It is\nstraightforward to verify that minimizing the CIB LOCIB is equivalent to minimizing its reparameter-\nization LCIB, provided that y is selected appropriately (see Proposition 18).\nIn this section, we will demonstrate the application of the CIB Lagrangian by finding its minimum\nfor three problems of increasing complexity and for different values of y, using the local search algo-\nrithms\u00b2(Simplex) Projected Gradient Descent (pGD) and (Simplex) Projected Simulated Annealing\nGradient Descent (pSAGD) described in Appendix E, and checking whether the results are what we\nexpect.\nSpecifically, in each experiment we will check whether:\n(a) If y = 1, the learned representation T coincides (modulo\u2243) with the ground truth T of\nthat experiment. That is, whether VI(T, T) = 0.\n(b) If y = 0, the learned representation has I(X;T) = 0.\n(c) Larger y values correspond to larger (or at least not smaller) values of I(Y | do(T)). That\nis, if Y1 > 2, one has that the causal information gain for the encoder learned when y = Y1\nis larger or equal to that of the encoder learned when y = \u00a52.\nIf (a), (b), and (c) hold, this provides evidence that the CIB can be used to learn representations\nthat maximize control (by setting \u03b3 = 1), maximize compression (by setting \u03b3 = 0), or strike a\nbalance between the two (by setting \u03b3\u2208 (0,1)). This is also evidence that the proposed local search\nalgorithms succeed in optimizing the CIB objective. Note that ground truth is only available for\ny = 1 and y = 0. For y = 1, the optimal solution will be clear for the proposed case studies.\nFor y = 0, the solution should be maximally compressive, regardless of causal control over Y.\nFor \u03b3 \u00a3 {0, 1}, there is no obvious ground truth, but the reasonableness of the results can still be\nassessed as described in (c). It is also noteworthy that encoders trained with different y values may\nachieve the same Ic(Y | do(T)), although increasing y should not decrease I(Y | do(T)). This\nproperty is analogous to the Information Bottleneck (IB) framework, where distinct \u1e9e values often\nyield the same sufficiency value (Kolchinsky et al., 2018)."}, {"title": "LEARNING ODD AND EVEN", "content": "Setup Consider the SCM in Figure 2a, which represents a scenario where the parity of X deter-\nmines the outcome Y with some uncertainty, parameterized by uy. To preserve the control that X\nhas over Y, a representation T of X should reflect the parity of X. Consequently, when T is binary\nand we aim to maximize the causal control of T over Y, T must be equivalent to T = X mod 2.\nThis will serve as the ground truth (modulo \u2243) for the case y = 1."}, {"title": "LEARNING ADDITION IN THE PRESENCE OF STRONG CONFOUNDING", "content": "Setup Consider the SCM in Figure 2b, which represents a situation where Y is controlled by\nX = (X1, X2) through the sum X1 + X2 \u2208 {0,1,2}, and W confounds X\u2081 and Y. Notice that\nW satisfies the backdoor criterion relative to (X, Y). To preserve the control that X has over Y, a\nrepresentation T of X should keep the value of X1+X2 \u2208 {0,1,2}. This is because, by construction\nof the Structural Causal Model (SCM), the sum of X1 and X2 is the only aspect of X that can\nbe manipulated to affect Y. Therefore, if T is chosen to be a 3-valued variable and one aims to\nmaximize causal control of T over Y, it follows that T should be equivalent to T = X1 + X2. This\nwill be the ground truth abstraction (modulo \u2243) for the case \u03b3 = 1."}, {"title": "GENETIC MUTATIONS", "content": "Setup Consider the SCM depicted in Figure 3, which represents a scenario of genetic mutations\nin mice like the one described in Section 1. Notice that S satisfies the backdoor criterion relative to"}, {"title": "RELATED WORK", "content": "There has been other work investigating a problem related to our search for an optimal causal repre-\nsentation. In Chalupka et al. (2017; 2014; 2016b;a)), the authors search for \u201ccausal macrovariables\"\nof higher-dimensional \u201cmicrovariables\u201d, described as coarse representations of the microvariables\nX and Y which preserve the causal relation between them\u00b3. Their definition of macrovariable is\ndistinct from our definition of optimal causal representations. Namely, it is not based on information-\ntheoretical concepts such as compression and sufficiency, but on clustering together the values of X\nresulting in the same post-intervention distributions over Y. It would be interesting to see in what\ncases our approach coincides with theirs. Note that the two approaches cannot align in general,\nsince their method does not incorporate causality, as their clustering procedure relies on conditional\ndistributions, which, as they demonstrate, necessarily yield a finer-grained representation than truly\ncausal clusters. Moreover, our abstractions T can, at least for y \u2260 1, be stochastic mappings of X,\nwhile their macrovariables are deterministic mappings of X. H\u00f6ltgen (2021); Jammalamadaka et al."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We extended the notion of optimal representation to the causal setting, resulting in an axiomatic\ncharacterization of optimal causal representations. Just as the information bottleneck (IB) method\ncan be used to learn optimal representations, so can the causal information bottleneck (CIB) method\nintroduced in this paper be used to learn optimal causal representations. The CIB, which depends on\nthe interventional distributions p(y | do(t)), needs to be computed during the learning procedure,\nwhich consists of solving a constrained minimization problem. The exact expression for the CIB will\ndepend on the causal structure of the system under study. We focused on cases where there is a set Z\nsatisfying the backdoor criterion relative to (X, Y). This allows us to derive a backdoor adjustment\nformula for p(y | do(t)), and thus successfully apply a minimization algorithm to minimize the CIB.\nSpecifically, we introduced a local search algorithm, referred to as projected simulated annealing\ngradient descent (pSAGD), which integrates simulated annealing and gradient descent techniques\nwith a projection operator to maintain constraint satisfaction throughout the minimization process.\nIn order to compare different representations learned by our algorithm, we introduced a novel notion\nof equivalence of representations, which partitions representations into equivalence classes, termed\nabstractions, and showed that the variation of information can be used to assess whether two repre-\nsentations are equivalent, as long as one of them is deterministic. We experimentally validated that\nthe learned representations in three toy models of increasing complexity align with our expectations.\nFuture research directions include exploring alternative methods for incorporating causality into the\ninformation bottleneck framework, such as focusing on causal properties other than causal control,\nlike proportionality (Pocheville et al., 2015). Our approach can also be extended to scenarios where\nthe backdoor criterion does not hold by leveraging do-calculus, allowing for the automatic com-\nputation of post-intervention distributions for interventions on representations. Another area worth\ninvestigating relates to fine-tuning the trade-off between compression and interventional sufficiency.\nKolchinsky et al. (2018) highlight that, in the context of the Information Bottleneck (IB), different\nvalues of \u1e9e can often result in the same sufficiency. Similarly, in our experiments we observed that\ndifferent values of y (and thus \u1e9e) often produced the same interventional sufficiency values. Fu-\nture work could explore strategies similar to those used by Kolchinsky et al. (2018) to address this.\nAdditionally, another natural next step would be to adapt the causal information bottleneck (CIB)\nmethod to continuous variables, for example by using variational autoencoders (Kingma & Welling,"}], "equations": ["\\arg \\min _{q_{T X} \\in \\mathbb{R}^{\\left|R_{T}\\right| \\times\\left|R_{X}\\right|}} I(X ; T) \\quad \\text { s.t. }\\left\\{\\begin{array}{l}\\forall x \\in R_{X}, q_{T \\mid X=x} \\in \\Delta^{\\left|R_{T}\\right|-1} \\\\I(Y ; T)=D\\end{array}\\right.", "\\arg \\min _{q_{T X} \\in \\mathbb{R}^{\\left|R_{T}\\right| \\times\\left|R_{X}\\right|}} I(X ; T) \\quad \\text { s.t. }\\left\\{\\begin{array}{l}\\forall x \\in R_{X}, q_{T \\mid X=x} \\in \\Delta^{\\left|R_{T}\\right|-1} \\\\I_{c}(Y \\mid d o(T))=D\\end{array}\\right.", "L_{C I B}\\left[q_{T\\mid X}\\right]:=I(X ; T)-\\beta I_{c}(Y \\mid d o(T)).", "p^{*}(x \\mid t):=\\frac{q(t \\mid x) p^{*}(x)}{\\sum_{\\tilde{x}} q(t \\mid \\tilde{x}) p^{*}(\\tilde{x})}", "p_{V}^{d o(T=t)}(v)=p(v \\mid d o(T=t)):=\\sum_{x} p^{*}(x \\mid t) p_{V}^{d o(X=x)}(v).", "\\begin{aligned}p(y \\mid d o(T=t)) &=\\sum_{z} p(z) \\sum_{x} p_{Y \\mid X, Z}(y \\mid x, z) p^{*}(x \\mid t) \\\\&=\\sum_{z} p(z) \\sum_{x} p_{Y \\mid X, Z}(y \\mid x, z) \\frac{q(t \\mid x) p^{*}(x)}{\\sum_{\\tilde{x}} q(t \\mid \\tilde{x}) p^{*}(\\tilde{x})},\\end{aligned}", "V I\\left(T_{1} ; T_{2}\\right):=H\\left(T_{1} \\mid T_{2}\\right)+H\\left(T_{2} \\mid T_{1}\\right).", "X_{1}:=\\min \\left(N_{X_{1}}+\\frac{W}{3} X_{2}, 1\\right)\nX_{2}:=N_{X_{2}}\nW:=3 N_{w}\nY:=W+\\left(X_{1}+X_{2}\\right) N_{Y}", "L_{w C I B}^{y}=\\frac{1}{1+\\beta} I(Y ; X)-\\frac{\\beta}{1+\\beta} I(Y \\mid d o(T))", "H_{c}(Y \\mid d o(X \\sim X^{*})):=\\mathbb{E}_{x \\sim p_{X^{*}}}[H(Y \\mid d o(X=x))].", "I_{c}(Y \\mid d o(X \\sim X^{*})):=H(Y)-H_{c}(Y \\mid d o(X \\sim X^{*})).", "p_{V}^{d o(T=t)}(y)=p(y \\mid d o(T=t))=\\sum_{v_{1}, \\ldots, v_{m}, x} p\\left(v_{1}, \\ldots, v_{m}, x, y \\mid d o(T=t)\\right)\n=\\sum_{x} p^{*}(x \\mid t) \\sum_{v_{1}, \\ldots, v_{m}, x} p\\left(v_{1}, \\ldots, v_{m}, x, y \\mid d o(X=x)\\right)=\\sum_{x} p^{*}(x \\mid t) p(y \\mid d o(X=x))", "\\begin{aligned}p(y \\mid d o(t)) &=\\sum_{x} p^{*}(x \\mid t) p_{C ; d o(X=x)}(y) \\\\&=\\sum_{x} p^{*}(x \\mid t) \\sum_{z} p_{C}(z) p_{C}(y \\mid x, z) \\\\&=\\sum_{z} p_{C}(z) \\sum_{x} p_{C}(y \\mid x, z) \\frac{q(t \\mid x) p^{*}(x)}{\\sum_{\\tilde{x}} q(t \\mid \\tilde{x}) p^{*}(\\tilde{x})},\\end{aligned}", "q_{T_{2} \\mid X}\\left(t_{2} \\mid x\\right)=q_{T_{2} \\mid X}\\left(\\sigma\\left(t_{1}\\right) \\mid x\\right)=q_{T_{1} \\mid X}\\left(t_{1} \\mid x\\right)=q_{T_{1} \\mid X}\\left(\\sigma^{-1}\\left(t_{2}\\right) \\mid x\\right).", "q_{T_{1} \\mid X}\\left(t_{1} \\mid x\\right)=q_{T_{2} \\mid X}\\left(\\sigma_{12}\\left(t_{1}\\right) \\mid x\\right)=q_{T_{3} \\mid X}\\left(\\sigma_{23}\\left(\\sigma_{12}\\left(t_{1}\\right)\\right) \\mid x\\right).", "\\Pi_{\\Delta}\\left(q_{T X}^{(t)}-\\alpha \\nabla_{q_{T X}} L_{C I B}\\left(q_{T X}^{(t)}\\right)\\right)", "\\Pi_{\\Delta^{|R T|-1}}\\left(q_{T \\mid X=x}^{(t)}-\\alpha \\nabla_{q_{T \\mid X=x}} L_{C I B}\\left(q_{T \\mid X=x}^{(t)}\\right)\\right)"]}