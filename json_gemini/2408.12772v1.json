{"title": "Symmetric masking strategy enhances the performance of Masked Image Modeling", "authors": ["Khanh-Binh Nguyen", "Chae Jung Park"], "abstract": "Masked Image Modeling (MIM) is a technique in self-supervised learning that focuses on acquiring detailed visual representations from unlabeled images by estimating the missing pixels in randomly masked sections. It has proven to be a powerful tool for the preliminary training of Vision Transformers (ViTs), yielding impressive results across various tasks. Nevertheless, most MIM methods heavily depend on the random masking strategy to formulate the pretext task. This strategy necessitates numerous trials to ascertain the optimal dropping ratio, which can be resource-intensive, requiring the model to be pre-trained for anywhere between 800 to 1600 epochs. Furthermore, this approach may not be suitable for all datasets. In this work, we propose a new masking strategy that effectively helps the model capture global and local features. Based on this masking strategy, SymMIM, our proposed training pipeline for MIM is introduced. SymMIM achieves a new SOTA accuracy of 85.9% on ImageNet using ViT-Large and surpasses previous SOTA across downstream tasks such as image classification, semantic segmentation, object detection, instance segmentation tasks, and so on.", "sections": [{"title": "1 Introduction", "content": "Masked image modeling (MIM) [2,26,10,43] is a self-supervised learning technique that aims to learn rich visual representations from unlabeled images by predicting the missing pixels in randomly masked regions. Inspired by masked language modeling (MLM) [14] in natural language processing (NLP) tasks, MIM [2] has gained popularity in the context of self-supervised learning using Vision Transformers (ViTs) [16]. In MIM, a subset of vision tokens is randomly masked, and the model learns to recover the original image through a Transformers network during training.\nSeveral concurrent research efforts [15,28,41] have focused on designing patch-level dictionaries and image tokenizers to establish appropriate learning objectives (e.g., vision token IDs) for MIM. While advanced results can be obtained,"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Self-supervised learning", "content": "In self-supervised representation learning (SSL), a model trains on a pretext task where supervision is derived solely from the input data, without relying on labeled data. Contrastive learning stands out as a popular approach within SSL, aiming to learn representations by contrasting positive and negative samples. Notable methods in this category include SimCLR [8], CPC [35], MoCo [25], MoCo-v2 [11], BYOL [22], and DINO [6]. Furthermore, group instance contrastive learning approaches, such as DeepCluster [4] and SwAV [5], incorporate clustering into contrastive learning to enhance the quality of learned representations. An alternative avenue for self-supervised learning involves generative modeling, where the focus lies in acquiring a generative model capable of capturing the underlying data distribution."}, {"title": "2.2 Masked Modeling", "content": "Masked modeling learns representations by reconstructing a masked portion of the input. Pioneering works in natural language processing (NLP) have introduced various pretraining objectives. BERT [14] utilizes a bidirectional transformer and demonstrates few-shot learning capabilities through masked language modeling. GPT [3,36], on the other hand, employs autoregressive, causal masking, showcasing multitasking, few-shot, and in-context learning capabilities. In the realm of computer vision, early works such as Stacked Denoising Autoencoders [40] and Context Encoder [10] explored masked image modeling as a means of denoising or representation learning.\nRecently, with the widespread adoption of transformers [16] as backbone architectures for vision tasks where images are divided into patches and tokenized as sequences-researchers aim to transfer the success of language sequence modeling to large-scale vision transformers. Notable early works, including BEiT [2], MAE [24], and SimMIM [43], have explored BERT-style pretraining for vision transformers. Interestingly, both MAE and SimMIM emphasize the necessity of a significantly higher mask ratio compared to NLP approaches to achieve effective visual representation learning. Furthermore, recent research extends masked pre-training to hierarchical architectures [7,43] and investigates the impact of data augmentation [7,18].\nCrossMAE, instead of relying on self-attention, employs cross-attention blocks in the decoder to reconstruct only a subset of the masked tokens during pretraining [1,20,29]. This approach aims to learn both effective representations and reconstruction capabilities. In contrast, BERT-style pretraining heavily utilizes self-attention, resulting in computational complexity that scales polynomially with sequence length.\nSiamese MAE [23], on the other hand, adopts an asymmetric masking pattern and decodes video frames conditioned on earlier frames. In this setting, all masked patches are reconstructed."}, {"title": "3 Symmetric Masked Image Modeling", "content": "In this section, we delve into the details of our approach. We commence by reviewing the standard Masked Image Modeling (MIM) method and highlighting its limitations. Subsequently, we introduce the symmetric masking strategy,"}, {"title": "3.1 Preliminaries", "content": "Transformer architectures [16] have gained widespread adoption in computer vision tasks. One prominent method for visual representation learning involves a pretext task known as masked image modeling. Specifically, a large proportion of image patches are randomly masked. Hence, the backbone network is trained to recover the token IDs of these corrupted images. To achieve this, a patch-level vision dictionary is employed. This dictionary is more fine-grained and is typically static and pre-defined by an existing image tokenizer [37,17]. The image tokenizer converts continuous visual signals into discrete keys. Notably, in the influential work called BEiT [2], a pre-trained discrete VAE (Variational Autoencoder) serves as the tokenizer. The ultimate goal is to predict the masked patches, which is formulated as a classification task with cross-entropy loss. In summary, masked image modeling within transformer architectures enhances visual representation learning by leveraging masked patches and a carefully designed vision dictionary as:\n$L_{mim}(x) = E_{j \\in M} [- log p (y_j | f(\\hat{x})_j)]$\nwhere $M$ denotes the set of masked patch indices, $\\hat{x}$ is the corrupted image after random masking, $y_j$ is the positive key index in the patch-level dictionary, and $p(\\cdot|\\cdot)$ indicates the probability that correctly identifies the recovered patch $f(\\hat{x})_j$ with a patch index of $j$."}, {"title": "3.2 Symmetric masking strategy", "content": "Patch alignment is a technique that divides an image into fixed-size patches, treating each patch as a token for vision Transformers. This approach facilitates representation learning by allowing the Transformer to capture both global and local dependencies among patches, thereby learning rich semantic features from the image. Additionally, patch alignment simplifies the masking strategy for self-supervised learning: each patch can be either fully visible or fully masked. The Transformer learns to create or infer missing signals from the context by predicting the masked patches.\nIn their work, [43] explore various patch-alignment masking strategies, including random masking, square masking, block-wise masking, and central region masking. However, adopting these strategies often involves multiple trials to determine the optimal masking ratio for each strategy and dataset, which is not an ideal approach. For instance, [43] perform more than 30 trials to identify the optimal strategy and ratio during pretraining with 800 epochs and fine-tuning for 100 epochs."}, {"title": "3.3 Momentum contrastive learning", "content": "In this paper, we propose a masking strategy that dynamically adjusts the size of the checkerboard pattern to strike a balance between local and global information within the input image. Specifically, we employ different mask sizes for the online network and the momentum encoder-two models sharing the same backbone but with distinct update rates.\n1. The online network employs smaller masks, effectively concealing more local information. This deliberate difficulty enhances the learning task.\n2. The momentum encoder, on the other hand, utilizes larger masks, effectively hiding more global information. This approach promotes consistency and diversity in the learned key features.\n3. The momentum encoder parameters are updated using a moving average of the online network parameters.\nThe reconstruction loss is computed in two ways:\nBetween the original image and the recovered image from the online network.\nBetween the recovered images from both the online network and the momentum encoder."}, {"title": "4 Experiments", "content": "We conduct self-supervised pretraining on the ImageNet-1K dataset, following the methodologies of MAE [24] and SimMIM [43]. The hyperparameters align with those used in SimMIM for both the ViT-Base and ViT-Large backbones. Specifically:"}, {"title": "4.1 ImageNet Classification", "content": "We fine-tuning pre-trained models on the ImageNet-1K dataset [13] to benchmark our proposed SymMIM. Our fine-tuning setup closely follows that of BEIT [2]. Specifically:\nFor ViT-Small, fine-tuning for total 200 epochs with a warm-up of 5 epochs and a layer decay of 0.8.\nFor ViT-Base fine-tuning for total 100 epochs with a warm-up of 20 epochs and a layer decay of 0.65.\nFor ViT-Large fine-tuning for total 50 epochs with a warm-up of 5 epochs and a layer decay of 0.75.\nFor fair comparison, we only compare with previous SOTA that fine-tuning on the same input image size as 224 x 224 pixels. Our observations reveal that models with the proposed symmetric masking strategy consistently surpass previous approaches. Comparing with previous SOTA such as SimMIM [43], ConMIM [44], MAE [24], SymMIM outperforms with a significant margin. However, SymMIM lacks behind the ConMIM\u2020 [44] which fine-tuning with higher resolution of 384 x 384.\nFor further evaluation, we record the results for ViT-Small and ViT-Large in 2 and 3, respectively. It clearly shows that SymMIM outperforms previous SOTA methods when using ViT-Small and ViT-Base as a backbone and is on par with MAE [24] when using ViT-Large. Notably, although ConMIM [44] achieves higher performance compared with our proposed SymMIM, they fine-tune the model on a larger resolution input of 384 \u00d7 384."}, {"title": "4.2 Object Detection and Instance Segmentation", "content": "For object detection and instance segmentation, we conduct an end-to-end fine-tuning of Mask R-CNN [26] on the COCO dataset [32]. Our approach involves replacing most attention blocks in the ViT backbone with windowed blocks to mitigate the quadratic complexity associated with self-attention. However, we retain four global blocks for cross-window interaction. Additionally, we evenly integrate four up-and-down-sample modules with the ViT architecture to generate the pyramid features required by FPN."}, {"title": "4.3 Semantic Segmentation", "content": "We assess the performance of SymMIM in downstream semantic segmentation using the ADE20K [45] benchmark, which comprises 25,000 images across 150 semantic categories. Our evaluation metric is the mean intersection over union (mIOU). For this task, we employ UperNet [42] and follow the same setup as BEIT [2]. The input images are resized to 512 \u00d7 512, and the model undergoes 160,000 iterations of fine-tuning. Additionally, we leverage intermediate fine-tuning to fully exploit the potential of pre-trained models, as per BEiT's approach [2]. Specifically, we first fine-tune the models on ImageNet-1K classification and then transfer them to the ADE20K dataset. Our results, detailed in 5, consistently outperform the baseline BEiT, achieving significant improvements.\nFor instance, our approach achieves 50.8% mIOU compared to BEiT's 47.7% on ViT-Base. Remarkably, even our SymMIM using ViT-Small achieves comparable performance to BEiT using ViT-Base (i.e. 47.7%). Notably, models pre-trained"}, {"title": "5 Ablations", "content": ""}, {"title": "5.1 Analysis of learning objective", "content": "In this section, we experiment with each additional learning objective in the total loss for the ablation study. We progressively test with different sets of loss terms from 4 and report the resulting accuracy on ImageNet-1K datasets. The results, detailed in 6 show an optimum when using the final objective in 4. Interestingly, we also see that contrastive objective in 3 contributes the most toward the overall performance."}, {"title": "5.2 Masking ratio probing", "content": "In this section, we conduct the ablation study on the masking ratio choices. Figure 2 shows the influence of the masking ratio towards the fine-tuning performance. The accuracy fluctuates and is not stable for different methods and ratios. This behavior shows that for different MIM methods and different datasets, we need to perform the probing again to fine the optimal ratio. In contrast to that, our proposed method, SymMIM, using only one ratio of 50% for both small"}, {"title": "5.3 Visualization", "content": "In this section, we delve into comprehending the proposed approach and critical designs through visualizations. All the example images are sourced from the ImageNet-1K validation set.\n3 illustrates the recovered images using various human-designed masks. These masks serve to reveal the capabilities learned through masked image modeling. Specifically, the human-designed masks include a random mask, a very small size mask, a small size mask, and a large size symmetric mask.\nAs we can see, the recovered image using a large size symmetric mask preserves much more global information, while the small and very small size masks are better at recovering local information. Therefore, the model trained with our proposed method, which utilizes both large and small size masks is more robust."}, {"title": "6 Conclusion", "content": "In this study, we introduce a straightforward yet impactful symmetric masking strategy that simplifies hyperparameter tuning and masking ratio exploration. We then present SymMIM, a novel masked image modeling framework that"}]}