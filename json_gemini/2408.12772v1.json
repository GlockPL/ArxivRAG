{"title": "Symmetric masking strategy enhances the performance of Masked Image Modeling", "authors": ["Khanh-Binh Nguyen", "Chae Jung Park"], "abstract": "Masked Image Modeling (MIM) is a technique in self-supervised learning that focuses on acquiring detailed visual representations from unlabeled images by estimating the missing pixels in randomly masked sections. It has proven to be a powerful tool for the preliminary training of Vision Transformers (ViTs), yielding impressive results across various tasks. Nevertheless, most MIM methods heavily depend on the random masking strategy to formulate the pretext task. This strategy necessitates numerous trials to ascertain the optimal dropping ratio, which can be resource-intensive, requiring the model to be pre-trained for anywhere between 800 to 1600 epochs. Furthermore, this approach may not be suitable for all datasets. In this work, we propose a new masking strategy that effectively helps the model capture global and local features. Based on this masking strategy, SymMIM, our proposed training pipeline for MIM is introduced. SymMIM achieves a new SOTA accuracy of 85.9% on ImageNet using ViT-Large and surpasses previous SOTA across downstream tasks such as image classification, semantic segmentation, object detection, instance segmentation tasks, and so on.", "sections": [{"title": "1 Introduction", "content": "Masked image modeling (MIM) [2,26,10,43] is a self-supervised learning technique that aims to learn rich visual representations from unlabeled images by predicting the missing pixels in randomly masked regions. Inspired by masked language modeling (MLM) [14] in natural language processing (NLP) tasks, MIM [2] has gained popularity in the context of self-supervised learning using Vision Transformers (ViTs) [16]. In MIM, a subset of vision tokens is randomly masked, and the model learns to recover the original image through a Transformers network during training.\nSeveral concurrent research efforts [15,28,41] have focused on designing patch-level dictionaries and image tokenizers to establish appropriate learning objectives (e.g., vision token IDs) for MIM. While advanced results can be obtained,"}, {"title": "2 Related Work", "content": "existing off-the-shelf image tokenizers (such as discrete VAE [12] used in BEIT) rely on additional training stages and domain-specific knowledge, resulting in a less flexible two-stage pre-training approach.\nDespite this limitation, MIM methods have proven effective for pre-training vision transformers, leading to state-of-the-art performance on various downstream tasks, including image segmentation [27] and exploration tasks [30]. However, it is worth noting that most MIM approaches still rely on a random masking strategy with a high masking ratio (e.g., 75% to 95%), which may not fully capture both the global and local features of the images. Additionally, the computational cost associated with finding the optimal masking ratio through extensive trials remains a challenge, especially when dealing with diverse datasets.\nIn this work, we revisit the masking strategy employed in conventional Masked Image Modeling (MIM) and related approaches. While conventional MIM and its derivatives are tailored for masked image modeling, they inherently encourage the network to capture fine-grained visual context and semantics. However, we take a different approach regarding the learning objective. Instead of using just pixel-wise MSE loss, we divide the training to two different patches with large and small size of the mask. Then, enforcing the contrastive loss between them to encourage the consistency between global and local features. This approach effectively structures the visual space, yielding semantically meaningful representations.\nOur proposed solution introduces a symmetric masking strategy that addresses existing limitations and enhances MIM performance. Specifically, we symmetrically mask image patches along both the horizontal and vertical axes, creating four quadrants of visible and masked patches. This design ensures that each masked patch corresponds to a visible patch containing similar semantic and structural information, facilitating the reconstruction task and feature learning. Moreover, our symmetric masking strategy simplifies hyperparameter tuning, relying solely on the size of the mask, thereby reducing overall complexity.\nLeveraging this symmetric masking strategy, we introduce SymMIM, a straightforward yet highly effective framework for Masked Image Modeling (MIM). SymMIM comprises a ViT encoder and projector that map the feature maps into low-level embedding space, then leveraging the contrastive learning with the EMA model to encourage the consistency. What sets SymMIM apart is its simplicity-it avoids complex designs such as masking strategy probing, discrete VAE, or clustering.\nDespite its straightforward approach, SymMIM outperforms previous MIM methods across various tasks, including image classification, object detection, and instance segmentation. Extensive experiments conducted on the ImageNet-1K dataset demonstrate the scalability and effectiveness of SymMIM, achieving a new state-of-the-art performance on several vision benchmarks when applied to larger models and diverse datasets."}, {"title": "2.1 Self-supervised learning", "content": "In self-supervised representation learning (SSL), a model trains on a pretext task where supervision is derived solely from the input data, without relying on labeled data. Contrastive learning stands out as a popular approach within SSL, aiming to learn representations by contrasting positive and negative samples. Notable methods in this category include SimCLR [8], CPC [35], MoCo [25], MoCo-v2 [11], BYOL [22], and DINO [6]. Furthermore, group instance contrastive learning approaches, such as DeepCluster [4] and SwAV [5], incorporate clustering into contrastive learning to enhance the quality of learned representations. An alternative avenue for self-supervised learning involves generative modeling, where the focus lies in acquiring a generative model capable of capturing the underlying data distribution."}, {"title": "2.2 Masked Modeling", "content": "Masked modeling learns representations by reconstructing a masked portion of the input. Pioneering works in natural language processing (NLP) have introduced various pretraining objectives. BERT [14] utilizes a bidirectional transformer and demonstrates few-shot learning capabilities through masked language modeling. GPT [3,36], on the other hand, employs autoregressive, causal masking, showcasing multitasking, few-shot, and in-context learning capabilities. In the realm of computer vision, early works such as Stacked Denoising Autoencoders [40] and Context Encoder [10] explored masked image modeling as a means of denoising or representation learning.\nRecently, with the widespread adoption of transformers [16] as backbone architectures for vision tasks where images are divided into patches and tokenized as sequences-researchers aim to transfer the success of language sequence modeling to large-scale vision transformers. Notable early works, including BEiT [2], MAE [24], and SimMIM [43], have explored BERT-style pretraining for vision transformers. Interestingly, both MAE and SimMIM emphasize the necessity of a significantly higher mask ratio compared to NLP approaches to achieve effective visual representation learning. Furthermore, recent research extends masked pre-training to hierarchical architectures [7,43] and investigates the impact of data augmentation [7,18].\nCrossMAE, instead of relying on self-attention, employs cross-attention blocks in the decoder to reconstruct only a subset of the masked tokens during pretraining [1,20,29]. This approach aims to learn both effective representations and reconstruction capabilities. In contrast, BERT-style pretraining heavily utilizes self-attention, resulting in computational complexity that scales polynomially with sequence length.\nSiamese MAE [23], on the other hand, adopts an asymmetric masking pattern and decodes video frames conditioned on earlier frames. In this setting, all masked patches are reconstructed."}, {"title": "3 Symmetric Masked Image Modeling", "content": "ConMIM [44] also leverage the contrastive loss to boost the representation learning of the MIM. However, in contrast with ours, they propose a different type of objective base on the InfoNCE loss for each pixels while we using the contrastive loss for low-level embeddings.\nThis paper investigates the necessity and impact of different masking methods for pretraining feature extractors. We conduct a comparative analysis between symmetric and random masking, considering various masking ratios, and evaluating their effects on downstream tasks. Additionally, we propose a straightforward yet effective approach to reduce hyperparameter tuning when selecting the optimal masking ratio."}, {"title": "3.1 Preliminaries", "content": "Transformer architectures [16] have gained widespread adoption in computer vision tasks. One prominent method for visual representation learning involves a pretext task known as masked image modeling. Specifically, a large proportion of image patches are randomly masked. Hence, the backbone network is trained to recover the token IDs of these corrupted images. To achieve this, a patch-level vision dictionary is employed. This dictionary is more fine-grained and is typically static and pre-defined by an existing image tokenizer [37,17]. The image tokenizer converts continuous visual signals into discrete keys. Notably, in the influential work called BEiT [2], a pre-trained discrete VAE (Variational Autoencoder) serves as the tokenizer. The ultimate goal is to predict the masked patches, which is formulated as a classification task with cross-entropy loss. In summary, masked image modeling within transformer architectures enhances visual representation learning by leveraging masked patches and a carefully designed vision dictionary as:\n$\\mathcal{L}_{mim}(x) = E_{j \\in M} [- \\log p (y_j | f(\\hat{x})_j)]$\nwhere $M$ denotes the set of masked patch indices, $\\hat{x}$ is the corrupted image after random masking, $y_j$ is the positive key index in the patch-level dictionary, and $p(\\cdot|\\cdot)$ indicates the probability that correctly identifies the recovered patch $f(\\hat{x})_j$ with a patch index of $j$."}, {"title": "3.2 Symmetric masking strategy", "content": "Patch alignment is a technique that divides an image into fixed-size patches, treating each patch as a token for vision Transformers. This approach facilitates representation learning by allowing the Transformer to capture both global and local dependencies among patches, thereby learning rich semantic features from the image. Additionally, patch alignment simplifies the masking strategy for self-supervised learning: each patch can be either fully visible or fully masked. The Transformer learns to create or infer missing signals from the context by predicting the masked patches.\nIn their work, [43] explore various patch-alignment masking strategies, including random masking, square masking, block-wise masking, and central region masking. However, adopting these strategies often involves multiple trials to determine the optimal masking ratio for each strategy and dataset, which is not an ideal approach. For instance, [43] perform more than 30 trials to identify the optimal strategy and ratio during pretraining with 800 epochs and fine-tuning for 100 epochs."}, {"title": "3.3 Momentum contrastive learning", "content": "In this paper, we propose a masking strategy that dynamically adjusts the size of the checkerboard pattern to strike a balance between local and global information within the input image. Specifically, we employ different mask sizes for the online network and the momentum encoder-two models sharing the same backbone but with distinct update rates.\n1. The online network employs smaller masks, effectively concealing more local information. This deliberate difficulty enhances the learning task.\n2. The momentum encoder, on the other hand, utilizes larger masks, effectively hiding more global information. This approach promotes consistency and diversity in the learned key features.\n3. The momentum encoder parameters are updated using a moving average of the online network parameters.\nThe reconstruction loss is computed in two ways:\nBetween the original image and the recovered image from the online network.\nBetween the recovered images from both the online network and the momentum encoder."}, {"title": "4 Experiments", "content": "We conduct self-supervised pretraining on the ImageNet-1K dataset, following the methodologies of MAE [24] and SimMIM [43]. The hyperparameters align with those used in SimMIM for both the ViT-Base and ViT-Large backbones. Specifically:"}, {"title": "4.1 ImageNet Classification", "content": "We fine-tuning pre-trained models on the ImageNet-1K dataset [13] to benchmark our proposed SymMIM. Our fine-tuning setup closely follows that of BEIT [2]. Specifically:\nFor ViT-Small, fine-tuning for total 200 epochs with a warm-up of 5 epochs and a layer decay of 0.8.\nFor ViT-Base fine-tuning for total 100 epochs with a warm-up of 20 epochs and a layer decay of 0.65.\nFor ViT-Large fine-tuning for total 50 epochs with a warm-up of 5 epochs and a layer decay of 0.75.\nFor fair comparison, we only compare with previous SOTA that fine-tuning on the same input image size as 224 x 224 pixels. Our observations reveal that models with the proposed symmetric masking strategy consistently surpass previous approaches. Comparing with previous SOTA such as SimMIM [43], ConMIM [44], MAE [24], SymMIM outperforms with a significant margin. However, SymMIM lacks behind the ConMIM\u2020 [44] which fine-tuning with higher resolution of 384 x 384.\nFor further evaluation, we record the results for ViT-Small and ViT-Large in 2 and 3, respectively. It clearly shows that SymMIM outperforms previous SOTA methods when using ViT-Small and ViT-Base as a backbone and is on par with MAE [24] when using ViT-Large. Notably, although ConMIM [44] achieves higher performance compared with our proposed SymMIM, they fine-tune the model on a larger resolution input of 384 \u00d7 384."}, {"title": "4.2 Object Detection and Instance Segmentation", "content": "For object detection and instance segmentation, we conduct an end-to-end fine-tuning of Mask R-CNN [26] on the COCO dataset [32]. Our approach involves replacing most attention blocks in the ViT backbone with windowed blocks to mitigate the quadratic complexity associated with self-attention. However, we retain four global blocks for cross-window interaction. Additionally, we evenly integrate four up-and-down-sample modules with the ViT architecture to generate the pyramid features required by FPN."}, {"title": "5 Ablations", "content": ""}, {"title": "5.1 Analysis of learning objective", "content": "In this section, we experiment with each additional learning objective in the total loss for the ablation study. We progressively test with different sets of loss terms from 4 and report the resulting accuracy on ImageNet-1K datasets. The results, detailed in 6 show an optimum when using the final objective in 4. Interestingly, we also see that contrastive objective in 3 contributes the most toward the overall performance."}, {"title": "5.2 Masking ratio probing", "content": "In this section, we conduct the ablation study on the masking ratio choices. Figure 2 shows the influence of the masking ratio towards the fine-tuning performance. The accuracy fluctuates and is not stable for different methods and ratios. This behavior shows that for different MIM methods and different datasets, we need to perform the probing again to fine the optimal ratio. In contrast to that, our proposed method, SymMIM, using only one ratio of 50% for both small"}, {"title": "5.3 Visualization", "content": "In this section, we delve into comprehending the proposed approach and critical designs through visualizations. All the example images are sourced from the ImageNet-1K validation set.\n3 illustrates the recovered images using various human-designed masks. These masks serve to reveal the capabilities learned through masked image modeling. Specifically, the human-designed masks include a random mask, a very small size mask, a small size mask, and a large size symmetric mask.\nAs we can see, the recovered image using a large size symmetric mask preserves much more global information, while the small and very small size masks are better at recovering local information. Therefore, the model trained with our proposed method, which utilizes both large and small size masks is more robust."}, {"title": "6 Conclusion", "content": "In this study, we introduce a straightforward yet impactful symmetric masking strategy that simplifies hyperparameter tuning and masking ratio exploration. We then present SymMIM, a novel masked image modeling framework that leverages this symmetric masking approach. SymMIM combines reconstruction loss and contrastive loss to enhance representation learning effectively. Despite its simplicity, SymMIM achieves remarkable performance across various benchmarks, setting a new state-of-the-art record. Notably, our proposed SymMIM significantly accelerates the training process\u2014requiring only a single pre-training step\u2014while other methods necessitate linear probing nine to ten times."}]}