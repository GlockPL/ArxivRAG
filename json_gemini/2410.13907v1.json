{"title": "NSMARK: Null Space Based Black-box Watermarking Defense Framework\nfor Pre-trained Language Models", "authors": ["Haodong Zhao", "Jinming Hu", "Peixuan Li", "Fangqi Li", "Jinrui Sha", "Peixuan Chen", "Zhuosheng Zhang", "Gongshen Liu"], "abstract": "Pre-trained language models (PLMs) have\nemerged as critical intellectual property (IP)\nassets that necessitate protection. Although\nvarious watermarking strategies have been\nproposed, they remain vulnerable to Linear\nFunctionality Equivalence Attack (LFEA),\nwhich can invalidate most existing white-box\nwatermarks without prior knowledge of\nthe watermarking scheme or training data.\nThis paper further analyzes and extends the\nattack scenarios of LFEA to the commonly\nemployed black-box settings for PLMs by\nconsidering Last-Layer outputs (dubbed\nLL-LFEA). We discover that the null space\nof the output matrix remains invariant against\nLL-LFEA attacks. Based on this finding, we\npropose NSMARK, a task-agnostic, black-box\nwatermarking scheme capable of resisting\nLL-LFEA attacks. NSMARK consists of three\nphases: (i) watermark generation using the\ndigital signature of the owner, enhanced by\nspread spectrum modulation for increased\nrobustness; (ii) watermark embedding through\nan output mapping extractor that preserves\nPLM performance while maximizing water-\nmark capacity; (iii) watermark verification,\nassessed by extraction rate and null space\nconformity. Extensive experiments on both\npre-training and downstream tasks con-\nfirm the effectiveness, reliability, fidelity, and\nrobustness of our approach. Code is available at\nhttps://github.com/dongdongzhaoUP/NSmark.", "sections": [{"title": "Introduction", "content": "During past decades, with the rapid development\nof artificial intelligence (AI), pre-trained language\nmodels (PLMs) have achieved superior perfor-\nmance and been applied in a wide range of fields.\nAt the same time, training high-performance PLMS\nrequires a large amount of data and computing re-\nsources, thus PLMs can be regarded as valuable\nintellectual property (IP). With the deployment\nand application of machine learning as a service\n(MLaaS) platforms, companies sell well-trained\nPLMs as commodities. Once sold models are ille-\ngally stolen, distributed or resold, the rights of the\nmodel owner will be seriously infringed. Therefore,\nit is necessary to protect IP of PLMs.\nWatermarking techniques have been widely ap-\nplied for protecting IP of deep learning models\n(Chen et al., 2024; He et al., 2024). By incorporat-\ning identifiable information, these methods serve\nas a means to verify model ownership and pro-\nvide proof of authenticity. Existing schemes can be\ndivided into white-box and black-box categories ac-\ncording to whether the model parameters need to be\naccessed in verification. Among them, black-box\nschemes are more applicable when model param-\neters are inaccessible in most real-life scenarios,\nsuch as deployment as API.\nHowever, protecting the IP of PLMs through wa-\ntermarking presents significant challenges. Since\nPLMs can be deployed for various downstream\ntasks post-training (Zhang et al., 2023), it is cru-\ncial for watermark schemes to be task-independent.\nBesides, recent studies have revealed vulnerabil-\nities and shortcomings in existing watermarking\ntechniques (Li et al., 2023a). The proposed Lin-\near Functionality Equivalence Attack (LFEA) is"}, {"title": "Related Work", "content": "Watermarking for PLMs. With the rise of pre-\ntraining in NLP, recent work has explored water-\nmarking specific to PLMs. BadPre (Jia et al., 2022)\nintroduces a task-agnostic backdoor attack only for\nMLM-based PLMs. Hufu (Xu et al., 2024) intro-\nduces a modality-agnostic approach for pre-trained\nTransformer models using the permutation equivari-\nance property. Explanation as a Watermark (Shao\net al., 2024) addresses the limitations of backdoor-\nbased techniques by embedding multi-bit water-\nmarks into feature attributions using explainable\nAI. (Shen et al., 2021; Zhang et al., 2023) propose\ntask-agnostic backdoor attacks by assigning high-\ndimensional vectors as trigger set labels, but their\neffectiveness is sensitive to downstream classifier\ninitialization. (Wang and Kerschbaum, 2021) intro-\nduced an auxiliary neural network for watermark\nembedding using weights from the main network.\n(Wu et al., 2022) proposed a task-agnostic embed-\nding loss function, but didn't consider the need for\ntriggers to reflect the model owner's identity. (Cong\net al., 2022) introduced a black-box watermarking\nscheme for PLMs, but its applicability is limited\ndue to the discrete nature of word tokens. Unfortu-\nnately, these schemes are vulnerable to attacks by\nLFEA or LL-LFEA in principle.\nWatermark Removal Attacks. DNN watermark-\ning faces various removal attempts. Common meth-\nods include fine-tuning (Adi et al., 2018) and prun-\ning (Han et al., 2015). Fine-pruning (Liu et al.,\n2018) combines these approaches for increased ef-\nfectiveness. Knowledge Distillation (Hinton, 2015)\ntechniques can also inadvertently remove water-\nmarks while reducing model size. (Lukas et al.,\n2022) propose a new attack method called Neuron\nReordering to swap neurons within the same hidden\nlayer of a DNN to disrupt embedded watermarks\nin the model's parameters. (Li et al., 2023a) intro-\nduce a powerful LFEA for white-box watermarks,\napplying linear transformations to model parame-\nters, effectively destroying embedded watermarks\nwhile preserving the model's original functionality.\nFraud attacks include overwriting (Wang and Ker-\nschbaum, 2019) and ambiguity attacks (Zhu et al.,\n2020) also pose a great threat to watermarks."}, {"title": "Method", "content": "3.1 Threat Model\nIn white-box watermarking schemes, high-\ndimensional model parameters are often used as\nwatermark information. For PLMs, since the out-\nput of the last layer is a high-dimensional vector,\nwe can use a method similar to the white-box water-\nmarking scheme to embed watermarks in the out-\nput. However, embedding identity information into\nthe high-dimensional output vector will face the\nthreat of LFEA-like attacks, which is proposed to\neasy to conduct and can invalidate most existing\nwhite-box watermarks without knowledge of the\nwatermarking scheme or the training data by ex-\nploiting linear invariance. Considering Last-Layer\noutputs, we further analyze and expand the attack\nscenarios black-box settings using model outputs\n(dubbed LL-LFEA).\nTo address these problems, we first explore the\ncharacteristics of the model output. We find that\nthe null space of the matrix composed of the out-\nput vectors of model is invariant under LL-LFEA.\nBased on this, we propose a new null space verifi-\ncation method that can resist LL-LFEA attack.\nThis method uses a new metric, the Null Space\nMatching Degree (NSMD). NSMD measures the\ndegree of match between the output matrix of the\nsuspicious model and the null space of the pro-\ntected PLM. Finally, we propose NSMARK, a null\nspace based task-agnostic black-box watermarking\nscheme for PLMs. NSMARK uses identity infor-\nmation to generate all watermark-related elements,\nand uses WER and NSMD to verify the watermark.\nSpread spectrum modulation technology and an\nextra extractor are also introduced to enhance the\nwatermark performance.\nOur contributions are summarized as follows:\n(i) We analyze the threat of LFEA on output-\nbased watermark, and propose LL-LFEA, which\ncan destroy the watermark embedded in the out-\nput vector without affecting the performance of\ndownstream tasks.\n(ii) We find that the null space of the matrix\ncomposed of the output vectors of model is invari-\nant under LL-LFEA and thus propose a new null\nspace verification method NSMARK that can resist\nLL-LFEA. Notably, NSMARK is task-agnostic that\nuses both new null space verification and signature\nverification to resist LL-LFEA.\n(iii) We conduct comprehensive experiments by\napplying NSMARK to various models of both pre-\ntraining and downstream tasks. Experimental re-\nsults demonstrate the effectiveness, fidelity, relia-\nbility and robustness of NSMARK."}, {"title": "Null Space Verification Theory", "content": "LL-LFEA performs linear transformation on out-\nput vector of PLM and can destroy the watermark\nembedded in it, which means previous watermark\nverification methods will be significantly affected.\nWe find that the null space of the matrix composed\nof the output vector is invariant under the LL-LFEA\nattack, and propose to use the null space matching\ndegree to verify whether the model is embedded\nwith watermarks.\nTheorem 1. Before and after LL-LFEA, the null\nspace of the output matrix of PLM remains un-"}, {"title": "Null Space Match Degree (NSMD)", "content": "We define NSMD by introducing the distribution of\nelements in a matrix, which is obtained by matrix\nmultiplication of the output matrix $A$ of any PLM\nwithout watermark and the null space matrix $N$\nof $f_{wm}$. In $H_{(n \\times p)} = A_{(n \\times m)} \\times N_{(m \\times p)}$, $H_{i,j} =$\n$a_i \\cdot B_j$ is the dot product of the i-th row vector of $A$\nand the j-th column vector of $N$. We define NSMD\nof $A$ and $N$ as:\n$\\text{NSMD}(A, N) = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{p} \\sqrt{|H_{i,j}|}$                                                                                                                                                                                                                                                                            (1)\nFurthermore, we give a detailed analysis of es-\ntimation of NSMD (in Appendix A.2). For exam-\nple, if n = 768 and p = 1500, we have NSMD\n> 27.48. If N is the null space matrix of A, NSMD\nis a minimum value close to 0. This difference is\namplified by the process of calculating the square\nroot, resulting in a significant difference between\nwhether A and N are matched. We use this differ-\nence to distinguish whether the model is embedded\nwith a watermark."}, {"title": "Overall Framework of NSMARK", "content": "As shown in Figure 3, NSMARK scheme includes\nthree modules: watermark generation, watermark\nembedding, and watermark verification:"}, {"title": "Watermark Generation", "content": "The workflow of watermark generation is shown\nin Algorithm 1. We hope that the generated wa-"}, {"title": "Watermark Embedding", "content": "Before the training starts, make a copy of $f_{wm}$\nas the frozen reference model $f_{ref}$. Then use the\nclean data set $D$ and the trigger set $D_T$ to train\n$f_{wm}$ and the extractor $E$. When taking ${D, D_T}$\nas input, $f_{wm}$ will output ${V, V_T}$ and $f_{ref}$ will\noutput ${V_{ref}, V_{ref}^T}$, respectively. For $V_T$, $E$\nmaps it to obtain the signature $\\text{sig}_{wm}$, and for\n${V, V_{ref}, V_{ref}^T}$, $E$ maps them to random vectors.\nAfter the training is completed, $f_{wm}$ is embedded\nwith watermark. Then using $D_v$ as input, the out-\nput vectors are concatenated into a matrix $A$, and\nthe corresponding null space matrix $N$ of $A$ is\ncalculated as part of the key.\nThree networks are involved in watermark em-\nbedding: the model $f_{wm}$ to be embedded with wa-\ntermark, the reference model $f_{ref}$, and the extractor\nmodel $E$. Compared with directly embedding $\\text{sig}$\ninto the output vector of $f_{wm}$, adding $E$ to map\ncan reduce the side effect of watermark on the orig-\ninal performance. Watermark capacity is increased\nat the same time. We use mean square error loss\n(MSE) and similarity function $\\text{sim}$ to implement\nthe above training process:\n$L_{match} = \\frac{1}{D_T} \\sum_{x \\in D_T} \\text{MSE} (E (f_{wm} (x)), \\text{sig}_{sm}),$                                                                                                                                                                                                                                                                                                                                                                                                            (2)\n$L_{random} = \\frac{1}{D} \\sum_{x \\in D} \\text{sim} (E (f_{wm}(x)), \\text{sig}_{sm})^2$\n$+ \\frac{1}{D_T} \\sum_{x \\in D_T} \\text{sim} (E (f_{ref}(x)), \\text{sig}_{sm})^2$                                                                                                                                                                                                                                                                                                                                                                                           (3)"}, {"title": "Watermark Verification", "content": "To better defend against possible attacks, NSMARK\nuses two metrics together to verify the ownership:\nWatermark Extracting Rate (WER) and NSMD.\nModel owner needs to submit key = (sig, E, \u039d)\nto Certification Authority (CA). CA generates\n$t$, $D_v$, $sm$ using $sig$. Input $D_v$ to the suspicious\nmodel $f_{susp}$ to get the output vector $A_{susp}$, and\npass $A_{susp}$ through $E$ to get the mapped vector\n$O_{susp}$. Then WER is calculated after despread\nspectrum. Despread spectrum is the inverse pro-\ncess of spread spectrum (detailed process in Ap-\npendix A.5.2). At last the signature is extracted as\n$\\text{sig}' = {a | a \\in {-1,0,1}, i \\in [0, n - 1]}$. WER\nis defined as the proportion of sig and sig' having\nthe same value:\n$\\text{WER} = \\frac{\\sum_{i=0}^{n-1}[a_i = a_i]}{n}.$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (4)\nwhere $[.]$ is Iverson bracket, which is 1 when the\nexpression in the bracket is True, otherwise it is 0.\nNSMD is calculated using $A_{susp}$ and N by Equa-\ntion 1. We define two thresholds, and whether\nWER> $T_w$ will be firstly verified. If it fails,\nwhether NSMD< $T_N$ will be further considered in\ncase of LL-LFEA."}, {"title": "Experiments", "content": "4.1 Experimental Setup\nDatasets. We use WikiText-2 (Merity et al., 2017)\nfor pre-training and watermark embedding. To\nevaluate the performance on downstream tasks, we\nselect many text classification datasets: SST-2 and\nSST-5 (Socher et al., 2013) for sentiment analysis,\nLingspam (Sakkis et al., 2003) for spam detection,\nOffensEval (Zampieri et al., 2019) for offensive\nlanguage identification, and AG News (Zhang et al.,\n2015) for news classification.\nModels. For PLMs, we use the base versions of\nBERT (Kenton and Toutanova, 2019), ROBERTa\n(Liu, 2019), DeBERTa (He et al., 2020) and XL-\nNet (Yang, 2019). All pre-trained weights are from\nHuggingFace\u00b9. The extractor network is a three-\nlayer linear network with hidden layers of 2048\nand 1024 neurons. The input dimension matches\nthe output dimension of the PLM. The output di-\nmension matches the size of sigsm.\nWatermark settings and training details. We se-\nlect a string containing owner information as the\nmessage m, e.g., \"BERT is proposed by Google\nin 2018\". The length of sig is 256 and is spread-\nspectrum with a spreading factor k = 3, resulting\nin a 768-bit sigsm. We use the labeled downstream\ndataset SST-2 as the candidate pool DNS. q, the\nlength of Dy, is 1500. The trigger is inserted to\nrandom positions for 5 times in trigger set. When\nperforming watermark embedding, $\\lambda_1$ = 0.5 and\n$\\lambda_2$ = 0.2 in LExtractor and Lwm. Batchsize is 4,\nand learning rates for both fwm and E are 10\u20134.\nfwm and E are trained alternately for 10 epochs.\nWhen fine-tuning on downstream tasks, the learn-\ning rate is 2 \u00d7 10-5 and batchsize is 8 for 3 epochs.\nMetrics. As mentioned before, We define two\nmetrics to verify the model's identity: WER and\nNSMD. Besides, we adopt accuracy (ACC, in %) to\nmeasure the performance of PLM on downstream\ntasks."}, {"title": "Performance Evaluation", "content": "4.2.1 Effectiveness\nEffectiveness means that the watermark can\nachieve expected effect in verification. Ideally, sig'\nextracted from the watermarked model should be\nconsistent with the original sig, and the output\nmatrix of fum for Dy should completely match\nN stored in the key, which means WER = 1 and\nNSMD = 0.  This\nproves the effectiveness of NSMARK.  Compar-\ning the values of fum and fclean shows that WER\nand NSMD will change obviously after the water-\nmark is embedded.  Although NSMD of different\nPLMs is different in value, they are all far from\n0.  Through these results, we can preliminarily de-\nfine verification thresholds of WER and NSMD\nas $T_w$= 0.6 and $T_N$ = 43, which are 0.6\u00d7 the\naverage gaps.  Thresholds can be further adjusted\naccording to different models and task types."}, {"title": "Reliability", "content": "The watermark key is a triple key = (sig, E, \u039d).\nNext, we analyze whether the watermark can be\nsuccessfully verified if an attacker provides an in-\ncorrect key.\nWrong signature sig. The trigger t and the output\nof fum are related to sig, but as sig and t are\nnot a one-to-one mapping relationship, there are\nsituations where only one of sig and t is correct.\nFigure 5 shows all possible scenarios.\nFor fum, (1) when trigger is wrong (tw) and sig-\nnature is correct (sigc), WER = 0, NSMD > TN.\nThis means that fum has learned the relationship\nbetween sig and t. Whether t is correct deter-\nmines whether fwm can produce the expected out-\nput, which in turn affects both the calculation of\nWER and NSMD. (2) When trigger is correct (tc)\nand signature is wrong (considering the most dan-\ngerous scenario sigw only consists of {\u22121,1}),\nWER \u2248 0.5. This is because te leads to the right\nsig', and its expectation of WER with a random\n{-1,1} string is 0.5. As the output matrix is cor-\nrectly generated by fum based on te, NSMD = 0\nin this case. (3) When both trigger and signature\nare wrong (tw and sigw), WER = 0, NSMD > TN,\nindicating that the watermark cannot be correctly\nverified without providing the correct key. (4) For\nmodel without embedded watermarks fclean, wa-\ntermarks cannot be extracted even if the correct key\nis provided.\nWrong extractor E. Since E is not involved in the"}, {"title": "Fidelity", "content": "We hope that NSMARK does not affect the per-\nformance on original tasks. Thus we add a down-\nstream network to fwm, and fine-tune the whole\nmodel Fwm with downstream dataset. Fclean with-\nout watermark is fine-tuned as baseline.  shows that watermark has almost no impact on the\nperformance of the model on the original task."}, {"title": "Defense against LL-LFEA", "content": "Defense against LL-LFEA. When designing NS-\nMARK, we focus on resisting LL-LFEA, and pro-\npose null space verification using NSMD."}, {"title": "Robustness", "content": "The robustness of watermark refers to whether the\nwatermark can be effectively verified after water-\nmark removal attacks. Next, we will analyze the\nrobustness of NSMARK against fine-tuning, prun-\ning, fine-pruning and overwriting attacks.\nRobustness against fine-tuning."}, {"title": "Further Analysis", "content": "Next we discuss the necessity of using the verifica-\ntion set with trigger to calculate NSMD. In \u00a7 3.4.1\nof watermark generation, we introduce the selec-"}, {"title": "Conclusion", "content": "This paper proposes NSMARK, a black-box water-\nmark framework for ownership verification using\nthe output of PLM. We first analyze and introduce\nLL-LFEA, and propose a solution that can use null\nspace invariance for watermark verification. We\nconduct an overall design from three aspects: wa-\ntermark generation, watermark embedding, and\nwatermark verification. Two indicators, WER and\nNSMD, are used to jointly verify the existence and\nidentity of the watermark. Experiments prove the\neffectiveness, reliability and fidelity of NSMARK,\nand it has satisfactory performance under various\nattacks. With the cooperation of two verification\nmethods, it works a secure and robust watermark-\ning scheme."}, {"title": "Limitations", "content": "Our approach has limitations in two main aspects.\nFirst, our method only focuses on LMs as we use\nthe vocab to build connection between sig and trig-\nger. Solutions applicable to other models (such as\nvisual or multi-modal models) need further explo-\nration. Second, further research is needed on this\nbasis to further protect the downstream network."}, {"title": "Proofs of Null Space Verification Theory", "content": "Proof. The null space N(A) of matrix $A_{(a \\times b)}$ is\nthe set of all b-dimensional vectors $x$ that satisfy\n$Ax = 0$ (Axler, 2015). That is, N(A) = {x \u2208\n$\\mathbb{R}^b, Ax = 0$}. Using $f_{wm}$ to denote PLM em-\nbedded with watermark, assuming $A_{1(n \\times m)}$ =\n{$f_{wm}(x), x \\in D_T$} is the matrix concatenated\nfrom the output vectors, where $D_T$ is the verifi-\ncation dataset with watermark trigger, m is the size\nof $D_T$ and n is the dimension of the output vector\nof the last layer of PLM. Let the null space matrix\nof $A_1$ be $N_1$, then $A_1 \\times N_1 = 0$.\nAfter performing LL-LFEA, assuming the new\noutput matrix of $f_{wm}(D_T)$ is $A_{2(n \\times m)}$, according\nto Sec.3.1, we have $A_2 = Q \\times A_1$. Then $A_2 \\times N_1 =$\n$(Q \\times A_1) \\times N_1 = Q \\times (A_1 \\times N_1) = 0$, which means\n$N_1$ belongs to the null space matrix of $A_2$. As $Q$ is\na reversible matrix, then $\\text{rank}(A_1) = \\text{rank}(A_2)$,\nand the null space of $A_1$ and $A_2$ have the same\ndimension. It can be concluded that $N_1$ is also the\nnull space matrix of $A_2$."}, {"title": "Estimation of NSMD", "content": "We define NSMD by introducing the distribution\nof elements in a matrix, which is obtained by ma-\ntrix multiplication of the output matrix $A$ of any\nPLM without watermark and the null space ma-\ntrix $N$ of $f_{wm}$. In $H_{(n \\times p)} = A_{(n \\times m)} \\times N_{(m \\times p)}$,\n$H_{i,j} = a_i \\cdot B_j$ is the dot product of the i-th row\nvector of A and the j-th column vector of N. It\nis proposed that the approximate distribution of\nthe angle between n random uniformly distributed\nunit vectors in space $\\mathbb{R}^m$ (Cai et al., 2013). In\nspace $\\mathbb{R}^m$, given two random vectors uniformly\ndistributed on the unit sphere, the angle $\\theta$ between\nthe two random vectors converges to a distribution\nwhose probability density function is:\n$f(\\theta) = \\frac{1}{\\sqrt{\\pi}} \\frac{\\Gamma(\\frac{m}{2})}{\\Gamma(\\frac{m-1}{2})} (\\sin \\theta)^{m-2}, \\theta \\in [0, \\pi].$                                                                                                                                                                                                                                                                                                                        (5)\nWhen $m = 2$, $f(\\theta)$ is uniformly distributed on\n$[0, \\pi]$; when $m > 2$, $f(\\theta)$ has a single peak at\n$\\theta = \\frac{\\pi}{2}$. When $m > 5$, the distribution of $f(\\theta)$ is\nvery close to the normal distribution. Most of the\n$C_2$ angles formed by m randomly uniformly dis-\ntributed unit vectors are concentrated around $\\frac{\\pi}{2}$, and\nthis clustering will enhance with the increase of the\ndimension m, because if $\\theta \\neq \\frac{\\pi}{2}$, then $(sin \\theta)^{m-2}$\nwill converge to 0 faster. This shows that in high-\ndimensional space, two randomly selected vectors\nare almost orthogonal.\nWe further derive the distribution of the dot prod-\nuct of two random vectors uniformly distributed\nand independently selected on the unit ball in space\n$\\mathbb{R}^m$. Let $\\alpha$ and $\\beta$ be unit vectors and $\\theta$ be the"}, {"title": "Trigger Generation Algorithm", "content": "The trigger generation algorithm is shown in\nAlgorithm 1."}, {"title": "Select Algorithm", "content": "The Select() algorithm is shown in Algorithm 2."}, {"title": "Process of Spread Spectrum Modulation\nand Despread Spectrum", "content": "A.5.1 Spread Spectrum Modulation\nAssume that the digital signature $sig$ is n bits,\n$sig = {a_i|a_i \\in {-1,1}, i \\in [0, n-1]}$, and set the\nspreading factor to k. Expand $sig$ horizontally by\nk times to obtain $sig_{repeat}$ = {$r_{aj}$|$r_{aj} = a_i, i =$\n$j \\mod n, r_{aj} \\in {-1,1}, j \\in [0, k \\times n - 1]$}. In-\nput $sig$ as a seed into the pseudo-random generator\nto obtain the key $sm = {b_j|b_j \\in {-1,1}, j \\in$\n$[0, k \\times n - 1]}$ for spread spectrum modulation.\nUse $sm$ to modulate $sig_{repeat}$ to obtain the spread\nspectrum modulated digital signature $sig_{sm}$ =\n{$s_{aj}$|$s_{aj} = r_{aj} \\times b_j, j \\in [0,k \\times n - 1]$}."}, {"title": "Despread Spectrum", "content": "Let the mapping vector output by E be O =\n{$o_j, j \\in [0, k \\times n - 1]$}, modulate it with $sm =$\n{$b_j|b_j \\in {-1,1}, j \\in [0,k \\times n - 1]$} to get\n$O_{repeat}$ = {$r_{oj}$|$r_{oj} = o_j/b_j, j \\in [0, k \\times n - 1]$},\nthen quantify $O_{repeat}$ to get $O_{quan}$ = {$q_{0j}$|$q_{0j} \\in$\n${-1,0,1}, j \\in [0, k \\times n - 1]$}. At last the sig-\nnature is extracted by counting the number of\nn positions that appears most often in k copies."}, {"title": "Defense against LL-LFEA+finetuning", "content": "After applying LL-LFEA, the attacker may add a\nnetwork to fLL-LFEA and fine-tune it for down-\nstream tasks. We hope the model after the LL-\nLFEA+fine-tuning attack can still maintain the wa-\ntermark."}, {"title": "Robustness against Fine-pruning", "content": ""}, {"title": "Feature Visualization", "content": "To demonstrate the effectiveness of our scheme, we\nuse t-SNE to visualize the feature distribution of\nthe watermarking model. As shown in Figure 9,\nthe input with trigger and the input without trigger"}]}