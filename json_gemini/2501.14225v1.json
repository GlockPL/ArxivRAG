{"title": "Multi-agent KTO: Reinforcing Strategic\nInteractions of Large Language Model in Language\nGame", "authors": ["Rong Ye", "Yongxin Zhang", "Yikai Zhang", "Haoyu Kuang", "Zhongyu Wei", "Peng Sun"], "abstract": "Achieving Artificial General Intelligence (AGI) requires AI agents that can not only make\nstrategic decisions but also engage in flexible and meaningful communication. Inspired\nby Wittgenstein's language game theory in Philosophical Investigations, we propose that\nlanguage agents can learn through in-context interaction rather than traditional multi-stage\nframeworks that separate decision-making from language expression. Using Werewolf, a\nsocial deduction game that tests language understanding, strategic interaction, and adaptabil-\nity, we develop the Multi-agent Kahneman & Tversky's Optimization (MaKTO). MaKTO\nengages diverse models in extensive gameplay to generate unpaired desirable and unac-\nceptable responses, then employs KTO to refine the model's decision-making process. In\n9-player Werewolf games, MaKTO achieves a 61% average win rate across various models,\noutperforming GPT-40 and two-stage RL agents by relative improvements of 23.0% and\n10.9%, respectively. Notably, MaKTO also demonstrates human-like performance, win-\nning 60% against expert players and showing only 49% detectability in Turing-style blind\ntests. These results showcase MaKTO's superior decision-making, strategic adaptation, and\nnatural language generation in complex social deduction games.", "sections": [{"title": "1 Introduction", "content": "Building language agents capable of both decision-making and dialogue represents a crucial\npathway toward Artificial General Intelligence (AGI) [1\u20134]. This pursuit necessitates a deep\nunderstanding of the intrinsic relationship between language and intelligence. Wittgenstein's\nLanguage Game Theory offers a profound insight: language derives its meaning through\nthe interplay of linguistic expressions and non-linguistic actions, governed by rules that\nemerge from our shared forms of life, where all these elements are interwoven into what he\nterms language games (Fig. 1(c)) [5, 6]. In this language theory framework, language itself\nconstitutes a form of action, with meaning emerging through practical use. This perspective\nrepresents a significant departure from his earlier work in Tractatus Logico-Philosophicus [7],\nwhere he viewed language as a rigid logical picture of reality (Fig. 1(a)). This theoretical\nperspective points to the value of grounding AI development in practical language use and\nauthentic interactive contexts [8\u201312].\nSocial deduction games emerge as an ideal testbed for validating these theoretical prin-\nciples. These games create self-contained language-game ecosystems that serve as excellent\nresearch and testing platforms [13\u201323]. They require multiple capabilities: 1) fundamental\nlinguistic skills, including natural language understanding, text generation, and hidden infor-\nmation inference; 2) strategic social interaction, including consistent role-playing and strategic\nplanning for competition and cooperation; and 3) adaptability, including strategy adjustment\nand error correction during gameplay. Furthermore, these games offer quantifiable metrics,\nsuch as completion and win rates, enabling systematic comparison between human and AI per-\nformance. The Werewolf/Mafia game (a brief introduction and game rules are in Appendix A),\nas a popular and typical social deduction game, exemplifies these characteristics, making it a\nchallenging test-bed for AI agent [13, 17, 24\u201326].\nCurrent approaches to building AI agents in teoften decouple language processing from\ndecision-making [25, 27\u201329] (as schematically shown in Fig. 1(b)), echoing the limitations\nof the picture theory in Fig. 1(a)\u2014 where complex social dynamics are oversimplified into\nrigid representations. For example, Wu et al. [25] applied RL policy for decision-making or\nintention generation, then followed by a large language model (LLM) for dialogue generation\nin Werewolf game. However, we find that that they compressed the language space into\nstructured facts as input for the decision module, which limited the system's generalization\ncapability in decision-making. This multi-staged framework also makes it difficult to transfer\nstrategies across different game environments and may reduce the success rate of cooperation\nwhen different types of agents are introduced.\nWittgenstein's language game theory emphasizes the inseparability of language and action,\nwhere meaning emerges from specific contextual use and purpose. Motivated by this, we\nintroduce Multi-agent Kahneman & Tversky's Optimization (Multi-agent KTO, MaKTO)\ntraining method. Unlike existing approaches where decision-making and language generation\nmodules are trained separately, we enable language models to learn through direct interactions\nwith different agents, embodying the core principle of Language Game Theory \u2013 the unified\nlanguage-action principle. Specifically, in addition to the MaKTO, our approach consists of\ntwo key components, as illustrated in Fig. 2. First, we implement behavior cloning (BC) using\nthree distinct data sources: (1) fundamental game comprehension data that explains specialized\ngame terminology, (2) advanced gaming techniques compiled from game strategy guides,\nand (3) authentic gaming behavior from our self-built dataset. This expert-annotated dataset,"}, {"title": "2 Results", "content": "In this section, we first demonstrate our approach's performance in 9-player Seer-Witch-Guard\nWerewolf games (detailed introduction and game rule are in Appendix A), including both\ninter-agent tournament evaluations and human-AI competitions (Sec. 2.1). Sec. 2.2 presents\nthe results of a Turing-style detectability test where human players attempt to distinguish\nbetween AI and human players. We then conduct the behavioral analysis of the model to\nexplain why MaKTO outperforms baseline models in competitive scenarios (Sec. 2.3). In"}, {"title": "2.1 Tournament Winning Rate Evaluation", "content": ""}, {"title": "2.1.1 Inter-agent Tournament Evaluations", "content": "Experimental Setup and Baselines. We evaluate our approach against several strong\nbaselines: API-based LLM agents (GPT-40, GPT-40-mini, and Claude-3.5-Sonnet\nusing chain-of-thought prompting [37]), a Mix agent combining LLM with RL policy for deci-\nsion making following [25], and SFT models (based on Qwen2.5-14b/72b-instruct)\ntrained on our expert-annotated dataset. Our experiments include both head-to-head competi-\ntion and random competition including randomly assigned roles from the models described\nabove.\nHead-to-head Competition. In head-to-head competitions, where one model controls the\nentire villager team (6 agents) and another controls the werewolf team (3 agents), MaKTO-72b\nachieved a 61% average win rate across 100 games (50 games per faction), significantly out-\nperforming all baselines (Table 1). Notably, while the Mix agent showed strong performance\nas villagers, it struggled as werewolves due to overly aggressive strategies and policy contra-\ndictions (Fig. 3). When comparing SFT models of different sizes (14B and 72B), we observed\nsimilar win rates but significantly fewer factual hallucinations in the 72B model's generated\nspeeches, indicating that win rates alone don't fully reflect model performance.\nRandom Competition. In random competitions with diverse role assignments across 260\ngames, MaKTO-72b achieved the highest TrueSkill rating [38] (Fig. 4). This format better\nreflects the model's adaptability across different roles and team compositions. Particularly,\nMaKTO-72b significantly outperformed GPT-40 when playing as the Seer, suggesting more"}, {"title": "2.1.2 Human-AI Tournament Evaluations", "content": "Experimental Setup. To evaluate our model's real-world applicability, we conducted com-\nprehensive human-AI interaction tests with 14 experienced human players (each with over\n1,000 games of experience), assessing both performance and human-likeness through three\ncomplementary evaluations. Unlike previous studies [24, 25] that only introduced single AI or"}, {"title": "2.2 Turing-style Detectability Test", "content": "We conduct rigorous Turing-style blind detectability tests in both competitions. We require\neach human player to explicitly judge whether every other participant is human or AI, without\nany prior knowledge of AI presence. This mandatory assessment provides a rigorous human\nsimilarity assessment. MaKTO achieves recognition accuracy of only 48.9% (Fig. 6), lower\nthan random chance, indicating that our model successfully passes this specialized Turing\ntest by convincingly emulating human-like gameplay characteristics and social behaviors. On\nthe contrary, GPT-40 has a much higher recognition accuracy (76.6%) due to the significant\ndifferences in speaking style and voting behavior from real people."}, {"title": "2.3 Behavioral Analysis", "content": ""}, {"title": "2.3.1 Comparison with Baseline Model", "content": "As shown in Figure 3, MaKTO outperformed SFT models in both werewolf and villager win\nrates. In order to understand why MaKTO has a higher win rate in tournaments, we analyze\nthe behaviors and decisions generated during the tournament. In our experiments, we chose\nGPT-40 as an opponent. For a fair comparison, we choose GPT-40 as the opponent.\nWhen the trained models played as villagers against GPT-40 werewolves, we evaluate\nvoting accuracy (Vote Acc.) and abstention rate (Abstention). For special roles, we examine:\nSeer's werewolf identification accuracy (Werewolf Check), Witch's first-night rescue rate\n(Save @ Night 1), werewolf poisoning accuracy (Correct Poison), villager mispoison rate\n(Mispoison), and Guard's special role protection rate (Protect God) and werewolf protection\nerrors (Misprotect). Table 3 shows that the MaKTO outperforms the SFT model across all\nthese metrics, resulting in a higher villager win rate. This improvement can be attributed to\nour stepwise decision rewards and penalties from the MaKTO training."}, {"title": "2.3.2 Comparison with Human", "content": "A distinctive feature of the Werewolf game lies in its prevalence of deception, particularly\namong the werewolf players. Werewolves never openly admit their true identity during game-\nplay; instead, they make up various identities and stories to protect themselves. For the villagers,\nit is necessary to correctly predict who is a werewolf and thus vote correctly. In this experiment,\nwe measure these and compare them to human.\nWe separate 51 matches from the annotated data as the test set. We ensure that the data in\nthe test data are not included in the instruction dataset. We primarily measured the model's\nperformance on 484 voting events and 5130 identity predictions. The results are presented in\nTable 5. In terms of voting, we principally evaluate the voting accuracy rate, that is, the accuracy\nof gods and villagers voting for werewolves; and the abstention rate. MaKTO-72b achieves\nthe highest voting accuracy. In terms of identity prediction, we evaluate the accuracy of side\nalignment, that is, correctly predicting gods and villagers as the good identity and werewolves\nas the bad identity; as well as the F1-score in predicting werewolves. The SFT series and\nMaKTO model achieve a higher alignment accuracy than human annotators. The trained\nmodels show significant improvement over the base model like Qwen2.5-14b-instruct\nand -72b-instruct [39], which demonstrate the effectiveness of the expert data we\ncollected."}, {"title": "2.4 Generalizing to Other Game Setup", "content": "Another advantage of our model lies in its cross-game generalization capability. We conduct\ntournament experiments in the Seer-Witch-Hunter setup, which introduces a new role - the\nHunter. The hunter can only launch the skill when eliminated either by Werewolves or through\nvoting. Upon elimination, he can choose to either shoot another alive player or conceal his\nidentity and leave the game quietly."}, {"title": "2.5 Ablation Studies", "content": "Ablation studies revealed two crucial design choices in MaKTO. First, stepwise preference\nselection proved superior to trajectory-based selection (Table 7), as the latter fails to distinguish\nbeneficial actions within losing trajectories and vice versa. This confirms our hypothesis that\ngame outcomes alone cannot accurately reflect the quality of individual decisions in complex\nsocial deduction games. Second, multi-agent play significantly outperformed self-play KTO,\nachieving a 4% higher average win rate. While self-play KTO showed competitive performance"}, {"title": "3 Methods", "content": "In this section, we describe in detail our training method (Fig. 2)in detail, including expert\ndata collection, behavior cloning, and multi-agent KTO."}, {"title": "3.1 Expert Data Collection", "content": "Despite the implementation of meticulously designed prompting methods [13, 18, 41, 42],\nLarge Language Models (LLMs) have failed to authentically emulate the role of Werewolf\ngame players, particularly in terms of linguistic style and strategic gameplay. To more accu-\nrately simulate human speech patterns and to instill appropriate game logic within LLMs, we\nrecruit a cohort of Werewolf game experts, including individuals with over a thousand games\nof experience and competitive tournament participants, to annotate our dataset.\nWhile the Automatic Speech Recognition (ASR) method enables the acquisition of sub-\nstantial data from online Werewolf games by converting audio to text [25], the informal\nnature of spoken communication and the absence of explicit reasoning processes underlying\ndecision-making remain as challenges. Therefore, we require advanced players to articulate\ntheir thoughts in textual format and document distinct thought processes while maintaining\nstandardization and authenticity of the Werewolf game. Specifically, our annotated dataset\nencompasses game regular record and thinking process data:\nGame regular record: this section of data encompasses the nighttime action records of\nspecial-role villagers and werewolves, daytime speeches of players, daytime votes of players,\nas well as the game review after its conclusion.\nThinking process data: action, speech, and voting constitute the core elements of the\nWerewolf game. We instruct expert players to annotate distinct thinking processes for each of\nthese three components.\n\u2022 Action: this section primarily annotates the rationale behind night actions, such as the\nseer's intention behind checking a certain player, or the werewolf's motive for killing.\n\u2022 Speech: this section primarily summarizes the content expressed after the statements,\nincluding the identity accorded to other players during the daytime speech and the call\nfor the vote. It can be regarded as an outline of the daytime speech.\n\u2022 Voting: this section annotates the detailed reasons for voting and makes predictions about\nthe identity of other players during the voting; players are also requested to distill the\nday's events into a consolidated record, create notes, and formulate a rudimentary strategy\nfor the next game phase.\n17 expert players provide annotations using our self-built annotation platform. We collect\n331 Werewolf games for training, including 278 9-player games (Seer, Witch, Guard or Hunter)\nand 53 7-player games (Seer, Guard or Witch). The specific rules for different game settings\nare provided in Appendix A. Each game features randomly involved participants and randomly\nassigned Werewolf roles to guarantee data diversity. The total duration of the game annotated\nby the players exceeds 1,000 hours, including 3,759 speech data entries (exceeding 540,000\ntokens), 2,698 action events, and 3,875 voting records.\nWe also leave 51 games with voting records and role predictions for offline evaluating the\nbasic ability of LLM in Section 2.3."}, {"title": "3.2 Behavior Cloning", "content": "Due to the scarcity of high-quality Werewolf game data, LLMs generally lack a profound\nunderstanding of the game and do not possess sufficient reasoning logic to support advanced\ngameplay. To address this limitation, we constructe a comprehensive, multi-level instruction\ndataset to supervised fine-tune (SFT) the model, as illustrated in Fig. 2(a). This approach"}, {"title": "3.3 Multi-agent KTO", "content": "Although the SFT training enhances the model's comprehensive understanding of the Werewolf\ngame, it faces two major challenges: the characteristics of the game and limitations in human-\nannotated data. In Werewolf, the impact of a single player's actions on the outcome is often\nsubtle, and a faction's success doesn't guarantee that every member made smart choices. For\nexample, the Seer's persuasive speech or the Witch's critical poisoning sway the situation.\nAdditionally, even annotated data from experts inevitably contains decisions of varying quality,\nbecause it includes both winners and losers. These factors collectively make it difficult to\naccurately assess the quality of individual decisions based on win-loss outcomes, which in"}, {"title": "Kahneman-Tversky Optimization.", "content": "We argue that KTO is particularly suitable for such\nmulti-agent dialogue game scenarios like Werewolf. 1) Multi-agent environments are more\ncomplex than single/two-agent scenarios, where single-agent interactions with the environment\noften yield clear feedback, while interactions between agents can have countless possibili-\nties. Moreover, multi-agent dialogues have huge action spaces, leading to sparse trajectory\nsampling. This makes online reinforcement learning algorithms slow to converge and hard\nto train. KTO, similar to offline RL, offers a viable solution. 2) Unlike preference opti-\nmization algorithms such as DPO [31] and its variants [32\u201336], it's challenging to generate\n\"prompt-chosen-reject\" paired data in the Werewolf game. In one interaction, you can\nonly determine whether an action is acceptable through game rules and feedback from other\nagents. KTO, not requiring paired data for training, thus makes it ideal for this \"You only try\nonce\" scenario in multi-agent preference optimization.\nWe assign desirable and undesirable behaviors manually and use KTO to update the model.\nGiven the prompt-response from dataset (x, y) \u2208 D, KTO uses the following loss to optimize\nthe policy \u03c0\u03c1:\n$r_\\theta(x, y) = \\log \\frac{\\pi_\\rho(y|x)}{\\pi_{ref}(y|x)}$\n$\\mathcal{v}(x,y) = \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[KL(\\pi_\\theta(y|x)||\\pi_{ref}(y|x))]$\n$L(\\pi_\\rho; \\mathcal{D}) = \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[-\\lambda_y\\sigma(\\beta(r_\\theta(x, y) - z_0)), if y \\sim \\mathcal{Y}_{desirable}|X \\\\     -\\lambda_y\\sigma(\\beta(z_0 - r_\\theta(x, y))), if y \\sim \\mathcal{Y}_{undesirable}|X]$\nTherefore,\n$\\mathcal{L}(\\pi_\\rho; \\mathcal{D}) = \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[ \\lambda_y - \\mathcal{v}(x, y)]$\nHere, \u03bbd and \u03bb\u03c5 are hyperparameters for the desirable and undesirable losses, respectively.\nThe parameter \u03bb\u1ef7 represents \u03bb\u0189 when y is desirable and \u03bb\u03c5 when y is undesirable."}, {"title": "Multi-agent Gameplay.", "content": "We find that in multi-agent environments, simple \"self-play\u201d might\nlead to fixed thinking patterns and loss of generalization ability (experimental analysis in\nSection 2.1.1). For example, we find that although the model performs well\nin scenarios with two players claiming to be the seer, its performance significantly deteriorates\nwhen only one or more than three players claim this role. This lack of robustness is inadequate\nfor the game's requirements. We argue that in multi-agent environments, the diversity of peers\nand opponents is crucial. Therefore, instead of relying on self-play, we employ a multi-agent\nplay approach. As shown in Fig. 2(b), we create a diverse model pool comprising SFT models\nbased on various base models like Llama3.1 and Qwen2.5 with different model sizes, a\nrange of off-the-shelf LLMs (such as GPT-3.5 and GPT-40, Claude etc.), as well as the"}, {"title": "Stepwise Preference Data Selection.", "content": "Defining all actions in an agent's trajectory as\ndesirable or not based solely on the faction's win/loss result is overly simplistic. Fortunately,\nWerewolf's alternating day-night gameplay allows for a more nuanced selection of the desirable\nand unacceptable step-wise process policies. As shown in Fig. 2(b) and Table 10, we employ\nthree methods:\n\u2022 Heuristic-based selection identifies actions based on game rules and common strategies\nfor different roles (werewolves, seers, villagers, etc.) at the game stages, focusing primarily\non nighttime actions and voting phases. For example, werewolves targeting special roles is\ndesirable (special roles have unique abilities that can disadvantage werewolves), while not\nattacking is unacceptable. For the seer, correctly identifying a werewolf is favorable, and for\nthe witch, successfully poisoning a werewolf is desirable, whereas poisoning a teammate\nis unacceptable. For voting, unified voting of the villagers against werewolves is preferred,\nwhile infighting or vote-splitting, thus weakening the villager team's position, is discouraged.\n\u2022 Staged voting-based selection evaluates the quality of model-generated speeches during the\ndiscussion phase based on voting results. In Werewolf, voting results in each phase serve as\nan excellent reward signal. Intuitively, if an agent is voted out, their speech that day was likely\nsuboptimal: if accused of being a werewolf, their speech might have revealed their identity or\nfailed to defend themselves effectively, thus losing others' trust. If they're an innocent voted\nout, it often means their speech contradicted others' observations or contained hallucinations\ngenerated. An agent receiving no votes suggests their speech is safe. Given the disguises and\ndeceptions, speeches from special role players (seer, witch) are particularly crucial. Thus, the\ndefinition of unacceptable speeches for these roles is stricter. For instance, a seer receiving\nmore than half of the villagers' votes would be considered an unacceptable policy.\n\u2022 Verifier-based selection evaluates whether the model-generated speeches align with known\nfacts in the game by introducing additional strong off-the-shelf LLMs, such as Claude\nor GPT-40. This approach addresses hallucination issues in our SFT model. The verifier\nperforms fact consistency checks, e.g., whether a player's speech contradicts their role and\nknown game events. It also validates logical coherence to detect whether the generated\ncontent contains self-contradictory statements or not."}, {"title": "3.4 Training Details", "content": "Given that our collected expert training data is based on Chinese, and considering a\nstronger understanding of the Chinese context, we choose Qwen2.5-14b-instruct\nand Qwen2.5-72b-instruct [39] as the base models for training. The SFT dataset\ncomprises 25k samples, including 380 samples of fundamental game comprehension data\nwith terminology explanations, 372 Q&As on advanced gaming techniques, 12k annotated\nauthentic gaming behavior data, and 12k general SFT corpus. We employed DeepSpeed\nZeRO-3 optimization with a learning rate of le-6, a warm-up ratio of 0.05, and trained for\n3 epochs. For the MaKTO phase, we collected 20k preference data entries from the Seer-\nWitch-Guard games, consisting of 12k desirable and 8k unacceptable samples. The model"}, {"title": "4 Discussion", "content": "Contributions and Framework Generalization. The contribution of our work lies in the\nproposed Multi-agent KTO, which demonstrates a successful approach to training language\nmodels for complex multi-agent interactions. While previous studies primarily focused on\nagent-agent interactions [13, 15, 20, 24, 25] that may deviate from realistic human participation,\nour framework shows remarkable performance in both agent-agent tournaments and human-AI\ninteractions, exhibiting capabilities in both cooperation and competition while maintaining\nhuman-like characteristics. We chose the Werewolf game as our testbed, on one hand, because\nit is a perfect testing ground for validating Wittgenstein's language game theory, and on the\nother hand due to the game itself, which, compared to general role-playing games [43], it\nprovides quantitative metrics for performance evaluation through win rates and behavioral"}, {"title": "Limitations and Future Directions.", "content": "Our work has several limitations to address in future\nresearch. First, our current implementation relies on turn-based conversations rather than\nfree-form interactions [44\u201347]. The challenge of modeling unrestricted multi-agent communi-\ncations, where agents can interact more naturally and flexibly, remains an important area for\nfuture research. Second, similar to the general limitations of LLMs, our model occasionally\nexhibits inconsistent behavior and hallucinations across long conversations, suggesting room\nfor improvement in long-text modeling capabilities, particularly in maintaining coherence dur-\ning extended social interactions. Finally, while Multi-agent KTO provides an easy yet effective\ntraining paradigm, it essentially operates as an offline learning method. We believe that online\nreinforcement learning in multi-agent scenarios could potentially achieve higher performance\nceilings, presenting another promising direction for future investigation."}, {"title": "Ethical Considerations.", "content": "While deception in social deduction games is a game mechanic,\ntraining AI models to master such behaviors raises ethical considerations. Our model's ability\nto detect and employ strategic deception in Werewolf demonstrates advanced social reasoning\ncapabilities. However, this also highlights the potential for LLMs to learn sophisticated\ndeceptive behaviors, albeit in a controlled gaming environment. We emphasize that these\ncapabilities are specifically developed within the context of social deception games, where\n\"deception\" is an accepted part of gameplay, similar to bluffing in poker. Such game-specific\nbluffing behaviors are fundamentally different from real-world deception, and we should\nensure these capabilities remain confined to appropriate gaming contexts."}, {"title": "5 Conclusion", "content": "In this paper, we propose Multi-agent KTO (MaKTO), a novel approach for optimizing LLM\nin complex social deduction games without relying on pairwise data. For the experiment, we\nconducted extensive evaluations in 9-player Werewolf games, including inter-agent and human-\nAI tournament evaluation, Turing-style detectability test, behavior analysis, etc. Our results\ndemonstrate that MaKTO outperforms GPT-40 by 11.4% in average win rate and achieves a\n60% win rate against human experts, while maintaining human-like conversation with only\n49% detectability. We also create a large-scale dataset of expert Werewolf players' utterances,\nactions, and thinking processes, providing valuable insights into decision-making strategies\nand enabling effective behavior cloning and fine-tuning of large language models. This work\ncontributes to advancing AI capabilities in multi-agent, partially observable environments,\npaving the way for more sophisticated and human-like AI agents in complex social interaction\nscenarios."}, {"title": "Appendix A Games Rules", "content": "Werewolf is one of the most popular social detection games, typically played with 7 to 15\nplayers. The game is set in a village where some players are secretly assigned the role of"}, {"title": "A.1 Game Objectives", "content": "In this game, players are usually divided into two camps: werewolves and villagers. Depending\non their roles, players have distinctive objectives:\n\u2022 Villagers aim to identify the werewolves and eliminate them through voting. Within the\nvillagers' camp, there are some special roles with distinctive abilities that can help the\nvillagers secure victory.\n\u2022 Werewolves' primary objective is to conceal their true identities, mislead others in\ndiscussions to avoid being voted out, and hunt villagers as covertly as possible."}, {"title": "A.2 Game Process", "content": "The game generally includes the following basic procedures.\n\u2022 Role Assignment: Upon entering the game, player roles are secretly assigned. Werewolves\nknow each other's identities, while villagers only know their own role.\n\u2022 Day-Night Alternation: The game alternates between day and night phases. At night,\nwerewolves secretly choose a villager to eliminate; some special roles can also activate their\nabilities at night. During the day, all players discuss and vote to eliminate the player they\nbelieve to be a werewolf, with the player receiving the most votes being eliminated."}, {"title": "A.3 Role Descriptions and Different Configurations", "content": "Standard configurations for 9-player and 7-player games incorporate six distinct roles: the\nSeer, the Witch, the Guard, the Hunter, the Werewolf, and the Villager. Different roles have the\nfollowing abilities.\n\u2022 Seer: During the night phase, the Seer can secretly select a player to learn their true identity\n(whether they are a werewolf or not).\n\u2022 Witch: The Witch has one healing potion and one poison potion, each usable only once.\nThe Witch cannot use both potions in the same night. The healing potion can save a player\nkilled by werewolves at night. The poison potion can eliminate a player suspected of being a\nwerewolf.\n\u2022 Guard: The Guard can protect one player each night from werewolf attacks. The Guard can\nchoose to protect himself or opt not to protect anyone, but cannot protect the same player on\nconsecutive nights.\n\u2022 Hunter: When the Hunter is killed by werewolves or eliminated during the voting event,\nhe can reveal his identity card and shoot a revenge bullet at any living player, causing that\nplayer to die as well. The Hunter can choose not to reveal his card, but once revealed, he\nmust take someone with him (Note: If the Hunter is poisoned by the Witch, he cannot reveal\nhis card or take anyone with him).\n\u2022 Werewolf: Werewolves can choose to eliminate a player during the night phase.\n\u2022 Villager: Villagers have no special abilities. They can only distinguish the werewolves\nthrough daytime speech and public information.\nThis paper encompasses four distinct configurations, including the 9-player werewolf\ngame with three special roles, and the 7-player werewolf game with two special roles:\n\u2022 Seer-Witch-Guard: Includes one Seer, one Witch, one Guard, three Werewolves, and three\nVillagers.\n\u2022 Seer-Witch-Hunter: Includes one Seer, one Witch, one Hunter, three Werewolves, and three\nVillagers.\n\u2022 Seer-Guard: Includes one Seer, one Guard, two Werewolves, and three Villagers.\n\u2022 Seer-Witch: Includes one Seer, one Witch, two Werewolves, and three Villagers."}, {"title": "Appendix B Prompts", "content": "Game Introdution"}]}