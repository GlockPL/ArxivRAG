[{"title": "CRYOFM: A FLOW-BASED FOUNDATION MODEL FOR CRYO-EM DENSITIES", "authors": ["Yi Zhou", "Yilai Li", "Jing Yuan", "Quanquan Gu"], "abstract": "Cryo-electron microscopy (cryo-EM) is a powerful technique in structural biology and drug discovery, enabling the study of biomolecules at high resolution. Significant advancements by structural biologists using cryo-EM have led to the production of over 38,626 protein density maps at various resolutions\u00b9. However, cryo-EM data processing algorithms have yet to fully benefit from our knowledge of biomolecular density maps, with only a few recent models being data-driven but limited to specific tasks. In this study, we present CRYOFM, a foundation model designed as a generative model, learning the distribution of high-quality density maps and generalizing effectively to downstream tasks. Built on flow matching, CRYOFM is trained to accurately capture the prior distribution of biomolecular density maps. Furthermore, we introduce a flow posterior sampling method that leverages CRYOFM as a flexible prior for several downstream tasks in cryo-EM and cryo-electron tomography (cryo-ET) without the need for fine-tuning, achieving state-of-the-art performance on most tasks and demonstrating its potential as a foundational model for broader applications in these fields.", "sections": [{"title": "1 INTRODUCTION", "content": "Cryo-electron microscopy is an important technique in structural biology and drug discovery, allowing the determination of high-resolution 3D structures of biomolecules that are difficult to study through conventional methods, offering insights into molecular mechanisms and aiding in fields like drug discovery (Nogales & Scheres, 2015). A major component of cryo-EM data processing involves reconstructing 3D structures from noisy 2D projections of particles. It can be formulated as an inverse problem (Bendory et al., 2020; Singer & Sigworth, 2020), which aims at recovering the signal $x \\in R^n$ (i.e., a clean protein density) from the observation $y \\in R^m$ 2. Under this formulation, following Bayesian statistics, the objective is to sample a density from the posterior $p(x|y)$, which can be factorized to $p(x|y) \\propto p(y|x)p(x)$. Thus, the prior distribution becomes essential for guiding the reconstruction process and improving the accuracy of the resulting structures.\nScheres (2012b) first introduced a Gaussian distribution as the prior $p(x)$ in 3D reconstruction for cryo-EM, effectively functioning as a frequency-dependent low-pass filter, which has proven useful in many cases. Building on this, recent works have explored more sophisticated regularizers rather than explicitly defining a prior distribution (Punjani et al., 2020; Tegunov et al., 2021; Kimanius et al., 2021; Li et al., 2023; Schwab et al., 2024; Kimanius et al., 2024; Liu et al., 2024). While these approaches were initially developed for 3D reconstruction tasks, their methodologies are closely aligned with the methods for density map modification and post-processing (Jakobi et al., 2017; Ram\u00edrez-Aportela et al., 2020; Terwilliger et al., 2020a; Kaur et al., 2021; Sanchez-Garcia et al., 2021; He et al., 2023). Both share the common objective of improving the quality of cryo-EM maps, whether during refinement or in post-processing. Among all these approaches, a line of methods has"}, {"title": "2 RELATED WORK", "content": "In this section, we will briefly review the mostly related work on cryo-EM and diffusion/flow matching for inverse problems. A more detailed discussion on these topics can be found in Section A.\nDensity modification and denoising in cryo-EM Density modification refers to the process of using known properties of the expected density in specific regions of a map to correct errors in observed cryo-EM density maps (Terwilliger et al., 2020b). Deep learning has increasingly contributed to cryo-EM map denoising and modification, with methods divided into two categories: pretrained models and self-supervised approaches. Pretrained models, like DeepEMhancer (Sanchez-Garcia et al., 2021) and EMReady (He et al., 2023), learn the posterior $p(x|y)$ from data to recover high-frequency details. In contrast, self-supervised approaches, such as M (Tegunov et al., 2021) and spIsoNet (Liu et al., 2024), are trained on the dataset being processed, offering more robustness but take longer to process one dataset.\nMissing wedge problem in cryo-ET Cryo-electron tomography (cryo-ET) is an imaging technique used to reconstruct 3D volumes of biological specimens from 2D images captured at various tilt angles (Lu\u010di\u0107 et al., 2005). The resulting 3D volumes, known as tomograms, provide detailed views of cellular structures, with smaller regions, called subtomograms, extracted to focus on specific structures like proteins (Wan & Briggs, 2016). A key challenge in cryo-ET is the missing wedge problem, caused by the limited range of tilt angles during data acquisition, which leaves a wedge-shaped region in Fourier space without information, leading to anisotropic resolution and artifacts. Traditional approaches use signal processing and regularization techniques to address the missing wedge (Goris et al., 2012; Deng et al., 2016; Yan et al., 2019; Zhai et al., 2020), while recent deep learning methods have shown promise in tackling this issue by leveraging data-driven models (Liu et al., 2022b; Van Veen et al., 2024).\nAb initio modeling in cryo-EM Ab initio modeling in cryo-EM involves estimating the 3D structure of a protein from 2D particle images with unknown orientations (Crowther et al., 1970). One of the early approaches involved using 2D class averages, which are representative images created by aligning and averaging particles with similar views to improve the signal-to-noise ratio (SNR) (Ludtke et al., 1999; Voss et al., 2010). However, since the introduction of cryoSPARC, modern methods now use raw particle images directly for model estimation, leveraging stochastic gradient descent (SGD) to bypass the need for class averages (Punjani et al., 2017).\nDiffusion/Flow Matching for Inverse Problem Inverse problems aim to restore the original sample from a degraded observation, such as in image super-resolution (Haris et al., 2018), inpainting (Yeh et al., 2017), deblurring (Kupyn et al., 2019), etc. Denoising diffusion probablistic models (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020) and flow-based models (Lipman et al., 2022; Liu et al., 2022a) have demonstrated their effectiveness in solving inverse problems through unsupervised approaches (Kawar et al., 2022; Chung et al., 2023b; Dao et al., 2023). Since a pretrained DDPM/Flow models the data distribution (prior distribution), it can help discover the posterior distribution given a degradation model (likelihood). Specifically, Chung et al. (2022) developed diffusion posterior sampling for general inverse problems, removing the need for strong assumptions like the linearity of the degradation operator. Another line of research examines the solution to the inverse problem when the degradation operator is unknown (Chung et al., 2023a; Kapon et al., 2024)."}, {"title": "3 PRELIMINARIES", "content": null}, {"title": "3.1 FLOW MATCHING", "content": "Flow matching (Lipman et al., 2022; Liu et al., 2022a) defines the generative process of a data point $x_o \\in R^n$ by progressively transforming a sample $x_1 \\in R^n$ from the noise distribution. The process is modeled in terms of an ordinary differential equation (ODE):\n$\\frac{d x_t}{dt} = v_\\theta(t, x_t) dt,$"}, {"title": "3.2 DIFFUSION POSTERIOR SAMPLING", "content": "Diffusion posterior sampling (DPS) has emerged as a promising approach for solving inverse problems (Song et al., 2021b;a; Chung et al., 2023b). Given a measurement $y \\in R^m$ derived from $x \\in R^n$ with a degradation operator $A : R^n \\rightarrow R^m$, the goal is to sample x from the posterior $p(x|y)$. Leveraging the DDPM which learns the score of the prior distribution $\\nabla_{x_t} log p_t(x_t)$, it is straightforward to plug a likelihood term, allowing us to compute the score of the posterior:\n$\\nabla_{x_t} log p_t(x|y) = \\nabla_{x_t} log p_t(x_t) + \\nabla_{x_t} log p_t(y|x_t).$\nHowever, $log p_t(y|x_t)$ is generally intractable since the tractable likelihood $log p_0(y|x_0)$ is only defined in the data space (i.e., t = 0). This term requires marginalizing over all possible $x_0 \\in p_0(x_0)$. Chung et al. (2022) proposed to use a Laplace approximation of the likelihood term so that $p_t(y|x_t) \\approx p_0(y|x_0)$. The conditional score can thus be approximated by:\n$\\nabla_{x_t} log p_t(x|y) \\approx \\nabla_{x_t} log p_t(x_t) + \\nabla_{x_t} log p_t(y|x_0(x_t)).$\nIf we assume the observation distribution $p_0(y|x_0)$ is Gaussian, taking derivative to the log-probability will produce:\n$\\nabla_{x_t} log p(y|x_t) \\propto \\nabla_{x_t} ||y - A x_0(x_t)||^2$.\nPutting everything together, we can approximate the score of the posterior distribution by:\n$\\nabla_{x_t} log p(x_t|y) \\approx \\nabla_{x_t} log p_t(x_t) + A^T \\nabla_{x_t} ||y - A x_0(x_t)||^2,$\nwhere $A_t$ is a hyper-parameter which controls the step of the likelihood term."}, {"title": "4 CRYOFM", "content": "In this section, we present the implementation of CRYOFM. First, we introduce the pretraining dataset in Section 4.1. Next, we illustrate the architecture of the neural network in Section 4.2. Finally, we propose a posterior sampling algorithm for the downstream tasks."}, {"title": "4.1 PRETRAINING DATASET", "content": "Our training dataset consists of deposited sharpened density maps from the EMDB (wwPDB Consortium, 2023), specifically those with: 1) a reported resolution better than 3.0 \u00c5, 2) structures resolved by single-particle cryo-EM, and 3) data entries that include half-maps, ensuring that resolution estimates are based on the \"gold standard\" Fourier shell correlation (FSC) (Henderson et al., 2012). We manually curated this subset by removing exceptionally large complexes, helical structures and problematic cases through visual inspection. We also excluded density maps with side lengths greater than 576 \u00c5. This curation resulted in a total of 3479 density maps for model training and testing. The density maps in the selected subset were lowpass filtered to 1.5 \u00c5/voxel and 3 \u00c5/voxel for CRYOFM-S and CRYOFM-L, respectively. A total of 32 density maps were selected as test set and excluded from training. For CRYOFM-S, we applied random cropping to volumes of size 64\u00b3 along with random rotations for augmentation, whereas data for CRYOFM-L were center cropped to volumes of size 128\u00b3 and augmented solely with random rotations."}, {"title": "4.2 ARCHITECTURE", "content": "A major challenge in applying the vanilla Transforemr architecture to 3D density $x_t \\in R^{D\u00b3}$ is the computational complexity, which is $O(D^6)$ \u2075. CryoFM processes the 3D input data through a Transformer with a hierarchical architecture, based on HDiT (Crowson et al., 2024). As illustrated in Fig. 2, the hierarchical structure downsamples the spatial dimension at initial levels and upsamples it at final levels, implemented via PixelUnshuffle and PixelShuffle layers (Shi et al., 2016). Consequently, the spatial dimension at the middle level is significantly reduced, making the model more easily scalable. Furthermore, at the two ends of the hierarchical structure, CRYOFM employs Neighborhood Attention (NA) (Hassani et al., 2023) since a localized attention layer is able to capture local dependencies while considerably reducing the number of tokens to attend.\nAdditionally, it is important to note that we normalize the data $x_0$ before training CRYOFM to avoid large changes in the variance of model's output. We empirically sample some data from the dataset and calculate a mean value of 0.04 and a standard deviation of 0.09."}, {"title": "4.3 FLOW POSTERIOR SAMPLING", "content": "Given a vector field $v_\\theta(x_t)$ that generates the prior distribution $p_0(x_0)$, we aim to convert it to a vector field $v_\\theta(x_t|y)$ that generates the posterior $p_0(x_0|y)$. We firstly correlate the vector field in Eq. (1) with the score function $\\nabla_{x_t} log p_t(x_t)$ in that (Dao et al., 2023; Song et al., 2021b):\n$u_t(x_t) = f_t(x_t) - \\frac{1}{g_t^2} \\nabla_{x_t} log p_t(x_t),$\nwhere $f_t$ is the drift term and $g_t$ is the diffusion coefficient in the forward-time SDE\u2076. Plugging the condition into both sides, we have:\n$u_t(x_t|y) = f_t(x_t|y) - \\frac{1}{g_t^2} \\nabla_{x_t} log p_t(x_t|y) = v_t(x_t) - \\frac{1}{g_t^2} \\nabla_{x_t} log p_t(y|x_t).$\nThus, adding a likelihood term weighted by the diffusion coefficient generates a vector field that models the posterior distribution. We follow Dao et al. (2023) to set $f_t(x_t) = -\\frac{x_t}{1-t}$ and $g_t = \\frac{t}{1-t}$ for the flow trajectory defined in Eq. (2), so the conditional vector field is:\n$v_t(x_t|y) = v_t(x_t) - \\frac{t}{1-t} \\nabla_{x_t} log p_t(y|x_t).$\nSimilar to Section 3.2, we estimate the likelihood term by:\n$v_t(x_t|y) \\approx v_t(x_t) + \\frac{t}{1-t} \\lambda \\nabla_{x_t} ||y - A x_0(x_t)||^2.$\nIn practice, we incorporate some tricks to avoid numerical instability. The detailed algorithm of flow posterior sampling is explained in Alg. 1. The algorithm can be adapted for different tasks; we refer the reader to Appendix E for the additional versions."}, {"title": "5 EXPERIMENTS", "content": "Fourier Shell Correlation (FSC) is a widely used metric that compares two density maps in Fourier space (Harauz & van Heel, 1986), allowing for the assessment of alignment between the ground truth and the reconstructed map. In the experiments, to evaluate the quality of the density maps, we use three primary metrics: $FSC_{AUC}, FSC_{0.5}$, and Fail Rate (FR). Specifically, $FSC_{AUC}$ measures the overall correlation across all spatial frequencies, and $FSC_{0.5}$ represents the resolution of the reconstructed map at the standard 0.5 cutoff (Rosenthal & Henderson, 2003). The Fail Rate (FR) identifies cases where the method fails to run or produces a result that significantly deviates from the ground truth. For the FSC metrics, we only report the results for cases that did not fail. More detailed explanation can be found in Section C.1."}, {"title": "5.1 SPECTRAL NOISE DENOISING", "content": "In cryo-EM reconstruction, spectral noise is the most commonly used noise model due to its ability to capture the varying noise characteristics across different spatial frequencies, which arise from factors such as the contrast transfer function (CTF) and detector imperfections. We consider the introduction of noise in the Fourier domain, where the variance of the added Gaussian noise is frequency-dependent, with higher frequencies exhibiting greater noise variance. Given a density $\\tilde{V} \\in C^{D \\times D \\times D}$ in the Fourier domain, the degradation model $A : C^{D \\times D \\times D} \\rightarrow C^{D \\times D \\times D}$ is:\n$A(V) = V + \\epsilon,$\nwhere $\\epsilon \\in R^{D \\times D \\times D}$ is a noise volume, whose values on the spherical shell with the same radius $v$ are the same ($v$ denotes the index of a component in the frequency space):\n$\\epsilon(\\nu) \\sim N(0, \\sigma_{noise}^2(\\nu)).$"}, {"title": "5.2 ANISOTROPIC NOISE DENOISING", "content": "Anisotropic noise in cryo-EM occurs when noise distribution varies by direction, affecting some orientations more than others and leading to uneven reconstruction quality. As a result, certain orientations have lower signal-to-noise ratios than others. To approximate this degradation in a simplified man-ner, we amplify the spectral noise by a factor when the particle orientation falls within a specific range of angles, simulating the increased uncertainty in less-sampled orientations. The degradation model $A : C^{D \\times D \\times D} \\rightarrow C^{D \\times D \\times D}$ is given by:\n$A(V) = \\begin{cases}\nV + \\alpha \\epsilon, & \\text{if } \\theta_{min} \\leq \\theta < \\theta_{max} \\\\\nV + \\epsilon, & \\text{otherwise}\n\\end{cases},$\nwhere $\\epsilon \\in R^{D \\times D \\times D}$ is the same as defined in Eq. (3), and $\\alpha > 1$ is a scaler that increases the noise for specific orientation angles, reflecting the heightened uncertainty in those directions. $\\Theta_{min}$ and $\\Theta_{max}$ are the angles that controls the portion of the noise being amplified as illustrated in Fig. 4."}, {"title": "5.3 MISSING WEDGE RESTORATION", "content": "The missing wedge problem in cryo-ET arises from the limited tilt range of the electron microscope during data acquisition, causing incomplete Fourier space sampling and artifacts in the subtomograms. We simulate this effect by applying a wedge-shaped mask in the Fourier domain, removing data from unmeasured orientations. The degradation model $A : C^{D \\times D \\times D} \\rightarrow C^{D \\times D \\times D}$ is given by:\n$A(V) = \\begin{cases}\nV, & \\text{if } \\theta_{min} \\leq \\theta < \\theta_{max} \\\\\n0, & \\text{otherwise}\n\\end{cases},$\nwhere $\\theta_{min}$ and $\\theta_{max}$ are typically set to -60\u00b0 and +60\u00b0 respectively, representing the common tilt angle limits in experimental setups as illustrated in Fig. 4. For posterior sampling, we slightly modify the flow posterior sampling algorithm to Alg. 2."}, {"title": "5.4 Ab initio MODELING", "content": "Ab initio modeling in cryo-EM involves reconstructing a coarse density map from 2D particle projections, which serves as a reference for refinement. Here, we simplify the problem by focusing on generating a coarse density map from a few clean 2D projections obtained via upstream 2D classification. This approach mirrors earlier ab initio methods (Ludtke et al., 1999; Voss et al., 2010) before cryoSPARC's introduction of SGD (Punjani et al., 2017). Given K projections, there exist K degradation operators, where the k-th degradation model is $A^{(k)} : R^{D \\times D \\times D} \\rightarrow ]R^{D \\times D}$ given by:\n$A^{(k)}(V) = P(\\phi^{(k)}, V), k \\in [1,2,\u2026\u2026, K],$\nwhere $P$ is a projection operator in the real space, and $\\phi^{(k)} \\in SO(3) \\times R^2$ is the pose of the 2D projections. In this task, $\\phi^{(k)}$ is unknown and can not be pre-determined, so it must be iteratively searched to find the optimal value (Scheres, 2012a; Punjani et al., 2017; Zhong et al., 2021b), we modify the flow posterior sampling to Alg. 3. Moreover, since ab initio modeling focuses on capturing the global shape at low resolution, we use CRYOFM-L for this task."}, {"title": "5.5 ABLATION & DISCUSSION", "content": "In this section, we perform ablation studies to analyze the impact of different design choices and hyper-parameters. We briefly present some conclusions here and refer the reader to Appendix F for details.\nModerate patchifying and downsampling parameters lead to efficient training with minimal performance degradation. As shown in Fig. 8, reducing both patchifying and downsampling pa-rameters lowers the test loss, since less downsampling causes less information loss. However, the reduction becomes marginal while the training compute (Gflops) increases significantly. To achieve a balance between the performance and training cost, we set the patchifying parameter to 4 and the downsampling parameter to 1, respectively.\nData normalization enhances downstream performance across all tasks. Fig. 9 demonstrates that models trained with normalized data consistently improves the $FSC_{AUC}$ metrics compared to the"}, {"title": "6 CONCLUSION", "content": "In this study, we present CRYOFM, a flow matching-based foundation model that learns the prior distribution of high-quality cryo-EM densities. During inference, we derive the degradation operators for specific tasks, allowing protein densities to be sampled from the posterior distribution based on given observations. CRYOFM demonstrates versatility by restoring protein densities across four distinct tasks without fine-tuning, showcasing the potential of deep generative models as a power-ful prior in cryo-EM. While promising, CRYOFM has limitations: though we use real data with synthetic noise for comparison, applying the method directly to real-world noisy densities remains challenging. Additionally, our work does not address reconstructing 3D densities from raw 2D particles, but we believe CRYOFM can contribute to solving these complex tasks in future work."}, {"title": "A ADDITIONAL RELATED WORK", "content": null}, {"title": "A.1 DENSITY MODIFICATION AND DENOISING IN CRYO-EM", "content": "Traditional density modification methods often apply Fourier space weighting, utilizing frequency-dependent scaling to suppress noise and enhance signal (Jakobi et al., 2017; Ram\u00edrez-Aportela et al., 2020; Terwilliger et al., 2020a; Kaur et al., 2021). These heuristic-based approaches, like Wiener-style deblurring (Ram\u00edrez-Aportela et al., 2020), tend to rely on local resolution estimates and fil-tering strategies but may struggle with intricate structural details due to limited priors. On the deep learning side, pretrained models like Blush (Kimanius et al., 2024) further refine density maps by denoising half-maps during iterative refinement. However, they can introduce hallucinated details in lower-resolution maps. In contrast, methods such as M (Tegunov et al., 2021) and spIsoNet (Liu et al., 2024) avoid external data, with M leveraging noise2noise (Moran et al., 2020) for de-noising independent half-maps, and spIsoNet addressing anisotropic signal distributions through self-supervised learning. Though slower, these approaches tend to reduce the risk of hallucinations and offer better robustness, but their power is often limited by not being data-driven."}, {"title": "A.2 MISSING WEDGE PROBLEM IN CRYO-ET", "content": "Cryo-ET enables detailed visualization of macromolecular complexes and cellular structures in their native environments. The missing wedge problem occurs due to the physical limitation of tilt angles during data acquisition, typically restricted to (-60\u00b0, +60\u00b0), resulting in missing data in Fourier space and artifacts such as elongation along the missing axes. Traditional methods that use sig-nal processing techniques and regularization strategies (Goris et al., 2012; Deng et al., 2016; Yan et al., 2019; Zhai et al., 2020) are often based on heuristic assumptions and have limitations in fully recovering lost data. Recently, deep learning models have been applied to this problem, offering improved recovery of complex patterns in tomograms (Liu et al., 2022b; Van Veen et al., 2024). However, these methods often focus on entire tomograms, making it difficult to incorporate specific prior knowledge of protein structures, limiting their effectiveness in subtomogram reconstructions."}, {"title": "A.3 Ab initio MODELING IN CRYO-EM", "content": "Earlier approaches to ab initio modeling relied on experimental techniques, such as image tilt pairs (Radermacher et al., 1986; Leschziner & Nogales, 2006), which provided indirect pose information, or negative stain (De Carlo & Harris, 2011), which improved SNR but at the cost of high-frequency detail. Computationally, 2D class averages were commonly used as input due to their higher SNR compared to raw particles, though they had limitations, particularly in fully sampling Fourier space due to the restricted range of particle orientations (Voss et al., 2010). While cryoSPARC's SGD ap-proach improved this by working directly with raw particles (Punjani et al., 2017), certain structural features or heterogeneity observed in 2D class averages may still be lost in the final 3D reconstruc-tion, especially in challenging samples."}, {"title": "A.4 DIFFUSION/FLOW-BASED MODEL", "content": "Flow-based models (Lipman et al., 2022; Liu et al., 2022a) and denoising diffusion probablistic models (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020) are two modern deep generative mod-els. They exhibit many similarities in technical details. DDPM implements an iterative refinement process by learning to gradually denoise a sample from a normal distribution. It has achieved the state-of-the-art results on many generative tasks, including image generation (Rombach et al., 2022; Podell et al., 2023; Peebles & Xie, 2023b), video generation (Blattmann et al., 2023), and molecule generation (Yim et al., 2023b; Abramson et al., 2024; Ingraham et al., 2023; Watson et al., 2023; Wang et al., 2024), etc. Recently, diffusion models have been adopted in the cryo-EM field. Kreis et al. (2022) traverses the latent space of cryoDRGN (Zhong et al., 2021a) with a diffusion model, while Wang et al. (2024) refines structures for model building by iteratively denoising the density. Flow-based models regress a vector field that generate a disired probability path. Their simple and efficient implementation enables fast learning. These models have shown success in various do-mains, including image generation (Esser et al., 2024) and moleculer generation (Yim et al., 2023b; Bose et al., 2023). The DDPM objective can also be unified into flow-based models by converting it into a probability flow ODE (Song et al., 2021b)."}, {"title": "A.5 VISION TRANSFORMERS FOR DIFFUSION MODELS", "content": "Diffusion transformers (Peebles & Xie, 2023a) have demonstrated significant scalability and gener-ative capabilities in image-related tasks (Esser et al., 2024; Hoogeboom et al., 2023; Hatamizadeh et al., 2024; Zhou et al., 2024). In particular, HDiT (Crowson et al., 2024) leverages the inher-ent hierarchical nature of visual patterns in its model design. By integrating the characteristics of Diffusion Transformer (Peebles & Xie, 2023a) and Hourglass transformers (Nawrot et al., 2022), and employing local attention mechanisms (Hassani et al., 2023), HDiT offers an efficient model structure suitable for training the diffusion model in the data space."}, {"title": "B ADDITIONAL DETAILED INFORMATION OF CRYOFM", "content": null}, {"title": "B.1 IMPLEMENTATION DETAILS", "content": "Fig. 12 illustrate the model architectures for input dimensions of 64\u00b3 and 128\u00b3. Fig. 13 shows the details of different attention structure we used in the transformer block. Tab. 4 details the specific model parameter configurations (aligned with the naming conventions used in Crowson et al. (2024)) and the training hyperparameters.\nIn all experiments, we employed the FairseqAdam (Ott et al., 2019) optimizer with a default learning rate of 1e-4, betas set to (0.9, 0.98), and a weight decay of 0.01. A linear warm-up strategy was applied during the first 2000 steps of training."}, {"title": "B.2 LIKELIHOOD ESTIMATION", "content": "Given a data point $x \\in R^n$, we estimate its likelihood by solving a probability flow ODE (Song et al., 2021b; Lipman et al., 2022; Chen et al., 2018). We start with the continuity equation of a velocity field $v_t : R^n \\rightarrow R^n$:\n$\\frac{d}{dt} log p_t(x_t) + \\nabla \\cdot v_t(x_t) = 0.$"}, {"title": "C ADDITIONAL INFORMATION OF THE EXPERIMENTS", "content": null}, {"title": "C.1 METRICS", "content": "$FSC_{AUC}$ The area under the curve (AUC) of the FSC curve ($FSC_{AUC}$) provides an overall correlation across all spatial frequencies, offering a comprehensive view of the alignment between the ground truth and the restored density map (Harauz & van Heel, 1986).\n$FSC_{0.5}$ The FSC resolution at the 0.5 cutoff ($FSC_{0.5}$) is a standard metric in cryo-EM, indicating the resolution of the reconstructed map relative to the ground truth (Rosenthal & Henderson, 2003)."}, {"title": "D ESTIMATION OF THE DEGRADATION OPERATORS", "content": null}, {"title": "D.1 SPECTRAL NOISE POWER ESTIMATION", "content": "Given two half maps from a cryo-EM reconstruction", "are": "n$\\tilde{V_1"}, "V + \\epsilon_1, \\epsilon_1(\\nu) \\sim N(0, \\sigma_{noise}^2(\\nu)),$\n$\\tilde{V_2} = V + \\epsilon_2, \\epsilon_2(\\nu) \\sim N(0, \\sigma_{noise}^2(\\nu)).$\nThe so-called \"Gold-Standard\" Fourier Shell Correlation (GSFSC) is the FSC between two half maps, which is defined as:\n$GSFSC(\\nu) = \\frac{\\tilde{V_1}(\\nu) \\tilde{V_2}(\\nu)}{\\|\\tilde{V_1}(\\nu) \\| \\|\\tilde{V_2}(\\nu) \\|},$\nwhere $\\tilde{V} (\\nu)$ represents the component of $\\tilde{V}$ at frequency $\\nu$ (i.e., all values on a spherical shell), and $\\|\\tilde{V}(\\nu) \\|^2$ is the radial power spectrum.\nFrom the GSFSC, the signal-to-noise ratio (SNR) can be computed as (Rosenthal & Henderson, 2003):\n$SNR(\\nu) = \\frac{E[\\sigma_{signal}^2(\\nu)"]}, {"have": "n$E[\\|\\tilde{V"}, ["V(\\nu) + \\epsilon(\\nu)\\|^2"], ["sigma_{signal}^2(\\nu)"], ["sigma_{noise}^2(\\nu)"], ["tilde{V_1}(\\nu)\\|^2"], ["tilde{V_2}(\\nu)\\|^2"], ["sigma_{signal}^2(\\nu)"], ["sigma_{noise}^2(\\nu)"], ["sigma_{noise}^2(\\nu)"], {}]