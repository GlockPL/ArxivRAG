{"title": "Beyond Sequence: Impact of Geometric Context for RNA Property Prediction", "authors": ["Junjie Xu", "Artem Moskalev", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "abstract": "Accurate prediction of RNA properties, such as stability and interactions, is crucial for advancing our understanding of biological processes and developing RNA-based therapeutics. RNA structures can be represented as 1D sequences, 2D topological graphs, or 3D all-atom models, each offering different insights into its function. Existing works predominantly focus on 1D sequence-based models, which overlook the geometric context provided by 2D and 3D geometries. This study presents the first systematic evaluation of incorporating explicit 2D and 3D geometric information into RNA property prediction, considering not only performance but also real-world challenges such as limited data availability, partial labeling, sequencing noise, and computational efficiency. To this end, we introduce a newly curated set of RNA datasets with enhanced 2D and 3D structural annotations, providing a resource for model evaluation on RNA data. Our findings reveal that models with explicit geometry encoding generally outperform sequence-based models, with an average prediction RMSE reduction of around 12% across all various RNA tasks and excelling in low-data and partial labeling regimes, underscoring the value of explicitly incorporating geometric context. On the other hand, geometry-unaware sequence-based models are more robust under sequencing noise but often require around 2 - 5\u00d7 training data to match the performance of geometry-aware models. Our study offers further insights into the trade-offs between different RNA representations in practical applications and addresses a significant gap in evaluating deep learning models for RNA tasks.", "sections": [{"title": "1 Introduction", "content": "RNA plays a central role in the machinery of life, serving as a crucial intermediary between nucleotide and amino acid worlds [39]. Beyond its messenger role, RNA is involved in diverse biological processes, including gene regulation, catalytic activity, and structural support within ribosomes [44, 50]. This versatility makes RNA a key target for fundamental biological research and therapeutic interventions. As our understanding of RNA complexity grows, so does the need for advanced computational tools for its analysis.\nModeling RNA is challenging due to its intricate secondary and tertiary structures, dynamic conformational changes, and interactions with cellular components [19]. Furthermore, RNA analysis is hindered by the practical challenges of RNA data acquisition which include sequencing errors [33], batch effects [53], incomplete sequencing [2], partial labeling [60], and high costs of obtaining"}, {"title": "2 Datasets and Models", "content": "Here, we discuss datasets selected for our study with RNA-level prediction labels. These datasets are selected to vary from small to large-scale and to encompass both nucleotide-level tasks and sequence-level tasks. We perform an extensive evaluation across these datasets, leveraging three different model families (1D, 2D, 3D) spanning 9 representative models in total."}, {"title": "2.1 Datasets", "content": "The datasets vary in size based on the number of sequences and sequence lengths: the small dataset Tc-Riboswitches [17], the medium datasets Open Vaccine COVID-19 [60] and Ribonanza-2k [22], and the large dataset Fungal [62]. All datasets provide regression labels. Detailed statistics for these datasets are provided in Appendix C.1.\n1. Tc-Riboswitches: 355 mRNA sequences (67-73 nucleotides) with sequence-level labels for tetracycline-dependent riboswitch switching behavior, important for optimizing gene regulation in synthetic biology and gene therapy.\n2. Open Vaccine COVID-19: 4,082 RNA sequences (each of 107 nucleotides) with nucleotide-level degradation rate labels, crucial for predicting RNA stability in mRNA vaccine development.\n3. Ribonanza-2k: 2,260 RNA sequences (each of 120 nucleotides) with nucleotide-level experimental reactivity labels, supporting RNA structure modeling and RNA-based drug design.\n4. Fungal: 7,056 coding and tRNA sequences (150-3,000 nucleotides) from 450 fungal species, used for sequence-level protein expression prediction."}, {"title": "2.2 Data Preprocessing and Curation", "content": "For the OpenVaccine COVID-19 dataset, we filter out sequences with a signal-to-noise ratio (SNR) below 1, as recommended by the dataset authors [60], to ensure that only sequences with a significant signal relative to background noise are included, thereby enhancing the reliability of modeling. For the other datasets, we use the original sequences since no SNR annotations are available.\nSince all the RNA datasets come with sequences only, we employ EternaFold [59] and RhoFold [45] to infer 2D and 3D molecular structures respectively. We selected EternaFold and RhoFold due to their state-of-the-art performances acknowledged in recent works [58, 60, 22] Additionally, RhoFold typically runs in seconds to a minute per sequence, unlike other 3D structure prediction tools which usually take hours, and hence not suitable for large datasets.\nFor 1D modeling, we use the original RNA sequences without structural augmentation which equates to processing a plain string of nucleotides. The 2D datasets represent each RNA sequence as a graph with nodes for nucleotides and edges for bonds between nucleotides. The node features are six-dimensional, incorporating one-hot nucleotide identity (\u2018A\u2019, \u2018C\u2019, \u2018G', \u2018U') alongside the sum and mean base-pairing probabilities (BPP), which are available from 2D structure prediction tools. In 3D, each RNA molecule is represented as a graph, with nodes corresponding to individual atoms. Node features represent one-hot atom identity."}, {"title": "2.3 Models", "content": "We select well-established model architectures recognized for their state-of-the-art performance for molecular property prediction tasks in various domains. 1D Model: Transformer1D [24, 20]; 2D Models: GCN [27, 61], GAT [55, 67], ChebNet [11, 28], Transformer1D2D [21], Graph Transformer [47, 31], and GraphGPS [38, 70]; 3D Models: SchNet [42, 18] and EGNN [40].\n1. Transformer1D: The Transformer1D model is a standard Transformer architecture adopted for RNA sequence processing. It includes an embedding layer to convert input tokens into dense vectors, positional encoding (PE) to retain sequence order, and a multi-layer Transformer encoder to capture long-range dependencies within the sequence.\n2. Transformer1D2D: An adaptation of Transformer1D that integrates sequence and 2D graph structure information. The model encodes each nucleotide and incorporates BPP features, combining standard Transformer with positional encoding and a convolutional layer on the graph adjacency matrix. This convolutional output is added to the attention matrix, enabling the model to capture both sequential and structural dependencies.\n3. Graph Convolutional Network (GCN): A basic model in graph learning that aggregates and processes node features from local neighborhoods to capture both node characteristics and graph structure, making it effective for tasks like node classification.\n4. Graph Attention Network (GAT): Enhances graph convolutions by assigning different importance to neighboring nodes through local attention mechanisms, allowing the model to focus on more relevant nodes during feature aggregation.\n5. ChebNet: A spectral GNN that utilizes Chebyshev polynomials to approximate the graph Laplacian, enabling graph convolutions with global structural context. This approach allows ChebNet to approximate global graph features with lower computational complexity.\n6. Graph Transformer: This model uses Laplacian positional encoding to integrate structural information from the graph's Laplacian into node features, which are then processed by Transformer layers. This enables aligning the sequential nature of transformer layers with graph topology.\n7. GraphGPS: A hybrid model combining GNNs with transformers to capture both local and global graph information. It uses GNNs for local feature aggregation and transformers for long-range dependencies, making it effective for complex graph tasks requiring both local and global context.\n8. SchNet: An SE(3)-invariant network designed for molecular property prediction on geometric graphs of atomic systems. It operates by modeling interactions through continuous-filter convolutional layers. Since the continuous-filter in Schnet is conditioned on distance features, it maintains invariance to rotations and translations of atom coordinates.\n9. E(n)-Equivariant Graph Neural Network (EGNN): An equivariant network for geometric graphs with rotations, translations, and reflections symmetry. The EGNN operates by as a non-linear message passing between scalar-invariant and vector equivariant quantities.\nTraining and evaluation All models were trained on a NVIDIA A100 GPU. To ensure hyper-parameter parity for each baseline, hyperparameters were optimized using Optuna [1], restricting the search to models with fewer than 10 million parameters that fit within the GPU memory constraint of 80GB. All model hyperparameters, training, and evaluation details are reported in Appendix E. We ran all models for 5 random data splits (train:val:test split of 70:15:15) and we report average performance with a standard deviation across splits. The mean column-wise root mean squared error (MCRMSE), introduced in [58], is used as the evaluation metric. It is defined as\n $MCRMSE(f, D) = \\sqrt{\\frac{1}{|D|}\\sum_{i=1}^{|D|} (\\hat{y}_i - y_i)^2}$, where f represents a model, D is the dataset, and $\\hat{y}_i$ and $y_i$ are the predicted and true values for data point i."}, {"title": "3 Task Definitions", "content": "Here, we introduce the downstream tasks for evaluating models for RNA property prediction. Each task is designed to quantify specific behaviors under various real-world experimental conditions."}, {"title": "Task 1: Impact of structural information on prediction performance", "content": "This task aims to evaluate how incorporating RNA structural information affects prediction quality. We compare the performance of models using 1D (sequence-only), 2D, and 3D RNA representations to determine if and to what extent geometric data improves property prediction."}, {"title": "Task 2: Model efficiency in limited training data settings", "content": "Acquiring high-quality comprehensive RNA datasets is often challenging and resource-intensive thus limiting the amount of labeled data for training [51, 7]. This task aims to investigate how model performance depends on the amount of training data used, evaluating the sample efficiency of each family of models. In other words, given a dataset D = {X,Y}, let Da = {Xa, Ya} be a subset where the training set is reduced to a fraction \u03b1. We train models on different sets of Da datasets with decreasing \u03b1."}, {"title": "Task 3: Performance with partial sequence labeling", "content": "Due to the high cost of measuring properties for every nucleotide in RNA sequence, real-world datasets often contain partial annotations [60] where labels are only available for the first small part of the sequence. This task is relevant for nucleotide-level datasets and it aims to investigate how well a model can generalize to a whole RNA sequence when labels are only available for a portion of it."}, {"title": "Task 4: Robustness to sequencing noise", "content": "Acquiring RNA data requires sequencing. In practice sequencing procedures may introduce sequencing errors (random mutations of nucleotides) that vary depending on the sequencing technology and platform [33, 14]. These errors affect the raw sequence data and propagate to structural noise in 2D and 3D. The goal of this task is to assess how well models can maintain reliable performance when trained and tested under the same distribution of realistic levels of sequencing noise observed in practice, ensuring robustness across a consistent noise environment. This reflects real-world cases where a specific sequencing method produces noisy data, but the noise characteristics are stable across training and deployment."}, {"title": "Task 5: Generalization to Out-of-Distribution (OOD) data", "content": "This task focuses on a different practical challenge: models trained on high-quality RNA sequences are often deployed in conditions where the data exhibits different noise characteristics due to batch effect [53] or the use of different sequencing platforms [52]. Here, the objective is to evaluate how well models generalize to OOD datasets with different levels of sequencing noise, assessing the extent of performance degradation as noise levels increase. This task simulates the scenario where a model encounters noisier data than it was trained on, highlighting its ability to adapt to unexpected experimental conditions."}, {"title": "4 Experiments and Results", "content": null}, {"title": "4.1 Impact of explicit geometry learning on model performance", "content": "We begin by addressing Task 1, where we compare the performance of model families when trained and evaluated on the downstream RNA datasets. Additionally, we provide runtime and memory comparison in Appendix B.\n2D models consistently outperform 1D model Results in Table 1 reveal that 2D methods consistently outperform the 1D sequence model across all datasets. Notably, the Transformer1D2D model, which simply augments the attention matrix with adjacency features alongside, achieves around 10% lower prediction MCRMSE on average across datasets than its geometry-free counterpart. This suggests that explicitly incorporating structural information is crucial, as learning from sequence data alone proves to be insufficient. Further experiments, detailed in the Appendix C.4, investigate the learned attention maps of both the Transformer1D2D and the Transformer1D model and their correlation with structural information and reveal that Transformer1D2D attention maps are much more closely aligned with the topological structure of nucleotide graph, reinforcing the conclusion that explicit encoding of structural information is essential for improved performance.\nSpectral GNN outperforms spatial GNNs in 2D ChebNet, a spectral method, outperforms spatial methods such as GCN, GAT, Graph Transformer, and GraphGPS, achieving a prediction MCRMSE 2.5% lower than the next best 2D model across datasets. Spatial GNNs aggregate node"}, {"title": "4.2 Model efficiency under limited data and partial sequence labeling", "content": "In this section, we combine the analysis of Tasks 2 and 3, assessing model performance in scenarios with limited training data or partial labels.\nTo analyze how the amount of training data influences model performance, we run experiments with varying portions of the full datasets (25%, 50%, 75%, and 100%) on the medium- and large-scale datasets: COVID, Ribonanza-2k, and Fungal (Appendix Fig. 8(a) for illustration). The small size of the Tc-Riboswitches dataset is excluded from the analysis, as training with lower ratios would have resulted in inadequate sample sizes for meaningful evaluation. Additionally, GPU memory constraints prevent the application of Transformer1D2D and 3D models on the Fungal dataset due to its large sequence length.\nWe also evaluate the impact of partial property labels for nucleotide-level tasks, a common occurrence owing to costly experimental measurements [58] to identify which models are best suited to handle the challenges of incomplete labels in RNA property prediction. For this, we use the COVID and Ribonanza datasets as these datasets contain nucleotide-level labels. We train the models using all training data but with varying proportions of labeled nucleotides (20%, 40%, 60%, 80%, and 100%) per sequence, thus simulating incomplete or sparse labeling, while testing on fully labeled test sets (Appendix Fig. 8(b) for illustration)."}, {"title": "More training data improves performance", "content": "Unsurprisingly, across all models and datasets, a clear trend emerges: increasing the amount of available data, whether through higher number of training data points or greater proportion of available labels leads to improved performance (Fig. 2, Fig. 3, Appendix Tables in Sec. F.1, F.2). However, the degree of performance improvement varies significantly between model types, as analyzed next."}, {"title": "2D models excel in low data and partial label regimes", "content": "Evaluating model performance at different training and label ratios reveals a notable trend: 2D models consistently outperform 1D and 3D models under low data and incomplete labeling regimes. Between 20-50% training data and label levels, 2D models such as ChebNet, Transformer1D2D, GraphGPS, and GAT significantly outperform Transformer1D, highlighting the role of additional structural information for model sample efficiency. Interestingly, Transformer1D and Transformer1D2D exhibit a faster rate of improvement when more labels are available (Fig. 3), suggesting that transformer-based architectures benefit from denser supervision. Notably, Transformer1D requires 2 \u2013 5\u00d7 more training data/labels to match the performance of the least effective 2D models, which achieve comparable results using only 20% to 50% of the training data needed by Transformer1D when trained on the full dataset."}, {"title": "3D models outperform 1D model in limited data regime despite structural noise", "content": "For the medium-scale datasets (COVID and Ribonanza), where 3D models can be evaluated, we observe that the 3D models generally outperform or are on par with the Transformer1D, even for lower data and labeling regimes. This suggests that despite the noise introduced by inaccuracies in 3D structure predictions, the explicit geometric encoding in 3D models still provides an advantage over 1D models. EGNN, in particular, is consistently better than or on par with Transformer1D across all training and label ratios. This further emphasizes that models incorporating explicit geometric encoding (whether 2D or 3D) are more data-efficient than those relying solely on sequence information. However, it is important to note that 3D models do not match the efficiency of 2D models in these scenarios, likely due to their susceptibility to noise as discussed in Section 4.1."}, {"title": "4.3 Model Robustness and Generalization Under Sequencing Noise", "content": "Tasks 4 and 5 both deal with model performance with noise in the data, but focus on different aspects, robustness to noise, and ability to generalize across unseen noise distributions. As explained in Sec. 3, sequencing noise is common depending on the sequencing method and platform used [14], thereby introducing errors in sequences that propagate into 2D and 3D structures. Additionally one of the deployment scenarios involves models trained on high-quality clean data applied for datasets acquired under noisy conditions owing to different sequencing platforms or experimental batch effects [52].\nTo explore these practical aspects, we design two sets of experiments:"}, {"title": "5 Conclusion", "content": "We present the first comprehensive study on the benefits and challenges of the effect of geometric context for RNA property prediction models. By providing a curated set of RNA datasets with annotated 2D and 3D structures, we systematically evaluate the performance of 1D, 2D, and 3D models under various real-world conditions, such as limited data, partial labeling, sequencing errors, and out-of-distribution generalization. Our results reveal that 2D models outperform 1D and 3D models, with spectral graph neural networks excelling even in low-data and partial labeling scenarios. For 3D models, we find that their potential benefits are hindered by the limited receptive field, computational complexity, and structural noise from RNA structure prediction tools. At the same time, 1D models demonstrate better robustness compared to 2D and 3D models in noisy and OOD conditions. This study highlights the value and limitations of using geometric context for RNA modeling. Future work could focus on ensembling 1D, 2D, and 3D models for complementary strengths, and on improving 2D and 3D models to better handle noise from structure prediction tools."}, {"title": "Appendix Overview", "content": "\u2022 Appendix A: Related Work.\n\u2022 Appendix B: Memory and Computational Constraints.\n\u2022 Appendix C: Additional Experimental Information.\n\u2022 Appendix D: Analysis of Noise in 3D Structures.\n\u2022 Appendix E: Reproduction.\n\u2022 Appendix F: Additional Results."}, {"title": "A Related Work", "content": "RNA property prediction RNA-specific models remain scarce, likely due to limited specialized datasets. Recent advancements in sequence modeling have shown promise, particularly with foundation models like RNA-FM [8], UTRBERT [66], and RINALMO [35], which use transformer-based architectures pre-trained on large RNA sequence corpora to predict various RNA functions and structures. While RNNs and CNNs have been applied to tasks like RNA methylation and protein binding [56], they struggle with long-range dependencies. Hybrid models like RNAdeg-former [21], combining convolutional layers with self-attention, improve predictions by capturing both local and global dependencies. Although some efforts integrate 2D structures with transform-ers, explicit 2D and 3D geometric modeling for RNA remains underexplored, with graph-based models mainly focusing on RNA-protein and RNA-drug interaction tasks rather than property pre-diction [29, 65, 4, 69].\nRNA structure prediction RNA 2D structure prediction has progressed from dynamic program-ming methods like Vienna RNAfold [23] to deep learning-based tools like SPOT-RNA2 [48] and UFold [16], which enhance accuracy by using neural networks and evolutionary data. Models such as E2Efold [9] and RNA-FM [8] employ transformer architectures to achieve state-of-the-art results in secondary structure prediction.\nRNA 3D structure prediction has progressed through ab initio, template-based, and deep learning approaches. Ab initio methods (e.g., iFoldRNA [43], SimRNA [6]) balance detail and efficiency but struggle with non-canonical interactions. Template-based models (e.g., FARNA/FARFAR [10], 3dRNA [68]) depend on existing structures but are limited by available data. Deep learning models like DeepFoldRNA [34], RhoFold [45], RoseTTAFoldNA [5], and trRosettaRNA [57] show promise in predicting 3D structures from sequence data but face challenges with novel RNA families due to RNA's conformational flexibility [30].\nDespite these advances, there is a gap in applying 2D and 3D modeling techniques to RNA property prediction. Most works focus on 1D representations and overlook the potential of geometric infor-mation from 2D and 3D structures. This study is the first to systematically explore the benefits and limitations of incorporating explicit structural data in deep learning-based RNA property prediction."}, {"title": "B Memory and Computational Constraints", "content": "In this section, we compare the models based on run times and GPU memory. Both Transformer1D2D and 3D models (even with nucleotide pooling) encounter out-of-memory (OOM) is-sues when processing longer sequences, such as those in the Fungal dataset (Table 1). This high-lights the need for optimization to handle longer sequences. Figure 7 shows that Transformer1D scales poorly in both runtime and memory due to its expensive attention mechanism, and Trans-former1D2D faces additional challenges by processing the sequence and adjacency matrix simulta-neously. In contrast, simpler 2D models like GCN, GAT, and ChebNet are more efficient. 3D models also scale poorly with sequence length due to the increasing number of atoms. Overall, 2D models provide a good balance between computational demands and performance for encoding structural information."}, {"title": "C Additional Experimental Information", "content": null}, {"title": "C.1 Dataset Statistics", "content": "Here, we present the statistics for each dataset used in the paper in Table 2. The datasets are catego-rized as small, medium, or large based on the number of sequences and sequence length. \"Target\" refers to the task the dataset is designed to predict, and \"# Avg. Atoms\" indicates the average number of atoms used in 3D models."}, {"title": "C.2 Comparison of partial training data and partial sequence labeling", "content": "To clarify the differences and provide a more detailed explanation, we illustrate two experiments: partial training data and partial sequence labeling (Figure 8)."}, {"title": "C.3 Details of noisy experiments: robustness and generalization", "content": "To create the noisy datasets, we vary the noise ratior across the values {0.05, 0.1, 0.15, 0.2, 0.25, 0.3}. For each given noise ratio, we independently mutate the nucleotide at each position in a sequence with probability r, as illustrated in Figure 9. The resulting mutated sequence is then passed to the 2D and 3D prediction tools to generate the corresponding structures. Figure 4 gives a comprehensive illustration of getting noisy 1D, 2D, and 3D structures."}, {"title": "C.4 Analysis of Transformer1D and Transformer1D2D", "content": "To further validate that incorporating structural information contributes to the final results, we an-alyze the attention maps generated by Transformer1D and Transformer1D2D. Fig. 10 illustrates the average attention maps across all heads before the final output layer for both the models for a randomly selected RNA sequence. The attention maps of Transformer1D2D exhibit a striking similarity to both the adjacency matrix and the BPP matrix, whereas the attention maps from the standard Transformer model seem to suggest that the model does not learn to attend to the structural features. Moreover, we quantify this observation by computing the cosine similarity between the attention maps of the models and the true adjacency matrix and BPP for all sequences in the COVID dataset. The results, reported in Table 3 show that Transformer1D2D achieves much higher sim-ilarity scores compared to the 1D Transformer alone. This reinforces the conclusion that explicit encoding of structural information is essential for improved model performance."}, {"title": "D Analysis of Noise in 3D Structures", "content": "As mentioned in the main context, predicted 3D structures consistently exhibit noise. In this section, we analyze this issue from two perspectives: sensitivity to sequence length and variability across different prediction tools."}, {"title": "D.1 Impact of Sequence Length on 3D Structure Prediction Noise", "content": "To investigate the hypothesis that longer sequences result in greater noise in 3D structure predic-tions, we randomly selected a COVID and Tc-Riboswitches dataset sequence and generated struc-tures using four state-of-the-art 3D structure prediction tools: RhoFold [45], RNAComposer [64], trRosetta [5], and SimRNA [6]. High variability among these predicted structures would indicate significant uncertainty in absolute atom positions. We quantified this noise by aligning the struc-tures using the Kabsch algorithm [26] and computing the pairwise RMSD values, resulting in a 4x4 matrix showing structural deviations between each pair of tools (see Table 4). The observed pairwise RMSD values ranged from 16 to 45 \u00c5 for the COVID dataset and from 11 to 15 \u00c5 for the Tc-Riboswitches dataset, reflecting substantial variability and suggesting considerable noise in the 3D predictions. This level of structural inaccuracy likely contributes to the poorer performance of 3D models. However, we found that 3D models outperform 1D models for shorter sequences, such as those in the Tc-Riboswitches dataset (67 to 73 nucleotides long). This improved perfor-mance is due to the lower noise in 3D predictions for shorter sequences, a phenomenon supported by previous studies [32, 37] and also exhibited by the comparatively smaller RMSD values reported in Table 4 for Tc-Riboswitches dataset. The reduced complexity of shorter sequences allows 3D models to capture structural details more accurately, thereby enhancing performance and validating that accurate 3D structure encoding can outperform 1D models."}, {"title": "D.2 Impact of different 3D prediction tools", "content": "In this section, we demonstrate the significant differences in 3D structures predicted by various tools. We compare the 3D structure obtained from RhoFold [45], which serves as our default method, with those predicted by RNAComposer [64], trRosetta [5], and SimRNA [6]. Each structure is visualized side by side with RhoFold in Figure 11 to facilitate a more intuitive comparison. As observed, these structures predicted by each tool vary considerably."}, {"title": "E Reproduction", "content": "This section outlines the necessary details to reproduce all experiments discussed in this paper. The code will be made publicly available upon acceptance."}, {"title": "E.1 Training details", "content": "All experiments were conducted on a single NVIDIA A100 GPU. For each baseline, hyperparam-eters were optimized using Optuna [1], restricting the search to models with fewer than 10 million parameters that fit within the memory constraints of an 80GB NVIDIA A100 GPU. Most base-line implementations were sourced from PyTorch Geometric [13]. The Transformer1D model was"}, {"title": "E.2 Hyperparameters", "content": "This section provides a comprehensive overview of the hyperparameters used in each baseline model, facilitating reproducibility and understanding of the model configurations.\nCommon hyperparameters across these models include in_channels, which specifies the number of input features, hidden, which determines the number of hidden units in each hidden layer, and out_channels, which defines the number of output features. The L parameter controls the number of layers in the network, and the dropout parameter sets the dropout rate for regularization. The lr parameter specifies the learning rate, and weight_decay sets the weight decay for regularization of the optimizer. For graph-level tasks, the pool parameter specifies the pooling method, which can be mean, max, or add.\nTransformer1D is a standard Transformer architecture for RNA sequence processing. It includes an embedding layer to convert input tokens into dense vectors, positional encoding (PE) to retain sequence order, and a multi-layer Transformer encoder to capture complex dependencies within the sequence. There are some hyperparameters from the original transformer [54]. nhead, which defines the number of attention heads in each Transformer layer; num_encoder_layers, which controls the number of encoder layers in the Transformer; d_model, which determines the dimensionality of the embeddings and the model; dim_feedforward, which sets the dimensionality of the feedforward network model. To shrink the search space, we set d model and dim feedforward as the same with a new hyperparameter hidden.\nTransformer1D2D is an adaptation of Transformer1D that integrates both sequence and 2D graph structure information. In addition to encoding each nucleotide, the model incorporates base pair probabilities (BPP) features for each nucleotide. It combines a standard Transformer with positional encoding and a convolutional layer applied to the graph adjacency matrix. This convolutional out-put is added to the Transformer's attention matrix, allowing the model to incorporate graph structure into its attention mechanism. This design captures both the sequential and structural dependen-cies in RNA data, improving predictive performance. The unique hyperparameter for this model is kernel_size, which specifies the size of the convolutional kernel.\nGAT includes the unique hyperparameters gat_heads, which specify the number of attention heads in each GAT layer.\nChebNet model has the unique hyperparameter power, which specifies the polynomial order for the Chebyshev convolution.\nGraphGPS and Graph Transformer includes heads, which specifies the number of attention heads in each layer, and pe_dim, which defines the dimensionality of positional encoding.\nEGNN and SchNet are 3D models that operate at two granularities within the network: atom layers and nucleotide layers. The two types of layers are connected through nucleotide pooling. Atom layers use atoms as nodes, while nucleotide layers use nucleotides as nodes. Both the atom layer and nucleotide layer employ a point cloud setting and calculate edges based on the distance between two nodes. An edge is considered to exist if the distance is smaller than a certain threshold. Therefore, EGNN and SchNet share the following hyperparameters: L_atom, which denotes the number of atom layers; L_nt, which specifies the number of nucleotide layers; threshold_atom, which is the threshold for edges in atom layers; and threshold_nt, which is the threshold for edges in nucleotide layers.\nFor SchNet, the unique hyperparameters include num_filters, which refers to the number of filters used in convolutional layers, and num_gaussians, which indicates the number of Gaussian func-tions used for radial filters. For a more detailed explanation of these hyperparameters, please refer to [42].\nTo ensure a fair comparison, the best hyperparameter configuration for each method was selected based on validation set performance. We report the mean performance and standard deviation across 5 random splits on the test set. For the COVID and Ribonanza datasets, we performed hyperparam-eter searching only on the COVID dataset and applied the same configuration to Ribonanza, as the two datasets share similar properties. The optimal hyperparameters are shown in Table 5."}, {"title": "F\nAdditional Results", "content": "In this section, we present the additional results supporting Figures 2, 3, 5, and 6 in main text."}, {"title": "F.1 Impact of data availability", "content": "The detailed results of partial training data from Figure 2 are shown in Tables 6, 7, and 8."}, {"title": "F.2 Impact of partial labeling", "content": "The detailed results of the partial labeling sequence from Figure 3 are shown in Tables 9 and 10."}, {"title": "F.3 Robustness to sequencing noise", "content": "The results of the robustness experiment from Figure 5 are shown in Tables 11, 12, 13, and 14."}, {"title": "F.4\nGeneralization to OOD data", "content": "The results of the generalization experiment from Figure 6 are shown in Tables 15, 16, 17, and 18."}]}