{"title": "NoLoR: An ASR-Based Framework for Expedited Endangered Language Documentation with Neo-Aramaic as a Case Study", "authors": ["Matthew Nazari"], "abstract": "The documentation of the Neo-Aramaic dialects before their extinction has been described as the most urgent task in all of Semitology today. The death of this language will be an unfathomable loss to the descendents of the indigenous speakers of Aramaic, now predominantly diasporic after forced displacement due to violence. This paper develops an ASR model to expedite the documentation of this endangered language and generalizes the strategy in a new framework we call NoLoR.", "sections": [{"title": "Introduction", "content": "Aramaic is the oldest continuously spoken Semitic language in the world. Today, the language is highly endangered and is expected to be within its final generation or two of fluent speakers. Speakers of the Neo-Aramaic dialects, which are comprised of Christian and Jewish communities across the Middle East (see Fig. 1), have been the victims of much violence over the past century, displacing them from their homelands. Such violence includes the Assyrian genocide of World War I (Gaunt 2020) and the systematic ethnic cleansing by Islamic State terrorist groups (Omer 2022). The death of this language will be an unfathomable loss for the descendents of these communities, to whom language is an essential part of their culture and identity.\nThis problem is not unique to Aramaic. Over 40% of spoken languages are endangered, and in the next 100 years, about 90% of spoken languages are expected to become extinct (Moseley and Nicolas 2010). Unfortunately, technologies like the internet and smartphones only further marginalize endangered language by disproportionally promoting the usage of high-resource languages like English (NLLB Team et al. 2022). Although the field of low-resource NLP appears to be addressing this disparity, it is predominantly focused on developing luxury technologoies, such as translators for already documented languages. However, such work overlooks languages that are underdocumented, without a written tradition, or with very few speakers remaining. Attention must be paid to such situations where the priority is to document the language before it is extinct."}, {"title": "Endangered Language Documentation", "content": "In order to preserve endangered languages, they must be documented. The field of endangered language documentation emerged as its own field of linguistics in the 1990s. In this field, researchers document the grammar and oral history of a language by collecting samples of speech accompanied with a written transcription. However, there is a \"transcription bottleneck\" limiting the scale of documentation efforts (Shi et al. 2021). Annotating speech data is an extremely costly, lengthy, and esoteric process only performable by the few experts of that language. This severely limits the speed and capacity at which documentation efforts can operate. Hence, it is important that tools and frameworks exist to address this problem.\nThis paper constructs a framework for documenting endangered languages, employing ASR to overcome the transcription bottleneck. The effectiveness of this strategy is demonstrated by the application of NoLoR in the documentation of the Neo-Aramaic dialect of the Assyrian Christians of Urmi (C. Urmi)."}, {"title": "The NoLoR Framework", "content": "The No- to Low-Resource (NoLoR) framework outlines the strategy of developing an ASR model to expedite the doc-"}, {"title": "Related Work", "content": "Researchers regularly use software to assist in documenting languages. Such software includes FieldWorks Language Explorer, which allows users to define a lexicon of the target language and to morphologically tag text (SIL International 2007). The software SayMore provides a means of file"}, {"title": "Within Low-Resource NLP", "content": "The low-resource NLP community has frequently used endangered languages as a case study in various tasks, but not often ASR (Hedderich et al. 2020; Ranathunga et al. 2021; Adams et al. 2017). A main goal of this field is to eliminate communication boundaries between communities. Almost no work has explicitly sought to uplift the status of endangered languages by improving the workflow of field-workers documenting them.\nThe exception to this is one work that offers a general-purpose ASR model to assist in the documentation of the endangered Muyu language of New Guinea (Zahrer, Zgank, and Schuppler 2020). This ASR model was trained to learn a universal phonetic representation of human speech by training on speech data from 3 languages (American English, Austrian German, and Slovenian). Such model can then be used on other lanuages, including endangered ones. To overcome the low-resource barrier, we also exploit a general representation of human speech by fine-tuning a wav2vec 2.0 pretrained on not 3 languages, but 53 languages then a phonetically adjacent one. This difference, alongside many others, results in the ASR model produced by NoLoR to vastly outperform with much less data (see Table 1). Additionally, NoLoR rewards researchers by improving the accuracy of the ASR as more documentation work is collected through periodic fine-tuning the ASR. Finally, this paper emperically demonstrates that the ASR model was able to meaningfully improve transcription times.\nExamining the literature more generally, there have been many attempts to train an ASR model with low training data. However, even works that fall under the category of \"extremely low-resource\" assume dozens of hours of unlabelled data (Xu et al. 2020). Our approach, using a phonemic orthography performing careful optimizations, proves to be on par with such work with far less data.\nWe also develop Assyrian Voices, an online application for crowdsourcing both speech samples of existing text examples and new text samples. This format is directly inspired by the Common Voice project started by Mozilla (Ardila et al. 2019). A seperate platform was developed for Neo-Aramaic in order to allow for the display of text in multiple transliteration schemes to maximize accessibility, something not possible with Common Voice."}, {"title": "Specifying a Phonemic Orthography", "content": "Specifying a phonemic orthography allows us to:\na) work with existing language documentation efforts or pave the way for future ones\nb) minimize the amount of initial data we need to collect to train a minimally viable ASR model\nIn the case of C. Urmi, we align ourselves with existing work and use the orthographic standard set by Geoffrey Khan in his description of the dialect (Khan 2016)."}, {"title": "The Importance of Orthography", "content": "In language documentation, the language of interest may not have a written tradition, or it may use a seperate esoteric literary language that is greatly misaligned with the vernacular. This was especially true for langauges of indigenous communities, such as the Navajo language of North America and the Neo-Aramaic dialects. In these cases, a very first step in documentation is to design an effective orthography (Cahill and Karan 2008). A phonemic orthography is one that assigns each unique unit sound in a language (phone) to a unique character. This scheme has been classically accepted by linguists as most ideal (Pike 1971). However, most contemperary opinions acknowledge the importance of other features of a language beyond phonemic analysis that ought to be represented in the orthography (Venezky 2004). In English, whose orthography is not phonemic, apostraphes to represent possessive constructs are an example of this. In the orthography we selected for C. Urmi, the double hyphen '=' indicates an enclitic boundary.\nUnsurprisingly, a phonemic orthography is a great written representation of spoken language in machine learning tasks since the written and the waveform representations of human speech are in principle being maximally aligned (Feng et al. 2021a; Khare et al. 2021). This is different from \"natural\" orthographies, like that of standard French or English, which have frequent irregularities and letters with multiple possible sounds, making it more difficult for speech recognition models to learn. One might then consider a phonetic"}, {"title": "Building an Initial Dataset", "content": "Building an initial dataset from preliminary documentation efforts allows us to train an ASR model that can expedite the transcription of future speech samples. In the case of C. Urmi, a labelled speech dataset of 35 minutes was built. This dataset has been made publically available\u00b9 and is released under a Creative Commons CC0 license to encourage follow-up work."}, {"title": "Optimizing Data for Speech Recognition", "content": "Preliminary documentation efforts will produce a dataset $D = \\{(x_i, Y_i)\\}$ of audio-transcript pairs. Since a priority in preserving oral history is eliciting speech in the form of interviews and folktales, each pair will be too long to train a speech recognition model. Not only that, but when completely done by hand, the transcripts $y_i \\in Y$ may essentialize the utterance by not transcribing mishaps like repeated words. Hence, a new dataset $\\tilde{D} = \\{(x_j, \\tilde{y}_j)\\}$ must be processed where $\\tilde{y}_j$ is the exact transcript of $x_j$ and all $x_i$ are singificantly shorter (see Fig. 3).\nThe preliminary data for C. Urmi consisted of 8 examples, some as long as 11 minutes in length. This was carefully segmented into roughly 600 samples each no longer than 15 seconds."}, {"title": "Training an ASR Model", "content": "Training an ASR model allows us to transcribe future collected samples more efficiently. Intuitively, the ASR model,"}, {"title": "Deep Learning Over Classical Models", "content": "With the immense quantity of speech data available today, deep learning has outperformed all traditional approaches to ASR (Malik et al. 2021). However, training these models can require up to 10,000 hours of labelled data, whereas traditional approaches generally require orders of magnitude less data. Training these traditional models is an extremely intensive process that would be difficult to perform repeatedly in NoLoR. This training data must also be phoneme-aligned, which does not work if simply collecting the unlabelled speech data is challenging enough."}, {"title": "Selecting a wav2vec 2.0 Checkpoint", "content": "Thankfully, end-to-end models like wav2vec 2.0 (Baevski et al. 2020) leverage pre-training to frontload the learning of human speech representations. This means that with very little training data, a wav2vec 2.0 model pretrained on 100,000+ hours of 53 multilingual speech data can be fine-tuned for other tasks (Chen and Rudnicky 2021), including ASR for other languages.\nThe wav2vec 2.0 architecture takes in as input raw waveform $x \\in X$. An encoder network $f : X_u \\rightarrow Z$ comprised of convolutional layers outputs a feature representation $z \\in Z$ of the original waveform. Context is encoded into the feature representation using the context network $g: Z^u \\rightarrow C$. This network mixes multiple latent representations $z_i, z_{i-1}, ..., z_{i-v}$ into a single contextualized representation $c_i$ using more convolutional layers. These contextualized representations can then be used as inputs into a language model and decoded into text.\nThe encoder and context networks are trained by self-learning from an enourmous amount of multilingual data. The initial dataset will not be large enough to fine-tune these layers to the specific phonology of the language of interest. If we don't fine-tune these layers, we run risk of being handicapped by generic contextualized representations. Hence,"}, {"title": "Language Model", "content": "A character-level tokenizer was used as a language model. The alternate to this would be using a subword tokenizer with a relatively small vocabulary size of 256 or 512. This was attempted but proved to have poor results. Training a subword tokenizer requires sample text to learn the the frequencies of each subword. However, with such a limited amount of text data in existance, such a tokenizer proved to be too specific and unable to identify novel words or verb inflections. This behavior is expected according to Zipf's law.\nThe loss used to decode the contextualized representations was a connectionist temporal classification (CTC) loss (Graves 2012). CTC loss is necessary for training on data that is not phoneme-aligned."}, {"title": "Fine-tuning", "content": "In order to fine-tune from the wav2vec 2.0 checkpoint, the encoder and context networks were frozen during training and the CTC language model was concatenated to the end of the context network for fine-tuning. By feeding the contextualized representations into a dense layer, a probability distribution over possible characters is generated which can be fed into a CTC loss.\nOverfitting on the training split of the dataset was a huge issue, as soon discussed. A very careful sweep of hyperparameters had to be done to discover which combination had the best regularizing effects."}, {"title": "Data Augmentation", "content": "As will be the case in many language documentaiton efforts, the number of speakers comprising the initial dataset may be very low. In the case of our dataset, all speech came from a single elderly woman. Machine learning models are interpolators, meaning it is often difficult to generalize their performance on out-of-distribution data (Neyshabur et al. 2017).\nIn our situation, it means the ASR model will overtrain on this specific speaker's voice by learning that her unique pronunciation habits are instrinsic to C. Urmi as a whole. We use data augmentation to combat this issue, which is a proven technique to improve model robustness and accuracy in low-resource NLP settings (Feng et al. 2021b). For each example, we apply augmentations (see Fig. 5) each at varying degrees. The augmentations were chosen specifically to counteract the speed of the speaker's utterance, the pitch of the speaker's voice, and the clarity of the microphone."}, {"title": "Evaluation and Impact", "content": "To evaluate the ASR model, we looked to the diasporic Assyrian population of Armenia and travelled to the southern Assyrian villages near Verin Dvin. Here, we collected several samples of people speaking about the history of the village as well as various jokes and anecdotes. One striking story, in particular, comes from a grandmother who describes in grief how she is discouraged to speak to her children in her language since her daughter had married an Armenian outside of the village. With our work, her voice and story will be documented.\nRegarding the efficacy of the ASR model, we clearly observe dramatic improvements to the speed of documentation when assisted by the ASR model (see Table 3). Although the ASR model achieved a test accuracy of 12.5% CER, the accuracy for certain samples reached as low as 27.5%. In these examples, the speakers exhibited alternate pronunciations of certain phones (like [b] for /j/ rather than [x]). This articulation, less common in the C. Urmi dialect (Khan 2016), had become ubiquitous in this diasporic community. Additionally, when inputting several minutes into the ASR at once, entire sentences were misconstrued which decreased the accuracy significantly. However, given these challenges, the ASR still provided adequate drafts that proved to be great aids in the transcription process, up to 6.3\u00d7 faster for longer, more difficult transcriptions.\nWe noticed that with the ASR model, we tended to assume we were done transcribing early, which meant we allowed mistakes to make it through which we corrected later. We observed this behavior even without the ASR model but to a smaller extent."}, {"title": "Expanding the Dataset", "content": "To expand this dataset, we collect and transcribe more speech samples as we did in Armenia. Beyond this, however, we developed AssyrianVoices, an online crowdsourcing ap-"}, {"title": "Conclusions", "content": "Language documentation is vital to the preservation of endangered languages, but transcribing speech samples is a costly and time-consuming process. Within the low-resource NLP community, there is virtually no discussion about how to facilitate the transcription process.\nThis paper provides the NoLoR framework for expedited endangered language documentation and demonstrates its efficacy using Neo-Aramaic as a case. By doing so, we produced an ASR model that helped document the oral history of the diasporic Assyrian community of Armenia. We then released our dataset of annotated speech samples and launched a crowdsourcing campaign with the goal of collecting data to further explore the role of AI in language documentation."}, {"title": "Next Steps", "content": "There are many future steps that will further improve the efficacy of NoLoR as well as develop it into a more out-of-the-box approach. One such improvement would involve building a model to segment the long-form audio samples automatically in order to shorten the step of building an initial dataset. Another direction is to implement a system where fieldworkers can simply dump their new transcribed samples and have the ASR model periodically fine-tune itself. This way, an engineer does not have to be on staff in order to continuously improve the ASR model. The most immediate work, however, is to continue language documentation efforts with C. Urmi and see how many iterations of NoLoR is necessary to eliminate the need for human intervention entirely.\nAn area of interest specific to Neo-Aramaic is creating programs to educate members of the community of the importance of documentation efforts like AssyrianVoices and teach them how to use it."}]}