{"title": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning", "authors": ["Zhe Xie", "Tieying Zhang", "Jianjun Chen", "Zeyan Li", "Xiao He", "Rui Shi", "Longlong Xu", "Xidao Wen", "Dan Pei"], "abstract": "Understanding time series is crucial for its application in real-world scenarios. Recently, large language models (LLMs) have been increasingly applied to time series tasks, leveraging their strong language capabilities to enhance various applications. However, research on multimodal LLMs (MLLMs) for time series understanding and reasoning remains limited, primarily due to the scarcity of high-quality datasets that align time series with textual information. This paper introduces ChatTS, a novel MLLM designed for time series analysis. ChatTS treats time series as a modality, similar to how vision MLLMs process images, enabling it to perform both understanding and reasoning with time series. To address the scarcity of training data, we propose an attribute-based method for generating synthetic time series with detailed attribute descriptions. We further introduce Time Series Evol-Instruct, a novel approach that generates diverse time series Q&As, enhancing the model's reasoning capabilities. To the best of our knowledge, ChatTS is the first MLLM that takes multivariate time series as input, which is fine-tuned exclusively on synthetic datasets. We evaluate its performance using benchmark datasets with real-world data, including six alignment tasks and four reasoning tasks. Our results show that ChatTS significantly outperforms existing vision-based MLLMs (e.g., GPT-40) and text/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a 25.8% improvement in reasoning tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal large language models (MLLMs) have recently achieved significant progress in vision-language tasks, showing exceptional performance even in scenarios requiring complex understanding and reasoning [4, 23, 29, 50]. However, this success has not been replicated in the time series domain. Even though some studies have attempted to integrate LLMs with time series, such as TimeLLM [18], they usually only focus on specific classical time series tasks (e.g., forecasting) rather than complex understanding and reasoning. Moreover, recent studies indicate that LLMs still struggle with zero-shot reasoning about time series [35]. This is particularly significant because time series analysis, widely applied in domains such as electricity [45], healthcare [38], traffic [25], weather [28], and finance [42], frequently requires understanding and reasoning about data to identify patterns, trends, and events over time. Therefore, the ability to reason using both text and time series data is a critical capability for MLLMs, enabling them to support human decision-making by providing natural language explanations that align with human logic. Figure 1 illustrates such an example in an AIOps [58] scenario where understanding and reasoning about multivariate system monitoring time series are achieved through natural language dialogue, thereby improving the diagnostic and troubleshooting process.\nExisting LLM-based methods for time series understanding and reasoning can be broadly categorized into text-based, vision-based, and agent-based approaches. Text-based methods directly use LLMs by structuring historical observations as raw text [3]. However, these methods are often constrained by the limitation of prompt length and generally perform poorly in understanding the global features of time series compared to vision-based methods. Vision-based methods utilize vision MLLMs, which accept plot figures of time series data [35], such as GPT-40 [1] or Qwen-VL [4]. While these methods can better capture global features, they are limited by the resolution of the plotted figures and face challenges in accurately interpreting detailed fluctuations. Recent works also demonstrate how agents can leverage time series analysis tools to interact with LLMs [44, 60]. However, the ability of agents to understand time series is restricted by the functionality of the tools.\nTherefore, there is a strong need for TS-MLLM, a MLLM that can naturally handle time series modality, akin to how vision MLLMs process images. Such models have the potential to unlock valuable insights from time series by providing intuitive, question-driven analysis capabilities. Specifically, TS-MLLMs can capture global and local features and relationships between multivariate time series (MTS), areas where existing LLMs and MLLMs have struggled. By incorporating textual modalities as input, these models can broaden their applicability and better contextualize time series data, aligning the analysis with user queries. If successful, TS-MLLMS could perform novel tasks such as citing patterns and events in time series as evidence for observations and inferences, drawing interpretable conclusions from complex dynamical systems, and recognizing and responding to temporal patterns [35].\nHowever, developing TS-MLLMs with effective time series understanding and reasoning ability faces several core challenges. First, multimodal time series data, especially language-time series pair data, is extremely scarce [8, 19, 35]. Unlike modalities such as images and audio, almost no research focuses on the language alignment of time series. As a result, there is a significant lack of time-series + text data, which makes the construction of time-series dialogue and reasoning datasets challenging. This is fundamental for TS-MLLMs to develop temporal understanding and reasoning capabilities. Second, time-series data contains abundant shape and numerical attributes (i.e., the types of local fluctuations and their amplitudes). Therefore, a diverse range of text is needed to comprehensively describe these attributes while ensuring accuracy to achieve effective alignment. Third, real-world time-series data are usually variable in length, multivariate, and of uncertain quantity. The correlations among MTS are often a focus of attention (as illustrated in Figure 1). In MLLMs for other modalities, such as images, few methods emphasize the relationships between multiple samples. However, such relationships are indispensable for understanding and reasoning about time series. Finally, there is a lack of evaluation data and methods for TS-MLLMs. Developing comprehensive and reasonable datasets and methodologies to evaluate their performance is necessary.\nTo address the challenges above, we innovatively propose a method to fine-tune a pre-trained LLM for TS-MLLMs solely using synthetic time series and text data. An important reason is that synthetic time series data for time series model training has shown good results [14]. However, current methods are difficult to apply directly because time series-text alignment tasks require both precise and diverse time series attribute descriptions. Therefore, we propose an attribute-based method for generating synthetic time series and precise text attributes to facilitate the modal alignment of time series with LLMs. Compared with existing studies on synthetic time-series generation [14, 53], the proposed attribute-based time-series generation method provides precise textual attributes for each detailed pattern of the time series, laying a foundation for generating diverse text data. Furthermore, to equip MLLM with enhanced time series understanding and reasoning capabilities, we propose the Time Series Evol-Instruct (TSEvol) algorithm. Through the diverse combinations of attributes and tasks, TSEvol can generate diverse time series Q&A datasets through evolutions, thereby enhancing the model's overall performance. To handle multivariate time-series inputs and fully preserve semantic information, we propose ChatTS, trained using the generated synthetic datasets. ChatTS employs a context-aware time-series encoder capable of encoding time series of (theoretically) arbitrary length and quantity while retaining their original numerical information. Finally, to support comprehensive evaluation regarding both language alignment and time series reasoning, we have collected evaluation datasets comprising both real and synthetic time series. These datasets include both alignment and reasoning tasks with uni/multivariate time series, ensuring a thorough assessment of the model's performance.\nOur contributions. This paper makes the following contributions.\n\u2022 We propose to align LLMs with time series using attribute-based synthetic time series and text data. Building on this, we further introduce Time Series Evol-Instruct (TSEvol), an algorithm that generates diverse, accurate, and multimodal training datasets of time series and text entirely through synthetic data generation.\n\u2022 We propose a context-aware TS-MLLM, ChatTS, designed for variable-length, multivariate time series input and trained using the generated synthetic data. To the best of our knowledge, ChatTS is the first TS-MLLM with multivariate time series as input.\n\u2022 We have collected evaluation datasets\u00b9 containing real-world time series data, including six alignment tasks and four reasoning tasks. Evaluation results across multiple datasets demonstrate that ChatTS significantly outperforms baseline models, including GPT-40, in both time series alignment and reasoning tasks."}, {"title": "2 PRELIMINARY AND MOTIVATION", "content": null}, {"title": "2.1 Problem Definition", "content": "The task of a TS-MLLM is to generate text-based responses based on the input textual query and MTS array. Given a set of time series \\(T = \\{T_1, T_2, . . . , T_n\\}\\), where each \\(T_i = \\{t_{i,1}, t_{i,2},..., t_{i,m_i};\\}\\) represents a sequence of \\(m_i\\) observed values over time for the i-th metric, and a natural language question Q, the goal is to generate an answer A that captures relevant patterns or relationships across T based on the context of Q. Formally, it can be defined as follows:\n\u2022 Input:\nA set of time series \\(T = \\{T_1, T_2, ..., T_n\\}\\), where \\(T_i \\in \\mathbb{R}^{m_i}\\) represents the values of the i-th metric over \\(m_i\\) time points.\nA natural language query Q specifies the information of interest within the time series data.\n\u2022 Output: A text answer A derived from the T analysis, providing insights based on Q.\nThe task of TS-MLLM can be expressed as a function:\n\\(f(Q,T) \\rightarrow A\\),\nwhere f denotes the model or algorithm responsible for interpreting the text query Q and generating the text answer A by analyzing relevant patterns and relationships across the time series in T."}, {"title": "2.2 Existing Methods", "content": "Although mainstream LLMs currently do not support the direct input of time series modality data, time series information can be provided to LLMs through alternative methods to do simple time series understanding and reasoning tasks as shown in Figure 2. Existing approaches can be broadly categorized into text-based, vision-based, and agent-based, each with distinct limitations.\nText-based methods encode time series values as raw text [3]. However, these methods are constrained by the length of prompts, limiting their global analysis capabilities and often resulting in an incomplete understanding of the data context (refer to Section 4).\nVision-based approaches, which use visual representations of time series data (e.g., time series plots) processed by vision MLLMs [1, 4], may face challenges in accurately capturing detailed information in time series, resulting in lower accuracy for data-intensive tasks and high computational overhead (refer to Section 4).\nAgent-based methods employ a reasoning and action strategy, breaking down complex tasks into a sequence of thoughts, observations, and actions conducted by external tools to analyze time series. While potentially more flexible, this approach is heavily dependent on expert knowledge and effectiveness of tools, token-intensive, and time-consuming, often requiring extensive token chains to handle MTS data. Additionally, hallucination becomes a significant problem [51] as the chains grow longer, reducing reliability in complex analytical tasks."}, {"title": "2.3 Time Series Multimodal LLM", "content": "TS-MLLM is a new type of MLLM that aims at overcoming the limitations of existing methods by natively integrating both textual and time series inputs (see Figure 2). It can process multiple time series data and textual descriptions, enabling a unified analysis that captures complex, multivariate relationships. Unlike previous methods, it does not rely on lengthy token chains or visual representations, thereby reducing computational overhead and mitigating issues with hallucination. Through the alignment of time series and text, TS-MLLM can perform both global and local analysis of the shape and numerical information of time series. This capability allows it to achieve higher accuracy and greater potential than existing methods."}, {"title": "3 METHODOLOGY", "content": null}, {"title": "3.1 Overview", "content": "Due to the scarcity of high-quality datasets that align time series with textual information, we propose to generate synthetic text-time series pairs for model training. Synthetic data is a common approach when there is a lack of sufficient real training data, and its effectiveness has been well validated in various fields [14, 31, 41]. However, as discussed earlier, \"time series + text\" data for TS-MLLM requires sufficient accuracy to ensure alignment precision, comprehensive coverage of time series attributes to guarantee effective multimodal alignment, and task diversity in the text to enhance QA and reasoning abilities. Unfortunately, existing time series generation methods [14, 53] fail to achieve these goals. A key reason is that we need a diverse set of time series and precise, detailed descriptions of time series patterns. Therefore, in this paper, we propose an attribute-based method to generate time series + text data, as illustrated in Figure 3:\n\u2022 Attribute Selector (Section 3.2): To produce highly controllable time-series data with precise attributes, we use a detailed feature set to describe time series. These attributes are aligned with real-world settings through an LLM selection.\n\u2022 Attribute-Based Time Series Generator (Section 3.2): Construct time series that correspond exactly to the attribute pool using a rule-based approach.\n\u2022 Time Series Evol-Instruct (Section 3.3): A novel Time Series Evol-Instruct module for creating large, diverse, and accurate datasets of time-series and text question-answering pairs for complex reasoning.\n\u2022 Model Design (Section 3.4): To handle MTS, we design a context-aware MLLM encoding for multiple time series input, along with a value-preserved time series encoding method.\n\u2022 Model Training (Section 3.5): A large-scale pretraining and a supervised fine-tuning are conducted to perform language alignment and improve time series-related reasoning ability."}, {"title": "3.2 Attribute-Based Time Series Generator", "content": "Diverse time series and precise, detailed textual attribute descriptions are essential to achieve accurate time series language alignment. Time series have rich pattern attributes, which can be roughly categorized into trend, periodicity, and remainder [39]. Much existing research on the generation of time series [13, 14] also adopts similar approaches to classify these attributes. Therefore, following existing studies, we classify time series attributes into four major categories, Trend, Periodicity, Noise, and Local Fluctuation, to construct the corresponding attribute set for time series.\nBased on this, we propose an attribute selector and an attribute-based time series generator that produces synthetic time series data (see Figure 4). First, we define an \"All Attribute Set\", which includes many specific attributes under different attribute categories. The All Attribute Set includes 4 types of Trend, 7 types of Seasonality, 3 types of Noise, and 19 types of local fluctuations. The complete list can be found in the source code. Different attributes within the same category can be combined. For example, a time series can contain multiple segments of trends and several local fluctuations. This allows the generator to generate an unlimited variety of attribute combinations theoretically. We also introduced a GPT Selector. Specifically, when generating an attribute set for time series, we randomly sample a metric from a large \"Metric Set\" that contains 567 predefined metric names\u00b2 from real-world scenarios and use GPT to choose a attribute subset from the all attribute set, based on the actual physical meaning of the metric and the predefined scenario. This helps align time series with real-world physical meanings.\nThen, the Attribute Sampler randomly samples a combination of attributes from the Attribute Subset. It also assigns specific numerical values, like position and amplitude, based on rules and constraints from the GPT Selector. These details are stored in the \"Attribute Pool\", which records all the detailed information about a time series. The Time Series Generator finally creates time series arrays that exactly match the attributes from the pool in a rule-based manner (more details can be found in the source code). This process allows us to generate diverse synthetic time series with precise attribute descriptions."}, {"title": "3.3 Time Series Evol-Instruct", "content": "To improve the model's question-answering and reasoning abilities, it is essential to have high-quality SFT training data that is diverse in format and tasks. However, due to the lack of time-series + text data, it is challenging to obtain sufficiently diverse time-series-related training data directly. To generate accurate time-series + text SFT data with rich question-answering formats, inspired by Evol-Instruct [47] and its multimodal version MMEvol [32], we innovatively propose Time Series Evol-Instruct (TSEvol).\nEvol-Instruct [47] is a data generation approach that incrementally evolves instructional prompts and their outputs to enhance the diversity and complexity of training datasets for LLMs. TSEvol builds upon Evol-Instruct by introducing a mechanism to incorporate time series attributes dynamically into each evolutionary step (see Figure 5). TSEvol relies on attribute pools of multivariate time series (see Section 3.2). Additionally, to enhance the model's ability to analyze correlations, we introduce a correlation pool, which records time series with related attributes (refer to the source code for details). During each step of the evolution process, a subset of attributes is randomly selected from the attribute pool and added as additional context, guiding the LLMs to generate Q&As about a broader set of time series attributes according to the evolution type. With TSEvol, generated Q&As can cover more attributes in the time series and avoid repetitive questions. We also added an attribute-based eliminator to ensure the Q&As match the time series attributes. In addition to the commonly used evolution types, we also add two more types, reasoning (reasoning-based questions)"}, {"title": "3.4 Time Series Multimodal LLM", "content": "In this subsection, we introduce the model structure of the proposed ChatTS, as shown in Figure 6. ChatTS takes multivariate time series and text, along with their contextual information as the input."}, {"title": "3.4.1 Context-Aware Time-Series Multimodal LLM", "content": "To handle the multimodal inputs, ChatTS first separates the input time series arrays and the text. Following the established practice in encoding time series for LLMs [18], the input time series arrays are divided into fixed-size patches, which enables the model to handle and encode temporal patterns more effectively. We employ a simple 5-layer MLP to encode each patch of the time series, as time series inherently have sequential patterns. Therefore, a simple structure can map the patch features to a space aligned with the text embedding. For text input, they are tokenized and then encoded through a text embedding layer. In this way, each patch of the time series and each text token are mapped to the same space.\nTo fully retain the contextual information of multivariate time series, we performed token-level concatenation based on the position of the time series in the original input. Specifically, the encoded patches corresponding to each time series were inserted between the surrounding text tokens. Unlike the method used in TimeLLM [18], this approach ensures that the contextual information of the time series is fully preserved. This is especially important in multivariate scenarios, where referencing the corresponding time series in textual form is often necessary. This process results in a sequence that reflects the multivariate structure of the data, enabling the LLM to capture both temporal and contextual dependencies across different metrics. This sequence is then fed into the LLM, which generates an answer that incorporates insights from both the time series data and the natural language query, achieving a multimodal understanding suited for complex question-answering tasks."}, {"title": "3.4.2 Value-Preserved Time Series Normalization", "content": "The numerical features of time series are essential, as real-world applications often involve specific numerical queries (e.g., asking for the maximum CPU utilization). However, normalization of time series data can lead to losing original numerical information. To address this, we introduce a value-preserved time series normalization scheme (as shown in Figure 7). First, we apply standard min-max normalization (0-1 scaling) to each time series array. Then, for each time series, we include the normalization parameters-\"Value Scaling\" (the scaling factor during normalization) and \"Value Offset\u201d (the offset applied during normalization)-in the text as part of a prompt. This approach leverages the numerical understanding capabilities of LLMs, enabling us to normalize time series features while preserving the original numerical information."}, {"title": "3.5 Model Training", "content": "ChatTS is trained based on QWen2.5-14B-Instruct [48]\u00b3, with a two-stage fine-tuning process: large-scale alignment pretraining and supervised fine-tuning (SFT). Table 1 shows the datasets we use during training."}, {"title": "3.5.1 Large-Scale Alignment Pretraining", "content": "In the first stage, we perform large-scale alignment pretraining using the attribute-based synthetic time series data. During the alignment stage, we created three datasets for large-scale pretraining based on a series of manually designed templates (which can be found in the supplemental material). The UTS dataset includes tasks for basic attribute descriptions of univariate time series. The MTS-Shape dataset consists of multivariate data with overall trend correlations designed to enhance the model's ability to analyze multivariate correlations. The MTS-Local dataset contains multivariate data with correlated local fluctuations, aiming to improve the model's capability in analyzing local features of multivariate data. This stage aims to establish an initial alignment between the text and time series modalities within the LLM, enabling it to align textual descriptions with attributes in time series data effectively. With a large amount of synthetic data, the model acquires foundational multimodal alignment capabilities, which are essential for alignment and reasoning tasks."}, {"title": "3.5.2 Supervised Fine-Tuning", "content": "In the second stage, we use SFT to develop the LLM's ability to perform complex question-answering and reasoning tasks. This stage utilizes two main types of training data: the Q&A datasets generated with TSEvol, designed to enhance the model's QA and reasoning ability about time series, and an instruction-following (IF) dataset, constructed based on a series of predefined templates, designed to enhance the model's ability to follow specific response formats. Together, these datasets train the multimodal LLM to respond accurately to time series-specific queries and follow task instructions, strengthening its capacity for complex, context-driven question-answering and reasoning tasks."}, {"title": "4 EVALUATION", "content": "In this section, we will comprehensively evaluate the performance of ChatTS by answering the following research questions (RQs):\n\u2022 RQ1. How well does ChatTS align with time series?\n\u2022 RQ2. How does ChatTS perform in time series reasoning tasks?\n\u2022 RQ3. Is the time series modalilty in ChatTS truly useful?\n\u2022 RQ4. Are attribute-based data generation and Time Series Evol-Instruct effective?\n\u2022 RQ5. Does ChatTS, with its native time-series multimodal capabilities, have advantages over agent-based methods?"}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Evaluation Tasks. To comprehensively evaluate the model's performance, we set two categories of evaluation tasks: alignment tasks and reasoning tasks, following the general evaluation methods of other multimodal LLMs [29, 32]. For each type of evaluation task, we designed a series of subtasks based on existing work. Some example QAs are shown in Figure 8 (more details can be found in the code and evaluation datasets). Specific tasks that rely heavily on domain-specific knowledge (e.g., classification and etiological reasoning) were excluded due to the lack of high-quality datasets that provide sufficient background information. Therefore, we primarily focused on the following tasks:\nAlignment tasks are divided into univariate and multivariate:\n\u2022 Univariate tasks. Identify trends, seasonality, noise, and local fluctuations. These tasks include both categorical subtasks and numerical subtasks.\n\u2022 Multivariate tasks. Correlation and clustering. These tasks are all categorical.\nThe reasoning tasks include inductive reasoning, deductive reasoning, causal reasoning, and comparison reasoning (MCQ2):\n\u2022 Inductive reasoning. Q&A task. Inductive summarization of the physical meaning reflected by a uni/multivariate time series.\n\u2022 Deductive reasoning. True/False (T/F) task. Reasoning based on a predefined condition in conjunction with univariate time series.\n\u2022 Causal reasoning. Multiple-choice task. Based on univariate time series, select the most likely cause.\n\u2022 Comparison reasoning (MCQ2). Multiple-choice task. Compare two time series and select the correct answer.\nMore details about the evaluation tasks can be found in the evaluation dataset.\n4.1.2 Evaluation Metrics. For categorical tasks in alignment evaluation, we match labels from the responses of LLMs using rule-based matching and use F1-Score as the metric. For numerical tasks in alignment evaluation, we extract numbers from the responses of LLMs and use relative accuracy (1.0 - relative error) as the metric:\n\\(relative\\_accuracy = max(1.0 - \\frac{|V_{answer} - V_{label}|}{V_{label}}, 0.0)\\)\nWe set a minimum value of 0.0 for relative accuracy to mitigate the impact of outlier results. For Q&A tasks in inductive reasoning, answers are evaluated using RAGAS [12], a keyword-matching approach through LLM-based fuzzy matching. T/F and MC tasks are directly evaluated through choice matching and the accuracy is calculated. All evaluation metrics are the higher, the better.\n4.1.3 Evaluation Datasets. Our evaluation is conducted on three datasets (see Table 2) to test the model's performance across both real-world and synthetic time series scenarios. Dataset A and B are collected by us, and Dataset MCQ2 is an open-source dataset [35]."}, {"title": "4.1.4 Baselines", "content": "Based on different modalities, we categorized the baseline methods into the following types:\n\u2022 Text-Based: These methods convert time series arrays into textual prompts as inputs for LLMs. We choose several mainstream LLMs as our base model (GPT-40/GPT-40-mini/GPT-4-Turbo/QWen2.5-14B-Instruct) for evaluation.\n\u2022 Vision-Based: These methods plot time series and input them into visual MLLMs. We choose mainstream vision MLLMs (GPT-40/GPT-40-mini) for evaluation.\n\u2022 Agent-Based: These methods employ the ReAct [49] framework to interact with multiple tools to analyze the time series. The tools used include single-point/range query, STL decomposition, anomaly detection (autoregression AD in adtk\u00f3), classification (Rocket [11]), and Pearson correlation. We choose GPT-40/GPT-40-mini as LLMs for the agent baselines. We also conducted additional experiments to explore further the capabilities of agent-based methods (Section 4.6), which studies the impact of tool accuracy.\n4.1.5 Implementation. For GPT-based models, we used OpenAI's API to infer and track token consumption. For ChatTS and QWen-based models, the training and inference are conducted locally on 8\u00d7A800 GPUs. The token consumption for ChatTS is calculated after the \"Reorder & Concat\" step. Full-parameter training is used for ChatTS with DeepSpeed7 and LLAMA-Factory [57], with Qwen2.5-14B-Instruct [48]8 as the base model. Inference for both Qwen and ChatTS is also conducted with DeepSpeed."}, {"title": "4.2 RQ1. Alignment Tasks", "content": "The evaluation results on alignment tasks are shown in Table 3. It can be observed that ChatTS achieves leading performance in almost all tasks and datasets. Overall, ChatTS achieves 46.0%-82.6% improvement in categorical metrics and 80.7%-153.1% in numerical metrics. This indicates that compared to current industry-leading models such as GPT-40, our ChatTS achieves significant superiority in overall time series alignment capability. Furthermore, this also demonstrates that even if only synthetic data are used, it is possible to achieve good alignment results with real-world time series.\nAt the same time, we find that the GPT-40 (Vision) model achieves the best result among the baselines. This suggests that existing vision-modal MLLMs have a basic capability to analyze the overall shape characteristics of time series. However, regarding numerical capabilities and detailed analysis (e.g., noise analysis), vision-based models perform slightly worse than text-based models. This is likely because LLMs have a certain degree of numerical computation ability, enabling text-based methods to perform calculations based on the numbers in text format. In contrast, vision-based methods may have limited accuracy in recognizing detailed information from images due to limited image resolution. Refer to Section 5 for a more detailed analysis. Compared with Text/Vision/Agent-based methods, ChatTS features a native multimodal time series capability, directly accepting time series arrays as input. This enables both global and detailed time series information to be preserved. Therefore, through large-scale alignment training, our model surpasses all the baseline models in global and local feature analyses.\nIn multivariate tasks, ChatTS demonstrates more advantages. For text-based models, the encoded input prompt becomes excessively long, requiring a very large context length, making it difficult to summarize the multivariate information accurately. For vision-based models, multivariate time series are plotted simultaneously on a single image, making it difficult to identify features from different time series accurately. In contrast, ChatTS is equipped with a context-aware time series encoding, which helps accurately analyze the referenced time series based on contextual information.\nAgent-based models performed poorly across all tasks compared with vision-based methods. This may be attributed to insufficient tool accuracy and overly long CoT, preventing it from accurately integrating the time series attributes. Refer to Section 4.6 for a more detailed analysis of agent-based models.\nAdditionally, we compared the number of input tokens consumed by different models and the estimated input token costs (the inference cost of ChatTS is estimated based on the official API inference pricing for QWen2.5). The results are presented in Table 3. It can be observed that, due to ChatTS's use of a native multimodal time series encoding, it requires only a minimal number of tokens to encode time series data, resulting in costs significantly lower than"}, {"title": "4.3 RQ2. Reasoning Tasks", "content": "The comparison results of our model and the baseline models for Reasoning Tasks are shown in Table 4. Reasoning tasks are typically more complex and better aligned with real-world application scenarios than alignment tasks. It can be found that ChatTS achieves consistent improvements over the baseline models across all reasoning tasks. In the Inductive Reasoning task, ChatTS achieved a 34.5% improvement compared to the baseline models, indicating that ChatTS can accurately associate time series attributes with their physical meanings in the real world. This demonstrates that the proposed attribute-based time series generation effectively enables the model to understand the patterns of the physical world reflected in time series. Moreover, ChatTS also achieved notable improvements in other reasoning tasks, which indicates that even with only synthetic training data, the model can be equipped with good reasoning capabilities related to time series. This further demonstrates the effectiveness of the proposed attribute-based time series generation method and TSEvol."}, {"title": "4.4 RQ3. Study of Time Series Modality", "content": "To investigate the effectiveness of the time series multimodality in ChatTS, we performed an ablation study based on a text-only version of ChatTS (w/o TS Modality). We remove the time series encoder in ChatTS (i.e. using the original QWen-2.5 model) and use the same training data with ChatTS (the time series arrays are encoded into text) in model training. The experimental results are shown in Figure 9 and Figure 10. Overall, the model using only the text modality performs significantly worse than the original ChatTS model. This indicates that encoding multimodal information is crucial for accurately capturing both shape and numerical information. However, in certain sub-evaluation metrics (e.g., noise), the text-only model outperforms the multimodal ChatTS, suggesting that text modality models still have strong capabilities for identifying small fluctuations. In MTS tasks, the text-only model is nearly incapable of answering any questions. This implies that even with extensive multivariate training data, text-only LLMs still struggle to handle multivariate problems due to excessively long context lengths because of severe hallucinations and inaccurate responses. This further demonstrates the effectiveness of using time series MLLMs in multivariate tasks."}, {"title": "4.5 RQ4. Ablation Studies on Synthetic Training Data", "content": "To further explore the effectiveness of attribute-based time series + text data generation and TSEvol, we conducted the following ablation studies: 1) w/o Attribute-Based. All of the training datasets (PT + SFT) are replaced by the GPT-generated datasets provided in [35]. This dataset was directly generated using GPT-produced Python code. Corresponding Q&As are also generated by GPT according to the code. By replacing all the training data (generated with the attribute-based generator), we can identify the contribution of the attribute-based data generation method to the training results. 2) w/o TSEvol. The SFT datasets (generated with TSEvol) are replaced with an SFT dataset directly generated using an LLM. TSEvol was not used to generate this dataset, but prompts were crafted to encourage the LLM to generate diverse QA pairs. For fairness, we also incorporated the instruct-following dataset into the training set for these ablation studies to ensure the IF capability of the models.\nThe evaluation results of the retrained models are shown in Figure 9 and Figure 10. It can be observed that the model trained on GPT-generated data performed significantly worse across various alignment evaluation tasks compared to ChatTS, in particular for tasks related to local fluctuation and numerical analysis. This is likely due to discrepancies between the GPT-generated time series data and the corresponding captions, which fail to describe local feature details and specific values accurately. Furthermore, we observed that models trained with TSEvol achieved significant improvements in reasoning capabilities compared to other models, along with more minor improvements in alignment tasks. This indicates that TSEvol effectively diversifies the forms of questions and formulates Q&As targeting different time series attributes, enabling the model to achieve better alignment and reasoning performance."}, {"title": "4.6 RQ5. Study of Agent-Based Methods", "content": "Agent-based methods are widely used in current applications. However, in the evaluations for RQ1 and RQ2, we found that the performance of Agent-Based models fell short of expectations. Two primary issues were identified: (1) the tools invoked by Agent models lacked accuracy, and (2) the output format of Agent models did not meet the required specifications, leading to parsing failures.\nTo further investigate the performance upper bound of Agent-Based models, we conducted a more detailed analysis:\n(1) Using attribute labels from the synthetic evaluation data, we developed a set of \u201cperfect tools\u201d with strictly controlled accuracy (by randomly using incorrect labels). Therefore, these tools rely on time series labels and are not applicable in real-world scenarios but are restricted to experimental investigations.\n(2) We removed responses from the Agent model that failed to parse, ensuring all responses were valid.\nEliminating the influence of tool implementation and formatting issues allows us to analyze the performance potential of Agent-Based models precisely. We conducted a sensitivity analysis of the Agent-Based model (GPT-40-mini) using the \u201cperfect tools\u201d with varying accuracy levels. The experimental results shown in Figure 11 reveal that the Agent-Based model is highly sensitive to tool accuracy within the range of [0.9, 1.0]. When the tools achieve perfect accuracy (accuracy = 1.0), the Agent-Based model slightly outperforms ChatTS in numerical metrics. However, its scores in numerical metrics and inductive reasoning tasks remain significantly lower than ChatTS. A critical factor contributing to this performance gap is their poor performance in multivariate tasks. In such cases, an increased number of tool calls and longer CoT tend to occur, leading to hallucinations and inaccuracies.\nThus, even with high-accuracy tools, the performance of Agent-Based models remains extremely sensitive to minor tool errors. Furthermore, in the analysis of multivariate time series, which involves multi-step reasoning and summarization, the overall capability of Agent-Based models is heavily constrained by the reasoning and summarization limitations of the underlying LLM. This significantly restricts their performance. In contrast, ChatTS has native multimodal time series capabilities, enabling it to analyze multiple time series simultaneously, effectively reducing reasoning complexity and improving accuracy."}, {"title": "5 CASE STUDIES AND APPLICATIONS", "content": null}]}