{"title": "Exploring the Learning Capabilities of Language Models using LEVERWORLDS", "authors": ["Eitan Wagner", "Amir Feder", "Omri Abend"], "abstract": "Learning a model of a stochastic setting often involves learning both general structure rules and specific properties of the instance. This paper investigates the interplay between learning the general and the specific in various learning methods, with emphasis on sample efficiency. We design a framework called LEVERWORLDS, which allows the generation of simple physics-inspired worlds that follow a similar generative process with different distributions, and their instances can be expressed in natural language. These worlds allow for controlled experiments to assess the sample complexity of different learning methods. We experiment with classic learning algorithms as well as Transformer language models, both with fine-tuning and In-Context Learning (ICL). Our general finding is that (1) Transformers generally succeed in the task; but (2) they are considerably less sample efficient than classic methods that make stronger assumptions about the structure, such as Maximum Likelihood Estimation and Logistic Regression. This finding is in tension with the recent tendency to use Transformers as general-purpose estimators. We propose an approach that leverages the ICL capabilities of contemporary language models to apply simple algorithms for this type of data. Our experiments show that models currently struggle with the task but show promising potential.", "sections": [{"title": "Introduction", "content": "Many statistical learning settings combine two types of challenges: discovering the underlying persistent structure or representation of the problem, and modeling the context-dependent variability (Bengio et al., 2013). These two factors differ in their generality \u2013 the structure is shared by many cases that might differ in their variability.\nFor example, assume we want to know how long a typical object will take to reach the ground when dropped from a building in some city. Assuming this knowledge has not been directly reported, we must acquire it from the data. We can conduct experiments by dropping different balls from different buildings. Learning involves acquiring two types of knowledge \u2013 one is the physical rules of free fall (e.g., that the mass does not influence the time of the fall), and the other is the distribution of the heights of the buildings in this city (assuming we cannot directly measure the heights). Both types are induced from experiments, but the first is universal, and as such it might already be known based on experiments conducted in a different place.\nRecent Large Language Models (LLMs) are used as general purpose learners, as almost any task that does not require multiple modalities can be formulated as text-to-text or text completion. Therefore, the model must learn both the world and stochastic model for effective learning. As typically models are trained with likelihood-based objectives, the models' output confidence for completions should reflect the underlying distribution of the data.\nThe density estimation task from observations is well-studied in statistics and machine learning. Different methods for parameter estimation differ in their assumptions (or parameterizations). Generally, a model with fewer assumptions can better fit the true unknown) distribution, compared to a model with many assumptions. This is described as having lower bias. On the other hand, when a model is more flexible it is more sensitive to noise in the specific training set, hurting generalization. This is described as having higher variance. This tradeoff between these two phenomena is known as the bias-variance tradeoff. The tradeoff has implications on the sample complexity of a model, as high variance might require training sets with more samples."}, {"title": "Related work", "content": "In pretrained LLMs.\nMany works evaluate and explore the extent to which pretrained LLMs encode world models. Abdou et al. (2021) show a correspondence between textual color representations in LMs and a perceptually meaningful color space. Gurnee and Tegmark (2024) show that LLMs learn linear representations of space and time across multiple scales. Vafa et al. (2024) propose evaluation metrics for world model recovery and show that world model representations are still inconsistent even for models with high accuracy in prediction tasks."}, {"title": "2.1 World Models", "content": "In pretrained LLMs.\nMany works evaluate and explore the extent to which pretrained LLMs encode world models. Abdou et al. (2021) show a correspondence between textual color representations in LMs and a perceptually meaningful color space. Gurnee and Tegmark (2024) show that LLMs learn linear representations of space and time across multiple scales. Vafa et al. (2024) propose evaluation metrics for world model recovery and show that world model representations are still inconsistent even for models with high accuracy in prediction tasks.\nLearning world models from examples. Some works demonstrate the capabilities of Transformers in learning artificial domains with deterministic rules. Demeter and Downey (2021) and Toshniwal et al. (2022a) show that Transformers trained on chess games can learn to track pieces and predict legal moves with high accuracy. Demeter and Downey (2021) additionally demonstrate the capabilities in baseball game states. Li et al. (2024) show that Transformers trained on Othello games can be linearly mapped to the true board.\nLiu et al. (2024b) show that Transformers can successfully learn dynamical systems from in-context sequences alone, thus expanding to probabilistic worlds that describe real events. Patel and Pavlick (2022) show that LLMs have the ability to map descriptions to actions in a grid world, based on in-context examples.\nIntegrating world models and language. Richens and Everitt (2024) show that robustness under distributional shifts requires an approximate causal model of the data generation. Chen et al. (2023) propose methods to incorporate structures, given as Bayesian Networks, into neural networks and apply these methods to tabular and visual data. Wong et al. (2023) propose a framework that combines neural language models with probabilistic models, enhancing the models' ability to capture and utilize world knowledge effectively. Feng et al. (2024) combine examples and natural language instructions to train a chess model. Carta et al. (2023) propose methods for grounding LLMs in interactive textual environments based on online Reinforcement learning."}, {"title": "2.2 Finetuning and In-Context Learning", "content": "A common training paradigm in NLP is to divide training into self-supervised pretraining and task-specific supervised finetuning (Devlin et al., 2019). Brown et al. (2020) showed that large-scale LMs can be used as few show learners, with the task-specific instructions given as a prompt. Performing tasks with prompts only is known as In-Context Learning (ICL) and is gaining popularity (Team et al., 2023; Dong et al., 2024).\nICL is more memory efficient than finetuning and some works argued that it generalizes better (Awadalla et al., 2022). Other works showed that for models with similar sizes, finetuning can generalize well or even better than ICL (Mosbach et al., 2023)."}, {"title": "2.3 Bias-variance Tradeoff", "content": "The bias-variance tradeoff is a fundamental concept in machine learning, where a model's capacity to generalize from training data is balanced against its ability to fit the training data accurately. Some recent work, such as Neal et al. (2019), shows that neural networks can defy the traditional bias-variance tradeoff with increased width. Similarly, Dar et al. (2021) discuss how overparameterization in neural networks can lead to better generalization. The tradeoff is important when considering the inductive biases of different models. Parametric models, which assume a specific form for the underlying distribution, often exhibit different bias-variance characteristics compared to nonparametric models, which do not make such assumptions."}, {"title": "3 The LEVERWORLDS Environment", "content": "As empirical validation for our arguments, we experiment with a case in which the world has a simple structure and assess the sample complexity when using language models. Understanding the underlying world model can significantly reduce the effective sample complexity of the task, allowing accurate estimations with a small amount of training data.\nWe design a framework for generating worlds that enables efficient sampling and estimation. For each generated world, we design models for estimating the distribution of data based on observed samples."}, {"title": "3.1 Setting", "content": "For the general framework, we construct worlds by placing weights on a lever. The lever is placed on a fulcrum with some random number of weights on each side. Each world setting is defined by a causal graph (see Figure 2), where different worlds differ by the number of weights (values of i), the distribution of the variables, and which variables are latent. The variables in the model are density ($\\rho$), volume (V), mass (m), distance from the fulcrum (d), and side (s). $\\rho$, V, m and d are real numbers and s can be $\\pm$1. The torques (T) are determined by the other variables according to the formula T = s \u00b7 d \u00b7 m. For balance (b), b = 1 if $\\sum_i T_i >= 0$ and otherwise b = -1, corresponding the Left and Right, respectively. Masses are determined by the density and volumes, if not latent, according to the formula m = $\\rho$ \u00b7 V.\nAn input x is a sequence of assignments to all the visible variables. We denote the sequence of assignments of the latent variables by l. Given x and l the outcome b is deterministic.\nIn each given world, sampling is straightforward \u2013 we follow the graph and sample the outcome, which is Left or Right. Since the true model is known, we can generate as many samples as we want. This way we can build training sets of arbitrary sizes. During inference, we focus on the output probability given the visible inputs.\nWe note that each world is defined by two components that must be learned. The first is the general structure of how the outcome depends on a fully observable input, determined by the laws of physics. The second is the case-specific variation which depends on the latent distribution.\nThe physical model is common to all the settings and is also faithful to the known laws of physics, so models that incorporate general knowledge do not need any samples for this. The latent model is independent between settings and must be learned based on samples in a density estimation process."}, {"title": "3.2 Evaluation", "content": "Distribution Similarity The true distribution p(y|x) is known. A learning model yields an estimator $\\hat{p}(y|x)$. Evaluation can be done by comparing p and $\\hat{p}$.\nThe main evaluation is simply the distance between the distributions. We decided to use the expected Total-Variation (TV) distance $E_x[d_{tv}(p(y|x),\\hat{p}(y|x))] = \\frac{1}{2} \\cdot E_x [|p - \\hat{p}|]$. Other measures, like the Jensen-Shannon distance, gave similar empirical results, so we decided to use TV due to the simplicity in deriving concentration bounds.\nStructure Similarity We also include an evaluation that addresses the dependency structure of a learned model by measuring the effect of minimal input changes on the output. For example, given a pair of inputs that have the same values except for the mass of an object on the left, then the outputted probability for \u201cL\u201d must be larger for the input with the larger mass.\nFormally, for a set of inputs $\\{x_i\\}_{i=1}^{m}$, we generate a modified set $\\{x_i^*\\}_{i=1}^{m}$, by randomly choosing one index j in $x_i$ and changing it. Denoting the side of the j-th index by $s_j$, we define $\\Delta x_i := s_j \\cdot (x_{i_j} - x_{i_j}^*)$ and $\\Delta p_i := p(b = L|x_i) - p(b = L|x_i^*)$. We know that, for the ground truth model, $sign(\\Delta x_i) = sign(\\Delta p_i)$. The structure score for a model M is then defined as\n$Score = \\frac{1}{m} |\\{ i | sign(\\Delta p_{real}) = sign(\\Delta p_M)\\}|$ (1)"}, {"title": "4 Experiments", "content": "In this section, we present the baseline and Transformer models that we use for the experiments. Transformers, as well as some baseline models, are task-agnostic, whereas some baselines leverage task-specific properties.\nA different way to describe the difference between models is by the strength of their assumptions about the physical setting (but also about the latent variable, since, e.g., the optimal model assumes it is normally distributed)."}, {"title": "4.1 Baseline Methods", "content": "Na\u00efve MLE. In this method, we make no assumptions regarding the relationship between inputs. We do, however, assume that we know what the random variables in the input are. Given this, the method independently estimates a Bernoulli distribution for the output, for each possible input, as the frequency of the output in the data.\nFormally, the estimator for each input x is\n$p(Y = 1 | x) = \\begin{cases}\\frac{N_{x,Y=1}}{N_x} & \\text{if } N_x > 0 \\\\0.5 & \\text{otherwise}\\end{cases}$ (2)\nwhere $N_x$ is the number of samples in which the input is x and $N_{x,Y=1}$ is the number of samples with input x and output 1.\nLinear Models. In another baseline, we perform Logistic Regression (LR) for the output given features of the input. We use polynomial features of degree up to 4. The model assumes simple relationships between the output and the inputs and their interactions. Also in this method, we assume that we know what the random variables are. In contrast to the first baseline (which requires estimators for each possible input), the second baseline requires a small number of parameters.\nMLE with knowledge of the full structure. In this method, the model knows the underlying rules of the output when all the variables are given. This must include the latent variables which we denote by L. Specifically, the function\nq(y|x, l) = 1$\\sum_i T_i > 0\nis provided, where y is the output, x is an input, and l is some value of the latent variables.\nLearning in this model is simply done by estimation of the distribution of the latent variables. Formally, the MLE density estimator for the latent variable l at point c is\np(l = c) = \\frac{1}{N} \\sum_i p(l = c / x_i, y_i)\nwhere the training data is $\\{(x_i, y_i)\\}_{i=1}^N$.\nThe estimator for the output is the marginal $p(y | x) = \\sum_c p(l = c) \\cdot q(x, l = c)$."}, {"title": "4.2 Transformer Fine-tuning", "content": "Our main investigation addresses the capabilities of general-purpose text models in simple tasks.\nFormulation as a text completion task. To formulate the task as language completion, we convert the data into text. We list the visible variables by their names with their values. We use the following template:\nobject1 density: <v\u2081>, object1 volume: <v\u2082>, object1 distance: <v\u2083>, object1 side: <v\u2084>, object1 mass: <v\u2085>, balance: <v\u2086>\nwhere v\u2081, v\u2082, ... represent the corresponding values. The values of the side and balance are given as \"L\" or \"R\".\nWe train generative language models to predict the outcome by generating \u201cL\u201d or \u201cR\u201d and we measure the probability of generating each one.\nWe can add a prompt to give additional information regarding the setting. However, in our experiments, we found that this prompt has little effect on the performance. We therefore report results without it.\nModels. We use the OPT models (Zhang et al., 2022) which come in various sizes. We used the 125m, 350m, 1.3b, and 6.7b parameter models. We used the released weights as initialization and then trained for the task.\nFor training, we use Low-Rank Adaptation (LORA, Hu et al. (2021)) with rank= 64. We trained the models for 10 epochs with learning-rate = 2$\\cdot$10\u207b\u2074. We found that although more epochs improve results, the gain is not substantial for larger epoch numbers.\nWe found that training with randomly initialized model weights is significantly more challenging compared to a pretrained model. We also found LORA to be more stable, compared to finetuning a full model, and it also better preserves the perplexity of language tasks. Therefore we provide results with these settings only."}, {"title": "4.3 Zero-shot Experiments", "content": "We evaluated our learning tasks with off-the-shelf models in two zero-shot settings. In one setting we instructed the model to learn the distribution of the data as in-context prompts. In the second setting, we instructed the model to write functions that parse the data and then input the parsed values to a simple logistic regression model.\nIn-Context Learning. In this setting, the model is given a prompt with a list of observations and is"}, {"title": "5 Results", "content": "Here we report and plot the performance of the various models and methods.\nWe conducted the main experiments on two randomly chosen world settings. The first setting (World-1) has visible variables of mass1, distance1, side1, mass2, and side2. The second setting (World-3) has visible variables of density1, vol"}, {"title": "6 Discussion", "content": "As World-3 is clearly more complex than World-1, it is interesting to see how this affects different models. In Na\u00efve-MLE we see a significant performance gap between the settings, with the"}, {"title": "7 Conclusion", "content": "In this paper, we presented a novel framework for generating experimental worlds from a common physical setting, with easily manipulable distributions. The framework allows sampling from the ground-truth model and enables carefully controlled experiments in learning the distributions. We applied various methods, from classic learning algorithms to various sizes of Transformers. The methods range from highly structure-aware to structure-agnostic.\nOur findings show that even in a very simple physical setting, models that make stronger assumptions as to the structure present better sample complexity. Specifically, in the given setting, simple structure-based models like Logistic Regression and full structure MLE can be substantially more sample-efficient compared to Transformer models. We further propose an approach to leverage LLMs as part of a pipeline that involves a classic learning algorithm. We show that models still struggle with this task but show a promising trend, as newer models show substantial improvements over older ones."}, {"title": "Limitations", "content": "We stress that our experiments with different models differ in the information that is given to the model. For example, the baselines receive tabular data variables whereas the Transformer receives text. Consequently, the comparison is for analysis purposes and is not a strict comparison between methods.\nWe also note that although inspired by real-world physical settings, the data in our experiments is not distributed in anything like naturally occurring text.\nRegarding the zero-shot experiments, we note that the development of our prompt was based on worlds that were not used in the main experiments. However, the worlds are all similar to some extent so the generality of the results is limited."}, {"title": "A Prompts For GPT4", "content": "For In-Context Learning, we used the prompt:\nAssume we have a model representing a lever on a fulcrum, with two objects on it. The first object is on the right and the second is on the left.\nI'll give you a list of partial observations of the states of the model. Notice that some values might be latent. Then I'll ask you to give me the probability for the continuation of some prompt, based on the distribution you can derive from the samples. Be prompt in your answer.\nSamples: <list of samples>\nQuestion: I'll give you a list of prompts. Give me a python list with the probabilities of \"L\", one probability for each"}, {"title": "A.1 In-Context Learning", "content": "For In-Context Learning, we used the prompt:\nAssume we have a model representing a lever on a fulcrum, with two objects on it. The first object is on the right and the second is on the left.\nI'll give you a list of partial observations of the states of the model. Notice that some values might be latent. Then I'll ask you to give me the probability for the continuation of some prompt, based on the distribution you can derive from the samples. Be prompt in your answer.\nSamples: <list of samples>\nQuestion: I'll give you a list of prompts. Give me a python list with the probabilities of \"L\", one probability for each"}, {"title": "A.2 Model Recommendation", "content": "Asking the model for a recommended learning method, we used the prompt:\nAssume we have a model representing physical setting. <add the first hint here> Here's a list of partial observations of the states of the model. <add the second hint here> <add the third hint here> Samples: <list of samples>\nI want to learn the distribution using Statistics or Machine Learning. Specifically, I want to use Logistic Regression to predict the balance probabilities of new samples. Here is an example of the code:\n```python\ndef fit_lr(X, y):\n from sklearn.linear_model \\\n import LogisticRegression\n model = LogisticRegression(\n max_iter=10000,\n solver='saga')\n model.fit(X, y)\n return model\n\ndef predict_lr(model, X):\n return model.predict_proba (X)\n```\nWrite me python function parse_samples(), that parses each sample and creates a feature function that can be used in the snippet above. Make sure the function is appropriate for both training and inference. Give me code only.\nThe hints that were (possibly) provided were:\n(1) We have a lever on a fulcrum with objects on the lever.\n(2) Notice that some variables might be latent.\n(3) Notice that the distance of the last object is latent."}, {"title": "B Theoretical Sample Complexities", "content": "Here we provide theoretical analysis for the sample complexity of the Naive-MLE baseline. For the loss function, we consider the expected squared TV distance.\nThe Naive-MLE estimates an independent distribution for y for each set of values of the other variables, x. For each assignment x, assume we have Nx samples, and estimator is $\\hat{p}(y = 1|x) = \\frac{N_{1_{y=1}}}{N_x}$. As a Bernoulli random variable, we know that the variance of $\\hat{p}$ is $E[(\\hat{p}-p)^2] = \\frac{p(1-p)}{N_x}$.\nThis can be bounded by $\\frac{1}{N_x}$.\nWe have\n$E[TV^2(p,\\hat{p})] = \\frac{1}{4} \\cdot E[(\\hat{p} - p)^2] < \\frac{1}{16N_x}$\nSo, for any $\\epsilon^2 > 0$, if we have $N_x \\geq \\frac{1}{16 \\epsilon^2}$ samples then the expected squared error will be $< \\epsilon^2$.\nNow, we need to bound the probability of $N_x < \\lceil\\frac{1}{16 \\epsilon^2}\\rceil \\leq N^*$, given a total number of samples N. $N_x$ is distributed as a Binomial random variable with parameters $n = N, p_x = p(X = x)$. Following Arratia and Gordon (1989), we can use the tail bound\n$Pr(N_x \\leq N^*) \\leq exp(-N D(\\frac{N^*}{N} || p_x))$ (3)\nfor $N^* < Np_x$.\nSince we assumed i.i.d. for the observed inputs, we can use an identical bound for each case. In the simple case, with 3 input variables with 5 values each (similar to World-1 when combining the distance with the side). For a squared error of less than $\\epsilon^2 = 0.05^2$ and get $N^* > 25$.\nTaking $N^* = 32$, the upper bound in B together with the union bound (for 125 options) gives probability $p > 1 - 0.0092$ for all inputs to have at least 25 samples. This choice for $N^*$ yields $N = \\frac{32}{1 - \\frac{32}{125}} = 4000$ which is similar to the empirical results we got.\nThis same bound for a case with 8 variables (similar to World-3) this bound goes up to $N = 32 \\cdot 5^8 = 1.25 \\cdot 10^7$ samples."}]}