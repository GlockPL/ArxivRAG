{"title": "Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation", "authors": ["Hugo Porta", "Emanuele Dalsasso", "Diego Marcos", "Devis Tuia"], "abstract": "Prototypical part learning is emerging as a promising approach for making semantic segmentation interpretable. The model selects real patches seen during training as prototypes and constructs the dense prediction map based on the similarity between parts of the test image and the prototypes. This improves interpretability since the user can inspect the link between the predicted output and the patterns learned by the model in terms of prototypical information. In this paper, we propose a method for interpretable semantic segmentation that leverages multi-scale image representation for prototypical part learning. First, we introduce a prototype layer that explicitly learns diverse prototypical parts at several scales, leading to multi-scale representations in the prototype activation output. Then, we propose a sparse grouping mechanism that produces multi-scale sparse groups of these scale-specific prototypical parts. This provides a deeper understanding of the interactions between multi-scale object representations while enhancing the interpretability of the segmentation model. The experiments conducted on Pascal VOC, Cityscapes, and ADE20K demonstrate that the proposed method increases model sparsity, improves interpretability over existing prototype-based methods, and narrows the performance gap with the non-interpretable counterpart models.", "sections": [{"title": "1. Introduction", "content": "In the last few years, deep learning-based semantic segmentation has seen rapid adoption in numerous fields, from industrial use cases such as autonomous driving [22,74,81] to the environmental sciences [5, 48, 65]. This expansion was driven by its increasing performance on a multitude of tasks and benchmarks, often coinciding with an increase in model complexity [11, 64, 86, 89]. This favors un-interpretable black-box models, to the detriment of their explainability.\nThe models' lack of interpretability is particularly harmful in high-stakes applications [68, 82] and sometimes prevents the wider adoption of deep learning models in applied fields involving high-stakes decision making [26,63,76]. Indeed, deep learning models can rely on spurious cues, such as Clever-Hans predictors, often symptoms of data contamination [2,49]. This is detrimental in real use cases due to the potential lack of generalization and opaqueness of the decision process. Issues in the generalization of deep learning models are also illustrated by adversarial examples, which can be engineered via small, imperceptible perturbations of images [47] or be inherent in natural images [34].\nThe field of explainable Artificial Intelligence (XAI) aims at alleviating the risks associated with the lack of model interpretability by presenting some aspects of the models' decision process into a form understandable to humans [20]. To produce explainable results, the model design often requires some simplifications that reduce the performance of the original, more complex black-box counterpart [53]. Most methods focus on classification and regression tasks [3, 27, 44, 46, 72], including prototype-based approaches [8]. A few works aim at making semantic segmentation models interpretable [35, 70, 71, 83]. However, no approach considers questions about the relationship between prototypes across semantic classes and scales, despite instances of objects appearing at different positions in the image or at multiple range of distances. Accounting for these redundancies allows for learning more diverse prototypes for a given object across scales, in turn leading to more explicit interpretability [57].\nIn this paper, we tackle these gaps and propose a method for multi-scale interpretable semantic segmentation, ScaleProtoSeg (see Figure 1), that (i) explicitly learns prototypes at several scales and (ii) groups the scale-specific prototypes thanks to a sparse grouping mechanism that provides information on the interaction between prototypical parts at multiple scales, while reducing the number of active prototypes contributing to the decision. Multi-scale prototype learning disentangles the information at different scales so that the model and the users have access to different levels of contextual information in the interpretable decision process. The sparse grouping mechanism allows a transparent understanding of the interaction of scalespecific representations such as object details and parts or similar parts at multi-scale (see Figure 1), while maintaining parts correspondences via regularization, avoiding altogether prototype pruning. We are the first to jointly leverage multi-scale representation and prototype learning in an interpretable semantic segmentation model.\nWe test our methods on three semantic segmentation benchmarks: Pascal VOC 2012 [21], Cityscapes [14], and ADE20K [100], by considering DeepLabv2 [9] as our base model architecture. We first show that multi-scale prototype learning improves the performance of single-scale prototype-based interpretable semantic segmentation methods (with similar amounts of prototypes) across all considered benchmarks. Moreover, thanks to the sparse grouping mechanism, we demonstrate that constraining the decision process to a small group of prototypes per class enforces interpretability while retaining competitive performance. The contributions of our paper can be summarized as follows:\n\u2022 we propose a multi-scale prototype layer that enforces the model to focus on the prototypical parts' representations at multiple scales;\n\u2022 we define a grouping procedure that learns sparse combinations of the scale-specific prototypes across all scales and increases the interpretability of the decision process;\n\u2022 not only we show the superiority of our ScaleProtoSeg method in three popular datasets in semantic segmentation over the prototype-based method [70], but also highlight its improved interpretability measured in terms of stability, consistency and sparsity."}, {"title": "2. Related works", "content": "Semantic segmentation. Fully Convolutional Networks (FCN) [55] are widely used in semantic segmentation methods. They are based on an encoder-decoder architecture, where the encoder extracts discriminative features from the input image and the decoder converts the learned semantic representation into per-pixel predictions. Following FCN, researchers focused on improving different aspects of the semantic segmentation methods such as enlarging the model receptive field while limiting the parameters increase [10,94,96], specifying boundary information [17,78,93], or providing contextual information [32, 52, 95]. Some methods proposed specific modules learning pixel affinities or attention [23,38,54,97] to allow the network to base its prediction also on similar pixels that do not lie in the direct vicinity of the pixel at hand. Furthermore, there has been a growing interest in how multi-scale information could be extracted. A common pattern in decoder architectures is the concatenation of scale-specific feature maps at multiple scales [64, 67, 89]. For instance, Chen et al. [9] introduce atrous spatial pyramid pooling (ASPP) to learn in parallel a multi-scale representation from a single-scale feature map. We focus on providing an interpretable version of this model relying on its widely employed decoder multi-scale architecture.\nExplainable artificial intelligence. XAI methods can be split into post-hoc vs by-design approaches. Post-hoc methods aim to explain black-box models after training by using an auxiliary method to generate explanations, while by-design approaches enforce interpretability in the model itself. Our proposed method falls into the latter category. Some examples of interpretable by-design approaches are concept bottleneck models [43, 46, 57, 62]; attention modules [66, 73, 98, 99] which point to critical parts of each input sample; generalized additive models [3, 31]; and prototype learning introduced in [8,51], where part of an encoded input image is compared to a set of class-specific prototypes, represented by training samples. Several extensions followed the original paper [8], aiming at enforcing orthogonality in the prototype construction [19, 84], reducing the number of prototypes [69], or even leveraging label taxonomy via a hierarchical structure [30]. Prototype learning can be extended to other tasks beyond image classification such as sequence learning [60] and time series analysis [25] and, most recently, semantic segmentation [70]. In this work, we rely on prototype learning for interpretable semantic segmentation.\nInterpretable semantic segmentation. Among the few existing methods tackling this problem, post-hoc approaches extending Grad-CAM to semantic segmentation have been explored [35, 83]. By-design approaches can provide explainable results by leveraging symbolic language [71], through the use of a semantic bottleneck [56] or by exploiting the attention mechanism [28]. Pro-"}, {"title": "3. Method", "content": "In this section, we present our multi-scale grouped prototypes method for interpretable semantic segmentation: ScaleProtoSeg (Figure 2). We first introduce the multi-scale prototype architecture (Section 3.1). Then we describe the proposed grouping mechanism to extract sparse groups of prototypes across scales (Section 3.2). Lastly, we detail the multi-stage training procedure used to learn both prototypes and groups (Section 3.3)."}, {"title": "3.1. Multi-scale prototype learning", "content": "Our model architecture for multi-scale prototype learning is presented in Figure 2: Stage 1. It is composed of a backbone network f, a multi-scale prototype layer gproto, and a linear layer hproto. For an input RGB image $x \\in \\mathbb{R}^{H\\times W\\times 3}$, f outputs a multi-scale feature map $f(x) \\in \\mathbb{R}^{H\\times W\\times S\\times d}$, representing scale-specific features at S scales, each one with d dimensions. The scalar r is a size reduction factor. In practice, we modify the ASPP layer from [9] to concatenate the scale-specific feature maps instead of summing them. At each scale $s \\in \\mathbb{S}$, let $z_s \\in \\mathbb{R}^d$ be a vector from the scale-specific feature map $f_s(x)$. As illustrated in Figure 2, this vector represents the features extracted from an area of the input image corresponding to the receptive field at scale s. The multi-scale prototype layer $g_{proto}$ is composed of M learnable scale-dependent prototypes, where the mth scale-dependent prototype (with $m\\in [1,..., M]$) is described by the vector $p_{s,m} \\in \\mathbb{R}^d$, which is randomly initialized and learned through gradient descent. For each feature vector $z_s$, the prototypes' activations are computed following ProtoPNet [8]:\n$g_{proto}(z_s, p_{s,m}) = log \\bigg(\\frac{1}{1 + \\frac{|| z_s - p_{s,m} ||^2}{|| z_s - p_{s,m}||^2 + \\epsilon}}\\bigg)$\n(1)\nwith $\\epsilon < 1$ a constant for numerical stability. Then, for each scale-specific feature vector $z_s$, the M activation scores $[g_{proto}(z_s, p_{s,1}), ..., g_{proto}(z_s, p_{s,M})]$ are concatenated across the S scales indexed by s. The linear layer $h_{proto}$, with weight matrix $w_{hproto} \\in \\mathbb{R}^{C \\times M \\cdot S}$, learns a mapping from those prototype activations to the Coutput classes probabilities. The classification probabilities map, obtained after processing all feature vectors $z = [z_1,..., z_S]$ in parallel, is of dimension H \u00d7 W \u00d7 C. To upscale it to the original image resolution and produce the final segmentation map, the classification probability is linearly interpolated.\nFor semantic segmentation, the objective is to learn prototypes that are assigned to a specific class $c \\in C$. This is enforced by initializing the weights $w_{hproto}^{(c,m)}$ of the linear layer $h_{proto}$ following [8], and replicating the same assignment to a specific class across all scales, such as with:\n$w_{hproto}^{(c,m)} = \\begin{cases} 1 & \\text{if } p_{s,m} \\in P_c \\\\ -0.5 & \\text{otherwise} \\end{cases}$\nwhere $P_c$ is the set of prototypes that we assign to class $c\\in C$, across all scales and $m\\in [1,...,|P_c|]$. Those weights are frozen for the majority of the training procedure (see Section 3.3) to maintain the steering of the prototypes toward class-specific patterns."}, {"title": "3.2. Prototype grouping", "content": "Our model architecture for prototype grouping is presented in Figure 2: Stage 2. Once the scale-specific prototypes $p_{s,m}$ are learned through the multi-scale learning stage as described in Section 3.1, we group them into sparse groups across scales. For this purpose, we use class-specific grouping functions: $g_c(z) = g_{group}(g_{proto}(z, P_c))$, which group prototypes assigned to the same class and compute"}, {"title": "3.3. Multi-stage training procedure", "content": "In order to train ScaleProtoSeg, we resort to a two-stage training procedure: first, we learn the multi-scale prototypes and project them to the training set patches, without any grouping. Then we learn the prototypes grouping functions with the prototypes fixed. The two stages are illustrated in Figure 2 and detailed below.\nStage 1: Multi-scale prototype learning. For the multi-scale prototype learning we apply a training procedure similar to [8,70], which consists of three steps. First, in a warmup step, the ASPP [9] and the scale-specific prototypes are trained while freezing the rest of the backbone f and the last linear layer $h_{proto}$. Second, we run a joint optimization stage where all the model is trained except the last linear layer $h_{proto}$. Third, for all scales $s \\in S$, the prototypes are projected to their nearest vector z, from the training set, and duplicates are removed. The fine-tuning stage from [70] is not necessary for ScaleProtoSeg, as we replace the last layer $h_{proto}$ with $h_{group}$ in the second stage below. Moreover, contrary to [8, 70], we do not need to run their pruning algorithm, as the sparse grouping mechanism will also enforce a natural decrease in the number of prototypes used."}, {"title": "4. Experiments", "content": "4.1. Experimental setup\nIn all the experiments presented in the section 4.2, we use DeepLabv2 [9] with ResNet-101 [33] pre-trained on ImageNet as the backbone. For the multi-scale prototype learning, we assign M = 3 scale-dependent prototypes per scale to each class and S = 4 scales, so 12 prototypes per class in total. Moreover, we set the number of groups per class to N = 3 for the grouping mechanism. We evaluate ScaleProtoSeg on (i) Pascal VOC 2012 [21] (made of 1464 train, 1449 validation, and 1446 test images with 21 classes; the Pascal VOC training set is extended to 10582 images following [29]), (ii) Cityscapes [14] (composed of 2975 train, 500 validation, and 1525 test images of street scenes, with 19 classes) and (iii) ADE20K (a scene-parsing dataset with 150 fine-grained semantic classes split in ~ 20000 training and 2000 validation images; for the training of ProtoSeg [70] on ADE20K we extend the number of prototypes to 12 for direct comparison with ScaleProtoSeg). A detailed description of the experimental setup is available in the supplementary materials in Section 6."}, {"title": "4.2. Results and discussion", "content": "Method Performance. In Table 1, we present the performance of our interpretable semantic segmentation method ScaleProtoSeg. Due to the well-known lack of faithfulness of saliency-based methods [1, 68] and the difficulty to compare against other by-design methods not based on prototypes (different benchmark datasets, different backbones and/or lack of public code repositories) [56, 71], we compare ScaleProtoSeg against ProtoSeg [70] (the closest previous methods in the literature aimed at prototype-based interpretability for semantic segmentation) and the non-interpretable counterpart DeepLabv2 [9]. It is worth mentioning that, as interpretability comes at the price of constrained and regularized training (namely through prototype projection to real training samples, the use of the diversity loss, and the sparse grouping mechanism), an interpretable model aims to close the gap with the noninterpretable counterpart, which acts as an upper-bound. In this regard, ScaleProtoSeg showcases a substantial improvement over ProtoSeg in terms of mIoU for Cityscapes and ADE20K, namely of ~ 1.5% and 4%. For ADE20K, the proposed ScaleProtoSeg goes beyond its original goal of enabling interpretability by surpassing the performance of its black-box counterpart. The Cityscapes and ADE20K datasets present more natural images with numerous objects at various scales, unlike in Pascal, where our method provides less of an advantage (results are on par with those of ProtoSeg). Indeed, we hypothesize that learning explicitly scale-specific prototypes at multiple scales is more advantageous in segmentation tasks with a large depth of field. In the supplementary materials, we further demonstrate the transferability of ScaleProtoSeg (a) to the medical domain, (b) to another segmentation architecture and (c) to a larger benchmark dataset (COCO-Stuff) in Section 7 and 8.\nThis observation is confirmed by the results presented in Table 2, which showcases the high improvement brought by the projection step of ScaleProtoSeg, performed during the proposed multi-scale prototype learning (see Figure 2: Stage 1). This improvement comes with an increase of solely two prototypes per class from 190 to 228 prototypes on Pascal VOC and 210 (201 after deduplication) to 252 on Cityscapes. Furthermore, in Table 2 we observe that, through the grouping mechanism and thresholding, we reduce the total number of prototypes used by our model by 51.3%, 48.0%, and 37.1% for Cityscapes, Pascal, and ADE20K respectively, compared to 32.6%, 36.7%, and 35.5% for ProtoSeg after deduplication and pruning. As a result, ScaleProtoSeg uses fewer prototypes than ProtoSeg despite a higher initial count except for ADE20K (see Section 4.1). The proposed grouping mechanism (see Figure 2: Stage 2) trades better interpretability and simplified decision process (interaction of only 3 groups per class) with only 0.4% to 0.8% mIoU loss."}, {"title": "5. Conclusion", "content": "In this paper, we present an interpretable semantic segmentation model, ScaleProtoSeg, that leverages multi-scale representations for prototype learning and introduces a novel grouping mechanism to learn prototype interactions across scales. We showed that our model results are particularly advantageous in complex datasets presenting many objects at different depths: Cityscapes and ADE20K. Moreover, through our analysis, we evaluated ScaleProtoSeg interpretability across 3 quantitative metrics: consistency, stability, and sparsity and inspected the different patterns learned by the prototypes across scales. This showcases the potential of multi-scale prototype learning to provide a deeper understanding of the effect of scales on object representations. Lastly, our novel grouping mechanism provides a clear representation of the final decision process: it shows how the model learns to group the prototypes across scales by limiting the number of active groups, allowing for their easy and sparse visualization for all images."}]}