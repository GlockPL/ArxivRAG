{"title": "Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework", "authors": ["Jiang Liu", "Bolin Li", "Haoyuan Li", "Tianwei Lin", "Wenqiao Zhang", "Tao Zhong", "Zhelun Yu", "Jinghao Wei", "Hao Cheng", "Hao Jiang", "Zheqi Lv", "Juncheng Li", "Siliang Tang", "Yueting Zhuang"], "abstract": "Efficient multimodal large language models (EMLLMs), in contrast to multimodal large language models (MLLMs), reduce model size and computational costs and are often deployed on resource-constrained devices. However, due to data privacy concerns, existing open-source EMLLMs rarely have access to private domain-specific data during the pre-training process, making them difficult to directly apply in device-specific domains, such as certain business scenarios. To address this weakness, this paper focuses on the efficient adaptation of EM-LLMs to private domains, specifically in two areas: 1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning. Specifically, we propose a tuning-free, adaptive, universAL Prompt Optimization Framework, abbreviated as IDEALPrompt which consists of two stages: 1) Predefined Prompt, based on the reinforcement searching strategy, generate a prompt optimization strategy tree to acquire optimization priors; 2) Prompt Reflection initializes the prompt based on optimization priors, followed by self-reflection to further search and refine the prompt. By doing so, IDEALPrompt elegantly generates the \"ideal prompts\" for processing private domain-specific data. Note that our method requires no parameter fine-tuning and only a small amount of data to quickly adapt to the data distribution of private data. Extensive experiments across multiple tasks demonstrate that our proposed IDEALPrompt significantly improves both efficiency and performance compared to baselines.", "sections": [{"title": "1 Introduction", "content": "In recent years, multimodal large language models (MLLMs) have achieved significant advancements, particularly in cross-modal information understanding and integration (Huang et al., 2023; Gong et al., 2023; Ye et al., 2023). Efficient multimodal large language models (EMLLMs) significantly reduce model size and computational costs compared to MLLMs, making them ideal for deployment on resource-constrained devices for enhanced processing efficiency (Jin et al., 2024). However, the scenarios encountered by different devices, including various private business contexts, can vary significantly. Moreover, ethical considerations and the necessity to protect trade secrets prevent the use of device data for pre-training by open-source, general-purpose EMLLMs. Consequently, a discrepancy between the distribution of pre-training data and device-specific data emerges, leading to reduced performance or even failure when EMLLMs process this unique and highly private domain data.\nA viable solution involves utilizing EMLLMs, e.g., InternVL2-2B (Chen et al., 2023b, 2024) and Qwen2-VL-2B (Wang et al., 2024), to perform either full-parameter (Chung et al., 2024; Zhang et al., 2024b) or efficient fine-tuning (Hu et al., 2021; Lin et al., 2024a) on private device data for domain-specific adaptation (Chang et al., 2019; Zhang et al., 2022, 2024c; Li et al., 2023). However, this approach incurs large training costs, necessitates large-memory GPUs, and entails long training times. The elevated training costs significantly increase the application threshold, thereby restricting the availability and generalizability of such methods on private domain data. Therefore, our goal is to minimize the adaptation cost for EM-LLMs when deployed on devices utilizing private domain-specific data.\nIn response to the aforementioned challenges, we propose a tunIng-free, adaptive, universAL Prompt optimization framework, referred to as IDEALPrompt. Our bootstrapping philosophy aims to strengthen EMLLMs' capability in private domain tasks by progressively and adaptively optimizing the prompt. IDEALPrompt consists of two progressive optimization steps: i) Reinforcement Warm-up Strategy (RWS), which employs a reinforcement learning tree search strategy for the general prompt optimization, encouraging combi-"}, {"title": "3 Methodology", "content": "This section describes the details of IDEALPrompt (Figure 2). We will present each module and its optimization procedure."}, {"title": "3.1 Problem Formulation and Notations", "content": "Data. The input query of the on-device private data is represented by $X$ (e.g., image for vision-language models). High-value critical bad cases sampled by IDEALPrompt from $X$ is represented by $X_{sub} \\subseteq X$. Correspondingly, we use $Y$, and $y_{sub}$ to represent their ground truths. For a sample, $x$ and $y$ represent a sample in $X$ and $Y$ respectively. Given a multimodal understanding task-t that is characterized by private domain data distribution $D_t$. Prompt set is represented by $V$, and a prompt in the $V$ is represented by $v$.\nModel. The EMLLM adapted to $D_t$ is represented by $G_E$, with parameters $\\Theta$. The general MLLM that assists the adaptation process of $G_E$ is represented by $G_M$. $f_E(\\cdot)$, $f_M(\\cdot)$ represents the forward propagation process of $G_E$, $G_M$ respectively, for example, $f_e(v, x; \\Theta)$ is the process of the prediction made by $G_E$ based on the prompt $v$ and the input query $x$ with parameters $\\Theta$.\nFormula. Our proposed IDEALPrompt avoids parameter fine-tuning; therefore, the comparison of formulas between parameter fine-tuning and IDEALPrompt is as follows:\nParameter Fine-tuning: Optimize the parameters $\\Theta$ based on the training data $X$.\n$\\max_{\\Theta} E_{([v,x],y) \\sim D_t}h(f_E(v, X; \\Theta), Y)$ (1)\nIDEALPrompt: Find the optimal $v$ and $X_{sub}$, without parameter fine-tuning and without optimize on the whole dataset.\n$\\max_{v \\in V, X_{sub} \\subset X} E_{([v, x_{sub}],y_{sub}) \\sim D_t}h(f_E(v, X_{sub}; \\Theta), y_{sub}),$ (2)\nwhere $h(\\cdot)$ is score function applied to measure the alignment between the EMLLM's output $f_e(v, x; \\Theta)$ and the ground truth $y$."}, {"title": "3.2 IDEALPrompt", "content": "We propose IDEALPrompt, a tuning-free, adaptive, prompt optimization framework for EMLLMs, which consists of two progressive optimization stages. Specifically, at the outset, several prompt optimization strategies are designed using human priors, referred to as the Strategy Pool S, which directly guides the prompts' optimization direction. Building on this, in the first stage, Reinforcement Warm-up Strategy (RWS), a strategy tree search based on reinforcement learning is conducted to acquire prior knowledge of optimization strategies. In the second stage, Empirical Self-reflective Optimization (ESO), the error distribution derived from the validation inference results is analyzed to identify critical bad cases, and self-reflection is subsequently applied to refine the prompts.\nDuring the optimization process, a highly capable general MLLM is utilized as the prompt optimizer, while EMLLMs are employed as the prompt inference model."}, {"title": "3.2.1 Human-aligned Strategy Pool", "content": "While some works have used MLLMs for text gradient-based prompt optimization, we instead leverage human-designed prompt optimization strategies constructed a priori that effectively optimize prompts.\nThe work of (Schulhoff et al., 2024) indicates a wide range of prompt optimization strategies. We carefully select several representative strategies that are beneficial for the private domain, as identified by human experts, to form our Strategy Pool $S = \\{S_1, S_2, ..., S_k\\}$, where $s_k$ denotes each individual strategy. Specifically, we identify the following optimization strategies: Reasoning, Reinterpretation, Simplification, Role-Prompting, Decomposition, Self-Criticism, Caption and Rephrasing. The details of the strategies are provided in Appendix A.2.1.\nNote that the strategies in strategy pool can be freely modified, removed or expanded to flexibly adapt various task requirements. In this work, we showcase the aforementioned strategies adapting to private domain data as our strategy pool."}, {"title": "3.2.2 Reinforcement Warm-up Strategy", "content": "Based on these human-aligned strategies within the strategy pool, Reinforcement Warm-up Strategy (RWS) conducts a reinforcement learning exploration-exploitation strategy tree search across tasks and models to optimize prompts, as illustrated in Figure 2 (a).\nSpecifically, the strategy nodes $s_k \\in S$ constitute the action space of reinforcement learning, while the evaluation results of private domain task-t on EMLLM-$G_E$ are considered action rewards. The reward distribution of actions is maintained in the memory module $M$. Note that we maintain a distinct distribution for each task type within each EMLLM, i.e., $M = \\{M_{t_0,G_{E_0}}, ..., M_{t_n,G_{E_m}}\\}$.\nIn the initial state of RWS, the framework has not encountered any private domain tasks nor performed any strategy search on any EMLLMs, i.e., the memory module $M$ is empty. When the EMLLM-$G_{E_0}$ encounters the first private domain task-$t_0$, the framework performs a complete strategy tree search based on the human-aligned strategies within the strategy pool, simultaneously updating the action-reward distribution $M_{t_0,G_{E_0}}$:\n$M_{t_0,G_{E_0}} = \\xi(S, t_0, G_{E_0}),$ (3)\nwhere $\\xi$ denotes the tree search operation.\nFor a new private domain task-$t_n$ on a new EMLLM-$G_{E_m}$, the framework utilizes prior information stored in the memory module $M$ and applies an $\\epsilon$-greedy strategy to balance exploration and exploitation. Specifically, the general MLLM $G_M$ first selects the distribution with the highest task-model similarity to task-$t_n$ and EMLLM-$G_{E_m}$ from the existing memory module $M$ as a reference:\n$M_{ref} = arg \\max_{M_{t,G_E} \\in M} \\rho((t, G_E), (t_n, G_{E_m})),$ (4)\nwhere $\\rho$ denotes the task-model similarity.\nThe distribution is then smoothed and homogenized based on task-model similarity to reduce bias:\n$M_{t_n,G_{E_m}} = \\sigma(M_{ref}, \\rho((t, G_E), (t_n, G_{E_m}))),$ (5)\nwhere $\\sigma$ denotes the smooth operation.\nThis distribution is used as the action-reward distribution $M_{t_n,G_{E_m}}$ for the private domain task-$t_n$ and EMLLM-$G_{E_m}$. An $\\epsilon$-greedy search is then performed based on this distribution, offering a $1-\\epsilon$ probability of selecting one of top-k historically effective strategies for exploitation, while with a $\\epsilon$ probability, the framework randomly searches other strategies for exploration:\n$S^* =  \\begin{cases} K(M_{t_n,G_{E_m}}) & \\text{prob. } 1 - \\epsilon, \\\\  (S \\backslash K(M_{t_n,G_{E_m}})) & \\text{prob. } \\epsilon,  \\end{cases}$ (6)\nwhere $k$ denotes the selection one of top-k operation, $\\epsilon$ denotes the random selection operation. $s^*$ represents the selected strategies.\nRWS fully utilizes prior knowledge of prompt engineering strategies. Compared to other search methods, it is more transparent and explicit, enhancing the interpretability of the prompt optimization process. We expect RWS to exhibit a degree of transferability across different private domain tasks and models, similar to pre-training in the MLLMs' training paradigm. This stage allows for the acquisition of a more generalizable combination of prompt strategies for private domain-specific tasks, showcasing significant advantages in multimodal, multi-task understanding in real-world business scenarios."}, {"title": "3.2.3 Empirical Self-reflective Optimization", "content": "After the RWS, we obtained the coarse optimized prompts as well as optimized priors for various private domain tasks and EMLLMs. Building on this foundation, Empirical Self-reflective Optimization (ESO) further refines the prompts by selecting critical bad cases from the error distribution of evaluation results and leveraging EMLLMs' self-reflection to better adapt to private domain data.\nSpecifically, the process of ESO is as follows: First, the coarse optimized prompts obtained from RWS are used as the initial prompts for ESO.\n$v^* = v_0 + S^*$ (7)\nBuilding upon this foundation, ESO begins iterative optimization with the EMLLM first performing evaluation on the validation set. During the optimization process, historical evaluation results of the validation set are considered, including previous prompts, accuracy rates, and error distributions. Optimization by the general MLLM $G_M$ is carried out based on two key aspects:\n*   i) Global Error Distribution Learning: the general MLLM $G_M$ analyzes error evaluation results to identify and assess patterns in error distribution of private domain, uncovering task descriptions and labels that are commonly misunderstood or not well comprehended by EMLLMs. This macro-level analysis provides insights into the EMLLM's weaknesses within specific tasks, guiding further optimization.\n*   ii) Local Case-based Learning: the general MLLM $G_M$ selects critical bad cases according to error distribution based on the principles of typicality and diversity to identify confusing or distinctive private domain bad cases. These cases are analyzed alongside ground truth labels to diagnose errors, refining private domain tasks and label definitions as needed.\nThe optimization process can be formalized as follows:\n$R(v_0) = E_{([v_0^*,x_{sub}],y_{sub}) \\sim D_t}h(f_E(V_0, X_{sub}; \\Theta), y_{sub})$\n$v_{i+1} = v_i + \\nabla_{v_i}R(v_i) \\text{ for } i = 0, 1, ...$ (8)\nThis process iterates until it reaches the specified number of iterations.\n$T(v_i) = \\text{True if } \\text{max_iterations} \\leq i$ (9)\nThus, ESO can utilize the priors obtained from RWS to develop fine-tuned prompts for unknown private domain tasks with minimal search steps, similar to supervised fine-tuning in the MLLMs' training paradigm."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experimental Setting", "content": "Datasets and Tasks. As our primary objective is to address tasks focused on multimodal content understanding of the private domain, we collect external data from Taobao's private domain involved multimodal content understanding and propose a benchmark, namely Taobao Private Domain Adaptation (Taobao-PDA) benchmark, encompassing 7 multimodal content understanding tasks across 3 categories, where the ground truth are labeled by GPT-4\\documentclass{article} \\begin{document}  4 \\end{document}  and further refined by human annotation. The detailed tasks are as follows:\n*   Image: Graphic Layout, Commodity Location.\n*   Commodity: Commodity Information, Commodity Background, Commodity Border, Concatenation Situation.\n*   Outfit: Outfit Style.\nEach task can be viewed as a visual question answering task in a distinct private domain of understanding. Figure 3 shows the detailed characteristics of Taobao-PDA, with additional details provided in Appendix A.11.\nBaselines. We compare IDEALPrompt with several prompt engineering baseline methods: (1) MM-CoT (Zhang et al., 2023), which generates rationales before generating the final answer; (2) APE (Zhou et al., 2022), which generates instruc-"}, {"title": "4.2 Main Results", "content": "We evaluate the performance of IDEALPrompt on the Taobao-PDA benchmark compared to the baseline methods, as shown in Table 1. Our observations are summarized as follows: (i) IDEALPrompt achieve the highest average performance"}, {"title": "4.3 Ablation Study", "content": "To investigate the effectiveness of the two-stage framework, we conduct an ablation study comparing the complete two-stage framework with versions that omit one of the stages. Table 2 shows that both stages of IDEALPrompt are effective, and combining both stages outperforms using each stage individually."}, {"title": "4.4 In-Depth Analysis", "content": "Analysis of the Strategy Optimization in RWS. We first compare the prompt optimized with a single strategy against the resulting coarse optimized prompt on the training set, as illustrated in Figure 4-(a). The ensemble strategy tree, derived through exploration-exploitation search, achieves a performance of 46.2, outperforming other individual strategies. Furthermore, we compare the exhaustive search of the strategy tree with the exploration-exploitation-based search, as illustrated in Figure 4-(b). For new tasks, exhaustive search through the strategy tree requires over 90 search steps, whereas our method needs only about 10 steps, achieving performance comparable to that of the exhaustive search.\nEffectiveness of Each Component in ESO. We conduct a performance comparison to evaluate the impact of the absence of various components within ESO. Specifically, we considered the following components during the generation of new prompts: (i) without selection of bad cases; (ii) random selection of samples as bad cases; (iii) without using error distribution; and (iv) without using historical inference results. Figure 4-(c) sequentially shows the performance improvement derived from the analysis of selected bad cases, while Figure 4-(d) sequentially shows the performance improvement derived from analyzing conclusions based on historical evaluation results.\nAdditional Baseline Comparision. We conduct the comparison of the average performance, inference time, and GPU memory usage between"}, {"title": "4.5 Case Study", "content": "Figure 5 shows a case of prompt optimization using the IDEALPrompt framework. The initial prompt requests the EMLLM to determine the value of the \"overall graphic layout label of the overall image information category\" based on the input image. After RWS, the optimal strategy combination, \"Role-Prompting + Decomposition\", is identified. Subsequently, RWS refines the label by providing a detailed definition, ultimately producing the final optimized prompt."}, {"title": "5 Conclusion", "content": "In this paper, we propose IDEALPrompt, a tuning-free, adaptive, universal prompt optimization framework consisting of two stages, which focuses on boosting EMLLMs' content understanding of private domain data. Specifically, IDEALPrompt incorporates human experts' prior optimization strategies along with a reinforcement learning-based strategy tree search and utilizes the model's self-reflection to refine prompts. In addition, we propose Taobao-PDA benchmark to study the private domain adaptation of IDEALPrompt. Experimental results demonstrate that IDEALPrompt outperforms existing prompt optimization approaches while significantly improving optimization efficiency."}, {"title": "Limitations", "content": "Our limitations and potential risks are as follows:\n*   Large Consumption of API Calls. Our method involves extensive searching and still requires a large number of API calls (although this is much less than existing methods), which is a common issue in automatic prompt optimization.\n*   Burden of human-aligned strategy definition. Strategies defined by human experts are intended to provide models with human-derived optimization priors. This is because humans can usually intuitively perceive better descriptions, but optimizing them directly is difficult. However, this increases the burden of manual definitions, thereby automatic construction of external strategies will be a key area for our future research."}, {"title": "Ethics Statement", "content": "In the process of collecting Taobao-PDA, we obtained TaoBao's permission and carried out desensitization processing. In addition, racial discrimination and gender discrimination were eliminated. Especially for the data of the outfit category involving people, we only consider some clothes with distinct characteristics, regardless of gender or race."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Data Detail", "content": ""}, {"title": "A.1.1 Task Definition", "content": "Figure 6 illustrates the detailed definition of tasks."}, {"title": "A.1.2 Initial Prompt", "content": "Our zero-shot initial prompts of various tasks are shown in Table 7."}, {"title": "A.2 Strategy Detail", "content": "In RWS, part of strategies use the {initial prompt + strategy prompt} as the strategy-based prompt, while other utilize GPT-4 to generate the strategy-based prompt. The top-k value is set to 3. \u03b5 is set to 0.3, meaning there is a 0.7 probability of exploitation and a 0.3 probability of exploration. Task-model similarity is assessed by GPT-4."}, {"title": "A.2.1 Strategy Definition", "content": "Detailed definition of strategies are followed:\n*   Reasoning represents that the prompt requires MLLMs to explicitly articulate their logical process and rationale within the output, enabling a deeper understanding of the question's essence and ensuring the response's validity.\n*   Reinterpretation represents that the prompt requires MLLMs to re-read and reinterpret the question before answering, ensuring comprehension is both accurate and contextually appropriate.\n*   Simplification represents that the prompt requires MLLMs to remove irrelevant information, ensuring that the content is concise and directly focused on the question.\n*   Role-Prompting represents that the prompt assigns a specific role to MLLMs, guiding their approach to generating responses and framing the context accordingly.\n*   Decomposition represents that the prompt requires MLLMs to decompose complex questions into simpler sub-instructions, addressing each sub-question individually for clarity and precision.\n*   Self-Criticism represents that the prompt requires MLLMs to critically reflect on their responses, identifying and addressing potential weaknesses or errors.\n*   Caption represents that the prompt requires MLLMs to provide a detailed description or caption of an image before generating an answer, en-"}, {"title": "A.2.2 Strategy Prompt", "content": "Our specific prompts to add strategies by general MLLMs are shown in Table 8."}, {"title": "A.3 Self-reflective Detail", "content": "The number of critical bad cases is set to 5."}, {"title": "A.3.1 Self-reflective Prompt", "content": "Table 9 and Table 10 shows the error analysis prompt and error summary prompt in Empirical Self-reflective Optimization, respectively."}, {"title": "A.4 Additional Experimental Results", "content": "Human Evaluation. We further conduct a human evaluation on Taobao-PDA. Specifically, we recruit 6 well-educated people to rank the randomly shuffled prompts optimized by APE, OPRO and IDEALPrompt. The scores range from 1 to 10 (10 means best) and are allowed to be equal for comparable instances. As shown in Figure 7. IDEALPrompt demonstrates best performance in human evaluation as well.\nExperiments on Public Domain Benchmark. To evaluate the effectiveness and universality of IDEALPrompt, we conducted experiments on public domain benchmark, comparing with zero-shot and OPRO. BBH is a public domain benchmark which is often used for verifying prompt optimization; therefore, we selected part of BBH tasks for evaluation on InternVL2-2B. Following OPRO, we select 20% of the data for each task as the training set, with the remaining 80% used as test set. Consequently, we present accuracies in the format of \"training / test / overall (training + test),\" as illustrated in Table 11. The results indicate that IDEALPrompt still achieves better performance on public domain benchmark, demonstrating its effectiveness not only on private domain-specific tasks."}]}