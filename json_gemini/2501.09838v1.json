{"title": "CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified Intermediate Representation", "authors": ["Alex Berian", "Daniel Brignac", "JhihYang Wu", "Natnael Daba", "Abhijit Mahalanobis"], "abstract": "Geospatial imaging leverages data from diverse sensing modalities-such as EO, SAR, and LiDAR, ranging from ground-level drones to satellite views. These heterogeneous inputs offer significant opportunities for scene understanding but present challenges in interpreting geometry accurately, particularly in the absence of precise ground truth data. To address this, we propose CrossModalityDiffusion \u00b9, a modular framework designed to generate images across different modalities and viewpoints without prior knowledge of scene geometry. CrossModalityDiffusion employs modality-specific encoders that take multiple input images and produce geometry-aware feature volumes that encode scene structure relative to their input camera positions. The space where the feature volumes are placed acts as a common ground for unifying input modalities. These feature volumes are overlapped and rendered into \"feature images\u201d from novel perspectives using volumetric rendering techniques. The rendered feature images are used as conditioning inputs for a modality-specific diffusion model, enabling the synthesis of novel images for the desired output modality. In this paper, we show that jointly training different modules ensures consistent geometric understanding across all modalities within the framework. We validate CrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset, demonstrating its effectiveness in generating accurate and consistent novel views across multiple imaging modalities and perspectives.", "sections": [{"title": "1. Introduction", "content": "Geospatial imaging has become increasingly abundant, with datasets spanning a wide range of image sensing modalities such as electro-optical (EO) [16], synthetic aperture radar (SAR) [27], and Light detection and ranging (LiDAR) [10]. Each sensing modality uniquely captures critical features of the Earth's landscape, making them indispensable for comprehensive scene analysis. However, understanding the underlying geometry of scenes from sparse images from various modalities remains a challenging problem.\nNeural Radiance Fields (NeRF) [25] revolutionized novel view synthesis (NVS) by learning scene geometry exclusively from images, enabling high-quality view generation. However, NeRF's reliance on dense input images and inability to extrapolate beyond observed viewpoints limits its utility in many scenarios. In contrast, GeNVS [6] explicitly models scene geometry, allowing it to infer structure from sparse inputs. GeNVS uses an encoder to produce geometry-aware feature volumes from each input image, which are aligned and rendered via volume rendering to create a feature image for a novel view. This feature image is then processed by a diffusion model [17, 20] to generate the final output.\nWhile NeRF and GeNVS have shown remarkable results for NVS with EO images, they work on a single imaging modality. Extending NVS to multiple imaging modalities introduces the problem of multi-modal novel view synthesis (MMNVS), where input and output images may belong to different imaging modalities. Existing approaches like GeNVS face significant limitations in MMNVS due to their dataset-specific nature, requiring retraining for each new modality. Additionally, their tightly coupled architecture restricts generalization.\nIn this work, we present CrossModalityDiffusion, a modular framework designed for MMNVS. We validate CrossModalityDiffusion on the ShapeNet cars [7] dataset, which we render in EO, perspective LiDAR (LiDAR(P)), range-angle LiDAR (LiDAR(RA)) and SAR modalities. Our results demonstrate that CrossModalityDiffusion effectively synthesizes accurate and consistent novel views across these diverse imaging modalities, showcasing its capability for generalized MMNVS.\nIn Figure 1 we observe CrossModalityDiffusion operating on EO to SAR, LiDAR(RA) to LiDAR(P), and LiDAR(P) to EO. We see from the feature images in Figure 1 that no matter the input modality, CrossModalityDiffusion produces geometrically consistent feature volumes.\nCrossModalityDiffusion decouples the three primary components of the GeNVS architecture (1) the encoder, (2) feature image rendering, and (3) the denoising diffusion model allowing independent modality-specific modules for input and output modalities. Namely, we use specific encoders and denoisers for each image modality. The feature volumes produced by the encoders are modality-agnostic, allowing for modality fusion. By jointly training the modules within the framework, we ensure the encoder modules learn consistent and transferable geometric representations for the different imaging modalities."}, {"title": "2. Related Work", "content": "NeRF [25] revolutionized novel view synthesis by implicitly training a multi-layer perception (MLP) to map 3D coordinates and view directions to color and density for volume rendering. While NeRF achieved state-of-the-art (SOTA) quality in NVS, it requires a dense set of input images to render photo-realistic views. Additionally, NeRF is scene-specific and cannot generalize to unseen targets. Many works incrementally improved NeRF in image quality [2, 3, 14, 41], training time [14, 37], and extended its capabilities [14, 29, 37, 42]. We particularly focus on few-shot NVS, the problem of generating novel views of a scene when given one or a small number of source images. Pixel-NeRF [44] addressed this problem by training an image encoder with a NeRF module for scene-agnostic performance. GeNVS [6] utilizes a PixelNeRF-like pipeline to produce geometry-aware priors for a powerful diffusion model [20] for few-shot NVS.\nDiffusion models are now the standard in high-quality image generation [17, 20, 26, 35, 36] by iteratively denoising the raw image space. Latent diffusion models (LDMs) [21, 31] instead iteratively denoise a latent space representation of data, efficiently generating high-quality images. DreamFusion [28] presented score-distillation-sampling (SDS), which showed that a web-scale text-to-image latent diffusion model [31] implicitly learned 3D information. SDS uses a fixed diffusion model as a critic to train a NeRF. SparseFusion [46] adopts SDS for few-shot NVS by conditioning the LDM with a view-aligned feature grid. Zero-1-to-3 [24, 32] extends the concept in SparseFusion to re-train the LDM with camera pose embeddings to produce better SDS scenes. DreamGaussian [38] uses the concept of SDS for Gaussian splatting [23] scenes instead of a NeRF.\nMore recently, various works [15, 43] show that when a diffusion model outputs better novel views, those images can be used to directly train a NeRF to get better results than SDS. Multi-view diffusion models, first presented in MVDream [33], utilize cross attention [12, 40] over multiple views of a scene to train a more 3D-aware diffusion model. CAT3D [15] uses a multi-view diffusion model trained at a web-scale to directly produce images for training a NeRF on any scene.\nPlanar scene representation methods [4, 5, 14, 18, 39], similar to PixelNeRF [44], project render points onto planes to interpolate feature vectors as input to a NeRF model. EG3D [5] trains a Style-GAN [22], to predict 3 planes to represent scenes and fixed NeRF to render the scenes. Large reconstruction models [4, 18, 39] use a similar concept to EG3D but utilize a large visual transformer [12] instead of a GAN."}, {"title": "3. Problem Statement", "content": "NVS is the problem of inferring a target image $x_t \\in \\mathbb{R}^{(128,128,3)}$ from S source images $\\{x_{si}\\}_{i=1}^S$. The target images and source images have associated camera pose matrices $P_t \\in \\mathbb{R}^{(4,4)}$ and $\\{P_{si}\\}_{i=1}^S$ respectively. We seek to create a NVS model $N_\\theta$ with parameters $\\theta$ that predicts novel views\n$x_t = N_\\theta(P_t, \\{x_{si}\\}_{i=1}^S, \\{P_{si}\\}_{i=1}^S)$.\nMMNVS extends the problem of NVS to where the images may come from M different modalities. Each source and target image has an associated modality, $(x_{si}, m_{si})$ and $(x_t, m_t)$ where $m \\in \\{1, 2, ..., M\\}$. In this paper, we assume modality information is known to the model, so NVS expression in (1) can be rewritten for MMNVS as\n$x_t = N_\\theta(P_t, m_t, \\{x_{si}\\}_{i=1}^S, \\{P_{si}\\}_{i=1}^S, \\{m_{si}\\}_{i=1}^S)$."}, {"title": "4. Background: GeNVS", "content": "GeNVS [6] encodes each source image $x_{si}$ image into feature volumes $W_i \\in \\mathbb{R}^{(128,128,64,16)}$ (point clouds of 16-dimensional features) using a modified DeepLabV3+ [8, 9, 19] segmentation model. The volumes are oriented within the camera field-of-view (frustrum) of their source camera poses $P_{si}$ and a dataset-specific $z_{near}$ to $z_{far}$.\nA 16-channel feature image $F \\in \\mathbb{R}^{(64,64,16)}$ of the feature volumes is rendered via volume rendering from a target camera pose $P_t$. Stratified sampling is used to select points along each ray $r$. A point $r \\in r$ is trilinearly interpolated in each feature volume to get a latent vector $W_i(r) \\in \\mathbb{R}^{16}$. The latent vectors for each feature volume are averaged together, then passed through a MLP $f$ to get a 16-channel color $c$ and density $\\sigma$.\n$c(r), \\sigma(r) = f(\\sum_{i=1}^S W_i(r)w_i)$.\nOnce the color and density of every point along a ray $r$ are predicted, volume rendering is used to predict the 16-channel pixel value $C(r)$. The ray rendering formula follows the NeRF [25] equations\n$C(r) = \\sum_{i=1}^N T_i (1 - e^{-\\sigma_i\\delta_i}) C_i$,\nwhere, $\\sigma_i$ and $c_i$ are the MLP output of the $i^{th}$ sample along the ray from the camera, where\n$T_1 = e^{-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j}$,\nand $\\delta_i = t_{i+1} - t_i$ is the distance between the $i^{th}$ and $(i + 1)^{th}$ samples.\nThe 16-channel feature image is used as conditioning input to a denoising diffusion [20] U-Net $U$ by concatenating it with 3-channel noise (or noisy target image in training). The denoising U-Net $U$ outputs 3 channels. When sampling new images with multiple denoising steps, the feature image is skip-concatenated to the U-Net's $U$ input. The final output target image $\\hat{x}_t$ is the output from the last denoising step."}, {"title": "5. Method", "content": "To better understand how we adapt GeNVS for MMNVS, we must first examine the dataset used to train the framework.\nFor the model to generalize to unseen images from unseen scenes, we need to first collect images from a variety of scenes. Then from each scene, we need to take a lot of images of it from different viewpoints. From each viewpoint, we need to use different sensors to capture the scene (EO, LiDAR, SAR, etc).\nIn our experiments, we use the ShapeNet Cars dataset. We get the EO and corresponding poses from the SRN-Cars [34] dataset. We then use BLAINDER [30] to generate LiDAR(RA)/LiDAR(P) images and RaySAR [1] to generate SAR images. This is shown in Figure 2."}, {"title": "5.2. Architecture", "content": "GeNVS has three main components. The encoder, feature image generation, and the denoising diffusion model. We begin building the model by training an unmodified GeNVS architecture from scratch exclusively on EO images. This allows for faster learning when training on other modalities. Once the EO-only GeNVS model is pre-trained, we initialize a new encoder and denoiser module for each input and output modality from the pre-trained EO encoder. This architecture is shown in Figure 3.\nGeNVS is composed of three major modules: the encoder, the MLP, and the denoiser. By allocating an encoder and denoiser module for each modality, we obtain a modular framework of adapters for MMNVS.\nTo unify the intermediate representation from the encoders, we only use one MLP to process the overlapping feature volumes created by the different modality encoders. Then according to the target modality, we select the appropriate denoiser for conditional diffusion."}, {"title": "5.3. Joint Training", "content": "To allow our framework to input multiple images from different modalities at the same time then output a novel view of the scene in any modality, we jointly train the encoder and denoiser modules for each modality at the same time.\nDuring training, we randomly select between one and three input images from random views and modalities, as well as one random target view and modality. This way, the encoders are incentivized to generate the same feature field despite the different source image modalities. By jointly training the different modules with random input and target modalities, we can create a unified, modality agonistic, implicitly learned intermediate representation of the scene.\nAny encoder can encode an image of its modality into the intermediate representation and any denoiser can decode any feature image of a novel view into an image of its modality."}, {"title": "6. Experiments", "content": "We follow the distribution of train/validation/test as in the SRN-Cars [34] dataset. EO data is directly taken from the SRN-Cars dataset, and we render the same viewpoints in other modalities. We use RaySAR [1] and BLAINDER [30] for generating SAR and LiDAR images respectively. At present, we use two modes of LiDAR images in the experiments: range-angle (RA), and perspective (P) images. For LiDAR and SAR images, we repeat the single-channel image across three channels for compatibility purposes.\nAll experiments are conducted on fixed modules trained with the joint training process described in section 5.3. Although the evaluation metrics used are primarily used for EO images, they also function as a useful metric for MMNVS."}, {"title": "6.1. Single Modality In, Single Modality Out", "content": "Results when all input images to CrossModalityDiffusion are of the same modality are shown in Table 1. There is only one output image at a time from the framework, so it is trivial to say all output images are the same modality.\nWe observe from Table 1 that the easiest of these tasks is EO in EO out (as shown by the 19.66 PSNR). This is because volume rendering captures color information, but not depth. The toughest tasks are any other modality in EO out. This is because the input images from non-EO modalities contain no color information. Images of a black and white car with otherwise consistent geometry would yield a large pixel difference."}, {"title": "6.2. Range-angle Images", "content": "Not all sensor modalities produce perspective-projection alike images. We challenge our framework to also work on range-angle images with no modification. We find the denoiser trained on range-angle images still performs well despite no geometry-aware rendering of feature images.\nHowever, we also found it beneficial to render the feature image as a range-angle image. We explicitly calculate the range and angle of all the sampled points and plot it on an empty image. Since the calculated range and angle may lie between pixels, we use bilinear interpolation weights to scale the feature based on how close it is to each of the four nearest pixels."}, {"title": "6.3. More Images, Better Results", "content": "Modality fusion is the process of combining data from different source modalities into the model to provide more reliable, accurate, and useful information. This is a common and important problem in autonomous systems that have multiple sensing modalities such as EO, LiDAR, radar, ultrasound (and so forth) from different locations and orientations. By jointly training the various modality-specific modules of CrossModalityDiffusion, we can fuse information of different modalities from multiple viewpoints into a single, more informative, feature volume representation of the scene.\nIn Table 2, we evaluate our framework on ShapeNet Cars by randomly selecting S input views from random modalities and comparing the average output image quality (also from random modalities) against when only one of those input images is used. The main conclusion from this experiment is that when more images are used as input, the output image quality improves. This is because CrossModalityDiffusion is fusing its geometric understanding of the scene from each input image. We also observe that as the number of input views increases (lower on Table 2) results improve. Note that the upper number in each row of Table 2 does not change much, because the CrossModalityDiffusion is always using one input image."}, {"title": "6.4. Benefits of Having a Variety of Sensors", "content": "Integrating the strengths of various sensing modalities into a unified, more informative representation is highly advantageous in geospatial imaging. As shown in Table 3, leveraging our framework with multiple sensing modalities captured from the same viewpoint enhances performance in the MMNVS task.\nIn this experiment, we evaluate every subset of {LiDAR (RA), SAR, LiDAR (P)} sensing modalities. For each test scene, we randomly select one view as input and three views to predict. The subset of sensing modality images serves as input to CrossModalityDiffusion, which generates novel views for evaluation. We observe the best results in the bottom row of Table 3. As expected, our framework achieves the best performance when all sensing modalities are utilized, even when taken from the same viewpoint. This result underscores the complementary nature of different sensing modalities; each captures unique details of the scene that, when combined, contribute to a richer and more accurate representation.\nAnother observation we make from Table 3 is that as the number input views increases, the model is using more parameters. In the bottom row of Table 3 more input modalities are used, hence more of the encoder modules as shown in Figure 3 are used. The same cannot be done with just one input modality, because the model would use the same weights multiple times."}, {"title": "6.5. Qualitative Discussion", "content": "In Figure 4 we show images from all input/output modalities with two inputs from the same modality. We confirm our hypothesis that EO out from other modalities in is difficult because there is no color information. We see in the EO out examples in Figure 4 that the denoiser outputs a white car when the car is actually yellow. EO to EO does not have this problem."}, {"title": "7. Conclusion", "content": "We introduced CrossModalityDiffusion, a modular framework for few-shot MMNVS using a unified intermediate representation. Our experiments demonstrate that CrossModalityDiffusion can effectively transform images across modalities and integrate information from diverse sensing modalities to enrich scene understanding. By jointly training multiple GeNVS models, we ensure that the learned intermediate representation remains consistent and modality-agnostic. Furthermore, we show that our framework can be adapted to handle non-perspective-projection-based images with minimal modifications.\nWhile CrossModalityDiffusion performs well in MMNVS, its computational demands for training and inference are significant, primarily due to its reliance on a diffusion-based backbone. Despite this limitation, we hope CrossModalityDiffusion inspires new directions in MMNVS and data fusion across sensing modalities. We believe the modality-agnostic intermediate representation has the potential for other downstream tasks beyond MMNVS. Additionally, CrossModalityDiffusion can address scenarios where data availability is imbalanced across sensing modalities, serving as a bridge to overcome such challenges."}]}