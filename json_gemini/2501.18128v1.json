{"title": "Unraveling the Capabilities of Language Models in News Summarization", "authors": ["Abdurrahman Odaba\u015f\u0131", "G\u00f6ksel Biricik"], "abstract": "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.", "sections": [{"title": "1. Introduction", "content": "In today's digital age, the amount of data being produced has grown exponentially. The rapid increase in data, particularly in the news sector, has made it crucial to summarize information quickly and accurately to stay informed. News plays an integral role in our daily lives by keeping us updated about global events and shaping our perspectives, knowledge, and opinions. However, staying well-informed without feeling overwhelmed is challenging [1, 2]. This study, therefore, focuses on the task of News Summarization, which involves presenting the key facts and significant details from a news article in a clear and concise format, allowing individuals to stay effectively informed about current events. Manual summarization, while good at maintaining the original meaning of the text, is impractical due to its time-consuming nature. But one valuable solution to this problem is Automatic Text Summarization (ATS), which efficiently aims to condense lengthy articles into brief summaries, focusing exclusively on the main aspects of the original content, saving time, effort and resources by making it simpler to rapidly comprehend the primary concepts without reading the entire document [1, 3, 4, 5].\nAs Generative Artificial Intelligence (GenAI) technologies continue to advance, along with the increasing number of Language Models (LMs) introduced every day, there is growing interest in leveraging their capabilities to enhance the efficiency and accuracy of not only news summarization but various Natural Language Processing (NLP) tasks [6, 7]. The release and widespread use of LMs like ChatGPT have undoubtedly not only showcased Al's potential but have also increased public awareness of its capabilities [7, 8, 9]. Imagine a world where you could instantly grasp the key points of any lengthy news article with just a brief glance-this is what we aim to achieve with LMs.\nWhile Large Language Models (LLMs) like GPT-4, Claude, etc. have demonstrated impressive capabilities, their substantial size-with parameters ranging from tens to hundreds of billions or maybe even more-leads to significant challenges including high computational power requirements, increased latency, costly training and maintenance, and limited flexibility. This has driven researchers to explore smaller language models. These compact models present a promising alternative for LLMs since they are more computationally efficient, require less memory and storage space, and offer more cost-effective deployment options, making them particularly interesting candidates for investigating effective NLP solutions. Nevertheless, important questions remain: Can such smaller language models manage the information load and ensure that critical news reaches its audience both efficiently and effectively through summarization? How effectively can these smaller-scale language models handle news summarization tasks while balancing efficiency and performance? How well do different LMs perform in summarizing news? Which are good, which are better, and which should we avoid?\nThere are a few studies in the field of news summarization"}, {"title": "2. Related Work", "content": "Automatic text summarization is one of the significant tasks in NLP, with a rich research history. It began with Luhn [13], who came up with the idea of summarizing scientific documents by extracting the most significant sentences. Early research on both abstractive and extractive summarization relied on various approaches along the course of the research journey, such as statistical, graph-based, structure-based, clustering-based, fuzzy logic-based, and machine learning approaches. Over time, deep learning techniques, including Feed Forward and Recurrent Neural Networks (RNNs), advanced summarization capabilities, with Sequence-to-Sequence (Seq2Seq) RNNS becoming a notable standard [1, 2].\nHowever, the introduction of attention mechanisms and the Transformer architecture by Vaswani et al. [14] catalyzed advancements in GenAI we see nowadays and caused an uproar in the NLP community, as Transformer models surpassed previous methods across a wide range of NLP tasks, leading to numerous studies investigating earlier Transformer-based Encoder-Decoder Language models such as BART, Pegasus, and T5 on news summarization tasks ([15, 16] and others).\nWith Transformer-based language models becoming increasingly advanced, featuring decoder-only architecture, enhancing their own generative capabilities, and encompassing a broader understanding of language structure and knowledge across various fields more than ever before, it has become essential to benchmark these models on specific NLP tasks, such as news summarization, considering that numerous models are being released by both commercial companies and open-source communities across different sizes and parameter scales [7, 17]. Yet recent developments have shown promising results of LMs built on non-Transformer architectures [18], which also show significant promise. However, these non-Transformer architectures fall outside the scope of our current study, which focuses exclusively on recent Transformer-based models.\nIn 2022, Goyal et al. examined how well models of different types performed on the news summarization task. They assessed a general-purpose model, GPT-3, against a task-specific fine-tuned model, BRIO, and another model optimized for numerous tasks, TO. They extended the scope of generic summarization to include keyword-based summarization, specifically requesting a summary of the text with an emphasis on a certain keyword (topic, person, etc.). In all their experiments, it was consistently observed that GPT-3 received lower scores on automatic metrics compared to other models. However, it significantly surpassed them in terms of human evaluation [10].\nZhang et al. conducted a thorough evaluation of LMs of different sizes. The number of benchmarked models has been expanded to ten, comparing different versions of OpenAI GPT-3 and InstructGPT models including Ada, Curie and Davinci versions, as well as other models like Anthropic-LM-v4, Cohere-XL, GLM and OPT. They performed both zero-shot, and few-shot prompting using five examples. Their research discovered that InstructGPT models -especially the davinci version- were capable of achieving news summarization levels that were comparable to those of human summaries [11].\nAnother study was carried out by Basyal and Sanghvi later in 2023 using popular news summarization benchmarks to compare several newer models-more precisely, tuned versions of the falcon-7b-instruct, mpt-7b-instruct, and the first model behind ChatGPT, the text-davinci-003. According to their experiments, text-davinci-003 outperformed the others [12].\nTo wrap up, while prior research has evaluated a limited number of LMs for news summarization, some of these models are outdated and do not reflect the capabilities of the latest advancements in the Generative Al field.\nAs previously stated, recent advancements are credited to the introduction of Transformer architecture, the availability of vast datasets, and improved computational resources. However, we note that the number of model parameters has continuously increased, from hundreds of millions to tens of billions, even to hundreds of billions. This increase in parameters is correlated with enhanced language understanding and better performance"}, {"title": "3. Experimental setup", "content": "In order to be able to assess the performance of different language models on the task of news summarization, several benchmark datasets have been utilized. These benchmark datasets share certain attributes that align with our research: Firstly, our focus was primarily on English datasets, comprised of English news articles and their related summaries, due to the wide usage and availability of resources in English. Secondly, datasets must be dedicated to the Single-Document News Summarization task (SDS), which involves extracting essential points from a singular news story and compressing them into a succinct summary. While Multi-Document Summarization (MDS) that combines and synthesizes information from several news articles, represents another important research direction, it falls outside of the scope of this paper. Therefore, we distinguish our datasets from others such as Document Understanding Conferences (DUC) benchmarks [19] and Multi-News [20], which are intended for use in the Multi-Document News Summarization task. Thirdly, the gold summaries provided in the dataset should ideally be abstractive, human-generated, and reasonably condensed. Lastly, the datasets should be widely recognized, characterized by their large size, comprising hundreds of thousands of news articles, and publicly accessible. Three prominent benchmarks that meet all these criteria are CNN/Daily Mail, also known as (CNN/DM), Newsroom, and Extreme Summarization, also known as (XSum) (presented in Table 1) [21, 22, 23, 24].\nInitially, the goal of the CNN/Daily Mail (CNN/DM) dataset was to facilitate the tasks of Passage-based Question Answering and Reading Comprehension, as it contained news articles as passages and abstractive short summaries in the form of bullet points [21]. However, in 2016 with a simple straightforward modification, Nallapati et al. adapted the original dataset to serve as a benchmark for ATS task by concatenating the highlight bullets to form a single, multi-sentence summary for each news article, where each bullet [22]. Although the method used to generate the gold summaries (highlights within the article) may not be optimal for evaluating ATS task and summaries of good quality should be in the format of a coherent paragraph, not separated sentences, where each of them may explain something different, this dataset still seems to be one of the famous benchmarks in this field [27].\nIn 2018, Grusky et al. released the Newsroom dataset, a large-scale collection sourced from 38 distinct major news publishers (such as Aljazeera, BBC, CNBC, Fox Sports, NY Daily News, Reuters, etc.), distinguishing itself from previous datasets that relied on a limited number of sources [23].\nWith the idea of creating extremely short summaries that consist of a single sentence, favoring abstractive summarization strategies over extractive ones to encourage the development of abstractive summarization models, Narayan et al. introduced the Extreme Summarization (XSum) Dataset that was created by gathering articles published in the British Broadcasting Corporation (BBC) website, where each article was paired with a pre-written introductory sentence crafted by the article's author, who argues that it should succinctly address the question 'What is the article about?' in one sentence by utilizing information from different sections of the article, and incorporating techniques of rephrasing, fusion and drawing inferences, unlike headlines, which are designed to catch the reader's attention [24]. Due to the nature of the summaries being one-sentence long, they exhibit greater conciseness compared to the summaries in the other datasets.\nIn order to gain a deeper understanding of the properties of these datasets, we studied and analyzed the datasets by utilizing visualizations, computing statistical measures such as overlap ratio, and viewing data point examples. Based on our comprehensive analysis, we observed the following:\n\u2022 Invalid data points: A small subset of data points was identified where the summary length (in words) exceeded the original text of the article. These instances appear to be"}, {"title": "3.2. Experimental design", "content": "In this study, we focused exclusively on inferring LMs, employing zero-shot and few-shot in-context learning to assess the performance of various LMs on the news summarization task. We conducted our experiments in the Google Colab Environment equipped with a high-performance GPU, specifically NVIDIA A100 with 40GB VRAM, which is necessary to utilize the language models.\nIn the zero-shot setting, the models were assessed on their ability to produce accurate summaries without any helpful context or examples. Conversely, in the few-shot setting, a limited number of examples were provided to give minimal guidance, aiming to help the models grasp the nuances of news summarization more effectively, thereby generating more precise and relevant summaries.\nThe primary reason for not conducting model fine-tuning was the poor quality of the previously examined datasets' summaries. Our analysis indicated that the gold summaries provided were of low quality, and fine-tuning models with such data could lead not only to suboptimal results but might also degrade the models' summarization abilities. The LMs, which inherently have good capabilities in summarization and other NLP tasks, could be negatively affected by the inferior data quality. Additionally, resource limitations were a significant challenge in this study. Fine-tuning requires considerable computational resources and time, yet we were restricted to the Google Colab's infrastructure, which provides finite compute units upon subscription, which are consumed based on the usage of computational resources. Although the initial amount was insufficient and we needed to purchase more and more units to conduct all inferences, the extensive resource requirements for evaluating multiple language models across three datasets in two different settings made fine-tuning both unfeasible and cost-prohibitive. Moreover, to ensure a clear and focused scope, we decided to concentrate on zero-shot and few-shot settings. These approaches also provide valuable insights into the models' generalization capabilities without the need for additional data.\nAll the aforementioned reasons apply to public models. However, for the tested private models (e.g., GPT-4 or Google Gemini Pro 1.5), fine-tuning was not even an option as this feature was unavailable at the time we conducted our experiments."}, {"title": "3.3. Experimental settings", "content": "To maintain data integrity and to ensure robustness and consistency in our work, we implemented a data cleaning procedure to identify and remove invalid data points from the training, validation, and test sets."}, {"title": "3.3.2. Few-Shots/Demonstrations", "content": "In the few-shot setting, given the constraint of varying context window lengths across different models, it was crucial to include the instructions, the article to be summarized, and the few examples within the prompt, ensuring we did not exceed each model's context window. The distinct tokenization process of each model further complicated this task, making it challenging to provide multiple examples without exceeding the context limit, particularly with longer articles. To address this, we decided to include only three examples. These examples were manually selected for their critical importance to the experiment, ensuring they were of high quality and represented a variety of genres and topics. In some cases, we removed extra, misleading information within the gold summary of those demonstrations to avoid negatively impacting the models' performance. By carefully choosing the shortest articles possible, we aimed to stay within the context window constraints while still providing effective guidance to the models."}, {"title": "3.3.3. Sampling", "content": "To evaluate the models, we selected a substantial sample of 1000 examples from the test set of each dataset as evaluation examples, respectively. This approach contrasts with other works that sampled only 25 to 100 examples as evaluation examples. We believe, evaluating on a larger sample size of 1000 examples\n\u2022 guarantees that the results are more statistically robust and reliable, as it reduces the impact of outliers and the variance in performance metrics, leading to more confident conclusions about the model's capabilities and limitations,"}, {"title": "3.3.4. Prompt Design", "content": "Several prompting techniques and strategies should be taken into account while designing effective prompts for In-Context Learning experiments, since these prompts play a pivotal role in determining the models' success by directing their interactions and outputs. This requires not only a deep understanding of the model's strengths and weaknesses but also domain expertise and a structured method to customize prompts based on each use case. For instance, when using large models, Zero-shot Learning -where the model performs tasks without any prior examples- often yields satisfactory results due to their advanced capabilities. However, smaller models tend to struggle in such scenarios, where employing advanced prompt engineering techniques becomes essential [7, 17]. Therefore, we ensured through our designed prompts that all LMs comprehended the task requirements regardless of their sizes and complexities to be able to obtain the desired response. The strategies used include\n\u2022 adopting specific role to guide the model's behavior and to shape the tone and style of the output,\n\u2022 clearly specifying the task to avoid ambiguity and help LMs to understand the scope and constraints,\n\u2022 breaking down the task into multiple steps for better understanding, employing a method akin to the Chain of Thought (CoT) technique which guides LMs through essential reasoning steps, thereby making their implicit processes explicit, and\n\u2022 providing concise and clear instructions, and delivering the article text as input to be summarized by the model."}, {"title": "3.4. Model Selection", "content": "The selection of models for our research was guided by several key criteria, which can be broadly categorized into constraints for large models and considerations for smaller models. For large models, our selection was based on their well-known high performance and dominance in various LLM leaderboards. Specifically, we included the private OpenAI GPTs and Google Gemini models, recognizing their established performance in the NLP field and demonstrated effectiveness in similar tasks, as well as their widespread use in daily applications such as ChatGPT and Google Gemini (formerly Bard). For smaller models, we considered the public LMs published on Hugging Face platform (Until May 2024), primarily focusing on the context window length and model size.\nWe constrained our model choices to those with a context window length of at least 4096 tokens to be able to fit both prompt and demonstrations in few-shot setting in worst case. Additionally, due to the computational resources available through the Google Colab Pro+ paid plan, we restricted our selection to models with a maximum parameter count of approximately 11 billion. This limitation was necessary to fit the models within the available GPU memory without requiring any type of quantization, which may lead to performance degradation. Furthermore, we prioritized the popular models due to their community support. Based on the aforementioned decisions, we considered benchmarking 20 distinct LMs in both zero-shot and three-shot settings. We list the LMs together with their details in table 2.\nIt is important to note that the generation settings (e.g., Temperature, Top-p, etc.) were not altered for either large or small models and kept at their default settings in order to maintain consistency and ensure a fair comparison across different models. By using the default settings, we acknowledge that the performance may not be optimal for our specific task, potentially leading to suboptimal results. However, our objective was to"}, {"title": "3.5. Postprocessing", "content": "During the inferring of private LLMs, we encountered a specific issue that some examples were blocked by the APIs (OpenAI API and Google GenerativeAI API) due to triggering content filters (such as violence, sexual content, self-harm, and hate speech). Consequently, no completions were generated for these examples, as the content or topics of the news articles activated the filters.\nTo ensure a fair and unbiased comparison across the LMs involved in this study, we include only the non-blocked examples in the evaluation process. This decision was based on several considerations; firstly, by concentrating on the non-blocked examples, we ensure that all models are assessed using the same set of inputs, thereby eliminating any potential bias introduced by content filtering mechanisms. This approach allows us to focus on the models' core summarization capabilities without the confounding factor of content moderation. Secondly, this method prevents penalizing models for complying with content safety guidelines, which is a crucial aspect of their deployment in real-world applications. Additionally, it is possible that one or more of the public models are also censored or equipped with similar filters behind the scenes and may likewise generate no completions for those examples, as observed with the private models. Thus, we excluded the blocked examples, resulting in a final dataset comprising 827 examples from CNN/DM, 923 examples from Newsroom, and 938 examples from XSum, out of the original 1000 in each case. The evaluation of the LMs will be based on these specific subsets.\nAnother problem faced was that some completions included hallucinated responses. For example, while generating the completion, the model would begin to summarize the provided news article but then deviate by producing irrelevant content, such as code snippets, solutions to unmentioned problems, or awkward questions. These hallucinations introduce noise into the evaluation process and detract from the models' primary task of content generation. To address this issue, we implemented a basic cleaning procedure to remove such irrelevant completions by identifying known specific patterns, such as (... \\n\\n ### Instructions: HALLUCINATED_TEXT) or the use of triple backticks (... python CODE_SNIPPET ```) for code snippets. However, given the complexity of natural language, it is impossible to anticipate all possible patterns. Therefore, the implemented cleaning procedure aims to minimize hallucinations in completions to the greatest extent possible. By eliminating these irrelevant continuations, we reduce the variability that could otherwise skew the evaluation outcomes."}, {"title": "3.6. Evaluation Framework", "content": "In this study, we implemented an evaluation framework covering automatic, human, and AI-based evaluations methodologies."}, {"title": "3.6.1. Automatic Evaluation", "content": "Since manual evaluation of generated summaries on the sampled testing sets is time-consuming, numerous automatic evaluation metrics have been proposed. These metrics typically involve comparing the generated summaries to gold reference summaries. Specifically, our study utilized ROUGE [40], METEOR [41], and BERTScore [42].\nThe ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric measures the quality of candidate (AI-generated) summaries based on the lexical overlap lexical overlap through different approaches (e.g., ROUGE-L examines the Longest Common Subsequence (LCS) between the candidate and reference summaries, capturing sentence-level structural similarities without a specific n-gram length). Despite its extensive use, its straightforward implementation and efficiency in measuring lexical overlap, ROUGE has its own limitations, such as its dependence on exact token matches, which means it does not account for synonymous phrases or the semantic meaning of words.\nTo address these limitations, our study employs METEOR (Metric for Evaluation of Translation with Explicit ORdering), which incorporates a more sophisticated approach by considering word order, and semantic similarity beyond exact word matches including stems, synonyms, and paraphrastic relationships in its evaluation process.\nHowever, its reliance on language-specific resources for synonym and paraphrase matching, and the complexity of its calculation led us to explore BERTScore, a significant metric that leverages contextual embeddings from pre-trained language models to address many of the previous metrics' limitations by capturing more nuanced semantic connections between tokens, moving beyond the constraints of exact word matches or predefined synonym sets. Unlike traditional metrics that struggle with semantic equivalence, unfairly penalizing valid paraphrases, BERTScore can more accurately assess summaries that convey identical meanings using different terminology, thereby correlating better with human judgments. Nevertheless, BERTScore faces its own challenges, including increased computational demands and potential biases inherent in the pre-trained language models used. In our study, we specifically utilized the \"roberta-large\" language model based on the implementation of the bert-score python package. We report F1-scores for these metrics in the results section."}, {"title": "3.6.2. Human Evaluation Protocol", "content": "Given the large number of summaries generated for each news article, the evaluation task was quite challenging. To manage this, we relied on volunteer evaluators who were willing to handle the substantial workload. Each evaluator was assigned 6 articles, with 2 articles from each dataset but generated under different settings. This approach ensured that each evaluator assessed a total of 120 summaries (20 summaries per article \u00d7 2 settings/articles \u00d7 3 datasets). This method was chosen because recruiting a large number of evaluators to handle smaller pieces of work would have required significant coordination and management, increasing the complexity and cost. By assigning a substantial number of summaries to each evaluator we ensured"}, {"title": "3.6.3. AI-based Evaluation Protocol", "content": "Using one powerful LM to evaluate others -commonly referred to as LLM-as-a-Judge concept provides a unique way to measure their effectiveness. This method not only supports human evaluation but also offers a cost-effective assessment process that could be dynamically adjusted to meet growing evaluation demands. We followed the same evaluation protocol used for human assessments but applied it to a strong LLM. We chose Claude 3 Sonnet, introduced by Anthropic [43], as a judge for this task due to its reputation for exceptional performance across a wide range of tasks and its high ranking on leaderboards. Importantly, Claude 3 Sonnet is not related to any of the models in our study, such as GPTs and Gemini, ensuring an unbiased evaluation. We used the same three criteria for evaluation. To guide the judge LLM, we created a detailed prompt. This prompt explained the task, provided the original article text and the candidate summary, and asked the judge LLM to score each criterion following a specific structure. The judge LLM was asked to assess summaries of five different articles per experiment (20 summaries per article \u00d7 5 articles \u00d7 3 datasets \u00d7 2 settings), resulting in 600 assessments in total."}, {"title": "4. Experimental results and discussion", "content": "In this section, we present the results of our zero-shot learning experiments aimed at evaluating the performance of the LMs considered in this study for the news summarization task."}, {"title": "4.1.1. Zero-shot Learning on CNN/DM dataset", "content": "GPT-3.5-Turbo obtained the highest scores in automated metrics on the CNN/DM dataset, with the exception of ROUGE, where Yi-9B had the greatest score. This suggests that while Yi-9B is effective at preserving lexical content and structure, it may be less capable of maintaining semantic coherence and fluency in generated summaries. Meanwhile Mistral-Instruct-v0.1 demonstrated the lowest performance across automatic metrics. We hypothesize that this is due to the model's tendency to truncate the summary generation process too early, leading to summaries that consist of only one or two words.\nIn this experiment, we can see that judge LLM preferred some models, such as SOLAR-Instruct-v1.0, Gemma-7B, and Yi-9B, which were not as well regarded by human assessors. Nonetheless, the remaining results were mostly comparable. Humans evaluators and the judge LLM both confirmed on the high performance of Qwen1.5-7B and Llama-3-Instruct regarding relevancy. Furthermore, both concur that Gemma-7B, Qwen1.5-7B, and the Llama-3 family produced relatively faithful summaries on the CNN/DM dataset. Finally, Llama-3-Instruct and Qwen1.5-7B were recognized as the superior models in structuring their summaries.\nAmong small models, Gemma-7B, Llama-3 models, Qwen1.5-7B, SOLAR-Instruct-v1.0 and Yi-9B perform particularly well in summarizing news articles into highlights, consistent with the CNN/DM dataset's characteristics."}, {"title": "4.1.2. Zero-shot Learning on Newsroom dataset", "content": "When analyzing the results for the Newsroom dataset shown in Table 4, we observe a decline in scores compared to the CNN/DM dataset. We attribute this drop to the greater variety of styles that summaries should adhere to, as articles are obtained from a larger number of sources (38 instead of 2). In addition, the lower quality of the gold summaries, which we previously discussed during our dataset analysis, is a contributing factor.\nThe automated metric scores reveal that GPT-3.5-Turbo demonstrates the highest performance across most evaluation metrics. In contrast, Llama-2-hf, Llama-3-Instruct, and SOLAR-v1.0 exhibit the lowest scores, suggesting limited effectiveness in this experiment. Furthermore, upon examining the summaries generated by these models, it is evident that Llama-2-hf and Llama-3-Instruct failed to produce summaries, resulting in empty completions. Specifically, they generated 190 and 203 empty summaries out of a total of 923 articles, respectively. While SOLAR-v1.0 tended to output further prompts and instructions rather than fulfilling the task of summarizing the provided news articles. These factors significantly impacted their scores in this experiment.\nClearly, we continue to observe that Yi models excel at generating summaries that include n-grams present in the gold summaries, resulting in the highest ROUGE score. Additionally, Qwen1.5-7B and SOLAR-Instruct-v1.0 models still perform well in zero-shot news summarization. Overall, human evaluators favored just two models in this experiment mostly: Qwen1.5-7B and Zephyr-Beta. However, judge LLM determined that the Llama-3 family, Mistral-v0.1, Qwen1.5-4B, and SOLAR-Instruct-v1.0 are notably also capable of delivering strong performance."}, {"title": "4.1.3. Zero-shot Learning on XSum dataset", "content": "Analogous to the scores on the Newsroom dataset, this dataset also shows a drop in scores, which can be attributed to the challenging task of generating highly succinct, single-sentence summaries.\nThe automatic evaluation scores for the XSum dataset in Table 5 manifest the outstanding performance of the Yi-9B model, which achieved the highest scores across ROUGEL (0.2534), and BERTScore (0.8884) metrics. This model outperformed even the very large private models, indicating its superior capability of generating extremely concise summaries consisting of one or at most two sentences.\nNotably, this is the first experiment in which the Gemini-1.5-Pro model outperformed the GPT models, achieving the highest METEOR score (0.2923) among all models.\nOn the other hand, the Gemma-7B, Llama-2-hf, and Llama-3-Instruct models displayed the lowest scores. These models encountered the same issues observed in the zero-shot newsroom experiment. Both Gemma-7B and Llama-2-hf generated a substantial number of empty summaries, 310 and 225 out of 938 articles, respectively, while the Llama-3-Instruct model produced unrelated prompts and instructions instead of actual summaries.\nThe judge LLM once again aligned with major human selections. Both humans and the judge LLM agreed that Zephyr-Beta, Qwen1.5-7B, Gemma-7B, and Solar are significant models for this experiment. Nonetheless, there were certain disagreements. Although humans recognized the efficacy of Mistral-v0.1, the judge LLM favored the Llama-3 family, Qwen1.5-1.8B, and Qwen1.5-4B."}, {"title": "4.2. Few-shot Learning Results", "content": "In this section, we detail the results of our few-shot (more specifically three-shot) learning experiments in carefully organized tables, followed by an analysis of the results."}, {"title": "4.2.1. Three-shot Learning on CNN/DM dataset", "content": "Compared to the scores of zero-shot experiment, the results of three-shot experiment for CNN/DM presented in Table 6 ensure that offering few-shot examples did not enhance the performance of the LMs; rather, it led to a decline in their scores. This is explained by the low quality of the gold summaries,"}, {"title": "4.2.2. Three-shot Learning on Newsroom dataset", "content": "When it came to small models in this experiment, Qwen1.5-7B regularly obtained outstanding scores that were consistent across evaluation methodologies. Aside from Qwen1.5-7B, models like Qwen1.5-4B, SOLAR-Instruct-v1.0, and Yi-9B illustrated significant effectiveness in automatic measures, although human assessors and judge LLM disagreed, concluding that Gemma-7B, Llama-3, and Zephyr-Beta performed excellently.\nOnce again, both Gemma-7B and Mistral-v0.1 continue to have serious challenges in generating summaries, failing to produce summaries for 822 and 589 out of 923 total articles, respectively."}, {"title": "4.2.3. Three-shot Learning on XSum dataset", "content": "Similarly, as observed in the zero-shot experiment on the same dataset (XSum), the Gemini-1.5-Pro model outperformed the GPT models within the category of large models. Excellent small models in this experiment that should be acknowledged are SOLAR-v1.0, Yi-6B, and Yi-9B. Furthermore, Yi-9B billion even received the highest ROUGE score among all models.\nNonetheless, humans and the judge LLM hold a different perspective. They recognized the strong performance of numerous models, including Gemma-2B, Llama-3, Qwen1.5-7B, SOLAR-v1.0, Zephyr-Beta, and for the first time, Phi-3-Mini-Instruct.\nUnfortunately, it appears that certain models, specifically Gemma-7B, Mistral-v0.1, and Llama-3, continue to experience similar performance issues noted in prior experiments. They failed to generate 893, 799, and 222 out of 938 summaries, respectively."}, {"title": "4.3. Discussion", "content": "It appears that the judge LLM (Claude 3 Sonnet) tends to assign scores more generously compared to human evaluators. But in general, the evaluator LLM aligns with human results in preferring the summaries generated by the large models. This is not surprising due to their advanced design, extensive training, and significantly larger parameter counts, which far exceed those of the public smaller models.\nAlthough GPT-3.5-Turbo achieved higher automatic evaluation scores compared to its successor GPT-4, GPT-4 slightly outperformed its predecessor in the majority of Human and AI-Based evaluation metrics. This could be attributed to the nature of the summaries generated by GPT-4 since they are more sophisticated, abstractive, and comparable to those that could be authored by humans.\nHowever, upon analyzing the scores of small models only, we noticed that the promising models could be separated into three different patterns regarding the performance:\n\u2022 Models such as Yi-6B, Yi-9B scored remarkably well on automatic evaluation metrics but received poor ratings from both human evaluators and the judge LLM. This suggests these models might be optimizing for surface-level patterns that automatic metrics can capture, rather than producing summaries that humans find useful or accurate."}, {"title": "5. Conclusion and future work", "content": "In this work", "datasets": "CNN/Daily Mail, Newsroom, and Extreme Summarization (XSum). Comprehensive experiments in both zero-shot and few-shot learning scenarios, coupled with diverse evaluation approaches, have provided significant insights into the current state of LMs in this domain.\nOur findings show upon analyzing the scores presented in all experiments that large models show superior performance and outperform the smaller models. Nevertheless, it appears that models like Qwen1.5-7B, SOLAR-Instruct-v1.0, Llama-3, and Zephyr-Beta are competitive with these large models, as they consistently achieve high scores across all datasets.\nSpecifically, in the few-shot setting, adding demonstration examples did not improve model performance and even negatively impacted on the ability in extracting the primary concepts mentioned in the article texts, rather than enhancing the model capabilities, primarily due to the low quality of the gold summaries. It is noteworthy that large models proved their resilience in maintaining their performance levels across different dataset styles, even in the few-shot setting.\nWe observed inconsistencies in performance across different datasets, with some models excelling on one dataset while underperforming on others. For example, Gemini-1.5-Pro, and SOLAR-v1.0 performed exceptionally on the XSum dataset, highlighting the potential for dataset-specific strengths among LMs.\nOur research has highlighted several areas where further investigation and refinement are needed to further improve"}]}