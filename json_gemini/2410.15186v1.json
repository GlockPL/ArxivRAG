{"title": "Fine-tuning foundational models to code diagnoses from veterinary health records", "authors": ["Mayla R. Boguslav", "Adam Kiehl", "David Kott", "G. Joseph Strecker", "Tracy Webb", "Nadia Saklou", "Terri Ward", "Michael Kirby"], "abstract": "Veterinary medical records represent a large data resource for application to veterinary and One Health clinical research efforts. Use of the data is limited by interoperability challenges including inconsistent data formats and data siloing. Clinical coding using standardized medical terminologies enhances the quality of medical records and facilitates their interoperability with veterinary and human health records from other sites. Previous studies, such as DeepTag and VetTag, evaluated the application of Natural Language Processing (NLP) to automate veterinary diagnosis coding, employing long short-term memory (LSTM) and transformer models to infer a subset of Systemized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) diagnosis codes from free-text clinical notes. This study expands on these efforts by incorporating all 7,739 distinct SNOMED-CT diagnosis codes recognized by the Colorado State University (CSU) Veterinary Teaching Hospital (VTH) and by leveraging the increasing availability of pre-trained large language models (LLMs). Ten freely-available pre-trained LLMS (GatorTron, MedicalAI ClinicalBERT, medAlpaca, VetBERT, PetBERT, BERT, BERT Large, ROBERTa, GPT-2, and GPT-2 XL) were fine-tuned on the free-text notes from 246,473 manually-coded veterinary patient visits included in the CSU VTH's electronic health records (EHRs), which resulted in superior performance relative to previous efforts. The most accurate results were obtained when expansive labeled data were used to fine-tune relatively large clinical LLMs, but the study also showed that comparable results can be obtained using more limited resources and non-clinical LLMs. The results of this study contribute to the improvement of the quality of veterinary EHRs by investigating accessible methods for automated coding and support both animal and human health research by paving the way for more integrated and comprehensive health databases that span species and institutions.", "sections": [{"title": "Introduction", "content": "The use of veterinary medical records for clinical research efforts is often limited by interoperability challenges such as inconsistent data formats and clinical definitions, and data quality issues [1, 6, 30, 31]. Clinical coding is used to transform medical records, often in free text written by clinicians, into structured codes in a classification system like the Systemized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) [2, 27]. SNOMED-CT is a \u201ccomprehensive clinical terminology that provides clinical content and expressivity for clinical documentation and reporting\". It is designated as a United States standard for electronic health information exchange and includes clinical findings, procedures, and observable entities for both human and non-human medicine [27]. SNOMED-CT constitutes a hierarchy of standardized codes beginning with a top-level code such as \"Clinical Finding\" (SNOMED: 404684003) and terminating with a leaf code such as \"Poisoning Due to Rattlesnake Venom\" (SNOMED: 217659000). This work aims to improve methods for automating the clinical coding of veterinary medical records using hand-coded records from the Veterinary Teaching Hospital (VTH) at Colorado State University (CSU) as a training set.\nThe coding of electronic health records (EHRs) into standardized medical terminologies allows for storage in interoperable data models such as the Observational Health Data Sciences and Informatics (OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) [26]. Worldwide, more than a billion medical records from over 800 million unique patients are stored in the OMOP CDM format [46], which is designed to promote healthcare informatics and network research. The OMOP CDM provides infrastructure that can facilitate necessary data linkages between EHRS both in veterinary and human medicine. Data linkage in medical research is a powerful tool that allows researchers to combine data from different sources related to the same patient, creating an enhanced data resource. However, several challenges hinder the full potential of its use, such as identifying accurate common entity identifiers, data inconsistencies or incompleteness, privacy concerns, and data sharing complexities [22].\nDespite these challenges, complex linkages like those involved in One Health research are a goal of veterinary and human medical institutions. One Health recognizes the interconnectedness of human, animal, and environmental health domains [10] and the importance of considering all three to optimize health outcomes. Integrating data from different sources such as human medical records, veterinary medical records, and environmental data is essential for One Health research [22]. Application of data linkages across human, animal, and environmental data can require multiple types of data linkages; household-level linkages between humans and animals involve (1) linking specific humans (patient identification) with their companion animals (animal identification) and (2) linking the human"}, {"title": "Related Work", "content": "Most veterinary medical record systems do not currently include clinical coding infrastructure. The free text nature of much of the EHR, especially in veterinary medicine, makes manual clinical coding resource-intensive and difficult to scale. Reliable, supplemental, automated coding is feasible using current Natural Language Processing (NLP) methods including symbolic, knowledge-based approaches and neural network-based approaches [6]. Symbolic AI makes use of symbols and rules to represent and model the standard practice of clinical coders. Neural networks use training data to \"learn\" how to match a patient's information to the appropriate set of medical codes [6]. Deep learning methods have been applied to clinical coding since around 2017 with the task formulated as a multi-classification problem, concept extraction problem, or Named Entity Recognition and Linking (NER + L) problem (see [7] for NER in EHRs review). How to best integrate knowledge and deep learning methods has not yet been determined and depends on the purpose of an automated clinical coding system [6]. No matter the purpose, it is important that coders are involved from model development discussions to prospective deployment to ensure quality and usability [1]. Expert clinical coders can provide insight into the large coding systems and complex EHRs to help with automation. SNOMED-CT contains more than 357,000 health care concepts [27], and EHRs contain many different types of documents and subsections, all of varying lengths and formats. How to best represent this varied data in an automated coding context is challenging. Research is ongoing to learn useful dense mathematical representations of patient data that accurately capture meaningful information from EHRs (e.g., [35]).\nMuch of this ongoing work is focused on creating foundational large language models (LLMs) for different domains through learning mathematical representations of large amounts of data (see [3, 38] for LLMs in biomedicine). These LLMs can then be fine-tuned for specific downstream tasks including clinical coding. For human medical data, Medical Information Mart for Intensive Care III (MI\u041c\u0406\u0421-III) [19] discharge summaries constitute a prominent publicly available foundational dataset used for benchmarking (MIMIC-IV [18] has recently become available). Several prominent clinical LLMs have been trained using this data (e.g., [14, 47, 48]). Other research groups have trained clinical LLMs on private, proprietary data either in addition to, or in place of, existing foundational data (e.g., [4, 9, 12, 20, 41, 43, 45, 48]). No freely-available foundational datasets like MIMIC-III for the veterinary space"}, {"title": "Motivating Work", "content": "Veterinary-specific automated clinical coding work has been done previously by other groups using a large set of manually coded veterinary medical record data from the CSU VTH [17, 25, 49]. Both Deep-Tag [25] and VetTag [49] used the text from 112,557 veterinary records hand-coded with SNOMED-CT diagnosis codes by expert coders at the CSU VTH. Over one million unlabeled clinical notes from a large private specialty veterinary group (PSVG) were additionally used for pre-training. Both models also made use of the SNOMED-CT hierarchy \"to alleviate data imbalances and demonstrate such training schemes substantially benefit rare diagnoses\" [49]. The supplied set of target codes for each record were supplemented with each code's hierarchical ancestry to form a hierarchical training objective. This hierarchical prediction approach \"first predicts the top level codes and then sequentially predicts on a child diagnosis when its parent diagnosis is predicted to be present\" [49]. It was found that utilizing the hierarchy improved performance on both a held-out test set from the CSU VTH labeled data set and a test set from a private practice clinical (PP) in northern California.\nDeepTag (2018) is a bidirectional long-short-term memory (BiLSTM) model, which achieved impressive performance (weighted F1 score of 82.4 on CSU and 63.4 on PP data) across a set of 42 chosen SNOMED-CT diagnosis codes. VetTag (2019) expanded on the original concept of DeepTag by employing a transformer [42] model architecture and expanding the size of the set of chosen SNOMED-CT codes to 4,577. Pre-training was performed using the PSVG data and fine-tuning was done on the CSU VTH data. The expanded scope and improved design of VetTag made it the state-of-the-art in the automated veterinary diagnosis coding field (weighted F1 score of 66.2 on CSU VTH and 48.6 on PP data). VetLLM (2023) [17] evaluated a recently developed foundational LLM, Alpaca-7B [37] in a zero-shot setting, supplying it 5,000 veterinary notes to identify nine high-level SNOMED-CT disease codes (weighted F1 score of 74.7 on CSU VTH and 63.7 on PP data). The DeepTag, VetTag, and VetLLM trained models and data are not publicly available due to use of proprietary medical data."}, {"title": "Human Clinical LLMs", "content": "FasTag [43], a study into the cross-applicability of human and veterinary clinical data, showed that applying human medical data to veterinary clinical tasks has promise considering the limited availability of veterinary LLMs. Building on FasTag, three prominent, publicly available human medicine LLMs that can be fine-tuned on a variety of downstream tasks including clinical coding were considered"}, {"title": "Veterinary Clinical LLMs", "content": "Much prior work has focused on automated clinical coding in human EHRs [6, 9, 20, 35, 41, 43, 45] for insurance claims and other higher level tasks such as enhancing patient outcomes [11]. However, limited clinical coding work has been done in the veterinary space with the lack of a similar financial"}, {"title": "Non-Clinical LLMs", "content": "Non-clinical pre-trained LLMs are relatively more abundant than those in the clinical domain due to less privacy concerns (e.g., HIPAA). Several prominent and tractably sized non-clinical LLMs considered and tested for this study were BERT (2018) [5], ROBERTa (2019) [21], and GPT-2 (2019) [33].\nBidirectional Encoder Representations from Transformers (BERT) was introduced by Google in 2018 as a novel encoder-only variation on the original transformer model [42]. It differs from previous transformer models by using masked language modeling (MLM) and next sentence prediction (NSP) pre-training and by removing the decoder component of the transformer. This procedure makes the model specifically designed for problems involving fine-tuning on a downstream task. Several other models discussed in this paper (i.e., MedicalAI ClinicalBERT, VetBERT, and PetBERT) were based on the BERT architecture. BERT was pre-trained on a corpus of 800 million words from Books Corpus [50] and 2.5 billion words from Wikipedia. It was benchmarked on eleven standard NLP tasks and achieved state-of-the-art results for its time.\nIt was later determined that the foundational BERT LLM was \"significantly undertrained;\" RoBERTa [21] was offered as a better-constructed alternative. An enhanced pre-training dataset was assembled"}, {"title": "Aims", "content": "This study aimed to extend and improve upon prior work in the field of automated clinical coding for veterinary EHRs to facilitate standardization of records, veterinary patient health and research, and the creation of data linkages to support One Health approaches to problem solving. Available manually coded data from the CSU VTH was used to determine if fine-tuning existing foundational models could achieve state-of-the-art results for automated veterinary clinical coding to 7,739 SNOMED-CT diagnosis codes, the largest set of diagnosis codes yet used in a veterinary context. It was found that fine-tuning the foundational model Gator Tron performed the best with an average weighted F1 score of 74.9 and an exact match rate of 51.6%.\nThe primary contributions of this paper are:\n\u2022 A demonstration that state-of-the-art results can be achieved for clinical coding of diagnoses from veterinary EHRs using the largest number of diagnosis codes to-date through fine-tuning of foundational models (best results from a fine-tuned GatorTron).\n\u2022 A comparison of current freely-available state-of-the-art foundational models for this task.\n\u2022 A further demonstration of the use of human and general foundational models in the veterinary space (see also FasTag).\n\u2022 A preliminary performance analysis of the models by frequency of codes, depth of codes in the SNOMED-CT hierarchy, and volume of training data."}, {"title": "Materials and Methods", "content": "The automatic coding of data from many clinical domains such as procedures, medications, lab tests, etc., was amenable to straightforward rules-based methods based on concrete mappings between standardized CSU VTH terms and codes from standardized medical terminologies (Figure 1). However, diagnoses were recorded in a free-text fashion (Figure 2) and were not associated with any standard map-able list. This work focused solely on mapping the diagnosis domain to SNOMED-CT."}, {"title": "Data", "content": "The CSU VTH EHRs contain multiple document types including medical summaries, summary addendums, consultations, surgical/endoscopy reports, diagnostic test results, and imaging reports, with each containing a variety of formats and subsections. The medical summary is the main EHR document and contains much of the clinical information for each animal: presenting complaint, history,\nassessment, physical exam, diagnosis, prognosis, follow-up plan, procedures and treatments, and others.\nInformation was manually coded directly from the diagnosis lists in the medical summary documents and ancillary reports, if present and applicable. The input for this automated clinical coding task was"}, {"title": "Manual Record Coding Process", "content": "All records were coded by the CSU VTH Medical Records team. The team processed a record once a visit record was closed and all associated documents were finalized (approved by a primary clinician).\nThey began by reviewing the medical summary and invoice for the visit. The medical summary pro-"}, {"title": "Input Fields and Preprocessing", "content": "To understand the contributions of two different sections of the medical summary on model performance, a basic analysis was conducted. In consultation with the CSU VTH Medical Records team, it was determined that the diagnosis section contained the primary information used to inform diagnosis coding. Additionally, the longer narrative assessment section was included for the broad context it provided to visits. Other sections of the EHR were not explored in detail due to model restrictions on input text length.\nThe final input text for this task was a concatenation of diagnosis and assessment sections, in that order. Minimal text cleaning was performed (only simple replacement of Hypertext Markup Language (HTML) characters, lowercase, etc.), and no de-identification methods were used. Records were tokenized according to the custom tokenizers associated with each pre-trained LLM and truncated to the corresponding maximum length (truncation rate did not exceed 23% for any tokenizer, and records contained 284 tokens on average). Target codes for each record were then one-hot encoded for training. Using a stratified sampling technique to account for class imbalances, the full fine-tuning dataset was split into training (80%), validation (10%), and test (10%) sets.\nThe output of fine-tuned models was a list of potential diagnosis codes that could be assigned to a record based on the probability of those diagnosis codes being relevant. Note that, unlike VetTag, the SNOMED-CT hierarchy was not considered in outputs but instead was used later for an assessment of code specificity. Raw model outputs for each record were a vector of length 7,739 with logit scores for each class. A Sigmoid activation function was then applied to the raw outputs to yield a vector of probabilities representing the probability that each diagnosis code could be correctly applied to the record text in question. Predictions were then generated by comparing all probabilities to a chosen prediction threshold (0.5, in accordance with VetTag); a prediction was made when the probability of a code being applied to the record text in question was greater than the chosen threshold."}, {"title": "Evaluation Metrics", "content": "Several key metrics were selected to evaluate the performance of automated coding models. Precision, recall, and F1 were computed through a weighted macro (class-wise) average method to account for class imbalances using the following formulas:\n$P = Number of diagnosis code classes$\n$N = Number of test records$\n$Ni = Number of occurrences for class i$\n$codestarget,j = Set of human \u2013 assigned codes for record j$\n$codespredicted,j = Set of model \u2013 predicted codes for record j$\n$Precisionweighted = \\frac{1}{\\sum_{i=1}^{P} Ni} \\sum_{i=1}^{P} ni \\times Precisioni$ (1)\n$Recallweighted = \\frac{1}{\\sum_{i=1}^{P} Ni} \\sum_{i=1}^{P} ni \\times Recalli$ (2)\n$Flweighted = \\frac{1}{\\sum_{i=1}^{P} Ni} \\sum_{i=1}^{P} ni \\times Fli$ (3)\n$Exact Match (EM) = \\frac{1}{N} \\sum_{j=1}^{N} (codestarget,j = codespredicted,j)$ (4)\nThese metrics were chosen in accordance with those reported by the VetTag team and as standard multi-label classification metrics where a skewed class distribution is present [34]."}, {"title": "Models", "content": "Existing state-of-the-art human clinical LLMS (GatorTron, MedicalAI ClinicalBERT, medAlpaca) and veterinary (VetBERT, PetBERT) and general knowledge LLMs (BERT, BERT Large, RoBERTa,\nGPT-2, and GPT-2 XL) were leveraged by fine-tuning them for veterinary clinical coding to 7,739\nSNOMED-CT diagnoses (Table 1). All models were fine-tuned under a consistent training framework that included a batch size of 32, a binary cross-entropy loss function, an AdamW optimizer, scheduled optimization with 5,000 warmup steps and a plateau learning rate (constant learning rate after warmup) of 3 * 10-5, and an early stopping criterion with five epoch patience (number of epochs without improved results required to trigger an early stop). All models' pooler layers were allowed to update, and dropout (rate of 0.25) and classifier layers were appended to each. Fine-tuning was performed on as many transformer blocks as possible, given computational constraints. The results for all fine-tuned models were compared to the results of the current state-of-the-art model in veterinary clinical"}, {"title": "Performance Analysis", "content": "To properly prioritize and caveat findings with the importance of medical diagnosis code assignment, model performance was analyzed to determine the strengths and weaknesses of the model. As initial assessments, performance with respect to code frequency, code specificity (depth in hierarchy), and volume of the fine-tuning data was assessed for the best performing model (i.e., fine-tuned GatorTron). An evaluation of model performance with respect to code frequency informed the amount of data required for each code. Further, an evaluation of model performance with respect to code specificity informed the level of granularity expected of model predictions. To analyze the minimum requirements for the volume of labeled fine-tuning data needed to achieve strong results, model results were collected based on fine-tuning performed using successively smaller fractions of the total training data available."}, {"title": "Computing Resources", "content": "All models were fine-tuned on either a DGX2 node or a PowerEdge XE8545 node:\n\u2022 DGX2 Node:\n\u2013 16 Tesla V100 GPUs, each with 32 GB of VRAM\n\u2013 One 96-thread Intel Xeon Platinum processor\n\u2013 1.4 terabytes of RAM\n\u2022 PowerEdge XE8545 Node:\n\u2013 4 Tesla A100 GPUs, each with 80 GB of VRAM\n\u2013 Two 64-core AMD EPYC processors\n\u2013 512 GB of RAM"}, {"title": "Results", "content": "State-of-the-art results were achieved on veterinary clinical coding to the largest set of diagnosis concepts (Table 2). GatorTron fine-tuned for this task achieved the best results with an average weighted F1 score of 74.9 and an exact match rate of 51.6% on a held-out test set (10% of the labeled CSU VTH data). Many smaller models considered for this study (e.g., GPT-2 and MedicalAI ClinicalBERT) still performed comparably to the larger GatorTron (Table 2). Overall, all models had higher precision compared to recall.\nAll models were also tested \"out-of-the-box\" with all pre-trained weights frozen during fine-tuning (Table 3). On average, models' F1 scores were 48.4 points lower in the \"out-of-the-box\" setting"}, {"title": "Input Formats", "content": "Preliminary experiments with the data input selection (Table 4) found that while the diagnosis and assessment sections of a visit record each individually provided some predictive capacity, the diagnosis"}, {"title": "Performance Analysis", "content": "There was a weakly positive trend between frequency of code usage and predictability (r = 0.51; see Figure 3). However, a subset of rarely appearing codes had perfect predictive performance (n = 398). Ignoring the extremes, there was little relationship between code frequency and model performance. It should be noted that all figures shown below are for the fine-tuned GatorTron.\nDiagnosis codes were further characterized by their position in the SNOMED-CT hierarchy (Figure 4). Model performance was not observably associated with code specificity (rF1 = 'Precision = Recall = 0.06). Strong average performance was observed at highly specific depths; however, assessment was limited as these depth categories contained few codes.\nPerformance was also observed to increase with larger amounts of fine-tuning data (rF1 = 0.85,\nPrecision = 0.72, Recall = 0.90, TEM 0.95). All metrics followed a similar pattern with diminishing marginal returns on increased volumes of labeled fine-tuning data: using only 25% of all available labeled fine-tuning data yielded an 11.5% erosion of model performance, according to F1."}, {"title": "Discussion", "content": "The rapidly increasing availability of pre-trained LLMs [4, 5, 8, 12, 15, 21, 33, 45, 47, 48] in recent years has greatly increased the ability of researchers to solve complex NLP tasks, including successful clinical coding with limited amounts of labeled fine-tuning data. The poor results (not reported here) of fine-tuning applied to several transformer models with randomly initialized weights highlighted the importance of significant pre-training for the task of clinical coding. GatorTron was pre-trained on"}, {"title": "Performance Analysis", "content": "Errors made in diagnosis predictions can take many forms, and understanding the nature of these errors is important in recognizing the strengths and weaknesses of automated clinical coding and in informing future improvements. Understanding weaknesses can help prevent unsuitable application of developed tools. Several potential sources of error for the task of automated clinical coding include synonymous diagnoses recorded differently by clinicians (e.g., kidney disease and renal disease), non-synonymous diagnoses with similar names (e.g., gingival recession and gingivitis), and diagnoses that are inferred from histopathology or other reports that may not be recorded on a medical summary (e.g., endometritis is a histopathology diagnosis). Additionally, determination of the acceptable level of model performance for specific clinical tasks has not yet been established.\nA weakly positive relationship between frequency of code usage and predictability was observed (Figure 3). Frequently appearing codes were generally associated with stronger model performance"}, {"title": "Applications", "content": "A fast and reliable automated clinical coding tool has significant applications in the curation of high-quality data for clinical and research purposes. One of the primary motivations of this project was the loading of standardized diagnosis codes inferred from free-text inputs into an OMOP CDM instance.\nThe developed model demonstrated the potential to perform large-scale automated coding."}, {"title": "Future Work", "content": "A major challenge to the growth of widespread use of AI in veterinary medicine is trust, both in clinical professionals and the public [28]. Evaluative tasks that help engender trust and confidence in model predictions will be included in future work in support of eventual applicability in a clinical setting. These tasks include increasing the explainability of the model (e.g., word saliency), demonstrating the generalizability of the model to applications beyond CSU (e.g., testing models against data from private practices or other academic veterinary institutions), performing in-depth error analyses by examining the source text associated with errors in collaboration with clinical experts (e.g., additional performance analyses), introducing new sources of textual data to model inputs (e.g., histopathology reports, patient history), and exploring more complex and potentially stronger [29] modeling processes (e.g., ensemble approaches).\nThe field of AI has been developing at an extraordinary pace. Since the inception of this project,"}, {"title": "Data Availability", "content": "The data used to fine-tune foundational models for this study are propriety medical records that are the property of Colorado State University; they cannot be shared publicly due to restrictions relating to safeguarding potentially identifying or sensitive patient information. Please contact Tracy.Webb@colostate.edu for questions or requests regarding access to study data."}]}