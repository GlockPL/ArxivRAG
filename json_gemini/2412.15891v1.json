{"title": "TelcoLM: collecting data, adapting, and benchmarking language models for the telecommunication domain", "authors": ["Camille Barboule", "Viet-Phi Huynh", "Adrien Bufort", "Yoan Chabot", "G\u00e9raldine Damnati", "Gw\u00e9nol\u00e9 Lecorv\u00e9"], "abstract": "Despite outstanding processes in many tasks, Large Language Models (LLMs) still lack accuracy when dealing with highly technical domains. Especially, telecommunications (telco) is a particularly challenging domain due the large amount of lexical, semantic and conceptual peculiarities. Yet, this domain holds many valuable use cases, directly linked to industrial needs. Hence, this paper studies how LLMs can be adapted to the telco domain. It reports our effort to (i) collect a massive corpus of domain-specific data (800M tokens, 80K instructions), (ii) perform adaptation using various methodologies, and (iii) benchmark them against larger generalist models in downstream tasks that require extensive knowledge of telecommunications. Our experiments on Llama-2-7b show that domain-adapted models can challenge the large generalist models. They also suggest that adaptation can be restricted to a unique instruction-tuning step, dicarding the need for any fine-tuning on raw texts beforehand.", "sections": [{"title": "1 Introduction", "content": "Large Language Models like GPT-4 (openai, 2023), PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), Falcon (ZXhang, 2023) or Mistral (Jiang et al., 2023) perform very well on a wide range of tasks, on a wide range of tasks, both when the knowledge required is general and when it relates to various domains. Still, their application to highly specialized tasks, requiring advanced and up-to-date knowledge, raises a number of challenges (Zhao et al., 2023): general and frequently discussed topics tend to be disproportionately represented in their corpus (Penedo et al., 2023), whereas highly domain-specific topics tend to be underrepresented, which inevitably leads to challenges in effectively learning them for domain-specific tasks. Additionally, many domain-specific knowledge resources are proprietary assets, critical to an organization's competitive edge, and cannot be readily shared with general-purpose LLMs.\nAdapting a language model to a target domain, called domain adaptation, is a deeply explored in the literature to solve these problems (Zhao et al., 2023). Basically, this can be performed by re-running the training steps of a baseline general-purpose LM on domain-specific data. As illustrated in Figure 1, these steps consists in (1) a pretraining step in a self-supervised manner on raw texts, (2) an instruction-tuning step on supervised datasets of instruction-output pairs, and (3) an alignement step to help the model generate texts which fits the behavioral expectations of the users.\nAmong domains of interests, several studies (Maatouk et al., 2023b; Bariah et al., 2023a,b) emphasize the importance of undertaking the adaptation of language models for the telecom industry because this sector not only exhibits a large amount of lexical, semantic and concept specificities (Bariah et al., 2023b; Holm, 2021), but it is also a significant provider of natural language processing tasks, from understanding technical documents to incident resolution or network modeling (Maatouk et al., 2023b). Furthermore, in industrial applications, adapting reasonable-size models can significantly reduce computational costs, and ease deployments in environments with limited resources.\nIn this paper, we present experimental work to design the most efficient approach for adaptation to the telco domain. In details, our contributions concern:\n\u2022 Data: We expose our collecting process to gather raw texts (800M tokens) and instructions (80k instructions) on the telco domain.\n\u2022 Evaluation: We present a benchmark of telco-specific downstream tasks, combined with general-purpose evaluation to measure potential performance drops.\n\u2022 Adaptation: We report a detailed comparison of various adaptation approaches by disabling or enabling some steps or some data sources.\nThe baseline model is Llama-2-7B (Touvron et al., 2023). The key conclusions are that the pretraining step can be skipped, and the best adapted models compete with GPT3.5.\nTo achieve these goals, we organize the paper as follows: Section 2 reviews the current methods' for specialized domain adaptation. Section 3 then describes the methodology used, including the data collection, how we process this domain adaptation and how we evaluate it. Experimental results including evaluation on several tasks related to telecom are then presented in Section 4. After discussing the performance of the various fine-tuning methods, we conclude the paper in Section 5."}, {"title": "2 Related Work", "content": "In this section, we examine the various approaches employed for addressing domain adaptation in the literature. Prominent among these are the LORA (Hu et al., 2021), QLoRA (Dettmers et al., 2023), and ReLoRA (Lialin et al., 2023b) fine-tuning methods."}, {"title": "Domain adaptation without modifying the model's weights", "content": "Domain adaptation can be simply adressed using a retriever (Guu et al., 2020) which gives to the model access to a wide range of external documents. A retrieval-augmented LM allows the model to have access, in the prompt context, to an external Knowledge Base (a new corpus) (Piktus et al., 2021). This retriever enables the model to access and focus on documents within an extensive corpus, such as domain internal knowledge, giving to the model access to many sources it hasn't seen during the pretraining and fine-tuning phases. However, adding retrieval augmentation to a language model alone is insufficient for incorporating knowledge into the model. (Zhang and Choi, 2021) demonstrates that even though retrieval-augmented models were capable of updating certain knowledge when the retrieval corpus was swapped, the performance of the retrieval-augmented language model on new knowledge-related questions is notably low. In contrast, the model performs significantly better on questions related to the knowledge present in the original training corpus. The researchers also noted that substantial improvements in handling new knowledge questions only occurred after fine-tuning the model with fresh data. This indicates that merely updating the corpora from which models retrieve passages is not enough to effectively integrate knowledge into a language model. The reason behind this behavior is elucidated by the memorization process that occurs in LLMs during their pretraining phase. This memorization hinders the model's ability to effectively handle knowledge sourced from external documents via a retriever (Longpre et al., 2022). During the pretraining phase, LLMs acquire knowledge through memorization, enabling them to generate competitive results solely based on their own parametric knowledge, without the need for access to relevant documents, but (Longpre et al., 2022) demonstrates that this memorization behavior contradicts the expectation that the model should provide responses consistent with the information it retrieves, thereby reducing the system's interpretability. Most problematically, this memorization behavior severely restricts the model's ability to generalize to new knowledge not present in its training data. Furthermore, (Longpre et al., 2022) highlights that the extent of memorization during the pretraining phase increases proportionally with the model's size, meaning that larger models exhibit a greater degree of memorization, and so less ability to generalize to new knowledge not present in its training data. Based on these demonstrations, the Retrieval-augmented methods are not suited to domain-adaptation, requiring lot of new and technical knowledge injection."}, {"title": "Domain adaptation by adding knowledge in model's weights", "content": "Domain adaptation by adding knowledge in model's weights can be addressed through two methods: either training a model from scratch on domain-specific data or retraining a pretrained model on domain data (DAPT)\nTraining a model from scratch on domain-specific data enables the training of a tokenizer tailored to that specific domain, rather than retraining the model using a domain-specific corpus, which is limited by the use of a pretrained tokenizer with a general vocabulary. (Boukkouri et al., 2022) highlighted that retraining a general model on a specialized corpus, such as biomedical text, ultimately yields better results and is also significantly more cost-effective than training a model from scratch. This advantage becomes even more pronounced when working with LLMs. Also, (Holm, 2021) points out that DAPT is less sensitive to data quality, making it less likely to fail and less costly than pre-training from scratch. However, the study has also shown that DAPT cannot perform well when the discrepancy between the specialized domain and the general domain is too large. But for the telco domain, the study shows that DAPT performs well."}, {"title": "Vocabulary adaptation", "content": "Many studies in the literature focus on how to manage the vocabulary gap between domain-specific terminology and the general vocabulary of LLMs. (Mosin et al., 2022) pointed out that traditional methods with random embeddings for expanding the vocabulary of a pretrained tokenizer have shown no gain on domain tasks compared to a DAPT model with its original tokenizer. In their study, they developed a more effective but more complex method for expanding the vocabulary of a pretrained tokenizer (VIPI) that has demonstrated strong performance. However, we have chosen not to modify the pretrained tokenizer, primarily due to the incompatibility of this method for PEFT fine-tuning techniques and the minimal improvement they offer on downstream tasks."}, {"title": "Several fine-tuning methods for DAPT", "content": "The DAPT process consists in \"continuing pre-training\" LLM on a domain corpus, which actually means that we are fine-tuning a LLM on the pretraining task, the same tasks on which the pretrained LLM has been initially trained. However, with LLMs, the fine-tuning process poses significant challenges, most prominently the constraint of limited resources: as LLMs become more voluminous and complex, they are becoming progressively difficult to adapt, thus falling within the purview of a select few privileged companies with the necessary resources. Recently, research efforts around parameter-efficient fine-tuning PEFT (Lialin et al., 2023a) have been geared towards developing fine-tuning methodologies that can optimally adapt these models with \"acceptable\"resource expenditure, striking a balance between performance and cost. Thanks to those parameter efficient fine-tuning methods, DAPT can be performed in different ways: Methods like LoRA and QLoRA can help to reduce both training and inference costs. This new possibility raises the question of which method would yield the best performance for domain adaptation, and if full fine-tuning is truly superior to a more cost-effective approach or not.\n1. Full-parameter fine-tuning is the traditional fine-tuning method, where gradient updates are applied to all the parameters of the model.\n2. LORA (Hu et al., 2021), which stands for Low-Rank Adaptation of LLMs, is a fine-tuning method based on the premise that the weight update matrix during fine-tuning has a larger dimension than its rank. Unlike conventional fine-tuning methods that directly modify the model's weights during backpropagation, LoRA is making these adjustments of the weights in a separate matrix. The final result of fine-tuning combines two components: the pretrained frozen model and the weight update matrix. The essence of the LORA method lies in its hypotheses that the weight update matrix is, in fact, quite compact; its dimension is significantly larger than its rank. Thus, LoRA involves decomposing this weight update matrix into two rank-x matrices (e.g., 8, 16, or more). Matrix decomposition is a mathematical technique used to break down a matrix into a set of simpler component matrices. The goal is to represent a complex matrix as a combination of simpler and smaller matrices. The outcome of this decomposition yields what is known as an \"adapter.\" As a result, we can load a pre-trained model off the shelf and attach its LoRA adapters (one for each fine-tuning task). Importantly, these adapters are notably compact, making them exceptionally cost-effective both in terms of storage and deployment.\n3. QLORA (Dettmers et al., 2023) makes some modifications to LoRA fine-tuning method: In QLORA, the pretrained model is loaded directly as a quantized (4bit or 8bit) model (converting all its weights to 4 or 8 bits). Thus, the adapters (weight update matrix) is updated with the gradient calculated from the 4 or 8 bits pretrained model, as opposed to what we do in traditional LoRA, which is calculating the gradient from a full or half weight precision model. In QLoRA, the memory is all the more optimized that the optimizer states are stored in CPU (this process is called: \"Paged Optimizers\"), to optimize all the more the GPU memory. QLoRA has been implemented to allow everyone to run very large models on a single GPU. The optimizer state is all the intermediate values calculated for the backpropagation and that need to be stored to push the gradient backward."}, {"title": "DAPT + TAPT: the best combination", "content": "By comparing DAPT (fine-tuning a pretrained model on the pretraining task (= retraining) on a domain corpus) and TAPT (fine-tuning a pretrained model on the evaluation task) on 4 specialized domain (biomedical, computer science, news and reviews) on eight classification tasks, (Smith, 2020) demonstrated that the best results are obtained with a combination of the two methods: DAPT + TAPT. This observation has been extended to larger models (Cheng et al., 2023a). Doing a combination of DAPT and TAPT is all the more important for LLMs as (Cheng et al., 2023a) highlighted that for a LLM such as Llama-2-7b, DAPT-only (prompting the adapted model directly to answer questions) is degrading the original model performance on most domain tasks. Contrary to the very satisfying results of DAPT-only on Roberta-base (Smith, 2020), results on DAPT-only on Llama-2-7b are disappointing, even worse than a non-adapted Llama-2-7b model. DAPT seems indeed to be drastically hurting LLMs prompting ability. However, (Cheng et al., 2023a) points out that further fine-tuning the adapted model on a specific task (QA, for instance) performs better than the original model (llama-7b) finetuned on the same task, for each task tested (such as QA, NER and sentiment analysis) and each domain (BioMed, Finance and Law)."}, {"title": "Identification of telco-domain downstream tasks valuable for the industry", "content": "(Maatouk et al., 2023b) examined different practical applications of LLMs that can be readily utilized by telecom-domain stakeholders with their existing data. One of the task they suggest is the comprehension of 3GPP specifications and the development of chatbots on these data. These chatbots, based on fine-tuned LLMs, could help engineers streamline their research, saving time and improving the accuracy of 3GPP standards implementation and related research. Another task might be the resolution of questions regarding the network modeling. LLM can be tasked to estimate energy consumption in a network based on selected features, for instance. All in all, question-answering on the network / telco legal specifications seems to be an interesting and valuable downstream task for LLMs in the telco domain."}, {"title": "3 Methodology", "content": "In this paper, we are comparing three different approaches (DAPT-only, DAPT + IAPT, IAPT-only) on Llama-2-7B (Touvron et al., 2023), to find the most effective way of adapting this model to the telecommunication industry. Since there is no consensus on the content of the instructions for domain-adaptation (domain-only instruction vs a blend of general and domain instructions) (Liu et al., 2023), we compare the performance of the adaptation using all those combinations. In the end, we test 6 different adaptation methodologies: DAPT-only, DAPT-IAPT with telco instructions, DAPT-IAPT with general instructions, DAPT-IAPT with a blend of telco and general insturctions, IAPT-only with telco instructions, IAPT-only with a blend of telco and general instructions."}, {"title": "3.1 Telco Dataset", "content": "Data quality plays a crucial role in producing a powerful model. A significant focus in recent research centers on emphasizing the importance of tasks such as data crawling, data generation and data cleaning (Penedo et al., 2023; Gunasekar et al., 2023; Longpre et al., 2023; Lee et al., 2023; Ben Allal et al., 2024; Lozhkov et al., 2024), which lay the foundation for data-centric development of LLMs. Inspired by these works, our work relies on a telco-specific dataset extracted from public resources, consisting of a 803M tokens for DAPT and associated instructions for IAPT."}, {"title": "3.1.1 Pretraining data for DAPT", "content": "The pre-training corpus includes raw texts (803M tokens) specialized in the telco domain, from publicly online repositories (Table. 1). Specifically, (i) 41% of tokens are extracted from technical specifications, white papers published by standards organizations (28%), such as 3GPP, ITU, ETSI, RFC, etc, as well as from research papers (13%), published in Arxiv, in the field of Networking and Internet Architecture; (ii) 11% tokens are retrieved from telco-related domains in Common Crawl archives (e.g. 5GAcia, 5GAmericas, Nokia, CTIA, GSMA, Huawei, IPv6, Juniper, etc) and in Stack Exchange archives (e.g. Networking Engineering, Signal Processing, Security, Quantum Computing categories); (iii) 47% of tokens are selected from more diverse sources via importance sampling (Data Selection for Language Models via Importance Resampling, DSIR) (Xie et al., 2023). Given the target telco dataset described in (i), (ii), DSIR aims to select relevant texts that are distributed like the target. With this method, we select telco-relevant samples from stack overflow (9% tokens), wikipedia (12%), open web mathematical texts (10%), arXiv papers in RedPajama dataset (15%).\nOur crawled data exists in two different formats: PDF, HTML. To make them available for use with LLM, we perform the extraction and cleansing processes, employing the following methods:\n\u2022 Web data (HTML): inspired by Falcon (Penedo et al., 2023), SlimPajama (Soboleva et al., 2023) and Gopher (Rae et al., 2021), we create a holistic pipeline for refining and deduplicating web data, consisting of steps such as language detection, document-wise and line-wise filtering, exact and fuzzy deduplication.\n\u2022 PDF data: for technical and math-heavy documents, we employ Nougat (Blecher et al., 2023), a visual Transformer to transform documents into markup language. For documents collected in Common Crawl, PDFMiner is used to extract the texts. To further assess the relevance and quality of the texts extracted, the Zephyr-7b (Tunstall et al., 2023) model was prompted to classify whether a text is linked to the telecommunication domain. The prompt we used for this can be found in Appendix C."}, {"title": "3.1.2 Instructions for IAPT", "content": "Diverse and high-quality Telco-related instruction could enhance the instruct-tuning of Telco-adapted LLMs. Instructions for IAPT were either collected from existing material or generated. The overview of the training and validation sets for intructions is given in Table 2. In the light of a multitude of works on using LLMs for instruction generation (Wang et al., 2023; Xu et al., 2023; Lian et al., 2023), we develop a partially synthetic instruction dataset, tailored to the Telco domain. The corpus is created by prompting GPT3.5 and Mixtral (Jiang et al., 2024) to produce tasks instructions and their input-output instances in JSON format {'instruction', \u2018input', \u2018output'} from pre-training raw text (Section 3.1.1), as following:\nGenerate instructions from Telco technical documents. Considering that technical documents are too long to be entirely provided to the LLMs for instruction generation, we adopted paragraph-level generation strategy. Documents are divided into paragraphs using '##' delimiter. Paragraphs that are less informative or exceed the context limit, such as ones containing fewer than 256 words or more than 3092 words, excessive punctuations, or insufficient alphabetic characters, are ignored. The prompt provided to the models are found in Appendix D.1. As a post-processing step, we eliminate generated instructions that refer to implicit information (i.e. figure, table, section, paragraph, clause, equation, annex, paper, text, document) or those with outputs fewer than 200 characters, which are typically of lower quality. For the later, a second prompt was used to expand the output into a more detailed and informative response, as described in Appendix D.2. We obtain, with this process, a total of 11,235 instructions / output pairs.\nGenerate instructions from Telco Question-Answer pairs. We rephrase 10,558 Stack Exchange question-answer pairs about network engineering, digital signal processing and security topics into well-formatted instructions by prompting LLMs. In addition, we transform 4,000 multiple-choice questions from TeleQnA dataset (Maatouk et al., 2023a) into instructions in the chain-of-thought style. This is achieved by prompting LLMs to produce an explanation path leading to the correct choice for each question provided. The employed prompts are shown in Appendix D.3.\nAlong with Telco instructions, we have also compiled a dataset of general instruction sets. We used the Slim Orca general instructions from Open-Orca (Mukherjee et al., 2023). This dataset serves to complement the specialized instructions by introducing a diverse range of general tasks. In order to transform MCQs into instructions into our IAPT training set, we added a prefix to the question-option-choices MCQs. For general datasets, we added only the instruction \"Which of a, b, c, and d is the right answer to the following question? question a. choice1, b. choice2, \u0441. choice3, d. choice4\" (adapting it to the amount of choices), while for telco MCQs datasets, we added the instruction \"You are a helpful assistant, specialized in telecommunication technologies. Which of a, b, c, and d is the right answer to the following question? {question} {a. choice1, b. choice2, c. choice3, d. choice4}\". This transformation allowed us to use these MCQs as instructions in our IAPT training set.\nThe development of a comprehensive benchmark for evaluating the knowledge of different models in the telecommunications sector represents a significant step forward in the domain of artificial intelligence research. Our benchmark encompasses perplexity measurement on raw texts, as well as task resolution. In each case, datasets from the telco and general domain were used. Studied tasks are Multiple-Choice Question Answering (MCQA), Open Question Answering (Open QA) and abstract generation from scientific papers. These tasks were meticulously designed to assess the models' understanding of telecommunications-related information. Statistics of the test datasets are given in Tables 3 and 4, for texts and task intructions respectively."}, {"title": "3.1.3 Evaluation datasets", "content": "For the MCQA telco benchmark, we have generated synthetic MCQs from select sources within our DAPT training set, namely ATIS, 3GPP, and ETSI standards, using GPT-4 with a 32k token model. To do so, we have selected some paragraphs having fewer than 256 words or more than 3092 words (as described in 3.1.2), and prompted GPT4-32k to generate MCQs questions based on each paragraphs. The prompt used for this generation can be found in Appendix E. You can also find some samples from the generated instruction test set in Appendix G. Each MCQ generated has a single correct answer and 5 choices per question. For this type of MCQs, questions are about knowledges present in the DAPT training set, since we have generated these MCQs based on these knowledges. Additionally, we incorporated to our telco MCQ benchmark the TeleQnA test set from Huawei (Maatouk et al., 2023a), composed of 900 questions. The original TeleQnA dataset was composed of 10k instructions that were split into train and test sets (9100/900 questions) as we described in 3.1.2. The TeleQnA dataset is a MCQ dataset referenced as the first benchmark dataset designed to evaluate the knowledge of Large Language Models (LLMs) in telecommunications. Huawei generated this dataset using diverse sources, including telecom standards and research articles. Each question has 4 to 5 choices and several potential correct answers. For this type of MCQs, we consider that some questions might be regarding knowledges present in the DAPT training set since TeleQnA was constructed based on the same sources. Finally, we have added to our telco MCQ benchmark real-world MCQs we extracted from signal processing exam questions of Nokia's official practice exams. We have extracted 632 MCQs (questions, choices and answers) from Nokia official practice exams for the Nokia NRS I, NRS II and SRA written exams. These MCQs are composed of 4 to 5 choices and multiple potential correct answers per question. For this type of MCQs, we consider that questions are far from the knowledges present in the DAPT training set since NOKIA MCQs are about signal processing questions, which are not exactly the topics we have in the DAPT training set. So these MCQs' answers are not explicitly present in any text or instruction from domain adaptation.\nAs for the QA telco benchmark, it is exclusively composed of the TeleQnA (Maatouk et al., 2023a) test set from Huawei, composed of 900 questions (the same we used for MCQs), but here, we have converted each MCQ into questions - answers pairs.\nFurthermore, to ascertain the impact of domain adaptation on retaining general knowledge, we introduced a general benchmarks. As for the general MCQ benchmark, it includes a variety of MCQs from the OpenBookQA (test set of OpenBookQA official MCQs dataset (Mihaylov et al., 2018) (500 MCQs) consisting of multiple-choice elementary-level science questions with a single good answer per question and 4 choices), TruthfulQA (the subset \"multiple choice\" of TruthfulQA official MCQs dataset (Lin et al., 2021) using the MC1 target, composed of 817 MCQs on general knowledges with a single good answer per question and 4 choices), and Big-bench datasets (subset \"abstract narrative understanding\" of the validation set of Big-bench dataset (Srivastava et al., 2022), composed of 600 MCQs on questions about narrative understanding of a given text, with a single good answer per question and 5 choices), encompassing elementary-level science questions, general knowledge queries, and narrative understanding assessments. As for the general QA benchmark, it is composed of the \"generation\" subset of the TruthfulQA dataset (Lin et al., 2021), which comprises 817 examples within its validation set. The questions are designed to evaluate the model's ability to generate coherent, accurate, and truthful answers across a spectrum of topics, thereby testing the model's grasp of factual information."}, {"title": "3.2 Adaptation strategies", "content": "We preprocessed the dataset by tokenizing the text and segmenting it into chunks of 4096 tokens each. We used the pretrained 'meta-llama/Llama-2-7b-hf' model and tokenizer. To harness the computational power necessary of the training, we deployed the training across 2 A100 80GB GPUs, utilizing data parallelism to effectively manage and distribute the computational workload. We adhered to a batch size was set to 32, using micro-batch size of 1 (gradient accumulation). This method effectively simulates a larger batch size of 32, enabling us to maintain the stability and benefits of larger batch training dynamics without the associated hardware demands. The training used a learning rate of 2 \u00d7 10^{-5} with a cosine scheduler. We loaded the model in half-precision (bfloat16) to balance computational efficiency with model performance. Additionally, we applied a weight decay of 0.01 to regularize and prevent overfitting. The entire training process was completed over the span of a single epoch, ensuring that the model was exposed to the full range of data while preventing the potential for overfitting that multiple epochs might introduce."}, {"title": "3.2.2 \u0399\u0391\u03a1\u03a4", "content": "In the IAPT process, instructions were formatted with special tokens, denoting the start [INST] and end [/INST] of each instruction, followed by the expected output. We then tokenize the instructions, and then concatenate them into blocks of 4096 tokens, ensuring that no instruction was cut off between blocks by using padding at the end of the blocks as necessary. Then, we fine-tuned the model on the auto-regressive training process. we employed the Deepspeed Zero3 parallelization of the training, which is an advanced system designed to make the parallelization over not only the data and the optimizer states, but also the weights of the model (Ren et al., 2021). We use here 2 NVIDIA A100 GPUs (40GB each). We use for hyperparameters a learning rate of 2 \u00d7 10^{-5} with a cosine scheduler and a weight decay of 0.1 to regularize the training and prevent overfitting. We train the model with a batch of size 64 with micro-batch of size 1. The training was carried out over 2 epochs.\nWe launched several IAPT trainings to find the most efficient way of adapting LLM to the telco domain: The IAPT was launched across various combinations of data, incorporating both domain-specific telco instructions and a set of general instructions to provide a comprehensive learning scope for the models."}, {"title": "4 Evaluations", "content": "Perplexity measures a model's predictive ability by averaging its performance across a test corpus, with lower values indicating better adaptation to the corpus's language traits. Perplexity comparisons require models to use the same tokenizer because the tokenizer directly influences the size and composition of the model's vocabulary, which are critical factors in calculating perplexity. Consequently, we limited our comparisons to models of the LLaMA-2 type to ensure consistency in tokenizer usage and accurate perplexity evaluations. The perplexity of each adapted models can be found in Table 5.\nAs anticipated, the DAPT-only model demonstrates the best perplexity results on DAPT test set. Interestingly, the IAPT models, when applied to telco-only instructions, achieves perplexity metrics that are quite comparable to the DAPT-only model. This suggests that the IAPT model, despite not being solely focused on the telco domain, is effective at adapting to domain-specific language."}, {"title": "4.2 MCQs results", "content": "In evaluating model performance, we primarily use multiple-choice questionnaires (MCQs) because evaluation on multiple-choice questions is a standard practice and is generally considered within the community to be a reliable proxy for testing a model's knowledge. This conviction stems from the straightforward, binary nature of MCQs, where each question presents a set of options, and only one holds the key to accuracy. Consequently, we employ 'accuracy' as our metric of choice. This binary framework, underpinning our MCQs, allows for an unequivocal evaluation, as the model's response is distilled to a singular letter, leaving no room for ambiguity-only the precision of 'correct' or the finality of 'incorrect'.\nFor inference on the datasets, the Llama-2-chat instruction template ([INST] {instruction} [/INST]) was consistently used for all Chat and IAPT models inferences. As for GPT3.5, GPT4, Base models and those with DAPT-only, the instruction was directly fed to the model (without the [INST] and [/INST] tags).\nWe launched the inference on the different Llama-2-7b-dapt, Llama-2-7b-dapt-iapt and Llama-2-7b-iapt models. As baseline commparison, we also launched the inference on meta-llama/Llama-2-7b-hf, on meta-llama/Llama-2-7b-chat-hf, as well as on GPT3.5 and GPT4. Note that the chat model from Huggingface (meta-llama/Llama-2-7b-chat-hf) is trained, in addition to instruction-tuning on meta-llama/Llama-2-7b-hf, with RLHF-based training, which differs from our Llama-2-7b-iapt and Llama-2-7b-dapt-iapt models. So our instruct-tuned models and the chat model from Llama-2 are not directly comparable.\nWe've set up a detailed process to clean up the answers the different models give. First, we use regular expressions (regex) to pick out the answer letters from different types of responses, like 'a. choice' or 'the answer is a.'. If the answers are not clear enough for this method, we use GPT-3.5 to retrieve the answer letters from natural language responses.\nAs exposed in Table 6, evaluations were performed on telco MCQs and general MCQs."}, {"title": "4.2.1 MCQs results on telco MCQs", "content": "Regarding the telecommunications-specific MCQs benchmark, as described in 3.1.3, we have 3 types of datasets:\n\u2022 MCQs with questions related to knowledge present in the DAPT and with similar IAPT instructions (ATIS, 3GPP, ETSI)\n\u2022 MCQs with some questions which might be related to knowledge present in the DAPT and with similar IAPT instructions (Tele-QnA)\n\u2022 MCQs whose answer is not explicitly present in any text or instruction from domain adaptation (Nokia)\nHere are our observations:\n\u2022 We first observe the same conclusion as (Cheng et al., 2023b) that DAPT-only adaptation results in a drastic drop in prompting performance of the model, making it unusable for domain-specific downstream tasks. However, coupled with IAPT adaptation, DAPT seems to result in telco-MCQ performance improvement. But the gain of DAPT (compared to IAPT-only adaptation) is not obvious (accuracy improved by +0.03 maximum). This leads us to conclude that DAPT is not necessary for domain adaptation, and that IAPT-only is sufficient.\n\u2022 We then observe that IAPT incorporating both telco and general instructions proves most effective, enhancing accuracy by +0.09 on average compared to the chat model for similar task datasets. However, telco-only IAPT modestly boosts outcomes by +0.03 and IAPT focusing solely on general instructions slightly decreases telco MCQs performance by -0.04.\n\u2022 We finally observe that our methods (dapt+iapt or iapt-only) are especially beneficial for specific knowledge MCQsS present either in the DAPT corpus or in the IAPT one, or in both. The results on the Nokia dataset were notably less favorable, primarily because this corpus demands a broader foundational knowledge than what is provided by the technical documents and instructions utilized in the adaptation process. This discrepancy highlights a crucial aspect of domain adaptation: while it significantly enhances a model's proficiency within a specific domain, its capacity for generalization beyond the scope of the adapted materials is inherently limited."}, {"title": "4.2.2 MCQs results on general MCQs", "content": "We have also conducted our experiments on general datasets to analyze whether or not domain adaptation leads the model to forget general knowledge. The general MCQs benchmarks is described in 3.1.3.\nResults of our experiences on these datasets can be found in"}]}