{"title": "JotlasNet: Joint Tensor Low-Rank and Attention-based Sparse\nUnrolling Network for Accelerating Dynamic MRI", "authors": ["Yinghao Zhang", "Haiyan Gui", "Ningdi Yang", "Yue Hua"], "abstract": "Joint low-rank and sparse unrolling networks have shown superior performance in dynamic MRI\nreconstruction. However, existing works mainly utilized matrix low-rank priors, neglecting the tensor\ncharacteristics of dynamic MRI images, and only a global threshold is applied for the sparse constraint\nto the multi-channel data, limiting the flexibility of the network. Additionally, most of them have\ninherently complex network structure, with intricate interactions among variables. In this paper, we\npropose a novel deep unrolling network, JotlasNet, for dynamic MRI reconstruction by jointly utilizing\ntensor low-rank and attention-based sparse priors. Specifically, we utilize tensor low-rank prior to\nexploit the structural correlations in high-dimensional data. Convolutional neural networks are used to\nadaptively learn the low-rank and sparse transform domains. A novel attention-based soft thresholding\noperator is proposed to assign a unique learnable threshold to each channel of the data in the CNN-\nlearned sparse domain. The network is unrolled from the elaborately designed composite splitting\nalgorithm and thus features a simple yet efficient parallel structure. Extensive experiments on two\ndatasets (OCMR, CMRxRecon) demonstrate the superior performance of JotlasNet in dynamic MRI\nreconstruction.", "sections": [{"title": "1. Introduction", "content": "Accelerating magnetic resonance imaging (MRI) has be-\ncome a highly crucial research direction in the MRI research\ncommunity [1]. For dynamic MRI that captures temporal\nchanges, a larger amount of data needs to be collected.\nThe demand for reducing imaging time is more urgent in\nthis context, and the required acceleration factor is also\nhigher. Currently, acquiring k-space data below the Nyquist\nsampling rate is the primary approach for accelerating MRI\n[2, 3]. However, this undersampling strategy inevitably leads\nto image aliasing artifacts, creating a trade-off between\nacceleration and image quality. Therefore, it is paramount\nto design reconstruction algorithms that can enhance image\nquality at a fixed acceleration factor or, in other words,\ntolerate higher acceleration factors while meeting clinical\nrequirements for image quality.\nIn recent decades, iterative optimization algorithms have\nemerged as promising tools for accelerating MRI [2]. Specif-\nically, explicit prior about MRI images is incorporated as a\nconstraint term, combined with the fidelity term of k-space\ndata, to construct an optimization reconstruction model.\nIterative algorithms like ADMM (Alternating Direction\nMethod of Multipliers) [4, 5] and ISTA (Iterative Shrinkage-\nThresholding Algorithm) [6] are commonly employed to\ntackle these optimization problems. Sparse priors, a funda-\nmental aspect of compressive sensing, have been widely em-\nployed in MRI reconstruction [3, 7-9]. Additionally, meth-\nods leveraging matrix/tensor low-rank priors [10, 11] have\nbeen proposed. Moreover, hybrid approaches combining"}, {"title": "2. Related Works", "content": "2.1. General Reconstruction Model of Dynamic\nMRI\nThe acquisition of dynamic MRI can be formulated as,\n$\\begin{equation}\nb = A(X) + n,\n\\end{equation}$\nwhere $b \\in C^M$ is the acquired k-space data, $X \\in C^{H \\times W \\times T}$\nis the distortion-free dynamic MRI image with H, W, and\nT representing the spatial dimensions and temporal frames,\nrespectively, $A : C^{H \\times W \\times T} \\rightarrow C^M$ is the acquisition\noperator, and $n \\in C^M$ is the noise. From the above equation,\nit can be observed that reconstructing the clean image X\nfrom the acquired undersampled k-space data b is a typical\nill-posed linear inverse problem. Specifically, based on the\nphysical principles of MRI, operator A can be expressed in\na more detailed manner,\n$\\begin{equation}\nA = F \\circ S = M \\circ F \\circ S,\n\\end{equation}$\nwhere $F_u$ is the undersampling operator, $S : C^{H \\times W \\times T} \\rightarrow\nC^{C \\times H \\times W \\times T}$ denotes the coil sensitive maps for multi-coil\nMRI with C coils, and the symbol o denotes the composite\noperation. When the sampling points are in the Cartesian\ngrid, $F_u = M \\circ F$ holds with F being the unitary Fourier\ntransform, and M denoting the sampling mask. As for single-\ncoil cases, S can be omitted."}, {"title": "2.2. Unrolling Networks using Single Prior", "content": "An unrolling network [14, 26, 27] generally involves\nthree factors: a prior-driven reconstruction model, an iter-\native solving algorithm, and unfolding. Actually, the first\ntwo factors can precisely constitute a complete optimization-\nbased reconstruction method.\nIn recent years, various unrolling reconstruction net-\nworks based on a single prior have emerged and become\nmainstream [1]. DCCNN [16], MoDL [28], and E2EVarNet\n[29] are representative examples that using CNNs to learn\nthe implicit image prior. HUMUS-Net [30] took the vision\nTransformer to substitute CNN. CineVN [31] employed a\nspatiotemporal E2EVarNet combined with conjugate gradi-\nent descent for optimized data consistency and improved im-\nage quality for dynamic MRI. However, the implicit nature\nleads to a lack of interpretability. ISTA-Net [17] and FISTA-\nNet [32] exploited the learned sparse prior and adopted the\nST operator [18] to enforce the sparsity constraint in a CNN-\nlearned multi-channel sparse domain. However, the ST op-\nerator applied the same threshold to all channels and the\ntwo CNNs that learn the sparse transforms are constrained\nto be inverse, limiting the flexibility of the sparse constraint.\nMoreover, T2LR-Net [21, 33] utilized the tensor low-rank\nprior and the tensor nuclear norm [20] to exploit the struc-\ntural correlations in high-dimensional data, in which CNNs\nwere also used to adaptively learn the low-rank transform\ndomain.\nAs for the underlying iterative solving algorithms that we\ntermed as the structure of the DUNs, algorithms like ADMM\n[4], ISTA [6], and others are widely used. However, although\nADMM is an effective algorithm for a lot of optimization\nproblems, it usually requires the conjugate gradient (CG)\nmethod to solve the subproblems, especially for the MRI\nreconstruction cases with non-Cartesian sampling and multi-\ncoil data. Inserting CG in DUNs will increase the complexity\nof the network and make the training process more difficult\n[28]. ISTA does not require CG iterations; it can be itera-\ntively solved through a gradient descent step and a projection\nstep. Another widely used approach is to directly simplify\nthe gradient descent step in ISTA through the use of the Data\nConsistency (DC) layer [16, 29]. The ISTA algorithm for\nsolving (3) can be formulated as,\n$\\begin{equation}\n\\begin{cases}\nX = X^n \u2013 \\mu \\nabla_x^n [\\frac{1}{2} ||A(X) \u2013 b||_2^2] = X^n \u2013 \\mu A^H [A(X^n) \u2013 b] \\\\\nX^{(k+1)} = arg \\underset{X}{min} \\frac{1}{2} ||X \u2013 X^n||_2^2 + \\mu \\lambda \\Phi(X) = prox_{\\mu \\lambda \\varphi}(X),\n\\end{cases}\n\\end{equation}$\nwhere the operator $prox_{\\mu \\lambda \\varphi}(\\cdot)$ is the projection onto the set\ndefined by the regularization \u03a6 with the threshold $\u03bc\u03bb$, with\n\u03bc being the step size."}, {"title": "2.3. Unrolling Networks using Composite Priors", "content": "Composite priors-driven DUNs in dynamic MRI recon-\nstruction mainly lie in the intersection of the low-rank and\nsparse priors. LplusS-Net [34] and SLR-Net [15] represent\ntwo different types, where the former follows a low-rank\nplus sparse scheme, while the latter adopts a joint low-rank\nand sparse formulation which is the focus of this paper.\nAdditionally, SOUL-Net [35] utilizes similar structures to\nSLR-Net but is applied in CT reconstruction. JSLR-Net [36]\nalso follows the joint low-rank and sparse scheme, but it is\nunfolded by the HQS algorithm, which may need conjugate\ngradient (CG) iterations to solve the subproblems. However,\nCG iterations will increase the complexity of the network\nand make the training process more difficult [28].\nIn SLR-Net, the low-rank prior is leveraged through\nthe Casorati matrix. The Casorati matrix is obtained by\nunfolding the dynamic MRI tensor & along the temporal\ndimension, resulting in an $HW \u00d7 T$ matrix. The nuclear\nnorm of this matrix is then utilized to explore the correla-\ntion of temporal evolution curves. However, this approach\nneglects spatial correlations, and the process of unfolding a\nhigh-dimensional tensor into a matrix inevitably loses the\nhigh-dimensional structural features of the data. Therefore,\nwe believe a superior approach would be to employ tensor\nlow-rank priors to construct the network.\nFurthermore, the joint low-rank and sparse model can be\nformulated as,\n$\\begin{equation}\n\\underset{x}{min} \\frac{1}{2} ||A(x) \u2013 b||_2^2 + \\lambda_1R(X) + \\lambda_2S(X),\n\\end{equation}$\nwhere R(X) and S(X) denote the LR and sparse priors,\nrespectively. $\u03bb_1$ and $\u03bb_2$ are the balancing parameters. SLR-\nNet has developed an algorithm that embeds ISTA within\nADMM. Specifically, using ADMM and introducing aux-\niliary variables T, the optimization problem (5) can be\nreformulated as,\n$\\begin{equation}\n\\begin{aligned}\n\\underset{x}{min} \\frac{1}{2} ||A(X) \u2013 b||_2^2 + \\frac{1}{2}||X \u2013 T + L||_2^2 + \\lambda_2S(X) \\\\\n\\underset{T}{min} \\frac{1}{2} ||X \u2013 T + L||_2^2 + \\lambda_1R(T) \\\\\nL = L + \\eta(X \u2013 T),\n\\end{aligned}\n\\end{equation}$\nwhere L is the Lagrangian multiplier. ISTA is then embed-\nded to solve the & subproblem, resulting in,\n$\\begin{equation}\n\\begin{aligned}\nZ = X \u2013 \\mu \\nabla_x [\\frac{1}{2}||A(x) \u2013 b||_2^2 + \\frac{1}{2}||X \u2013 T + L||_2^2] \\\\\n\\underset{X}{min} \\frac{1}{2} || - Z||_2^2 + \\lambda_2S(X) \\\\\n\\underset{T}{min} \\frac{1}{2} ||X \u2013 T + L||_2^2 + \\lambda_1R(T) \\\\\nL = L + \\eta(X \u2013 T).\n\\end{aligned}\n\\end{equation}$\nNote that we have omitted the indexing notation for iterations\nfor the sake of brevity. From the above equation, it is evident\nthat the interactions among the three temporary variables, Z,\nT, and L, are relatively complex. This complexity renders\nthe design of composite-prior DUNs difficult and cum-\nbersome, while also hindering efficient backpropagation to\nsome extent, consequently leading to suboptimal reconstruc-\ntion results. We believe that a more simple iterative solving"}, {"title": "3. Method", "content": "3.1. Composite Splitting Algorithm for Joint\nLow-Rank and Sparse Model\nThe dynamic MRI image is a 3-way tensor and exhibits\nhigh-dimensional data structures and correlations. There-\nfore, we construct the low-rank and sparse regularizations\nfrom the perspective of tensor.\nSpecifically, the tensor low-rank regularization is con-\nstructed based on the transformed tensor nuclear norm\n(TTNN) [21] under the framework of t-SVD [20], which is\ndefined\n$\\begin{equation}\nR(X) = \\sum_{i=1}^{T} ||Y^{(i)}||_*.\n\\end{equation}$\nIn the above equation, the operator || ||* denotes the nuclear\nnorm of a matrix, which is the sum of its singular values,\nand T is a CNN-learned transformation that maps the input\ntensor to a low-rank domain. The subscript (i) denotes the\ni-th frontal slice of the tensor, i.e., for a tensor Y = T(X),\n$y^{(i)} = y(:, :, i), i = 1, 2, ..., T$ holds.\nThe TTNN can be interpreted as the sum of the nuclear\nnorms of the frontal slices in the transformed domain. The\ntransformation T applied to the entire tensor allows for\nthe extraction of high-dimensional structural information.\nConsequently, within the transformed domain, any frontal\nslice may encompass all the information from the original\nimage domain. While we aim to provide a comprehensive\ninterpretation of TTNN, it is essential to note that TTNN\npossesses a complete and rigorous mathematical definition\nand derivation (see [21]).\nThe tensor sparse regularization is constructed based on\nthe tensor 1\u2081 norm in the CNN-learned domain, which is\ndefined as,\n$\\begin{equation}\nS(X) = ||D(X)||_1,\n\\end{equation}$\nwhere D is a CNN-learned transformation that maps the\ninput tensor to a sparse domain. The tensor 1\u2081 norm is the\nsum of the absolute values of all elements in the tensor.\nTo efficiently solve the joint low-rank and sparse model\n(5), the optimization problem (5) can be decomposed into\nthe following subproblems using the composite splitting\nalgorithm [24, 37],\n$\\begin{equation}\nX = X^n \u2013 \\mu A^H [A(X^n) \u2013 b],\n\\end{equation}$\n$\\begin{equation}\nY_1 = arg \\underset{V_1}{min} \\frac{\\omega_1}{2\\mu} || -||_F^2 + \\lambda_1 R(X),\n\\end{equation}$\n$\\begin{equation}\nY_2 = arg \\underset{V_2}{min} \\frac{\\omega_2}{2\\mu} || -||_F^2 + \\lambda_2 S(X),\n\\end{equation}$\n$\\begin{equation}\nX^{n+1} = \\omega_1 V_1 + \\omega_2 V_2,\n\\end{equation}$\nwhere the first step is the gradient descent, the second and\nthe third steps are the projection steps, and $\u03c9_1$ and $\u03c9_2$ with\n$\u03c9_1 + \u03c9_2 = 1$ are the balancing parameters.\nFrom (8) and the corresponding transformed tensor sin-\ngular value thresholding algorithm [21], the $Y_1$ subproblem\ncan be solved as,\n$\\begin{equation}\nV_1 = T^H \\circ SVT_{\\frac{\\mu \\lambda_1}{\\omega_1}} \\circ T(X),\n\\end{equation}$\nwhere SVT(\u00b7) is the singular value thresholding operator\nwith threshold t for each frontal slice of the tensor and $T^H$ is\nthe adjoint transformation of T. We further employ a widely\nused trick [15] to allocate a distinct threshold to each frontal\nslice individually (more details in Section 3.2.1).\nThe V2 subproblem can be solved by the soft threshold-\ning operator [18] in the transformed domain, i.e.,\n$\\begin{equation}\nV_2 = D^H \\circ ST_{\\frac{\\mu \\lambda_2}{\\omega_2}} \\circ D(X),\n\\end{equation}$\nwhere ST(\u00b7) is the soft thresholding operator with threshold\n\u03c4 for each element of the tensor. It is worth noting that the\nST operator only uses a single threshold for all elements in\nthe tensor, which may limit the flexibility of the sparse con-\nstraint. We propose a novel attention-based soft thresholding\noperator in Section 3.2.2 to address this issue.\nFrom the above algorithm, it can be observed that our\ndesigned algorithm possesses an efficient yet simple struc-\nture. Intermediate variables & and Y interact only within\nthe current iteration, avoiding intricate relationships between\niterations, unlike the algorithm (7) in SLR-Net [15]. More-\nover, our algorithm can be considered a reasonable extension\nof ISTA, exhibiting a similar alternation of two ISTA (R(X)\nand S(X)-constrained ISTAs) with the addition of a linear\ncombination of two proximal mappings in the last iteration\nstep (13). Therefore, building upon the widespread recogni-\ntion and application of ISTA in the DUN community, our\nalgorithm also inherits the advantages of ISTA for DUNs.\nAdditionally, similar to ISTA's ability to incorporate\nthe Nesterov acceleration step for improved convergence\nspeed, and as demonstrated by FISTA-Net [32], adding the\nacceleration step can enhance reconstruction effectiveness.\nThus, inspired by these findings, we also integrate Nesterov\nacceleration steps into our algorithm. In summary, our algo-\nrithm is depicted in Alg.1."}, {"title": "3.2. JotlasNet", "content": "Unfolding Alg.1 into a DUN allows for transformations\nand hyperparameters therein becoming neural networks and\nlearnable parameters. Therefore, we can also further enhance\nthe flexibility by assigning a unique set of parameters to\neach iteration. Finally, we propose the JotlasNet as shown\nin Fig.1, which is unfolded from the following algorithm.\n$\\begin{equation}\n\\begin{cases}\nX = X^n \u2013 \\mu^n A^H [A(X^n) \u2013 b] \\\\\n\\Upsilon_1 = \\\u0164^n SVT_{\\frac{\\mu^n}{\\omega_1^n} th^n} \\circ \\T^n(X)\\\\\n\\Upsilon_2 = \\breve{D}^n \\circ AST_{\\frac{\\mu^n}{\\omega_2^n}} \\circ \\breve{D}^n(X)\\\\\nZ^n = \\omega_1^n\\Upsilon_1 + \\omega_2^n\\Upsilon_2 \\\\\nX^{n+1} = Z^n + \\frac{t^{n-1}}{t^{n+1}}(Z^n \u2013 Z^{n-1})\n\\end{cases},\n\\end{equation}$\nwhere we replace $\u03bc\u03bb_1$ and with a single learnable pa-\nrameter $th^n$ and t, respectively. The ST operator with a single\nthreshold $\u03bc\u03bb_2$ is replaced by the proposed AST operator\nwith a vector of thresholds 7 for each channel of the input\ndata. The red-marked symbols in Fig.1 are learnable and the\nsuperscript n denotes they are unique for each iteration.\nThe JotlasNet consists of five layers, the Gradient De-\nscent (GD), Low-Rank (LR), Sparse (S), combination and\nAcceleration (ACC) layers, corresponding to the five steps\nof (16), respectively. Except for the LR and S layers, the\nremaining layers are not incorporated with neural networks,\nsuch as CNN. Instead, they are computed in the deep learn-\ning framework according to the iterative algorithm's formu-\nlas, with only the parameters becoming trainable. It is worth\nnoting that $\u03bc > 0, \u03c9_1 + \u03c9_2 = 1, and t \u2208 [0,1]$. We can\napply ReLU, softmax, and sigmoid functions to reasonably\nconstrain the parameters accordingly. The LR and S layers\nare described in detail as follows."}, {"title": "3.2.1. The LR Layer", "content": "This layer corresponds to the \u2081 step in Eq.(16). We\nutilize two independent CNNs to learn the transformations\nTH and T in Alg.1, denoted as \u0164 and T, respectively. These\ntwo CNNs share the same structure, consisting of three 3D\nconvolutional layers with kernel sizes of 3 \u00d7 3 \u00d7 3 and the\nstride of 1. The output channels for the three convolutional\nlayers are Nc, Nc, and 2, respectively. The reason for\nhaving 2 channels in the last convolutional layer is that we\nsplit the MRI complex data into real and imaginary parts.\nTwo ReLU activation functions are interspersed between the\nthree convolutional layers, with the last convolutional layer\nnot being activated to ensure that negative values are not\ntruncated in the output. In the CNN transformation domain,\nwe synthesize the dual-channel real data into complex data,\nwhich we denote as F\u2081. The learned SVT operator in the\nmiddle is applied to each frontal slice of F\u2081. For the i-th\nfrontal slice with its singular value decomposition (SVD) as\nF(i) = U\u03a3VH, the learned SVT operates as follows.\n$\\begin{equation}\nSVT_{th/\\omega_1}(F^{(i)}) = U \\cdot ReLU(\\frac{\\Sigma}{ \\text{sigmoid}(th) \\cdot \\sigma_{max}}),\\cdot V^H,\n\\end{equation}$\nwhere we omit the superscript n for brevity, omax is the max-\nimum singular value of F(i), and sigmoid(\u00b7) is the sigmoid\nfunction.\nIt is worth noting that during the unrolling process, we\nrelax the constraint in Alg.1 that TH and T must be adjoint,\nallowing the two CNNs to learn freely from the data. This\napproach enables us to simultaneously utilize explicit low-\nrank priors and implicit image prior information extracted by\nCNNs, thereby enhancing the reconstruction performance\n[21]. Specifically, if the threshold th is learned to be close to\nzero or a very small value during training, the LR layer will\neffectively become a pure CNN layer, capable of learning\nadditional prior information beyond low-rank constraints\nthat may be required for reconstruction."}, {"title": "3.2.2. The S Layer", "content": "Similar to the LR layer, this layer corresponds to the V2\nstep in Eq.(16). Similarly, we use two independent CNNs to\nlearn the transformations DH and D in Alg.1, denoted as \u010e\nand D, respectively. Unlike the LR layer, the output channels\nof the third convolutional layer used to learn D are set to\nNc to increase data dimensionality and redundancy, thereby\nextracting richer sparse prior information. This is mainly\ndue to the sparse constraint not requiring SVD, resulting in\nlower computational complexity. Additionally, in the CNN\ntransform domain, we do not synthesize dual-channel real\ndata into complex data; instead, we directly impose sparse\nconstraints on each channel.\nCurrent DUNs implement sparse constraints using an\nST operator, where only one threshold is used to constrain\ndata across all channels. We believe that this approach may\nlimit the expressive power of sparse constraints, as data in\ndifferent channels may exhibit different sparsity patterns.\nTherefore, inspired by [38], we propose a novel Attention-\nbased Soft Thresholding operator (AST), assigning a learn-\nable threshold to each channel of the data in the CNN-\nlearned sparse domain. The diagram of AST is shown in\nFig. 1. Specifically, for an input data F\u2082 of size Nc \u00d7 H \u00d7\nW \u00d7 T, AST first takes the absolute value and performs\nglobal average pooling (GAP) to obtain a vector f of size\nNc \u00d7 1. Then, two fully connected layers separated by\nReLU activation and later the sigmoid function are utilized\nto obtain a self-attention weight vector w. Element-wise\nmultiplication of w and f yields an adaptive sparse threshold\nvector 7 = [\u03c41, \u03c42, \u2026, TNc]T. Finally, the ST operator with\nthe threshold \u03c4\u2081 is applied to the i-th channel of the data F[i]\nas follows.\n$\\begin{equation}\nST_{\\frac{\\tau_i}{\\omega_2}}(F^{[i]}) = \\frac{F^{[i]}}{ |F^{[i]}| } \\cdot ReLU(\\frac{ |F^{[i]}| }{ |F^{[i]}| } - \\frac{\\tau_i}{\\omega_2}),\n\\end{equation}$\nwhere $|F^{[i]}|$\\ denotes the absolute value of $F^{[i]}$\\ and $\\frac{F^{[i]}}{ |F^{[i]}| }$\\ is\nthe element-wise divide operation, i.e., the sign function of\n$F^{[i]}.$."}, {"title": "3.2.3. Loss Function", "content": "We utilize the mean squared error (MSE) as the loss\nfunction, i.e.,\n$\\begin{equation}\nLoss = \\sum_{(X_{GT},b)\\in \\Omega} ||X_{GT} \u2013 f_{net} (b|\\theta)||_2,\n\\end{equation}$\nwhere \u03a9 is the training set, $X_{GT}$ is the ground truth,\n$f_{net} (b|\\theta)$ is the output of the network with parameters $\u03b8 =$\n{$T^n$,\u0164", "D^n$,\u010e": "\u03bc^n$,$th^n$, ,", "FC": "n = 1\u2026 N}, and\nFC denotes the two fully connected layers in the AST\noperator."}, {"title": "3.2.4. Implementation Details", "content": "Considering the trade-off between computational burden\nand reconstruction accuracy, we set the number of iterations\n(N) for JotlasNet to 15. The number of channels for convolu-\ntional layers or hidden units for fully connected layers (N)\nis set to 16, resulting in a total of about 708k parameters.\nWe train the network using the Adam optimizer [39] with\nparameters $\u03b2_1$ = 0.9, $\u03b2_2$ = 0.999, \u0454 = $10^{\u22128}$, and an initial\nlearning rate of 0.001, decayed by a factor of 0.95 [40]. The\nmodel is implemented using the TensorFlow framework [41]\nwith a batch size of 1."}, {"title": "3.2.5. Computational Complexity", "content": "Suppose that the input of JotlasNet X has the size of H \u00d7\nW \u00d7 T with H > W, and the number of coils is denoted as C.\nThe GD layer only consists of linear operations and repeated\nfast Fourier transforms on H and W spatial dimensions\nacross temporal dimension T and coil channel C, resulting\nin a complexity of O(HW log(HW)TC). For the S layer,\nthe total complexity of the two CNNs (the transform D and\nits transpose) is HWT \u00d7 33 \u00d7 16 \u00d7 (16 + 16 + 2) \u00d7 2,\ni.e., O(HWT). Here, we directly use the kernel size and\nthe number of channels from the implementation details to\nsimplify the calculation. The cost of the AST operator is\ndominated by linear operations, leading to a complexity of\nO(HWT). Note that the calculation of the attention maps\ndepends on the number of channels, which is negligible\ncompared to the linear operations. For the LR layer, the two\nCNNs have the same complexity as the S layer, resulting in\nO(HWT). The SVT operator is dominated by the tensor\nSVD, whose complexity is O(HW2T). The ACC layer\nonly involves linear operations, resulting in a complexity of\nO(HWT). Therefore, for N iterations, the total complexity\nof JotlasNet is O(NHWT(log(HW)C + W))."}, {"title": "4. Experiments and Results", "content": "4.1. Datasets and Experimental Setup\nWe conducted experiments on two publicly available\ncardiac cine MRI datasets, OCMR and CMRxRecon. The"}, {"title": "4.4. Abalation Study", "content": "Under the radial-16 case, we conducted ablation exper-\niments using OCMR single-coil data, as shown in Tab.6.\nModel-1 represents our JotlasNet; model-2 utilizes the algo-\nrithm proposed by SLR-Net, as described in (7), to solve our\nproposed tensor low-rank and sparse model and unfold it into\na network. Model-3 is the network without the sparse prior.\nModel-4 and 5 represent networks without the low-rank\nprior, which are unfolded from the ISTA algorithm as (4).\nThe distinction is that model-4 employs the AST operator\nproposed by us, while model-5 uses the traditional ST oper-\nator with only one threshold. From the comparison between\nmodel-1 and 2, it can be observed that the network unfolded\nby our proposed Alg.1 achieves better reconstruction results\nsince Alg.1 exhibits a simple yet efficient structure similar\nto the single-prior ISTA. Comparing model-1 with 3 and\n4, it can be seen that the dual-prior network achieves su-\nperior results compared to the single-prior network. From\nthe comparison between model-4 and 5, it can be concluded\nthat our proposed AST operator effectively enhances the\nnetwork's ability to exploit sparsity, thereby achieving better\nreconstruction results.\nAs mentioned in Section 3.2, the CNNs learning the low-\nrank or sparse transformations are subjected to no constraints\nin our implementation. We use model-4 as the baseline\nto analyze the impact of CNN constraints on the network.\nSpecifically, on top of the MSE loss in model-4, we in-\ncorporate the inverse constraint on the CNN-learned sparse\ntransforms, akin to ISTA-Net [17] and SLR-Net [15],\n$\\begin{equation}\nLoss = \\sum_{(X_{GT},b)\\in \\Omega} ||X_{GT} \u2013 f_{net} (b|\\theta)||_2\n+ \\sum_{n=1}^{N} \\zeta ||\\breve{D}^{n}D(n-1) \u2013 &n-1||,\n\\end{equation}$\nwhere is the hyperparameter controlling the strength of\nthe inverse constraint. We set ( to 0.001, 0.01, and 0.1 to\ninvestigate the impact of the CNN constraint on the network.\nThe results are shown in model-6 to 8. Compared with\nmodel-4 where ( = 0, it can be observed that the network's\nperformance deteriorates as the CNN constraint strengthens.\nThis indicates that inverse constraint forces CNNs to solely\nlearn explicit sparse characteristics, without further integrat-\ning their implicit feature extraction capabilities.\nAdditionally, we also investigated the optimal number of\niterations of JotlasNet under the radial-16 pattern in single\ncoil scenario, as the results are shown in Fig.5. It can be\nobserved that the PSNR increase slowly when the number\nof iterations exceeds 15. Therefore, in order to balance the\nreconstruction performance and computational efficiency,\nwe set the number of iterations to 15 in our experiments."}, {"title": "5. Discussion", "content": "5.1. Interpretive Insight of LR and S layers\nWe show the intermediate results from different iter-\nations of JotlasNet under radial-16 pattern in single coil\nscenario in Fig.6. In the early iterations (1-5), the LR layer\nfocuses on the background while the S layer captures the\nedges, as indicated by the red arrows. In the middle iterations\n(6-10), the LR layer tends to smooth the textures due to\nits ability to capture correlation information, while the S\nlayer adds the image details inside the smooth regions, as\nindicated by the red boxes. In the later iterations (11-15), the\nLR along with the S layer jointly remove the noise and refine\nthe textures, as the PSNR increases slowly."}, {"title": "5.2. Rethinking the Structure of Neural Networks", "content": "We compare the composite splitting algorithm that in-\ncorporates joint low-rank and sparsity priors with the ISTA\nalgorithm that utilizes low-rank plus sparse priors. We an-\nalyze the DUNs derived from these two algorithms to re-\nconsider neural network architecture. The former has been\nelaborately described earlier, while the latter utilizes the\nfollowing model and can be directly solved by ISTA [34]:\n$\\begin{equation}\n\\underset{x}{min} \\frac{1}{2} ||A(L + S) \u2013 b||_2^2 + \\lambda_1R(L) + \\lambda_2S(S),\n\\end{equation}$\nThe simplified diagrams of the networks unfolded from\nequations (16) and (22) are illustrated in Fig. 7. Fig. 7a depicts\nthe structure of unfolding equation (16), representing the\nsimplified diagram of our proposed network, while Fig.7b\nillustrates the structure of unrolling equation (22). The pro-\njection operations associated with low-rank and sparse con-\nstraints are represented by LR and S layers, respectively. Op-\nerations involving gradient descent and interactions between\nlow-rank and sparse outputs are depicted by translucent\nsquares, and interactions between iterations are represented\nby translucent lines. It is observed that for the network\nunfolded from equation (16), which exploits the joint uti-\nlization of low-rank and sparse priors, LR and S layers ap-\npear in a parallel configuration. Conversely, for the network\nunfolded from equation (22), which separates low-rank and\nsparse parts, LR and S layers appear in a serial configuration.\nThis observation may provide partial interpretability to the\nstructure of neural networks and offer insights for the design\nof DUNs."}, {"title": "5.3. Limitations and Future Work", "content": "Our proposed JotlasNet leverages jointly low-rank and\nsparse prior. Utilizing the low-rank prior often involves\ncomputing SVD. However, as mentioned in Section 4.2, the\ngradients of SVD exhibit numerical instability, particularly\nwhen two repeated singular values occur, leading to NaN\ngradients and hindering the training process [49"}]}