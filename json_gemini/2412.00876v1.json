{"title": "DYNAMIC-LLAVA: EFFICIENT MULTIMODAL LARGE LANGUAGE MODELS VIA DYNAMIC VISION-LANGUAGE CONTEXT SPARSIFI-CATION", "authors": ["Wenxuan Huang", "Zijie Zhai", "Yunhang Shen", "Shaoshen Cao", "Fei Zhao", "Xiangfeng Xu", "Zheyu Ye", "Shaohui Lin"], "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs. Existing methods attempt to reduce the vision context redundancy to achieve efficient MLLMs. Unfortunately, the efficiency benefits of the vision context reduction in the prefill stage gradually diminish during the decoding stage. To address this problem, we proposed a dynamic vision-language context sparsification framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision context in the prefill stage and decreases the memory and computation overhead of the generated language context during decoding. Dynamic-LLaVA designs a tailored sparsification inference scheme for different inference modes, i.e., prefill, decoding with and without KV cache, to achieve efficient inference of MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by ~75% in the prefill stage. Meanwhile, throughout the entire generation process of MLLMs, Dynamic-LLaVA reduces the ~50% computation consumption under decoding without KV cache, while saving ~50% GPU memory overhead when decoding with KV cache, due to the vision-language context sparsification. Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient inference for MLLMS with negligible understanding and generation ability degradation or even performance gains compared to the full-context inference baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have achieved outstanding performance and made a significant impact in real-world applications (Zheng et al., 2023; Team, 2023; Touvron et al., 2023a;b; Achiam et al., 2023; Jiang et al., 2024). In particular, within the vision-language multimodal fields, the LLaVA paradigm (Liu et al., 2024b;a; Bai et al., 2023; Zhu et al., 2023; Zhao et al., 2024) has emerged as the mainstream approach of Multimodal Large Language Models (MLLMs). This paradigm involves mapping visual data, through feature extractors and projecting embeddings, into the same feature distribution of LLMs for processing. It has shown notable success in enabling general capabilities in vision understanding, reasoning, and interaction.\nHowever, LLMs often adopt the decoder-only Transformer as the base architecture and typically contain an extremely large number of parameters. As output tokens are generated during decoding, the computational consumption progressively increases, leading to a substantial computational burden (Vaswani, 2017). To alleviate this issue, modern LLMs frequently employ the KV cache technique during decoding, which accelerates inference speed by storing previously computed KV"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 TOKEN REDUCTION FOR EFFICIENT MLLMS", "content": "The existing vision context sparsification (Chen et al., 2024a; Shang et al., 2024; Ye et al., 2024; Arif et al., 2024; Song et al., 2024) and efficient vision encoder/projector (Li et al., 2024a; Kar et al., 2024; Li et al., 2024b; Bai et al., 2023; Cha et al., 2024; Chen et al., 2024b; Chu et al., 2024) methods have attempted to reduce the image token to accelerate the prefill stage. However, with the decoding of MLLMs, the efficiency benefits gradually diminish during the decoding. Our proposed Dynamic-LLaVA framework achieves consistently efficient inference throughout the entire generation phase via the vision-language context sparsification. The detailed discussions are presented in Appendix A.3.1."}, {"title": "2.2 KV CACHE COMPRESSION FOR EFFICIENT LLMS", "content": "Some previous works (Zhang et al., 2024b; 2023; Li et al., 2024c; Xiao et al., 2023; Ge et al., 2023; Han et al., 2023; Yang et al., 2024; Zhang et al., 2024a) compress KV cache in order to reduce the GPU memory overhead of LLM inference during decoding, which has a similar goal to our Dynamic-LLaVA when considering decoding with KV cache. However, these methods typically necessitate using the subset of past generated KV cache to facilitate the selection of critical activations for participation during inference. In other words, it is imperative to obtaion the generated KV cache to make the decision for which activations should be removed during LLM inference. Dynamic-LLaVA, when decoding with KV cache, can be regarded as \u201conline KV cache compression\", i.e., it utilizes the features of the current output text token to determine whether to retain the generated activations without relying on past KV cache. This capability is crucial, as it enables Dynamic-LLaVA to enhance efficiency during the stages of prefill and decoding without KV cache, which do not involve KV cache utilization. Moreover, the additional efficiency gained in prefill and decoding without KV cache modes is of comparable importance for MLLMs (Liu et al., 2024b; Li et al., 2024d; Chen et al., 2024a; Huang et al., 2024a; Cha et al., 2024; Leviathan et al., 2023; Chen et al., 2023a; Liu et al., 2023a). A detailed discussion of the proposed Dynamic-LLaVA framework and traditional LLM KV cache compression methods is provided in Appendix A.3.2."}, {"title": "3 METHOD", "content": ""}, {"title": "3.1 PRELIMINARIES AND NOTATIONS", "content": "Multimodal large language model (MLLM), e.g., LLaVA (Liu et al., 2024b), continues the autoregressive model paradigm (Radford et al., 2019; Liu et al., 2024a) and typically includes prefill and decoding stages during inference. In the prefill stage, features from different modalities are mapped into the same feature distribution as the input embeddings of large language model (LLM). These multimodal features, in conjunction with text tokens, are simultaneously processed by LLM to generate the initial output text token. During the decoding stage, the tokens in the prefill stage, together with all subsequently generated output text tokens, are used in an autoregressive manner to predict the next output text token. In the context of LLM with L Transformer decoder layers, we denote distinct index sets for various types of tokens processed across the stages of the model. Specifically, the index set for image tokens during prefill is denoted as ${T}^{I} = \\{1,2,\\ldots,\\ldots, {N}^{I}\\}$, for text tokens during prefill as ${T}^{T} = \\{1,2,\\ldots,\\ldots, {N}^{T}\\}$, while for output text tokens during decoding as ${T}^{OT} = \\{1,2,\\ldots,\\ldots, {N}^{OT}\\}$, where ${N}^{I}$, ${N}^{T}$ and ${N}^{OT}$ are the counts of image tokens, text tokens and output text tokens of l-th decoder, respectively. The sets of image tokens, text tokens, and output text tokens processed by the l-th LLM decoder layer are denoted as ${S}^{I}_{l} = \\{{t}^{I}_{i}; \\forall i \\in {I}_{l}\\}$, ${S}^{T}_{l} = \\{{t}^{T}_{i}; \\forall i \\in {I}^{T}\\}$ and ${S}^{OT}_{l} = \\{{t}^{o}_{i}; \\forall i \\in {I}^{OT}\\}$, respectively, where ${t}^{i}_{i}$, ${t}^{f}_{i}$, ${t}^{o}_{i} \\in {R}^{d}$ denotes the i-th token of the"}, {"title": "different token sets for the l-th decoder. The sets of all tokens and the corresponding size are defined as $S_{l} = S^{I}_{l} \\cup S^{T}_{l} \\cup S^{OT}_{l}$ and $N_{l} = N^{I}_{l} + N^{T}_{l} + N^{OT}_{l}$, respectively.", "content": "Considering the computation of the standard LLM (Touvron et al., 2023a), the prefill stage of the l-th decoder is simply described as follows:\n$S^{l+1} = FFN(MHA(S_{l}, S^{P}_{l}, S^{P}_{l})),$ (1)\nwhere ${S}^{P}_{l}$ = ${S}^{I}_{l} \\cup {S}^{T}_{l}$. MHA($\\cdot$, $\\cdot$, $\\cdot$) and FFN($\\cdot$) denote the Multi-Head Attention Block and the Feed-Forward Networks in l-th LLM decoder layer (Vaswani, 2017).\nDuring the decoding stage, there are typically two methods available, i.e., decoding without and with KV cache. First, we consider decoding without KV cache as an extension of the prefill stage. The one-pass decoding is computed as follows:\n${S}^{l+1}_{OT} \\cup S^{T}_{l} = FFN(MHA(S_{l} \\cup S^{T}_{l}, S^{P}_{l} \\cup S^{T}_{l}, S^{P}_{l} \\cup S^{T}_{l})).$ (2)\nFor the decoding with KV cache, we further split the MHA operation as follows:\nQ, K, V = $\\{W^{Q}_{l}{S}^{OT}_{N^{OT}_{l}}\\}$, $\\{W^{K}_{l}{S}^{K}_{l}\\}$, $\\{W^{V}_{l}{S}^{V}_{l}\\}$,\n${S}^{K} = {S}^{K} \\cup K, {S}^{V} = {S}^{V} \\cup V,$\nO = ${W}^{O}_{l} Attention(Q, {S}^{K}, {S}^{V}),$ (3)\n${S}^{OT}_{N^{OT}_{l}} = FFN(O),$\nwhere ${W}^{Q}_{l}$, ${W}^{K}_{l}$, ${W}^{V}_{l}$ and ${W}^{O}_{l}$ are the linear layers to obtain the activation sets Q, K, V and O in l-th LLM decoder layer. Attention($\\cdot, \\cdot, \\cdot$) is the Scaled Dot-Product Attention operation (Vaswani, 2017). And ${S}^{OT}_{N^{OT}_{l}}$ is the last output text token (N^OT-th output text token) in the l-th decoder layer during decoding, while ${S}^{K}_{l}$ and ${S}^{V}_{l}$ are KV cache of the l-th decoder layer.\nThe primary computation cost of the prefill and decoding without KV cache operations involve processing various sets of tokens and most overhead is in the activation of linear layers. Specifically, the cost of l-th deocder layer during the prefill stage is dictated by the combined size of the image and text token sets, denoted as Computation(Prefill)$_{l}$ $propto$ $|S^{I}_{l} \\cup S^{T}_{l}|$, where $| \\cdot |$ denotes the number of tokens. For the decoding stage without KV cache, the computation overhead is determined by the sum of the aforementioned components plus the size of the generated output text tokens, represented as Computation(Decoding${w/o cache}$)$_{l}$ $propto$ $|S^{I}_{l} \\cup S^{T}_{l} \\cup S^{OT}_{l}|$. Unlike the previous two methods, decoding with KV cache operation requires only the last output text token to be activated. This decoding method yields lower computation overhead compared to decoding without KV cache. However, it needs additional GPU memory to store the the activated intermediate variables, thereby introducing a higher GPU memory cost (Luohe et al., 2024). The GPU memory overhead of l-th KV cache is also determined by the quantity of the token sets, defined as Memory(Decoding${w/cache}$)$_{l}$ $propto$ $|S^{I}_{l} \\cup S^{T}_{l} \\cup S^{OT}_{l}|$. Given a l-th LLM decoder layer, reducing the sizes $|S^{I}_{l}|$, $|S^{T}_{l}|$ and $|S^{OT}_{l}|$ results in a corresponding reduction in the number of tokens for layers beyond the l-th decoder layer (Huang et al., 2024b). This reduction decreases the computation consumption and GPU memory overhead during the prefill and decoding stage of LLM."}, {"title": "3.2 MOTIVATION", "content": "As mentioned above, the token number affects the computation consumption and GPU memory overhead, while the previous works (Chen et al., 2024a; Shang et al., 2024; Li et al., 2024a; Ye et al., 2024; Arif et al., 2024; Song et al., 2024) focus on reducing the image token set $|S^{I}|$. However, the reduction in the number of image tokens often primarily accelerates the prefill stage and gradually diminishes in benefit during the decoding stage.\nComputation(Prefill)$_{l}$ $propto$ $|S^{I}_{l} \\cup S^{T}_{l}|$,\nComputation(Decoding${w/ocache}$)$_{l}$ $propto$ $|S^{I}_{l} \\cup S^{T}_{l} \\cup S^{OT}_{l}| \\approx |S^{OT}_{l}|$,\nMemory(Decoding${w/cache}$)$_{l}$ $propto$ $|S^{I}_{l} \\cup S^{T}_{l} \\cup S^{OT}_{l}| \\approx |S^{K}_{l} \\cup S^{V}_{l}|$, (4)\nwhere, $|S^{OT}| \\rightarrow \\infty$.\nAs shown in Eq. 4 and Fig. 1, with the generation of output text tokens during the decoding stage, the computation consumption and GPU memory overhead increase progressively. Meanwhile, the prefill"}, {"title": "3.3 DYNAMIC VISON-LANGUAGE CONTEXT SPARSIFICATION", "content": ""}, {"title": "3.3.1 OVERVIEW", "content": "To compress the image token set and output token set, we use two binary masks ${M}^{I}_{l}$ and ${M}^{OT}_{l}$ to determine whether their tokens should be retained or discarded in l-th decoder layer, where ${M}^{I}_{l} = \\{{m}^{i}_{i} | {m}^{i}_{i} \\in \\{0, 1\\} \\land \\forall i \\in {I}_{l}\\}$ and ${M}^{OT}_{l} = \\{{m}^{o}_{i} | {m}^{o}_{i} \\in \\{0,1\\} \\land \\forall i \\in {I}^{OT}\\}$. The reduced image token set and output text token set are defined as $S^{I*}_{l} = \\{{S}_{i,i} | {M}^{I}_{l} = 1 \\land \\forall i \\in {I}_{l}\\}$ and ${S}^{OT*}_{l} = \\{{S}_{i,i} | {M}^{OT}_{l} = 1 \\land \\forall i \\in {I}^{OT}\\}$, respectively, where $|S^{I*}| < |S^{I}|$ and $|S^{OT*}| < |S^{OT}|$. In this way, the computation consumption and GPU memory overhead in both the prefill and decoding stages, as described in Eq. 4, are simultaneously reduced. Such optimization contributes to efficient resource utilization during the entire generation process of LLM. In particular, Dynamic-LLaVA uses two learnable predictors ${P}^{I}_{l}$ and ${P}^{OT}_{l}$ to generate the binary masks ${M}^{I}_{l}$ and ${M}^{OT}_{l}$, respectively. The learnable predictors are applied only once, after the l-th decoder layer of the MLLM. Specifically, once the predictor makes the decisions on tokens, this decisions are shared across all subsequent layers. This one-time token reduction strategy circumvents the need for complex adjustments to sparsity ratios of MLLM decoders. In practice, we follow the work (Chen et al., 2024a) and set l to 2. The ablation study for l is presented in Tab. 6. Note that the learnable predictors consist of lightweight, multi-layer neural networks that introduce only marginal additional computation costs (less than 1% of the total). The detailed real GPU latency and memory of the Dynamic-LLaVA framework are shown in Tab. 4 and Tab. 14. Detailed architectures are shown in Appendix A.4.1.\nIn the following sections, we introduce how to perform the sparsification inference in the prefill and decoding stages of MLLMs in Sec. 3.3.2. We also introduce the end-to-end training for the learnable predictors in Sec. 3.3.3."}, {"title": "3.3.2 SPARSIFICATION INFERENCE", "content": "As shown in Fig. 2, we apply different sparsification strategies tailored to specific generation stages of MLLMs. For the token sets ${S}^{I}_{l}$ and ${S}^{T}_{l}$ processed by l-th decoder layer in the prefill stage, we consider the reduction of the image tokens. The image predictor leverages the features of image tokens and predicts the binary mask to select the tokens that participate in the prefill computation."}, {"title": "Specifically, we consider the set ${S}^{I} \\in {R}^{N^{I} \\times d}$ as 2D matrices and the pipeline is defined as:\n${D}^{I} = {P}^{I}({S}^{I}) \\in {R}^{N^{I} \\times 2},$\n${M}^{I} = argmax_{j}({D}^{I}),$\n$S^{I*} = \\{{S}_{i,i} | {M}^{I}_{l} = 1 \\land \\forall i \\in {I}_{l} \\},$ (5)", "content": "where ${P}^{I}(\\cdot)$ is the forward propagation of the image predictor, ${D}^{I}$ is a feature sampling decision generated by ${P}^{I}$, and $argmax_{j}$ indicates that we sample values of ${D}^{I}$ along the second dimension. This indicates that a token should be kept if the second value is greater than the first value in the second dimension of ${D}^{I}$. Then we replace the reduced token set ${S}^{P*} = S^{I*} \\cup S^{T}$ with ${S}^{I}$ in Eq. 1 to perform the prefill stage.\nFor the decoding stage, we first consider decoding without KV cache. This operation is similar to the prefill stage except that the last output token generates the next output token during decoding. Thus we use the pipeline in Eq. 5 with ${S}^{OT}$ and ${P}^{OT}$, while keeping ${M}^{OT} = 1$ to obtain ${S}^{OT*}$, where ${M}^{OT}$ is the ${N^{OT}}$-th value of the mask ${M}^{OT}$. We further replace the above ${S}^{P*}$ and ${S}^{OT*}$ with ${S}^{P}$ and ${S}^{OT}$ in Eq. 2 to conduct the decoding stage. Note that in both the prefill and decoding without KV cache stages, after the predictors reduce the token set ${S}_{l}$ to ${S*}_{l}$, all subsequent layers use this reduced token set for computation, i.e., the length of computed token sets is smaller to introduce the computation efficiency.\nFor decoding with KV cache, our primary goal is to reduce the sizes of KV cache, specifically ${S}^{K}_{l}$ and ${S}^{V}_{l}$, to achieve GPU memory savings during the decoding process. We use the last output token ${S}^{OT}_{N^{OT}_{l}}$ and ${P}^{OT}$ to get a binary decision ${M}^{OT}_{l} \\in \\{0, 1\\}$ by Eq. 5 and the decoding operation in Eq. 3 is modified to the following operation:\nQ, K, V = $\\{W^{Q}_{l}{S}^{OT}_{N^{OT}_{l}}\\}$, $\\{W^{K}_{l}{S}^{K}_{l}\\}$, $\\{W^{V}_{l}{S}^{V}_{l}\\}$,\nO = ${W}^{O}_{l} Attention(Q, {S}^{K} \\cup K, {S}^{V} \\cup V)$,\n$\\begin{cases}\n{S}^{K} = {S}^{K} \\cup K, {S}^{V} = {S}^{V} \\cup V, if {M}^{OT}_{l} = 1, \\\n{S}^{K} = {S}^{K} \\cup \\varnothing, {S}^{V} = {S}^{V} \\cup \\varnothing, otherwise,\n\\end{cases}$ (6)\n${S}^{OT}_{N^{OT}_{l}} = FFN(O),$\nwhere the binary mask ${M}^{OT}_{l}$ determines whether KV activations of the current output text token should be added to KV cache. Meanwhile, the decision to add the current token's activations to the KV cache is also propagated to subsequent layers, and the size of the KV cache for subsequent layers is reduced accordingly. Note that we still include this token in the current attention computation, regardless of whether it will be discarded in subsequent calculations. This ensures that this token contributes to the attention mechanism immediately, even if it does not persist in KV cache for future decoding steps."}, {"title": "3.3.3 END-TO-END SPARSIFICATION TRAINING", "content": "Unlike the sparsification inference phase, where the token sets are directly reduced, we employ the full-token sets along with binary masks to optimize the predictors and LLM in an end-to-end training process, as inspired by (Veit & Belongie, 2018; Herrmann et al., 2020; Rao et al., 2021; Lin et al., 2023). This approach ensures the model dynamically adjusts and learns which tokens are essential during training while maintaining the full set of tokens for comprehensive optimization. The details of the end-to-end sparsification training are presented in Fig. 3 and Appendix A.2.\nGiven the binary masks of the token sets, we need to use these masks to isolate the influence of non-essential tokens on the important tokens during the LLM training computation. One native idea is to directly set the values of the unnecessary tokens to zero vectors. However, this method introduces a problem: when we sparsify output text tokens during the parallel training of LLM, discarding the value of an output text token will prevent it from autoregressively predicting the next output text token, making it impossible to compute the loss. This \u201chard training\" result is presented in Tab. 7. To address this, we apply the masks to the Softmax($\\cdot$) operation in Attention($\\cdot$) during training. Specifically, we firstly obtain a binary mask of the full-token set ${M} = {M}^{I} \\cup \\{1\\}^{N_{l}^{T} \\cup {N}^{OT}_{l}}$."}, {"title": "Then we use the full-token set mask to get the binary mask matrix $\\mathbb{G} = \\{\\mathbb{M}\\}^{N_{l}}_{i \\in []} \\in {\\mathbb{R}}^{N_{l} \\times N_{l}}$ and let $diag(\\mathbb{G}) = 1$. The Softmax($\\cdot$) is changed to MaskedSoftmax($\\cdot, \\cdot$) operation during training:", "content": "$\\begin{aligned}\nMaskedSoftmax(X_{i,j}, \\mathbb{G}) = \\frac{exp(X_{i,j})\\mathbb{G}_{i,j}}{\\sum^{N_l}_{k=1} exp(X_{i,k})\\mathbb{G}_{i,k}}\n\\end{aligned}$ (7)\nwhere $X \\in {\\mathbb{R}}^{N_{l} \\times N_{l}}$ is the input of Softmax($\\cdot$) operation. Eq. 7 allows our framework to maintain parallelism in training while ensuring that the non-essential tokens do not influence the output, without breaking the autoregressive process essential for loss calculation. Note that this form of masking and predictors maintains uniformity between training and inference. Specifically, during training, the presence of the causal attention mask (Brown, 2020) ensures that each token focuses only on prior context information, while the use of MLP in the output predictor ${P}^{I}_{l}$ ensures that decisions are based solely on its own features, consistent with the process used during decoding.\nIn addition to the issues mentioned above, we have the gradient flow problem in the backward propagation of training. The argmax($\\cdot$) operation we performed to obtain ${M}^{I}_{l}$ and ${M}^{OT}_{l}$ is non-differentiable and impedes end-to-end training. Thus we use the Gumbel-Softmax (Jang et al., 2016) with Straight-Through Gradient Estimator (Bengio et al., 2013) to avoid the gradient flow problem. Taking the reduction of the image token set as an example and the argmax($\\cdot$) in Eq. 5 is modified to the differentiable operation during training. The forward propagation is formulated as follows:\n${D}^{I\\dagger} = GumbelSoftmax_{\\tau}({D}^{I}, \\tau),$\n${M}^{I} = argmax_{j}({D}^{I\\dagger}),$ (8)\nwhere $\\tau$ is the temperature of the Gumbel-Softmax function. When $\\tau$ becomes smaller, ${D}^{I\\dagger}$ smoothly approaches the discrete distribution. Following the work (Lin et al., 2023), Dynamic-LLaVA employ the exponential decay for $\\tau$ from 1 to 0.1 during training. Meanwhile, in the backward propagation, we propagate the gradient flow from ${M}^{I}$ to ${D}^{I\\dagger}$:\n$\\begin{aligned}\n\\frac{\\partial \\mathbb{L}}{\\partial {D}^{I\\dagger}} = \\frac{\\partial \\mathbb{L}}{\\partial {M}^{I}} \\frac{\\partial {M}^{I}}{\\partial {D}^{I\\dagger}} \\end{aligned}$ (9)\nwhere $\\mathbb{L}$ is the objective loss function.\nFurthermore, we incorporate a constraint regularization term to constrain binary masks according to a pre-defined keep rate for token sets. This constraint ensures that the predictors retains a certain proportion of tokens, adhering to the desired sparsification strategy. It is worth noting that we found that applying sparsification to ${S}^{OT}_{l}$ on samples with short output tokens during training lead to instability training and performance degradation. To avoid this problem, we apply ${S}^{OT}_{l}$ sparsification only to samples where output text tokens exceed a pre-defined length $LEN_{OT} \\in {\\mathbb{N}}_{0}$, where ${\\mathbb{N}}_{0}$ denotes the set of non-negative integers. The constraint regularization term $\\mathbb{R}$ can be defined as:\n$\\mathbb{R} = \\sum_{l} [|\\| sum({M}^{I}) / |S^{I}| - r^{I}\\|\\|_{F} + \\begin{cases} |sum({M}^{OT})/|S^{OT}| - r^{OT}| & if |{S}^{OT}| \\geq LEN^{OT} \\\n0 & otherwise,\\end{cases}]$ (10)\nwhere $sum(\\cdot)$ is the summation operation and $|\\cdot||_{F}$ denotes the Frobenius norm. $r^{I}$ and $r^{OT}$ are the pre-defined keep rates of ${S}^{I}_{l}$ and ${S}^{OT}_{l}$, respectively. We directly add this term to the original objective loss function, with a hyper-parameter $\\lambda$ used to control its weight. The trade-off analysis of vision context ($r^{I}$) and language context ($r^{OT}$) are presented in Tab. 5, while the ablation study of the sample used for training should have a minimum output text token length ($LEN_{OT}$) and the weight of the regularization term ($\\lambda$) are shown in Tab. 6."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Datasets, Benchmarks, and Metrics. During training, we use the instruction-tuning dataset 656K Mixture Dataset, as in LLaVA-1.5 (Liu et al., 2024a). For vision understanding evaluations, we use the commonly used vision understanding benchmarks to evaluate the performance similar as LLaVA-1.5 (Liu et al., 2024a), such as VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VizWiz (Gurari et al., 2018), SciQA (Lu et al., 2022), TextVQA (Singh et al., 2019), POPE (Li"}, {"title": "et al., 2023b), MMBench (en) (Liu et al., 2023b), SEED (image) (Li et al., 2023a) and MM-Vet (Yu et al., 2023). Furthermore, we also use the vision-centric vision understanding benchmarks, such as MMVP (Tong et al., 2024b), RealWorldQA (xAI, 2024) and CVBench-2D (Tong et al., 2024a). We employ the same evaluation metrics of the above benchmarks to assess the results.", "content": "Specially, to evaluate the generation ability of MLLM before and after the generated language context sparsification. We constructed LVIS-VQA single- and multi-round Benchmarks based on the subset of LVIS-Instruct4V (Wang et al., 2023). Details about LVIS-Instruct4v and LVIS-VQA can be found in Appendix A.4.3. Meanwhile, we constructed a single-round long generation text benchmark ShareGPT4V-VQA by ShareGPT4V dataset (Chen et al., 2023b). Details are presented in Appendix A.4.4. To ensure a fair comparison, none of the images selected for our benchmarks were included in the training set of the base MLLMs and Dynamic-LLaVA. We evaluate the fluency of the generated responses of MLLMs using the Perplexity Metric (PPL) (Radford et al., 2019; Jelinek et al., 1977), and the similarity of the generated answers to the standard answers using the Metric for Evaluation of Translation with Explicit ORdering (METEOR) (Banerjee & Lavie, 2005).\nImplementations. All of the methods are trained on 8 NVIDIA A100 (80G) using Pytorch (Paszke et al., 2019). For fair comparisons, we utilize the open-source MLLMs weights available in the official GitHub repositories and continue one-epoch instruction-tuning for MLLMs and learnable predictors. The hyper-parameters of which decoder layer to sparsify the tokens (l), the sample used for training should have a minimum output text token length (LENOT) and the weight of the regularization term ($\\lambda$) are set to 2, 50 and 100 in all experiments, respectively. The details are presented in Appendix A.4.2."}, {"title": "4.2 MAIN RESULTS", "content": "Vision understanding. As demonstrated in Tab. 1, our proposed Dynamic-LLaVA framework achieves superior performance compared to SoTA vision context sparsification methods"}, {"title": "5 CONCLUSION", "content": "In this paper, we proposed a dynamic vision-language context sparsification framework, termed Dynamic-LLaVA. This framework is designed with tailored sparsification inference strategies for the inference modes of MLLMs and can be integrated into MLLMs' training in an end-to-end manner."}]}