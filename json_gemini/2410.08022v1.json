{"title": "Probabilistic Satisfaction of Temporal Logic Constraints in Reinforcement Learning via Adaptive Policy-Switching", "authors": ["Xiaoshan Lin", "Sad\u0131k Bera Y\u00fcksel", "Yasin Yaz\u0131c\u0131o\u011flu", "Derya Aksaray"], "abstract": "Constrained Reinforcement Learning (CRL) is a subset of machine learning that introduces constraints into the traditional reinforcement learning (RL) framework. Unlike conventional RL which aims solely to maximize cumulative re-wards, CRL incorporates additional constraints that represent specific mission requirements or limitations that the agent must comply with during the learning process. In this paper, we address a type of CRL problem where an agent aims to learn the optimal policy to maximize reward while ensuring a desired level of temporal logic constraint satisfaction throughout the learning process. We propose a novel framework that relies on switching between pure learning (reward maximization) and constraint satisfaction. This framework estimates the probability of constraint satisfaction based on earlier trials and properly adjusts the probability of switching between learning and constraint satisfaction policies. We theoretically validate the correctness of the proposed algorithm and demonstrate its per-formance and scalability through comprehensive simulations.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) relies on learning optimal policies through trial-and-error interactions with the environ-ment. However, many real life systems need to not only max-imize some objective function but also satisfy certain con-straints on the system's trajectory. Conventional formulations of constrained RL (e.g. [1], [2], [3]) focus on maximizing reward functions while keeping some cost function below a certain threshold. In contrast, robotic systems often require adherence to more intricate spatial-temporal constraints. For instance, a robot should \"pick up from region A and deliver to region B within a specific time window, while avoiding collisions with any object\".\nTemporal logic (TL) is a formal language that can ex-press spatial and temporal specifications. In recent years, RL subject to TL constraints has gained significant inter-est, especially in the robotics community. One common approach involves encoding constraint satisfaction into the reward function and learning a policy by maximizing the cumulative reward (e.g., [4], [5]). Another approach focuses on modifying the exploration process during RL, such as the shielded RL proposed in [6] that corrects unsafe ac-tions to satisfy Linear Temporal Logic (LTL) constraints. Similarly, [7] constructs a safe padding based on maximum likelihood estimation and Bellman update, combined with a state-adaptive reward function, to maximize the probability of satisfying LTL constraints. A model-based approach is introduced for safe exploration in deep RL by [8], which employs Gaussian process estimation and control barrier functions to ensure a high likelihood of satisfying LTL constraints. Although these approaches focus on maximizing the probability of satisfaction, they do not provide guarantees on satisfying TL constraints with a desired probability during the learning process. Moreover, [9] proposes a method based on probabilistic shield and model checking to ensure the satisfaction of LTL specifications under model uncertainty. However, this method lacks guarantees during the early stages of the learning process. Finally, [10] and [11] assume partial knowledge about the system model and leverage it to prune unsafe actions, thus ensuring the satisfaction of Bounded Temporal Logic (BTL) with a desired probabilistic guarantee throughout the learning process. However, these two methods require learning over large state-spaces which lead to scalability issues. Furthermore, [10], which is the closest work to this paper, is only applicable to a more restrictive family of BTL formulas.\nDriven by the need for a scalable solution that offers de-sired probabilistic constraint satisfaction guarantees through-out the learning process (even in the first episode of learning), we propose a novel approach that enables the RL agent to alternate between two policies during the learning process. The first policy is a stationary policy that prioritizes satisfy-ing the BTL constraint, while the other employs RL to learn a policy on the MDP that only maximizes the cumulative reward. The proposed algorithm estimates the satisfaction rate of following the first policy and adaptively updates the switching probability to balance the need for constraint satisfaction and reward maximization. We theoretically show that the proposed approach satisfies the BTL constraint with a probability greater than the desired threshold. We also validate our approach via simulations."}, {"title": "II. PRELIMINARIES: BOUNDED TEMPORAL LOGIC", "content": "Bounded temporal logics (BTL) (e.g., Bounded Linear Temporal Logic [12], Interval Temporal Logic [13], and Time Window Temporal Logic (TWTL) [14]) are expressive languages that enable users to define specifications with explicit time-bounds (e.g., \"visit region A and then region B within a desired time interval\u201d). We denote the set of positive integers by $\\mathbb{Z}_{+}$, the set of atomic propositions by AP, and the power set of a finite set $\\Sigma$ by $2^{\\Sigma}$. In this paper, we focus on BTL that can be translated into a finite-state automaton.\nDefinition 1. (Finite State Automaton) A finite state automa-ton (FSA) is a tuple $A = (Q,q_{init},2^{\\Sigma},\\delta,F)$, where\n$\\bullet$ $Q$ is a finite set of states;\n$\\bullet$ $q_{init}$ is the initial state;\n$\\bullet$ $2^{\\Sigma}$ is the input alphabet;\n$\\bullet$ $\\delta: Q \\times 2^{\\Sigma} \\rightarrow Q$ is a transition function;\n$\\bullet$ $F$ is the set of accepting states."}, {"title": "III. PROBLEM STATEMENT", "content": "We consider a labeled-Markov Decision Process (MDP) denoted as $M = (S,A,\\Delta_M,R,l)$, where $S$ represents the state space, and $A$ denotes the set of actions. The probabilistic transition function is defined as $\\Delta_M : S \\times A \\times S \\rightarrow [0, 1]$, while $R : S \\rightarrow \\mathbb{R}$ represents the reward function. Additionally, $l : S \\rightarrow 2^{AP}$ is a labeling function that maps each state to a set of atomic propositions. An example MDP is shown in Fig. 1.\nGiven a trajectory $s = s_1s_2...$ over the MDP, the output word $\\sigma = \\sigma_1\\sigma_2 ...$ is a sequence of elements from $2^{AP}$, where each element $\\sigma_i = l(s_i)$. The subword $\\sigma_i...\\sigma_j$ is denoted by $\\sigma_{i,j}$.\nDefinition 2 (Deterministic Policy). Given a labeled-MDP $M = (S,A,\\Delta_M,R,l)$, a deterministic policy is a mapping $\\pi : S \\rightarrow A$ that maps each state to a single action.\nWe address the problem of learning a policy that maxi-mizes the reward while ensuring the satisfaction of a BTL specification with a probability greater than a desired thresh-old throughout the learning process. Accordingly, while"}, {"title": "IV. PROPOSED ALGORITHM", "content": "We propose a solution to Problem 1 by introducing a switching-based algorithm that allows switching between two policies: 1) a stationary policy derived from the product of the MDP and FSA for maximizing the probability of con-straint satisfaction based on the available prior information, and 2) a policy learned over the MDP to maximize rewards. Before each episode, the RL agent determines whether to follow the stationary policy or the reward maximization policy based on a computed switching probability. The proposed approach, separating constraint satisfaction from reward maximization, eliminates the need for a time-product MDP often used in the state-of-the-art and improves the scalability of learning (as discussed in Sec. V).\nA. Policy for Constraint satisfaction\nConsider a task \u201ceventually visit A and then B\". Suppose that the agent is at C. The agent must select an action that steers it towards 1) B if A has visited before; or 2) A if A has not visited yet. Hence, the selection of actions is determined by the agent's current state and the progress of constraint satisfaction, which can be encoded by a Product MDP.\nDefinition 3 (Product MDP). Given a labeled-MDP $M = (S,A,\\Delta,R,l)$ and an FSA $A = (Q,q_{init},\\Sigma,\\delta,F)$, a product MDP is a tuple $P = M \\times A = (S_p, S_{p,init}, A, \\Delta_p, R_p, F_p)$, where\n$\\bullet$ $S_p = S \\times Q$ is a finite set of states;\n$\\bullet$ $S_{p,init} = \\{(s,\\delta(q_{init},l(s))|\\forall s \\in S\\}$ is the set of initial states, where $\\delta$ is the transition function of the FSA;\n$\\bullet$ $A$ is the set of actions;\n$\\bullet$ $\\Delta_p : S_p \\times A \\times S_p \\rightarrow [0,1]$ is the probabilistic transition relation such that for any two states, $p = (s,q) \\in S_p$ and"}, {"title": "V. SIMULATION RESULTS", "content": "We present some case studies to validate the proposed algorithm and compare it with [10]. The simulation results are implemented on Python 3.10 on a PC with an Intel i7-10700K CPU at 3.80 GHz processor and 32.0 GB RAM.\nWe consider a robot operating on an 8x8 grid. The robot's action set is A = {N,NE,E,SE,S,SW,W,NW,Stay}, and the possible transitions under each action are shown in Fig. 3. Action \"Stay\" results in staying at the current position with probability 1. Any other action leads to the intended transition (blue) with a probability of 90% and unintended transitions (yellow) with 10%. This transition model is unknown to the robot. Instead, a conservative transition uncertainty $\\varepsilon \\ge 0.1$ is available ($\\varepsilon = 0.1$ is the actual transition uncertainty).\nWe consider a scenario where the robot periodically per-forms a pickup and delivery task while monitoring high reward regions in the environment. In Fig. 4, the light gray, dark gray, and all other cells yield a reward of 1, 10, and 0, respectively. The pickup and delivery task is formalized using a TWTL formula: $[H^1_P]^{[0,20]} . ([H^1_{D_1}]^{[0,20]} \\vee [H^1_{D_2}]^{[0,20]}) .[H^1_{Base}]^{[0,20]}$, which specifies that the robot must \u201creach the pickup location P and stay there for 1 time step within the first 20 time steps, then immediately reach one of the delivery locations, $D_1$ or $D_2$, and stay there for 1 time step within the next 20 time steps; afterward, return to the Base and stay for 1 time step, within 20 steps.\" Based on the time bound of the formula, each episode's length is set at 62 time steps.\nCase 1. We illustrate sample trajectories using the policy learned by the algorithm in [10] and the proposed algorithm (with tabular-Q learning) after training for 40,000 episodes. The results are shown in Fig. 4. Both [10] and our proposed algorithm not only satisfy the TWTL constraint with a desired probability but also effectively explore the high-reward regions. The proposed algorithm switches between two different behaviors based on the selected mode while [10] finds a single behavior that satisfies the constraint and maximizes the reward.\nIn Cases 2 and 3, we consider a fixed number of episodes ($N_{episode} = 1000$) and diminishing $\\epsilon$-greedy policy in RL algorithms (with $\\epsilon_{init} = 0.7$ and $\\epsilon_{final} = 0.0001$). In this way, we compare the performances of both algorithms under fixed learning episodes and exploration/exploitation behavior. The learning rate and discount factor are set to 0.1 and 0.95, respectively. We set the $z$ score to 2.58 to ensure the probabilistic constraint satisfaction with high confidence.\nCase 2. We tested both the algorithm in [10] and the proposed algorithm (with tabular-Q learning) under varying $P_{rdes}$. The results are presented in Fig. 5, where each algo-rithm was run through 10 independent training sessions. For each run, the rewards and satisfaction rates were smoothed using a moving window average. The solid lines represent the average reward and satisfaction rate at each episode, calculated as the mean of the moving window averages from all 10 runs. The upper and lower bounds of the shaded areas indicate the maximum and minimum moving window averages over the 10 runs at each episode. In all scenarios, the proposed algorithm consistently surpasses the benchmark in terms of maximizing the cumulative reward regardless of the selected RL algorithm under 1000 episodes. Moreover, the benchmark tends to be over-cautious and enforces a higher satisfaction rate in all cases, while our proposed algorithm effectively balances constraint satisfaction with reward maximization, with the satisfaction rate adaptively aligned with the desired threshold. As $P_{rdes}$ increases, we notice a decrease in the collected rewards in both algorithms, due to a more restrictive constraint.\nCase 3. This case study investigates the impact of the parameter $\\varepsilon$ on the performance of both algorithms. As in Fig. 6, we observe that $\\varepsilon$ has a minimal impact on the proposed algorithm. However, the benchmark's performance is significantly affected by $\\varepsilon$; a higher $\\varepsilon$ leads to reduced reward collection and an increased satisfaction rate. This difference arises because the benchmark uses $\\varepsilon$ to compute the lower bound of satisfaction and prune actions accord-ingly, and thus a larger $\\varepsilon$ will result in a more restricted action set. On the other hand, the proposed algorithm does not incorporate $\\varepsilon$ in policy derivation, thereby maintaining a consistent performance.\nCase 4. We compare the closed-form solution (5) with the recursive one (Alg. 2) in terms of their ability to evaluate the lower bound and their computation efficiency. In Table I, we present the computed lower bounds for some selected product MDP state $p$ at a time step $k = 17$ for a TWTL task $[H^1_P]^{[0,8]} . [H^1_{D_1}]^{[0,8]}$ (\u201cvisiting $P$ within 8 time steps and holding 1 time step at P, after which visiting $D_1$ within 8 time steps and holding 1 time step at $D_1$\u201d). The results indicate that the recursive solution consistently generates higher (less conservative) lower bounds than the closed-form solution.\nIn Table II, we analyze the computation time for the closed-form and recursive solutions under various TWTL tasks. Case 4a: We consider a TWTL task of the form $[H^1_{P_1}]^{[0,l_1]} . [H^1_{P_2}]^{[0,l_2]} ....$ By incrementally adding subtasks and adjusting their durations, while keeping the total task duration $T$ constant, we increase the number of states in the product MDP and, consequently, the count of state-time pairs ($p$,$k$). The results in Table II (Case 4a) reveal that the computation time of the recursive algorithm increases with the number of ($p$,$k$) pairs while the closed-form solu-tion is not affected. This result aligns with the expectation that the recursive algorithm's computational load increases due to the iterative solving of optimization problems for each ($p$,$k$) pair. Case 4b: We consider a TWTL task of the form $[H^1_P]^{[0,l_P]} . ([H^1_{D_1}]^{[0,l_{D_1}]}|[H^1_{D_2}]^{[0,l_{D_2}]}) . [H^1_{Base}]^{[0,l_{Base}]}]$. While maintaining a fixed number of subtasks, we vary the duration of each subtask to alternate the total task duration $T$. As shown in Table II, the computation time for the closed-form solution rises significantly with the increase in task duration $T$, thus the closed-form solution is more computa-tionally efficient than the recurseive solution as expected."}, {"title": "VI. CONCLUSION", "content": "We proposed a switching-based algorithm for learning policies to optimize a reward function while ensuring the"}]}