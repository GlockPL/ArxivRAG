{"title": "Measuring Pre-training Data Quality without Labels\nfor Time Series Foundation Models", "authors": ["Songkang Wen", "Vasilii Feofanov", "Jianfeng Zhang"], "abstract": "Recently, there has been a growing interest in time series foundation models that\ngeneralize across different downstream tasks. A key to strong foundation models is\na diverse pre-training dataset, which is particularly challenging to collect for time\nseries classification. In this work, we explore the performance of a contrastive-\nlearning-based foundation model as a function of the data used for pre-training.\nWe introduce contrastive accuracy, a new measure to evaluate the quality of the\nrepresentation space learned by the foundation model. Our experiments reveal the\npositive correlation between the proposed measure and the accuracy of the model\non a collection of downstream tasks. This suggests that the contrastive accuracy\ncan serve as a criterion to search for time series datasets that can enhance the\npre-training and improve thereby the foundation model's generalization.", "sections": [{"title": "1 Introduction", "content": "Nowadays, the success of large foundation models (Bommasani et al., 2021) such as GPT-4 (Achiam\net al., 2023) or Llama (Touvron et al., 2023) cause dramatic changes in the research and applications.\nInstead of training a tailored model for a particular task, the foundation model is pre-trained on a\ncollection of datasets with the goal to generalize simultaneously across various downstream tasks\neither by fine-tuning the model or directly using its output. This workflow effectively simplifies the\nchoice of the model architecture while reducing the requirement for amount of labeled data. The\nwave of foundation models has now reached the time series domain, including forecasting (Rasul\net al., 2023a; Woo et al., 2024) and classification models (Lin et al., 2024).\nWhen it comes to real deployment of a time series foundation model (TSFM), a very important\nquestion is whether the pre-training dataset is sufficiently diverse, so the model generalizes well to\nnew downstream tasks. Usually, this is verified by directly evaluating the performance on several\nsupervised downstream tasks thereby requiring availability of annotated data samples and introducing\na high cost for assesing the quality of different pre-trained datasets. Therefore, in this paper, we ask\nthe following research question:\nCan we evaluate the quality of pre-training data in an unsupervised manner?\nBy focusing on time series classification and contrastive pre-training, we show that it is possible to\nleverage important information from the representation space learned by the TSFM. More precisely,\nif new data points are not similar to pre-training data, their embeddings will tend to not satisfy the\nuniformity property of contrastive learning (Wang and Isola, 2020), directly impacting the foundation\nmodel's performance. Based on this observation, we introduce a new metric called contrastive\naccuracy that evaluates how well spread the data points are in the embedding space. We empirically\nshow that the proposed metric positively correlates with the model's accuracy on the tasks unseen\nduring pre-training, allowing us to use it for measuring the quality of pre-training examples without\nthe need to regularly test the TSFM on supervised downstream tasks."}, {"title": "2 Related Work", "content": "Time series representation learning has got a high attention in recent years, resulting in a wealth\nof excellent works. Many of them use the contrastive learning scheme for pre-training, including\nTS2Vec (Yue et al., 2022) and TS-TCC (Eldele et al., 2021) with a CNN backbone and TF-C (Zhang\net al., 2022) with a ResNet backbone, demonstrating good performance on classification datasets, e.g,\nthe UCR collection (Dau et al., 2019).\nWith the success of large language models (LLMs), popularity of the transformer architecture for time\nseries analysis increases (Nie et al., 2023; Ilbert et al., 2024), while the development of foundation\nmodels is becoming a prevalent task. Numerous TSFMs have been proposed recently, including\nLag-Llama (Rasul et al., 2023b), Time-LLM (Jin et al., 2023), One Fits All (Zhou et al., 2023),\nBrant (Zhang et al., 2023), MOIRAI (Woo et al., 2024). While some of them try to adapt LLM for\ntime series data, others train foundation models from scratch on a large volume of time series data. In\nour work, we are based on the latter approach, studying how to evaluate the quality of pre-training\ndata, which, to our knowledge, has not been addressed before.\nFinally, we would like to notice that our framework reminds unlabeled performance estimation\n(Donmez et al., 2010) where the goal is to predict model's performance on unlabeled test data.\nHowever, most of these approaches focus on single-task classification for computer vision problems\n(Hendrycks and Gimpel, 2016; Yu et al., 2022; Xie et al., 2024), implying a fundamentally different\nmethodology. To the best of our knowledge, time series foundation models have never been considered\nin this domain before."}, {"title": "3 Proposed Method", "content": "First, in Section 3.1, we introduce the setup we are working in. Then, we present briefly the\narchitecture of our foundation model and how it was pre-trained in Section A, respectively. Finally,\nin Section 3.3, we propose a new evaluation metric called the contrastive accuracy."}, {"title": "3.1 Problem Setup", "content": "We consider the task of unsupervised pre-training where an unlabeled pre-training set $X_o$ is given\nwith $m$ pre-training sequences of length $t$. The goal is to design a foundation model $F : \\mathbb{R}^t \\rightarrow \\mathbb{R}^{d_{hid}}$\nthat projects any time series $x \\in \\mathbb{R}^t$ to a discriminative hidden space $\\mathbb{R}^{d_{hid}}$. In this work, the quality\nof the foundation model is evaluated on a collection of downstream tasks $D = \\{D_i\\}_{i=1}^I$, where $D_i$\nconsists of observations $X_i$ and labels $Y_i$. For each downstream task, we perform a train-test split,\nappend $F$ with a linear head, fine-tune it on the training set and compute the accuracy score on the test\nset. The performance of $F$ with a pre-training dataset $X_o$ that is averaged over all downstream tasks\nis denoted by $P_{test}(X_0)$. Similarly, $P_{train}(X_0)$ denotes the average performance when it is evaluated\non the training set."}, {"title": "3.2 Architecture and Pre-training", "content": "Similarly to Nie et al. (2023) and Lin et al. (2024), we use the ViT (Dosovitskiy et al., 2021) as the\nbackbone, but our implementation slightly differs from theirs. Instead of dividing a sequence into\ndisjoint patches, we employ a single CNN layer allowing them to be overlapped. Similarly to other\nTSFMs, we reshape any time series to a fixed sequence length equal to 512. More details on the\narchitecture are given in Appendix A.\nFor pre-training of the foundation model, we use the contrastive learning that aims to train a such\nencoder that outputs similar representations for two random augmentations of the same sample\n(positive pair) and dissimilar representations for augmentations of two different samples (negative\npair). More formally, let $\\mathcal{T}$ be a considered space of transformations (augmentations) such that\n$\\forall\\varphi \\in \\mathcal{T}, x \\in \\mathcal{X}$ we have $f(x) \\in \\mathcal{X}$. In our experiments, we have considered the RandomCropResize\nwith a crop length varying from 70% to 80%. To measure similarity of two embeddings, we first\nproject the output of the foundation model $F(x)$ to a lower dimension using a MLP projector"}, {"title": "3.3 Contrastive Accuracy", "content": "$\\mathfrak{g} : \\mathbb{R}^{d_{hid}} \\rightarrow \\mathbb{R}^{d_{hid}}$ and then compute the cosine similarity between the two vectors defined as follows:\n$$\\mathcal{S}_{cos} (q, k) := \\frac{q \\cdot k}{||q||||k||}, \\forall (q, k) \\in \\mathbb{R}^{2d_{hid}}.$$\nGiven a batch $\\mathcal{B} = \\{x_i\\}_{i=1}^b$, for each example $x_i$, we sample two augmentation functions $\\varphi$ and $\\psi$\nuniformly from $\\mathcal{T}$, i.e., $\\varphi, \\psi \\sim U(\\mathcal{T})$, compute the pairwise similarities between all the examples in\nthe following way:\n$$\\mathcal{S}_i(\\varphi, \\psi) = [\\mathcal{S}_{cos} (\\mathfrak{g} \\circ F \\circ \\varphi(x_i), \\mathfrak{g} \\circ F \\circ \\psi(x_j))]^b_{j=1} \\in \\mathbb{R}^b.$$\nFollowing Oord et al. (2018) as well as He et al. (2020) and denoting the cross-entropy error function\nby $\\ell_{ce}: \\mathbb{R}^b \\times \\{1, ..., b\\} \\rightarrow \\mathbb{R}$, we update the weights of $F$ and $\\mathfrak{g}$ by minimizing the contrastive loss\nb\ndefined by $\\sum_{i=1}^b \\ell_{ce} (\\frac{\\mathcal{S}_i(\\varphi, \\psi)}{\\tau}, i)$, where $\\tau \\in (0, +\\infty)$ is a temperature.\nIt has been shown that a good representation learned by contrastive learning should satisfy uniformity\nproperty, i.e., to have a feature distribution that tends to be uniform on the unit hypersphere in order\nto preserve maximal information (Wang and Isola, 2020). Based on this observation, we introduce\nthe contrastive accuracy metric (CA, denoted by $A_{con}$), that measures how scattered the embeddings\nof the evaluation examples $\\mathcal{X}' = \\{x'_i\\}_{i=1}^n$ are in the representation space. Specifically, we count\nhow many examples have two embeddings (obtained from the two augmentations) to be the nearest\nneighbors with respect to the other examples in the dataset:\n$$A_{con}^{\\mathcal{X}'}(X_o) := \\mathbb{E}_{\\varphi \\sim U(\\mathcal{T})} \\mathbb{E}_{\\psi \\sim U(\\mathcal{T})} \\bigg[ \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I} \\bigg[ \\underset{j = \\{1, ..., n\\}}{argmax} [\\mathcal{S}_i(\\varphi, \\psi)]_j = i \\bigg] \\bigg] ,$$\nwhere $X_o$ denotes the pre-training dataset on which the foundation model $F$ was trained. When the\ndataset size $n$ is large, we split the data into disjoint batches and evaluate $\\mathcal{S}_i(\\varphi, \\psi)$ only on those\nexamples that belong to the same batch as $i$. Further, we will experimentally show that the contrastive\naccuracy is able to hint what generalization performance on the downstream tasks we can expect\nfrom the foundation model pre-trained on $X_0$."}, {"title": "4 Experiments", "content": "In our experiments, we use the UCR (Dau et al., 2019) collection to demonstrate the benefit of the\nmethodology proposed in Section 3. To minimize the effect of randomness, we run each experiment\n5 times and report the averaged values. All experiments were performed on a single NVIDIA Tesla\nV100-32GB GPU card."}, {"title": "4.1 Correlation with Performance", "content": "In our first experiment, we show that the contrastive accuracy is able to correlate with the general-\nization performance of the pre-training model. Given a pre-training data $X_o$, we randomly draw a\nsubsample $X_o^{(r\\%)}$, which contains $r\\%$ examples of $X_o$, then evaluate $A_{con}^{X_o^{(r\\%)}}(X_o^{(r\\%)})$ and compare it\nwith the train and the test performance $P_{train} (X_o^{(r\\%)})$ and $P_{test} (X_o^{(r\\%)})$. As $X_o$, we have picked one\nof the four largest datasets in the UCR (ElectricDevices, Crop, FordB, and FordA), and evaluate the\nperformance on the rest 127 datasets in average. Figure 1 illustrates the results for ElectricDevices\nwhen varying $r \\in [10\\%, 100\\%]$, and the other results can be found in Appendix B. We can observe\nthat the contrastive accuracy correlates well with the performance, approximating its growth with\nincreasing subsampling ratio. This allows us to perform model selection without directly testing\nthe pre-training model on the downstream tasks. For example, this experiment may help to identify\nsufficient number of pre-training examples per dataset when the model is trained on a collection of\ndifferent pre-training datasets."}, {"title": "4.2 Improvement Prediction", "content": "In this experiment, in addition to the pre-training dataset $X_0$, we consider having another one $X_{new}$\nand ask whether it is possible to predict the performance improvement from including $X_{new}$ to the\npre-training data, i.e., $\\Delta P(X_0, X_{new}) := P(X_0 \\cup X_{new}) - P(X_0)$. For this, we propose to consider\n$\\Delta A_{con}(X_0, X_{new}) := A_{con}^{X_{new}}(X_0 \\cup X_{new}) - A_{con}^{X_0}(X_0)$ and measure its correlation with $\\Delta P(X_0, X_{new})$.\nFor this experiment, we split the UCR collection into two disjoint sets denoted by $\\mathcal{C} = \\{X_i\\}_{i=1}^{121}$ and\n$\\mathcal{D} = \\{X_i\\}_{i=122}^{129}$, where the latter is used to evaluate the performance as described in Section 3.1 (more\ndetails are given in Appendix B). In the first experiment, we fix the initial pre-training dataset $X_o$ by\ntaking one from $\\mathcal{C}$ and vary $X_{new}$ within $\\mathcal{C} \\setminus \\{X_i\\}$. In the second experiment, we fix $X_{new}$ and vary $X_o$\nin the similar way. For each pair $(X_0, X_{new})$, we compute $\\Delta P(X_0, X_{new})$ and $\\Delta A_{con} (X_0, X_{new})$ and\nplot the results for the two experiments in Figure 2 for AllGestureWiimoteX dataset and in Appendix\nB for the rest 11 datasets. In Figure 2, we can observe a positive correlation between the difference in\ncontrastive accuracy and the performance improvement suggesting that the proposed unsupervised\ncriterion can help to search data that can be included to the pre-training dataset in order to improve\nthe representaion and thereby the generalization performance of the foundation model."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we studied the task of evaluating the effect of pre-training data on the foundation\nmodel's performance. We proposed the contrastive accuracy and experimentally showed its promise\nas a criterion to select pre-training data. As a future work, we would like to test our approach with\nlarger pre-training datasets and explore the limits of contrastive pre-training for time series data.\nParticularly, unlike in computer vision, there is still an open question regarding which augmentation\ntechniques are relevant for contrastive learning in time series data and what their impact is."}, {"title": "A Architecture and Implementation Details", "content": "In this paper, similarly to Nie et al. (2023) and Lin et al. (2024), we employ the ViT (Vision\nTransformer) as the backbone. Our goal is to retain information effectively, so we simultaneously\nuse both overlapping and non-overlapping patches. Here is how we achieve it: we apply a one-layer\n1D-CNN to handle the overlapping patches and use mean pooling to transform their embeddings\nto match the number of non-overlapping patches. Next, we embed the $\\mu$ and $\\sigma$ values from the\nnon-overlapping patches and concatenate the output with the overlapping patches. This result together\nwith the class (CLS) token is then fed into the transformer. The entire network framework is depicted\nin Figure 3.\nThroughout all our experiments, we maintain the same model parameters. The chosen parameter\nvalues for different layers are outlined in Table 1. To better train the model, we apply a linear learning\nrate warm-up in the first 10 epochs (Loshchilov and Hutter, 2016) and a cosine learning rate decay in\nthe subsequent epochs."}, {"title": "B Experiments", "content": "In Figure 4, we display the complete results for the experiment presented in Section 4.1.\nNext, in Figure 5 and 6, we give the complete results of the experiment described in Section 4.2.\nFor the pre-training collection $\\mathcal{C}$, we chose the following 12 UCR datasets: AllGestureWiimo-\nteX, CricketY, EOGVerticalSignal, Haptics, MelbournePedestrian, PLAID, Phoneme, ScreenType,\nUWaveGestureLibraryX, WordSynonyms, WormsTwoClass, Yoga."}]}