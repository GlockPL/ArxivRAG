{"title": "FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression", "authors": ["Zhenheng Tang", "Xueze Kang\u2260", "Yiming Yin\u2260", "Xinglin Pan", "Yuxin Wang#", "Xin He*", "Qiang Wang", "Rongfei Zeng", "Kaiyong Zhao", "Shaohuai Shi\u00a7", "Amelie Chi Zhou#", "Bo Li\u2020", "Bingsheng He\u00b9", "Xiaowen Chu\u2260"], "abstract": "To alleviate hardware scarcity in training large deep neu-\nral networks (DNNs), particularly large language models\n(LLMs), we present FusionLLM, a decentralized training sys-\ntem designed and implemented for training DNNs using\ngeo-distributed GPUs across different computing clusters or\nindividual devices. Decentralized training faces significant\nchallenges regarding system design and efficiency, including:\n1) the need for remote automatic differentiation (RAD), 2)\nsupport for flexible model definitions and heterogeneous soft-\nware, 3) heterogeneous hardware leading to low resource\nutilization or the straggler problem, and 4) slow network\ncommunication. To address these challenges, in the system\ndesign, we represent the model as a directed acyclic graph of\noperators (OP-DAG). Each node in the DAG represents the\noperator (or say layers) in the DNNs, while the edge repre-\nsents the data dependency between operators. Based on this\ndesign, 1) users are allowed to customize any DNN without\ncaring low-level operator implementation; 2) we enable the\ntask scheduling with the more fine-grained sub-tasks, offer-\ning more optimization space; 3) a DAG runtime executor can\nimplement RAD withour requiring the consistent low-level\nML framework versions.\nTo enhance system efficiency, we implement a workload\nestimator for each operator and design an OP-Fence sched-\nuler to cluster devices with similar bandwidths together and\npartition the DAG to increase throughput. Additionally, we\npropose an AdaTopK compressor to adaptively compress\nintermediate activations and gradients at the slowest com-\nmunication links. To evaluate the convergence and efficiency\nof our system and algorithms, we train ResNet-101 and GPT-\n2 on three real-world testbeds using 48 GPUs connected with\n8 Mbps ~ 10 Gbps networks. Experimental results demon-\nstrate that our system and method can achieve 1.45 - 9.39\n\u00d7 speedup compared to baseline methods while ensuring\nconvergence.", "sections": [{"title": "1 Introduction", "content": "The evolution of machine learning (ML) has given rise to\nlarge language models (LLMs) with vast parameters, such\nas GPT-3 [50, 55, 71], PaLM [11], and DALL-E2 [57], which\ncan mimic diverse linguistic styles and facilitate advanced\nhuman-like interactions, symbolizing a significant break-\nthrough in artificial intelligence [41, 91]. The burgeoning\nparameters and datasets in LLMs necessitate substantial GPU\nmemory and computational power, exceeding the advance-\nments in hardware development. Pre-training a GPT-3 of\n175B parameters requires H100 to run at least 13.17 years or\nRTX 4090 with 60.28 years [5, 25].\nThe widening gap between hardware development and\nthe rapid evolution of LLMs presents significant challenges\nto researchers and engineers without substantial hardware\nresources. Concurrently, data privacy has emerged as an\nincreasingly prominent concern in recent years [16, 25, 45,\n66, 68]. Collecting user data with large companies to train or\nacquire LLM services can expose user privacy. Such hardware\nscarcity and privacy concerns motivate us to think about a\nquestion: Is it possible to collaborate with GPUs from multiple\ngeo-distributed persons or parties to train an LLM?"}, {"title": "2 Background and Motivation", "content": ""}, {"title": "2.1 LLM Training", "content": "A typical loop of training Deep neural networks (DNNs) con-\nsists of loading and passing inputs layer by layer to generate\noutputs. A loss function is defined to measure different be-\ntween outputs and actual data labels. Then, the autograd is\nconducted to automatically obtain gradients on parameters\n(weights) of DNNs. At last, parameters are adjusted based\non gradients with some optimizers like Stochastic Gradient\nDescent (SGD).\nThe training of contemporary LLMs imposes immense\ncomputational and memory requirements. For instance, Ope-\nAI's GPT-3 [50, 55, 71], a model that has set milestones in\nnatural language understanding and generation, consists of\na staggering 175 billion parameters. This sheer size necessi-\ntates not just vast computational power, but also immense\nmemory capacity to store intermediate activations, weights,\nand gradients during training. We compare different GPUs\nin training a GPT-3 with 175B parameters in Table 1, which\nshows that pre-training GPT-3 requires H100 to run at least\n13.17 years or RTX 4090 with 60.28 years. This trajectory un-\nderscores the pressing need to explore innovative methods\nto efficiently and feasibly train such behemoths."}, {"title": "2.2 Distributed Training", "content": "Traditional training and inference of DNNs are distributed\nwithin a single organization via high-speed local area net-\nwork (LAN). In Data parallelism (DP), input data is di-\nvided into subsets and processed on duplicated models across\ndifferent machines, with the new gradients or parameters\nfrom these models being aggregated to update the origi-\nnal model [61, 61, 63, 69]. However, the scalability of DP\nis limited by the inefficiency of large-batch SGD [44], high\ncommunication costs of the whole model size [61, 62, 67],\nand the inability to load an LLM onto a single GPU.\nure 1 shows two different model parallel schemes, which\nare widely used when a DNN is too large to be loaded in\na single GPU. Pipeline parallelism (PP) dissects model\nlayers into multiple stages and executes them sequentially\non different devices, communicating intermediate results\nbetween stages [28]. Improving the pipelines between execu-\ntion and communication can reduce bubble time and improve\noverall efficiency [47]. However, the layer-wise dependency\nof forward and backward processes limits the scalability of\nPP [52, 67]. Tensor parallelism (TP) vertically splits model\nstages across multiple devices, with each group aggregating\ncomputing results and sending them to the other group for\nthe next model stage. TP is suitable for high homogeneous\ncommunication-bandwidth environments [48], because its\nmore fine-grained computation and communication requires\nperfect overlapping, and introduces higher frequency of com-\nmunication."}, {"title": "2.3 Motivation, Challenges and Opportunities", "content": "Motivation: aggregating GPUs from multiple individu-\nals or parties. Table 1 shows that consumer-level GPUs like\nRTX 4090 might have a much higher GPU days/price ratio\nthan data-center GPUs like H100. There exists an untapped\nreservoir of computational power in the form of decentral-\nized GPUs, distributed across various locations and devices.\nThese GPUs, often underutilized, can be harnessed collec-\ntively to collaboratively train large models. GPU providers\nmay be motivated to contribute their GPUs for several rea-\nsons, ranging from financial incentives to sharing usage\nrights of LLM and environmental considerations. By decen-\ntralizing the training process and capitalizing on these dis-\npersed resources, we not only democratize access to high-end\nmodel training but also potentially reduce costs. Instead of\ninvesting heavily in dedicated clusters, leveraging the collec-\ntive power of decentralized GPUs could offer a more econom-\nical and scalable solution for training the next generation of\nAl models.\nChallenge 1: supporting remote automatic differen-\ntiation and the general system design. Current ML frame-\nworks lack supports for automatic differentiation across\nInternet networks, hindering seamless remote computational\ngraph processing and gradient calculations. Additionally,\nthe heterogeneity in software environments among real-\nworld participants complicates matters further; variations\nin Cuda versions and ML frameworks across different nodes\nmake it tedious to synchronize software environments, as\na crucial aspect often neglected in existing decentralized\ntraining works [58, 66, 67, 75, 85].\nChallenge 2: heterogeneous hardware performance.\nA pivotal challenge distinct from traditional data center is\nthe inherently heterogeneous hardware landscape of decen-\ntralized systems. In such an environment, CompNodes con-\ntribute diverse hardware configurations that span a spec-\ntrum of GPU and CPU architectures, memory capacities,\nnetwork bandwidths, and overall computational power. This\nheterogeneity manifests in variable task completion times\nacross devices. This variability and the strong dependencies\nduring forward and backward propagation of sub-models,\nresult in the \"straggler problem\"-wherein certain devices\nlag behind, causing extended waiting periods and potentially\nthrottling the collective training throughput. Addressing this\nrequires meticulous model partitioning strategies, ensuring\nsub-models are judiciously allocated to devices in a man-\nner that mitigates the straggler effect and optimizes overall\nsystem performance.\nChallenge 3: low network bandwidth. Decentralized\ntraining of models across geographically dispersed devices\nnecessitates communication over the Internet [61, 66]. How-\never, one of the inherent challenges in such a setup is the\noften low network bandwidth typical of many Internet con-\nnections. This limited bandwidth can dramatically escalate\nthe communication time, becoming particularly problematic\nwhen considering the voluminous data exchanges inherent\nto training, such as weight updates or gradient sharing. Con-\nsequently, the system's throughput, quantified as processed\nsamples per second, can be critically bottlenecked by this\nprotracted communication duration. The prominence of this\nchallenge underscores the imperative need for innovative\nstrategies to mitigate its impact. One promising avenue is the\ncompression of communicated data-whether it be interme-\ndiate features or gradient values-thereby reducing the data\npayload and aiming to alleviate the prolonged transmission\ntimes that can hamstring decentralized training endeavors.\nOpportunity 1: heterogeneous inter-layer partition.\nAmidst the challenges posed by diverse and heterogeneous\nhardware configurations in decentralized training, heteroge-\nneous inter-layer DNN partitioning emerges as a promising\nsolution. First and foremost, it enables adaptive load bal-\nancing, ensuring that devices, regardless of their varying\ncomputational capacities and architecture specifics, receive\nworkload allocations commensurate with their capabilities.\nThis tailored allocation optimizes device utilization, ensur-\ning that no single device becomes a bottleneck due to over-\nburdening or underutilization. Additionally, heterogeneous\nDNN partitioning offers a strategic advantage over intra-\nlayer partitioning by curtailing frequent communication as\nshown in Figure 1. In the context of an Internet environment,\ncharacterized by higher latency per message, intra-layer par-\ntitioning can exacerbate communication costs, making the\ntraining process inefficient and cost-prohibitive. By adopting\ninter-layer partitioning, we can judiciously minimize these\ncommunication touchpoints, thereby streamlining data ex-\nchanges and reducing the latency-induced overheads that\ncan stymie decentralized training performance.\nOpportunity 2: sparsification property of deep learn-\ning. Deep learning models exhibit intrinsic properties of\nsparsification that hold significant implications for both com-\nputational efficiency and model robustness. To begin with,\nthese over-parameterized DNNs possess an abundance of\nparameters, many of which can be pruned or quantized into\nfewer bits without compromising model accuracy [14, 19, 27,\n39, 65, 88]. This property is extensively harnessed for dual\npurposes: communication compression [3, 39, 40, 61, 79],\nwhere it aids in reducing data transfer volumes, and the ac-\nceleration of inference, where it optimizes the real-time per-\nformance of deployed models. On another front, the concept\nof dropping out intermediate activations has been widely\nadopted as a regularization strategy, staving off the detri-\nmental effects of over-fitting [64]. Such inherent sparsity in\nactivations and, by extension, gradients, offers an inspiring\ninsight: the potential to further sparsify these intermediate\nvalues. This becomes especially crucial in decentralized se-\ntups with constrained network bandwidth, where sparsified\nactivations and gradients can substantially curtail commu-\nnication time, making the training process more agile and\nresilient to slow network conditions."}, {"title": "3 FusionLLM Overview", "content": ""}, {"title": "3.1 Design Goals", "content": "General design to support remote automatic differen-\ntiation for different ML softwares. In the fast-evolving\nlandscape of ML, the dynamism and diversity of ML frame-\nworks have become a hallmark of the field [1, 53]. A system's\ngenerality to support the ML framework with different ver-\nsions to lesser-known but equally significant ones can help\nCompNode flexibly and swiftly join the computing. By im-\nplementing RAD, users do not need to care for the systematic\ndetails and focous on the ML aspects. This general design en-\nsures that FusionLLM remains agnostic to the ever-changing\npreferences in the ML community, offering broader appeal\nand utility.\nCompatible design to support convenient customized\nmodel architectures. The world of ML is replete with cus-\ntomized models tailored to address specific tasks or chal-\nlenges. A system's flexibility in accommodating these cus-\ntomized models, irrespective of their unique architectures\nor requirements, ensures that it caters to a vast audience\nranging from industry professionals to academic researchers.\nIn essence, a general design underscores a commitment to\ninclusivity and future-readiness, placing the system at the\nforefront of innovation and ensuring its relevance in the\never-progressing deep learning epoch."}, {"title": "3.2 System Overview", "content": "The system design of FusionLLM is shown in Figure 2. We\nabstract the definitions of models, datasets, forward func-\ntions, loss functions, optimization methods, and training\nalgorithms into the intermediate representation (IR) plane.\nUsers only need to provide the definitions to a broker in\nthe IR plane, without need to care about the executing de-\ntails and scheduling in decentralized training. The broker\nis designed as a coordinator to receive job definitions, then\nbuilds the FP and BP DAG (in \u00a7 3.3) of the whole ML models.\nEach layer (operator) is abstracted as an OP node in DAG.\nThe input, output, and gradients of each layer are saved in a\nunified data structure (in \u00a7 3.4) to be communicated between\nOP nodes and devices.\nWith the DAG, model definitions and parallel training\ninformation, we design a scheduling algorithm to automat-\nically partition and allocate different parts of the model as\nsub-DAGs onto different computing devices.\nBased on the estimation of computing, communication\nand hardware performance (in \u00a7 3.5), the broker estimates\nsystem workloads and throughput (in \u00a7 3.6) and solves the\noptimization problem to (in \u00a7 4). Then, the configurations of\npartitioned sub-DAGs are sent to allocated computing nodes\n(CompNode). Each CompNode builds sub-models based on\nsub-DAGs, and executes FP, BP, and communication oper-\nations. For the slowest links indicated by the broker, data\ncommunication will be compressed to reduce communica-\ntion time (in \u00a7 5).\nWhen executing training or inference, we abstract the\ncomputing of operators in the DAG, message passing and\ncommunication between operators and computing nodes\ninto the execution plane. The execution plane is responsible\nfor designing and implementing general interfaces to adapt\ndifferent ML Engines to execute each operator defined DAGs\nand communicate data. Thus, the CompNodes can utilize het-\nerogeneous devices and different ML frameworks according\nto their preference. And there is a DAG execution engine that\nis responsible for loading and feeding data between different\noperators."}, {"title": "3.3 DAG Partition", "content": "The whole training process can be divided into several pro-\ncedures: FP"}, {}, {"title": "3.4 OP Data Structure", "content": "We design a uniform data structure that standardizes mes-\nsage exchanges between diverse operators and nodes", "attributes": "nName identifies the originating operation (OP) nodes that\ngenerate the data, providing traceability and aiding in de-"}]}