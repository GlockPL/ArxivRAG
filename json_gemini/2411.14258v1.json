{"title": "Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective", "authors": ["Ernests Lavrinovics", "Russa Biswas", "Johannes Bjerva", "Katja Hose"], "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) based applications including automated text generation, question answering, chatbots, and others. However, they face a significant challenge: hallucinations, where models produce plausible-sounding but factually incorrect responses. This undermines trust and limits the applicability of LLMs in different domains. Knowledge Graphs (KGs), on the other hand, provide a structured collection of interconnected facts represented as entities (nodes) and their relationships (edges). In recent research, KGs have been leveraged to provide context that can fill gaps in an LLM's understanding of certain topics offering a promising approach to mitigate hallucinations in LLMs, enhancing their reliability and accuracy while benefiting from their wide applicability. Nonetheless, it is still a very active area of research with various unresolved open problems. In this paper, we discuss these open challenges covering state-of-the-art datasets and benchmarks as well as methods for knowledge integration and evaluating hallucinations. In our discussion, we consider the current use of KGs in LLM systems and identify future directions within each of these challenges.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are anticipated to have a substantial impact on domains such as law, cyber security, education, and healthcare due to their ability to generalize well on language technology tasks, such as text summarization, question-answering (QA), and others (Augenstein et al., 2024). A major flaw that prevents widespread deployment of LLMs is factual inconsistencies, also referred to as hallucinations, which impair trust in Al systems and even pose societal risks in the form of generating convincing false information (Augenstein et al., 2024, Puccetti et al., 2024).\nHallucinations are a multifaceted problem as there are conceptually different types such as hallucinations with respect to world knowledge, self-contradictions, with respect to prompt instructions or given context (Huang et al., 2023, Zhang et al., 2023), see Figure 1. While Perkovi\u0107 et al. (2024) points out that hallucinations can be useful for brainstorming or generating artwork, they are a limiting factor for contexts where factuality is a priority, including use cases that require large-scale text processing, such as question answering, information retrieval, summarization, and recommendations. Therefore, research towards robust methods of generating consistent output with LLMs given factual and informative inputs is still an active and ongoing direction. A na\u00efve approach to updating LLM internal knowledge is through means of retraining the model which is a time-consuming and expensive process.\nRecent research (Pan et al., 2024, 2023) has identified Knowledge Graphs (KGs) as relevant structured information of knowledge for factual grounding that LLMs can be synergized with and conditioned on to improve general factual consistency of an LLM's output. KGs are structured representations of knowledge in a graph-like structure consisting of entities, relationships, and attributes that encode factual information about real-world objects in a machine-readable format. KGs can alleviate the need for full retraining by providing a factual basis"}, {"title": "2. Available Resources for Evaluating Hallucinations", "content": "Considering the boom of LLMs in recent years, evaluation of hallucinations has become increasingly important due to the anticipated high value that LLMs can provide for problem solving. This has sparked an increase in dedicated evaluation datasets and benchmarks, Table 1 shows an overview.\nFor the LLM hallucination evaluation to be holistic, we argue that evaluation needs to broadly cover different domains as well as different tasks to test for different types of hallucinations. One of the major objectives for LLMs to be useful for practical applications is generalizability to multiple domains. Table 1 reveails that many of the datasets cover evaluation on a multi-domain basis such as law, politics, medical, science and technology, art, finance, and others.\nMost of the datasets are primarily focused towards evaluating hallucination detectors that output information about whether a hallucination is present in a piece of text on a response, sentence, or span level. While this does not explicitly model hallucination evaluation for a given LLM, the data points can be re-purposed for hallucination evaluation.\nIt is also evident from Table 1 that most of the datasets are actually benchmarks, therefore not providing dedicated training splits that can be used to train parametric knowledge integration models. All datasets in Table 1 except SemEval2025-MuShroom are available only in English therefore neglecting any kind of multilingual evaluation, and thus limiting the accessibility of LLM technology. Additionally, knowledge subgraphs as additional context are not a popular feature of any of the datasets, therefore again limiting the methods that the evaluation and training can be performed on. Given either textual, context, or Web pages as a resource, the primary use case is the evaluation of models based on retrieval augmented generation (RAG) using unstructured text.\nFurthermore, previous work Mizrahi et al. (2024) outlines the need for a multiprompt evaluation, as the output of LLMs can depend on the phrasing of the input. The only dataset that evaluates such robustness and consistency is Rahman et al. (2024) by accompanying each question-answer datapoint with 15 different paraphrasings of the same question.\nTherefore, we conclude that there are many gaps in high-quality evaluation and training resources that have to be closed before they can be used for hallucination evaluation and mitigation, especially through using KGs."}, {"title": "3. Feasibility of Hallucination Mitigation", "content": "Previous works criticize LLMs based on the hallucination phenomena and outline through defined formalisms that LLMs will not be 100% free from the risk of hallucinations Xu et al. (2024), Banerjee et al. (2024). On the other hand, Xu et al. (2024) outlines that access to external knowledge can be an effective mitigator of hallucinations although the scalability remains unclear. This raises essentially two requirements for improving reliability of LLM systems, namely: (1) enabling output interpretability, allowing the end-user to scrutinize the output due to proneness of hallucinations; (2) conditioning an LLM on a reliable external knowledge source for mitigating hallucinations.\nTo this end, KGs are useful under the assumption that the knowledge graph triples are factually correct with respect to the user query. If an LLM uses the KG triples effectively, then its output can be mapped back to the knowledge graph that information originates from so it can be cross-checked and scrutinized as needed."}, {"title": "4. Detection of Hallucinations", "content": "Hallucination detection is the task of determining whether a particular piece of text generated by an LLM contains any form of hallucinations. This is a difficult task due to the multi-faceted nature of the problem.\nGraphEval Sansford et al. (2024) proposes a two-stage method for detecting and mitigating hallucinations with respect to a given textual context as ground-truth. The detection methodology proposes extracting atomic claims from the LLM output as a sub-graph by LLM-prompting and comparing each triple's entailment to the given textual context.\nSimilarly, Rashad et al. (2024) extracts KG subgraphs between source and generated text based on named entities (organizations, places, people, etc.) to then compare the alignment between the two graphs. Classification of the hallucination is done by thresholding the alignment. If a KG is built around named entities, this could lead to information loss on more abstract concepts, therefore improvements can be made towards more comprehensive relation extraction. KGR Guan et al. (2024) also performs hallucination detection through designated system modules for claim extraction, fact selection, and verification. Fact selection relies on the information extraction abilities of LLM's, which in themselves are prone to hallucinations, therefore this raises a problem of effective and reliable query generation based on the given claims. Fleek Fatahi Bayat et al. (2023) is a system demonstration aimed for fact-checking. The authors extract relevant claims as structured triples and verify them against a KG or a Web search engine by generating questions with a separate LLM based on the extracted claims.\nThe general trend of evaluating claims on an atomic level by representing them as KG structures enables output interpretability by allowing to return the inconsistent triples. This enables highlighting of problematic text spans and scrutiny of the output. Manual evaluation can also benefit understanding problematic use cases. However, none of these methods demonstrate results that would suggest the task at hand being solved, and the limited evaluation datasets also do not provide an insight into how truly generalizable these methods are.\nConsidering the inherent limitations of LLMs, we raise skepticism over the scalability and robustness of methods that use multi-stage pipelines for extracting and validating claims if fully relying on LLM prompting to make the judgements at each processing step. We therefore call for the need for further evaluation of such methods of more diverse datasets or at least reporting on fine-grained analysis of each submodule's error rates and performance. We also advocate for lines of research that perform the task without primary reliance on LLMs.\nFurthermore, it is unclear how the mixture of hallucination detection methods scales when combined with other fundamentally different approaches, e.g., LLM uncertainty for hallucination detection Zhang et al. (2023). We therefore propose to investigate the synergy between uncertainty and KG-based hallucination detection."}, {"title": "5. Methods for Integrating Knowledge from KGs in LLMS", "content": "Many previous methods explore integrating external knowledge as part of a larger LLM system as depicted in the categorization in Figure 2. Knowledge from KGs can be integrated at different stages of an LLM system, whether its pretraining, inference, or post-generation. In the following, we discuss the methodological qualities of each of these stages.\nKnowledge in Pretraining. Factually informed pretraining has been explored through incorporating KG triples as part of the training pipeline Sun et al. (2021). The contribution proposes a methodology for fusing KG triples with raw text input by a masked entity prediction task. For a sentiment analysis task Li et al. (2023) information from KGs is combined with text through a dedicated fusion module. Additionally, adapter-based techniques Hou et al. (2022) have been proposed that encode knowledge from KGs acting as low-parameter add-ons to an LLM architecture. This creates factually aware neural modules that, when plugged into a larger LLM architecture, suggest to boost factuality.\nKnowledge During Inference. A common na\u00efve method to integrate external knowledge is through prompting. Given a prompt P, the LLM input can be formed through pairs of knowledge K and queries Q resulting in $P = \\{K, Q\\}$. This is used in RAG applications to append full documents or knowledge triples Lewis et al. (2020), Sun et al. (2023).\nSuch an approach is problematic as the LLM output depends on hand-crafting the prompt template through the overall phrasing of the query, quality of the relevant evidence, fixed context window lengths and lack of control over efficient usage of the prompt text by the model. These problems are also outlined by Mizrahi et al. (2024) and we support the call for more robust evaluation methodologies atleast through multi-prompt evaluations. To this end, reliance on prompting can also be observed in other previous works Guo et al. (2024), Jin et al. (2024), Mou et al. (2024).\nWith respect to Table 1, the only dataset that provides such multi-prompt evaluation is DefAn Rahman et al. (2024), which is a QA dataset where each data point is accompanied by ten different rephrasings of a question. Prompt-based knowledge injection is also limited by the context window size and does not deal with cases where the model's internal knowledge may conflict with the provided evidence. Therefore, context-aware decoding Shi et al. (2024) proposes a strategy for prioritizing in-prompt knowledge through a learnable parameter. It is worth noting that context-aware decoding requires two inference passes to generate a final output, therefore increasing the computational cost twofold.\nRecently, Lageweg and Kruit (2024) proposed a method for generating S-expressions based on extracted entities from a knowledge graph given a user query. The method is fully grounded on the data available in the knowledge graph, therefore while improving factuality, there are open questions towards generalizability and supporting cases, where KG data is incomplete or missing.\nAdditionally, knowledge integration via adapter networks has been explored. Tian et al. (2024), for example, proposes a method for dynamically injecting knowledge graph information in the latent space. This method is supported as it allows to encode rich metadata of KG triples which otherwise cannot be done at scale with prompting, and it enables rapid knowledge updates.\nWe hypothesize that a reliable LLM system development could contain a mixture of these mitigation strategies although it is unclear to what extent different methods complement one another."}, {"title": "Post-Generation", "content": "Another line of work Guan et al. (2024) proposes retrofitting LLM output factuality by consulting an external KG once an answer is generated by an LLM. The methodology follows a 5-stage pipeline, where an output is generated, claims are extracted, cross-checked against an external KG, and afterwards the original output is patched up as needed according to a claim verification module. Each of the five stages in the pipeline relies on an LLM performing the designated task.\nMultilinguality. Recent studies suggest that hallucinations are more prone in lower-resource languages (Chataigner et al., 2024), and language models can have inconsistent knowledge representations (Qi et al., 2023) and disparities Jin et al. (2024) across languages. Additionally, Kaffee et al. (2023) outlines that multilingual KGs can be particularly useful for low-resource languages, where training data is limited with applications towards question answering, fact extraction, and others. Therefore, we identify multilingual knowledge integration as a necessary research direction that can be supported by reliable multilingual KGs, we refer the reader to a previous survey for details regarding multilingual KGs themselves Kaffee et al. (2023).\nA previous method Hou et al. (2022) was proposed to improve a multilingual language model by statically encoding knowledge of a multilingual KG through a set of adapters. This results in helping the model align entities and knowledge in a multilingual space, thus making the internal knowledge representations more language-agnostic. The results of this work suggest mutual benefits for KGs and LLMs with applications on KG completion and KG entity alignment, as well as language understanding tasks, such as named entity recognition and question answering.\nLimitations. Reliance on knowledge pre-training means that the knowledge is encoded statically. While the methods suggest factual and task-specific improvements, this approach does not solve the fundamental problem of rapid knowledge updates required by use cases where knowledge develops continuously. Additionally, common reliance on LLMs and prompting during knowledge integration, inference, and post-generation gives more room for error, especially as the number of submodules involved in the processing pipelines grows. Prompting is also limited due to fixed context-window lengths, fragility of the handcrafted prompt templates and lack of control over the model's usage of the prompt. This creates a trade-off for system designers for balancing between expensive inference passes for potential factuality gains.\nSimilarly, as for hallucination detection, we call for in-depth reporting on the error rate of each submodule, as well as researching methodologies that move away from solving subtasks based on textual prompting. Additionally, we note that there are different types of fundamental approaches to using KGs for hallucination mitigation, such as incorporating them as part of pretraining, inference, or using KGs to retrofit LLM outputs. Therefore, we also call for research that explores the effects of stacking these approaches together and investigating the extent"}, {"title": "6. On Hallucination Evaluation", "content": "Evaluation of LLM factuality can easily become very complex depending on how the task is framed. The MedHalt and TruthfulQA Pal et al. (2023), Lin et al. (2022) datasets contain subtasks that model question answering evaluation as a multi-choice task, meaning that an LLM is required to choose an answer from a predefined list of answers. While experiments can still be designed around this by measuring correct and incorrect responses with and without added knowledge, an LLM can still choose a correct answer simply by chance thus not leading to quality insights on the quality of models' internal knowledge representations.\nTo this end, framing the problem as a generative task and evaluating the semantics can be seen as a more robust approach. Metrics such as BERTScore Zhang* et al. (2020) or BARTScore Yuan et al. (2021) are employed, which are neural-based approaches. Other lines of work use auxiliary LLMs Zheng et al. (2023), Li et al. (2023) to evaluate whether a given LLM's output is hallucinatory or satisfactory for correct answers, thus scoring either correctly or incorrectly. These approaches themselves are prone to errors as language models are prone to hallucinations, this is especially evident in low-resource language settings Kang et al. (2024). Therefore, we argue for the importance of human-based evaluation, in at least a subset of the results, to indicate reliability in a similar spirite as Li et al. (2023) as well as considering languages beyond English for hallucination evaluation.\nA recent work Min et al. (2023) proposes a factuality estimation method FactScore. The methodology is based on two core ideas: (i) break an LLM output into atomic facts, and (ii) compare an atomic fact with respect to external knowledge. Although the original contribution defines the methodology by comparing the atomic facts to Wikipedia articles through entailment, this can be expanded to KGs as the atomic fact extraction allows to compare overlaps with KG triples therefore quantifying factuality on a more fine-grained, interpretable manner. We call for research towards robust methods of claim extraction from LLM output, which would then allow to perform evaluations with respect to KGs as sources of ground truth."}, {"title": "7. Conclusions and Future Work", "content": "In this paper, we highlight and discussed the importance of using KGs as a potential solution for mitigating the hallucination phenomenon. By surveying current literature and analyzing the limitations, we identified useful research directions within resources, hallucination detection, and external knowledge integration. While previous methods suggest improvements, hallucination mitigation is still an ongoing research problem with no single solution that is general enough to solve the task at hand. We believe the semantic web and NLP communities together can solve the problem by combining expertise"}]}