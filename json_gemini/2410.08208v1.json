{"title": "SPA: 3D SPATIAL-AWARENESS ENABLES\nEFFECTIVE EMBODIED REPRESENTATION", "authors": ["Haoyi Zhu", "Honghui Yang", "Yating Wang", "Jiange Yang", "Limin Wang", "Tong He"], "abstract": "In this paper, we introduce SPA, a novel representation learning framework that\nemphasizes the importance of 3D spatial awareness in embodied AI. Our ap-\nproach leverages differentiable neural rendering on multi-view images to endow a\nvanilla Vision Transformer (ViT) with intrinsic spatial understanding. We present\nthe most comprehensive evaluation of embodied representation learning to date,\ncovering 268 tasks across 8 simulators with diverse policies in both single-task\nand language-conditioned multi-task scenarios. The results are compelling: SPA\nconsistently outperforms more than 10 state-of-the-art representation methods,\nincluding those specifically designed for embodied AI, vision-centric tasks, and\nmulti-modal applications, while using less training data. Furthermore, we con-\nduct a series of real-world experiments to confirm its effectiveness in practical\nscenarios. These results highlight the critical role of 3D spatial awareness for\nembodied representation learning. Our strongest model takes more than 6000\nGPU hours to train and we are committed to open-sourcing all code and model\nweights to foster future research in embodied representation learning.", "sections": [{"title": "1 INTRODUCTION", "content": "Vision systems have made remarkable progress in understanding 2D images (He et al., 2020; Chen\net al., 2020a; He et al., 2022; Feichtenhofer et al., 2022; Tong et al., 2022; Yang et al., 2023; Oquab\net al., 2023; Radford et al., 2021; Fang et al., 2023b; Chen et al., 2024). However, achieving true\nvisual intelligence necessitates a comprehensive understanding of the 3D world. This is crucial for\nembodied AI, where agents must perceive, reason, and interact with complex 3D environments.\nExisting visual representation learning methods for embodied AI (Nair et al., 2022; Radosavovic\net al., 2023; Majumdar et al., 2023; Karamcheti et al., 2023; Shang et al., 2024; Yang et al., 2024b)\nlargely rely on paradigms from 2D vision, predominantly employing contrastive-based or masked\nautoencoder (MAE)-based approaches. However, they often struggle to fully capture the spatial\nrelationships and 3D structures inherent in the physical world. This limitation arises from their\nprimary emphasis on 2D semantic understanding, which, though valuable, is still insufficient for\nthe sophisticated spatial reasoning required in embodied AI tasks, where agents need to navigate\nenvironments, manipulate objects, and make decisions using their 3D spatial awareness.\nIn this paper, we introduce SPA, a general 3D spatial-aware representation learning framework for\nembodied AI. SPA leverages neural rendering (Mildenhall et al., 2021) as the pre-training pre-text\ntask on multi-view images. Unlike explicit 3D representations like point clouds or meshes\u2014which\nprior work (Wang et al., 2024b;a; Ze et al., 2024; Zhu et al., 2024) has shown to outperform pure\n2D inputs in robot learning-multi-view images are easier to process and more readily available,\nmaking them ideal for large-scale training, such as from internet videos. Specifically, given a vanilla\n2D image backbone, e.g. a Vision Transformer (ViT) (Dosovitskiy et al., 2021), we first extract\nmulti-view feature maps from the input images. Using known camera poses, we then construct a\nfeature volume from these feature maps and sample rays to apply differentiable neural rendering.\nThis process generates multi-view RGB-D images and semantic maps for supervision without labels,\nenabling the pre-training of a 2D image backbone to enhance 3D spatial awareness."}, {"title": "2 METHODOLOGY", "content": "In this section, we first describe our process for handling multi-view image inputs and feature\nextraction in Sec. 2.1. Subsequently, we construct an explicit feature volume from these multi-view\nfeatures, detailed in Sec. 2.2. Finally, we explain the image rendering from the feature volume and\nloss functions for network optimization in Sec. 2.3 and Sec. 2.4. Our pipeline is visualized in Fig. 2."}, {"title": "2.1 INPUT PROCESS AND FEATURE EXTRACTION", "content": "Given a set of multi-view images $I = {I_1, I_2, . . ., I_N}$, where each $I_i \\in \\mathbb{R}^{3\\times H\\times W}$ and $N \\in \\mathbb{Z}^+$,\nwe utilize a 2D image backbone $F$, such as a ViT. The images are processed separately through $F$,\nyielding latent features $L = {l_1, l_2, ..., l_N}$, where each $l_i = F(I_i) \\in \\mathbb{R}^{L\\times C}$. Following MAE, we\napply random masking to input images to enhance robustness, but without a ViT decoder and MAE's\npixel reconstruction objective. For each $l_i$, masked positions are filled with a mask token, and we\nconcatenate the global class token with other patch tokens as read-out tokens similar to DPT (Ranftl\net al., 2020). We then unpatchify them to obtain a latent feature map of size $\\frac{H}{P} \\times \\frac{W}{P}$, where $P$ is the\nViT patch size. Finally, two simple upsampling layers transform this into a feature map $M_i$ matching\nthe input resolution. Each upsampling layer includes a convolution, a GELU (Hendrycks & Gimpel,\n2016) activation, and a pixel shuffle layer (Shi et al., 2016) with an upscale factor of $\\sqrt{P}$."}, {"title": "2.2 DYNAMIC VOLUME CONSTRUCTION", "content": "To enable multi-view interaction, we construct a 3D feature volume from multi-view feature maps,\nM. Unlike the bird's-eye view (BEV) construction in autonomous driving (Li et al., 2022), which\nusually relies on a fixed scene range around ego vehicle, our method dynamically adjusts the scene\nrange based on the spatial extents of the environment to accommodate varying datasets. Specifically,\nthe scene's bounds are first estimated using available depth data, sparse points, or pre-defined rules.\nWe then partition the scene into a volume of size $X \\times Y \\times Z$, with voxel size dynamically adjusted\nto capture either fine object details or larger environments. Voxel features, $V$, are initialized with\nlearnable positional embeddings. Each voxel is projected onto the multi-view feature maps using\nthe known transformation matrix $T$. Deformable attention (Zhu et al., 2021) is then applied, where\nthe multi-view features act as keys and values, and the voxel features as queries. Finally, a 3D\nconvolution refines the output volume features to obtain $V$. The process can be formulated as:\n$V = Conv3D(DeformAttn(\\tilde{V}, M, T)).$    (1)"}, {"title": "2.3 DIFFERENTIABLE VOLUMETRIC RENDERING", "content": "After constructing the feature volume, we employ differentiable neural rendering (Mildenhall et al.,\n2021) to connect 2D and 3D domains. For better geometry representation, we utilize the implicit\nsigned distance function (SDF) field modeling as in Neus (Wang et al., 2021). The SDF represents\nthe 3D distance from a query point to the nearest surface, implicitly capturing the 3D geometry.\nGiven a feature volume $V$, we apply a shallow 3D CNN $\\Phi$ to directly produce three outputs: an\nSDF feature volume $S \\in \\mathbb{R}^{X \\times Y \\times Z}$, a spherical harmonic (SH) (Yu et al., 2021; Zhu et al., 2023a)\ncoefficient field $K \\in \\mathbb{R}^{D\\times X\\times Y\\times Z}$ (where $D = 3(l_{max} + 1)^2$) for color rendering, and a semantic\nfeature volume $F \\in \\mathbb{R}^{C_{semantic} \\times X\\times Y\\times Z}$.\n$S \\in \\mathbb{R}^{X\\times Y\\times Z}, K \\in \\mathbb{R}^{D\\times X\\times Y\\times Z}, F \\in \\mathbb{R}^{C_{semantic} \\times X\\times Y\\times Z} = \\Phi(V).$ (2)\nUnlike prior work (Huang et al., 2023; Zhu et al., 2023b; Yang et al., 2024a), which employs an MLP\nto compute the attributes of each sampled point individually, we directly apply a 3D CNN to $V$. This\neliminates the need for pointwise MLP computations, reducing redundant processing and enabling\nmore efficient execution. Consequently, our approach leads to substantial improvements in both time\nand memory efficiency, especially when sampling a large number of points during rendering."}, {"title": "2.4 Loss FUNCTIONS", "content": "During pre-training, we randomly sample $K$ pixels from multi-view inputs in each iteration. The\nrendering loss is calculated based on the differences between the input pixel values and the predicted\nvalues. For the semantic feature map, we use the feature map from AM-RADIO (Ranzinger et al.,\n2024) as supervision. Our framework has the capability to distill knowledge from multiple vision\nfoundation models by adding multiple rendering heads. However, this paper does not explore that\napproach, as it is not the primary focus. The rendering loss is expressed as:\n$\\mathcal{L}_{render} = \\frac{1}{K} \\sum_{i=1}^K (\\lambda_{color} \\cdot ||C_i - \\hat{C}_i|| + \\lambda_{depth} \\cdot ||D_i - \\hat{D}_i|| + \\lambda_{semantic} ||F_i - \\hat{F}_i||).$ (7)\nAdditionally, we incorporate the Eikonal regularization loss $\\mathcal{L}_{eikonal}$, near-surface SDF supervision\nloss $\\mathcal{L}_{sdf}$, and free space SDF loss $\\mathcal{L}_{free}$, which are standard in neural surface reconstruction. Detailed\ndefinitions of these losses are provided in Appendix A. The total loss is defined as:\n$\\mathcal{L}_{total} = \\mathcal{L}_{render} + \\lambda_{eikonal} \\cdot \\mathcal{L}_{eikonal} + \\lambda_{sdf} \\cdot \\mathcal{L}_{sdf} + \\lambda_{free} \\cdot \\mathcal{L}_{free}.$   (8)"}, {"title": "3 LARGE-SCALE EMBODIED EVALUATION", "content": "Unlike the CV or NLP communities, where large-scale benchmarks are common, embodied represen-\ntations have not been thoroughly assessed. The largest previous evaluation, VC-1 (Majumdar et al.,\n2023), includes only 17 tasks. This may lead to randomness and bias. Therefore, we have created\nthe largest embodied evaluation to date, encompassing 268 tasks across 8 simulators over 15\ntimes larger than VC-1's evaluation. Additionally, unlike previous approaches (Majumdar et al.,\n2023; Nair et al., 2022; Radosavovic et al., 2023) that used a small MLP policy under single-task\nsettings, our evaluation spans multiple policy types (e.g. MLP, diffusion, transformer) and includes\nboth single-task and language-conditioned multi-task settings. This unprecedented scale and diversity\nensure robust and convincing conclusions. During all evaluations, we adhere to standard practices\nby freezing the pre-trained representation model. Our detailed evaluation settings can be found in\nAppendix B."}, {"title": "4 TRAINING AND IMPLEMENTATION DETAILS", "content": "In this section, we present the step-by-step implementation and training of our SPA model. We first\npre-train a ViT-base (ViT-B) backbone using a small dataset and evaluate this model on the VC-1\nbenchmark to examine the effects of hyperparameters (Sec. 4.1). We then compile several multi-view\ndatasets, training ViT-B models on each to assess the impact of different datasets (Sec. 4.2). Finally,\nwe integrate all factors and scale up both data and model size to train the strongest version of SPA\nusing a ViT-large (ViT-L) backbone (Sec. 4.3). More details can be found in Appendix C."}, {"title": "5 EXPERIMENT RESULTS", "content": "In this section, we present the results of our large-scale evaluation. Our experiments are designed to\naddress the following research questions:"}, {"title": "5.1 OVERALL COMPARISONS (Q1, Q2)", "content": "Evaluation Metrics. We follow prior work (Majumdar et al., 2023; Zhu et al., 2024) in reporting\ntwo metrics: Mean Success Rate (Mean S.R.) and Mean Rank. Mean S.R. is the average success rate\nacross all tasks, indicating overall performance, while Mean Rank reflects the average ranking of each\nmethod's success rate across tasks, providing a measure of relative performance. Since RLBench has\nfixed train and test sets, we report a single result for this benchmark.\nBaselines. We evaluate 9 state-of-the-art representation learning models, all using the same ViT-L\nbackbone, categorized into vision-centric, multi-modal, and embodied-specific. Details of the models are\nsummarized in Tab. 3.\nFinding 1: We observe that SPA demonstrates superior performance in both mean success rate and\nmean rank. While no method ranks first across all individual benchmarks, consistent with the findings\nby Majumdar et al. (2023), SPA achieves the best or second-best mean success rate in 11 out of 13\nbenchmarks. Additionally, it ranks in the top 3 for over 65.5% of individual tasks, surpassing the\nsecond and third highest percentages of 46.8% for MAE and 46.0% for VC-1, respectively. These\ntrends demonstrate the robustness and superiority of SPA.\nFinding 2: We observe that for vision-centric methods, superior performance on vision tasks does not\nnecessarily translate to better embodied performance. Despite using 10 times more data, DINOV2\nperforms worse than MoCoV3 and MAE. Notably, MAE performs exceptionally well, likely due to\nits reconstruction objective, which enhances 2D spatial awareness. Interestingly, methods like MVP\nand VC-1, which are MAE models pre-trained on human interaction data, show no clear advantage"}, {"title": "5.2 ADDITIONAL COMPARISONS (Q1)", "content": "We primarily compare with SOTA methods using the ViT-L backbone, which is commonly available\nand pre-trained on large-scale datasets. However, some embodied-specific models are only offered in\nViT-B variants."}, {"title": "5.3 STUDY ON 3D AWARENESS OF SPA (Q3)", "content": "Firstly, we aim to provide clear evidence that the\nperformance improvements of SPA are due to its\n3D awareness."}, {"title": "5.4 REAL-WORLD EXPERIMENTS (Q4)", "content": "We conduct several real-world experiments to further\ninvestigate the generalization ability of different rep-\nresentations."}, {"title": "6 RELATED WORK", "content": "Representation Learning for Computer Vision. Recent advances in computer vision have increas-\ningly focused on unsupervised and self-supervised learning to utilize large amounts of unlabeled\ndata.\nRepresentation Learning for Embodied AI. Inspired by computer vision, recent work has applied\nmethods such as contrastive learning (Nair et al., 2022; Yang et al., 2023) and masked autoen-\ncoders\nNeural Rendering. Recent advances in 3D vision, particularly in neural rendering (Mildenhall et al.,\n2021), have enabled the encoding of scenes using neural networks, which support differentiable ren-"}, {"title": "7 CONCLUSION AND LIMITATIONS", "content": "In this work, we propose that 3D spatial awareness is crucial for embodied AI and introduce SPA,\na novel framework that pre-trains a standard ViT backbone with 3D spatial awareness. To validate\nour hypothesis, we conduct the largest-scale embodied evaluation to date, over 15 times larger\nthan previous studies. Our experiments demonstrate the clear superiority of SPA and highlight the\nimportance of 3D awareness. Despite strong results across simulated and real robotic tasks, limitations\nremain. Our evaluation is currently restricted to imitation learning (specifically behavior cloning), and\nexploring SPA's performance in other settings, such as reinforcement learning, presents an exciting\nfuture direction. Additionally, SPA currently focuses on static multi-view scenes; extending it to\ndynamic, temporal scenarios could enhance its generality. Lastly, while we use the ViT encoder for\nfair comparison, the volume decoder's multi-view interaction knowledge could be leveraged in policy\nlearning, offering further potential for improvement."}, {"title": "A ADDITIONAL RENDERING LOSSES", "content": "Here we detail the three additional rendering losses we have applied in Sec. 2.4.\nEikonal Regularization Loss. The Eikonal regularization loss, denoted as $\\mathcal{L}_{eikonal}$, is a widely used\nloss function for the regularization of signed distance functions (SDFs) (Gropp et al., 2020). It is\ndefined as:\n$\\mathcal{L}_{eikonal} = \\frac{1}{N_r N_p} \\sum_{i=1}^{N_r} \\sum_{j=1}^{N_p} (||\\nabla s(p_{i,j})|| - 1)^2,$ (9)\nwhere $\\nabla s(p_{i,j})$ represents the gradient of the SDF s at the location $p_{i,j}$. Since the SDF is a distance\nmeasure, $\\mathcal{L}_{eikonal}$ encourages the gradients to have unit norm at the query point.\nNear-Surface and Free Space Loss for SDF. To improve SDF estimation, we incorporate additional\napproximate SDF supervision, similar to iSDF (Ortiz et al., 2022) and GO-Surf (Wang et al., 2022).\nSpecifically, for near-surface points, the difference between rendered depth and ground-truth depth\nserves as pseudo-SDF ground-truth supervision. For points far from the surface, a free space loss is\nused to further regularize the SDF values.\nTo compute the approximate SDF supervision, we define an indicator $b(z)$ for each sampled ray point\nwith ray length $z$ and corresponding ground-truth depth $D$:\n$b(z) = |D-z|.$ (10)\nThe value $b(z)$ can be considered a credible approximate SDF value when it is small. Let $t$ be a\nuser-defined threshold, set to 0.05 in our experiments. For sampled ray points satisfying $b(z) < t$, we\napply the near-surface SDF loss to constrain the SDF prediction $s(z_{i,j})$:\n$\\mathcal{L}_{sdf} = \\frac{1}{N_r N_p} \\sum_{i=1}^{N_r} \\sum_{j=1}^{N_p} |s(z_{i,j}) - b(z_{i,j})|.$ (11)\nFor the remaining sampled ray points, we utilize a free space loss:\n$\\mathcal{L}_{free} = \\frac{1}{N_r N_p} \\sum_{i=1}^{N_r} \\sum_{j=1}^{N_p} max(0, e^{-\\alpha \\cdot s(z_{i,j})} - 1, s(z_{i,j}) - b(z_{i,j})).$ (12)\nwhere $\\alpha$ is set to 5, following Ortiz et al. (2022); Wang et al. (2022). Due to the presence of noisy\ndepth images, $\\mathcal{L}_{sdf}$ and $\\mathcal{L}_{free}$ are applied only to rays with valid depth values.\nIn our experiments, we adopt a similar weighting scheme to GO-Surf (Wang et al., 2022), setting\n$\\lambda_d = 10.0$, $\\lambda_D = 1.0$, $\\lambda_{sdf} = 10.0$, and $\\lambda_{free} = 1.0$. We observe that the Eikonal term can lead to\noverly smooth reconstructions, so we use a small weight of 0.01 for the Eikonal loss."}, {"title": "B EVALUATION SETUPS", "content": "Here we detail the setups of our large-scale evaluation in Sec. 3.\nB.1 SINGLE-TASK BENCHMARKS\nVC-1 (Majumdar et al., 2023). This benchmark includes several simulators. We selected four:\nAdroit (Kumar, 2016), Meta-World (Yu et al., 2020), DMControl (Tunyasuvunakool et al., 2020),\nand TriFinger (W\u00fcthrich et al., 2020). The Adroit subset focuses on dexterous manipulation with\n2 tasks. The Meta-World subset addresses two-finger gripper manipulation with 5 tasks. The\nDMControl subset is for locomotion control, also with 5 tasks. The TriFinger subset targets three-\nfinger manipulation with 2 tasks. For all tasks, we use a 3-layer MLP as the policy network for\neach single-task training, following the original implementation. Each task is trained with 100"}, {"title": "B.2 LANGUAGE-CONDITIONED MULTI-TASK BENCHMARKS", "content": "RLBench (James et al., 2020). This benchmark is a prominent language-conditioned multi-task\nrobot learning framework. PolarNet (Chen et al., 2023) has categorized all tasks into 9 groups. We\nselected 71 tasks from RLBench that can be successfully executed and split them into two groups\nuniformly on categories: Group 1 with 35 tasks and Group 2 with 36 tasks. Each task includes 100\ntraining demonstrations and 25 testing rollouts. For each group, we train a language-conditioned\nmulti-task agent. We employ RVT-2 (Goyal et al., 2024), the state-of-the-art (SOTA) method on\nthis benchmark, as our policy. RVT-2 takes multiple images rendered from point clouds as inputs\nand uses a convolutional block to generate feature maps. We substitute the convolutional block with\ndifferent pre-trained ViTs, unpatchifying the latent vectors concatenated with the global [CLS]\ntoken to obtain feature maps. All other architectures and hyperparameters remain consistent with the\noriginal RVT-2 implementation.\nLIBERO (Liu et al., 2024). Built upon Robosuite (Zhu et al., 2020), LIBERO (Liu et al., 2024)\ngenerates a total of 130 language-conditioned tasks across five suites: LIBERO-Spatial, LIBERO-\nObject, LIBERO-Goal, LIBERO-10, and LIBERO-90. Each suite contains 10 tasks, except for\nLIBERO-90, which includes 90 tasks. We train a language-conditioned multi-task policy for each\nsuite, adopting the transformer policy provided by LIBERO. The image encoders are modified from\ndefault CNNs to frozen pre-trained ViTs, utilizing the [CLS] token for feature extraction. To\nexpedite policy training, we use only 20 demonstrations per task and forgo augmentations, allowing\nfor pre-extraction of all image features during training. After training for 25 epochs, the checkpoints\nfrom the 20th and 25th are evaluated with 20 rollouts per task, and the best checkpoint's performance\nis taken. Finally, the results are averaged on 3 random seeds."}, {"title": "C MORE IMPLEMENTATION DETAILS", "content": "C.1 DATASET DETAILS\nThe datasets used for SPA include ScanNet, ScanNet++, Hypersim, ADT, S3DIS, and Droid.\nScanNet consists of 1.89 million frames in total. Each epoch includes 1.5 times the dataset size. For\neach scene, a random starting frame is selected, followed by the sampling of 1 to 8 frames at random,\nwith an interval of 8 frames between them.\nScanNet++ comprises 0.11 million frames. Each epoch includes 5 times the dataset size. For each\nscene, a random starting frame is selected, followed by the sampling of 1 to 8 frames at random, with\nan interval of 5 frames between them.\nHypersim contains 0.03 million frames. Each epoch includes 8 times the dataset size. For each\nscene, we randomly select 1 to 8 continuous frames."}, {"title": "C.2 PRE-TRAINING DETAILS", "content": "For stability during pre-training, we apply the Exponential Moving Average (EMA) with a decay rate\nof 0.999. The model is trained for 2000 epochs on 80 NVIDIA A100-80G GPUs, using a gradient\nclipping threshold of 1.0. Each GPU processes a batch size of 2, with 8 gradient accumulation steps,\nresulting in a total effective batch size of 2 \u00d7 80 \u00d7 8 = 1280. We employ the AdamW optimizer\nwith a weight decay of 0.04. The base learning rate is set to 5 \u00d7 10-6, and the actual learning rate is\nscaled by a factor of 8 times the effective batch size. A OneCycle learning rate scheduler is used,\nwith a percentage start of 0.05, a divide factor of 100, and a final divide factor of 1000.\nTo facilitate faster convergence and improve stability, we initialize the encoder with ImageNet pre-\ntrained weights from the Masked Autoencoder (MAE), applying a learning rate layer decay of 0.8."}, {"title": "D DETAILED RESULTS OF EACH TASK", "content": "We present the results of all individual tasks in Tab. 9, Tab. 10, Tab. 11, Tab. 12, Tab. 13, and Tab. 14."}, {"title": "E CAMERA POSE ESTIMATION DETAILS", "content": "We adopt a setup similar to that of El Banani et al. (2024) for camera pose estimation using the NAVI\ndataset (Jampani et al., 2023). Given an image pair from different viewpoints, we first extract features\nfrom each image using a frozen, pre-trained Vision Transformer (ViT) encoder. Following standard\nprotocols for embodied evaluation, we use the [CLS] token as the feature representation. The two\n[CLS] tokens are then concatenated and passed through a BatchNorm layer and a Multi-Layer\nPerceptron (MLP) to regress the camera pose. The MLP consists of four linear layers with three\nReLU activations, using hidden sizes of 512, 256, and 128 units, and outputs a 7-dimensional pose\nvector. The first three dimensions represent the xyz translation, while the last four dimensions\ncorrespond to the rotation quaternions."}, {"title": "FREAL-WORLD EXPERIMENT DETAILS", "content": "Our real-world hardware setup is based on the open-source Low-Cost-Robot project (Koch, 2024).\nWe utilize two Intel RealSense D415 cameras for image capture. A visualization of our platform is\nprovided in Fig. 7. For teleoperation, policy training, and evaluation, we leverage the open-source\nRealRobot project (Contributors, 2024). The policy used is the ACT policy (Zhao et al., 2023).\nFor each task, we collect 50 demonstrations, and during evaluation, we conduct 25 rollouts, each\nwith randomized object locations and orientations. The model is trained for 10,000 epochs using\nfour NVIDIA A100 GPUs. We employ the AdamW optimizer with a learning rate of 5 \u00d7 10-5 and\na weight decay of 0.05. Additionally, a OneCycle learning rate scheduler is used, with a starting\npercentage of 0.1, a division factor of 10, and a final division factor of 100."}, {"title": null, "content": "The geodesic distance\nbetween two quaternions q1 and q2 is defined as:\n$\\theta=2 \\arccos(|q_1 \\cdot q_2|),$ (13)\nwhere q1 and q2 are normalized quaternions, and $\\cdot$ denotes the quaternion dot product. The Euclidean\ndistance d between two translation vectors $t_1 = (x_1, y_1, z_1)$ and $t_2 = (x_2, y_2, z_2)$ is given by:\n$d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}.$ (14)"}]}