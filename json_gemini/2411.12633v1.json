{"title": "INSTANT POLICY: IN-CONTEXT IMITATION\nLEARNING VIA GRAPH DIFFUSION", "authors": ["Vitalis Vosylius", "Edward Johns"], "abstract": "Following the impressive capabilities of in-context learning with large transform-ers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics.\nWe introduce Instant Policy, which learns new tasks instantly (without further\ntraining) from just one or two demonstrations, achieving ICIL through two key\ncomponents. First, we introduce inductive biases through a graph representation\nand model ICIL as a graph generation problem with a learned diffusion process,\nenabling structured reasoning over demonstrations, observations, and actions.\nSecond, we show that such a model can be trained using pseudo-demonstrations\narbitrary trajectories generated in simulation as a virtually infinite pool of\ntraining data. Simulated and real experiments show that Instant Policy enables\nrapid learning of various everyday robot tasks. We also show how it can serve as a\nfoundation for cross-embodiment and zero-shot transfer to language-defined tasks.\nCode and videos are available at https://www.robot-learning.uk/instant-policy.", "sections": [{"title": "1 INTRODUCTION", "content": "Robot policies acquired through Imitation Learning (IL) have recently shown impressive capabili-ties, but today's Behavioural Cloning (BC) methods still require hundreds or thousands of demon-strations per task (Zhao et al.). Meanwhile, language and vision communities have shown that when\nlarge transformers are trained on sufficiently large and diverse datasets, we see the emergence of\nIn-Context Learning (ICL) (Brown, 2020). Here, trained models can use test-time examples of a\nnovel task (the context), and instantly generalise to new instances of this task without updating the\nmodel's weights. This now offers a promising opportunity of In-Context Imitation Learning (ICIL)\nin robotics. To this end, we present Instant Policy, which enables new tasks to be learned instantly:\nafter providing just one or two demonstrations, new configurations of that task can be performed\nimmediately, without any further training. This is far more time-efficient and convenient than BC\nmethods, which require numerous demonstrations and hours of network training for each new task.\nICL in language and vision benefits from huge and readily available datasets, which do not exist for\nrobotics. As such, we are faced with two primary challenges. 1) Given the limited available data, we\nneed appropriate inductive biases in observation and action representations for efficient learning in"}, {"title": "2 RELATED WORK", "content": "In-Context Learning (ICL). ICL is an emerging paradigm in machine learning which allows mod-els to adapt to new tasks using a small number of examples, without requiring explicit weight updates\nor retraining. Initially popularised in natural language processing with models like GPT-3 (Brown,\n2020), ICL has been applied to enable robots to rapidly adapt to new tasks by using foundation\nmodels (Di Palo & Johns, 2024), finding consistent object alignments (Vosylius & Johns, 2023a),\nidentifying invariant regions of the state space (Zhang & Boularias, 2024), and directly training\npolicies aimed at task generalisation (Duan et al., 2017; Fu et al., 2024) or cross-embodiment trans-fer (Jang et al., 2022; Jain et al., 2024; Vogt et al., 2017). Despite these advancements, challenges\nremain in achieving generalisation to tasks unseen during training and novel object geometries. In-stant Policy addresses this by leveraging simulated pseudo-demonstrations to generate abundant and\ndiverse data, while its structured graph representation ensures that this data is utilised efficiently.\nDiffusion Models. Diffusion models (Ho et al., 2020) have garnered significant attention across\nvarious domains, due to their ability to iteratively refine randomly sampled noise through a learned\ndenoising process, ultimately generating high-quality samples from the underlying distribution. Ini-tially popularised for image generation (Ramesh et al., 2021), diffusion models have recently been\napplied to robotics. They have been utilised for creating image augmentations (Yu et al., 2023; Man-dlekar et al., 2023) to help robots adapt to diverse environments, generating 'imagined' goals (Kape-lyukh et al., 2023) or subgoals (Black et al., 2023) for guiding robotic policies, and learning precise\ncontrol policies (Chi et al., 2023; Vosylius et al., 2024). In contrast, our work proposes a novel use\nof diffusion models for graph generation, enabling structured learning of complex distributions.\nGraph Neural Networks (GNNs). Graph Neural Networks (GNNs) allow learning on structured\ndata using message-passing or attention-based strategies. These capabilities have been applied\nacross a wide range of domains, including molecular chemistry Jha et al. (2022), social network\nanalysis (Hu et al., 2021), and recommendation systems (Shi et al., 2018). In robotics, GNNs have\nbeen employed for obtaining reinforcement learning (RL) policies (Wang et al., 2018; Sferrazza\net al., 2024), managing object rearrangement tasks (Kapelyukh & Johns, 2022), and learning affor-dance models for skill transfer Vosylius & Johns (2023b). In our work, we build on these foundations\nby studying structured graph representations for ICIL, enabling learning of the relationships between\ndemonstrations, observations, and actions."}, {"title": "3 INSTANT POLICY", "content": "3.1 OVERVIEW & PROBLEM FORMULATION\nOverview. We address the problem of In-Context Imitation Learning, where the goal is for the robot\nto complete a novel task immediately after the provided demonstrations. At test time, one or a few\ndemos of a novel task are provided to define the context, which our trained Instant Policy network\ninterprets together with the current point cloud observation, and infers robot actions suitable for\nclosed-loop reactive control (see Figure 1). This enables instantaneous policy acquisition, without\nextensive real-world data collection or training. We achieve this through a structured graph repre-\nsentation (Section 3.2), a learned diffusion process (Section 3.3), and an abundant source of diverse\nsimulated pseudo-demonstrations (Section 3.4).\nProblem Formulation. We express robot actions a as end-effector displacements $T_{EA} \\in SE(3)$(which, when time-scaled, correspond to velocities), along with binary open-close commands for\nthe gripper, $a_g \\in \\{0,1\\}$. Such actions move the robot's gripper from frame E to a new frame\nA and change its binary state accordingly. Our observations, $o_t$ at timestep t, consist of seg-\nmented point clouds $P_t$, the current end-effector pose in the world frame W, $T_{WE} \\in SE(3)$,\nand a binary gripper state $s_g \\in \\{0,1\\}$. Formally, our goal is to find a probability distribution,\n$p(a_{t:t+T} | O_t, \\{(O_{ij}, a_{ij})\\}_{j=1}^N{}^L_{i=1})$, from which robot actions can be sampled and executed. Here,\nT denotes the action prediction horizon, while L and N represent the demonstration length and the\nnumber of demonstrations, respectively. For conciseness, from now onwards we refer to the demon-\nstrations, which define the task at test time and are not used during training, as context C, and the\naction predictions as a. Analytically defining such a distribution is infeasible, therefore we aim to\nlearn it from simulated pseudo-demonstrations using a novel graph-based diffusion process."}, {"title": "3.2 GRAPH REPRESENTATION", "content": "To learn the described conditional probability of actions, we first need to choose a suitable repre-sentation that would capture the key elements of the problem and introduce appropriate inductive\nbiases. We propose a heterogeneous graph that jointly expresses context, current observation, and\nfuture actions, capturing complex relationships between the robot and the environment and ensuring\nthat the relevant information is aggregated and propagated in a meaningful manner. This graph is\nconstructed using segmented point cloud observations, as shown in Figure 2.\nLocal Representation. The core building block of our representation is the observation at time\nstep t, which we express as a local graph $G_i(P_t, T_{WE}, s_g)$ (Figure 2, left). First, we sample M\npoints from a dense point cloud $P_t$ using the Furthest Point Sampling Algorithm and encode lo-\ncal geometry around them with Set Abstraction (SA) layers (Qi et al., 2017), obtaining feature\nvectors F and positions p as $\\{F^i, p^i\\}_{i=1}^M = \\phi(P_t)$. The separately pre-trained $\\phi$, an implicit oc-"}, {"title": "3.3 LEARNING ROBOT ACTION VIA GRAPH DIFFUSION", "content": "To utilise our graph representation effectively, we frame ICIL as a graph generation problem and\nlearn a distribution over previously described graphs $p_{\\theta}(G)$ using a diffusion model, depicted in\nFigure 3. This approach involves forward and backward Markov-chain processes, where the graph\nis altered and reconstructed in each phase. At test time, the model iteratively updates only the parts of\nthe graph representing robot actions, implicitly modelling the desired conditional action probability.\nTraining. Training our diffusion model includes, firstly, the forward process, where noise is itera-tively added to the samples extracted from the underlying data distribution $q(G)$. In this phase, we\nconstruct a noise-altered graph by adding noise to the robot actions according to Ho et al. (2020):\n$q(G^k | G^{k-1}) = G(G_i (\\mathcal{N}(a_k; \\sqrt{1 - \\beta_k} a_{k-1}, \\beta_k I), G_c)), k = 1,..., K$ \nHere, $\\mathcal{N}$ represents the normal distribution, $\\beta_k$ the variance schedule, and K the total number of\ndiffusion steps. This process gradually transitions the ground truth graph representation into a graph\nconstructed using actions sampled from a Gaussian distribution.\nInversely, in the reverse diffusion process, the aim is to reconstruct the original data sample, in\nour case the graph, from its noise-altered state, utilising a parameterised model $p_{\\theta}(G^{k-1} | G^k)$.\nIntuitively, such a model needs to learn how the gripper nodes representing the robot actions should\nbe adjusted, such that the whole graph moves closer to the true data distribution $q(G)$. Formally, the\nparameterised model learns a denoising process of actions using our graph representation $G(a)$ as:\n$G^{k-1} = G(G_i(a(\\hat{a}_k - \\gamma \\epsilon_{\\theta}(G^k, k)) + \\mathcal{N}(0, \\sigma^2 I)), G_c)$\nHere, $\\epsilon_{\\theta}(.)$ can be interpreted as effectively predicting the gradient field, based on which a single\nnoisy gradient descent step is taken (Chi et al., 2023). As we represent actions as collections of\nnodes with their associated positions p and features, that depend on the binary gripper actions $a_g$,\nsuch a gradient field has two components $\\epsilon_{\\theta} = [\\nabla p, \\nabla a_g]^T$. As we will discuss later, $\\nabla a_g$ can\nbe used directly in the diffusion process, while a set of $\\nabla p$ predictions is an over-parameterisation\nof a gradient direction on the SE(3) manifold, and additional steps need to be used to compute a\nprecise denoising update. However, this can result in a large translation dominating a small rotation,\nand vice versa, preventing precisely learning both components well. To address this, we represent\nthe denoising directions as a combination of centre-of-mass movement and rotation around it, ef-\nfectively decoupling the translation and rotation predictions while remaining in Cartesian space as\n$[\\hat{v}_t, \\hat{p}_r] = [t_{EA} - \\hat{t}_{EA}, R_{EA} \\hat{p}_{kp} - \\hat{R}_{EA} \\times \\hat{p}_{kp}]^T$, with $\\nabla \\hat{i}_p = \\nabla p_t + \\nabla o_i$, representing flow\n(red arrows in Figure 3, left). Here, $t_{EA} \\in \\mathbb{R}^3$ and $R_{EA} \\in SO(3)$ define the SE(3) transformation\nrepresenting actions $T_{EA}$. Thus we learn $\\epsilon_{\\theta}$ by making per-node predictions $\\epsilon_k \\in \\mathbb{R}^7$ and optimis-\ning the variational lower bound of the data likelihood which has been shown (Ho et al., 2020) to be\nequivalent to minimising $MSE(\\epsilon_k - \\epsilon_{\\theta}(G^k))$. As our parameterised model, we use a heterogeneous\ngraph transformer, which updates features of each node in the graph, $F_i$, as (Shi et al., 2020):\n$\\hat{F}_i = W_1 F_i + \\sum_{j \\in \\mathcal{N}(i)} att_{i,j} (W_2 F_j + W_5 e_{ij}); att_{i,j} = softmax\\left(\\frac{(W_3 F_i)^T (W_4 F_j + W_5 e_{ij})}{\\sqrt{d}}\\right)$\nHere, W represent learnable weights. Equipping our model with such a structured attention mech-anism allows for selective and informative information aggregation which is propagated through\nthe graph in a meaningful way, while ensuring that memory and computational complexity scales\nlinearly with increasing context length (both N and L). More details can be found in Appendix C.\nDeployment. During test time, we create the graph representation using actions sampled from the\nnormal distribution, together with the current observation and the demonstrations as the context. We\nthen make predictions describing how gripper nodes should be adjusted, and update the positions of\nthese nodes by taking a denoising step according to the DDIM (Song et al., 2020):\n$\\hat{p}^{k-1} = \\sqrt{\\alpha_{k-1}} p^g + \\frac{\\sqrt{1 - \\alpha_{k-1}}}{\\sqrt{1 - \\alpha_k}} (p_0^g - \\sqrt{1-\\alpha_k}p^g)$.\nHere, $p^g = p_0^g + \\Delta p_t + \\Delta p_r$. This leaves us with two sets of points $\\hat{p}^{k-1}$ and $p_0^g$, that implicitly\nrepresent gripper poses at denoising time steps k-1 and k. As we know the ground truth corre-spondences between them, we can extract an SE(3) transformation that would align them using a\nSingular Value Decomposition (SVD) (Arun et al., 1987) as:\n$T_{k-1,k} = arg \\underset{T_{k-1,k} \\in SE(3)}{min} || \\hat{p}^{k-1} - T_{k-1,k} \\times p^g ||^2$\nFinally, the $\\hat{a}^{k-1}$ is calculated by applying calculated transformation $T_{k-1,k}$ to $\\hat{a}^k$. Note that for\ngripper opening and closing actions utilising Equation 4 directly is sufficient. This process is re-peated K times until the graph that is in distribution is generated and, as a byproduct, final $\\hat{a}^0$\nactions are extracted, allowing us to sample from the initially described distribution $p(a | o_t, C)$."}, {"title": "3.4 AN INFINITE POOL OF DATA", "content": "Now that we can learn the conditional distribution of actions, we need to answer the question of\nwhere a sufficiently large and diverse dataset will come from, to ensure that the learned model can\nbe used for a wide range of real-world tasks. With In-Context Learning, the model does not need to\nencode task-specific policies into its weights. Thus it is possible to simulate 'arbitrary but consistent'\ntrajectories as training data. Here, consistent means that while the trajectories differ, they 'perform'\nthe same type of pseudo-task at a semantic level. We call such trajectories pseudo-demonstrations.\nData Generation. Firstly, to ensure generalisation across object geometries, we populate a simu-\nlated environment using a diverse range of objects from the ShapeNet dataset (Chang et al., 2015).\nWe then create pseudo-tasks by randomly sampling object-centric waypoints near or on the objects,\nthat the robot needs to reach in sequence. Finally, by virtually moving the robot gripper between\nthem and occasionally mimicking rigid grasps by attaching objects to the gripper, we create pseudo-\ndemonstrations \u2013 trajectories that resemble various manipulation tasks. Furthermore, randomising\nthe poses of the objects and the gripper, allows us to create many pseudo-demonstrations performing\nthe same pseudo-task, resulting in the data that we use to train our In-Context model.\nIn practice, to facilitate more efficient learning of common skills, we bias sampling towards way-points resembling tasks like grasping or pick-and-place. Note that as the environment dynamics and\ntask specifications, such as feasible grasps, are defined as context at inference, we do not need to\nensure that these trajectories are dynamically or even kinematically feasible. In theory, with enough\nrandomisation, the convex hull of the generated trajectories would encapsulate all the possible test-\ntime tasks. More information about the data generation process can be found in Appendix D.\nData Usage. During training, we sample N pseudo-demonstrations for a given pseudo-task, using\nN - 1 to define the context while the model learns to predict actions for the Nth. Although pseudo-\ndemonstrations are the primary training data, our approach can integrate additional data sources in\nthe same format, allowing the model to adapt to specific settings or handle noisier observations."}, {"title": "4 EXPERIMENTS", "content": "We conducted experiments in two distinct settings: 1) simulation with RLBench (James et al., 2020)\nand ground truth segmentations, and 2) real-world everyday tasks. Our experiments study perfor-\nmance relative to baselines, to understand the effect of different design choices, to reveal the scaling\ntrends, and to showcase applicability in cross-embodiment and modality transfer. Videos are avail-\nable on our anonymous webpage at https://www.robot-learning.uk/instant-policy.\nExperimental Setup. Here, we describe parameters used across all our experiments unless explic-\nitly stated otherwise. We use a single model to perform various manipulation tasks by providing N=2\ndemos, which we express as L=10 waypoints as context and predict T=8 future actions. We train\nthis model for 2.5M optimisation steps using pseudo-demonstrations that are being continuously\ngenerated. When we discuss integrating additional training data beyond pseudo-demonstrations, we\nrefer to models fine-tuned for an additional 100K optimisation steps using a 50/50 mix of pseudo-\ndemonstrations and new data. For more information, please refer to Appendix E.\nBaselines. We compare Instant Policy to 3 baselines which also enable In-Context Imitation Learn-ing, namely: BC-Z (Jang et al., 2022), Vid2Robot (Jain et al., 2024), and a GPT2-style model (Rad-ford et al., 2019). BC-Z combines latent embedding of the demonstrations with the current observa-\ntion and uses an MLP-based model to predict robot actions, Vid2Robot utilises a Perceiver Resam-pler (Jaegle et al., 2021) and cross-attention to integrate information from the context and current\nobservation, and GPT2 uses causal self-attention to predict the next tokens in the sequence, which in"}, {"title": "4.2 INSTANT POLICY DESIGN CHOICES & SCALING TRENDS", "content": "Our next set of experiments investigates the impact of various hyperparameters on the performance\nof our method, focusing on design choices requiring model re-training, inference parameters that\nalter model behaviour at test time, and scaling trends as model capacity and training time increase.\nFor the design choices and inference parameters, we calculate the average change in success rate on\n24 unseen RLBench tasks, with respect to the base model used in the previous set of experiments,\nwhile for the scaling trends, we report validation loss on a hold-out set of pseudo-demonstrations to\nsee how well it can capture the underlying data distribution.\nDesign Choices. We now examine the following: action mode, diffusion mode, and the prediction\nhorizon. For action modes, we compare our proposed parameterisation, which decouples translation\nand rotation, against an approach without such decoupling, and more conventional approaches like\ncombining translation with quaternion or angle-axis representations. For diffusion mode, we evalu-ate predicting flow versus added noise, direct sample, and omitting diffusion, regressing the actions\ndirectly. Lastly, we assess the impact of predicting different numbers of actions. The results, shown\nin Table 2 (left), show that these choices greatly influence performance. Decoupling translation and\nrotation in Cartesian space allows for precise low-level action learning. The diffusion process is\nvital for capturing complex action distributions, with predicting flow showing the best results. Fi-nally, predicting multiple actions is helpful, but this also increases computational complexity. For a\ndetailed discussion of other design choices, including unsuccessful ones, please refer to Appendix H.\nInference Parameters. Using a diffusion with a flexible representation that handles arbitrary con-text lengths allows us to adjust model performance at inference. We investigate the impact of the\nnumber of diffusion steps, the demonstration length, and the number of demonstrations in the con-text, as shown in Table 2 (right). Results show that even with just two denoising steps, good perfor-\nmance can be achieved. Demonstration length is critical; it must be dense enough to convey how the\ntask should be solved, as this information is not encoded in the model weights. This is evident when\nonly the final goal is provided (demonstration length = 1), leading to poor performance. However,\nextending it beyond a certain point shows minimal improvement, as the RLBench tasks can often be\ndescribed by just a few waypoints. For more complex tasks, dense demonstrations would be crucial.\nFinally, performance improves with multiple demonstrations, though using more than two seems to\nbe unnecessary. This is because two demonstrations are sufficient to disambiguate the task when\ngeneralising only over object poses. However, as we will show in other experiments, this does not\nhold when the test objects differ in geometry from those in the demonstrations.\nScaling Trends. The ability to continuously\ngenerate training data in simulation allows our\nmodel's performance to be limited only by\navailable compute (training time) and model ca-\npacity (number of trainable parameters). To as-\nsess how these factors influence our approach,\nwe trained three model variants with different\nnumbers of parameters and evaluated them af-\nter varying numbers of optimisation steps (Fig-\nure 6). The results show that the model's\nability to capture the data distribution (as re-\nflected by decreasing validation loss) scales\nwell with both training time and model com-\nplexity. This offers some promise that scaling\ncompute alone could enable the development\nof high-performing models for robot manipu-"}, {"title": "4.3 REAL-WORLD EXPERIMENTS", "content": "In real-world experiments, we evaluate our method's ability to learn everyday tasks and generalise\nto novel objects, unseen in both the training data and the context. We use a Sawyer robot with a\nRobotiq 2F-85 gripper and two external RealSense D415 depth cameras. We obtain segmentation by\nseeding the XMem++ (Bekuzarov et al., 2023) object tracker with initial results from SAM (Kirillov\net al., 2023), and we provide demonstrations using kinesthetic teaching. To help the model handle\nimperfect segmentations and noisy point clouds more effectively, we further co-fine-tuned the model\nused in our previous experiments using 5 demos from 5 tasks not included in the evaluation.\nReal-World Tasks. To evaluate our model's ability to tackle various tasks in the real world, we\ntested it and the baselines on 16 everyday tasks (Figure 7). We evaluated all methods using 10\nrollouts, randomising the poses of the objects in the environment each time. From the results (Ta-\nble 3), we can see that Instant Policy is able to complete various everyday tasks from just a couple\nof demonstrations with a high success rate, outperforming the baselines by a large margin.\nGeneralisation to Novel Objects. While all of\nour previous experiments focused on evaluat-\ning our method's performance on the same ob-\njects used in the demonstrations, here we aim\nto test its ability to generalise to novel object\ngeometries at test time. We do so by provid-\ning demonstrations (i.e., defining the context)\nwith different sets of objects from the same se-\nmantic category, and testing on a different ob-\nject from that same category. For the evalua-\ntion, we use four different tasks (Figure 8), each\nwith six sets of objects (four for the demon-\nstrations/context and two for evaluation). We\nevaluate our method with a different number"}, {"title": "4.3.1 DoWNSTREAM APPLICATIONS", "content": "Cross-embodiment transfer. Since our model\nuses segmented point clouds and defines the\nrobot state by the end-effector pose and grip-\nper state, different embodiments can be used to\ndefine the context and roll out the policy, pro-\nvided the mapping between them is known. We\ndemonstrate this by using human-hand demon-\nstrations with a handcrafted mapping to the\ngripper state, allowing us to transfer the policy\ndirectly to the robot. In qualitative experiments,\nModality change. While obtaining a policy immediately after demonstrations is a powerful and\nefficient tool, it still requires human effort in providing those demonstrations. We can circumvent\nthis by exploiting the bottleneck of our trained model, which holds the information about the context\nand the current observation needed to predict actions. This information is aggregated in the gripper\nnodes of the current observations. If we approximate this bottleneck representation using different\nmodalities, such as language, we can bypass using demonstrations as context altogether. This can\nbe achieved with a smaller, language-annotated dataset and a contrastive objective. Using language-\nannotated trajectories from RLBench and rollout data from previous experiments, we qualitatively\ndemonstrate zero-shot task completion based solely on language commands. For more details, see\nAppendix J, and for videos, visit our webpage at https://www.robot-learning.uk/instant-policy."}, {"title": "5 DISCUSSION", "content": "Limitations. While Instant Policy demonstrates strong performance, it has several limitations. First,\nlike many similar approaches, we assume the availability of segmented point clouds for sufficient\nobservability. Second, point cloud observations lack colour or other semantically rich information.\nThird, our method focuses on relatively short-horizon tasks where the Markovian assumption holds.\nFourth, Instant Policy is sensitive to the quality and downsampling of demonstrations at inference.\nFifth, it does not address collision avoidance or provide end-to-end control of the full configuration\nspace of the robot arm. Finally, it lacks the precision needed for tasks with extremely low tolerances\nor rich contact dynamics. However, we believe many of these limitations can be addressed primarily\nthrough improvements in generation of the pseudo-demonstrations, such as accounting for colli-\nsions, incorporating long-horizon tasks, and by improving the graph representation with additional\nfeatures from vision models, force information, or past observations.\nConclusions. In this work, we introduced Instant Policy, a novel framework for In-Context Imitation\nLearning that enables immediate robotic skill acquisition following one or two test-time demonstra-tions. This is a compelling alternative paradigm to today's widespread behavioural cloning methods,\nwhich require hundreds or thousands of demonstrations. We showed that our novel graph structure\nenables data from demonstrations, current observations, and future actions, to be propagated ef-\nfectively via a novel graph diffusion process. Importantly, Instant Policy can be trained with only\npseudo-demonstrations generated in simulation, providing a virtually unlimited data source that is\nconstrained only by available computational resources. Experiments showed strong performance\nrelative to baselines, the ability to learn everyday real-world manipulation tasks, generalisation to\nnovel object geometries, and encouraging potential for further downstream applications."}, {"title": "APPENDIX", "content": "A GEOMETRY ENCODER\nHere, we describe the local geometry encoder used to represent observations of the environment\nas a set of nodes. Formally, the local encoder encodes the dense point cloud into a set of feature\nvectors together with their associated positions as: $\\{F^i,p^i\\}_{i=1}^M = \\phi(P)$. Here, each feature $F^i$\ndescribes the local geometry around the point $p^i$. We ensure this by pre-training an occupancy\nnetwork (Mescheder et al., 2019), that consists of an encoder $\\phi_e$, which embeds local point clouds,\nand a decoder $\\psi_e$ which given this embedding and a query point is tasked to determine whether\nthe query lays on the surface of the object: $\\psi_e(\\phi_e(P), q) \\rightarrow [0,1]$. The high-level structure of our\noccupancy network can be seen in Figure 9. Note that each local embedding is used to reconstruct\nonly a part of the object, reducing the complexity of the problem and allowing it to generalise more\neasily.\nWe parameterise $\\phi_e$ as a network composed of 2 Set Abstraction layers (Qi et al., 2017) enhanced\nwith Nerf-like sine/cosine embeddings (Mildenhall et al., 2021). It samples M centroids from the\ndense point cloud and embeds the local geometries around them into feature vectors of size 512.\nInstead of expressing positions of points relative to the sampled centroids $p^i$ as $p_j - p^i\\in \\mathbb{R}^3$,\nwe express them as $(sin(2^0 \\pi (p_j - p_i)), cos(2^0 \\pi (p_j - p_i)), ..., sin(2^q \\pi (p_j - p_i)), cos(2^q \\pi (p_j\n- p_i)))$, enabling the model to capture high-frequency changes in the position of the dense points and\ncapture the local geometry more precisely. We parametrise $\\psi_e$ as an eight-layer MLP with residual\nconnections, that uses the same Nerf-like embeddings to represent the position of the query point.\nWe use objects from a diverse ShapeNet (Chang et al., 2015) dataset to generate the training data\nneeded to train the occupancy network. For training Instant Policy, we do not use the decoder and\nkeep the encoder frozen.\nB TRAINING\nTraining our diffusion model involves a forward and backward Markov chain diffusion process,\nwhich is outlined in Equations 1 and 2. Intuitively, we add noise to the ground truth robot actions\nand learn how to remove this noise in the graph space (see Figure 10).\nIn practice, training includes 4 main steps: 1) noise is added to the ground truth actions, 2) noisy ac-\ntions are used to construct our graph representation, 3) the network predicts how nodes representing\nrobot actions need to be adjusted to effectively remove the added noise, and 4) the prediction and\nground truth labels are used to calculate the loss function, and weights of the network are updated\naccordingly.\nTo add noise to the action expressed as $(T_{EA} \\in SE(3), a_g \\in \\mathbb{R}$, we first project $T_{EA}$ to $se(3)$\nusing a Logmap, normalise the resulting vectors, add the noise as described by Ho et al. (2020),\nunnormalise the result and extract the noisy end-effector transformation $\\hat{T}_{EA}$ using Expmap. Such\na process can be understood as adding noise to a SE(3) transformation in its tangential space. We"}, {"title": "C NETWORK ARCHITECTURE", "content": "Here we describe the neural network used to learn the denoising process on graphs, enabling us\nto generate graphs G and implicitly model the conditional action probability. Our parametrised\nneural network takes the constructed graph representation as input and predicts the gradient field\nfor each gripper node representing the actions: $\\epsilon_{\\theta}(G^k)$. These predictions are then used in the\ndiffusion process allowing to iteratively update the graph and ultimately extract desired low-level\nrobot actions. In practice, for computational efficiency and more controlled information propagation,\nwe are using three separate networks $\\sigma$, $\\phi$ and $\\psi$, updating relevant parts of the graph in sequence\nas:\n$\\epsilon_{\\theta}(G^k) = \\psi(G(\\sigma(G_i), \\phi(G_c(\\sigma(G^\\dagger), \\{\\sigma(G^L\\_i)\\}))))$  (6)\nHere, $\\sigma$ operates on local subgraphs $G_i$ and propagates initial information about the point cloud ob-servation to the gripper nodes, $\\phi$ additionally propagates information through the demonstrated tra-jectories and allows all the relevant information from the context to be gathered at the gripper nodes\nof the current subgraph. Finally, $\\psi$ propagates information to nodes in the graph representing the ac-tions. Using such a structured and controlled propagation of information through the graph, together\nwith the learnable attention mechanism described in Equation 3, allows the model to continuously\naggregate relevant information from the context and make accurate predictions about the actions.\nAdditionally, it also results in a clear and meaningful bottleneck in the network with all the relevant\ninformation from the context aggregated in a specific set of nodes $(\\phi(G_c(\\sigma(G^\\dagger), \\{\\sigma(G^L\\_i)\\})))$.\nThis bottleneck representation could be used for retrieval or as shown in our experiments, to switch\nmodalities, for example to language, via a smaller annotated dataset and a contrastive objective.\nEach of the three separate networks is a heterogeneous graph transformer (Equation 3) with 2 lay-ers and a hidden dimension of size 1024 (16 heads, each with 64 dimensions). As we are using\nheterogeneous graphs, each node and edge type are processed with separate learnable weights and\naggregated via summation to produce all node-wise embeddings. This can be understood as a set of\ncross-attention mechanisms, each responsible for processing different parts of the graph representa-tion. We use layer normalisation layers (Ba, 2016) between every attention layer and add additional"}, {"title": "D DATA GENERATION", "content": "Our data generation process, firstly"}]}