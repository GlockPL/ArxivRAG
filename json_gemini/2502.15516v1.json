{"title": "Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection", "authors": ["Yue Sun", "Yeqiang Qian", "Chunxiang Wang", "Ming Yang"], "abstract": "Safety and reliability are crucial for the public acceptance of autonomous driving. To ensure accurate and reliable environmental perception, intelligent vehicles must exhibit accuracy and robustness in various environments. Millimeter-wave radar, known for its high penetration capability, can operate effectively in adverse weather conditions such as rain, snow, and fog. Traditional 3D millimeter-wave radars can only provide range, Doppler, and azimuth information for objects. Although the recent emergence of 4D millimeter-wave radars has added elevation resolution, the radar point clouds remain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast, cameras offer rich semantic details but are sensitive to lighting and weather conditions. Hence, this paper leverages these two highly complementary and cost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D radar spectra with depth-aware camera images and employing attention mechanisms, we fuse texture-rich images with depth-rich radar data in the Bird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally, we propose using GAN-based networks to generate depth images from radar spectra in the absence of depth sensors, further improving detection accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Safety and reliability are key factors for the widespread acceptance of autonomous driving technology. Hence, developing robust all-weather environmental perception algorithms is essential. Common sensors used in autonomous driving for 3D object detection include cameras, LiDAR, and millimeter-wave radar. Cameras can provide rich visual information such as color and texture but are sensitive to lighting and weather. LiDAR offers dense point clouds but faces limitations in adverse weather and is high-cost. Millimeter-wave radar performs exceptionally well under challenging conditions, is cost-effective, and can measure speed and detect objects at long ranges, making it promising for autonomous driving [1]. However, its main challenge is the sparsity of measurements, exacerbated by traditional processing methods like Side Lobe Suppression (SLS) [2] and Constant False Alarm Rate (CFAR) [3], leading to severe information loss. Recent advancements in 4D millimeter-wave radar provide elevation information but still suffer from low data density and noise.\nTo achieve accurate and reliable perception, autonomous vehicles typically fuse data from multiple sensor modalities for environmental perception and 3D object detection.\nFusing signals from cameras and millimeter-wave radars enables achieving a comprehensive and cost-effective perception of the surrounding environment in terms of contours, color, texture, distance, and speed. Additionally, the fusion system can operate continuously under all weather conditions and varying light levels. However, challenges remain in fusing camera and radar signals [4]. Millimeter-wave radar data can be represented in various data formats depending on the processing stage, including raw ADC signals, radar spectra after applying Fast Fourier Transform (FFT) on ADC signals, and radar point clouds after further applying SLS and CFAR operations [5]. Currently, radar point clouds are mostly used in 3D object detection algorithms but face issues due to their high sparsity and inherent differences from LiDAR point clouds, making them less effective for detection tasks. Moreover, there are challenges related to camera-radar sensor fusion in terms of resolution variations and perceived dimensionality [4].\nTo address the aforementioned issues, this paper proposes using raw 4D radar spectra and depth-aware images as input data. Features are extracted separately from the RGB and depth images to obtain corresponding multi-scale features, which are then fused to produce multi-scale image features with depth information. Following a similar approach to EchoFusion [6], image and radar features are fused in the BEV feature space under polar coordinates based on column-wise attention for image features and row-wise cross-attention for radar features, aggregating high resolution of image features in the elevation direction and radar features in the depth direction. The resulting BEV features and object queries are then applied in the polar decoder [7] and detection head for 3D object predictions.\nThe main contributions of this paper are as follows:\n1) We propose an all-weather object detection algo-rithm that integrates depth-aware images and 4D millimeter-wave radar spectra in the BEV feature space to achieve optimal 3D object detection per-formance.\n2) We introduce a method to augment features from RGB images with depth images before camera-radar data fusion, and we propose using a GAN-based depth generator to produce depth images from radar spectra when a depth sensor is unavailable.\n3) Our model achieves good detection performance while significantly reducing network complexity."}, {"title": "II. RELATED WORK", "content": "Millimeter-wave radar uses radio waves in the millimeter-wave frequency range (30 GHz to 300 GHz) for environmental sensing. Typically, each transmitting antenna (TX) emits a sequence of Frequency Modulated Continuous Waves (FMCW), called chirps, with linearly increasing frequency [8]. Reflected waves from objects are captured by receiving antennas (RX) and processed to determine distance, speed, and angle. Traditional 3D radars, with horizontally arranged antennas, provide only horizontal detection and lack elevation information. In contrast, 4D radars use both horizontal and vertical antenna arrays, enabling the measurement of elevation angles as well. 4D radar data can be represented as ADC signals, radar spectra, or point clouds, depending on the processing stage.\nB. Camera-Radar Datasets\nAlthough many public datasets now include millimeter-wave radar data alongside images and LiDAR data, these radar datasets contain fewer samples compared to the latter two. Most existing datasets, such as NuScenes [9], RADDet [10], Zendar [11], RADIATE [12], and CAR-RADA [13], use traditional 3D radar, providing data along the range, Doppler, and azimuth dimensions but lacking elevation information. This limitation makes accurate 3D bounding box estimation challenging in 3D object detection. With the emergence of 4D millimeter-wave radar, more public datasets have begun incorporating 4D radar to capture data with elevation information.\nHowever, most of these datasets provide radar point clouds processed by methods like CFAR, such as View-of-Delft [14], Astyx [15], and RadarScenes [16]. Due to the sparsity of radar signals, we aim to use 4D radar that provides elevation information, and the radar data should be in the form of lossless raw radar data before CFAR operations, including ADC data and radar spectra. Among available datasets, RADIal and K-Radar meet these criteria. However, RADIal initially only offered 2D annotations, and although Liu et al. [6] recently added 3D annotations, it remains limited in scope and lacks data under adverse weather conditions. Therefore, the K-Radar dataset is the most suitable for this research.\nK-Radar contains 35K frames of 4D radar spectrum/tensor (4DRT) with measurements across the range, Doppler, azimuth, and elevation dimensions, alongside multi-view images, high-resolution LiDAR point clouds, annotated 3D bounding boxes, and other relevant data. K-Radar also covers various challenging road structures (e.g., urban, suburban roads, alleyways, and highways) and adverse weather conditions (e.g., fog, rain, and snow) [17]."}, {"title": "III. METHODOLOGY", "content": "Our overall architecture is illustrated in Fig. 2. It takes depth-aware images from a front camera and radar spectra from a 4D millimeter-wave radar as inputs. For the radar spectrum input branch, we use a convolutional neural network (CNN) to extract multi-scale radar features. For the image input branch, we first separately extract multi-scale features from RGB images and depth images, and then fuse these features to obtain multi-scale image features with depth information. In the BEV space under polar coordinates, polar queries at each scale fuse image features and radar features at that scale through an attention mechanism to produce multi-scale BEV features. Finally, the multi-scale BEV features and object queries are fed into the polar decoder [7] and detection heads to estimate the classes and bounding boxes of 3D objects.\nSince each pixel value in a depth image represents the spatial depth information of the corresponding location of the object, while each pixel value in an RGB image corresponds to the color and texture information, feature extraction should be performed separately for each. We preprocess the single-channel depth image by expanding it to three channels through simple repetition. Then, we use the lightweight convolutional network ResNet18 [23] as the backbone to extract multi-scale features from both RGB and depth images. The resulting multi-scale features are concatenated along the channel dimension and fed into a Feature Pyramid Network (FPN) [24] for feature fusion, which outputs multi-scale image features with depth information.\nFor the BEV feature space at the 1 - level in the polar coordinate system, we first initialize it uniformly with $R^{l} \\times A^{l}$ BEV queries, where $R^{l}$ and $A^{l}$ are the range and azimuth resolutions of the BEV features at level 1, and each query is represented as $q^{l}(r_{bev}, a_{bev})$.\nThe fusion of image feature $F^{I}$ and radar feature $F^{R}$ is based on the fact that each polar column with the same azimuth $a_{bev}$ in the BEV features corresponds to a column $x$ in the image feature, and each polar row with the same range $r_{bev}$ in the BEV features corresponds to a row in the radar feature $R$. Following the polar-aligned Attention (PAA) technique proposed in EchoFusion [6], we fuse the image and radar features as follows:\nWe use cross-attention to update the BEV queries with image features as follows:\n$Q^{I} = CrossAttention(Q, K^{I}, V^{I})$ (1)\n$Q^{I}_{1} = all \\ q^{I}(r_{bev}, a_{bev})$ with the same $a_{bev}$ (2)\n$K^{I} = V^{I} = all \\ f^{I}(x_{1}, y_{1})$ with the same $x_{1}$ (3)\nThen, we update the BEV queries with radar features based on:\n$Q^{R} = CrossAttention(Q^{I}, K^{R}, V^{R})$ (4)\n$Q^{R}_{2} = all \\ \\hat{q^{I}}(r_{bev}, a_{bev})$ with the same $r_{bev}$ (5)\n$K^{R} = V^{R} = all \\ f^{R}(a_{1}, r_{1})$ with the same $r_{1}$ (6)\nDepth images are vital for improving the accuracy of vision-based object detection [26]. While depth sensors usually provide these images, they are sometimes absent in autonomous vehicles due to size and cost constraints. Inspired by work in [25], [27], [28], we propose us-ing a GAN-based depth generator to generate accurate depth images with high resolution from millimeter-wave radar spectra. By generating depth images before applying attention-based feature fusion, we can augment images with depth information, allowing image features to include depth information and thereby improve our model's overall detection performance. The depth generator network is similar to the one in [25], except that skip connections are not used in the generator (as shown in Fig. 3). The training data comes from K-Radar sequences 12-20, with ground truth depth obtained by projecting LiDAR point clouds onto images.\nWe use a similar architecture as used in [6] and [7], but with the number of decoder layers reduced from 6 to 3, and we limit the object queries to 20 instead of 30 to minimize computation consumption. The resulting object queries are then fed into classification and regression detection heads to obtain the predicted categories $c$ and 3D bounding boxes $b = (x,y,z,l,w,h,q)$ of detected 3D objects.\nDuring model training, a Hungarian matching-based set-to-set loss is employed, which consists of Focal Loss for the classification network and L1 Loss for the regression network."}, {"title": "IV. EXPERIMENTS", "content": "We use a subset of the K-Radar dataset [17], specifically sequences 1-11, for our experiments. This choice is due to the fact that only sequences 1-20 have been uploaded to Google Drive and are readily accessible. Additionally, we initially reserved sequences 12-20 for training the depth generator.\nWe conduct all experiments on two NVIDIA GeForce RTX 3090 GPUs with a batch size of 4. Due to the limited size of our dataset, we reorganize the training and testing sets. A total of 7,755 frames from K-Radar sequences 1-11 are split into an approximately 9:1 ratio, with 6,972 frames used for training and 783 frames for testing. We follow the same training strategy as used in [6], but we train the model for only 10 epochs as we use about half the data they used for training. Given the limited hardware resources and the relatively large network structure, we restrict our training and testing to the Sedan category, which predominates in K-Radar sequences 1-11, accounting for over 77.3% of all objects.\nWe evaluate the following models on our test dataset as mentioned in Subsection IV-B.\n*   EchoFusion [6]: Configured with its default settings.\n*   EchoFusion-small: A light version of EchoFusion, featuring a smaller network as used in our model that replaces ResNet50 with ResNet18, reduces the number of decoder layers from 6 to 3, and utilizes only 20 object queries instead of 30.\n*   Ours-depth64: Our model, as described in Section III, with the input depth images generated by projecting 64-line LiDAR point clouds onto the images.\n*   Ours-depth128: The same as Ours-depth64, but using 128-line LiDAR instead.\nWe use the KITTI protocol to evaluate the Average Precision (AP) of all the models mentioned above, with Intersection over Union (IoU) thresholds set at 0.3, 0.5, and 0.7. We highlight the best result for each metric in bold. As shown in Table I, Ours-depth128 and Ours-depth64 sig-nificantly outperform EchoFusion-small in all AP metrics except for the BEV AP at a threshold of 0.5. Moreover, while our methods are slightly inferior to EchoFusion in the BEV AP at thresholds of 0.5 and 0.7, both Ours-depth128 and Ours-depth64 surpass EchoFusion in all the 3D AP metrics, except when the 3D AP threshold is 0.3, where Ours-depth64 is slightly inferior to EchoFusion. When the threshold is set at 0.3, both the BEV AP and 3D AP of Ours-depth128 exceed EchoFusion by more than 10%.\nWe present the errors for the outputs of all models in Table II. Using an evaluation method similar to that of NuScenes, we first match the output bounding boxes with the ground truth. We then calculate the average Euclidean distance between the centers of the output and ground truth (GT) bounding boxes as the Average Translation Error (ATE), the average ratio of their diagonal lengths as the Average Scaling Error (ASE), and the average difference in their angles as the Average Orientation Error (AOE). The table indicates that, except for the slightly larger angle error in Ours-depth128, all the error metrics of our methods are lower than those of EchoFusion and EchoFusion-small."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this work, we propose a novel 3D object detection al-gorithm based on the fusion of cameras and 4D millimeter-wave radar for multimodal perception. This approach aims to enhance detection performance, cost-effectiveness, and robustness in all-weather environments. We are the first to utilize depth-aware images and 4D millimeter-wave radar spectra as inputs for 3D object detection, and we propose generating depth images from radar spectra when depth sensors are unavailable. Experimental results demonstrate that our model maintains strong detection performance while requiring less memory and computational resources.\nHowever, while numerous studies have shown that using raw radar data can yield better detection performance compared to radar point clouds, raw radar data also re-quires significant storage capacity and contains consider-able noise. In the future, we aim to explore radar data preprocessing techniques to reduce the size of the radar spectrum and remove noise before inputting it into the model."}]}