{"title": "Exploring the Capability of ChatGPT to Reproduce Human Labels for Social Computing Tasks (Extended Version)", "authors": ["Yiming Zhu", "Peixian Zhang", "Ehsan-Ul Haq", "Pan Hui", "Gareth Tyson"], "abstract": "Harnessing the potential of large language models (LLMs) like ChatGPT can help address social challenges through inclusive, ethical, and sustainable means. In this paper, we investigate the extent to which ChatGPT can annotate data for social computing tasks, aiming to reduce the complexity and cost of undertaking web research. To evaluate ChatGPT's potential, we re-annotate seven datasets using ChatGPT, covering topics related to pressing social issues like COVID-19 misinformation, social bot deception, cyberbully, click-bait news, and the Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise in handling these data annotation tasks, albeit with some challenges. Across the seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%. Its performance excels in clickbait news annotation, correctly labeling 89.66% of the data. However, we also observe significant variations in performance across individual labels. Our study reveals predictable patterns in ChatGPT's annotation performance. Thus, we propose GPT-Rater, a tool to predict if ChatGPT can correctly label data for a given annotation task. Researchers can use this to identify where ChatGPT might be suitable for their annotation requirements. We show that GPT-Rater effectively predicts ChatGPT's performance. It performs best on a clickbait headlines dataset by achieving an average F1-score of 95.00%. We believe that this research opens new avenues for analysis and can reduce barriers to engaging in social computing research.", "sections": [{"title": "1 INTRODUCTION", "content": "Crowd-sourced human intelligence is commonly used for text data annotation [2, 15, 38]. Such annotations are then used for training various models, including stance detection [13], hate speech detection [17], sentiment analysis [31], and bot detection [11]. While unsupervised methods are being introduced for classification tasks, such methods usually require large data samples [40, 42]. Thus, social computing research still relies heavily on human annotations. This, however, creates significant barriers for less well-funded research labs, resulting in a lock-out effect. For example, the annotation of a 10,000-post social media dataset by three human annotators would take approximately five hours. With a rate of $25 per individual worker, this would cost hundreds of dollars [9, 36]. When performing comparative analyses across multiple datasets, these costs can easily escalate to thousands of dollars.\nRecently, the release of ChatGPT has uncovered a range of cases where large language models (LLMs) can help substitute human intelligence [5, 14, 45]. Several works compare the use of ChatGPT to human methods [37, 41, 44]. For instance, researchers have investigated the use of ChatGPT for automatic bug fixing [37], misinformation detection [32], and even generating academic writing [4]. In this paper, we explore the potential of using ChatGPT for five text-based social computing annotation tasks. We primarily seek to understand whether ChatGPT has the potential to reproduce human-generated annotations. ChatGPT's annotations can highlight its usefulness against crowd-sourced annotations. To achieve this, we first use ChatGPT to label seven annotation datasets on five distinct social problems \u2013 COVID-19 controversies (3x), social bot deception, cyberbully, clickbait news, and Russo-Ukrainian War stance detection. We compare ChatGPT's labels with the human assigned labels on those datasets. Our results show that ChatGPT does have the potential to perform data annotation tasks. Performance is highest for the clickbait headlines dataset, with ChatGPT correctly annotating 89.66% of headlines. In contrast, performance is worst for the COVID-19 hate speech task, which only correctly annotates 52.24% of posts. Closer inspection reveals that performance varies substantially across individual labels. We observe significant gaps (over 25%) exist between labels' accuracy on five out of seven datasets. For instance, in social bot detection, while ChatGPT manages to identify 81.1% of human tweets, it can only identify 45.5% of bot tweets.\nThus, we surmise that researchers must be careful in selecting which tasks they use ChatGPT annotation for. This raises the question of how researchers can identify which tasks ChatGPT annotations would be suitable for. To address this, we build a tool, GPT-Rater, to predict whether ChatGPT will be able to reproduce the correct label for given text. This allows researchers to estimate how suitable ChatGPT is for their annotation requirements. Results shows that GPT-Rater effectively predicts the correctness of ChatGPT's annotations. On the clickbait headlines dataset, GPT-Rater manages to predict ChatGPT's annotation correctness with high accuracy (\u03bc = 90.59%, \u03c3 = 0.13%) and F1-scores (\u03bc = 95.00%, \u03c3 = 0.05%). Moreover, for five out of seven datasets, GPT-Rater attains average F1-scores above 75%, with standard deviations lower than 3%. This implies that our tool can effectively give researchers a preview into how suitable ChatGPT automation will be for their annotation tasks.\nWe believe this work can open up new lines of analysis and can act as a basis for future research into the use of ChatGPT for human annotation tasks. Our contributions are as follows:\n(1) We evaluate the efficacy of ChatGPT at replacing human annotators for five important social computing data tasks, spanning seven datasets.\n(2) We show that ChatGPT can replace human annotators, yet this varies across tasks and datasets. Performance is highest for a clickbait headlines detection dataset (89.66% accuracy), and lowest for hate speech detection (52.24% accuracy).\n(3) We propose GPT-Rater, a tool that estimates how suitable ChatGPT is as an annotator for a given task. GPT-Rater demonstrates high effectiveness, with its best performance observed for the clickbait detection task, achieving 90.59% accuracy and 95.00% F1-score. GPT-Rater achieves average F1-scores exceeding 75% for five out of seven datasets. GPT-Rater exhibits robust performance even on a subset of annotated data, showing its potential to assist researchers in estimating ChatGPT's annotation capability with just a small number of human labels."}, {"title": "2 RELATED WORK", "content": "Recent research has looked at using ChatGPT for annotating misinformation [5, 32] and hate speech [19]. Huang et al. [19] report that ChatGPT is able to correctly annotate 80% of the implicit hateful tweets from the LatentHatred dataset [10]. In addition, the authors show that ChatGPT's explanations can reinforce human annotators' perception of the target text in explaining why tweets would be annotated as hateful or not. Our study also examines how well ChatGPT performs in annotating hate speech. However, we do not limit the annotation to a binary decision for whether tweets are hateful or not, and further include a neutral label. Note, existing literature has highlighted the importance of this, stating that tweets with neutral expressions can act as a defense against the spread of hateful content [24, 33].\nSimilar to us, others have used ChatGPT for performing stance detection [1, 44]. Zhang et al. [44] evaluate ChatGPT's performance on detecting political stance on two prevalent datasets, SemEval-2016 [25] and P-Stance [23]. The authors report that ChatGPT can outperform most state-of-art stance detection models in zero-shot settings, suggesting ChatGPT's potential to handle stance annotation tests. Major challenges still remain though. Aiyappa et al. [1] show that ChatGPT's performance varies across different model versions. The authors also point out that such variance is due to the possibility of data leakage, where past prompts are used for training the next ChatGPT generation. Nonetheless, it remains unclear how well ChatGPT performs in annotating individuals' stances in a broader context, such as those who are in favor or against a particular issue. Our analysis also finds further challenges, e.g., that ChatGPT has a tendency to overestimate neutral stances.\nIn addition to the aforementioned themes, others have experimented with ChatGPT to perform tasks such as genre identification [21], topic identification [12], sentiment analysis [5], and fake news detection [5, 18]. However, most of this literature only focuses on a single annotation task. In contrast, we seek to perform a comparative analysis across different data annotation tasks aiming to solve social problems. We further propose a tool, GPT-Rater, that can predict if ChatGPT will correctly annotate a given post."}, {"title": "3 METHODOLOGY", "content": "We follow a comparative approach to analyze the differences in human annotations and ChatGPT annotations by utilizing seven different datasets. We select seven annotation datasets covering five social problems that are commonly used in academic research: (i) COVID-19 controversies (vaccine arguments, anti-Asian hate speech, and COVID-19 fake news) [17, 28, 30], (ii) social bot deception [3], (iii) cyberbully [20], (iv) clickbait news [8], and (v) Russo-Ukrainian War [16, 29, 47]. For these seven datasets, we then attempt to recreate the human annotations using ChatGPT. In the following subsections, we describe our datasets (\u00a73.2), approaches for performing ChatGPT annotation (\u00a73.3)."}, {"title": "3.2 Datasets", "content": "We use seven public social computing datasets related to distinct social problems. Our dataset selection strategy is based on the following requirements: (i) The datasets must be in English to avoid differences in language provision [34], (ii) The datasets must be annotated by human annotators, as we wish to compare the human annotations with ChatGPT. We list our targeted annotation tasks and datasets below.\nDataset 1: Vaccine Stance. We select stance detection dataset towards twitter users' attitudes to COVID-19 vaccine. This dataset, denoted as Vaccine Stance [30], contains 5,926 tweets related to COVID-19 vaccine and ChatGPT annotates 5,832 of them.\nDataset 2: COVID-19 Hate Speech. We select an anti-Asian hate and counterspeech detection dataset called COVID-HATE [17]. We utilize ChatGPT to annotate 2,280 items from a subset released with the human annotations of COVID-HATE.\nDataset 3: COVID-19 Fake News. We select a fake news detection dataset denoted as COVID-19 Fake News [28]. This dataset contains news-sharing posts related to COVID-19. We utilize ChatGPT to annotate 10,540 items from COVID-19 Fake News.\nDataset 4: Social Bot. We select a deepfake detection dataset called TweepFake [11]. This dataset contains tweets posted by 23 bots and 17 human accounts verified by the dataset's authors. We utilize ChatGPT to annotate 16,235 items from TweepFake.\nDataset 5: Anti-LGBT Cyberbullying. We select a cyberbully detection dataset denoted as Anti-LGBT Cyberbullying. This dataset is derived from a large-scale hate speech dataset [20] and re-annotated for building a binary-classifier to identify anti-LGBT cyberbully. We utilize ChatGPT to annotate 4,258 items from Anti-LGBT Cyberbullying.\nDataset 6: Clickbait Headlines. We select a clickbait detection dataset denoted as Clickbait Headlines [8]. This dataset contains news headlines of Wikinews posts and articles published on well-known clickbait websites (e.g., BuzzFeed, Upworthy, ViralNova). We utilize ChatGPT to annotate 31,084 items in from Clickbait Headlines.\nDataset 7: Russo-Ukrainian Stance. We also investigate how ChatGPT performs on timely and recent domains that have arisen since its launch (23th November, 2022). Thus, we select a tweets dataset, denoted as Russo-Ukrainian Stance, for stance detection regarding to debates on the Russo-Ukrainian War [29]. This dataset collects relevant tweets annotated based on its stance towards the invasion. We utilize ChatGPT to annotate 1,321 items from Russo-Ukrainian Stance."}, {"title": "3.3 ChatGPT Annotation", "content": "For each annotated dataset, we try to recreate the same annotations using ChatGPT. We utilize OpenAI API, configured with ChatGPT module gpt-3.5-turbo-0631, to annotate each target dataset. We rely on an official prompt example for classification tasks from OpenAI.\nIn the official document, most prompts are imperative sentences starting with a verb. As such, we choose \"Classify\", which is frequently used in annotation work. We use this verb to design our prompt. According to the official template, we find that starting a new row with a word describing the subject and object in the prompt is effective. Thus, we follow this pattern, injecting the subject and objective of the annotation task here. Another benefit of this template is that it is flexible to inject text input to annotate and specify a desired format for ChatGPT to respond with its annotation.\nBased on this template, we modify this example into a generalized prompt template applicable to our seven distinctive datasets. The template can be adjusted to be applied for annotating text in different context as shown as follows:\nClassify the text about {Topic} with a label from [Label 1, Label 2, ...].\nText: \"{text to classify}\".\nDesired format: <label_for_classification>\nwhere {Topic} refers to the topic or background of the text; The [label1, label2, ...] refers to the set of candidate labels for ChatGPT to annotate the text; and text to classify} refers to the text input for ChatGPT to produce label. In addition, the plain-language index Desired format indicates that ChatGPT should only respond using the label without any other text.\nWe then apply this template to generate a ChatGPT prompt according to the dataset's original annotation strategy. For example, the COVID-HATE tweets focus on COVID-19 and are classified into three labels - Neutral, Counterspeech, and Hate. Accordingly, we inform ChatGPT of the tweets' topic (as COVID-19) and ask it to classify the tweets into the same three labels. Below is an example of such a prompt, where the bold text refers to the content we edited within the generalized template:\nClassify the text about COVID-19 with a label from [Hate, Counterspeech, Neutral].\nText: \"for the last f**king time.. CORONAVIRUS IS NO EXCUSE TO BE RACIST AGAINST ASIANS https://t.co/nBHTadCKzK\".\nDesired format: <label_for_classification>\nFollowing this, we pass all data to ChatGPT for annotation. When ChatGPT responds to such a prompt, it is necessary to parse the response and extract its annotation. We consider a response is parsable only it follows the desired format \u2013 only providing a label without any other text. Thus, we extract ChatGPT's annotation by matching its response to any candidate labels for the dataset.\nIn all, we only encounter an average of 3.13% (SD = 2.99%) responses per-dataset that fail to be parsed. Note, a small number (0.1%) of failed cases are because ChatGPT states there is not enough information for it to make a decision. For example, a failed response from Russo-Ukrainian Stance dataset states - \"Cannot classify the given text about Russo-Ukrainian War with the label [Pro-Russia, Pro-Ukraine] as it does not provide any relevant information about the topic.\"\nWe emphasize that there are many ways in which our methodology could be expanded and refined. Our future work will involve exploring alternative forms of prompt formulation and response mining."}, {"title": "4 RESULTS AND ANALYSIS", "content": "To evaluate the performance of ChatGPT, we compare ChatGPT's annotation against the original human annotations contained within each dataset. We treat the original human annotations as the gold standard that ChatGPT must predict. As such, we treat ChatGPT as a prediction engine, which we can then evaluate using traditional classifier metrics. We use weighted F1-score to evaluate ChatGPT's performance on data annotation. Given a dataset, a higher F1-score indicates that ChatGPT provides annotations that are more similar to humans."}, {"title": "4.1 Annotation Data Summary", "content": "Table 1 presents statistics for the ChatGPT annotation results. The annotated datasets contain 73,493 text items. ChatGPT annotates 71,484 (97.27%) items. For the remainder, 2% of responses do not match any candidate label in the dataset, and 0.73% of responses fail due to API errors. This confirms that ChatGPT can generate easily extractable annotation labels in desired format in most cases."}, {"title": "4.2 Results Summary", "content": "Table 2 presents the weighted recall, precision, and F1-score for ChatGPT's predictions for each dataset. For the seven datasets, ChatGPT achieves an average weighted F1-score of 72.00% (SD = 13.90%). This suggests that, as a data annotator, ChatGPT has the potential to generate some annotations similar to humans. However, its performance varies across different domains. ChatGPT presents perform well on COVID-19 Fake News, Anti-LGBT Cyberbullying, Clickbait Headlines, and Russo-Ukrainian Stance datasets (weighted F1-score > 75%). In contrast, ChatGPT performs poorly on Vaccine Stance, COVID-19 Hate Speech, and Social Bot datasets (weighted F1-score < 65%).\nWe next explore how ChatGPT performs on different labels in each annotation dataset. Figure 1 presents the confusion matrices for the seven datasets. For each matrix, the y-axis refers to the ground truth human labels, and the x-axis refers to ChatGPT's labels. The value in a cell, row i and column j, presents the proportion of text with label i that are annotated with label j by ChatGPT. For all annotation tasks, the proportions of correctly annotated tweets by ChatGPT vary per distinct label. For example, Figure 1a presents the confusion matrix for the Vaccine Stance dataset. Here, we see that ChatGPT corrects labels just 39.6% of \"Anti-vaccine\" vs. 50% for \"Pro-vaccine\u201d vs. 81% for \"Neutral\". In this case, ChatGPT is significantly more accurate at annotating text expressing neutral stance speech than those expressing anti-vaccine or pro-vaccine stance. In all, five out of seven datasets present such an imbalance, where a gap of more than 25% exists between labels' accuracy. These results suggest that for a given annotation task, ChatGPT's accuracy varies heavily across different labels."}, {"title": "4.3 Task Analysis and Implications", "content": "Next, we dive into each annotation task to present implications according to ChatGPT's performance. To assist the following analyses on ChatGPT's label-wise performance, we also present ChatGPT's recall, precision, and F1-score for each annotation label in Table 3.\nRank 1st: Clickbait Headlines. For Clickbait Headlines, ChatGPT's overall performance ranks first out of seven annotation tasks by a weighted F1-score of 89.56%. ChatGPT correctly identifies 89.66% clickbait or non-clickbait news headlines. This is the highest accuracy among the annotation datasets. In addition, the weighted precision rate of 90.92% suggests that ChatGPT can effectively annotate clickbait news headlines, while only introducing a small number of false positives. In addition, ChatGPT attains recall, precision, and F1-score exceeding 80% across all labels. Specifically, the corresponding confusion matrix (Figure 1f) shows that ChatGPT can correctly identify 80.6% of clickbait and 98.5% of non-clickbait headlines. In conclusion, ChatGPT shows better overall performance on Clickbait Headlines than the other six annotation datasets.\nRank 2nd: COVID-19 Fake News. For COVID-19 Fake News, ChatGPT's overall performance ranks second out of seven annotation tasks, with a weighted F1-score of 83.43%. ChatGPT correctly distinguish 83.75% of news as real or fake. However, ChatGPT's performance across different news varies when measured by recall and precision. For real news, ChatGPT achieves a high recall of 95.63%, but a lower precision of 78.18%. For fake news, ChatGPT only attains a recall of 70.7%, but a higher precision of 93.65%. Such a pattern is caused by the existence of many false positives of real news annotated by ChatGPT. Moreover, corresponding confusion matrix (Figure 1c) shows that, while ChatGPT can identify 95.6% real news, 29.3% fake news are misidentified as real. These results suggest that, for COVID-19 Fake News, ChatGPT is able to annotate real news with high accuracy, but also has a tendency to misidentify fake news as real. As fake news detection mainly relies on news content when training models [35, 46], we conjecture such a limitation is caused by a lack of comprehensive COVID-19 fake news data for training ChatGPT.\nRank 3rd: Anti-LGBT Cyberbullying. For Anti-LGBT Cyberbullying annotation, ChatGPT's overall performance ranks third out of seven tasks, with a weighted F1-score of 80.03%. Overall, ChatGPT correctly annotates 79.08% posts as cyberbully or not. However, when annotating cyberbully posts, ChatGPT achieves 98.28% recall, but only attain 58.43% precision. Such a pattern indicates the existence of false positives among cyberbully posts annotated by ChatGPT. The confusion matrix supports this by showing that ChatGPT only correctly annotates 71.2% non-cyberbully posts and 28.8% non-cyberbully posts are misidentified as cyberbully. To summarize, ChatGPT is better at annotating cyberbully posts (using our prompt) when compared against other posts containing non-cyberbully expressions. However, ChatGPT often overestimates non-cyberbully posts as cyberbully.\nRank 4th: Russo-Ukrainian Stance. For the Russo-Ukrainian Stance detection dataset, ChatGPT's overall performance ranks fourth out of seven tasks, with a weighted F1-score of 76.26%. Note, the invasion of Ukraine took place after the training date of our ChatGPT version. Overall, ChatGPT correctly annotates 75.85% of tweets' stance. We find that, for the pro-Ukraine tweets, ChatGPT reports a high precision of 90.02%, with a low recall of 69.35%. In contrast, for the pro-Russia tweets, ChatGPT reports a high recall of 86.91%, but with a low precision of 62.50%. This means that, when ChatGPT annotates pro-Ukraine tweets, it is usually correct (high precision), but the same is not true for annotating a tweet pro-Russia (low precision). The corresponding confusion matrix (Figure 1g) shows that 30.6% pro-Ukraine tweets are mis-annotated as pro-Russia by ChatGPT. In summary, when ChatGPT labels a tweet as expressing a pro-Ukraine stance, it is usually correct. Yet, this comes at the cost of a low recall rate. In contrast, ChatGPT has more false positives when labeling tweets as pro-Russian, but does have a higher recall.\nRank 5th: Social Bots. For the Social Bot dataset, ChatGPT's overall annotation performance ranks fifth out of seven annotation datasets, with a weighted F1-score of 63.70%. Overall, ChatGPT correctly annotates 64.96% of tweets as posted by humans or bots. For precision, ChatGPT only attains 64.16% for human tweets and 66.75% for bot tweets. This suggests ChatGPT's ability to reproduce precise annotations is still limited on social bot detection, i.e., ChatGPT often reports false positives for both human and bot tweets. For recall, ChatGPT attains 81.12% for human tweets but only 45.54% for bot tweets. Such a significant difference implies that, while ChatGPT manages to identify most human tweets, it often fails to distinguish bot tweets from human ones. The corresponding confusion matrix supports this by showing that 54.5% of bot tweets are mis-annotated, compared to only 18.9% human tweets. In summary, ChatGPT has the potential to act as an annotator to distinguish most human content from bot content. Yet, ChatGPT's capability is limited when identifying content generated by social bots. In our case, ChatGPT often accidentally annotates social bot tweets as human-generated. As a result, its annotation for human tweets can involves lots of false positives.\nRank 6th: Vaccine Stance. For the Vaccine Stance dataset, ChatGPT's overall performance ranks sixth out of seven datasets, with a weighted F1-score of 59.17%. Overall, ChatGPT correctly annotates 59.81% of tweets' stance towards COVID-19 vaccine. ChatGPT is more precise when detecting tweets expressing anti-vaccine (precision = 80.13%) and pro-vaccine stance (precision = 74.03%), compared to tweets with neutral expression (precision = 50.25%). In the meantime, ChatGPT's recall varies across these three labels. While ChatGPT achieves a high recall of 80.99% for neutral tweets, it attains only 39.65% for anti-vaccine tweets and 50.02% for pro-vaccine tweets. This indicates that ChatGPT is conservative when annotating tweets' stance towards COVID-19 vaccine and often mis-labels tweets' stance as neutral. According to the corresponding confusion matrix (Figure 1a), ChatGPT mis-annotates 57.5% of anti-vaccine tweets and 47.2% pro-vaccine tweets as neutral tweets. Therefore, ChatGPT performs poorly in labeling tweets' stance on the COVID-19 vaccine. This is because many anti-vaccine or pro-vaccine tweets are mislabeled as neutral by ChatGPT.\nRank 7th: COVID-19 Hate Speech. For the COVID-19 Hate Speech dataset, ChatGPT's performance ranks seventh out of seven, with a weighted F1-score of 51.88%. Overall, ChatGPT correctly annotates 52.24% of tweets as expressing or countering anti-Asia hate. Specifically, for hate speech, ChatGPT attains a F1-score of 62.25% with a high recall of 88.27%. For counterhate speech, ChatGPT attains a very low F1-score of 32.54%, with a recall of 33.33%. This suggests that ChatGPT's performance varies when annotating hate and counterhate content on COVID-19. Importantly, ChatGPT's precision for all three labels is lower than 50%, confirming its annotation for each label involves many false positives. As highlighted by the corresponding confusion matrix (Figure 1b), ChatGPT mislabels 27.1% of neutral tweets as hate tweets, and 24.9% neutral tweets as counterhate tweets. Meanwhile, it mis-annotates 58.1% of counterhate tweets as neutral. In summary, ChatGPT's annotations are inaccurate for the COVID-19 Hate Speech task. ChatGPT often mis-annotates neutral content as hate speech, and fails to distinguish counterhate speech from neutral content."}, {"title": "5 GPT-RATER: PREDICTING CHATGPT SUITABILITY", "content": "The previous section has revealed that ChatGPT's annotation performance varies on a per-task and per-label basis. For it to become an applicable tool, it is therefore necessary to formulate mechanisms to predict for which tasks and data items ChatGPT will perform well. To this end, we propose a tool, GPT-Rater, that can estimate how likely ChatGPT will give the correct annotation for a data item. This can help inform researchers on ChatGPT's suitability for their task."}, {"title": "5.1 GPT-Rater Implementation", "content": "GPT-Rater performs a binary classification task. Its goal is to predict if ChatGPT will be able to correctly select the label for a given task and data item. The input feature set is the document embedding of the data item, and the prediction target is a binary label: 1 indicating that ChatGPT will correctly allocate the same label as the human annotator's, and 0 indicating otherwise. For the document embedding, we employ OpenAI's text-embedding-ada-002 [27] model.\nClassifiers and parameters. We have experimented with five common classification algorithms for GTP-Rater. We summarize their corresponding implementation below:\n\u2022 Support vector machine (SVM): We implement a SVM classifier using the cuml python package, with parameter setting (kernel = \"rbf\", gamma = \"scale\", C = 1, random_state = 42).\n\u2022 Random forest: We implement a random forest classifier using the cuml python package, with parameter setting (split_criterion = \"gini\", n_streams = 1, random_state = 42).\n\u2022 Logistic regression: We implement a logistic regression classifier using the cuml python package, with default parameter setting.\n\u2022 Extreme Gradient Boosting (XGBoost): We implement a XGBoost classifier using the xgboost python package, with parameter setting (tree_method = \u201cgpu_hist\u201d, predictor = \u201cgpu_predictor", "binary": "logistic\u201d, random_state = 42).\n\u2022 Light gradient-boosting machine (LightGBM): We implement a LightGBM classifier using the lightgbm python package, with parameter setting (boosting_type = \u201cgbdt\u201d, device =", "80": 20}, {"title": "5.2 GPT-Rater Performance", "content": "Overall performance. Table 4 summarizes the average and standard deviation of accuracy and F1-score of GPT-Rater across all classifiers on our seven datasets. As an initial step, each dataset involves training GPT-Rater on 80% of annotated data and testing their performance on the remaining 20%. Our results shows that GPT-Rater predicts ChatGPT's annotation performance well. GPT-Rater performs best on the Clickbait Headlines dataset by consistently achieving very high accuracy (\u03bc = 90.59%, \u03c3 = 0.13%) and F1-scores (\u03bc = 95.00%, \u03c3 = 0.05%). Overall, all classifiers consistently achieve high average F1-scores exceeding 75% on five out of seven datasets, with small standard deviations below 3%. We also note that the ranking of GPT-Rater's average accuracy and F1-score aligns closely with the ranking of ChatGPT's annotation performance across all seven datasets. A Spearman correlation analysis reports a significant (p < 0.001) positive rank correlation between accuracy/F1-score for ChatGPT vs. GPT-Rater on the seven datasets (accuracy: r = 0.964, F1-score: r = 1.000) For example, ChatGPT annotations achieve the highest weighted F1-score (89.56%) for the Clickbait Headlines dataset, while GPT-Rater also gains its best performance for this dataset. In contrast, in domains where ChatGPT's performance is less robust (such as Vaccine Stance and COVID-19 Hate Speech), all five GPT-Rater classifiers experience a performance drop, with average F1-scores around 70% for all datasets. This means that GPT-Rater's performance seems reliable only in domains where ChatGPT also performs better.\nTo summarize, GPT-Rater shows that ChatGPT's annotation performance is predictable, and this predictability remains consistent. Furthermore, predictions are more successful in domains where ChatGPT excels. We argue that GPT-Rater can be useful for researchers wishing to estimate how suitable ChatGPT would be for performing their annotation task.\nPerformance on subsets. A key limitation of GPT-Rater is that it must be trained for each task and dataset individually. This means that a researcher working on a new task will need to re-train GPT-Rater from scratch. To evaluate the overhead of this, we next inspect what size of training data is required for GPT-Rater to become usable. For this, we randomly sampled subsets of our seven datasets respectively in proportions from [0.1, 0.2, 0.3, ..., 1.0]. We then split these subsets by a ratio of 80:20 to retrain and test GPT-Rater classifiers. We repeat the sampling and prediction for 100 times to assess classifiers' overall performance on these subsets.\nFigure 2 presents distributions of F1-scores of GPT-Rater classifiers under different training size settings. The \"X\" mark represent the location where a GPT-Rater classifier can achieve a F1-score within 1% of the F1-score in the corresponding full dataset (trained on 80% of the full annotated data). We observe that GPT-Rater still attains good performance even with a small proportion of annotated data. For the four datasets where ChatGPT annotates accurately (F1-score > 75%), GPT-Rater attains good F1-scores on less than 20% of labeled data. However, on the three datasets where ChatGPT gains an F1-score below 65%, GPT-Rater requires more annotated data by the researcher to attain good F1-scores. For instance, on the COVID-19 Hate Speech task, GPT-Rater requires over 56% of data to labeled by the researcher to attain a F1-score within 1% of the F1-score it attains when trained on 80% of annotated data. This means that it is inappropriate for such tasks. This shows that GPT-Rater is robust when trained on a subset of annotated data for certain tasks. In domains where ChatGPT excels, GPT-Rater only requires a small proportion of annotated data by the researcher to perform good prediction for ChatGPT's annotation performance."}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "Summary. This study has investigated ChatGPT's potential to act as a data annotator for social computing data, related to pressing social issues. We have compared ChatGPT's performance on seven datasets against the original human annotations. ChatGPT is able to reproduce human labels, achieving an average F1-score of 72.00%. That said, we discover significant variations in its performance across different domains. This raise the needs for methodologies to identify for which data item ChatGPT will perform well. For this, we propose GPT-Rater to estimate the likelihood that ChatGPT will give the correct annotation for a given data item. Our results show that GPT-Rater attains high average F1-scores, exceeding 75% with small standard deviations below 3%. This suggests that ChatGPT's annotation performance can be consistently predicted. Additionally, we show that GPT-Rater is robust when trained on a small subset of data annotated by the researcher. This implies its potential to help researchers to preview ChatGPT's annotation capability with far fewer human labels.\nImplication. GPT-Rater can predict ChatGPT's annotation using a small subset of data items labeled by the researcher. Yet the amount of human labels needed may vary depending on the domain. Our analysis shows that GPT-Rater's prediction is precise, and ChatGPT performs well when GPT-Rater achieves a high F1-score. From our experiences, we advice researchers to only use GPT-Rater's predictions when the F1-score is above 75%. If this is not reached, a researcher can either label more data to train GPT-Rater or, alternatively, use more domain-specific knowledge. After this, a researchers can select a personal threshold for how accurate it needs the ChatGPT annotations to be. For example, if GPT-Rater predicts that ChatGPT will only correctly annotate 20% of the data, then it may be better to use human annotators.\nLimitations & Future work. We note that our work has a number of limitations, which form the basis of our future work. First, this study only examines ChatGPT's annotation```json\nperformance with a small number of datasets covering five social issues. We would like to further inspect ChatGPT's potential as a annotator on other domains (e.g., named-entity recognition [26] and topic categorization [22]), or on other social problems (e.g., sexual harassment [6] and political polarization [39]). Second, we acknowledge that we only use a single prompt for annotation, and there are many variants that could be experimented with. Prompt design is a major theme of future work, which we believe we yield better results. We are keen to explore how state-of-art prompt-tuning methods, like few-shot prompting [7] and Chain-of-Thought technique [43], can assist with prompt formulation. Third, the design of GPT-Rater is simple, only considering single text embedding as input and five machine learning algorithms. We will fine-tune GPT-Rater to achieve better performance by expanding its input. We further wish to reduce the need for researchers to manually label subsets of data to train GPT-Rater. We hope that our work can act as a catalyst for further work in the area of automated text annotation."}]}