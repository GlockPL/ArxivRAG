{"title": "Exploring bat song syllable representations in self-supervised audio encoders", "authors": ["Marianne de Heer Kloots", "Mirjam Kn\u00f6rnschild"], "abstract": "How well can deep learning models trained on human-\ngenerated sounds distinguish between another species' vocal-\nization types? We analyze the encoding of bat song syllables\nin several self-supervised audio encoders, and find that models\npre-trained on human speech generate the most distinctive rep-\nresentations of different syllable types. These findings form first\nsteps towards the application of cross-species transfer learning\nin bat bioacoustics, as well as an improved understanding of\nout-of-distribution signal processing in audio encoder models.", "sections": [{"title": "1. Introduction", "content": "Many researchers in bioacoustics would benefit from robust and\naccurate feature spaces that can handle graded vocalizations in\nreal-world field recordings, for example for the purpose of auto-\nmatic classification. In the domain of human speech and sound\nprocessing, much recent progress is driven by so-called self-\nsupervised audio encoder models [1, 2], which learn rich rep-\nsentations of acoustic signals through a masked audio seg-\nment prediction task on unlabelled data. Training such models\nfrom scratch for non-human species is currently still infeasi-\nble, due to the limited size of most bioacoustic datasets [3, 4].\nHowever, existing pre-trained models still offer promising op-\nportunities through their use in cross-species transfer learning,\nproviding a new tool to explore divergences and commonali-\nties between species [5]. Here, we explore how a variety of\nself-supervised audio models trained on human and non-human\ngenerated sounds encode bat song syllable types in field record-\nings of one species' territorial song."}, {"title": "2. Data", "content": "We use a dataset of 20 territorial songs produced by males of\nthe Greater Sac-Winged Bat (Saccopteryx bilineata), recorded\nin Costa Rica using an ultrasonic microphone (Avisoft USG\n116Hme with condenser microphone CM16; frequency range\n1-200 kHz). These multisyllabic vocalizations are acquired by\nimitation from tutor males during ontogeny [6] and encode per-\nsonal information about the singer such as individual identity,\ngroup affiliation and regional origin [7]. Territorial songs are\ncomposed of up to six different syllable types [8], five of which\nare present in our dataset and manually labelled for analyses\n(420 syllables in total; including 135, 97, 92, 9, and 87 instances\nof syllable types A, B, C, D, and E, respectively)."}, {"title": "3. Analyses", "content": "3.1. Data pre-processing\nSeveral pre-processing steps were performed before feeding\nour dataset of S. bilineata territorial songs through the pre-\ntrained audio encoder models. For denoising, we used the\nnoise reduction algorithm implemented in the software Avisoft\nSASLab Pro, which automatically recognizes syllables and re-\nmoves noise below a user-defined threshold in the frequency do-\nmain. Depending on the noise floor of each recording, threshold\nlevels were between -60 to -75 dB. Detected noise was reduced\nby 90dB. We further applied a high-pass filter of 10 kHz.\nVocalizations of the recorded S. bilineata population have\na species mean fundamental frequency (FO) around 15.5 kHz\n(SD: 2 kHz), but also contain much energy above 20 kHz. Such\nhigher frequencies are mostly inaudible to humans and outside\nthe training distribution of the pre-trained audio encoders stud-\nied here. After denoising, we therefore move the songs into the\nhuman auditory range by slowing down all recordings in our\ndataset by a factor of 8. In the slowed down recordings, mean\nsyllable duration is 235 ms (SD: 135 ms) and most energy is\ncontained within the 1-8 kHz frequency band for all syllable\ntypes (F0 mean: 2.3 kHz, SD: 900 Hz). Finally, we downsam-\nple all recordings to 16 kHz, as required for processing by the\npre-trained audio encoders.\n3.2. Feature extraction\nOur set of four self-supervised models comprises two different\narchitectures and three different sets of pre-training data (see\nTable 1). The AVES model is an audio representation model\ndeveloped for encoding animal vocalizations; we here use the\nAVES-bio-base configuration pre-trained on a large set of an-\nimal sounds from various species. We also include another\nHuBERT-based model trained exclusively on human speech\n(Librispeech audiobooks [9]), as well as a Wav2Vec2.0 model\ntrained on the same data, and a second Wav2Vec2.0 model\ntrained exclusively on music (from the Free Music Archive\n10]). Each model consists of a CNN-based waveform encoder\nfollowed by 12 Transformer layers, ultimately generating 768-\ndimensional feature sequences at a frame rate of 20 ms."}, {"content": "We aim to assess how distinctively S. bilineata territorial song\nsyllables are encoded in each feature space. For this purpose,\nwe first project each set of syllable features into its 4 most\ndiscriminative directions using Linear Discriminant Analysis\n(LDA). Figure 1 visualizes every syllable's location along the\nfirst two directions of each projected feature space. This reveals\nthat the self-supervised audio encoder models encode each of\nthe 5 syllable types into distinguishable subspaces, which are\nlinearly decodable from their final layer representations. In con-\ntrast, the LFCC and MFCC features show much more entangle-\nment between syllable types.\nTo more precisely quantify the separability between differ-\nent syllable types in each feature space, we compute silhouette\ncoefficients for each syllable type cluster based on Mahalanobis\ndistances between samples."}, {"title": "3.3. Separability analyses", "content": "The silhouette coefficient for each sample is defined as (b\na)/max(a, b), where a is the mean distance to all other points\nin the same cluster, and b is the mean distance to all other points\nin the next nearest cluster. The mean silhouette coefficients per\nLDA-projected feature space are visualized in Figure 2. This\nshows that syllable type separability is highest in the two self-\nsupervised models trained on human speech, followed by the\nmodel trained on animal vocalizations, and finally the model\ntrained on music."}, {"title": "4. Discussion & Conclusions", "content": "We find that the syllable types in our territorial song recordings,\nwhen slowed down to the human hearing range, are distinc-\ntively encoded by self-supervised audio encoders. Represen-\ntations learned by such models thus encode useful features for\nS. bilineata syllable identification, even when only pre-trained\non sounds generated by other species.\nInterestingly, syllable types are most separable in the mod-\nels pre-trained on human speech. This indicates that rich repre-\nsentations optimized for a single species' vocal repertoire might\nform a more promising basis for cross-species transfer learning\nthan those optimized to encode a large variety of species, or\nnon-vocal sound sources like musical instruments. However,\nthe animal vocalization model included in our current compar-\nison set was pre-trained on a substantially smaller amount of\naudio than the speech and music models (Table 1). A compari-\nson against models pre-trained on fewer hours of speech would\nbe needed to determine whether training dataset size could ex-\nplain the difference between models trained on human speech\nvs. multiple species. Between the speech-trained models, the\nHuBERT architecture showed a slight syllable separability ad-\nvantage compared to the Wav2Vec2 architecture. This could\nbe due to the clustering objective that is part of the HuBERT\ntraining procedure [2], potentially driving the model's internal\nrepresentations towards generally more separable subspaces.\nOur current findings indicate that self-supervised audio en-\ncoders pre-trained on human speech generate useful represen-\ntations for distinguishing between S. bilineata song syllable\ntypes. However, territorial songs in this species are known to\nalso encode singer identity, and several other features [7]\nmodels might differ in which features they most prominently\nencode. Representations from self-supervised models can be\noptimized by supervised fine-tuning to encode the most relevant\nfeatures for specific classification and detection tasks. In future\nwork, we aim to further investigate what interpretable features\ncontribute to the distinctive syllable type representations across\neach of the audio encoders' internal layers, and test the applica-\nbility of our approach to other tasks in bat bioacoustics, such as\nsyllable detection, species and dialect identification."}]}