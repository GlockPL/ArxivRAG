{"title": "Cross Feature Fusion of Fundus Image and Generated Lesion Map for Referable Diabetic Retinopathy Classification", "authors": ["Dahyun Mok", "Junghyun Bum", "Le Duc Tai", "Hyunseung Choo"], "abstract": "Diabetic Retinopathy (DR) is a primary cause of blind- ness, necessitating early detection and diagnosis. This paper focuses on referable DR classification to enhance the applicability of the pro- posed method in clinical practice. We develop an advanced cross-learning DR classification method leveraging transfer learning and cross-attention mechanisms. The proposed method employs the Swin U-Net architec- ture to segment lesion maps from DR fundus images. The Swin U-Net segmentation model, enriched with DR lesion insights, is transferred to generate a lesion map. Both the fundus image and its segmented lesion map are used as complementary inputs for the classification model. A cross-attention mechanism is deployed to improve the model's ability to capture fine-grained details from the input pairs. Our experiments, utiliz- ing two public datasets, FGADR and EyePACS, demonstrate a superior accuracy of 94.6%, surpassing current state-of-the-art methods by 4.4%. To this end, we aim for the proposed method to be seamlessly integrated into clinical workflows, enhancing accuracy and efficiency in identifying referable DR.", "sections": [{"title": "1 Introduction", "content": "Diabetic Retinopathy(DR) is a serious and prevalent complication of diabetes mellitus, posing a significant public health concern due to its potential to cause irreversible blindness if left untreated[2]. DR develops as chronic hyper- glycemia damages the small blood vessels within the retina. When these ves- sels are compromised, they can leak fluid or blood, leading to visual impair- ment. Without rapid treatment, DR can advance from mild, non-proliferative stages to severe proliferative stages, characterized by the growth of abnormal new blood vessels on the retinal surface, potentially causing significant vision loss or blindness [3,12]. As depicted in Fig. 1, Microaneurysms, the earliest clin- ical signs of DR, manifest as small red dots caused by weakened capillary walls.\nHemorrhages occur when these fragile vessels rupture, resulting in small blot or flame-shaped red spots. Exudates, which are lipid residues, appear as yellow or white spots on the retina, often forming a circinate pattern. Proliferative DR is distinguished by neovascularization, where new and abnormal blood vessels form on the retinal surface, posing a high risk of severe vision impairment if they bleed into the vitreous humor.\nDespite significant advancements in applying Convolutional Neural Net- work(CNN) for DR detection, achieving high accuracy in referable DR classifi- cation remains a challenging task. The subtle and heterogeneous characteristics of retinal lesions and the diversity of data collection environments can degrade the performance of deep learning networks, so integrating lesion-specific insights into the classification process can address these challenges more effectively[5]. In medical applications, where diagnostic accuracy is paramount, inaccuracies can lead to delayed treatments or unnecessary interventions, impacting patient outcomes. While CNN has shown promise, its limitations necessitate the devel- opment of more robust methods. Therefore, this study proposes a novel approach that combines transfer learning and cross-attention mechanisms to improve the precision and reliability of DR classification. By incorporating lesion-specific in- sights into the classification process, our method overcomes the limitations of existing models and provides a more effective tool for early DR detection.\nIn this paper, we propose a cross-learning framework to enhance the classi- fication of referable DR. Firstly, we introduce a DR segmentation method that uses the Swin U-Net architecture for segmenting lesion maps from retinal fundus images. Secondly, we generate lesion maps using the model trained in the first stage. Then, using the generated lesion maps, i.e. pseudo-lesion maps and the original images, we perform cross-learning to train the classification model. The integration of original images with pseudo-lesion maps via cross-fusion repre- sents a novel approach in the medical imaging field. Segmentation annotations for medical images are costly to obtain and typically limited to small datasets."}, {"title": "2 Related Work", "content": "DR classification has seen significant advancements with deep learning tech- niques. CNN has been widely employed for the automatic detection and clas- sification of DR, demonstrating high accuracy and robustness in various stud- ies. For example, ensemble models combining multiple CNN architectures have been shown to enhance performance by leveraging the strengths of individual models[1,21,22]. Techniques such as transfer learning, where pre-trained models on large-scale datasets are fine-tuned for DR classification tasks, have signifi- cantly reduced the need for extensive labeled data[9]. Additionally, integrating attention mechanisms has allowed models to focus on clinically relevant regions in retinal images, improving diagnostic accuracy. These approaches have collec- tively contributed to the development of reliable and efficient DR classification systems[12]. Recent advancements have also seen the use of hybrid models that combine CNN with other architectures to improve performance. For instance, attention-based CNN has been employed to enhance the focus on relevant regions of retinal images, thereby improving the model's ability to detect and classify lesions accurately[11]. Additionally, studies have explored the use of generative adversarial networks (GANs) to augment training data, addressing the issue of limited labeled datasets and further enhancing model performance[19].\nAdvanced deep learning models demonstrate exceptional performance in var- ious medical image analysis tasks, enabling more effective detection and diagno- sis of complex lesions. For example, the Swin Transformer effectively captures long-range dependencies, making it ideal for processing high-resolution medi- cal images. Its hierarchical and shifted window mechanism excels in detailed lesion detection, proving particularly useful for retinal diseases and other com- plex lesions[25]. The high-resolution Swin Transformer achieves high accuracy in medical image segmentation, especially in accurately segmenting images of tissues with complex structures such as accurately delineating the boundaries of neural tumors, thereby contributing to high-precision diagnostics and treatment"}, {"title": "3 Methodology", "content": "Our proposed method improves referable DR classification performance through a two-step learning process: pixel-level lesions segmentation and image-level clas- sification. Effective feature extraction is essential to enhance performance, and our proposed method sequentially utilizes the Swin U-Net and Swin-T models. As shown in Fig. 2, the first training process uses the Swin U-Net model to extract lesion maps from input fundus images. We use the trained model to generate lesion maps from the input fundus data. This domain adaptation step ensures the model leverages the lesion map features obtained from the first train- ing step. Next, both the original fundus images and the lesion maps generated in the first step are fed into the Swin-T model pre-trained on ImageNet. Training steps combine the original fundus image embedding sequences with the lesion map embedding sequences through a cross-attention block. The cross-attention mechanism enhances the model's ability to focus on highly relevant regions, improving the classification accuracy for referable DR. As part of the training process, the \"Update\" step indicates the model parameter updates during back- propagation, which is critical for optimizing model performance. This proposed two-stage learning method integrates lesion map segmentation, transfer learning, and cross-attention to provide a robust and accurate classification system."}, {"title": "3.1 Model Architecture", "content": "In Step 1, the Swin U-Net architecture is utilized for DR lesion segmentation. The Swin Transformer, known for its hierarchical and shifted window atten- tion mechanisms, serves as the encoder, capturing rich contextual information. The encoder consists of multiple stages, each with several Swin Transformer blocks that process image patches using self-attention mechanisms, capturing long-range dependencies and fine-grained details essential for accurate segmen- tation. The encoder also includes patch merging, which combines smaller image patches into larger ones to reduce the spatial dimensions while preserving es- sential features. The decoder, composed of upsampling layers and convolutional"}, {"title": "3.2 Proposed Method", "content": "Transfer learning and a cross-attention mechanism are applied to enhance model performance. Fig. 3 shows the architecture of the proposed method. The encoder of the Swin U-Net segmentation model, pre-trained on the FGADR dataset, is used to input the EyePACS dataset's original images to obtain lesion maps. This retains lesion-specific features learned during segmentation, providing a rich fea- ture set for the classification task. The cross-attention mechanism integrates information from both the original fundus image and the generated lesion map, i.e. pseudo-lesion map, improving focus on relevant regions. This dual approach ensures the model leverages global and local features, enhancing diagnostic accu- racy. By incorporating cross-attention, the model dynamically prioritizes regions of interest in both input modalities, resulting in more precise and reliable DR classification.\nIn Step 1, the network is trained to segment lesion maps from retinal fundus images using the FGADR dataset. The Swin U-Net initially utilizes pre-trained weights from the ImageNet dataset for general visual knowledge. Subsequently, the model is fine-tuned on the FGADR dataset to learn specific lesion segmen- tation. The trained model generates a lesion map of the EyePACS data during the classification task. This two-step transfer learning process captures both gen- eral and domain-specific features, enhancing segmentation accuracy. Fine-tuning involves adjusting model weights to better fit the FGADR dataset, ensuring op- timal performance in segmenting DR lesions.\nStep 2 includes two end-to-end phases: generating lesion maps from the pre-trained model and classifying DR using a cross-attention mechanism. Af- ter training the segmentation model in Step 1, the model is transferred to the classification model.\nBy utilizing the pre-trained model, the classification model benefits from rich lesion-specific features, enhancing its ability to differentiate between various stages of DR. The model's input consists of the original fundus images and the lesion maps generated by the pre-trained Swin U-Net, and it outputs predictions regarding the presence of DR. The input tensors X (original image) and Y (lesion map) are transformed into Query Q, Key K, and Value V as follows:\n$Q = XW_Q, K = YW_K, V = YW_V$ (2)\nThe attention weights A are calculated as:\n$A = Soft Max(\\frac{QK^T}{\\sqrt{d_k}})$ (3)\nUnlike multi-head attention, which focuses on self-attention, our method's cross- attention mechanism effectively differentiates foreground and background by learning the interaction between the two inputs. This enhances the focus on critical lesion areas, improving the model's performance. Consequently, this ap- proach significantly improved classification accuracy, demonstrating excellent performance in detecting subtle lesions and accurately distinguishing different stages of DR. The cross-attention mechanism further enhances this process by allowing the model to focus on the most relevant regions of the input images, ensuring accurate classification. This stage includes fine-tuning the classification model to optimize performance. Dropout layers are incorporated to prevent over- fitting and improve generalization by randomly deactivating some neurons during training. This addresses issues from diverse and complex data[2]. The learning rate of the stochastic gradient descent optimizer diminishes over time, and bi- nary cross entropy is employed for classification. Fine-tuning ensures the model achieves optimal performance, balancing accuracy and generalization. This stage entails meticulous adjustment of hyperparameters to ascertain the model's ro- bustness and efficacy in clinical settings."}, {"title": "4 Experiments and Results", "content": "4.1 Datasets and Experimental Setup\nThe EyePACS[6] and FGADR[28] datasets were used to validate the proposed model. The EyePACS dataset contains a large collection of image-level reti- nal fundus images of varying qualities and characteristics, captured using vari- ous fundus cameras. Images in the EyePACS dataset range from 1024\u00d71024 to"}, {"title": "4.2 Experiments and Results", "content": "The model's performance was evaluated using multiple metrics. For the segmen- tation task, we used the Dice coefficient and Intersection over Union (IoU) to measure the accuracy of lesion segmentation. The Dice coefficient gauges the similarity between the predicted lesion map and the ground truth by calculat- ing the overlap, ranging from 0 to 1, where 1 indicates perfect agreement. IoU measures the ratio of the intersection of the predicted and true lesion areas to their union, with a value closer to 1 indicating better segmentation perfor- mance. For the classification task, we employed True Positive Rate(TPR), True Negative Rate(TNR), Area Under the Curve(AUC), and overall accuracy (ACC). TPR, or sensitivity, measures the proportion of actual positives correctly identi- fied by the model, while TNR, or specificity, measures the proportion of actual negatives correctly identified. AUC evaluates the trade-off between sensitivity and specificity across different thresholds, with higher values indicating better performance. ACC provides a general measure of the model's performance by calculating the ratio of correctly predicted instances to the total instances. These metrics provide a comprehensive evaluation of the model's ability to accurately classify referable DR and measure the accuracy of lesion segmentation, ensuring robust performance in clinical settings.\nThe proposed method achieved superior performance compared to exist- ing state-of-the-art methods. In the segmentation task, the Swin U-Net model achieved a Dice coefficient of 0.89 and an IoU of 0.86 on the FGADR dataset. In the classification task, the Swin-T model, incorporating the cross-attention mechanism, achieved an AUC of 96.2% and an overall accuracy of 94.6% on the EyePACS dataset. Fig. 6 provides a confusion matrix and an ROC curve il-"}, {"title": "5 Conclusion", "content": "In this study, we have developed an advanced cross-learning method for the clas- sification of referable DR using deep learning techniques. Our method leverages the Swin U-Net architecture to enhance classification accuracy by generating lesion maps and incorporating a cross-attention mechanism. By integrating the Swin-T and U-Net architectures for segmentation and combining them with an optimized thresholding approach, we have constructed a highly accurate model that can address the challenge of limited segmentation annotations. We leverage a relatively small amount of labeled data from the FGADR dataset to generate segmentation annotations for a large unlabeled dataset, namely EyePACS. Ex- perimental results on the FGADR and EyePACS datasets demonstrate superior performance compared to existing state-of-the-art methods, achieving AUC of 96.2% and an overall accuracy of 94.6%. Future research will include more diverse datasets to enhance generalizability, implement strategies to improve robustness, incorporate Grad-CAM for better interpretability, and investigate scalability to handle larger datasets and different medical imaging challenges. The findings from this study have significant clinical implications, enhancing the accuracy and efficiency of automated DR screening and improving patient outcomes by facilitating timely therapeutic interventions. This study lays a strong foundation for using advanced deep learning models in automated DR diagnosis, address- ing dataset diversity and model interpretability to enhance performance and reliability in clinical applications further."}]}