{"title": "RLInspect: An Interactive Visual Approach to Assess Reinforcement Learning Algorithm", "authors": ["Geetansh Kalra", "Divye Singh", "Justin Jose"], "abstract": "Reinforcement Learning (RL) is a rapidly growing area of machine learning that\nfinds its application in a broad range of domains, from finance and healthcare\nto robotics and gaming. Compared to other machine learning techniques, RL\nagents learn from their own experiences using trial and error, and improve their\nperformance over time. However, assessing RL models can be challenging, which\nmakes it difficult to interpret their behaviour. While reward is a widely used metric\nto evaluate RL models, it may not always provide an accurate measure of training\nperformance. In some cases, the reward may seem increasing while the model's\nperformance is actually decreasing, leading to misleading conclusions about the\neffectiveness of the training. To overcome this limitation, we have developed\nRLInspect - an interactive visual analytic tool, that takes into account different\ncomponents of the RL model - state, action, agent architecture and reward, and\nprovides a more comprehensive view of the RL training. By using RLInspect,\nusers can gain insights into the model's behaviour, identify issues during training,\nand potentially correct them effectively, leading to a more robust and reliable RL\nsystem.", "sections": [{"title": "Introduction", "content": "Machine learning systems have made impressive advances due to their ability to learn from high\ndimensional, non-linear, and imbalanced data [1]. However, learning from complex data often leads\nto complex models, which require rigorous evaluation. Over the time, lot of performance metrics\n[2, 3] and visualisation tools [4\u20138] have been proposed and are being used in different supervised\nlearning settings. These metrics and tools have helped in qualifying performance and understanding\ninner working of supervised learning models.\nReinforcement learning (RL) is a branch of ML where the agent (decision making entity) interacts\nwith the environment and learns based on the reward (feedback) received. In recent years RL has seen\nan increase in real-world interest and applications due to its ability to interact with the environment,\nmaking it a valuable tool for addressing real-world problems across multiple domains [9]. It is\nincreasingly being used to solve tasks from different areas like everyday life assistance [10-12],\npolicy making [13-16] and high-stakes domain like finance [17] and clinical decision systems [18].\nHowever, there is a lack of consistent metrics making it difficult to assess true performance of RL\n[19]. Due to the complexity of the real-world problems, selecting a suitable metric for RL becomes\neven more important, as flawed or incomplete metric can lead to inconsistent performance and\nreproducibility [20].\nA suitable and robust metric can either be a numerical summary or can be in the form of a visual\nrepresentation. Although numerical metrics can summarise the performance in a few numbers, visual\ntools can often provide a more intuitive and immediate understanding of complex information [21].\nThis is because visual representations can convey patterns, trends, and relationships in data and can\nalso help keep track of what manipulations are being applied at what stage. While static images\ncan offer analytical insights, their effectiveness is constrained when dealing with complex data or\nrepresentations [22]. Therefore, interactiveness plays an essential role in a visual tool for providing\nthe flexibility to focus on the different regions of the visualisation, as and when needed.\nAn RL training process consists of three main components which are input states, actions and reward\nand also neural network in case of deep RL. In this paper, we present RLInspect, an interactive\nvisual tool for understanding and potentially debugging the RL training process. It provides different\nmethods to assess the behaviour of RL algorithms with respect to the aforementioned components\nduring the training process, which can help evaluate the quality of the RL training process and in-turn,\nthe trained model."}, {"title": "Related Work", "content": "The recent interest in understanding, explaining and interpreting the RL models has led to development\nof analysis methods and techniques. [23] in their report talk about the need for verifying RL and what\ncan be verified offline and during runtime. This section categorises previous work into two groups\none which focuses on explaining RL and the other, where a visual tool is developed for understanding\nRL algorithms.\nExplaining RL: There has been some work done on explaining the behaviour of the RL model.\n[24] proposes an object saliency map to understand the extent of influence various objects in an\nimage have on the q-values. Their method is limited to images as input and depends on the user's\nunderstanding of the best action given the object saliency map, which is not always possible. There\nis also some work done on measuring reliability of RL algorithms [25], which looks at conditional\nvalue-at-risk and dispersion of the cumulative rewards, both during training and testing. This can then\nbe used to analyse results and compare different algorithms. Another work proposes carrying out\nmultiple trials followed by calculating two metrics. First is performance percentile using empirical\ncumulative distribution functions of average returns and second is calculating confidence intervals\nusing their proposed method called performance bound propagation [20]. In another work, [26]\nextracted the interestingness element for explaining RL by introducing the introspection framework.\nThis framework looks at the frequency state and state-action pair, execution certainty, transition value\nand state-action sequence from local minima to local maxima. This idea was further extended in\nXRL-DINE which enhanced and combined interestingness elements and rewards decomposition to\nexplain self-adaptive systems [27].\nVisual tools: The increasing interest in visual analysis methods have led to development of many\nvisual tools for analysing RL algorithms. MDPVis provides simulator-visualization interface, gen-\neralized MDP information visualization and exposes model components to the user [28]. A visual\nanalytic tool called DQNViz is presented in [29] which provides visual analytics in the form of\nfour coordinated views - statistics view, epoch view, trajectory view and segment view. Later, [30]\npresented DRLViz which is a visual analytics interface that focuses on understanding connection\nbetween latent memory representation and decisions of a trained agent. The authors have used\nVizDoom to demonstrate capabilities of DRLViz in identifying elements responsible for low-level\ndecision making and high-level strategies."}, {"title": "RLInspect", "content": "RLInspect is an intuitive and user-friendly tool that aims to improve the performance of Reinforce-\nment Learning models by providing users with a comprehensive understanding of their model's\nbehaviour. It offers a range of interactive visual plots to analyze and gain insights into potential issues."}, {"title": "Architecture of RLInspect", "content": "RLInspect is designed with a plug-able architecture, empowering users to select the most suitable\nmodules for their training purposes or even customize the modules and their analysis to cater to\ntheir unique requirements. It is comprised of three main components: I/O, Analyzers, and Report\nGenerator (Figure 1). The I/O is responsible for managing all input and output operations and is\nequipped with a central hub for data management known as the Data Handler. Upon initialization of\nRLInspect, this DataHandler is created to manage data flow. The Analyzers, on the other hand,\nare created and registered in the Analyzers Registry of RLInspect, though they can also be used\nindependently. Through the use of the DataHandler, the Analyzers gather the necessary data.\nEach analyzer is equipped with its own analysis to extract insights from the collected data. To enhance\ninterpretability, each analyzer's analysis generates interactive plots using Plotly [31], providing a\nmore user-friendly and comprehensive visualization of the results. Finally, the ReportGenerator\naggregates all the generated plots into a single HTML file for visualization. This process flow is\nshown in Figure 2.\nIn the upcoming sections we will present an overview of each module of RLInspect and demonstrate\nhow they provided insights for the RL agent trained on a Cartpole environment of OpenAI Gym [32]."}, {"title": "State Module", "content": "The State Module in RLInspect is designed to assist users in gaining a comprehensive understanding\nof the state-space on which their RL model is being trained. This module utilizes scatter plots to\nprovide users with insights into the distribution of the state-space, which may include a combination\nof exploration-exploitation trade-off states, as well as specific states that were sampled during training\nand their corresponding locations within the state-space. However, visualizing state-spaces can be\nparticularly challenging when state variables are multidimensional and can take on values that are\neither continuous or discrete. To overcome this challenge, RLInspect utilizes Incremental Principal\nComponent Analysis (IPCA) [33] which allows the visualization of high-dimensional states in a\nlower-dimensional space. State Module offers three data distribution analysis:\nState-space Distribution: This analysis is used for visualizing the distribution of states which\nprovides valuable insights into the underlying structure of the state-space and assess the quality of the\ndata coverage. Figure 3 shows the state-space explored by the RL agent for the cartpole environment.\nExploration vs Exploitation Distribution: Exploration-exploitation trade-offs are critical in RL,\nas they dictate the agent's behaviour in an uncertain and dynamic environment. Agents must balance\nthe exploration of new state-action pairs with exploiting the current knowledge to achieve their\ngoals. Identifying areas in the state-space where the model over or under explores, can help fine-tune\nalgorithms and improve overall performance. To check this trade-off, RLInspect generates two\nfacet columns within each scatter plot, allowing for side-by-side comparisons of the exploration and\nexploitation states. In Figure 4, it can be observed that the cartpole-agent has under-explored the\nstate-space.\nTraining States Distribution: Analyzing the training state distribution helps identify if the model is\nbeing uniformly trained across the state-space or if there is a bias towards a region. This is important\nbecause if the model is biased towards a particular region, it may not perform well when making\npredictions for other regions. As observed in figure 5, the distribution of training states matches\nclosely with the non-training states. This suggests that the model has been effectively trained on all\nthe states it has encountered, enabling it to make accurate predictions."}, {"title": "Action Module", "content": "The action module focuses on the behaviour of RL algorithm's decision making. Looking at how the\ndecision making evolves through the training run gives insight into stability and confidence of the\nactions. All the analyses in the action module are carried out on a predefined set of k states and the\naverage across their analysis is taken as a single number representative of the model behaviour. The\naction module consists of following analysis techniques:\nAction confidence: This analysis looks at the confidence of an action taken by the RL algorithm.\nThe confidence of an action is defined as the function of policy distribution, $\\pi(s)$, which is the\nprobability density function or probability mass function of action for a given state.\n$\\text{confidence}(s) = 1 - \\text{entropy}(\\pi(s))$\nFor well behaved RL algorithm, the confidence of an action converges to one as the training converges.\nProof. The update rule for action value in TD(0)is given as:\n$Q_{t+1}^\\pi(s_t, a_t) = Q_t^\\pi(s_t, a_t) + \\alpha (R_t + \\gamma \\max_a Q_t^\\pi(s_{t+1}, a) \u2013 Q_t^\\pi(s_t, a_t))$\nwhere, $R_t +\\gamma \\max_a Q_t^\\pi(s_{t+1}, a)$ is the target and $R_t +\\gamma \\max_a Q_t^\\pi(s_{t+1}, a) -Q_t^\\pi(s_t, a_t)$ is the error.\nFor positive reward and optimal action, target will be greater than the current action value resulting in\npositive error which would result in $Q_{t+1}^\\pi(s_t, a_t) > Q_t^\\pi(s_t, a_t)$. As policy distribution is a function\nof action value, $\\pi_{t+1}(s,a) > \\pi_t(s, a)$, giving us $\\text{entropy}(\\pi_{t+1}(s)) < \\text{entropy}(\\pi_t(s))$. Hence,\n$\\text{confidence}_{t+1}(s) > \\text{confidence}_t(s)$. If we continue to train the RL algorithm, the confidence\nshould be monotonically increasing. For the limiting case,\n$\\lim_{t\\to \\infty} \\pi_t(s) \\to \\begin{cases} \\delta(s), & \\text{continuous} \\\\  \\delta_{a = \\max_a Q_t^\\pi(s_t,a)}, & \\text{discrete} \\end{cases}$\nwhere, d(s) is the delta distribution of actions for a given state which is Dirac's delta distributionand\n$\\delta$ is the Kronecker's delta. As entropy for a delta distribution is zero and given the definition of\nconfidence in eq.1\n$\\lim_{t\\to \\infty} \\text{confidence}_t(s) \\to 1$\nThis is also applicable when using functions approximation, where the target and error are captured\nin the loss,\n$\\mathcal{L}(\\theta_t) = \\mathbb{E}_{s_t,a_t,r_t,s_{t+1} \\sim D}[(R_t + \\gamma \\max_a Q_t^\\pi(s_{t+1}, a; \\theta_t) \u2013 Q_t^\\pi(s_t, a_t; \\theta_t))^2]$\nSimilarly, same can be shown for value iteration with Monte Carlo and TD(A) and policy iteration.\nIn RLInspect, action confidence is implemented for discrete action space, A and q-value is converted\nto policy distribution as\n$\\pi(s,a_i) = \\text{softmax}(q\\text{-value})$\n$\\pi(s,a_i) = \\frac{e^{q(s,a_i)}}{\\sum_i e^{q(s,a)}} $\n$\\forall i = 1, 2, ... |A|$\nAnd for calculating entropy, logarithm with base of size of action space is used.\n$\\text{entropy}(\\pi(s)) = -\\sum_a p(a|s) \\log_{|A|} p(a|s)$\nwhere, p(als) is the probability of action a given state s.\nAction convergence: Action for a given state converges to a particular action as the RL algorithm\ntraining converges. This analysis looks at how decision making of the RL algorithm evolves through\ntraining process.\nProof. In eq.2 as error \u2192 0, $Q_{t+1}^\\pi(s_t, a_t) \\to Q_t^\\pi(s_t, a_t)$ which follows $\\pi_{t+1}(s,a) \\to \\pi_t(s,a)$.\nSame is applicable when using functions approximation, where $\\theta_{t+1} \\to \\theta_t$ would correspond to\n$Q_{t+1}^\\pi(s, a; \\theta_{t+1}) \\to Q_t^\\pi(s, a; \\theta_t)$.\nRLInspect is capable of evaluating this strategy for both continuous and discrete action space. This\nconvergence is evaluated by calculating distance between action vectors from consecutive training\nupdates\n$\\text{change}_{t+1} = d(a_t, a_{t+1})$\nwhere, change+1 is the change in training update t and t+1 and d(at, at+1) is the distance between\naction vector at t and t+1. Euclidean distance is used for continuous action space and Jaccard distance\nin case of discrete action space.\nPolicy Divergence: This analysis focuses on changes in policy distribution between two training\nupdates. To see how the policy is changing between two consecutive training updates, Jensen-Shannon\ndivergence (JSD) [34] with natural logarithm is calculated. JSD is chosen because firstly, it is well\nbehaved for very small probabilities. Secondly, JSD with natural log has an upper bound of natural\nlogarithm of 2 (ln(2)).\nAs the RL algorithm training converges, the divergence between the policy distribution of two\nconsecutive training updates converges to zero.\nProof. The proof for this continues from the proof for action convergence. As $Q_{t+1}^\\pi(s_t, a_t) \\to$\n$Q_t^\\pi(s_t, a_t)$, $D(Q_{t+1}^\\pi(s,a)||Q_t^\\pi(s,a)) \\to 0$. The policy distribution is calculated as defined in eq.3,\nwhich makes $D(\\pi_{t+1}(s,a)||\\pi_t(s,a)) \\to 0$.\nThis strategy can also identify the behaviour of action value update from eq.2. The divergence is\ngoverned by the change between action values from two consecutive training updates. From eq.2, it\ncan be noted that the change in action values is attributed to learning rate, a and error.\n$Q_{t+1}^\\pi(s_t, a_t) \u2013 Q_t^\\pi(s_t, a_t) = \\alpha (R_t + \\gamma \\max_a Q_t^\\pi(s_{t+1}, a) \u2013 Q_t^\\pi(s_t, a_t))$\nIf the divergence value is constantly high, then either of the two could contribute to this higher value.\nHowever, if the divergence sees a sudden spike and the learning rate, a, is constant throughout the\ntraining, then it is due to higher error.\nIn RLInspect, this policy is implemented for discrete action space and entropy for divergence is\ncalculated as defined in eq.4."}, {"title": "Agent Architecture Module", "content": "To monitor gradient-related issues that can hinder deep learning model performance, observing\nweight, bias, and gradients is crucial [35]. In deep reinforcement learning, where the agent's learning\nis guided by gradients, RLInspect's agent architecture module provides weight, bias, and gradient\ndistribution analysis to monitor model performance. For instance, the gradient distribution analysis,\nas illustrated in Figure 7, reveals the presence of the vanishing gradient problem for the cartpole\nexample. This issue was observed during Episode 666-672, which coincided with a performance\ndrop in the action module. By monitoring these parameters, users can make informed decisions to\nfine-tune their model and improve its overall performance."}, {"title": "Reward Module", "content": "The reward module in RLInspect provides a comprehensive understanding of the behaviour of the\nreward function. As a first step, the module removes outliers from the data, which makes its analysis,\nsuch as average and exponential moving average per episode, more robust. In addition to these two\nanalysis reward module offers two more analysis which include:\nVolatility: Measuring the volatility in rewards can help in understanding the stability and con-\nsistency of the reward signal in reinforcement learning. High volatility in rewards means that the\nreward signal is inconsistent and may lead to unstable behaviour in the agent. On the other hand, low\nvolatility indicates a stable and consistent reward signal, which can lead to better and more reliable\nbehaviour from the agent.\n$\\sigma = \\sqrt{E[(r \u2013 E[r])^2]}$\nwhere o is the volatility and r is the reward.\nAs seen in the agent architecture module of the cartpole example, the vanishing gradient issue caused\na significant drop in the model's performance around Episode 650-670. This drop in performance\nis also evident in the volatility of the rewards, as demonstrated in Figure 8. The graph depicts an\nincrease in reward volatility during this period, suggesting that the model is facing more uncertainty\nthan the previous few episodes."}, {"title": "Extendibility", "content": "RLInspect is developed considering its extendibility beyond the modules, analyses and data handler\nthat are available by default. The user can perform custom analysis by extending or implementing\ntheir own analyse and plot methods. If the user wants to collect data of their choosing and carry\nout analysis, the base Analyzer can be extended to create a custom analyser. Further, the user can\nRisk-Reward Ratio: This analysis is for evaluating an agent's risk aversion. This represents the\nbalance between the potential rewards and the risks it faces while taking actions. An agent with a\nhigher risk-reward ratio would takes high risk to achieve the same reward compared to an agent with\nlower risk-reward ratio.\n$CV = \\frac{\\sigma}{\\mu}$\nwhere o is the standard deviation of the rewards per episode and u is the mean of the rewards per\nepisode."}, {"title": "Limitations and Future works", "content": "Dimensionality reduction: The state module uses dimensionality reduction to embed high dimen-\nsional state onto two dimensional space for visualisation. RLInspect uses incremental PCA for\nembedding high dimentional state vectors to two dimensions. However, it suffers from its own\ndrawbacks [36\u201338]. Another candidate for dimensionality reduction was metric multidimensional\nscaling (mMDS) [39] but it was soon discarded as it becomes computationally expensive for large\ndata. Therefore, other dimensionality reduction algorithms like t-sne [40] needs to be explored as\nalternative solutions.\nAction confidence for continuous action space: Currently action module is able to calculate action\nconfidence and policy divergence only for discrete action space. As future work, these will be\nextended for continuous action space."}, {"title": "Conclusion", "content": "In this paper, we introduced RLInspect - an intteractive visual analytics tool, that provides compre-\\hensive understanding of RL training and can potentially help in improving RL algorithm. The tool\nlooks at four major components of RL training i.e. state, action, reward and agent architecture, to\nassess the overall behaviour of an RL algorithm. The modular design of RLInspect provides user an\noption to choose what analysis to run. The extendability aspect of RLInspect enables user to define\ntheir custom analysis and data handler which can further help in a much more detailed analysis of the\nRL algorithm."}]}