{"title": "KAA: KOLMOGOROV-ARNOLD ATTENTION FOR ENHANCING ATTENTIVE GRAPH NEURAL NETWORKS", "authors": ["Taoran Fang", "Chunping Wang", "Wei Chow", "Lei Chen", "Tianhong Gao", "Yihao Shang", "Yang Yang"], "abstract": "Graph neural networks (GNNs) with attention mechanisms, often referred to as attentive GNNs, have emerged as a prominent paradigm in advanced GNN models in recent years. However, our understanding of the critical process of scoring neighbor nodes remains limited, leading to the underperformance of many existing attentive GNNs. In this paper, we unify the scoring functions of current attentive GNNs and propose Kolmogorov-Arnold Attention (KAA), which integrates the Kolmogorov-Arnold Network (KAN) architecture into the scoring process. KAA enhances the performance of scoring functions across the board and can be applied to nearly all existing attentive GNNs. To compare the expressive power of KAA with other scoring functions, we introduce Maximum Ranking Distance (MRD) to quantitatively estimate their upper bounds in ranking errors for node importance. Our analysis reveals that, under limited parameters and constraints on width and depth, both linear transformation-based and MLP-based scoring functions exhibit finite expressive power. In contrast, our proposed KAA, even with a single-layer KAN parameterized by zero-order B-spline functions, demonstrates nearly infinite expressive power. Extensive experiments on both node-level and graph-level tasks using various backbone models show that KAA-enhanced scoring functions consistently outperform their original counterparts, achieving performance improvements of over 20% in some cases. Our code is available at https://github.com/LuckyTiger123/KAA.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph neural networks (GNNs) have achieved great success in graph data mining (Kipf & Welling, 2017; Hamilton et al., 2017; Xu et al., 2019; Wu et al., 2019) and are widely applied to various downstream tasks, such as node classification (Jiang et al., 2019a), link prediction (Kipf & Welling, 2016), vertex clustering (Ramaswamy et al., 2005), and recommendation systems (Ying et al., 2018). To further enhance the expressive power of GNNs, attentive GNNs (Sun et al., 2023; Chen et al., 2024) incorporate an attention mechanism (Vaswani et al., 2017) into GNN models, allowing them to adaptively learn the aggregation coefficients between a central node and its neighbors (Gilmer et al., 2017). This capability grants attentive GNNs greater expressive power and superior theoretical performance in various downstream tasks.\nDespite their theoretical advantages, the practical performance of existing attentive GNNs in real-world tasks often falls short of expectations. Even with larger parameter sizes and more flexible"}, {"title": "2 RELATED WORK", "content": "GNNs have garnered significant interest and been widely adopted over the past few years. However, most models (e.g., GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), and GIN (Xu et al., 2019)) assign equal importance to each neighbor of the central node, which limits their ability to capture the unique local structures of different nodes. In response to this limitation, GAT (Veli\u010dkovi\u0107 et al., 2018) was the first to introduce a simple layer with an attention mechanism to compute a weighted average of neighbors' representations. The strong performance of GAT across various downstream tasks has sparked widespread interest among researchers in attentive GNN models. Attentive GNNs can be broadly categorized into two main types: GAT-based models (Veli\u010dkovi\u0107 et al., 2018; Kim & Oh, 2021) and Graph Transformers (Nguyen et al., 2022; Zhang et al., 2020). GAT-based models assign varying weights to nodes during the feature aggregation process based on their respective influences. Since the introduction of GAT, numerous variants have emerged, such as C-GAT (Wang et al., 2019), GATv2 (Brody et al., 2021), and SuperGAT (Kim &\nOh, 2021). Additionally, many models (Jiang et al., 2019a; Cui et al., 2020; Yang et al., 2021; Lin et al., 2022; Zhang & Gao, 2021) have modified how GAT combines node pair representations and are also categorized as GAT-based models. Graph Transformers (Rong et al., 2020) are a class of models based on Transformers (Vaswani et al., 2017), which can directly learn higher-order graph representations. In recent years, Graph Transformers have rapidly advanced in the field of graph deep learning. These models (Dwivedi & Bresson, 2020; Kreuzer et al., 2021; Ying et al., 2021; Xia et al., 2021; Shi et al., 2020) typically employ a query-key-value structure similar to that of Transformers and demonstrate superior performance on graph-level tasks."}, {"title": "2.1 ATTENTIVE GRAPH NEURAL NETWORKS", "content": "GNNs have garnered significant interest and been widely adopted over the past few years. How-ever, most models (e.g., GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), andGIN (Xu et al., 2019)) assign equal importance to each neighbor of the central node, which limits their ability to capture the unique local structures of different nodes. In response to this limitation,GAT (Veli\u010dkovi\u0107 et al., 2018) was the first to introduce a simple layer with an attention mecha-nism to compute a weighted average of neighbors' representations. The strong performance of GATacross various downstream tasks has sparked widespread interest among researchers in attentiveGNN models. Attentive GNNs can be broadly categorized into two main types: GAT-based mod-els (Veli\u010dkovi\u0107 et al., 2018; Kim & Oh, 2021) and Graph Transformers (Nguyen et al., 2022; Zhanget al., 2020). GAT-based models assign varying weights to nodes during the feature aggregationprocess based on their respective influences. Since the introduction of GAT, numerous variants haveemerged, such as C-GAT (Wang et al., 2019), GATv2 (Brody et al., 2021), and SuperGAT (Kim &Oh, 2021). Additionally, many models (Jiang et al., 2019a; Cui et al., 2020; Yang et al., 2021; Linet al., 2022; Zhang & Gao, 2021) have modified how GAT combines node pair representations andare also categorized as GAT-based models. Graph Transformers (Rong et al., 2020) are a class ofmodels based on Transformers (Vaswani et al., 2017), which can directly learn higher-order graphrepresentations. In recent years, Graph Transformers have rapidly advanced in the field of graphdeep learning. These models (Dwivedi & Bresson, 2020; Kreuzer et al., 2021; Ying et al., 2021;Xia et al., 2021; Shi et al., 2020) typically employ a query-key-value structure similar to that ofTransformers and demonstrate superior performance on graph-level tasks."}, {"title": "2.2 KOLMOGOROV-ARNOLD NETWORKS", "content": "Kolmogorov-Arnold Network (KAN) (Liu et al., 2024b;a) is a novel neural network architecture inspired by the Kolmogorov-Arnold representation theorem (Kolmogorov, 1957; Braun & Griebel, 2009), designed as an alternative to MLP. Unlike MLP, KAN does not utilize linear weights. In-stead, each weight parameter between neurons is replaced by a univariate function parameterized as a spline. Compared to MLPs, KAN demonstrates enhanced generalization ability and interpretabil-ity (Liu et al., 2024b). However, achieving good performance with KAN in practical applications can be challenging (Altarabichi, 2024a; Le et al., 2024; Altarabichi, 2024b). Drawing inspiration from KAN's remarkable success in symbolic regression (Ranasinghe et al., 2024; Seguel et al., 2024; Shi et al., 2024) and PDE solving (Wang et al., 2024; Rigas et al., 2024; Wu et al., 2024a), we recognize KAN's significant potential for fitting mappings with multi-dimensional inputs and one-dimensional outputs. Our use of KAN in this work also reflects its empirical effectiveness in these contexts."}, {"title": "3 PRELIMINARY", "content": "Let $\\mathcal{G} = (\\mathcal{V},\\mathcal{E}) \\in \\mathcal{G}$ represents a graph, where $\\mathcal{V} = \\{v_1, v_2, ..., v_N \\}$, $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$ denote the node set and edge set respectively. The node features can be denoted as a matrix $X = \\{x_1, x_2,...,x_N\\} \\in \\mathbb{R}^{N \\times d}$, where $x_i \\in \\mathbb{R}^d$ is the feature of the node $v_i$, and $d$ is the dimensionality of original node features. $A \\in \\{0,1\\}^{N \\times N}$ denotes the adjacency matrix, where $A_{ij} = 1$ if $(v_i, v_j) \\in \\mathcal{E}$. Given a GNN model $f$, the node representation $h_i$ can be obtained layer by layer using the following expression:\n$h_i^{(l+1)} = \\text{UPDATE}^{(l)}(h_i^{(l)}, \\text{AGG}_{v_j \\in \\mathcal{N}(i)}(h_i^{(l)}, h_j^{(l)}))$", "subsections": [{"title": "Graph Data and Graph Neural Networks", "content": "Let $\\mathcal{G} = (\\mathcal{V},\\mathcal{E}) \\in \\mathcal{G}$ represents a graph, where $\\mathcal{V} = \\{v_1, v_2, ..., v_N \\}$, $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$ denote the node set and edge set respectively. The node features can be denoted as a matrix $X = \\{x_1, x_2,...,x_N\\} \\in \\mathbb{R}^{N \\times d}$, where $x_i \\in \\mathbb{R}^d$ is the feature of the node $v_i$, and $d$ is the dimensionality of original node features. $A \\in \\{0,1\\}^{N \\times N}$ denotes the adjacency matrix, where $A_{ij} = 1$ if $(v_i, v_j) \\in \\mathcal{E}$. Given a GNN model $f$, the node representation $h_i$ can be obtained layer by layer using the following expression:\n$h_i^{(l+1)} = \\text{UPDATE}^{(l)}(h_i^{(l)}, \\text{AGG}_{v_j \\in \\mathcal{N}(i)}(h_i^{(l)}, h_j^{(l)}))$"}, {"title": "Unified Scoring Functions in Attentive Graph Neural Networks", "content": "Attentive GNN models introduce an attention mechanism in the aggregation function AGG(\u00b7) to adaptively assign different weight coefficient \u03b1j to each neighboring node vj, which can be expressed as:\n$\\text{AGG}_{j \\in \\mathcal{N}(i)} (h_i, h_j) = \\sum_{j \\in \\mathcal{N}(i)} \\alpha_j h_j$\nwhere \u03b1j is obtained by normalizing the result of the scoring function. Here, the scoring functions(\u00b7) calculates the importance score of a neighboring node to the central node based on their respective representations. The scoring functions of existing attentive GNN models can be categorized into two types (Sun et al., 2023): GAT-based and Transformer-based. Their representative forms of scoring functions are as follows:\n$s(h_i, h_j) = \\text{LeakyReLU}(a^T \\cdot [Wh_i||Wh_j]) \\quad \\quad \\quad (GAT-based)$\n$s(h_i, h_j) = (W_qh_i)^T \\cdot W_kh_j \\quad \\quad \\quad (Transformer-based)$\n$\\alpha_j = \\frac{\\text{exp}(s(h_i, h_j))}{\\sum_{j' \\in \\mathcal{N}(i)} \\text{exp}(s(h_i, h_{j'}))}$     (Normalization)\nFor GAT-based scoring functions, many variants substitute the concatenation operation of the two representations with alternative operations, such as vector addition or subtraction. In con-trast, Transformer-based scoring functions often introduce additional scaling factors, such as\u221adimd\u221adimk. We find that both types of scoring functions can be unified into the following general form:\ns(hi, hj) = \u03a8 \u25e6 AF(hi, hj)\nwhere AF(\u00b7) : Rd \u00d7 Rd \u2192 Rd' is an alignment function without learnable parameters, such as concatenation or dot product. Its purpose is to combine the representations of the central node and the neighboring node. Additionally, \u03a8 : Rd' \u2192 R is the score mapping with learnable parameters, typically comprising several linear transformations and potentially some activation functions. Specifically, for the scoring function in Formula 3, the alignment function AF(\u00b7) is concatenation, while the score mapping \u03a8 consists of two consecutive linear transformations followed by an activa-tion function. For the scoring function in Formula 4, the alignment function AF(\u00b7) can be expressed as AF(hi, hj) = hj, and its score mapping is given by \u03a8 = (Wqhi)TWk."}]}, {"title": "4 METHODOLOGY", "content": "Inspired by the remarkable success of KAN in symbolic regression (Ranasinghe et al., 2024; Seguel et al., 2024; Shi et al., 2024), which typically involves multi-dimensional inputs and one-dimensional outputs, as well as in PDE solving (Wang et al., 2024; Rigas et al., 2024; Wu et al., 2024a), where common PDEs such as the heat equation or wave equation also feature multi-dimensional inputs and one-dimensional outputs, we find that KAN demonstrates significant potential for fitting mappings of the form f : Rn \u2192 R with multi-dimensional inputs and one-dimensional outputs."}, {"title": "4.1 KOLMOGOROV-ARNOLD ATTENTION", "content": "According to Formula 6, the score mapping \u03a8 also conforms to this form. Therefore, we propose Kolmogorov-Arnold Attention (KAA), which replaces \u03a8 in the original scoring function with KAN, expressed as:\n$s(h_i, h_j) = \\text{KAN} \\circ AF(h_i, h_j)$\nAfter obtaining the scores for node pairs, we compute the specific weight coefficients according to Formula 5 and perform the GNN aggregation as outlined in Formula 2. This design is highly flexible, allowing KAA to be applied to nearly all existing attentive GNN models."}, {"title": "4.2 COMPARISON OF EXPRESSIVE POWER", "content": "In this section, we analyze the expressive power of different scoring functions. The primary purpose of designing the scoring function is to enable the model to adaptively assign varying aggregation coefficients to different nodes. This means that the scoring function should be capable of ranking the importance of neighboring nodes, allowing the central node to receive more valuable information. According to the unified form of scoring functions presented in Formula 6, AF(hi, hj) is derived from a predefined alignment function AF(\u00b7) and the inherent inputs (hi and hj), which cannot be modified during training. Consequently, the expressive power of the scoring function hinges on whether the learnable score mapping \u03a8 can effectively map the various AF(hi, hj) values to any desired importance ranking. We conduct a quantitative analysis and comparison of the expressive power of different forms of scoring functions. First, we examine the limitations of existing standards used to evaluate the expressive power of score functions. Next, we introduce a more comprehensive evaluation metric: Maximum Ranking Distance. Finally, we quantitatively compare the maximum"}, {"title": "4.2.1 LIMITATION OF EXISTING MEASUREMENT", "content": "Many existing works (Qiu et al., 2018; Brody et al., 2021) have explored the expressive power of scoring functions in attentive GNNs. Brody et al. (2021) initiated this line of inquiry by analyzing the scoring function of GAT and proposed two standards for evaluating the expressive power of scoring functions: static and dynamic attention. Specifically, if the scoring function assigns the highest score to the same neighboring node (key) for all central nodes (queries), it is said to compute static attention. On the other hand, if the scoring function can assign different neighboring nodes (keys) the highest score for different central nodes (queries), it computes dynamic attention. More formal and detailed definitions of these two attention types are provided in Appendix A.2. Clearly, scoring functions that compute dynamic attention have stronger expressive power. However, applying this standard to evaluate the scoring functions of various existing attentive GNNs presents several chal-lenges. First, most existing scoring functions already compute dynamic attention, making it difficult to differentiate their expressive power using this criterion. Second, scoring functions that compute static attention can easily be adjusted to compute dynamic attention. For example, the GAT scoring function in Formula 3 initially computes static attention, but by modifying it to the following form, it can be converted to compute non-static attention:\n$s(h_i, h_j) = -\\text{LeakyReLU}(\\text{Abs}(a^T \\cdot [Wh_i||Wh_j]))$"}, {"title": "4.2.2 DESIGNING COMPREHENSIVE MEASUREMENT", "content": "We observe that existing measurements are too coarse because they focus solely on the neighbor node with the highest score. In reality, the scoring function assigns scores to all neighboring nodes, forming an importance ranking of the neighbors, which we define as follows:\nGiven a scoring function s(\u00b7), a central node with representa-tion hi and all its neighbor nodes with representations {hj|j \u2208 N(i)}, an importance ranking \u03c3 is a permutation of |N(i)|, where \u03c3 is a bijective mapping \u03c3 : {1, ..., |N(i)|} \u2192 {1, ..., |N(i)|} that satisfies:\ns(hi, h\u03c3(1)) \u2264 s(hi, h\u03c3(2)) \u2264 \u00b7\u00b7\u00b7 \u2264 s(hi, h\u03c3(|N(i)|))\nIn contrast to static and dynamic attention, which only focus on identifying the most important neighbor, importance ranking captures the relative importance of all neighboring nodes, offering a more comprehensive evaluation. In practice, scoring functions may not always achieve the optimal importance ranking under ideal conditions; instead, they approximate it as closely as possible. To quantitatively assess the difference between two rankings, we define the ranking distance as follows:\nGiven two rankings, \u03c31 and \u03c32, of N nodes, the ranking distanceRD between them can be calculated using the following formula:\n$\\text{RD}(\\sigma_1, \\sigma_2) = \\sqrt{\\sum_i (\\sigma_1^{-1}(i) - \\sigma_2^{-1}(i))^2}$"}, {"title": "4.2.3 THEORETICAL COMPARISON", "content": "In this section, we delve into the analysis of the MRD for various scoring functions, specifically ex-amining three types: linear transformation-based attention, MLP-based attention, and our proposed KAA. These correspond to cases where the score mapping in Formula 6 is a linear transformation, MLP, and KAN, respectively. Without loss of generality, we assume that the selected central node is connected to all other nodes in the graph (as many graph transformers do), and that AF(hi, hj) \u2208 Rd. We denote the alignment matrix of all AF(hi, hj) as P \u2208 RN\u00d7d, where N is the number of nodes. For analytical simplification, we assume P is derived from the first d columns of a full-rank circulant matrix C\u2208 RN\u00d7N, with N = d\u00b2 (i.e., N \u226b d). A more detailed and specific elaboration can be found in Appendix A.4."}, {"title": "Linear Transformation-Based Attention", "content": "This type of scoring function is the most common, and the majority of existing attentive GNNs (Sun et al., 2023) can be classified under this category. Many practical implementations of scoring functions employ multiple consecutive learnable linear transformations (as seen in Formulas 3 and 4), but in theory, this does not enhance the expressive power compared to using a single fully learnable linear transformation, as multiple transformations are equivalent to a single transformation equal to their product. Furthermore, scoring functions like the one in Formula 3 often include a non-linear activation function. However, since most common non-linear activation functions are monotonic (e.g., ReLU family and tanh), they do not alter the ranking of the scores. Therefore, we calculate the MRD for a single linear transformation as:\nGiven the alignment matirx P \u2208RN\u00d7d, for a scoring function in the form of s(hi, hj) = W \u00b7 AF(hi, hj), where W \u2208 Rd\u00d71, itsMRD satisfies the following inequality:\n$\\text{MRD}(S_{LT}, P) \\geq \\frac{1}{\\sqrt{12}} (N^3 - N - d^3 + 3d^2 - 2d)$"}, {"title": "MLP-Based Attention", "content": "To address the limitations of linear transformations in scoring functions, Brody et al. (2021) utilized a multi-layer perceptron (MLP) as the score mapping. While an MLP is a universal approximator in ideal conditions (Cybenko, 1989; Hornik, 1991), its expressive power is often constrained by its limited width and depth in practical applications. Increasing the width and depth excessively can lead to challenges in model convergence and may result in significant overfitting (Oyedotun et al., 2017). To ensure consistency with the MLP size used by Brody et al. (2021), we calculate the MRD for the scoring function where the score mapping consists of a two-layer equal-width MLP:\nGiven the alignment matirx P\u2208 RNxd, for ascoring function in the form of s(hi, hj) = W2 \u00b7 (ReLU(W1\u00b7 AF(hi, hj))), where W1 \u2208 Rd\u00d7d and W2 \u2208 Rd\u00d71, its MRD satisfies the following inequality:\n$\\frac{1}{\\sqrt{12}} (N^3 - N - \\lambda) \\geq \\text{MRD}(S_{MLP}, P) \\geq \\frac{1}{\\sqrt{12}} ((N - d)^3 - (N - d) - \\lambda)$"}, {"title": "Kolmogorov-Arnold Attention", "content": "Finally, we calculate the MRD of our proposed KAA. The score mapping in KAA is KAN, which possesses a representation theorem similar to that of MLP. To ensure a fair comparison, we utilize a single-layer KAN with a comparable number of parameters to those in Proposition 2 as the score mapping:"}, {"title": "5 EXPERIMENT", "content": "In this section, we systematically evaluate the effectiveness of KAA across various tasks, including node-level tasks such as node classification and link prediction, as well as graph-level tasks like graph classification and graph regression."}, {"title": "5.1 EXPERIMENT SETUP", "content": "Our proposed KAA can be applied to both GAT-based scoring functions, as shown in Formula 3, and Transformer-based scoring functions, as shown in Formula 4. Consequently, we select three classical GAT-based attentive GNN models with varying alignment func-tions as our backbone models: GAT (Veli\u010dkovi\u0107 et al., 2018), GLCN (Jiang et al., 2019b), and CFGAT (Cui et al., 2020). Additionally, we choose two Transformer-based models with distinct scoring functions as our backbone models: GT (Dwivedi & Bresson, 2020) and SAN (Kreuzer et al., 2021). The specific forms of the scoring functions are detailed in Table 1."}, {"title": "5.2 PERFORMANCE ON NODE-LEVEL TASKS", "content": "We conduct node classification and link prediction tasks to validate the effectiveness of KAA in node-level applications. For the node classification task, we select four citation network datasets (Sen et al., 2008; Hu et al., 2020) of varying scales: Cora, CiteSeer, PubMed, and ogbn-arxiv, along with two product network datasets (Shchur et al., 2018), Amazon-Computers and Amazon-Photo. The objective for the citation networks is to determine the research area of the papers, while the product networks involve categorizing products. For the link prediction task, we select three citation network datasets: Cora, CiteSeer, and PubMed, to predict whether an edge exists between pairs of nodes. More details and statistical information about these datasets can be found in Appendix B.1. Additionally, for all tasks, we employ three classical GNN models: GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), and GIN (Xu et al., 2019), as baselines."}, {"title": "5.3 PERFORMANCE ON GRAPH-LEVEL TASKS", "content": "We conduct graph classification and graph regression tasks to validate the effectiveness of KAA in graph-level applications. For the graph classification task, we select four datasets (Ivanov et al., 2019; Zitnik & Leskovec, 2017) from bioinformatics and cheminformatics: PPI, MUTAG, ENZYMES, and PROTEINS. The downstream tasks for these datasets involve predict-ing the properties of proteins or molecules. For the graph regression tasks, we select two datasets: ZINC (G\u00f3mez-Bombarelli et al., 2018) and QM9 (Wu et al., 2018). Both ZINC and QM9 are molec-ular datasets. The task for ZINC is to predict the constrained solubility of molecules, while QM9 involves regression tasks for 19 different molecular properties. More details and statistical informa-tion about these datasets can be found in Appendix B.1. We also employ GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), and GIN (Xu et al., 2019) as baseline models."}, {"title": "6 CONCLUSION AND OUTLOOK", "content": "In our paper, we introduce Kolmogorov-Arnold Attention (KAA), which integrates KAN into the scoring functions of existing attentive GNN models. Through thorough theoretical and experimen-tal validation, we demonstrate that KAN is highly effective for the scoring process, significantly enhancing the performance of KAA. Furthermore, our successful implementation of KAN in atten-tive GNNs may inspire advancements in KAN in other domains. Our comparative analysis of the theoretical expressive power of KAN-based learners versus MLP-based learners under constrained parameters offers valuable insights for future research."}, {"title": "A EXTRA MATERIALS FOR SECTION 4", "content": "In fact, KAA is not at odds with advanced attentive GNN techniques and does not lose effectiveness due to their strong performance. This is because existing methods focus more on determining what constitutes important attention information, and advanced methods often excel at capturing such crucial information through the attention mechanism. KAA, on the other hand, enhances the actual construction process of the attention mechanism. This means that regardless of the type of attention distribution being learned, KAA can improve the success rate of accurately modeling that distri-bution. Therefore, despite the significant advancements in existing attentive GNNS, KAA can still provide performance improvements, as it represents an enhancement from a different perspective."}, {"title": "A.1 CONNECTION BETWEEN KAA AND EXISTING ATTENTIVE TECHNIQUES", "content": "In fact, KAA is not at odds with advanced attentive GNN techniques and does not lose effectiveness due to their strong performance. This is because existing methods focus more on determining what constitutes important attention information, and advanced methods often excel at capturing such crucial information through the attention mechanism. KAA, on the other hand, enhances the actual construction process of the attention mechanism. This means that regardless of the type of attention distribution being learned, KAA can improve the success rate of accurately modeling that distribution. Therefore, despite the significant advancements in existing attentive GNNS, KAA can still provide performance improvements, as it represents an enhancement from a different perspective."}, {"title": "A.2 DETAILS OF EXISTING MEASUREMENT", "content": "A pioneering work (Brody et al., 2021) defines static and dynamic attention. Here, we excerpt their formal definitions from the original text to understand the differences between static and dynamic attention and our defined MRD."}, {"title": "(Static Attention)", "content": "A (possibly infinite) family of scoring functions $F\\subseteq (\\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R})$ com-putes static scoring for a given set of key vectors $K= \\{k_1, ..., k_n\\} \\subset \\mathbb{R}^d$ and query vectors $Q= \\{q_1, ..., q_m \\} \\subset \\mathbb{R}^d$, if for every $f \\in F$ there exists a \u201chighest scoring\u201d key $j_f \\in [n]$ such that for every query $i \\in [m]$ and key $j \\in [n]$ it holds that $f (q_i, k_{j_f}) \\geq f (q_i, kj)$. We say that a family of attention functions computes static attention given K and Q, if its scoring function computes static scoring, possibly followed by monotonic normalization such as softmax."}, {"title": "(Dynamic Attention)", "content": "A (possibly infinite) family of scoring functions $F \\subset (\\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R})$ com-putes dynamic scoring for a given set of key vectors $K = \\{k_1, ..., k_n\\} \\subset \\mathbb{R}^d$ and query vectors $Q = \\{q_1, ..., q_m\\} \\subset \\mathbb{R}^d$, if for any mapping $\\phi: [m] \\rightarrow [n]$ there exists $f \\in F$ such that for any query $i \\in [m]$ and any key $j \\neq \\phi(i) \\in [n]$: $f (q_i, k_{\\phi(i)}) > f (q_i, kj)$. We say that a family of atten-tion functions computes dynamic attention for K and Q, if its scoring function computes dynamic scoring, possibly followed by monotonic normalization such as softmax."}, {"title": "A.3 TRANSFORMING GAT INTO NON-STATIC ATTENTION", "content": "According to Formula 10, we add a negative sign and an absolute value function to the original scoring function of GAT, which can be expanded as follows:\n$s(h_i, h_j) = -\\text{LeakyReLU}(\\text{Abs}(a^T \\cdot [Wh_i||Wh_j]))$\n$\\quad\\quad\\quad\\quad\\quad\\quad = -\\text{LeakyReLU}(|b_1h_i + b_2h_j|)$"}, {"title": "A.4 ASSUMPTIONS AND SETUP FOR THEORETICAL ANALYSIS", "content": "In this section, we elaborate on the theoretical setup and assumptions, as well as some simplifica-tions. Without loss of generality, we assume that the selected central node is connected to all other nodes in the graph (as most graph transformers compute the relationships between all nodes in this way). Additionally, for clarity in notation, we assume that AF(hi, hj) \u2208 Rd, and denote the align-ment matrix composed of all AF(hi, hj) \u2208 Rd values as P \u2208 RN\u00d7d, where N is the number of"}, {"title": "A.5 DETAILS FOR MRD OF LINEAR TRANSFORMATION-BASED ATTENTION", "content": "In this section, we provide a detailed proof of Proposition 1.\nGiven the alignment matirx P \u2208RN\u00d7d, for a scoring function in the form of s(hi, hj) = W \u00b7 AF(hi, hj), where W \u2208 Rd\u00d71, itsMRD satisfies the following inequality:\n$\\text{MRD}(S_{LT}, P) \\geq \\frac{1}{\\sqrt{12}} (N^3 - N  - d^3 + 3d^2 - 2d)$"}, {"title": "A.6 DETAILS FOR MRD OF MLP-BASED ATTENTION", "content": "In this section, we provide a detailed proof of Proposition 2."}, {"title": "A.7 DETAILS FOR MRD OF KOLMOGOROV-ARNOLD ATTENTION", "content": "In this section, we provide a detailed proof of Proposition 3."}]}