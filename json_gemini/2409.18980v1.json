{"title": "IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web", "authors": ["Hongcheng Guo", "Wei Zhang", "Junhao Chen", "Yaonan Gu", "Jian Yang", "Junjia Du", "Binyuan Hui", "Tianyu Liu", "Jianxin Ma", "Chang Zhou", "Zhoujun Li"], "abstract": "Recently advancements in large multimodal models have led to significant strides in image comprehension capabilities. Despite these advancements, there is a lack of the robust benchmark specifically for assessing the Image-to-Web conversion proficiency of these large models. Primarily, it is essential to ensure the integrity of the web elements generated. These elements comprise visible and invisible categories. Previous evaluation methods (e.g., BLEU) are notably susceptible to significant alterations due to the presence of invisible elements in Web. Furthermore, it is crucial to measure the layout information of web pages, referring to the positional relationships between elements, which is overlooked by previous work. To address challenges, we have curated and aligned a benchmark of images and corresponding web codes (IW-BENCH). Specifically, we propose the Element Accuracy, which tests the completeness of the elements by parsing the Document Object Model (DOM) tree. Layout Accuracy is also proposed to analyze the positional relationships of elements by converting DOM tree into a common subsequence. Besides, we design a five-hop multimodal Chain-of-Thought Prompting for better performance, which contains five hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout. 4) Inferring Web code. 5) Reflection. Our benchmark comprises 1200 pairs of images and web codes with varying levels of difficulty. We have conducted extensive experiments on existing large multimodal models, offering insights into their performance and areas for improvement in image-to-web domain.", "sections": [{"title": "1 Introduction", "content": "The development of large multimodal models has emerged as a new trend, starting with GPT-4 (OpenAI, 2023b). An increasing number of multimodal models have been introduced (Anil et al., 2023; Liu et al., 2023a; Zhu et al., 2023a; Bai et al., 2023; Dai et al., 2023), extending the powerful comprehension capabilities of large language models to multiple tasks (Lu et al., 2018; Yang et al., 2023; Sidorov et al., 2020; Guo et al., 2022; Yang et al., 2024). Recently, the task of converting images into web code (Lauren\u00e7on et al., 2024; Patil et al., 2020; Beltramelli, 2017) has garnered significant attention due to its impressive performance. In Figure 1, this task tests the synthesis abilities of large multimodal models, encompassing the fine-grained recognition of components within images, the assessment of the relative positioning of elements in webpages, and the capability to generate code. The prowess of large models in generating front-end code has been a source of astonishment, yet, notably, there has been almost no evaluation benchmark related to large multimodal models for this domain.\nConstructing a benchmark for appraising Image-to-Web tasks is markedly more intricate than the conventional programming challenges (Davody et al.). This heightens complexity stems from the multifaceted nature of web code, encompassing HTML, CSS, and JavaScript. It demands scrutiny not only of functional correctness but also of visual elements like layout, design, and user interaction. Predominant evaluation frameworks for the Image-to-Code task (Patil et al., 2020; Bhambure et al.) often hinge on similarity metrics such as BLEU. Nevertheless, these metrics encounter substantive limitations in the context of web evaluation. The intricate web of JavaScript functionalities and CSS styling options means that disparate web codes can produce indistinguishable visual outcomes. Consequently, such conventional metrics falter in precisely assessing the completeness of elements depicted in images and their layout intricacies. A further complication arises from non-visible elements, introducing a fragility in these evaluation methods. Varied development ways can lead to substantial discrepancies in these non-visible components, even among web pages that appear visually identical. Recent WebSight (Lauren\u00e7on et al., 2024) is finetuned on numerous image-code pairs, however it has not conducted performance evaluations. These challenges underscore the imperative for a more sophisticated and comprehensive benchmark.\nTo address the challenges, we introduce IW-BENCH to assess the capabilities of large multimodal models in the Image-to-Web. Firstly, we design a data construction pipeline, comprising 1200 entries of three difficulty levels: Level I (simple), Level II (medium), and Level III (complex). For metric innovation, we propose Element Accuracy and Layout Accuracy. Specifically, to effectively measure the completeness of web elements, the element accuracy tests the completeness of the elements from six dimensions (Tag, Text Content, Attribute, Style, JavaScript, Children) by parsing the Document Object Model (DOM) tree. To analyze the relative positional relationships of elements, we first traverse the DOM Tree in the same manner (in-order traversal) to obtain a list of elements, then we calculate the overlap between the common sub-sequence and the ground truth for the layout accuracy. Moreover, we establish a effective five-hop multimodal chain-of-thought method to enhance the model performance, including five specialized hops: SoM prompt injection, Inferring Elements, Inferring Layout, Inferring Web code, and Reflection.\nTo summarize:\n\u2022 Benchmark Construction. We have meticulously curated 1200 three challenging levels of image-web pairs for our benchmark dataset. This dataset serves as a rigorous benchmark for assessing the capabilities of large multimodal models in the task of converting images to web code.\n\u2022 Metric Innovation. We introduce innovative metrics to evaluate web elements and layout information accurately. Specifically, we have developed the Element Accuracy metric to assess the fidelity of web elements and the Layout Accuracy metric to evaluate the precision of layout information.\n\u2022 Multimodal Chain-of-Thought. The five-hop multimodal Chain-of-Thought method is proposed by us, significantly enhancing the performance in image-to-web domain.\n\u2022 Extensive Evaluation. A substantial evaluation of large multimodal models has been conducted, showcasing their capabilities and limitations in our context. Besides, ablation on our five-hop chain-of-thought demonstrates the effect of our method."}, {"title": "2 Related work", "content": "Multimodal benchmarks for large multimodal models (Bitton et al., 2023; Yu et al., 2023; Liu et al., 2023c; Xu et al., 2023b; Shao et al., 2023) have assess the instruction-following and reasoning capabilities. As these foundation models become more relevant to real-world applications, unlike prior work, we propose IW-BENCH to benchmark their generation capabilities of the hot Image-to-Web area on a diverse set of visual contexts. To the best of our knowledge, WebSight (Lauren\u00e7on et al., 2024) is the only relevant benchmark. However, it is more like a fine-tuning dataset for large multimodal models rather than a benchmark for evaluation. More related work on image-to-web, large language models, and chain-of-thought methods are in Appendix C."}, {"title": "3 IW-BENCH", "content": "Overview: We design a pipeline for the construction of IW-BENCH in Figure 3. Specifically, to obtain complex-level data, we crawl publicly available web pages and perform de-identification and simplification. For simple and medium level data, we prompt GPT-4 (OpenAI, 2023a) to generate them. Finally, we filter out some low-quality samples from the obtained image-code pairs.  We present examples in Appendix I."}, {"title": "3.1 Data Collection", "content": "We initially intend to assign the task of determining complexity to human workers and exclusively use real web. However, tests reveal that websites in real-world scenarios are too complex, resulting in uniformly poor evaluation with minimal differentiation between models. To better quantify differences among models, we opt to create simpler data by prompting GPT4 (\u041e\u0440\u0435nAI, 2023a) following the previous work (Wang et al., 2023c; Xu et al., 2023a). The process begins with GPT-4 randomly selecting a relevant domain in Appendix E.1. Based on the desired complexity, GPT-4 then chooses specific elements. Finally, it selects the JavaScript events in Apppendix D.1 to be integrated into the web page. In IW-BENCH, 18% of the data comes from real web pages, covering a wide range of types, including public repositories, open-source projects, and websites representing various content categories. The data collection process strictly follows the robots.txt protocol and complies with website policies. To extract images from web code, we use Playwright 1."}, {"title": "3.2 Data Processing", "content": "The processing of real web page data includes two parts: de-identification and simplification. For de-identification, we replace the image components and modify the logos within our dataset to avoid copyright or other legal concerns. More details are in Appendix A. For simplification, this involves the removal of non-essential invisible elements such as whitespace, code comments, and styles. We also streamline HTML by condensing verbose code and transferring inline styles to external stylesheets, which contributes to a more uniform and computationally efficient dataset. Further, we standardize visible elements like images and text by replacing them with placeholders and normalizing stylistic features.\nWe carefully validate the simplifications to ensure that the functionality and layout of the original webpage remain unaffected. Specifically, we start by launching a headless browser using Playwright and capturing the initial screenshot of the webpage. After that, we employ BeautifulSoup to parse the HTML document. Subsequently, we traverse through all HTML elements, with the exception of fundamental tags such as html, head, body, title, and meta, as these are typically crucial components of the structure. We initiate the removal process and then scrutinize whether there is any discernible alteration in the visual presentation. To determine this, we render the modified HTML content and compare it with the original one. If the new screenshot matches the original one, we retain the deleted element and document it accordingly."}, {"title": "3.3 Expert Review", "content": "Establishing IW-BENCH requires a significant amount of manpower. In the initial collection phase,"}, {"title": "3.4 Metric Design", "content": "We propose Element Accuracy and Layout Accuracy. To effectively compare visible and invisible elements, the initial step involves constructing a Document Object Model (DOM) tree. After the DOM tree is constructed, we employ a consistent traversal strategy to inspect and analyze these elements. During the traversal, we identify each element and collect detailed information about them, such as their tag types, classes, IDs, style attributes, and their hierarchical relationships. This allows us to comprehensively compare visible elements (like text, images, and buttons) with invisible elements (such as script tags and metadata).\nFor element accuracy, we gain the average score from six perspectives: Tag, Text Content, Attribute, Style, JavaScript, Children.. Specifically, when we compare tag, we only check if the tag names of two elements are identical. If the tag names are an exact match (e.g., both are < div > or < p >), the tag_score is 1, otherwise, the tag_score is 0. For the score of text_content, we calculate the content similarity of two elements with SequenceMatcher 2. To gain the attribute_score, we first define how different HTML tags should compare their attributes, if the corresponding attributes of two elements are an exact match, they score higher.\nFor style_score, we filter out key style properties (e.g., color, font-size) and ignore default values. Then we compare these filtered style properties, considering them a match if both elements have the same value for the same property. More details of style are in Appendix H. After that, JavaScript event bindings on two elements are compared, we consider specific events (e.g., onclick, onload) and checks if each event is bound to both elements compared to ground truth. The total events are listed in Appendix E. The final score is the number of matching events divided by the total number of events. Let S be the score from perspective i, for i \u2208 {Tag, Text Content, Attribute, Style, JavaScript, Children}. The set of test elements is Etest, the set of label elements is Elabel For each element Ej in the testset Etest, the average score calculated as:\nEj = 1/n * \u2211_{i=1}^{n} Si   (1)\nGiven the threshold T. The final element accuracy,\nEA, is then:\nEA =  (\u2211jEtest (Ej > \u03a4)) / |Elabel|   (2)\nFor layout accuracy, we apply the Longest Common Subsequence (LCS) to derive a web elements into a structured list. Subsequently,list that encapsulates the consistent elements across the layouts. To quantify the similarity index, we divide the length of this common subsequence by the total length of the element list.\nLA = LCS(L1, L2) / Len(L1)   (3)\nwhere LA is the layout accuracy, let L1 be the list of web elements from the label HTML, and L2 be the list from the generated Web code."}, {"title": "4 Five-hop multimodal Chain-of-Thought", "content": "In this section, we design a five-hop multimodal Chain-of-Thought Prompting for image-to-html task in Figure 4, which contains five hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout. 4)Inferring Web code. 5) Reflection."}, {"title": "4.1 SoM Prompt Injection", "content": "Motivated by previous work (Yang et al., 2023), we aim to enhance images with Scenes of Meaning (SoM) as shown in Figure 5. We first identify key elements to understand the core theme. Using specialized tools, we seamlessly integrate SoM cues like text labels, arrows, and highlights into the images. These cues guide multimodal models to focus on important elements, improving image comprehension and precision in recognizing fine details."}, {"title": "4.2 Inferring Elements", "content": "After integrating the SoM prompts, we can more precisely infer the elements within the image. This enhanced inference covers various aspects, including categorizing elements based on their types and understanding their functional roles within the image. SoM prompts provide significant assistance, enabling us to delve deeper into understanding both visible and invisible elements in the image, as well as their contributions to the overall meaning."}, {"title": "4.3 Inferring Layout", "content": "In the third hop, we prompt the models to infer the layout information in the image. This includes a detailed analysis of the spatial arrangement and organization of the web elements depicted in the image. We examine factors such as the alignment, proximity, and hierarchy of elements to understand their functional relationships and visual impact. This phase is crucial for understanding the overall structure and flow of the user interface, enabling models to recognize how users might interact with and navigate through the web content. By accurately decoding the layout information, large multimodal models can gain insights into the applied design principles and subsequently refine their generated content."}, {"title": "4.4 Inferring Web Code", "content": "Once we have a detailed understanding of the interface, we can then generate HTML code based on the provided instructions. The HTML code will reflect the layout, style, and interactive features of the interface, ensuring that the final web page visually and functionally aligns with the analyzed design."}, {"title": "4.5 Reflection", "content": "In reflection, we first re-render the generated HTML code and produce screenshots. Then, we require the large multimodal models to compare the element completeness and layout information of two screenshots from an image perspective. Following this comparison, we generate analysis content based on the results and feed this content back into the previous hop, assisting in the generation of code once again. This is an iterative process, and we control the number of iterations with a hyperparameter N. This method ensures that the generated HTML code is not only accurate in terms of code but also visually consistent with the original design, thereby enhancing the quality of the final result."}, {"title": "5 Evaluation", "content": "We select recent multimodal language models as baselines. GPT4V (gpt-4-1106-preview) (OpenAI, 2023b), Qwen-VL-Chat,Qwen-VL-Plus, Qwen-VL-Max (Bai et al., 2023), miniGPT-4-LLaMA-2-7B (Zhu et al., 2023a), LLaVA-LLaMA-2-13B (Liu et al., 2023a), mPLUG-OWL2 (Ye et al., 2023), LLaMA-Adapter-V2-7B (Gao et al., 2023), WebSight (Lauren\u00e7on et al., 2024), Gemini Pro (Anil et al., 2023), Claude3 Opus 3. All experiments are conducted with 16 NVIDIA A100 GPUs (80G). The T in our experiment is 0.9. N is default set to be 3 and we choose the best result. We do not conduct Five-hop MCOT on Websight model as it fails to accommodate simultaneous input of images and text."}, {"title": "5.2 Experimental Results", "content": "Our results in Table 5 are divided into two main sections: the first section outlines the results of web code directly generated by the model while the second section details outcomes following the five-hop multi-modal chain-of-thought technique. We evaluate the efficacy of various models by examining their accuracy in identifying elements and arranging layouts across three distinct levels of complexity: simple, medium, and complex."}, {"title": "5.2.1 Overall Performance Comparison", "content": "WebSight stands out with the highest averages in both element accuracy at 48.9% and layout accuracy at 47.9%, which demonstrates the effect of the supervised finetuning. GP4V with Five-hop MCOT also shows significant improvement, with average results of 45.8% for element accuracy and 44.5% for layout accuracy, compared to the 30.4% and 29.4% without enhancement respectively. The model with the lowest overall performance is LLaMA-Adapter-V2-7B. Models with the same size indeed show varying performances, but an examination of a series of models, such as the Qwen family, reveals that the size of the models significantly affects their effectiveness. This unequivocally proves the effectiveness of our benchmark."}, {"title": "5.2.2 Performance by Complexity Level", "content": "Through comparison, we gain the overall results. As complexity increases, the performance of all models decreases. Across all levels, WebSight emerges as the strongest model, consistently leading in both Element and Layout Accuracy. Gemini Pro and Qwen-VL-Max show good performance but with a greater drop as complexity increases. Most models like LLaVA-LLaMA-2-13B exhibit a more substantial decrease in performance as complexity increases, which may suggest that these models are better suited to simple tasks."}, {"title": "5.2.3 Element vs. Layout Accuracy", "content": "On average, across all complexity levels, element accuracy tends to be slightly higher than layout accuracy. This suggests that models are better at identifying and understanding individual elements than how those elements are arranged. The gap between two metrics tends to widen as the complexity of the task increases, which indicates that as tasks become more complex, it becomes more challenging for the models to understand of the layout information. WebSight is the top performer in balancing both element and layout accuracy."}, {"title": "5.2.4 Impact of Enhancements", "content": "Models with Five-hop MCoT all have an improvement than without Five-hop MCoT. It means Five-hop MCoT enhancement has a pronounced and positive impact on both element accuracy and layout accuracy across all levels of complexity, but the magnitude of the impact varies. The impact is most notable in the 'Simple' and 'Medium' complexity levels."}, {"title": "5.2.5 Human-in-the-loop Evaluation", "content": "First, we invite 20 front-end technology professionals to rate 100 web pages generated by different models. We meticulously design a questionnaire with multiple assessment dimensions. We compare the differences between the ranking based on our metrics and the ranking according to the average scores from the human evaluation panel. The Pearson coefficient between two rankings is 0.8 and P-Value is 0.104. Generally, when the coefficient is greater than 0.7, it is usually considered to be a strong correlation."}, {"title": "6 Ablation", "content": "In this section, we conduct the ablation on our Five-hop Multimodal Chain-of-Thought method, all experiments are conducted on GPT4V."}, {"title": "6.1 Ablation on SoM prompt injection", "content": "In Table 7, the results clearly demonstrate the significant impact of the SoM prompt injection module on the performance of GPT4V. When we conduct ablation by removing the SoM module from the architecture, we observe a substantial drop in performance. Specifically, GPT4V without the SoM module experiences a decrease of 4.2% and 5.1% on average element accuracy and layout accuracy, underscoring the crucial role that the SoM prompt injection module plays in enhancing the ability of models. As the complexity of the tasks increases, the accuracy of both models declines. The models perform better on simple and medium level."}, {"title": "6.2 Ablation on Reflection", "content": "We conduct ablation experiments on the reflection module, and obtain the accuracy covering different times(N) of reflection in Figure 6. Element accuracy (red) shows a significant increase from N=0 to N=2, then stabilizes. Layout accuracy (blue) exhibits minimal variation across the range of N values, with an overall stable trend. For most of the range, element accuracy are higher than those of layout accuracy. We can see that element accuracy and layout accuracy both improve significantly with an increase in Reflection iterations and then levels off."}, {"title": "7 Visualization", "content": "In this section, we present web page renderings after various reflections, highlighting progressive enhancements. In Figure 7, the original design features a prominent title, three subheadings\u2014Business Planning, Cards, Consulting\u2014and the corresponding text. The navigation bar includes Home, Blog, Properties, About, GitHub, and language options (EN, ES). The first reflection accurately captures the structure of navigation bar, though the text does not completely match. The alignment of the main title and subheadings requires adjustment. By the second reflection, text fidelity improves, aligning closer with the original content, and the navigation bar layout becomes more compact. The third reflection brings further precision to text and tightens the overall page layout, aligning visual elements more accurately and reducing layout errors. The use of color and contrast also more closely mirrors the original, enhancing visual consistency."}, {"title": "8 Conclusion", "content": "In this paper, we introduce IW-BENCH. Two evaluation metrics are proposed: Element Accuracy to assess the completeness of elements, and Layout Accuracy to evaluate the positional relationships of elements. Furthermore, we outline a five-hop Multimodal Chain-of-Thought method aimed at enhancing image-to-web conversion. We evaluate large multimodal models and provide an analysis of the results."}, {"title": "A Data De-identification", "content": "For real web pages, we perform de-identification to ensure sensitive information is not exposed:\n\u2022 The image components within our dataset have been replaced to avoid copyright or other legal concerns.\n\u2022 For content that includes logos or is characteristic of certain brands, we have also made necessary modifications.\nThese measures ensure that identifiable elements are not directly recognized, thus adhering to copyright and trademark laws. This approach protects the interests of copyright holders while ensuring our project remains legal and ethical."}, {"title": "B Limitations", "content": "Language Scope. Currently, the benchmark is limited to only two languages. Expanding the scope to include additional languages would enhance its applicability and relevance to a more diverse global audience.\nData Quantity. The benchmark dataset requires a significant increase in samples to ensure robustness and reliability. More comprehensive data coverage across different scenarios and contexts will improve the validity of benchmark results."}, {"title": "C More Related Work", "content": ""}, {"title": "C.1 Image-to-Web", "content": "Recent advancements in the field of web development have seen researchers exploring innovative ways to convert webpage images into HTML code (Patil et al., 2020; Bhambure et al.; Davody et al.). Researcher (Patil et al., 2020) focuses on automatically generating HTML code from webpage mock-ups. This involves the use of pix2code (Beltramelli, 2017), which trains a CNN-based model on various webpage structure mock-ups, showcasing the potential of automating GUI creation. Another method (A\u015f\u0131ro\u011flu et al., 2019) employs computer vision to identify objects and deep systematic analysis for result generation. Sketch2Code (Jain et al., 2019) divides the problem into three parts: object recognition, bounding box creation, and the creation of a functional prototype application. Recently, attention has shifted towards GPT4V for its image understanding and code generation capabilities."}, {"title": "C.2 Large Language Model", "content": "Recent advancements in AI have led to the development of generative foundation models (Bommasani et al., 2021) like GPT-3, ChatGPT, GPT-4, Claude and LLaMA (Brown et al., 2020; OpenAI, 2022, 2023a; Anthropic, 2023; Touvron et al., 2023; Zhang et al., 2023b), which excel in a variety of text-based tasks without specific finetuning. Their performance has been evaluated across disciplines such as QA, math, and science (Chen et al., 2021; Sun et al., 2023; Wang et al., 2023b; Huang et al., 2023, 2022; Liu et al., 2023b; Guo et al., 2024; Zhang et al., 2023a). On the vision-language side, there are several generative foundation models such as Qwen-Max, Qwen-VL, LLaVA, MiniGPT4, InstructBLIP, Flamingo,, Multimodal Bard (Bai et al., 2023; Zhu et al., 2023a; Dai et al., 2023; Alayrac et al., 2022; Awadalla et al., 2023; Gao et al., 2023; Google, 2023) that are trained on extensive image-text data, paving the way for multimodal learning (Schuhmann et al., 2022; Sharma et al., 2018; Lin et al., 2014; Zhu et al., 2023b). In addition, models specialized versions for document understanding are proposed (Zhang et al., 2023c; Ye et al., 2023). Benchmarks, like Visit-Bench, MM-Bench (Bitton et al., 2023; Yu et al., 2023; Liu et al., 2023c; Xu et al., 2023b; Shao et al., 2023), have assess the instruction-following and reasoning capabilities. As these foundation models become more relevant to real-world applications, unlike prior work, we plan to benchmark their capabilities of the hot Image-to-Web area on a diverse set of visual contexts."}, {"title": "C.3 Chain-of-Thought", "content": "We have witnessed the remarkable capabilities of Large Language Models (LLMs), with their reasoning abilities significantly enhanced through approaches such as Chain-of-Thought (CoT) (Wei et al., 2022), Program-of-Thought (PoT) (Chen et al., 2022), and Inductive Reasoning (Wang et al., 2023a; Tan and Motani, 2023; Guo et al., 2023). For multimodal CoT, recent work includes MCOT (Zhang et al., 2023d), HoT (Yao et al.,"}, {"title": "D Prompt for Benchmark Generation", "content": ""}, {"title": "D.1 General prompt for JavaScript Events", "content": ""}, {"title": "E List of Pre-defined JavaScript Event Bindings", "content": ""}, {"title": "E.1 General prompt for Domains", "content": ""}, {"title": "E.2 Prompt for Simple-level Element generation", "content": ""}, {"title": "E.3 Prompt for Medium-level Element generation", "content": ""}, {"title": "F Prompt for Direct Web Generation", "content": ""}, {"title": "G Web Page Quality and User Experience Questionnaire", "content": "We have designed this questionnaire to evaluate the quality of web pages and user experience across multiple dimensions. Please rate each statement on a scale from 1 to 5, where 1 represents 'very dissatisfied/very difficult to achieve' and 5 represents 'very satisfied/very easy to achieve.'"}, {"title": "H Details of CSS Style", "content": ""}]}