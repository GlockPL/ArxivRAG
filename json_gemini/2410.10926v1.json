{"title": "Federated Data-Efficient Instruction Tuning for Large Language Models", "authors": ["Zhen Qin", "Zhaomin Wu", "Bingsheng He", "Shuiguang Deng"], "abstract": "Instruction tuning helps improve pretrained large language models (LLMs) in terms of the responsiveness to human instructions, which is benefited from diversified instruction data. Federated learning extends the sources of instruction data by exploiting the diversified client-side data, making it increasingly popular for tuning LLMs. Existing approaches of federated LLM tuning typically traverse all local data during local training, bringing excessive computation overhead and posing a risk of overfitting local data. Thus, a federated data-efficient instruction tuning approach, which consumes relatively little data from the entire dataset, is needed. In response, this work introduces an approach of federated data-efficient instruction tuning for LLMs, FedHDS, which utilizes a representative subset of edge-side data, coreset, to tune the LLM. It reduces the redundancy of data samples at both intra-client and inter-client levels through a hierarchical data selection framework performed by jointly selecting a small number of representative data samples for local training without sharing the raw data. Extensive experiments conducted across six scenarios with various LLMs, datasets and data partitions demonstrate that FedHDS significantly reduces the amount of data required for fine-tuning while improving the responsiveness of the instruction-tuned LLMs to unseen tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) exhibit remarkable performance across a wide range of natural language tasks. Instruction tuning [49, 50] is a key technique for enabling LLMs to effectively respond to human instructions, a capability essential for many web-based LLM service providers. Diverse high-quality data is essential for effective instruction tuning [19, 25, 46, 48], particularly when addressing unseen tasks [50]. As publicly available high-quality data is expected to be depleted in the near future [44], leveraging data directly from its sources - such as edge devices - will be critical for the continued improvement of LLMs. However, privacy concerns and regulations such as GDPR [45] make utilizing edge data challenging. Federated learning (FL) [30], a privacy-preserving paradigm that leverages diverse edge-side data, is gaining popularity for tuning LLMs [56], especially for generalization to unseen tasks [2, 32].\nPrior works on federated instruction tuning mainly optimize communication and memory costs by parameter-efficient fine-tuning (PEFT) technologies [1, 6, 17, 56, 57] and zeroth-order optimization [32, 54], and have shown commendable performance in their targeted scenarios. Nevertheless, based on our investigation, a crucial factor highly related to the computation overhead of LLM tuning has yet to be thoroughly investigated in FL contexts, i.e., data efficiency. Existing approaches consume all local data [23, 32, 54, 56], leading to a higher number of iterations, which suffers two issues. (1) Efficiency: As shown in Figure 1, tuning LLMs exceeds the memory capacity of popular edge GPUs. CPU+GPU hybrid training reduces GPU memory cost but significantly increases per-step time, making the time of traversing the full dataset unbearably long (Figure 2). (2) Generalization: In FL, most client-side data typically consists of a small number of task types, excessive tuning steps with redundant data may harm the generalization to unseen tasks due to overfitting. The primary challenge to enable data-efficient instruction tuning in FL lies in how to minimize data consumption for tuning while"}, {"title": "2 RELATED WORK", "content": "Federated Fine-Tuning for LLMs\nFederated tuning for LLMs is gaining widespread attention as it enables the utilization of edge-side data without direct access. Given the large number of parameters in LLMs, many studies have developed tuning methods based on PEFT techniques to reduce memory footprint and the communication cost of transmitting the optimized model parameters [1, 6, 56, 57]. Among PEFT techniques, LoRA [13] gains significant attention [56], including initialization [1], partial matrix freezing for consistency between federated and local objectives [40], and heterogeneous adapters for heterogeneous client resources [2]. Some works are dedicated to optimizing corresponding costs, e.g. low communication costs by transmitting only loss values and fixed random seeds based on zeroth-order optimization [32], and low memory cost by training with quantization [54].\nDuring local training, existing works typically traverse all local data [17, 32, 40, 52, 54, 56]. On one hand, this leads to a significant computational overhead. On the other hand, due to the non-IID characteristics of FL, client-side data is often composed of a large number of samples from a few categories/domains. Repeated training on this data can result in overfitting to the local data, leading to reduced generalization ability on unseen tasks. Different from existing approaches, this work takes a data-centric perspective [55], which puts special efforts into data efficiency by finding a small number of representative data samples for LLM tuning, aiming at better efficiency and generalization on unseen tasks.\nData-Efficient LLM Instruction Tuning\nExisting research shows that LLMs learn knowledge primarily from pretraining, and can be taught to produce better outputs with a limited amount of instruction data [58]. Some works focus on using fewer data samples to achieve tuning results similar to or even better than those obtained with the full dataset [47], from the perspective of data quality [7, 10, 20, 21, 27, 58] or representativeness [8, 53]. Quality-oriented works either filter out low-quality samples from the training set by heuristics [7], quality indicators [7, 21] and the assistance of third-party LLMs [20], or curating a small set of high-quality samples by manual efforts [58] and third-party LLMs"}, {"title": "2.1", "content": "2.2"}, {"title": "3 PROBLEM FORMULATION", "content": "Assuming there are N clients in an FL system, indexed by i, each of which holds a private dataset Di containing several instances of instruction data. Given an LLM $w \\in \\mathbb{R}^d$ initialized by the pretrained weight $w^0$, federated instruction tuning aims at optimize w with the private data held by clients towards the following objective,\n$\\min_{w} f(w) = \\sum_{i=1}^{N} \\lambda_i \\cdot \\mathbb{E}_{x \\sim D_i} [L(w; x)],$ (1)\nwhere L(w; x) is the loss function evaluated on model w for any data instance x sampled from Di, and di is the weight of client i which follows \u03bb\u00a1 > 0 and $\\sum \\lambda_i = 1$. Symbol x is adopted as the batch size is set to 1 for lower memory cost on devices [28, 32]. To solve the objective in Eq. (1), FL iterates multiple rounds. In each round r, several clients participating in FL download the latest model parameters $w^r$ from the server and perform several steps of local optimization such as stochastic gradient descent (SGD),\n$w_{i,t+1} = w_{i,t} - \\eta \\cdot \\nabla_w L(w; x), \\forall w \\in D_i,$\n(2)\nwhere $w_{i,t}$ denotes model w of client i at the t-th local step in round r, and \u03b7 is the learning rate. Typically, the above process iterates over the local dataset Di for one or more epochs [32, 54, 56] in a cross-device FL scenario. After local training is finished, each active client transfers the optimized model parameters to the server. Given the large number of parameters in LLMs, FL typically incorporates PEFT techniques [57], where only a small subset of model parameters is trained and transmitted to reduce memory and communication costs. Similarly, this work employs LoRA [13] to train and transmit only LoRA adapters as [56].\nGiven a function of data selection f(x) that returns 0 or 1, a coreset of Di can be constructed by $D_{i}' = \\{ x \\mid f(x \\mid \\cdot) = 1, \\forall x \\in D_i \\}$. If f is effective in selecting a coreset, Eq. (2) can be optimized with $D_{i}'$ while w is still on par with or even surpass that obtained with"}, {"title": "4 APPROACH", "content": "Overview\nThe processes of FedHDS in each round of FL are illustrated in Figure 3. Recall the problem formalization in Section 3, the key difference between FedHDS and vanilla FL lies in the addition of a data selection process prior to the local training.\nFedHDS identifies the representative data samples with their latent features. In round r, after downloading the global model $w^r$, each client first conducts a process of data selection before local training, which contains two groups of sequential steps, i.e., intra-client selection and inter-client selection. In intra-client selection, client i takes each data sample x \u2208 Di as input and gets the token-wise hidden states of x in each Transformer layer, where the last hidden state of each layer is concatenated into a one-dimensional vector, serving as the raw features for x (\u2460 in Figure 3). Then, a dimensionality-reduction algorithm is performed to fuse the features from different transformer layers, taking the one-dimensional features of all data samples in Di into consideration (\u2461 in Figure 3). Next, a clustering algorithm is performed on the fused low-dimensional features to partition these data into several groups {G1, G2, ...} (\u2462 in Figure 3), where we adopt a density-based clustering, HDBSCAN [4], since it is hard to manually determine the number of clusters such as adopting K-means [26]. Each group Gj provides an approximate centroid cj that does not correspond to an individual data point. After that, the client sends these centroids {c1, c2, ..., } to the server (\u2463 in Figure 3).\nUpon receiving the centroids from all active clients, the server initiates the inter-client clustering. It applies HDBSCAN to all received centroids, resulting in several groups, denoted by {G], G, ...}, each associated with a corresponding centroid c ( in Figure 3). In each $G^I_i$, the centroid closest to c is designated as the chosen one. Next, the server notifies each client regarding which of their submitted centroids are selected (\u2465 in Figure 3). Thereafter, each client i adds the data sample closest to each of the selected centroid within the corresponding group to coreset D\u012f (\u2466 in Figure 3).\nAfter Di is constructed, the clients perform local training on the coresets, as in [17, 56]. The above data selection processes are summarized in Algorithm 1 of Appendix A. In the following subsections, we discuss the detailed design of FedHDS."}, {"title": "4.1"}, {"title": "4.2 Intra-Client Data Selection", "content": "The first step in selecting data is to extract its features. Given an LLM with l Transformer layers, it can extract the data features in a manner of layer by layer and token by token, as\n$\\begin{bmatrix}\nh_{j}^{1,1} & h_{j}^{1,2} & ... & h_{j}^{1,-1} \\\\\nh_{j}^{2,1} & h_{j}^{2,2} & ... & h_{j}^{2,-1} \\\\\n... & ... & ... & ... \\\\\nh_{j}^{l,1} & h_{j}^{l,2} & ... & h_{j}^{l} \\\\\n\\end{bmatrix}$\n(3)\nwhere $h_{j}^{l,b} \\in \\mathbb{R}^v$ is the hidden states of the b-th token from the l-th Transformer layer for the j-th data sample, and b = -1 corresponds to the last token. In terms of token-level feature, FedHDS uses the hidden state of the last token as [21] for two considerations: 1) since the Transformer computes the feature of the current token by considering all preceding tokens, the hidden state of the last token already contains information about all previous tokens, while performing averaging incurs additional computational costs, and 2) when an LLM receives input, the instructions are typically embedded within a system prompt template, resulting in minimal differences in the logits of the first few tokens among data samples.\nAs for the Transformer layer from which the features are derived, existing work typically adopts the final layer [8, 21, 53]. Since different layers of the Transformer apply varying degrees of abstraction to the data, the final layer may not be a universally optimal choice. We demonstrate this through a toy example on Dolly-15K dataset which contains 8 categories of instruction data, using K-means with the number of clusters aligned with that of the categories in Dolly-15K to partition these data. Two internal metrics, i.e., Calinski-Harabasz Index [3] and Silhouette Coefficient Index [36] and one metric with category label, i.e., F1-Score are employed to evaluate the clustering results. Figure 4 shows that clustering metrics based on features from different layers yield varying results, suggesting that the final layer may not be the best for distinguishing these data, and no single layer achieves the best results across all metrics.\nIt is challenging to predict the optimal layer for feature extraction and calculating clustering metrics across all layers would result in significant computational costs, fusing features from all layers may serve as a feasible solution. One naive method is to concatenate the hidden states of the last token in all Transformer layer, as\n$[h_{j}^{1,-1};h_{j}^{2,-1}...h_{j}^{l,-1}].$\n(4)\nHowever, as shown in Figure 4, the features from some layers may incur poor data separability, which may have a negative impact. Generally, dimensionality reduction helps to alleviate the impact of inappropriate dimensions, e.g., PCA [15] selects the most significant principal components based on the variance, and t-SNE [43] prioritizes preserving the relative distances between local data points during the dimensionality reduction process where the low-variance dimensions may have minimal impact on the distance calculation. Therefore, we perform dimensionality reduction $P: \\mathbb{R}^{l \\times v} \\to \\mathbb{R}^k, k \\ll l \\times v$ on the concatenated features to fuse the"}, {"title": "4.3 Inter-Client Data Selection", "content": "As shown in Figure 5, although the data distribution across clients is non-IID, there is still some similarity in data among clients. Thus, further selection among clients is performed so that the final data for tuning contains as little redundant information as possible.\nAs described in Section 4.2, client-side data samples are partitioned into several groups, each centered around an approximate centroid. Treating these centroids as representatives of their corresponding groups, we can cluster the centroids to filter out redundant data groups. Thus, we applied HDBSCAN a second time, as\n$G = \\{G_1^I, G_2^I, ...\\} = HDBSCAN(\\{c_1, c_2, ...\\})$\n(6)\nSimilar to the intra-client HDBSCAN process in Section 4.2, each $G_i^I$ is centered around an approximate centroid c. For each group $G_s^I$ in the second-layer selection, the server identifies the first-layer"}, {"title": "4.4 Data-Efficient Federated Fine-Tuning", "content": "After the coresets in clients are determined, each client i performs local training only with the data in coreseti, as\n$w_{i,t+1} = w_{i,t} - \\eta \\nabla_{w_{i,t}}L(w'; x), \\forall x \\in D_i,$\n(9)\nfollowed by submitting the optimized $w'$ after the last local steps $w_{i,t-1}$ to the server. Then, the server performs federated averaging [30] on the updated parameters received from clients, as\n$w^{r+1} = \\sum_{i \\in M_r} \\lambda_i w_{i,t-1}^r,$\n(10)\nwhere Mr contains the indices of clients which participate in the r-th round of FL. Finally, the server starts the next round."}, {"title": "4.5 Enhancement for FedHDS", "content": "Enhancement on Efficiency. Although FedHDS reduces the number of data samples significantly by retaining only one sample from each data group, it still requires forward propagation across all the local data with the LLM w needs to be tuned, which consumes a certain amount of time when applied to edge devices. Motivated by studies on retrieval augmented generation (RAG) which can adopt a light-weight retrieval model to extract data features for a better efficiency [12], we replace w by a language model with a smaller number of parameters, e.g., GPT-2 [34], to enhance FedHDS's feature extraction efficiency. We term this approach as FedHDS-Turbo.\nEnhancement on Privacy. The local centroids are transmitted to the server after intra-client selection, which are approximate representative points that do not correspond to real data samples, and the dimensionality of the centroids is significantly lower than that of the original data features. Nevertheless, FedHDS offers additional information compared to vanilla FL [30], such as insights into the spatial distribution of representative samples from the clients. To further minimize the risk of malicious exploitation of additional information, such as membership inference attacks [14], we adopt a straightforward differential privacy approach. Specifically, we scale each dimension of the centroids, which are submitted to the server, to the range [-1,1] using the tanh function. 1 Gaussian noise is then added to the scaled centroids before transmission. The differential privacy of the centroids (Theorem 1) can be derived"}, {"title": "4.5.1", "content": "4.5.2"}, {"title": "5 EVALUATIONS", "content": "Experimental Setup\nBaselines. First, we introduce some federated instruction tuning approaches with full local data as baselines, including:\nFedAvg (FP) [30] that tunes and transmits full LLM parameters. It is introduced just for reference due to its high costs;\nFedPTuning [17] that adopts with P-Tuning [24] as the PEFT technique trained with Adam [16] optimizer;\nFedPrompt [17] that replaces the PEFT technique in FedPTuning with Prompt Tuning [18].\nFedIT [56]: a federated instruction tuning approach designed based on vanilla FL with LoRA [13], optimized with Adam;\nFedIT-SGD: a variation of FedIT that replaces Adam with SGD.\nGiven the current lack of data-efficient methods for federated LLM tuning, we develop two federated baselines that tune the LLM on a subset of local data, and introduce a centralized training approach based on coreset [8], including:\nCoreset-Cent [8]: A centralized approach that extracts features with the last Transformer layer and selects a fixed ratio of central data samples from groups obtained by K-means.\nRandom: It randomly selects a certain proportion of local data to perform fine-tuning, with FedIT as the framework. Although being native, it is a strong baseline [22, 37], since it preserves the distributions of the original dataset.\nPerplexity: It selects a subset of data with the lowest perplexity scores [7], as these data samples are easy to learn by the LLM.\nDatasets and Evaluations. We evaluate these approaches on two instruction datasets, Natural Instructions [49] (NI) and Dolly-15K [9], which are popular in many federated instruction tuning studies [2, 17, 32]. Following [32], we employ evaluation on held-out tasks with Rouge-L as the evaluation metrics as in [32, 49]. After preprocessing (detailed in Appendix B.3), NI contains 738 training tasks, each of which is assigned to a unique client, providing non-IIDness with feature skew, and the natively provided 119 test tasks are used for evaluation. For Dolly-15K with 8 tasks, the last one is taken for evaluation while the rest are partitioned to 200 clients via Dirichlet distribution with a set to 0.5 and 5.0, respectively.\nImplementation. Considering this work is tailored for cross-device federated tuning, we randomly select 5% of the clients to participate in FL in each round. Approaches with coresets perform 60 rounds of FL on Dolly-15K with LLaMA-3B, and 40 rounds for the rest scenarios. Rouge-L scores of these approaches are evaluated with the tuned LLM after the last round of FL. Each client"}, {"title": "5.1", "content": "5.1.1"}, {"title": "5.1.2", "content": "5.1.3"}, {"title": "5.2 Comparison on Accuracy", "content": "We compare the performance of LLMs after instruction tuning across these approaches in Table 1. Note that the evaluation results of FedAvg (FP), FedPTuning, FedPrompt, and FedIT in Table 1 are derived from those reported in [32] under the same experimental settings. The results of other approaches are their best values obtained within the search space of hyperparameters.\nFrom Table 1, FedHDS and FedHDS-Turbo outperform federated instruction tuning approaches with full data across the six scenarios with consumed data samples less than 1.5% of them. Particularly, on Natural Instructions with DataJuicer-1.3B, FedHDS and FedHDS-Turbo relatively improve the Rouge-L score over the best-performing baseline with full data, i.e., FedIT, by 19.5% and 16.3%, respectively. Compared to the two federated approaches built with training on subsets, i.e., Random and Perplexity, FedHDS and FedHDS-Turbo exhibit superior performance in most of the six scenarios, only on NI with DataJuicer-1.3B that Random slightly outperforms FedHDS-Turbo. Note that in 7 out of the 12 scenarios related to the Random and Perplexity methods, the data proportions with the best Rouge-L results are inspired by the data proportion automatically acquired in FedHDS and FedHDS-Turbo. As demonstrated in [5], determining the optimal data proportion for the Random and Perplexity methods requires extensive experiments to be conducted manually, whereas the data proportion used in FedHDS and FedHDS-Turbo is automatically determined based on the distribution of the client data. These comparisons demonstrate the effectiveness of FedHDS and FedHDS-Turbo in selecting coresets for federated data-efficient instruction tuning.\nCompared to federated approaches using full data, those with coresets generally perform better on unseen tasks, especially on Natural Instructions, which has abundant instruction data. This may result from cross-device settings, where clients' training data mainly consists of tasks from a few categories. Tuning on complete local data causes excessive local training steps, leading the LLM to overfit local data and reducing performance on unseen tasks. Thus, employing a data selection strategy for tuning LLMs with fewer samples in cross-device FL is promising, as it can lower computation costs while improving performance on unseen tasks.\nCompared to the centralized baseline with data selection, Coreset-Cent, FedHDS and FedHDS-Turbo still show a noticeable gap on the more complex dataset, Natural Instructions, indicating room for further improvement in federated tuning methods."}, {"title": "5.3 Performance on Efficiency", "content": "Table 2 presents the efficiency comparison among several selected baselines and our approaches. It is evident that by reducing the number of local training steps (and thus the volume of consumed data samples), federated LLM tuning methods with coresets offer varying degrees of efficiency improvements compared to the best-performing federated tuning approach on the full dataset, i.e., FedIT. When simulating BP conducted entirely on a GPU, Random typically achieves the highest speedup ratio and the shortest training time, as it does not require forward propagation to extract features. Considering the limited GPU memory on edge devices, when hybrid training using both CPU and GPU is employed during BP to reduce peak memory usage, the forward propagation (FP) in FedHDS and FedHDS-Turbo is significantly faster than the hybrid BP. This results in a markedly lower time proportion for feature extraction. Consequently, FedHDS-Turbo achieves a higher speedup ratio due to the fewer data samples required for better Rouge-L scores compared to federated tuning baselines based on coresets. Particularly, on Natural Instructions with DataJuicer-1.3B, FedHDS-Turbo achieves a speedup ratio of 48.8\u00d7 compared to FedIT. Since FedHDS-Turbo is often comparable to FedHDS in terms of Rouge-L scores as in Table 1, FedHDS-Turbo demonstrates greater practical value in real-world applications due to its higher efficiency."}, {"title": "5.4 Ablation Studies", "content": "Ablation on Two-Layer Selection. As described in Section 4, FedHDS is characterized by intra-client and inter-client selections. To better clarify the effectiveness and contribution of both selections, we provide two approaches for ablation studies:\nFedDB: It extracts features based on the last Transformer layer, and group data samples with HDBSCAN. Then, in each group, one data sample closest to the corresponding centroid is selected.\nFedHDS (Intra): It implements only intra-client selection (Section 4.2), which groups data samples with HDBSCAN based on the"}, {"title": "5.4.1"}, {"title": "5.4.2 Ablation on Methods for Feature Fusion.", "content": "During intra-client selection, the features are primarily integrated using the dimensionality reduction algorithm t-SNE. Given the existence of other dimensionality reduction techniques, we also explored replacing t-SNE with alternative methods including PCA [15] and Kernel PCA [38] to further investigate the impact of dimensionality reduction algorithms on our approach. As presented in Table 3, in some cases, FedHDS with PCA achieved the second-best performance. On the relatively simpler datasets (Dolly-15K), replacing the dimensionality reduction algorithm did not lead to significant differences. However, on Natural Instructions, substituting t-SNE with PCA-based dimensionality-reduction algorithms occasionally resulted in negative effects. Therefore, in practical applications, we recommend implementing FedHDS and FedHDS-Turbo using the more advanced dimensionality reduction algorithm, t-SNE, for more stable and effective coreset selection."}, {"title": "5.5 Performance with Differential Privacy", "content": "Despite 1) the feature dimensions used in our method are significantly lower compared to the original token-level hidden states, which often have thousands of dimensions, and 2) the shared centroids do not correspond to actual data points, FedHDS still provides additional information compared to the standard FL paradigm [30, 56], such as the spatial distribution of client-side centroids. As Section 4.5.2, scaling the elements of the shared centroids to the range [-1,1] and adding Gaussian noise can enhance the privacy protection of the proposed approach. To clarify the impact of adding noise, we present example experiments by FedHDS-Turbo on Dolly-15K with DataJuicer-1.3B, with adjusting the power of the noise by varying its variance as [39]. As shown in Figure 6, when the"}, {"title": "5.6"}, {"title": "6 CONCLUSION", "content": "Existing federated instruction tuning approaches for LLMs typically train LLMs with traversing all local data during local training, which brings significant computational costs and poses a risk of overfitting to local data that may reduce the generalization capability on unseen tasks. Considering that the demand for instruction data in instruction tuning for LLMs is not necessarily better with more data, we pioneered an exploration into selecting a small subset of core data on each client for federated LLM tuning. To fulfill this, we propose FedHDS, a two-layer coreset selection framework for federated data-efficient instruction tuning, which progressively solves the intra-client and inter-client data redundancy based on two sequential clustering processes. To provide high-quality data representations for distinguishing coresets, we propose to fuse data features of varying levels of abstraction obtained from different Transformer layers with a dimensionality reduction algorithm. Extensive experiments conducted on two real-world instruction datasets in 6 scenarios with different non-IIDness and LLMs demonstrate that FedHDS can outperform baselines of federated instruction tuning with full data and data selection in terms of accuracy, while only consuming less than 1.5% of the full data, significantly speeding up the federated instruction tuning of LLMs.\nThis work may raise new attention on data efficiency in federated instruction tuning, while enhancing the practicality of federated instruction tuning on devices by reducing the computation cost."}, {"title": "A DETAILED ALGORITHM", "content": "To facilitate a better understanding of each step of the proposed approach, we provide Algorithm 1 to explain how our method selects the coresets, where lines 2-4 and 7-9 are performed individually by each client, and lines 1 and 5-6 are performed by the server."}, {"title": "B REPRODUCIBILITY", "content": "Experimental Environments\nWe implement these approaches mentioned above with PyTorch [31] 2.0.1, Transformers [51] 4.31.0, scikit-learn 1.5.1, PEFT [29] 0.4.0, and hdbscan 0.8.37. Numerical experiments in Tables 1 and 3 are performed on platforms equipped with NVIDIA A100 or NVIDIA A6000 GPUs, installed with Python 3.10 and CUDA 12.4. Efficiency results in Table 2 is obtained on a platform with an NVIDIA A6000 GPU, installed with Python 3.10, CUDA 12.4 and DeepSpeed 0.15.2.\nStatistics in Figures 1 and 2\nThe memory footprint and time consumption in Figures 1 and 2 are measured with a maximum token list length of 1,024 where excessively long data will be truncated. The adopted platform is equipped with an NVIDIA A6000 GPU, installed with Python 3.10, CUDA 12.4 and DeepSpeed 0.15.2. For memory footprint, the 95th percentile is calculated. The selected two GPUs are based on the most popular desktop and laptop GPUs identified from the Steam Hardware Survey (Sep 2024).\nDetailed Experimental Setups\nDuring intra-client selection in FedHDS and FedHDS-Turbo, the minimum cluster of HDBSCAN is set to the default value, i.e., 5 for Natural Instructions and 2 for Dolly-15K considering the relatively small size of this dataset.. During inter-client selection in FedHDS and FedHDS-Turbo, the minimum cluster of HDBSCAN is uniformly set to 2. The rank, alpha and dropout of LoRA adapters are set to 8, 16 and 0.05 for FedHDS and FedHDS-Turbo, respectively.\nAs described in Section 5.1.3, we perform preliminary hyperparameter search for for federated approaches, and adopt the advantageous settings for each approach in each scenario for large-scale experiments. For federated approaches with coresets, we first search the learning rate in {3 \u00d7 10\u22124, 1 \u00d7 10\u22124, 3 \u00d7 10\u22125}. Then, for Random and Perplexity, we search the ratio of data samples in final selected subsets to the full data samples in {0.2%, 1.5%, 2%, 5%}. Note that the thresholds of 0.2% and 1.5% are inspired by the proportion automatically obtained by FedHDS and FedHDS-Turbo. Considering the importance of an appropriate data ratio [5], these two baselines have benefited to some extent from the data proportion provided by"}, {"title": "B.1", "content": "B.2"}, {"title": "B.3"}]}