{"title": "Two Stage Segmentation of Cervical Tumors Using PocketNet", "authors": ["Awj Twam", "Megan Jacobsen", "Rachel Glenn", "Ann Klopp", "Aradhana M. Venkatesan", "David Fuentes"], "abstract": "Cervical cancer remains the fourth most common malignancy amongst women worldwide.\nConcurrent chemoradiotherapy (CRT) serves as the mainstay definitive treatment regimen for locally advanced cervical cancers and includes external beam radiation followed by brachytherapy. Integral to radiotherapy treatment planning is the routine contouring of both the target tumor at the level of the cervix, associated gynecologic anatomy and the adjacent organs at risk (OARs). However, manual contouring of these structures is both time and labor intensive and associated with known interobserver variability that can impact treatment outcomes. While multiple tools have been developed to automatically segment OARs and the high-risk clinical tumor volume (HR-CTV) using computed tomography (CT) images, the development of deep learning-based tumor segmentation tools using routine T2-weighted (T2w) magnetic resonance imaging (MRI) addresses an unmet clinical need to improve the routine contouring of both anatomical structures and cervical cancers, thereby increasing quality and consistency of radiotherapy planning. This work applied a novel deep-learning model (PocketNet) to segment the cervix, vagina, uterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture was evaluated, when trained on data via 5-fold cross validation. PocketNet achieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for tumor segmentation and 80% for organ segmentation. These results suggest that PocketNet is robust to variations in contrast protocols, providing reliable segmentation of the ROIs.\nKeywords: cervix, vagina, uterus, tumor segmentation, T2w MRI, deep learning, robustness, dataset training, cervical cancer", "sections": [{"title": "Introduction", "content": "In 2022, the Global Cancer Observatory documented 662,301 cases of uterine cervical cancer worldwide, ranking it as the 9th leading cause of cancer-related mortality, with 348,874 recorded deaths. Cervical cancer primarily occurs due to the integration of human papillomaviruses (HPV) in the host genome. While cervical cancer remains a largely preventable disease through HPV vaccination and routine screening, the incidence and mortality rates of cervical cancer have not changed in the past twenty years. Diagnosis and treatment of cervical cancers heavily rely on imaging, with magnetic resonance imaging (MRI) as the primary modality for staging and evaluation of locally advanced cervical cancers (LACCs) due to its superior soft tissue contrast as compared to other clinical imaging modalities, such as ultrasound or CT. The main form of treatment for locally advanced cervical cancer is definitive radiation therapy (RT), accompanied by cisplatin-based chemotherapy. Definitive RT for cervical cancer includes external beam RT and brachytherapy (BT), both of which necessitate manual segmentation of the cervical tumor before therapy. Although an essential part of treatment, manual segmentation can be a time-consuming task and contribute to a delay in the start of BT following applicator insertion. Automated segmentation, if available, could facilitate adaptive planning, which requires daily re-contouring in order to customize the treatment plan to daily changes in anatomy due to movement of bowel and bladder or to account for the tumor volume reduction that occurs over the course of treatment. Although adaptive planning has been demonstrated to reduce radiation doses to normal tissues, it is not widely employed due to the time intensive process of manual segmentation. Manual segmentation is also subject to significant inter- and intraobserver variability, which limits researchers' ability to train deep-learning models using large datasets.\nAutomatic segmentation addresses these challenges and presents an opportunity for advancement in the development of multi-institutional models for cervical cancer outcomes. Beyond segmentation, DL models, particularly convolutional neural networks (CNNs), have shown great potential in the diagnosis of uterine cervical cancer. CNNs applied to T2-weighted MR images of the female pelvis have demonstrated high accuracy (90.8%) when determining the binary presence or absence of cervical cancer, which was comparable to the performance of experienced readers. Building on this success, CNNs have emerged as a powerful class of DL models, further advancing the capabilities of medical image analysis.\nOne of the most popular fully CNN architectures for medical image segmentation is the U-Net. The U-Net's success in medical image segmentation is largely due to a combination of feature extraction and reconstruction, which enables it to accurately delineate complex anatomical structures. However, the U-Net's architecture is also resource-intensive, requiring a significant number of parameters, which can make it challenging to deploy in environments with limited computational resources. Therefore, we propose using a PocketNet approach to segment the uterus, vagina, cervix and tumors. PocketNet builds off the U-Net backbone but dramatically reduces the number of parameters to minimize computational burden while producing similar performance to that of full-sized networks. This work evaluated the performance of a two-stage PocketNet approach for image segmentation of cervical cancers on T2w MRIs in patients treated with definitive RT."}, {"title": "Materials and Methods", "content": "We collected over 300 pre-treatment T2wMR images from 102 patients diagnosed with cervical cancer who ultimately underwent definitive chemoradiotherapy. Two volumetric contours were manually segmented for each patient: 1) the combined uterus, cervix, vagina, and tumor, collectively labeled as 'organ' and 2) the cervical tumor alone. Manual segmentations were performed by trained staff and approved by a fellowship-trained abdominal radiologist with 22 years' experience and were used to define ground truth. The initial PocketNet architecture was trained on 35 patients for segmentation of the \u2018organ'. A second PocketNet model was trained on 102 patients including the organ mask and tumor, with the requirement that the final tumor segmentation was contained within the organ . Dice similarity coefficients (DSC) and the Hausdorff distance (HD) were calculated for each contour as a measurement of the accuracy and maximum distance between ground truth and automated contours, respectively."}, {"title": "Data", "content": "Preparation of imaging data is essential to ensure accurate and reproducible predictions.\nInclusion criteria for MR images used for training, validation, and testing were as follows: 1) Visibility of the entire cervix, vagina, uterus, and tumor within the image; 2) presence of entire organ segments; 3) acceptable image quality such that the boundary organ and tumor was identifiable without the aid of a pre-existing contour. Images were manually inspected, and quality assurance was conducted over the entire dataset to confirm MR images met these criteria. Preprocessing steps included reorienting the images to the right-anterior-inferior direction. Table 1 lists the patch size and pixel spacing of data included in the analysis."}, {"title": "Preprocessing", "content": null}, {"title": "Network architecture", "content": "In a PocketNet architecture, the number of feature maps resulting from each convolution layer remains constant regardless of spatial resolution (Figure 2). Traditional CNNs double the number of features when going from higher to lower resolutions. With the use of PocketNet, this results in faster training and inference times while simultaneously lowering memory usage and requirements."}, {"title": "Training protocols and hyperparameters", "content": "We used an Nvidia Quadro RTX 8000 graphical processing unit (GPU) with a batch size equal to the number of patients included in each cascade step, trained with 1,000 epochs. During training, network weights were determined using the ADAM optimizer with a learning rate set to 0.0001 and use of a cosine scheduler. Additionally, automatic mixed precision was used during training to reduce the time and memory requirements. Classification tasks include the Dice with categorical cross-entropy as our loss function. Regularization features include deep supervision and segmentation tasks using the L2 norm. In addition, residual blocks were also utilized in the architecture to improve training by resolving the degradation problem with the use of skip-connections around convolution layers. Finally, we postprocess predictions by taking the largest connected component, which eliminates noise and small islands distant to the body of the contour. \u03a4\u03bf evaluate the validity of a predicted segmentation mask, the DSC and the 95th percentile Hausdorff distance (Haus95) are used as outcome metrics to evaluate segmentation accuracy.\nIn addition to the parameters described above, performance of PocketNet with default parameter settings was conducted separately to compare results. The results revealed that inclusion of auxiliary parameters described above, highlighted in Appendix A, and included in our training model increased average dice scores by 5.04% for organ and 7.95% for tumor as opposed to the default settings."}, {"title": "Experimental Design", "content": "Using the data and model described above, we performed a two-stage segmentation, outlined in Figure 3, using PocketNet to evaluate the architecture's performance for T2w MRI-based automated segmentation of the cervical tumor and gynecologic organs.\nFirst, we performed a five-fold cross-validation on the initial training dataset of 35 cancer patients to generate an organ model containing the tumor. Inference was then performed to generate organ prediction masks on 102 patients. Then, we combine the organ and tumor masks manually segmented into the same dataset of 102 patients. We perform another five-fold cross-validation on this dataset which includes the T2-weighted image and mask (organ and tumor combined)."}, {"title": "Ethics approval", "content": "All data used in this study was collected and analyzed under an approved Institutional Review Board protocol (IRB)."}, {"title": "Results", "content": null}, {"title": "Quantitative evaluation", "content": null}, {"title": "Qualitative evaluation", "content": "Figure 5 presents two examples of automatic contours that performed well. The image on the top had a Dice score of 0.917 for organ and 0.883 for tumor, indicating a high degree of overlap between the automatically generated contours (C) and the ground truth (B). The example directly below it demonstrated even better performance for the organ, with a Dice score of 0.946, and a comparable score of 0.889 for the tumor. These high Dice scores reflect the algorithm's strong capability to accurately delineate both the organ and tumor in these cases.\nIn contrast, Figure 6 displays instances where the auto-segmentation algorithms did not perform as well. The top example in Figure 6 had a dice score of 0.786 for organ and 0.528 for tumor, indicating a noticeable decrease in accuracy, particularly for the tumor. The bottom example in the same figure had a Dice score of 0.550 for the organ and a low score of 0.018 for organ and tumor. The contrast between the results in Figures 5 and 6 emphasizes the importance of continuous refinement and validation of automatic segmentation algorithms."}, {"title": "Discussion", "content": "Accurate and reliable identification of these anatomical regions and tumors is critical for improving accuracy during diagnosis and radiotherapy treatment planning. A study conducted by Chung et al. demonstrated the practical impact of DL based autosegmentation systems in clinical workflows. The results showed that using an autosegmentation system reduced the average contouring time by approximately half an hour. Among the participating physicians, 84% reported satisfaction with the autosegmentation model, considering it to be helpful in clinical practice. Moreover, the widespread acceptance of autosegmentation tools across multiple institutions suggests the potential for these models to become standard features in radiotherapy planning. The primary objective of this study was to develop and apply a deep learning model for the robust segmentation of cervical, uterine, vaginal, and tumor tissues with T2w MRI.\nTo achieve this, we employed the PocketNet model, a compact yet powerful deep learning architecture designed for efficient image segmentation tasks. The PocketNet paradigm introduces several optimizations that make it particularly well-suited for resource-constrained environments while maintaining high performance levels. One of the most notable advantages of PocketNet is its reduced GPU memory usage while decreasing inference times compared to a traditional U-net. Another significant benefit of PocketNet is the reduction in training time.\nCelaya et al. evaluated the performance of PocketNet in a variety of medical imaging datasets, comparing its efficiency and effectiveness to that of full-sized architectures like U-Net. These results demonstrate that the PocketNet architecture reduces memory usage and speeds up training time for every batch size.\nThe performance of automatic segmentation algorithms in cervical cancer treatment presents both opportunities and challenges, as evidenced by the varying results highlighted in Figures 5 and 6. A key observation from these results in the apparent relationship between tumor size and segmentation accuracy. Larger tumors, as seen in Figure 4, tend to be segmented more accurately, resulting in higher Dice scores, while smaller tumors, like those in Figure 5, often yielded lower Dice scores. Smaller tumors may be entirely confined to the cervix or lower uterine segment with less signal difference between the normal tissue and tumor on T2w imaging. In contrast, larger masses are more likely to involve the full thickness of the cervix or have exophytic components that are relatively simple to differentiate from the surrounding normal tissue. This finding underscores the need for further refinement and validation of these models, such as the inclusion of diffusion-weighted MRI to identify regions of restricted diffusion that are specific to cervical tumors.\nRecent advancements in deep learning have brought about significant improvements in the automatic segmentation of medical images, including that of cervical cancer. In a recent study conducted by Kano et al., a deep learning model applied to DWI achieved a mean DSC of 0.77 for cervical tumor segmentation. This result demonstrates the robustness of the PocketNet model as compared to existing DL methods. The comparable performance of our T2w-based model, which achieved a mean DSC of 0.71, to this DWI-based model suggests that PocketNet has the potential to achieve high accuracy across various imaging sequences with further development. Another study comparing the segmentation performance of a DL algorithm (specifically, a 3D U-Net) with manual segmentations provided by two radiologists (R1 and R2) reported median DSCs of 0.60 for R1 and 0.58 for R2. In addition, a study utilizing a U-Net based triple-channel model using sagittal diffusion-weighted MRI (DWI) with b-values of 0 and 1000 s/mm\u00b2 and apparent diffusion coefficient (ADC) maps produced a promising average DSC of 0.82, further demonstrating the capability of DL models to enable accurate segmentation of cervical cancer. One of the most promising applications of DL models in cervical cancer treatment is their use in developing daily online dose optimization strategies. This application of DL models has the potential to eliminate the need for internal target volume expansions, ultimately resulting in the reduction of toxicity events related to intensity modulated radiation therapy.\nDespite the promising results obtained from this institutional dataset, this study has limitations, including the size of this dataset and the lack of multi-center training data. To further improve segmentation accuracy, expanding the training dataset to include a larger and more diverse set of images is an essential next step. By incorporating a more diverse dataset, the model can be trained on a broader range of tumor sizes, morphologies, and degrees of soft tissue contrast, leading to more consistent and accurate segmentations. Integrating additional imaging sequences such as DWI may also enhance the model's robustness. In addition, ensuring that these diverse datasets are collected using consistent, standardized imaging protocols will minimize variability and improve the model.\nIn conclusion, this preliminary work employing PocketNet to segment the cervix, uterus, vagina, and tumor(s) on T2w MRI in patients undergoing definitive RT for cervical cancer demonstrated reliable segmentation of both tumor and the gynecologic organs, suggesting the utility of this time efficient approach to automated segmentation. Further work will refine the performance and validate this approach, with the goal to translate this methodology to support routine clinical practice Continued advancements in data harmonization, model training, and cross-validation will be key drivers to achieve this goal, ultimately leading to greater standardization of definitive radiotherapy and improved clinical outcomes for patients with cervical cancer."}, {"title": "Conclusion", "content": "Although essential for cervical cancer treatment planning, manual segmentation of the cervix, uterus, vagina, and tumors is a relatively time-consuming process that is subject to variability.\nOur proposed deep learning segmentation model, PocketNet, with its compact architecture and optimizations for reduced GPU memory usage and faster inference times, addresses these challenges, by providing an approach that automates the segmentation of anatomical regions of interest for cervical cancer radiation treatment planning. These results demonstrate that PocketNet achieved an average DSC of 81% for organ segmentation and 71% for tumor segmentation. Future work employing this model is anticipated to further refine and validate this approach for automated segmentation."}, {"title": "Data Availability", "content": "The MR images from MD Anderson are not publicly available at this time."}, {"title": "Appendix A: Training Protocols and Hyperparameters", "content": null}]}