{"title": "Contractual Reinforcement Learning: Pulling Arms with Invisible Hands", "authors": ["Jibang Wu", "Siyu Chen", "Mengdi Wang", "Huazheng Wang", "Haifeng Xu"], "abstract": "The agency problem emerges in today's large scale machine learning tasks, where the learners are unable to direct content creation or enforce data collection. In this work, we propose a theoretical framework for aligning economic interests of different stakeholders in the online learning problems through contract design. The problem, termed contractual reinforcement learning, naturally arises from the classic model of Markov decision processes, where a learning principal seeks to optimally influence the agent's action policy for their common interests through a set of payment rules contingent on the realization of next state. For the planning problem, we design an efficient dynamic programming algorithm to determine the optimal contracts against the far-sighted agent. For the learning problem, we introduce a generic design of no-regret learning algorithms to untangle the challenges from robust design of contracts to the balance of exploration and exploitation, reducing the complexity analysis to the construction of efficient search algorithms. For several natural classes of problems, we design tailored search algorithms that provably achieve  \u00d5(VT) regret. We also present an algorithm with  \u00d5(T^{2/3}) for the general problem that improves the existing analysis in online contract design with mild technical assumptions.", "sections": [{"title": "Introduction", "content": "The \"invisible hand\" metaphor by Adam Smith illustrates how properly designed incentive structures can guide self-interested individuals to inadvertently promote the greater social good. This concept is increasingly relevant in the realm of machine learning, as the scale of applications expands and the conflict of economic interests intensifies. For example, an Internet platform wants to estimate the ad revenues from serving different types of content, but it is up to the creators to decide what content to produce. While the platform seeks high-quality content to boost its long-term growth, creators may opt to minimize their production costs. This misalignment has prompted platforms to implement revenue-sharing models, fueling the growth of the creator economy, projected to exceed half a trillion by 2027. However, current incentive models are inadequate, especially in light of their roles in exacerbating the proliferation of clickbait and misinformation online [31, 54, 55]. Moreover, this issue of misalignment extends well beyond content platforms. E-commerce sites rely on sufficient consumers experimenting with new products for accurate preference assessments. Gig platforms depend on freelance workers accepting tasks to gather essential operational data. Even recommender systems are paying users for their engagement in order to effectively optimize their algorithms [3]. In these cases, the learner's hands are tied, and decision-makers interacting with the environment have their own objectives, dooming the system to under-exploration regardless of the learner's objective. Hence, there is a pressing need to pursue formal treatments of incentive alignment problems between the learners and decision-makers and to design principled learning algorithms with statistical and computational efficiency guarantees.\nContributions. On the conceptual side, the presence of self-interested decision-makers challenges our common assumption in online learning, where a single learner controls all the interactions with the environment. This paper introduces the contractual reinforcement learning (RL) problem in the principal-agent Markov decision process (PAMDP), where we adopt the principal-agent model from contract theory [24, 27] to capture strategic interactions between the learner and decision-maker. As illustrated in Figure 1, the learner (henceforth, principal/she) collects the rewards from the actions of decision-maker (henceforth, agent/he). Without any incentive design, the agent simply optimizes his policy in a standard Markov decision process (MDP) based on his cost function. However, since the agent's optimal policy is not necessarily in the principal's best interest, the principal is motivated to properly incentivize the agent to act in her favor by designing contracts that specify the payment rules contingent on the realization of the next state. The core challenge in this design problem is the information asymmetry at two levels: (1) the principal cannot observe the agent's action a priori and has to condition her payment on the probabilistic outcome of the action a phenomenon known as the moral hazard in economics; (2) the agent is far-sighted that he is willing to take suboptimal actions at one step in order to reach a more favorable state in future steps a major barrier for theoretical analysis in multi-agent learning problems.\nOn the technical side, this paper provides a comprehensive solution framework to address the unique learning and computational challenges when moral hazard meets far-sighted agency in contractual RL problems. In Section 2, we define state value functions for both the agent and principal, from which we derive a new class of Bellman equations to characterize the intricate correspondence be-tween the principal and agent's optimal policy. This leads to our Theorem 1, which shows that the principal's optimal planning problem can be solved by a clean formulation of dynamic programming in polynomial time. The learning problem is more involved, so we begin with the contractual bandit learning problem (episode length H = 1) in Section 3 to focus on the challenges from moral hazard. In particular, to achieve low regret, the principal's learning algorithm must balance exploration and exploitation while continuously improving its estimation of the agent's preferences to determine cost-efficient contracts. In Theorem 2, we construct a generic algorithm that reduces the learning problem into a standard online learning problem and an efficient search problem for the agent's decision boundary. As a consequence, we are able to obtain sublinear regret guarantee under different setups, summarized in Table 1. The efficient search algorithm we designed for learning the outcome distribution difference in the simplex may be of interest for general use. With these insights, we delve into the full contractual RL problem in Section 4 and show a provably efficient learning algorithm under several technical assumptions in Theorem 3. Meanwhile, the general result highlights a trade-off between statistical and computational tractability, leaving an intriguing open question on the existence of the best-of-both-worlds solution. The complexity of search is in logarithmic order yet with a large constant in the Markovian setup, and we expect an improved analysis by organically combining the search and exploration in the algorithm design."}, {"title": "Problem Formulation", "content": "Let us first recall the standard reinforcement learning problem in a (finite-horizon) Markov de-cision process (A, S, {P_h, r_h}_{h=1}^H, P_0), where we have the agent's action space A, the environ-ment's state space S, the transition kernel P_h : S \u00d7 A \u2192 \u0394(S), the expected reward function r_h: S\u00d7A\u2192 [0, 1], the initial state distribution P_0 \u2208 \u0394(S) and the horizon length H. The contrac-tual reinforcement learning problem simply extends the MDP to a principal-agent Markov decision process (A, S, {P_h,r_h, c_h}_{h=1}^H, P_0) with the additional cost function c_h : S \u00d7 A \u2192 [0, 1]. In this process, the agent interacts with the environment by taking actions and bearing the costs, whereas the principal receives the reward from the environment. Unable to directly interact with the envi-ronment, the principal has to instead design and implement contracts to incentivize the agent to take actions in her interest. Below, we formalize the design of their policies.\nFollowing from a standard MDP, the agent's action policy \\pi = {\u03c0_h : S \u2192 \u0394(A)}_{h=1}^H specifies that at each step h, given the state s, the agent would take the action a ~ \u03c0_h(s). In the following subsection, we will discuss how the agent chooses his action policy and that it suffices to only consider deterministic action policies. Meanwhile, the principal's contract policy x = {x_h : S \u00d7 S \u2192 \\mathbb{R}_+}_{h=1}^H is a sequence of non-liable payment rules x_h, where x_h(s_h, s_{h+1}) specifies the payment to the agent if the next state s_{h+1} is realized, given the current state s_h at the h-th step. The non-liability constraint ensures that the principal's payment in the contract for any realization of the next state must be non-negative; the problem would otherwise degenerate with an trivially optimal solution for the principal (see e.g., [24]). Denote II, X as the agent and principal's policy space, respectively. Let |S| = S, |A| = A and thus |II| = (SA)H.\nThe typical setting of the PAMDP problems can be summarized by the following steps. In the beginning of each episode, the initial state s_1 ~ P_0 is realized and observed by both the principal and the agent. Afterwards, the principal commits to a contract policy \u00e6 and the agent accordingly chooses an action policy \u03c0. Their interactions then proceed as follows at each step h \u2208 [H],\nIn this step, the principal's utility is r_h(s_h, s_{h+1}) - x_h(s_h, s_{h+1}), her reward minus the payment to agent, whereas the agent's utility is x_h (s_h, s_{h+1}) - c_h(s_h, a_h), the payment from principal minus his cost. The reward noise has zero mean such that r_h(s, a) = \\mathbb{E}_{s'~P_h(s,a)} r_h(s, s'), \u2200s \u2208 S, a \u2208 A. We refer the readers to Appendix B.1 for a summary of notations and Appendix B.2 for a full discussion of our modeling choices.\nWithout any contract design, the model reduces to a standard MDP (A, S, {P_h, c_h}_{h=1}^H, P_0) for the agent and the principal passively collects the reward from the agent's policy. This outcome could be suboptimal for both the principal and agent. Instead, by reshaping the agent's reward environment through the design of contract policy, the principal could induce the agent adopt some action policy with higher social surplus. This motivates the problem of designing the optimal contract policy. We focus on a realistic yet challenging setup in the face of a long-lived, far-sighted and Bayesian rational agent who is also planning optimally for his cumulative reward we expect the case of myopic agents can be worked out with simpler approach. In particular, since the agent's utility is not necessarily 0 under the principal's optimal contract at any state due to moral hazard, a far-sighted agent could take certain actions that are sub-optimal in the current step, yet secure him toward certain future states where he can obtain higher cumulative utility.\nWe extend notions of value functions and optimal policies from MDP to PAMDP. Under any action policy \u03c0 and contract policy x, we define the principal's state value function at the h-th step as,\nV_h^{x,\\pi}(s) := \\mathbb{E} \\bigg[\\sum_{\\tau=h}^H r_\\tau(s_\\tau, a_\\tau) - x_\\tau(s_\\tau, s_{\\tau+1})| \\{tr\\}_{t=h}^H, s_h = s\\bigg],\nand the agent's state value function at the h-th step as,\nU_h^{x,\\pi}(s) := \\mathbb{E} \\bigg[\\sum_{\\tau=h}^H x_\\tau(s_\\tau, s_{\\tau+1}) - c_\\tau(s_\\tau, a_\\tau)| \\{tr\\}_{t=h}^H, s_\\tau = s\\bigg],\nwhere the expectation in both V, U are with respect to the randomness of the trajectory (due to the stochasticity of state transitions and action policy). Let V^{x,\\pi} := \\mathbb{E}_{s~P_0} V_1^{x,\\pi}(s) and U^{x,\\pi} := \\mathbb{E}_{s~P_0} U_1^{x,\\pi}(s). The principal's goal is to maximize her value V^{x,\\pi}, given the agent's optimal response \u03c0, which equivalently maximizes V_1^{x,\\pi}(s) at any initial state s with P_0(s) > 0. Hence, we define the principal's optimal contract policy x^* = {x_h^*}_{h=1}^H and the corresponding optimal value"}, {"title": "The Optimal Contract Policy", "content": "V^* as the optimal solution and value of the following bi-level optimization problem,\nV^*, x^* := \\max_{x \\in X} V^{x,\\pi} \\quad \\text{s.t.} \\quad \\pi^* = \\arg\\max_{\\pi \\in \\Pi} U^{x,\\pi},\nwhere \"maxarg\" is a convenient operator notation on an optimization problem that returns the op-timal objective value followed by its optimal solution. For notational convenience, we will denote the agent's optimal action policy in response to contract policy \u03c0as \u03c0x = argmax\u03c0\u2208\u03c0 Ux,\u03c0, and use shorthands Vxh := V^{x,\\pi_x}, Uxh := U^{x,\\pi_x} for the principal's and agent's value function under contract policy x at the h-th step given that the agent responds optimally. Meanwhile, we denote x^* = argmaxx\u2208X Vx,\u03c0 s.t. \u03c0 = argmax\u03c0\u2208\u03a0 Ux,\u03c0 as the principal's optimal contract policy to induce the agent's action policy \u03c0. We use similar shorthands V\u03c0 := Vx,\u03c0, Ux := Ux,\u03c0 for the principal's and agent's value function under contract policy x\u03c0 at the h-th step given that the agent responds optimally. Notably, since the optimization problem (2.1) hinges on the intricate correspon-dence between x and \u03c0, it is unclear for now if the principal can efficiently plan his optimal policy adopting the standard approach in MDP.\nSolving for the Agent's Optimal Policy. One key observation is that the correspondence between \u03c0and\u00e6 has a clean characterization through the Bellman equation. Specifically, both functions {\u03c0h, U_h}_{h=1}^H can be solved through backward induction with U_{H+1}(s) = 0:\ngiven U_{h+1}, U_h(s), \u03c0_h(s) = \\arg\\max_{a \\in A} \\mathbb{E}_{s'\\sim P_h(s, a)} \\cdot [x_h(s) + U_{h+1}(s')] - c_h(s, a).\nNotice that since \u03c0h(s) is a maximizer of a linear function, the agent's best responding policy \u03c0h is deterministic without loss of generality. With \u03c0x, the principal's value function under x can also be computed iteratively from V_{H+1}(s) = 0:\ngiven V_{h+1}, V_h(s) = r_h(s, \u03c0_h(s)) + \\mathbb{E}_{s'~P_h(s, \u03c0_h(s))} \\cdot [V_{h+1} - x_h(s)].\nDue to the space limit, we only solve the agent's best response \u03c0x at any given \u00e6. We refer the reader to Appendix B.3 for the more involved formulation to solve the optimal policy x\u03c0 for any given \u03c0.\nValue Decomposition. Another key observation is that the value functions can decomposed into parts that are only depends on the agent's action policy. This is analogous to the standard contract design where principal's and agent's utility sums up to the social surplus, i.e., the difference between the reward and cost of the agent's action. Here, let the principal's expected reward and agent's expected cost function in the h-th step be\nR_h^{\\pi}(s) := \\mathbb{E} \\bigg[\\sum_{\\tau=h}^H r_\\tau(S_\\tau, A_\\tau)| \\{tr\\}_{t=h}^H, S_h = s\\bigg], C_h^{\\pi}(s) := \\mathbb{E} \\bigg[\\sum_{\\tau=h}^H c_\\tau(S_\\tau, A_\\tau)| \\{tr\\}_{t=h}^H, S_h = s\\bigg].\nBy linearity of expectation, for any policy x \u2208 \u03a7, \u03c0 \u2208 \u03a0, at any state s of any step h, we have\nV_h^{x,\\pi}(s) = R_h^{\\pi}(s) - C_h^{\\pi}(s) - U_h^{x,\\pi}(s).\nBoth functions R, C are fixed to the agent's action policy \u03c0, regardless of the contract policy x. In addition, C_h^{\\pi}(s) - U_h^{x,\\pi}(s) captures the total amount of expected payment transferred from the principal to the agent since h-th step at state s. Since the total reward is fixed under any given \u03c0, the principal's value is maximized under the minimal total payment, U_h^{*}(s) := C_h^{\\pi}(s) + U_h^{x,\\pi}(s). The function Uxh thus serves as the equivalent optimization objective in the least-payment Bellman equation in Appendix B.3.\nSolving for the Optimal Contract Policy. With the two observations above, it is clear that the principal's the optimal value and policy V^* = max\u03c0\u2208\u03a0 V\u03c0 can be determined by computing V\u03c0 for every \u03c0, according to the least-payment bellman equation in Appendix B.3. However, this maximization problem is still intractable, as there are exponentially many possible \u03c0. Instead, we have to interleave the process of solving for the optimal policy with least payment and maximum reward. This enables the following construction of a bi-level backward induction that iteratively solves for the optimal contract policy x*."}, {"title": "Bellman Equations of PAMDP", "content": "Theorem 1 (Bellman Equations of PAMDP). The optimal contract policy can solved by dynamic programming in polynomial time, from h = H to 1 with UH+1(s), VH+1(s) = 0,\u2200s \u2208 S, a \u2208 A,\nW_h(s, a; x) = \\mathbb{E}_{s'~P_h(s, a)} \\cdot [x + U_{h+1}] - c_h(s, a),\nx_h^{*}(s; a) = \\arg\\min_{x:S\u2192\\mathbb{R}_+} {\\mathbb{E}_{s'~P_h(s, a)} \\cdot x | W_h(s, a; x) \u2265 W_h(s, a'; x), \u2200a' \u2208 A},\nQ_h(s,a) = r_h(s, a) + \\mathbb{E}_{s'~P_h(s, a)} \\cdot [V_{h+1} - x_h(s; a)],\nV_h^{*}(s), \u03c0_h(s) = \\arg\\max_{a \\in A} Q_h(s, a), x_h(s) = x_h^{*}(s; \u03c0_h(s)), U_h^{*}(s) = W_h (s, \u03c0_h(s); x_h(s)),\nTo interpret the Bellman equation above, x_h^{*}(s; a) denotes the contract with the least payment to induce the agent to take action a at state s in step h. Given that \u03c0_h(s) is the best agent action for the principal to induce, the optimal contract at state s in step h can be determined as x_h(s) = x_h^{*}(s; \u03c0_h(s)). Q_h (s, a), W_h (s, a; x) are respectively the principal's and agent's total expected utility from h-th step under policy {x^*}_{H=h+1} and {\u03c0^*}_{H=h+1}, which can be viewed as their optimal state-action value function at h-th step, serving as the intermediate variable for the computation. See Appendix B.4 for the proof of correctness."}, {"title": "The Contractual Reinforcement Learning Problem", "content": "We now introduce the reinforcement learning problem in PAMDP, where the principal acts as the learner and seeks to adaptively improve its contract policy by interacting with the agent. Following the online learning convention, we use the expected regret to evaluate the learning performance in T episodes, \\text{Reg}(T) := \\sum_{t=1}^T V^* \u2013 V^{x_t},\\pi_t, where x_t is the principal's contract policy in the t-th episode.\nThis paper makes a few assumptions for the analysis of reinforcement learning problems. First, the far-sighted agent has perfect knowledge of his cost function and the state transition kernel {P_h, c_h}_{h\u2208 [H]} such that he can always chooses the best response. This is realistic because in appli-cations of our interest, agents are the experts (e.g., content creators, freelance workers, ride-sharing drivers) in the fields who has learnt about the environment sufficiently well whereas principal as the system designer does not know. Second, the agent at time t is assumed to best respond to xt. This can be equivalently interpreted as the agent at each time t showing up only once. This is motivated by the reality of Internet applications where each individual agent's participation only accounts for a negligible portion of the system's traffic hence has little influence over the entire system's learn-ing policy, so the best response (regardless of the learning policy) is optimal for each individual. Thirdly, we assume that the design space of contract is restricted to {x_h : S \u00d7 S \u2192 [0, \u03b7]} at any step h\u2208 [H]. This reflects the practical concern of contract design under randomness: while con-tract with bounded payment may sacrifice the optimality, it regularizes the variance in the payment transfer and reduces the risk for both the principal and agents. Moreover, as long as the environment parameters have finite precision, the parameter \u03b7 can be matched to the finite bit complexity of the optimal contract. Though this assumption is without loss of generality from a modeling perspec-tive, we expect future work to develop tighter analysis techniques to relax the dependency on \u03b7. For other regularity assumptions necessary to obtain tractable complexity results, we defer to the technical sections."}, {"title": "Warm-up: Solving the Contractual Bandit Learning Problem", "content": "In this section, we consider an important special case of the contractual reinforcement problem with H = 1, which allows us to first focus on the learning challenge from moral hazard without the concern of far-sight agency. We refer to this problem as the contractual bandit learning problem. Below, we first describe the contractual bandit learning problem with much simplified notations, since it suffices to omit the current state and the time step in the subscripts given that H = 1. We then showcase a generic analysis of the statistical complexity of contractual bandit learning problem."}, {"title": "The Contractual Bandit Learning Problem", "content": "In this setup, the agent's policy space is simply its action space A, i.e., the set of bandit arms. P: A\u2192\u25b3(S) specifies an outcome distribution for each action, where the outcome space S could naturally capture the reward stochasticity of each arm in bandit learning problems. The principal designs the contract x : S \u2192 R+, contingent on the outcome space S, to influence the agent's choice of action. The principal's reward and agent's cost are both function of the agent's action, r, c : A\u2192 [0, 1]. We consider a contractual bandit learning problem with T rounds. In the beginning of each round t, the principal commits to a contract xt and interacts with the agent as follows:\nHere, the noisy reward function satisfies Es~P(at) l(s) = r(at), and we assume the agent's action always maximizes his expected utility, i.e., at \u2208 argmaxaca{\\mathbb{E}_{s~P(a)} Xt(s) \u2212 c(a)}. To determine the principal's optimal contract, let us recall the notion of least payment function from the general setup. We similarly define \u03b6 : A \u2192 R+ such that for any given action a \u2208 A, it outputs the least amount of the expected payment necessary to induce a, \u03b6(a) := minx\u2208Xa P(a) \u2022 x, where Xa = {x \u2208 X : [P(a) \u2212 P(a')] \u2022 x \u2265 c(a) \u2212 c(a'), \u2200a' \u2260 a} denotes the set of all contracts under which the agent would respond with action a. Hence, the principal can determine the optimal action to induce, a* = maxa\u2208A \u2211t=1T[rt(a) \u2212 \u03b6(a)] with the optimal contract x* = argminx\u2208Xa* P(a*) \u2022 x. With the benchmark of the optimal contract x* that induces a* with the least payment \u03b6(a*), we can measure the learning performance in T rounds with the expected regret as follows, Reg(T) = maxa*\u2208A \u2211t=1T[r(a*)\u2212\u03b6(a*)] \u2212 \u2211t=1T[r(at) \u2212 Pt(at)\u2022xt]. This problem is a strict generalization of standard online learning, as it degenerates to the standard notion of regret when \u03b6(a) = 0, \u2200a \u2208 A. However, with the additional \u03b6 function, the no-regret learner must not only obtain good estimation of both r and \u03b6 towards the optimal action, but also implement the contracts that induce the optimal action and have expected payment approaching towards \u03b6.\nA Simpler Case with Direct Incentives. We remark that a special case of the contractual bandit learning problem assumes the principal is able to design her contract contingent on the agent's action. This enables the principal to implement any payment rule x : A \u2192 R+, and the agent responds with his optimal action a* = argmaxa\u2208Ax(a) \u2212 c(a). With this relaxation, \u03b6 = c, since the optimal x to induce any action a is to set a direct incentive with x(a) = c(a), x(a') = 0, \u2200a' \u2260 a. The expected regret reduces to Reg(T) = maxa*\u20ac\u0391 \u2211t=1T[r(a*)-c(a*)] \u2212 \u2211t=1T[r(at)-xt(at)]. As we will see in this paper, the learning problem becomes more tractable in this setup, since the principal can directly learn the cost function c to determine the least payment to induce each action. In Appendix C.2 and C.3 we showcases the multi-armed bandits and linear bandits under direct incentives, both of which have been recently studied by Scheid et al. [46]."}, {"title": "A Generic Approach to Contractual Bandit Learning", "content": "We begin with a natural assumption that enable us to simply employ existing techniques in online learning to obtain tractable complexity results for a large class of contractual bandit learning prob-lems.\nAssumption 1 (A-Inducibility). For any action a \u2208 A, there exists an event e \u2208 {0,1}S as a distribution of outcomes such that [P(a) \u2212 P(a')] \u2022 e \u2265 1,\u2200a' \u2260 a.\nThis assumption ensures the regularity of the problem instance in the sense that each action is dom-inantly capable of inducing a set of outcomes over others such that for any cost function c and any action a, there exists a contract x to induce a. To see this, one can explicitly construct such contract as x = emaxa, c(a)-c(a'), where e is the event such that [P(a) \u2212 P(a')]\u2022e \u2265 1,\u2200a' \u2260 a. Otherwise, if x \u2264 0, then there could be some action that is never the agent's best response under any contract.\nWe now propose a generic approach to design statistically efficient algorithm for contractual bandit learning problem. The key idea of our approach is to decouple the learning of the contract from the learning of the optimal action. In particular, let us first assume an oracle in Definition 1 that is able to construct a robust contract set for each action a \u2208 A, despite the uncertainty in parameter estimation. We use the robust contract set to determine the optimistic action and eventually learn"}, {"title": "Solving Contractual Bandit Problems under Moral Hazard", "content": "Theorem 2. Under Assumption 1, with a \u03f5-margin contract set for every action a \u2208 A, there is a generic algorithm with regret \u00d5 (\u03b7\u221aT + T\u03f5/\u03bb) for the contractual bandit learning problems.\nThe key step of the proof is Lemma 1, which shows the contracts solved from LP (C.1) have bounded suboptimality from the least payment contract (both in estimation and in execution) depending on parameter estimation error e and the robustness margin 8. This allows us to simply adopt an upper confidence bound argument to bound the regret. See Appendix C.1 for the full proof and the con-struction of the generic algorithm. The rationale behind Theorem 2 is to separate the learning of the contract sets from the learning of the optimal action. In particular, the learning and construc-tion procedure of such contract sets has been a well-established problem in variants of Stackelberg games [37, 43]. We abstract this problem into the design of a \u03c7(\u03f5)-learning procedure defined below.\nDefinition 2 (\u03c7(\u03f5)-Learning Procedure). For a \u03c7(\u03f5)-learning procedure, after any \u03c7(\u03f5) number of rounds, it can construct a robust contract set Xa\u03f5,\u2200a \u2208 A such that Xa\u03f5(\u03b5) \u2286 Xa \u2286 Xa.\nBased on the concept in Definition 2, an immediate implication of Theorem 2 is that if there is an O(1/\u03f5)-learning procedure, a simple \u201cprepare-then-commit\u201d style algorithm can achieve O(\u221aT) regret in the contractual bandit problem. That is, it first prepares for a warm start by running the learning procedure for T1/2 rounds to obtain the O(T-1/2)-margin contract sets, then commits to follow Algorithm 2 for the remaining T \u2013 T1/2 rounds. Futhermore, using the standard doubling trick [15], we can convert \u201cprepare-then-commit\" style algorithm into an anytime algorithm with the same O(\u221aT) regret guaruntee that is agnostic to the time horizon T during its construction. Therefore, the difficulty of solving the contractual bandit learning problem hinges on the statistical efficiency of the learning procedure, which heavily depends on the problem structure.\nSolving Bandit Problems under Direct Incentives. As a direct application of Theorem 2, we show that the O(1/\u03f5)-learning procedure can be constructed for the two bandit problems under direct incentives and thus admits O(\u221aT) regret online learning algorithm. The construction of the efficient search algorithm essentially relies on the binary search for the cost of each arm. In addition, the binary search algorithm can be generalized to cases with infinitely many arms. Such problem is known as the contextual search, and recent work [38] have established clean solutions with nearly optimal performance. We defer their detailed construction and proofs to Appendix C.2 and C.3.\nCorollary 2.1. Multi-armed bandits and linear bandits under direct incentives have \u00d5(\u221aT) regret.\nSolving Contractual Bandit Problems under Moral Hazard. The construction of efficient learn-ing procedure is difficult in general contractual bandit learning. We instead start with sufficient knowledge of P to construct an O(1/\u03f5)-learning procedure under the following assumption. This assumption is motivated by the practice, where the principal would ask the agent to provide a listing of desired conditions for him to perform different level of services. The search problem is otherwise known to have exponential sample complexity lower bound in Stackelberg games [43].\nAssumption 2 (Preliminary Contracts). For any a \u2208 A, the principal has the preliminary knowledge to construct an non-liable contract x that induces the agent's action a with constant payment.\nWe defer the construction of this learning procedure and its proof to Appendix C.4. As a result, we can construct an explore-then-commit style algorithm O(T2/3) regret for general contractual bandit learning, as. Specifically, this algorithm induces the agent to take each action uniformly random for T2/3 rounds under the Assumption 2. Then, given that the outcome distribution is estimated with error up to T-1/3, it can efficiently estimate the difference of cost up to error T-1/3 and thus construct an T-1/3-optimal contract to induce the optimal action a* in the remaining rounds.\nCorollary 2.2. Under Assumption 1 and 2, \u00d5(T2/3) regret can be achieved for contractual bandit learning problems."}, {"title": "The Complexity of Contractual Reinforcement Learning", "content": "This result reveals the core challenge of learning the optimal contract under moral hazard. That is, constructing the contract to induce the optimal action, [P(a*) \u2013 P(a')] \u2022 x \u2265 c(a) \u2013 c(a'), \u2200a' \u2260 a* already requires a sufficiently good estimate of P for all actions (including the suboptimal ones). This observation raises the question on whether it is possible to learn P(a') without playing the costly sub-optimal action a' the barrier to achieve o(T2/3) regret. The answer turns out to be \"Yes\" but with some catches. The solution is to implement a binary search procedure for contract x near the hyperplane formed by the linear system [P(a) \u2013 P(a')] \u2022 x = c(a) \u2013 c(a'),\u2200a' \u2260 a. We want to solve the parameters c(a) \u2013 c(a') and P(a) \u2013 P(a'),\u2200a' \u2260 a in the linear system with bounded errors using a number of contracts x that almost satisfy the linear system. This is however impossible unless knowing at least one set of parameters in the linear system to ensure it has full rank.\nCorollary 2.3. Under Assumption 1 and with the knowledge of agent's cost, \u00d5(T1/2) regret can be achieved for contractual bandit learning problems.\nIn Appendix D, we formally show that, knowing the agent's cost, there is an efficient learning proce-dure for the unknown parameters P(a) \u2013 P(a'),\u2200a' \u2260 a with small errors under mild assumptions. This allows us to attain \u00d5(\u221aT) for the general contractual bandit problem, and we showcase its ap-plication in designing contractual RL algorithms in the next section. Since the design and analysis of the learning procedure is highly technical, we also demonstrate the high-level idea on a simplified instance in Example 1 of Appendix D. More generally, we expect similar learning procedure exists if we alternatively assume some predictive state s in P such that the principal knows P(s0|a), \u2200a \u2208 A, since it would also eliminate one extra degree of freedom in the linear system above.\nIf we treat each stationary policy in contractual RL as an arm and its induced visitation measure (see its formal definition in Appendix E.1) as an outcome in the contractual bandit problem, the generic algorithm from Section 3.2 already provides a \u00d5(T2/3) regret bound. However, the computational and statistical complexity of both Algorithm 2 and 3 has polynomial dependence on the size of action space, which has become exponential as |II| = (SA)H. Moreover, as pointed out above, it requires a uniformly good knowledge over the transition kernel P to constructing the near-optimal contract policy under the moral hazard. In this section, we provide an improved analysis for the complexity of contractual reinforcement learning, given that the agent's cost function {ch}_{h=1}^H is known initially. This assumption allows us to leverage the learning procedure designed in the last section to efficiently learn the parameters \u03bc_h(s, a, a') := P_h(s,a) \u2013 P_h(s, a') for all h \u2208 [H], s \u2208 S, a, a' \u2208 A."}, {"title": "Contractual RL with Warm Start", "content": "Theorem 3. With high probability"}, {"title": "Contractual Reinforcement Learning: Pulling Arms with Invisible Hands", "authors": ["Jibang Wu", "Siyu Chen", "Mengdi Wang", "Huazheng Wang", "Haifeng Xu"], "abstract": "The agency problem emerges in today's large scale machine learning tasks, where the learners are unable to direct content creation or enforce data collection. In this work, we propose a theoretical framework for aligning economic interests of different stakeholders in the online learning problems through contract design. The problem, termed contractual reinforcement learning, naturally arises from the classic model of Markov decision processes, where a learning principal seeks to optimally influence the agent's action policy for their common interests through a set of payment rules contingent on the realization of next state. For the planning problem, we design an efficient dynamic programming algorithm to determine the optimal contracts against the far-sighted agent. For the learning problem, we introduce a generic design of no-regret learning algorithms to untangle the challenges from robust design of contracts to the balance of exploration and exploitation, reducing the complexity analysis to the construction of efficient search algorithms. For several natural classes of problems, we design tailored search algorithms that provably achieve  \u00d5(VT) regret. We also present an algorithm with  \u00d5(T^{2/3}) for the general problem that improves the existing analysis in online contract design with mild technical assumptions.", "sections": [{"title": "Introduction", "content": "The \"invisible hand\" metaphor by Adam Smith illustrates how properly designed incentive structures can guide self-interested individuals to inadvertently promote the greater social good. This concept is increasingly relevant in the realm of machine learning, as the scale of applications expands and the conflict of economic interests intensifies. For example, an Internet platform wants to estimate the ad revenues from serving different types of content, but it is up to the creators to decide what content to produce. While the platform seeks high-quality content to boost its long-term growth, creators may opt to minimize their production costs. This misalignment has prompted platforms to implement revenue-sharing models, fueling the growth of the creator economy, projected to exceed half a trillion by 2027. However, current incentive models are inadequate, especially in light of their roles in exacerbating the proliferation of clickbait and misinformation online [31, 54, 55]. Moreover, this issue of misalignment extends well beyond content platforms. E-commerce sites rely on sufficient consumers experimenting with new products for accurate preference assessments. Gig platforms depend on freelance workers accepting tasks to gather essential operational data. Even recommender systems are paying users for their engagement in order to effectively optimize their algorithms [3]. In these cases, the learner's hands are tied, and decision-makers interacting with the environment have their own objectives, dooming the system to under-exploration regardless of the learner's objective. Hence, there is a pressing need to pursue formal treatments of incentive alignment problems between the learners and decision-makers and to design principled learning algorithms with statistical and computational efficiency guarantees.\nContributions. On the conceptual side, the presence of self-interested decision-makers challenges our common assumption in online learning, where a single learner controls all the interactions with the environment. This paper introduces the contractual reinforcement learning (RL) problem in the principal-agent Markov decision process (PAMDP), where we adopt the principal-agent model from contract theory [24, 27] to capture strategic interactions between the learner and decision-maker. As illustrated in Figure 1, the learner (henceforth, principal/she) collects the rewards from the actions of decision-maker (henceforth, agent/he). Without any incentive design, the agent simply optimizes his policy in a standard Markov decision process (MDP) based on his cost function. However, since the agent's optimal policy is not necessarily in the principal's best interest, the principal is motivated to properly incentivize the agent to act in her favor by designing contracts that specify the payment rules contingent on the realization of the next state. The core challenge in this design problem is the information asymmetry at two levels: (1) the principal cannot observe the agent's action a priori and has to condition her payment on the probabilistic outcome of the action a phenomenon known as the moral hazard in economics; (2) the agent is far-sighted that he is willing to take suboptimal actions at one step in order to reach a more favorable state in future steps a major barrier for theoretical analysis in multi-agent learning problems.\nOn the technical side, this paper provides a comprehensive solution framework to address the unique learning and computational challenges when moral hazard meets far-sighted agency in contractual RL problems. In Section 2, we define state value functions for both the agent and principal, from which we derive a new class of Bellman equations to characterize the intricate correspondence be-tween the principal and agent's optimal policy. This leads to our Theorem 1, which shows that the principal's optimal planning problem can be solved by a clean formulation of dynamic programming in polynomial time. The learning problem is more involved, so we begin with the contractual bandit learning problem (episode length H = 1) in Section 3 to focus on the challenges from moral hazard. In particular, to achieve low regret, the principal's learning algorithm must balance exploration and exploitation while continuously improving its estimation of the agent's preferences to determine cost-efficient contracts. In Theorem 2, we construct a generic algorithm that reduces the learning problem into a standard online learning problem and an efficient search problem for the agent's decision boundary. As a consequence, we are able to obtain sublinear regret guarantee under different setups, summarized in Table 1. The efficient search algorithm we designed for learning the outcome distribution difference in the simplex may be of interest for general use. With these insights, we delve into the full contractual RL problem in Section 4 and show a provably efficient learning algorithm under several technical assumptions in Theorem 3. Meanwhile, the general result highlights a trade-off between statistical and computational tractability, leaving an intriguing open question on the existence of the best-of-both-worlds solution. The complexity of search is in logarithmic order yet with a large constant in the Markovian setup, and we expect an improved analysis by organically combining the search and exploration in the algorithm design."}, {"title": "Problem Formulation", "content": "Let us first recall the standard reinforcement learning problem in a (finite-horizon) Markov de-cision process (A, S, {P_h, r_h}_{h=1}^H, P_0), where we have the agent's action space A, the environ-ment's state space S, the transition kernel P_h : S \u00d7 A \u2192 \u0394(S), the expected reward function r_h: S\u00d7A\u2192 [0, 1], the initial state distribution P_0 \u2208 \u0394(S) and the horizon length H. The contrac-tual reinforcement learning problem simply extends the MDP to a principal-agent Markov decision process (A, S, {P_h,r_h, c_h}_{h=1}^H, P_0) with the additional cost function c_h : S \u00d7 A \u2192 [0, 1]. In this process, the agent interacts with the environment by taking actions and bearing the costs, whereas the principal receives the reward from the environment. Unable to directly interact with the envi-ronment, the principal has to instead design and implement contracts to incentivize the agent to take actions in her interest. Below, we formalize the design of their policies.\nFollowing from a standard MDP, the agent's action policy \\pi = {\u03c0_h : S \u2192 \u0394(A)}_{h=1}^H specifies that at each step h, given the state s, the agent would take the action a ~ \u03c0_h(s). In the following subsection, we will discuss how the agent chooses his action policy and that it suffices to only consider deterministic action policies. Meanwhile, the principal's contract policy x = {x_h : S \u00d7 S \u2192 \\mathbb{R}_+}_{h=1}^H is a sequence of non-liable payment rules x_h, where x_h(s_h, s_{h+1}) specifies the payment to the agent if the next state s_{h+1} is realized, given the current state s_h at the h-th step. The non-liability constraint ensures that the principal's payment in the contract for any realization of the next state must be non-negative; the problem would otherwise degenerate with an trivially optimal solution for the principal (see e.g., [24]). Denote II, X as the agent and principal's policy space, respectively. Let |S| = S, |A| = A and thus |II| = (SA)H.\nThe typical setting of the PAMDP problems can be summarized by the following steps. In the beginning of each episode, the initial state s_1 ~ P_0 is realized and observed by both the principal and the agent. Afterwards, the principal commits to a contract policy \u00e6 and the agent accordingly chooses an action policy \u03c0. Their interactions then proceed as follows at each step h \u2208 [H],\nIn this step, the principal's utility is r_h(s_h, s_{h+1}) - x_h(s_h, s_{h+1}), her reward minus the payment to agent, whereas the agent's utility is x_h (s_h, s_{h+1}) - c_h(s_h, a_h), the payment from principal minus his cost. The reward noise has zero mean such that r_h(s, a) = \\mathbb{E}_{s'~P_h(s,a)} r_h(s, s'), \u2200s \u2208 S, a \u2208 A. We refer the readers to Appendix B.1 for a summary of notations and Appendix B.2 for a full discussion of our modeling choices.\nWithout any contract design, the model reduces to a standard MDP (A, S, {P_h, c_h}_{h=1}^H, P_0) for the agent and the principal passively collects the reward from the agent's policy. This outcome could be suboptimal for both the principal and agent. Instead, by reshaping the agent's reward environment through the design of contract policy, the principal could induce the agent adopt some action policy with higher social surplus. This motivates the problem of designing the optimal contract policy. We focus on a realistic yet challenging setup in the face of a long-lived, far-sighted and Bayesian rational agent who is also planning optimally for his cumulative reward we expect the case of myopic agents can be worked out with simpler approach. In particular, since the agent's utility is not necessarily 0 under the principal's optimal contract at any state due to moral hazard, a far-sighted agent could take certain actions that are sub-optimal in the current step, yet secure him toward certain future states where he can obtain higher cumulative utility.\nWe extend notions of value functions and optimal policies from MDP to PAMDP. Under any action policy \u03c0 and contract policy x, we define the principal's state value function at the h-th step as,\nV_h^{x,\\pi}(s) := \\mathbb{E} \\bigg[\\sum_{\\tau=h}^H r_\\tau(s_\\tau, a_\\tau) - x_\\tau(s_\\tau, s_{\\tau+1})| \\{tr\\}_{t=h}^H, s_h = s\\bigg],\nand the agent's state value function at the h-th step as,\nU_h^{x,\\pi}(s) := \\mathbb{E} \\bigg[\\sum_{\\tau=h}^H x_\\tau(s_\\tau, s_{\\tau+1}) - c_\\tau(s_\\tau, a_\\tau)| \\{tr\\}_{t=h}^H, s_\\tau = s\\bigg],\nwhere the expectation in both V, U are with respect to the randomness of the trajectory (due to the stochasticity of state transitions and action policy). Let V^{x,\\pi} := \\mathbb{E}_{s~P_0} V_1^{x,\\pi}(s) and U^{x,\\pi} := \\mathbb{E}_{s~P_0} U_1^{x,\\pi}(s). The principal's goal is to maximize her value V^{x,\\pi}, given the agent's optimal response \u03c0, which equivalently maximizes V_1^{x,\\pi}(s) at any initial state s with P_0(s) > 0. Hence, we define the principal's optimal contract policy x^* = {x_h^*}_{h=1}^H and the corresponding optimal value"}, {"title": "The Optimal Contract Policy", "content": "V^* as the optimal solution and value of the following bi-level optimization problem,\nV^*, x^* := \\max_{x \\in X} V^{x,\\pi} \\quad \\text{s.t.} \\quad \\pi^* = \\arg\\max_{\\pi \\in \\Pi} U^{x,\\pi},\nwhere \"maxarg\" is a convenient operator notation on an optimization problem that returns the op-timal objective value followed by its optimal solution. For notational convenience, we will denote the agent's optimal action policy in response to contract policy \u03c0as \u03c0x = argmax\u03c0\u2208\u03c0 Ux,\u03c0, and use shorthands Vxh := V^{x,\\pi_x}, Uxh := U^{x,\\pi_x} for the principal's and agent's value function under contract policy x at the h-th step given that the agent responds optimally. Meanwhile, we denote x^* = argmaxx\u2208X Vx,\u03c0 s.t. \u03c0 = argmax\u03c0\u2208\u03a0 Ux,\u03c0 as the principal's optimal contract policy to induce the agent's action policy \u03c0. We use similar shorthands V\u03c0 := Vx,\u03c0, Ux := Ux,\u03c0 for the principal's and agent's value function under contract policy x\u03c0 at the h-th step given that the agent responds optimally. Notably, since the optimization problem (2.1) hinges on the intricate correspon-dence between x and \u03c0, it is unclear for now if the principal can efficiently plan his optimal policy adopting the standard approach in MDP.\nSolving for the Agent's Optimal Policy. One key observation is that the correspondence between \u03c0and\u00e6 has a clean characterization through the Bellman equation. Specifically, both functions {\u03c0h, U_h}_{h=1}^H can be solved through backward induction with U_{H+1}(s) = 0:\ngiven U_{h+1}, U_h(s), \u03c0_h(s) = \\arg\\max_{a \\in A} \\mathbb{E}_{s'\\sim P_h(s, a)} \\cdot [x_h(s) + U_{h+1}(s')] - c_h(s, a).\nNotice that since \u03c0h(s) is a maximizer of a linear function, the agent's best responding policy \u03c0h is deterministic without loss of generality. With \u03c0x, the principal's value function under x can also be computed iteratively from V_{H+1}(s) = 0:\ngiven V_{h+1}, V_h(s) = r_h(s, \u03c0_h(s)) + \\mathbb{E}_{s'~P_h(s, \u03c0_h(s))} \\cdot [V_{h+1} - x_h(s)].\nDue to the space limit, we only solve the agent's best response \u03c0x at any given \u00e6. We refer the reader to Appendix B.3 for the more involved formulation to solve the optimal policy x\u03c0 for any given \u03c0.\nValue Decomposition. Another key observation is that the value functions can decomposed into parts that are only depends on the agent's action policy. This is analogous to the standard contract design where principal's and agent's utility sums up to the social surplus, i.e., the difference between the reward and cost of the agent's action. Here, let the principal's expected reward and agent's expected cost function in the h-th step be\nR_h^{\\pi}(s) := \\mathbb{E} \\bigg[\\sum_{\\tau=h}^H r_\\tau(S_\\tau, A_\\tau)| \\{tr\\}_{t=h}^H, S_h = s\\bigg], C_h^{\\pi}(s) := \\mathbb{E} \\bigg[\\sum_{\\tau=h}^H c_\\tau(S_\\tau, A_\\tau)| \\{tr\\}_{t=h}^H, S_h = s\\bigg].\nBy linearity of expectation, for any policy x \u2208 \u03a7, \u03c0 \u2208 \u03a0, at any state s of any step h, we have\nV_h^{x,\\pi}(s) = R_h^{\\pi}(s) - C_h^{\\pi}(s) - U_h^{x,\\pi}(s).\nBoth functions R, C are fixed to the agent's action policy \u03c0, regardless of the contract policy x. In addition, C_h^{\\pi}(s) - U_h^{x,\\pi}(s) captures the total amount of expected payment transferred from the principal to the agent since h-th step at state s. Since the total reward is fixed under any given \u03c0, the principal's value is maximized under the minimal total payment, U_h^{*}(s) := C_h^{\\pi}(s) + U_h^{x,\\pi}(s). The function Uxh thus serves as the equivalent optimization objective in the least-payment Bellman equation in Appendix B.3.\nSolving for the Optimal Contract Policy. With the two observations above, it is clear that the principal's the optimal value and policy V^* = max\u03c0\u2208\u03a0 V\u03c0 can be determined by computing V\u03c0 for every \u03c0, according to the least-payment bellman equation in Appendix B.3. However, this maximization problem is still intractable, as there are exponentially many possible \u03c0. Instead, we have to interleave the process of solving for the optimal policy with least payment and maximum reward. This enables the following construction of a bi-level backward induction that iteratively solves for the optimal contract policy x*."}, {"title": "Bellman Equations of PAMDP", "content": "Theorem 1 (Bellman Equations of PAMDP). The optimal contract policy can solved by dynamic programming in polynomial time, from h = H to 1 with UH+1(s), VH+1(s) = 0,\u2200s \u2208 S, a \u2208 A,\nW_h(s, a; x) = \\mathbb{E}_{s'~P_h(s, a)} \\cdot [x + U_{h+1}] - c_h(s, a),\nx_h^{*}(s; a) = \\arg\\min_{x:S\u2192\\mathbb{R}_+} {\\mathbb{E}_{s'~P_h(s, a)} \\cdot x | W_h(s, a; x) \u2265 W_h(s, a'; x), \u2200a' \u2208 A},\nQ_h(s,a) = r_h(s, a) + \\mathbb{E}_{s'~P_h(s, a)} \\cdot [V_{h+1} - x_h(s; a)],\nV_h^{*}(s), \u03c0_h(s) = \\arg\\max_{a \\in A} Q_h(s, a), x_h(s) = x_h^{*}(s; \u03c0_h(s)), U_h^{*}(s) = W_h (s, \u03c0_h(s); x_h(s)),\nTo interpret the Bellman equation above, x_h^{*}(s; a) denotes the contract with the least payment to induce the agent to take action a at state s in step h. Given that \u03c0_h(s) is the best agent action for the principal to induce, the optimal contract at state s in step h can be determined as x_h(s) = x_h^{*}(s; \u03c0_h(s)). Q_h (s, a), W_h (s, a; x) are respectively the principal's and agent's total expected utility from h-th step under policy {x^*}_{H=h+1} and {\u03c0^*}_{H=h+1}, which can be viewed as their optimal state-action value function at h-th step, serving as the intermediate variable for the computation. See Appendix B.4 for the proof of correctness."}, {"title": "The Contractual Reinforcement Learning Problem", "content": "We now introduce the reinforcement learning problem in PAMDP, where the principal acts as the learner and seeks to adaptively improve its contract policy by interacting with the agent. Following the online learning convention, we use the expected regret to evaluate the learning performance in T episodes, \\text{Reg}(T) := \\sum_{t=1}^T V^* \u2013 V^{x_t},\\pi_t, where x_t is the principal's contract policy in the t-th episode.\nThis paper makes a few assumptions for the analysis of reinforcement learning problems. First, the far-sighted agent has perfect knowledge of his cost function and the state transition kernel {P_h, c_h}_{h\u2208 [H]} such that he can always chooses the best response. This is realistic because in appli-cations of our interest, agents are the experts (e.g., content creators, freelance workers, ride-sharing drivers) in the fields who has learnt about the environment sufficiently well whereas principal as the system designer does not know. Second, the agent at time t is assumed to best respond to xt. This can be equivalently interpreted as the agent at each time t showing up only once. This is motivated by the reality of Internet applications where each individual agent's participation only accounts for a negligible portion of the system's traffic hence has little influence over the entire system's learn-ing policy, so the best response (regardless of the learning policy) is optimal for each individual. Thirdly, we assume that the design space of contract is restricted to {x_h : S \u00d7 S \u2192 [0, \u03b7]} at any step h\u2208 [H]. This reflects the practical concern of contract design under randomness: while con-tract with bounded payment may sacrifice the optimality, it regularizes the variance in the payment transfer and reduces the risk for both the principal and agents. Moreover, as long as the environment parameters have finite precision, the parameter \u03b7 can be matched to the finite bit complexity of the optimal contract. Though this assumption is without loss of generality from a modeling perspec-tive, we expect future work to develop tighter analysis techniques to relax the dependency on \u03b7. For other regularity assumptions necessary to obtain tractable complexity results, we defer to the technical sections."}, {"title": "Warm-up: Solving the Contractual Bandit Learning Problem", "content": "In this section, we consider an important special case of the contractual reinforcement problem with H = 1, which allows us to first focus on the learning challenge from moral hazard without the concern of far-sight agency. We refer to this problem as the contractual bandit learning problem. Below, we first describe the contractual bandit learning problem with much simplified notations, since it suffices to omit the current state and the time step in the subscripts given that H = 1. We then showcase a generic analysis of the statistical complexity of contractual bandit learning problem."}, {"title": "The Contractual Bandit Learning Problem", "content": "In this setup, the agent's policy space is simply its action space A, i.e., the set of bandit arms. P: A\u2192\u25b3(S) specifies an outcome distribution for each action, where the outcome space S could naturally capture the reward stochasticity of each arm in bandit learning problems. The principal designs the contract x : S \u2192 R+, contingent on the outcome space S, to influence the agent's choice of action. The principal's reward and agent's cost are both function of the agent's action, r, c : A\u2192 [0, 1]. We consider a contractual bandit learning problem with T rounds. In the beginning of each round t, the principal commits to a contract xt and interacts with the agent as follows:\nIn this step, the noisy reward function satisfies Es~P(at) l(s) = r(at), and we assume the agent's action always maximizes his expected utility, i.e., at \u2208 argmaxaca{\\mathbb{E}_{s~P(a)} Xt(s) \u2212 c(a)}. To determine the principal's optimal contract, let us recall the notion of least payment function from the general setup. We similarly define \u03b6 : A \u2192 R+ such that for any given action a \u2208 A, it outputs the least amount of the expected payment necessary to induce a, \u03b6(a) := minx\u2208Xa P(a) \u2022 x, where Xa = {x \u2208 X : [P(a) \u2212 P(a')] \u2022 x \u2265 c(a) \u2212 c(a'), \u2200a' \u2260 a} denotes the set of all contracts under which the agent would respond with action a. Hence, the principal can determine the optimal action to induce, a* = maxa\u2208A \u2211t=1T[rt(a) \u2212 \u03b6(a)] with the optimal contract x* = argminx\u2208Xa* P(a*) \u2022 x. With the benchmark of the optimal contract x* that induces a* with the least payment \u03b6(a*), we can measure the learning performance in T rounds with the expected regret as follows, Reg(T) = maxa*\u2208A \u2211t=1T[r(a*)\u2212\u03b6(a*)] \u2212 \u2211t=1T[r(at) \u2212 Pt(at)\u2022xt]. This problem is a strict generalization of standard online learning, as it degenerates to the standard notion of regret when \u03b6(a) = 0, \u2200a \u2208 A. However, with the additional \u03b6 function, the no-regret learner must not only obtain good estimation of both r and \u03b6 towards the optimal action, but also implement the contracts that induce the optimal action and have expected payment approaching towards \u03b6.\nA Simpler Case with Direct Incentives. We remark that a special case of the contractual bandit learning problem assumes the principal is able to design her contract contingent on the agent's action. This enables the principal to implement any payment rule x : A \u2192 R+, and the agent responds with his optimal action a* = argmaxa\u2208Ax(a) \u2212 c(a). With this relaxation, \u03b6 = c, since the optimal x to induce any action a is to set a direct incentive with x(a) = c(a), x(a') = 0, \u2200a' \u2260 a. The expected regret reduces to Reg(T) = maxa*\u20ac\u0391 \u2211t=1T[r(a*)-c(a*)] \u2212 \u2211t=1T[r(at)-xt(at)]. As we will see in this paper, the learning problem becomes more tractable in this setup, since the principal can directly learn the cost function c to determine the least payment to induce each action. In Appendix C.2 and C.3 we showcases the multi-armed bandits and linear bandits under direct incentives, both of which have been recently studied by Scheid et al. [46]."}, {"title": "A Generic Approach to Contractual Bandit Learning", "content": "We begin with a natural assumption that enable us to simply employ existing techniques in online learning to obtain tractable complexity results for a large class of contractual bandit learning prob-lems.\nAssumption 1 (A-Inducibility). For any action a \u2208 A, there exists an event e \u2208 {0,1}S as a distribution of outcomes such that [P(a) \u2212 P(a')] \u2022 e \u2265 1,\u2200a' \u2260 a.\nThis assumption ensures the regularity of the problem instance in the sense that each action is dom-inantly capable of inducing a set of outcomes over others such that for any cost function c and any action a, there exists a contract x to induce a. To see this, one can explicitly construct such contract as x = emaxa, c(a)-c(a'), where e is the event such that [P(a) \u2212 P(a')]\u2022e \u2265 1,\u2200a' \u2260 a. Otherwise, if x \u2264 0, then there could be some action that is never the agent's best response under any contract.\nWe now propose a generic approach to design statistically efficient algorithm for contractual bandit learning problem. The key idea of our approach is to decouple the learning of the contract from the learning of the optimal action. In particular, let us first assume an oracle in Definition 1 that is able to construct a robust contract set for each action a \u2208 A, despite the uncertainty in parameter estimation. We use the robust contract set to determine the optimistic action and eventually learn"}, {"title": "Solving Contractual Bandit Problems under Moral Hazard", "content": "Theorem 2. Under Assumption 1, with a \u03f5-margin contract set for every action a \u2208 A, there is a generic algorithm with regret \u00d5 (\u03b7\u221aT + T\u03f5/\u03bb) for the contractual bandit learning problems.\nThe key step of the proof is Lemma 1, which shows the contracts solved from LP (C.1) have bounded suboptimality from the least payment contract (both in estimation and in execution) depending on parameter estimation error e and the robustness margin 8. This allows us to simply adopt an upper confidence bound argument to bound the regret. See Appendix C.1 for the full proof and the con-struction of the generic algorithm. The rationale behind Theorem 2 is to separate the learning of the contract sets from the learning of the optimal action. In particular, the learning and construc-tion procedure of such contract sets has been a well-established problem in variants of Stackelberg games [37, 43]. We abstract this problem into the design of a \u03c7(\u03f5)-learning procedure defined below.\nDefinition 2 (\u03c7(\u03f5)-Learning Procedure). For a \u03c7(\u03f5)-learning procedure, after any \u03c7(\u03f5) number of rounds, it can construct a robust contract set Xa\u03f5,\u2200a \u2208 A such that Xa\u03f5(\u03b5) \u2286 Xa \u2286 Xa.\nBased on the concept in Definition 2, an immediate implication of Theorem 2 is that if there is an O(1/\u03f5)-learning procedure, a simple \u201cprepare-then-commit\u201d style algorithm can achieve O(\u221aT) regret in the contractual bandit problem. That is, it first prepares for a warm start by running the learning procedure for T1/2 rounds to obtain the O(T-1/2)-margin contract sets, then commits to follow Algorithm 2 for the remaining T \u2013 T1/2 rounds. Futhermore, using the standard doubling trick [15], we can convert \u201cprepare-then-commit\" style algorithm into an anytime algorithm with the same O(\u221aT) regret guaruntee that is agnostic to the time horizon T during its construction. Therefore, the difficulty of solving the contractual bandit learning problem hinges on the statistical efficiency of the learning procedure, which heavily depends on the problem structure.\nSolving Bandit Problems under Direct Incentives. As a direct application of Theorem 2, we show that the O(1/\u03f5)-learning procedure can be constructed for the two bandit problems under direct incentives and thus admits O(\u221aT) regret online learning algorithm. The construction of the efficient search algorithm essentially relies on the binary search for the cost of each arm. In addition, the binary search algorithm can be generalized to cases with infinitely many arms. Such problem is known as the contextual search, and recent work [38] have established clean solutions with nearly optimal performance. We defer their detailed construction and proofs to Appendix C.2 and C.3.\nCorollary 2.1. Multi-armed bandits and linear bandits under direct incentives have \u00d5(\u221aT) regret.\nSolving Contractual Bandit Problems under Moral Hazard. The construction of efficient learn-ing procedure is difficult in general contractual bandit learning. We instead start with sufficient knowledge of P to construct an O(1/\u03f5)-learning procedure under the following assumption. This assumption is motivated by the practice, where the principal would ask the agent to provide a listing of desired conditions for him to perform different level of services. The search problem is otherwise known to have exponential sample complexity lower bound in Stackelberg games [43].\nAssumption 2 (Preliminary Contracts). For any a \u2208 A, the principal has the preliminary knowledge to construct an non-liable contract x that induces the agent's action a with constant payment.\nWe defer the construction of this learning procedure and its proof to Appendix C.4. As a result, we can construct an explore-then-commit style algorithm O(T2/3) regret for general contractual bandit learning, as. Specifically, this algorithm induces the agent to take each action uniformly random for T2/3 rounds under the Assumption 2. Then, given that the outcome distribution is estimated with error up to T-1/3, it can efficiently estimate the difference of cost up to error T-1/3 and thus construct an T-1/3-optimal contract to induce the optimal action a* in the remaining rounds.\nCorollary 2.2. Under Assumption 1 and 2, \u00d5(T2/3) regret can be achieved for contractual bandit learning problems."}, {"title": "The Complexity of Contractual Reinforcement Learning", "content": "This result reveals the core challenge of learning the optimal contract under moral hazard. That is, constructing the contract to induce the optimal action, [P(a*) \u2013 P(a')] \u2022 x \u2265 c(a) \u2013 c(a'), \u2200a' \u2260 a* already requires a sufficiently good estimate of P for all actions (including the suboptimal ones). This observation raises the question on whether it is possible to learn P(a') without playing the costly sub-optimal action a' the barrier to achieve o(T2/3) regret. The answer turns out to be \"Yes\" but with some catches. The solution is to implement a binary search procedure for contract x near the hyperplane formed by the linear system [P(a) \u2013 P(a')] \u2022 x = c(a) \u2013 c(a'),\u2200a' \u2260 a. We want to solve the parameters c(a) \u2013 c(a') and P(a) \u2013 P(a'),\u2200a' \u2260 a in the linear system with bounded errors using a number of contracts x that almost satisfy the linear system. This is however impossible unless knowing at least one set of parameters in the linear system to ensure it has full rank.\nCorollary 2.3. Under Assumption 1 and with the knowledge of agent's cost, \u00d5(T1/2) regret can be achieved for contractual bandit learning problems.\nIn Appendix D, we formally show that, knowing the agent's cost, there is an efficient learning proce-dure for the unknown parameters P(a) \u2013 P(a'),\u2200a' \u2260 a with small errors under mild assumptions. This allows us to attain \u00d5(\u221aT) for the general contractual bandit problem, and we showcase its ap-plication in designing contractual RL algorithms in the next section. Since the design and analysis of the learning procedure is highly technical, we also demonstrate the high-level idea on a simplified instance in Example 1 of Appendix D. More generally, we expect similar learning procedure exists if we alternatively assume some predictive state s in P such that the principal knows P(s0|a), \u2200a \u2208 A, since it would also eliminate one extra degree of freedom in the linear system above.\nIf we treat each stationary policy in contractual RL as an arm and its induced visitation measure (see its formal definition in Appendix E.1) as an outcome in the contractual bandit problem, the generic algorithm from Section 3.2 already provides a \u00d5(T2/3) regret bound. However, the computational and statistical complexity of both Algorithm 2 and 3 has polynomial dependence on the size of action space, which has become exponential as |II| = (SA)H. Moreover, as pointed out above, it requires a uniformly good knowledge over the transition kernel P to constructing the near-optimal contract policy under the moral hazard. In this section, we provide an improved analysis for the complexity of contractual reinforcement learning, given that the agent's cost function {ch}_{h=1}^H is known initially. This assumption allows us to leverage the learning procedure designed in the last section to efficiently learn the parameters \u03bc_h(s, a, a') := P_h(s,a) \u2013 P_h(s, a') for all h \u2208 [H], s \u2208 S, a, a' \u2208 A."}, {"title": "Contractual RL with Warm Start", "content": "Theorem 3. With high probability, Algorithm 1 has \u00d5 ((H2SA\u22121/2 + \u03ba\u22121/2) H2\u221aT) regret using the solver in Algorithm 6 and \u00d5 ((H2SA\u22121/2 + \u03b7\u03bb\u2212HK\u22121/2)\u221aT) regret using the solver in Al-gorithm 7 in contractual RL under mild assumptions.\nx(\u03f5)-Learning Procedure in Contractual RL. One challenge in the construction is the need to separate the stepwise interference among {th}_{h=1}^H. Otherwise, the actual response space for the agent is (SA)H, which is unacceptable even for doing binary search. Our solution is due to the observation that if we fix xh+1, . . ., xH and tune xh only, the agent's expected profits Uh+1,..., UH remain unchanged. This allows us to set xh without influencing the agentcontinue generating"}]}]}