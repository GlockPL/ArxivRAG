{"title": "Contractual Reinforcement Learning: Pulling Arms with Invisible Hands", "authors": ["Jibang Wu", "Siyu Chen", "Mengdi Wang", "Huazheng Wang", "Haifeng Xu"], "abstract": "The agency problem emerges in today's large scale machine learning tasks, where the learners are unable to direct content creation or enforce data collection. In this work, we propose a theoretical framework for aligning economic interests of different stakeholders in the online learning problems through contract design. The problem, termed contractual reinforcement learning, naturally arises from the classic model of Markov decision processes, where a learning principal seeks to optimally influence the agent's action policy for their common interests through a set of payment rules contingent on the realization of next state. For the planning problem, we design an efficient dynamic programming algorithm to determine the optimal contracts against the far-sighted agent. For the learning problem, we introduce a generic design of no-regret learning algorithms to untangle the challenges from robust design of contracts to the balance of exploration and exploitation, reducing the complexity analysis to the construction of efficient search algorithms. For several natural classes of problems, we design tailored search algorithms that provably achieve $\\tilde{O}(VT)$ regret. We also present an algorithm with $\\tilde{O}(T^{2/3})$ for the general problem that improves the existing analysis in online contract design with mild technical assumptions.", "sections": [{"title": "Introduction", "content": "The \"invisible hand\" metaphor by Adam Smith illustrates how properly designed incentive structures can guide self-interested individuals to inadvertently promote the greater social good. This concept is increasingly relevant in the realm of machine learning, as the scale of applications expands and the conflict of economic interests intensifies. For example, an Internet platform wants to estimate the ad revenues from serving different types of content, but it is up to the creators to decide what content to produce. While the platform seeks high-quality content to boost its long-term growth, creators may opt to minimize their production costs. This misalignment has prompted platforms to implement revenue-sharing models, fueling the growth of the creator economy, projected to exceed half a trillion by 2027 [1, 2, 16, 25]. However, current incentive models are inadequate, especially in light of their roles in exacerbating the proliferation of clickbait and misinformation online [31, 54, 55]. Moreover, this issue of misalignment extends well beyond content platforms. E-commerce sites rely on sufficient consumers experimenting with new products for accurate preference assessments. Gig platforms depend on freelance workers accepting tasks to gather essential operational data. Even recommender systems are paying users for their engagement in order to effectively optimize their algorithms [3]. In these cases, the learner's hands are tied, and decision-makers interacting with the environment have their own objectives, dooming the system to under-exploration regardless of the learner's objective. Hence, there is a pressing need to pursue formal treatments of incentive alignment problems between the learners and decision-makers and to design principled learning algorithms with statistical and computational efficiency guarantees.\nContributions. On the conceptual side, the presence of self-interested decision-makers challenges our common assumption in online learning, where a single learner controls all the interactions with the environment. This paper introduces the contractual reinforcement learning (RL) problem in the principal-agent Markov decision process (PAMDP), where we adopt the principal-agent model from contract theory [24, 27] to capture strategic interactions between the learner and decision-maker. As illustrated in Figure 1, the learner (henceforth, principal/she) collects the rewards from the actions of decision-maker (henceforth, agent/he). Without any incentive design, the agent simply optimizes his policy in a standard Markov decision process (MDP) based on his cost function. However, since the agent's optimal policy is not necessarily in the principal's best interest, the principal is motivated to properly incentivize the agent to act in her favor by designing contracts that specify the payment rules contingent on the realization of the next state. The core challenge in this design problem is the information asymmetry at two levels: (1) the principal cannot observe the agent's action a priori and has to condition her payment on the probabilistic outcome of the action a phenomenon known as the moral hazard in economics; (2) the agent is far-sighted that he is willing to take suboptimal actions at one step in order to reach a more favorable state in future steps a major barrier for theoretical analysis in multi-agent learning problems.\nOn the technical side, this paper provides a comprehensive solution framework to address the unique learning and computational challenges when moral hazard meets far-sighted agency in contractual RL problems. In Section 2, we define state value functions for both the agent and principal, from which we derive a new class of Bellman equations to characterize the intricate correspondence be- tween the principal and agent's optimal policy. This leads to our Theorem 1, which shows that the principal's optimal planning problem can be solved by a clean formulation of dynamic programming in polynomial time. The learning problem is more involved, so we begin with the contractual bandit learning problem (episode length $H = 1$) in Section 3 to focus on the challenges from moral hazard. In particular, to achieve low regret, the principal's learning algorithm must balance exploration and exploitation while continuously improving its estimation of the agent's preferences to determine cost-efficient contracts. In Theorem 2, we construct a generic algorithm that reduces the learning problem into a standard online learning problem and an efficient search problem for the agent's decision boundary. As a consequence, we are able to obtain sublinear regret guarantee under different setups, summarized in Table 1. The efficient search algorithm we designed for learning the outcome distribution difference in the simplex may be of interest for general use. With these insights, we delve into the full contractual RL problem in Section 4 and show a provably efficient learning algorithm under several technical assumptions in Theorem 3. Meanwhile, the general result highlights a trade-off between statistical and computational tractability, leaving an intriguing open question on the existence of the best-of-both-worlds solution. The complexity of search is in logarithmic order yet with a large constant in the Markovian setup, and we expect an improved analysis by organically combining the search and exploration in the algorithm design."}, {"title": "Problem Formulation", "content": "Let us first recall the standard reinforcement learning problem in a (finite-horizon) Markov de- cision process $(\\mathcal{A}, \\mathcal{S}, \\{P_h, r_h\\}_{h=1}^H, P_0)$, where we have the agent's action space $\\mathcal{A}$, the environ- ment's state space $\\mathcal{S}$, the transition kernel $P_h : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$, the expected reward function $r_h: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$, the initial state distribution $P_0 \\in \\Delta(\\mathcal{S})$ and the horizon length $H$. The contrac- tual reinforcement learning problem simply extends the MDP to a principal-agent Markov decision process $(\\mathcal{A}, \\mathcal{S}, \\{P_h,r_h, c_h\\}_{h=1}^H, P_0)$ with the additional cost function $c_h : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$. 6 In this process, the agent interacts with the environment by taking actions and bearing the costs, whereas the principal receives the reward from the environment. Unable to directly interact with the envi- ronment, the principal has to instead design and implement contracts to incentivize the agent to take actions in her interest. Below, we formalize the design of their policies."}, {"title": "The Principal-Agent Markov Decision Process", "content": "Following from a standard MDP, the agent's action policy $\\pi = \\{\\pi_h : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})\\}_{h=1}^H$ specifies that at each step $h$, given the state $s$, the agent would take the action $a \\sim \\pi_h(s)$. In the following subsection, we will discuss how the agent chooses his action policy and that it suffices to only consider deterministic action policies. Meanwhile, the principal's contract policy $x = \\{x_h : \\mathcal{S} \\times \\mathcal{S} \\rightarrow \\mathbb{R}_+\\}_{h=1}^H$ is a sequence of non-liable payment rules $x_h$, where $x_h(s_h, s_{h+1})$ specifies the payment to the agent if the next state $s_{h+1}$ is realized, given the current state $s_h$ at the $h$-th step. The non-liability constraint ensures that the principal's payment in the contract for any realization of the next state must be non-negative; the problem would otherwise degenerate with an trivially optimal solution for the principal (see e.g., [24]). Denote $\\Pi, \\mathcal{X}$ as the agent and principal's policy space, respectively. Let $|\\mathcal{S}| = S, |\\mathcal{A}| = A$ and thus $|\\Pi| = (SA)^H$.\nThe typical setting of the PAMDP problems can be summarized by the following steps. In the beginning of each episode, the initial state $s_1 \\sim P_0$ is realized and observed by both the principal and the agent. Afterwards, the principal commits to a contract policy $x$ and the agent accordingly chooses an action policy $\\pi$. Their interactions then proceed as follows at each step $h \\in [H]$,\n1.  The agent takes an action $a_h \\sim \\pi_h(s_h)$ and bears the cost $c_h (s_h, a_h)$.\n2.  The next state $s_{h+1} \\sim P_h(s_h, a_h)$ is realized and observed by both the principal and agent.\n3.  The principal receives a noisy reward $\\tilde{r}_h(s_h, s_{h+1})$ and pays the agent $x_h(s_h, s_{h+1})$.\n4.  The principal observes the agent's action $a_h$.\nIn this step, the principal's utility is $\\tilde{r}_h(s_h, s_{h+1}) - x_h(s_h, s_{h+1})$, her reward minus the payment to agent, whereas the agent's utility is $x_h (s_h, s_{h+1}) - c_h(s_h, a_h)$, the payment from principal minus his cost. The reward noise has zero mean such that $r_h(s, a) = \\mathbb{E}_{s'\\sim P_h(s,a)} \\tilde{r}_h(s, s'), \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}$. We refer the readers to Appendix B.1 for a summary of notations and Appendix B.2 for a full discussion of our modeling choices."}, {"title": "The Optimal Contract Policy", "content": "Without any contract design, the model reduces to a standard MDP $(\\mathcal{A}, \\mathcal{S}, \\{P_h, c_h\\}_{h=1}^H, P_0)$ for the agent and the principal passively collects the reward from the agent's policy. This outcome could be suboptimal for both the principal and agent. Instead, by reshaping the agent's reward environment through the design of contract policy, the principal could induce the agent adopt some action policy with higher social surplus. This motivates the problem of designing the optimal contract policy. We focus on a realistic yet challenging setup in the face of a long-lived, far-sighted and Bayesian rational agent who is also planning optimally for his cumulative reward we expect the case of myopic agents can be worked out with simpler approach. In particular, since the agent's utility is not necessarily 0 under the principal's optimal contract at any state due to moral hazard, a far-sighted agent could take certain actions that are sub-optimal in the current step, yet secure him toward certain future states where he can obtain higher cumulative utility.\nWe extend notions of value functions and optimal policies from MDP to PAMDP. Under any action policy $\\pi$ and contract policy $x$, we define the principal's state value function at the $h$-th step as,\n$V_h^{x,\\pi}(s) := \\mathbb{E} \\Big[\\sum_{\\tau=h}^H r_{\\tau}(s_{\\tau}, a_{\\tau}) - x_{\\tau}(s_{\\tau}, s_{\\tau+1})| \\{tr\\}_{t=h}^H, s_{\\tau} = s\\Big]$\nand the agent's state value function at the $h$-th step as,\n$U_h^{x,\\pi}(s) := \\mathbb{E} \\Big[\\sum_{\\tau=h}^H x_{\\tau}(s_{\\tau}, s_{\\tau+1}) - c_{\\tau}(s_{\\tau}, a_{\\tau}) | \\{tr\\}_{t=h}^H, s_{\\tau} = s\\Big]$\nwhere the expectation in both $V, U$ are with respect to the randomness of the trajectory (due to the stochasticity of state transitions and action policy). Let $V^{x,\\pi} := \\mathbb{E}_{s \\sim P_0} V_1^{x,\\pi}(s)$ and $U^{x,\\pi} := \\mathbb{E}_{s \\sim P_0} U_1^{x,\\pi}(s)$. The principal's goal is to maximize her value $V^{x,\\pi}$, given the agent's optimal response $\\pi$, which equivalently maximizes $V_1^{x,\\pi} (s)$ at any initial state $s$ with $P_0(s) > 0$. Hence, we define the principal's optimal contract policy $x^* = \\{x_h^*\\}_{h=1}^H$ and the corresponding optimal value"}, {"title": "Solving for the Agent's Optimal Policy", "content": "One key observation is that the correspondence between $\\pi$and$x$ has a clean characterization through the Bellman equation. Specifically, both functions $\\{\\pi, U\\}_{h=1}^H$ can be solved through backward induction with $U_{H+1}(s) = 0$:\ngiven $U_{h+1}$, $U_h(s), \\pi_h(s) = \\underset{a\\in \\mathcal{A}}{\\text{maxarg}} \\mathbb{E}_{s'\\sim P_h(s, a)} [x_h(s) + U_{h+1}] - c_h(s, a)$.\nNotice that since $\\pi_h(s)$ is a maximizer of a linear function, the agent's best responding policy $\\pi_h$ is deterministic without loss of generality. With $\\pi^x$, the principal's value function under $x$ can also be computed iteratively from $V_{H+1}(s) = 0$:\ngiven $V_{h+1}$, $V_h(s) = r_h(s, \\pi_h(s)) + \\mathbb{E}_{s'\\sim P_h(s, \\pi_h(s))} [V_{h+1} - x_h(s)].\nDue to the space limit, we only solve the agent's best response $\\pi^x$ at any given $x$. We refer the reader to Appendix B.3 for the more involved formulation to solve the optimal policy $x^\\pi$ for any given $\\pi$.\nAnother key observation is that the value functions can decomposed into parts that are only depends on the agent's action policy. This is analogous to the standard contract design where principal's and agent's utility sums up to the social surplus, i.e., the difference between the reward and cost of the agent's action. Here, let the principal's expected reward and agent's expected cost function in the $h$-th step be\n$R_h^{\\pi}(s) := \\mathbb{E} [\\sum_{\\tau=h}^H r_{\\tau}(s_{t}, a_{\\tau})| \\{tr\\}_{t=h}^H, s_{\\tau} = s] , C_h^{\\pi}(s) := \\mathbb{E} [\\sum_{\\tau=h}^H c_{\\tau}(s_{t}, a_{\\tau})| \\{tr\\}_{t=h}^H, s_{h} = s]$\nBy linearity of expectation, for any policy $x \\in \\mathcal{X}, \\pi \\in \\Pi$, at any state $s$ of any step $h$, we have\n$V_h^{x,\\pi}(s) = R_h^{\\pi}(s) - C_h^{\\pi}(s) - U_h^{x,\\pi}(s)$.\nBoth functions $R, C$ are fixed to the agent's action policy $\\pi$, regardless of the contract policy $x$. In addition, $C_h^{\\pi}(s) - U_h^{x,\\pi}(s)$ captures the total amount of expected payment transferred from the principal to the agent since $h$-th step at state $s$. Since the total reward is fixed under any given $\\pi$, the principal's value is maximized under the minimal total payment, $\\zeta_h^{\\pi}(s) := R_h^{\\pi}(s) - C_h^{\\pi}(s) + U_h^{x,\\pi}(s)$. The function thus serves as the equivalent optimization objective in the least-payment Bellman equation in Appendix B.3.\nWith the two observations above, it is clear that the principal's the optimal value and policy $V^* = \\underset{\\pi\\in \\Pi}{\\text{max}} V^{x,\\pi}$ can be determined by computing $V^{\\pi}$ for every $\\pi$, according to the least-payment bellman equation in Appendix B.3. However, this maximization problem is still intractable, as there are exponentially many possible $\\pi$. Instead, we have to interleave the process of solving for the optimal policy with least payment and maximum reward. This enables the following construction of a bi-level backward induction that iteratively solves for the optimal contract policy $x^*$"}, {"title": "Bellman Equations of PAMDP", "content": "The optimal contract policy can solved by dynamic programming in polynomial time, from $h = H$ to 1 with $U_{H+1}(s), V_{H+1}(s) = 0,\\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}$,\n$W_h^x(s, a; x) = \\mathbb{E}_{s'\\sim P_h(s, a)} [x + U_{h+1}] - c_h(s, a)$,\n$x_h^*(s; a) = \\underset{x:\\mathcal{S}\\rightarrow \\mathbb{R}_+}{\\text{argmin}} \\{\\mathbb{E}_{s'\\sim P_h(s, a)} x | W_h^x(s, a; x) \\geq W_h^x(s, a'; x), \\forall a' \\in \\mathcal{A}\\}$,\n$Q_h^*(s,a) = r_h(s, a) + \\mathbb{E}_{s'\\sim P_h(s, a)} [V_{h+1} - x_h^*(s; a)]$,\n$V_h^*(s), \\pi_h^*(s) = \\underset{a\\in \\mathcal{A}}{\\text{maxarg}} Q_h^*(s, a), x_h^*(s) = x_h^*(s; \\pi_h(s)), U_h^*(s) = W_h^x(s, \\pi_h(s); x_h^*(s))$,\nTo interpret the Bellman equation above, $x_h^*(s; a)$ denotes the contract with the least payment to induce the agent to take action $a$ at state $s$ in step $h$. Given that $\\pi_h^*(8)$ is the best agent action for the principal to induce, the optimal contract at state $s$ in step $h$ can be determined as $x_h(s) = x_h^*(s; \\pi_h(s))$. $Q_h^* (s, a), W_h^x(s, a; x)$ are respectively the principal's and agent's total expected utility from $h$-th step under policy $\\{x^*\\}_{h=h+1}^H$ and $\\{\\pi^*\\}_{h=h+1}^H$, which can be viewed as their optimal state- action value function at $h$-th step, serving as the intermediate variable for the computation. See Appendix B.4 for the proof of correctness."}, {"title": "The Contractual Reinforcement Learning Problem", "content": "We now introduce the reinforcement learning problem in PAMDP, where the principal acts as the learner and seeks to adaptively improve its contract policy by interacting with the agent. Following the online learning convention, we use the expected regret to evaluate the learning performance in T episodes, $\\text{Reg}(T) := \\sum_{t=1}^T V^* - V^{x^t}$, where $x^t$ is the principal's contract policy in the $t$-th episode.\nThis paper makes a few assumptions for the analysis of reinforcement learning problems. First, the far-sighted agent has perfect knowledge of his cost function and the state transition kernel $\\{P_h, c_h\\}_{h \\in [H]}$ such that he can always chooses the best response. This is realistic because in appli- cations of our interest, agents are the experts (e.g., content creators, freelance workers, ride-sharing drivers) in the fields who has learnt about the environment sufficiently well whereas principal as the system designer does not know. Second, the agent at time $t$ is assumed to best respond to $x_t$. This can be equivalently interpreted as the agent at each time $t$ showing up only once. This is motivated by the reality of Internet applications where each individual agent's participation only accounts for a negligible portion of the system's traffic hence has little influence over the entire system's learn- ing policy, so the best response (regardless of the learning policy) is optimal for each individual. Thirdly, we assume that the design space of contract is restricted to $\\{x_h : \\mathcal{S} \\times \\mathcal{S} \\rightarrow [0, \\eta]\\}$ at any step $h \\in [H]$. This reflects the practical concern of contract design under randomness: while con- tract with bounded payment may sacrifice the optimality, it regularizes the variance in the payment transfer and reduces the risk for both the principal and agents. Moreover, as long as the environment parameters have finite precision, the parameter $\\eta$ can be matched to the finite bit complexity of the optimal contract. Though this assumption is without loss of generality from a modeling perspec- tive, we expect future work to develop tighter analysis techniques to relax the dependency on $\\eta$. For other regularity assumptions necessary to obtain tractable complexity results, we defer to the technical sections."}, {"title": "Warm-up: Solving the Contractual Bandit Learning Problem", "content": "In this section, we consider an important special case of the contractual reinforcement problem with $H = 1$, which allows us to first focus on the learning challenge from moral hazard without the concern of far-sight agency. We refer to this problem as the contractual bandit learning problem. Below, we first describe the contractual bandit learning problem with much simplified notations, since it suffices to omit the current state and the time step in the subscripts given that $H = 1$. We then showcase a generic analysis of the statistical complexity of contractual bandit learning problem."}, {"title": "The Contractual Bandit Learning Problem", "content": "In this setup, the agent's policy space is simply its action space $\\mathcal{A}$, i.e., the set of bandit arms. $P: \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ specifies an outcome distribution for each action, where the outcome space $\\mathcal{S}$ could naturally capture the reward stochasticity of each arm in bandit learning problems. The principal designs the contract $x : \\mathcal{S} \\rightarrow \\mathbb{R}_+$, contingent on the outcome space $\\mathcal{S}$, to influence the agent's choice of action. The principal's reward and agent's cost are both function of the agent's action, $r, c : \\mathcal{A} \\rightarrow [0, 1]$. We consider a contractual bandit learning problem with $T$ rounds. In the beginning of each round $t$, the principal commits to a contract $x_t$ and interacts with the agent as follows:\n1.  The agent takes the action $a_t$.\n2.  The outcome $s_t \\sim P(a_t)$ is realized and observed by both the principal and agent.\n3.  The principal receives the noisy reward $r_t(s_t)$ and pays the agent $x_t(s_t)$.\n4.  The principal observes the agent's action $a_t$.\nHere, the noisy reward function satisfies $\\mathbb{E}_{s \\sim P(a_t)} l(s) = r(a_t)$, and we assume the agent's action always maximizes his expected utility, i.e., $a_t \\in \\underset{a\\in \\mathcal{A}}{\\text{argmax}}\\{\\mathbb{E}_{P(a)} \\cdot x_t - c(a)\\}$. To determine the principal's optimal contract, let us recall the notion of least payment function from the general setup. We similarly define $\\zeta : \\mathcal{A} \\rightarrow \\mathbb{R}_+$ such that for any given action $a \\in \\mathcal{A}$, it outputs the least amount of the expected payment necessary to induce $a$, $\\zeta(a) := \\underset{x \\in X_a}{\\text{min}} \\mathbb{E} P(a) \\cdot x$, where $X_a = \\{x \\in \\mathcal{X} : [P(a) - P(a')] \\cdot x \\geq c(a) - c(a'), \\forall a' \\neq a\\}$ denotes the set of all contracts under which the agent would respond with action $a$. Hence, the principal can determine the optimal action to induce, $a^* = \\underset{a\\in \\mathcal{A}}{\\text{max}} \\sum_{t=1}^T [r_t(a) - \\zeta(a)]$ with the optimal contract $x^* = \\underset{x\\in X_{a^*}}{\\text{argmin}} P(a^*) \\cdot x$. With the benchmark of the optimal contract $x^*$ that induces $a^*$ with the least payment $\\zeta(a^*)$, we can measure the learning performance in $T$ rounds with the expected regret as follows, $\\text{Reg}(T) = \\underset{a^* \\in \\mathcal{A}}{\\text{max}} \\sum_{t=1}^T [r(a^*) - \\zeta(a^*)] - \\sum_{t=1}^T [r(a_t) - P(a_t) \\cdot x_t]$. This problem is a strict generalization of standard online learning, as it degenerates to the standard notion of regret when $\\zeta(a) = 0, \\forall a \\in \\mathcal{A}$. However, with the additional $\\zeta$ function, the no-regret learner must not only obtain good estimation of both $r$ and $\\zeta$ towards the optimal action, but also implement the contracts that induce the optimal action and have expected payment approaching towards $\\zeta$.\nWe remark that a special case of the contractual bandit learning problem assumes the principal is able to design her contract contingent on the agent's action. This enables the principal to implement any payment rule $x : \\mathcal{A} \\rightarrow \\mathbb{R}_+$, and the agent responds with his optimal action $a^* = \\underset{a\\in \\mathcal{A}}{\\text{argmax}} x(a) - c(a)$. With this relaxation, $\\zeta = c$, since the optimal $x$ to induce any action $a$ is to set a direct incentive with $x(a) = c(a), x(a') = 0, \\forall a' \\neq a$. The expected regret reduces to $\\text{Reg}(T) = \\underset{a^* \\in \\mathcal{A}}{\\text{max}} \\sum_{t=1}^T [r(a^*)-c(a^*)] - \\sum_{t=1}^T [r(a_t)-x_t(a_t)]$. As we will see in this paper, the learning problem becomes more tractable in this setup, since the principal can directly learn the cost function $\\zeta$ to determine the least payment to induce each action. In Appendix C.2 and C.3 we showcases the multi-armed bandits and linear bandits under direct incentives, both of which have been recently studied by Scheid et al. [46]."}, {"title": "A Generic Approach to Contractual Bandit Learning", "content": "We begin with a natural assumption that enable us to simply employ existing techniques in online learning to obtain tractable complexity results for a large class of contractual bandit learning prob- lems.\nFor any action $a \\in \\mathcal{A}$, there exists an event $e \\in \\{0,1\\}^S$ as a distribution of outcomes such that $[P(a) - P(a')] \\cdot e \\geq 1, \\forall a' \\neq a$.\nThis assumption ensures the regularity of the problem instance in the sense that each action is dom- inantly capable of inducing a set of outcomes over others such that for any cost function $c$ and any action $a$, there exists a contract $x$ to induce $a$. To see this, one can explicitly construct such contract as $x = \\underset{a\\in \\mathcal{A}}{\\text{e max}} c(a)-c(a')$, where $e$ is the event such that $[P(a) - P(a')]\\cdot e \\geq 1, \\forall a' \\neq a$. Otherwise, if $x \\leq 0$, then there could be some action that is never the agent's best response under any contract.\nWe now propose a generic approach to design statistically efficient algorithm for contractual bandit learning problem. The key idea of our approach is to decouple the learning of the contract from the learning of the optimal action. In particular, let us first assume an oracle in Definition 1 that is able to construct a robust contract set for each action $a \\in \\mathcal{A}$, despite the uncertainty in parameter estimation. We use the robust contract set to determine the optimistic action and eventually learn"}, {"title": "The Complexity of Contractual Reinforcement Learning", "content": "If we treat each stationary policy in contractual RL as an arm and its induced visitation measure (see its formal definition in Appendix E.1) as an outcome in the contractual bandit problem, the generic algorithm from Section 3.2 already provides a $\\tilde{O}(T^{2/3})$ regret bound. However, the computational and statistical complexity of both Algorithm 2 and 3 has polynomial dependence on the size of action space, which has become exponential as $|\\Pi| = (SA)^H$. Moreover, as pointed out above, it requires a uniformly good knowledge over the transition kernel $P$ to constructing the near-optimal contract policy under the moral hazard. In this section, we provide an improved analysis for the complexity of contractual reinforcement learning, given that the agent's cost function $\\{c_h\\}_{h=1}^H$ is known initially. This assumption allows us to leverage the learning procedure designed in the last section to efficiently learn the parameters $\\mu_h(s, a, a') := P_h(s,a) - P_h(s, a')$ for all $h \\in [H], s \\in \\mathcal{S}, a, a' \\in \\mathcal{A}$.\nWe sketch the no-regret learning algorithm in contractual RL in Algorithm 1, which cuts the number of episodes $T$ into two phases and can be improved to be agnostic to $T$ with the doubling trick. It begins by running the $\\chi(\\epsilon)$-learning procedure to efficiently obtain the estimated parameter $\\hat{\\mu}$ for the construction of robust contract policy. Then, the algorithm use a solver to determine the robust contract policy $x^t$ that induces an optimistic action policy $\\pi^t$ with almost optimal payment. In Theorem 3, we state the complexity results under two different solvers that work under different technical assumption and provides different trade-offs in statistical and computational complexity. Here, $\\kappa, \\lambda_{\\mathcal{S}}$ in the regret bound are constants in the regularity assumptions, and omit log T terms from learning $\\mu_h$, though the effect of these constants can be canceled out only for sufficiently large $T$; we defer the details to Appendix E. Below we zoom into the construction of each component."}]}