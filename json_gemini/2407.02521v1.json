{"title": "Performance Comparison of Deep RL Algorithms for Mixed Traffic Cooperative Lane-Changing", "authors": ["Xue Yao", "Shengren Hou", "Serge P. Hoogendoorn", "Simeon C. Calvert"], "abstract": "Lane-changing (LC) is a challenging scenario for connected and automated vehicles (CAVs) because of the complex dynamics and high uncertainty of the traffic environment. This challenge can be handled by deep reinforcement learning (DRL) approaches, leveraging their data-driven and model-free nature. Our previous work proposed a cooperative lane-changing in mixed traffic (CLCMT) mechanism based on TD3 to facilitate an optimal lane-changing strategy. This study enhances the current CLCMT mechanism by considering both the uncertainty of the human-driven vehicles (HVs) and the microscopic interactions between HVs and CAVs. The state-of-the-art (SOTA) DRL algorithms including DDPG, TD3, SAC, and PPO are utilized to deal with the formulated MDP with continuous actions. Performance comparison among the four DRL algorithms demonstrates that DDPG, TD3, and PPO algorithms can deal with uncertainty in traffic environments and learn well-performed LC strategies in terms of safety, efficiency, comfort, and ecology. The PPO algorithm outperforms the other three algorithms, regarding a higher reward, fewer exploration mistakes and crashes, and a more comfortable and ecology LC strategy. The improvements promise CLCMT mechanism greater advantages in the LC motion planning of CAVs.", "sections": [{"title": "I. INTRODUCTION", "content": "Connected and automated vehicle (CAV) technologies offer a promising solution to improve traffic safety, efficiency and reduce traffic emissions as CAV driving maneuvers can be designed and controlled for certain purposes. According to motion planning of Society of Automotive Engineers (SAE) Level 4 or Level 5 [1], an autonomous vehicle should know when and how to make the proper decision as well as execute the action safely under various traffic scenarios. Nevertheless, defining the robust motion planning strategy is a persistent challenge because of the unpredictable behaviors of surrounding vehicles in mixed traffic (mixed with both CAVs and Human-driven Vehicles (HV)). Specifically, lane-changing (LC) can be a more complex task in motion planning as both longitudinal and lateral behaviors should be considered [2], [3].\nLC motion planning is a sequential decision-making problem with uncertainty. Classical approaches generally formulated LC motion planning as a distributed optimal control problem [4]. The dynamics of autonomous vehicles are formulated by mathematical equations while uncertainty from surrounding vehicles is modeled by a probabilistic model or a set of representative scenarios. Then, model predictive control (MPC) or mixed-integer nonlinear programming (MINLP) is leveraged to find the optimal LC solutions [5]. Nevertheless, these approaches usually can not meet the real-time decision requirement because solving MPC or MINLP can be computationally intensive. Moreover, modeling the uncertainty of the traffic environment is challenging.\nBy modeling the LC process as a Markov Decision Process (MDP), model-free deep reinforcement learning (DRL) is an alternative approach to overcoming conundrums faced by classical mathematical approaches. DRL approaches aim to learn an optimal policy by trial and error by interacting with the designed environment. After training, DRL approaches can be deployed in real-time, free from the online computation [6]. For example, deep Q-learning (DQN) was used to learn a vehicle control strategy, in which three-dimensional control actions, namely acceleration, deceleration, and maintaining, were considered [7], [8]. Jaritz et al. [9] applied an Asynchronous Actor-Critic (A3C) method to learn a vehicle control policy where the control action space includes 32 discrete values. Rather than using discrete action space, Wang et al. [10] formulated the LC problem as an MDP with continuous action spaces and utilized Deep Deterministic Policy Gradient (DDPG) algorithm to define the optimal LC planning strategy. Results demonstrated the effectiveness of continuous MDP action formulation and DRL algorithm solution in addressing LC motion planning in pure CAV traffic by providing optimal strategies with high efficiency.\nLC motion planning in mixed traffic is a more complex task because of the high unpredictability of surrounding HVs [2]. For instance, HVs can be non-cooperative in the process of lane changes, such as adopting a hostile motion, which impedes the lane-changing maneuver. The behavior of these HVs is highly stochastic and unpredictable and can not be directly controlled. To solve this challenge, previous research [1] proposed a cooperative lane-changing mechanism in mixed traffic (CLCMT) by considering uncertainty in traffic environments. Then, the twin delayed DDPG (TD3) algorithm is used to define the optimal LC motion planning strategy. Results demonstrated the effectiveness of TD3 on the CLCMT problem in terms of safety, efficiency, comfort, etc. Nevertheless, the proposed CLCMT only considers uncontrolled behaviors of HVs, while the interactions among vehicles were ignored, e.g., the collision warnings triggered during the LC process. Besides, there are no silver bullet algorithms since DRL algorithms based on different theories and characteristics would have diverse performances in solving CLCMT tasks. A fair comparison is required to facilitate an optimal option for solving CLCMT problems.\nIn this paper, we fill the research gaps with two novel contributions: (i) A more realistic CLCMT mechanism is developed, which considers the uncertainty of HVs and the microscopic interactions between HVs and CAVs. (ii) A performance comparison for DRL algorithms including DDPG, TD3, SAC, and PPO is conducted to leverage the capability of different DRL algorithms in solving the formulated CLCMT tasks."}, {"title": "II. METHODOLOGY", "content": "In this section, the cooperative lane-changing problem in mixed traffic is first introduced. Then the DRL-based CLCMT mechanism is presented."}, {"title": "A. Brief Review of CLCMT", "content": "The process of proposed CLCMT in [1] can be outlined in several steps. First, the current speed and desired speed of the target vehicle are acquired. If the former is less than the latter by a threshold of e, a request for lane-changing is triggered. Next, the DRL agent receives traffic states from environment, including leader-follower compositions as well as their gaps in adjacent lanes. These states serve as potential lane-changing scenarios. Then, detailed maneuver control actions behind each potential scenario are executed. This is achieved by DRL-based policies to learn actions including two-dimensional accelerations of  $V_{ego}$  (and longitudinal acceleration of  $V_{lead}$  and  $V_{lag}$  if they are controlled). Based on various lane-changing strategies learned by DRL algorithms, the feedback module computes the utilities of each cooperative lane-changing strategy under different scenarios. According to pre-calculated utilities (including safety, efficiency, comfort, and ecology) and a personalized evaluation function, the feedback module recommends the optimal lane-changing strategy for the decision-making layer where the lane-changing strategy is determined.\nIn this CLCMT, vehicles controlled by the DRL agent update their position and speed by following kinematic models, see Equation 1, and other vehicles' behavior follows the Intelligent Driver Model (IDM) car-following model [11], as shown in Equation 2.\n$\\begin{array}{l}x(t_{i+1}) = x(t_i) + v_x(t_i) \\Delta t + \\frac{1}{2} a_x (\\Delta t)^2\\\\y(t_{i+1}) = y(t_i) + v_y(t_i) \\Delta t + \\frac{1}{2} a_y (\\Delta t)^2\\\\v_x(t_{i+1}) = v_x(t_i) + a_x \\Delta t\\\\v_y(t_{i+1}) = v_y(t_i) + a_y \\Delta t\\end{array}$   (1a)\n  (1b)\n  (1c)\n  (1d)\nwhere x, y are the positions,  $v_x$  and  $v_y$  are longitudinal and lateral speed, respectively.  $a_x$  and  $a_y$  are longitudinal and lateral acceleration, respectively. And  $\\Delta t = t_{i+1} - t_i$ , denotes the time step.\n$a(t_{i+1}) = a_1 \\left[1-\\left(\\frac{v(t_i)}{v_o}\\right)^\\delta -\\left(\\frac{s^*(v(t_i), \\Delta v)}{s_o}\\right)^2\\right]$\n$s^*(v(t_i), \\Delta v) = s_0 + \\max \\left(0, v(t_{i+1}) T - \\frac{v(t_i), \\Delta v}{2a_1 b_1}\\right)$  (2a)\n (2b)\nhere  $a_1$  is the maximum acceleration/deceleration of the follower,  $\\delta$  is the acceleration index,  $v_o$  is the desired speed and  $s_o$  is the minimum distance gap.  $s^*(v(t_i), \\Delta v)$  means the desired gap, which is a function of  $v(t_i)$  and  $\\Delta v$  as shown in Equation (2b), in which T is the safety time gap and  $b_1$  is the comfortable deceleration."}, {"title": "B. Problem Description", "content": "Our focus is the pre-calculation procedure in maneuver control of CLCMT where the DRL agent should calculate utilities of all potential lane-changing scenarios and forward results to the feedback module. As shown in Figure 1, the ego vehicle ($V_{ego}$), the leader ($V_{lead}$), and the follower ($V_{lag}$) in the target lane are directly involved in lane-changing and are potential objects in the cooperative control problem. Other vehicles, such as the preceding vehicle in the current lane ($V_{pre}$) and surrounding vehicles in the target lane ($V_{sur}$), are considered in building lane-changing environments. Red and yellow denote CAVs, and blue and gray represent HVs. CAVS can strictly comply with the DRL cooperative maneuver control (CMC) while HVs are provided with cooperative maneuver control recommendation (CMCR) through V2X communication, such as suggested speed and acceleration. Assume that an HV has the probability of p to adopt the CMCR, i.e., behaving as a CAV. The goal of the DRL agent is to learn a feasible LC policy  $\\pi$  to execute the LC process in a safe, efficient, comfortable, and ecology way. In a two-lane scenario, the leader-follower ($V_{lead}$-$V_{lag}$) that forms a lane-changing gap in the target lane can have four compositions: CAV-CAV, HV-CAV, CAV-HV, and HV-HV, as shown in Figure 2. The ego vehicle  $V_{ego}$  may face any of them during its DRL learning process."}, {"title": "C. MDP Formulation", "content": "The CLCMT problem is formulated as a Markov Decision Process (MDP), which is defined by 5-tuple (S, A, P, R,  $\\Lambda$ ), where S represents the set of system states, A is the set of action, P and R denote the state transition probability and the reward function, respectively.  $\\Lambda$  is the discount factor"}, {"title": "D. DRL Policy-based Algorithms", "content": "The formulated MDP consists of continuous state and action spaces, which are difficult to solve by classical RL algorithms, such as Q-learning, due to their poor scalability features [6]. By leveraging the generalization and fitting capability of DNNs, DRL algorithms have shown good performance when dealing with this challenge. In valued-based DRL algorithms, the action-state function Q is iteratively updated to indirectly define a deterministic policy, for which the foundation is the Bellman optimality equation:\n$Q^*(s, a) = R(s, a) + \\gamma \\sum_{s' \\in S} P(s' \\vert s, a) \\max_{a' \\in A} Q^*(s', a')$  (14)\nHere, the optimal policy can be derived as  $\\pi^*(s) = \\arg\\max_{a'\\in A} Q^*(s, a)$  when the optimal value function Q(s, a) is estimated. Value-based DRL algorithms use DNNs to approximate the Q-function, dealing with continuous state spaces. However, continuous action MDP problems require a full scan of the action space when executing policy improvement, i.e.,  $\\pi^*(s) = \\arg\\max_{a'\\in A} Q^*(s, a)$ , leading to a dimensionality problem. Instead, policy-based DRL algorithms directly search for the optimal policy, which is usually modeled with a parametric function, denoted as  $\\pi_\\theta(s|a)$.Based on the policy gradient theorem, the policy gradient is expressed as  $\\nabla_\\theta J(\\theta) = E_{\\pi}[Q^{\\pi} (s, a) \\nabla_\\theta \\ln \\pi_\\theta (a \\vert s)]$,  $\\theta$  can be updated towards the direction suggested by  $\\nabla_\\theta J(\\theta)$  to find the policy  $\\pi_\\theta$  that leads to the highest expected returns.\nWe compare the performance of DRL algorithms in solving the CLCMT problem formulated MDP problem, including two off-policy, deterministic algorithms, i.e., DDPG, TD3, and two stochastic algorithms, i.e., Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO). In general, DRL algorithms interact with the environment to collect sequential data, which is then used to update the critic DNNs parameters based on the temporal difference (TD) algorithm. Then, the critic network is used to update actor DNNs parameters based on policy gradient theory. A more detailed explanation of policy-based algorithms can be found in [13]."}, {"title": "III. EXPERIMENTS AND NUMERICAL RESULTS", "content": "In this section, we first introduce the environment settings and training process of CLCMT. Then experimental results and discussions are provided."}, {"title": "A. Experimental Settings", "content": "In the proposed CLCMT training environment, a 150 m road with two lanes is constructed. The width of each lane is set to be 3.75 m. The y-axis and x-axis denote the longitudinal and lateral motion of driving, respectively. The length of each vehicle is  $l_{veh}$  and the spacing (distance between the leader and the follower) is denoted as L. The position of each vehicle is depicted by (x, y) coordinate at time t. The simulation time step is 0.1 s, and the original position of the target vehicle is set at (60, 1.875). Spacing L"}]}