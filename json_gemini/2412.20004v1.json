{"title": "Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous Devices", "authors": ["Jun Liu", "Yunming Liao", "Hongli Xu", "Yang Xu", "Jianchun Liu", "Chen Qian"], "abstract": "Federated fine-tuning (FedFT) has been proposed to fine-tune\nthe pre-trained language models in a distributed manner. How-\never, there are two critical challenges for efficient FedFT in\npractical applications, i.e., resource constraints and system\nheterogeneity. Existing works rely on parameter-efficient fine-\ntuning methods, e.g., low-rank adaptation (LoRA), but with\nmajor limitations. Herein, based on the inherent characteris-\ntics of FedFT, we observe that LoRA layers with higher ranks\nadded close to the output help to save resource consumption\nwhile achieving comparable fine-tuning performance. Then\nwe propose a novel LoRA-based FedFT framework, termed\nLEGEND, which faces the difficulty of determining the num-\nber of LoRA layers (called, LoRA depth) and the rank of each\nLORA layer (called, rank distribution). We analyze the cou-\npled relationship between LoRA depth and rank distribution,\nand design an efficient LoRA configuration algorithm for het-\nerogeneous devices, thereby promoting fine-tuning efficiency.\nExtensive experiments are conducted on a physical platform\nwith 80 commercial devices. The results show that LEGEND\ncan achieve a speedup of 1.5-2.8\u00d7 and save communication\ncosts by about 42.3% when achieving the target accuracy,\ncompared to the advanced solutions.", "sections": [{"title": "1 Introduction", "content": "The emergence of transformer [1] and its variants [2-4]\nhas catalyzed significant advancements in natural language\nprocessing (NLP), unveiling the immense potential for de-\nploying NLP models on various devices. The existing NLP\nparadigm encompasses two stages, i.e., pre-training and fine-\ntuning [3, 5]. Specifically, the language model (LM) is first pre-\ntrained on a large corpus to learn general features and patterns.\nSubsequently, the LM is further fine-tuned on domain-specific\ndata generated on devices to enhance the model performance\nfor a specific task. However, it is infeasible to collect enough\ndomain-specific data from devices for centralized fine-tuning\ndue to data privacy [6-8]. To fully utilize the massive data on\ndevices, federated fine-tuning (FedFT) has been proposed to\nperform fine-tuning in a distributed manner [7]. In the typical\nFedFT framework, e.g., FedNLP [7], participating devices pe-\nriodically fine-tune the LMs on their local data, and push the\nlocal LMs to the parameter server (PS) for global aggregation\nwithout exposing their raw data. The fine-tuning procedure\nis repeated for multiple rounds until the LM converges or\nreaches the target accuracy [6, 9, 10]. FedFT not only protects\nindividual privacy but also fully utilizes plenty of computing\nresources on devices to enhance the fine-tuning performance\nof the LMs.\nChallenges of FedFT. Although FedFT has demonstrated\nits advantages, it still faces the following two challenges in\npractical applications: (1) Resource constraints. Many devices\nsuch as smartphones and in-vehicle devices typically have\nlimited resources (e.g., memory, computing power), which are\norders of magnitude weaker than cloud servers [11-14]. How-\never, existing LMs, e.g., Llama [15], typically involve billions\nof parameters, requiring substantial computing power for fine-\ntuning [16], while resource-constrained devices always lead\nto very slow fine-tuning rates. (2) System heterogeneity. The\ndevices commonly possess varying computing capabilities\n(e.g., CPU frequency) or communication capabilities (e.g.,\nbandwidth), which could differ from each other by more than\ntenfold [17-19]. Specifically, there are huge gaps of com-\nputing/communication capabilities among different types of\ndevices, and even among devices of the same type with di-\nverse configurations (e.g., smartphones, laptops) [13]. Due to\nsystem heterogeneity, fast devices should be forced to wait\nfor slow ones, leading to prolonged waiting time and poor\nfine-tuning efficiency.\nStatus Quo and Limitations. To handle the issue of re-\nsource constraints, existing works rely on parameter-efficient\nfine-tuning methods [20], e.g., Adapter [21] and low-rank\nadaptation (LoRA) [16], which only fine-tune additional\nlightweight parameters (typically less than 1%). The Adapter\nmethod inserts additional blocks between two continuous\ntransformer layers and only updates the parameters of the"}, {"title": "2 Background and Motivation", "content": "Language Models. From design to deployment, training\ntransformer-based LMs typically involves two main stages:\npre-training and fine-tuning [1, 7, 10]. In the first stage, the\nLMs are pre-trained on the large-scale corpus, e.g., Wikipedia\n[30], C4 [31], to learn the ubiquitous linguistic structure,\nwhich is independent of downstream tasks [10]. The pre-\ntraining stage demands massive computing resources, typ-\nically undertaken by large tech companies [3, 5], such as\nGoogle and Microsoft. Fine-tuning adapts the pre-trained\nLMs to various downstream tasks, such as text classification\nand text generation, which require substantial data to update\nthe entire LM for a giving task. However, fine-tuning the en-\ntire LM usually demands excessive resources, e.g., computing\nand communication resources, leading to slow fine-tuning\nspeeds on resource-constraint devices [10,32].\nLORA for LMs. Herein, low-rank adaptation (LoRA) adds\ntrainable rank decomposition matrices to each transformer\nlayer of the LM while freezing the pre-trained weights of the\nLM to improve fine-tuning efficiency [16]. For a pre-trained\nweight matrix $M \\in \\mathbb{R}^{m\\times q}$ (m and q are the dimension sizes\nof M), LoRA injects low-rank decomposition $\\Delta M = BA$\nas the trainable parameters. Note that, $B\\in \\mathbb{R}^{m\\times r}$ and $A \\in$\n$\\mathbb{R}^{r\\times q}$ are the project-down matrix and the project-up matrix\nrespectively, where r denotes the rank of LoRA and is much\nsmaller than both m and q. Formally, for a simple linear layer\n$y = Mx$ in the transformer layer, LoRA modifies the forward\npropagation as:\n$$y = Mx+BAx$$"}, {"title": "2.1 Federated Fine-Tuning LMs with LoRA"}, {"title": "2.2 Importance of LoRA Position", "content": "Existing LoRA-based frameworks (e.g., FedLoRA [20] and\nHetLoRA [27]) typically add LoRA layers to all transformer\nlayers, which requires the complete backpropagation process\nto update all LoRA layers [34] and results in a slow con-\nvergence rate for fine-tuning pre-trained LMs on resource-\nconstrained devices. As illustrated in Figure 2, we divide an\nLM (e.g., RoBERTa) into three parts representing different\npositions, i.e., shallow, medium and deep, respectively. Based\non the backward direction of the backpropagation process,\nfine-tuning the continuous LoRA layers added to the trans-\nformer layers at deep position is computationally efficient\nwhile achieving satisfactory fine-tuning performance. Wei\net al. [35] provide rigorous theoretical insights into the con-\nvergence of partial LoRA layer fine-tuning. Besides, only\nthe LORA layers at deep position need to perform the back-\npropagation process and be transmitted to the PS, effectively\nreducing computing/communication overhead and speeding\nup the fine-tuning process. In addition, since pre-trained LMs\nhave acquired powerful language understanding and gener-\nation capabilities during the pre-training stage, fine-tuning\nthe added LoRA layers to partial transformer layers at deep\nposition can essentially achieve comparable fine-tuning per-\nformance [7,36,37].\nTo demonstrate the importance of LoRA position, we con-\nduct a set of experiments for federated fine-tuning RoBERTa\n[38] on SST-2 [39] with 10 devices (more experimental setup\ndetails in Section 6.1). We train a 12-layer RoBERTa with\nLORA layers added to partial transformer layers at different\npositions (as illustrated in Figure 2), including all layers (de-\nnoted as Layers-A), shallow layers {#0, #1, #2, #3} (denoted\nas Layers-S), medium layers {#4, #5, #6, #7} (denoted as\nLayers-M), and deep layers {#8, #9, #10, #11} (denoted as\nLayers-D). As shown in Figure 3, we can derive the following\nconclusions:"}, {"title": "2.3 Importance of LoRA Depth", "content": "In addition to the results of adding LoRA layers to partial\ntransformer layers at deep position, we further explore the\nimpact of the number of transformer layers at deep position\nto add extra LoRA layers (called LoRA depth) on fine-tuning\nperformance. In general, LoRA depth is closely related to\nfine-tuning performance and resource consumption. Specifi-\ncally, the larger LoRA depth means fine-tuning more layers at\ndeep positions to enhance fine-tuning performance but leads\nto a longer backpropagation process (i.e., slower convergence\nrate). Although the smaller LoRA depth can shorten the back-\npropagation process to speed up the fine-tuning process, it\nconstrains the number of tunable transformer layers and thus\nrestricts the task-fitting capability of the LM. Therefore, it\nis critical and non-negligible to determine the appropriate\nLORA depth for achieving the trade-off between fine-tuning\nperformance and resource consumption.\nTo illustrate the impact of the LoRA depth, we conduct a\nset of experiments for fine-tuning RoBERTa on SST-2 with\ndifferent LORA depths (from 1 to 12) and rank of 8. We can\ndraw the following conclusions from Figure 4:\n1) As LoRA depth increases, resource consumption (e.g.,"}, {"title": "2.4 Importance of LoRA Rank Distribution", "content": "We further explore the impact of rank distribution on fine-\ntuning performance. In general, the rank of LoRA layer is\nclosely related to the model capacity, and increasing rank is\nessential for better performance [40, 41]. Given a fixed total"}, {"title": "3 System Overview", "content": "As illustrated in Figure 6, LEGEND consists of two key com-\nponents with a total of six main modules, i.e., four modules\non the PS and two modules on each device. The details of\neach module are as follows:\nInitialization and Update. The PS distributes different\nLORA layers for heterogeneous devices to adapt their capa-\nbilities. Hence, the devices need to initialize and update the\nlocal model (1) for local fine-tuning in each round.\nLocal Fine-Tuning. Based on the initialized local model,\nthe devices perform local fine-tuning and record the fine-\ntuning status information (e.g., computing time and commu-\nnication time). After that, the devices send the updated LoRA\nlayers (2) and the status information (3) to the PS.\nCapacity Estimation. To make effective LoRA configu-\nrations, the PS estimates the capabilities of each device (4)\nby calculating the moving average of the historical status\ninformation of the devices.\nLORA Configuration. In this module, the PS simultane-\nously determines the appropriate LoRA depth with rank dis-\ntribution, i.e., LoRA configurations (\u2464), for the devices.\nLORA Aggregation. The PS performs adaptive weighted\naggregation for the collected LoRA layers with different\nLORA depths from all devices to obtain the aggregated global\nLORA layers (\u2465).\nLORA Assignment. Due to the diverse LoRA configura-\ntions of devices, the PS needs to assign the specific LoRA\nlayers (7) to each device based on the aggregated model."}, {"title": "4 System Design", "content": "In each round, the device receives a set of LoRA layers from\nthe PS to initialize and update the local model for local fine-\ntuning. These LoRA layers are tailored to the device's capac-\nity (e.g., computing and communication capacity), including\ninformation on which transformer layers to fine-tune and the\nranks of those LoRA layers. Specifically, device i receives the\nLORA layers $\\Theta^{h}_{i}$ = {$\\theta^{h}_{i,l} | l\\in [L-k,L-1]$} to initialize and up-\ndate k transformer layers close to the output, where L denotes"}, {"title": "4.1 Initialization and Update"}, {"title": "4.2 Local Fine-Tuning", "content": "After local model initialization, device i fine-tunes the model\non local dataset Di. During the process of local fine-tuning\nin round h, device i is associated with the local loss function\nfi($\\Theta^{h}_{i}$), where $\\Theta^{h}_{i}$ = {$\\theta^{h}_{i,l}$} is the local model. The loss of\ndevice i on the local dataset D; in round h are as follows:\n$$f_i(\\Theta^{h}_{i}) = \\frac{1}{|D_i|} \\sum_{\\xi_i\\in D_i}F_i(\\Theta^{h}_{i}; \\xi_i)$$\nwhere \u03be; is a batch of data samples in D\u2081, and F\u2081($\\Theta^{h}_{i};\\xi_i$) is the\nlocal loss over \u03be\u2081. In general, the devices leverage stochastic\ngradient descent, e.g., AdamW [33], to iteratively update the\nLORA layers based on the computed gradients over each batch\nof data samples in D; [7, 10]. Specifically, for a batch of local\ndata samples \u03be; on device i, the process of updating the LORA\nlayers $\\theta^{h}_{i,l}$ on device i at local step t in round h is expressed as:\n$$\\theta^{h,t}_{i,l} = \\theta^{h,t-1}_{i,l} - \\eta \\cdot \\nabla f_i(\\theta^{h,t-1}_{i,l})$$\nwhere \u03b7 is the learning rate and $\\nabla f_i(\\theta^{h,t-1}_{i,l})$ is the gradient\nof the loss for LoRA layers $\\theta^{h,t-1}_{i,l}$. When completing local\nfine-tuning, the devices send the updated local LoRA layers\nto the PS for aggregation. Simultaneously, the devices upload\nthe relevant computing and communication information of\nthe current round to PS for device resource estimation."}, {"title": "4.3 Capacity Estimation", "content": "The estimation of device capabilities (e.g., time-varying com-\nputing and communication capabilities) is essential for LEG-\nEND to determine reasonable LoRA configurations for hetero-\ngeneous devices. For arbitrary device i in round h, LEGEND\nutilizes \\mu to denote the time required for updating all LoRA\nlayers in a transformer layer during backpropagation, which\ncan be recorded by the devices directly during the process\nof local fine-tuning, to indicate the computing capability. Be-\nsides, since the upload bandwidth is usually much smaller"}, {"title": "4.4 LORA Configuration", "content": "LEGEND addresses the challenges of resource constraints\nand system heterogeneity by determining appropriate LORA\nconfigurations, i.e., LoRA depth and rank distribution, for het-\nerogeneous devices to promote fine-tuning efficiency. Based\non the observation in Sections 2.3 and 2.4, LEGEND adap-\ntively assigns LoRA depth for device i in round h with grad-\nually increasing rank distribution. The LoRA configuration\nof device i in round h can be denoted as the rank distribution\n$R^h_i$ = {$r_{i,l} | l\\in [L-k, L - 1]$} that includes the LoRA depth $k_i^h$,\nwhere $r_{i,l}$ is the rank of the LoRA layers in l-th transformer\nlayer. For simplicity, we use L = [L\u2212k, L \u2212 1] to represent\nthe index of the deep $k_i^h$ transformer layers. Assuming that\nthe total rank of all L transformer layers is \\Psi, the constraints\nof rank distribution for device i in round h are as:\n$$r_{i,l-1} \\leq r_{i,l}$$\n$$\\sum_{l\\in L} r_{i,l} \\leq \\Psi$$\nIn round h, based on the estimation of computing and com-\nmunication capacities, the completion time (including com-\nputing and communication time) on device i is expressed as:\n$$t^h_i = f_i + k_i^h \\mu + \\sum_{l \\in L}r_{i,l} \\cdot \\beta$$\nwhere fi represents the computing time of forward propaga-\ntion in one round of local fine-tuning, \\mu represents the\ntotal time for backpropagation during the process of local\nfine-tuning, and $\\sum_{l \\in L}r_{i,l} \\cdot \\beta$ denotes the uploading time. Ad-\nditionally, the waiting time for device i can be represented\nas $t^h - t^h_i$, where $t^h$ = max{$t^h_i | i \\in [1,n]$} denotes the comple-\ntion time of the slowest device in round h. Then, the average\nwaiting time of all devices in round h can be formulated as:\n$$W^h = \\frac{1}{n} \\sum_{i=1}^{n}(t^h - t^h_i)$$"}, {"title": "4.5 LoRA Aggregation", "content": "After receiving the updated LoRA layers from all devices, the\nPS performs global aggregation. Since the LoRA depth varies\nacross devices while the rank of each LoRA layer remains con-\nsistent across all devices, the PS performs adaptive layer-wise\naggregation on the collected LoRA layers, i.e., aggregating\neach layer based on the number of devices contributing to that\nlayer. We use $\\Theta^{h+1}_l$ to represent the global LoRA layers in\nl-th transformer layer obtained by aggregating the respective\nLORA layers from $n_l$ devices. Formally, the adaptive layer-wise\naggregation of LoRA layers in transformer layer l can\nbe expressed as follows:\n$$\\Theta^{h+1}_l = \\frac{1}{n_l}\\sum_{i=1}^{n_l} \\theta^{h}_{i,l}$$\nAfter aggregation, PS obtains a set of global LoRA layers\n$\\Theta^{h+1}$ = {$\\Theta^{h+1}_l | l\\in [0, L -1]$}, which will be used for assigning\ndifferent LoRA layers for each device."}, {"title": "4.6 LORA Assignment", "content": "According to the obtained LoRA configuration, the PS assigns\nspecific LoRA layers for each device. Specifically, based on\nthe LoRA configuration $R^h_i$ = {$r_{i,l} | l\\in L$} and the aggregated\nset of global LoRA layers that can be expressed as:\n$$\\tilde{\\Theta}^h = {\\Theta^{h+1}_l | l \\in [0, L -1]}$$\nLEGEND generates LoRA layers $\\Theta^{h+1}_{i}$ for device i by selecting\nthe LoRA layers from global LoRA layers $\\tilde{\\Theta}^h$ as follows:\n$$\\Theta^{h+1}_{i} = {\\Theta^{h+1}_l | \\theta^{h}_{i,l} \\in \\tilde{\\Theta}^h}$$\nwhere $\\theta^{h}_{i,l} \\in \\tilde{\\Theta}^h$. Since the LoRA layers are all continu-\nous layers at deep position, there is no need to send a sepa-\nrate LoRA configuration for local initialization and update.\nThus, after obtaining the LoRA layers, the PS immediately\ndistributes the LoRA layers to the respective devices."}, {"title": "5 Implementation", "content": "We implement our FedFT prototype based on the open-source\nFedPETuning framework [48], extending its functionality\nwith approximately 2.1K lines of custom code. The prototype\nis designed to support heterogeneous computing platforms,\nwith a specific focus on commercial NVIDIA Jetson devices\n[49], including Jetson TX2, Jetson NX, and Jetson AGX. The\nsoftware platform is built based on Docker Swarm [50, 51],\na distributed software development kit that helps build dis-\ntributed systems with the ability to monitor the status of each\ndevice. For computing, we utilize PyTorch [52] to facilitate\nthe implementation of model fine-tuning on devices, ensuring\nplatform independence while allowing platform-specific back-\nend acceleration. To accelerate the on-device fine-tuning, we\nutilize NVIDIA-provided development packages to take full\nadvantage of the underlying hardware capabilities. For com-\nmunication, we adopt MPI (Message Passing Interface) [53],\nwhich includes a collection of sending and receiving func-\ntions, e.g., comm.send(data, dest, tag)/comm.recv(sour, tag),\nto streamline communication between the PS and devices.\nThe prototype provides simple APIs to abstract away the\nproblem of federated fine-tuning on heterogeneous comput-\ning platforms. For example, in the PS, we use docker API\ndocker stack deploy to deploy the fine-tuning process on both\nPS and the heterogeneous device with the customized con-\ntainer for different devices. The implementation addresses\nkey challenges in supporting heterogeneous devices by devel-\noping flexible communication and fine-tuning protocols that\ncan adapt to varying computational resources."}, {"title": "6 Evaluation", "content": "Extensive experiments are conducted\non the implemented prototype system with one PS and 80"}, {"title": "6.1 Methodology"}, {"title": "6.2 Overall Performance", "content": "Firstly, we conduct sets of experiments to evaluate the per-\nformance of LEGEND and the baselines. The fine-tuning\nprocesses and the completion time of the general language un-\nderstanding task are presented in Figures 7 and 8, respectively.\nBy the results, LEGEND achieves the fastest convergence\nrate, outperforming the other approaches by a significant mar-\ngin on all tasks. By assigning smaller LoRA depths with an\noptimized rank distribution for resource-constrained devices,\nLEGEND effectively enhances fine-tuning performance while\nreducing the time for local fine-tuning. For instance, by Fig-\nures 7(a) and 8(a), LEGEND takes only 1,479s to achieve\n85% accuracy on SST-2, while FedAdapter, HetLoRA, and\nFedLoRA take 2,412s, 2,503s, and 4,074s, respectively. Com-\npared to FedAdapter, HetLoRA, and FedLoRA, LEGEND\nprovides 1.6\u00d7, 1.7x, and 2.8\u00d7 speedup, respectively. By Fig-\nures 7(b) and 8(b), LEGEND also outperforms the other base-\nlines in terms of completion time for QNLI. Similarly, Figures\n7(c) and 8(c) show that LEGEND takes 121,156s to achieve\n87% accuracy for QQP, while FedAdapter, HetLoRA, and\nFedLoRA consume 173,375s, 209,196s, and 269,749s, re-"}, {"title": "6.3 Ablation Study", "content": "There are two key factors in LEGEND, i.e., LoRA depth and\nrank distribution, which are developed to enhance the per-\nformance of FedLoRA. Herein, we conduct several sets of\nablation experiments on SST-2 and QNLI to validate the ef-\nfectiveness of the two essential factors. We adopt LEGEND,\nLEGEND without LoRA depth (denoted as LEGEND w/o\nLD), LEGEND without rank distribution (denoted as LEG-\nEND w/o RD) as the baselines. As illustrated in Figure 13,\nboth LoRA depth and rank distribution are essential in LEG-\nEND, but they contribute to the system in different ways.\nFor instance, by Figures 13(a) and 13(b), LEGEND w/o LD\nachieves similar final test accuracy, e.g., 95.3% on SST-2 and\n91.6% on QNLI, compared with LEGEND, while LEGEND\nw/o RD slightly degrades to 94.6% on SST-2 and 90.5% on\nQNLI, respectively. Besides, by Figure 13(a), when achiev-"}, {"title": "7 Related Works", "content": "Natural Language Processing. The development of mod-\nern natural language processing (NLP) traces back to the\nintroduction of the transformer architecture by Vaswani et al.\nin 2017 [1]. Upon transformer architecture, groundbreaking\nlanguage models, such as BERT [3], GPT-2 [60], and more\nrecently Llama [15], have achieved state-of-the-art results\nacross various NLP tasks. Specifically, the language model\n(LM) is first pre-trained on a large corpus to learn general\nfeatures and patterns. Subsequently, the LM is further trained\non domain-specific data generated on devices to enhance the\nmodel performance for a specific task [3, 5]. However, due to\ndata privacy, it is impractical to collect enough data from the\ndevices for centralized fine-tuning [6, 7].\nFederated Fine-Tuning. To fully utilize the massive data\non devices, federated fine-tuning (FedFT) has been proposed\nto perform fine-tuning in a distributed manner [7]. However,\nthe high resource costs associated with FedFT pose significant\nchallenges to its practical implementation. Modern FedFT\nutilizes parameter-efficient fine-tuning (PEFT) methods, such\nas Adapter [10, 21], prompt-tuning [61, 62], to reduce on-\ndevice resources costs. For example, Cai et al. [10] first apply\nAdapter in FedFT and propose FedAdapter, which dynami-\ncally searches for the optimal Adapter structure to improve"}, {"title": "8 Conclusion", "content": "In this paper, we review the intrinsic properties of FedFT and\npropose an efficient LoRA-based FedFT framework, called\nLEGEND, to address resource constraints and system hetero-\ngeneity. We analyze the coupled relationship between LoRA\ndepth and rank distribution, and design an efficient LoRA\nconfiguration algorithm for heterogeneous devices, thereby\npromoting fine-tuning efficiency. Extensive experiments are\nconducted on a real platform of 80 wireless devices. The\nexperimental results show that LEGEND significantly out-\nperforms the existing methods, providing a speedup of 1.5-\n2.8x and saving communication costs by about 42.3% while\nachieving the target accuracy."}]}