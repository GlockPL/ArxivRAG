{"title": "WILT: A MULTI-TURN, MEMORIZATION-ROBUST INDUCTIVE LOGIC BENCHMARK FOR LLMS", "authors": ["Eryk Banatt", "Jonathan Cheng", "Skanda Vaidyanath", "Tiffany Hwu"], "abstract": "While large language models (LLMs) have shown impressive capabilities across a wide range of domains, they still encounter significant challenges in reasoning tasks that require gathering evidence over multiple turns and drawing logical conclusions from this evidence. These challenges present significant obstacles for LLM chat user interfaces, which rely on multi-turn interactions to facilitate effective collaboration. This limitation leads to real-world issues; for example, service chatbots must gather necessary information from customers over multiple turns to diagnose and resolve problems effectively. Despite the multi-turn nature of many real-world LLM use cases, most existing benchmarks rely on carefully curated single-turn tests, which often blur the line between memorization and genuine reasoning. To address this, we introduce the Wason Inductive Logic Test (WILT), a simple yet challenging multi-turn reasoning benchmark designed to resist memorization. WILT is inspired by the Wason 2-4-6 task (Wason, 1960), where participants must infer a basic boolean function involving three variables (e.g., $x < y < z$) by proposing test cases (such as (2,4,6)). In WILT, each test starts from a clean slate, with only the initial instructions provided, preventing models from relying on pre-learned responses. Over several turns, models must interact with the environment by suggesting test cases to narrow the possible hypotheses and ultimately infer the hidden function based on the outcomes. Our findings reveal that LLMs struggle with this task, exhibiting distinct strengths and weaknesses: some are better at narrowing down the hypothesis space by proposing valuable test cases, while others are more adept at deducing the hidden function from observed cases. Despite these variations, the best-performing model achieves only 28% accuracy, highlighting a significant gap in LLM performance on complex multi-turn reasoning tasks.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) powered by the transformer architecture (Vaswani, 2017) have enabled a new computing paradigm driven by natural language. These models are increasingly integrated into day-to-day life beyond the machine learning research space, where they help many people with common tasks. These models interact with users through multi-turn conversations, a capability of next-token-prediction models bolstered via instruction-tuning (Mishra et al., 2021) and alignment post-training phases (Ouyang et al., 2022). Despite their impressive single-turn performance, LLMs often falter in scenarios requiring multi-turn reasoning and evidence gathering.\nThe reasoning capability of LLMs, particularly in multi-turn scenarios, is of substantial interest. A commonly reported failure pattern for LLMs is the \u201cdoom loop\", where after an initially unsatisfactory response, the model repeatedly responds with a near-identical message to one of its earlier messages, providing minimal utility. For example, in code generation tasks like the example shown in Figure 1, a model may repeatedly reply with code blocks identical to ones it has already seen or produced, which impairs its subsequent usefulness. Strong multi-turn performance can be thought of as collecting evidence over multiple steps to reduce the hypothesis space of the model's possible responses to only include useful ones: an important capability which remains critical beyond the first turn even in everyday LLM use cases."}, {"title": "WILT", "content": "The Wason Inductive Logic Test (WILT) is a benchmark for LLM reasoning inspired by the Wason 2-4-6 task (Wason, 1960). Models begin with the instruction that they must uncover the hidden rule and may pose up to 30 test cases of that rule. For example, they can pose the tuple (2, 4, 6) and the test will respond with \"True, 29 Attempts Remaining.'\nAll hidden rules take three numbers and return a boolean. These rules are simple and non-stochastic, so there is no additional value to posing the same test multiple times. Valid inputs include any float or integer that can be typed in three or fewer characters, excluding signs and the decimal point (e.g. -999, 1.23, 5). The hidden rules are written as Python lambda functions. After a maximum of thirty tries (or any turn before then), the model may make one attempt to guess the function, after which the test will terminate. The model must return a Python lambda function that is the same as or equivalent\u00b9 to the hidden rule in order to receive full points.\nWILT is conceptually simple, but very challenging. Humans are able to identify simple rules despite the infinitely large hypothesis space, the unbounded difficulty of a hidden function, and the impossibility of verifying the correctness of your response (Tweney et al., 1980; Tukey, 1986). Without effective priors for what constitutes a \u201csimple\u201d or \u201creasonable\u201d answer, it becomes trivial to construct an arbitrary counterexample to a current hypothesis. For example, consider the canonical\nFor example, $x < y = z$ is equivalent to $y + z = x$"}, {"title": "RELATED WORK", "content": "Compared to other reasoning benchmarks, WILT stands out as both highly multi-turn focused and unusually robust to memorization. In contrast to other benchmarks, WILT requires models to interact with an environment by proposing their own test cases to uncover a hidden function without relying on pre-provided examples. This setup reduces the risk of overfitting, as each test begins with the same initial instructions, and the model must generate and interpret its own data."}, {"title": "REASONING BENCHMARKS", "content": "There are a wide variety of reasoning benchmarks used to evaluate large language models. Some very notable among these are MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), CommonsenseQA (Talmor et al., 2018), StrategyQA (Geva et al., 2021), BIG-BENCH (Srivastava et al., 2022), SciBench (Wang et al., 2023b), SVAMP (Patel et al., 2021), ARC-AGI (Chollet, 2019), MMLU (Hendrycks et al., 2020), GPQA (Rein et al., 2023), and HumanEval (Chen et al., 2021). These benchmarks are the standard for measuring LLM reasoning capabilities, but are overwhelmingly carefully chosen single-turn problems which aim to meaningfully separate the performance of different models on reasoning-like outputs such as math, code, or logic puzzles. However, these benchmarks are subject to train-on-test leakage, even if efforts are made to decontaminate the dataset (Yang et al., 2023), and the majority are explicitly single-turn tests. Our benchmark directly measures the model's ability to navigate multi-turn scenarios and does not require careful hiding of a test set to prevent misleading results.\nWith respect to reasoning about simple functions, a benchmark that stands out as similar to ours is CRUXEval (Gu et al., 2024), which assembles a list of 800 simple Python functions and input-output pairs, evaluating language models on their ability to predict input from output and output from input. Our work could be seen as a multi-turn, more difficult extension of this work \u2013 one where the function is replaced with a black box, where helpful and informative input-output pairs are not provided but instead need to be searched for by the language model, and where the objective is to infer the hidden function rather than the input or output."}, {"title": "MULTI-TURN BENCHMARKS", "content": "There are a handful of multi-turn benchmarks used to evaluate LLMs. PlanBench (Valmeekam et al., 2022) is one prominent benchmark that attempts to measure the ability of LLMs to navigate planning problems. This is a class of problems that is solved easily by classical planning algorithms such as STRIPS (Fikes & Nilsson, 1971), and like our benchmark poses a significant challenge to LLMs. PlanBench is a primarily multi-step, single-turn benchmark with a multi-turn component (i.e. replanning based on unexpected events), which contrasts with our benchmark's more direct multi-turn focus. This can be observed in the ol models performing comparatively well on PlanBench (Valmeekam et al., 2024), since scaling inference time compute within a single turn would be expected to improve performance substantially.\nClosest to ours are MINT (Wang et al., 2023c) and Aidan-bench (McLaughlin, 2024), which have more direct multi-turn focus. MINT repurposes existing single-turn benchmarks by allowing models"}, {"title": "HYPOTHESIS SPACE REDUCTION", "content": "Hypothesis space representation is a commonly used framing in inductive logic tasks for LLMs. In Wang et al. (2023a), the authors show a technique called hypothesis search where the model will propose hypotheses in natural language and then implement these hypotheses as Python programs. This technique was shown to improve performance on ARC-AGI (Chollet, 2019), but a similar approach could be used along with chain-of-thought (Wei et al., 2022) for WILT as well."}, {"title": "EXPERIMENTS", "content": null}, {"title": "WILT FULL SPLIT RESULTS", "content": "We evaluated several state-of-the-art LLMs on the full split of the WILT task. Experimental details can be found in Appendix A.1. Our results for this test can be found in Table 1. Claude 3.5 Sonnet narrowly performs the best on this task, with OpenAI's o1 models close behind despite using fewer guesses.\nIn Table 1, we include a column approximately correct, measuring the number of rules in which the model was able to correctly identify some critical behavior of the rule, but returned a rule with failing"}, {"title": "HYPOTHESIS SPACE REDUCTION", "content": "To compare the LLMs' ability to efficiently reason about the task, we estimate how effectively each model reduces the hypothesis space (Wang et al., 2023a). At best, an LLM should always propose a triplet that eliminates as many untested hypotheses as possible. At worst, a model repeatedly proposes a triplet confirming previously covered hypotheses. For example, an LLM that has already guessed (2, 4, 6) retreads much of the same hypothesis space by guessing (4, 6, 8) rather than (0.01, -1, 100)."}, {"title": "EVALUATING FUNCTION INVERSION CAPABILITY", "content": "To succeed at the WILT task, models must succeed at both gathering evidence (hypothesis reduction) and drawing logical conclusions from evidence (function inversion). To distinguish a model's ability to do one or the other, we perform an experiment where models attempt to guess the rule using tests from another model. Rather than asking the model to gather evidence, we directly provide it all the reasoning-stripped\u2074 input-output pairs generated by another model for the same rule, and ask the model to guess the rule in a single turn. Without the original reasoning and subsequent observation before and after each test case, we expect most models to underperform relative to the full test even when provided their own cases. Likewise, we expect models stronger at single-turn to perform better in this experiment relative to other models subject to the same evidence. Our results can be found in Figure 3."}, {"title": "RESPONSE COMPLEXITY", "content": "To determine how well the models employ Occam's Razor, we explore different metrics to gauge whether the models find the simplest rule that covers the examples. From existing Bayesian models of cognition (Tenenbaum, 1999; Tenenbaum & Griffiths, 2001), the size principle uses hypothesis size as a measure of simplicity. In these Bayesian models, hypothesis size is calculated as the number of values that match the hypothesis. Calculating hypothesis size in this manner is only possible when the test values are within a limited countable range. In our case, the possible test values are infinite, requiring some alternative metrics to gauge hypothesis size. We use three metrics:\n1. Number of Operators: We count the number of operators used in the rule expression.\n2. Response Length: We calculate the string length of the rule expression. The longer the length, the more complex it is likely to be. As longer outputs tend to be arbitrarily preferred by automatic evaluators (Dubois et al., 2024), it is particularly important to measure the brevity of the response for cases where simplicity is desired.\n3. Set Inclusion: We generate a grid of integer-float tuples and apply them to guessed and actual rules to generate sets of tuples returning \"True\". If the set of the guessed rule is a subset or superset of the actual rule, we then calculate their set size ratio. A ratio of 1 is ideal, > 1 suggests a less complex guess, and < 1 a more complex one."}, {"title": "CONCLUSION", "content": "In this work, we highlight the importance of the multi-turn setting for understanding the performance of large language models in everyday use cases. We introduce a new benchmark that attempts to measure performance in this common setting and show its difficulty even for state-of-the-art LLMs. With the models exhibiting non-overlapping strengths, the resulting performance of these models on the WILT task provides useful context on the types of tasks for which they would be useful, beyond simply suggesting that the top-performing model is the best one to use for all applications.\nOur benchmark identifies a critical way that typical benchmarks differ from day-to-day use of LLMs: whereas effective multi-turn collaboration often requires asking clarifying questions, performing experiments, and gathering new evidence, typical benchmarks are overwhelmingly designed to be solved immediately. This may, in turn, create an implicit incentive for models to end conversations as quickly as possible, even where that may be ineffective or inappropriate.\nWith this work, we aim to provide a benchmark that measures a model's capacity for exploring an environment and reasoning based on its own decisions across multiple turns. We believe that this paradigm offers a more direct measurement of reasoning capability compared to other benchmarks. Likewise, we believe an explicit focus on multi-turn reasoning capability more directly shines a light upon the most common uses of LLMs."}, {"title": "APPENDIX / SUPPLEMENTAL MATERIAL", "content": null}, {"title": "EXPERIMENTAL DETAILS", "content": null}, {"title": "MODEL CONFIGURATIONS", "content": "Models are accessed when possible with default configurations and settings provided by their respective proprietary APIs (i.e. Anthropic, OpenAI, Mistral, DeepSeek, and Vertex APIs). Llama 3.1 models are run via the Amazon Bedrock API. Gemma 2 9b and llama 3 70B are run via the Groq API Abts et al. (2022)."}, {"title": "PROMPTS", "content": "All models begin from the same initial prompt.\nYou are an AI tasked with discovering a hidden rule that takes three numbers as input and returns a boolean. These numbers can be any float or integer number that can be typed in 3 or fewer digits, not including signs and decimal characters (e.g. -11.3 is a valid value). In each response, you will provide your step-by-step reasoning for your future plans on this task, and then you will take one of two actions:\n1. Provide a single test case to observe the output of a rule (e.g. \"Test Case: (1, 2, 3) which corresponds to x=1, y=2, z=3. You may only propose one test case at a time.\n2. Provide a final \"guess\" in the form of a Python lambda function (e.g. \"Final Guess: ```lambda x,y,z: x < y < z```\u201d). You only get to select this action one time, so don't use it unless you are confident or are forced to.\nYou will have up to 30 attempts at test cases, and you will have exactly one attempt at guessing the final function. Make sure all of the tests you provide are unique from other ones you have already tried \u2013 the functions are all deterministic, and will be the same every time. You should prioritize getting the rule as correct as possible, since an incorrect answer will get no points. You should therefore not guess the function unless you are quite confident, or if you are very close to running out of attempts.\nYou must return your responses in the format laid out above at the very bottom of your message. For example, if you want to submit a test case, you must conclude with the string \"Test Case: (x,y,z)```\", where x,y,z are replaced with your guesses. If you want to submit a final guess, you must conclude with the string \u201cFinal Guess: ```<function>```\u201dwhere <function> is replaced with a Python lambda function. Do not include any comments or additional text on the same lines as these two things.\nMake sure to include your reasoning for your tests \u2013 what you are testing for, why you selected that test, etc.\nResponses by the models are pulled out via regular expressions matching the formatting in the prompt. We find that after controlling for various formatting eccentricities (Python blocks, markdown, bold characters, etc) that all listed models are capable of providing test cases in this format."}, {"title": "VERIFYING EQUIVALENT FUNCTIONS", "content": "To verify two provided lambda functions are equivalent, we generate a large number of test cases and ensure the provided rules match on all outputs. Specifically, we create three sets of cases:\n1. Integer Grid Cases - We construct a 40x40x40 grid of integer triplets from -20 to 20, inclusive, leading to 64,000 triplet cases.\n2. Random Uniform Cases - We construct a list of 10,000 uniformly random float triplets from -200 to 200, inclusive.\n3. Special Cases - We hand-design a small set of test cases to ensure all hidden rules in the full split are adequately tested.\nWe mark a rule as incorrect if any test cases generated above show different behavior between the hidden rule and the guessed rule, and mark it correct otherwise."}, {"title": "TEST CASE NOVELTY", "content": "Test case novelty is an interesting second order metric for success upon the WILT task. Broadly speaking, models that reuse fewer tests are rewarded with more information for which to solve the"}, {"title": "RESPONSE COMPLEXITY IN TEST SWAP EXPERIMENTS", "content": "In Figure 6 we show the set inclusion ratios in the case where a model is provided another model's test cases. That is, we show whether an error in the final guess of a model is likely to be smaller / less than one (e.g. $x < y < z$ instead of $x \u2264 y \u2264 z$), or larger / greater than one (e.g. $x > 0$ instead"}, {"title": "FAILURE CASE EXAMPLES", "content": null}, {"title": "DOOM LOOP WITHOUT REASONING FAILURE", "content": "Table 7 shows an example of a model (Deepseek-Chat-v2.5) entering a doom loop during the test proposal phase, but where that doom loop does not constitute a reasoning failure. Reasoning has been removed for brevity."}, {"title": "APPROXIMATELY CORRECT", "content": "Table 8 shows an example of o1-mini getting a very difficult test case (co-primality) approximately correct, failing only because it adds an additional arbitrary constraint upon the magnitude of the values despite no such constraint existing. Reasoning has been removed for brevity."}, {"title": "CONFIRMATION BIAS", "content": "Table 9 shows an example of ol-preview failing a relatively easy test case ($x > y \u2265 z$) due to a confirmation bias error. The model uses only 9 test cases and correctly identifies that the rule returns true when all three are equal, but submits five test cases confirming that and none exploring other rules which are true when three items are equal. Reasoning has been removed for brevity."}, {"title": "SAME TEST FOR NEW REASON", "content": "Table 10 shows an example of Claude Sonnet 3.5 repeating a test, where it will mistakenly generate the same test for a different stated reason. We see the model notice it has repeated a test only after it has already submitted the test. Other tests have been removed for brevity."}, {"title": "FULL CONVERSATION EXAMPLES", "content": null}, {"title": "DOOM LOOP ON CODING TASK", "content": "Below we provide a full example of a \"doom loop\" interacting with an LLM in a multi-turn conversation in a real world coding task. We use llama 3 8b to generate code for a pytorch variational autoencoder, and after 2 turns of debugging it repeats an already mentioned suggestion."}]}