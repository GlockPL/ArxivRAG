{"title": "MageBench: Bridging Large Multimodal Models to Agents", "authors": ["Miaosen Zhang", "Qi Dai", "Yifan Yang", "Jianmin Bao", "Dongdong Chen", "Kai Qiu", "Chong Luo", "Xin Geng", "Baining Guo"], "abstract": "LMMs have shown impressive visual understanding capabilities, with the potential to be applied in agents, which demand strong reasoning and planning abilities. Nevertheless, existing benchmarks mostly assess their reasoning abilities in language part, where the chain-of-thought is entirely composed of text. We consider the scenario where visual signals are continuously updated and required along the decision making process. Such vision-in-the-chain reasoning paradigm is more aligned with the needs of multimodal agents, while being rarely evaluated. In this paper, we introduce MageBench, a reasoning capability oriented multimodal agent benchmark that, while having light-weight environments, poses significant reasoning challenges and holds substantial practical value. This benchmark currently includes three types of environments: WebUI, Sokoban, and Football, comprising a total of 483 different scenarios. It thoroughly validates the agent's knowledge and engineering capabilities, visual intelligence, and interaction skills. The results show that only a few product-level models are better than random acting, and all of them are far inferior to human-level. More specifically, we found current models severely lack the ability to modify their planning based on visual feedback, as well as visual imagination, interleaved image-text long context handling, and other abilities. We hope that our work will provide optimization directions for LMM from the perspective of being an agent. We release our code and data at https://github.com/microsoft/MageBench.", "sections": [{"title": "1. Introduction", "content": "The advent of Large Language Models (LLMs) [9, 14, 19, 68, 69, 104], and Large Multimodal Models (LMMs) [6, 8, 46, 67] has revolutionized the fields of natural language processing and computer vision. These models have demonstrated remarkable capabilities across a variety of classical tasks, including translation [56, 75, 87, 100], summarization [7, 59, 105], VQA [11, 25, 58, 64], captioning [72, 89] and etc. The more recent o1 model [4] also stands out due to its exceptional reasoning abilities, particularly on math and coding. The leap in reasoning capability of LLMs has paved the way for the development of LLM-based agents, which harness the power of these models to autonomously perform a range of sophisticated tasks, from engaging in meaningful dialogue to executing intricate problem-solving strategies.\nCompared to LLM-based agents, the LMM-based agents can be more attractive with the involvement of visual signals, which explicitly expands the boundaries of applications, e.g. robotics and autonomous driving. Consequently, there remains a critical need for comprehensive benchmarks that can evaluate the reasoning and planning capabilities of LMMs from the perspective of agents. Unfortunately, rare efforts have been made - existing benchmarks for LMMs mainly focus on the simple VQA problems [25, 33, 38, 49, 93, 95]; their reasoning assessment generally relies on the language part, which does not require interleaved involvement of visual signals [23, 52, 55, 101, 107, 109]. Such evaluations are not suitable for the evolving demands of visual agents. While there exist plenty works of LMM agent [12, 74, 112] that can be adapted to evaluate models, they often focus too much on the environment or agent solution and have a complex and specific scenario. This not only makes the evaluations costly, but also leads to limited generalization capability when environment changed.\nIn this work, we target at building a reasoning capability oriented benchmark for evaluating LMM's potential of being an agent regarding complex visual tasks. When defining 'complex visual task,' we expect not only the reasoning of initial visual input, but also the continuous understanding of visual feedback throughout the entire process. These tasks require models to dynamically interact with visual information, continually updating their understanding and decisions based on new visual cues, much like a human would. We refer to this novel reasoning paradigm, which integrates other modality (vision) into the reasoning chain, as Vision-in-the-Chain (ViC), as illustrated in the last block of Figure 2. Technically, the ViC paradigm is fundamentally different from previous reasoning paradigms, e.g. text chain-of-thought (CoT) [35, 79, 80, 97, 106] and visual CoT [23, 55, 101, 107, 109]. The latter two paradigms only perform text-based reasoning with multiple intermediate steps, without incorporating the visual signals at each step,"}, {"title": "2. Related Work", "content": "Large Multimodal Models. The advent of large language models (LLMs) [6, 9, 14, 19] has demonstrated remarkable reasoning capabilities [79, 80] and the potential for general intelligence [20]. By employing a single model with different prompts, a multitude of tasks can be accomplished [18, 63]. A natural extension of this concept is to apply similar methods to other modalities to achieve general multimodal intelligence. Flamingo [8] was among the first to explore multimodal in-context learning [83, 89, 113], followed by the emergence of numerous large multimodal models [1-3, 6, 46, 67]. These models employ various technical approaches [37, 43-46, 66]. As technology has progressed, product-level multimodal large models such as GPT-4V [6], GPT-4O [2], Gemini [67], Claude [1], and Grok-2 [3] have showcased state-of-the-art performance.\nVisual Reasoning. Chain-of-thought prompting [35, 79, 80], flow engineering [62], self-reflection [65, 91], and their various variants [110] have demonstrated significant improvements. In visual tasks, the primary evaluation datasets for visual reasoning are those based on VQA tasks, such as ScienceQA [52] and MathVista [53]. Due to the limitations of these evaluation datasets, many existing studies [23, 55, 101, 107, 109] on visual reasoning using \u201cCoT\u201d as a keyword mainly focus on extracting information from multimodal problems, and then utilize text-based intermediate processes such as captioning [107], rationales [52], relational graphs [55], and question tables [109].\nSome recent works have attempted to incorporate procedural information from other modalities, not just text, into the reasoning process to accomplish classic visual tasks. For instance, Image-of-thought [111] introduces image editing tools to facilitate better information perception by the model. Similarly, DetToolChain [84] incorporates auxiliary metrics and editing tools to enhance the model's capability in performing high-precision object detection tasks. However, these ViC type reasoning works are constrained to classical vision tasks.\nLarge Model Based Agents. Al agents are artificial entities that sense their environment, make decisions, and take actions. With the development of large models, there has been an explosive increase in research treating large models as the brains of agents [85]. Researchers are utilizing LLMs to study single-agent systems [17, 22, 71, 88], multi-agent collaboration [10, 27, 30, 39], and human-agent collaboration [28, 50] in specific environments. The emergence of LMMs has further enhanced the perceptual modalities of agents, enabling LMM agents to perform tasks in multimodal scenarios [70, 86, 108]. These efforts focus more on the specific environment itself and the agent solutions. They often come with complex dependencies (both hardware and software) [12, 98, 108, 114], require database setups [90, 112], making them not convenient and general to directly evaluate across models.\nBenchmarking LMMs. There are numerous evaluation datasets for LMMs that comprehensively assess various capabilities such as general VQA [25, 33, 38], visual expertise [96], visual perception [21], and visual reasoning [52, 53]. From the perspective of application domains, some studies also evaluate models' understanding of tables [34], charts [54, 81, 115], geometry [29, 102], and real-world scenarios [40]. Most of these evaluations are presented in the form of multiple-choice questions, which simplify and abstract real-world problems.\nAnother approach to evaluating models is to deploy them on agents for task-level end-to-end assessments. Previous attempts such as AgentBench [47] and VisualAgentBench [48] integrate several environments in one repository. However, existing works are environment or task oriented, but we are reasoning oriented, i.e., not all meaningful environments require meaningful and representative reasoning"}, {"title": "3. Introducing MageBench", "content": "MageBench aims to select most simple, representative environments from the perspective of reasoning and with generalization ability. We investigate dozens of environments and select those meet the criteria below:\n\u2022 Representativeness on Reasoning: To be an ideal agent, LMMs need real-world engineering knowledge to assist humen (WebUI). To manipulate in real-world scene or virtual UI, they need spatial understanding and planning (Sokoban). In the future, we may also expect the extension to multi-agent system (Football). Many robotics simulation environments(e.g., [32, 60, 98] and OmniGibson in [48]), Virtual reality game (e.g., [12, 42, 114] and Mi- craft in [48]) and app manipulation (e.g., [16, 41, 57, 74, 76]), although more real and practical with complex actions, their high level planning are actually simple and direct. For example, to move an object in simulation environments or search an commodity in a webpage, models may make instruction following errors and perceiving errors, but there isn't much operability at the planning level. So we exclude them.\n\u2022 Visual Feedback: The visual feedback must be indispensable for the relevant tasks. In other words, we will not select an agent environment if it can be effectively completed without image input. For example, we found using text feedback of the webpage is enough for many of the tasks in webpage operating environments [90, 112].\n\u2022 Simplicity: The environments are highly streamlined, with minimal database, library, and hardware requirements. They offer fast simulation speeds and good scalability. On one hand, the simplicity of these environments allows researchers to quickly get started and facilitate dissemination. On the other hand, low latency, minimal cost, and reduced energy consumption are essential for potential future RL training and scaling.\nBased on the above criteria, we have selected and constructed the following three environments: WebUI, Sokoban, and Football. There are a few that also meet the requirements but are of a similar type, and we have chosen these three as representatives. We will introduce each of our environments in detail below."}, {"title": "3.1. WebUI", "content": "We collected minimal web projects from GitHub, strictly adhering to the corresponding licenses, consisting of only a few HTML, JavaScript, and CSS files. For each webpage, we will create a Markdown-formatted webpage description. The webpage description is a image-text interleaved document that provides sufficient information to fully reconstruct the website. It includes detailed webpage descriptions, external resources, and screenshots before and after various interactions and etc. The task of WebUI is to reconstruct the website based on the description and a Google Chrome web driver.\nThe evaluation of WebUI is based on comparing the CSS properties of atomic elements. We first define atomic elements as follows: Suppose webpage B is a reproduction of webpage A. An HTML tag in webpage A is considered atomic if any attribute it contains is guaranteed to appear in a corresponding tag in webpage B. For example, all headings, texts, and images in webpage A may exist in webpage B as different types of tags (e.g., both h1 and span tags can display text). However, we know that there must be a corresponding tag in webpage B. Such tags that necessarily exist are termed as atomic. In the evaluation process, we first match the atomic elements in the target webpage with the elements in the webpage generated by the agent using a carefully designed matching algorithm. Next, we compare the CSS property similarity of the successfully matched elements, typically using metrics such as relative error or checking if the values are equal. Finally, we provide a weighted similarity score as the evaluation result. We refer to this metric as Atomic Element Similarity (AES). The technical details involved are extensive and will be presented in the Appendix. A.1.3."}, {"title": "3.2. Sokoban", "content": "Sokoban is a well-known two-dimensional logic video game where the task is to maneuver a character to push all boxes onto designated target areas. The game is highly challenging due to the presence of numerous losing states and traps, necessitating strong planning abilities [61]. The planning and reasoning capabilities of LMMs may effectively mitigate this issue, making this environment ideal for testing an agent's path finding, planning, error correction, and foresight abilities. We utilize the rendering environment provided by [61], simplifying actions to four directions (up, down, left, right) by combining move up and push up. We generated and stored 182 levels of varying difficulty. Further details can be found in Appendix. A.2.\nWe also adapt the reward value defined in [61] to evaluate the LMMs. However, unlike their approach, we use the historically optimal reward throughout the trajectory rather than the final reward. This is because the reward includes a penalty for the number of steps taken. Based on extensive testing, we found that given the current capabilities of LMMs, using the final reward tends to be dominated by factors such as the model's output length, the number of steps we set (for the online setting), the length penalties and etc. This is an outcome we aim to avoid."}, {"title": "3.3. Football", "content": "Football, as one of the most competitive and cooperative sports, can fully demonstrate an agent's spatial intelligence and collective intelligence, potentially providing a research foundation for future LMM multi-agent systems. We chose the rendering platform provided by Google Football Research [36] for our study. We generated 108 scenarios as initial states, with each initial scenario serving as a level (analogous to a level in Sokoban). These levels cover different areas of the football field and are categorized into personal (scenarios where good passing routes are unavailable, requiring players to showcase individual skills), teamwork (scenarios suitable for demonstrating team collaboration and passing), and real-world (scenarios from actual World Cup matches). The agent will use text outputs to perform 18 actions (including moving, long passing, and shot), starting from each scenario and simulating up to 400 frames until a goal is scored or the ball is intercepted. More details can be found in Appendix. A.3. We also designed an automatic rendering algorithm (see Appendix. A.3.4.) that reduces the average number of API calls needed per scenario from 80 to less than 20, without affecting the results.\nDuring the simulation process, the LMM agent will control only one player (always the player in possession of the ball), while the other players are controlled by built-in AI bots. The presence of numerous agents results in a highly stochastic environment simulation. Using metrics such as win-rate leads to high variance and requires extensive repetitions. To address this issue, we carefully designed a more dense reward system to comprehensively evaluate the model's capabilities. The design of this reward system is as follows:\n$R(t) = \u03bb_1 \u03b4_{move}^{(t)} + \u03bb_2 \u03b4_{oppo}^{(t)} + \u03bb_3 \u03b4_{scored}^{(t)} + \u03bb_4 \u03b4_{stole}^{(t)} + \u03bb_5 \u03b4_{pass}^{(t)} S_{pass}^{(t)} + \u03bb_6 \u03b4_{shot}^{(t)} S_{shot}^{(t)}$\nwhere $\\delta_{event}^{(t)}$ is a indicator function that event happened at time step t. $S_{move}$ and $S_{oppo}$ represent the reward values obtained after processing the distance the ball has been moved forward and the number of opponents surpassed, respectively. $S_{pass}$ and $S_{shot}$ are metrics that quantify the quality of passing and shot. For additional details and specific parameters, please refer to Appendix. A.3.3."}, {"title": "4. LMM-as-Agents", "content": "We represent our Agent-Environment system and its evaluation using a Partially Observable Markov Decision Process (S, A, T, R, O), indicating that the agent $\\pi_\\theta$ cannot directly access the complete state from state space $S_t \\in S$. Instead, the agent has to observe vision feedback (and probably few text feedback for WebUI) in observation space $o_t \\in O$, and leverage its planning ability to generate actions within a discrete action space $a_t \\in A$. The environment will serve as the transition function to update the state based on the agent's actions: $T : S \\times A \\rightarrow S$. Finally, our designed reward assigning function R will provide the evaluation results.\nThere will also be some pre-defined prompts involved in the agent. System prompt $P_{sys}$ introduce the rule, target, available actions of the system. In WebUI, task prompt $P_{task}$ is a specific description of a Webpage. For other environment, $P_{task} = o_1$. CoT prompt $P_{cot}$ and IO prompt $P_{io}$ designate the model's inner thought flow and output format so that we can parse the text-based output to actions."}, {"title": "5. MageBench Results and Analysis", "content": "We use the MageBench and the unified agent setting as the carrier to study what kind of LMMs have the potential to become an agent. It is worth stating that these models may perform better in MageBench with specially designed agents, prompts, and settings, but for the sake of a fair comparison, we will use the same standard settings below.\n\u2022 Except for WebUI, the $P_{cot}$ will prompt the model to analyze before actions, but will not specify how to. This is similar to zero-shot CoT [35] and the prompt is shared across all environment. This is more effective in validating the model's ability to self-plan rather than being compensated for by the instruction-following ability.\n\u2022 Online planner will use AM = 5 and OM = 1, so that we can fairly evaluate models without multi-images capability. According to our tests, these two types of memory do not have a significant impact on the performance for current models, See Appendix. B.2.\n\u2022 In order to minimize the influence of non-reasoning factors, we will give the model two retries only if it caused a parsing error.\n\u2022 Agent-based evaluation always has a large stochastic variance, and in addition to the careful design we did for the metric, we also require all experiments to be repeated and averaged. For MageBench mini set, Sokoban and WebUI should repeat 3 times and Football repeat 10 times. For MageBench complete set (in Appendix. B.1), all experiments reported are averaged over 3 repetition.\nWe select LMMs that are trained for general usages and"}, {"title": "5.1. Standard setting", "content": "We use the MageBench and the unified agent setting as the carrier to study what kind of LMMs have the potential to become an agent. It is worth stating that these models may perform better in MageBench with specially designed agents, prompts, and settings, but for the sake of a fair comparison, we will use the same standard settings below.\n\u2022 Except for WebUI, the $P_{cot}$ will prompt the model to analyze before actions, but will not specify how to. This is similar to zero-shot CoT [35] and the prompt is shared across all environment. This is more effective in validating the model's ability to self-plan rather than being compensated for by the instruction-following ability.\n\u2022 Online planner will use AM = 5 and OM = 1, so that we can fairly evaluate models without multi-images capability. According to our tests, these two types of memory do not have a significant impact on the performance for current models, See Appendix. B.2.\n\u2022 In order to minimize the influence of non-reasoning factors, we will give the model two retries only if it caused a parsing error.\n\u2022 Agent-based evaluation always has a large stochastic variance, and in addition to the careful design we did for the metric, we also require all experiments to be repeated and averaged. For MageBench mini set, Sokoban and WebUI should repeat 3 times and Football repeat 10 times. For MageBench complete set (in Appendix. B.1), all experiments reported are averaged over 3 repetition.\nWe select LMMs that are trained for general usages and"}, {"title": "5.2. Best-of-N Result", "content": "Previous subsection has demonstrated the feasibility of existing LMMs as agents. In the following, we use best-of-N scaling curves to investigate the potential of the models in relevant tasks in Fig. 6. Firstly, we observe that in the Football environment, many models can surpass human performance by computing the best-of-N, with a steep upward slope. This indicates that there is a significant opportunity to achieve substantial improvements in this task using RL algorithms. However, in the Sokoban task, the best-of-N curve exhibits slow growth, suggesting that directly using"}, {"title": "5.3. Error Statistics and Analysis", "content": "We have categorized the types of errors for different models on MageBench. For Sokoban and Football, since most errors stem from model decision failures, we additionally categorize parsing errors during the parsing process, as shown in Table 2. Among these, \u201cInvalid Actions\u201d refer to errors arising from outputs that do not conform to the required format, leading to parsing errors or incorrect actions. \"Repeating Actions\" is a common comprehensive error in LLMs, which refers to the phenomenon where the same token is repeated multiple times. If such output is successfully parsed, it results in a large number of repetitive actions.\nAs shown in Table 2, we observed that open-source models exhibit a significant number of IA and RA type errors in the online setting, and Claude's performance is similar. This phenomenon is highly consistent with the online results presented in Table 1. Regarding RA type errors, some studies have pointed out that they are commonly found in scenarios where the training and testing domains do not match. This indicates that both the open-source models and Claude have deficiencies in training with the Vision-in-the-Chain type of data studied in this paper."}, {"title": "6. Summarization", "content": "In this paper, we introduce a new benchmark called MageBench, which is designed with a capability-oriented approach and includes three lightweight yet highly challenging environments. We conducted tests on a wide range of both open-source and close-source LMMs, with two baseline agent settings. The results indicate that current models lack ViC type reasoning abilities, as well as capabilities in cross-modal long context comprehension, visual imagination, and spatial planning. Our findings and the provided environments may inspire future research directions in the development of LMMs, which we discuss further in the Appendix. D. We hope to offer LMM developers valuable insights and optimization directions, and we will release the code and data as open-source in the near future.\nLimitations and future work. Currently, we have a limited number of environments. Additionally, to validate the model's capabilities, we use a standardized and simple agent setup. If this work garners attention within the community, we plan to substantially increase the number"}, {"title": "A.4. Environment selection", "content": "In this section, we will elaborate on the factors considered during our selection process for the environment and discuss the environments that were excluded from our study.\nSimplicity. One of the critical requirements in evaluating environments is simplicity and ease of use. In the assessment of large language models (LLMs), tools like eval-harness enable the evaluation of dozens of datasets within a unified framework with minimal modifications, and their installation can be accomplished with a single command. Most LMM evaluations based on VQA datasets follow a similar pattern. However, this is not the case with existing agent environments, which are often highly complex or have significant hardware dependencies.\nMany physical simulation platforms, such as Habitat, VirtualHome, and OmniGibson in the VisualAgentBench suite, have stringent requirements for GPU types and CUDA versions, often conflicting with the typically high-version torch and CUDA dependencies of LLM code. Some agent frameworks propose controlling UI operating systems such as Windows and Ubuntu, and software applications on these systems like office suites. These environments impose high demands on the platform, presenting prohibitive costs for researchers seeking to quickly validate model capabilities. Additionally, certain environments are inherently large and costly to install. For instance, WebArena, VisualWebArena, and WebShop require complex installation processes and extensive web databases. Some environments aim for realism by running on large games via platforms like Steam, such as \"Black Myth: Wukong,\" which can be tens or even hundreds of gigabytes in size, making them impractical.\nEach of these unmet requirements can lead to slow environment simulation speeds, inability to perform parallel validations, and hinder rapid dissemination of findings.\nVisual Feedback. We require that our environment must include visual feedback, and that such visual feedback is irreplaceable. In certain web-based environments, aside from images, the web pages themselves contain substantial information such as HTML and captions. Our observations indicate that tasks can be effectively accomplished using this information. In fact, in real-world scenarios, web automation testing and agent embedding tend to interact directly with HTML elements rather than relying on visual information as an intermediary. Consequently, we exclude such environments, along with those that do not necessitate visual participation, such as LLMAgentBench, from the scope of this paper.\nRepresentativeness on Reasoning. For each environment we select, we must be able to clearly identify the required capabilities. This implies that the environment must have substantial planning space and the possibility for errors, with a high degree of freedom at both the high-level"}, {"title": "B. More Results", "content": "B.1. MageBench results\nIn Table 4, we present the test results of additional models on the larger MageBench full set. We selected models of varying sizes from 14 different model families, totaling 26 models. Given the large number of scenarios, each model and scenario only needed to be tested three times to compute the mean, resulting in a very small variance. The related conclusions are similar to those discussed in the main part, so they will not be reiterated here. Notably, we did not measure the results for Claude-3.5-Sonnet and human participants, as we were unable to bear the associated economic costs for the full dataset. Additionally, as mentioned in the main text, the Online setting in the current WebUI environment is essentially non-functional. Consequently, we did not allocate resources to testing in this setting.\nB.2. Agent memory\nWe selected the most robust model in the Sokoban environment under the online setting, specifically GPT-40, to conduct memory ablation experiments. In this context, \u201cAM\u201d refers to action memory, which denotes the number of historical actions, while \"OM\u201d signifies observation memory, representing the number of observations inputted for generating the next action. Here, observations refer to the game's visual frames. We have chosen AM values of 1, 5, and 10, and OM values of 1, 2, and 3 for our combinations. For instance, when AM=10 and OM=2, the model's input and output can be described as follows (For the sake of brevity, we have omitted Psys, Pcot, Pio.):\n$\\pi_\\theta (a_{t-9}, ..., a_{t-2}, a_{t-1}, O_{t-1}, a_t, O_t) \\rightarrow a_{t+1}$.\nIn our expectations, as the context length increases, the model, having access to more preceding information, should be able to perform the task better. However, in reality, we have not observed a stable correlation between context length and performance, as shown in Table 5. The values presented in the table are all within the range of variance, thus it cannot be concluded that the impact is stable. This result indicates that the model's performance, based solely on the previous frame and action, does not differ from its performance when more memory is added. This is inconsistent with human behavior. It suggests that the LMM still has deficiencies in handling long text-visual context tasks.\nC. Benchmark generalization capability"}, {"title": "C.1. Robotics and embodied AI", "content": "We hope to assert that our ViC-type reasoning could be significant in fields such as robotics. However, real-world mechanical arm environments are extremely costly, and the generalizability across different mechanical systems is often poor. In virtual physical environments, the input for mechanical arms typically consists of numerical values like torque and angles, and it is evidently unrealistic for LMMs to generate these values directly. One potential alternative is for LMMs to generate only high-level planning, with the execution of actions relying on predefined action models such as PDDL. However, this approach does not effectively evaluate the planning capabilities of LMMs. This is because the predefined actions are very limited, and if the actor model makes an error, LMM has no means to correct it.\nTo perfectly decouple planning and control while obtaining a rich planning space, we have designed a human-in-the-loop testing method. Simply put, this involves using humans as robots, with the LMM providing detailed natural language instructions for actions. This way, humans can accurately execute the actions as directed by the LMM. Consequently, the quality of the final outcome is solely determined by the effectiveness of the LMM's planning. We have designed two tasks for LMM in real-world scenarios:"}, {"title": "C.1.1. Overall results", "content": "In Figure 16, we directly present the performance of three models, i.e., GPT-40, Claude-3.5-Sonnet, and InternVL2-7B, under the two newly constructed tasks. The figure illustrates the final environmental scenarios for these tasks. Overall, it can be observed that the planning capabilities in real-world scenarios are highly correlated with the test scores from our MageBench.\nGPT-40 successfully achieved the preset objectives in both scenarios, albeit with a number of seemingly meaningless actions in between. In contrast, Claude demonstrated"}, {"title": "C.1.2. Visualization", "content": "Figures 18 and 19 illustrate the outputs of GPT-40 and the environment feedback across two tasks, encapsulating our proposed Vision-in-the-Chain (ViC) framework. It is evident that GPT-40 exhibits a significant number of repetitive and meaningless actions in the Book Hanoi Tower task. Furthermore, other models, such as InternVL2-7B, predominantly demonstrate errors characterized by the initiation of repetitive and identical analyses and actions after performing two or three initial actions correctly. This pattern aligns with the error types analyzed in MageBench, suggesting that due to the lack of ViC-type and multi-turn chat data, these models encounter instruction-following failures when faced with tasks in an online setting."}, {"title": "C.2. Structured Visual Generation", "content": "In MageBench, our WebUI environment is designed to reconstruct a target website. Our goal is to assess whether this testing environment can accurately reflect the model's capability in structured visual generation, with an eye towards"}, {"title": "D. Open questions and future researches", "content": "In the process of proposing MageBench and conducting our evaluations, we have identified some capabilities that existing models lack, feasible research directions, and open questions. We hope that these summaries can assist and inspire future work:\n\u2022 Existing open-source models exhibit a significant deficiency in ViC-type reasoning capabilities. How to obtain large amounts of ViC-type training data is an open question worthy of research.\n\u2022 The models exhibit a lack of imagination and long-term planning abilities in visual spatial reasoning tasks (such as Sokoban). The Best-of-N results indicate that the current models do not possess such potential, necessitating fundamental, mechanism-level research.\n\u2022 For most tasks that exhibit potential, the models show considerable improvement in the Best-of-N results. How can we design a reinforcement learning approach to enhance the models, enabling them to accumulate explainable, text-expressible experiences, similar to how humans accumulate tips and tricks for playing games?\n\u2022 Theoretically, an Online Agent should benefit from having a longer memory when completing tasks such as Sokoban and Football, or when making multiple rounds of modifications to a generated webpage. However, our observations show counterintuitive results where not only is there no benefit, but there is even a slight decline in performance. From an engineering perspective, this could be due to the model's lack of ability to handle long contexts, multi-turn dialogues, and tasks involving multiple images."}, {"title": "A.1. WebUI", "content": "A.1.1. Web page collection and prepossessing\nWe searched and collected a large number of open-source, minimalist web design examples from GitHub. These small projects are often educational examples, and we filtered them to include only web projects composed of a small number of HTML, JavaScript, and CSS files. We ensure that the entire process of data collection, processing, application to our benchmark, and re-release as open source fully complies with the original open-source licenses of the GitHub repositories.\nWe then pre-process the projects with the following steps:\n\u2022 Resource Download: First, we download images and other resources from the webpage to a local folder. We then rename these files and correspondingly update the image address names in the webpage.\n\u2022 Content Adjustment: We made several content adjustments to the web pages, including layout and attributes. Overly long web pages were split into multiple pages. Additionally, some web page content was modified to prevent existing models from having encountered these examples during data scraping on GitHub for training purposes.\n\u2022 Atomic Element Annotation: In the website's HTML files (including parts of JavaScript that contain HTML code), we identify all atomic elements (as defined in the main part). For each corresponding HTML tag, we add two attributes: data-filter-by and data-evalby. The data- filter-by attribute contains a single HTML or CSS property (such as the text property of a paragraph or the URL property of an image tag). This attribute, present in only a few elements, highlights the characteristics of the atomic element and is used to optimize the precision of atomic element matching. The data-evalby attribute consists of multiple CSS properties (such as font, color, background, etc.). It displays the scoring criteria of the atomic element, and the evaluation of the element is based on the similarity of the CSS properties listed in data-evalby. You can find an example of the annotated origin webpage in \u0395.1.1.\n\u2022 Web Interaction Identification: We identify and design meaningful web interactions (such as scrolling, clicking buttons, entering content, etc.) for each webpage. These interactions are implemented as Python functions using Google Chrome and the Selenium library. Additionally, we capture and store screenshots of the webpage after each interaction. An example interaction python code can be found here E.1.2.\n\u2022 Task Description: We write a website reconstruction task description in markdown format of interleaved"}, {"title": "A.1.2. System prompt Psys", "content": "The system prompt for this environment is shown as follow:\nYour job is to re-implement a Web page\nUI given target description and screen\nshot", "index.html": "and\nprobably 'style.css' and `script.js`\nif needed.\nYou will be provided a task description\nand available actions. Actions including\nwriting to files and interact with\nyour generated webpage.\nHere are the action ID and explaination:\n'write_html' Input: str"}, {"index.html": "if called second\ntimes", "Input": "str", "script.js": "if called\nsecond times"}, {"Input": "str, Write the input\\"}]}