{"title": "Recurrent Diffusion for Large-Scale Parameter Generation", "authors": ["Kai Wang", "Dongwen Tang", "Wangbo Zhao", "Yang You"], "abstract": "Parameter generation has struggled to scale up for a long time, significantly limiting its range of applications. In this study, we introduce Recurrent diffusion for large-scale Parameter Generation, called RPG. We first divide the trained parameters into non-overlapping parts, after which a recurrent model is proposed to learn their relationships. The recurrent model's outputs, as conditions, are then fed into a diffusion model to generate the neural network parameters. Using only a single GPU, recurrent diffusion enables us to generate popular vision and language models such as ConvNeXt-L and LoRA parameters of LLAMA-7B. Meanwhile, across various architectures and tasks, the generated parameters consistently perform comparable results over trained networks. Notably, our approach also shows the potential to generate models for handling unseen tasks, which largely increases the practicality of parameter generation. Our code is available here.", "sections": [{"title": "1. Introduction", "content": "Looking back on the journey of deep learning, the scaling up of neural networks is one of the most important keys to its remarkable success across various tasks [22, 33, 44]. In contrast, neural network parameter generation, from Hyper-Networks [21] to recent diffusion-based methods [51, 66, 69], has struggled to scale up effectively, limiting its practical applications. As shown in Fig. 1, the scale gap between vision (or language) models and the generated parameters is at least 104, posing significant challenges for this field.\nTo figure out the key challenges in scaling up parameter generation, we first analyze its unique requirements. Unlike traditional deep learning models that typically process data such as images or text, parameter generation involves network parameters in the training process. The size of parameters could be significantly larger than images or texts size. This fundamental difference in input format presents a significant challenge when scaling up. As the size of generated parameters increases, the GPU memory requirement quickly becomes prohibitive.\nRecently, p-diff [69] and D2NWG [66] attempted to address the challenge of balancing memory constraints with large-scale parameter generation. P-diff mainly generates a subset of the entire neural network parameters, while D2NWG employs a two-step process of synthesizing parameter parts and then combining them to form a complete model. However, these methods may overlook the inherent correlations among the parameter parts. To study the impact of these correlations, we conduct an experiment: exchanging partial parameters between two ViT-Tiny [14] models trained on CIFAR-10 [32] dataset. As illustrated in the following table, the significant performance degradation highlights the critical importance of parameter correlations.\nHow can we model parameter relationships and leverage them for efficient parameter generation? Vision transformers often outperform CNNs by modeling patch relationships and capturing global information via self-attention. In language tasks, LLMs use next token prediction to model token relationships, capturing long-range dependencies. Inspired by these approaches, we can consider treating parameter parts as tokens in neural network parameter generation, potentially enabling methods to model inter-parameter relationships and capture dependencies."}, {"title": "2. Related Works", "content": "Diffusion models. Diffusion models [13, 23, 48] gain increasing popularity in recent years, due to their superiority in image generation. Ever since its advent, many works have been done focusing on improving the generation quality and efficiency of diffusion models. For generation quality, Rombach et al. [55] propose to conduct diffusion in the latent space, enabling high-resolution image synthesis. Peebles and Xie [50] leverage the transformer [68] to explore scalability of diffusion models, proving the possibility of generating higher quality images by increasing model size. As for efficiency problem, efficient samplers [42, 64, 65], efficiency models [16, 62, 74], and global acceleration approaches [43, 49] are proposed to increase diffusion models' efficiency. These methods facilitate generating high quality images with less computational and/or memory cost. Although diffusion models for image generation have achieved great success, how to improve quality and efficiency in large-scale parameter generation remains to be explored.\nRecurrent models. Recurrent neural networks (RNNs) were first proposed to process sequential data, i.e., the text. To tackle the vanishing gradient problem in early RNNs, long short-term memory (LSTM) [24, 25]. In recent years, transformer-based models [68] exhibits excellent potential in sequential data processing, due to their parallelized training and scalability. Although most transformers [53, 67] are used in auto-regressive manner, they can be seamlessly adapted to the recurrent paradigm. However, transformer-based models are suffering from the the quadratic complexity during their generation process, which greatly hinders their efficiency. Recently, various attempts, such as linear attentions [7, 70], RWKV [52], Mamba [11, 19], and xL-LSTM [2], have been proposed to tackle this problem.\nParameter generation. The core idea of parameter generation is to learn the distribution of trained parameters. Stochastic neural networks [5, 18, 46, 57, 63, 71] and Bayesian neural networks [4, 17, 30, 31, 47, 54] model the priors or probability distributions over the parameters. These approaches mainly employ the learned prior knowledge of parameters to improve robustness and generalization, and to mitigate overfitting and uncertainties. However, these methods are limited by their generality to large-scale generation or more complex real-world scenarios.\nHyperNetworks [21], i.e., a small network, is proposed to generate various architectures' parameters for a larger network. Smash [6] extends the range of architectures via a memory read-writes scheme. With the development of diffusion models, many works [8, 15, 29, 37, 38, 51, 66, 69] adopt diffusion models to generate neural network parameters. G.pt [51] collects 23 million checkpoints as the training data and uses conditional diffusion to learn the distribution of the parameters. Besides the heavy cost of collecting model parameters, G.pt can only generate less than 10K parameters. P-diff proposes unconditional diffusion to simulate the parameter updating. HyperRepresentations [58, 59, 61] use an autoencoder to capture the latent distribution of trained models. COND P-DIFF [29] and Tina [37] introduce task-controlled or text-controlled parameter generation method. Unfortunately, the above methods have a common drawback: can not generate large-scale parameters, such as whole parameters of ResNet, ViT, ConvNeXt, or LoRA. Therefore, our approach brings new inspiration to the field of parameter generation."}, {"title": "3. Large-Scale Parameter Generation", "content": "3.1. Overview\nOur approach comprises two key components: parameter tokenization and recurrent diffusion. We show the inference process of recurrent diffusion in Fig. 2. The permutation state and position embedding are fed into the recurrent model. Then, the outputs of the recurrent model serve as conditions for the diffusion process, which generates the entire neural network parameters.\n3.2. Parameter Tokenization\nInspired by the success of language and vision models [14, 68], we propose parameter tokenization that divides network parameters into non-overlapping tokens. Considering the distribution shifts across different layers, we first categorize the trained parameters according to their respective layer indices. Then, we apply normalization (subtracting the mean and dividing by the standard deviation) on each layer. These operations can be formulated as follows,\n$\\begin{array}{c}  \\begin{array}{c}  \\text { W } \\\\  \\text { divide by layer } \\  \\text { normalize }  \\end{array} \\rightarrow\\left[w\\^{[1]}, \\ldots, w\\^{[i]}, \\ldots, w\\^{[I]}\\right] \\\\  \\text { \\& } \\rightarrow\\left[\\widehat{w}\\^{[1]}, \\ldots, \\widehat{w}\\^{[i]}, \\ldots, \\widehat{w}\\^{[I]}\\right], \\\\  -\\mu \\text { and } / \\sigma  \\end{array}$\nwhere $W$ denotes the trained parameters. $\\mu$ and $\\sigma$ denote the mean and standard deviation values of parameters. $W_i$ and $\\widehat{w}\\_i$ are the original and normalized parameters of the i-th layer, respectively.\nThe number of parameters varies across these layers, which is not conducive to efficient batch training. To this end, we slice each layer parameter into a set of tokens with the same token size, which can be written as follows,\n$\\widehat{w}\\_i \\stackrel{\\text { tokenize }}{\\rightarrow} K\\^{[i]}=\\left[k\\^{i}\\_1, \\ldots, k\\^{i}\\_j, \\ldots, \\text { padding }\\left(k\\^{i}\\_J\\right)\\right],$\nwhere the $k\\^{i}\\_j$ represents the j-th token of i-th layer. For the last token of each layer, we apply padding operation to ensure that all layers have tokens of uniform length. It is worth noting that the padded regions are excluded from the loss calculation.\n3.3. Recurrent Diffusion\nPermutation state. Neural network symmetries [1, 34] do not affect the model outcomes but increase the difficulty of learning the parameter distribution. To address this, we introduce a unique state for each trained model W via one-hot embedding. This operation provides a guide for the generated parameters to mitigate the influence of parameter symmetries. For simplicity, we use S to represent the permutation state of W.\nPosition embedding. Inspired by ViT [14], we also encode the layer and token information described in the parameter tokenization (Sec. 3.2) using a two-dimensional sinusoidal position encoding. Specifically, the first dimension encodes the layer index of the token in the original model, while the second dimension encodes the position of the token within its layer. For i-th layer parameter tokens $K\\^{[i]}$, the position embedding can be formulated as follows,\n$K\\^{[i]} \\stackrel{\\text { position embedding }}{\\rightarrow} e\\^{[i]}=\\left[e\\^{i}\\_1, e\\^{i}\\_2, \\ldots, e\\^{i}\\_J\\right],$\nwhere $e\\^{i}\\_j$ denotes the position embedding of the parameters belong to j-th token of i-th layer.\nRecurrent model. After obtaining the parameter tokens, permutation states, and position embeddings, we use a recurrent model to learn the representation of the parameter tokens. For clarity, we will refer to the output of the recurrent model as the 'prototype' in the following. This operation can be written as follows:\n$P\\^{i}\\_j=f\\left(H\\^{i}\\_j\\^-{1}, e\\^{i}\\_j, S\\right), i \\in[1, I], j \\in[1, J],$\nwhere $P\\^{i}\\_j$ and $H\\^{i}\\_j$ denote the prototype and hidden state of the parameters belonging to j-th token of i-th layer, respectively. $f(\\cdot,\\cdot,\\cdot)$ denotes the state transition function. The structure of the recurrent model is simple. Considering efficiency, we use Mamba [20] followed by an MLP to project the feature dimension to the required size for the diffusion model. We also conduct ablation studies with other recurrent model architectures, such as LSTM [26] and transformer with its decoder in a causal manner [68].\nParameter diffusion. Inspired by p-diff [69] and MAR [35], we use 1D convolution to build the diffusion model. In this part, the parameter prototypes, serving as conditions, are fed into the diffusion process along with random noise. We optimize our approach through the following equation:\n$L\\_{\\text {diff }}=\\mathbb{E}\\_{t, K, \\epsilon}\\left[\\left\\|\\epsilon-\\epsilon\\_{\\theta}\\left(K\\_t, t, P\\right)\\right\\|\\_2^2\\right],$"}, {"title": "4. Experiments", "content": "4.1. Setup\nDatasets and architectures. We mainly evaluate our method across a wide range of tasks, including ImageNet-1K [12] for the classification, ADE20K [77] for the semantic segmentation, COCO [39] for the object detection, and BoolQ [9], PIQA [3], SIQA [56], HellaSwag [75], and ARC [10] for the commonsense reasoning tasks. To verify the scalability of our approach, we conduct experiments on various architectures with parameter counts ranging from several to hundred million. Details of parameter counts can be found in Tab. 1, 2, 3.\nTrained parameters collection. We take parameters collection on the ImageNet-1K as an example. To save the cost, we finetune the full parameters of the models released in timm\u00b9 and save 50 checkpoints as the training data. For each checkpoint, we assign a unique permutation state to guide the generated parameters.\nTraining details. We default to using Mamba [20] as the architecture of the recurrent model. The length of parameter tokens, permutation states, position embeddings, and prototypes is set to 8192. It is worth noting that the permutation states and position embeddings are fixed during the training by default. We also study the influence of the token length, varying it from 1024 to 16384. The parameter diffusion consists of 1D convolutional layers. More details about the model architectures, hyperparameters, and training process can be found in Appendix B.1.\nInference details. We input permutation states and position embeddings into the recurrent model to generate the prototypes. Then, the diffusion model utilizes the prototypes as"}, {"title": "4.2. Results of Large-Scale Parameter Generation", "content": "In this section, we present the results of our approach across a range of tasks including classification, semantic segmentation, object detection&instance segmentation, and language tasks. As most previous works encounter the out-of-memory issue at million-scale parameter generation, we mainly compare with the results from the trained networks, which we denote as 'original'.\nResults on ImageNet-1K. Tab. 1 presents performance comparisons across seven architectures on ImageNet-1K [12]. These architectures encompass the ResNet [22], ViT [14], and ConvNeXt [41] series, with parameter counts ranging from 3 to 197 million. Based on the results in Tab. 1, several crucial observations can be made as follows: i) Our approach successfully generates model parameters at hundred-million scales, overcoming the out-of-memory issues faced by previous works [29, 51, 66, 69]. ii) The performances of the generated models are comparable with the original ones.\nResults on ADE20K and COCO. We also investigate the generalization of our approach to semantic segmentation as well as object detection and instance segmentation tasks. We choose ADE20K [77] and COCO [39] as our benchmark datasets. For semantic segmentation, following Zhao et al. [76], we adopt UperNet [72] as the segmentation model and train it on ADE20K to prepare checkpoints. For object detection and instance segmentation, we finetune ViTDet [36] on COCO to collect checkpoints and report the results of mAP Bbox and mAP Seg, respectively. All experiments here are conducted based on ViT-B [14]. Tab. 2 presents the strong generalization of our approach to these two tasks. Specifically, compared to the original models, we achieve comparable or even slightly better results over all the above metrics.\nResults on commonsense reasoning. We conduct experiments on language tasks to evaluate the generalization of our approach. We employ DoRA [40], an upgraded version of LoRA [28], to fine-tune LLaMA-7B [67] for commonsense reasoning tasks and save the checkpoints as the training data. We report the results across 7 sub-tasks with rank"}, {"title": "4.3. Ablation Studies and Analysis", "content": "In this section, we examine the influences of key factors on our method. We present the results of the generated ViT-Tiny [14] on the ImageNet-1K [12], unless stated otherwise.\nThe effect of recurrent model. We employ the recurrent model to learn the relationship among parameter tokens. To keep other factors consistent, we simply remove the state transition function from the recurrent model for comparison, denoted as \u2018- recurrent model'. The experimental results from Tab. 4a confirm that the recurrent model plays a crucial role in parameter generation. Without the state transition function, our approach learns each parameter token individually, overlooking the relationships among these tokens. As a result, the generated parameters perform extremely poorly.\nThe manner of position embeddings. In ViT [14], the position embeddings are learnable by default. Here, we mainly conduct the experiments with three different position embedding manners and show the details as follows:\n\u2022 learnable: Initializing with 2D sinusoidal positional encoding and set to be learnable.\n\u2022 encoded by index: Using 1D sinusoidal positional encoding, irrespective of the original network structure, with indices assigned from front to back.\n\u2022 encoded by layer (default): Using 2D sinusoidal positional encoding to represent layer and token indices.\nAs shown in Tab. 4b, the learnable embeddings perform slightly better than the other two manners. However, we still recommend using fixed position embeddings, as they offer comparable performance while significantly reducing storage requirements compared to the learnable position embedding scheme.\nThe manner of tokenization. Considering the differences among various layers, we divide the parameters into tokens within each layer. P-diff [69] directly flattens the parameters into 1-dimensional vectors, while SANE [61] divides the parameters by channel within each layer. We conduct experiments to analyze the 3 strategies mentioned above and compare their results in Tab. 4c. Our default strategy achieves better results than the others. Directly flattening results in a single token containing parameters from different layers, which poses challenges for optimization. Tokenizing by the channel may result in excessive padding values for each token, as the number of channels is usually much smaller than the default token size.\nThe structure of recurrent model. We mainly explore three structures of the recurrent model, including LSTM [26], Transformer [68], and Mamba [20]. In Tab. 5, we report the performances, training time, and memory cost of generating ViT-Tiny parameters on ImageNet-1K. All results are obtained on a NVIDIA H100 80G GPU. All three structures can achieve good results. However, considering the training time and memory cost, our default Mamba is the best structure of the recurrent model.\nToken size. In Tab. 6, we show the results of generated ViT series with tokens of different sizes ranging from 1024 to 16384. The performance of generated models become better as the token size increases. When token is of small size, it will contain limited information that is hard to learn. De-"}, {"title": "The effect of permutation state", "content": "RPG incorporates a permutation state operation to address parameter symmetries, which become particularly pronounced when collecting checkpoints from multiple training runs. To evaluate this, we collect checkpoints from different numbers of training runs (1, 3, and 10) and compare the performance with and without permutation state. These results demonstrate that permutation state operation effectively addresses parameter symmetries and enables stable results even when incorporating checkpoints from multiple training runs.\nEfficiency of generating large-scale parameters. Rapid synthesis of large-scale parameters is crucial for evaluating the practicality of our approach. As illustrated in Tab. 8, we present the time cost for generating models of ViT-Base and ConvNeXt-L across various DDIM [64] sampling steps. All results are obtained with a single NVIDIA H100 80G GPU. Our approach shows the capability to generate models within minutes. Notably, even for ConvNeXt-L (197.7 M parameters), we can synthesize the entire parameter within 1.3 minutes. Even with only 20 sampling steps, we can achieve promising results. Meanwhile, the inference memory requirement is approximately 20GB, so RPG can be deployed on NVIDIA RTX 3090 or similar-level GPUs.\nSimilarity analysis of generated models. In this section, we demonstrate that our method offers significant advantages over simply adding noise to the original models. Following p-diff [69], we choose Intersection of Union (IoU) as the metric for measuring similarity. It compares the agreement of output results from classification models across a large number of samples to evaluate the similarity. We calculate IoU of models from target groups with all the original models and select the maximum IoU value (nearest neighbor) as the measure of similarity.\nIn Fig. 3, we compare original models adding various levels of noise with models generated by our method in terms of accuracy and similarity. As the noise level increases, the similarity and accuracy of the models both decreases. The points representing our generated models are distributed in the upper left region relative to the area with added noise, indicating that our models can enhance diversity while maintaining accuracy."}, {"title": "4.4. Comparisons with Previous Methods", "content": "We compare our approach with four previous works, i.e., SKDE30 [58], p-diff [69], D2NWG [66], and SANE [61]. As shown in Tab. 9, our approach consistently achieves the best results on various architectures, while previous works are hard to achieve comparable performances as original models. Another key issue is that the previous works usually fail to generate large-scale neural network parameters. We also provide comparisons between RPG and previous works in Tab. 10. RPG demonstrates superior capabilities in all three aspects: scalability, performance, and architecture support."}, {"title": "5. RPG's Potential in Unseen Tasks", "content": "Until now, experimental results have demonstrated that our approach can efficiently generate large-scale neural network parameters if these models are included in the training set. In this section, we mainly investigate whether our approach"}, {"title": "5.1. Experiment Designs.", "content": "Build seen and unseen tasks. To assess RPG's capability in generating models for unseen tasks, we construct various"}, {"title": "5.2. Results for Unseen Tasks", "content": "Performance comparisons. We compare the results of our approach and original models on unseen binary embeddings in Tab. 11. Considering the space limitation, we randomly select 10 unseen binary embeddings for comparison. Notably, RPG yields commendable performance in these unseen tasks, even without being trained on the specific unseen embeddings. That demonstrates the strong practicality and potential of our approach in generating models under unseen tasks. The results of the remaining unseen binary embeddings and more analysis are shown in Appendix B.4.\nPerception of embedding changes. In addition to comparing results, we further investigate our approach's ability to perceive embedding changes. We select two tasks with opposite binary embeddings in each element and report the results in Tab. 12. Our approach demonstrates a remarkable capacity to accurately detect changes in the tasks and generate corresponding model parameters. It is worth noting that the accuracy would hover around 50% if our approach were not aware of the embedding changes.\nVisualizations of original and generated model parameters. We visualize the original and generated models for both seen and unseen tasks in Fig. 5. For seen tasks, our approach generates diverse models compared to the original ones. Surprisingly, as shown in Fig. 5b, we find that our approach can learn unseen parameter patterns. This demonstrates the potential generalization ability of our method.\nEfficiency comparison. To evaluate efficiency on unseen tasks, we compare 3 approaches: i) training ViT-Tiny from scratch, ii) finetuning an ImageNet-pretrained ViT-Tiny, and iii) finetuning a RPG-initialized model. As shown in"}, {"title": "6. Discussion and Conclusion", "content": "Our approach demonstrates promising results in large-scale parameter generation across various vision and language tasks. However, we acknowledge that achieving true 'AI creating Al' remains a distant goal. Firstly, while our method shows potential in generating models for unseen tasks, it currently faces limitations in generating parameters for novel model architectures. Secondly, our approach is constrained by modeling parameter relationships within a single task, potentially limiting its practical applicability. More importantly, future work should focus on simultaneously modeling parameter relationships across diverse architectures and tasks. Such an approach could yield a more powerful and versatile parameter generator, potentially advancing us closer to the 'AI creating Al' era. We hope our approach will inspire future research in this field."}, {"title": "A. Discussion with More Related Works", "content": "A.1. Discussion with HyperRepresentations.\nWe mainly compare with three HyperRepresentation methods [58, 59, 61]. These methods use an autoencoder to learn the latent features of trained models, so they call the latent feature HyperRepresentation. This HyperRepresentation is then used for analyzing the model's performance or characteristics, or for sampling to generate new models or pre-trained parameters.\n\u2022 [58] utilizes kernel density estimation (KDE) to sample model parameters on the learned HyperRepresentation space. They also emphasize the importance of layer-wise loss normalization in the learning process of HyperRepresentation. This work achieves parameter generation in small CNNs from Model Zoos [60] with 2864 parameters.\n\u2022 [59] focuses on using HyperRepresentation to sample the pre-trained model parameters. They also evaluate the ability of transfer learning by using a trained parameter autoencoder to initialize on unseen dataset. This work can be regarded as a cheap parameter initialization method.\n\u2022 [61] utilizes a sequential autoencoder for neural embeddings (SANE) to divide the neural network weights into subsets. Then, an autoencoder processes these subsets in a sliding window. This work can generate the entire parameter of ResNet-18. However, the performance of generated ResNet-18 is poor and exists a large gap with the original trained ResNet-18. For example, the performance of generated ResNet-18 is 68.6%, while the original model can achieve an accuracy of over 90% on CIFAR-10 (see in Tab. 9).\nWe summarize the main differences as follows:\n\u2022 Hyper-representations as generative models is hard to achieve comparable results as their original models that are used for training, but our approach obtains comparable results.\n\u2022 Hyper-representation for pre-training and transfer learn-"}, {"title": "A.2. Details and limitations of G.pt.", "content": "A primary limitation of G.pt [51] is the training data collection cost. By default, they collect 23 million checkpoints to train the parameter generator. Besides, they only evaluate the effectiveness of G.pt on small architectures, such as a low-dimensional MLP layer or a Convolutional layer with limited channels. The maximum number of generated parameters does not exceed 10,000."}, {"title": "A.3. Details and limitations of p-diff.", "content": "P-diff [69] directly flattens all parameters into a single-dimensional vector, disregarding the inter-layer parameter relationships. Furthermore, p-diff faces challenges in scaling up to large-scale parameter generation."}, {"title": "B. Experimental Settings and More Results", "content": "B.1. Training recipe\nIn this section, we provide detailed training recipes and supplementary information. The number of parameters generated by our approach ranges from approximately 3K to 200M. The significant disparity necessitates different training settings. Generally, as the number of parameters increases, the learning process becomes more challenging, requiring higher training costs, particularly for generating parameters beyond 50 million. Therefore, our training settings are divided into two categories: the default setting and the setting for parameters exceeding 50 million, as is shown in Tab. 14."}, {"title": "B.2. Datasets", "content": "In this section, we introduce the datasets used in the paper, including those for classification, semantic segmentation, object detection&instance segmentation, and commonsense reasoning.\nClassification\n\u2022 ImageNet-1k [12] is a large-scale visual database for visual object recognition research. It contains over 1 million images across 1000 categories and is widely used for training and benchmarking deep learning models.\n\u2022 CIFAR-10 [32] dataset consists of 60,000 32\u00d732 colorful images in 10 different classes. It is commonly used for training machine learning and computer vision algorithms, providing a standard benchmark for image classification task.\nSemantic segmentation\n\u2022 ADE20K [77] is a dataset for semantic segmentation and scene parsing, containing over 20,000 images annotated with pixel-level labels for 150 object categories. It is used to train models to understand and segment various objects and scenes in an image, making it valuable for applications in autonomous driving, robotics, and image editing.\nInstance segmentation & Object detection\n\u2022 COCO [39] dataset is a large-scale object detection, segmentation, and captioning dataset. It contains over 330,000 images, with more than 200,000 labeled instances across 80 object categories. COCO is widely"}, {"title": "B.3. The detailed structure of recurrent diffusion", "content": "In this section, we provide specific details about the proposed recurrent model and diffusion model in RPG. More detailed configurations can be found in Tab. 15.\nDetails of recurrent model. By default, the recurrent model consists of two Mamba layers [19]. As the increasing of parameters to generate, we need a larger recurrent model to capture the information in these parameters. The size of the recurrent model is mainly determined by the token size, which varies according to the number of parameters to be generated. Based on the token size, we categorize our model into four versions: Tiny, Small, Base, and Large.\nDetails of diffusion model. Following p-diff [69], our diffusion model adopts a one-dimensional convolutional architecture. The parameters of the diffusion model are significantly fewer than those of the recurrent model. We feed the prototypes from the recurrent model as conditions into the diffusion model by directly adding them to the feature map."}, {"title": "B.4. More results of Section 5", "content": "Results of generating models for unseen tasks. In Section 5, we show the potential of our approach in generating models for unseen tasks. In this part, we provide more results. First, we compare the performance of original and generated models using all unseen embeddings in Tab. 16. Results demonstrate that our approach consistently achieves good results in unseen tasks.\nPCA visualization of classification head parameters. We also provide a visualization of the parameters of the classification head (a two-layer fully connected structure with total 38,976 parameters) for 1022 tasks as described in Section 5 using Principal Component Analysis (PCA), which presents the structure of the parameter space in Fig. 6a. Our generated model achieves an average accuracy of 91.2% across all binary classification tasks, which indicates that our method has effectively learned this structure. Furthermore, we evaluate the parameters corresponding to unseen tasks and compared their positions in Fig. 6b between the original and generated parameters. It is noteworthy that, even though the original parameters of these tasks are not included in the training data, the generated parameters con-"}, {"title": "B.5. Training memory cost analysis", "content": "In this section, we analyze the GPU memory utilization during training. GPU memory consumption is usually highly correlated with two factors: i) the size of the generative model and ii) the size of generated parameters. We analyzed the impact of these two factors on the GPU memory utilization during the training of our approach.\nGPU memory v.s. token size We visualize the GPU memory usage with different token sizes in Fig. 8a. As the token size increases, the scale of the recurrent model significantly grows, leading to a notable increase in GPU memory consumption. This implies that, when the performance of the generated models is comparable, we prefer to use models with smaller token sizes.\nGPU memory v.s. parameter counts We conduct experiments to show the relationship between GPU memory and generated parameter counts in Fig. 8b. In previous methods, the relationship between GPU memory consumption and the number of parameters in the generated model was quadratic [58] or directly proportional [69]. This limits their practicality and application range. In contrast, our approach demonstrates remarkable efficiency: with equivalent GPU memory usage, it can generate models with 34 to 960 times more parameters compared to previous methods."}, {"title": "B.6. Inference memory cost and sampling time", "content": "In this section, we present more information about the sampling, including memory usage, inference time, and the balance between sequential and parallel inference. In Tab. 8, we show the sampling time and memory usage for ViT-Base and ConvNeXt-L. Here, we present the sampling time and memory usage for other models. In Tab. 18, we adopt DDPM as the solver and conduct 1000-step sampling. Since the diffusion model in RPG is shared among all the parameter tokens, we can adopt different inference modes to find a balance between memory usage and inference speed:\n\u2022 fully parallel: All tokens are fed into the diffusion model simultaneously. This approach results in a high memory usage but achieves a high generation speed.\n\u2022 sequential: Tokens are fed into the diffusion model one by one. This approach significantly reduces memory usage, as the model only occupies memory for inferring a single token at a time. This enable us to generate parameters of models listed on a GPU with less than 8GB of memory.\n\u2022 partially parallel (default): In partial parallel mode, we set 256 tokens as a batch for the diffusion model inference. This approach significantly boosts speed with a slight increase in GPU memory usage, reaching an optimal trade-off between memory and speed. We adopt this as the default setting."}, {"title": "B.7. Parameter sensitivity v.s. performance", "content": "According to conventional understanding, larger parameter quantities are generally more challenging to learn. However, our experiments reveal that this rule is not absolute and demonstrates instability in learning some small model parameters.\nThis motivates us to investigate the relationship between parameter sensitivity and generation quality. Specifically, we add Gaussian noise with weights of 0.01, 0.10, and 1.00 to the original parameters to measure model sensitivity, as shown in Tab. 19. We observe that as noise weight increases, performance decreases for all models, with smaller models being more affected than larger ones. This indicates that smaller models are relatively more sensitive. Additionally, we notice that the performance gap between the original and generated models widens as sensitivity of the model increases. This demonstrates a strong correlation between a model's sensitivity and the difficulty of generating its parameters."}, {"title": "B.8. Why not auto-regression?", "content": "It is worth noting that our approach does not employ an auto-regressive method, i.e., we do not feed the output back as input again. Our method takes position embedding and permutation state as inputs and synthesizes neural network parameters as outputs, forming a standard recurrent neural network. We have attempted to train our model using an auto-regressive approach such as a decoder-only transformer architecture, whose results are shown in Tab 17. Due to the accumulation of errors during the auto-regressive process in inference, the parameters generated at the end of sequence become nearly indistinguishable from noise, leading to poor performance. In contrast, in RPG, noise is only introduced in the diffusion model and does not accumulate in the recurrent process, ensuring stable parameter generation."}]}