{"title": "FLEXMOTION: LIGHTWEIGHT, PHYSICS-AWARE, AND CONTROLLABLE HUMAN MOTION GENERATION", "authors": ["Arvin Tashakori", "Arash Tashakori", "Gongbo Yang", "Z. Jane Wang", "Peyman Servati"], "abstract": "Lightweight, controllable, and physically plausible human motion synthesis is crucial for animation, virtual reality, robotics, and human-computer interaction applications. Existing methods often compromise between computational efficiency, physical realism, or spatial controllability. We propose FlexMotion, a novel framework that leverages a computationally lightweight diffusion model operating in the latent space, eliminating the need for physics simulators and enabling fast and efficient training. FlexMotion employs a multimodal pre-trained Transformer encoder-decoder, integrating joint locations, contact forces, joint actuations and muscle activations to ensure the physical plausibility of the generated motions. FlexMotion also introduces a plug-and-play module, which adds spatial controllability over a range of motion parameters (e.g., joint locations, joint actuations, contact forces, and muscle activations). Our framework achieves realistic motion generation with improved efficiency and control, setting a new benchmark for human motion synthesis. We evaluate FlexMotion on extended datasets and demonstrate its superior performance in terms of realism, physical plausibility, and controllability.", "sections": [{"title": "INTRODUCTION", "content": "Generating controllable and realistic human motion is a critical task with applications in various domains, including animation Zhu et al. (2023), sports and rehabilitation Yang et al. (2023); Zhang et al. (2024a); Cheng et al. (2023); Tashakori et al. (2022), virtual reality Zhu et al. (2023), robotics Tashakori et al. (2024), and human-computer interaction Arkushin et al. (2023); Zhang et al. (2022); Servati et al. (2024). Human motion involves complex interactions between joint movements, contact forces, and muscle activations, necessitating a comprehensive approach that can capture both kinematic and dynamic aspects. Despite the remarkable progress in human motion generation, challenges remain in developing models that effectively balance physical realism, computational efficiency, and fine-grained controllability.\nTraditional methods often fail to control the intricate biomechanics of human movement, which involve complex interactions between kinematics, dynamics, and environmental context Tripathi et al. (2023b); Zhang et al. (2024b); Xie et al. (2021a); Chiquier & Vondrick (2023). This deficiency is particularly notable in applications such as sports and rehabilitation, where the precision of muscle activations and contact forces is crucial for accurate simulation Chiquier & Vondrick (2023). Furthermore, current methods focused on physical plausibility often demand high computational resources, such as physics engines, rendering them impractical for real-time applications Yuan et al. (2023); Xie et al. (2021a); Tripathi et al. (2023a).\nWe propose FlexMotion, a novel, lightweight, and physics-aware framework that generates multimodal human motion sequences conditioned on text and diverse kinematics and dynamics information. FlexMotion leverages a multimodal, physically plausible pre-trained Transformer encoder-decoder, learning the relationship between joint trajectories, contact forces, joint actuations, and muscle activations to ensure that the generated motions are aligned with human biomechanics. FlexMotion operates in the latent space, significantly reducing the computational cost for training and inference compared to traditional human motion generation methods. Our model also introduces a"}, {"title": "RELATED WORK", "content": ""}, {"title": "HUMAN MOTION GENERATION", "content": "Motion generation literature has focused on two main approaches: first, using autoregressive models, which use past generated frames to generate the subsequent frames recursively Zhu et al. (2023). Second, sequence-based models which generate the entire sequence at once Zhu et al. (2023); Feng et al. (2024); Lou et al. (2023); Zhong et al. (2023); Xu et al. (2023); Ma et al. (2024); Dabral et al. (2023). In the second approach, researchers have employed a variety of generative models to achieve this, including Generative Adversarial Networks (GANs) Zhu et al. (2023), Variational Autoencoders (VAEs) Zhu et al. (2023); Jiang et al. (2024), Normalizing Flows (NFs) Zhu et al. (2023), and Diffusion Models (DMs) Zhu et al. (2023); Tevet et al. (2023), as well as a task-specific model known as Motion Graphs (MGs) Zhu et al. (2023). Among these approaches, Diffusion Models, particularly Denoising Diffusion Probabilistic Models (DDPMs), have demonstrated promising and diverse results Tevet et al. (2023); Zhu et al. (2023).\nDDPMs have been particularly successful due to their ability to model complex data distributions and generate high-quality samples without mode collapse, a common issue in other generative models like GANs Stypu\u0142kowski et al. (2024). These models have found applications beyond motion sequence generation, including image synthesis Ruiz et al. (2023); Li et al. (2024); Wang et al. (2024); He et al. (2022); Zhang et al. (2023c;a); Van Le et al. (2023), video generation Zhou et al. (2024); Zhang et al. (2023a); Wu et al. (2023), and other domains where generating realistic sequences is essential.\nMotion synthesis research aims to produce realistic and natural human motion patterns under various conditions, leveraging the flexibility and robustness of models like DDPMs. These conditions can include text Tevet et al. (2023); Zhang et al. (2024c; 2023b); Zhao et al. (2023); Chen et al. (2023); Qian et al. (2023), audio Zhu et al. (2023), action Tevet et al. (2023), music Zhu et al. (2023), images Zhu et al. (2023), 3D scene Zhu et al. (2023), spatial contexts Xie et al. (2023); Karunratanakul et al. (2023), objects Zhu et al. (2023), or a combination of such conditions Ling et al. (2023); Jin et al. (2024). These generated sequences can consist of either key point locations, joint rotation Zhu et al. (2023); Delmas et al. (2023), or mesh parameters extracted from models such as SMPL Loper et al. (2015).\nDespite the advancements in generating realistic human motion sequences, current models often perform inadequately when tasked with synthesizing complex dynamic movements that adhere to biomechanical and physical laws. These models produce noticeable physical artifacts, such as unnatural joint rotations, unrealistic muscle dynamics, and incorrect contact points during environmental interactions. This deficiency arises especially while generating long motion sequences, primarily because existing approaches lack an explicit understanding of the relative mapping between muscle activations, joint torques, and contact forces, which are crucial for generating physically plausible motions Zhang et al. (2024c). To address these limitations, we employ a pretrained autoencoder that encodes motion properties in the latent space while preserving essential biomechanical information, with a decoder that integrates physics-based constraints to ensure physically accurate motion reconstruction, as detailed in the method section."}, {"title": "PHYSICS-AWARE HUMAN MOTION MODELING", "content": "To generate physically plausible motion sequences, researchers have employed two primary approaches: first, interaction with physics simulators Xie et al. (2021b); Lee et al. (2019); Yuan et al. (2023), and second, the integration of physics-based constraints into the reconstruction loss function Tevet et al. (2023). While physics simulators provide detailed physical interactions, they are computationally expensive and non-differentiable Zhang et al. (2024b); Tripathi et al. (2023b), significantly limiting their utility. The non-differentiability obstructs gradient backpropagation, hindering the effective optimization and refinement of generated motions Zhang et al. (2024b); Tripathi et al. (2023b). Alternatively, integrating physics-based constraints directly into generative models offers a more computationally efficient approach to maintaining physical realism without needing full simulation. In the second category, MDM integrates pose consistency, foot placement, and velocity loss Tevet et al. (2023), while IPMAN-R introduces stability and ground interaction loss to enhance motion realism in dynamic environments Tripathi et al. (2023b). PhysPT integrates contact points, force, and Euler-Lagrange consistency loss to accurately simulate physical interactions Zhang et al. (2024b). Additionally, authors in Xie et al. (2021a) incorporate dynamic constraints, contact points, penetration avoidance, and smooth transition terms to produce realistic motion estimation. However, despite these advancements, both approaches struggle to capture complex dynamic motions thoroughly and often fail to adhere to biomechanical laws, leading to noticeable physical artifacts, especially in long sequence generations Zhang et al. (2024c). Our work addresses these challenges by utilizing a pretrained autoencoder architecture to generate physically plausible outputs, detailed in the method section."}, {"title": "CONTROLLABLE MOTION GENERATION", "content": "Controlling motion generation algorithms is essential for creating realistic and contextually appropriate human motions, especially when these motions must adhere to specific spatial constraints or trajectories. Recent advancements have been inspired by techniques from image generation, such as ControlGAN Li et al. (2019), ControlNet Zhang et al. (2023c), ControlNet+ Li et al. (2024), and the T2I adapter Mou et al. (2024); Cao et al. (2024), which offer nuanced control over generated outputs. OmniControl Xie et al. (2023) introduces flexible spatial control signals into a diffusion-based motion model. It allows precise control over various joints over time and improves motion realism Xie et al. (2023). Guided Motion Diffusion (GMD) Karunratanakul et al. (2023) enhances spatial accuracy by integrating constraints such as predefined trajectories and obstacles, using feature projection and dense guidance to ensure coherence between spatial information and generated motions Karunratanakul et al. (2023).\nHowever, both OmniControl and GMD only control joint trajectories and do not account for crucial kinematic parameters such as joint actuation, ground contact forces, or muscle activation, which limits their ability to generate refined motion sequences that adhere to the physical constraints required for realistic and application-specific human movement synthesis. We propose a spatial controllability module to incorporate these critical kinematic and biomechanical factors, enabling more precise motion generation, as detailed in the method section."}, {"title": "PROPOSED METHOD", "content": "The objective of FlexMotion is to generate realistic motion sequences $\\{x_t\\}_{t=1}^T$ conditioned on a text prompt $c$ and a wide range of spatial conditions $\\{c_t\\}_{t=1}^T$, where $T$ is desired sequence length. The overall architecture of FlexMotion, illustrated in Fig.2, consists of three main components. First, the Physics-aware Multimodal Autoencoder (Sec.3.1) learns to map detailed kinematic and dynamic properties of motion to a latent space that captures the essential features of human motion while enforcing physical plausibility through physics-based constraints during motion sequence reconstruction in the decoder. Second, the FlexMotion Diffusion Model (Sec.3.2) generates desired latent embedding sequences conditioned on the text prompt. Third, the Spatial Controllability Module (Sec.3.3) provides fine-grained spatial control over critical motion parameters, enabling precise and contextually appropriate motion generation. In the following sections, we discuss each component in detail. Further details and pseudocode for both the training and inference processes are provided in the appendix."}, {"title": "PHYSICS-AWARE MULTIMODAL AUTOENCODER", "content": "To model complex human motions while enforcing physical plausibility, we use a transformer-based autoencoder architecture capable of handling multiple motion modalities, similar to the architecture introduced in Zhang et al. (2024b). Fig. 3 provides an overview of the proposed multimodal autoencoder. At each time step $t$, the motion data $x_t$ consists of various components, including joint positions $p_t \\in \\mathbb{R}^{J\\times 3}$, joint rotations $r_t \\in \\mathbb{R}^{J\\times 3}$, joint velocities $\\dot{r}_t \\in \\mathbb{R}^{I\\times 3}$, joint accelerations $\\ddot{r}_t \\in \\mathbb{R}^{J\\times 3}$, muscle activations $a_t \\in \\mathbb{R}^{M}$, joint torques $\\tau_t \\in \\mathbb{R}^{J\\times 3}$, and contact forces $\\lambda_t \\in \\mathbb{R}^{J\\times 3}$, where $J$ and $M$ denotes total number of joints and muscles respectively. These modalities are concatenated into a single feature vector, as described in Eqn. 1, where $D$ represents the dimensionality of the input feature vector, calculated based on the dimensions of each modality.\n$x_t = [p_t, r_t, \\dot{r}_t, \\ddot{r}_t, a_t, \\tau_t, \\lambda_t] \\in \\mathbb{R}^D$ (1)\nThe encoder $\\mathcal{E}(.)$ processes the input sequence $x_t \\in \\mathbb{R}^D$ and maps it to a sequence of latent representations $x \\in \\mathbb{R}^d$ (Eqn. 2), where $\\theta_{\\mathcal{E}}$ is the encoder parameters, and $d$ is latent space dimension where $d << D$.\n$x = \\mathcal{E}(x_t; \\theta_{\\mathcal{E}})$ (2)\nThe decoder $\\mathcal{D}(.)$ reconstructs the motion sequence from the latent representations $x \\in \\mathbb{R}^d$ (Eqn. 3), with $\\theta_{\\mathcal{D}}$ being the decoder parameters. Here, $\\hat{x}_t \\in \\mathbb{R}^D$ denote the reconstructed output."}, {"title": "LATENT SPACE MOTION DIFFUSION MODEL", "content": "To generate diverse and realistic motion sequences, we employ a diffusion model operating in the latent space $X^0 = \\{x\\}_{t=1}^T$ obtained from the trained Transformer encoder $\\mathcal{E}(.)$. Following Tevet et al. (2023), we define a forward and reverse diffusion process. An overview of the model can be found in Fig. 2.\nThe forward process gradually adds Gaussian noise to the latent variables, where $\\beta_n$ is a variance schedule. The noise is sampled from a Gaussian distribution with mean $\\sqrt{1 - \\beta_n}X_{n-1}$ and variance $\\beta_nI$ (Eqn. 9).\nq(X_n | X_{n-1}) = \\mathcal{N}(X_n | \\sqrt{1 - \\beta_n}X_{n-1}, \\beta_n I)$ (9)\nThe reverse process learns to remove noise step by step, where $\\mu_\\theta$ is a neural network parameterized by $\\theta$ that predicts the noise at each iteration $n$ (Eqn. 10).\np_\\theta(X_{n-1} | X_n) = \\mathcal{N}(X_{n-1} | \\mu_\\theta(X_n, n, c), \\sigma_n I)$ (10)\nTo train the diffusion model, we freeze the pretrained Transformer encoder-decoder and optimize $\\mathcal{L}_{diff}$, where $\\epsilon_{\\theta}(X, n, c)$ Murphy (2023) (Eqn. 11), and $N$ is number of diffusion steps.\n$\\mathcal{L}_{diff} = \\mathbb{E}_{X^0 \\sim q(\\mathcal{E}(X^0)|c), n \\sim Unif(0, N-1), \\epsilon \\sim \\mathcal{N}(0,1)} [||\\epsilon - \\epsilon_{\\theta}(X, n, c)||^2]$ (11)"}, {"title": "SPATIAL CONTROLLABILITY MODULE", "content": "We integrate a spatial controllability module inspired by Zhang et al. (2023c) to enable fine-grained control over the generated motion. At each time step $t$, the control inputs $c_t \\in \\mathbb{R}^D$ can include desired joint positions, velocities, muscle activations, or other motion parameters that resemble the modalities in $x_t$. We use the frozen encoder $\\mathcal{E}(.)$, trained in section 3.1, to map the sequence of control signals $\\{c_t\\}_{t=1}^T$ into a latent space $C_{\\epsilon} \\in \\mathbb{R}^d$. We freeze the trained diffusion model described in section 3.2 and introduce a trainable copy between two zero-initialized convolution layers, $Z$, as shown in Eqn. 12.\n$\\mathcal{E}_{total}(X_n, C_{\\epsilon}, n, c) = \\epsilon_{\\theta}(X_n, n, c) + Z(\\epsilon_{\\theta_c}(X + Z(C_{\\epsilon}, \\theta_{z1}), n, c), \\theta_{z2})$ (12)\nThis design ensures that initially, the control module's output does not interfere with the pretrained diffusion model, allowing the network to learn how to gradually incorporate control conditions during training. The overall learning objective can be described as Eqn. 13.\n$\\mathcal{L}_{total} = \\mathbb{E}_{X^0 \\sim q(\\mathcal{E}(X^0)|c), C_{\\epsilon}, n \\sim Unif(0, N-1), \\epsilon \\sim \\mathcal{N}(0,1)} [||\\epsilon - \\mathcal{E}_{total}(X, C_{\\epsilon}, n, c)||^2]$ (13)"}, {"title": "EXPERIMENTAL RESULTS", "content": "Datasets. We evaluate FlexMotion on three popular datasets: HumanML3D Guo et al. (2022a), KIT-ML Plappert et al. (2016), and Flag3D Tang et al. (2023). HumanML3D, derived from AMASS and HumanAct12, contains 14,616 motion sequences with 44,970 textual annotations. KIT-ML provides 3,911 motion sequences with 6,278 descriptions, while Flag3D offers 180,000 videos spanning 60 fitness activities.\nData Augmentation. To support physics-aware motion modeling, we augmented these datasets using OpenSim Delp et al. (2007); Seth et al. (2018), a popular and widely acceptable software for biomechanics research and motor control science Delp et al. (2007); Seth et al. (2018), incorporating detailed muscle activations, contact forces, joint positions, rotations, actuation, and velocity. We employed a full-body OpenSim model Van Horn & Team (2016) with 21 body segments, 29 degrees of freedom, and 324 musculotendon actuators, providing rich detail for lumbar movements and trunk muscle dynamics. This augmentation enhances the biomechanical fidelity of the motion data, which is critical for realistic motion synthesis. We provide more details in the appendix.\nEvaluation Metrics. FlexMotion's performance is comprehensively evaluated across several key metrics, encompassing naturalness, textual relevance, diversity, physical plausibility, and spatial control accuracy. Naturalness is quantified using the Fr\u00e9chet Inception Distance (FID) Tevet et al. (2023), which compares the distribution of generated motions to real data. Textual relevance is measured via R-Precision Tevet et al. (2023), assessing how well the generated motions align with textual descriptions. To ensure variability, diversity (DIV) is evaluated by computing the pairwise distance between generated motions Tevet et al. (2023). Physical plausibility is verified through metrics like Foot Skating, Penetration, Contact Force Accuracy, and Joint Actuation Consistency Xie et al. (2023). Biomechanical plausibility is ensured by checking that Muscle Activation Limits stay within realistic physiological constraints Lee et al. (2019). Finally, spatial control accuracy is assessed using the Trajectory Error metric Xie et al. (2023), focusing on how well the generated motions adhere to intended spatial trajectories. All results are reported as mean across ten independent runs, ensuring robustness and reproducibility. More details can be found in the appendix.\nImplementation Details. FlexMotion is built on the MDM framework Tevet et al. (2023), leveraging CLIP Radford et al. (2021) for text encoding and employing classifier-free guidance during"}, {"title": "COMPARISON WITH STATE-OF-THE-ART METHODS", "content": "We evaluated FlexMotion against several state-of-the-art human motion generation methods-including MD Zhang et al. (2022), GMD Karunratanakul et al. (2023), PriorMDM Shafir et al. (2023) and MDM Tevet et al. (2023), MLD Chen et al. (2023), OmniControl Xie et al. (2023), and PhysDiff Yuan et al. (2023)\u2014across three datasets: HumanML3D (Table 1), KIT-ML (Table 2), and FLAG3D (Table 3).\nMethodological standpoint. FlexMotion advances human motion generation by addressing the critical limitations of existing models. Unlike MDM Tevet et al. (2023), MLD Chen et al. (2023), and OmniControl Xie et al. (2023), FlexMotion integrates physics-based constraints and muscle dynamics directly into the generation process, ensuring motions that are both visually realistic and biomechanically accurate. Compared to PhysDiff Yuan et al. (2023), which also aims for physical plausibility using a physics simulator, FlexMotion offers enhanced controllability and efficiency by"}, {"title": "CONCLUSION", "content": "In this paper, we presented FlexMotion, a novel framework for efficiently generating controllable and physically plausible human motion. By utilizing a diffusion model in the latent space and a physics-aware Transformer-based autoencoder, FlexMotion achieves computational efficiency while ensuring realism. The model captures key biomechanical aspects such as joint locations, contact forces, and muscle activations without relying on physics simulators, making it suitable for real-time applications. FlexMotion also introduces a spatial controllability module that enables fine-grained control over motion parameters, such as trajectories and muscle activations, enhancing its versatility for various tasks. Our experiments on HumanML3D, KIT-ML, and Flag3D datasets show that FlexMotion outperforms state-of-the-art models in realism, physical plausibility, and computational efficiency. The framework achieves higher R-Precision and lower FID scores, indicating better alignment with textual descriptions and realistic motions. Additionally, it demonstrates lower foot skating, penetration, and muscle activation errors, making it more physically consistent and feasible. FlexMotion's reduced computational complexity further allows for faster inference, positioning it as a promising practical solution for animation, robotics, and virtual reality. Future work could explore more complex dynamics and real-time applications. While FlexMotion leverages physics-informed modeling, a sim-to-real gap persists due to differences between simulated dynamics and real-world variability. Future work will address this gap by integrating real-world data and improving alignment with experimental benchmarks to enhance its applicability in diverse real-world scenarios."}, {"title": "APPENDIX", "content": ""}, {"title": "TRAINING PIPELINE", "content": "The training of FlexMotion proceeds in three stages:\n1. Stage 1: Training the Physics-aware Multimodal Autoencoder\n\u2022 Train the encoder-decoder to reconstruct motion sequences while enforcing physics-based constraints and muscle coordination.\n\u2022 Optimize the total loss $L_{AE}$ as defined in Eqn. 8.\n2. Stage 2: Training the Diffusion Model\n\u2022 Freeze the pretrained encoder-decoder parameters.\n\u2022 Train the diffusion model in the latent space using the loss $L_{diff}$ as defined in Eqn. 11.\n3. Stage 3: Training the Spatial Controllability Module\n\u2022 Freeze both the encoder-decoder and diffusion model parameters.\n\u2022 Train the controllability module (ControlNet) to incorporate control conditions by minimizing the total loss $L_{total}$ as defined in Eqn. 13."}, {"title": "INFERENCE", "content": "FlexMotion generates motion sequences during inference by sampling from the diffusion model, guided by control conditions provided to the spatial controllability module. The inference process involves:\n1. Input: Text prompt $c$ and control conditions $\\{c_t\\}_{t=1}^T$\n2. Step 1: Initialize the latent variable $z_N \\sim \\mathcal{N}(0, I)$\n3. Step 2: For $n = N$ down to 1, perform the reverse diffusion process:\n(a) Adjust the predicted noise with control conditions:\n$\\epsilon = \\mathcal{E}_{total}(z_n, C_{\\epsilon}, n, c)$\n(b) Update the latent variable:\n$z_{n-1} = \\frac{1}{\\sqrt{\\bar{a}_n}} (\\frac{1 - a_n}{\\sqrt{1 - \\bar{a}_n}} z_n - \\epsilon) + \\sigma_n z$\nwhere $z \\sim \\mathcal{N}(0, I)$ if $n > 1$, else $z = 0$\n4. Step 3: Decode the final latent representation:\n$\\{x_t\\}_{t=1}^T = \\mathcal{D}(z_0; \\theta_{\\mathcal{D}})$\n5. Output: Generated motion sequence $\\{x_t\\}_{t=1}^T$ adhering to control conditions and text prompt."}, {"title": "ADDITIONAL IMPLEMENTATION DETAILS", "content": "Hyperparameter settings. In our experiments, the weighting factors for the loss terms in the physics-aware Transformer encoder-decoder were set as follows: $\\lambda_{pos} = 1.0$, $\\lambda_{rot} = 1.0$, $\\lambda_{vel} = 0.1$, $\\lambda_{acc} = 0.1$, $\\lambda_{torque} = 0.5$, $\\lambda_{force} = 0.5$, $\\beta_{euler} = 1.0$, and $\\beta_{muscle} = 1.0$. These values were chosen to balance the importance of accurately reconstructing each modality while enforcing physical plausibility. For the diffusion model, we used a linear variance schedule for the noise parameters $\\beta_t$, with $T = 1000$ diffusion steps during training. During inference, we employed the DDIM sampler Song et al. (2023) with 100 steps for efficient sampling without significant loss in motion quality.\nDataset preprocessing. All motion sequences were downsampled to 20 frames per second to reduce computational complexity while retaining essential motion characteristics. The joint positions and rotations were normalized based on the mean and standard deviation computed over the training set to facilitate stable neural network training.\nDatasets. We evaluate the proposed FlexMotion on several extended datasets, including Hu-manML3D Guo et al. (2022a), KIT-ML Plappert et al. (2016), and Flag3D Tang et al. (2023), each"}, {"title": "IMPLEMENTATION OF PHYSICS-BASED CONSTRAINTS", "content": "In this section, we provide detailed explanations of how the physics-based constraints are implemented in FlexMotion.\nComputation of the mass matrix $M(r_t)$. The mass matrix $M(r_t)$ represents the inertia of the system and is computed based on the configuration of the skeletal model at time $t$. Each joint and limb contributes to the overall mass and inertia, which are derived from the physical properties (mass and moment of inertia) of the body segments. The mass matrix is assembled by summing the contributions of each body segment using the principles of rigid body dynamics Lee et al. (2019); Zhang et al. (2024b). Mathematically, the mass matrix is computed as Eqn. 14, where $J_i(r_t)$ is the Jacobian matrix of segment $i$ with respect to the joint angles $r_t$, and $I_i$ is the inertia matrix of segment $i$ Lee et al. (2019); Xie et al. (2021a).\n$M(r_t) = \\sum_{i=1}^{N_{segments}} J_i^T(r_t) I_i J_i(r_t)$ (14)\nCoriolis and centrifugal forces $C(r_t, \\dot{r}_t)$. The Coriolis and centrifugal forces account for the effects of joint velocities on the dynamics of the system. These forces are computed using Christoffel symbols, which involve the partial derivatives of the mass matrix with respect to the joint angles. The Coriolis and centrifugal forces are calculated as Eqn. 15. In practice, we approximate these forces by computing the necessary partial derivatives numerically or using analytical expressions provided by the musculoskeletal model Lee et al. (2019); Xie et al. (2021a).\n$C(r_t, \\dot{r}_t) \\dot{r}_t = \\frac{1}{2} (\\frac{\\partial M}{\\partial r_t} + \\frac{\\partial M^T}{\\partial r_t} - \\frac{\\partial M}{\\partial r_t}) \\dot{r}_t \\dot{r}_t$ (15)\nGravitational forces $G(r_t)$. The gravitational forces are calculated based on the positions of the body segments and the gravitational acceleration $g$. The gravitational forces are computed as Eqn. 16, where $m_i$ is the mass of segment $i$ Lee et al. (2019); Xie et al. (2021a).\nG(r_t) = \\sum_{i=1}^{N_{segments}} J_i^T(r_t) m_i g (16)\nContact jacobian $J_c(r_t)$. The contact Jacobian relates the joint velocities to the velocities at the contact points with the environment (e.g., the ground). It is computed as Eqn. 17, where $p_c(r_t)$ represents the positions of the contact points Lee et al. (2019); Zhang et al. (2024b) (Eqn. 17).\n$J_C(r_t) = \\frac{d p_c(r_t)}{dr_t}$ (17)\nIntegration into training. The computed dynamics components are integrated into the physics-based loss $L_{euler}$ as described in Eqn. 6. During training, we ensure that all computations are differentiable to allow gradient backpropagation through the physics-based loss terms."}, {"title": "MUSCLE ACTIVATION MODEL", "content": "The muscle activation model aims to compute muscle activations $a_t$ that produce the desired joint accelerations $\\ddot{r}_t$ while minimizing excessive muscle effort.\nDerivation of the mapping matrix $L$. The mapping matrix $L$ relates muscle activations to joint accelerations and is derived from the musculoskeletal model's moment arms and muscle force-generating properties. For each muscle $m$ and joint $j$, the moment arm $r_{jm}$ represents the torque produced at joint $j$ per unit muscle force from muscle $m$. The mapping matrix $L$ is constructed as Eqn. 18, where $M$ is the mass matrix, $R$ is the matrix of moment arms $r_{jm}$, and $F_{max}$ is the diagonal matrix of maximum isometric muscle forces Lee et al. (2019)."}, {"title": "ABLATION STUDY ON PHYSICS-BASED CONSTRAINTS", "content": "We conducted an ablation study to assess the impact of the physics-based constraints on the model's performance. We trained variants of FlexMotion with and without the Euler-Lagrange loss term $L_{euler}$ and the muscle loss $L_{muscle}$. To have a consistent comparision, we report the results on the HumanML3D dataset, when there is no spatial condition applied. The results are summarized in Table 8.\nThe results in Table 8 demonstrate that the inclusion of physics-based constraints significantly improves physical plausibility metrics without compromising motion naturalness."}, {"title": "EFFECT OF LATENT SPACE DIMENSIONALITY", "content": "We conducted an ablation study on the HumanML3D dataset (without spatial conditions) to evaluate the effect of latent space dimensionality on FlexMotion's performance. As shown in Table 9, performance improves with larger dimensions, peaking at $x \\in \\mathbb{R}^{1\\times 1024}$, after which further increases result in slight degradation."}, {"title": "TRADE-OFFS BETWEEN REALISM AND PHYSICAL ACCURACY", "content": "To analyze the trade-offs, we conducted experiments where we varied the weights of the physical constraints in our loss function. Specifically, we adjusted the parameters $\\lambda_{euler}$ and $\\lambda_{muscle}$, which control the influence of the Euler angle regularization and muscle activation limits, respectively. By observing the impact of these adjustments on both realism and physical accuracy metrics, we can provide valuable insights into how these aspects interact.\nWe present the results in Table 1, which compares our model's performance under different settings of $\\lambda_{euler}$ and $\\lambda_{muscle}$ on the HumanML3D dataset.\nFrom the results, we observe that decreasing the weights of the physical constraints (e.g., $\\lambda_{euler} = 0.0$, $\\lambda_{muscle} = 0.0$) leads to improved realism metrics, such as higher R-Precision and lower FID, indicating that the generated motions are more perceptually similar to real data. However, this comes at the cost of physical accuracy, as evidenced by higher values in metrics like Skate, Float, and Penetrate.\nConversely, increasing the weights of the physical constraints (e.g., $\\lambda_{euler} = 2.0$, $\\lambda_{muscle} = 2.0$) enhances physical accuracy, with lower values in physical metrics, but slightly degrades realism metrics.\nThis trade-off suggests that there is a balance to be struck depending on the application requirements. For scenarios where physical accuracy is paramount, higher weights on physical constraints are advisable. In contrast, applications prioritizing perceptual realism might benefit from lower weights on these constraints."}]}