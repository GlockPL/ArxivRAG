{"title": "Lessons in Cooperation: A Qualitative Analysis of Driver Sentiments towards Real-Time Advisory Systems from a Driving Simulator User Study", "authors": ["Aamir Hasan", "Neeloy Chakraborty", "Haonan Chen", "Cathy Wu", "Katherine Driggs-Campbell"], "abstract": "Real-time Advisory (RTA) systems, such as navigational and eco-driving assistants, are becoming increasingly ubiquitous in vehicles due to their benefits for users and society. Until autonomous vehicles mature, such advisory systems will continue to expand their ability to cooperate with drivers, enabling safer and more eco-friendly driving practices while improving user experience. However, the interactions between these systems and drivers have not been studied extensively. To this end, we conduct a driving simulator study (N=16) to capture driver reactions to a Cooperative RTA system. Through a case study with a congestion mitigation assistant, we qualitatively analyze the sentiments of drivers towards advisory systems and discuss driver preferences for various aspects of the interaction. We comment on how the advice should be communicated, the effects of the advice on driver trust, and how drivers adapt to the system. We present recommendations to inform the future design of Cooperative RTA systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Drivers everywhere rely on Real-time Advisory (RTA) systems almost every day, popularly in the forms of navigational [1-4] and eco-driving assistants [5\u201310]. With the improvements in intelligent driving systems, new RTA systems are continually proposed to aid in driving scenarios such as controlled takeovers [11-14] and safe decision making [15\u201318]. These RTA systems are deployed inside vehicles through onboard systems developed by car manufacturers in the form of Advanced Driver Assistant Systems (ADAS) at Society of Automotive Engineers (SAE) level 2 autonomy [19], or through the use of smartphones [20, 21]. With increasing demand for such RTA systems, Autonomous Vehicles (AV) pose as the natural solution to encapsulate issues that these systems address. However, the market penetration of AVs is likely to be delayed due to safety risks [22, 23] and economic factors [24- 26], particularly in developing countries with large driving populations [27]. Thus, the development of these systems for such underserved markets, usually through SAE level 2 systems, can have a significant impact on our future.\nThese new developments continue to increase the time granularity of how frequently RTA systems advise drivers ranging from landmark-based events in navigation and turning assistants [3, 4] to second-by-second advice in eco-driving and speed advisory assistants [28\u201330]. As this time granularity increases, the interactions between these systems and drivers must be formulated with careful consideration. One avenue of improving user experiences with such time-precise systems is through the inclusion of cooperative features [31-33], resulting in Cooperative Real-time Advisory (CoRTA) systems (illustrated in Figure 1). Such CoRTA systems are cognizant of driver behavior and preferences, and thus allow for enhanced interactions with drivers. These interactions are likely to not only inform user experiences, but also to impact driver trust in automation, and hence, influence the efficiency of downstream Human-Vehicle teaming tasks (e.g. eco-driving) [34, 31].\nThe influence of AV behaviors on driver-vehicle interactions are widely studied at the SAE Level 3 or higher. Particularly, the impact of different AV behaviors on driver attitudes (e.g. trust, comfort) towards these systems has been extensively researched, to inform future designs using driving simulator studies [35, 33, 36], semi-structured interviews [37, 34], and meta-surveys [38, 31]. For example, a driving simulator study to capture the effects of assertive AV takeover maneuvers (i.e. the transfer of control of the vehicle between automation and the driver) by Kraut et al. [35] found that such behaviors invoked a faster"}, {"title": "response time in drivers without influencing driver stress or safety. Similarly, Helldin et al. [36] found an inverse effect of providing uncertainty information on driver trust in AVs during shared-control driving.\nWhile such evaluations are abundant for systems with SAE Level 3 and above, similar treatments of driver attitudes for SAE Level 2 autonomy, particularly CoRTA systems, are sparse. Works that do address these gaps are informative for future designs of CoRTA systems but recommend further studies on understanding how the system behaviors influence driver trust and reliance [39]. Some works only evaluate singular factors such as the User Interface (UI) [11, 40], while others primarily aim to test the effectiveness of the systems [8, 30].\nConsidering solely eco-driving CoRTA systems, previ- ous works have used different evaluation approaches. Some works forgo human-in-the-loop testing and rely completely on software simulations, either due to limiting assumptions that render them hard to test, or unsatisfactory performance in non-idealized settings [41, 42, 7, 30]. Others such as Birrell et al. [8] test their solutions in naturalistic studies, but only discuss the efficiency of their methods and its impact on user behavior without evaluating driver preferences on the interactions. Meanwhile, driving simulator studies that address driver preferences have been conducted but only evaluate certain aspects of the UI. For example, Shahab et al. [28] test auditory modes to communicate how drivers should change their speeds based on pitch and Di Lena et al. [10] test the function of a gamified UI. Both works successfully show the validity of their UIs, but do not discuss driver preferences for either the systems' behaviors or the advice provided by such systems. On the other hand, Chada et al. [5] capture quantitative metrics on the potential impact and user acceptance of their novel system [6], but do not discuss qualitative opinions of drivers. Moreover, new learning-based eco-driving CORTA systems that could significantly improve the efficiency of downstream tasks [43\u201349, 30, 50] are untested with humans. Therefore, their interactions with users have not been studied extensively. Thus, the following questions for interactions between drivers and (learned) CoRTA systems require more attention:", "content": "RQ1 What are driver expectations for the behavior of CORTA systems?\nRQ2 How do Driver-CoRTA systems interactions affect driver behavior, trust in automation, and user experience?\nRQ3 What are the UI requirements desired by drivers in their interactions with CoRTA systems?\nIn this article, we seek to address these questions in capturing driver sentiments towards CoRTA systems to enable better cooperation by combining a driving simulator study with semi- structured interviews. Particularly, we first conduct a human- in-the-loop driving simulator study to allow participants to directly interact with CoRTA systems that exhibit a variety of behaviors. Then, we conduct semi-structured interviews to qualitatively understand the impact of the diverse behaviors on drivers and their perceptions to capture user preferences for their interactions with CoRTA systems. We analyze the sentiments expressed by drivers and discuss how their interactions can alter driver behavior. Particularly, participants in our case study interacted with 9 CoRTA congestion mitigation assistants in the form of eco-speed recommending Piecewise Constant Policies [49] (described in Section II-A) and provided their firsthand sentiments on the interaction via semi-structured interviews. These recommendations and findings are relayed in this article to impact the design and development of future CORTA systems.\nThus, our main contributions are:\n1) A driving simulator user study (N=16), focused on young adults, that captures their sentiments towards CORTA systems through a case study on congestion mitigation assistants.\n2) A qualitative analysis of driver preferences to provide insights into driver interactions with CORTA systems.\n3) Recommendations in designing future CoRTA assistants.\nTo the best of our knowledge, we are the first to qualitatively investigate the broad sentiments held by drivers towards CoRTA systems through our user study."}, {"title": "II. METHOD", "content": "In this section, we first provide a short summary of the policies used in our CoRTA systems. We also introduce our user study, its procedure, and the user interface employed for the user study."}, {"title": "A. The Advisory Policies", "content": "As shown in Figure 1, CORTA systems are designed to provide a driver with some advice on how they should act in real-time while adapting to drivers' style and situational environmental factors. Piecewise Constant (PC) Policies [49] exemplify such cooperative systems to address congestion mitigation, i.e. the reduction or dampening of stop-and-go waves in traffic [51, 48]. PC policies are policies trained using reinforcement learning that require the current action, i.e. the advice, to be held constant for a period of time \u2013 referred to as the hold-length, \u03b4. Sridhar and Wu [49] designed this hold period to account for the time taken by drivers to react to and act upon the advice, and hence enabled the policies to be cooperative. Residual variants [30] of such policies enable further cooperative behaviors through the use of driver trait inference modules. These policies aim to advise drivers of the speed that should be maintained in order to mitigate congestion. In our case study, we use PC policies and two of their residual variants\u00b9 that observe the driver and their environment to provide speed advice (See Appendix A for details). Particularly, we utilize 9 different policies: 3 policy types \u00d7 3 different hold-lengths: d = {5,7,10} seconds. The policies exhibited diverse behaviors in the speed advice they recommend and how the speed advice changes in real-time. For example, the advised speeds range between slow, \"just right\", or fast speeds, and there are either frequent or no changes, and even incremental or large changes in the advised speed. Some policies advise a sequence of speeds that when acted upon by drivers are intended to amount to a different, more effective speed advice i.e. such policies are not direct in communicating their advised speeds. Other policies are transparent and advise drivers on"}, {"title": "B. The Simulation Environment", "content": "The simulated environment (shown in Figure 2) consists of 40 vehicles driving on a single lane circular ring road with inner circumference of 628m and outer circumference of 654m in the CARLA Simulator [52]. Only one vehicle was controlled by the participant, while the other 39 vehicles were controlled using the IDM car following model [53]. The weather in the simulation was set to a clear sunny day across all trials. No other agents (other than the 40 vehicles) were present in the simulation to ensure minimal distractions to participants. This simulation setup is designed to be consistent with the congestion mitigation literature [51, 48, 43]."}, {"title": "C. The User Interface", "content": "The participants' viewpoint is from the driver's seat, which includes the front view through the windshield and the rear- view mirror. The speedometer is shown at the bottom of the screen. Figure 4 shows the user interface for the study where the advice from the policies is communicated to the drivers via the speedometer, similar to other studies providing real-time speed advice [9, 40]. The speedometer shows the speed of the vehicle in the form of a gauge and a number at the center of the gauge in a large font. We display the advised speed on the speedometer as a red line surrounded by a \u00b11m/s (~2.5 mph) green range. Participants were told that the red line indicated the exact speed that the policy was advising, but that they should try to keep their speed within the green range. The green range effectively acted as an acceptable error bar for the drivers. The number on the speedometer was highlighted in green when the participant's speed was within the acceptable error bar (See Figure 4(b))."}, {"title": "D. The Procedure", "content": "Figure 5 illustrates an overview of the procedure for the user study. We conducted a 3-person pilot study before recruitment to validate the study procedure. To begin the study, participants were first introduced to the goal of the study and briefed about their task. Participants were told that the aim of the study was to test congestion mitigation assistants and capture driver sentiments towards interactions with the assistants. Additionally, participants were also told that their task was to (1) drive safely (avoid collisions, stay on-road, etc.) and (2) follow the advice provided by the system. All participants were explicitly instructed to ignore the advice of the policy if they determined that it would lead to unsafe driving or a loss in control of the vehicle. The study was conducted in a controlled lab environment and actions were taken to ensure that there were no external distractions present during the study procedures. Participants first signed a consent form and then answered a pre-study questionnaire consisting of questions on demographics and driving experience. Participants were then given"}, {"title": "III. RESULTS AND DISCUSSION", "content": "In this section, we provide our conclusions on the do's and don'ts for CORTA systems, that are summarized in Table I. All participant responses were edited by a research assistant to remove filler words (e.g. 'like', 'umm') to shorten responses that were verbose or repetitive. Then, thematic coding was performed using NVivo14 on the responses by a single coder to seek sentiments relevant to our research questions. After all responses had been coded, resulting themes and significant quotes were identified and used to construct the analysis presented here. To enhance quote understanding we replace verbiage and add context (between [brackets]) for some responses. We attest that the context and sentiments of participants are presented as they were intended. Note that we refer to each participant as P## where ## is the number assigned to them between 1 and 16."}, {"title": "A. \"What did you think of the advice?\"", "content": "\"I think it's the best one... It seemed like the suggested speeds were very accurate, like if I did them, nothing bad would be happening.\" - P11\nOur interviews revealed multiple behaviors and characteristics of real-time advisory systems that significantly affected participant experiences. Unsurprisingly, the most significant attribute of the advice that drivers seek is accuracy (as it pertains to them). Although in the quote above, P11 described accuracy as the ability of the policy to keep them safe, other factors such as the value of the advised speeds, the frequency of speed changes, the magnitude of the changes, time period to react to advice, and the understandability of the advice were all important contributors. These behaviors affected drivers' trust in the system, and led to drivers adapting their driving style, and, in some extremes, ignoring the advice completely.\nThe Value of the Advice: Users expect the advice provided by the system to be reasonable, i.e. the speed should reflect"}, {"title": "the current state of the environment and task, and not request the users to take an action that can be seen as extreme or out of line with social conventions. Multiple participants describe the notion of a \"sweet spot\" (P05) that they were hoping the advice would hit. The specification for the sweet spot varied between participants in terms of the advised speed range and the headway to their leader. In our study, some participants preferred advice in the range of 25 to 30 mph, while others preferred advice in the slightly higher range of 30 to 35 mph. Similarly, some drivers preferred leaving the length of one car's distance between them and their leader, while others preferred much larger distances of up to the length of three cars. Systems that suggested advice corresponding to the driver's sweet spot are viewed favorably.", "content": "\"It actually kept me at... 25 mph for a longer period of time, which I liked. When the cars were like, you know, a [good] distance in front of me.\" - P04\nIn contrast, participants disliked when the advice did not fit their sweet spot requirement. As it pertains to congestion mitigation, speed advice that was either too slow or too fast was not preferred and considered as \u201cextreme", "safe\" (P13) and provided \u201cgood control of the car": "P13), they also viewed them as counter-intuitive and mentioned that slow speeds led to inefficiencies, i.e. were \u201ccausing a pile up", "pressure of seeing that long line of cars behind [them": " (P10) when following such slow advice as it led to a violation of the social norms that they were accustomed to. Particularly, users were weary of being blamed by the cars behind them for holding up traffic when they were simply trying to follow the advice from the system. In addition to being stressed, frustrated, and \"irritated\" (P10) by these aspects of slow advice, participants said that they would \u201cnot want to follow these instructions, even if they [were] below the speed limit", " P02\nAll 16 drivers reported that they felt very uncomfortable following advice that was too fast as it would result in unsafe behavior, such as collisions or off-roading. Participants unanimously chose to \"just ignore the recommendation and follow [their] own mind[s]": "P12), on being recommended"}, {"title": "advice that was too fast. Such advice was \"hard to follow\" (P02) and made users \"lose control\" (P12) of the vehicle and ultimately led to participants losing faith in the abilities of the system. Thus, we recommend that CoRTA systems should carefully craft advice to appear accurate as defined by drivers and their personal sweet spots. We believe that this finding further motivates the development of adaptive technology in CoRTA systems that enables personalization to different drivers in line with the findings of Walch et al. [31] for SAE Level 3 systems.", "content": "The Changes in Advice: Participants disliked policies that advised significant speed jumps (i.e. large changes in the magnitude of the advice) and frequent or sudden unexpected changes. The primary rationale for this sentiment was the increase in mental load in order to react to large and/or rapid changes that made driving less enjoyable. For example, P02 mentioned that the wavering of the recommended range between \"a high range and then suddenly going down to [a] little range... It was a little stressful for me.\" Such behaviors also made the advice hard to follow and left drivers unsatisfied, ultimately leading to them \u201cnot following it", "frustrating\" (P04). P11 mentioned that such advice felt \u201cpointless,\" as trying to follow any of the speeds in the set was \\\"causing an issue\\\" with the rest of the traffic, where they were forced between two undesirable behaviors \u2013 they were either \u201ccatching up to the rest of the pack or [they were": "the one that was creating a pack", "speeds": "n\"It was constantly bouncing between 15/20 mph and 50/60 mph. And so, the effect was, realistically, my speed was somewhere between. Which I can see why it would want to keep me at that speed. But it was frustrating as a user... Well, if it wants me to go that speed, why doesn't it just put the bar there? Because that's where I'm driving, in effect, when it bounces back and forth like that.\" - P04\nInterestingly, on the other end of the spectrum, advice that was constant, i.e. did not change for a long period, was also"}, {"title": "not preferred as it led to driver disengagement, even though it was easy and (usually) safer to follow. Drivers referred to policies suggesting such constant advice over multiple hold- length periods as \u201cconservative", "less reactive\" (P01). This constant behavior also led to interesting artifacts where the system would not ask drivers to slow down when they were approaching the car in front of them which users described as unsafe and dangerous.": "content\": \"\"I think it would be better if the speed changed in a more continuous fashion and that's like feasible with the acceleration and deacceleration of the car. Instead of jumping from like 30 mph to 80 mph. If it slowly moved up that way, then it's okay \u2013 I can follow that.\" - P04\nDrivers expressed that they preferred advice that was \u201cchanging", "in this case it's a congestion. So, I think it makes sense to change the speed\" (P12). Similar to the sweet spot in advice value, drivers prefer policies that provided smooth, gradual changes, where the rate of these changes is dependent on the drivers' style. Advice that changed gradually, i.e. in small increments, and allowed users to react comfortably enhanced the driving experience. P11 in particular mentioned that having \u201cless big\" jumps between consecutive actions made it \u201cmore manageable to get to the\" advised action. They viewed such actions as \u201csomething I could actually do\" as opposed to \u201can unreachable goal\" that they attributed to the advice provided by the systems with large jumps. Participants specifically mentioned that they would be more likely to use and follow a system that provided smooth advice to them.\nTherefore, we further recommend that the cooperative systems should also include the drivers' preferred rate of change of advice in its sweet spot determination. Furthermore, we also recommend that CoRTA systems should be transparent and direct in their intent, i.e. the system should not communicate auxiliary advice that is meant to lead drivers into causing the effects of the actual intended advice. Such manipulative interactions are viewed highly unfavorably by drivers.\nThe Hold-length: The time that the advised action was designed to be held constant, i.e. the hold-length \u03b4 of the policies, had a significant effect on the users' experience. Generally, longer hold-lengths (10s in our case) are preferred as they allow users with ample time to follow the advice. Meanwhile, shorter hold-lengths (both 5s and 7s) either did not give enough time for drivers to react or were stress inducing. Particularly, P13 mentioned that the policies with shorter hold- lengths were asking them to \\\"do too much in too little time.\\\" Participants felt unsatisfied by policies that would change the advice as soon as they had achieved it as it left them frustrated and annoyed.\"\n    },\n    {\n      \"title\": \"\\\"The fact that it kept changing as soon as I reached the desired speed. The first question I marked here was this one [pointing to the frustration question. Researcher: \\\"Super frustrating?\\\"": "I think if it was a real-life scenario and it did that, I would just completely throw it out the window. [Talking to the policy] You don't deserve to be telling me what to do. You don't know what you're doing.\" - P10", "content": "We hypothesize that there exists an ideal dynamic time- period that would satisfy drivers. Such an ideal time-period would depend on the drivers' personal preferences, attributes of the advice (e.g. the magnitude of change), and environmental factors. A dynamic hold-length that depends on the above factors would ideally address the issues discussed above and lead to better, more reactive and cooperative systems. We present quantitative results in the form of ANOVA tests on metrics collected from the questionnaires that corroborate these qualitative findings in Appendix A.\nThe Understandability of the Advice: Aside from wanting accurate advice, drivers are also particular about understanding the advice provided to them. Systems that relayed advice that matched what drivers would do normally were touted as more \"realistic\" (P14) and hence more usable. P09 called their experience with a policy \u201cfun", "I like that it almost matched what we were doing... it almost made sense what kind of behavior it was trying to achieve.\\\"\n\\\"I did like it, because I think in that one it made sense was trying to be achieved and the driving speed was one that was more natural.\\\" - P09\nIn contrast, advice that differed from the drivers' world view was seen as \\\"annoying\\\" (P04) and \\\"frustrating\\\" (P10), and hence influenced drivers to ignore or \\\"disobey\\\" (P15) the system. For example, P02 called a system \\\"garbage\\\" as its suggestions were \u201cthe opposite of what I would do.\" Additionally, advice that was perceived as confusing negatively impacted participants' trust in the systems' abilities.\n\\\"I was also confused why it was changing the speed on me when the cars in front of me were pretty far ahead for most of the simulation, so I didn't really understand why all of a sudden [it would": "decide I should be going 5 miles[sic] faster, oh, now I need to go 5 miles[sic] slower. So, because of that, I was kind of like, okay, I don't know how much I should really follow this advice... I just, I couldn't understand why it was telling me what it was\" - P04\nIn the case of traffic advisory systems, users prefer instruc- tions that reflect the current state of traffic and allow them to blend into the flow of traffic, to be viewed as reasonable by others on the road. For example, P05 mentioned that they \"didn't feel like [they were] fitting into like the rest of the traffic\" when the policy was advising a speed of \u201c20 mph and there [were] no cars in front of [them].", "throw [the system": "out the window"}, {"title": "We highly recommend that the advice given to drivers should also be easily understandable, in addition to the aforementioned personalizations. We foresee the inclusion of these personalizations as a major challenge for the development CORTA systems as they would be required to address the various user needs and still also fulfill the downstream tasks that they are designed to address. Furthermore, participants noted their understanding of the system as a determinant of their trust in the system.", "content": "\"It felt like it had no understanding of the distance between me and the car in front. So, because of that, I didn't really trust the system that much.\" - P04\nTrusting the Advice: Users mentioned that the presence of inconsistencies during a trial decreased their trust in the system, even if they had experienced desirable behaviors during the trial. For example, P03 very bluntly stated during the post-study interview that \u201cI would not fully trust any of them", "trials": "I'm in conflict with that advised speed a lot\", \"mismatch is because of [my] driving style\", \"I'm contradictory with the system.\"\nFurthermore, the contrast between the drivers' past driving experiences and the advice given by the system was a significant factor that influenced the drivers' trust in the system. To this point, P10 recalled that the system \"was just going after every basic instinct and foundation of driving my father taught me.\" They specifically stated that this disagreement made them not want to listen to the advice: \"I don't like listening to this.\" P10 also added that as a consequence of this loss in trust they ", "it": "f they were driving", "world": "ather than in the simulator.\n8 of 16 participants mentioned that they would ignore the advice in some form due to their loss in trust in the system. Drivers wanted to partially ignore the advice if the system was somewhat reasonable and only faltered sometimes (e.g. provided incredibly high speed advice once while being stable otherwise). In particular, P16 noted that if the system \"gave me some like unreasonable commands that's pretty high, but also in some [cases] lower ones which are kind of reasonable,\" that they would \u201cjust completely ignore the higher ones", "just like an indicator": "o", "bit": "In more extreme circumstances, i.e. if the system exhibited unstable behavior or inconsistencies, drivers reported wanting to completely ignore the system and were only following the advice solely because they were asked to for the study. This is illustrated in the following interaction with P10:\nResearcher: What did you think of this trial?\nP10: It was great once I opted to ignore all the inputs [laughs]\nResearcher: What did you think about what it was asking you to do?\nP10: Unreasonable. I think it was asking me to slow down when there was clearly a lot of room in front of me. Sometimes it was asking me to speed up when I could see [that] I would have to slam the brakes if I tried to do that. And so, I was like, well, I have a better decision making than this system.\nTherefore, we also recommend that CORTA systems should adapt to the long-term driver behaviors and their past histories so that participants deem the system to be reasonable. We posit that such adaption would lead to an increase in trust and therefore usage which would directly impact the downstream task that the system is designed to address."}, {"title": "B. \u201cWhat did you think of the User Interface?\"", "content": "Participants found the use of the speedometer to communicate the advice controversial. P06 said that the UI was \"definitely pretty easy to follow... but at times it was a bit distracting.\" This sentiment was shared by 8 other participants, where P09, in particular, felt strongly that \"staring at the speedometer is not safe.\" Users shared that driving was already a load incentive task due to the requirement of paying attention to the road and the other vehicles and adding \"any kind of instructions is actually another layer of burden that you impose on the driver\" (P12). Thus, the increased load diverted drivers away from the main driving task \u201coverwhelmed", "eased the amount of mental load a little bit": "P05) but adversely affected how precise they were in following the advice. Furthermore, the use of the green bar to indicate an acceptable range was touted by drivers as \"helpful\" (P12) as it gave them more \"flexibility\" (P14). P11 mentioned that the green bar facilitated easier instruction adherence as finding", "the margin was a little small": "P16) and wished that the green range had been a little larger to allow for more leeway in how precisely the advice was followed,", "deal": "P16).\nParticipants also reacted positively to coloring the speed value (displayed on the speedometer) green when they were driving within the advised range and proclaimed it as a \u201cgood touch", "made me look at the actual [gauge] less and just look at the big number and that didn't take as much attention": "The color choice also helped in minimizing the load on participants as they were \"looking for green"}, {"title": "in their peripheral vision. Therefore, we conclude that using attractive colors decreases the mental load on participants and enables easier instruction following. Additionally, allowing a comfortable acceptable error bar (green range) would improve user experiences, but we find that this range differed between drivers. We encourage further work to estimate the width of the acceptable error range based on the driver preferences.", "content": "The use of the color green and the dynamics of changing colors if the advice was followed, \"gamified (sic)\" (P10) the system. The green color scheme improved interactions with the system and also kept drivers engaged. Participants were actively aiming to visualize green on their speedometer simply because \"seeing green is fun\" (P10). In this particular scenario, P10 was disappointed when the policy would change the advised action not soon after they achieved it because they did not \"get the satisfaction of getting the green number. And so that was frustrating.\" They went on to say that they would have been less frustrated \"if it gave me one little second of being green", "gamified\" reactions might be unsafe. We observed that it was easy for participants to get distracted and lose sight of \"what's in front of [them": "as much because [they're] focused on putting the white[sic] inside the green part", "voice command[s": "or the indicator light would be better"}, {"title": "C. \"Did You Find Yourself Adapting to the System?\"", "content": "While users have preferences for the systems' behavior, we observed that drivers adapted their own driving during their interactions with the system in an effort to improve their experience. Towards the latter trials, some users formed simple rules around when and how they would follow advice. These rules differed for each participant and were informed by their previous experiences with the different policies and their behaviors. For example, P16 said that they would always check the distance to the car ahead of them first. Then if they determined the distance was large enough and their speed was less than or at 30 mph they would \"feel comfortable driving a little faster,\" if the policy suggested it. Else, they would only listen to the policy if it advised them to slow down. Otherwise, they would \"do nothing.\" Alternately, we observed that P07 would ignore all advice that was over 20 mph as they felt \"unable to control the vehicle at larger speeds\", regardless of their distance to the car ahead of them. While P07 did not provide a reason for their decision, we posit that they formed"}, {"title": "their rule due to their limited driving experience (1 year or less), and after almost crashing during an early trial. Similarly, P15 mentioned that they also formulated similar rules and \"games\" in real life:", "content": "\u201cI do this when I'm driving a regular car as well, where I try to keep the speed at a certain number and just hold it there consistently. I think of it kind of like a game where you're holding the pedal down just enough where it's not going up or down.\" - P15\nAdditionally, participants also exhibited behaviors based on rationalizations that they arrived at by anticipating certain behaviors of the policy. When asked why P09 had driven faster than the advice given during a trial, they mentioned that \"it was going slow, and I didn't want it to go slow because the distance between the next car and us was quite large... We had just slowed down, so I knew we weren't going to slow down again any time soon.\" Particularly, participants expected the policies to behave similarly, even though the policies clearly exhibited different behaviors. Note that, information about the policies (if they were similar or different) was intentionally withheld from participants to observe if such expectations would arise. For example, in one instance P04 purposefully disregarded an advised action as they were anticipating the policy to change its advice at a later time, due to their experience in previous trials.\n\"I've been gaming the system a little bit because I've been expecting it to, you know, it'll recommend something high and then recommend something low. And this time it felt like it would recommend two things that were high. And then it'd be like, I guess I'm going to accelerate now. And then it would drop low, but it wouldn't drop it like far below what I had accelerated to. It felt like this is a little bit easier to match the speed. It's like I accelerated and now I just have to go back down a little bit.\" - P04\nParticipants viewed the need to adapt to the system as normal and said that it was not cumbersome or annoying. For example, P08 mentioned that using the system gave them ", "before": "P13 elaborated that using the system was akin to the", "experience": "ith \u201cany car you drive for the first time", "you kind of have to get used to the features.": ""}, {"title": "D. \u201cWould you use these systems?\"", "content": "From the results of the questionnaires and interactions with participants we conclude that the policies tested during the study are not deployable without modifications, due to the drawbacks discussed previously. However, while the systems might not suggest perfectly accurate advice, participants still found auxiliary uses for the policies. First, participants mentioned using the system to affirm their own driving styles and speeds. Particularly, P12 mentioned the advised speed \u201creassure[d] that the speed that [they] want to drive is correct", "matches [their": "optimal speed in [their] mind", "very validated in [their] driving choices": "P10). Second, users said that they would suggest the usage of this system as a"}, {"title": "teaching tool for new drivers. For example, P13 revealed: \"my younger sister is learning how to drive right now. If you told me I could put [the system] in her car, I would", "case": "If [I", "more": "P14).", "content": "Participants proclaimed that they would want to adopt CORTA systems if there was a clear benefit for their usage. P08 said that they would employ the system if they were \"convinced by whatever benefits\" of the system. In particular, P08 mentioned environmental benefits of congestion mitigation agents, if proven to work, would motivate them. Others agreed that they would use such a system were it be proven to have altruistic benefits (e.g. environmental benefits).\n\"So, I feel like it's kind of asking the user to [make a] contribution to society... It could result in a good way, in a sense that everyone contributes to that and make the whole[sic] traffic better. So, those type of [motivations], I buy that myself. So, I think it's good. I feel like, people would buy that. But there are definitely some users who just don't care.\" - P16\nIn general participants had a positive outlook towards ideal CORTA systems. P06 mentioned that this driving technology was \"something that should definitely be looked into for the future.\" They acknowledged that, while self-driving solutions might address such goals, they thought that these systems are \"definitely something that should be looked into\" for \"having advice for non-self-driving vehicles.\" Therefore, we posit that future CORTA systems built for altruistic purposes would be widely accepted by young drivers, if they are personalized as postulated in our discussion.\""}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "Limitations and Future Work: While the findings presented above provide important insights into driver interactions with advisory systems, our methods are not without limitations. Firstly, we believe that a larger user study with a more diverse population is merited due to the smaller range in participant experiences in our study. Particularly, while our population addresses the sentiments of a significant portion of drivers, it does not reflect the needs of the majority. Secondly, our participants experienced all study procedures in a lab environment with a driving simulator. While driving simulators are helpful in evaluating user sentiments in automotive settings, they lack the psychological and physical level feedback mechanisms offered by naturalistic driving. A naturalistic driving study in a controlled environment to evaluate CORTA systems would provide further credence to our findings. Lastly, our study was designed to capture qualitative user sentiments for one style of UI design. Performing a comparative study with multiple other UI options would be conducive in arriving at a definite UI framework for CoRTA systems. Therefore, a systematic study that includes such definite quantitative methods to capture user perceptions would corroborate our findings."}, {"title": "Conclusions: In this article, we present our findings from a driving simulator user study (N=16) conducted with young adults. We present insights into how drivers interact with cooperative real-time advisory systems and discuss various preferences and reactions for such interactions. Our findings suggest that users prefer smooth-flowing, easy to understand advice that is also personalized to reflect their driving styles. We also find that the use of attractive colors in the UI of these advisory systems improves user experiences and driver engagement. Additionally, agreement in systems' behavior and users' past experiences contributes significantly to drivers' trust in the system. Lastly, find that drivers anticipate advice from the systems and adapt their driving mannerisms accordingly. We anticipate the use of these lessons in cooperation in the design and development of future automotive assistive systems.", "content": ""}, {"title": "APPENDIX", "content": "The quantitative results in Table II are in agreement with the qualitative feedback discussed in Section III-A. The table shows aggregates for the Raw TLX Score computed from the NASA TLX questionnaire, the SUS Score, and the usage score computed with simple Likert scale statistical analysis (1=Very Unlikely; 5=Very Likely) on the responses to the question \"How likely are you to use this system while driving on the highway?\".\nWe observe a clear trend where systems with larger hold- lengths are determined to be less load inducing (lower TLX) and more usable (higher SUS) in Table II. Our findings are further corroborated by Univariate analysis of ANOVA tests that indicate statistically significant differences in the effect of the hold-length as the independent variable with the RAW TLX Score ($F_{2,144}$ = 51.143, p < 0.01) and the SUS Score ($F_{2,144}$ = 40.456, p < 0.005) as dependent variables, respectively. While the TLX and SUS scores indicate that participants would prefer to use the systems with larger hold- lengths, the usage scores presented correspond to a general \"Neutral\" feeling. Therefore, we conclude that the systems in their current state would not be desirable for use in the real world without enhancements to address driver concerns\u00b9.\nHowever, this low score is only a reflection of the current accuracy of the tested congestion mitigation systems, and not of the concept of CORTA systems as a whole. We envision a better reception for future real-time advisory systems that are developed to address the recommendations presented in this work.\nWe note here that neither the policy type nor the order in which policies were tested were statistically significant determinants for any qualitative metric. Particularly, for the policy type as the independent variable we observed $F_{2,144}$ = 4.179, p > 0.1 and $F_{2,144}$ = 0.504,p > 0.5 with the RAW TLX and SUS Scores as dependent variables, respectively. Additionally, we also observed no significant difference with the order in which the different policies were introduced to participants as the independent variable. The ANOVA tests produced F statistics of $F_{8,144}$ = 0.280, p > 0.9 and $F_{8,144}$ = 0.228, p > 0.9 for the Raw TLX and SUS scores as dependent variables, respectively.\nFormally, PC policies, $\u03c0_{PC}$ : S \u2192 A, process an observed state $s_{t} \u2208 S$ and output an action $a_{t}^{advice} = a_{t} \u2208 A$, such that $q_{t}^{advice} = a_{t}$ = $a_{t+1}$ = ... = $a_{t+\u03b4\u22121}$. The driver is recommended the action $a_{t}^{advice}$ and acts upon it in the vehicle according to their wishes. We refer interested readers to the works by Sridhar and Wu [49], Li et al. [56], Cho et al. [50], Jayawardana and Wu [29], and Hasan et al. [30] for a more in depth discussion on PC policies and their variants\u00b9.\nIn this work, we study cooperative real-time advisory systems"}]}