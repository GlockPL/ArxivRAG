{"title": "CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning", "authors": ["Junghun Oh", "Sungyong Baik", "Kyoung Mu Lee"], "abstract": "Aiming to incrementally learn new classes with only few samples while preserving the knowledge of base (old) classes, few-shot class-incremental learning (FSCIL) faces several challenges, such as over-fitting and catastrophic forgetting. Such a challenging problem is often tackled by fixing a feature extractor trained on base classes to reduce the adverse effects of overfitting and forgetting. Under such formulation, our primary focus is representation learning on base classes to tackle the unique challenge of FSCIL: simultaneously achieving the transferability and the discriminability of the learned representation. Building upon the recent efforts for enhancing transferability, such as promoting the spread of features, we find that trying to secure the spread of features within a more confined feature space enables the learned representation to strike a better balance between transferability and discriminability. Thus, in stark contrast to prior beliefs that the inter-class distance should be maximized, we claim that the closer different classes are, the better for FSCIL. The empirical results and analysis from the perspective of information bottleneck theory justify our simple yet seemingly counter-intuitive representation learning method, raising research questions and suggesting alternative research directions. The code is available here.", "sections": [{"title": "1 Introduction", "content": "Owing to its strong representation power, deep neural networks (DNNs) boast outstanding performance across various fields. However, such feats require tremendous human effort and time to collect an immense amount of data with accurate annotation. The data hunger of DNNs poses a challenge, especially in dynamic real-world environments, where DNNs are required to learn new concepts with few examples while retaining previously learned concepts. To tackle the challenge, few-shot class-incremental learning (FSCIL) [44] aims to design artificial intelligence systems that can learn new classes with few examples while maintaining performance on previously seen classes."}, {"title": "2 Related Works", "content": "Few-Shot Class-Incremental Learning (FSCIL). Towards the development of real-world artificial intelligence systems, Tao et al. [44] have initially introduced few-shot class incremental learning, subsequently fostering numerous studies in the field [1,6,9-11,39,56]. Most of the works [23, 25, 31, 35, 43,50,51,55,57] bypass"}, {"title": "3 Proposed Method", "content": ""}, {"title": "3.1 Background: Problem Formulation", "content": "Following the formulation of few-shot class incremental learning (FSCIL) [44], we assume a sequence of training sessions with the corresponding datasets {$\\mathcal{D}^{(0)}, \\mathcal{D}^{(1)}, ..., \\mathcal{D}^{(T)}$}. $\\mathcal{D}^{(t)}$ consists of training examples $x_i^{(t)}$ with its class labels $y_i^{(t)} \\in \\mathcal{C}^{(t)}$ (for simplicity, we will exclude the superscript), where $\\mathcal{C}^{(t)}$ is the set"}, {"title": "3.2 Background: Baseline", "content": "Let a classification network consist of a feature extractor $f_{\\theta}(\\cdot)$ and a classification layer with its weights $\\phi$. The training objective of the base session is simply the softmax cross-entropy (SCE) loss with the cosine similarity $sim(\\cdot, \\cdot)$ as logits [16]:\n$\\mathcal{L}_{ce} = -\\frac{1}{B} \\sum_{i=1}^{B} \\log\\frac{\\exp(sim(z_i, \\phi_{y_i}))}{\\sum_{j=1}^{C} \\exp(sim(z_i, \\phi_{y_j}))}$                               (1)\nwhere $z_i = f_{\\theta}(x_i)$; $B$ is the batch size and $\\tau$ is the temperature parameter. Incrementally updating weights with few examples in incremental sessions can make the network vulnerable to both catastrophic forgetting and overfitting. To bypass the problems, several works [23,51] suggest minimizing weight updates by freezing the feature extractor after the base session and using feature-average class prototype representation [42]. Specifically, after the base session, trained $\\phi$ is replaced with class prototypes, a process we refer to as classifier replacement (CR), and new-class prototypes are obtained in the subsequent incremental sessions. The $i$-th class prototype is acquired by averaging the features of training samples of the $i$-th class:\n$\\phi_i = \\frac{1}{N_c} \\sum_{(x_i,y_i) \\in \\mathcal{D}^{(0)}} \\mathbb{1}_{[y_i=i]}f_{\\theta}(x_i),$                                                                                                                                (2)\nwhere $N_c$ is the number of training samples associated with the $i$-th class and $\\mathbb{1}[\\cdot]$ indicates 1 if the subscript condition is True and 0 otherwise. For an input $x$, the classification score for the $i$-th class is computed by $sim(f_{\\theta}(x), \\phi_i)$.\nAlthough this baseline bypasses the forgetting and overfitting issues, it heavily relies on the quality of the representation trained solely on the base classes. Consequently, the main focus of this paper is to investigate the important factors that influence representation learning for FSCIL and strategies to improve them."}, {"title": "3.3 Transferability, feature spread, and its adverse effects on FSCIL", "content": "As shown in Fig. 1a, the baseline method exhibits a narrow intra-class distribution, widely perceived as representation collapse [34]. Recent studies [7,24,49] have"}, {"title": "3.4 Inter-Class Distance Matters", "content": "As discussed in Section 3.3, learning shareable features through representation spreading proves advantageous for transferability. Yet, it also harms discriminability in the process of CR. Based on the observation in Fig. 1b, we hypothesize that such a dilemma appears to arise from a large inter-class distance since the representation spreading could push features into the extensive inter-class space, which may dilute the information on the base classes. Furthermore, we argue that the large inter-class distance may impede effective feature sharing among classes, undermining the transferability of learned representations. Consequently, to retain the knowledge on base classes while promoting effective feature sharing, we introduce a novel loss function that minimizes the inter-class distance:\n$\\mathcal{L}_{inter} = \\frac{1}{ \\sum_{i=1}^B\\sum_{j>i}^B \\mathbb{1}[y_i \\neq y_j] } \\sum_{i=1}^B \\sum_{j>i}^B \\mathbb{1}[y_i \\neq y_j] \\cdot sim(z_i, z_j).$         (4)\nFig. 1c displays that the spread of intra-class features is well regulated by applying $\\mathcal{L}_{inter}$ and Fig. 3 demonstrates the performance decline from CR is alleviated by minimizing $\\mathcal{L}_{inter}$ (indicated by the purple line). Moreover, the results indicated by the skyblue line show that reducing inter-class distance improves the performance on the new classes, corroborating our hypothesis.\nOur assertion initially seems counter-intuitive, diverging from the common belief of prior works [23,43,50,54] that maximizing inter-class distance may be"}, {"title": "3.5 Information Bottleneck Theory Perspective", "content": "In this subsection, we provide theoretical support for the proposed representation learning objective in Eq. (6) from the perspective of the information bottleneck (IB) theory [3,40,45]. The IB theory describes the goal of representation learning as finding a good trade-off between complexity and accuracy through finding minimal information from inputs necessary to preserve maximal information about the targets. As discussed by Cui et al. [14], we consider the objective of IB theory to be closely related to learning transferable representations since while achieving the objective, a network could be guided to learn intrinsic knowledge rather than task-irrelevant shortcuts [20], leading to better transferability.\nFor an image classification task, let $X \\in \\mathbb{R}^{h \\times w \\times 3}$, $Y \\in \\mathbb{R}^{C}$, and $Z = \\frac{f_{\\theta}(X)}{||f_{\\theta}(X)||} \\in \\mathbb{R}^d$ denote the input, label, and normalized latent representation variables, respectively, where $h$ and $w$ are the spatial sizes of the image, $C$ is the number of classes, and $d$ is the dimension of the latent representations. The aforementioned trade-off has been formulated as the following objective:\n$\\max I(Y; Z) - \\beta I(X; Z),$                      (7)\nwhere $I( \\cdot; \\cdot)$ indicates the mutual information between two random variables and $\\beta> 0$ is a Lagrange multiplier. In this work, we consider an alternative trade-off objective, $\\max \\frac{I(Y; Z)}{I(X; Z)}$, which is adopted by several works [32,41]. After omitting $\\beta$ for simplicity and modest assumptions, we derive the following lower bound of the IB trade-off objective:\n$\\frac{I(Y; Z)}{I(X; Z)} > 1 - \\frac{\\frac{d \\cdot log(2\\pi e)}{2} + \\frac{1}{C}\\sum_{i=1}^C log|\\Sigma_w|}{\\frac{d \\cdot log(2\\pi e)}{2} + log|\\Sigma_r|},$                                                  (8)\nwhere $\\Sigma_w$, and $\\Sigma_r$ indicate the covariance matrices of $Z$ within the $i$-th class and overall classes, respectively. As proven by Lemma S1, both the numerator and denominator in the fractional term of the lower bound in Eq. (8) are negative, leading to the following theorem:"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Details", "content": "Dataset. Following the benchmark settings proposed by Tao et al. [44], we evaluate the proposed method on CIFAR100 [29], miniImageNet [46], and CUB200 [47]. For CIFAR100 and miniImageNet, the total number of classes is 100: 60 base classes and 40 new classes. The 40 new classes are split into 8 disjoint sets of 5 classes, each of which is sequentially provided with 5 training examples per class (5-way 5-shot) in each incremental session. As for CUB200, there total number of classes is 200, with 100 base classes and 100 new classes. 100 new classes are split into 10 disjoint sets of 10 classes, each of which is sequentially provided with 5 training examples per class (10-way 5-shot) in each incremental session.\nImplementation. Following Zhang et al. [51], we use ResNet-20 [22] for CIFAR100 experiments and ResNet-18 [22] for both miniImageNet and CUB200 experiments. We follow the conventions to use the ResNet-18 model pre-trained on the ImageNet dataset [38] for CUB200. We set the mini-batch size to 128, 128, and 256 for CIFAR100, miniImageNet, and CUB200 experiments, respectively. The temperature parameter $\\tau$ for the baseline method is 1/16 and 'low temperature' indicates $\\tau = 1/32$. We set $A_{ssc}$ as 0.1,0.1, and 0.01 for CIFAR100, miniImageNet, and CUB200, respectively, and $A_{inter}$ as 1, 0.5, and 1.5 for CIFAR100, miniImageNet, and CUB200, respectively. These hyper-parameters are searched via validation using synthesized validation sets. For mutual information estimation, we adopt MINE [4] method and implement it based on the open-source code\u00b2. More details are provided in Section S5.\nEvaluation. We use the accuracy on base (AB), new (AN), and the whole classes (Aw) as metrics to assess the discriminability, transferability, and the trade-off between them in learned representations. Additionally, we use the performance drop (PD) between the accuracy at the end of the base session (session 0) and"}, {"title": "4.2 Comparison with the Existing Works", "content": "We compare the proposed method, dubbed CLOSER, with prior arts on CUB200 (Table 1), CIFAR100 (Table 2), and miniImageNet (Table 3). We observe that CLOSER achieves state-of-the-art performance on both CUB200 and CIFAR100 datasets, surpassing the results of previous methods by a large margin with respect to Aw and PD. With miniImageNet, the proposed method exhibits substantially higher Aw than the method with the lowest PD and achieves lower PD than the method with the highest Aw, which means that the proposed method achieves a better balance between the performance on base and the new classes. It is worth noting that CLOSER shows such outstanding performance without any assistance from the storage of previous samples (F2M, ERDIL, IDLVQ-C, and CABD), additional computational modules (CEC, CLOM, NC-FSCIL, SAVC, and MetaFSCIL), and test-time data augmentation (S3C and SAVC), suggesting the critical importance of learning effective representations in FSCIL."}, {"title": "4.3 Ablation Studies", "content": "To verify the efficacy of the individual components in the proposed method, we perform ablation studies, which are shown in Table 4. The increase in AN when employing a lower temperature in the softmax cross-entropy loss (low T) or a self-supervised contrastive loss ($\\mathcal{L}_{ssc}$) confirms the advantage of feature spread in enhancing the transferability of representations. While transferability can be greatly improved by using low $\\tau$ and $\\mathcal{L}_{ssc}$, a substantial decline in base-class performance AB is observed, as discussed in Section 3.4. The result from the case where all components are utilized demonstrates that this issue can be effectively resolved by minimizing inter-class distance. Furthermore, as discussed in Section 3.4, reducing inter-class distance is observed to improve transferability, especially when used with the representation spreading methods. The comprehensive results of ablation studies confirm and support our claims."}, {"title": "4.4 Information Bottleneck trade-off Analysis", "content": "In Section 3.5, we show the connection between the proposed objective function and the information bottleneck (IB) trade-off. To validate our analysis, we measure the mutual information between representations and inputs $I(X; Z)$ and the mutual information between representations and targets $I(Y; Z)$ for the baseline, representation spread approach, and our method CLOSER, as shown in Fig. 5. In the figure, the more left (lower $I(X; Z)$) and upper (higher $I(Y; Z)$) regions imply better IB trade-off. Our proposed method CLOSER demonstrates to have found a better IB trade-off, especially when considering whole classes, including base and new classes. The results are encouraging in that CLOSER is able to find a better IB trade-off for all classes, when the feature extractor is only trained on base classes and fixed afterwards. The results demonstrate that CLOSER is effective in tackling a particularly difficult challenge of FSCIL: learning transferable and discriminative features from base classes."}, {"title": "4.5 T-SNE Analysis", "content": "For qualitative evaluation, we visualize the learned representation trained by different configurations of the proposed losses, which is illustrated in Fig. 6. The value of $T(f_{\\theta})$ in the right below in each figure is for measuring the transferability of representation. The baseline representation (a) shows the characteristics of the base class, represented by the features of the new classes mapped to those of the base classes, also indicated by the low value of $T(f_{\\theta})$. Spreading of features (b) largely resolves the overfitting issues, exhibited by larger distances between new classes and base classes; i.e., the increase in $T(f_{\\theta})$. Finally, $\\mathcal{L}_{inter}$ is used to compensate for the decreased performance on the base classes due to the spread representation. Reducing inter-class distance also enhances the separability between the new class samples and the clusters of base classes, as evidenced by the further increase in $T(f_{\\theta})$."}, {"title": "5 Limitations and Discussions", "content": "Our work focuses on learning representation that is discriminative yet transferable to unseen classes to tackle the challenges of FSCIL. As such, our work does not consider an option of updating representation with new classes. Continual update of representation can improve the performance on new classes, however at the cost of sacrificing the old-class performance. Thus, it is a question of stability-plasticity dilemma, where our method focuses more on stability while improving plasticity through discriminative and transferable representation. Furthermore, similar to other works on FSCIL, our method is limited to classification tasks. But, we believe our work can have an impact on other domains, akin to the impact of representation spread methods, such as contrastive learning."}, {"title": "6 Conclusion", "content": "To tackle convoluted challenges in few-shot class-incremental learning (FSCIL), we focus on representation learning, which plays a crucial role. In contrast to previous FSCIL methods that have focused on maximizing the distance between classes, our experimental and theoretic analysis suggests that the closer classes are, the better for FSCIL, especially when feature sharing is encouraged between classes. Upon the analysis, we propose a simple, yet seemingly counter-intuitive idea: bring classes closer for a better transferability-discriminability Pareto front. Our experimental results and information-bottleneck-theory-based analysis suggest that our work can provide a promising research avenue. As such, we hope that our work will inspire future works and discussions in this research direction."}, {"title": "S1 Proof of Theorem 1", "content": "In this section, we present the proof of Theorem 1 of the main manuscript.\nFor an image classification task, let $X \\in \\mathbb{R}^{h \\times w \\times 3}$ and $Y \\in \\mathbb{R}^{C}$ denote the input and label, respectively. Our goal is to train an encoder with parameters $\\theta$, $f_{\\theta} : \\mathbb{R}^{h \\times w \\times 3} \\to \\mathbb{R}^{d}$. The latent representation $Z$ is obtained by normalizing the network embedding, i.e. $Z = \\frac{f_{\\theta}(X)}{ \\lVert f_{\\theta}(X) \\rVert } \\in S_d$ where $S_d$ denotes the surface of the $d$-dimensional unit hypersphere. Let $\\Sigma_w$, and $\\Sigma_r$ denote the covariance matrices of representations within the $i$-th class and whole classes, respectively.\nWe consider the trade-off objective of information-bottleneck (IB) theory as solving $\\max \\frac{I(Y; Z)}{I(X; Z)}$, where $I( \\cdot; \\cdot)$ denotes the mutual information between two variables and $\\beta > 0$, as discussed in the main manuscript. After omitting $\\beta$ for simplicity and modest assumptions, we prove the Theorem 1 as follows.\nProof. Let $H(\\cdot)$ denote a differential entropy of a continuous random variable. Then, $I(Y; Z) = H(Z) - H(Z|Y)$ and $I(X; Z) = H(Z) - H(Z|X)$. Since $f_{\\theta}$ is deterministic and there are finite examples in the dataset, $I(X; Z) = H(Z)$ [14]. We then obtain:\n$\\frac{I(Y; Z)}{I(X; Z)} = \\frac{H(Z) - H(Z|Y)}{H(Z)} = 1 - \\frac{H(Z|Y)}{H(Z)}$ (S1)\nBy representing $H(Z|Y)$ as the weighted sum of the entropy of $Z$ conditioned on each possible value of $Y$, we derive\n$\\frac{I(Y; Z)}{I(X; Z)} = 1 - \\frac{\\sum_{i=1}^{C} P(Y = y_i)H(Z|Y = y_i)}{H(Z)}$ (S2)\nwhere $y_i \\in \\mathbb{R}^{C}$ is a one-hot vector containing a single 1 in the $i$-th element, hence denoting the label of $i$-th class. Using the property of differential entropy [12], we obtain the following inequality on $H(Z)$:\n$H(Z) \\leq \\frac{d}{2} \\log(2 \\pi e) + \\frac{1}{2} \\log \\vert \\Sigma_r \\vert .$ (S3)\nBy assuming that the representation distribution of each class follows a multi-variate Gaussian distribution, the following equality holds for each $H(Z|Y = y_i)$:\n$H(Z|Y = y_i) = \\frac{d}{2} \\log(2 \\pi e) + \\frac{1}{2} \\log \\vert \\Sigma_{w_i} \\vert , \\qquad i = \\{1,2,..., C\\}.$ (S4)"}, {"title": "S2 Generalization Ability of CLOSER", "content": "In this section, we examine the generalization-ability of CLOSER concerning both dataset and architecture. To validate the effectiveness of CLOSER on a"}, {"title": "S3 Comparison with prior works on quality of learned representation", "content": "In addition to the comparisons in Tables 1 to 3, we compare against the mentioned previous representation-learning-based few-shot class incremental learning works [43,58], with respect to the quality of learned representations. Specifically, we quantify the quality of representations using $T(f_{\\theta})$ (defined in Eq. (5) of the main manuscript) and the accuracy on new (AN), base (AB), and whole classes (Aw). We measure $T(f_{\\theta})$ to quantify how the representations of new classes are distinguishable from those of base classes. The results presented in Table S3 indicate that the representations obtained by previous works display relatively high AB but relatively low values for $T(f_{\\theta})$ and An, indicating their lack of transferability of learned representation. By contrast, our CLOSER achieves the comparable AB and the highest $T(f_{\\theta})$ and AN, indicating that CLOSER can yield representations with better trade-off between discriminability on base classes and transferability to new classes. Although the prior works attempt to improve the trade-off between discriminability and transferability of the learned representation, they still rely on enlarging inter-class distance [43] or the spread of representation via negative class margin [58]. Thus, these results indicate that reducing inter-class distance is significantly effective for striking a better balance between discriminability and transferability."}, {"title": "S4 Analysis on Class Margin", "content": "In this section, we compare the effects of the class margin parameter (m) and the temperature parameter ($\\tau$) in the softmax cross-entropy loss on representation learning in the context of FSCIL. The results in Fig. S1 show the accuracy on the whole (Aw), base (AB), and new classes (AN) at the end of all training sessions with varying margin and temperature values. As noted in the previous works [28, 30, 58], when the margin and temperature decrease, An tends to increase, while AB tends to decrease (except the case when m = -0.2 and $\\tau$ = 1/32 due to unstable training). However, we find that the impact of the margin becomes marginal when the temperature is high. For example, when $\\tau$ = 1/8, the difference between the highest and lowest An is roughly 3%, a relatively minor variation compared to the approximately 9% observed with a"}, {"title": "S5 Implementation Details", "content": "Self-Supervised Contrastive Learning. For self-supervised contrastive learning (SSCL) discussed in Section 3.3, we generate different views for each image in a mini-batch via data augmentation. For both CIFAR100 and miniImageNet experiments, we apply random resized cropping, random horizontal flipping with probability 0.5, and random AutoAugment [13] with probability 0.5. For CUB200 experiments, we apply the random resized cropping and the random horizontal flipping with probability 0.5 but without AutoAugment since color information is crucial for fine-grained classification of the CUB200 dataset. Unlike the previous methods on SSCL [8,21], we do not use either a non-linear projection head or a momentum encoder since we found that the performance difference is marginal.\nOptimization. For optimization, we adhere to standard protocols from previous works [51]. We use the stochastic gradient descent optimizer with weight decay of 5$\\cdot$10$^{-4}$ and Nesterov momentum 0.9. We set the initial learning rate as 0.1, 0.1, and 0.005 for CIFAR100, miniImageNet, and CUB200 experiments, respectively, and decay them by 0.1 at the 80% and 90% of the total training epochs. We"}]}