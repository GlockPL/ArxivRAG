{"title": "A Comparative Study of DSPy Teleprompter Algorithms for Aligning Large Language Models Evaluation Metrics to Human Evaluation", "authors": ["Bhaskarjit Sarmah", "Sachin Tiwari", "Kriti Dutta", "Stefano Pasquali", "Anna Grigoryan", "Dhagash Mehta"], "abstract": "We argue that the Declarative Self-improving Python (DSPy) optimizers are a way to align the large language model (LLM) prompts and their evaluations to the human annotations. We present a comparative analysis of five teleprompter algorithms, namely, Cooperative Prompt Optimization (COPRO), Multi-Stage Instruction Prompt Optimization (MIPRO), BootstrapFewShot, BootstrapFewShot with Optuna, and K-Nearest Neighbor Few Shot, within the DSPy framework with respect to their ability to align with human evaluations. As a concrete example, we focus on optimizing the prompt to align hallucination detection (using LLM as a judge) to human annotated ground truth labels for a publicly available benchmark dataset. Our experiments demonstrate that optimized prompts can outperform various benchmark methods to detect hallucination, and certain telemprompters outperform the others in at least these experiments.", "sections": [{"title": "INTRODUCTION", "content": "In the rapidly evolving field of generative artificial intelligence (GenAI), and more specifically large language models (LLMs), optimizing the input instructions to the LLMs [1], also called 'prompts', has become one of the crucial areas of research and development [2]. Optimizing prompts directly impacts the quality, accuracy, and efficiency of outputs generated by the LLMs; ensures that they align with task-specific requirements and data constraints; and, reduces reliance on manual trial-and-error approaches.\nWe note that a similar problem is encountered even in traditional machine learning paradigm with tabular data, where certain datasets often come with expert-given labels, but their alignment with unsupervised clustering outcomes can vary depending on feature selection and distance metrics [3]: if human-annotated ground-truth labels are available for the training dataset and an alignment with these labels is desired, then a supervised or semi-supervised learning approach is required rather than an unsupervised approach that does not use the ground-truth labels at all during the training.\nIn Refs. [4, 5], a post hoc approach to align human-annotated ground-truth labels to a corresponding LLM evaluation metric proposed where once the chosen LLM evaluation metric is computed on all the training samples, another linear or non-linear model is trained to learn the mapping between the LLM-computed scores and the human-annotated labels. In the present work, we propose to optimize the LLM as a judge prompt itself with respect to the ground-truth labels.\nDeclarative Self-improving Python (DSPy) [6] is a recently proposed framework that abstracts LLM pipelines into declarative modules which enable systematic optimization of these pipelines with respect to a chosen objective (e.g., accuracy of reproducing a given target variable). DSPy introduces a set of prompt optimizers, called teleprompters, which systematically refine prompts with the objective being maximizing the performance of the LLM on the given specific tasks and datasets. In other words, these teleprompters automate the process of experimenting with prompt variations and evaluating their effectiveness against predefined metrics on the given training dataset and roughly mimic the supervised learning process as in the traditional machine learning paradigm.\nWe emphasis that DSPy and model fine-tuning [7, 8] are alternative approaches to optimizing LLMs for a given task. DSPy focuses on optimizing prompts and pipeline configurations through declarative modules and automated teleprompters without modifying the pre-trained LLM's weights. Because the weight parameters of the model are not tuned, this approach is relatively faster and cost-effective as it requires less computational resources. On the other hand, fine-tuning involves updating the LLM's weights using task-specific training data and may yield more robust performance for narrowly defined applications but is usually computationally more intensive. It also requires significant amount of labeled data, and may sometimes lead to over-fitting. In the present work, we focus only on teleprompters for prompt optimization, and the fine-tuning and combination between the two will be discussed in a future work.\nA key innovation of teleprompters lies in their ability to leverage few-shot prompting [2] which is a prompting technique where a prompt is supplemented with a few example pairs of inputs and desired outputs to guide the LLM's response. Few-shot prompting has been shown to substantially improve LLM performance across diverse tasks by enabling models to better align their outputs with task-specific requirements. Teleprompters automate the selection and incorporation of high-quality examples, further refining the optimization process to produce more accurate and contextually aligned outputs."}, {"title": "DSPY TELEPROMPTERS", "content": "DSPy[6] is a programming model that is designed to optimize the prompts in a more structured (specifically, programmatic) and efficient manner. To set up some terminologies, let's first define modules as small units or building blocks of an abstracted LLM pipeline and are reusable components that may be used for different tasks. A signatures is a short declarative specification that informs what a text transformation module should take as an input and produce as an output (e.g., the signature \"question -> answer\" means that the input will be a question in the text format and output will be an answer in the text format).\nThen, a module called predictor DSPy module that generates an output from an input based on a given signature that uses an LLM to \"predict\" the answer (e.g., for a given question, it will predict an answer using an LLM provided the signature is \"question -> answer\"). Similarly, the adapter module transforms the input to a desired format that is suitable for a different kind of module or a step in the pipeline. This module is used for tasks like formatting, changing data types, adding additional information, etc. One can also device modules such as ChainOfThought that takes a question, thinks step-by-step, and gives an answer using an LLM. One can also such a module with another module, for example, ChainOfThought can explain the reasoning before giving an answer from another module.\nAn assertions is a condition (or, a set of conditions) that must be true during pipeline execution (e.g., ensuring that an output is not empty). A metric is an objective evaluation metric (e.g., exact match between two sentences) to quantitatively measure the performance of the LLM for the given data for the given task. To optimize a given module, modules or the entire pipeline, DSPy uses an optimizer that tunes parameters such as which examples are used for training or which prompts work best.\nA special type of optimizer in DSPy is called a teleprompter which is an optimizer that further improves the quality of modules via prompting or fine-tuning. It follows the below steps to optimize the modules:\n(1) Candidate Generation: As the first step, the teleprompter starts by finding all instances of the Predictor modules in a given DSPy program and generates potential candidates for parameters (e.g., prompt templates, instructions, field descriptions, demonstrations for input-output examples, etc.) using simulation or sampling from a teacher model to create new examples. E.g., for a module with a signature \"question -> answer\", the teleprompter will run the module multiple times with different prompts and gather the output as potential examples.\n(2) Parameter Optimization: Now, the teleprompter optimizes the candidate parameters using methods such as random search or hyperparameter tuning, and selects the best combination of parameters by running trials and evaluating based on a specific metric. This stage might also involve fine-tuning a smaller language model based on the gathered examples to ensure a better fit. E.g., the teleprompter might evaluate different prompt examples with an accuracy metric to see which set provides the highest accuracy for answering questions. If fine-tuning is being used, it might update the language model weights with the best examples to improve consistency.\n(3) Higher-Order Program Optimization: Once the optimized module parameters are obtained, the teleprompter may further optimize the pipeline by changing its overall structure by, for example, combining multiple copies of the optimized program into an ensemble. This stage may also introduce advanced strategies such as automatic backtracking or conditional logic to improve the robustness and adaptability of the pipeline. E.g., if multiple versions of a question-answering pipeline exist, the teleprompter could create an ensemble of these versions that then aggregates their outputs to get a more reliable answer.\nIn the present study, we employ a systematic approach to analyze the performance of the five DSPy teleprompter algorithms that are available in the literature along with publicly available implementations: COPRO, MIPRO, BootstrapFewShot, BootstrapFewShot with Optuna, and KNN Few Shot. Below is a brief description of each algorithm:"}, {"title": "DATASETS", "content": "We use the recently publicly released HaluBench\u00b9 dataset to evaluate the performance of all DSPy teleprompter algorithms. HaluBench is an open-source hallucination evaluation benchmark dataset comprising around 15K Context-Question-Answer triplets as well as human-annotated ground-truth labels 'Pass' (i.e., not hallucinated) and 'Fail' (hallucinated). It is the first benchmark to include hallucination tasks from real-world domains like finance, medicine, etc. The dataset was constructed by sourcing examples from publicly available datasets, including FinanceBench, PubmedQA, CovidQA, HaluEval, DROP, and RAGTruth, which we call sub-datasets. By providing a diverse set of real-world scenarios, HaluBench enables comprehensive evaluation of hallucination detection in language models.\nFor our purposes, the presence of context and ground truth answers is crucial as it allows for a more accurate assessment of their optimization capabilities. The RAG system (which is not available in the dataset though) used to generate the answers in this dataset may leverage the provided context to generate answers, enabling a comprehensive evaluation of the algorithms' performance in a controlled environment."}, {"title": "METHODOLOGY AND COMPUTATIONAL DETAILS", "content": "In this Section, we describe the methodology of our experiments and also provide computational details for reproducibility purposes.\n4.1 Dataset Preparation\nTo ensure high-quality data for evaluating faithfulness, we applied several data-cleaning and sampling steps across six sub-datasets: CovidQA, FinanceBench, HaluEval, PubMedQA, DROP, and RAGTruth.\n4.1.1 Data Cleaning. As part of data cleaning, we removed any sample where the ground-truth answer contained three or fewer tokens, as these samples were deemed insufficient for meaningful evaluation (e.g., extracting 'statements' or 'claims', that is required by many methodologies faithfulness scores, from such short answers is not feasible nor meaningful in most cases.). This filtering step reduced the dataset to a total of 9,616 samples, ensuring that the retained samples contained substantive responses for a more accurate assessment of model faithfulness.\nDue to computational limitations, we further reduced the dataset by applying stratified sampling, ensuring proportional representation across both the six sub-datasets and the binary classes. Eventually, we focused on a total of 1,500 samples, distributed into three subsets: training set with 750 samples, validation Set with 375 samples and test set with 375 samples. Stratification also ensured that each sub-dataset and both target classes were proportionally represented in all three subsets. This approach preserved the original data distribution while maintaining fairness across datasets and binary classes, supporting robust and consistent evaluation across all stages of our experiments.\nInspired by the prompt format proposed in Ref. [9], we adapted the input structure to exclude the QUESTION component, focusing solely on the DOCUMENT (i.e., context) and ANSWER. This was done to evaluate faithfulness independently of question semantics.\n4.2 DSPy Optimizers and Configurations\nWe utilized various DSPy optimizers\u00b2, each with tailored hyperparameters for faithfulness evaluation tasks as summarized in Table 1.\n4.3 Experimental Workflow\nWe conducted experiments using OpenAI's GPT-40 model as the baseline LLM. To ensure experimental consistency and reproducibility, the following hyperparameters were fixed across all evaluations:"}, {"title": "RESULTS", "content": "The effectiveness of DSPy optimizers was evaluated in improving hallucination detection (as annotated in the ground-truth labels in the HaluBench dataset) using OpenAI's GPT-40 model as a base evaluator. We report two sets of results: aggregated metrics across all datasets, highlighting overall performance; and, detailed dataset evaluations are presented in Appendix A, where we explore individual dataset performance and specific trends. The primary evaluation metrics are Micro, Macro, and Weighted F1 scores, which measure the ability of models to balance overall accuracy, minority class detection, and proportional representation. Additionally, accuracy is reported as it was used as the objective for training the optimizers.\nset.\nTables 3 and 4 present the aggregate level results for the hold-out test\n5.1 Key Insights and Trends\nBaseline GPT-40 demonstrated strong overall performance but faced challenges in maintaining class-level balance, as evidenced by its low Macro F1 scores. In contrast, the best-performing approaches showcased remarkable"}, {"title": "RESULTS DISCUSSION", "content": "The results demonstrate that while baseline GPT-40 achieves reasonable performance, it struggles with class imbalance, as evidenced by its low macro F1 scores. This underscores the need for tailored optimization frameworks to improve sensitivity to minority classes.\nClass-Level Balance: Optimizers such as Bootstrap Few Shot with Random Search and with Optuna improved minority class detection by balancing class-specific F1 scores. Bootstrap Few Shot with Random Search was particularly effective due to its exploration of diverse prompt configurations.\nBatch and Structured Learning: MIPROv2 achieved the best Weighted F1 score by leveraging structured learning techniques and batching. This method balanced both class-specific sensitivity and global accuracy.\nPrompt-Specific Adaptations: COPRO and KNN Few Shot struggled with datasets like FinanceBench, where prompt-specific adaptations were less effective due to the dataset's variable structure.\nStructured Datasets: Optimizers excelled in datasets with well-defined contexts, such as CovidQA and PubMedQA, achieving high Macro F1 scores due to clear prompt-document-answer alignment.\nUnstructured Datasets: Datasets like DROP and FinanceBench presented more complex inputs, where hallucination detection proved more challenging due to fragmented context and high answer variability.\nTeleprompter Optimization Analysis: During teleprompter optimization, the DSPy routines select examples used to guide the model's output based on maximizing the overall evaluation metric, specifically exact match across the entire dataset. However, the specific dataset at hand in the present work consists of multiple heterogeneous sub-datasets arising from diverse sources. While optimizing on the overall dataset produced strong aggregate results, a detailed analysis of data source-level performance revealed inconsistencies. In particular, some data sources exhibited significantly lower performance, despite high aggregate evaluation scores. i.e., the teleprompters optimized for maximum overall performance may rely heavily on examples from only a subset of the data sources, specifically those where the model already performed well.\nSince the optimization process focuses on selecting the most impactful examples to increase the evaluation score, this process may inadvertently overfit to high-performing data sources while neglecting examples from lower-performing sources. As a result, the optimized prompts may fail to generalize effectively across all data sources such as FinanceBench sub-dataset in our case.\nTo address this limitation, we propose using a stratified sampling approach during teleprompter optimization in the future. This approach would ensure that examples selected for prompt optimization are proportionally sampled from each data source, rather than solely focusing on maximizing the overall evaluation metric. By ensuring that each data source contributes representative examples, the teleprompters could be made more robust, improving generalization and reducing bias toward specific data types. Instead of selecting only the \"best\" examples that maximize the overall performance, the optimization process should prioritize diversity by ensuring that examples from underrepresented or challenging data sources are also included.\nPotential Model Bias due to Public Data Exposure: An additional consideration relates to the model's observed performance on publicly available datasets. While we cannot definitively determine GPT-4's pretraining data,"}, {"title": "CONCLUSION", "content": "This study systematically evaluated DSPy optimizers for improving hallucination detection using GPT-40 as a base evaluator. Our experiments yield that baseline GPT-40 delivers strong overall performance but exhibits limitations in sensitivity to minority classes, as evidenced by its low Macro F1 scores. This shortfall highlights its difficulty in achieving balanced performance across all categories, particularly when addressing nuanced or less frequent patterns. These challenges underscore the need for targeted optimization to improve class-level balance.\nOptimizers such as Bootstrap Few Shot Random Search and MIPROv2 effectively address these limitations, demonstrating significant advancements in hallucination detection. Their performance is particularly pronounced in structured datasets, where clear patterns enable optimization techniques to achieve robust results. However, the variability and complexity of unstructured datasets present ongoing challenges, emphasizing the critical role of dataset structure in determining the success of optimization strategies.\nThe study highlights the need for adaptive optimizers, dynamic prompt generation, and dataset-specific evaluation frameworks and demonstrates the importance of combining optimization with advanced techniques like instruction fine-tuning to enhance hallucination detection across diverse datasets.\nBuilding on these findings, future research should prioritize the combined use of instruction fine-tuning and prompt optimization to enhance language models' adaptability across diverse datasets. This integrated approach aligns model outputs more closely with specific tasks, leading to improved performance in both structured and unstructured contexts [15]. Moreover, an optimized prompting strategy for hallucination detection and other metrics may provide cleaner distributions of the corresponding scores with respect to the ground-truth labels and, in turn, crisper thresholds for the corresponding evaluation metrics [5].\nBeyond hallucination detection, the DSPy techniques can also scale to address other critical evaluation tasks, such as reasoning, toxicity, and bias assessment. Extending these methods will contribute to the broader goal of enhancing LLM reliability and robustness, ensuring their effectiveness across a wider range of applications. By addressing these areas, future advancements can significantly improve both the adaptability and trustworthiness of language models in diverse operational settings."}]}