{"title": "Towards Explainable Goal Recognition Using Weight of Evidence (WoE): A Human-Centered Approach", "authors": ["Abeer Alshehri", "Amal Abdulrahman", "Hajar Alamri", "Tim Miller", "Mor Vered"], "abstract": "Goal recognition (GR) involves inferring an agent's unobserved goal from a sequence of observations. This is a critical problem in AI with diverse applications. Traditionally, GR has been addressed using 'inference to the best explanation' or abduction, where hypotheses about the agent's goals are generated as the most plausible explanations for observed behavior. Alternatively, some approaches enhance interpretability by ensuring that an agent's behavior aligns with an observer's expectations or by making the reasoning behind decisions more transparent. In this work, we tackle a different challenge: explaining the GR process in a way that is comprehensible to humans. We introduce and evaluate an explainable model for goal recognition (GR) agents, grounded in the theoretical framework and cognitive processes underlying human behavior explanation. Drawing on insights from two human-agent studies, we propose a conceptual framework for human-centered explanations of GR. Using this framework, we develop the eXplainable Goal Recognition (XGR) model, which generates explanations for both why and why not questions. We evaluate the model computationally across eight GR benchmarks and through three user studies. The first study assesses the efficiency of generating human-like explanations within the Sokoban game domain, the second examines perceived explainability in the same domain, and the third evaluates the model's effectiveness in aiding decision-making in illegal fishing detection. Results demonstrate that the XGR model significantly enhances user understanding, trust, and decision-making compared to baseline models, underscoring its potential to improve human-agent collaboration.", "sections": [{"title": "1. Introduction", "content": "Goal Recognition (GR) is the problem of predicting an agent's intent by observing its behavior. The task of GR has numerous potential and practical applications, such as smart homes (Hegde & Kenchannavar, 2019) and workplace safety (Inam, Raizer, Hata, Souza, Forsman, Cao, & Wang, 2018), among others (Wayllace, Ha, Han, Hu, Monadjemi, Yeoh, & Ottley, 2020; Singh, Miller, Newn, Velloso, Vetere, & Sonenberg, 2020). Research on GR uses different inference techniques to predict the ultimate goals of the agents being observed. It advances with increasingly complex domain models and better approaches. However, understanding and fostering human trust in these systems is challenging due to their lack of explainability. This becomes particularly crucial in safety-critical applications like social care, military planning, and medical support, where the system's decisions can have major consequences. Systems must be capable of explaining the decisions made and communicating their decisions in a way that is understandable to people (Masters & Vered, 2021; Meneguzzi & Pereira, 2021; Van-Horenbeke & Peer, 2021).\nThe vast majority of work has focused on improving the explicability of agent behavior (Yolanda, R-Moreno, Smith, et al., 2015; Sohrabi, Riabov, & Udrea, 2016; Vered, Kaminka, & Biham, 2016; Hu, Xu, Subagdja, Tan, & Yin, 2021; Hanna, Rahman, Fosong, Eiras, Dobre, Redford, Ramamoorthy, & Albrecht, 2021). This typically involves making the behavior more understandable to observers, either by aligning it with their expectations or ensuring the interpretability of the inference process. In shifting the focus from interpretability to justification, the goal is to develop an explainable GR agent that provides context and rationale for each predicted goal, rather than merely making the inference process interpretable.\nThis paper contributes to the ongoing work in eXplainable AI (XAI) by developing an explainable GR model from a human-centered perspective. Studies in social psychology indicate that people use the same conceptual framework that they apply to humans to explain artificial agents' behavior, and they also expect artificial agents to adopt this framework (De Graaf & Malle, 2017). Therefore, our approach is to provide comprehensible explanations that align with how people rationalize outcomes and interpret information. This effort aims to bridge the gap between mere prediction and understandable explanation.\nThere are two main contributions of this work. First, we propose a conceptual framework for the explanation process in GR tasks and identify key concepts people use when reasoning about goal prediction. Using a bottom-up approach, the framework is derived from an analysis of human explanations of recognition tasks. We study this in two different domains to increase the generalizability of our model: Sokoban and StarCraft games. We examine the frequency, sequence, and relationships between the basic components of these explanations. Using the thematic analysis process, we identified 11 concepts from 864 explanations of agents operating in various scenarios. Incorporating insights from the folk theory of mind and behavior (Malle, 2006b), we propose a human-centered model for GR explanations.\nOur second contribution is building an eXplainable Goal Recognition (XGR) model based on the proposed conceptual framework. The model generates explanations for GR agents using the information theory concept of Weight of Evidence (WoE) (Good 1985; Melis et al. 2021). We define the problem of explanation selection using the main concept from our conceptual model, which we call an observational marker, i.e., the observation with"}, {"title": "2. Related Work", "content": "Goal recognition (GR) involves identifying an agent's unobserved goal based on a sequence of observations. Various approaches exist to address the GR problem. Common methods include library-based GR algorithms, which use specialized plan recognition libraries to represent all known methods for achieving known goals (Sukthankar, Geib, Bui, Pynadath, & Goldman, 2014); model-based GR algorithms (Ram\u00edrez & Geffner, 2010; Sohrabi et al., 2016; Vered et al., 2016), where GR agents leverage domain knowledge through planners to generate the necessary plans for achieving a goal (Masters & Vered, 2021); and machine learning GR approaches that rely on large training datasets from which algorithms learn domain constraints (Min, Ha, Rowe, Mott, & Lester, 2014; Pereira, Vered, Meneguzzi, & Ram\u00edrez, 2019; Meneguzzi & Pereira, 2021; Fitzpatrick, Lipovetzky, Papasimeon, Ramirez, & Vered, 2021). Well-established algorithms have shown high performance in labeling action sequences with corresponding goals (Ram\u00edrez & Geffner, 2010; Vered et al., 2018; Pereira, Oren, & Meneguzzi, 2020), yet explaining why the algorithm arrived at a particular conclusion remains under-explored.\nPrior work has focused on explaining GR in the form of answering the question: what goal is the agent trying to achieve? A long line of work has suggested explaining goal inference, which is a form of 'inference to the best explanation', also called abduction (Van Rooij, Haselager, & Bekkering, 2008; Baker, Goodman, & Tenenbaum, 2008; Baker,"}, {"title": "2.2 Human behavior explanation", "content": "We outline work on social attribution, which defines how people attribute and explain others' behavior. Social attribution focuses not on the actual causes of human behavior but"}, {"title": "3. Human-Agent Study: Insights from Human Explanation", "content": "In this section, we present our conceptual model for explaining GR, grounded on empirical data from two different human-agent studies."}, {"title": "3.1 Study Objective", "content": "The goal of this study is to investigate how humans explain a GR agent's behavior and identify the key concepts present in such explanations, their frequency, and the relationships among these concepts. Based on our findings, we construct a conceptual framework for GR explanation. We have two case studies with different scenarios and assumptions. The first case study is set in a general domain where no specific expertise is required, such as the Sokoban domain, a classic puzzle game where a player pushes boxes to designated storage locations within a grid. In contrast, the second case study involves explanations provided by domain experts, as seen in the StarCraft domain, a complex real-time strategy game that requires strategic planning, resource management, and tactical combat."}, {"title": "3.2 Goal Markov Decision Process (Goal MDP)", "content": "For both case studies, we employ the Goal Markov Decision Process (Goal MDP) framework to capture an observer's view of the world. A Goal MDP (Ramirez & Geffner, 2011) represents the possible actions that can be taken and the causal relationships of their effects on the world's states. Formally, it is defined as a tuple II = (S, SG, A, P, C), where S is a non-empty state space, S\u00e7 is a non-empty set of goal states, A is a set of actions, Pa(s' | s) is the probability of transitioning from state s to state s' given action a, and C(s, a, s') is the cost of that transition. The solution to a Goal MDP is a policy \u03c0 : S \u2192 A that maps states to actions with an overall minimum expected cost.\nWe describe an observer's worldview as a Goal MDP. Our description is based on the following assumptions: (1) the observer perceives the world as a finite set of discrete states and actions; (2) the observer interprets transitions between states in a deterministic manner, while the probability reflects the observer's confidence or uncertainty about the transition; (3) the observer values actions based on costs, aiming to minimize them over time; (4) the observer's reasoning is based on their internal world state representation, which may not necessarily match the actual state observed by the player/agent; (5) the player's preferences are not observable, meaning the observer must infer information and rely on observable actions to make judgments; and (6) the observer is assumed to have full observability, meaning they have access to the complete state of the environment."}, {"title": "3.3 Case Study 1: Sokoban Game", "content": "Sokoban is a classic puzzle game (Figure 2) set in a warehouse environment, where the player or agent navigates through a grid-like layout to move boxes onto designated storage locations. The objective is that each box must be pushed, one at a time, to its assigned spot. The challenge lies in navigating movement constraints and spatial limitations; boxes can only be pushed into empty spaces and cannot be pulled or pushed against walls or other boxes. We modified the Sokoban game rules to allow the player to push multiple boxes simultaneously. This modification transforms the game from a straightforward navigational task into a strategic challenge with multiple objectives, where the player aims to minimize the number of steps taken. We used a STRIPS-like discrete planner to generate plan hypotheses derived from the domain theory and observations as our ground truth."}, {"title": "3.3.1 STUDY DESIGN", "content": "We defined the observer's worldview within the framework of Goal MDPs as follows:\n\u2022 State Space S: Represents the snapshot of the world, including the player's position, walls, boxes, and storage locations at any given point in time.\n\u2022 Action Space A: Encompasses the various actions the player can take, either moving or pushing the box(es) in one of the following directions: up, down, right, or left.\n\u2022 Cost Function C(s,a,s'): Assigns a cost for each action to encourage making the fewest amount of moves.\n\u2022 Goal State SG: Includes all possible goals that the player can achieve within the current state of the world. Each goal represents a desired configuration of boxes in relation to storage locations."}, {"title": "3.3.2 DATA", "content": "We recruited 36 participants (22 male, 14 female), allocated evenly and randomly to each condition, aged between 20 and 65, with a mean age of 38. We limited the study to participants from the United States who are fluent in English. Recruitment was conducted via Amazon Mechanical Turk. Participants were compensated $6.50 for completing the task and a bonus of $3.50 for providing more thoughtful answers.\nWith three different game versions, six scenarios per game, and 12 participants per condition, a total of 864 textual data points were collected (Table 2). We used several methods to filter out deceptive participants. We excluded explanations with fewer than three words or containing gibberish. We also used the time taken to complete the survey as a threshold. This left us with a total of 828 explanations.\nWe used participants' open-ended explanations to better identify the concepts they used to explain the player's predicted goal. The word count of given answers within the dataset is between 1 and 98 words (1\u2081 = 22.96, \u03c3\u03b9 = 15.52) for the first condition, 3 and 81 words (12 = 26.57, \u03c32 = 15.63) for the second condition, and 1 and 64 words (13 = 20.38, \u03c33 = 11.65) for the third condition.\nEach participant was randomly assigned to one of three conditions:\n\u2022 'Why' condition: participants were asked to: \"Explain why you have rated that/those goal(s) as the most likely?\"\n\u2022 'Why-not' condition: participants were asked to: \"Explain why you have not rated that/those goal(s) as the most likely?\"\n\u2022 'Dual' condition: participants were asked to explain both why and why you have not in that order.\nWe collected data for the first and second conditions to analyze the differences between why and why not, and for the third condition to analyze how people answer why not if they have already answered why, and how the answer of why differs if they know there is a why not."}, {"title": "3.4 Case Study 2: StarCraft Game", "content": "StarCraft is a real-time strategy (RTS) game (Figure 3) where players manage an economy, produce units and buildings, and compete for control of the map with the ultimate aim of defeating all opponents. As an RTS game, StarCraft has several defining characteristics (Ontan\u00f3n, Synnaeve, Uriarte, Richoux, Churchill, & Preuss, 2013):\n\u2022 Players engage in a Simultaneous Move Game, where they execute actions concurrently such as moving units, building structures, and managing resources-demanding effective multitasking skills;\n\u2022 The Partially Observable Domain limits players' visibility of the game map and opponents' setups, necessitating strategic reconnaissance for informed decisions;"}, {"title": "3.4.1 STUDY DESIGN", "content": "We defined the worldview of shoutcasters, i.e. observers, within the framework of Goal MDPs as follows:\n\u2022 State Space S: Encapsulates the game environment's configurations and conditions at any given time. This includes the positions of units, resources, and other relevant game elements.\n\u2022 Action Space A: Includes low-level actions of specific game units and high-level actions related to game strategies and tactics (sequences of actions)."}, {"title": "3.4.2 DATA", "content": "We obtained the dataset from Penney et al. (2021), which was collected from professional StarCraft tournaments available as videos on demand from 2016 and 2017. They selected 10 matches and then randomly chose one game from each match. Each of the 10 videos features two shoutcasters (expert commentators) providing commentary.\nTo obtain representative samples, we identified six clusters within the dataset (1387 instances divided by 6 clusters equals approximately 231 instances per cluster). We then randomly selected one sample of 50 instances from each cluster. The resulting sample size was 300 instances (6 clusters * 50 instances each). We only considered instances involving predictions, specifically when shoutcasters explain their recognition process of what the agents/players aim to achieve (their goals) and so ended up having a total of 132 instances out of the six samples. As the data source is public, we provided supplementary material of coded data to support future research."}, {"title": "3.5 Method", "content": "We used a hybrid approach of deductive and inductive reasoning, employing thematic anal-ysis as outlined by Braun and Clarke (2006) to analyze our data. The analysis process was divided into six phases: familiarization with the collected data, development of codes, sort-ing different codes into potential themes, reviewing themes, defining and naming themes, and writing up the report.\nInitially, the collected data was re-read multiple times to ensure immersion before pro-ceeding to the coding phase. The coding process was inductive, aiming to identify basic concepts regarding how people explain goals, and deductive by relating them to the ex-isting literature on explaining human behavior (Malle, 2006b). Malle's explanation model (Malle, 2006b) shows that people reason over others' beliefs, values, and desires to explain intentional actions. Following that model, we apply these concepts to our coded data. Subsequently, the codes were grouped into defined themes based on their similarities.\nIn the context of this study, the proposed themes are linked to our research topic of explaining human behavior in goal recognition scenarios. After establishing a set of can-didate themes, the refinement process focused on ensuring internal homogeneity\u2014i.\u0435., \u0430 cohesive pattern within each candidate theme to accurately reflect the overall data set. Relationships, links, and distinctions between themes were identified during this phase."}, {"title": "3.6 Results", "content": ""}, {"title": "3.6.1 THE CONCEPTUAL MODEL OF GOAL RECOGNITION EXPLANATION", "content": "We developed the conceptual model by integrating insights from both studies. Initially, we built a model based on the findings from the Sokoban study. This model was then extended to encompass the complexities of the Starcraft study, ensuring that it accurately reflects both studies' dynamics and unique aspects. We present our conceptual model of an explainable Goal Recognition (GR) agent in Figure 4. The figure highlights the common elements (concepts) that encode an observer's view of the world and the different representations of given explanations. The model process is guided by two levels; situational awareness and situational conveyance. The situational awareness model, presented by Endsley's in (Endsley, 1995), is a widely used situation awareness model consisting of three consecutive stages:\n1. Perceived input: In this stage, the observer perceives the basic elements and their properties in the environment object, quantity, quality, spatial and temporal infor-mation.\n2. Reasoning process: Based on the perceived inputs, the observer makes causal infer-ences between their beliefs (including the actor's mental model) and goals, generates counterfactuals, and associates them with their preferences.\n3. Projection: This stage presents the observer's predictions of future actions guided by their expected goals and the uncertainty level based on the understanding of the previous stages.\nIn this level, two types of reasoning occur due to the two mental model representations (Felli, Miller, Muise, Pearce, & Sonenberg, 2015). The first type is stereotypical reasoning, in which the observer reasons about others' mental states (what the observer would have done). An example from the data corpus is: \"It might just do old classic seven gate [a game strategy].\" The second type is empathetic reasoning, where the observer casts themselves into the actor's mental model and reasons as they would (what the actor would have done). An example is: \"This is exactly what I was talking about, you do something to try to force them.\" In practice, the observer often contrasts both views in a single explanation-what they think should happen vs. what the actor is likely to do. For example: \"It's actually going to look for a run by here with this scan it looks like but unfortunately unable to find it with the ravager here poking away.\" This was observed when explanations were provided for either a failed plan or a sub-optimal one toward achieving a goal. For simplicity, we assume a local perspective in this work, where the actor's mental state is equal to the observer's through the data coding and model implementation process.\nThe second level is a situational conveyance, where different explanations are formed and communicated by the observer. At this level of the model, the explaining process requires additional strategic knowledge. This includes reasoning about contrastive, conditional, temporal, and spatial cases of problem-solving tasks, allowing for more than just a causal representation of a given explanation."}, {"title": "3.6.2 CONCEPTS", "content": "Table 3 shows the different codes and concepts that emerged from across the two studies. The given explanations include factual and experiential knowledge (\u2018belief'), subjective likes and objective assessments (\u2018preference'), the desired state to be achieved (\u2018goal\u2019), and possible future actions ('plan'). When people explain others' actions, they infer their goals to provide better explanations (McClure & Hilton, 1997). When explaining a recognized goal, people infer the most relevant evidence from their belief state. Thus, we break down the belief concept into an 'observational marker', an observed precondition that most influences goal prediction. This concept applies not only to optimal behavior, measured by traditional efficiency metrics such as time and shortest route, but also to suboptimal behavior.\nSince recognition problems activate counterfactual thinking (Epstude & Roese, 2008), explanations of GR reasoning also include 'counterfactuals' observational markers, plans, and goals. Explaining counterfactual plans implies having counterfactual goals where the actor has no plan to achieve them. We introduced an uncertainty code as we found that observers use words expressing uncertainty to indicate their confidence level. Additionally, as the problem is to explain goals, 'goal' and 'counterfactual goal' codes were also included. Finally, our data show the presence of different reasoning processes in the explanations."}, {"title": "3.6.3 FREQUENCIES", "content": "Figure 5 illustrates the frequency distribution of 17 codes across two case studies. Given the navigational nature of both domains, observers predominantly referenced objects and their spatial properties to reflect the players' strategies.\nAmong the various codes, the observational marker emerges as the most significant finding. This code, which ranks fourth in frequency, provides crucial insights into how observers infer players' intentions and strategies. For instance, in the Sokoban game dataset, an observer noted, \"The player positioned itself on top of the box, leading me to believe it is going to push down on the box to reach goal 2.\" Here, the action \"positioned itself on top of the box\" is used to explain the entire observed sequence, forming a critical precondition for achieving the predicted goal. This demonstrates how a single observed action can be pivotal in understanding the player's overall strategy. The counterfactual observational marker"}, {"title": "3.6.4 QUESTIONS", "content": "In human studies, two forms of causal reasoning are used to answer certain questions (Hoff-man & Klein, 2017): retrospective reasoning involves explaining past events through coun-terfactual reasoning, which considers what could have happened if the observed facts were different, prospective reasoning involves explaining future events through transfactual rea-soning, which considers what could happen in the future if certain conditions are met.\nIn the Sokoban game, we asked two questions: 'Why?' and 'Why Not', since it has been proved that they are the most demanded explanatory questions (Lim, Dey, & Avrahami, 2009). \"Why\" questions typically demand contrastive explanations, which are addressed through counterfactual reasoning (Miller, 2019a). In such explanations, people answer \u2018Why A' in the form of \u2018Why A instead of B?', where B is some counterfactual goal(s) that did not happen. From the data collected, we classified the provided contrastive explanations into three categories: implicit, where observers implicitly contrast and identify relevant causes for A (the predicted goal(s)); explicit, where observers explicitly contrast and identify relevant causes for B (the counterfactual goal(s)); and extensive, where observers provide explanations for both by identifying relevant causes for A and also for B. It is important to note that the observer answered a \"why\" question in the 'why' condition, a \"why not\" question in the 'why not' condition, and both questions sequentially in the 'dual' condition.\nFigure 6 illustrates the differences between conditions. In the dual condition, observers tended to adopt an implicit mode when they answered the why question-after having an-"}, {"title": "3.6.5 DISCUSSION", "content": "Causal reasoning is essential for constructing mental representations of events (Pearl et al., 2000; Malle, 2006b; Luo & Baillargeon, 2010). These representations form causal chains that illustrate how a sequence of causes leads to an outcome. In the realm of explainable AI, an agent aiming to explain observed events may need to use abductive reasoning to identify a plausible set of causes (Miller, 2019b). While numerous causes can contribute to an event, individuals typically select a subset they deem most relevant for their explanation (Miller, 2019b).\nOur human studies focus on understanding how people explain others' predicted goals based on observed behavior. By coding these explanations, we identify a key concept, re-ferred to as an 'observational marker' that participants use to build their explanations. Our findings align with social and cognitive research indicating that people prefer explanatory causes that seem sufficient in the given context for the event to occur (Spellman, 1997; Lipton, 1990; Woodward, 2006; Lombrozo, 2010).\nPeople make causal inferences about others' beliefs and goals based on their observed behavior and prior domain knowledge (Baker, Saxe, & Tenenbaum, 2009). A key aspect to explain those inferences is the ability to decide to what degree the observed evidence from a causal chain supports a goal hypothesis. To this end, we propose an explanation model for GR agents based on concepts such as causality and observational markers."}, {"title": "4. Preliminaries", "content": "In this section, we provide the essential background needed to follow the rest of the paper."}, {"title": "4.1 Planning", "content": "Planning aims to find a sequence of actions given an environment model, a current situation, and the goal to be achieved (Geffner & Bonet, 2022). The concept of planning is key to understanding GR algorithms that use planners in the recognition process. We build upon the following planning problem definition as defined in (Pereira, Oren, & Meneguzzi, 2017):\nDefinition 4.1. A planning task is represented by a triple (\u039e, \u0399, 9), in which \u039e = \u3008F, A) is a planning domain definition that consists of a finite set F of facts that define the state of the world, and a finite set A of actions; I is the initial state, and g is the goal state. A solution to a planning task is a plan that reaches a goal state g from the initial state I by following transitions defined in E. Since actions have an associated cost, we assume that this cost is 1 for all actions."}, {"title": "4.2 Goal Recognition (GR)", "content": "Goal recognition (GR) involves identifying an agent's goal by observing its interactions within an environment (Sukthankar et al., 2014). We consider GR definition as defined by (Shvo & McIlraith, 2020).\nDefinition 4.2. A goal recognition problem is a tuple (\u039e, \u0399, G, O), in which \u039e = (F, A) is a planning domain definition where F and A are sets of facts and actions, respectively; I is the initial state; G = {91, 92, ..., gm} is the goals set, and O = (01, 02, ..., On) is a sequence"}, {"title": "4.2.1 THE MIRRORING GR ALGORITHM", "content": "We focus on providing explanations for the output of the Mirroring GR algorithm (Vered & Kaminka, 2017; Kaminka, Vered, & Agmon, 2018). However, our approach is agnostic of the underlying GR algorithm and will work for any GR algorithm that fits Definition 4.2.\nThe Mirroring algorithmis inspired by people's ability to perform online GR, originating from the brain's mirror neuron system, which is responsible for matching the observation and execution of actions (Rizzolatti, 2005). The approach falls under the plan recognition as planning GR methods (Ram\u00edrez & Geffner, 2010; Masters & Vered, 2021) and uses a planner within the recognition process to compute alternative plans.\nSpecifically, the Mirroring algorithm uses a planner to compute optimal plans from an initial state I to each goal gj \u2208 G and to compute suffix plans from the last observation o\u00a1 \u2208 O to each goal gj \u2208 G. Observations are processed and evaluated incrementally. These suffix plans are then concatenated with a prefix plan (the observation sequence O at time step t) to generate new plan hypotheses. The algorithm subsequently provides a likelihood distribution, i.e., posterior probabilities P(gj | O) for each gj \u2208 G by evaluating which of the generated plans, incorporating the observations O, best matches the optimal plan. The following example illustrates The Mirroring approach to solving a GR task.\nExample 4.1. Figure 8 presents a navigational domain where an agent can navigate through the unblocked grid to reach one of 3 possible goal locations. The GR task is composed of an initial state where an agent at the start is located (marked I), a set of goal hypotheses, G = {91, 92, 93}, and a sequence of observations O = (01, .., 08) (represented as blue arrows). The domain definition \u039e = (F, A) includes a fact set F comprising the cells (45 states in total) and an action set A defined by four types of moves: up, down, left, and right, all with equal cost. The domain model is deterministic and discrete, meaning each action has only one possible outcome although our model does not assume deterministic actions. A goal state specification G is defined as the agent being in one of the three possible"}, {"title": "4.3 Weight of Evidence (WoE)", "content": "The principle of rational action (Hempel, 1961) states that people explain goal hypotheses by assessing the extent to which each observed action contributes to a specific goal hypothesis over others. Building on this idea, Bertossi (2020) defines a causal explanation as the set of features most responsible for an outcome. By incorporating this approach, we model our explanation framework using the Weight of Evidence (WoE) concept.\nWeight of Evidence (WoE) is a statistical concept used to describe the effects of variables in prediction models (Good, 1985). It is defined in terms of log-odds (see supplementary material) to measure the strength of evidence e supporting a hypothesis h against an al-ternative hypothesis h', conditioned on additional information c. Assuming uniform prior probabilities\u00b9, WoE is expressed as:\n$$woe(h/h': e | c) = log \\frac{P(h | e, c)}{P(h' | e, c)}$$\nMelis et al. (2021) propose a framework based on WoE for explaining machine learning clas-sification problems, arguing that it aligns with how people naturally explain phenomena to one another (Miller, 2019a). They found that WoE effectively captures contrastive state-ments, such as evidence for or against a particular outcome. This helps answer questions like why a goal g is predicted, why not goal g', and what should have happened instead if the goal is g'. We adopt this concept and apply it to GR problems."}, {"title": "5. eXplainable Goal Recognition (XGR) Model", "content": "Building on insights from the human studies discussed in Section 3, we propose a simple and elegant explainability model for goal recognition algorithms called eXplainable Goal Recognition (XGR). This model extends the WoE framework to generate explanations for goal hypotheses. In the following section, we use the navigational GR example (Example 4.1) as a running example to support the definitions."}, {"title": "5.1 Overview", "content": "Extending Melis et al.'s (Melis et al., 2021) WoE framework, our model addresses\u2018why' and 'why not' questions, which are the most demanded explanatory questions (Lim et al., 2009). Lim et al. (Lim et al., 2009) compared a range of intelligibility-type questions and showed that explanations describing why a system behaved in a certain way resulted in better understanding and increased trust in the system, which is also supported by our findings in Section 3.6.4 (Figure 7).\nThe model accepts four components as an input, which any GR model can provide:\n1. The observed action sequence O.\n2. The set of predicted goals, Gp \u2286 G."}, {"title": "5.2 Explanation Generation", "content": "We generate explanations by extending the WoE framework presented by Melis et al. (Melis et al., 2021). By generating explanation lists using WoE, we can measure the relative importance of each observation to the goal hypotheses. This approach enables us to explain using the observational marker, which is the predominant concept used in the explanation of the agent's goals (refer to Section 3.6.3).\nReferring to Equation 1, we substitute the hypotheses h and h' with a predicted goal and counterfactual goal hypotheses, g and g'. The evidence e is replaced by an observed action of and the posterior probabilities are represented as P(g | O) and P(g' | O). A complete explanation is defined as follows:\nDefinition 5.1. A complete explanan for a goal g is a list of pairs (woe(g/g' : 0\u017c | O), 0\u017c), in which the conditional woe(g/g' : 0\u017c | O) for each paired hypothesis g and g' is computed for each added observation or to the observed sequence O. The WoE is computed as follows:\n$$woe(g/g': o_i | O) = log \\frac{P(g | O_i)}{P(g' | O_i)}$$\nInformally, this defines a complete explanan for a goal g as the complete list of com-puted WoE scores for each observation. An algorithm for extracting this is shown below (Algorithm 1).\nIn the navigational GR scenario presented previously (Figure 8), the WoE would be the same for all goal hypotheses after the first three observations, 00 to 03. This is because the Mirroring GR algorithm predicts them as equally likely since the first three observations are part of the optimal plan to achieve all three goals. However, this uniformity does not hold for the rest of the observation sequence. For the observations 04 to 06, the Mirroring GR outputs would be goal g2 and g3 as equally likely since the observed actions are consistent with the optimal actions needed to reach either goal, with the counterfactual goal being g1.\n presents the posterior probabilities and WoE values associated with either g2 or 93 as the leading goal candidate. The model computes the WoE value of each observed action for the pair of the predicted and counterfactual goals."}, {"title": "5.3 Explanation Selection", "content": "Explaining the output of a GR algorithm in terms of the complete observation sequence can be tedious or even impossible, especially in scenarios where the domain model contains hundreds of thousands of states and actions. XAI best practice deems that for explanations to be effective they should be selective, focusing on one or two possible causes instead of all possible causes for a decision or recommendation (Miller, 2019a). In the context of GR explanations, we found that people pointed to the observational marker and the counterfactual observational marker when they answered 'why' and 'why not' questions ("}]}