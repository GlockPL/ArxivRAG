{"title": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions", "authors": ["Qingbin Zeng", "Qinglong Yang", "Shunan Dong", "Heming Du", "Fengli Xu", "Yong Li"], "abstract": "This paper considers a scenario in city navigation: an AI agent is provided with language descriptions of the goal location with respect to some well-known landmarks; By only observing the scene around, including recognizing landmarks and road network connections, the agent has to make decisions to navigate to the goal location without instructions. This problem is very challenging, because it requires agent to establish self-position and acquire spatial representation of complex urban environment, where landmarks are often invisible. In the absence of navigation instructions, such abilities are vital for the agent to make high-quality decisions in long-range city navigation. With the emergent reasoning ability of large language models (LLMs), a tempting baseline is to prompt LLMs to \"react\" on each observation and make decisions accordingly. However, this baseline has very poor performance that the agent often repeatedly visits same locations and make short-sighted, inconsistent decisions. To address these issues, this paper introduces a novel agentic workflow featured by its abilities to perceive, reflect and plan. Specifically, we find LLaVA-7B can be fine-tuned to perceive the direction and distance of landmarks with sufficient accuracy for city navigation. Moreover, reflection is achieved through a memory mechanism, where past experiences are stored and can be retrieved with current perception for effective decision argumentation. Planning uses reflection results to produce long-term plans, which can avoid short-sighted decisions in long-range navigation. We show the designed workflow significantly improves navigation ability of the LLM agent compared with the state-of-the-art baselines. Our code and datasets are available: https://anonymous.4open.science/r/PReP-13B5", "sections": [{"title": "1 Introduction", "content": "Navigation in complex and unknown urban environment is an important task for artificial intelligent agents. This paper studies goal-directed agent navigation in the city, where an agent is provided with visual perception and goal location described by the relation to some well-known landmarks, e.g., \"the destination is approximately 300 meters northeast from the Skyscraper A\u201d. The agent should visually identify the landmarks from street view images, use them as anchors to infer the direction of and distance from the goal, and take a series of actions to navigate to the goal without instructions. The task is challenging because it requires the agent to be aware of its own location and acquire spatial understanding of complex urban environment, where landmarks are sometimes invisible. In the absence of navigation instructions and maps, such abilities are vital for the agent to make high-quality decisions in long-range navigation.\nExisting literature does not provide a ready-to-use solution to this task. A few recent works [1] [2] assume the availability of step-by-step language instructions and thus are not applicable to our task. Another branch of literature focus on designing reinforcement learning models [3] [4] [5], which often facing challenge of inefficient data and sensitivity to perturbations of the environment.\nWe explore the use of large language models (LLMs) for this task. React [6] is a straightforward baseline to ground reasoning ability of LLMs in city environment. At each step, this method visually perceives the street views which is used to make an action decision. This process is iteratively performed until reaching the goal or running out of the navigation budget. While React has some success attempts in indoor environments, it performs poorly in complex urban environments, which can be attributed to two main reasons. First, because each action decision is based only on the current observation, the agent may repeat actions previously taken and find itself going around in circles. Second, React is short-sighted, focusing only on the immediate step. Without considering long-term action sequences, the agent would be prone to taking more actions than actually needed.\nThis paper proposes an effective agentic workflow that improves the goal-directed city navigation ability of LLMs. To avoid isolated decision making, we propose a memory scheme. The historical trajectories and observations are stored and summarized to learn an intrinsic spatial representation of the environment, i.e., an internal city map. The agent combines the historical experience and current observation to infer the goal direction. To improve over short-sighted actions, we resort to long-term planning. Specifically, considering the reflections and current road network connection, the agent decompose the full navigation path into several sub-goals, ensuring consistent and reasonable movement to the final goal during long-range navigation. In addition, we find that the fine-tuned LLaVA can perceive the direction and distance of landmarks with sufficient accuracy for navigation."}, {"title": "3 Task Description, Dataset, and Baseline", "content": ""}, {"title": "3.1 Task Description", "content": "In this study, an agent navigates in the urban environment to find the goal with visual perception and goal description. To define the task exactly, we give following definitions:\nDefinition 1 (Urban Environment) The urban environment for navigation task can be described as an undirected graph $G = < V, E >$. Each node $v_i \\in V$ represents an position in the road, while street views $S_i = {s_i^1, s_i^2, ..., s_i^{|S_i|}}$ are the attached visual information for node $v_i$. The edge $l_{ij} \\in E$ donates the moving path between $v_i$ and $v_j$. Define $E_i$ as the collection of all edges connect to the node $v_i$. What's more, landmarks $LM = {lm_1, lm_2, ..., lm_n} \\in V$ are defined as isolated vertex in the urban environment G. As the graph corresponds to a real urban scene, and the relative positions of each node are fixed. We define the relative position relationship of $v_i$ with respect to $v_j$ as $R(v_i, v_j)$.\nDefinition 2 (Urban Navigation Task) The urban navigation task can be formulated as finding a path from the start node $v_s$ to the goal node $v_g$ in the graph $G$. To be exact, given a navigation task $T = < v_s, v_g, D >$, the target is to find a shortest path to navigate to the goal. Description $D = {R_1, R_2}$ is to determine the goal, containing $R_1 = {R(v_g,lm)|lm \\in LM}$ and $R_2 = {R(lm_i, lm_j)|lm_i,lm_j \\in LM}$, where $R_1$ is the relative position between goal and landmarks visible in goal nodes, and $R_2$ is the relative position among all landmarks in the environment.\nDefinition 3 (Agent for Urban Navigation Task) At timestamp t and node $v_t$, the agent makes a decision to move to the next node $v_{t+1} = agent(T, S_t, E_t)$, utilizing the street views and road connectivity. The agent recognize the landmarks from the street views and then infer the goal direction and distance $R(v_t, v_g)$. Utilizing the above information, the agent navigates in the city street to find a path to the goal $p = [v_s, ..., v_t, v_{t+1}, ..., v_g]$."}, {"title": "3.2 Dataset", "content": "We collect data from central business districts (CBDs) of Beijing and Shanghai, which have a radius of a few kilometers. From this range, road network data are extracted and discritized at intervals of 50 meters forming the urban environment G. Each node of the road network is associated with the corresponding street view images. The number of street view images is related to the degree of the node. Road network visualization and a few examples of the dataset are shown Fig. 2(a).\nSpecifically, the selected area in Beijing is the Guomao CBD area, with a radius of approximately 3 kilometers, which includes a total of 1,134 nodes and 2,742 street view images, along with 10 landmark buildings. In Shanghai, the selected area is the Lujiazui CBD area, also with the similar radius, containing a total of 1,038 nodes and 2,366 street view images, along with 10 landmark buildings. The landmarks chosen in the dataset are well-known buildings with unique features. Street view images were obtained from Baidu Map Street View API, with a field of view of 90\u00b0 and an elevation angle of 20\u00b0 for each image. The image resolution is 1,024\u00d7512 pixels."}, {"title": "3.3 Baseline", "content": "A straightforward baseline iterates between two steps: visual perception and react. Specifically, after perceiving a street view from its current location, the agent predicts the direction of and distance to the target. Then, based on these predictions, the agent decides the next move. These two steps iterate until the agent reaches the goal or runs out of the navigation budget."}, {"title": "4 Proposed Agent Workflow", "content": ""}, {"title": "4.1 Workflow Overview", "content": "The overall agent workflow is shown in the Figure 3, which is consist of three parts: visual perception, reflection with memory, and planning. While visual perception uses LLaVA, both reflection and memory uses large language models (LLMs). As described in Section 4.2, visual perception allows the agent to recognize the landmarks in the street view images and predict the direction and distance of the target. Perception results are passed to the reflection part, where the agent reevaluates the perception results and reflects the goal location. In reflection, long-term memory is set up to summarize and learn from the historical trajectory for constructing intrinsic map representations, a topological map between nodes that the agent has visited. The planning module is the decision core of the workflow. It generates a navigation plan by considering both the goal direction after reflection and the current road connections. The agent follows this plan to make the next move. Then the agent state will be updated to start next iteration. These steps compose a workflow that enables the agent to sense its surroundings, remember past experience, and plan its actions. As shown in Section 5, this workflow yields significantly higher navigation success rates compared to the 'React' baseline."}, {"title": "4.2 Perception", "content": "Perceiving the landmarks. At timestamp t and node $v_t$, the agent gets the street views $S_t = {s_t^1, s_t^2,..s_t^{|S_t|}}$. The agent then detects the landmarks and estimates their direction and distances $R_{lm} = {R(lm_i, v_t)|lm_i \\in LM}$ to the agent itself. We perform the task using LLaVA [25]. Zero-shot LLaVA has poor recognition accuracy because the landmarks we use are probably not in its training data. We therefore fine-tunes LLaVA using the LoRA method [26]. To do so, we collect 5,000 landmark images and generate 30k Q&A conversation data. More details can be seen in the Appendix A.1."}, {"title": "4.3 Reflection", "content": "Reflection is critical in our workflow, which summarizes past experience and reflects on visual perception results. This step has two main components: long-term memory and working memory. Long-term memory consists of episodic memory and semantic memory, where episodic memory stores navigation data and semantic memory saves summary of history navigation experience. Working memory serves as a data buffer to process the visual perception results and retrieved memory.\nEpisodic memory. Episodic memory is a list of the navigation data in natural language. When the agent moves from $v_t$ to $v_{t+1}$, this action and the perception results $R_t^g$ in $v_t$ are processed into a sentence and stored. The detail of the sentence format is in Appendix E. Since the past navigation data are stored, the agent can retrieve the goal inference at history location and detect whether connected nodes are visited. For example, when the agent has saved navigation data in $v_t$ and navigated to $v_i$, it can retrieve $R_t$ and the status of connected nodes $E_i$, to help reflection and planning. These retrieved memories are buffered in working memory space for further processing.\nSemantic memory. While episode memory records the experiences, the agent uses LLMs to summarize and learn from the episodic memory to form the semantic memory. The semantic memory is a high-level cognitive function that assists the agent in constructing an intrinsic representation of the navigation map. Like a human, it can understand the environment based on historical experience and learn more advanced navigation strategies, such as detours required to reach the destination. These strategies can be retrieved to working memory and further beneficial to the planning process. As the agent navigates, episodic memory and semantic memory are updated accordingly.\nWorking memory. Working memory receives visual perception results $R_t^g$ and retrieves relevant experiences from long-term memory. It has an anticipation-reevaluation mechanism to solve the problem where the agent cannot detect any landmarks in the street views and loses goal direction.\nSpecifically, the agent uses historical perception results $R_t^g$ and moving direction to anticipate the potential goal direction $R_t^g$, if any landmark can be observed, the agent reevaluates whether the current inference $R_t^g$ is reasonable, and synthesizes a new inference $R_t^g = LLM_{reflect}(R_t^g, R_t^g)$.\nThe output of working memory to the planning part are the synthesized inference $R_t^g$ and retrieved memory. This enables the agent to tackle complex environments regardless whether landmarks are visible or not, making the agent more flexible and robust."}, {"title": "4.4 Planning", "content": "Instead of reacting directly to observations, we use a planning module in our workflow. It involves long-term planning and short-term decision-making. Specifically, long-term planning uses reflected goal inference $R_t^g$, retrieved memory $M_t$, and the old plan $P_{t-1}$ as input. It updates the navigation plan at timestamp t $P_t = LLM_{plan}(P_{t-1}, R_t^g, M_t)$ by analysing the old plan and breaking down the possible path into sub-goals. The agent firstly analyses what stage does the old plan has been carried out, and then synthesizes the goal inference, retrieved memory and the connection status to decide whether the plan should be updated. If so, the agent updates the plan by predicting the possible route to the goal, and breaking down the full route into few sub-goals like \"move [east] until [an intersection]\". Meanwhile, the short-term decision maker translates the plan into action $a_t = LLM_{action}(P_t, E_t)$ based on the road connections $E_t$. The action $a_t$ means a move from node $v_t$ to node $v_{t+1}$, making the agent updates its location and explore the goal in the environment."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental setup", "content": "We experimentally evaluate the performance of the proposed agentic workflow on the simulated urban navigation task described in section 3. We use success rate (SR) and success rate weighted by path length (SPL) to measure system effectiveness and efficiency, respectively [27].\nWe use two test sets, one for Beijing CBD, and the other for Shanghai CBD. Each test sets have 100 different navigation tasks with different starting points. Each starting point is at a road node, where at least one landmark must be visible; otherwise, the agent will randomly wander. The minimum number of steps required from the starting point to the goal followed a normal distribution with $\\mu$=30 steps and $\\sigma$=10 steps. Because each steps translates to 50 meters on the map, it means the average step is 30, and average navigation distance is 1,500 meters. We set the iteration limit as 2.5 times as the minimum steps. If the agent moves more than the limited step, we think the navigation task is failed."}, {"title": "5.2 Main Evaluation", "content": "Comparison with existing methods that might give a solution. We compared PReP with existing language-based methods, including code as policies (CaP) [21], ProgPrompt [22], inner monologue [17], and chain of thought (CoT) [23]. These method are using well-designed prompts to fit our urban navigation task and we list the prompts for different methods in Appendix E. We also implement two baselines that are not based on LLMs. The 'random' method selects a random direction from the current connection each time. Reinforcement learning (RL) [9] is trained for 1 million steps in the environment to learn the policy for finding the goal. The perception module for all the methods is the same. All the language-based methods use GPT-4-turbo as the base model, and all the hyper-parameters of LLMs are the same for a fair comparison.\nFrom Table 1, we clearly observe that PReP yields the best navigation performance compared with existing methods. We have two observations. First, the success rate of Random is nearly 0, indicating the significant challenge of this task. Existing language-based methods have improved performance, suggesting that LLMs possess the capability to navigate in cities based on goal direction. Second, we achieve SR = 63% and 57% on Beijing and Shanghai test sets, respectively, and SPL = 47.67% and 42.15% on Beijing and Shanghai test sets, respectively. The second best method is CoT, the SR of which is 5% and 17% lower than our method, on Bejing and Shanghai test sets, respectively. It indicates the effectiveness of PReP.\nEffectiveness of the proposed planning and reflection methods. We conduct ablation studies to validate the usefulness of the reflection and planning methods. We keep perception part unchanged, assuming all variants can recognize landmarks and infer goal directions. Results are summarized in Table 2. 'PReP' indicates the complete PReP workflow, which includes both planning and reflection methods. 'w/o Reflection' indicates that the agent receives the perception and retrieved the episode memory but without constructing semantic memory and reflection on goal inference. \u2018w/o Planning\u2019 indicates that the agent normally get the reflected goal inference and retrieved memory, but makes decisions without formulating a long-term plan. 'Plain' is the combination of 'w/o Reflection' and 'w/o Planning', where the agent makes decisions directly based on perception results and history experience in a list without planning and reflection.\nWe clearly see that the full system has the best performance. For example, on the Beijing test set, the full system yields a higher success rate of 30%, 4%, and 20% over 'Plain', 'PReP w/o Planning', and 'PReP w/o Reflection', respectively. This indicates the necessity of having both steps in our system. We also observe that removing reflection leads to larger performance drop comparing with removing planning. It suggests the importance of reflection."}, {"title": "5.3 Further Analysis", "content": "Benefit of fine-tuning LLaVA over zero-shot LLaVA. In Table 3, we compare fine-tuned LLaVA with zero-shot LLaVA. Zero-shot LLaVA has much poorer performance: on the Shanghai test sets, its SR and SPL is 46% and 33.26% lower than its fine-tuned version. It indicates that LLaVA does not naturally recognize landmarks through Baidu Maps. But interestingly, zero-shot LLaVA still has 10+% success rate. This can be explained by its 19% accuracy, 6% precision, 93% recall, and 0.64$ IoU. In fact, zero-shot LLaVA has good building detection capacity and assume that most images contain landmarks, leading to a high recall. Its precision is low (6%), but sometimes is fine for the agent to find the goal.\nWe also compare with an oracle setting, where the perception results are replaced with groundtruth directions and distances measured by GPS. Compared with oracle results, fine-tuned LLaVA is 4% and 5% lower in SR on Beijing and Shanghai test sets, respectively. This is not a significant gap, indicating the effectiveness of fine-tuning.\nComparing different LLMs. We now use different LLMs to perform inference (blue boxes in Fig. 4). These models include GPT-3.5-turbo, GLM-4 [28], Mistral-7B [29], LLaMA3-8B [30] and GPT-4-turbo. From Table 4, we observe that GPT-4-turbo significantly outperforms other LLMs without fine-tuning. Moreover, we then use the question-answering data generated by GPT-4-turbo to fine-tune LLaMA3 [31]. The fine-tuned LLaMA3 model achieves performance that was second only to GPT-4-turbo among all models.\nImpact of goal distance. We analyze whether the distance between the goal and the agent has an impact on success rate. In Fig. 6a, we observe that the success rate does not decrease a lot when the goal is as far as 2 kilometers (40 - 50 steps) from the agent. We also notice that the curves drops at 20-40 steps but increases at 40-50 steps. The possible reason is that the iteration limit increases as the distance increases, the agent may fully explores the environments and find the goal more easily.\nFurthermore, in Fig. 6b, we study how the goal distance affects the number of steps taken by PReP. As the distance increases, generally the plots become more scattered, indicating the task is becoming more challenging. Basically the agent takes 1 or 2 times more of the minimum number of steps to reach the goal.\nImpact of landmark visibility. When the agent performs a navigation task, some nodes it passes through can observe landmarks, while others cannot. We study how landmark visibility along its path impacts its success rate. In Fig. 6c, we plot the relationship between the number of nodes in the path that can observe any landmark and the total number of nodes in successful tasks. From the plot, we can find some valuable results. When the PReP agent performed tasks in the Beijing test set, it could identify landmarks in approximately 50% of the nodes traveled on average. In the Shanghai test set, this percentage dropped to less than 20%. Despite this challenge, performance in the Shanghai test set was not significantly lower than in the Beijing test set. This demonstrates that the PReP agent can efficiently navigate even in environments with sparse landmark visibility.\nComputational cost. The primary training cost is LLaVA fine-tuning, which requires one NVIDIA GeForce RTX A100 GPU with 80G memory for approximately 3 hours with 30k conversation data. Each request-response cycle of the fine-tuned LLaVA on the same GPU takes 6 to 8 seconds, while calling the LLM API takes 2 to 5 seconds (varies among different models). Each iteration for an agent step takes about 12 seconds. In future we will work to optimize the inference process.\nA case study. We conduct a case study to illustrate the role of reflection and planning (see Fig. 6). In this case, the PReP agent deviates from the shortest path in the beginning but still successfully reaches the goal. Initially, the agent infers that the goal was located to the east. However, as there was no direct path to due east, the agent planned a detour: it first heads northeast and then turns towards either east or southeast. The agent continuously makes inferences of the goal location each time it observes a landmark along its route. When the agent cannot perceive landmarks in the street views and lose the goal direction, it can reflect on history memory including the moving trajectory and goal inference, and then anticipate the goal direction from current position. When the agent moves in the deviate direction, it can reflect its trajectory and re-plan the right route."}, {"title": "6 Conclusion", "content": "We propose an agentic workflow for goal-directed city navigation without step-by-step language instructions or maps. The workflow includes a fine-tuned LLaVA model for spatial perception, a memory module for synthesizing and reflecting perception results and retrieved memory, and a planning module for navigation route planning. As our approach only requires training the visual perception part, it is a more data-efficient solution compared to RL methods. Owing to the well-designed reflection and planning part, the agent can perform the long-term navigation task in complex environment and achieves a success rate of about 60%. Further experiments show that the agent performs well in two cities and various difficulty levels, demonstrating robustness and flexibility. Our contributions not only present an effective agentic workflow for using LLMs in goal-directed urban navigation, but also validate the potential of LLMs for complex spatial tasks."}, {"title": "A Additional Experimental Details", "content": ""}, {"title": "A.1 Fine-tuning LLaVA", "content": "The methodology for fine-tuning LLaVA uses the LoRA (Low-Rank Adaptation) technique. This approach introduces trainable low-rank matrices to simulate parameter updates, enabling rapid task adaptation without significantly increasing model complexity. We collected 250 images for each of the 20 landmarks from the Internet, totaling 5,000 images. These images were manually annotated with the binary visibility and bounding box of each landmark. We split the data into an 80% training set and a 20% test set. Using these images, we generated dialogue data in a question-and-answer format. The questions we asked are as following step by step, aiming to form a pattern of CoT to improve its understanding and reasoning ability.\n1. \"Is the landmarki visible in the image?\"\n2. \"The landmarki is visible in the image, what's the bounding box of it in the image?\"\n3. \"The landmark is visible in the image and its bounding box is (Xmin, Ymin, X\u0442\u0430\u0445, \u0423\u0442\u0430\u0445), how far is it actually away from the camera?\"\nWe generated about 30k turn dialogues to fine-tune the llava-v1.5-7b. The fine-tuning process was carried out on an NVIDIA GeForce RTX A100 GPU and took about 3 hours. The scripts to fine-tuning is modified from the official repository with the default parameters.\nThe outcome of this fine-tuning process is a model that demonstrates remarkable accuracy in landmark recognition and segmentation. While the distance estimation is not very accurate, the rough estimation results are still effective in subsequent navigation steps. The fine-tuned LLaVA model is essential for the agent to perceive the environment and obtain goal information."}, {"title": "A.2 Fine-tuning LLaMA3-8B", "content": "We are trying to transfer knowledge from a much large model (GPT-4-turbo) to a small model (LLaMA3-8B). The method involves using data generated during navigation with GPT-4-turbo to fine-tune LLaMA-8B. We filter the successful samples from all the saved data and process them into the ShareGPT format. To avoid data breaches, we separate the Beijing and Shanghai datasets. This means we use data generated in Beijing to fine-tune LLaMA, which is then tested in Shanghai, and vice versa. We generated about 20k dialogue turns for each city dataset and fine-tuned LLaMA-8B using the LORA method with one NVIDIA GeForce RTX A100 GPU. This process took about 30 minutes using the LLaMA-Factory tool 3. All parameters in the fine-tuning process were set to default.\nThe results are shown in Table 4. We see that while LLaMA3-8B performs slightly worse than GPT-3.5-turbo in our task, it significantly outperforms other LLMs (except GPT-4-turbo) after fine-tuning. Although there is still a gap compared to GPT-4-turbo, increasing the amount of fine-tuning data may improve its performance."}, {"title": "B Limitations", "content": "Despite the advancements and innovative approaches in our work, there are several limitations to consider. A notable challenge is the dependence on powerful closed-source models, such as GPT-4-turbo, for superior results. Although we fine-tuned the open-source LLaMA-8B model, its performance, while better than other LLMs, does not reach half that of GPT-4-turbo. This highlights a significant gap. Future work should explore more effective fine-tuning strategies for open-source models to enhance their capabilities, ensuring our progress in urban navigation can be widely adopted and further developed. Another issue is the limited size of our test set, which may cause fluctuations in the success rate. A larger and more diverse test set could provide a more comprehensive evaluation of the model's performance and robustness."}, {"title": "C Ethical Analysis", "content": "The data we collected is open access including Baidu StreetViews API and Open Street Map, without privacy issues. Our ethical analysis confirms that all data was gathered in compliance with the code of ethics. We ensured that no personally identifiable information was collected, maintaining the anonymity and privacy of individuals. By adhering to the ethical guidelines, we avoid any potential privacy concerns and support the broader scientific community's efforts to build on and verify our work."}, {"title": "D Broader Impacts", "content": "Developing the LLM agent for goal-directed city navigation has many positive social impacts. This technology extends beyond navigation robots, providing invaluable help to the visually impaired by enhancing their mobility and independence. Easier navigation in urban environments can greatly improve their quality of life and help them participate more fully in society. Additionally, using this technology in disaster relief can save lives by helping rescue teams navigate affected areas quickly and efficiently. However, there are potential negative social impacts to consider. Relying on AI for navigation might decrease human spatial awareness and problem-solving skills. To maximize the positive impact and minimize negative consequences, it is crucial to develop and implement this technology ethically with strong safeguards in place."}, {"title": "E Prompt for Different Methods", "content": ""}, {"title": "E.1 PREP Prompt", "content": "Perception\nQ:\nIs the Beijing_Zhoxin_Building visible in the image?\nA:\nYes. Its voc bounding box is the (0.58984", "image.\nQ": "nThe Beijing_Zhoxin_Building is visible in the image and its voc bounding box is (0.58984", "camera?\nA": "nThe Beijing_Zhoxin_Building is about 1600 meters away from the camera.\nReflection\nQ:\nYou are conducting a navigation task and here is your memory list in time sequence.\n1. You were at (14", "Northeast": "North", "West": "South", "East": "Northwest", "Southwest": "from here. You chose to move to East.\nYou then arrived at (16"}, {"West": "East"}, {"West": "East", "Northeast": "West"}, {"Northeast": "Southwest"}, {"Northeast": "Southwest"}, {"Northeast": "Southwest", "South": "East", "Southwest": "North", "North": "South", "Answer": "In the past 15 steps, you initially wandered between East and West. You then headed East\nand then North to reach your current position. Now you are on a north-south road. If you keep going South, you will\nreach an intersection (1, -5) which can move to North, West and East. If you keep going North, you will reach a\ndead end (1, 5) with only one navigable direction.", "nA": "nIn the past 10 steps", "movement.\nQ": "nYou are now at (22", "Thought_Q1": "n'Answer_Q1'", "Thought_Q2": "Answer_Q2", "Answer_Q1": "The goal coordinates are most likely to be\n(3", "Answer_Q2": "The goal (3, 2) is in Southeast (more towards east) from current position\n(0,0)."}, {"nA": "n\"Thought_Q1\": \"To determine the most likely goal coordinates", "coordinates": 54}]}