{"title": "NEURAL NETWORKS FOR VEHICLE ROUTING PROBLEM", "authors": ["L\u00c1SZL\u00d3 KOV\u00c1CS", "ALI JLIDI"], "abstract": "Abstract: The Vehicle Routing Problem is about optimizing the routes of vehicles to meet the needs of customers at specific locations. The route graph consists of depots on several levels and customer positions. Several optimization methods have been developed over the years, most of which are based on some type of classic heuristic: genetic algorithm, simulated annealing, tabu search, ant colony optimization, firefly algorithm. Recent developments in machine learning provide a new toolset, the rich family of neural networks, for tackling complex problems. The main area of application of neural networks is the area of classification and regression. Route optimization can be viewed as a new challenge for neural networks. The article first presents an analysis of the applicability of neural network tools, then a novel graphical neural network model is presented in detail. The efficiency analysis based on test experiments shows the applicability of the proposed NN architecture.", "sections": [{"title": "1. INTRODUCTION", "content": ""}, {"title": "1.1. Vehicle routing problem and TSP", "content": "The Vehicle Routing Problem (VRP) is a very important problem domain in transportation and logistics. It is about organizing vehicle routes efficiently to meet the needs of customers. Basically, VRP involves sending vehicles from certain places to deliver goods to customers and then returning. In literature, many different types of VRP have developed to meet the needs of different industries. These types include options like specific delivery times, sending goods through different stages (from warehouses to local distribution points and then to customers), routes that don't return to the start, handling many kinds of products, using many vehicles, and planning routes for electric vehicles. VRP can be about moving things inside a facility or outside, and it can even cover the whole process of getting products from start to end. As transportation tasks get more complex, with the need to deliver many kinds of products, use different types of vehicles, and deal with various delivery points, using computers to help plan and manage these tasks is becoming very important for being efficient.\nThe core module of the VRP tasks is to find the shortest path to connect the operation sides. This problem domain is called the Traveling Salesman Problem (TSP) problem. The objective of the TSP family is to discover the shortest Hamiltonian cycle within a weighted graph, where each vertex is visited exactly once. The path's length is determined by the sum of the corresponding edge weights. Solving the problem involves finding an optimal permutation of vertices to create the shortest route. TSP algorithms are versatile and can be applied to various scenarios such as:\n\u2022 Optimizing transportation routes for delivery and collection tasks [1].\n\u2022 Heuristically solving knapsack optimization problems [2]."}, {"title": "1.2. Neural Networks", "content": "The neural network (NN) architecture is the current dominating approach in machine learning to support decision making processes. The NN architecture is a special function approximator which can learn the prediction model from information stored in a training dataset. The main methods in application of NN are the classification and the regression. The input training dataset is given in the form of a list of <X (feature vector), Y (label)> pairs. Each feature vector corresponds to an object instanced in the training space, where the coordinate values represent the different attribute values. In the case of classification, the label denotes the target discrete category, while for the regression problem, the label is the related continuous value.\nThe main goal of the training process is the build a mathematical model\nNN: X \u2192 Y\nwith a good accuracy. The base NN architecture consists of connected neurons organized into layers. Each neuron performs a linear space separation (elementary decision) and generates a non-linear output as the result level of the elementary decision. The generated elementary output values are later compiled into a new hidden feature vector in a high dimensional space. Based on this architecture, the NN can approximate any complex functions. The largest NN systems, like the GPT models for ChatGPT contain more than $10^{12}$ parameters.\nThe NN architectures are widely used tools for category or value prediction as the model provides the following key properties:\n\u2022 Non-linearity: Neural networks employ activation functions, enabling them to model complex, non-linear relationships between inputs and outputs.\n\u2022 Parallel Processing: Neural networks are capable of parallel processing, allowing for efficient computation of large-scale problems across multiple nodes or layers simultaneously.\n\u2022 Adaptability: Neural networks can adapt and learn from data through training processes, adjusting their parameters to optimize performance on specific tasks.\n\u2022 Generalization: Trained neural networks can generalize their learned patterns to new, unseen data, making them effective in various prediction and classification tasks.\n\u2022 Scalability: Neural networks can scale to handle large datasets and complex problems by increasing the number of layers, neurons, or using parallel processing techniques."}, {"title": "2. ROUTE \u039f\u03a1\u03a4\u0399\u039c\u0399\u0396\u0391\u03a4\u0399ON METHODS", "content": "The TSP problem falls into two main domains: the decision problem (DTSP) and the optimization problem (OTSP). DTSP involves determining whether a Hamiltonian cycle with a length not exceeding a given value exists. On the other hand, OTSP aims to find the shortest Hamiltonian cycle. This paper focuses on analysing the OTSP optimization problem, typically formulated as a linear programming problem [5]. OTSP is NP-hard [6], and brute-force algorithms have a time complexity of O(N!). Therefore, heuristic approaches are essential for approximating the global optimum efficiently. TSP heuristic optimization is extensively studied in combinatorial optimization research.\nMinimizing the route length holds paramount importance as it significantly im-pacts profitability, directly correlating with factors such as fuel consumption, human resource expenses, the requisite number of vehicles, among others. Thus, route length emerges as a pivotal determinant warranting meticulous consideration. The standard goal of the TSP optimization [7] is to minimize\n$\\sum_{i1=1}^{n} \\sum_{i2=1}^{n} \\sum_{j1=1}^{n} \\sum_{j2=1}^{n} \\sum_{k=1}^{v} \\sum_{t=1}^{t} TDBN_{i1,i2,j1,j2,k,t,m}z_{i1,i2,j1,j2}^{k,t,m}$ \nfor all product types m, where\n$z_{i1,i2,j1,j2}^{k,t,m}$ is the decision variable and $TDBN_{i1,i2,j1,j2,k,t}$ is the distance matrix.\nThe value of the decision variable is either 1 or 0. In the formulas, k denotes the vehicle index, t is for the time period, i is the level index and j is the node index."}, {"title": "3. NEURAL NETWORKS FOR TSP", "content": "Neural networks have been explored in various ways to tackle the Traveling Sales-man Problem (TSP). The main approaches in application of NN in TSP:\nNeural Network-based Heuristics [16]: Neural networks can be trained to act as heuristics, providing approximate solutions to the TSP. These networks take as input the problem instance (e.g., node positions, distances between nodes) and output a permutation of nodes representing a potential solution. Reinforcement learning techniques can be employed to train these networks to generate high-quality solutions through trial and error.\nLearning Policies for Optimization Algorithms [13]: Neural networks can learn policies to guide optimization algorithms (such as local search or genetic algorithms) in the search for optimal or near-optimal solutions. The network learns to make decisions at each step of the algorithm based on the current state of the solution, improving the algorithm's effectiveness.\nEnd-to-End Solution Approaches [14]: Neural networks can be used to directly learn mappings from problem instances to solutions without relying on traditional optimization algorithms. These end-to-end approaches often involve training a neural network to approximate the mapping function from problem instances to solutions using large datasets of TSP instances and their corresponding optimal solutions."}, {"title": "3.1. Hopfield Neural Network Implementations", "content": "The first neural network model for solving TSP was the Hopfield neural network which was presented in 1985 [18]. The network has a recurrent architecture, as it has feedback connections that allow it to store and retrieve patterns from its state. This recurrent nature allows it to exhibit dynamic behaviour over time. Each neuron in the Hopfield network is fully connected to every other neuron, forming a symmetric weight matrix. This connectivity ensures that every neuron influences every other neuron in the network. Considering the operability, the Hopfield network is an energy-based model where each state of the network corresponds to an energy level. The network aims to minimize this energy to reach stable states, which correspond to stored patterns. The weights in a Hopfield network are typically updated using a Hebbian learning rule, which strengthens connections between neurons that fire simultaneously. This learning rule is unsupervised and is based on the principle of association between neurons. In the adaptation of the Hopfiled NN for TSP, the authors defined the following energy function as objective function in the training process Eqs. (5) and (6).\n$E = \\frac{A}{2} \\sum_x \\sum_i \\sum_j X_{xi} X_{xj} + \\frac{B}{2} \\sum_x \\sum_i \\sum_y X_{xi} X_{yi} + \\frac{C}{2} (\\sum_x \\sum_i X_{xi} - N)^2 + \\frac{D}{2} \\sum_{xy} d_{xy} \\sum_i X_{xi} (Y_{xi'} - Y_{xi'})$"}, {"title": "3.2. Graph Neural Network", "content": "The other neural network architecture suitable for solving the TSP problem is the Graph Neural Network [19]. Graph Neural Networks (GNNs) are a class of neural networks designed to process data represented in graph structures. GNNs operate on data structured as graphs, which consist of nodes (vertices) connected by edges (links or relationships). This enables them to model relationships and dependencies between elements in the data. GNNs can handle both node and edge features, allowing them to incorporate rich information associated with each node and edge in the graph. These features can be continuous or discrete, and they capture attributes of the elements in the graph. The feature vector describes among others the context of the nodes and edges, i.e. the properties of the neighbourhood elements."}, {"title": "3.3. GNN for shortest path", "content": "The GNN example presented in [20], presents a GNN network for finding the shortest path between two nodes. The implementation of the GNN is based on the Tensor-flow-GNN library, which includes among others the following operators:\n\u2022 tfgnn.keras.layers.EdgeSetUpdate\n\u2022 tfgnn.keras.layers.NodeSetUpdate\n\u2022 tfgnn.keras.layers.ContextUpdate\n\u2022 tfgnn.keras.layers.GraphUpdate\n\u2022 fgnn.keras.layers.MapFeatures\nThe framework generates training sets from connected graphs which are converted into GNN network. The network stores hidden features for all nodes and edges which denote whether the given component does belong to the shortest path or not. The main goal of the training is to determine the nodes and edges which are the components of the shortest path connecting to given nodes.\nThe implemented architecture corresponds to the deep GNN model defined in [21]. The generation of the local MLP networks to predict the labels for the edges and nodes, are based on the following algorithm:\nmlp = tf.keras.Sequential(name=\"mlp\")\nfor layer_i, size in enumerate(output_sizes):\nlayer_activation = activation\nif not activate_final and layer_i == len(output_sizes) - 1:\nlayer_activation = None\nmlp.add(tf.keras.layers.Dense(\nsize,\nactivation=layer_activation,\nuse_bias=True,\nkernel_initializer=\"variance_scaling\"\n)\nif use_layer_norm:\nmlp.add(tf.keras.layers.LayerNormalization(\nname=f\"{name}/layer_norm\"))\nreturn mlp\nAs it can be seen the network uses the standard Dense layers. In each iteration of the training, the nodes send the hidden status values to the neighbouring nodes using a message passing mechanism proposed by [22]."}, {"title": "3.4. Proposed Neural Network Architecture for TSP", "content": "In our research project we focused on the development of a special neural network for the TSP problem. In the proposed architecture, the network is aimed as optimizing the length of the cycle route. In order to optimize a global function, we re-structured the architecture of the neural network. In the proposed architecture, the error of the training process is measured not with the usual\n$E = \\sum_i (y_i \u2212 y'_i)^2$\ndifference sum function, but with the\n$E = \\sum_i |p_i|$\nwhere p\u2081 denotes the i-th section of the Hamiltonian cycle. This step requires the development of custom neural network layers which can be implemented with the graph flow model of the Tensorflow framework.\nAs the usual optimization engine of the neural network architecture, the backpropagation module, implements a gradient descend optimization strategy, the located optimum is usually only a local optimum position. In the different optimization methods, some random relocation steps are involved to give chance of a cost improvement in the undiscovered areas of the optimization space. In the case of genetic algorithm, the following genetic operators are utilized for stochastic steps:\n\u2022 crossover: exchange of different sections of the feature vectors\n\u2022 mutation: modification of some section of the feature vector\nThe proposed architecture contains a modified optimization engine which expands the gradient descend approach with additional crossover and mutation operations. This modification can improve the general efficiency of the embedded backpropagation engine. We remark that one interesting consequence of the structural extension is that we have a unique cost function run during the training, as is shown in Fig. 3."}, {"title": "4. TEST EXPERIMENTS", "content": "The implementation of the test environment was developed in the popular Python framework. The neural network objects were built up with Tensorflow-Keras library. In this case, the NN object can be constructed by adding the component layers to the current architecture in a graph flow formalism. The main benefit of this approach is that is more flexible, and it provides more functionality than the usual sequential network model.\ninputs = Input(shape=(1,))\noutput = MG_SN_Layer(N_points,Dim,f_modelb,\nactivation='linear',name=\"mydense\")(inputs)\nmg_sn_model = Model(inputs, output)\nmg_sn_model.compile(optimizer='sgd', loss='mse')\nsprecbn =\nCustomCallbackBN(N_points,Dim, Lcalc,Blen\nIn the new model, the calculation of the current cost (error) function depends on the problem domain, thus a custom layer is needed to implement these calculations. As the next code snippet shows, we can use the Tensorflow library to efficiently implement any calculations within the custom layer.\ndef call (self,inputs):\nt1 = tf.constant([0.3,0.7])\nt2 = self.kernel - tl\nt3 = tf.multiply(t2,t2)\nt4 = tf.reduce\\_sum(t3,axis=0)\nt5 = tf.constant([1.3])\nt6 = t5 + t4\nreturn t6\nRegarding the test results, we can see that this approach provided superior efficiency compared to the previous approaches, it generated very good approximations in the tested cases with small and medium size networks. Figure 4 shows some key steps in the training process to determine the winner cycle."}, {"title": "5. CONCLUSION", "content": "The VRP and TSP are very important problem domains in transportation and logistics. Due to the complexity of the problem, some kind of heuristics are used to solve the problems of practical sizes. Beside the dominating conventional methods, like the Lin-Kernighan and Helsgaun method, there is an increasing interest in application of so mind of machine learning approaches. In this paper, we investigated the application potential of neural network technology.\nIn the first approaches of NN-TSP integration, the Hopfield network was the dominating architecture model, it provided the best efficiency. In the current years, the focus of the research turned to the application of the graph neutral network architecture as TSP is defined on graph structure. The performed analysis shows that the standard GNN model has a low efficiency for TSP as it works with local optimization methods while TSP requires a global optimization scope.\nAs novel approach, we presented an optimization-oriented network architecture which implements a genetic algorithm optimization layer on the top of the standard gradient descend method. The prototype of the proposed architecture was implemented with custom layer elements of Tensorflow framework. The performed tests show that the proposed model dominates the regular GNN-based approaches."}]}