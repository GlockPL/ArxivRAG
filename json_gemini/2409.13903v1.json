{"title": "CI-Bench: Benchmarking Contextual Integrity of AI Assistants on Synthetic Data", "authors": ["Zhao Cheng", "Diane Wan", "Matthew Abueg", "Sahra Ghalebikesabi", "Ren Yi", "Eugene Bagdasarian", "Borja Balle", "Stefan Mellem", "Shawn O'Banion"], "abstract": "Advances in generative AI point towards a new era of per- personalized applications that perform diverse tasks on behalf of users. While general AI assistants have yet to fully emerge, their potential to share personal data raises significant pri- vacy challenges. This paper introduces CI-Bench, a com- prehensive synthetic benchmark for evaluating the ability of AI assistants to protect personal information during model inference. Leveraging the Contextual Integrity framework, our benchmark enables systematic assessment of information flow across important context dimensions, including roles, in- formation types, and transmission principles. We present a novel, scalable, multi-step synthetic data pipeline for generat- ing natural communications, including dialogues and emails. Unlike previous work with smaller, narrowly focused evalu- ations, we present a novel, scalable, multi-step data pipeline that synthetically generates natural communications, includ- ing dialogues and emails, which we use to generate 44 thou- sand test samples across eight domains. Additionally, we formulate and evaluate a naive AI assistant to demonstrate the need for further study and careful training towards per- sonal assistant tasks. We envision CI-Bench as a valuable tool for guiding future language model development, deployment, system design, and dataset construction, ultimately contribut- ing to the development of AI assistants that align with users' privacy expectations.", "sections": [{"title": "Introduction", "content": "Autonomous AI assistants (Gabriel et al. 2024; Wang et al. 2024) based on language-based models have increasingly gained the capability to make use of data from users thanks to recent advances in external memory (Wu et al. 2022; Kim, Kim, and Bak 2023), larger context windows (Chen et al. 2023; Peng et al. 2023; Fischer 2023; Rana et al. 2023), calls to external memory or APIs (Ng et al. 2023; Huang et al. 2023). AI assistant access to users' data-whether via model parameters, input within the context window, or tool calls to external memory or other APIs-enables various person- alized applications, such as email composition, form fill- ing, calendar management, and conversational engagement. However, these applications can also introduce privacy risks and inadvertently expose users' information (Carlini et al. 2020; Wang et al. 2023a). To assess the privacy risks of an AI assistant with access to user information, we employ the contextual integrity (CI) framework, which defines privacy as appropriate informa- tion flow according to norms that are specific to the rele- vant context (Nissenbaum 2009, 2004). The CI theory out- lines several key parameters in analyzing information flows for potential violations in sharing user information: context, actors, information type, and transmission principles (Barth et al. 2006). The context contains features related to activi- ties during the information flow. Three actors are involved in a data transmission: the sender (AI assistant), the recipient, and the data subject. Information type reflects the attributes to be shared, while the transmission principle outlines terms and conditions that the sender and recipient adhere to. For example, when booking a medical appointment on behalf of a user, the user's AI assistant transfers their medical history from some private source to the medical office staff under the physician-patient privilege, including associated regula- tions, by default."}, {"title": "Background", "content": "Analyzing personal information protection capabilities in privacy-conscious AI assistants with contextual integrity faces many limitations and has not been sufficiently stud- ied. Few works have considered inference-time privacy leak- age at all, and these either consider specific application ar- eas or do not address real-world complexities. For exam- ple, the performance analysis of many proposed AI models and systems is often based on limited experimentation, such as focusing on a single application domain (Ghalebikesabi et al. 2024), a single data format (Bagdasaryan et al. 2024), or specific platforms (Kumar et al. 2020), leaving unknown generalization of said systems to broader, real-world appli- cations. Wang et al. (2023a); Sun et al. (2024) provide comprehen- sive benchmarks for LLM safety with parts of the dataset fo- cusing on privacy. While the privacy tasks focus on training data leakage, they could be repurposed to assess inference- time privacy reasoning capabilities. However, their bench- marks are limited to uniform protection of sensitive infor- mation regardless of context. Mireshghallah et al. (2023) in- creases the complexity of test cases by adding related con- text features but they do not consider norms that govern the information exchange. Their privacy-related dataset is fur- ther limited by a very small sample size, in the hundreds. Concurrent work (Bagdasaryan et al. 2024) analyzes the ca- pabilities of LLMs with respect to leakage of contextually inappropriate information in Q&A tasks. Not only is this task simpler than the ones we consider here, but the dataset is also annotated automatically and does not provide labels following privacy norms as elicited by human ratings. More importantly, existing evaluations (Bagdasaryan et al. 2024; Shvartzshnaider et al. 2019; Kumar et al. 2020) usually con- tain narrowly focused test cases in a single domain such as Q&A and lack a systematic view of what specific skill constrains the AI assistant's capability in protecting user in- formation. Notably, these evaluations fail to distinguish be- tween models that exhibit proficiency in identifying sensi- tive data but demonstrate weakness in aligning it with appro- priate contexts, and models that effectively align data with suitable contexts yet exhibit limitations in discerning the ap- propriateness of the data flow."}, {"title": "Contributions", "content": "Our primary contribution is a comprehensive benchmark that enables fine-grained understanding of AI assistants' ability to assess the appropriateness of information flow based on all parameters defined by the CI framework. CI- bench consists of a new dataset that covers both structured information flow scenarios (comprising context, actors and information attributes, etc.) and unstructured task scenarios encompassing these information flows (such as dialogues or email exchanges), and corresponding tasks on context un- derstanding, norm identification and appropriateness judg- ment. We also present a scalable data generation pipeline that leverages real-world structured data to generate such synthetic, unstructured conversational data. We argue that CI-Bench is the most comprehensive benchmark today for personal data transmission, encom- passing diverse domains, contextual parameters, and data formats. CI-Bench can be used for evaluating AI assis- tants' ability to protect personal information, facilitating fu- ture model development and/or system design, and guiding dataset construction for model post-training and fine-tuning. Since general-purpose AI assistants are not yet readily available, we demonstrate the effectiveness of our bench- mark by evaluating AI assistants prototyped with large lan- guage models such as Gemini (Gemini Team, 2023). The experimental results demonstrate that existing models have critical room for improvement on appropriateness judging tasks. We also find that small models suffer at context un- derstanding, which we consider foundational for reason- ing about the appropriateness of information flows. Finally, well-defined rules and guidelines for data flow significantly improve the accuracy of models in assessing information ex- changes."}, {"title": "Benchmark", "content": "In this section, we outline our desired criteria for a bench- mark designed to assess AI assistants' proficiency in manag- ing information flow while protecting user privacy. Building on existing research and recent breakthroughs in LLMs, we break down the intricate process of information flow medi- ation. This exercise not only enables a thorough evaluation of AI assistant capabilities but also highlights specific areas where they may be lacking in safeguarding user information. Previous research (Kumar et al. 2020; Shvartzshnaider et al. 2019) has operationalized CI in NLP systems through a two-phase process: extracting CI parameters from user- provided data and comparing data flows against established norms. This approach allows for separate evaluations of a model's contextual understanding and its ability to judge ad- herence to norms. Recent advances in LLMs (Yang et al. 2023) suggest a new paradigm that expands AI assistant capabilities with two additional phases. LLM-based AI as- sistants can learn relevant norms from human instructions (Ouyang et al. 2022; Wang et al. 2023b; Bai et al. 2022), and then generate appropriate responses from user requests according to learned norms. As shown in Figure 1, a typ- ical AI assistant facilitates information exchange between the user and various third-party entities, including other AI assistants, human users, APIs, and other groups of agents. It accesses user information and user expectations, interacts with another party to achieve specific goals, and ultimately executes tasks on the user's behalf. Building upon this understanding, our benchmark dissects information flow mediation into four fundamental compo- nents:\nContext Understanding Accurately extracting contextual parameters of the information flow, especially in raw, un- structured data like dialogues is crucial for AI assistants. Nissenbaum (2009) proposes five required CI parameters: sender, recipient, data subject, information attribute, and transmission principle. We incorporate additional elements for a more holistic understanding. These elements include user intention, the domain of the interaction, and traces (pre- vious interactions), all of which contribute to a richer inter- pretation of the context.\nExpectation Identification Recognizing both social norms and individual preferences that govern information sharing is essential. While this benchmark focuses on social norms, future work will delve into personalized preferences. This step involves identifying the most relevant norms from a pool of potentially applicable ones. We highlight the difficulty of this tasks as norms can manifest at different levels of detail, ranging from broad principles to rules that govern particular situations.\nAppropriateness Judgment This step assesses the AI as- sistant's ability to align its actions with applicable norms, whether implicitly embedded within the model or explic- itly stated. This involves making nuanced judgments about whether sharing specific information within a given context adheres to established norms.\nResponse Generation Based on the appropriateness judg- ment, the AI assistant generates a response to the third party, ensuring it complies with relevant norms and user expecta- tions. These responses may include portions of user infor- mation when the request for information is granted. While it is not strictly required that an assistant executes these steps explicitly as part of its normal operation-it could operate as a single black box that takes in the relevant inputs and generates outputs directly an AI assistant that handles user information flow should be able to solve these subproblems accurately if explicitly asked. Moreover, this decomposition enables us to understand what specific skill constrains the AI assistant's capability in protecting user in- formation, a desirable property for the CI benchmark.\nIllustrating the Framework: A Case Study Figure 2 presents a case study demonstrating this framework in ac- tion. The scenario involves a user interacting with a doctor's office staff via an AI assistant, whom has access to user in- formation and user expectations.\nUnderstand Context: The AI assistant must accurately interpret the context of the user's request. A multiple- choice question (MCQ) tests the identification of the in- formation attribute requested in the dialogue, location. Conflating location (choice A) with investment portfolio (choice B) could lead to errors in whether the requested information can be shared.\nIdentify Expectation: From a set of potential norms, the AI assistant needs to identify the relevant one, consid- ering factors like the user tends to find available appoint- ments at alternative offices when the original office is un- available.\nJudge Appropriateness: Based on the identified norm, the AI assistant must determine if sharing the user's location is appropriate.\nGenerate Response: Finally, the AI assistant generates a response that shares the user's location, and note that re- sponding with the user's birthday is inappropriate. This case study highlights the importance of each compo- nent in ensuring responsible and privacy-aware information sharing by AI assistants."}, {"title": "Dataset", "content": "This section details the CI-Bench dataset, designed to assess AI assistant's ability to understand and apply the principles of CI within conversational settings. Our ultimate goal is to create a suite of sub-datasets encompassing diverse real- world scenarios, such as fulfilling online orders, make ap- pointments, etc. For this paper, we focus on two key target scenarios: chat assistants and email writing assistants, re- flecting the increasing prevalence of AI in these domains. To address these use cases, we introduce two data formats: di- alogues and email threads. Both datasets apply CI concepts in a multi-turn format, mimicking realistic interactions be- tween users and AI assistants. Before detailing our current dataset construction ap- proach, we reflect on prior efforts and insights gained from their evaluation. Our initial attempts to build a challenging benchmark by injecting adversarial requests to real-world conversations proved insufficient. Models easily deciphered risk levels due to the overwhelming presence of harmless dialogues, even with the added adversarial injections. This imbalance, alongside limitations in synthetic dialogue gen- eration, necessitated a more advanced approach to dataset creation. Thus, we introduce structured inputs as a founda- tional element in our dataset construction process. The CI-Bench dataset is presented in text format, each test case containing a prompt (input) and a label (expected out- put). The input prompt consists of three components: task scenario, user information, and user expectations. The task scenario is a required component for all the evaluation tasks, providing the AI assistant with relevant information about the task. The user information and user expectations are op- tional components, necessary for generating responses, but can be ignored for tasks like context understanding. Test cases are organized in three layers. The first layer contains two data formats: multi-turn dialogue and email ex- changes. Each data format encompasses four benchmark phases, as detailed in the Benchmark section. Each benchmark phase contains several evaluation tasks. Overall, the CI-Bench dataset comprises a total of 44,100 test cases, with a neg- ative to positive label ratio of 7.4:1, spanning the eight dis- tinct domains illustrated in Table 1. To generate the data samples, we establish a multi-step pipeline that synthetically generates natural communica- tions, such as dialogues and emails, from structured sce- nario data. This pipeline first extracts key characteristics from publicly available real-world dialogues and fills in val- ues for each column of the data structure in Table 2. It then utilizes an LLM to synthetically generate realistic task sce- narios based on these structured scenarios. Finally, it pro- grammatically generates test questions for each benchmark phase introduced in the Benchmark section. All test ques- tions are presented alongside essential background informa- tion, such as the task scenario, user data, and/or user expe\u0441- tations.\nStructured Scenarios Each structured scenario encom- passes four categories: context, actors, information at- tributes, and transmission principles. Within the context category, we include a domain defining the task prob- lem, a user intention capturing the user's goal during the task, and a trace containing interaction history and de- tails. This structure allows us to permute values within each category. For instance, we compiled a set of 50 unique information attributes for user data, as shown in the Appendix.\nTask Scenarios We utilized an LLM model to generate realistic task scenarios based on the structured scenar- ios, employing a prompt template, as shown in the Ap- pendix. In designing this prompt, we ensured the accu- rate inclusion of all CI data components within the gen- erated scenario, while avoiding irrelevant context switch- ing that could arise from multi-turn communications. As demonstrated in Table 3, user intention, information at- tributes, and other relevant context elements are precisely conveyed in the conversation.\nEvaluation Tasks We design choice questions to assess AI assistants' capabilities. For example, to assess context understanding, we designed MCQs where the AI must choose the relevant context element for a given scenario. The context element, such as an information attribute or user intention, has one correct answer and a few incorrect choices randomly selected from other possible values of that feature."}, {"title": "Experiments", "content": "To validate the utility and discriminative power of the pro- posed dataset, we performed evaluations on AI assistants prototyped with Gemini 1.0 models in three different sizes, Ultra, Pro, and Nano (Gemini Team, 2023). Experiments re- vealed that AI assistants demonstrated robust overall per- formance, yet exhibited potential for improvement in dis- cerning subtle distinctions and distinguishing among various types of information flow. Smaller models struggled more, highlighting the impact of model size on accuracy. Well-defined guidelines significantly enhanced performance, sug- gesting a key strategy for future development."}, {"title": "Understanding Context", "content": "To assess language models' ability to understand and extract relevant information from complex contexts, CI-Bench in- cludes MCQ tasks for each contextual information parame- ter. Figure 3 presents the evaluation results across different model sizes. In this evaluation, models generally performed well in two common features: information attributes and user in- tention. However, they consistently underperformed in re- cipient. This may suggest that the model we tested has a better understanding of information attributes and user in- tention, leading to better performance. It also suggests a po- tential insensitivity to the recipient or its changes. We have also observed that the AUC numbers increase with model size. Given the reliance of subsequent tasks on accurate con- textual understanding, these smaller models are expected to face more challenges in these areas. No significant perfor- mance difference was observed when comparing data for- mats between dialogue and emails. This suggests that all models can handle these formats relatively well."}, {"title": "Identifying Relevant Norms", "content": "While a model may be able to understand the context, it may not have a natural understanding of what societal norms are relevant and appropriate in a given setting. We frame this as an MCQ, asking the model to identify the correct norm. The remaining options are variants of the correct norm, where CI parameters have been changed to either be irrelevant or incorrect for the scenario. We evaluate performance for the same set of models, reporting AUC."}, {"title": "Judging Appropriateness", "content": "We next investigated the influence of expectations on a model's ability to assess the appropriateness of information sharing. To do this, we framed a task as True/False ques- tions, where models judged the appropriateness of a given context. These test cases were annotated by human experts, providing the ground truth for AUC computation. The anno- tation process involved assigning a True/False label to each sample within the structured scenarios (Table 2), encom- passing all relevant parameters. This task was undertaken by a subgroup of the paper's authors, who engaged in thorough discussions and reviews on appropriateness of the informa- tion flow defined within this structured data. We separated our analysis into scenarios where expecta- tions were explicit in the provided text and where they were implicit. AI assistants based on Gemini models, aligned with human values, can judge appropriateness even without ex- plicit expectations. The results are illustrated in Figure 5. As expected, performance for the implicit case of the Nano model was significantly worse, approaching randomness in the worst case. However, performance improved with larger models and significantly improved when the norm was made explicit. In Figure 6, we presented the performance by domains without providing explicit norms. While there were some minor variations in AUC numbers, we did not observe sig- nificant performance differences among the domains evalu- ated. However, performance consistently improved with in- creasing model size, which aligns with our earlier findings."}, {"title": "Generate Response", "content": "Generating the response for an information request is the fi- nal step in the proposed framework. We framed this task as an open-ended question, given the user's information and the scenario context. The model was asked to generate a short sentence to the request. If sharing the user's information is appropriate, the response may include it. Otherwise, the re- quest will be refused. We used another LLM to evaluate the generated response against annotations by human experts. The result is available in the Appendix."}, {"title": "Discussion", "content": "CI-Bench provides a crucial first step toward develop- ing Al systems capable of navigating the complex land- scape of assistant-driven information sharing. By using CI-Bench to evaluate prototype AI assistants backed by exist- ing large models, we provide a more granular understanding of current capability gaps. Meanwhile, we acknowledge CI- Bench's limitations below and encourage community con- tributions in addition to our future work to address them. We aim to collaboratively refine this benchmark and drive progress towards AI assistants that are both effectively and ethically aligned."}, {"title": "Experimental Takeaways", "content": "The three main takeaways from our experimental results are as follows:\nStrong performance, but room for growth: Naive AI Assistants built on top-tier language models demonstrate impressive capabilities in understanding contextual in- formation, but struggle in the more nuanced task of judg- ing appropriateness given that context. They frequently fail to notice subtleties, such as identifying when a data subject's role shifts (e.g., from customer to employee).\nModel size matters: Assistants built on smaller mod- els face challenges in understanding the broader context and distinguishing between different types of informa- tion flow. This can lead to overly cautious behavior, flag- ging normal information exchanges as inappropriate due to an inability to differentiate subtleties.\nClear norms enhance performance: Well-defined rules and guidelines for data flow significantly improve the accuracy of models in assessing information exchanges. This finding offers a clear path for future enhancements."}, {"title": "Limitations and Future Work", "content": "Although CI-Bench is larger and more diverse than previous datasets in the CI domain, there are many directions for it- eration and expansion. Among key limitations that warrant further investigation are:\nLabel bias and subjectivity: The manual labeling pro- cess, conducted by two experts, may introduce bias and inconsistencies. Observed disagreements between ex- perts underscore the inherent subjectivity in judging ap- propriateness, which may vary greatly from person to person. Ground truth labels may not always exist in any universal way.\nCultural norms: Societal norms regarding data sen- sitivity are not universal, but rather are subject to regional, cultural, and sociopolitical systems differ- ences. The human labels used in these studies are likely to be skewed by such variations. Future it- erations of the benchmark could incorporate diverse cultural perspectives. To address this limitation and gain a more comprehensive understanding of this phe- nomenon, future research should prioritize sampling from a more diverse rater pool, encompassing a wider range of perspectives and backgrounds.\nPersonal preferences: Even within a shared geopoliti- cal, sociocultural landscape, different individuals have their own preferences related to privacy and informa- tion sharing, and these preferences can be strong. As above, future research would benefit from a more di- verse rater pool. In addition, there is a significant op- portunity to examine strategies for reconciling per- sonal and shared privacy expectations.\nLack of justification: Currently, the benchmark lacks a mechanism for models to provide justifications for their judgments. Enabling models to articulate their reasoning would offer valuable insights into their decision-making processes and facilitate more nuanced evaluations. This could involve incorporating an additional verifier or em- ploying explainability techniques.\nBinary label: For simplicity, contextual appropriateness is measured using binary appropriate/inappropriate la- bels, without considering more complicated cases like \"unknown\" or \"maybe\". Alternative labeling schemes, such as Likert scales (Joshi et al. 2015), could provide a more nuanced understanding of model capabilities, es- pecially for the difficult and borderline cases which are, by nature, most interesting.\nSelf-contained samples: The tasks contained in the dataset assume each dataset sample to be self-contained and stateless, without access to memory or history traces beyond itself, and with no assumed correlation among roles for sender, recipient, and data subject. For simplic- ity, we only include one context in each interaction ses- sion, never multiple contexts nor context transitions. Fu- ture work could involve incorporating multi-context ses- sions and make use of extraneous information to produce more challenging tasks and better exercise the capabili- ties of modern long-context models.\nHonesty: Role authenticity is not verified, assuming fully real and honest interactions. Adversarial interac- tions have been considered in some recent work such as (Bagdasaryan et al. 2024), and incorporation of that space into CI assistant evaluation benchmarks will even- tually be necessary to determine their suitability for real- world deployment. Future work here could include role verification.\nDisentangling performance issues: Suboptimal perfor- mance on CI-Bench can arise from limitations of the un- derlying model, fine-tuning, prompting, or inherent is- sues within the task setup. We encourage interested par- ties to delve deeper into particular models and variations to better understand how to improve those models.\nMultimodality: CI-Bench deals only with natural lan- guage text, but future AI assistants may also interact with other modalities of data that bring their own sensitivi- ties and contextual associations. Future work could ex- plore multimodal user data, norms about non-text data, as well as multimodal interactions with third parties, such as sharing or conversing about imagery."}, {"title": "Conclusion", "content": "We introduce CI-Bench, a comprehensive benchmark de- signed to evaluate AI assistants' ability to protect per- sonal information during inference, leveraging the Contex- tual Integrity framework and a new, scalable data genera- tion pipeline. Our diverse data samples, spanning various domains and context features, enables a fine-grained un- derstanding of model capabilities in navigating informa- tion flow. Initial evaluations reveal that while state-of-the- art language models demonstrate promising zero-shot per- formance on the task, they still encounter challenges with nuanced scenarios involving multiple topics and context switching. We also find that providing explicit context- specific norms significantly improves a model's ability to judge information appropriateness, underscoring the impor- tance of well-defined guidelines in guiding models towards privacy-preserving behavior."}, {"title": "Synthetic Data Generation", "content": "Test cases in the dataset comprises 49 distinct information pieces, systematically categorized into seven types: personal identifiers, demographic information, behavioral informa- tion, financial information, health information, psycholog- ical information, and other sensitive information, as detailed in Table 4. Each information piece represents a unique at- tribute that could potentially be disclosed within a conversa- tional context. To rigorously assess the model's capacity to navigate diverse conversational scenarios, we have meticulously crafted 50 distinct scenarios spanning eight domains, as elu- cidated in Table 5. These scenarios are designed to encom- pass a broad spectrum of real-world conversational contexts, thus ensuring a comprehensive evaluation of the model's performance in handling sensitive information across vari- ous situations. The prompt templates employed in our evaluation frame- work are presented in Table 6. The initial two prompts serve to generate scenarios, incorporating parameters as specified in Table 2 of the paper. The subsequent four prompt tem- plates are dedicated to the evaluation of the four core tasks delineated in Section Benchmark. All of these prompt tem- plates receive the scenario (either in dialogue or email for- mat) as input."}, {"title": "Additional Experiment Results", "content": "All experimental results presented in this paper were derived from zero-shot responses elicited from the AI assistant. For tasks necessitating open-ended responses, the AI assistant leveraged the Ultra model with a temperature setting of 0.9. In contrast, choice-based evaluations, such as selecting one option from four or making binary Yes/No decisions, were conducted using the Gemini model, employing a greedy in- ference approach. In this latter scenario, probabilities were assigned to each candidate choice, with the selection ulti- mately determined by the choice exhibiting the highest prob- ability."}, {"title": "Judgment by Domain and Information Category", "content": "Following Figure 5 in the paper, we conducted a granular analysis of the experimental results, dissecting them by both domain and information category. As detailed in Table 4, each category encompasses a specific set of information at- tributes that were strategically incorporated into the scenar- ios. To gauge an AI system's proficiency in preventing inad- vertent leakage of sensitive information, we employed speci- ficity as our metric of choice. Specificity is defined as the probability of test cases being accurately classified as \"not appropriate to share,\" given the entire set of genuinely neg- ative instances. Figure 7 presents the appropriateness judgment results specifically for dialogues. Within this figure, each cell value corresponds to the specificity of appropriate judgment ex- hibited by a system utilizing the Gemini Ultra model, with consideration given to each domain and information cat- egory. The results reveal that demographic and financial information are generally handled adeptly by the system. In contrast, safeguarding behavioral information presents a more formidable challenge, as evidenced by its compara- tively lower specificity value. When examining the perfor- mance across different problem domains, it becomes appar- ent that Healthcare and Finance emerge as particularly de- manding, especially when juxtaposed with domains such as Education and eCommerce. Further investigation into the experimental results un- veiled a subtle yet noteworthy trend: the specificity metric experiences a slight dip in test cases presented within email scenarios, as depicted in Figure 8. Echoing the observations from the dialogue analysis, demographic and financial in- formation once again attained higher specificity metric val- ues. Conversely, test cases involving PII exhibited dimin- ished performance within certain problem domains, under- scoring the heightened sensitivity and complexity associated with handling such data."}, {"title": "Generate Response", "content": "In alignment with the methodology outlined in Section Gen- erate Response, we present the experiment results. The effi- cacy of the generated responses is assessed through a com- posite metric encompassing two distinct groups. In scenarios where the requested information is deemed shareable (pos- itive test group), we calculate the probability of instances where the information is indeed shared and the response ac- curately conveys the pertinent details. Conversely, in scenar- ios where the information should remain confidential (nega- tive test cases), we compute the probability of responses that explicitly decline the information request. It is important to note that a range of responses are classified as incorrect. For instance, even when the model opts to share information, it might inadvertently disclose only a partial segment of the re- quested data or divulge unrelated information. Furthermore, any response that fails to explicitly reject an inappropriate information request is also categorized as incorrect. In summation, this particular evaluation task proves to be considerably more intricate than the preceding three tasks. It necessitates a synergistic integration of skills honed in prior tasks, encompassing context comprehension, norm identifi- cation, and appropriateness judgment. Moreover, it demands the ability to assimilate personal information and subse- quently articulate it fittingly within a textual response. As demonstrated in Figure 9, the AUC metric values across all three models fall within the range of 0.3 to 0.5. This rel- atively modest value underscores the formidable nature of this task within the context of the system under scrutiny. We earnestly encourage future research endeavors to address and enhance this aspect, as elaborated further in the \"Limi- tations and Future Work\" section of this paper."}, {"title": "Utility vs Privacy-Consciousness", "content": "We quantified the inherent trade-off between utility and privacy-consciousness by employing the metrics of sensi- tivity and specificity. Utility, synonymous with sensitivity in this context, strives to maximize the volume of perti- nent information shared or utilized. Conversely, privacy- consciousness, analogous to specificity, prioritizes the mini- mization of sensitive or personal data disclosure. Achieving an optimal equilibrium between these two competing objec- tives is paramount, and we represent this dynamic interplay through a two-dimensional sensitivity-specificity plot, as il- lustrated in Figure 10. Furthermore, our experiment result underscores a note- worthy distinction in performance across different interac- tion modalities. The AI assistants exhibited superior per- formance in conversational settings (dialogues) compared to email scenarios, suggesting a heightened aptitude for comprehending and generating contextually appropriate re- sponses within the dynamic ebb and flow of a conversation. This observation accentuates the critical importance of fac- toring in both model size and the specific interaction format when embarking on the development and evaluation of AI assistants."}]}