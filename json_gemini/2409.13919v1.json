{"title": "MEASURING ERROR ALIGNMENT FOR DECISION-MAKING SYSTEMS", "authors": ["BINXIA XU", "ANTONIS BIKAKIS", "DANIEL ONAH", "ANDREAS VLACHIDIS", "LUKE DICKENS"], "abstract": "Given that AI systems are set to play a pivotal role in future decision-making processes, their trustworthiness and reliability are of critical concern. Due to their scale and complexity, modern AI systems resist direct interpretation, and alternative ways are needed to establish trust in those systems, and determine how well they align with human values. We argue that good measures of the information processing similarities between AI and humans, may be able to achieve these same ends. While Representational alignment (RA) approaches measure similarity between the internal states of two systems, the associated data can be expensive and difficult to collect for human systems. In contrast, Behavioural alignment (BA) comparisons are cheaper and easier, but questions remain as to their sensitivity and reliability. We propose two new behavioural alignment metrics misclassification agreement which measures the similarity between the errors of two systems on the same instances, and class-level error similarity which measures the similarity between the error distributions of two systems. We show that our metrics correlate well with RA metrics, and provide complementary information to another BA metric, within a range of domains, and set the scene for a new approach to value alignment.", "sections": [{"title": "1. INTRODUCTION", "content": "With significant advancements in AI development, the alignment of AI with human values has increasingly drawn attention within the community. Generally, AI alignment focuses on aligning the performance of AI systems towards goals [67, 43, 55], preferences [59], and social norms [28, 13] intended by humans. Improved human alignment can help build more reliable and trustworthy AI systems. Considering the potential for AI systems to play a crucial role in future decision-making processes, the trustworthiness and reliability of these systems emerge as critical concerns, particularly in applications such as medical diagnosis and autonomous driving. Studies in cognitive science have demonstrated that a model more closely aligned with the internal mechanism of the human brain can improve the robustness of visual-based decision-making tasks [7]. Studies that compare internal representations used by systems are called representational alignment (RA). RA approaches are often presented as the gold-standard, as similarities between representational structures of systems can provide deep insights about the alignment between the internal processing of these systems [35, 26]. Nonetheless, RA studies are often limited by the high-costs and practical challenges associated with collecting and comparing complex, heterogeneous and difficult to access internal representations, e.g. via fMRI in humans. In contrast, the study of how a system behaves can also inform us about both animal and human systems [58, 18, 52] as well as computational systems [15, 16]. However, questions remain about how much behavioural alignment (BA) approaches can tell us about deeper similarities in the internal processing of systems [25, 52, 60]."}, {"title": "2. METRICS OF ERROR ALIGNMENT", "content": "As argued in [14], investigating whether two systems consistently make errors on the same stimuli can help to investigate the similarity of decision-making strategies behind the response. They propose error consistency (EC), to measure behavioural similarity between two classification systems in these terms. More precisely, consider dataset $\\mathcal{D} = \\{(x_n,t_n)\\}_{n=1}^N$ comprising inputs in data (stimulus) space $x_n \\in \\mathcal{X}$, and target labels in finite label space $t_n \\in \\mathcal{Y}$ of C classes. Let A (resp. B) be a classification system that makes prediction $y_n^A$ ($y_n^B$) for each input $x_n \\in \\mathcal{X}$.  On $\\mathcal{D}$, there are $N_c$ jointly correct instances ($t_n = y_n^A = y_n^B$), and $N_e$ jointly incorrect instances ($y_n^A \\neq t_n \\neq y_n^B$). The observed error overlap is the proportion on which A & B agree, $P_{obs} = (N_e + N_c)/ N$. This is contrasted using the error overlap expected by chance:\n\n$P_{exp} = P_aP_b + (1 - p_a)(1 - p_b)$\n\nwhere $p_a$ ($p_b$) is the accuracy of A (B). And the EC measure is Cohen's kappa ($\\kappa$) [4] based on these values:\n\n$EC(A, B) = \\frac{P_{obs} - P_{exp}}{1 - P_{exp}}$"}, {"title": "2.1. Misclassification Agreement.", "content": "To directly address this insensitivity to differences in the dual-misclassification region, we propose a novel metric called Misclassification Agreement (MA). We first motivate this with an everyday analogy - student performance in exams. In"}, {"title": "2.2. Class-Level Error Similarity.", "content": "Unlike MA, which is based on instance-level comparisons (referred to as trial-by-trial in [14]), our second metric Class-Level Error Similarity (CLES) seeks to measure the similarity between predictions of two systems A & B at the class-level, again based on system errors. More precisely, we define, for system $g\\in \\{A,B\\}$, the error"}, {"title": "3. EXPERIMENT", "content": "This section describes evaluations on our two new BA alignment metrics, alongside pre-existing BA and RA alignment metrics. We aim to investigate what these metrics can tell us about the similarities between pairs of systems both within, and across domains, including human and deep neural network systems. We also wish to evaluate whether, and to what degree, different metrics provide overlapping or complementary information about the similarities between systems. This includes, to our knowledge, the first systematic study on correlations between BA and RA metrics across a range of settings, datasets and system types."}, {"title": "3.1. Dataset and Metrics.", "content": "Our experiments include two groups of classification datasets: synthetic and naturalistic. The synthetic group contains 14 subsets of the modelvshuman image dataset [16], with both machine and human predictions. The naturalistic group comprises three challenging datasets: one image recognition task - ImageNet-A [24]), and two video-based Human Activity Recognition (HAR) tasks \u2013 MPII-Cooking [53] and Epic-Kitchen [6]. For the image datasets, we evaluate a selection of ImageNet1K pre-trained models (see supplementary materials), and use the complete modelvshuman and ImageNet-A data for testing. For video datasets, models were trained on the corresponding training set. In all cases, alignment measurements are based on corresponding testing sets. For every pair of systems, across all datasets, we evaluate the three BA metrics already described: EC, MA and CLES, alongside 3 RA metrics: CKA [34]; SOC, the average Jenson-Shannon Divergence (JSD) between the confidences (soft-max logits) of the two systems; and SOCE, the SOC measure applied to jointly incorrect predictions only. CKA is a state-of-the-art RA metric able to reliably measure multivariate similarity between arbitrary representational spaces. SOC (and SOCE) embodies our own approach to measuring confidence alignment. It is similar in character to the soft cross-entropy loss from [49], but SOC is symmetric and more gracefully deals with zero confidences. SOC is also similar to the Hellinger distance based approach from [37], except SOC better reflects the information geometry of the space of confidences. More dataset and RA metric details are given in supplementary materials."}, {"title": "3.2. Error Patterns between Systems.", "content": "We evaluate the alignment of each distinct pair of systems by different BA metrics, producing a single score for each model pair on each metric."}, {"title": "3.3. Correlations across Levels of Alignment.", "content": "From Figure 4, we observe some evidence of correlation and some complementarity among the BA metrics, and we argue that the complementarity may partly arise due to the influence of domain conditions on different metrics. To explore this further, we report in-domain Spearman's r between pairs of metrics EC, MA and CLES in Table 1 (columns 1, 2 & 3), for all synthetic domains and naturalistic domains. We use Spearman's r to measure rank correlations as this captures relationships even when they are non-linear (as between MA and CLES). Note that, EC and MA are weakly-negatively correlated, with r = -0.23 (r = -0.29) when all synthetic (naturalistic) scores are grouped together. However, they consistently show (typically strong) positive correlation within domain. Strong positive in domain correlation can also typically be seen for EC vs CLES and MA vs CLES, supporting our claim that CLES captures elements of both metrics. Notable exceptions include: EC vs MA on color, and EC vs CLES and MA vs CLES on Epic-Kitchen. The table also presents in-domain rank correlations between our gold-standard RA metric, CKA, and the three BA metrics. As discussed, questions remain about how effectively BA metrics can stand in for RA metrics, when the latter are infeasible. These results show that, across a"}, {"title": "4. RELATED WORK", "content": "Investigating the alignment of representations of different information processing systems is crucial for the understanding of decision-making strategies behind those systems. In RA studies, researchers have focused on measuring the similarity between internal representations of systems [35, 34, 60]. Measuring alignments across individuals usually requires the collection of signals from human brain [35, 44, 56]. For the exploration of DNN models, hidden activations or confidences are required. For example, [45, 51] conduct comprehensive comparison for the architectures of different models by use of CKA [34]. The alignment between model and humans can also be used to make the prediction of a DNN model more like humans. Peterson et al. [49] propose a novel approach that leverages human perceptual uncertainty to improve robustness of DNNs by adjusting the confidence. Several complementary works [48, 17, 11, 36, 42] have instead used behavioural patterns to reveal the difference between the predictions of neural network models and human results. The alignment of prediction can be done at the instance (trial-by-trial) level [14, 16], based on class level behaviours, e.g. confusion matrices, as in [52, 32, 31], or at the semantic level [64]. We argue that our MA metric complements methods from previous instance level approaches as it captures different features of system-system BA. Moreover, unlike previous class-level approaches, our CLES metric is more sensitive to differences as it excludes the influence of correct predictions and is based on the Jensen-Shannon divergence (JSD) which gives a symmetric, parametrization-independent differences between error distributions."}, {"title": "5. CONCLUSION", "content": "In this work, we propose two new metrics for error alignment: MA and CLES, which measure the similarity of errors between classification systems. We evaluate these metrics under a range of conditions and find they correlate well with other BA and RA metrics, and this includes the first systematic study on BA and RA correlations. In particular, our human-model findings correspond with other recent works indicating current image models to be poorly aligned with humans, although more studies are expected to be conducted on the naturalistic datasets. We argue that the latter provides a route to establishing trustworthiness guarantees for systems"}, {"title": "APPENDIX A. APPX.1: DERIVATION FOR MA", "content": "Given a classification task in domain $\\mathcal{X}$ (dataspace or stimulus space) with C unique classification targets in the set $\\mathcal{Y} = [1..C]$, and two systems A, B, dataset $\\mathcal{D} = \\{(x_n, t_n)\\}_{n=1}^N$ comprises instances $x_n \\in \\mathcal{X}$ with associated ground-truth $t_n \\in \\mathcal{Y}$. Let A (resp. B) be a classification system that makes prediction $y_n^A$ ($y_n^B$) for each input $x_n$. The error dataset for some system g, $\\mathcal{D}_{err}^g$ is those data on which g disagrees with ground truth, i.e. $\\mathcal{D}_{err}^g = \\{(x_n,t_n) \\in \\mathcal{D} : y_n^g \\neq t_n\\}$, while the joint error dataset between A and B, $\\mathcal{D}_{err}^{AB}$, is the intersection of the two systems' error datasets, i.e. $\\mathcal{D}_{err}^{AB} = \\mathcal{D}_{err}^A \\cap \\mathcal{D}_{err}^B$.\n\nThe error agreement matrix is defined as $M_{err}^{AB} \\in \\mathbb{Z}^{CxC}$. Then count the frequency of joint error predictions from systems A and B. That is the (i, j)th element counts the number of instances in $\\mathcal{D}_{err}$ for which $y_n^A = i$ and $y_n^B = j$, i.e.\n\n$[M_{err}]_{ij} = |\\{(x_n, t_n) \\in \\mathcal{D}_{err}^{AB} : y_n^A = i, y_n^B = j\\}|$\n\nMA between A and B is then Cohen's $\\kappa$ [4] of $M_{err}$, which is defined as:\n\n$MA(A, B) = \\kappa(M_{err}) = \\frac{P_o - P_e}{1 - P_e}$\n\nwhere $P_o$ the relative observed agreement among raters, and $P_e$ the probability of chance agreement under the null hypothesis that agreement is uncorrelated. The difference here is that these are calculated on the error only agreement counts. So, $P_o$ is the relative observed error agreement between the predictors:\n\n$P_o = \\frac{N_{err}}{N_{err}}$\n\nwhere the overall number of dual-error predictions:\n\n$N_{err} = \\sum_{i=1}^C\\sum_{j=1}^C [M_{err}]_{ij}$\n\nand the number of observed error agreement instances for both predictors:\n\n$N_{err} = \\sum_{i=1}^C [M_{err}]_{ii}$\n\nA higher (lower) $P_o$ indicates that two predictors tend to make more (fewer) of the same types of errors.\n\nThe hypothetical probability of chance agreement, $P_e$ for $M_{err}$, is what We would expect if the two predictors were independent on this error set. For each element $i \\in \\mathcal{Y}$, the marginal probability that predictor A would predict class i on some element of $\\mathcal{D}_{err}$ is estimated as the proportion of total counts that are present in row i of $M_{err}$, namely\n\n$\\widehat{P_i}^{(A)} = \\frac{\\sum_j [M_{err}]_{ij}}{N_{err}}$\n\nSimilarly, the counts within the j column of $M_{err}$, are used to estimate the marginal probability that system B predicts class j on a datapoint from $\\mathcal{D}_{err}$. So\n\n$\\widehat{P_j}^{(B)} = \\frac{\\sum_i [M_{err}]_{ij}}{N_{err}}$"}, {"title": "APPENDIX B. APPX.2: DERIVATION FOR CLES", "content": "Here is a more explicit description of CLES, including intuition and derivation. We assume that System A observe $n^a$ observations and System B observe $n^b$ observations given C classes. For two confusion matrices $F^{(A)}$ and $F^{(B)}$, the sum of each row $n_c^a$ and $n_c^b$ represent the number of ground truth for each c class. Then, for each instance with ground truth class c, We store the prediction results of two systems in vectors $f^A\\in \\mathbb{N}^{n^a}$ and $f^B\\in \\mathbb{N}^{n^b}$ respectively. Let's say that $f_c^A$ and $f_c^B$ are the observed values for a setting c, and there are two hypotheses about the observed data:\n\n$\\bullet$ The null hypothesis $H_0$ says that both observations are repeated i.i.d. draws from the same categorical distribution $Cat(\\pi_0)$, where $\\pi_{0_k}$ is the probability of observing class c at any one draw. In other words, they are both draws from multinomials with the same probability vector, i.e. $p^a \\sim Mult(n_c^a, \\pi_0)$ and $p^b \\sim Mult(n_c^b, \\pi_0)$.\n$\\bullet$ The alternative hypothesis $H_1$ says that both observations are repeated i.i.d. draws from the categorical distributions, but they are different: $f_c^A$ are data drawn i.i.d. from $Cat(\\pi^A)$ and $f_c^B$ are data drawn i.i.d. from $Cat(\\pi^B)$, where $\\pi_{c_k}$ is the probability of observing class c at any one draw. In other words, they are drawn from different multinomials, i.e. $p^A \\sim Mult(n_c^a, \\pi^A)$ and $p^B \\sim Mult(n_c^b, \\pi^B)$.\n\nIt is expected that if two systems are using more similar decision-making strategies when predicting data belonging to the c class, the $H_0$ would be harder to reject. Typically, when comparing the prediction patterns of two systems, there are two points about these two hypotheses We wish to know: 1) Is there sufficient evidence to reject $H_0$ in favour of $H_1$; 2) If there is sufficient evidence to reject $H_0$, then what is the effect size? In fact, in the experiments shown in this paper, for all pairs of models for the majority of classes, there is enough evidence to reject $H_0$ under even the strictest of significance levels. More importantly, the rejection significance depends on the number of datapoints which can vary substantially between situations, which inspires the interests in the effect size.\nEffect size for single pair of error distributions. Within this section, the true probability vector is denoted $\\pi_c$ while the estimate for this is denoted $\\widehat{\\pi_c}$. For simplicity, We drop the subscript c in the following description. For simplicity here, probability vector estimates are estimated as maximum likelihood estimates of the multinomial distribution from which they are assumed to be drawn, e.g. for s = {0, A, B}\n\n$\\widehat{\\pi} ^s = \\frac{f^s}{n_s}$  in other words, $\\widehat{\\pi}_k^S = \\frac{f_k^s}{n_s}$\n\nThe comparisons are constructed in terms of divergences, and in order to account for the non-Euclidean topology of the space in which sit, We will base these on information theoretic divergences. I've defined a finite set of integers from 1 to C$\\in$\\N as [[C]] (or [1..C]). The set of all probability distributions over some set X with Sigma-algebra $\\Sigma$ will be denotes $\\mathcal{P}^{\\Sigma}(X)$, where $\\Sigma$ is obvious from context, We will drop the subscript. Hence, the set of all"}, {"title": "APPENDIX C. APPX.3: REPRESENTATIONAL ALIGNMENT METRICS", "content": "Centered Kernel Alignment (CKA).. As argued in [34], a reliable similarity index for measuring representations with high dimensions should hold three properties: 1) it should not be invariant to invertible linear transformation; 2) it should be invariant to orthogonal transformation; 3)"}, {"title": "APPENDIX D. APPX.4: DATASETS", "content": "Model-vs-Human.  [16] includes 17 subsets of manually Out-of-Distribution (OOD) data, which are distorted images from the ImageNet [12] dataset. We use 14 subsets from the original 17 subsets, in each of which around 1K data samples are included. This dataset creates a mapping from 16 entry-level categories, such as dog, car, or chair, to their corresponding ImageNet categories using the WordNet hierarchy [41]. The human recognition for each data sample is provided alongside the images.\nImageNet-A.  is a challenging image dataset which contains classes from ImageNet but presents a significant performance drop for existing models. There are in total 7500 images in this dataset with 200 classes from the original ImageNet classes.\nMPII-Cooking.  [53] is a video dataset recording human subjects cooking a diverse set of dishes. There are 30 subjects performing 87 different types of fine-grained activities, across more than 14K clips. We follow their split of training set but include validation and testing data, around 2.5K video samples, for the evaluation.\nEpic-Kitchen.  [6] is a large-scale dataset in first-person vision capturing daily activities in the kitchen, which includes 90K action segments and 97 classes of activities in total. We follow their training-testing split, preserving around 10K video samples for the evaluation."}, {"title": "APPENDIX E. APPX.5: MODELS", "content": "E.1. Models for image recognition tasks. In the image recognition tasks, We conducted experiments with models from both CNN and ViT families to compare the differences in representations and prediction behavioUrs between these two architectures.\nFor the CNN families, We use ImageNet-1K pretrained model for image classification from the model zoo of Pytorch. In the overall experiment, We consider the following models:\n$\\bullet$ VGG: We use VGG with different layers VGG11, VGG13, VGG16, VGG19 [57].\n$\\bullet$ ResNet: We use ResNet with different layers ResNet18, ResNet50, ResNet101, ResNet152 [22].\n$\\bullet$ DenseNet: We use DenseNet with different layers DenseNet121, DenseNet161, DenseNet169, DenseNet201 [27].\n$\\bullet$ Inception: We use Inception-V3 [61].\nFor the Vision Transformer families, We use both ImageNet-1K pretrained model and pretrained models based on self-supervised training on large datasetS.\n$\\bullet$ ViT: Vanilla Vision Transformers [10] are a set of models that adapt the transformer architectures to the vision task and leverage self-attention mechanisms to understand spatial hierarchies and dependencies within the image data. We consider both vanilla ViT-Base (ViT-b-16, ViT-b-32) and ViT-Large (ViT-1-16, ViT-1-32) models.\n$\\bullet$ Swin T: Swin Transformer [39] utilises shifted windowing schemes to efficiently handle the computation of self-attention in hierarchical vision transformers. In the experiments, We consider Swin-t, Swin-b, Swin-s architectures."}, {"title": "E.2. Models for video classification tasks.", "content": "In the Human Activity Recognition (HAR) tasks, We trained models on the training set and performed inference on the testing set for two datasets. All of the models are trained on a server with an Nvidia Tesla V100 32G GPU, 8 Intel (R) Xeon (R) Gold 6133 @ 2.50 GHz CPU, and 38G memory. We include the following widely explored architectures of video classification in the experiments:\nConvNet: We trained ResNet50 [22] with ImageNet-1K pretrained weights on either the first frame (single-frame-0) or the middle frame (single-frame-8) of the sequence. For the MPII-Cooking dataset, We used an SGD [54] optimizer with a learning rate of 0.005, a batch size of 64, and a dropout of 0.3. For the Epic-Kitchen dataset, We used an Adam [33] optimiser with a learning rate of 0.005, a batch size of 64, and a dropout of 0.2. For each dataset, We trained each model for 5 hours.\nConvNet+LSTM:. In this architecture, the ConvNet captures the spatial information of each frame and the LSTM captures the temporal information of the sequence [9]. We use ResNet50 [22] as the ConvNet component and We train the whole model either with ImageNet-1K pretrained: Conv-LSTM-pre or without pretrained: Conv-LSTM. For MPII-Cooking, We use an SGD optimiser with a learning rate of 0.005, a batch size of 10 and a dropout of 0.2. For Epic-Kitchen, We use an SGD optimiser with a learning rate of 0.005, a batch size of 10 and a dropout of 0.1. Under all settings, the hidden size of LSTM is 250 with 2 layers. For each dataset, We trained each model for 48 hours.\n3D ConvNet: Typically, 2D CNNs encode only the spatial dimensions of images, whereas 3D CNNs extend this capability to also capture temporal information from video sequences [30]. Here, We use the 3D-ResNet architecture in [21] for training. We train the 3D-ResNet18 and 3D-ResNet50 either from scratch or with pretrained weights. We use either Kinetics-700 (K) pretrained weights or Kinetics-700 + Moments in Time (KM) pretrained weights for both 3D-ResNet18 and 3D-ResNet50 8 Our implementation of model architecture is based on https://github.com/kenshohara/3D-ResNets-PyTorch/tree/master, resulting in six different settings: ResNet18-3d, ResNet18-3d-km-pre, ResNet18-3d-k-pre, ResNet50-3d, ResNet50-3d-km-pre, ResNet50-3d-k-pre. We use an SGD optimiser with a learning rate of 0.005 and a batch size of 64. For the MPII-Cooking dataset, We trained each model 12 hours on the GPU, while for Epic-Kitchen, We trained each model 24 hours.\nVideo Transfomer: Video transformer architecture such as ViViT [2], TimeSformer [3] extract spatiotemporal tokens from input videos, employing transformer layers for encoding. These video transformer models present SoTA performance across numerous video classification tasks. We train ViViT and Timesofmer both from the Kinetic-600 pretrained models. 9 We use an AdamW [40] optimiser. For each experimental setting, We train one week on the V100 GPU."}]}