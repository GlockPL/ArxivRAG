{"title": "MACHINE LISTENING IN A NEONATAL INTENSIVE CARE UNIT", "authors": ["Modan Tailleur", "Vincent Lostanlen", "Jean-Philippe Rivi\u00e8re", "Pierre Aumond"], "abstract": "Oxygenators, alarm devices, and footsteps are some of the most\ncommon sound sources in a hospital. Detecting them has scientific\nvalue for environmental psychology but comes with challenges of\nits own: namely, privacy preservation and limited labeled data. In\nthis paper, we address these two challenges via a combination of\nedge computing and cloud computing. For privacy preservation,\nwe have designed an acoustic sensor which computes third-octave\nspectrograms on the fly instead of recording audio waveforms. For\nsample-efficient machine learning, we have repurposed a pretrained\naudio neural network (PANN) via spectral transcoding and label\nspace adaptation. A small-scale study in a neonatological intensive\ncare unit (NICU) confirms that the time series of detected events\nalign with another modality of measurement: i.e., electronic badges\nfor parents and healthcare professionals. Hence, this paper demon-\nstrates the feasibility of polyphonic machine listening in a hospital\nward while guaranteeing privacy by design.", "sections": [{"title": "1. INTRODUCTION", "content": "Sound is a reliable and non-invasive carrier of information about hu-\nman health [1]. Yet, historically, the subfield of medical acoustics\nhas mainly focused on analyzing sounds as produced by patients:\nstutter [2], crackles [3], cough [4], and so on. Much less is known\nabout the sounds as heard by patients in a clinical setting: as ex-\nperimental psychologists have pointed out, the detailed description\nof acoustic events in intensive care units (ICU's) is typically over-\nlooked in favor of sound pressure level measurements (SPL) [5].\nMeanwhile, exposure to anthropogenic noise at unsafe SPL levels\nis known to induce stress, cognitive impairment and sleep disorders\nin children [6] and adults [7], thus calling for urgent remediation.\nThe case of neonatal intensive care units (NICU's), where pre-\nmature babies receive special care to grow and survive, presents an\neven greater gap in research than adult ICU's [8]. During their time\nin the NICU, preterm infants are exposed to unpredictable sensory\nstimuli while undergoing a protracted period of rapid brain growth,\ncausing lasting effects on cognitive ability [9]. Unfortunately, the\nauditory physiology and cognition of neonates have received insuf-\nficient attention from scientists until recently [10].\nWhat is known with certainty is that parents have an essential\nrole to play in the development of their newborn babies [11]. In-\ndeed, an approach sometimes described as \"kangaroo care\" involves\nprolonged periods of skin-to-skin contact between the baby and ei-\nther of the two parents, in addition to incubator placement. Pro-\nmoting this approach requires to take the well-being of parents into\nconsideration so that they feel included into collective care work.\nFor this purpose, we have launched a project on \"listening to\nfamily experiences in the neonatological ward\", or LIFEWARD for\nshort. Here, the word \"listening\" is understood as both qualita-\ntive and quantitative: i.e., as enacted by interviews with parents\nas well as autonomous acoustic sensors. Although there is sci-\nentific consensus around the value of semi-structured interviews\nfor neonatology-see, for example, [12]\u2014the same cannot be said\nabout machine listening. This is for at least three reasons. First, the\ndeployment of acoustic sensors in a hospital raises pressing con-\ncerns about privacy preservation and cybersecurity. Secondly, the\napplication of machine learning to the NICU is not straightforward,\nfor lack of annotated training data. Thirdly, and perhaps most fun-\ndamentally, machine listening systems have not yet demonstrated\ntheir ability to reconstruct objective information about social bonds\nin the NICU. Addressing these three challenges is necessary before\nenvisioning the integration of machine listening instruments within\nthe toolkit of patient experience research.\nIn this article, we present a proof of feasibility of machine lis-\ntening for neonatology. Prior work in this domain has focused\non a single class of sound event-namely, the spontaneous cries\nof preterm newborns [13]. Meanwhile, our system is a multilabel"}, {"title": "2. METHODS", "content": "Our acoustic sensor is a Raspberry Pi, inspired by previous work\non urban noise monitoring [14]. It acquires audio from an exter-\nnal USB microphone, specifically, the micW i436. The i436 is an\nomnidirectional electret microphone with a capsule diameter of ap-\nproximately 7 mm, in compliance with NF EN 61672 Class-2 stan-\ndards. Its sensitivity and frequency response has been calibrated\nmanually by the manufacturer. After digital-analog conversion, the\nsample rate is 32 kHz. Our sensor is powered by the grid and \"air-\ngapped\", i.e., physically isolated from the public Internet and from\neach other. This is to reduce the risk of malicious data access."}, {"title": "2.2. Third-octave spectrogram", "content": "We use fast Fourier transforms (FFT) to design a third-octave fil-\nterbank with bands ranging from 20 Hz to 12.5 kHz, in compli-\nance with the ANSI S1.1-1986 and IEC 61260-1:2014 standards\n[15]. We extract the magnitude response of each filter over non-\noverlapping subbands of duration 125 ms. These operations are\nimplemented in the C language, compiled for the Raspberry Pi, and\nexecuted in real time. The result is stored incrementally on a non-\nvolatile memory (\"SD card\").\nA perceptual evaluation on twelve subjects has shown that the\nthird-octave spectrogram does not contain sufficient information to\nrecover intelligible speech, at least via classical signal processing\ntechniques-namely, Moore-Penrose pseudoinverse and Griffin-\nLim algorithm for phase retrieval [15]. Thus, the third-octave spec-\ntrogram representation can be said to be privacy-aware, in the sense\nit mitigates the severity of a security breach should the SD card were\nto be lost or stolen in the healthcare facility.\nAnother advantage of computing third-octave spectrograms on\nthe edge resides in its bitrate: around 3.71 kilobytes per second\n(kbps). This is lower than MP3 (128-320 kbps) and lossless audio\n(around 1 Mbps). The bitrate of third-octave spectrograms trans-\nlates to around 320 megabytes per day, or 117 gigabytes per year."}, {"title": "2.3. Spectral transcoder", "content": "Previous work in urban environments has shown the potential of the\nthird-octave spectrogram as a feature for sound event classification,\nboth in supervised and self-supervised scenarios [16]. Yet, this pre-\nvious work is unapplicable in the context of the NICU, for lack of\nannotated training data. Furthermore, note that it would not be pos-\nsible to launch our own annotation campaign because, as explained\nbefore, our sensors do not record audio. We propose to circum-\nvent this problem by relying on a pretrained audio neural network\n(PANN) for multilabel sound event detection and classification [17].\nHere, a second issue arises: PANN does not operate upon\nthe third-octave spectrogram but on a mel-frequency spectrogram,\nwhich has a finer temporal resolution (hop size of 10 ms) and a\nfiner spectral resolution (64 bins on the mel scale). In principle, the\nrequired change of resolution could be achieved by a linear non-\nuniform resampler. Yet, in practice, this produces a blurry time-\nfrequency representation which is not recognized by PANN as con-\ntaining any events of interest. Against this issue, a deep neural net-\nwork was developed by Tailleur et al. [18], which we call spectral\ntranscoder, so as to recover a plausible mel-frequency spectrogram\nfrom a third-octave spectrogram measurement.\nThe spectral transcoder is a convnet with six layers. It is\ntrained on TAU Urban Acoustic Scenes 2020 Mobile dataset [19]\nin a \"teacher-student\" scenario. The teacher is the composition of\nmel-frequency spectrogram and PANN whereas the student is the\ncomposition of third-octave spectrogram, spectral transcoder, and\nPANN. In other words, the spectral transcoder is not trained to min-\nimize its mean square error with the mel-frequency ground truth\n(as a linear model would) but to generate a mel-frequency spectro-\ngram whose spectrotemporal content has the same distribution of\nsound events as the ground truth. The training process involves min-\nimizing a binary cross-entropy loss, computed between the PANN\noutput of the student and that of the teacher, by updating solely\nthe transcoder's parameters. This is a kind of super-resolution pro-\ncedure in which the implicit knowledge about the spectrotemporal\ncharacteristics of natural audio sounds is distilled from PANN into\nthe spectral transcoder under the form of convnet weights. We refer\nto [18] for more details on the spectral transcoder."}, {"title": "2.4. AudioSet classification with PANN", "content": "Our PANN of choice is a residual network with 38 layers, or\nResNet38 for short. It contains around 74M parameters. To this day,\nit is regarded as one of the most accurate general-purpose multilabel\naudio classifier among those which take the mel-frequency spectro-\ngram as input. The PANN is trained on AudioSet, a dataset which\ncontains over 2M 10-second audio clips which were extracted from\nYouTube videos. In the next sections, we refer to the composition of\npretrained spectral transcoder and PANN as \"PANN-1/3oct\" model.\nWe refer to [17] for further details on PANN.\nSince PANN is a multilabel classifier, its output vector is unnor-\nmalized. For the sake of visualization, we have found it beneficial to\nrank predictions in decreasing order, normalize rank by the number\nof classes, and apply an inverse power transform. Given predic-\ntions x[k] for each class k, this procedure yields the a-compressed"}, {"title": "2.5. Label space adaptation", "content": "The PANN-1/3oct model analyzes a third-octave spectrogram snip-\npet of duration equal to 10 seconds and returns a vector of dimen-\nsion 527, corresponding to the classes in AudioSet dataset. These\nclasses are a subset of the 623-class AudioSet ontology, which has\nbeen defined by a Google Research team after scraping web-scale\ntext data for \"Hearst patterns\", i.e., of either of these forms [20]:\n[...] sounds such as X or Y [...]\n[...] X, Y, and other sounds [...]\nThis approach has proven fruitful for general-purpose audio classi-\nfication: we refer to [21] for a review. Yet, it is unsuitable for the\nICU, whose distribution of sound events is inadequately represented\nby textual mentions of sound events on the web. At the same time,\ntraining a classifier from scratch on a new taxonomy is out of the\nquestion for reasons of privacy preservation, as explained earlier.\nInstead, we simply run the PANN-1/3oct model on third-octave\nspectrogram data from the NICU and look for some frequently oc-\ncurring AudioSet classes. We find four activity patterns of interest:\n\"conversation\", \"walk, footsteps\", \"train\", and \"electronic music\".\nAlthough the former two sound events are plausible, the latter two\nare clearly not. Yet, after interviewing NICU employees, we may\nhypothesize that they yield indirect information: i.e., that \"train\"\nactually corresponds to the rumble of the oxygenator while \"elec-\ntronic music\" corresponds to the ringtone of the hospital phone. We\nsummarize this correspondence in Table 1."}, {"title": "3. APPLICATION", "content": "Since 2018, a design company have been partnering with Nantes\nUniversity hospital and a nonprofit organization to enhance the in-\nclusion of parents in the NICU. The nonprofit organization collab-\norated with designers to refurnish a care room so as to facilitate the\npresence of parents alongside their newborn. In this context, the\nLIFEWARD sensor has offered the necessary guarantees for a safe\nand privacy-aware deployment in the NICU. We have obtained the"}, {"title": "3.2. Visualization of sound events", "content": "We now collect a few waveform-domain samples from the sound\nevents of interest in a real NICU environment. This data collection\nstage is carried out with a handheld device, over short durations,\nand with the collaboration of NICU professionals. Specifically, we\nring various kinds of alarms, activate oxygenators and other pumps,\nstomp our feet, and so forth. Admittedly, these sounds are too few\nto offer an independent quantitative evaluation of PANN-1/3oct: we\nrefer to [18] for that matter. Still, they may serve as suggestive evi-\ndence for the fact that the correspondences which we hypothesized\nin Table 1 are adequate and useful in practice.\nFigure 2 illustrates our findings for each of the four classes of\ninterest. For example, we notice vertical patterns of high energy in\nthe recording of footsteps, versus horizontal patterns in the record-\nings of oxygenator. These simple observations corroborate the pre-\ndiction of the PANN-1/3oct model with label space adaptation: see\nTable 1. Those examples demonstrate one of the advantages of us-\ning the transcoder: one can double-check model predictions by dis-\nplaying the transcoded spectrogram, despite not being able to listen\nto the underlying audio waveform."}, {"title": "3.3. Proof of feasibility for continuous monitoring", "content": "The previous section has confirmed the interest of the PANN-1/3oct\nmodel in the context of isolated sounds from the NICU, as acquired\nby a handheld device. It remains to be seen if this model remains\ninformative in a real-world polyphonic context, as acquired by the\nLIFEWARD sensor. For this purpose, we propose to compare the\ndetected events with another modality of measurement: i.e., elec-\ntronic badges worn by parents and healthcare professionals. Via\nnear-field communication (NFC), these badges yield information\nabout who is present in the care room at any given time. Hence, they\noffer indirect confirmation for the feasibility of machine listening in\nthe NICU, while remaining non-invasive and privacy-aware.\nFigure 3 shows an example of PANN-1/3oct predictions from\nour real-world NICU dataset, together with timestamps from elec-\ntronic badges. We notice that segments during which two adults\nare present in the room coincide with a rise in the presence of\nconversation-and, to a lesser degree, of footsteps. Meanwhile, the\nlowest values for the \"conversation\" class correspond to segments in\nwhich only one adult is present in the room. Yet, we recognize that\nthese are only anecdotal observations. Future research is needed to\nexpand the comparison of acoustical and non-acoustical informa-\ntion to a larger scale; i.e., multiple days and multiple rooms."}, {"title": "4. CONCLUSION", "content": "The DCASE community has a key role to play at the intersection\nbetween sound design and healthcare. Yet, fulfilling this role comes\nwith challenge of its own, such as: privacy, cybersecurity, and lim-\nited labeled data. In this article, we have presented a first prototype"}]}