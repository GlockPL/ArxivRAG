{"title": "Skill Generalization with Verbs", "authors": ["Rachel Ma", "Lyndon Lam", "Benjamin A. Spiegel", "Aditya Ganeshan", "Roma Patel", "Ben Abbatematteo", "David Paulius", "Stefanie Tellex", "George Konidaris"], "abstract": "It is imperative that robots can understand natural language commands issued by humans. Such commands typically contain verbs that signify what action should be performed on a given object and that are applicable to many objects. We propose a method for generalizing manipulation skills to novel objects using verbs. Our method learns a probabilistic classifier that determines whether a given object trajectory can be described by a specific verb. We show that this classifier accurately generalizes to novel object categories with an average accuracy of 76.69% across 13 object categories and 14 verbs. We then perform policy search over the object kinematics to find an object trajectory that maximizes classifier prediction for a given verb. Our method allows a robot to generate a trajectory for a novel object based on a verb, which can then be used as input to a motion planner. We show that our model can generate trajectories that are usable for executing five verb commands applied to novel instances of two different object categories on a real robot.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots that interact with humans should be equipped with the means to interpret and follow commands in natural language. Manipulation commands are commonly expressed as verbs applied to a given object. We therefore propose that robots that can efficiently learn how to perform various manipulation tasks from natural language commands must be able to generate a motor skill that matches a given verb, and apply it to manipulate a novel object. For instance, opening a door is similar to opening a microwave; therefore, a robot that has learned a skill appropriate for the verb \"open\" applicable to a door should be able to: 1) know what an \"open\" microwave looks like given a \"closed\" microwave, and 2) execute the \"open\" action on a microwave with minimal additional learning. However, most works in natural language grounding and generalization either do not apply multiple actions across multiple object categories [1, 2, 3], assume robots know goal states for primitive verbs [4, 5], or rely on demonstration data [5, 6, 4, 7, 8, 9].\nTo address the problem of generalizing verb-labeled skills to novel object categories, we propose a model with two components\u2014a classifier and an optimizer\u2014for producing"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Manipulation tasks are commonly modeled as Markov Decision Processes (MDP) [12], where given a task, a robot chooses an action and updates its action policy based on reward given its observations of the world. We are particularly motivated by the work of Rodriguez-Sanchez and Patel [13], which suggests that verbs ground to actions in MDPs. We focus on specifically grounding verbs to motor skills, via the options framework [14]. Each motor skill is modeled by an option o that consists of three components: the option policy $\\pi_o$, which is executed and maps low-level states to low-level actions; the initiation set $I_o$, which describe states where the option can be executed; and the termination condition $B_o$, which is the probability of the option terminating in each state [12].\nWe apply these concepts to develop a model for generating an object trajectory that achieves the intended goal of a verb given visual input of the object. Given the example of applying the verb \"open\" to a door, the initiation set would be the current image of the door, which captures the notion of the door in a closed state. The termination set would be an image of the final state of the door, which captures the notion of a door in its open state-specifically, an image of the door angled ajar. The option policy generates a predicted trajectory of the object when the verb is applied."}, {"title": "B. Verbs", "content": "Verbs play a crucial role in natural language commands. Hovav and Levin [15] propose that verbs can be classified as manner verbs that specify the manner of carrying out the action, and result verbs, that specify the reaching of a resulting state. Result verbs can be classified further into three categories: change of state verbs, which specify a change of state of a property of the object the verb is applied to, inherently directed motion verbs, which contain movement in relation to an object, and incremental theme verbs, which specify a change in volume or area of object. Following Gao et al. [16], and unlike Hovav and Levin, we consider changes of location, volume, and area to also constitute \"change of state\".\nTo realize the effects of change in state verbs, we propose that at least one of the following is required: termination state (the state of the object after the verb is applied), both initiation (the state of the object before the verb is applied) and termination state, object trajectory, and/or robot arm trajectory. Verbs like open and close can be minimally differentiated by termination state. Verbs like rotate can be minimally differentiated by initiation and termination state by looking at the difference of angles between the two steps, while verbs like throw and toss can be minimally differentiated by the combination of object and robot arm trajectories. We focus on change in state verbs that can be realized through initiation and termination, and/or object trajectory, where the agent manipulates a physical object."}, {"title": "C. Related Work", "content": "Existing work has focused on grounding language in a visual representation of objects in the world and generalizing manipulation skills for robots. We situate our work between these bodies of work and outline them below.\nGrounding Language to Vision and Manipulation: Many works assume that robots already know the goal state of a given object after applying primitive/atomic verbs. Ichter et al. [4] and Sharma et al. [5] focused on breaking down complex natural language commands into simpler primitive actions or tasks. Ramesh et al. [17] created a model for text to image generation, which could be useful for predicting images of objects after manipulation. However, given the text prompt \"closed oven\", such a model produces mostly open ovens, and we have found that it has trouble with differentiating between physical states of an object. Paulius et al. [2] proposed a motion taxonomy for describing action verbs as binary strings known as motion codes, which can be used to describe action and discern between the meaning of actions in a manipulation-centric embedding space. However, they did not account for manipulation on objects across different categories.\nOther work in natural language and robotics addressed language-conditioned imitation learning [18] or trajectory modification with natural language commands [19, 20, 8]. However, these works do not address generalization of tasks or skills across objects or of skills. Rather, they focus on imitation learning, which requires large amounts of demonstration data, or active parsing and interpretation of commands from humans during a task to accomplish the skill.\nLearning Generalizable Skills: Contrary to our work, where we focus on generalizing skills by the effect or action of the verb, a common approach is generalizing through object articulation. Eisner and Zhang [3] proposed a model to learn and predict 3D articulation flow for various objects; this output was then used to execute a motion planner to achieve the maximum articulation. However, there is no explicit integration of language nor mention of multiple verbs or skills being applied to each object instance. Abbatematteo et al. [21] investigated how to estimate the kinematic model and configuration of novel objects for manipulation; however, they do not explore generalization of skills. Hewlett et al. [1], Jang et al. [22], and Sugiura and Iwahashi [9] incorporated the presence of another human in the environment/loop or human demonstration data, which is inefficient and thus limits the amount of verbs that the approach can handle. Furthermore, while Hewlett et al. [1] required a human in the loop of identifying the verb for a demonstrated trajectory, our approach exploits deep neural networks for verb prediction."}, {"title": "III. SKILL GENERALIZATION WITH VERBS", "content": "We propose a model that generalizes verb-labeled skills to novel object categories. Our goal is to train a model that takes as input a verb, paired with a kinematic model of an object and its initial state, and outputs a trajectory that can be applied on the object to undergo the effect of that verb."}, {"title": "A. Classifier", "content": "In order to realize these change of state verbs, robots must be able to: 1) predict the goal/target state of the object if a verb were to be applied, and 2) adapt verb skills (specifi- cally the object trajectory) across multiple (and potentially novel) object categories with minimal additional learning. To achieve these, the model should be able to realize and differentiate the verb that is depicted for a given trajectory, and be able to change the state of the object to achieve the desired verb. We do this through two main components after extracting kinematic parameters for an object instance: a classifier that will output a predicted probability for a given verb command and images of the object trajectory, coupled with an optimizer which uses this classifier to generate a trajectory from a given initiation state of a novel object to achieve the verb command. Figure 2 shows an overview of the planning portion of our model pipeline."}, {"title": "1) Architecture:", "content": "Our experiments use a simple CNN with the following structure: two CNN layers (32 and 64 units respectively with 3 \u00d7 3 kernel sizes and max pooling layers) and three fully connected layers (64, 32, and N units, where N is the total number of verbs). The inputs to the classifier are sequences of RGB images of a trajectory sequence (where each image has dimensions of 128 \u00d7 128 \u00d7 3). Each image of a sequence is concatenated together in the network. The final layer is a softmax layer of N units that will output probabilities of the sequence achieving the effects of N verbs;\nthe predicted verb is that with the highest probability value. Other state-of-the-art activity and object recognition methods can be used (refer to Section V)."}, {"title": "2) Training:", "content": "Given k object categories, we perform train- ing and testing in an all-but-one procedure (i.e., k-fold cross validation), where we train the classifier using k - 1 object"}, {"title": "3) Prediction:", "content": "When given an input RGB trajectory se- quence for an object instance of an novel object category, the classifier outputs a per-verb probability array."}, {"title": "4) Accuracy:", "content": "The accuracy of the classifier is measured by generating predictions on RGB trajectory sequences from the novel test object categories, and comparing the verb label of the most likely prediction with the ground truth verb label associated with the trajectory."}, {"title": "B. Trajectory Optimizer", "content": "The second step of our model generates an object trajec- tory for the desired verb command given a URDF description of the object representing its links and joints. The optimizer searches over trajectories of the degrees of freedom of the object (i.e., 6-DoF pose and articulated state) to maximize the classifier's returned probability for the desired verb. We used the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) optimizer algorithm [23], an efficient state-of-the- art optimization algorithm, to search over object trajectories."}, {"title": "1) Trajectory Parameterization:", "content": "A candidate trajectory is parameterized by a vector of length (6 + n), corresponding to the per-timestep change in 6-DoF position and orientation of the object as well as the state of its n articulated joints. Articulated states are clipped to lie within the joint limits provided in the URDF description. This trajectory is then rendered and scored by the classifier."}, {"title": "2) Measuring Loss:", "content": "The loss minimized by CMA-ES is calculated as the categorical cross-entropy between the verb probability array produced from our CNN classifier and a one-hot target array indicating the target verb."}, {"title": "C. Verb Selection and Data Collection", "content": "To train and evaluate our model, we require visual data of objects undergoing manipulations corresponding to verbs, and their underlying kinematic and geometric descriptions."}, {"title": "1) Visual Data Collection:", "content": "We require an environment that provides the following: multiple object categories and multiple instances of objects in those categories, the capa- bility to manipulate parts or the entire object for a variety of verbs, and a way to extract multiple images of the scene. We considered datasets such as ALFRED [24], AI2THOR [25], New Brown Corpus [26], SAPIEN [11], and RL Bench [27]. We chose the PartNet-Mobility Dataset from SAPIEN due to the presence of 2347 object instances over 46 object categories as URDF files. Each file describes the kinematics (links and joints) and geometry of an object. Our approach performs verb-based manipulation of each object by simulating them in SAPIEN and taking RGB snapshots. For each object-verb pairing, we generate 21 RGB images/snapshots (each 128 by 128) representing 21 timesteps along the object trajectory when the verb is applied to the object. We generate these for our chosen verbs applied for a total of 812 object instances present in the 13 chosen object categories: Box, Dishwasher, Door, Lap- top, Microwave, Oven, Refrigerator, Safe, Stapler, Storage Furniture, Toilet, Trash Can, and Washing Machine. A total of 41688 trajectories are generated.\nWe assume maximally distinct initiation and termination states for collecting data needed to train the CNN classifier. For instance, the initiation state for the \"open\" verb on door is when the door is completely closed, while the termination state for he \"open\" verb on a door is when the door is open to the upper joint limit. We also assume that for \"open\" and part-based translation verbs applied to objects with multiple non-fixed joints, only one joint is manipulated. To prevent the model from generating object trajectories where multiple verbs occur simultaneously, data for a \"none\" category is generated, which features objects undergoing multiple verbs at once in a single trajectory."}, {"title": "2) Verb Selection:", "content": "Our requirements when selecting verbs are whether they can be achieved via the URDF files of object instances and if they can be applied to multiple object categories that are present in SAPIEN. With these objectives in mind, we examined the verbs in VerbNet for change in state verbs. The final selected verbs are: translation verbs (specifically Push, Pull, Raise, Lower, TranslateLeft and TranslateRight, and RemoveWhole\u2014when removing an object from the scene), part-based translation (RemovePart\u2014 when a single part of the object is removed and Insert- Part-when a single part of the object is inserted), Open, Close, and rotation verbs (Roll, Turn, Flip by 270 degrees)."}, {"title": "IV. EXPERIMENTS", "content": "The aim of our evaluation is to test the hypothesis that our model can successfully transfer verbs to novel object categories, both in simulation and with a real robot. We do so by selecting object categories for training the classifier, and then selecting a novel object category for evaluation by the classifier and for manipulation with the optimizer."}, {"title": "A. Classifier", "content": "We measure the accuracy of the classifier on RGB image trajectories from the test object categories. The object cate- gories that we use for our experiments are: Box, Dishwasher, Door, Laptop, Microwave, Oven, Refrigerator, Safe, Stapler, Storage Furniture, Toilet, Trash Can, and Washing Machine."}, {"title": "B. Object Trajectory Optimization", "content": "With the trained models from our classifier, we run the CMA-ES optimizer on object instances and qualitatively assess the performance. We set the initial covariance pa- rameter to 0.33, the population size to 40, and the number of generations to 60. Since we trained our CNN classifier with five timesteps sampled at regular intervals (such that the change is equal between chosen timesteps), the optimizer predicts a single change in state (of size 6 + n) that is applied at each timestep to generate the predicted trajectory. Further, we identified that the verbs studied include change in at most one degree of freedom; therefore, to prevent extraneous motion, we take only the maximum predicted per- timestep change in state and set the other dimensions to zero when scoring trajectories. This property is straightforward to compute from the data but could be learned for each verb."}, {"title": "C. Robot Demonstration", "content": "Finally, we demonstrate that our model enables a real robotic system to execute verb commands on novel ob- ject categories. The robot is a KUKA LBR iiwa7 with a Schunk Dexterous Hand 3-fingered gripper. Five commands were executed: Open (applied to a novel instance of the StorageFurniture object category) as well as Turn, Transla- teRight, TranslateLeft, and Push (applied to a novel instance of the Box object category). After performing the offline trajectory optimization, the robot executes the desired object trajectories using motion planning. Plans for the Turn, Push, and Translate verbs were computed as simple motions in Cartesian space with the object placed in the robot's gripper. The Open command was executed by providing the robot with a grasp on the cabinet door (e.g., as though from an off-the-shelf grasp detection algorithm [28]), and computing a trajectory that moves the end-effector to execute the object trajectory produced by our system with MoveIt! [29]."}, {"title": "V. DISCUSSION", "content": "In our experiments, we used a small set of manipulation verbs that can be realized either through object trajectory or the difference between initiation and termination states. However, our model can easily incorporate verbs realized through both robot arm trajectory and object arm trajectory by generating RGB trajectory sequences that include both the robot arm and the object (e.g., the verbs Throw and Toss). Our implementation allows for flexibility in the length of the object trajectories and the frequency at which they are rendered. For most of our experiments, we used only five out of the total 21 generated timesteps (initiation step, Step 5, Step 10, Step 15, termination step), and we assume that there is a constant amount of time for all the object instances used for training our networks. One could prefer"}, {"title": "VI. CONCLUSION", "content": "We have proposed a two-part model consisting of a classifier and an optimizer to generalize manipulation skills to novel object categories using verbs. We present a classifier that can recognize which verb is being performed in a given trajectory, and enables verb generalization to new object instances and new object categories. This classifier achieves an average of 76.69% accuracy over 13 object categories and 14 verbs. The optimizer is responsible for finding kinematic trajectories of an object that scores highly on the classifier for the desired verb command. Our model can generalize skills across novel objects, and we conducted robot demonstrations to show that robots can use our model with motion planning for execution on novel objects."}]}