{"title": "Leveraging MLLM Embeddings and Attribute Smoothing for Compositional Zero-Shot Learning", "authors": ["Xudong Yan", "Songhe Feng", "Yang Zhang", "Jian Yang", "Yueguan Lin", "Haojun Fei"], "abstract": "Compositional zero-shot learning (CZSL) aims to recognize novel compositions of attributes and objects learned from seen compositions. Previous works disentangle attribute and object by extracting shared and exclusive parts between image pairs sharing the same attribute (object), as well as aligning them with pretrained word embeddings to improve unseen attribute-object recognition. Despite the significant achievements of existing efforts, they are hampered by three limitations: (1) the efficacy of disentanglement is compromised due to the influence of the background and the intricate entanglement of attribute with object in the same parts. (2) existing word embeddings fail to capture complex multimodal semantic information. (3) overconfidence exhibited by existing models in seen compositions hinders their generalization to novel compositions. Being aware of these, we propose a novel framework named Multimodal Large Language Model (MLLM) embeddings and atTribute smoothIng guiDEd diseNTanglement (TRIDENT) for CZSL. First, we leverage feature adaptive aggregation modules to mitigate the impact of background, and utilize learnable condition masks to capture multigranularity features for disentanglement. Then, the last hidden states of MLLM are employed as word embeddings for their superior representation capabilities. Moreover, we propose attribute smoothing with auxiliary attributes generated by Large Language Model (LLM) for seen compositions, addressing the issue of overconfidence by encouraging the model to learn more attributes in one given composition. Extensive experiments demonstrate that TRIDENT achieves state-of-the-art performance on three benchmarks.", "sections": [{"title": "1. Introduction", "content": "As for the study of compositional generalization ability inherent to humans, compositional zero-shot learning (CZSL) [22, 26, 33] is proposed to enable machines to recognize unseen attribute-object compositions by leveraging knowledge of attributes and objects (i.e., primitives) learned from seen compositions. Specifically, in the training phase, models are provided with images and compositional labels (e.g., ripe orange and peeled apple). During the testing phase, given an image depicting a novel composition (e.g., peeled orange), models are assigned to classify the image into the corresponding category [44].\nPrior works [22, 27] focus on mapping the visual features and the word embeddings of compositions into a joint space. These methods have poor generalization capability to unseen compositions, as they fail to learn primitives. Therefore, recent studies [8, 14, 38] consider visual disentanglement. Among them, some prominent works deploy a triplet of images to disentangle: a given image (noted as the main image), and two supplementary images, each sharing either the same attribute or the same object as the main image. The triplet of images is treated as two image pairs for subsequent analysis. These approaches aim to disentangle attribute and object by analyzing the shared and exclusive features of the image pair, as well as aligning them with word embeddings (e.g., GloVe [32]), as shown in Figure 1. Although these pioneer research studies have achieved great progress, they exhibit three limitations:\nL1: Disentanglement is impeded due to the influence of the background and the intricate entanglement of attribute with object in the same parts of image. On the one hand, models tend to extract the background feature unique to one image in the pair as the disentangled exclusive features. On the other hand, some existing methods [36, 38] compute the similarity of image pairs for disentanglement at the spatial level. However, disentangling attribute and object at the spatial level presents significant challenges because they entangle in the same spatial features. Taking an image of ripe apple as an example, the spatial regions corresponding to the attribute \"ripe\" and the object \"apple\" are fully co-located.\nL2: Existing word embeddings lack the depth needed to capture complex multimodal semantic information. To"}, {"title": "2. Related Work", "content": "Compositional zero-shot learning (CZSL). Prior works in CZSL can be broadly divided into two main streams. One main stream is to learn representations of compositions in a joint space. SymNet [15] proposes to learn symmetry property in compositions. Co-CGE [20] leverages a Graph Convolutional Neural Network to learn compositional representations. The other main stream aims at disentangling visual representations of primitives to reduce composition learning into primitive learning. SCEN [13] leverages contrastive loss to excavate discriminative prototypes of primitives. OADis [38] disentangles primitives by affinity modules. CANet [42] learns conditional attribute conditioned on the recognized object and the input image.\nMore recent works [9, 18, 28] focus on leveraging the encyclopedic knowledge of pretrained vision-language models (VLM), such as Contrastive Language-Image Pre-training (CLIP) [35] and Context Optimization (CoOp) [46], to encode and align images and texts.\nLarge language model (LLM). LLMs have realized significant advancements thanks to the scaling up of training data and the increase in the number of parameters. Early models, such as BERT [6] and GPT-2 [34], initially exhibit strong capabilities in understanding and generating human-"}, {"title": "3. Approach", "content": "3.1. Task Formulation\nCompositional zero-shot learning (CZSL) aims at learning a model that can recognize unseen compositions of attributes and objects that are learned from seen compositions. Given an attribute set A and an object set O, the attributes and objects are composed to form a composition set C = A \u00d7 O. The composition set C is divided into two disjoint sets: the seen composition set C, and the unseen composition set C, where C, \u2229 C, = \u00d8 and C, \u222a C, = C. The model is trained with a seen training set D,, = {(x,, c,)}, where x, \u2208 X, is an image from the seen image set X, corresponding to the seen composition set C,, and c, \u2208 C, is the label of x,. Following the Generalized CZSL [33], the model is evaluated on a predefined test set D,, = {(x,,, c,,,)}, where x,, \u2208 X,, is an image from the unseen image set X,, corresponding to the composition subset C,, of C, i.e., C,, \u2286 C, and c,, \u2208 C,, is the label of x,,. The aim of CZSL task is to learn a model M : X,, \u2192 C,, that predicts labels c,, from C,, for the input images x,, \u2208 X,,.\n3.2. TRIDENT\nAs the major novelty, we propose a novel framework named MLLM embeddings and attribute smoothing guided disen-"}, {"title": "3.2.1. Visual Feature Extraction", "content": "As shown in Figure 2, we denote a given image with the attribute-object composition label (e.g. ripe apple) as the main image x, and randomly sample an image with the same attribute x\" (i.e., ripe orange), as well as an image sharing the same object x\u00ba (i.e., peeled apple) to comprise a triplet image set. For the convenience of expression, we simply use x,,, (where img \u2208 {m, a, o}) to collectively denote the images as they are processed using the same module.\nVisual feature extraction backbone. As mentioned before, since LLaVA v1.5 is used as our fundamental MLLM, we directly leverage the visual encoder, ViT, and cross-modal connector (CMC) from the model to extract visual features. Specifically, the image x,,, is partitioned into n patch tokens, which are subsequently put into ViT along with the [CLS] token. Afterward, the output of patch tokens before the last layer of ViT is fed into the CMC module, as implemented in LLaVA v1.5. To align the dimension of patch tokens output by CMC with that of [CLS] token produced by ViT, the patch tokens output by CMC are input into a linear layer. Consequently, we obtain one feature vector of [CLS] token f,,, \u2208 \u211d\" and a patch feature matrix of n patch tokens F,,, \u2208 \u211d\", where d is the dimension of the features.\nLocal features extraction. Intuitively, the composition (e.g., ripe apple) only occupies a few parts of the image. Since each patch token usually corresponds to one local region of the image, to filter out background noise and focus on related regions, we deploy a set of feature adaptive aggregation (FAA) modules to derive p relevant local features of x,,, where each FAA module is formulated as follows:\n{ v = agg(F\"patch)\nagg = \u03c3(Conv(F\"patch)) (1)\nwhere Conv() represents the 1 \u00d7 1 convolution layer, \u03c3(\u00b7) denotes the sigmoid activation function, agg \u2208 \u211d\" is the weight vector, the k-th element of agg is the weight for k-th patch feature. represents matrix product, and v \u2208 \u211d\" is the local feature obtained by an FAA module. We vertically concatenate the local features produced by p FAA modules to obtain the local feature matrix F,,, \u2208 \u211d\".\nGlobal features extraction. Normally, the ViT output of [CLS] token is regarded as containing various global information of the image, which highly entangles both attribute and object features together[8]. To disperse multi-granularity global information into different representa-"}, {"title": "3.2.2. Attribute-Object Disentanglement", "content": "As mentioned before, one of the key challenges for CZSL task is to disentangle attribute and object from visual features. To overcome such challenge, we propose a novel weighted disentanglement module to disentangle primitives, as illustrated in Figure 2. For brevity, one image pair x, and x\" from the triplet image set is taken as an example to elaborate on this module, while another image pair x, and x\u00ba follows the same architecture.\nWeights computation. The features of x, and x\" (i.\u0435., F\" and F\u00ba) are vertically concatenated and fed into two MLP modules to derive their respective weights of shared attribute features relative to each other, and subsequently utilize them to compute the weights of their own exclusive object features as follows:\n{ w\"m2a = \u03c3(MLP\"m2a([F\", F\"]))\nw\"obj\" = 1 \u2212 w\"attr (5)\n{ w\"a2m = \u03c3(MLP\"a2m([F\", F\"]))\nw\"obj\" = 1 \u2212 w\"attr\""}, {"title": "3.2.3. Feature Alignment", "content": "Inspired by [24] that leverages the last hidden states as the representation embeddings, we consider the last hidden states of LLaVA v1.5 [16] as our MLLM embeddings for words. Moreover, to tackle the problem that the ineffective overconfidence exhibited by the models in terms of the ground-truth attribute hinders them from generalizing to unseen compositions, GPT 3.5 is employed to generate several auxiliary attributes that describe an object with only one ground-truth attribute and perform label smoothing during attribute alignment. Now we detail each part of feature alignment.\nGenerating auxiliary attribute words by LLM. Since only attribute text needs to be generated, we leverage a LLM, GPT-3.5, instead of MLLM, to generate several auxiliary attributes for each composition. Specifically, the following prompt is input to LLM: 'Please give me t adjectives that can describe the visual feature of a photo of a/an ... well.', where t is the number of auxiliary attributes and attribute-object composition (e.g., peeled apple) is filled in '...'. Please refer to Appendix A for more details about the generation of auxiliary attributes by GPT-3.5. Subsequently, the generated auxiliary attribute words form a set A,. Therefore, the set of all words Y is obtained, including attributes, objects and auxiliary attributes as follows:\nY = A \u222a O \u222a A\" (8)\nObtaining MLLM embeddings for words and compositions. Each word y \u2208 Y is fed into LLaVA v1.5 to get the last hidden states, i.e., LLaVA\"ih,(\u00b7). Please refer to Appendix B for more details about the obtainment of the last hidden states of LLaVA v1.5 for an input word. Subsequently, they are passed through an MLP layer to get embeddings E\"ord(\u00b7) of aligned dimension with visual features. And for a composed pair c of attribute a and object o, i.\u0435., c = (a, o), we get the last hidden states of LLaVA v1.5 for a and o, respectively, which are then horizontally concatenated and fed into a linear layer Linc,(\u00b7) to get the composed pair embedding E,(\u00b7). The process is formulated as follows:\nE\"ord(y) = MLP\"d(LLaVA\"ih,(y)) (9)\nE,(c) = Linc,(Cat(LLaVA\"ih,(a), (LLaVA\"ih,(o))) (10)\nWord expanding. Prior works compute cosine similarities of disentangled features and word embeddings only within the respective domains of attributes or objects, which results in the disentangled attributes and objects still retaining the information of each other. To address the problem, we propose a word expanding strategy, which computes cosine similarities of visual features and the embeddings of all words, including attributes and objects, and treats all words except the ground-truth word as negative labels.\nAlignment by cross-entropy. Similar to [19], we use cross-entropy to process the cosine similarity of visual features and word embeddings. Assume that f is the visual embedding and E\"ord(w,) is the word embedding for the word w, \u2208 Y in a joint space. The classifier logit from f to E\"ord(w,) is defined as follows:\nCE(f, w,) = e^(\u03b4\u00b7cos(f,E\"ord(w,)))\n\u2211_y\u2208Y e^(\u03b4\u00b7cos(f,E\"ord(y))) (11)\nwhere \u03b4 is the temperature variable, and cos(\u00b7, \u00b7) denotes cosine similarity function. Thus cross-entropy with/without label smoothing can be uniformly formulated as follows:\nH(f, y) = \u2212\u2211 z log(CE(f, y)) (12)\ny\u2208Y\nwith z = {1-\u03b1, if y is ground truth label\n\u03b1/t, if y is auxiliary label\n0, otherwise"}, {"title": "4. Experiment", "content": "4.1. Experiment Setup\nDatasets. We evaluate our model on three challenging CZSL benchmark datasets: MIT-states [11], C-GQA [25], and VAW-CZSL [38]. We present the introduction and common data splits of the three datasets in Appendix C.\nMetrics. Following the common generalized CZSL setting [33], we evaluate our model on seen and unseen pairs separately. Based on them, a calibration bias trades off between the accuracies of seen and unseen pairs. We calculate area under the curve AUC (in %) using seen and unseen classification accuracies at different biases in test data. The best seen and unseen accuracies Seen and Unseen (in %) of the curve are also reported. In addition, we calculate the harmonic mean of seen and unseen classification accuracies at difference biases and report the best one HM (in %).\nImplementation details. We use the visual encoder of LLaVA v1.5, Vit-large-14-336px as our frozen feature extractor, whose outputs contain 577 tokens (1 [CLS] and 576 patch tokens) of 1024 dimensions. The cross-modal connector of LLaVA v1.5 maps the features to the dimension of 4096, the same as last hidden states of based LLM Vicuna v1.5 [45]. Image embedder and the MLP for words map them to the dimension of 1024 for faster training. TRIDENT and all baseline models are trained with 128 batch size for 50 epochs. The number of global features is set to 6, 2, 4 for the three datasets, respectively, and the number of local features is twice that of global features. The label smoothing factor is set to 0.09, 0.03, 0.03 for the three datasets, respectively. The number of generated auxiliary attributes for each composition is set to 3. Refer to Appendix D for more information about implementation.\nBaselines. We compare our TRIDENT with recent and prominent approaches in the task of CZSL: SymNet [15], CompCos [19], Co-CGE [20], SCEN [13], OADis [38], INV [44], CANet [42], and ProCC [10]. We replace their backbone with Vit-large-14-336px and retrain all models with the same epoch for the sake of fairness. In addition, although comparing TRIDENT with CLIP-based CZSL methods, which rely on the dual-tower architecture, is very unfair due to the significant addition of trainable parameters and training overhead for both the text and visual encoders, we still choose the foundational CLIP and CoOp models as baselines for their strong zero-shot classification abilities."}, {"title": "4.2. Results and Discussion", "content": "In this section, we compare TRIDENT with state-of-the-art methods. As shown in Table 1, TRIDENT surpasses other models by a substantial margin in general. For MIT-States, TRIDENT boosts AUC, HM, and Unseen from 13.6%, 29.8%, and 39.9% of CANet to the new state-of-the-art performance of 14.2%, 30.9%, and 40.0% with 0.6%, 1.1%, and 0.1% improvement, respectively. Our model achieves competitive performance on MIT-States benchmark, despite considerable label noise [2]. However, for the more challenging benchmark C-GQA, TRIDENT achieves 8.0%, 22.6%, 39.5%, and 24.1% on the metrics of AUC, HM, Seen, and Unseen, providing 2.3%, 3.7%, 4.7%, and 3.6% improvements on the previous state-of-the-art model CANet. For the existing most challenging benchmark dataset VAW-CZSL, TRIDENT attains performance of 8.3%, 23.4%, 23.4%, and 33.3%, surpassing CANet by 1.6%, 2.4%, 2.2%, and 3.7% in terms of AUC, HM, Seen, and Unseen. The largest improvement is observed in the Unseen metric, indicating that attribute smoothing helps enhance the generalization ability of the model. We observe TRIDENT performs significantly better than CANet regarding all metrics on two challenging and low-noise benchmark dataset C-GQA and VAW-CZSL, indicating the efficacy of our approach. This improvement arises from the utilization of MLLM embeddings and attribute smoothing, which enhance attribute-object disentanglement and consequently facilitate the recognition of unseen compositions while maintaining performance on seen compositions.\nIn addition, we compare TRIDENT with dual-tower CLIP and CoOp after fine-tuned for the CZSL task. Since they are trained on a large amount of image-text data, they possess zero-shot image classification capabilities, which leads to better classification results for unseen images. Regarding Unseen metric, CoOp outperforms TRIDENT by 7.6% and 2.7% on MIT-States and C-GQA, respectively. However, TRIDENT surpasses CoOp by 0.7% and 1.1% on the core metrics of AUC and HM on MIT-States, as well as 3.6% and 5.5% on C-GQA, which suggests TRIDENT performs better than CLIP and CoOp in CZSL task."}, {"title": "4.3. Ablation Study", "content": "Effectiveness of each component. We ablate certain module of TRIDENT to evaluate the contribution of each mod-"}, {"title": "5. Conclusion", "content": "In this work, we propose a novel framework termed TRIDENT to address the challenging CZSL task. First, we leverage feature adaptive aggregation modules to mitigate the impact of background, and utilize learnable condition masks to capture multi-granularity features for attribute-object disentanglement. In addition, we exploit the last hidden states of MLLM to replace ordinary word embeddings, as they can capture complex multimodal semantic information. Moreover, we leverage LLM to generate auxiliary attributes and perform attribute smoothing to diminish over-confidence of models in seen compositions, which enables models to generalize to unseen compositions better. Extensive experiments have been conducted on three challenging datasets, and the results demonstrate the effectiveness of TRIDENT. In the future, we plan to extend our method to harness the powerful capabilities of LLMs, MLLMs, and CLIP to more effectively address the CZSL task."}, {"title": "A. Auxiliary Attributes Generation by LLM and Post-process", "content": "As mentioned before, we leverage GPT-3.5 to generate some auxiliary attributes for attribute smoothing. The auxiliary attributes are generated based on the contextual composition of the object and its attribute, such as ripe apple or sliced apple. The model takes into account the unique characteristics that arise from the combination of attribute and the object's context. This ensures that the auxiliary attributes accurately capture the nuances of the specific composition, rather than general object-level attributes. Since the generation ability of LLM is affected by many factors, in this section, we first explore the impact of different prompts on LLM-generated content. Then we study on the influence of the number of auxiliary attributes t. In addition, we introduce post-processing of generated text to eliminate potential noise.\nImpact of prompt input to LLM. Users typically interact with LLMs through prompts to request answers to questions, generate text, complete tasks, and more. The model generates text based on the provided prompt, striving to meet the user's requirements [37]. Therefore, the good design of prompt is significant for stimulating knowledge of LLMs, which enables them to better follow our instructions and generate auxiliary attributes with high quality.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of the number of auxiliary attributes t. In Table 4, we observe that the generated attributes describe the compositions to varying degrees, with later items in the sequence being less relevant generally. Therefore, we study on the influence of the number of auxiliary attributes t.\nTable 7 and Table 8 show the generated text using different t of compositions large garden and young girl. The results demonstrate that the greater the number, the more generic adjectives with irrelevant information are included, for example, Captivating is generated for both compositions. In addition, with t increasing, the noise in the generated text due to the uncertainty of the model"}, {"title": "B. Obtainment of The Last Hidden States of MLLM", "content": "We input the attribute (object) word into LLaVA v1.5, which first tokenizes the word into z tokens. These tokens pass through all attention blocks in the MLLM, ultimately generating z embeddings of dimension d, after the last block, named the last hidden states. Subsequently, we apply average pooling to these z embeddings of dimension d to obtain a d-dimensional embedding that represents the attribute. Since the last hidden states are designed to generate the next token rather than for representation, Muennighoff et al. [24] leverages instruction to fine-tune the model. Therefore, we fine-tune the last hidden states with a low learning rate during the training phase of TRIDENT.\nIt is important to note that although LLaVA v1.5 may have seen certain images during training, the model is asked to generate textual descriptions of images in an autoregressive manner during training. The textual descriptions focus on the main content of the image, rather than the \"attribute-object\" label. Therefore, there is no issue of data leakage when training TRIDENT."}, {"title": "C. Data Statictics", "content": "We evaluate our model on three challenging CZSL benchmark datasets: MIT-states [11], C-GQA [25], and VAW-CZSL [38]. MIT-states consists of diverse real-world images labeled by early image search engine technology. C-GQA and VAW-CZSL are two more challenging benchmark datasets that consist of broad collections of in-the-wild images. C-GQA has more one-to-one compositions, while objects in VAW-CZSL share more attributes. Table 6 shows detailed data statistics following the common data splits of MIT-States [11], C-GQA [25] and VAW-CZSL [38]. MIT-States contains 53753 images, with 115 attributes and 245 objects. It comprises 1262 seen compositions and 300/400 (validation/test) unseen compositions. C-GQA is a natural image dataset which contains 39298 images, with 413 attributes and 764 objects. It includes 5,592 seen compositions and 1,040/923 (validation/test) unseen compositions. VAW-CZSL is a larger dataset which contains 440 attributes and 541 objects for 238040 images, and it is split into 11175 seen and 2322/2470 unseen compositions for training and validation/testing, respectively."}, {"title": "D. Implementation details", "content": "We use NVIDIA PTX 3090 GPU to train all models under the Pytorch framework [31]. Since L leverages image features during training, we use a Batch Normalization, ReLU and 0.3 dropout for Image embedder. We train TRIDENT by Adam optimizer with weight decay 5e-5, learning rates 1.5e-6 for word embedding as well as 2e-4 for other modules on three datasets. We decay the learning rate by 10 at epoch 30 and 40. The temperature variable of cosine similarity \u03b4 is set to 0.05. For weighting coefficients \u03b3\"ortho, \u03b3\"comp, \u03b3\"attr, and \u03b3\"obj, we set them to 0.1, 1, 0.5, and 0.5, respectively."}, {"title": "E. Impact of Hyperparameters", "content": "To provide more insight into the effect of visual features and label smoothing, we study on the performance of TRIDENT with respect to different numbers of visual features and different label smoothing factors, respectively. Experiments exploring the impact of hyperparameters are conducted on datasets MIT-States and C-GQA.\nImpact of the number of visual features. We illustrate the performance of TRIDENT influenced by different numbers of attribute features in Figure. In Figure 4a, the performance of our model on MIT-States generally improves with the increasing number of visual features, but subsequently declines. This trend is reasonable, as a greater number of Visual features contains more useful information, thereby enhancing the performance. However, the number of useful features is limited; thus, an excessive number of visual features may introduce redundancy and noise, ultimately hampering the performance of the model.\nHowever, in Figure 4b, as the number of visual features increases, the performance of the model on C-GQA tends to decline overall. This may be attributed to the model's strong expressive capability in handling composition reasoning. In the low-noise C-GQA dataset, optimal performance can be achieved using only two features. Increasing the number of features, however, results in heightened model complexity without tangible benefits, potentially impairing generalization to unseen compositions. In contrast, the MIT-States dataset exhibits significant noise; thus, while the increase of visual features may introduce more noise, it also necessitates a greater amount of useful information, which can effectively mitigate the impact of the noise.\nImpact of the number of label smoothing factor. The label smoothing factor \u03b1 modulates the extent to which the model's confidence in seen compositions is attenuated. Figure 5a shows that as \u03b1 increases, the model's performance on MIT-States initially improves before subsequently declining. This is because if alpha is too small, label smoothing fails to enhance generalization, while if alpha is too large, it adversely affects the model's ability to learn the representation of the original labels, resulting in more losses than gains. However, as shown in Figure 5b, the model achieves the best performance with C-GQA \u03b1 smaller \u03b1. This may be attributed to the fact that, compared to everyday objects, LLMs are less familiar with in-the-wild objects, leading to relatively lower quality in the generated auxiliary attributes; thus, a smaller smoothing factor can mitigate the impact."}]}