{"title": "Efficient line search for optimizing Area Under the ROC Curve in gradient descent", "authors": ["Jadon Fowler", "Toby Dylan Hocking"], "abstract": "Receiver Operating Characteristic (ROC) curves are useful for evaluation in binary classification and changepoint detection, but difficult to use for learning since the Area Under the Curve (AUC) is piecewise constant (gradient zero almost everywhere). Recently the Area Under Min (AUM) of false positive and false negative rates has been proposed as a differentiable surrogate for AUC. In this paper we study the piecewise linear/constant nature of the AUM/AUC, and propose new efficient path-following algorithms for choosing the learning rate which is optimal for each step of gradient descent (line search), when optimizing a linear model. Remarkably, our proposed line search algorithm has the same log-linear asymptotic time complexity as gradient descent with constant step size, but it computes a complete representation of the AUM/AUC as a function of step size. In our empirical study of binary classification problems, we verify that our proposed algorithm is fast and exact; in changepoint detection problems we show that the proposed algorithm is just as accurate as grid search, but faster.", "sections": [{"title": "1 Introduction", "content": "In supervised machine learning problems such as binary classification [Cortes and Mohri, 2004] and changepoint detection [Rigaill et al., 2013], the goal is to learn a function that is often evaluated using a Receiver Operating Characteristic (ROC) curve, which is a plot of True Positive Rate (TPR) versus False Positive Rate (FPR) [Egan and Egan, 1975]. For data with n labeled examples, a predicted value \\hat{y}_i \\in R is computed for each labeled example i \\in {1, ..., n}, and in binary classification the threshold of zero is used to classify as either positive (\\hat{y}_i > 0, True Positive=TP for a positive label, False Positive=FP for a negative label) or negative (\\hat{y}_i < 0, True Negative=TN for a negative label, False Negative=FN for a positive label). Computing overall true positive and false positive rates yields a single point in ROC space, and the different points on the ROC curve are obtained by adding a real-valued constant c\\in R to each predicted value \\hat{y}_i (Figure 1). Large constants c result in FPR=TPR=1 and small constants result in FPR=TPR=0; a perfect binary classification model has an AUC of 1, and a constant/bad model has an AUC of 0.5.\nWhile AUC is often used as the evaluation metric in machine learning, it can not be used to compute gradients, because it is a piecewise constant function of predicted values. Recently, Hillman and Hocking [2023] proposed the AUM, or Area Under Min(FP,FN), as a surrogate loss, and showed that minimizing the AUM results in AUC maximization, in unbalanced binary classification and supervised changepoint detection. More specifically, minimizing the AUM encourages points on the ROC curve to move to the upper left (Figure 1). In this paper, we propose a new gradient descent learning algorithm, which uses the gradients"}, {"title": "1.1 Contributions and organization", "content": "Our main contribution is a new log-linear algorithm for efficiently computing a line search to determine the optimal step size (learning rate) in each step of gradient descent, when learning linear model with the AUM loss. In Section 3, we define the AUM line search problem, then in Section 4, we prove efficient update rules for computing changes in AUM and AUC, which results in a complete representation of the piecewise linear/constant AUM/AUC as a function of step size. Remarkably, even though AUC can not be used to compute gradients (since it is piecewise constant), we show that it can be used as the criterion to maximize (on either the subtrain or validation set) during the computationally efficient log-linear line search algorithm. In Section 5, we provide an empirical study of supervised binary classification and changepoint detection problems, showing that our new line search algorithm is fast (sub-quadratic), and just as accurate as a standard/slow grid search. Section 6 concludes with a discussion of the significance and novelty of our findings."}, {"title": "2 Related work", "content": "There are two groups of approaches to dealing with class imbalance in binary classification: re-weighting the typical logistic/cross-entropy loss, and using other loss functions (typically defined on pairs of positive and"}, {"title": "3 Models and Definitions", "content": "In supervised binary classification, we are given a set of n labeled training examples, {(x_i, Y_i)}_{i=1}^n where x \\in R^P is an input feature vector for one example and y_i \\in {-1,1} is a binary output/label. The goal of binary classification is to learn a function f: R^P \\rightarrow R which is used to compute real-valued predictions \\hat{y}_i = f(x_i) with the same sign as the corresponding label y_i. Whereas typically the logistic/cross-entropy loss is used for learning f, our proposed algorithm uses the AUM loss [Hillman and Hocking, 2023].\nIn supervised changepoint detection, we assume for each labeled training example i \\in {1, . . ., n} there is a corresponding data sequence vector z_i and label set L_i [Rigaill et al., 2013]. For example in Figure 2 we show two data sequences, each with a single label. Dynamic programming algorithms can be used on the data sequence z_i to compute optimal changepoint models for different numbers of changepoints {0,1,...} [Maidstone et al., 2016]. For example in Figure 2 (left) we show models with 0-4 changepoints. The label set L_i can be used to compute the number of false positive and false negative labels with respect to any predicted set of changepoints (false positives for too many changepoints, false negatives for not enough changepoints). Each example i also has a model selection function \\kappa_i: R^+ \\rightarrow {0,1,... } which maps a non-negative penalty value \\hat{\\lambda}_i to a selected number of changepoints \\kappa(\\hat{\\lambda}_i), and corresponding FP/FN error values (Figure 2, right)."}, {"title": "3.1 Definition of false positive and false negative functions", "content": "In this paper, we assume the following general learning context in which supervised binary classification and changepoint detection are specific examples. For each labeled training observation i, we have a predicted value \\hat{y}_i = f(x_i) \\in R, and one or more labels which let us compute the contribution of this observation to the false negative rate FN_i(\\hat{y}_i) \\in [0, 1] and false positive rate FP_i(\\hat{y}_i) \\in [0,1] (for example, Figure 2, right). The FP_i starts at zero (no false positives for very small predicted values), the FN_i ends at zero (no false negatives for very large predictive values). These functions are piecewise constant, so can be represented by breakpoint tuples (v, \\Delta FP, \\Delta FN), where v \\in R is a predicted value threshold where there are changes \\Delta FP, \\Delta FN (discontinuity) in the error functions. In binary classification with n^+ positive examples and n^- negative examples, these functions can be exactly represented by the breakpoint (v = 0, \\Delta FP = 0, \\Delta FN = -1/n^+) for all positive examples, and (v = 0, \\Delta FP = 1/n^-, \\Delta FN = 0) for all negative examples. In supervised changepoint detection, there can be more than one breakpoint per error function (for example, Figure 2, right, shows three breakpoints per error function). These breakpoints will be used in our proposed learning algorithm, since they give information about how the predicted values affect the ROC curve."}, {"title": "3.2 Linear model predictions and errors as a function of step size", "content": "Let there be a total of B breakpoints in the error functions over all n labeled training examples, where each breakpoint b \\in {1, . . ., B} is represented by the tuple (v_b, \\Delta FP_b, \\Delta FN_b, I_b). The I_b \\in {1, ..., n} is an example index, so there are changes \\Delta FP, \\Delta FN at predicted value v_b \\in R in the error functions FP_{I_b}, FN_{I_b}.\nFor example the labeled data sequences shown in Figure 2 represent n = 2 labeled training examples, with B = 6 breakpoints total. For a linear model f(x) = w^\\intercal x_i, we can compute a descent direction d \\in R^P based on the directional derivatives of the AUM [Hillman and Hocking, 2023], which gives the following predictions, as a function of step size s (learning rate),\n\\hat{y}_i(s) = (w + sd)^\\intercal x_i.\nFor each breakpoint b, we define the following function, which tells us how its threshold evolves as a function of step size, in the plot of error rates (Figures 1 and 3) as a function of the constant c added to predicted values in ROC curve computation.\nT_b(s) = v_b - \\hat{y}_{I_b}(s) = v_b - (w + sd)^\\intercal x_{I_b}.\nWe can see from the equation above that T_b(s) is a linear function with slope \\delta_b = -d^\\intercal x_{I_b}, and intercept v_b = v_b - \\hat{y}_{I_b}(0) = v_b - w^\\intercal x_{I_b}. This equation can be used to plot the threshold T_b(s) as a function of the step size s (Figures 3-4). Given the B observation error breakpoints, a prediction vector \\hat{y} = [\\hat{y}_1\\dots \\hat{y}_n]^\\intercal \\in R^n, and a descent direction d, we define the error functions\nFP(s) = \\sum_{j:T_j(s) < T_b(s)} \\Delta FP_j, \\qquad FN(s) = \\sum_{j:T_j(s) \\geq T_b(s)} -\\Delta FN_j.\nThe FP(s), FN_i(s) \\in [0,1] are the error rates before the threshold T_b(s), in the plot of label error rates as a function of the constant c added to predicted values (for example, Figure 3, bottom left). Additionally we define M(s) = min{FP_i(s), FN(s)} which is the min of the two error rates."}, {"title": "4 Proposed line search algorithm", "content": "In this section, we provide theorems which state the update rules of our proposed algorithm."}, {"title": "4.1 Initialization", "content": "Let K > 1 be the max number of iterations, and let k \\in {1, 2, ..., K} be the counter for the iterations of our proposed algorithm. For each iteration k, there is a step size \\sigma_k, and the initial step size is \\sigma_1 = 0. The proposed algorithm computes an exact representation of the piecewise constant error rates (FPR,FNR) as a function of the constant c added to predicted values (same as in ROC curve computation, see Figure 1). At each step size, there are B error rates (FPR, FNR) which must be computed, one for each breakpoint b \\in {2, ..., B} in label error functions. We use the notation FP_b^k to denote the false positive rate before breakpoint b, at iteration k. Note that we use the superscript k to clarify the presentation of the update rules in this paper, but the algorithm only stores values for the current k, using O(B) storage. Let q^1 = (q_1, \\dots, q_B) be a permutation of (1,..., B) such that the threshold functions are sorted, T_{q_1}(0) <\\dots \\leq T_{q_B}(0). We also have for all b \\in {2,..., B}, initialize FP_b^k = FP_{q_1} (0) and FN_b^k = FN_{q_{b+1}} (0) (also TP = 1 - FN and M = min{FP, FN}}) which is possible in log-linear time, by first sorting by threshold values, T_{q_b} (0), then using a cumulative sum (3). Note that these initializations start at index 2 and end at index B; the first index is missing because the Min below the first interval is always zero (by assumption that the False Positive Rate starts at zero). Similarly, the Min after the last interval is always zero, by assumption that the False Negative Rate ends at zero. That is, for any iteration k, we have M_1 = 0 and M_{B+1} = 0. In the first iteration, we"}, {"title": "4.2 Update rules for error functions", "content": "The initialization is valid for any step sizes s \\in (0,\\sigma_2), where \\sigma_2 is the smallest step size such that the permutation q is no longer a valid ordering of the threshold functions (there is an intersection between two or more threshold functions T_b at \\sigma_2). More generally, for any iteration k \\in {2, ..., K}, we assume that at step size \\sigma_k, there is an intersection, T_{q_\\beta}(\\sigma_k) = T_{q_{\\beta-1}}(\\sigma_k), and \\beta is the index of the function which is larger before"}, {"title": "4.3 Update rules for AUM and AUC", "content": "Next, we state our first main result, the constant time update rule for the AUM slope D^k.\nTheorem 1. For data with B breakpoints in label error functions, the initial AUM slope is computed via (6) in log-linear O(B log B) time. If \\beta\\in {2,...,B} is the index of the function T_\\beta which is larger before an intersection at step size \\sigma_{k+1}, then the next AUM slope D^{k+1} can be computed from the previous D^k in constant O(1) time, using (13).\nD^{k+1} = D^k + (\\delta_{q_{\\beta}} - \\delta_{q_{\\beta-1}})(I[\\beta B]M_{\\beta+1}^{k+1} + M_{\\beta-1}^{k+1} - M_{\\beta-1}^{k} - M_{\\beta}^{k+1}).\nProof. The result can be derived by writing the terms in D^{k+1} and D^k, then subtracting:\nD^{k+1} \u2013 D^k = (\\sum_{b=2}^{B} (\\delta_{q_b}^k - \\delta_{q_{b-1}}^k) M_b^{k+1}) - (\\sum_{b=2}^{B} (\\delta_{q_b}^k - \\delta_{q_{b-1}}^k) M_b^{k}).\n=(\\delta_{q_{\\beta}}^k - \\delta_{q_{\\beta-1}}^k)(I[\\beta"}, {"title": "4.4 Implementation Details", "content": "The update rules which we proposed in the previous section require identification of a pair of threshold evolution functions which are the next to intersect, T_{q_\\beta} (\\sigma_k) = T_{q_{\\beta-1}}(\\sigma_k). To efficiently perform this identification, we propose an algorithm which begins by sorting the linear T_b functions by intercept, then using intercept/slope values to store intersections of all B - 1 possible pairs of adjacent functions. There may be fewer than B - 1 intersections to store, because some adjacent pairs may have parallel lines, or intersection at negative step sizes. Typically intersections involve only two lines, but when there are more, they can be handled using the following data structures:\n* An Interval Group is a collection of lines that intersect at the same step size and threshold.\n* An Interval Column is an associative array where each key is an intersection threshold, and each value is an Interval Group. This contains all of the intersections at a given step size.\n* A Interval Queue is a C++ STL map from step sizes to Interval Columns (red-black tree, Figure 4, right). The map container allows constant time lookup of the next intersection (Interval Column with smallest step size), and log-linear time insertion of new entries.\nThe algorithm starts by creating an Interval Queue and filling it with all of the intersections between every line and the line after it. Each iteration looks at the first Interval Column in the queue which at the start will be the one with the step size closest to 0. We update AUM slope and AUC using Theorems 1-2, and insert up to two new intersections, each of which takes O(log B) time using the STL map (red-black tree). We run this algorithm until we have reached the desired number of iterations, or we have found the first local min AUM, or first local max AUC. Asymptotic complexity is O(B) space and O([B + I] log B) time, where I is the number of iterations. Finally we note that the update rules can be implemented with respect to either the subtrain set (to guarantee decreasing AUM at every step), or with respect to the validation set (to search for max AUC that avoids overfitting)."}, {"title": "5 Empirical Results", "content": "Hillman and Hocking [2023] provided a detailed empirical study of the AUM loss relative to other baseline loss functions (logistic loss, re-weighting, squared hinge loss defined on all pairs of positive and negative examples), so the empirical study in the current paper focuses on characterizing the time complexity of the proposed line search algorithm. No special computer/cluster was required for computational experiments."}, {"title": "5.1 Empirical asymptotic time complexity analysis in benchmark classification data sets", "content": "Goal and expectation. In this section, our goal was to empirically estimate the asymptotic complexity of the proposed line search, with the three proposed variants (linear, quadratic, first min, see Figure 4). For a binary classification data set with n labeled examples (and therefore B = n breakpoints in label error functions), we expected that: (1/linear) line search with n iterations should be log-linear time, O(nlog n); (2/quadratic) line search exploring all intersections should be quadratic, O(n^2); (3/first min) line search exploring up until the first min AUM should be faster than quadratic (but guaranteed to find the same solution, due to the convexity of the AUM loss function).\nData and algorithm setup. We considered four benchmark binary classification data sets: CIFAR10 [Alex, 2009], FashionMNIST [Xiao et al., 2017], MNIST [LeCun et al., 1998], STL10 [Coates et al., 2011]. For each data set, there are ten classes, so we converted them into an unbalanced binary classification problem by using the first class as negative label, and the other classes as positive label. For various data sizes n starting with at least 10 examples of the minority class, and then increasing n, we randomly initialized the linear model near zero (four random seeds, standard normal), then implemented gradient descent with the three versions of AUM line search (full gradient method, batch size n), continuing until the AUM stops decreasing (within 10^-3).\nExperimental results. In Figure 5 (top), we show the mean number of line search iterations per gradient descent step, for each of the three line search variants, along with the maximum possible number of iterations for a given n (black line, max intersections of n lines is n(n - 1)/2, a quadratic upper bound on the number of iterations/step sizes considered by our proposed line search). Interestingly, the number of line search iterations of the first min variant appears to be sub-quadratic (smaller slope on log-log plot), indicating that performing an exact line search is computationally tractable, nearly log-linear O(nlog n) (amortized average number of iterations over all steps of gradient descent). Also, we observed that the number of gradient descent steps for first min is asymptotically small for the first min variant (same as quadratic), whereas the linear variant is relatively large. Overall, these results suggest that the proposed line search is fast and exact in benchmark binary classification data sets."}, {"title": "5.2 Accuracy and computation time in supervised changepoint problems", "content": "Motivation and setup. We were also interested to examine the accuracy of the line search, as measured by the max validation AUC over all steps of gradient descent. We expected that the proposed line search should be faster than standard grid search, and be just as accurate. We tested this expectation using the chipseq supervised changepoint data set from the UCI repository [Asuncion and Newman, 2007]. In one representative supervised changepoint data set (H3K4me3_TDH_immune), we used 4-fold cross-validation to create train/test splits, then further divided the train set into subtrain/validation sets (four random seeds). We initialized the linear model by minimizing a Ll-regularized convex surrogate of the label error [Rigaill et al., 2013], with L1 penalty chosen using the validation set, then ran AUM gradient descent using the proposed line search or grid search (using gradients from the subtrain set, batch size n), until AUM stops decreasing (within 10^-5).\nExperimental results. After every step of gradient descent, the AUC was computed on the validation set, and we report the max AUC in Figure 6 (bottom). It is clear that the proposed algorithms achieve a similar level of validation AUC, as the grid search baseline (and all are significantly more accurate than the validation AUC at initialization of gradient descent). Also, we computed timings (Figure 6, top), and observed that the fastest method was the proposed line search (first min variant). Interestingly, we observed that the slowest method overall was the linear variant, which stops the line search after B iterations. Using that method, each iteration of gradient descent is guaranteed to be log-linear O(B log B), but it takes more time overall because it must take more steps of gradient descent (each of which has a relatively small step size / learning rate). Overall, these empirical results show that the proposed line search yields useful speedups relative to grid search, when learning a linear model to minimize AUM and maximize AUC."}, {"title": "6 Discussion and conclusions", "content": "This paper proposed a new algorithm for efficient line search, for learning a linear model with gradient descent. The proposed algorithm exploits the structure of the piecewise linear/constant AUM/AUC, in order to get a complete representation of those functions, for a range of step sizes, which can be used to pick the best learning rate in each step of gradient descent. For future work, we are interested in exploring extensions to neural networks with the ReLU activation function, which is piecewise linear, so could potentially be handled using a modification to our proposed algorithm."}, {"title": "Broader Impacts", "content": "The proposed algorithm could save time if applied to real binary classification or changepoint problems; negative impacts include potential misuse, similar to any algorithm."}, {"title": "Limitations", "content": "The proposed line search only works for a linear model."}, {"title": "Reproducible Research Statement", "content": "All of the software/data to make the figures in this paper can be downloaded from a GitHub repository: https://github.com/tdhock/max-generalized-auc. We also provide a free/open-source C++ implementation of the proposed algorithm, as the function aum_line_search in the aum R package, on CRAN and https://github.com/tdhock/aum."}]}