{"title": "VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying Misinformation of Short Videos", "authors": ["Weihao Zhong", "Yinhao Xiao", "Minghui Xu", "Xiuzhen Cheng"], "abstract": "Short video platforms have become important channels for news dissemination, offering a highly engaging and immediate way for users to access current events and share information. However, these platforms have also emerged as significant conduits for the rapid spread of misinformation, as fake news and rumors can leverage the visual appeal and wide reach of short videos to circulate extensively among audiences. Existing fake news detection methods mainly rely on single-modal information, such as text or images, or apply only basic fusion techniques, limiting their ability to handle the complex, multi-layered information inherent in short videos. To address these limitations, this paper presents a novel fake news detection method based on multimodal information, designed to identify misinformation through a multi-level analysis of video content. This approach effectively utilizes different modal representations to generate a unified textual description, which is then fed into a large language model for comprehensive evaluation. The proposed framework successfully integrates multimodal features within videos, significantly enhancing the accuracy and reliability of fake news detection. Experimental results demonstrate that the proposed approach outperforms existing models in terms of accuracy, robustness, and utilization of multimodal information, achieving an accuracy of 90.93%, which is significantly higher than the best baseline model (SV-FEND [1]) at 81.05%. Furthermore, case studies provide additional evidence of the effectiveness of the approach in accurately distinguishing between fake news, debunking content, and real incidents, highlighting its reliability and robustness in real-world applications.", "sections": [{"title": "INTRODUCTION", "content": "With the rise of social media platforms such as Twitter and Weibo in China, these platforms have become essential channels for people to access the latest news and freely express their opinions [2]. However, the convenience and openness of social media have also facilitated the rapid spread of misinformation, which includes news containing intentionally false information. Misinformation not only disrupts the order of cyberspace but also has significant negative impacts on real-world events. For instance, in the political domain, during the month leading up to the 2016 U.S. presidential election, Americans were exposed to an average of one to three pieces of fake news from well-known publishers [3], which inevitably misled voters and influenced the election outcome. In the economic domain, a piece of fake news claiming that Barack Obama had been injured in an explosion caused a loss of $130 billion in stock value [4]. In the social domain, in India, numerous innocent people were killed by locals due to a widely circulated fake news story about child trafficking [5]. Similarly, during the early stages of the COVID-19 pandemic in China, rumors spread widely on social media platforms like Weibo, including false claims that drinking strong alcohol could prevent infection or that certain medicines could cure the virus [6]. These rumors fueled widespread panic and misinformation, complicating public health responses and delaying effective measures. Therefore, the automatic detection of fake news has become an urgent and critical issue in recent years [7].\nThe advancement of multimedia technology has propelled the evolution of user-generated news, transforming it from text-based posts to multimedia posts incorporating images and videos. This shift has garnered greater consumer attention and enabled more credible storytelling. On the one hand, visual content, such as images and videos, is more engaging and attention-grabbing than plain text, thus accelerating the spread of news. For example, tweets that include images receive 18% more clicks, 89% more likes and 150% more retweets compared to those without images [8]. On the other hand, visual content is often used as evidence to support a narrative, enhancing its credibility. Unfortunately, this advantage is also exploited by fake news. In order to spread rapidly, fake news often features misrepresented or even tampered images or videos to attract and mislead consumers. As a result, visual content has become a key element of fake news, making multimedia fake news detection an emerging challenge.\nExisting fake news detection methods have primarily focused on text-based or text-image combined news. In contrast, the detection of fake news in videos remains a relatively emerging and under-explored area of research. Traditional video-based fake news detection methods rely on single-modal information, such as extracting text or image features from videos for detection [9], [10]. These methods often overlook the synergistic effect of multimodal information within the video, failing to effectively combine audio, visual, and contextual data. As a result, they show limitations when dealing with complex fake news that relies on multimodal fusion, which impacts detection accuracy. To address this issue, an increasing number of studies have adopted multimodal fusion techniques, attempting to enhance detection by combining text, audio, and visual information. However, these methods still face challenges related to modal inconsistency, especially in the alignment of content between different modalities. For instance, some methods are able to combine text and comments for detection [11] or apply sentiment analysis [12] to strengthen their judgments, but they fail to adequately consider the complex interaction between visual cues and emotions, resulting in room for improvement in detection performance. In addition, recent studies have incorporated more advanced techniques, such as counterfactual evidence and cross-modal reasoning, to improve model interpretability by introducing more diverse sources of evidence [13]. Although these methods have made progress in overcoming the limitations of multimodal models, they still face difficulties in handling dynamically manipulated videos, particularly with subtle visual and audio changes. Therefore, despite advancements in multimodal fusion and interpretability in video fake news detection, each method continues to face unique challenges: single-modal methods suffer from limited feature representation; multimodal methods struggle with modal inconsistency; and counterfactual and interpretability-based methods need further refinement to address complex manipulations in videos.\nTo address the challenges in fake news video detection, we propose VMID (Video Multimodal Information Detection), a multimodal framework that processes and integrates various data from short videos for input into a large language model (LLM). VMID utilizes pre-trained models such as Whisper [14] for audio transcription, CogVLM2 [15] for visual frame analysis, and VSE (Video-subtitle-extractor) [16] for aligning textual and visual content to create a unified multimodal representation. Furthermore, it incorporates metadata, including upload time, engagement metrics, and user comments, which provide valuable social context signals. The combined data is then structured into a prompt for evaluation by a LoRA (Low-Rank Adaptation)-tuned LLM, enabling the precise identification of misinformation within short videos.\nOur contributions are threefold:\n\u2022\tProposing an End-to-End Multimodal Fake News"}, {"title": "APPROACH", "content": "In this section, we outline the overall framework for analyzing video content. As shown in Fig 1, our framework abstracts the challenges of misinformation detection by integrating various data sources. We first introduce our problem formulation, which captures the complexities involved in identifying false information across different modalities. Next, we present the structure of our methodology, designed to effectively address these challenges. It is important to note that we describe our research approach at a high level, without delving into specific implementation details, as these may vary depending on the use case.\nA. Problem Formulation\nIn this subsection, we formally define the core problem of misinformation detection by constructing a threat model and outlining the potential risks associated with the propagation of misinformation in short videos. The goal of this study is to utilize a multimodal framework to determine whether the content of short videos conveys misleading information or serves a debunking purpose.\nShort videos have become a high-risk medium for misinformation due to their rapid dissemination and high user engage-"}, {"title": "Key Challenges", "content": "b) Key Challenges: In misinformation detection, we face the following main challenges, and we propose specific solutions for each:\nChallenge 1: Effective Multimodal Data Integration \u2014 Integrating text, audio, visual, and metadata presents significant technical challenges due to variations in data format and relevance across modalities. To address this, we utilize the VMID framework to convert multimodal data into a unified text input format for efficient processing by the LLM.\nChallenge 2: Accurate Contextual Analysis \u2014 Maintaining content intent consistency across different modalities to determine whether it propagates misinformation or serves as debunking content. The VMID framework integrates all modalities into a unified context to allow the LLM to accurately identify subtle contextual information.\nChallenge 3: Temporal and Spatial Consistency \u2014 Ensuring that information extracted from different timestamps and visual frames remains contextually consistent. VMID integrates subtitle, audio transcript, and keyframe information into a single prompt to maintain temporal and spatial consistency across modalities.\nIn conclusion, the VMID framework develops a comprehensive mechanism for video content analysis through the integration of multimodal data and the maintenance of contextual consistency, thereby improving the capability of the LLM to detect misinformation in short videos."}, {"title": "Overall Structure of the Methodology", "content": "B. Overall Structure of the Methodology\nThe overall structure of the methodology, as shown in Fig 1, consists of several key steps to enhance misinformation detection through multimodal data integration.\n1)\tText Processing Workflow: Figure 2 illustrates the detailed workflow of the text processing pipeline, which includes keyframe extraction, text detection, feature extraction, subtitle area localization, text recognition, and output generation. Each step plays a vital role in ensuring that subtitles are accurately"}, {"title": "Subtitle Extraction", "content": "Subtitle Extraction: The subtitle extraction process relies on Optical Character Recognition (OCR) techniques and consists of the following steps:\n1)\tKeyframe Extraction: The video is analyzed to extract keyframes that correspond to moments when subtitles appear. This step isolates the frames most relevant for subtitle detection and recognition, ensuring that only the most informative frames are processed.\n2)\tText Detection: The extracted frames are passed through the Text Detection module, which utilizes the PFHead structure for preliminary adjustments. The PFHead module operates as follows: after the initial transposed convolution, the output is split into two branches. One branch undergoes upsampling and a 3x3 convolution to produce the final output, while the other branch progresses through an additional transposed convolution. The two branches are then merged via a 1x1 convolution, and the sum of the 1x1 convolution output and the second transposed convolution output yields a probability map that indicates potential text regions.\n3)\tFeature Extraction: The processed images are passed to the PP-LCNetV3 backbone for feature extraction. PP-LCNetV3, an advanced extension of the PP-LCNet family, combines high precision with efficient inference, making it suitable for various downstream tasks. Key optimizations include a learnable affine transformation module, enhanced re-parameterization techniques, an improved activation function, and modifications to network depth and width, all of which contribute to robust feature extraction for text detection and recognition.\n4)\tDetection Box Rectification: The features are passed to the Detection Box Rectification module, which refines the localization of text regions within each frame. To improve detection accuracy and robustness, we employ a Dynamic Scaling Ratio (DSR) strategy that progressively adjusts the scaling ratio during training. Specifically, as the training progresses, the scaling ratio $s$ increases linearly from 0.4 to 0.6. The scaling ratio $s$ at epoch $t$ out of total epochs $T$ is defined as:\n$s = 0.4 + \\frac{0.2 \\cdot t}{T}$                                                                                                                                                                                                          (1)\nThis strategy enhances the ability of the model to adapt to varying text sizes across different frames, improving localization accuracy and robustness.\n5)\tText Recognition: The identified text areas are processed by the Text Recognition module, which uses a Transformer-based No-Recurrence Sequence-to-Sequence (NRTR) model. The recognition is trained using Connectionist Temporal Classification (CTC) loss, a popular approach for sequence-to-sequence tasks where alignment between input and output sequences is not guaranteed.\nThe CTC loss function computes the probability $p(Y|X)$ by summing over all possible alignments $\\pi$ between the input sequence $X$ and the target sequence $Y$, where $\\pi$ represents a valid alignment path. This can be written as:\n$CTC \\text{ Loss } = -log \\left(\\sum_{\\pi \\in Align(X, Y)} p(\\pi | X)\\right)$                                                                                                                                                                                                           (2)\nThis formulation allows for flexible alignment, making it well-suited for OCR tasks where the text in video content can vary in length and position across frames. The advantages of using CTC include its ability to handle variable-length sequences, which is crucial for recognizing complex subtitle structures, and its improved recognition accuracy, particularly when combined with the NRTR model, which enhances performance for subtitles spanning multiple frames or with complex transitions.\n6)\tOutput Generation: The system outputs the extracted text along with its corresponding timestamps, preserving both the textual content and its temporal alignment with"}, {"title": "Audio Processing", "content": "2)\tAudio Processing: We utilized Whisper [14] to process video audio, transcribing it into text to support natural language understanding tasks. This conversion from spoken to written form is essential for capturing semantic and emotional cues embedded in speech, such as tone, speech rate, and word choice, which convey the emotional state and intent of the speakers. By capturing these nuances, we enhance the depth and precision of our analysis.\nAudio Preprocessing: We standardize the audio signal to a uniform format, typically a 16 kHz sampling rate, involving resampling and gain adjustment. Audio features are then extracted, including the Mel spectrogram, which is calculated as:\n$S(t, f) = \\sum_{k} |X(k)|^2 \\cdot h_m(t, f)$                                                                                                                                                                                                                                                         (3)\nwhere $S(t, f)$ is the Mel spectrogram value at time $t$ and frequency $f$, $X(k)$ is the Fourier transform of the audio signal, and $h_m(t, f)$ represents the Mel filter. This step improves recognition by enhancing critical frequency features in the audio.\nFeature Encoding: The extracted audio features are input to a Transformer-based encoder, which converts them into contextual embeddings by capturing long-range dependencies in the sequence. Using multi-head self-attention, the encoder maps the features through query (Q), key (K), and value (V) matrices, described by:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V$                                                                                                                                                                                                           (4)\nwhere Q, K, and V are learned linear transformations of the audio feature embeddings, and $d_k$ is the dimensionality of the key vectors. In this context, the feature sequence, initially representing the raw characteristics of the audio signal, is transformed through the Q, K, and V matrices, which capture the relationships between different segments of the audio sequence. Specifically, Q represents the elements that \"query\" others, K contains the \"keys\" providing potential matches, and V stores the \"values\" or responses. This process allows the model to focus on relevant temporal relationships, such as rhythm and pauses in speech.\nThis mechanism enables the system to dynamically assign weights to different parts of the audio, prioritizing sections that convey meaningful information (e.g., intonation changes indicating emotional cues). In this way, the attention mechanism enhances the capability of the model to accurately interpret the context of each audio segment, leading to a deeper understanding of the spoken content.\nDecoding: A Transformer-based decoder predicts the most probable text sequence using beam search:\n$BestSequence = arg \\max_{seq \\in C} score(seq)$                                                                                                                                                                                                             (5)\nwhere C represents possible decoding sequences, and score evaluates the likelihood of each sequence, ensuring optimal transcription accuracy.\nMulti-task Training: Whisper employs a unified prompt format to specify language and task with special markers, enhancing multilingual and multi-task adaptability by utilizing a large-scale supervised dataset.\nThrough these processes, Whisper surpasses traditional audio processing methods, delivering a unified representation of diverse audio modalities and establishing a reliable data foundation for in-depth analysis."}, {"title": "Image Processing", "content": "3)\tImage Processing: For the visual component of videos, particularly keyframes and video segments, we use the CogVLM2 model [15]. It adeptly understands and describes visual content, creating detailed text descriptions that capture the essential information of the video. Carefully selected keyframes represent the most significant parts of the video, effectively transforming visual elements into meaningful textual descriptors. These keyframes highlight important scene changes, emotional expressions, or information transmission, providing a more intuitive understanding for viewers. The strength of CogVLM2 lies in its deep learning capabilities, which enable it to identify and parse complex visual features, generating contextually relevant descriptions.\nTechnical Details and Mathematical Principles:\nKeyframe Selection: In video processing, selecting representative keyframes is crucial. We analyze video frame rates and scene changes, considering factors such as scene transitions and emotional expressions, to select the most representative keyframes. Specifically, we use two methods: 1. Frame Difference Method: We calculate the pixel value differences between adjacent frames and select frames with significant differences as keyframes. If the pixel difference between two frames exceeds a predefined threshold, the frame is considered to contain important information changes. 2. Motion Vector Analysis: We use optical flow estimation techniques to analyze the direction and speed of object movements in the video. Frames with significant motion changes are selected as keyframes. This involves computing the motion vectors of objects in each frame and selecting frames with notable changes in these vectors."}, {"title": "Visual Feature Extraction", "content": "Visual Feature Extraction: We utilize the CogVLM2 model to extract complex visual features from keyframes, including objects, actions, and colors. The process involves:\n1.\tInputting keyframes into a pre-trained deep learning model, which extracts high-level features through multiple layers of convolutional neural networks (CNN). 2. Using a Vision Transformer (ViT [18]) encoder to segment the image into fixed-size patches. Each patch is treated as an individual token, and positional embeddings are added to maintain positional information. These patch tokens interact through a multi-head self-attention mechanism to generate the final feature representation."}, {"title": "Text Description Generation", "content": "Text Description Generation: Based on the extracted visual features, we generate detailed text descriptions to capture the core information of the video, providing viewers with a more intuitive understanding. The process includes:\n1.\tInputting the extracted visual features into a natural language generation model, which generates descriptive text using multiple layers of recurrent neural networks (RNN) or transformers. 2. Utilizing a cross-attention mechanism during the description generation process to integrate visual and linguistic features, helping the model focus on the most relevant parts of the image for accurate and rich descriptions."}, {"title": "Multimodal Fusion", "content": "Multimodal Fusion:\nMultimodal Adapter: A 2x2 convolutional layer and the SwiGLU module align visual features with linguistic representations, achieving nearly lossless transformation. The SwiGLU module is defined as:\n$SwiGLU(x) = x \\odot \\sigma(W_1 x + b_1) + W_2 x + b_2$\nwhere $W_1$ and $W_2$ are weight matrices, $b_1$ and $b_2$ are bias terms, $\\sigma$ is an activation function (commonly ReLU), and $\\odot$ denotes element-wise multiplication. The SwiGLU module helps align visual features with linguistic representations, ensuring nearly lossless transformation. Specifically, it combines the input $x$ with a gating activation function $\\sigma(W_1 x + b_1)$ to control the flow of information. The gating activation function determines which parts of the input should pass through, and the element-wise multiplication ensures that only relevant features are retained. This mechanism allows for efficient and effective integration of visual and linguistic features, enhancing the model's ability to generate rich and accurate descriptions.\nMultimodal Description Generation: The multimodal adapter fuses visual features with linguistic features to generate comprehensive descriptions, integrating both visual information and linguistic context for richness.\n4)\tMetadata Extraction: We gather additional metadata, such as video upload time, comment counts, like counts, and author information. This metadata plays a crucial role in contextualizing the content and understanding viewer engagement. For instance, the upload time can indicate the recency of information, while comment counts may reflect audience interest and perception. By analyzing this metadata, we gain deeper insights into how videos are perceived and the factors influencing viewer engagement.\n5)\tMulti-modal Integration: After extracting information from each modality, all processed data is integrated into high-dimensional vectors and input into a LLM for comprehensive analysis. This integration allows the model to leverage its extensive knowledge base and reasoning capabilities to assess whether the video content propagates misinformation or serves to debunk false claims.\nThe integration of various modality vectors can be mathematically represented as follows:\n$V_{combined} = Concat(V_{text}, V_{audio}, V_{visual})$                                                                                                                                                                                                                    (6)\nwhere $V_{combined}$ represents the combined high-dimensional vector, and $V_{text}, V_{audio}, and V_{visual}$ denote the vectors obtained from the text, audio, and visual modalities, respectively. Once the vectors are combined, they undergo self-attention processing in the LLM, which can be expressed as Equation 4. Finally, the output is generated through a decoder, which translates the model's hidden states into a natural language classification decision.\n6)\tPerformance Optimization: To further enhance model performance, we employ LoRA fine-tuning techniques and multi-modal feature fusion methods. These techniques have been extensively applied in our experiments, significantly improving the model's precision in identifying potential misinformation and enhancing its adaptability to complex content scenarios.\nTechnical Details and Mathematical Principles:\nLORA Fine-Tuning: LoRA is a technique for fine-tuning pre-trained models by incorporating low-rank matrices into the original model parameters. This method maintains the primary parameters of the model while updating only a small set of newly added parameters, thereby reducing computational costs and minimizing the risk of overfitting.\nFor each original weight matrix $W$, LoRA adds two low-rank matrices $A$ (column matrix) and $B$ (row matrix). The updated weights can be expressed as:\n$W_{new} = W + \\alpha \\cdot A \\cdot B$                                                                                                                                                                                                                                                                         (7)\nwhere $\\alpha$ is a scalar that controls the intensity of the update. In our experiments, we first load the pre-trained model weights $W$ and initialize the low-rank matrices $A$ and $B$. By adjusting $\\alpha$, we gradually update the model parameters during the fine-tuning process, reducing computational costs and avoiding overfitting. We initialize the low-rank matrices $A$ and $B$ with small random values and only update these matrices during training, keeping the original weights $W$ unchanged. We use cross-validation to tune $\\alpha$ and employ cosine annealing schedulers and learning rate warm-up techniques to dynamically adjust the learning rate, ensuring the stability and efficiency of the training process."}, {"title": "Multi-Modal Feature Fusion", "content": "Multi-Modal Feature Fusion: To comprehensively capture the information in video content, we adopt multi-modal feature fusion methods. Specifically, we use pre-trained models to extract descriptions from the video's text, audio, and visual modalities. For instance, we use a pre-trained speech recognition model to convert audio into text and a pre-trained image recognition model to generate text descriptions of visual content. These descriptions are combined into a single fused text prompt, which is then input into a LLM for comprehensive analysis. The LLM processes this fused text prompt, converting it into a high-dimensional vector representation and performing multi-modal fusion within the model. To ensure consistency, a multi-modal adapter adjusts the spatial or channel dimensions of the features, allowing for seamless"}, {"title": "Implementation", "content": "IV. IMPLEMENTATION\nThis section details the implementation of our multimodal framework for short video content analysis. Figure 1 illustrates the overall architecture, which integrates text, audio, and visual processing modules for comprehensive multimodal fusion and misinformation detection.\nA. Text Processing Enhancement\nWe enhanced text processing by optimizing the VSE system to handle various subtitle formats and encodings, improving the accuracy of text extraction. In cases where subtitles are unavailable, Whisper transcription, combined with noise reduction preprocessing, significantly enhances transcription quality. To further analyze the extracted text, we apply advanced NLP (Natural Language Processing) techniques such as lemmatization, part-of-speech tagging, and named entity recognition. These techniques enrich the semantic analysis of the language and support subsequent sentiment and topic modeling.\nB. Audio Processing\nFor audio processing, we utilized the Whisper pre-trained model, leveraging its robust multilingual transcription capabilities without requiring additional fine-tuning. This model enhances efficiency by reliably transcribing audio content directly, ensuring high accuracy across various languages and contexts.\nC. Visual Processing Enhancements\nKeyframe extraction is essential for accurately capturing video content. In CogVLM2, we employed a segment-based processing technique and a dynamic frame sampling strategy:\n1)\tSegment Processing Technique: We divided each video into segments of a set duration $segment\\_duration\\_seconds$ seconds to ensure diverse key information is captured. The number of segments is calculated as follows:\n$segments =  \\lceil \\frac{total\\_duration}{segment\\_duration\\_seconds} \\rceil $                                                                                                                                                                                                                                                                                                                (8)\nwhere total_duration represents the total length of the video, and segment_duration_seconds indicates the duration of the segment.\n2)\tFrame Selection Strategy: Within each segment, we employed various frame selection strategies to capture keyframes:\nUniform Frame Selection: A base strategy that selects frames at uniform intervals. The formula for selecting frame IDs is:\n$frame\\_ids = np.linspace(F_a, F_b - 1, N_f, dtype=int)$                                                                                                                                                                                                                       (9)\nwhere $F_a$ and $F_b$ denote the starting and ending frames of the segment, and $N_f$ is the number of frames to select.\nTimestamp-based Matching: We also used a timestamp-based approach, selecting frames that closely match timestamps to ensure no key information is missed.\n3)\tSimilar Frame Filtering: To reduce redundancy, we implemented a cosine similarity-based filtering mechanism. This method compares adjacent frames and retains only those with cosine similarity below a predefined threshold filter_threshold. The similarity calculation is as follows:\n$similarity = \\frac{frame \\cdot frame_{-1}}{||frame|| \\cdot ||frame_{-1}||}$                                                                                                                                                                                                                                                                          (10)\nFrames with similarity above filter_threshold are discarded, ensuring that only unique frames contribute to visual analysis."}, {"title": "Metadata Processing", "content": "D. Metadata Processing\nWe developed a robust API to collect metadata, including video upload times, comment counts, likes, and user profile information. This metadata enables a deeper understanding of social context by analyzing user engagement patterns and behavior, which are essential for assessing credibility and identifying misinformation."}, {"title": "Multimodal Fusion and Weight Optimization", "content": "E. Multimodal Fusion and Weight Optimization\nThe final multimodal fusion integrates text, audio, and visual data into a unified format, fed into an LLM fine-tuned with LoRA for comprehensive analysis. To enhance fusion accuracy, we implemented a weight optimization strategy that dynamically allocates importance to each modality based on contextual relevance, allowing the model to prioritize the most informative modality in different scenarios. This adaptive weighting improves misinformation detection by enhancing the sensitivity of the model to context-specific signals.\nBy focusing on these enhancements, our framework demonstrates the practical effectiveness of multimodal misinformation detection in short video content, providing a reliable foundation for real-world applications."}, {"title": "Performance Evaluation", "content": "V. PERFORMANCE EVALUATION\nIn this section, we present a comprehensive account of the experiments designed to evaluate the effectiveness of the proposed VMID method for detecting fake news videos. This includes an in - depth look at the baseline methods, the evaluation metrics employed, the experimental results obtained, and a detailed comparative analysis against existing models."}, {"title": "Baseline Methods", "content": "A. Baseline Methods\nTo thoroughly assess the performance of the VMID approach and establish a comprehensive benchmark, we compare it with a set of well-established baseline methods that utilize multiple modalities.\n1)\tMultiModality: We use several existing state-of-the-art methods as multimodal baselines:\nHou et al 2019 [19]: Use the linguistic features from the speech text, acoustic emotion features, and user engagement features and a linear kernel SVM (Support Vector Machine) to distinguish the real and fake news videos. This method comprehensively considers information from multiple aspects for judging real and fake news.\nMedina et al 2020 [20]: Extract tf-idf vectors from the title and the first hundred comments and use traditional machine learning classifiers including logistic regression and SVM. It attempts to identify fake news by processing text information.\nChoi and Ko 2021 [21]: Use the topic distribution difference between title and comments to fuse them, and concat them with the visual features of keyframes. An adversarial neural network is used as an auxiliary task to extract topic-agnostic multimodal features for classification. This method focuses on the fusion and feature extraction of different modal information.\nShang et al 2021 [22]: Use the extracted speech text to guide the learning of visual object features, use MFCC features to enhance the speech textual features, and then use a co-attention module to fuse the visual and speech information for classification. It emphasizes the interaction and information integration between different modalities.\nIn addition to the above, we also have the following notable multimodal baseline methods:\nSV-FEND [1]: This method capitalizes on multimodal information by integrating text, images, and user comments for fake news video detection. It has demonstrated robust performance and an outstanding ability to capture crucial multimodal cues, making it a significant benchmark in the field.\nSVRPM [17]: SVRPM detects misleading content in short videos through modality tampering analysis. It also offers interpretability by highlighting discrepancies across different modalities, thereby facilitating effective misinformation identification."}, {"title": "Evaluation Metrics", "content": "B. Evaluation Metrics\nThrough this backtracking analysis, peak attention aligns with the item most pertinent to the query.\nWe adopted several key evaluation metrics to accurately measure the performance of each model under consideration:\nAccuracy (ACC): Computed as the ratio of correctly predicted samples to the total number of samples, this"}, {"title": "Experimental Results", "content": "C. Experimental Results\nThe performance results of the different models in the fake news video detection task are presented in Table I. As evident from the results, our VMID method significantly outperforms all baseline models across the evaluated metrics.\n1)\tThe SV-FEND model, with an accuracy of 81.05%, presents a strong performance. However, VMID eclipses this with an impressive accuracy of 90.93% and an F1 score of 90.89%. This superiority showcases the enhanced capability of VMID in capturing and leveraging essential multimodal cues for accurate fake news detection, highlighting its advanced design and effectiveness.\n2)\tThe FakeSV model, despite its significance in highlighting the challenges associated with complex multimodal datasets through its average accuracy below 0.8, pales in comparison to VMID. The consistent accuracy of 90.93% and F1 score of 90.89% vividly demonstrate the robustness and superiority of VMID in handling the complexities inherent in such datasets.\n3)\tOther baseline models, such as Hou et al., 2019 and Medina et al., 2020, exhibit accuracies not exceeding 76%, clearly falling short of VMID. These results spotlight the limitations of traditional feature extraction and classification techniques when faced with the intricate nature of complex multimodal data, further highlighting the innovative approach of VMID.\n4)\tNotably, VMID not only achieves high accuracy but also excels in precision (90.88%) and recall (90.93%). This comprehensive performance underlines the prowess of VMID in the domain of fake news video detection. In contrast, the SVRPM model, with an accuracy of only 79.34%, reveals limitations in modality tampering detection, further highlighting the advantage of VMID."}, {"title": "Loss Analysis", "content": "1)\tLoss Analysis: To better understand the training dynamics and the effectiveness of the VMID method, we plot the loss curves during the training process. The loss curves provide insights into the convergence behavior of the model and highlight how well the model is learning over time. As shown in Figure 4, the loss steadily decreases over the training epochs, indicating that the model is progressively improving its performance. The sharp decline in the early stages of training suggests that the model is quickly learning the key features, while the more gradual reduction later on shows a fine-tuning phase where the model stabilizes. The plot demonstrates that VMID effectively optimizes its parameters for the task of fake news detection, further supporting the results reported in Fig4.\nMoreover, we investigated the performance of different VMID models trained with various LLM architectures on the FakeSV dataset, as presented in Table II. The results demonstrate that VMID maintains a relatively high level of performance regardless of the specific LLM used."}, {"title": "Case Studies", "content": "D. Case Studies\nIn Fig. 3, we demonstrate the effectiveness of the VMID system in classifying video content into three categories: real, fake, and debunking. The system utilizes four key inputs\u2014title, audio, keyframes, and video summary\u2014which are highly correlated and provide comprehensive cues for the inference process. Title, audio, and keyframes serve as the multimodal features of the video, while the \"Video Summary,\" obtained through image processing, offers additional contextual information. Together, these inputs enable the model to gain a more thorough understanding of the video content during the classification process. By integrating these related cues, VMID is able to accurately identify the content type of the video."}, {"title": "Related Work", "content": "VI. RELATED WORK\nEarly research on fake news detection primarily focused on single-modal information processing, such as text, images, or videos. These studies attempted to identify misinformation by analyzing user behavior and information dissemination patterns on social media platforms. For instance, (Papadopoulou et al., 2019) [27] proposed a feature-based SVM classifier that utilizes video metadata, linguistic features of titles, and the credibility of comments to identify rumors. Similarly, (Li et al., 2022) [28] and (Ran et al., 2022) [29] constructed heterogeneous graph models incorporating social media user information, which demonstrated improved performance in rumor detection tasks. Furthermore, (Medina et al. 2020) [30] used tf-idf vectors based on titles and comments, combined with co-conspiratorial relationships among comments, to enhance detection accuracy. While these single-modal approaches laid the foundation for fake news detection, they exhibit limitations in integrating multiple cues to improve applicability and comprehensiveness. In particular, in the context of short videos, single-modal methods often fail to cover all relevant information sources, making the comprehensive analysis of multimodal data crucial for improving detection accuracy and robustness. In comparison, the VMID model integrates text, video, and social context information, thereby improving accuracy and performance in fake news detection.\nB. Multimodal Fake News Detection\nWith the increasing prevalence of short video content, multimodal fake news detection has become a prominent research"}, {"title": "Applications of Large Language Models in Fake News Detection", "content": "C. Applications of Large Language Models in Fake News Detection\nLLMs have demonstrated powerful interpretative capabilities, opening new possibilities for fake news detection. Since fake news is primarily disseminated via text on social media, LLMs such as GPT-2 (Zellers et al., 2019) [34] and BERT (Singhal et al., 2020) [35] have been widely employed for both fake news generation and detection. Recently, ChatGPT has exhibited exceptional performance in understanding complex texts, showing substantial potential in the field of fake news detection. For example, (Huang et al., 2023) [36] introduced the FakeGPT framework, by using rationale-aware prompting strategies to significantly improve the detection performance of ChatGPT, this approach highlights the potential of LLMs in fake news detection\nDespite the strong performance of LLMs in text analysis, they face limitations when dealing with multimodal data such as video and images. In contrast, VMID overcomes these limitations by leveraging the strengths of LLMs in text processing while effectively integrating video and social context information, enabling the handling of more complex multimodal data. Furthermore, by utilizing deep integration of social context and video content, VMID addresses the bottlenecks of traditional LLMs in multimodal processing, further enhancing performance in fake news detection."}, {"title": "Conclusion", "content": "VII. CONCLUSION\nThis paper introduces VMID, a novel multimodal model designed to tackle the challenges of fake news detection in short videos. By integrating various modalities\u2014such as video content, metadata, and social context\u2014VMID significantly improves the accuracy of fake news detection, overcoming key limitations of existing methods that primarily rely on single-modal approaches or inadequate fusion techniques. Extensive experiments on the FakeSV dataset show that VMID outperforms current models"}]}