{"title": "HOW TO WEIGHT MULTITASK FINETUNING?\nFAST PREVIEWS VIA BAYESIAN MODEL-MERGING", "authors": ["Hugo Monz\u00f3n Maldonado", "Thomas M\u00f6llenhoff", "Nico Daheim", "Iryna Gurevych", "Mohammad Emtiyaz Khan"], "abstract": "When finetuning multiple tasks altogether, it is important to carefully weigh them\nto get a good performance, but searching for good weights can be difficult and\ncostly. Here, we propose to aid the search with fast previews to quickly get a rough\nidea of different reweighting options. We use model merging to create previews by\nsimply reusing and averaging parameters of models trained on each task separately\n(no retraining required). To improve the quality of previews, we propose a Bayesian\napproach to design new merging strategies by using more flexible posteriors. We\nvalidate our findings on vision and natural-language transformers. Our work shows\nthe benefits of model merging via Bayes to improve multitask finetuning.", "sections": [{"title": "1 INTRODUCTION", "content": "Multitask finetuning has recently gained popularity due to the success of large pretrained models, but\na careful weighting of tasks is crucial to get good performances (Liu et al., 2023; Xu et al., 2024;\nChung et al., 2024). Similarly to the traditional multi-task learning (Caruana, 1997; Ruder, 2017), the\nweighting is useful in tackling data imbalance, task interference, negative transfer, and also effects of\nvariable task difficulty (Raffel et al., 2020; Liu et al., 2023). When left unresolved, these can lead to\nissues, for instance, regarding safety (Jan et al., 2024). Weighting is also useful for pre-finetuning\nrecently used for multi-lingual transfer and continual pretraining (Aghajanyan et al., 2021; Gemma\nTeam, 2024a; Martins et al., 2024; Fujii et al., 2024).\nDespite its importance, little has been done to address task weighting for multitask finetuning. For\nlarge models, an exhaustive search over weights is out of the question, but even if we could try a few\nweighting configurations, which ones should we try? There is no guide for that. Weights are often\nchosen arbitrarily and sometimes heuristically but these are not sufficient; see, for example, Liu et al.\n(2023, Sec. 6). The weighting methods used for deep learning and pretraining can be adapted to\nsearch for good weights (Ren et al., 2018; Chen et al., 2018; Raffel et al., 2020; Groenendijk et al.,\n2021; Du et al., 2022; Yan et al., 2022; Xie et al., 2023; Thakkar et al., 2023), but a quick guidance\non reasonable search areas will still be useful to assist the search.\nIn this paper, we propose to aid the search with fast previews of performances, estimated to obtain\nweights that improve accuracy of multitask finetuning. We use model merging to create the previews\nwhere we train and store models on each task separately and reuse them later to create previews by\nsimply averaging the model parameter for a wide-range of weights (Fig. 1). Our main contribution is\na Bayesian approach to design new merging strategies that yield better previews over a wider range\nof weights. This differs fundamentally from previous works which only focus on the best performing\nweights (Don-Yehiya et al., 2023; Jiang et al., 2023; Feng et al., 2024; Stoica et al., 2024; Yang et al.,\n2024). Such strategies do not always yield good-quality previews; see Fig. 2a for an illustration.\nWe propose a Bayesian approach where more flexible posteriors yield better previews but with slightly\nhigher costs (Figs. 2b and 2c). For instance, we show that Task Arithmetic (Ilharco et al., 2023) for"}, {"title": "2 WEIGHTED MULTITASK FINETUNING", "content": "Multitask finetuning aims to finetune on multiple tasks altogether. For example, given a Large\nLanguage Model (LLM) trained for English, we may want to finetune it on multiple languages (Muen-\nnighoff et al., 2023), for instance, German, French, Chinese, Japanese, etc. Denoting the loss of each"}, {"title": null, "content": "task by $l_t(\\theta)$ for the model parameters $\\theta$, we want to finetune over a weighted loss\n$\\sum_{t=1}^T \\alpha_t l_t(\\theta)$, where $\\alpha_t > 0$ for $t = 1, 2, ..., T$.\n(1)\nWe denote the loss-weight vector by $\\alpha = (\\alpha_1, \\alpha_2,..., \\alpha_T)$ and the finetuned parameters obtained\nwith the weighting by $\\theta_\\alpha$. We will generally assume that $\\alpha_t$ sum to 1 but this is not strictly required.\nSuch multitask finetuning has recently become important for LLM alignment and usability. For\nexample, it is used for improving instruction-following abilities (Chung et al., 2024; Ouyang et al.,\n2022), different kinds of safety tuning (Gemma Team, 2024a), combining coding tasks (Liu et al.,\n2023), and mixing coding and math skills into LLMs, which is useful even when they are designed\nfor other tasks like machine translation (Martins et al., 2024).\nIn practice, it is important to choose $\\alpha$ carefully for reasons that are true for any multi-task learning\nproblem (Caruana, 1997; Ruder, 2017). For instance, one issue is due to data-imbalance: different\ntasks may contain different types of information and some of higher quality than others. There is\nalso task interference, for example, a model that does math well, may not necessarily be the best at\nlanguages. Additionally, learning some tasks might hurt the performances in the other tasks, and\nthen there is variability in task difficulty: some tasks are harder to learn and we do not want those to\nimpact the tasks that are relatively easier to learn.\nThe effects of these issues are often felt in practice. For example, adding too much safety data\ncan make the model more conservative and reduce its usefulness (Bianchi et al., 2024); too much\ninstruction finetuning can undo safety alignment and open new vulnerabilities (Qi et al., 2024; Jan\net al., 2024). Such problems can be avoided by careful task weighting. Weighting is also useful\nduring pre-finetuning where we try to balance multiple tasks differently during the last, say, 10% of\npretraining (Aghajanyan et al., 2021; Gemma Team, 2024a; Martins et al., 2024; Fujii et al., 2024).\nDespite its importance, not much work has been done to find good ways to set the weights. Decades\nof work exist for multitask learning but multitask finetuning is a relatively new area and is still\nunder-explored. Similarly to multi-task learning, an exhaustive search over the whole $\\alpha$-space is not\nfeasible when T is large. With little guidance, arbitrary values are tried to get an idea, for example,\nFujii et al. (2024) try only two values of $\\alpha$ for continual pretraining. Sometimes heuristics are\nused and meta-learning approaches are also adopted, but the results are not always satisfactory, for\nexample, see Liu et al. (2023, Sec. 6) who report such a result for LLMs trained for code generation.\nOur goal in this paper is to provide a fast (and cheap) method to assist the search of good $\\alpha$\nvalues. Such a guiding tool is useful to restrict the search to a few values and also to warm-start\nthe optimization process. For this, we need a fast but accurate approach to quickly estimate the\nperformance of $\\theta_\\alpha$ for a wide-range of $\\alpha$ values. Model merging is a useful tool for this, but the\nchoice of merging strategy matters to get a high quality preview. We show that simple merging\nmethods are not satisfactory because they can be quite inaccurate and may only yield good estimates\nfor a small region in the $\\alpha$ space (see Fig. 2a). Our main technical contribution is to address this\nwith a Bayesian approach to expand the region by designing a more accurate merging strategy. The\nquality is improved at the expense of cost but the approach still remains fast enough to be employed\nin practice. Existing methods on model merging in this space only focus on the best performing\nweights (Don-Yehiya et al., 2023; Jiang et al., 2023; Feng et al., 2024; Stoica et al., 2024; Yang et al.,\n2024). Our work instead aims to design merging strategies that work for a wide range of $\\alpha$ values."}, {"title": "3 FAST PREVIEWS VIA BAYESIAN MODEL-MERGING", "content": "Our goal is to create fast previews, that is, we want to estimate $\\theta_\\alpha$ obtained by finetuning over\n$\\sum_t \\alpha_t l_t$ for a wide-variety of $\\alpha$ values. The previews are useful to choose the $\\alpha$ that give the most\naccurate results for joint multitask finetuning over all tasks. Our approach does this in three steps:\n1. Finetune T models (denoted by $\\theta_t$) each separately over their own task $l_t(\\theta)$.\n2. Use Bayesian learning to build surrogate $l_t \\approx l_t$ by using $\\theta_t$.\n3. Create previews by finetuning over $\\sum_t \\alpha_t \\hat{l_t}$ for many $\\alpha$ values.\nIn step 2, we design accurate $\\hat{l_t}$ by using exponential-family posteriors. Such posterior always has a\nclosed-form merging formula which enables step 3. We start by describing model merging."}, {"title": "3.1 MODEL MERGING AS A WEIGHTED-SURROGATE MINIMIZATION", "content": "Model merging uses simple parameter averaging but it can also be seen as minimization of a weighted\nsum of surrogates. For example, consider the simple averaging (SA) (Wortsman et al., 2022),\n$\\begin{aligned}\n\\hat{\\theta}_\\alpha^{SA} &= \\sum_{t=1}^T \\alpha_t \\theta_t \\\n&= \\arg \\min_\\theta \\underbrace{||\\gamma \\theta||^2}_{R_\\theta(\\theta)} + \\sum_{t=1}^T \\alpha_t \\underbrace{\\Big( l_t(\\theta_t) + \\frac{1}{2} ||\\theta - \\theta_t||^2 \\Big)}_{=\\hat{l}_t(\\theta)},\n\\end{aligned}$\n(2)\nwhere the surrogate $\\hat{l}_t$ is a quadratic function. The term $l_t (\\theta_t)$ is a constant and can be ignored. The\n$R_\\theta$ is a regularizer with $ \\gamma = 1 - \\sum_t \\alpha_t$, which disappears if the $\\alpha_t$ sum to 1. The equality can be\nverified by setting the derivative to zero. Other merging techniques can also be interpreted this way.\nFor example, Task Arithmetic (TA) (Ilharco et al., 2023) uses finetunes of an LLM with parameters\n$\\theta_{LLM}$ and can be seen as the following weighted-surrogate minimization with a regularizer,\n$\\hat{\\theta}_\\alpha^{TA} = \\theta_{LLM} + \\sum_{t=1}^T \\alpha_t (\\theta_t - \\theta_{LLM}) = \\arg \\min_\\theta \\underbrace{||\\theta - \\theta_{LLM}||^2}_{R_\\theta(\\theta)} + \\sum_{t=1}^T \\alpha_t ||\\theta - \\theta_t||^2$.\n(3)\nHere, we removed the constant $\\hat{l_t}(\\theta_t)$ for clarity. In general, many model merging methods can be\ninterpreted as weighted-surrogate minimization, including Wortsman et al. (2022); Matena & Raffel\n(2022); Jin et al. (2023); Ortiz-Jimenez et al. (2023); Daheim et al. (2024).\nThe interpretation highlights a major source of error when estimating performance over a wide range\nof $\\alpha$ values: the accuracy of the surrogates. The surrogates used above can be seen as a simplistic\nTaylor approximation where we assume $\\nabla l_t(\\theta_t)$ to be zero (due to local optimality) and the Hessian\n$\\nabla^2 l_t(\\theta_t)$ is set to identity,\n$l_t(\\theta) \\approx l_t(\\theta_t) + \\nabla l_t(\\theta_t) (\\theta - \\theta_t) + \\frac{1}{2} (\\theta - \\theta_t)^T \\nabla^2 l_t(\\theta_t) (\\theta - \\theta_t)$\n$\\approx l_t(\\theta_t) + \\frac{1}{2} ||\\theta - \\theta_t||^2$.\n(4)\nThe surrogates $\\hat{l_t}$ are tight at only one point and their inaccuracy increases as we move away from it.\nModel merging can be seen as using $\\sum_t \\alpha_t \\hat{l_t}$ (along with the regularizer $R_\\theta$) as a proxy to estimate\nthe results of finetuning the original $\\sum_t \\alpha_t l_t$. However, when using a wide range of $\\alpha$ values, this\ninaccuracy can lead to poor estimates in some regions in the $\\alpha$ space. Essentially, the errors in\ndifferent $\\theta$-regions become relevant and ultimately lead to a poor estimate.\nIdeally, we would like the surrogates to be designed such that they are not only locally accurate but\nalso in a wider region. We expect such surrogates to be globally accurate and beyond the point used\nin their definition, but how can we design them? Is there a general recipe to do so? We will now\npropose a Bayesian approach to answer these questions."}, {"title": "3.2 A BAYESIAN APPROACH TO MODEL MERGING", "content": "The surrogate minimization approach can be seen as a special case of distributed Bayesian computa-\ntion where there is a natural way to merge information distributed in different locations. We will first\ndescribe this approach and then connect it to surrogate minimization to design better surrogates.\nConsider a multitask setup in a Bayesian model where there are t tasks each using a likelihood\n$p(D_t|\\theta)$ over data $D_t$ and a common prior $p_0(\\theta)$. For this case, there is a closed-form expression to\nquickly get the weighted multitask posterior. We first compute T posteriors $p_t(\\theta) \\propto p(D_t|\\theta)p_0(\\theta)$,\nseparately over their own likelihoods. The weighted posterior can be obtained by reusing the\nposterior $p_t$ by using the fact that the likelihood can be written as the ratio of posterior and prior\n$p(D_t|\\theta) = p_t(\\theta) / p_0(\\theta)$. This is shown below,\n$p_\\alpha(\\theta) \\propto p_0(\\theta)^{\\gamma} \\prod_{t=1}^T p(D_t|\\theta)^{\\alpha_t} \\propto p_0(\\theta)^{\\gamma} \\prod_{t=1}^T \\frac{p_t(\\theta)^{\\alpha_t}}{p_0(\\theta)^{\\alpha_t}}.$\n(5)\nThe $\\gamma = 1 - \\sum_{t} \\alpha_t$ is the same scalar used in front of the regularizer in Eqs. 2 and 3. Such posterior\nmerging is a popular method in Bayesian literature, for example, see Bayesian committee machine\n(Tresp, 2000) or Bayesian data fusion (Mutambara, 1998; Durrant-Whyte & Stevens, 2001)."}, {"title": null, "content": "In fact, by choosing the Bayesian model appropriately, we can even exactly recover the solution for\nthe weighted multitask problems. For example, suppose we want to recover the minimizer $\\theta_\\alpha$ by\nminimizing the objective $\\gamma R_0 + \\sum_t \\alpha_t l_t$, then we can choose\n$p(D_t|\\theta) \\propto \\exp(-l_t(\\theta)), \\quad p_0(\\theta) \\propto \\exp(-R_0(\\theta))$.\nThese choices are valid within the generalized-Bayesian framework (Zhang, 1999; Catoni, 2007;\nBissiri et al., 2016). With these, the minimizer $\\theta_\\alpha$ is simply the maximum-a-posterior (MAP) solution\nof the merged posterior $p_\\alpha$, that is,\n$\\begin{aligned}\n\\theta_\\alpha = \\arg \\max_\\theta \\log p_\\alpha(\\theta) &= \\arg \\max_\\theta \\log p_0(\\theta)^{\\gamma} + \\sum_{t=1}^T \\alpha_t \\log p_t(\\theta).\n\\end{aligned}$\n=-R_0(\\theta) \\qquad \\qquad =-l_t(\\theta) \n(6)\nHere, we assumed that all single-task problems use the same prior but the same principle can be used\nto extend it to the case where one uses different priors. Comparing this objective to Eqs. 2 and 3, we\nsee that the Bayesian framework suggests to use the surrogate $\\hat{l_t}(\\theta) = - \\log p_t(\\theta)$ and regularizer\n$R_0(\\theta) = -\\log p_0(\\theta)$. These surrogates are perfect because using them recovers the exact solution,\nbut computing them exactly is also difficult. Our key idea is to use approximate Bayesian learning,\nspecifically variational learning, to obtain posterior approximations and use them as surrogates."}, {"title": "3.3 VARIATIONAL BAYESIAN LEARNING TO BUILD EXPONENTIAL-FAMILY SURROGATES", "content": "To build surrogates $\\hat{l_t}$ for each task, we propose to use variational learning which finds a posterior\napproximation $q_t(\\theta)$ to the exact posterior $p_t(\\theta)$,\n$q_t(\\theta) = \\arg \\min_{q \\in \\mathcal{Q}} \\mathbb{E}_q [l_t (\\theta)] + D_{KL} [q(\\theta) \\mid\\mid p_0 (\\theta)].$\n(7)\nWe choose $\\mathcal{Q}$ to be the set of exponential-family approximations or their mixtures, for example,\nGaussian distribution. For such posteriors, we have good optimizers that perform well at large scale\n(Khan & Rue, 2023). For instance, for Gaussians, the above problem can be optimized by using\nAdam-like optimizers (Shen et al., 2024). Even Adam can be seen as solving this problem (Khan\net al., 2018). We can just use such optimizers to compute the posterior $q_t$.\nMotivated by Eq. 6, we propose to use $q_t (\\theta)$ to build the surrogate and estimate $\\theta_\\alpha$, as shown below:\n$\\hat{l_t}(\\theta) = - \\log q_t (\\theta) \\quad \\longrightarrow \\quad \\theta_\\alpha = \\arg \\max_\\theta -R_0 (\\theta) - \\sum_{t=1}^T \\alpha_t \\hat{l_t}(\\theta)$.\n(8)\nAs for the prior $R_0$, we will use a Gaussian prior, although other choices are also possible. These\nchoices can recover existing model-merging strategies. For example, we recover Eq. 2 up to a constant\nby choosing $q_t$ to be the Gaussian approximation shown below:\n$q_t(\\theta) = \\mathcal{N}(\\theta|\\theta, I) \\quad \\longrightarrow \\quad \\hat{l_t}(\\theta) = \\frac{1}{2} ||\\theta - \\theta_t||^2 + const$.\n(9)\nThe posterior $q_t (\\theta)$ is a Laplace approximation obtained as a special case of variational learning (Khan\n& Rue, 2023, Eq. 25). The prior can be chosen to be isotropic Gaussian: $p_0(\\theta) = \\mathcal{N}(\\theta|0, I)$. Simi-\nlarly, Task Arithmetic can be derived just by simply changing the prior to $p_0(\\theta) = \\mathcal{N}(\\theta | \\theta_{LLM}, I)$.\nNext, Hessian-Weighted merging methods are obtained by using a full-Gaussian posterior, again\nwith the Laplace approximation $q_t (\\theta) = \\mathcal{N}(\\theta | \\theta_t, H_t^{-1})$ using Hessian $H_t$. With this, we get the\nsquared Mahalanobis distance $\\hat{l_t}(\\theta) = (\\theta - \\theta_t)^T H_t (\\theta - \\theta_t)$ as the surrogate, which yields a\nHessian-Weighted merging (detailed derivation is included in App. A.1),\n$\\hat{H}_\\alpha^{hess} = (\\sum_t \\alpha_t H_t)^{-1} \\sum_t \\alpha_t H_t \\theta_t.$\n(10)\nHere, we use $p_0(\\theta) = \\mathcal{N}(\\theta|0, I)$ and $\\sum_t \\alpha_t = 1 - \\gamma$. Given the prior $p_0(\\theta) = \\mathcal{N}(\\theta | \\theta_{LLM}, H_0^{-1})$,\nthe posterior corresponding to the Laplace approximation takes the following form $q_t (\\theta) =$\n$\\mathcal{N}(\\theta|\\theta_t, (H_t + H_0)^{-1})$, which leads to the surrogates $\\hat{l_t}(\\theta) = (\\theta - \\theta_t)^T(H_t + H_0)(\\theta - \\theta_t)$,\nrecovering the method proposed by Daheim et al. (2024). A derivation is included in App. A.1."}, {"title": "3.4 IMPROVED MERGING VIA MIXTURES OF EXPONENTIAL-FAMILIES", "content": "Here, we extend to mixture of exponential-family distributions which provide more expressive\nposteriors and therefore even more accurate surrogates. For simplicity, we assume that $\\sum_t \\alpha_t = 1$,\nso there is no regularizer. While mode finding for mixtures is still tractable, it requires an iterative\nexpectation-maximization (EM) procedure which should still be cheap if it converges within a few\nsteps. We assume that the k'th mixture component is an EF with natural parameter $\\Lambda_{tk}$. Each\ncomponent is weighted by $\\pi_k > 0$ and $\\sum_k \\pi_k = 1$. Then, the posterior and surrogate take the\nfollowing form:\n$\\begin{aligned}\nq_t &\\propto \\sum_{k=1}^K \\pi_k e^{\\Lambda_{tk} T(\\theta)} \\qquad \\longrightarrow \\qquad \\hat{l_t}(\\theta) = - \\log \\sum_{k=1}^K \\pi_k e^{\\Lambda_{tk} T(\\theta)}\n\\\\ \\qquad \\qquad \\propto \\sum_{k=1}^K \\pi_{tk} (\\theta) \\qquad \\qquad \\qquad k=1\n\\end{aligned}$\n(11)\nClearly, the surrogate is much more expressive than the quadratic surrogates used in model merging.\nDespite the non-concavity of the objective, we can maximize it using an iterative Expectation-\nMaximization (EM) approach where each step has a closed-form solution. A detailed derivation is\nin App. A.3. As a special case, consider mixture-of-Gaussians (MoG) posterior where the updates\ntake the following form similarly to Eq. 10:\n$\\begin{aligned}\n\\hat{H}_\\alpha^{(i+1)} &\\longleftarrow \\Big(\\sum_{t,k} \\hat{\\pi}_{tk}^{(i)} \\alpha_t H_{tk}\\Big)^{-1} \\sum_{t,k} \\hat{\\pi}_{tk}^{(i)} \\alpha_t H_{tk} \\theta_{tk}, \\quad where \\quad \\hat{H}_\\alpha^{(i)} = \\sum_{t,k} \\alpha_t \\hat{\\pi}_{tk}^{(i)} H_{tk}\n(12)\n\\end{aligned}$\nThe main difference is that each component is now weighted by $\\hat{\\pi}_{tk} \\propto \\alpha_t \\pi_k \\mathcal{N}(\\theta_\\alpha^{(i)} |\\theta_{tk}, H_{tk}^{-1})$, nor-\nmalized over k. This update generalizes the fixed-point algorithm of Carreira-Perpi\u00f1\u00e1n (2000,\nSection 5) which was proposed to find the modes of Gaussian mixtures."}, {"title": "3.5 PRACTICAL ALGORITHMS FOR FAST MULTITASK FINETUNING PREVIEWS", "content": "Our methods to generate previews follow the 3-step procedure described in Sec. 3. Below we list\nthree versions where different algorithms are used to generate $q_t (\\theta)$."}, {"title": "4 EXPERIMENTS & RESULTS", "content": "We compare multitask finetuning and model merging on image classification using ResNets (Sec. 4.1)\nand vision transformers (Sec. 4.2), text classification with masked language models (Sec. 4.3), and\nmachine translation using LLMs (Sec. 4.4). In App. C.1 we show results for multitask learning on"}, {"title": "4.1 IMAGE CLASSIFICATION ON CIFAR-10", "content": "Next, we move to a neural network finetuning. We first pretrain ResNet-20 (He et al., 2016) with 260k\nparameters on a subset of CIFAR-10 and then finetune this checkpoint on the remaining examples.\nThe finetuning tasks are: (1) airplane, car, ship, truck; (2) cat, dog; (3) deer, dog, frog, horse. We\ncompare Hessian-Weighted (IVON-HESS) and Mixture-Weighted (MULTIIVON-HESS) to simple\naveraging and Exact Solution (Joint training). Hyperparameters are in App. B.3.\nResults are shown in Fig. 3 and again show that better posterior approximations yield better previews.\nFor example, Simple Merging misses that performance is still good in any of the corners, especially\nthe top and bottom right one. Hessian-Weighted merging misses the best-performing region and\nwould suggest exploring a region slightly above it. Mixture-Weighted previews are much better and\nimprove with the number of components. Interestingly, the best-performing region of this approach\nmoves further down and right, that is, closer to that of the joint training solution."}, {"title": "4.2 VISION TRANSFORMERS", "content": "Next, we experiment with multitask finetuning ViT-B/32 models (Dosovitskiy et al., 2021) based on\nCLIP (Radford et al., 2021) for image classification. First, we use GTSRB (Houben et al., 2013),\nRESISC45 (Cheng et al., 2017) and SVHN (Netzer et al., 2011); then we use EuroSAT (Helber et al.,\n2019), Stanford Cars (Krause et al., 2013) and SUN397 (Xiao et al., 2010). We compare Simple\nMerging to Hessian-Weighted (ADAMW-SG) and provide. Further details on training and evaluation\nin App. B.4. The joint solution uses a grid with spacing 0.05 to explore the possible sets of $\\alpha$.\nThe results are shown in Fig. 4. In both cases, the joint solution is similar and shows that almost\nall weightings that are not directly at the borders (where one of the tasks gets a very small weight)\nhave good performance. While for EuroSAT, Cars, SUN397 there is a smaller region with accuracy\nover 84 points in accuracy, any weightings in the large contour of above 80 points will still be good.\nIdeally this should be reflected by a preview from merging. Hessian-based merging shows the flat\ntriangular shape of the joint solution better than the simple method. Training times highlight the"}, {"title": "4.3 MASKED LANGUAGE MODELS", "content": "In this section, we show results when multitask finetuning masked language models for text classi-\nfication. We follow Daheim et al. (2024) and train RoBERTa (Liu et al., 2019) first on the IMDB\nsentiment classification task Maas et al. (2011). Then, we finetune on Rotten Tomatoes (RT) (Pang &\nLee, 2005), SST-2 (Socher et al., 2013), and Yelp (Zhang et al., 2015), and merge the resulting models.\nWe use two settings: the first merges all combinations of two of the three finetuned models; the second\nmerges all three. Due to heavy compute requirements, we run joint training with a coarser grid than\nmodel merging in the latter. We compare Simple Merging and Hessian-Weighted (ADAMW-SG).\nResults for merging two models are shown in Fig. 5 where we see that even simple merging can\noften produce good previews but fails for specific weightings. For example, on SST2 and RT the\nbest-performing factors for Simple Merging ($\\alpha_1 = 0.9$) are the worst-performing in the joint solution.\nUsing a diagonal Gaussian instead of an isotropic one shows a more similar trend to this joint solution."}, {"title": "4.4 MACHINE TRANSLATION WITH FINETUNED LLMS", "content": "Next, we show that our methods apply to LLMs with more than one billion parameters, also if they\nare finetuned using parameter-efficient finetuning strategies such as LoRA. In particular, we merge\ntwo GEMMA-2B-it (Gemma Team, 2024b) models finetuned on IWSLT2017 (Cettolo et al., 2017)\nde-en and fr-en, respectively, and compare them to training jointly on both language pairs. We use\nIVON-HESS for Hessian-Weighted merging. Details about the experimental set-up are in App. B.6.\nResults are shown in Fig. 7. There, we find that Simple Merging does not always match the shape of\nthe joint training solution, especially around $\\alpha = 0.4$. Using Hessian-Weighted merging improves\nthis. Overall, this shows that the our method can also scale to larger models and datasets, even if\nonly a small subset of the parameters is adapted during finetuning. One run of multitask finetuning\nfor a specific weight takes around 17 hours while merging takes just 1 minute (plus 8-9 hours for\nfinetuning on each task separately)."}, {"title": "5 CONCLUSION", "content": "Multitask finetuning is a crucial ingredient in many neural network training recipes but good weight-\nings between tasks are hard and expensive to find. Here, we propose to aid the search for such\nweightings with previews obtained from model merging, where single task models can be reused\nfor many weight combinations. We show that model merging strategies can be derived using a\nBayesian framework by defining suitable surrogate losses to the multitask objective for exponential-\nfamily-based distributions. We use this to outline various preview and merging strategies, including a\nnew mixture-based algorithm for improved model merging. Along various experiments including\nimage classification with Vision Transformers and machine translation with LLMs we show that\nmodel merging can effectively be used to preview multitask finetuning weightings. Flexible model"}, {"title": "A.1 DERIVATION OF HESSIAN-WEIGHTED MERGING", "content": "First", "problem": "n$\\theta_t = \\arg \\min l_t(\\theta) + \\frac{1"}, {"surrogates": "n$R_0(\\theta) = \\frac{1"}, {}]}