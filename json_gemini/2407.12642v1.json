{"title": "Zero-shot Text-guided Infinite Image Synthesis with LLM guidance", "authors": ["Soyeong Kwon", "Taegyeong Lee", "Taehwan Kim"], "abstract": "Text-guided image editing and generation methods have diverse real-world applications. However, text-guided infinite image synthesis faces several challenges. First, there is a lack of text-image paired datasets with high-resolution and contextual diversity. Second, expanding images based on text requires global coherence and rich local context understanding. Previous studies have mainly focused on limited categories, such as natural landscapes, and also required to train on high-resolution images with paired text. To address these challenges, we propose a novel approach utilizing Large Language Models (LLMs) for both global coherence and local context understanding, without any high-resolution text-image paired training dataset. We train the diffusion model to expand an image conditioned on global and local captions generated from the LLM and visual feature. At the inference stage, given an image and a global caption, we use the LLM to generate a next local caption to expand the input image. Then, we expand the image using the global caption, generated local caption and the visual feature to consider global consistency and spatial local context. In experiments, our model outperforms the baselines both quantitatively and qualitatively. Furthermore, our model demonstrates the capability of text-guided arbitrary-sized image generation in zero-shot manner with LLM guidance.", "sections": [{"title": "1 Introduction", "content": "Recently the field of image generation has witnessed a significant advancement in synthesizing high-resolution images from text inputs. However, the existing studies [6,13,14,19] face difficulties in generating arbitrary-size image from text with diverse context because of the following challenges. Firstly, there is a lack of high-resolution text-image paired datasets with diverse contexts. Several high-resolution images [24] may not include rich context since most of them are online shopping product photos or individual portraits. Secondly, it is not just about repetitive expansion; it is essential to expand image depicting rich content based on given text description, while maintaining visual consistency [14]. Most prior"}, {"title": "2 Related Work", "content": "Image Inpainting. Text-guided image inpainting, which involves filling in a portion of an image based on input text, is closely related to text-guided image outpainting [4]. Existing image inpainting methods [2, 5, 17, 18, 22, 29] include models based on GANs and diffusion-based methods. Recently, various works [2,8,18,22] have focused on enhancing inpainting capabilities across general domains with diffusion models. Stable Diffusion Inpainting [22], Blended-Latent Diffusion [2] and PowerPaint [31] involve taking an image and a mask as input and then filling in the image based on the text. These studies effectively edit the masked portions of given images from text, understanding the content well.\nImage Outpainting. There are various studies [4, 7, 11, 14, 25, 27] aimed at infinitely expanding images. InfinityGAN [14], a GAN-based model, proposes a method for generating arbitrarily sized images unconditionally. This approach is trained on landscape image dataset aiming to capture both local and global consistency while generate realistic arbitrarily sized images without repetitive patterns. Additionally, InOut [4], which uses GAN inversion for image outpainting, avoids the need of sequential outpainting. While previous models [4, 12-14] have attempted to address the challenging task of image outpainting, the lack of high-resolution text-image paired dataset still leads these methods to focus on limited categories, such as natural landscapes.\nText-guided Image Outpainting. The task of arbitrarily extending images from text is more challenging than unconditional image outpainting due to the scarcity of datasets and the difficulty of maintaining global and local consistency. Nuwa-Infinity [13] successfully performs text-guided image outpainting in an autoregressive manner. However, due to the lack of high-resolution datasets containing rich content, Nuwa-Infinity, like previous studies [4, 12, 14], performs text-guided image outpainting on limited datasets [4,30] such as nature landscapes. To the best of our knowledge, we are the first to arbitrarily expand images from general text using LLM and diffusion model in a zero-shot manner."}, {"title": "3 Method", "content": "In the training stage, we train our model conditioned on a global caption, local caption, and visual features. In the inference stage, we expand the given image conditioned on the global caption, generated local caption and the visual feature. Through this approach, our model is able to perform the text-guided image outpainting task without high-resolution text-image paired datasets."}, {"title": "3.1 Global Caption Generation for Training", "content": "To train the model without a high-resolution text-image paired dataset, we generate imaginary global captions describing the expanded image based on the local captions using the LLM in training step. We consider a 512\u00d7512 resolution image as a local image, and an annotated caption of the image as a local caption. We generate a global caption that depicts diverse contexts from the annotated caption by leveraging the LLM. To generate a global caption, we follow two steps. Firstly, using an annotated caption as a local caption, we create imaginary local captions that describe the surroundings of the given image by using the LLM. As seen in Figure 1, in the stage (a), we input an annotated caption, \"A boy and a girl playing on the beach.\", to the LLM with the instruction, \u201cImagine caption for what happen outside of these caption without sound.\". Then the LLM generates several local captions following the content of the given caption, such as \"A loving couple meanders along the sandy shores of the beach, basking in the serene\""}, {"title": "3.2 Training Pipeline", "content": "To expand images from general text, we fine-tune a pre-trained Stable Diffusion model [22]. As shown in Figure 3, first, we take local masked images M\u2081, each masked on the top, bottom, left, and right.\nTo maintain spatial information and global visual consistency of the images generated thus far, we input a generated global image Gi to the CLIP [20] vision encoder to extract visual feature E. Since there is no high-resolution image available in the training step, we use an unmasked area of the local masked image Mi as the generated global image Gi. Also, as shown in Figure 2 and Equation 1, we concatenate the embeddings Eg of global caption Pg with embeddings El of local captions Pr. Then we extract the fused textual feature by compressing the concatenated vector through a Multi-Layer Perceptron (MLP) composed of two linear layers. As we fine-tune our model conditioned on the compressed textual feature, our model can reflect both global and local contexts when generating images."}, {"title": "3.3 Inference Pipeline", "content": "We perform inference as shown in Figure 5. First, a local image and a global caption are inputted. We then apply a mask to the image in the direction of the desired expansion to expand this image. And then, we generate an imaginary local caption with the LLM to fill in the local masked image. Figure 4 illustrates the process of generating an imaginary local caption. We input a local image and the instruction \u201cCreate a short sentence outside of the given image to expand this image to the left.\" into the LLM to generate the local caption. By providing the expanding direction with the instruction, the LLM can effectively imagine the local caption which describes the scene surrounding the given local image.\nNext, we shift the local masked image autoregressively. To expand a local image that incorporates the details of the local caption while considering the global semantic context, we use both the global and local captions as text condition. After extracting the embeddings of these captions, we concatenate the vectors. Then we input the vector into the MLP layer. By compressing the vector, we extract the textual feature from global and local captions, Et (77 \u00d7 768). Additionally, to maintain visual consistency and understand the spatial information of the previously generated image, we use the CLIP image embedding of the generated global image as the visual feature, E\u2081 (77 \u00d7 768). Then we create a conditioning vector, W (154 \u00d7 768) by concatenating both textual and visual features. Our model expands an image with each step conditioning on the vector, W, with an expanded cross-attention dimension (154 \u00d7 768). This enables us to generate an output image by considering on the textual and visual features. Also we can arbitrarily extend the input local image in an autoregressive manner while maintaining global coherence and local consistency.\""}, {"title": "4 Experiment", "content": "4.1 Experimental Setup\nImplementation detail. We use 100,000 text-image pairs from the MS-COCO [15] dataset. We construct global captions on MS-COCO [15] using GPT 3.5 [3] following the Section 3.1. We fine-tune Stable Diffusion 1.5 [22] for 25 epochs with a batch size of 20, using two NVIDIA A100 GPUs. We use LLAVA 1.6 [16] to generate the local captions during the inference. We provide the training dataset examples to the supplementary material.\nBaselines. Since we focus on text-guided infinite image synthesis in zero-shot manner, it is challenging to select the baseline models. For example, previous models [4, 12-14], such as InfinityGAN [14] performs the unconditional image outpainting and NuWA-Infinity [13] is mainly focused on the limited categories such as natural landscapes. Also as NuWA-Infinity [13] require high resolution training dataset and do not provide the official code, we cannot compare with it. Therefore, we compare our model with the text-guided inpainting models such as SD Inpainting model [22], Blended Latent Diffusion [2] and PowerPaint [31] which can be applied to text-guided image outpainting, and for which pre-trained models are available. We use only global caption as the text condition for the baselines with the same masking setting as ours.\nEvaluation Datasets. To evaluate the text-guided image outpainting performance, we utilize image captioning datasets, MS-COCO [15], Flickr 8k [10] and"}, {"title": "4.2 Quantitative Result", "content": "To evaluate the performance of our model, we compare our model with SD Inpainting model (SD Inp) [22], Blended Latent Diffusion (BLD) [2] and Power-Paint (PP) [31] on three datasets [10, 15, 21].\nImage Extension \u00d74 experiment. We expand the image four times, and the resolution of the expanded image is 1536\u00d7512 or 512\u00d71536. As shown in Table 1, our model outperforms the baselines [2, 22,31] in terms of IS [23] and CLIPSIM [20]. Since our model expands an image conditioned on a local caption generated by LLM, which represents the details within a global caption, the expanded image is faithful to the global caption while preserving its contextual coherence. However, the baseline models repetitively expand images and do not contain the rich context beyond the global caption.\nImage Extension \u00d78 experiment. We expand the image eight times, and the resolution of the expanded image is 2560\u00d7512 or 512\u00d72560. As shown in Table 1, our model shows better performance than the baseline models in IS [23] and CLIPSIM [20]. These results show that our model can maintain visual quality and global coherence while generating images with a more diverse context as it extends more images."}, {"title": "4.3 Qualitative Analysis", "content": "We qualitatively analyze the generated results of our model and baselines, specifically focusing on the aspects, \"text matching\", \"image quality\", and \"global coherence\". Also we provide more generated samples with larger resolutions in the supplementary material."}, {"title": "4.5 Ablation Study", "content": "To explore the impact of the proposed components, we conduct an ablation study with different models. Also we provide the human evaluation results in the supplementary material, which show that our model is preferred than ablated models. All experimental settings are the same as in Section 4.1 and Section 4.4.\nEffect of the LLM guidance and CLIP visual feature. To see the effect of the LLM guidance and CLIP visual feature, we compare our model with the w/o all model which generates an image with only a global caption. In Figure 7, the w/o all model simply reflects the keywords of the global caption, while failing to maintain global consistency and diverse context. This indicates that the w/o all model expands an image repetitively that depicts the same content without considering the overall structure. As shown in Table 3, our model outperforms the w/o all model in both IS [23] and CLIPSIM [20]. This indicates that our model can expand image better than the w/o all model in aspect of image quality and text faithfulness."}, {"title": "Effect of the local caption with LLM guidance", "content": "We compare our model with the w/o LLM model which generates an image with a global caption and the CLIP visual feature. In Figure 7, the w/o LLM model fails to incorporate content beyond the global caption since it is conditioned only on the global caption as a textual condition. Also, the extended image does not appear as a single image but rather as a collage of the images. For example, in Figure 7 (d), our model expands the image by imagining the full view of the \u201cbaseball stadium with spectators"}, {"title": "Effect of the CLIP visual feature", "content": "We compare our model with the w/o CLIP model which generates an image with a global caption and a local caption generated with the LLM. In Figure 7, comparing with our model, the w/o CLIP model often generates images with slightly lower image quality and global consistency, as it does not consider the visual feature of the overall expanded image. Figure 7 shows that the w/o CLIP model is unable to enhance the image while maintaining visual coherence. In Table 3, our model outperforms the w/o CLIP model in terms of the IS. This demonstrates that the CLIP visual feature helps the model to generate an image with better image quality. Also for CLIPSIM [20], even though the w/o CLIP model is conditioned on both global and local captions, our model generates an image that closely matches with the global caption."}, {"title": "Effect of the global caption", "content": "We compare our model with the w/o GC model which generates an image with a local caption generated with the LLM and CLIP visual feature. Figure 7 shows that, in comparison to our model, the w/o GC model generates images that do not maintain global consistency well. Also, since it does not consider the global context of the expanded image, the expanded images fail to maintain overall harmony. In Table 3, our model outperforms the w/o GC model in terms of IS and CLIPSIM. This demonstrates that the our model can generate images that maintain global consistency by effectively reflecting the global caption."}, {"title": "Effect of mask ratio", "content": "To explore various masking behaviors, we train our model on the dataset with a masking ratio of 3:1. As shown in Figure 8 (c), we found that although we can generate more content at once, it becomes more challenging to maintain global consistency when the provided(unmasked) input content gets smaller. This result demonstrates that our mask ratio is effective."}, {"title": "4.6 Exploring Other Model Architectures", "content": "We explore the effect of our model architecture by comparing with two alternative model architectures: 1) In the all-in MLP model, we compress the global caption, local caption and CLIP visual feature by the MLP layer, as a compressed vector (77\u00d7768) then the model generates an image conditioned on the vector. 2) In the all-in cross attention model, we concatenate the global caption, local caption and CLIP visual feature (231\u00d7768) then the model generates an image conditioned on the concatenated vector through the expanded U-Net.\nIn Figure 8 (a), the all-in MLP model produces images with blurred edges and indistinct objects, likely due to difficulty in representing both textual and visual features. Figure 8 (b) shows the all-in cross-attention model generating repetitive \"berry\" images, possibly influenced by textual content. In Figure 8 (c), our model achieves semantic and visual consistency with both global and local captions.\nIn Table 5, our model performs better than the all-in MLP and all-in cross-attention model in both IS [23] and CLIPSIM [20]. This shows that our model architecture can reflect the content of text and visual features effectively."}, {"title": "5 Conclusion and Limitation", "content": "In this work, we propose a novel zero-shot text-guided image outpainting model by addressing the two main challenges: 1) the lack of high-resolution text-image paired datasets that have rich context; 2) preserving global coherence and understanding the context. In contrast to prior research, which generates images in limited categories, we leverage the LLMs to imagine the outside scene of the given image. During inference, we utilize LLMs to generate imaginary prompts to expand images. This allows us to expand the image to arbitrary size with diverse contexts. Additionally, by conditioning on the visual context, we can maintain global consistency and spatial local context. The experimental results demonstrate that our model can extend images arbitrarily in a zero-shot manner, and it offers promising opportunities for text-guided image outpainting approaches.\nOur model has a limitation as it relies on a pre-trained text-to-image model, but the generated images can contain rich visual contents. For future work, we will expand to image outpainting through stories or other modalities, such as sound."}], "equations": ["Et = MLP(Eg, E1), W = Concat(Ei, Et)"]}