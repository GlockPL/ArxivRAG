{"title": "From System 1 to System 2: A Survey of Reasoning Large Language Models", "authors": ["Zhong-Zhi Li", "Duzhen Zhang", "Ming-Liang Zhang", "Jiaxin Zhang", "Zengyan Liu", "Yuxuan Yao", "Haotian Xu", "Junhao Zheng", "Pei-Jie Wang", "Xiuyi Chen", "Yingying Zhang", "Fei Yin", "Jiahua Dong", "Zhijiang Guo", "Le Song", "Cheng-Lin Liu"], "abstract": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAl's 01/03 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time GitHub Repository to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.", "sections": [{"title": "INTRODUCTION", "content": "A\nCHIEVING human-level intelligence requires refining\nthe transition from System 1 to System 2 reasoning\n[1]\u2013[5]. Dual-system theory suggests that human cognition\noperates through two modes: System 1, which is fast, auto\nmatic, and intuitive, enabling quick decisions with minimal\neffort, and System 2, which is slower, more analytical, and\ndeliberate [6], [7]. While System 1 is efficient for routine\ntasks, it is prone to cognitive biases, especially in complex or\nuncertain situations, leading to judgment errors. In contrast,\nSystem 2 relies on logical reasoning and systematic thinking,\nresulting in more accurate and rational decisions [8]\u2013[11]. By\nmitigating the biases of System 1, System 2 provides a more\nrefined approach to problem-solving [12]\u2013[15].\nThe development of foundational Large Language Mod\nels (LLMs)\u00b9 has marked a major milestone in Artificial\nIntelligence (AI). Models such as GPT-40 [16] and DeepSeek\nv3 [17] have demonstrated impressive capabilities in text\ngeneration, language translation, and a variety of perception\ntasks [18]\u2013[28]. These models, trained on extensive datasets\nand utilizing advanced algorithms, excel in understanding\nand generating human-like responses. However, despite\ntheir impressive achievements, foundational LLMs operate\nin a manner similar to System 1 reasoning, relying on fast,\nheuristic-driven decision-making. While they perform ex\nceptionally well in providing rapid responses, they often fall\nshort in scenarios requiring deep, logical analysis and preci\nsion in complex reasoning tasks. This limitation becomes\nespecially clear in situations involving intricate problem\nsolving, logical analysis, or nuanced understanding, where\nthese models do not yet match human cognitive abilities.\nIn contrast, reasoning LLMs represent a significant ad\nvancement in the evolution of language models."}, {"title": "1.1 Structure of the Survey", "content": "This survey offers a comprehensive overview of the key con\ncepts, methods, and challenges involved in the development\nof reasoning LLMs. As illustrated in Figure 2, this survey is\norganized as follows:\n1) Section 2 offers a concise overview of the progress in\nfoundational LLMs (Section 2.1) and the early develop\nment of key System 2 technologies, including symbolic\nlogic systems (Section 2.2), Monte Carlo Tree Search\n(MCTS) (Section 2.3), and Reinforcement Learning (RL)\n(Section 2.4), highlighting how their combination has\npaved the way for reasoning LLMs.\n2) Section 3 introduces reasoning LLMs and outlines their\nconstruction process. Specifically, Section 3.1 presents\nthe characteristics of reasoning LLMs from two per\nspectives: output behavior (Section 3.1.1) and training\ndynamics (Section 3.1.2), emphasizing their differences\nfrom foundational LLMs. Section 3.2 identifies the core\nmethods necessary for achieving advanced reasoning\ncapabilities, focusing on five aspects: Structure Search\n(Section 3.2.1), Reward Modeling (Section 3.2.2), Self\nImprovement (Section 3.2.3), Macro Action (Section"}, {"title": "1.2 Contribution of the Survey", "content": "Recently, several analyses and replications of specific tech\nnical approaches have been conducted [48]\u2013[55], yet there\nremains a lack of systematic analysis and organization.\nResearch [56] has focused only on slow-thinking methods\nduring testing. Meanwhile, studies [57]\u2013[59] have primarily\nconcentrated on training or achieving reasoning LLMs, often\nfrom the perspective of RL.\nOur survey distinguishes itself from and contributes to\nthe existing literature in the following ways:\n1) Rather than focusing on a single technical approach, we\noffer a comprehensive overview of the key concepts,\nmethods, and challenges involved in reasoning LLMs.\n2) We summarize the key advancements of early System 2\nand how they have paved the way for reasoning LLMs,"}, {"title": "FOUNDATIONS OF REASONING LLMS", "content": "In this section, we provide a concise overview of the\nprogress in foundational LLMs and the early development\nof key System 2 technologies, highlighting critical advance\nments that, when combined with foundational LLMs, have\npaved the way for reasoning LLMs. These advancements\ninclude symbolic logic systems, MCTS, and RL."}, {"title": "2.1 Foundational LLMs", "content": "The development of foundational LLMs saw significant\nadvancements with the introduction of pretrained Trans\nformers [18] in 2018-2019, notably through BERT [19] and\nGPT [21]. These models leveraged unsupervised pretrain\ning on vast text corpora, followed by fine-tuning for task\nspecific applications. This approach enabled them to de\nvelop a broad language understanding before specializing\nin tasks such as sentiment analysis, entity recognition, and\nquestion answering. BERT's bidirectional context processing\nimproved word understanding, while GPT excelled in text\ngeneration with its unidirectional design.\nThe release of GPT-2 [22] in 2019, with 1.5 billion param\neters, marked a significant leap in generative performance,\nthough it also raised ethical concerns. GPT-3 [23], with\n175 billion parameters, further demonstrated the power\nof unsupervised pretraining, excelling in few-shot learning\nand performing well across a wide range of NLP tasks. In\nsubsequent years, multimodal models like CLIP [60] and\nDALL-E [61] emerged, integrating text and visual inputs.\nThese models enabled new tasks, such as generating images\nfrom text, and enhanced human-computer interaction."}, {"title": "2.2 Symbolic Logic Systems", "content": "Symbolic logic systems mark the earliest phase of AI, utiliz\ning rules and logical principles to represent knowledge and\ndraw conclusions [88], [89]. They are particularly effective in\nstructured domains, where formal logic ensures precision.\nProlog, a logic programming language based on first\norder logic, allows users to define facts, rules, and reason\nthrough queries. It has been pivotal in symbolic reasoning\nsystems, especially in NLP and expert systems [90]\u2013[92].\nLogic-based systems like Prolog employ propositional and\npredicate logic for formal reasoning [93], [94]. From the\n1960s to the early 1980s, this approach dominated AI, with\nsystems like IBM's LISP [95] for symbolic computation and\nResolution Theorem Provers [96] for automated reasoning.\nIn the 1970s, Marvin Minsky introduced Frames, which or\nganized knowledge into structured frameworks, influencing\nboth expert systems and cognitive science [97].\nSummary: Symbolic logic systems were pivotal milestones\nin early Al development. Based on formal logic, they ex\ncelled in well-defined problems, particularly in structured\nenvironments. However, they also exposed the limitations\nof rigid, rule-based systems. Despite these constraints, sym\nbolic logic remains foundational to the progress of AI.\nRecent advancements in reasoning LLMs have greatly\nenhanced the emulation of human-like System 2 cogni\ntive processes through sophisticated thought architectures,"}, {"title": "2.3 Monte Carlo Tree Search", "content": "MCTS is a simulation-based search algorithm for decision\nmaking and planning [98]. It constructs a search tree\nthrough four steps: Selection, which chooses the child node\nwith the highest priority using the UCB1 formula:\n$UCB1 = \\frac{w_i}{n_i} + C\\sqrt{\\frac{ln N}{N_i}}$        (1)\nwhere $w_i$ is the total reward of node $i$, $n_i$ is its visit count, $N$\nis the parent node's visit count, and $c$ balances exploration\nand exploitation. Expansion adds new nodes, Simulation per\nforms random rollouts to evaluate them, and Backpropagation\nupdates node statistics. MCTS has been widely used in tasks\nsuch as optimizing strategies in board games like Go [99]\nand in robotic path planning, where it helps robots navigate\ndynamic environments effectively [100].\nSummary: MCTS has played a crucial role in the develop\nment of reasoning LLMs, particularly in Structural Search\n(Section 3.2.1). By simulating potential future reasoning\npaths and backpropagating estimated rewards, MCTS helps\nfoundational LLMs efficiently identify the most promising,\nhigh-reward paths. This process mirrors human-like plan\nning, where future consequences of decisions are considered\nbefore taking action. By dynamically exploring multiple\nreasoning trajectories, MCTS enables models to avoid get\nting stuck in suboptimal paths, making it easier to navigate\ncomplex decision spaces. This integration has significantly\nenhanced the ability of LLMs to handle intricate and dy\nnamic reasoning problems, such as those requiring long\nterm planning or multi-step logical inferences. It has al\nlowed LLMs to make more strategic and informed decisions,\nimproving their overall performance in tasks that involve\nnuanced reasoning and strategic exploration."}, {"title": "2.4 Reinforcement Learning", "content": "RL is a type of machine learning where an agent learns\nto make decisions by interacting with an environment\nand receiving feedback in the form of rewards, aiming\nto maximize cumulative rewards over time [101]. Early\nbreakthroughs in RL, such as Q-learning [102] and DQNs\n[103], revolutionized the field by enabling the handling of\ncomplex state spaces using Deep Neural Networks (DNNs)\n[104]. These methods paved the way for scaling RL to real\nworld tasks, where traditional tabular approaches fell short.\nThe advent of deep RL marked a significant step forward,\ncombining the power of deep learning with RL to process\nhigh-dimensional inputs, such as images and unstructured\ndata.\nA landmark achievement in deep RL was AlphaGo,\nwhich demonstrated RL's potential by defeating a world\nchampion in the complex game of Go through self-play"}, {"title": "3 BLUEPRINTING REASONING LLMS", "content": "In this section, we first analyze the features of reasoning\nLLMs from both output behavior and training dynamics\nperspectives. We then provide a detailed overview of the\ncore methods that enable their advanced reasoning capa\nbilities. Finally, we summarize the evolution of reasoning\nLLMs. A comprehensive comparison of traditional reason\ning models and reasoning LLMs is shown in Figure 3."}, {"title": "3.1 Analysis of the Features of Reasoning LLMs", "content": "3.1.1 Output Behaviour Perspective\nExplore and Planning Structure: Recent empirical studies\nhave revealed that reasoning LLMs demonstrate a strong\ntendency for exploratory behavior in their output structures,\nespecially when compared to models such as WizardMath\n[109] and DeepSeekMath [110], which primarily rely on\nconventional CoT reasoning approaches. This exploratory\nbehavior is evident in the formulation of novel hypotheses\nand the pursuit of alternative solution paths. Research by\n[49] suggests that slow-thinking models engage in a la\ntent generative process, particularly noticeable during the\nprediction of subsequent tokens. This claim is supported\nby [31], which observes that similar behaviors naturally\narise during RL scale training. Furthermore, the Quiet-STaR\nframework [111] introduces an auxiliary pre-training phase\nfocused on next-token prediction, highlighting the critical\nrole of internal deliberation and exploratory mechanisms"}, {"title": "3.1.2 Training Dynamic Perspective", "content": "Amazing Data Efficiency: Unlike traditional approaches\nthat focus on expanding instruction sets with uniformly\ndistributed difficulty levels, Studies [52], [54] suggest that"}, {"title": "3.2 Core Method", "content": "In this section, we provide an overview of the core methods\nthat drive the advanced reasoning capabilities of reasoning\nLLMs, as shown in Figure 4. These include Structure Search,\nReward Modeling, Self Improvement, Macro Action, and\nReinforcement Fine-Tuning. We also highlight representa\ntive reasoning LLMs for each method."}, {"title": "3.2.1 Structure Search", "content": "Reasoning LLMs aim to achieve high accuracy and depth in\nsolving complex problems by emulating the deliberate na\nture of human reasoning. However, despite recent advance\nments, current foundational LLMs face inherent limitations\nwhen addressing intricate reasoning tasks. These limitations\narise from their lack of an internal world model to simulate\nenvironmental states, their inability to predict the long-term\noutcomes of reasoning paths, and their failure to iteratively\nrefine reasoning steps based on future states or rewards [8].\nAs a result, these shortcomings hinder foundational LLMs\nfrom effectively balancing exploration and exploitation in\nvast reasoning spaces, creating challenges in tasks that re\nquire multi-step reasoning, such as complex mathematics,\nlogical inference, or strategic decision-making [139].\nMCTS, a powerful search and optimization algorithm,\neffectively addresses these challenges by providing a struc\ntured framework to explore and evaluate reasoning paths\nsystematically. It operates by constructing a reasoning tree,\nwhere each node represents a reasoning state, and ac\ntions expand the tree by considering potential next steps.\nThrough the simulation of future states and the iterative\nbackpropagation of estimated rewards, MCTS allows foun\ndational LLMs to efficiently identify high-reward reasoning\npaths, mirroring human planning processes. This approach\naligns with the core principles of reasoning LLMs, where\nthorough analysis and deliberate exploration are essential\nfor generating well-reasoned outputs. Recent methods, such\nas RAP [14], enhance foundational LLMs by integrating\nMCTS with a world model, enabling the system to itera\ntively refine intermediate reasoning steps and improve fu\nture predictions. Similarly, Forest-of-Thought [125] utilizes\nMCTS to dynamically explore multiple reasoning trajecto\nries, revisiting flawed paths and refining outcomes.\nThe application of MCTS in reasoning tasks extends\nbeyond traditional problem-solving to highly specialized\ndomains. For example, frameworks like SRA-MCTS [134]\nand MC-NEST [133] showcase the utility of MCTS in tack\nling technical challenges such as code generation and math\nematical reasoning, where intermediate steps are iteratively\nevaluated and refined. In fields like instructional alignment,\nframeworks such as SPaR [127] and Marco-o1 [112] leverage\nMCTS to refine responses and align reasoning trajectories\nwith human preferences or desired outcomes. Additionally,\ntask-specific implementations like HuatuoGPT-01 [113] un\nderscore MCTS's crucial role in navigating highly special\nized domains, such as medical reasoning, where accuracy\nand robustness are paramount.\nMCTS also enables models to go beyond single-pass\nreasoning methods, such as CoT or Tree-of-Thought, by\nincorporating mechanisms to revisit, critique, and refine\nreasoning steps dynamically [131], [140]. This iterative ca\npability is essential for tackling tasks with vast decision\nspaces or those requiring long-term planning, where ear\nlier decisions can significantly impact final outcomes. By\nallowing LLMs to simulate, evaluate, and refine multiple\nreasoning paths, MCTS introduces a level of adaptability\nand strategic exploration that traditional approaches lack.\nAs shown by AlphaZero-like tree-search [124] and Search\n01 [117], MCTS enables reasoning LLMs to not only achieve\nbetter performance on specific tasks but also exhibit en\nhanced generalization capabilities across diverse domains.\nThe integration of MCTS into LLMs depends on defining\nactions and rewards to guide reasoning path exploration\nand assess quality. As shown in Table 1, we classify the\nactions in prior work into four categories:\n1) Reasoning Steps as Nodes: Actions represent inter\nmediate reasoning steps or decisions, such as select\ning rules, applying transformations, or generating sub\nquestions [14], [124], [125], [139].\n2) Token-level Decisions: Actions involve generating to\nkens or sequences (e.g., the next word, phrase, or code\nsnippet) [126]\u2013[128], [141].\n3) Task-specific Structures: Actions are domain-specific,\nsuch as moving blocks in blocksworld, constructing\ngeometry in geometry problem-solving, or modifying\nworkflows in task planning [129], [130], [142].\n4) Self-correction and Exploration: Actions focus on re\nvisiting, refining, or backtracking to improve previous\nreasoning steps [131], [132], [143].\nAdditionally, as illustrated in Table 1, we classify the"}, {"title": "3.2.2 Reward Modeling", "content": "Two primary training paradigms are used to tackle multi\nstep reasoning tasks: outcome supervision and process su\npervision. Outcome supervision emphasizes the correctness\nof the final answer at a higher level of granularity, and the\nresulting model is referred to as the Outcome Reward Model\n(ORM) [32], [158]. In contrast, process supervision provides\nstep-by-step labels for the solution trajectory, evaluating\nthe quality of each reasoning step. The resulting model is\nknown as the Process Reward Model (PRM) [37], [159],\n160]. The main distinction between ORM and PRM is\nillustrated in Figure 5.\nPRM offers significant advantages [147], [161] in com\nplex reasoning tasks for several key reasons. First, it pro\nvides fine-grained, step-wise supervision, allowing for the\nidentification of specific errors within a solution path. This\nfeature is especially valuable for RL and automated error\ncorrection. Second, PRM closely mirrors human reasoning\nbehavior, which relies on accurate intermediate steps to\nreach correct conclusions. Unlike ORM, PRM avoids situ\nations where incorrect reasoning can still lead to a correct fi\nnal answer, thus ensuring more robust and interpretable rea\nsoning. While PRM has primarily been applied to complex\nmathematical problems, its benefits have recently driven\napplications in other fields. For instance, ORPS [155] uti\nlizes PRM to address complex code generation challenges,\nwhile Step-DPO [156] combines process supervision with\nthe Direct Preference Optimization (DPO) algorithm [162]\nto improve long-chain mathematical reasoning. A summary\nof Reward Modeling method is presented in Table 2.\nSummary: Despite the advantages of PRMs, they present\nseveral challenges. The primary difficulty is obtaining pro\ncess supervision-labeled data, which is often both costly and\ntime-consuming. To address concerns related to scalability,\nefficiency, and accuracy, researchers have explored vari\nous automated annotation methods. For example, MATH\nSHEPHERD [147] utilizes the correctness of the final an\nswer to define the quality of intermediate steps based on\ntheir potential to lead to the correct outcome, automating"}, {"title": "3.2.3 Self Improvement", "content": "Reasoning LLMs exemplify a progression from weak to\nstrong supervision, while traditional CoT fine-tuning faces\nchallenges in scaling effectively. Self improvement, using the\nmodel's exploration capabilities for self-supervision, gradu\nally enhances LLMs performance in tasks such as translation\n[167], mathematics [163], [168], and multimodal perception\n[171]. This approach fosters exploration and application\nwithin reasoning LLMs [166], [184]\u2013[186]. A summary of\nSelf Improvement method is presented in Table 3.\nTraining-based self improvement in LLMs can be cat\negorized based on exploration and improvement strate\ngies. The exploration phase focuses on data collection to\nfacilitate subsequent training improvements, with notable\nvariations in approach. STaR [163] uses few-shot examples\nfor data gathering, while ReST [167], ReST-EM [168], and\nENVISIONS [169] rely on multiple samplings of complete\ntrajectories. Quiet-STaR [111] explores at the token level,\nintroducing concepts like meta-tokens and non-myopic loss\nto enhance supervision. Additionally, ReST-MCTS* [151]\nand rStar-Math [166] generate training data through MCTS.\nImprovement strategies also exhibit significant diversity.\nFor instance, STaR and its derivatives, such as V-STaR [?]\nand B-STaR [165], combine filtering with SFT. ReST and its\nvariants typically introduce innovative reward calculation\nmethods to enhance RL training for policy models. RISE\n[170] incorporates external feedback, recording rewards and\nrefining responses through distillation during the improve\nment process. Notably, rStar-Math [166] demonstrates that\nsmall models have achieved System 2 reflective capabilities\nthrough self-evolving training approaches.\nTest-time self improvement leverages the consistency\nof a model's internal knowledge to correct hallucinations\nduring inference. These approaches can be categorized\ninto three main types: methods that refine answers using\nprompts [174], [175], approaches that utilize external tools\n[176], and techniques that leverage logits without the need\nfor external tools or prompts [182], [183]."}, {"title": "3.2.4 Macro Action", "content": "Recent advancements in LLMs have driven progress in emu\nlating human-like System 2 cognitive processes via sophisti\ncated thought architectures, often referred to as macro action\nframeworks. These structured reasoning systems go beyond\ntraditional token-level autoregressive generation by intro\nducing hierarchical cognitive phases, such as strategic plan"}, {"title": "3.2.5 Reinforcement Fine-Tuning", "content": "Reinforcement Fine-Tuning (RFT) [207] is an innovative\ntechnique recently introduced by OpenAI, designed to en\nable developers and engineers to fine-tune existing models\nfor specific domains or complex tasks. Unlike general SFT,\nRFT focuses on optimizing the model's reasoning process by\nusing a reward mechanism to guide the model's evolution,\nthereby enhancing its reasoning capabilities and accuracy.\nThe core of RFT lies in improving the model's performance\nin a specific domain with minimal high-quality training data\n[208], an appropriate reward model [209], and a stable opti\nmization process in long-context [210]\u2013[213]. A summary of\nRFT method is presented in Table 5.\nDeepSeek-R1 [31], which employs a verifier reward\nbased strategy, has shown significant performance improve\nments compared to traditional methods like SoS [214]. Key\nadvantages include:\n1) Simplified Training Pipeline: RL supervision stream\nlines data construction and training processes, eliminat\ning the need for complex stepwise search mechanisms.\n2) Enhanced Scalability: Online RL training facilitates\nefficient scaling on large datasets, particularly for com\nplex reasoning tasks.\n3) Emergent Properties: DeepSeek-R1 [31] demonstrates\nunique emergent capabilities, such as Long-CoT reason\ning, which are difficult to achieve through SFT alone.\nDespite its strengths, RFT faces the following challenges:\n1) Unclear Mechanism behind Reasoning: The underly\ning mechanisms driving the reasoning improvements in\nDeepSeek-R1 remain poorly understood. For example,"}, {"title": "3.3 Evolutionary of Reasoning LLMs", "content": "The evolution of reasoning LLMs has progressed by several\ndistinct stages, with various strategies developed to over come the limitations of direct autoregressive inference and\nbuild more advanced slow-thinking reasoning architectures."}, {"title": "4 BENCHMARKING REASONING LLMS", "content": "The development of a robust benchmark is crucial for doc\numenting the advancements in reasoning LLMs capabilities\nand for identifying promising research directions for future"}, {"title": "4.1 Benchmark Categories", "content": "We categorize reasoning benchmarks by task type, which\ncan be broadly divided into math, code, scientific, agent,\nmedical, and multimodal reasoning. The detailed statistics\nfor these benchmarks are presented in Table 6."}, {"title": "4.1.1 Benchmark Introduction", "content": "1) Math Problems: We document the current popular\ncompetition-level mathematical benchmarks to show\ncase the capabilities of reasoning LLMs, including\n\u0391\u0399\u039c\u0395 2024 [246], MATH-500 [37], AMC 2023 [247], and\nOlympiad Bench [248].\n2) Code Problems: Code problems requires solid founda\ntion and high logical thinking to evaluate the reasoning\nability of reasoning LLMs such as Codeforces, SWE\nbench [249], and LiveCodeBench [250].\n3) Scientific Problems: Scientific benchmarks, i.e., GPQA\nDiamond [251] and MMLU-Pro [253], involve multi\ndomains reasoning about chemistry, biology, and\nphysics, which requires extensive knowledge accumu\nlation and integrated reasoning.\n4) Agent Reasoning: Realistic tasks often involve complex\nplanning and tool usage, leading to the creation of agent\nreasoning benchmarks [268]. For example, WebShop\n[254] and WebArena [255] focus on web operations,\nwhile SciWorld [256] and TextCraft [257] are centered\naround scientific research.\n5) Medical Reasoning: Medicine fundamentally involves\ncomplex reasoning, spanning tasks from diagnostic de\ncision making to treatment planning. Benchmarks of\nJAMA Clinical Challenge [258], Medbullets [258], and\nMedQA [259] offer model measurements that mimic the\ndoctor's disease diagnosis.\n6) Multimodal Reasoning: Multimodal reasoning, such\nas benchmarks of MMMU [260] and MathVista [261],\nrequires cross-modal thinking in combination with\ntext and images. Especially for those visual-centered\nproblems, in benchmarks MathVision [262], MathVerse"}, {"title": "4.1.2 Summary", "content": "The field of LLMs has advanced rapidly in recent years,\nwith benchmark performance consistently improving. Sim\nple reasoning benchmarks, such as GSM8K [32], MATH-500\n[37], and ScienceQA [270], have approached performance\nsaturation. Recent studies on reasoning LLMs [54], [166]\nshow that models designed for long reasoning chains do not\nsignificantly outperform those designed for shorter chains\non these benchmarks. This highlights the urgent need to\nestablish new benchmarks that more effectively assess the\nreasoning capabilities of reasoning LLMs. Moreover, current\nbenchmarks are limited, focusing mainly on solid reasoning\ntasks. Soft reasoning benchmarks, lacking explicitly defined\ncorrect answers, offer a more nuanced evaluation, better\ncapturing the complexities and subtleties of human-like\nreasoning. Furthermore, it is essential to address the issue\nof data leakage in evaluation processes [271]. Ensuring the\nconfidentiality and neutrality of evaluation data is critical to\npreserving the integrity and reliability of benchmark results."}, {"title": "4.2 Evaluation Metrics", "content": "Depending on task types, technical proposals, and rea\nsoning paradigms, various evaluation metrics have been\nintroduced for reasoning LLMs as shown in Figure 6. These\nmetrics are designed to more accurately assess the model's\nperformance in handling complex reasoning tasks, ensuring\nthat both the quality and coherence of the generated solu\ntions are effectively measured."}, {"title": "4.2.1 Task Types", "content": "In terms of benchmark categories, mathematical reasoning\ntypically uses two main metrics: Pass@k and Cons@k. The\nPass@k metric evaluates the model's ability to generate a\ncorrect solution within k attempts, measuring the likelihood\nof success within a limited number of tries. On the other\nhand, Cons@k assesses whether the model consistently pro\nduces correct or logically coherent solutions, highlighting\nthe stability and reliability of its reasoning capabilities. For\ncode tasks, the key metrics are Elo and Percentile, both"}, {"title": "4.2.2 Technical Proposals", "content": "Based on technical routes, the schemes with ORM or PRM\noften leverage RM@k and Best-of-N two evaluation indica\ntors. RM@k measures whether the reward model can rank\nthe good answer higher in the top k candidates according\nto reward score, and Best-of-N chooses the solution with\nhighest score from N generated reasoning trajectories. Meth\nods for self-consistency are evaluated using Greedy Decoding,\nBeam Search, and Major@k. Greedy Decoding and Beam Search\ncontrol the randomness of the inference process by limiting\nthe sampling range. Major@k selects the solution with the\nmost consistent results from k candidate solutions. In RL,\nmetrics reflect both performance in achieving desired out\ncomes and the efficiency of the learning process. For exam\nple, Cumulative Reward measures the total reward received\nby the agent over time, while Sample Efficiency assesses the\nefficiency of the agent's sample usage during learning."}, {"title": "4.2.3 Reasoning Paradigms", "content": "For reasoning paradigm of the multi-turn solution gener\nation in reasoning LLMs, Outcome Efficiency and Process\nEfficiency [122] are proposed recently to evaluate the effi\nciency of long thinking specifically. Outcome Efficiency metric\nempirically evaluates how effectively later solutions con\ntribute to accuracy improvements, formulating as the ratio\nof efficient tokens that contribute to reaching the correct\nanswer, to all output tokens. Process Efficiency metric eval\nuates the contribution of later solutions to solution diversity\nempirically, concretely representing as the ratio of tokens of\ndistinct solutions to all solution tokens. These two indicators"}, {"title": "4.2.4 Summary", "content": "Most of the existing evaluation metrics are judged according\nto the final answer. It is imperative to develop a com\nprehensive assessment framework that considers various\naspects of the reasoning process in view of the large in\nference computation consumption. Current popular eval\nuation frameworks, such as LMMs-Eval [278], OpenCom\npass [279], and PRMBench [280], lack efficiency and their\nmetrics do not adequately account for the computational\nand temporal efficiency of the reasoning process. To address\nthese shortcomings, we highly recommend exploring more\nefficient proxy tasks as potential solutions. By identifying\nand utilizing tasks that better capture the nuances of long\nreasoning chains, we can develop more robust and effective\nevaluation metrics to enhance the overall assessment frame\nwork, ensuring that it not only measures the accuracy of the\nfinal output but also evaluates the efficiency and coherence\nof the reasoning process throughout."}, {"title": "4.3 Performance Comparison", "content": "In this section, we compare the performance of different rea\nsoning LLMs and their corresponding foundational LLMs\non plain text benchmarks, such as math and code problems,\nas well as on multimodal benchmarks. The comprehensive\nreal-time leaderboard is available on Arena."}, {"title": "4.3.1 Performance on Plain Text Benchmarks", "content": "As shown in Table 7, reasoning LLMs, such as DeepSeek-R1\n[31] and OpenAI-01/03 [29], [30], demonstrate exceptional\nperformance across a wide range of tasks, including math,\ncoding, and other general tasks. These models achieve high\nscores on multiple plain-text benchmarks, such as AIME\n2024, MATH-500, and LiveCodeBench, showcasing their\nrobust text-based reasoning abilities. In contrast, founda\ntional LLMs, like GPT-40 [62], Claude-3.5-Sonnet [272], and"}, {"title": "4.3.2 Performance on Multimodal Benchmarks", "content": "As shown in Table 8, reasoning LLMs continue to excel\nin multimodal tasks. OpenAI-01 [29", "62": "by 7.2% on MathVista. However, the perfor\nmance improvement in multimodal tasks is less pronounced\ncompared to text-only tasks. This can be attributed in part\nto the limitations of current multimodal"}]}