{"title": "CHEEMS: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch", "authors": ["Xueru Wen", "Jie Lou", "Zichao Li", "Yaojie Lu", "XingYu", "Yuqiu Ji", "Guohai Xu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun", "Debing Zhang"], "abstract": "Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancement of large language models (Yang et al., 2024a; Grattafiori et al., 2024), post-training has emerged as a critical challenge to ensure their safety, reliability, and alignment with human values (Hou et al., 2024; Lin et al., 2024). Reward models (Palan et al., 2019; Ouyang et al., 2022), as core components of LLM post-training, play a pivotal role in capturing human preferences and guiding models to adhere more closely to human needs (Bai et al., 2022). By providing reward signals, RMs can guide parameter optimization during training (Ibarz et al., 2018; Ouyang et al., 2022) or directly intervene in outputs during decoding(Khanov et al., 2024; Li et al., 2024a). Despite the crucial role of RMs in post-training, current research is mainly focused on English. For instance, Skywork-Reward (Liu et al., 2024a) and UltraRM (Cui et al., 2023) leverage high-quality English preference datasets (Zheng et al., 2023; Ji et al., 2024) and benchmarks (Lambert et al., 2024) to achieve superior performance. In contrast, the development of Chinese RMs faces significant challenges due to a lack of large-scale, high-quality preference datasets and comprehensive evaluation benchmarks. Existing Chinese resources are often small in scale (Huozi-Team, 2024; Yucheng, 2023) and limited to specific domains (Yang, 2024; Xinlu Lai, 2024; Xu et al., 2023), making them insufficient for LLM post-training. Moreover, existing RM mainly rely on synthetic data, which struggles to accurately reflect human preferences. To address this critical gap, this paper constructs a comprehensive and human-centric Chinese RM resource from scratch\u00b9. It consists of two key datasets: (1) CheemsBench, a fully human-annotated and extensive Chinese RM evaluation benchmark that verifies whether RMs accurately capture and reflect human preferences; and (2) CheemsPreference, a large-scale, diverse Chinese preference dataset that provides supervised signals for training Chinese RMs, enabling them to effectively learn and model human preferences. As shown in Figure 1, unlike most RM resources that rely on machine-generated annotations (Zhou et al., 2024), CheemsBench and CheemsPreference are built on human supervision, thereby more accurately capturing realistic human values. Moreover, while traditional RM benchmarks (Lambert et al., 2024) typically rely on pairwise comparisons, recent studies (Wen et al., 2024) have highlighted their limitations in reflecting downstream performances. CheemsBench introduces a multi-response evaluation mechanism, which aligns closely with downstream tasks. In CheemsBench, we combine open-source prompts and real-world human instructions with a comprehensive taxonomy to evaluate RM performance To better align with downstream tasks and reduce preference-induced noise (Zhang et al.,"}, {"title": "2 Related Works", "content": "Reinforcement Learning from Human Feedback. Reinforcement Learning from Human"}, {"title": "3 Chinese RM Benchmark", "content": "In this section, we introduce CheemsBench, a benchmark designed to comprehensively evaluate Chinese RMs. Our benchmark is characterized by: (1) High coverage: We incorporate a wide range of prompts and sampling models, ensuring broad evaluation across diverse scenarios. (2) High-quality annotation: We derive a reliable preference ranking through multiple rounds of manual triple-wise comparisons and conflict resolving. Figure 2 illustrates the overall construction process."}, {"title": "3.1 Data Construction", "content": "Prompt Collection. We sample Chinese prompts from various open datasets, including Humaneval-XL (Peng et al., 2024), MathOctopus (Chen et al., 2024), GAOKAO-Bench (Zhang et al., 2024b), HalluQA (Cheng et al., 2023), Flames (Huang et al., 2023), CLiB (Lee, 2023), AlignBench (Liu et al., 2023), and COIG-CQIA (yuelin bai, 2023). We manually map their original categories into a unified system shown in Figure 8. We also include real-world human instructions for out-of-distribution evaluation. To ensure thorough converge across different scenarios, we build a comprehensive categorization system as illustrated in Figure 9. In total, we select 1,146 prompts from open-source datasets and 1,346 from human instructions. Responses Collection. To ensure a wide range of response quality and distribution, we sample 5 re-"}, {"title": "3.2 Benchmark Labeling", "content": "Human Annotation. To accurately capture human preferences, CheemsBench relies entirely on human judgment for its annotation process. Given a prompt and its corresponding 5 responses, we pre-design five annotation tasks, each comprising a triple-wise comparison of three adjacent responses. These tasks are distributed to different annotators who perform preference comparisons independently. All annotation results are then used to construct a ranked list of responses. Conflict Resolving. However, conflicts may arise due to the human preferences ambiguity and potential annotation errors. To derive reliable results, we develop a dedicated conflict resolving algorithm, as shown in Algorithm 1. Specifically, we first transform the annotation results into a directed preference graph, where responses and preferences represent nodes and edges respectively. We then employ depth-first search to identify cycles in the graph, which indicate conflicts. These cycles are merged into larger nodes, and this process is repeated until no cycles remain in the graph. Finally, we perform topological sorting to obtain a partial ranking\u00b3."}, {"title": "3.3 Evaluation Metrics", "content": "Given multiple responses per prompt, there are many potential metrics for evaluation (Wen et al., 2024). We first convert a partial ranking into multiple pair-wise comparisons and evaluate the accuracy as in the typical setting (Lambert et al., 2024):\n$$Accuracy = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbb{I}(r_{w} > r_{i})$$\nwhere N is the total number of pair-wise comparisons after transformation, and the indicator function \\(\\mathbb{I}\\) checks if the reward score for the preferred response \\(r_{w}\\) is greater than that of its counterpart \\(r_{i}\\). Additionally, the exact match rate can be employed, which measures the proportion of prompts where all pair-wise comparisons are correctly sorted:\n$$Exact Match = \\frac{1}{M}\\sum_{j=1}^{M} \\mathbb{I}((\\forall k:\\quad r_{j}^{w} > r_{j}^{i})$$\nwhere M is the number of prompts, and the indicator function verifies if all comparisons are ordered correctly. We obtain the final result by averaging the metrics from subsets of different categories."}, {"title": "4 Chinese Preference Dataset", "content": "In this section, we present the construction of CheemsPreference, as depicted in Figure 3. Our dataset is characterized by: (1) Scale and diversity: We amass 27k real human instructions, featuring a comprehensive multi-tier categorization system, and sample multiple responses from a variety of models for each prompt. (2) High-quality annotation: We employ a distant supervision algorithm, which integrates both human annotations and GPT-4o to establish reliable partial preference ranks."}, {"title": "4.1 Data Construction", "content": "Prompt Collection. Diverse and high-quality instruction data are crucial for ensuring the robustness of RMs. To this end, we collect 27,861 real-world human instructions. To ensure extensive coverage of downstream scenarios, we develop a comprehensive multi-tier categorization system, which encompasses eight main categories with dozens of refined subcategories, as illustrated in Figure 10. Response Collection. We sample responses from a broad range of models: (1) Open-source models: Qwen2-7B/72B-Instruct (Yang et al., 2024a), Qwen2.5-7B/14B/32B/72B-Instruct (Team, 2024),"}, {"title": "4.2 Distant Supervision", "content": "The quality of preference data (Gao et al., 2024) is essential for the training of RM. While human annotation ensures high quality, it is expensive and challenging to obtain in large quantities. Conversely, GPT-based annotation is scalable but often inconsistent and biased (Stureborg et al., 2024). To construct large-scale, high-quality Chinese preference data, we implement a distant supervision strategy for annotation. We initially engage human annotators to label a small subset of data, following the protocol detailed in Section 3.2. Subsequently, GPT-4o is employed to annotate a larger dataset. For a set of N responses, GPT-4o performs \\(C_{N}^{2}\\) pair-wise comparisons between each response pairs. To mitigate positional bias (Li et al., 2024b), the order of responses in each comparison is randomized. Although these GPT-4o"}, {"title": "5 Chinese Reward Model", "content": "In this section, we introduce our reward model training methodology. In contrast to typical preference datasets constructed by pair-wise comparisons (Cui et al., 2023; Ji et al., 2024), CheemsPreference has two distinct characteristics: (1) each prompt is associated with multiple responses, and (2) these responses form only a partial preference chain. Thus, we employ following loss according to Bradley-Terry Model (Bradley and Terry, 1952):\n$$C' = - \\mathbb{E}_{x\\sim X,\\: y_{w}, \\: y_{l} \\forall y \\text{V}\\_x} [\\log(\\sigma(r(x, y_{w}) - r(x, y_{l})))]$$\nwhere X stands for the distribution of the prompt x and \\(V_{x}\\) denotes the distribution of responses y given the prompt x. We employ a greedy sample-based batch logic for calculating this loss. Specifically, during each forward pass, we determine if all responses for a given prompt can be included in one batch. If feasible, they are added to the batch; otherwise, any excess responses are allocated to subsequent batches. This method might bypass some pair comparisons, but it ensures that no response is duplicated across batches, thereby"}, {"title": "6 Experiments", "content": "We first assess the performance of open-source RMs and datasets on CheemsBench (Section 6.1). Next, we examine our benchmark's correlation with downstream tasks (Section 6.2). For CheemsPreference, we conduct an ablation study to demonstrate its effectiveness (Section 6.3) and test the scaling trend (Section 6.4)."}, {"title": "6.1 Benchmark Results", "content": "Reward Models Evaluation We thoroughly assess the performance of current RMs in the Chinese context, including discriminative reward models and generative models as reward models (Zheng et al., 2023). Table 2 demonstrates the results of top-ranked RMs on CheemsBench. We find that (1) The accuracy of the leading models significantly drops when applied to CheemsBench. This performance gap indicates considerable room for improvement of RMs in Chinese settings. (2) These RMs perform better on open-source prompts than on human instructions. This is expected, as our human instructions are collected from the real world and thus can be more out-of-distribution than open-source prompts. (3) For prompts with relatively deterministic answers, RM can assess the quality of the responses more accurately. Figure 4 details the performance of these RMs on different subcategories. On the open-source prompt subset, RMs show competence in \"Reasoning\" but struggle in other categories. On the human instruction subset, models excel in \"Reasoning\" and \"Complex Instructions\" but perform poorly in tasks involving \"Understanding\". These observations emphasize the need for targeted enhancements in these tasks. Preference Datasets Evaluation We evaluate various Chinese and English preference datasets on CheemsBench by training RMs based on Qwen2.5-72B-Instruct (Team, 2024). The experimental results are presented in Table 3. Notably, among the Chinese datasets, \"Huozi\" (Huozi-Team, 2024) performs best. Meanwhile, the \"Ultrafeedback\" (Cui et al., 2023) leads among English"}, {"title": "6.2 Downstream Correlation", "content": "In this section, we explore the correlation of CheemsBench with various downstream tasks by employing a Best-of-32 sampling strategy for optimization on three tasks: Human Win-rate, MT-bench-zh (Huozi-Team, 2024), and MT-bench (Zheng et al., 2023). For the Human Win-rate task, we use 87 unique Chinese instructions that are not included in CheemsBench. For each prompt, we obtain a fixed baseline response from Qwen2-72B- Instruct. Then we sample 32 responses from the same model and have human annotators score each one. They assign 1 if a response exceeds the baseline and -1 if it doesn't, which allows us to compute win rates. For MT-bench-zh and MT-bench, responses are sampled from Qwen2-7B-Instruct, with RMs performing Best-of-32 sampling on two-turn prompts, and GPT-4o is employed as the judge. We select 26 distinct open RMs, differing in training data and structures, for correlation assessment. Our baselines include RewardBench (Lambert et al., 2024), RMB (Zhou et al., 2024), and alternatives of our benchmarks annotated by GPT-4o, named as Open Prompt GPT and Human Instruction GPT. The results in Figure 5 illustrate that: (1) Our benchmark exhibits significantly stronger correlations with downstream tasks compared to other baselines, whether in Chinese or English tasks. (2) The benchmarks annotated by GPT demonstrate suboptimal correlation, underscoring the necessity of human judgment, which can achieve better generalization on downstream tasks."}, {"title": "6.3 Dataset Construction Ablation", "content": "We conduct an ablation study to assess the effectiveness of the dataset construction strategies outlined in Section 4.2. We train RMs based on Qwen2.5-72b-instruct (Team, 2024) to perform experiments and report performances in Table 4. The results reveal several key insights: (1) Neither Human nor GPT subsets alone are sufficient. The GPT subset underperforms on our benchmark, indicating the inability of GPT-4o to fully capture human preferences. Conversely, the Human subset performs poorly on RewardBench, likely due to its"}, {"title": "6.4 Scaling Trend", "content": "We validate scaling trends on CheemsPreference. Figure 6 shows that RM performance improves with increased data volume on Open Prompt and Human Instruction subsets, indicating that larger training dataset leads to superior performance. This phenomenon also highlights the potential of our distant supervision approach. We then assess model scaling trending by training RM on different sizes of Qwen-2.5 series models (Team, 2024). Figure 7 illustrates that increasing the model size from 0.5B to 72B significantly enhances performance, demonstrating that larger models capture complex preference patterns more effectively. Moreover, there is no significant difference when"}, {"title": "7 Conclusion", "content": "In this paper, we address the challenges of developing Chinese RMs by introducing CheemsBench, a comprehensive RM benchmark, and CheemsPreference, a high-quality Chinese preference dataset. Using these resources, we evaluate the progress of RMs in the Chinese context and validate the effectiveness of our dataset construction strategies. Our work narrows the gap between English and Chinese RMs and sets the foundation for future research."}, {"title": "Limitations", "content": "This work addresses the resource insufficiency in Chinese reward models. However, by focusing primarily on the Chinese language, the datasets may not fully capture all regional variations, potentially introducing language and cultural biases. Additionally, while the importance of human annotations is evident, the subjective nature of human judgment and the particular group of annotators involved can lead to biased preferences. Moreover, our findings, while tailored to the Chinese context, require further validation to ensure applicability beyond Chinese and English languages."}, {"title": "Ethical Considerations", "content": "Several ethical considerations are central to this work. Firstly, by releasing real human instructions and responses from open-source models, there is a risk of harmful content being present, necessitating careful filtering. Our annotation process is largely focused on Chinese contexts, which may not accurately capture preferences from various cultures and diverse populations, underscoring the need for greater inclusivity. Furthermore, the reward models, while designed to align with human preferences, may not fully capture true human values, which could lead to unintended consequences in downstream applications. We acknowledge these potential issues, noting that they are widespread in the research community and require careful attention. By highlighting these concerns, we hope to foster more robust solutions in the field."}, {"title": "A Prompt Category", "content": "Our instruction dataset is constructed using a dual-source collection strategy. The primary source comprises real human queries collected from production environments, ensuring authenticity and practical relevance. This is complemented by GPT-enhanced open-source data that undergoes rigorous human curation to maintain quality standards. To ensure comprehensive coverage and diversity, we developed a systematic taxonomy to guide our data collection process. This taxonomy helps categorize instructions across various dimensions, including task types (e.g., comprehension, knowledge-based, creative, reasoning, and mathematical), complexity levels, and application scenarios. Each collected prompt is carefully reviewed and categorized according to this taxonomy, allowing us to maintain a balanced distribution across different types of instruction. The prompt category taxonomy for CheemsBench is illustrated in Figure 8 to 9, while the promot category taxonomy for CheemsPreference is illustrated in Figure 10."}, {"title": "B Annotation Prompts", "content": "In this work, we leverage GPT-4o for constructing our preference dataset. We utilize the structured judge prompt presented in Figure 11 to assess response quality, emphasizing an objective and unbiased comparison between different model outputs. Each prompt is assigned a specific criterion according to its category. These criteria ensure that the evaluations are consistent and comprehensive across different contexts. Figure 13 provides a detailed overview of the criteria in Chinese, covering linguistic and logical aspects. It also accounts for the safety and complexity of instructions."}, {"title": "C Conflict Resolving", "content": "In this section, we introduce an algorithm designed to address potential annotation conflicts that arise from human evaluations. The Conflict Resolving Algorithm, as outlined in Algorithm 1, operates by systematically integrating conflicting responses into larger nodes, based on the understanding that these responses exhibit comparable quality. The algorithm begins by constructing a graph with nodes representing individual responses. Directed edges are established based on preference relationships between responses. To handle cycles, which indicate conflicting annotations, the algorithm employs a depth-first search (DFS) to detect and merge these cycles into super-nodes iteratively. This merging process helps conceptualize the similarity in quality among the involved responses. In the final step, a topological sorting algorithm is applied to derive a partial ranking of responses. We report the conflict rate between human annotations and GPT annotations on the Open Prompts and Human Instruction subsets in Table 5. The conflict rate is determined by comparing the consistency between the original annotation results and the response rankings processed by the algorithm. We find that, overall, GPT is more inconsistent than human annotators. Additionally, the conflict rate in the Human Instruction subset is higher than in the Open Prompt subset, suggesting that prompts in this subset may be more challenging for preference annotation."}, {"title": "D Human Annotation", "content": "We employ a team of 29 professional annotators, each holding a bachelor's degree, who work standard business hours (8 hours of active annotation time per day). On average, an annotator completes approximately 40 triple-wise comparisons per day, with the flexibility to use any necessary tools and resources for fact-checking and verification."}, {"title": "D.1 Annotation Pipeline", "content": "Our prompt assignment system divides tasks according to the prompt category and distributes them to annotators based on their domain expertise and performance history. To ensure data quality, we implement a comprehensive multi-stage verification process, which has been tested and improved through more than six months of practical applications in preference dataset production before being applied to the CheemsBench annotation process. Specifically, each prompt first undergoes double-blind annotation where two independent annotators must achieve 90% agreement. When discrepancies occur, annotators engage in alignment discussions to reach consensus based on established annotation guidelines rather than personal judgment. When significant disagreements cannot be resolved, the cases are forwarded to data delivery teams, data operations teams, and finally algorithm developers for further review and guidance. For quality assurance, we employ a cascading single-blind review system. First, data delivery teams verify 30% of the annotated data, which is then passed to data operations teams for another independent 30% verification. The final results are validated by research teams. To ensure review quality under this single-blind setting, we have developed a dynamic verification mechanism where ground truth samples are continuously established through collaborative alignment among teams and regularly embedded into review tasks. Our multi-stage process provides strong accountability, as each stage's work is reviewed by subsequent stages, and approved annotations can be rejected in later reviews, which incentivizes thorough independent assessment rather than simple agreement. We adopt the single-blind approach due to practical constraints: while our quality control reviewers are more experienced and highly qualified, their limited number compared to regular annotators necessitates this approach to maximize quality check coverage."}, {"title": "D.2 Annotation Guideline", "content": "Our annotation guidelines are built upon three core dimensions as shown in Table 6. We ask annotators to score each response according to the criteria in Table 7 while conducting preference annotations. For responses with identical scores, we require annotators to perform bucket-wise pairwise comparisons for further ranking. In the comparison process, annotators are instructed to assign 'g' (good) if response A is preferred over B, 'b' (bad) if B is preferred over A, or \u2018s\u2019 (same) if both responses are considered equally good. The comparison is based on overall user preference without detailed scoring criteria. After completing all comparisons, annotators are required to integrate their pairwise judgments to establish a complete ranking (e.g., A>C>B=D>E). The annotators then cross-validate this final ranking against their initial scoring to ensure consistency and resolve any potential contradictions. Beyond the general guidelines, we also devel-"}, {"title": "D.3 Annotation Bias", "content": "We explore the preferences of both human and GPT annotators in terms of response length and position, as shown in Figure 15. It can be observed that GPT-40 generally prefers responses that are placed later, whereas human annotators do not exhibit a significant preference for position. Additionally, when the response length difference is moderate, both human and GPT annotators tend to favor longer responses. However, as the length difference becomes too large, humans tend to prefer shorter ones. Overall, the specific preferences of the annotators are not very pronounced."}, {"title": "E Benchmark Results", "content": "In this section, we present comprehensive results on CheemsBench. Table 8 reports the performance of both discriminative RMs and generative models serving as RMs. The evaluated discriminative RMs include Skywork-series (Liu et al., 2024a), Llama-3.1-Nemotron-70B-Reward (Wang et al., 2024c), Llama-3-OffsetBias-RM-8B"}]}