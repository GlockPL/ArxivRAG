{"title": "Transforming Indoor Localization: Advanced Transformer Architecture for NLOS Dominated Wireless Environments with Distributed Sensors", "authors": ["Saad Masrur", "Jung-Fu (Thomas) Cheng", "Atieh R. Khamesi", "\u0130smail G\u00fcven\u00e7"], "abstract": "Indoor localization in challenging non-line-of-sight (NLOS) environments often leads to mediocre accuracy with traditional approaches. Deep learning (DL) has been applied to tackle these challenges; however, many DL approaches overlook computational complexity, especially for floating-point operations (FLOPs), making them unsuitable for resource-limited devices. Transformer-based models have achieved remarkable success in natural language processing (NLP) and computer vision (CV) tasks, motivating their use in wireless applications. However, their use in indoor localization remains nascent, and directly applying Transformers for indoor localization can be both computationally intensive and exhibit limitations in accuracy. To address these challenges, in this work, we introduce a novel tokenization approach, referred to as Sensor Snapshot Tokenization (SST), which preserves variable-specific representations of power delay profile (PDP) and enhances attention mechanisms by effectively capturing multi-variate correlation. Complementing this, we propose a lightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer) model, designed to reduce computational complexity without compromising localization accuracy. Together, these contributions mitigate the computational burden and dependency on large datasets, making Transformer models more efficient and suitable for resource-constrained scenarios. The proposed tokenization method enables the Vanilla Transformer to achieve a 90th percentile positioning error of 0.388 m in a highly NLOS indoor factory, surpassing conventional tokenization methods. The L-SwiGLU ViT further reduces the error to 0.355 m, achieving an 8.51% improvement. Additionally, the proposed model outperforms a 14.1 times larger model with a 46.13% improvement, underscoring its computational efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of wireless communication technologies, fueled by the deployment of 5G and the emerging vision of 6G, is driving transformative innovations in location-based services. Applications such as vehicle-to-everything (V2X) communication, urban air mobility (UAM), extended reality (XR), near-real-time robotic operations, and Smart X systems [1], [2] are at the forefront of this technological evolution. These advancements require the large-scale integration of Internet of Things (IoT) infrastructures, thereby intensifying the demand for accurate and efficient localization methods with reduced computational complexity to ensure the stringent quality of service (QoS) necessary for these cutting-edge applications. Effective indoor localization is particularly critical for applications such as navigation, safety and rescue operations, and resource optimization for customized user services. However, the need for reduced computational complexity is paramount, as most indoor positioning devices are constrained by limited computing power. Addressing these challenges requires the development of efficient and precise localization methods capable of operating within these constraints while supporting the demands of modern wireless communication systems.\nGlobal Navigation Satellite Systems (GNSS), commonly used for outdoor localization, face significant challenges in indoor environments due to severe channel conditions such as shadowing, fading, and noise, as well as the high probability of non-line-of-sight (NLOS) situations. Conventional wireless signal-based localization techniques [3], including Time of Arrival (TOA), Time Difference of Arrival (TDOA), Time of Flight (TOF), and Angle of Arrival (AOA), often perform poorly in highly NLOS environments, which are typical in indoor settings like factories, hospitals, shopping malls, and military installations. Additionally, these techniques frequently rely on empirical models that exhibit high complexity and a strong dependence on varying channel conditions.\nFingerprinting [4] has emerged as a widely adopted approach for indoor localization, relying on signal measurements such as Received Signal Strength Indicator (RSSI) or Channel State Information (CSI) to create location-specific fingerprints. However, traditional machine learning (ML) based fingerprinting methods often struggle to adapt to environmental variability and fail to capture the complex spatial and temporal relationships in the data. To address these limitations, researchers have increasingly turned to deep learning (DL) techniques, such as Convolutional Neural Networks (CNNs) and Multi-Layer Perceptrons (MLPs) [3], which are capable of learning more robust and discriminative features from the fingerprint data. DL-based fingerprinting techniques have shown promising results in various studies, but such techniques also increase system complexity, requiring substantial memory and processing power. This added complexity can hinder real-time implementation and scalability. Therefore, addressing these challenges is crucial, and this paper aims to present an approach that balances performance improvements with reduced system overhead, making fingerprinting techniques more efficient and adaptable.\nThe Transformer model [5] offers several advantages over traditional sequence processing techniques such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, including parallelization capability, reduced training time, and improved handling of long-range dependencies [6]. Transformers, despite their proven success in fields such as natural language processing (NLP), computer vision (CV), and machine translation [5], [7], have not been extensively used for indoor localization. This underutilization is largely due to the lack of inductive biases such as locality and translation invariance, which are"}, {"title": null, "content": "naturally present in CNNs but absent in Transformers, and due to the Transformer's dependence on large datasets to capture both local and global features [7]. These challenges are further amplified by the way inputs are tokenized, which can impact the model's training effectiveness.\nIn highly NLOS indoor wireless environments, multi-antenna systems face significant challenges due to multipath propagation, where signals undergo reflection, diffraction, and scattering from walls, furniture, and other obstacles. This complex propagation leads to signal superposition, causing phase shifts, fading, and interference, which further complicate the resolution of spatial and temporal variations in such environments. Moreover, in the context of indoor localization, access to large-scale datasets is limited, as collecting data of this magnitude is both computationally expensive and cost-inefficient. Additionally, existing network infrastructures lack the high computational resources necessary to test models with billions of FLOPs, which further constrains the deployment of computationally intensive models. These challenges underscore the need for more efficient approaches that reduce the dependence of Transformer-based models on large datasets while simultaneously lowering their computational complexity, enabling practical deployment in complex indoor wireless environments.\nOne critical yet often overlooked aspect of Transformers, particularly within the wireless communication domain, is tokenization. Conventional tokenization methods typically aggregate multiple variables, such as delayed events and physical measurements, into a single representation. This aggregation can obscure variable-specific features, leading to less informative attention maps and degrading model performance. Consequently, the model's dependency on extensive datasets increases, as larger datasets are required to effectively map complex multi-antenna radio signals to precise locations. Therefore, in this paper, we investigate the Transformer architecture and enhance its learning capabilities by proposing a sophisticated and physically interpretable tokenization technique referred to as Sensor Snapshot Tokenization (SST) that leverages the characteristics of wireless communication systems, specifically addressing inherent channel independence and enabling the Transformer to effectively learn the multivariate correlations using the tokens form multi-antenna radio system. This approach facilitates the development of variate-centric representations, resulting in meaningful attention maps and leading to a decreased dependence of the Transformer on large models and extensive datasets.\nTo further enhance efficiency and reduce computational complexity, we introduced modifications to several components of the vanilla Transformer architecture. These include replacing conventional normalization layers, redesigning the feed-forward network, and redefining the prediction mechanism using tokens from the final encoder block. The resulting enhanced Transformer model is termed the Lightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer), optimized for improved performance in resource-constrained scenarios. Furthermore, we perform a comprehensive evaluation of various architectural configurations within Transformers across datasets of varying sizes. This analysis underscores the efficacy of the proposed methodology, illuminating potential avenues for further research and development.\nThe primary contributions of this work are summarized as follows:\nWe define indoor localization in multi-antenna systems"}, {"title": null, "content": "under highly NLOS conditions as a multivariate modeling challenge, addressed using Transformers to capture complex channel dynamics.\nWe propose a novel tokenization technique, SST, which enables the Transformer to capture multivariate correlations and generate meaningful attention maps, significantly reducing reliance on large datasets.\nWe propose L-SwiGLU Transformer, a modified Transformer architecture that enhances computational efficiency and positioning accuracy by replacing the MLP-based feed-forward network (FFN) with a Swish gated linear unit (GLU)-based FFN. Additionally, we replace standard components, such as the class token and normalization layers, with more efficient counterparts. Furthermore, by introducing a global average pooling layer, we demonstrate that positional embeddings can be omitted without adversely affecting positioning accuracy.\nWe conduct an extensive evaluation of various Transformer architectures across datasets and models of varying sizes, demonstrating the proposed methodology's superior accuracy and computational efficiency.\nThe structure of the paper is organized as follows. Section II provides a comprehensive review of the existing literature. The system model is described in detail in Section III. Section IV explains the preprocessing steps and the tokenization methods. The design of transformer architectures for processing distributed sensor signals is discussed in Section V. Extensive evaluation studies and analyses are presented in Section VI. Finally, the paper concludes in Section VII."}, {"title": "II. LITERATURE REVIEW", "content": "In the literature, indoor positioning is classified into two major categories: geometric-based methods and fingerprinting-based methods [3]. Geometric approaches, such as trilateration and triangulation, use parameters such as AoA, TDoA, ToF, and ToA for positioning. Although these algorithms are well-established and extensively studied, they perform poorly in indoor scenarios due to outlier distortion caused by NLOS and multipath challenges [3].\nConversely, fingerprinting methods involve creating a database by collecting signals from various locations and extracting features from them. This constructed database is then used to predict the location of new signals. Compared to geometric approaches, fingerprinting methods are relatively simple, easily integrated into smart devices, and capable of achieving acceptable accuracy with support from existing wireless infrastructure, which explains their widespread exploration in the literature [3], [8].\nMost of the work involving ML-based fingerprinting has utilized shallow ML algorithms like K-nearest neighbors (KNN), Decision Trees (DTs), Support Vector Machines (SVM), and Random Forests. These algorithms are computationally expensive to train on large datasets and their performance tends to degrade. Furthermore, the performance of traditional ML methods will not scale with larger, more complex datasets [9]. With the advent of DL models, researchers have begun exploring algorithms like MLPs, RNNs, LSTMs, and CNNs for fingerprinting. Comprehensive details about deep learning-based fingerprinting methods, including the pros and cons of different methods and various types of inputs used for positioning, are provided in [3], [8].\nNumerous DL techniques, including CNNs and LSTMs, have achieved remarkable success in indoor localization"}, {"title": null, "content": "[10], [11]. In [12], a fully complex-valued neural network (CVNN) was proposed for positioning in environments with moderate Line-of-Sight (LoS) conditions, demonstrating high accuracy in 2D localization tasks. Building on this work, [13] extended the approach by employing a complex-valued ResNet model tailored for highly NLoS scenarios.\nWith the recent success of Transformer-based models in various applications, researchers have become increasingly interested in exploring their potential for indoor localization. In [14], the Vision Transformer (ViT) [7] which in encoder only Transformer model was employed for fingerprinting-based indoor localization. The approach involved using a CNN to create patches from the CSI matrix, which were then fed into the Transformer encoder blocks for localization. Similarly, the authors in [15] used ViT for WiFi-assisted indoor localization. They applied Principal Component Analysis (PCA) for input normalization and created RSSI gray images from the normalized RSSI data, achieving a 50th percentile error of 1.788 m. However, the use of PCA makes this approach impractical for real-world applications, as the same transformation cannot be consistently applied during testing, leading to performance degradation. Additionally, the dataset used in this study is not publicly available, making it difficult to assess the proportion of LoS.\nIn [9], a Transformer encoder-only network is utilized for Received Signal Strength (RSS) based WiFi fingerprinting, where the values from wireless access points (WAPs) serve as inputs. The study focuses on building floor and room prediction tasks using RSS data. However, since only a single RSS value per WAP is available, the model's ability to extract meaningful spatial or temporal features is inherently limited. Furthermore, the use of PCA as a pre-processing step may not be optimal.\nThe authors in [16] proposed a hybrid model incorporating CNNs and ViT for localization in Long Range Wide Area Network (LoRaWAN). In this model, CNN is used to learn local features while ViT captures global features. Similarly, in [17], the authors presented a CNN-aided ViT-based indoor localization method. In this approach, the CSI matrix is treated as an image and fed into a CNN, with the resulting features used as patches for the ViT. In the context of indoor localization for MIMO systems, the study in [18] adopts a similar hybrid approach, utilizing a variant of the ViT alongside a CNN for feature extraction.\nAn approach described in [19] employs a Transformer for signal source localization using the AoA, creating images based on the reference point's location and arrival angle. However, the process for generating these images is not explained. Additionally, this method utilizes both the encoder and decoder parts of the Transformer, which is unusual since typically only the encoder is used for classification and regression tasks. The work and rationale for this architectural choice are also not provided. ViT have also been investigated for human activity recognition (HAR) using WiFi CSI [20]. In this approach, the raw CSI data is transformed into a CSI spectrogram, which is then used as input to the ViT model for effective HAR.\nIn the studies discussed, the input data was predominantly treated as images, with ViTs employed as localization algorithms without any modifications to their architecture. Although this approach capitalizes on ViT's inherent strengths in processing image data, it fails to fully exploit the unique characteristics of wireless communication systems. Specifically, wireless environments exhibit phenomena such as channel independence, signal fading, and multipath prop-"}, {"title": null, "content": "agation, which are critical for accurate localization. By treating the data solely as visual inputs, these approaches overlook the rich temporal and spatial correlations that are fundamental to wireless communication. Ineffective utilization of the transformer architecture results in an increased reliance on larger Transformer models and necessitates the use of extensive datasets. Furthermore, in all these studies, the computational aspects of the resource-intensive Transformer models are largely overlooked, highlighting a significant challenge for their implementation in real-time, resource-constrained environments."}, {"title": "III. SYSTEM MODEL", "content": "We consider an indoor factory scenario with dense clutter and High sensor height (InF-DH) as in Fig. 1, which illustrates the spatial layout of the 3GPP InF-DH environment, including the arrangement of the sensor nodes connected to a 5G base station, operating at 3.5 GHz with a signal bandwidth of 100 MHz. A total of $N_s$ = 18 sensor nodes are uniformly distributed, spaced 20 m apart, with the perimeter sensor nodes located 10 m from the walls. The heights of the sensor nodes and devices are fixed at 8 m and 1.5 m, respectively. The spatial coordinates of the r-th sensor are represented as a $\\psi_r$ = [$x_r$, $y_r$, $z_r$]$^T \\in \\mathbb{R}^3$. The positions of all $N_s$ sensors are then collectively represented by $P$ = [$\\psi_1$ $\\psi_2$ ... $\\psi_{N_s}$] $\\in \\mathbb{R}^{3 \\times N_s}$, where each column corresponds to the coordinates of a single sensor. A single transmit antenna port is assumed for the device. Each sensor node is equipped with a dual-polarized receive antenna, enabling the reception of multi-antenna signals (MAS) through $A$=2 antenna ports. The RF channels between a device and the sensor nodes are generated using the 3GPP channel model documented in 3GPP Technical Report 38.901 [21].\nThe 3GPP InF-DH environment features a variety of obstructions, such as small to medium-sized metallic equipment and irregularly shaped items like assembly and assorted machinery. In this paper, we focus on the InF-DH {60%, 6 m, 2 m} configuration where 60% of the environment is covered by clutters of 6m height and 2m width/length. The LoS probability between a device and a sensor node is 0.008, based on empirical observations from the dataset for the InF-DH scenario [21]. That is, a LoS link between a device and the distributed sensor nodes is very unlikely. To locate a device, the device is configured to transmit the 5G sounding reference symbols (SRS) {$R[k]$}$_{k=0}^{(N_{SRS}/2)-1}$, where $N_{SRS}$ = 3264 is the number of SRS subcarriers, in the uplink to be received by the 18 sensor nodes. The device transmit power is assumed 23 dBm. The SRS signals have a comb spacing of $K_{TC}$ = 2"}, {"title": null, "content": "subcarriers with $\\Delta f$ = 30 kHz subcarrier spacing and offset between $N_{symb}$ = 2 consecutive OFDM symbols. The signal being sampled by each sensor node at a rate of $f_s$ = 4096 $\\Delta f$ = 122.88 MHz is converted into the frequency domain (FD) using an FFT of size $N_{FFT}$ = 4096. Assuming L channel taps with complex gains {$C_{a,l}$} and delays {$\\tau_l$}$_l$, the received SRS at the (k+1)-th sub-carrier at receive antenna port a is given by\n$R_a[k] = S_k\\sum_{l=0}^{L-1} C_{a,l} \\exp(-j2\\pi k\\Delta f\\tau_l) + W_a[k],$ (1)\nwhere $W_a[k]$ is the received noise. The bandwidth of the SRS signal is $N_{SRS} \\cdot \\Delta f$ = 97.92 MHz. The measured FD channel response (CR) is obtained as\n$H_a[k] = \\frac{R_a[k]}{S_k}.$ (2)\nAssuming low mobility, the combed but offset FD CRs from the two consecutive OFDM symbols are coalesced into a combined FD CR for all subcarriers. The measured time domain (TD) channel impulse response (CIR) is obtained by further applying an inverse FFT:\n{$h_a[d]$}$_d \\triangleq IFFT_{N_{FFT}}({$H_a[k]$}). (3)\nwhere d represents the discrete delay index. The TD power delay profile (PDP) is obtained by summing the powers over the antenna ports for each sample:\n$p[d] = \\sum_{a=0}^{A-1} |h_a[d]|^2.$ (4)\nThe PDP can be truncated to the first $N_{ts}$ = 128 time samples without losing much information about the link between a device and a sensor node. Note that we intentionally keep the processing of the received RF signals to the minimum and leave any signal processing (e.g., filtering, smoothing, or channel estimation) to the DL model.\nWith $N_s$ = 18 sensor nodes, each device position can be considered to map uniquely to a PDP matrix $\\Psi$ of dimensions $N_s \\times N_{ts}$, with each row capturing the PDP at a different sensor. We considered three datasets, namely small, medium, and large, sampling 10,000, 20,000, and 40,000 uniformly randomly placed device locations, respectively, and recorded the corresponding PDPs. For the test datasets, we sample another 4,000 uniformly randomly placed device locations and the corresponding TD PDPs. The PDPs collected from the $N_s$ = 18 sensor nodes are used as inputs to the DL model (to be defined in Section V) for joint processing, which will then output the 2D coordinates of the device."}, {"title": "IV. PROPOSED FRAMEWORK FOR PREPROCESSING AND TOKENIZATION", "content": "In this section, we present the proposed preprocessing technique designed for efficiently handling distributed MIMO sensor signals. Following this, we detail the conventional tokenization methods employed as benchmarks and introduce the proposed tokenization technique, which represents a significant advancement in the processing pipeline. This novel method is specifically tailored to address the limitations of existing approaches, thereby enhancing the representation and utilization of the sensor signal data."}, {"title": "A. Pre-processing for RF Signals from Distributed Sensors", "content": "One major issue with processing distributed sensor network signals in a DL model is the wide dynamic range of the received signals. Because of path losses, reflection, shadowing, and fast fading effects, the received signal powers at different sensors can differ by over 35 dB as illustrated by the cumulative distribution function (CDF) in Fig. 2. Without proper handling, a DL model will ignore weaker received signal streams.\nTo address this dynamic range issue, we apply the power compression algorithm [13] on the PDP before feeding the signals to the DL model. For a length-$N_{ts}$ PDP $p[d]$, the power-compressed signal is given by:\n$\\tilde{p[d]} = S^2\\|\\|{p[d]}\\|\\|^{r-1} p[d],$ (5)\nwhere $\\|\\|{p[d]}\\|\\| = \\sum_{d=0}^{N_{ts}-1} p[d]$ is the total received power at a sensor over all its antenna ports, r is the amplitude compression ratio parameter and S is the target scale parameter after compression. For our indoor factory scenario in Fig. 1, we use r = 5 and S = 10 to compress the received signal powers into a narrower range between 0 and 10 dB illustrated in Fig. 2. Furthermore, we found better performance is obtained when using square root PDP, $\\sqrt{p[d]}$, instead of straight PDP."}, {"title": "B. Conventional Tokenization Approaches", "content": "Patch-based Tokenization (PBT): In the wireless positioning literature, Transformer inputs are often represented in image format, with some approaches treating the CSI matrix as an image [17], and others converting RSSI and AoA data into images [15], [19]. For the first tokenization approach, similar to methods used in wireless literature, the PDP $\\Psi$ which is a matrix of dimensions $N_s \\times N_{ts}$, is treated as a 2D image. This image is partitioned into fixed-size patches (referred to as tokens) with dimensions $W_h \\times W_w$, where $W_h$ and $W_w$ denote the height and width of each token, respectively. Consequently, the total number of tokens generated from the PDP is given by $N_{tk} = \\frac{N_s}{W_h} \\cdot \\frac{N_{ts}}{W_w}$. Each token is then reshaped into a flattened vector, resulting in a spatial size (i.e., number of sample per token) of $N_{st} = W_h \\cdot W_w$, this tokenization technique is referred to as Patch-based Tokenization (PBT). However, transforming inputs into image format and applying patching techniques fuses information from distinct physical measurements without considering the physical meaning of samples, which may hinder the Transformer from learning meaningful attention"}, {"title": "C. Proposed Tokenization Approaches", "content": "In this section, we introduce two novel tokenization approaches\u00b9-Time Snapshot Tokenization (TST) and Sensor Snapshot Tokenization (SST). While both methods are designed to enhance feature representation in wireless communication systems, SST is the primary focus of this work due to its superior ability to capture spatial features and address the limitations of traditional input formulations. TST is presented as a complementary approach to highlight the strengths of SST.\nTime Snapshot Tokenization (TST): We have $N_s$ = 18 sensors; for illustration, assume that the representation of the PDP from 3 sensors is depicted in Fig. 3. The second tokenization technique, referred to as Time Snapshot Tokenization (TST), represents the PDP values from all $N_s$ sensors at a single time step as a single token, resulting in $N_{tk} = N_{ts}$ tokens, each with a dimensionality of $N_{st} = N_s$, effectively embedding multivariate temporal information within each token. When these temporal tokens are input into the multi-head attention (MHA) (to be defined in Section V) mechanism, the model tends to prioritize numerical values over the semantic relationships inherent among samples taken at the same time. Furthermore, since the values in each token are derived from different sensors, they may embody entirely distinct meanings, leading to the loss of multivariate correlations.\nTokens generated from this tokenization technique suffer from excessively localized receptive fields and will not be able to convey meaningful information, as each token is"}, {"title": null, "content": "based on a single time sample across $N_s$ sensors. Given that variations in time series data are heavily influenced by the order of the sequence, the permutation-invariant nature of the MHA mechanism is ill-suited for such data structures. As a result, the Transformer's ability to capture essential time series representations and portray multivariate correlations is diminished, thereby limiting its capacity and generalization ability across diverse time series datasets.\nSensor Snapshot Tokenization (SST): Considering the limitations of the previously discussed tokenizations we reflect on the Transformer's poor performance and propose an efficient way for tokenization technique called Sensor Snapshot Tokenization (SST). This approach independently embeds each PDP series received by a sensor into a distinct token, as illustrated by the black dashed boxes in Fig. 3. Where the number of tokens is given by $N_{tk} = N_s$, with each token having a dimensionality of $N_{st} = N_{ts}$. This independent embedding enhances the receptive field, enabling the Transformer model to learn meaningful representations. Importantly, this tokenization strategy acknowledges that the information from each sensor operates independently, reflecting channel independence between sensors. Each token encapsulates a unique physical meaning, allowing Transformer to focus more on the semantic relationships among time samples rather than their numerical values. The resulting tokens represent the global characteristics of the series, producing variate-centric tokens, and this approach allows the MHA to cross-query and correlate received signals from all sensor nodes in parallel to capture multi-variate correlations. This is aligned with the conventional signal processing principle of processing signals from all sensors/antennas jointly. Capturing multi-variate correlation relationships enhances the interpretability of our models, especially since the spatial arrangement of the sensors remains consistent throughout, these correlations reflect the relative positions and interactions between sensors, which can provide valuable clues about the location of the signal source. These factors will decrease the heavy dependence of Transformer on large datasets. Additionally, an MLP will effectively derive generalizable representations from these tokens.\nThe second critical aspect of the SST technique is its potential to achieve reduced computational complexity. The MHA layer, the most computationally heavy block of a Transformer, exhibits quadratic complexity with respect to the number of tokens. In the first approach, PBT, the number of tokens depends on the parameters $W_h$ and $W_w$; smaller parameter values result in a higher number of tokens. Larger values lead to less number of tokens but will aggregate information across multiple physical measurements. This aggregation can dilute variable-specific representations and lead to less informative attention maps. In contrast, the second approach TST maintains a token count of $N_{tk} = N_{ts}$, while the third approach SST has $N_{tk} = N_s$. Given that $N_s \\ll N_{ts}$, the complexity of the third approach is significantly lower, resulting in greater computational efficiency."}, {"title": "V. DESIGNING TRANSFORMER MODEL FOR DISTRIBUTED SENSOR NETWORKS", "content": "With the recent success of encoder-only Transformer architectures like ViT in computer vision, there has been growing interest in adapting them for indoor positioning. However, directly applying ViT to this domain results in suboptimal performance and significantly increases model complexity. In this paper, we propose leveraging an encoder-"}, {"title": null, "content": "only Transformer model for device localization in cluttered indoor environments characterized by highly NLOS conditions. Specifically, the Transformer is designed to learn the complex mapping f between the device's position $\\hat{y}$ = [$x_d$, $y_d$] and the PDP $\\Psi$ received from all sensor nodes $N_s$, as expressed by:\n$\\hat{y} = f(P, \\Psi; \\Theta),$ (6)\nwhere $\\Theta$ encapsulates the model parameters, and $P$ is the sensor position vector as defined in Section III. The PDP, obtained from channel measurements at the sensors, encodes the multipath propagation characteristics by capturing the power and delay of reflected and scattered signals. This information inherently represents the spatial geometry and structure of the environment, enabling the Transformer to accurately learn the relationship between the PDP and the device's position $\\hat{y}$.\nIn this section, we describe the architecture of the vanilla Transformer model (i.e., ViT) and the proposed lightweight Swish-Gated Linear Unit-based Transformer model, referred to as L-SwiGLU Transformer. The complete architecture of vanilla Transformer and L-SwiGLU Transformer model can be seen in the Fig. 4."}, {"title": "A. Vanilla Transformer Architecture", "content": "Fig. 4a illustrates the architecture of the vanilla Transformer model. The localization process using a Transformer can be categorized into 3 main steps: 1) Input Embedding Preparation 2) Encoder Mechanism, and 3) Position Estimation.\n1) Input Embedding Preparation: In the embedding preparation, the PDP input is first converted into tokens/patches $X_{PDP} \\in \\mathbb{R}^{N_{tk}\\times N_{st}}$ using the tokenization method, where $N_{tk}$ is the number of tokens generated from PDP input, and $N_{st}$ is the number of samples in each token. After this, the PDP tokens $X_{PDP}$ are linearly projected to create embeddings from the input using a learnable projection, $E\\in \\mathbb{R}^{N_{st} \\times D_{emb}}$ resulting in $X_{inp}$ = $X_{PDP}E \\in \\mathbb{R}^{N_{tk}\\times D_{emb}}$ where $D_{emb}$ is the dimensionality of the embeddings. Embeddings are a way to represent data in a continuous vector space, where each PDP token is mapped to a dense, one-dimensional representation. This process captures the essential features of the PDP tokens, making it easier for the model to learn patterns and make predictions.\nThe next step involves appending a learnable embedding, referred to as the class token $X_{cls} \\in \\mathbb{R}^{1 \\times D_{emb}}$, to the sequence of embeddings generated from the input $X_{inp}$. This class token serves as a global representation for the model. A classification head will be attached to it for making predictions. The resulting embedding sequence is represented as $X_{emb} = [X_{inp}, X_{cls}] \\in \\mathbb{R}^{\\tilde{N}_{tk}\\times D_{emb}}$. The total number of tokens in embedding $X_{emb}$ will be $\\tilde{N}_{tk}$ = $N_{tk}$ + 1, where 1 comes from the additional class token $X_{cls}$. Embeddings are further encoded by positional embeddings resulting in:\n$Z = X_{emb} + E_{pos},$ (7)\nwhere $E_{pos} \\in \\mathbb{R}^{\\tilde{N}_{tk} \\times D_{emb}}$ are learnable parameters that capture positional information."}, {"title": "2) Encoder Mechanism", "content": ": The encoded PDP tokens \u017d are subsequently processed by the $N_l$ Transformer encoder blocks, with each encoder block comprised of four primary components: the Multi-Head Attention (MHA) block, the MLP, residual connections, and a normalization layer. The main component that distinguishes the transformer from other DL models is the MHA mechanism. In this mechanism, the embeddings interact to exchange and share information. Within the attention block, communication is facilitated by projecting the encoded embedding matrix \u017d to generate the Queries (Q), Keys (K), and Values (V) matrices. This is achieved using learnable weight matrices $W_q \\in \\mathbb{R}^{D_{emb}\\times d_q}$, $W_k \\in \\mathbb{R}^{D_{emb}\\times d_k}$, and $W_v \\in \\mathbb{R}^{D_{emb}\\times d_v}$, respectively, as follows:\n$Q = \\check{Z}W_q, K = \\check{Z}W_k, V = \\check{Z}W_v,$\n$Q\\in\\mathbb{R}^{\\tilde{N}_{tk}\\times d_q}, K\\in\\mathbb{R}^{\\tilde{N}_{tk}\\times d_k}, V\\in \\mathbb{R}^{\\tilde{N}_{tk}\\times d_v}$ (8)\nThe attention mechanism (Fig. 5) is applied to (Q), (K),"}, {"title": null, "content": "and (V) matrices to compute the correlation matrix C E\n$\\mathbb{R"}, {"as": "n$C = \\frac{QK^T"}, {"as": "n$\\bar{A"}, "j (Q, K, V) = \\text{Softmax}(C)V = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$ (10)\nwhere $j\\in \\{1, 2, ..., N_L\\}$. This operation assigns a weighted sum of the values (V) to each embedding, where the weights are determined by the similarity between the query (Q) and key (K) vectors. The similarity calculation is essential, as it enables the model to learn relationships between tokens by distributing attention based on relevance, rather than solely relying on exact matches between Q and K.\nIn [7"], "follows": "n$H_i = \\bar{A}_i (Q = \\check{Z}W_q, K = \\check{Z}W_k, V = \\check{Z}W_v)$,\nMHA(\\check{Z}) = Concat ($H_1, ..., H_{N_h}$) $W_o$ (11)\n$\\check{Z}_{j}^{MHA} = \\text{MHA} (\\text{LN} (\\check{Z}_{j-1})) + \\check{Z}_{"}