{"title": "VCAT: Vulnerability-aware and Curiosity-driven Adversarial Training for Enhancing Autonomous Vehicle Robustness", "authors": ["Xuan Cai", "Zhiyong Cui", "Xuesong Bai", "Ruimin Ke", "Zhenshu Ma", "Haiyang Yu", "Yilong Ren"], "abstract": "Autonomous vehicles (AVs) face significant threats to their safe operation in complex traffic environments. Adversarial training has emerged as an effective method of enabling AVs to preemptively fortify their robustness against malicious attacks. Train an attacker using an adversarial policy, allowing the AV to learn robust driving through interaction with this attacker. However, adversarial policies in existing methodologies often get stuck in a loop of overexploiting established vulnerabilities, resulting in poor improvement for AVs. To overcome the limitations, we introduce a pioneering framework termed Vulnerability-aware and Curiosity-driven Adversarial Training (VCAT). Specifically, during the traffic vehicle attacker training phase, a surrogate network is employed to fit the value function of the AV victim, providing dense information about the victim's inherent vulnerabilities. Subsequently, random network distillation is used to characterize the novelty of the environment, constructing an intrinsic reward to guide the attacker in exploring unexplored territories. In the victim defense training phase, the AV is trained in critical scenarios in which the pretrained attacker is positioned around the victim to generate attack behaviors. Experimental results revealed that the training methodology provided by VCAT significantly improved the robust control capabilities of learning-based AVs, outperforming both conventional training modalities and alternative reinforcement learning counterparts, with a marked reduction in crash rates. The code is available at https://github.com/caixxuan/VCAT.", "sections": [{"title": "I. INTRODUCTION", "content": "AVs have gradually increased their market presence but have also become one of the sources of threats to public safety [1]. However, it is extremely challenging to comprehensively enhance the robustness due to sparse corner cases. Adversarial training provides an effective method [2]. By allowing attackers, i.e., traffic vehicles, to create safety-critical scenarios, learning-based AVs are expected to learn how to avoid risks under safety expectations, thereby further enhancing robustness. In general, existing adversarial training methods face two challenges: insufficient utilization of the victim's intrinsic information and the limited variety of the attacker's attack modes."}, {"title": "A. Problems and Challenges", "content": "Exploitation of intrinsic vulnerability of victim. Prevailing studies often utilize fused environmental observation via optimization [3] or learning [4] methods to pinpoint the desired attack, while often neglecting the exploitation of the victim (i.e., target AV)'s intrinsic vulnerabilities. This oversight is consequential; reliance on mere observational data can yield substantial pitfalls, as attackers may struggle to identify unfavorable states of the black-box victim, making it difficult to launch effective attacks, particularly under conditions where safety-critical frames are rare. Such occurrences are quite common in AVs where the \"long-tail effect\" [5] exists.\nExploration of policy space of victim. Traditional attack methods might only set binary collision or not, or a continuous probability distribution [6]. However, such tactics may falter due to inadequate exploration, leading to a phenomenon known as mode collapse, particularly under conditions of sparse rewards [7]. This vulnerability is often exacerbated by the propensity for local optimization intrinsic to learning-based techniques."}, {"title": "B. Main Contribution", "content": "To address the above issues, we propose the VCAT framework, with its key contributions summarized as adversarial training framework, attack method, and rigorous experimentation.\nAdversarial Training Framework: VCAT. We have constructed a vulnerability-aware and curiosity-driven adversarial training (VCAT) framework. This framework exploits identified weaknesses within the AV to fabricate a diverse spectrum of scenarios. Consequently, it enhances the AV's competency in acquiring robust defensive driving strategies when faced with critical edge cases.\nAttack Method: Inspired by the victim-aware and curiosity [8] mechanism, we have developed a curiosity-driven deep reinforcement learning (DRL) attack paradigm, that leverages vulnerabilities of the victim by focusing on areas that the attacker has not fully understood or explored.\nAdversarial Training Experiment: To rigorously evaluate the effectiveness of the VCAT framework, we conducted extensive adversarial training simulations. The results of these experiments reveal that our proposed method markedly bolsters the risk mitigation capabilities of AVs, thereby substantially elevating the safety standards in autonomous driving."}, {"title": "C. Construction", "content": "The overall structure of the paper is as follows. Section II reviews related research work on DRL-powered attack and adversarial training. In Section III, we propose the VCAT framework, following a two-stage approach of adversarial attack and defense training [9]. Subsequently, in Section IV, the proposed method is conducted in a simulation experiment, and the results are analyzed. Finally, the conclusion and future works are summarized in Section V. Some commonly acronyms are also adopted, including w.r.t. (with respect to) and w.l.o.g. (without loss of generality)."}, {"title": "II. RELATED WORKS", "content": null}, {"title": "A. DRL-powered Attack", "content": "Attack methods employing DRL have accumulated substantial academic achievements by teaching adversarial agents to launch attacks. Especially in the field of AVs, artificial intelligence (AI) attacking AI is a common way. Through adversarial training, one can enhance the robustness of the target AI agent, a concept commonly seen in Generative Adversarial Network (GAN) [10], Generative Adversarial Imitation Learning [11], and Game Theory [12].\nIn response to the limitations of traditional adversarial DRL, some literature aims to improve the performance in specific autonomous driving adversarial training and validation tasks. For instance, the series RL method proposed by Cai et al. [13] considerably diversified the range of adversarial scenarios. Huang et al. [14] leveraged Stackelberg game dynamics by factoring in the adaptivity of the agent, generating challenging yet solvable environments, thus enhancing the stability and robustness of RL training.\nDespite extensive research suggesting that constructing adversarial environments with RL aids in the training and validation, the benefits of integrating vulnerability-evaluation and curiosity-exploration of adversarial algorithms in learning tasks remain to be investigated."}, {"title": "B. Adversarial Training", "content": "Adversarial training is a crucial method for enhancing the robustness of AI agents [15], and it has accumulated substantial empirical research. The perspectives on adversarial phenomena can be dichotomized into adversarial attack and defensive strategies, or alternatively, they can be synthesized within an integrated framework of adversarial training. In terms of adversarial attacks, Ding et al. [16] devised a generative adversarial network aimed at stabilizing adversarial training to enhance contextual prediction in AVs through the restoration of visually degraded images. Kloukiniotis et al. [17] reviewed denoising techniques as a countermeasure to adversarial attacks on AVs, emphasizing the role of adversarial training in improving adversarial robustness. In response to adversarial attack methods, adversarial defense is essential. Zhang et al. [18] introduced a closed-loop adversarial training framework aimed at improving the robustness and safety of AV control.\nHowever, existing adversarial training methods have not exploited the intrinsic vulnerability nor explored the policy space of the victim, which hinders the advancements in the robustness of AI-driven AVs."}, {"title": "III. PROPOSED METHOD", "content": "This section introduces the novel VCAT method, devised to bolster the safety of AVs via adversarial training. It first provides an overview of the VCAT framework and then elaborates on the proposed adversarial attack and defense protocols."}, {"title": "A. Overview of the VCAT Framework", "content": "The overview of the proposed VCAT framework is illustrated in Fig. 1. It divides into dual stages of adversarial attack and defense, based on the victim's state, which alternates between being fixed (frozen) or variable (thawed) during the training and evaluation phases. In essence, the adversarial training studied in this paper models the game between the attacker and the victim as a two-player Markov Game (MG), which models the strategies of agents as part of the Markov Decision Process. In MGs, multiple agents perform a series of actions to maximize their collective or individual benefits. Specifically, two-player zero-sum MGs [19] involve a pair of agents with completely opposite interests. This study relaxes the zero-sum game problem due to the complicated traffic interactions."}, {"title": "B. Adversarial attack", "content": "Before conducting adversarial attacks, it is imperative that the victim (target AV) be subject to extensive training using standard datasets (e.g., road-collected or random-generated data) to ascertain it possesses fundamental navigational proficiencies, albeit with a deficiency in managing anomalous or edge cases. Once the AV agent has been thoroughly trained, its parameters should be frozen to play the role of the victim, $v$, thus being attacked by the training attacker, $a$.\nVictim and attacker constitute a two-player MG. When both are RL-driven, their value functions are $V^v(s)$ and $V^a(s)$, also known as expected rewards [20]. Therefore, the goal of adversarial attack is for the attacker to learn to adeptly discern and exploit the victim's vulnerabilities, specifically by minimizing $V^v(s)$. Given the network parameters $\\theta$ remain frozen, policy $\\pi_{\\theta^v}$ is thereby fixed, which effectively incorporates the victim as an integral component of the environmental construct. Thus, the objective of the adversarial attack is quantified as:\n$J = \\arg \\max_{\\theta_a} (V^a(\\pi_{\\theta_a}(s)) - V^v_{\\theta^a}(\\pi_{\\theta_a}(s)))$  (1)\nTherefore, an important insight is that if we can estimate the victim's $V^v_{\\theta^v}(s)$, it would help find its weaknesses more accurately. The Proximal Policy Optimization (PPO) paradigm [20] is used to train the $\\pi_{\\theta_a}$.\n1) Victim Value Approximation Network: We use an approximation network (parameterized by $\\theta_v$) to fit the state-value function of $v$, which aids in the explicit formulation of Eq.1. Adopting the Temporal Difference (TD) learning"}, {"title": "paradigm", "content": "we define the loss function of the approximation network equivalent to the TD-error:\n$\\arg \\min_{\\theta_v} ||r^v(\\mathbf{s}_t, a_t) + \\gamma \\mathbb{E}_{s_{t+1} \\sim P} [V_{\\theta^v_v}(\\mathbf{s}_{t+1}) - V_{\\theta^v_v}(\\mathbf{s}_t)]||$ (2)\nwhere $\\gamma$ is the discount factor, $P$ is the state transition probability, $a_t = (a_v, a_a)$ is the sampled joint action, and $r_v$ is the estimated reward function for the victim, w.l.o.g., under the black-box assumption. This is done to extend the victim's generality, which is beneficial for comprehensive training and validation endeavors:\n$r = \\lambda_1 \\cdot r_{\\text{target}} - \\lambda_2 \\cdot r_{\\text{acc}} - \\lambda_3 \\cdot r_{\\text{collision}}$ (3)\nwhere $\\lambda$ is the weight, $r_{\\text{target}}$ is the reward for the victim reaching the goal, $r_{\\text{acc}}$ is the acceleration reward, and $r_{\\text{collision}}$ is the collision reward.\n2) Curiosity-Driven Exploration: The value approximation network (VAN) is capable of approximating the significance of the current state $s_t$ w.r.t. the victim; hence, $\\theta_v$ encapsulates dense information about the state value of $v$ at the said $s_t$. If a certain $s_t$ represents an unfamiliar state to $v$, it becomes imperative to induce the adversarial agent to probe and exploit this state. Inspired by the random network distillation mechanism (RND) [8], two networks are constructed: a stationary target network, $\\rho$, and a dynamic predictor network, $\\hat{\\rho}$. The parameter of $\\rho$ is randomized and then fixed, while $\\hat{\\rho}$ is continuously optimized after randomization, with the aim of continuously approximating $\\rho$. This iterative optimization process is driven by the intent to minimize prediction disparities. When $\\hat{\\rho}$ encounters a fresh state, the prediction error will be high, resulting in a high intrinsic reward output. Considering that the last hidden layer of $\\theta_v$ (denoted as $\\phi_v$) due to its potent representation of the characteristics of the dense state w.r.t. $V_v$, it is used as input for RND. Therefore, the mean square error of RND is:\n$r_{\\text{ins}} = ||\\hat{\\rho}(\\phi_v(\\mathbf{s}_t)) - \\rho(\\phi_v(\\mathbf{s}_t))||^2$ (4)\nwhere $r_{\\text{ins}}$ is the intrinsic reward, which can adaptively adjust the exploration value of $s_t$ to steer the exploration of the attacker.\n3) Attacker Policy Training: The crux of the PPO lies in the calculation of the advantage function. Algorithm 1 incorporates $r_{\\text{ins}}$ into the advantage function, simultaneously coordinating $r_v$ and $r_a$. This training requires the initialization of six networks. The calculations of the advantage functions $A^a$, $A^v$, and $A_{\\text{ins}}$ are shown from lines 7 to 9. Subsequently, the training objective for PPO as in Eq.2 can be computed:\n$\\arg \\max_{\\kappa} \\mathbb{E}_{(\\mathbf{a}, \\mathbf{s}_t) \\sim \\pi_{\\theta_a, \\kappa}}[\\min (p_t A_{\\kappa}, \\text{clip}(p_t, 1 - \\epsilon, 1 + \\epsilon) A_{\\kappa}) - \\min (p_t A_{\\kappa}, \\text{clip}(p_t, 1 - \\epsilon, 1 + \\epsilon) A_{\\kappa})]$ $\\ \\ p_t = \\frac{\\pi_{\\theta_a}(\\mathbf{a} | \\mathbf{s}_t)}{\\pi_{\\theta_a, \\kappa}(\\mathbf{a} | \\mathbf{s}_t)}$$\n$\n$\\begin{aligned}\nA_{\\kappa} &= A^a(\\mathbf{a}, \\mathbf{s}_t) \\\\\nA &= A^v(\\mathbf{a}, \\mathbf{s}_t) + \\lambda A_{\\text{ins}}(\\mathbf{a}, \\mathbf{s}_t)\n\\end{aligned}$ (5)\nwhere $\\lambda$ denotes a hyperparameter that signifies the degree of exploration. This objective function is designed to leverage both the victim's value function and the intrinsic value of exploration, aiming to expeditiously navigate towards a state that maximizes expected rewards. Consequently, $r_{\\text{ins}}$ can be internalized as the advantage function of PPO, with $\\lambda$ balancing exploitation and exploration, and its value setting is referenced to [21]."}, {"title": "Algorithm 1 Adversarial Attack", "content": "Require: $V^v_{\\theta_v}$: state value of the victim; $\\rho$: target network of the RND; $\\hat{\\rho}$: predictor network of the RND; $V_{\\text{ins}}$: state value of the intrinsic reward; $V^a_{\\theta_a}$: state value of the attacker; $\\pi_{\\theta_a}$: attacker policy;\n1: for $n$ = 1, 2, ..., $N$ do\n2:  while not done do\n3:   $s_t$ = env.step($v$, $a$)\n4:   Collect trajectory: $T$.append($s_t$)\n5:  end while\n6:  Compute $r_{\\text{ins}}$ in each step of $T$ $\\triangleright$ Based on Eq.4\n7:  for $i$ = 1, 2, ..., $T$ in $T$ do\n8:   $A^a = r^a + \\gamma V^a(\\mathbf{s}_{t+1}) - V^a(\\mathbf{s}_t)$\n9:   $A^v = r^v + \\gamma V^v(\\mathbf{s}_{t+1}) - V^v(\\mathbf{s}_t)$\n10:  $A_{\\text{ins}} = r_{\\text{ins}} + \\gamma V_{\\text{ins}}(\\mathbf{s}_{t+1}) - V_{\\text{ins}}(\\mathbf{s}_t)$\n11: end for\n12: Update $\\pi_{\\theta_a}$ by minimizing the loss $\\triangleright$ Based on Eq.5\n13: Update $V^v_{\\theta_v}$, $V_{\\text{ins}}$, $V^a_{\\theta_a}$ by minimizing the TD error\n14: Update $\\hat{\\rho}$ by minimizing the loss $\\triangleright$ Based on Eq.4\n15: end for\n16: return $T$"}, {"title": "C. Adversarial Defense", "content": "Upon successful execution of the adversarial attack training, that is, once the policy governing the attacker has satisfied the pre-established criteria for convergence, the network parameters attributed to the attacker are henceforth frozen. Concurrently, the victim's parameters are thawed to learn defensive strategy against the onslaught of the well-trained attacker. Similarly, inverting Eq.1 as follows:\n$J = \\arg \\min_{\\theta_v} (V^a(\\pi_{\\theta_a}(s)) - V^v_{\\pi_{\\theta_v}}(s))$ (6)\nwhere the parameters $\\theta_a$ are frozen, meaning $\\pi_{\\theta_a}$ is fixed, while $\\theta_v$ is thawed to learn to minimize Eq.6.\nNote that the victim can be any construct, but the PPO is adopted as the model to assess the potency of the adversarial training."}, {"title": "IV. EXPERIMENT", "content": "This study selects three scenarios for experiments. The simulation is conducted on a desktop PC equipped with a CPU Core i7 and a GPU NVIDIA 4070 Ti, using the highway-env [22]. This section details the experiment setup, research questions, results, and analysis."}, {"title": "A. Experiment Setup", "content": "1) Scenario Setup: The experiments set up three typical interactive scenarios, as illustrated in Fig.2, all of which are interactive dual-vehicle intersections that are recognized as hotspots for vehicular collisions. The black attacker (referred to as the traffic vehicle) is equipped with an adversarial protocol, $\\pi_{\\theta_a}$, enabling it to methodically engineer safety-critical situations that challenge the response robustness of the victim (referred to as the target AV dominated by $\\pi_{\\theta_v}$)."}, {"title": "B. Research Questions", "content": "Prior to the initiation of experimental procedures, we have articulated three research inquiries to steer the experimental design and execution:\nRQ.1. What is the efficacy of the VCAT in supporting adversarial attacks?\nRQ.2. Does the VCAT provide a superior level of resilience against adversarial maneuvers compared to others?\nRQ.3. How does each component of the VCAT contribute to the attack capability (i.e., ablation studies)?"}, {"title": "C. Experiment Result", "content": "1) RQ1. Efficacy of Adversarial Attack: Metrics. The crash rate characterizes the efficiency of generating safety-critical collisions w.r.t. the attack method [24]. A more rapid increase in the crash rate signifies greater efficiency. To measure the coverage of attack methods, t-SNE [21] is used to visualize all action vectors from the slice trajectories of the victim interacting with different attackers in 2-D space. The wider the coverage of t-SNE, the richer the behaviors activated by $\\pi_{\\theta_a}$, and the more vulnerabilities exposed. The number of crashes is another metric specifically used to measure the diversity of different types of edge scenarios [29]; the richer, the better. For the features of all the crashes, we distinguish four categories to examine the richness of the scenarios generated.\nResults. Fig.3 shows the crash rates under the three scenarios. The following characteristics can be identified: 1) Many baseline methods struggle to form effective attacks with the sparse incentives, prone to mode collapse in the limited time, such as DDPG, PPO, SAC, etc., in the first scenario. The proposed method, however, can avoid this issue, with the crash rate rising to a high level. 2) The proposed adversarial attack method experiences a distinct \"V\"-shaped phase of decline followed by an increase during early stages, as emphasized by the orange V-shaped arrows. Fig.4 presents the 2-D t-SNE visualization of the victim's action vector. It can be observed that the data distribution of the proposed is more widespread, suggesting that, compared to other counterparts, it can activate a richer policy within the victim, helping to uncover more vulnerabilities. Fig.5 illustrates the number of crashes during adversarial attack training. The proposed method, although not the most prevalent in each category, exhibits the best average performance across the three scenarios.\nAnalysis. The method introduced herein adeptly circumvents mode collapse and assimilates potent adversarial patterns, achieving a higher crash rate. The V-shaped feature in Fig.3 and the extensive data distribution in Fig.4 further demonstrate the enhanced exploration capability of our approach without the exploitation of the internal knowledge within the victim. Although the proposed does not consistently achieve the highest crash rate, as seen in the second scenario where it performs slightly worse than TD3 and DDPG, it improves the learning efficiency of RL under the sparse incentive condition, maintaining a balanced exploration and exploitation, especially suitable for such rare safety-critical conditions. For instance, DDPG exhibits mode collapse in the other two scenarios."}, {"title": "preferable.", "content": "To test the effectiveness of adversarial training, cross-training and validation are employed. Taking the second column in Tab.II as an example, the adversarial training method uses MC, followed by the validation method using the proposed, to test whether the victim can withstand the attack from the proposed after being trained in MC.\nResults. We selected MC, DDPG, and TD3 as baselines and compared four cross-adversarial training and validation categories. 1) AT: MC+Val.: Prop.: Despite training under MC, the victim exhibits a lower non-crash rate when confronted with the proposed attack, suggesting inadequate training; 2) AT: DDPG+Val.: Prop.: Following adversarial training with DDPG, the non-crash rate generally increases compared to the MC one, except in the first scenario where it fails; 3) AT: Prop.+Val.: MC: Training with the proposed method results in a consistently high non-crash rate, indicating that the victim agent can effectively handle universal scenarios; 4) AT: Prop.+Val.: TD3: When the validation method is switched to TD3, the non-crash rate remains high, demonstrating that the proposed training method is robust against maliciously trained attacks.\nAnalysis. The proposed method effectively uncovers a comprehensive attack space, encompassing a broader range of edge scenarios. Adversarial training with this approach significantly enhances the victim's robustness, enabling it to effectively handle universal MC scenarios and resist TD3's malicious attacks to a large extent. However, despite successful adversarial training, other methods exhibit limited policy action activation exploration, thereby constraining their generalization performance.\n3) RQ3. Ablation Studies: We focus on the ablation studies of attacking efficacy. Ablation baseline:"}, {"title": "PPO", "content": "The raw PPO adversarial attack method;\nPPO-VA: Vulnerability-aware PPO, in which the curiosity exploration hyperparameter is set to zero, i.e., $\\lambda$ = 0;\nProposed: The full method introduced in this paper, $\\lambda$ = 0.2.\nResults. The ablation experiment results are shown in Tab.III. MC is clearly inferior to the PPO method. However, the PPO still exhibits low attack efficiency, with a maximum of only about 21.6%. When the vulnerability-aware module is incorporated, the improvement in the crash rate is minimal and even decreases, with a maximum increase of only about 1.3%. When the proposed method is fully implemented, the crash rate significantly increases, particularly achieving a high crash rate of 83.4% in the first scenario.\nAnalysis. The utility of using the vulnerability-aware module alone is limited. This is because without the introduction of the exploration mechanism, it merely weights the states where the attacked victim may have vulnerabilities. However, some error exists in the estimated reward (see Eq.3), making it difficult to achieve improvements using only the VAN. The curiosity mechanism must be combined to explore a larger space; otherwise it will result in excessive exploitation."}, {"title": "V. CONCLUSIONS AND FUTURE WORKS", "content": "This paper proposes a vulnerability-aware and curiosity-driven adversarial training (VCAT) framework to overcome the challenge of adequately enhancing exploration while achieving a balance with exploitation, especially in the sparse, safety-critical scenarios. A pioneering adversarial training framework is constructed, consisting of two stages: adversarial attack and adversarial defense, to enhance the robustness of autonomous driving. In the adversarial attack phase, a vulnerability-aware and curiosity-driven module that enhances attack robustness and efficacy is introduced, enabling the traffic attacker to learn to generate sufficient rare safety-critical data. In the adversarial defense phase, the autonomous vehicle victim gradually learns how to defend against malicious attacks from the pretrained attacker through interactions. Experimental results demonstrated that the proposed adversarial training method can significantly better enhance the robustness of autonomous driving compared to other counterparts.\nFuture work will focus on incorporating real-world data into the training process, expanding the range of adversarial scenarios, and strengthening the system's resilience against adaptive adversaries."}]}