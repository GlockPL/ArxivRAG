{"title": "Safe Reinforcement Learning for Real-World Engine Control", "authors": ["Julian Bedeia", "Lucas Koch", "Kevin Badalian", "Alexander Winkler", "Patrick Schaber", "Jakob Andert"], "abstract": "This work introduces a toolchain for applying Reinforcement Learning (RL), specifically the Deep Deterministic Policy Gradient (DDPG) algorithm, in safety-critical real-world environments. As an exemplary application, transient load control is demonstrated on a single-cylinder internal combustion engine testbench in Homogeneous Charge Compression Ignition (HCCI) mode, that offers high thermal efficiency and low emissions. However, HCCI poses challenges for traditional control methods due to its nonlinear, autoregressive, and stochastic nature.\nRL provides a viable solution, however, safety concerns \u2013 such as excessive pressure rise rates \u2013 must be addressed when applying to HCCI. A single unsuitable control input can severely damage the engine or cause misfiring and shut down. Additionally, operating limits are not known a priori and must be determined experimentally. To mitigate these risks, real-time safety monitoring based on the k-nearest neighbor algorithm is implemented, enabling safe interaction with the testbench.\nThe feasibility of this approach is demonstrated as the RL agent learns a control policy through interaction with the testbench. A root mean square error of 0.1374 bar is achieved for the indicated mean effective pressure, comparable to neural network-based controllers from the literature. The toolchain's flexibility is further demonstrated by adapting the agent's policy to increase ethanol energy shares, promoting renewable fuel use while maintaining safety.\nThis RL approach addresses the longstanding challenge of applying RL to safety-critical real-world environments. The developed toolchain, with its adaptability and safety mechanisms, paves the way for future applicability of RL in engine testbenches and other safety-critical settings.", "sections": [{"title": "1. Introduction and Motivation", "content": "Reinforcement Learning (RL) is a powerful Machine Learning (ML) paradigm which offers distinct advantages over traditional methods in the context of adaptive control.\nIts model-free algorithms eliminate the need for explicit system modeling, thus significantly reducing engineering effort, and its policies - often represented as artificial neural networks (ANNs) \u2013 enable rapid execution, making them suitable for real-time applications. Furthermore, RL agents can uncover hidden patterns in the environment, potentially surpassing domain experts' knowledge (Badalian et al., 2024; Picerno et al., 2023).\nCentral to RL's effectiveness is its learning mechanism based on interactions with the environment. By evaluating and refining its policy based on feedback from the environment, an RL agent can adapt to dynamic, high-dimensional systems without relying o.n precise models. These characteristics make RL a compelling solution for complex control problems.\nHowever, applying RL in real-world settings, particularly in safety-critical systems, remains challenging. RL's reliance on exploration to optimize behavior inherently involves the risk of unsafe or suboptimal actions during the learning process, which can compromise safety by destabilizing the system, causing mechanical damage or even threatening human health. These safety concerns represent a major barrier to deploying RL in real-world environments (Dulac-Arnold et al., 2021) and making it an active area of research (Kwon and Kwon, 2023).\nThis challenge is particularly evident in internal combustion engine control, where RL has already demonstrated its potential for automated function development (Koch et al., 2023), boost pressure control (Hu et al., 2019), and emission reduction (Picerno et al., 2023). However, these RL applications are often limited to virtual environments due to the aforementioned concerns. To address this, additional measures must be implemented to monitor the agent's actions to guarantee safety. In (Norouzi et al., 2023), a safe RL approach for emission control in diesel engines using the Deep Deterministic Policy Gradient (DDPG) algorithm is proposed, where actions are constrained through a quadratic programming solver to prevent unsafe actions. Nevertheless, this approach remains confined to simulation.\nIn contrast, real-world applications of RL for engine control remain exceedingly rare. In (Maldonado et al., 2024), a control policy for adjusting fuel injection is learned using Q-Learning, though the action space is limited to a single action with a narrow range, eliminating the need for additional safety mechanisms. In (Hu and Li, 2021), a Deep Q Network (DQN) is employed for boost control of a real-world diesel engine using a safety shield presented in (Alshiekh et al., 2018). However, employing DQN limits their method do discrete action spaces. To the best of the authors' knowledge, there have been no RL applications for real-world combustion engines involving multiple actions and continuous action spaces.\nTo overcome these limitations, in this work, we introduce DDPG \u2013 suited for continuous state and action spaces \u2013 with a multi-dimensional safety monitoring to preemptively identify unsafe actions in real-time, preventing them from being applied. As an exemplary application, we consider transient load control of an engine operating in Homogeneous Charge Compression Ignition (HCCI) mode. HCCI is a promising low-temperature combustion technique that achieves both high thermal efficiency and low emissions (Li et al., 2001; Kulzer et al., 2009). Unlike conventional spark-ignition and compression-ignition engines, HCCI utilizes the auto-ignition of a homogeneously mixed charge of air, gasoline and residual gas. The latter is trapped in the cylinder by negative valve overlap (NVO) and transferred into the next combustion cycle, raising the mixture temperature. This results in rapid, low-temperature combustion, reducing nitrogen oxide and particulate matter emissions (Yao et al., 2009; Brassat, 2013; Wick et al., 2018) while increasing efficiency.\nHowever, controlling HCCI is challenging due to nonlinearities and high cyclic variability (Hellstr\u00f6m et al., 2012), which arise from autoregressive coupling through transfer of residual gas from cycle to cycle. This can lead to stochastic outlier cycles, characterized by incomplete combustion or misfires. As a result, traditional control methods, such as rule-based or model-free controllers (Wick et al., 2018, 2019; Gordon et al., 2019), often encounter difficulties in maintaining operational stability and efficiency under varying loads and conditions. Although model predictive control (MPC) has shown potential for HCCI control (Albin et al., 2015; Bengtsson et al., 10/4/2006 - 10/6/2006; Ebrahimi and Koch, 2018; Nuss et al., 2019; Chen et al., 2023), the key challenge is to identify an accurate model. Due to real-time constraints, these models often need to be simplified, compromising their precision.\nIn response to these challenges, the HCCI research field has increasingly adopted learning-based approaches. Given the high-dimensional, multiple-input, multiple-output behavior and nonlinear characteristics of HCCI, data-driven methods, particularly those employing ANNs, have proven to be promising solutions. These controllers often rely on cycle-integral values to characterize combustion, such as the total heat released Q, the combustion phasing \u03b150 defined as the crank angle where 50% of the fuel has burned \u2013 and the indicated mean effective pressure (IMEP), representing the engine's load. Successful control implementations with ANNs utilize Extreme Learning Machines (Vaughan and Bohac, 2013) and the inversion of the system dynamics (Wick et al., 2020; Bedei et al., 2023a,b). Recent advancements have further enabled the integration of recurrent neural networks (RNNs) with long short-term memory (LSTM) into nonlinear MPC frameworks, significantly enhancing control performance in HCCI applications (Gordon et al., 2024).\nRL extends these traditional learning-based methods by enabling agents to learn directly through interaction with the environment. Unlike other data-driven paradigms, RL combines data generation with the learning of an optimized control policy, allowing controllers to adapt effectively to real-time variations in HCCI combustion dynamics. The nonlinear, autoregressive, and stochastic nature of HCCI, combined with the lack of sufficiently accurate models, necessitates data generation through direct interaction with the engine (Wick et al., 2020). This real-world interaction is essential for capturing the complex cross-couplings between states and actions, highlighting RL's significant potential in this application.\nMoreover, RL's data generation capability facilitates transfer learning, allowing agents to adapt to system drifts, new boundary conditions, or changing objectives without starting the learning process from the beginning. For combustion engines, RL can, for instance, directly explore the behavior of untested renewable fuels in a testbench environment by leveraging previously trained policies. This eliminates the need for entirely new extensive datasets, accelerating and refining assessments of renewable fuels directly within a real-world setting. This adaptability surpasses the capabilities of traditional control methods, presenting novel opportunities for research and development.\nTo fully realize these potential benefits of RL, safe exploration within the real-world environment is required. This work introduces a safe RL approach designed to ensure safe interaction within such environments. First, we outline the RL fundamentals and the experimental setup, followed by the development of a toolchain based on the Learning and Experiencing Cyclic Interface (LExCI), a free and open-source tool, developed in (Badalian et al., 2024), which enables RL with embedded hardware. This toolchain is integrated into the HCCI testbench to facilitate RL in a real-world setting. We then detail the methodology employed for safety monitoring to ensure operational safety. Finally, we validate the toolchain by comparing it to an ANN-based reference strategy developed in (Bedei et al., 2023a). Additionally, we demonstrate the transfer learning abilities by adaptation of the agent's policy to increase the proportion of renewable fuels, specifically ethanol, substituting part of the gasoline \u2013 highlighting potential future directions for RL research in the context of real-world engine control."}, {"title": "2. Reinforcement Learning Fundamentals", "content": "RL is based on the Markov Decision Process (MDP), which models the interaction between an agent \u2013 a decision-making entity that selects actions to maximize a cumulative reward \u2013 and its environment. The environment is represented by a state space S, an action space A, transition probabilities P (si | Si\u22121, \u0111i) and rewards r(si-1, di, si). At each discrete time step, in case of HCCI control each combustion cycle i, the agent observes the current state -1, selects actions a\u2081, resulting in states and receives a reward ri, which evaluates the quality of the chosen actions. From this, an experience tuple T\u2081 = (si-1, di, Si, ri, di) is formed, where di is a binary termination indicator marking the end of an episode 8 = (T1, T2, ..., Tn), which is a sequence of consecutive experiences T.\nThe agent's goal is to find an optimal policy \u03bc*, which maximizes its return G over time. The return is the cumulative reward, computed using a discount factor y \u2264 1, which weights future rewards compared to immediate ones:\n$G_{i} = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{i+k}$    (1)\nTo iteratively update the policy in order to maximize the return, typically an evaluation function, such as the action-value function (Q-function), is used. The Q-function describes the expected return when taking a specific action di in a given state 3-1 and then following the policy \u03bc:\n$Q(\\vec{s}_{i-1}, a_{i}) = \\mathbb{E}_{\\mu} \\big[G_{i} | S_{0} = \\vec{s}_{i-1}, A_{0} = a_{i}\\big] = r_{i}(\\vec{s}_{i-1}, a_{i}) + \\gamma \\mathbb{E}_{\\mu} \\big[\\sum_{k=0}^{\\infty} \\gamma^{k}r_{i+k} | S_{0} = \\vec{s}_{i}, A_{0} = a_{i} \\big]$ (2)\nTo apply RL to HCCI control, the specific problem requirements lead to the following considerations that must be taken into account when selecting an RL algorithm:\n1. Accuracy of existing process models insufficient: Model-free approach required.\n2. Capability to leverage existing data for offline learning.\n3. High data efficiency for reduced training time in a real-world environment.\n4. Stability and robustness of the learning process.\n5. Suitability for continuous state and action spaces.\nThe DDPG algorithm is a model-free, off-policy, actor-critic algorithm that satisfies the requirements outlined above. Specifically, it is model-free, meaning it does not require a process model and can learn from direct interactions with the environment, making it ideal for the control of HCCI engines. Additionally, as an off-policy algorithm, DDPG is capable of leveraging existing data through its experience replay buffer, enabling it to learn also from data that have not been generated with the agent's policy itself. The replay buffer also ensures high data efficiency and improved convergence behavior (Lin, 1992), allowing DDPG to learn from relatively few interactions with the environment, which is crucial for reducing training time, especially in real-world settings. Moreover, DDPG is designed to work in continuous state and action spaces, making it particularly suited for real-time control of processes like HCCI, where both states and actions are continuous. Therefore, DDPG is employed for HCCI control in the following. The key features of the DDPG algorithm and its mathematical foundations are discussed in detail in (Lillicrap et al., 2015)."}, {"title": "3. Experimental Setup and Toolchain Integration", "content": "This study utilizes a single-cylinder research engine (SCRE) with a displacement of VH = 0.5 L and a compression ratio of 12. The SCRE is equipped with two direct injectors for fuel and ethanol, respectively. Additionally, the SCRE features a fully variable electromechanical valve train (EMVT), where the opening and closing of the valves is achieved by alternating energization of two solenoid coils. This enables HCCI operation with NVO to leverage internal exhaust gas recirculation, contributing to elevated mixture temperatures that support auto-ignition during the compression phase. It also enables throttle free operation, reducing gas exchange losses significantly. Both the fuel injections and NVO can be adjusted on a cycle-to-cycle basis, making them suitable variables for process control.\nThe SCRE is controlled using a dSPACE Microautobox (MABX) III (1403/1513/1514) with the Multi I/O Board DS1552B1 (dSPACE GmbH, 2024). In addition to a quad-core ARM Cortex-A15 real-time processor running at 1.4 GHz, this control unit features a Xilinx Kintex-7 XC7K325T Field-Programmable Gate Array (FPGA) with a task rate of 12.5 ns. Furthermore, a Raspberry Pi 400 (RPI) (Raspberry Pi Foundation, 2024), equipped with a quad-core ARM Cortex-A72 processor with a base clock speed of 1.8 GHz, is integrated into the testbench.\nThe algorithms implemented in this research are allocated between the FPGA, the processor and the RPI, depending on the specific requirements of each calculation."}, {"title": "4. Problem Formulation", "content": "4.1. Definition of the State-Action-Space\nA requirement for applying an MDP is fulfilling the Markov property, which states that transition probabilities P (si | Si-1, \u0111i) depend solely on the current state 3-1 and not on previous ones. Consequently, the current state must capture all relevant information for predicting future states. Prior research has demonstrated, via partial autocorrelation, that the HCCI process memory in stable operation spans only one combustion cycle (Stuart Daw et al., 2007; Andert et al., 2018). Therefore, it is assumed that HCCI fulfills the Markov property in stabilized, closed-loop operation. Thus, for state description, it is sufficient to use cycle i \u2013 1 to determine actions for cycle i.\nPrior studies indicate that the combustion phasing @50,i-1, IMEP\u00a1-1 and heat release Qi-1 adequately represent the current thermodynamic mixture state for the purpose of combustion control (Wick et al., 2019; Nuss et al., 2019; Bedei et al., 2023a). Additionally, the maximum pressure gradient dpmax,i\u22121 is included, as it must be constrained to mitigate mechanical stress on the engine and improve acoustic behavior. In addition to these pressure-based variables, features of the ion current \u2013 the maximum UIon,Max and integral IUIon are used, as they have been shown to enhance control performance in the literature (Bedei et al., 2023b). Finally, the load setpoint for cycle i, IMEPSet,i, is provided to the agent to address transient load control, while the previous cycle's target load IMEPSet,i-1 supplies information on any load steps in the current cycle.\nThe action space includes adjusting the NVO duration through the angle interval anvo,i, which specifically controls the amount of fresh air and residual gas fraction. Additionally, both gasoline tGas,Inj,i and ethanol tEth,Inj,i injection durations are applied, allowing the engine's power output to be distributed between the two fuels:\n$S_{i-1} = \\begin{bmatrix}\\alpha_{50,i-1} \\\\ Q_{i-1} \\\\ IMEP_{i-1} \\\\ dp_{Max,i-1} \\\\ UIon.Max,i-1 \\\\ IUIon,i-1\\\\ IMEP_{Set,i-1} \\\\ IMEP_{Set,i} \\end{bmatrix}  \\quad  a_{i} = \\begin{bmatrix} \\Delta NVO,i \\\\ Gas,Inj,i \\\\ Eth,Inj,i  \\end{bmatrix}$  (9)\n4.2. Reward Function\nDefining an effective reward function is a key challenge in RL, as it provides feedback on the quality of the agent's actions and guides it toward an optimal policy. Thus, a well-designed reward function can improve training efficiency and accelerate convergence (Hu et al., 2020). Beyond the primary goal of precise load tracking, additional objectives like safety, efficiency and minimizing process fluctuations are incorporated."}, {"title": "5. Results and Discussion", "content": "To validate our safe RL approach, first, an initially untrained agent is trained purely through direct interaction with the real-world testbench environment. Secondly, the agent's adaptability to changing objectives is demonstrated.\n5.1. Policy Training in the Real-World Environment\nIn this feasibility study, the agent learns exclusively through experiences gathered from direct interaction with the real-world testbench environment. Those hyperparameters were selected iteratively by manual tuning until the agent's performance was satisfactory, ensuring they effectively supported the tracking, stability, and safety objectives defined in Section 4.2."}]}