{"title": "Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis", "authors": ["Taihang Hu", "Linxuan Li", "Joost van de Weijer", "Hongcheng Gao", "Fahad Shahbaz Khan", "Jian Yang", "Ming-Ming Cheng", "Kai Wang", "Yaxing Wang"], "abstract": "Although text-to-image (T2I) models exhibit remarkable generation capabilities, they frequently fail to accurately bind semantically related objects or attributes in the input prompts; a challenge termed semantic binding. Previous approaches either involve intensive fine-tuning of the entire T2I model or require users or large language models to specify generation layouts, adding complexity. In this paper, we define semantic binding as the task of associating a given object with its attribute, termed attribute binding, or linking it to other related sub-objects, referred to as object binding. We introduce a novel method called Token Merging (ToMe), which enhances semantic binding by aggregating relevant tokens into a single composite token. This ensures that the object, its attributes and sub-objects all share the same cross-attention map. Additionally, to address potential confusion among main objects with complex textual prompts, we propose end token substitution as a complementary strategy. To further refine our approach in the initial stages of T2I generation, where layouts are determined, we incorporate two auxiliary losses, an entropy loss and a semantic binding loss, to iteratively update the composite token to improve the generation integrity. We conducted extensive experiments to validate the effectiveness of ToMe, comparing it against various existing methods on the T2I-CompBench and our proposed GPT-40 object binding benchmark. Our method is particularly effective in complex scenarios that involve multiple objects and attributes, which previous methods often fail to address. The code will be publicly available at https://github.com/hutaihang/ToMe.", "sections": [{"title": "1 Introduction", "content": "Text-to-image generation has seen significant advancements with the recent introduction of diffusion models [57, 59, 62], with their capabilities of generating high-fidelity images from text prompts. Despite these achievements, aligning the generated images with the text prompts, which is referred to as semantic alignment [30, 43], remains a notable challenge. One of the most common issues observed in existing text-to-image (T2I) generation models is the lack of proper semantic binding, where a given object is not properly binding to its attributes or related objects. For example, as illustrated in Fig. 1, even a state-of-the-art T2I model such as SDXL [53] can struggle to generate content that accurately reflects the intended nuances of text prompts. To address the persistent challenges of aligning T2I diffusion models with the intricate semantics of text prompts, a variety of enhancement strategies [35, 46, 87] are proposed, either by optimizing the latent representations [69, 82, 83], guiding the generation by layout priors [54, 71, 85] or fine-tuning the T2I models [21, 34]. Despite these advancements, these methods still encounter limitations, particularly in generating high-fidelity images involving complex scenarios where an object is binding with multiple objects or attributes.\nIn this paper, we categorize semantic binding into two categories. First, attribute binding involves correctly associating objects with their attributes, a topic that has been studied in prior work [58]. Second, object binding, which entails effectively linking objects to their related sub-objects (for example, a 'hat' and 'glasses'), is less explored in the existing literature. Previous methods often struggled to address this aspect of semantic binding. One of the main problems is the misalignment of objects with their corresponding sub-objects. Existing solutions address this through an explicit alignment process of the attention maps [7, 43] or by factorizing the generation projects into layout phases and generation phase [55]. In this paper, we propose a simple solution to the attention alignment problem called token merging (ToMe). Instead of multiple attention maps, which can be misaligned, we join these objects in a single composite token that represents the object and its attributes and sub-objects. This composite token has a single cross-attention map that ensures semantic alignment. The composite token is simply constructed by summing the CLIP text embeddings of the various tokens it represents. For example, the phrase \u201ca dog with hat\u201d is abbreviated as \u201ca dog*\u201d by aggregating the text embeddings corresponding to the last three words, as shown in Fig. 4. To justify the applied embedding addition in ToMe, we experimented with the semantic additivity of the text embeddings (in Fig. 3). Furthermore, to mitigate potential semantic misalignment in the end tokens from the long sequences, we propose end token substitution (ETS) technique.\nAs the T2I generation predominantly determines the layout during earlier phases [27], we introduce an entropy loss and a semantic binding loss to update the token embeddings in early steps, integrating ToMe with an iterative update for the composite tokens. The entropy loss is defined as the entropy of the cross-attention map corresponding to the updated composite token. This loss aims to enhance generation integrity by ensuring diverse attention across relevant areas of the image, thereby pre-venting focusing on non-essential regions. The semantic binding loss encourages the new learned token to infer the same noise prediction as the original corresponding phrase. This alignment further reinforces the semantic coherence between the text and the generated image.\nOur final method ToMe is quantitatively assessed using the widely adopted T2I-CompBench [31] and our proposed GPT-40 [1] object binding benchmark. Comparative evaluations against various types of approaches reveal that ToMe outperforms them by a significant margin. Remarkably, our approach is user-friendly, requiring no dependence on large language models or specific layout information. In qualitative evaluations, we notably achieve superior generation quality, particularly in scenarios involving multi-object multi-attribute generation. This further underscores the superiority of our method. In summary, the main contributions of this paper are as follows:\n\u2022 We analyze the problem of semantic binding, and highlight the role of the [EOT] token (Fig. 2), and the problems with misaligned cross-attention maps (Fig. 7). In addition, we explore token additivity as a possible solution (Fig. 3).\n\u2022 We introduce a training-free approach called Token Merging (Fig. 4), denoted as ToMe, as a more efficient and robust solution for semantic binding. It is further enhanced by our proposed end token substitution and iterative composite token updates techniques.\n\u2022 In experiments conducted on the widely used T2I-CompBench benchmark and our GPT-40 object binding benchmark, we compared ToMe with various state-of-the-art approaches and consistently outperformed them by significant margins."}, {"title": "2 Related works", "content": "A critical drawback of current text-to-image models is related to their limited ability to faithfully represent the precise semantics of input prompts, commonly referred to as semantic alignment. Various studies have identified common semantic failures and proposed mitigation strategies. They can be roughly categorized into four main streams.\nOptimization-based methods primarily adjust text embeddings [20, 65] or optimize noisy signals to strengthen attention maps [26, 48, 63, 69, 82, 83]. These methods are basically inspired by the observations from text-based image editing methods [27, 40, 64, 66], suggesting that the layouts of objects are determined by self-attention and cross-attention maps from the UNet of the T2I diffusion models. For example, Attend-and-Excite [7] improves object existence by exciting the attention score of each object. Divide-and-Bind [43] improves by maximizing the total variation of the attention map to prompt multiple spatially distinct attention excitations. SynGen [58] syntactically analyzes the prompt to identify entities and their modifiers, and then uses attention loss functions to encourage the cross-attention maps to agree with the linguistic binding reflected in the syntax. A-star [2] proposes to minimize concept overlap and change in attention maps through iterations. Composable Diffusion [45] decomposes complex texts into simpler segments and then composes the image from these segments. Structure Diffusion [20] attempts to address this by leveraging linguistic structures to guide the cross-attention maps. Rich-Text [24] enriches textual prompts by incorporating various formatting controls and decomposes the generation task into merging inferences from multiple region-based diffusions. However, these methods often fail in complex scenarios that generate multiple objects or multiple attributes.\nLayout-to-Image methods [4, 9, 14, 17, 25, 32, 36, 47] are widely using layouts, particularly in the form of bounding boxes or segmentation maps, as a popular intermediary to bridge the gap between text input and the generated images. For example, BoxDiff [73] encourages the desired objects to appear in the specified region by calculating losses based on the maximum values in cross-attention maps. Similarly, Attention-Refocusing [52] modifies both cross-attention and self-attention maps to control object positions. BoxNet [67] first trains a network to predict the box for each entity that possesses the attribute specified in the prompt, and then force the generation to follow the attention mask control. Additionally, InstanceDiffusion [68] enhances text-to-image models by providing extra instance-level control. There are also finetuning methods [5, 42, 50, 79] allow for additional layout conditions after fine-tuning over pair images, which are not specifically designed to solve the semantic alignment problem. Despite their promise, these methods obviously prolong the training time. Furthermore, the application of layout priors is challenging when it comes to global background descriptions or abstract elements. This limitation constrains the versatility of these techniques, making it difficult to deploy them effectively across real scenarios where non-specific spatial arrangements are crucial.\nLLM-augmented methods are mainly following text-to-layout-to-image generation pipelines [15, 23, 33, 44, 55, 65, 80, 81, 86], first to generate layouts from large language models (LLMs) and force the T2I generations to follow this guidance as the previous layout-guided methods. Some methods, such as RPG [75] and MuLan [39], harness the powerful chain of thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models.\nFinetuning-based methods [13, 76] update the model parameters over huge datasets to augment the semantic alignment. Among them, CoMat [34] proposes an end-to-end fine-tuning strategy for text-to-image diffusion models by incorporating image-to-text concept matching. ELLA [30] equips text-to-image diffusion models with powerful Large Language Models (LLM) to enhance text alignment by bridging these two pre-trained models with trainable semantic alignment connectors. More recently, Ranni [21] improves T2I generation by bridging the text and image with a semantic panel with LLMs and is fine-tuned over an automatically prepared semantic panel dataset. There"}, {"title": "3 Methods", "content": "Semantic binding in T2I generation refers to the crucial requirement of establishing accurate associa-tions between objects and their relevant attributes or related sub-objects. This process avoids semantic misalignment in the generated images, ensuring that each visual element aligns correctly with its descriptive cues in the text. In this section, we begin by providing the preliminaries. Subsequently, we illustrate the motivation through a series of experimental analyses (Sec. 3.1). Finally, we elaborate on our methods in detail (Sec. 3.2). An illustration of our method ToMe is shown in Fig. 4.\nLatent Diffusion Models. We build our novel approach for semantic alignment on the standard SDXL [53] model. The model is composed of two main parts: an autoencoder (i.e., a encoder & and a decoder D) and a diffusion model (i.e., \u03f5\u03b8 with parameter \u03b8). The model \u03f5\u03b8 is updated by the loss:\n$L_{LDM} := E_{z_t \\sim \\mathcal{E}(x), y, \\epsilon \\sim \\mathcal{N}(0,1), t \\sim Uniform(1,T)} [||\\epsilon - \\epsilon_{\\theta} (z_t, t, \\tau_{\\xi} (P))||^2]$,\nwhere \u03f5\u03b8 is a UNet, conditioning a latent input zt, a text embedding \u03c4\u03be(P) and a timestep t ~ Uniform(1,T). More specifically, text-guided diffusion models aim to generate an image from random noise zt and a conditional input prompt P. To distinguish from the general conditions in LDMs, we itemize the textual condition as C = \u03c4\u03be(P), where \u03c4\u03be is the CLIP text encoder [56]. The cross-attention map is obtained from \u03f5\u03b8(zt, t, C). Let fzt be a feature map output of the network \u03f5\u03b8. We get a query matrix Qt = lo(fzt) with projection network lo. Similarly, given a textual embedding C, we compute a key matrix K = lk(C) with projection network lk. Then the attention map is computed according to: At = softmax(Qt\u00b7KT /\u221ad) where d is the latent dimension, and the cell [At]ij defines the weight of the j-th token on the i-th token."}, {"title": "3.1 Text Embedding Analysis", "content": "To address the semantic binding problem, we concentrate on the text embeddings utilized during the diffusion model generation process, as they predominantly dictate the content of the generated images. For a given text prompt P, it is tokenized by the CLIP text model by padding a start"}, {"title": "3.2 ToMe: Token Merging", "content": "Suppose the initial prompt P contains K entities indicated by noun words and their corresponding tokens as {n1,...,nk}. Each entity is often related to a token with relevant objects or attributes set as (nk, ak). For example, in the sentence \u201ca cat wearing glasses and a dog with a hat\u201d, n1 =<cat>, a1 = {<wearing>,<glasses>}, n2 = <dog>, a2 = {<with>,<a>,<hat>}.\n3.2.1 Token Merging techniques\nThe semantic additivity of token embeddings inspires us to achieve co-expression of entities and attributes by explicitly binding tokens together. We employ element-wise addition to accomplish semantic merging of tokens. For a prompt P containing K entities, we fuse each subject-attribute pair (nk, ak) into \u0109k = nk + \u2211ak , referred to as a composite token. This innovative approach introduces an additional benefit by utilizing a single composite token to condense a lengthy prompt sequence, resulting in a unified cross-attention map, thus avoid semantic misalignment. Such observations are further shown in the ablation study and appendix.\nEnd Token Substitution (ETS). Meanwhile, as the semantic information contained in [EOT] can interfere with attribute expression, we mitigate this interference by replacing [EOT] to eliminate attribute information contained within them, retaining only the semantic information of each subject. For instance, when the prompt is \"a cat wearing hat and a dog wearing sunglasses,\" we use the [EOT] obtained from the prompt \"a cat and a dog\" to replace the original [EOT] . As illustrated in Fig. 4-a, the final text embedding after subject-attribute enhancement and EOT replacement is C=[CSOT,...,c\u2217cat,c\u2217dog,c\u2217EOT,...,c\u2217EOT]. Here, dog* and EOT* respectively denote tokens after token merging and end token substitution.\n3.2.2 Iterative composite Token Update\nSemantic binding loss. As stated in section 3.1, the semantic information of each token embedding is inherently linked. After strengthening the relationship between subjects and their attributes, it becomes crucial to eliminate any irrelevant semantic information within the composite tokens to prevent misrepresentation of attributes. As illustrated in Fig. 4-(b), to ensure that the semantics of the composite tokens correspond accurately to the noun phrases they are meant to represent, we employ a clean prompt as a supervisory signal. Specifically, for a composite token embedding \u0109dog, which corresponds to the noun phrase \"a dog wearing hat\", we aim for the diffusion model to exhibit consistent noise prediction for this composite token and the full phrase. In mathematical terms, this objective can be expressed as ensuring that \u03f5\u03b8(zt,\u0109dog,t) \u2248 \u03f5\u03b8(zt, C, t). This effectively aligns \u2207ztlogP\u03b8(zt|\u0109dog) \u2248 \u2207ztlogP\u03b8(zt|C) [18, 28]. At time step t, we use the semantic binding loss to align token semantics Lsem = \u2211k\u2208[1,K] ||\u03f5\u03b8 (zt, \u0109k, t) \u2013 \u03f5\u03b8 (zt, C, t)||2."}, {"title": "4 Experiments", "content": "4.1 Experimental Setups\nEvaluation Benchmarks and Metrics. We evaluate the effectiveness of ToMe over T2I-CompBench [31], a comprehensive benchmark for open-world compositional T2I generations, encompassing attribute binding and object relationships. We focus on the semantic binding problem, where T2I-CompBench predominantly evaluates through three attribute subsets (i.e., color, shape, and texture). We follow the evaluation protocol [21, 30, 34] that using 300 validation prompts for evaluation under each subset and the BLIP-VQA score[31] as the evaluation metrics. Following that, we adopt the ImageReward [74] model to evaluate human preference scores, which comprehensively measure image quality and prompt alignment. To comprehensively evaluate object binding perfor-mance, we introduce a new GPT-40 Benchmark of 50 prompts using the template \"a [objectA] with a [itemA] and a [objectB] with a [itemB].\". For example, objectA and objectB are objects like \"cat\" and \"dog\" while itemA and itemB are associated items \u201chat\" and \"glasses\u201d. Afterward, we used the multimodal model GPT-40 [1] to compute the consistency score between the generated images and the prompts for objective assessment. More details are available in the Appendix C.5.\nImplementation Details. We used SDXL [53] as our base model. To automate image generation for evaluation, we employed SpaCy [29] for syntactic parsing of prompts to identify each object and its corresponding attributes for token merging. The iterative composite token update is performed during the first 20% of the denoising steps Topt = 0.2T.\nComparison Methods. To evaluate our method's effectiveness, we compared the current state-of-the-art methods. These primarily encompass: (1) state-of-the-art T2I diffusion models, including SDXL [53], Playground-v2 [37] (2) Finetuning-based methods, including CoMat [34], ELLA [30] (3) Optimization-based method SynGen [58] (4) LLM-augmented finetuning-based method Ranni [21]. More comparison results are shown in the Appendix E.\n4.2 Experimental Results\nQuantitative Comparison. As shown in Table 1, ToMe consistently outperforms or performs comparably to existing methods in BLIP-VQA scores across the color, texture, and shape attribute binding subsets, indicating its effectiveness in avoiding attribute confusion. Human-preference scores evaluated through the ImageReward[74] model(note that the model scores are logits and can be negative) suggest that images generated by ToMe can better align with prompts. Specifically, despite ELLA's[30] use of LLama or T5-XL to replace the CLIP Text Encoder for stronger text embeddings, our method still achieves higher BLIP-VQA scores compared to ELLA. The significant improvement in GPT-40 scores also demonstrates the effectivenes of ToMe in object binding.\nQualitative Comparison. Following SynGen [58], we classify the failure cases of attribute binding into three main categories. (i) Semantic leak in prompt, where the attribute ak is not corresponding to its entity nk; (ii) Semantic leak out of prompt, where the attribute ak is describing the background or some entity not referred to in the prompt P; (iii) Attribute neglect, where the attribute ak is totally ignored in the image generation. Fig. 5 presents our qualitative comparison results with other methods. The first three rows show more complex object binding results, while the last two rows demonstrate attribute binding results. The semantic binding errors in images generated by SDXL[53]"}, {"title": "5 Conclusion", "content": "In this paper, we investigate a critical issue in text-to-image (T2I) generation models known as semantic binding. This phenomenon refers to instances where T2I models struggle to accurately interpret and visually bind the related semantics. Recognizing that previous methods often entail extensive fine-tuning of the entire T2I model or necessitate explicit specification of generation layouts by large language models, we introduce a novel training-free approach called Token Merging, denoted as ToMe, to tackle semantic binding issues in T2I generation. ToMe incorporates innovative techniques by stacking up the object token with its relevant tokens into a single composite token. This mechanism eliminate the semantic misalignment by unifying the cross-attention maps. Furthermore, we assist the ToMe with end token substitution, and iterative composite token updates technique to strengthen the semantic binding. In extensive experiments, we quantitatively compare it against various existing methods using the T2I-Compbench and our proposed GPT-40 benchmarks. The results demonstrate its ability to handle intricate and demanding generation tasks more effectively than current methods, especially for object binding cases that are ignored in previous research."}, {"title": "A Limitations", "content": "Since our method is optimized for inference based on SDXL, it inherits some inherent limitations of SDXL. For example, it may produce artifacts in generated images and is unable to create images with complex layouts. Additionally, the ToMe technique relies on the CLIP text encoder to generate text embeddings, which may be subject to the limitations of the encoder itself. For instance, the CLIP encoder might not fully capture all the subtle semantic nuances in the text, which could restrict the performance of ToMe when processing certain types of text prompts. Addressing these limitations and advancing our understanding in these areas will help improve image generation technology."}, {"title": "B Broader Impacts", "content": "ToMe enhances the semantic binding capability in text-to-image synthesis by enhancing text embed-dings. However, it also carries potential negative implications. It could be used to generate false or misleading images, thereby spreading misinformation. If ToMe is applied to generate images of public figures, it poses a risk of infringing on personal privacy. Additionally, the automatically generated images may also touch upon copyright and intellectual property issues."}, {"title": "C Implementation Details", "content": "C.1 Method details\nWe extract the cross attention maps from the first three layers of the decoder in the UNet backbone, which contain rich semantic information, with a resolution of 32 \u00d7 32. For Iterative composite Token Update, since the early timesteps of the denoising process determine the layout of the image[27], we execute it only during the first 20% of the denoising process. All experiments were conducted on an NVIDIA-A40 GPU.\nC.2 Baseline methods implementation\nFor the quantitative comparison in Tab. 1, we used the official implementations of Ranni[21], ELLA[30], SyGen [58], and CoMat[34]. Since the SDXL versions of the Ranni[21], ELLA[30], and CoMat[34] methods have not been open-sourced, we refer to the BLIP-VQA scores reported in their respective papers. SynGen[58], like our method, performs optimization during inference. To ensure a fairer comparison, we adapted SynGen to SDXL.\nC.3 Text embedding analysis\nFig. 9's statistical analysis further demonstrates the information coupling property and semantic additivity of text embeddings. We employed MMDetection[12]and GLIP[38] to detect the probability of specified objects in images, referred to as DetScore, as shown in Fig. 9-(a). Fig. 9-(b) presents statistical results on 100 generated images, showing that the probability of detecting a hat in images generated from the text embedding corresponding to \u201ca dog\u201d is 0%. However, in images generated from the element-wise \"[dog+hat]\" additive embedding, the probability of detecting a hat is 68.61%, which is close to the probability of 73.12% for images generated using the prompt 'a dog wearing a hat'.\nThe information coupling of token embeddings is also reflected in the entropy of cross-attention for each token. Taking the prompt \"a cat wearing sunglasses and a dog wearing a hat\" as an example, we can extract the cross-attn map Ak \u2208 R1024 for each token, averaged over 50 time steps and multiple heads. After normalizing each map to 1.0(i.e., Ak[i] :=\u2211j\u2208[1,32] Ak[i] ), we calculate the token's infomation entropy as \u2211pi\u2208Ak \u2212pi log(pi). As shown in Fig. 9-(c), we conducted statistics on 100 generated images and found that tokens positioned later in the prompt tend to have higher entropy, indicating more dispersed cross-attn maps. This phenomenon might be attributed to CLIP's[56] masked attention mechanism, where each token can interact with all preceding tokens, and tokens\nC.4 Time complexity\n\nC.5 GPT-40 Score\nIn order to better demonstrate the binding ability of our model for complex prompts. We have constructed a set of high-difficulty prompts, where the content primarily uses nouns to describe the subject. We use OpenAI's latest release, GPT-40, to evaluate the quality of images generated by various models because GPT-40 excels in image discernment, allowing for precise evaluation of the generated outputs. As show in Fig. 10, We designed nine scoring levels, ranging from 0 to 100 points,"}, {"title": "D Additional Ablation Studies", "content": "D.1 More Configures and ETS ablation\nAs an example in Fig. 11, the original SDXL (Config.A) suffered from attribute binding errors due to divergent cross-attention maps. When only applying token merging (Config B), the co-expression of entities and attributes resulted in a dog wearing a hat in the image, but the attribute leakage issue remained due to the divergent cross-attention maps. When only applying the entropy loss Lent (Config E), although the cross-attention maps corresponding to each token are more concentrated, they may focus on wrong regions. Only by applying both token merging and Lent techniques (Config\nD.2 Different prompts splice\nIn Sec. 3.2.1, we fuse each object and its corresponding attributes. At this stage, both the object token embedding and the attribute token embedding are derived from the text embedding obtained by processing the same prompt through the CLIP Text Encoder, potentially causing the information between them to be coupled. We also experimented with splicing token embeddings from different prompts, as illustrated in Fig. 13. While keeping other components of ToMe unchanged, the resulting images often exhibit a missing of the object. We hypothesize that this may be due to the lack of contextual semantics between token embeddings from different prompts[8]."}, {"title": "E Additional Results", "content": "As shown in Tab. 4, we have added quantitative comparison results with additional methods. Our method consistently outperforms or is on par with the existing methods. Fig. 14 presents more qualitative comparison results, demonstrating that our method achieves good performance in attribute binding, object binding, and the composite binding of attribute and objects. ToMe can also generate images with subjects or backgrounds featuring multiple attributes(Fig. 14, the last line), in this scenario, we find that using an additional positional loss[19] based on the attention map is effective.\nWe also conduct a user study with 20 participants to enrich the evaluation. Here we compare our method ToMe with SDXL[53], SynGen[58], Ranni [21] and ELLA[30]. As shown in Fig. 15, we ask the participants to rate the semantic binding into 4 levels and calculate the distribution of each comparison method over these four diverse levels. We can observe that our method better achieve the semantic binding performance by mainly distribute in the highest level 1, while the other methods struggle to obtain user satisfactory results."}, {"title": "NeurIPS Paper Checklist", "content": "1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes", "nJustification": "Abstract and Sec. 1\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made", "Limitations\nQuestion": "Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes"}, {"nJustification": "Appendix A\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations", "Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion": "For each theoretical result", "proof?\nAnswer": ["Yes"]}, {"nJustification": "Sec. 3\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022 All the theorems", "Reproducibility\nQuestion": "Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes"}, {"nJustification": "Sec. 4\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments", "reviewers": "Making the paper reproducible is important", "code\nQuestion": "Does the paper provide open access to the data and code", "material?\nAnswer": ["Yes"]}, {"nJustification": "Supplementary Material\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code.\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.\n\u2022 While we encourage the release of code and data", "No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https": "nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\u2022 The authors should provide instructions on data access and preparation", "Setting/Details\nQuestion": "Does the paper specify all the training and test details (e.g.", "results?\nAnswer": ["Yes"]}, {"nJustification": "Sec. 4\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n\u2022 The full details can be provided either with the code", "Significance\nQuestion": "Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\nAnswer: [Yes"}, {"nJustification": "Sec. 4\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments."}]}