{"title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation", "authors": ["Yunhai Feng", "Jiaming Han", "Zhuoran Yang", "Xiangyu Yue", "Sergey Levine", "Jianlan Luo"], "abstract": "Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues. In this paper, we introduce a novel test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a \u201creflection\u201d mechanism it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://reflect-vlm.github.io.", "sections": [{"title": "1. Introduction", "content": "Complex multi-stage manipulation tasks remain a fundamental challenge in robotics (Luo et al., 2024a; Kroemer et al., 2020; Cui & Trinkle, 2021), particularly when they require reasoning about sophisticated physical interactions and their consequences over long time horizons. These tasks often involve intricate sequences of actions where each step must account for physical constraints and potential consequences, making them particularly challenging for planning systems. Success requires not only understanding the immediate effects of actions but also their long-term implications, the ability to adapt plans based on execution outcomes, and generalizing to novel scenarios.\nWhile classical planning approaches, such as task and motion planning (TAMP) (Kaelbling & Lozano-P\u00e9rez, 2011; Garrett et al., 2020a), can in principle address such problems, their reliance on predefined symbolic representations and explicit state estimation makes them difficult to apply in settings without known models that require visual perception (Driess et al., 2020; Wang et al., 2021). This limitation has motivated the search for more flexible approaches to robotic planning. Recent advances in vision-language models (VLMs) have shown remarkable capabilities in processing visual scenes and natural language instructions by leveraging internet-scale knowledge (Chen et al., 2023; Bai et al., 2023; OpenAI, 2024a; Google, 2024; Liu et al., 2023). These models can effectively parse complex visual environments and comprehend high-level task descriptions expressed in natural language, making them promising candidates for robotic planning problems (Driess et al., 2023; Brohan et al., 2023b;a; Shi et al., 2024; Liu et al., 2024a),. However, state-of-the-art VLMs still struggle with complex physical reasoning tasks, and this limitation becomes particularly pronounced when precise physics concepts and long-horizon planning are involved (Gao et al., 2024; Chen et al., 2024).\nIn this paper, we study how to effectively leverage VLMs' Internet-scale knowledge while addressing their limitations in physical reasoning and long-horizon planning. We focus on a challenging class of robotic manipulation problems that involve sequentially manipulating interlocking objects to achieve desired configurations, as illustrated in Fig. 5. These tasks are particularly difficult as they require precise understanding of physical constraints, careful reasoning about action sequences, and the ability to plan over extended horizons while maintaining physical feasibility at each step.\nTo address these challenges, we present a novel test-time computation framework that significantly enhances VLMs' capabilities for multi-stage robotic manipulation tasks. The key insight of our method, ReflectVLM, is that by combining VLMs with a reflection mechanism and targeted post-training, we can create a system that better understands physical constraints and their implications for action planning. We use the term \"reflection\" to refer to a process where a VLM iteratively refines its decisions by critically examining the predicted outcomes of its proposed actions, akin to self-critique methods in large language models (Huang et al., 2024; Wang et al., 2023; Madaan et al., 2024). Our approach introduces two key components: (1) a look-ahead mechanism that uses a diffusion-based dynamics model to generate visual predictions of future states resulting from planned actions, and (2)a reflection process that allows the VLM to critique and refine its planned actions by analyzing these predicted outcomes. This combination of visual prediction and iterative refinement allows the VLM to develop a more sophisticated understanding of physical constraints and improve its decision-making capabilities without requiring extensive retraining.\nExperimental results demonstrate that our approach significantly outperforms both the latest commercial state-of-the-art VLM models and traditional planning approaches like Monte Carlo Tree Search (MCTS) on this class of problems. Notably, our method achieves superior performance compared to post-training techniques such as supervised fine-tuning (SFT) while using the same amount of labeled data and maintaining computational efficiency. The success of our approach suggests that enhancing VLMs with structured reasoning mechanisms at test time can be a powerful strategy for improving their performance on physically-grounded tasks.\nOur primary contribution is the mentioned test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. Through extensive experiments, we demonstrate that our approach not only outperforms existing methods but also maintains computational efficiency. Importantly, while we demonstrate our framework's effectiveness on manipulation tasks, it is designed to be general and can be readily extended to other domains requiring visual understanding and sequential decision-making. This generality suggests broader applications in robotics and autonomous systems where physical reasoning and long-horizon planning are essential."}, {"title": "2. Related Work", "content": "Our framework incorporates a VLM with the reflection mechanism to solve long-horizon robotic planning problems. We therefore survey reflection techniques in the broader context in large models, VLM for robotic planning, as well as existing techniques for solving robot task and motion planning."}, {"title": "2.1. Reflection", "content": "Recent work has shown that large language models can benefit from reflection mechanisms - processes where models iteratively refine their outputs through self-critique and revision (Renze & Guven, 2024; Shinn et al., 2024; Pan et al., 2023; Madaan et al., 2024; Asai et al., 2023; Wang et al., 2023; Huang et al., 2024). For example, Madaan et al. (2024) introduced an iterative refinement approach where models critique and improve their own outputs through self-feedback. Chain-of-thought prompting and its variants (Wei et al., 2022; Wang et al., 2022; Yao et al., 2024) demonstrated that guiding models to show their reasoning process leads to better performance. Similarly, Cheng et al. (2024); Yu et al. (2025) extended such reflection mechanisms to vision-language models.\nHowever, these approaches focus primarily on language-only or visual comprehension tasks, without addressing physical reasoning or robotics applications. Our work extends reflection to long-horizon robotic planning by incorporating a diffusion model that generates imagined future visual states. This allows the VLM to reflect on and revise its plans based on concrete visual predictions rather than relying solely on symbolic reasoning."}, {"title": "2.2. VLM for Robotic Planning", "content": "In robotics, several recent works have explored using VLMs for planning (Driess et al., 2023; Brohan et al., 2023b;a; Hu et al., 2023; Huang et al., 2023; Belkhale et al., 2024; Nasiriany et al., 2024; Liu et al., 2024a; Shi et al., 2024; Wake et al., 2024). However, these approaches either rely on symbolic state representations or make decisions in a single-step manner based only on current observations, without explicitly reasoning about future consequences or utilizing reflection mechanisms.\nWhile ReplanVLM (Mei et al., 2024b) and GameVLM (Mei et al., 2024a) use VLMs to replan robot actions based on execution feedback, they still rely on symbolic state representations rather than visual imagination of future states. Black et al. (2023) utilized a diffusion model to generate future visual states and executed them with a low-level goal-conditioned policy, but did not leverage these predictions for plan reflection or revision. Du et al. (2023) combines a VLM with video prediction for beam search, but suffers from prediction error accumulation and struggles with physics-based reasoning tasks.\nOur framework addresses these limitations by enabling VLMs to imagine and evaluate potential future states through a diffusion-based dynamics model. This allows for sophisticated multi-step planning while maintaining the benefits of VLMs' pre-trained visual-language understanding. The reflection mechanism further enables the VLM to critique and refine its plans based on these imagined futures, leading to more robust long-horizon manipulation."}, {"title": "2.3. Robotic Task and Motion Planning", "content": "Robotic Task and Motion Planning (TAMP) has been extensively studied (Kaelbling & Lozano-P\u00e9rez, 2011; Garrett et al., 2020a;b). Traditional approaches often combine symbolic planning with motion planning but struggle with real-world physical interactions and visual inputs. Learning-based methods (Wang et al., 2021; Driess et al., 2020) show promise in handling uncertainty and complex dynamics but typically require significant task-specific engineering.\nOur approach bridges this gap by leveraging VLMs' broad knowledge while adding structured physical reasoning through visual imagination and reflection. This enables robust long-horizon planning without requiring extensive task-specific engineering or large amounts of training data."}, {"title": "3. Preliminaries and Problem Statement", "content": "We formulate the multi-stage robotic manipulation planning problem as a partially observable Markov decision process (POMDP), defined by the tuple (S, A, T, O, Z). Here, S is the state space containing the full physical state of the environment, including object poses and physical properties; A is the action space consisting of high-level manipulation primitives {pick up, insert, reorient, put down} \u00d7 {objects}, assuming a failure rate \u03f5 for each primitive; T(st+1|st, at) represents the transition dynamics capturing physical interactions; O is the observation space of RGB images; and Z(ot|st) is the observation model mapping states to images.\nGiven a goal state sg, the objective is to find a policy \u03c0 that generates a sequence of actions to reach sg. Due to partial observability, the policy only has access to image observations, taking the form \u03c0(at|It, Ig) where It is the current observation and Ig is the goal image. The policy is instantiated as a VLM agent \u03c0VLM, which takes a multi-modal input of images and text, and generates action primitives in the form of text.\nOur framework includes a pre-training phase and a post-training phase. The post-training phase builds on the framework of interactive imitation learning (Ross et al., 2011; Kelly et al., 2018), which learns a policy by interacting with environment and receiving expert supervision in real-time. Thus under the standard assumption, we assume access to an interactive expert policy \u03a0E that generates near-optimal actions \u03b1\u2217 = \u03c0E(s) for any state s at training time. In this paper, we instantiated such an expert policy with access to the full state of the environment to generate optimal actions, though it could be obtained via other formats as well, e.g., human demonstrations. However, the VLM policy will only have access to image observations."}, {"title": "4. Reflective Planning with Vision Language Models", "content": "To address the challenges of physical interaction and long-horizon reasoning, we present a framework that incorporates VLMs with reflective planning. Our approach combines two key components: (1) a diffusion-based dynamics model that enables the VLM to imagine and evaluate future states, and (2) an interactive learning mechanism that allows the VLM to reflect on and revise its decisions based on these imagined outcomes. As shown in Fig. 1, these components work together to enable more robust manipulation planning while preserving the benefits of pre-trained VLMs."}, {"title": "4.1. Interactive VLM Policy Post-Training", "content": "While VLMs can generate actions based on visual inputs, they may hallucinate physically implausible solutions without actual interaction experience. To overcome this limitation and enable long-horizon reasoning, we introduce an interactive learning algorithm that teaches the VLM to reflect on and improve its decisions through direct interaction with the physical environment. This process further enhances a base VLM policy, which is initially trained on a fixed set of expert demonstrations. Similar to DAgger (Ross et al., 2011), we iteratively collect new data by rolling out the VLM policy in the environment and finetune the VLM policy with the aggregated data. As formulated in Algorithm 1, N trajectories are collected in each iteration. At each timestep, we generate a learner action a\u2021t by prompting the VLM with the images of the goal and current states, as well as an expert action at from the oracle policy. The pairs ((Ig, It), a) are then added to the dataset for finetuning. To facilitate convergence, we execute the learner action a\u2021t with a probability of p and the expert action at with a probability of 1 \u2212 p, instead of always following the actions from the learner.\nTo generate training data for reflection, we can simply relabel a trajectory after it is terminated, as also illustrated in Fig. 2. Specifically, the image It+H, which is a future observation following the action sequence at:t+H\u22121, is added to the context for reflection at timestep t, and the VLM is still supervised to output the same expert action at. Intuitively, this image provides additional information about the effect of executing the action sequence as a feedback, which can be leveraged by the VLM to decide whether the initially proposed action sequence leads to a promising future state.\nIn essence, we are generating two forms of question answering examples from interaction with the environment. The first is to predict an optimal action given images of the goal and current state, and the second is to reflect and revise an initial action sequence proposal by looking into an additional future image. Since a VLM can flexibly take any text and images as input, these two tasks can be handled by a single VLM with two different prompt templates, as \nminE_{\\mathcal{D}} \\left[ L_{CE}^{propose} (\\pi_{VLM}^{propose}(a_t | I_g, I_t), a) + L_{CE}^{reflect} (\\pi_{VLM}^{reflect}(a_t | I_g, I_t, I_{t+H}, a_{t:t+H-1}), a) \\right] (1)"}, {"title": "4.2. Diffusion Dynamics Model", "content": "A key component in reflective planning is predicting future states accurately when evaluating potential action sequences. While our interactive learning mechanism enables the VLM to learn from physical interactions, we need an additional capability during inference - the ability to imagine and evaluate hypothetical futures without actually executing actions in the environment. To address this, we develop a diffusion-based dynamics model (DDM) that efficiently generates predicted visual observations by conditioning on the current observation and a proposed action sequence. This allows the VLM to simulate the consequences of its actions before committing to them.\nBuilding on advances in diffusion-based generative models (Rombach et al., 2021; Ho et al., 2020; Song et al., 2021), we formulate the forward dynamics prediction as an image-to-image translation task. Our diffusion dynamics model takes the current observation It and action at as input to predict the next observation It+1. Rather than training a diffusion model from scratch, which would require substantial computational resources and training data, we leverage the pretrained Instructpix2pix model (Brooks et al., 2022) that has been trained on large-scale image editing datasets as our base model.\nData. We curate a dataset for training the diffusion model. To encourage broader coverage of visisted states, the data collection policy is a noised version of the oracle policy. Due to the difficulty of this task, we also include a few test data points to improve the fidelity and accuracy of the DDM. Details can be found in App. D.2.\nArchitecture. The model architecture is shown in Fig. 3. For the input (It, at), we first encode them into latent representation zt and zat with pretrained latent encoder and text encoder. Then we feed zt, a sampled noise N and the action condition zat into the diffusion UNet for de-noising. Finally, we decode the predicted zt+1 into a future observation It+1 with a latent decoder.\nTraining. The training of DDM consists of two separate phases: UNet training and decoder training. The UNet training phase is to learn transformations from zt to zt+1 conditioned on zat, while the latent decoder training is to adapt the pretrained VAE models into our task domain because our task requires precise reconstruction of small pieces on the table. Since we keep the latent encoder frozen, we can train the two phases in parallel."}, {"title": "4.3. Reflective Planning", "content": "With the VLM policy trained via interactive learning and the diffusion model serving as a dynamics proxy to imagine future outcomes, we now introduce our reflective planning mechanism for decision making at inference time. Alg. 2 shows the detailed process. We use \u0128 and \u00e3 to denote the generated image and action, which are not actually observed or executed in the environment. To get the future image after H steps, where H is the planning horizon, we perform H iterations of action proposal and diffusion generation. At each iteration, the VLM policy is prompted by the goal image Ig and the generated image \u0128t+k at the previous iteration to propose an action \u00e3t+k. The diffusion model \u0164 then generates the future image \u0128t+k+1 conditioned on the previous image \u0128t+k and the action \u00e3t+k. For the first iteration, the input image \u0128t is just the current observation It. After this process of imagination, the generated future image \u0128t+H and the plan \u00e3t:t+H\u22121 are concatenated with the goal and current observation, and fed into the VLM policy for reflection. The VLM policy will then output the final action at to be executed. Again, action proposal and reflection are performed by the same VLM policy with two different prompt templates, as indicated by the superscripts \"propose\" and \"reflect\"."}, {"title": "5. Multi-Stage Robotic Manipulation Tasks", "content": "Inspired by Luo et al. (2024b), we procedurally generated a suite of multi-stage long-horizon manipulation tasks that require understanding of physical interactions and reasoning about the effects of long-term action sequences. The task is initialized with a board and a set of small pieces randomly placed on a table. The goal is to fully assemble the board by inserting the pieces into the board one by one. Examples of the initial and goal configurations are shown in Fig. 5. Detailed task generation process is included in App. A. Notably, most tasks include inter-locking pieces so that they can be inserted into the board only in a specific order. This requires strategically choosing the object to be manipulated at each step and inferring possible interaction between this object and the other objects already in the board. As an example, Fig. 5(b) shows the dependencies between the pieces in one of the tasks. The interlocking feature further necessitates the agent's ability to replan, enabling it to recover from failures caused by previous mistakes or bad initialization.\nWe focus on the high-level planning of this long-horizon manipulation task. We define a set of actions in the form of \u201c[act] [obj]\u201d, where [act] \u2208 {pick up, insert, reorient, put down} is an action primitive, and [obj] denotes the object to be manipulated. Specifically, \u201cpick up\u201d grasps a piece that is not in hand and picks it up. It can then be inserted into the board using the \u201cinsert\u201d action, or put back on the table using \u201cput down\u201d. By invoking \u201creorient\u201d, the object in hand can be reoriented with the black fixture if necessary, so that it is in a suitable pose for insertion. Each action primitive is implemented as a rule-based script controller; however, integrating other low-level controllers, such as learning-based policies like behavior cloning, is also possible. We also designed an expert policy for the mentioned motor primitives, see App. C for implementation details."}, {"title": "6. Experiments", "content": "Our experiments evaluate the effectiveness of our method and analyze its key components. We aim to answer three key research questions. First, how well does our method perform in long-term planning, particularly when handling complex physical interactions? Second, how effectively does our method generalize across different object configurations and types, while maintaining the ability to reason and plan reactively in dynamic environments? Third, what is the impact of the reflection mechanism on the overall performance of our method? To address these questions, we conduct comprehensive experiments comparing ReflectVLM against: (1) state-of-the-art VLM models tested in zero-shot fashions, (2) model-based planning approaches like MCTS, and (3)"}]}