{"title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait", "authors": ["Taekyung Ki", "Dongchan Min", "Gyoungsu Chae"], "abstract": "With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.", "sections": [{"title": "1. Introduction", "content": "Animating a single image using a driving audio (i.e., audio-driven talking portrait generation) has gained significant attention in recent years for its great potential in avatar creation, video conferencing, virtual avatar chat, and user-friendly customer service. It aims to synthesize natural talking motion from audio signals, including accurate lip synchronization, rhythmical head movements, and fine-grained facial expressions. However, generating such motion solely from audio is extremely challenging due to its one-to-many correlation between audio and motion. In the earlier stage of this field, many works [9, 23, 34, 54, 58, 98] focus on generating accurate lip movements by relying on learned audio-lip alignment losses [10, 52].\nTo comprehensively extend the range of motion, some works [52, 74, 96] incorporate probabilistic generative models, such as VAE [35] and normalizing flow [60], turning the motion generation into probabilistic sampling. However, these models still lack expressiveness in generated motion due to the limited capacity of these generative models.\nRecent talking portrait generation methods [8, 25, 31, 43, 51, 70, 76, 80, 86, 89], powered by diffusion-based generative models [27, 68], successfully mitigate this expressiveness issue. Specifically, EMO [76] introduces a promising approach to this field [8, 31, 80, 86, 89] by employing a strong pre-trained image diffusion model (i.e., StableDiffusion [61]) and lifting it into video generation [29]. However, it still faces challenges in generating temporally coherent videos and achieving sampling efficiency, requiring tens of minutes for a few seconds of video. Moreover, they heavily rely on auxiliary facial prior, such as bounding boxes [76, 89], 2D landmarks and skeletons [8, 31, 94], or 3D meshes [86], which significantly restricts the diversity and the fidelity of head movements due to their strong spatial bias.\nIn this paper, we present FLOAT, an audio-driven talking portrait video generation model based on flow matching generative model. Flow matching [42, 44] has emerged as a promising alternative to diffusion models due to its fast and high-quality sampling. By modeling talking motion within a learned motion latent space [85], we can more efficiently sample temporally consistent motion latents. This is achieved by a simple yet effective transformer-based [79] vector field predictor, inspired by DiT [55], which also enables natural emotion-aware motion generation driven by speech. Our contributions are summarized as follows:\n\u2022 We present, FLOAT, flow matching based audio-driven talking portrait generation model using a learned motion latent space, which is more efficient and effective than pixel-based latent spaces.\n\u2022 We introduce a simple yet effective transformer-based flow vector field predictor for temporally consistent motion latent sampling, which also enables the speech-driven emotional controls.\n\u2022 Extensive experiments demonstrate that FLOAT achieves state-of-the-art performance compared to both diffusion- and non-diffusion-based methods."}, {"title": "2. Related Works", "content": "2.1. Diffusion Models and Flow Matching\nDiffusion Models. Diffusion models or score-based generative models [14, 27, 53, 61, 67, 68] are generative models that gradually diffuse input signals into Gaussian noise and learn the denoising reverse process for the generative modeling. They have shown remarkable results in various generation tasks, such as unconditional image and video generation [4, 18, 55], text-to-image generation [59, 61, 62], text-to-video generation [4, 24], conditional image generation [29, 94], and 3D human generation [37, 71, 75].\nAccelerating Diffusion Models. While diffusion models demonstrate superior performance, their iterative sampling nature still bottlenecks the efficient generation compared to VAEs [35], normalizing flow [60], and GANs [22]. To overcome this limitation, several works have been developed to boost the sampling speed of the diffusion models. StableDiffusion (SD) [61] partially mitigates this problem by moving the diffusion process from the pixel space to the spatial latent space, establishing itself as a pivotal framework among diffusion models. Another line of research has developed the sampling solvers [47, 48] based on ordinary differential equations (ODEs). Meanwhile, model distillation [26] has been introduced to transfer the knowledge of the learned diffusion models into a student model, enabling one (or a few) steps of generation [32, 41, 45, 49, 69]. However, these approaches involve substantial effort to create a well-trained diffusion model and suffer from training instability.\nFlow Matching. Flow matching [42, 44] stands out as an alternative to diffusion models for its high sampling speed and competitive sample quality compared to diffusion models [11, 20, 39, 42, 57]. It belongs to the family of flow-based generative models, which estimates a transformation (referred to as a flow) between a prior distribution (e.g., Gaussian) and a target distribution. Unlike the normalizing flow [15, 60] that directly estimates the noise-to-data transformation under specific architectural constraints (e.g., affine coupling), flow matching regresses the time-dependent vector field that generates this flow by solving its corresponding ODEs [7] with flexible architectures. One specific design of flow matching is an optimal transport (OT) based one, which transforms the data distribution along the straight path with constant velocity [42].\nOur audio-driven talking portrait method employs flow matching to generate the natural talking motions. Thanks to the architectural flexibility of flow matching, we use transformer-encoder architecture [79] to estimate the generating vector field, allowing us to take the video temporal consistency into account."}, {"title": "2.2. Audio-driven Portrait Animation", "content": "Audio-driven portrait animation is the task of generating a realistic talking portrait video using a single portrait image and driving audio [52, 82, 96, 99, 100]. Since audio-to-motion relation is basically a one-to-many problem, several works utilize additional facial prior for driving conditions, e.g., 2D facial landmarks [8, 25, 31, 80, 86, 100], 3D prior [9, 50, 51, 91, 96], or emotional labels [30, 73, 90]. In earlier stages, most works [9, 23, 34, 58] focused on generating accurate lip motion from audio by utilizing the lip-sync discriminator [10]. These approaches have advanced to generating audio-related head poses in a probabilistic way. For example, StyleTalker [52] uses normalizing flow [15, 60] to generate the head motion from audio, while SadTalker [96] uses audio-conditional variational inference [35] to learn the 3DMM coefficients [2], bridging the intermediate representations of a pre-trained portrait animator [83].\nMeanwhile, several works [30, 73, 81, 87] focus on an emotion-aware talking portrait generation. In particular, EAMM [30] considers an emotion as the complementary displacement of facial motion, and learns these displacement from an emotion label extracted from the image.\nRecent audio-driven talking portrait methods powered by diffusion models show remarkable results [8, 31, 43, 51, 76, 80, 86, 89, 90]. Specifically, EMO [76] and subsequent extensions [8, 80, 86, 89] utilize the pre-trained SD [61] as their backbone to leverage generative prior trained on the large-scale image datasets. They introduce additional modules, e.g., ReferenceNet [29] and Temporal Transformer [24], to preserve input identity and enhance the video temporal consistency, respectively. However, these modules introduces additional computational cost, requiring several minutes for a few seconds of video, and still suffer from video-level artifacts, such as noisy frames, and flickering. VASA-1 [90] addresses the sampling time issue by sampling motion latents [16], producing lifelike talking portraits.\nOur method can generate a talking motion through the flow matching [42] in a learned motion latent space and successfully addresses the inefficient sampling of diffusion-based methods. Moreover, our method can generate emotion-aware talking motions by using speech-driven emotional labels [56]."}, {"title": "3. Preliminaries: (Conditional) Flow Matching", "content": "Let $x \\in \\mathbb{R}^d$ be a data, $t \\in [0,1]$ be the time, and $q$ be a unknown target distribution. We can define a flow as a time-dependent transformation $\\varphi_t : [0,1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ that transforms a tractable prior distribution $p_0$ to the distribution $p_1 \\approx q$. This flow $\\varphi_t$ further introduces a probability flow path $p_t: [0,1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}_{>0}$ and a generating vector field $v_t : [0,1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ where $p_t$ is defined by the push-forwarding\n$p_t(x) = p_0(\\varphi_t^{-1}(x)) \\left| \\operatorname{det} \\frac{\\partial \\varphi_t^{-1}}{\\partial x} \\right|,$ (1)\nand $v_t$ generates $\\varphi_t$ by means of an ordinary differential equation (ODE) [7]:\n$\\frac{d}{dt} \\varphi_t(x) = v_t(\\varphi_t(x)) \\quad and \\quad \\varphi_0(x) = x.$ (2)\nFlow matching [42] aims to estimate the target generating vector field $u_t$ with a neural network parameterized by $\\theta$:\n$\\mathcal{L}_{FM}(\\theta) := ||v_t(x; \\theta) - u_t(x)||^2,$ (3)\nwhere $t \\sim U[0, 1]$ and $x \\sim p_t(x)$. However, the target generating vector field $u_t$ and the sample distribution $p_t$ are intractable. To address this issue, [42] proposes a method for constructing a \"conditional\" probability path $p_t(x|x_1)$ as well as target \"conditional\" vector field $u_t(x|x_1)$ using a sample $x_1 \\sim q$ as a condition. And they prove that the following objective\n$\\mathcal{L}_{CFM}(\\theta) := ||v_t(x; \\theta) - u_t(x|x_1)||^2,$ (4)\nwhere $t \\sim U[0,1]$ and $x \\sim p_t(x|x_1)$, is equivalent to (3) with respect to the gradient $V_\\theta$.\nOne natural way of constructing $u_t(\\cdot|x_1)$ is a \"straight line\" that connects $x_0 \\sim p_0$ and $x_1 \\sim q$, drawing an optimal transport (OT) path with constant velocity [42]. Specifically, a linear time interpolation between $x_0$ and $x_1$ gives us the flow $x_t = \\varphi_t(x) = (1 - t)x_0 + tx_1$, the conditional probability path $p_t(x|x_1)$ defined via the affine transformation $p_t(x|x_1) = \\mathcal{N}(x|tx_1, t^2I)$, and the target generating vector field $u_t(x|x_1) = x_1 - x_0$. This specific choice turns the objective (4) into\n$\\mathcal{L}_{OT}(\\theta) := ||v_t((1 - t)x_0 + tx_1;\\theta) - (x_1 - x_0)||^2,$ (5)\nwhere $t \\sim U[0, 1]$, $x_0 \\sim p_0$, and $x_1 \\sim q$, all of which are tractable.\nClassifier-free Vector Field. [11] formulates a classifier-free vector field (CFV) technique for flow matching, which enables class-conditional sampling more controllable manner without any extra classifier trained on noisy trajectory. Formally, CFV compute the modified vector field $\\tilde{v_t}$ by\n$\\tilde{v}_t(x_t, c; \\theta) \\approx \\gamma u_t(x_t, c; \\theta) + (1 - \\gamma) v_t(x_t, c = 0; \\theta),$ (6)\nwhere $\\gamma$ denotes the guidance scale. $v_t(x_t, c = 0; \\theta)$ is the predicted vector field without a driving condition $c$. For more details, please refer to [11, 42]."}, {"title": "4. Method: Flow Matching for Audio-driven Talking Portrait", "content": "We provide an overview of FLOAT at Fig. 2. Given source image $S \\in \\mathbb{R}^{3 \\times H \\times W}$, and a driving audio signal $a_{1:L} \\in \\mathbb{R}^{L \\times d_a}$ of length L, our method generates a video\n$\\hat{D}_{1:L} = (\\hat{D}'_l)_{l=1}^L \\in \\mathbb{R}^{L \\times 3 \\times H \\times W}$ (7)\nof L frames, featuring audio-synchronized talking head motions, including both verbal and non-verbal motions. Our method consists of two phases. First, we pre-train a motion auto-encoder, which provides us with the expressive and smooth motion latent space for the talking portraits (Sec. 4.1). Next, we employ flow matching [42] to generate a sequence of motion latents with a transformer-based vector field predictor using the driving audio, which is decoded to the talking portrait videos (Sec. 4.2). Thanks to simple yet powerful vector field architecture, we can also incorporate speech-driven emotions as the driving conditions, enabling emotion-aware talking portrait generation.\n4.1. Motion Latent Auto-encoder\nRecent talking portrait methods utilize the VAE of StableDiffusion (SD) [61] due to its rich semantic pixel-based latent space. However, they often struggle to generate temporally consistent frames when lifted to video generating tasks [8, 29, 76, 89, 101]. Thus, our first goal for realistic talking portrait is to obtain good motion latent space, capturing both global (e.g., head motion) and fine-grained local (e.g., facial expressions, mouth movement, pupil motion) dynamics. To achieve this, we employ the latent image animator (LIA) [85] as our motion auto-encoder instead of the VAE of SD. The key difference lies in the training objective: LIA is trained to reconstruct a driving image from a source image sampled from the same video clip, which enforces latent encoding to contain implicit motions that can capture both temporally adjacent and distant motions.\nOur motion auto-encoder can encode the source S into the latent $w_s \\in \\mathbb{R}^d$ with following explicit decomposition:\n$w_s = w_{s\\rightarrow r} + w_{r \\rightarrow s},$ (8)\nwhere $w_{s\\rightarrow r} \\in \\mathbb{R}^d$ is the identity latent and $w_{r\\rightarrow s} = \\sum_{m=1}^{M} A_m(S) \\cdot v_m \\in \\mathbb{R}^d$ is the motion latent with $\\lambda(S) = (A_m(S))_{m=1}^M \\in \\mathbb{R}^M$ being the source-dependent motion coefficients that span the learned source-agnostic motion orthonormal basis $V = \\{v_m\\}_{m=1}^M \\subseteq \\mathbb{R}^d$. In this space, a motion has M distinct (orthogonal) motions with its intensity $A_m(S)$. As shown in [85], this explicit decomposition is accomplished with introducing the source-agnostic motion basis V.\nSince the expressiveness of generated motions and the image fidelity are determined by the motion latent space, we scale the original LIA architecture to synthesize higher resolution from $256^2$ to $512^2$. Additionally, we introduce a simple yet effective facial component perceptual loss using [66, 95] that significantly improves the image fidelity (e.g.,\n4.2. Flow Matching in Motion Latent Space\nIn the learned motion latent space, we predict a vector field $v_t(x_t, c_t; \\theta) \\in \\mathbb{R}^{L \\times d}$ where $x_t$ is the sample at flow time $t\\in [0, 1]$, and $c_t \\in \\mathbb{R}^{L \\times h}$ represents the driving conditions for L consequent frames. This vector field generates the flow $f_t : [0,1] \\times \\mathbb{R}^{L \\times d} \\rightarrow \\mathbb{R}^{L \\times d}$ of L frames by solving ODE (Eq. (2)). As illustrated in Fig. 3, we build our vector field predictor upon the transformer encoder [79] architecture. Specifically, we adopt DiT [55] architecture to decouple frame-wise conditioning from time-axis attention mechanism, which enables us to model temporally consistent motion latents. We refer to this predictor as flow matching transformer (FMT).\nIn DiT [55], distinct semantic tokens are modulated by a single diffusion time step embedding and class embedding through adaptive layer normalization (AdaLN). In contrast, FMT modulates each l-th input latent with its corresponding l-th condition and then combines their temporal relations through a masked self-attention layer that attends to 2 \u00b7 T neighboring frames. Formally, for each l-th frame, frame-wise AdaLN and frame-wise gating are computed by\n$\\gamma \\times LN(X) + \\beta \\in \\mathbb{R}^h \t\\text{ and } \\alpha \\times X \\in \\mathbb{R}^h,$ (9)\nrespectively, where $i \\in \\{1, 2\\}$, h is the hidden dimension, LN denotes layer normalization [40], and X is the l-th input for each operation at flow time $t \\in [0,1]$. The coefficients $\\alpha, \\beta, \\gamma \\in \\mathbb{R}^h$ are computed from the condition $c \\in \\mathbb{R}^h$ through a linear layer, ToScaleShift, as depicted in Fig. 3.\nSpeech-driven Emotional Labels. How can we make talking motions more expressive and natural? During talking, humans naturally reflect their emotions through their voices, and these emotions influence talking motions. For instance, a person who speaks sadly may be more likely to shake the head and avoid eye contact. This non-verbal motion derived from emotions crucially impacts the naturalness of a talking portrait.\nExisting works [30, 81, 90] use image-emotion paired data or image-driven emotion predictor [63] to generate the emotion-aware motion. In contrast, we incorporate speech-driven emotions, a more intuitive way of controlling emotion for audio-driven talking portrait. Specifically, we utilize a pre-trained speech emotion predictor [56] that produces softmax probabilities of seven distinct emotions: angry, disgust, fear, happy, neutral, sad, and surprise, which we then input into the FMT.\nHowever, as people do not always speak with a single, clear emotion, determining emotions solely from audio is often ambiguous [30]. Naive introduction of speech-driven emotion can make emotion-aware motion generation more challenging. To address this issue, we inject the emotions together with other driving conditions at training phase and modify them at inference phase.\nDriving Conditions. We concatenate the audio representation $a_{1:L} \\in \\mathbb{R}^{L \\times d_a}$ of a pre-trained Wav2Vec2.0 [1], the speech emotion label $w_e \\in \\mathbb{R}^7$, and the source motion latent $w_{r\\rightarrow s} \\in \\mathbb{R}^d$. Next, we add the flow time step embedding $Emb(t) \\in \\mathbb{R}^h$ to these conditions, producing $c_t \\in \\mathbb{R}^{L \\times h}$ via a linear layer, ToCondition, as depicted in Fig. 2, where $Emb(t)$ is computed using the sinusoidal position embedding [79]. For smooth transitions of sequences longer than the window length L, we follow the convention of [71, 75], extending $c_t \\in \\mathbb{R}^{L \\times h}$ by appending the last L' audio features from the preceding window. We also use the last L' frames of target vector field $u_t$ as additional input latents. Note that $w_e$ and $w_{r\\rightarrow s}$ are shared across the L' + L frames.\nTraining. We train FLOAT by reconstructing a target vector field computed from driving frames using the corresponding audio segments and a source motion latent. We choose a pair of driving motions and corresponding audio $(W_{r\\rightarrow \\hat{D}_{1:L}}, a_{1:L})$, and construct the target vector field $u_t(x|W_{r\\rightarrow \\hat{D}_{1:L}}) = W_{r\\rightarrow \\hat{D}_{1:L}} - x_0 \\in \\mathbb{R}^{L \\times d}$ with noisy input $\\varphi_t(x_0) = (1 - t)x_0 + tW_{r\\rightarrow \\hat{D}_{1:L}}$ ($t \\sim U[0, 1]$ and $x_0 \\sim \\mathcal{N}(0_{L':L}, I)$). Additionally, we take the preceding audio feature $a_{-L':0}$ and target vector field $u_t(x|W_{r\\rightarrow \\hat{D}_{-L':0}})$ of L' frames.\nThe flow matching objective $\\mathcal{L}_{OT}(\\theta)$ is defined by\n$\\mathcal{L}_{OT}(\\theta) = ||v_t (\\varphi_t(x_0), c_t; \\theta) - u_t(x|W_{r\\rightarrow \\hat{D}_{-L':L}})||, (10)\nwhere $c_t \\in \\mathbb{R}^{(-L'+L) \\times h}$ is the driving condition consisting of $[t, w_{r\\rightarrow s}, w_e, a_{1:L}, a_{-L':0}]$. Here, we omit $u_t(x/W_{r\\rightarrow \\hat{D}_{-L'1:0}})$ for simplicity. Since we predict the vector field rather than noise, we incorporate a velocity loss [75] to supervise temporal consistency:\n$\\mathcal{L}_{vel}(\\theta) = ||\\Delta v_t - \\Delta u_t||, (11)\nwhere $\\Delta v_t$ and $\\Delta u_t$ are the one-frame difference along the time-axis for $v_t$ and $u_t$, respectively. The total objective $\\mathcal{L}_{total}$ is\n$\\mathcal{L}_{total}(\\theta) = \\lambda_{OT} \\mathcal{L}_{OT}(\\theta) + \\lambda_{vel} \\mathcal{L}_{vel}(\\theta),$ (12)\nwhere $\\lambda_{OT}$ and $\\lambda_{vel}$ are the balancing coefficients. During training, we apply dropout to $w_r, w_e$, and $a^{1:L}$ with a probability of 0.1 for CFV. Additionally, we apply dropout to the preceding audio and vector field with a probability 0.5 for smooth transition in the initial window.\nInference. During inference, we sample the generating vector field from noise $x_0$, using the driving conditions $W_{r\\rightarrow s}, w_e$, and $a_{1:L}$, as well as the $L'$ frames of preceding audio and generated vector field.\nWe extend the CFV [11] to an incremental CFV to separately adjust the audio and emotion, inspired by [3]:\n$v_t \\approx v_t(x_0, c_t|\\{a_{1:L},w_e\\})$\n$+ \\gamma_a [u_t(x_0, c_t|w_e) - v_t(x_0, c_t|\\{a_{1:L},w_e\\}]$\n$+ \\gamma_e [u_t(x_0, c_t) - v_t(x_0, c_t| w_e)],$ (13)\nwhere $\\gamma_a$ and $\\gamma_e$ are the guidance scales for audio and emotion, respectively. $c_t|\\{x,y\\}$ denotes the driving condition without the condition x and y.\nAfter sampling, ODE solver receives the estimated vector field to compute the motion latents through numerical integration. We experimentally find that FLOAT can generate reasonable motion with around 10 number of function evaluation (NFE).\nLastly, we add the source identity latent to the generated motion latents and decode them into video frames using the motion latent decoder."}, {"title": "5. Experiments", "content": "5.1. Dataset and Pre-processing\nFor training the motion latent auto-encoder, we use three open-source datasets: HDTF [97], RAVDESS [46], and VFHQ [88]. When training FLOAT, we exclude VFHQ because it does not support the synchronized audio. HDTF [97] is for high-definition talking face generation, containing videos of over 300 unique identities. RAVDESS [46] includes more than 2,400 emotion-intensive videos of 24 different identities. VFHQ [88] is designed for high-resolution video super-resolution and includes a large number of unique identities, which compensates the limited number of identities of the preceding datasets. Following the strategy of [65], we first convert each video to 25 FPS and resample the audio into 16 kHz. Then, we crop and resize the facial region to 5122 resolution using Face-alignment [5].\nAfter the pre-processing, for HDTF, we use a total of 11.3 hours of 240 videos featuring 230 different identities for training, and videos of 78 different identities, each 15 seconds long, for test. For RAVDESS, we use videos of 22 identities for training, and videos of the remaining 2 identities for test, with each 3-4 seconds long and representing 14 emotional intensities. Note that the identities in the training and test are disjoint in both datasets.\n5.2. Implementation Details\nWe use the Euler method [42] as the ODE solver. The motion latent dimension is set to d = 512 with M = 20 distinct orthogonal directions. For FMT, we use 8 attention heads,\na hidden dimension h = 1024, and an attention window length T = 2. We generate L = 50 frames with preceding L' = 10 frames at once, encompassing 2.4 seconds of video. We employ the Adam optimizer [36] with a batch size of 8 and a learning late of 10\u20135. We use L1 distance for the norm || || in the training objective. We set the balancing coefficients to \u03bb\u03bf\u03c4 = Xvel = 1. The entire training takes about 2 days for 2,000k steps on a single NVIDIA A100 GPU.\n5.3. Evaluation\nMetrics and Baselines. For evaluating the image and video generation quality, we measure Fr\u00e9chet Inecption Distance (FID) [64] and 16 frames Fr\u00e9chet Video Distance (FVD) [78]. For facial identity, expression and head motion, we measure Cosine Similarity of identity embedding (CSIM) [12], Expression FID (E-FID) [76] and Pose FID (P-FID), respectively. Lastly, we measure Lip-Sync Error Distance and Confidence (LSE-D and LSE-C [58]) for audio-visual alignment.\nWe compare our method with state-of-the-art audio-driven talking portrait methods whose official implementations are publicly available. For non-diffusion methods, we compare with SadTalker [96] and EDTalk [74]. For diffusion methods, we compare with AniTalker [43], Hallo [89], and EchoMimic [8].\nComparison Results. In Tab. 1 and Fig. 4, we show the quantitative and qualitative comparison results, respectively. FLOAT outperforms other methods on most of the metrics and visual quality in both datasets.\n5.4. Ablation Studies\nAblation on FMT and Flow Matching. We compare FMT, which uses frame-wise AdaLN (and gating), followed by masked self-attention to separate conditioning from attending, with a cross-attention-based transformer that performs conditioning and attending simultaneously. As shown in Tab. 2, both approaches achieve competitive image and video quality, while FMT provides better expression generation and lip synchronization.\nWe also conduct ablation studies on flow matching by comparing it with two types of diffusion models: \u03f5-prediction and $x_0$-prediction. The former predicts noise, while the latter predicts the signal itself [59, 75]. In both cases, we adopt our FMT architecture as denoising networks, and use diffusion 500 steps with a cosine noise scheduler. Notably, diffusion and flow matching achieve competitive results on image quality while the latter achieves the better lip synchronization. In Fig. 5, we also compare the forward pass efficiency by measuring frames per second (FPS) of each model. Thanks to the compact motion latent representation, they run 125\u00d7 faster than Hallo [89], and FLOAT further achieves higher FPS due to the fast sampling of flow matching.\nAblation on NFE. In general, increasing the number of function evaluation (NFE) reduces the solution error of ODEs. As shown in Tab. 3, even with small NFE = 2, FLOAT can achieve competitive image quality (FID) and lip synchronization (LSE-D). However, it struggles to capture consistent and expressive motions (FVD and E-FID), resulting in shaky head motion and a static expression. This is because FLOAT generates the motion in the latentspace, while image fidelity is determined by the auto-encoder.\nAblation on Guidance scales. In Tab. 4, we conduct ablation studies on guidance scales: $\\gamma_a$ and $\\gamma_e$, with the emotion intensive dataset RAVDESS [46]. Note that increasing $\\gamma_a$ leads to better temporal consistency (FVD) and lip synchronization quality (LSE-D). Moreover, increasing $\\gamma_e$ improves video consistency (FVD) and expressiveness (E-FID). This enables balanced control over emotional audio-driven talking portrait generation."}, {"title": "5.5. Further Results", "content": "Additional Driving Signals. In Tab. 5 and Fig. 6, we experiment additional driving conditions: driving head poses and image-driven emotion labels, to investigate more controllable talking portraits. In applications, it is often beneficial to create a talking portrait with pose driving manner (e.g., stitching back to existing video). To achieve this, we employ 3DMM head pose parameters $p \\in \\mathbb{R}^6$ [2] extracted from [13]. We concatenate a sequence of pose parameters $p_{1:L} \\in \\mathbb{R}^{L \\times 6}$ with the other driving conditions frame by frame, and then map them to $c_{1:L}' \\in \\mathbb{R}^{L \\times h}$. We also conduct experiments on the driving emotion by dropping the speech-driven emotion and replacing it with the image-driven emotion [63].\nNotably, introducing pose parameters significantly improves the image and video metrics. This is because the driving head poses help to capture the head poses of the target distribution. Moreover, both speech-driven emotion and image-driven emotion consistently improve the generated motion quality in the emotion-intensive dataset, where the image-driven emotion achieves slightly better metrics. This is because the image-driven approach is slightly less ambiguous compared to the speech-driven approach.\nRedirecting Speech-driven Emotion. Since FLOAT is trained on the emotional-intensive video dataset [46], we can change a generated emotion-aware talking motion to a different emotion at inference by manually redirecting the predicted emotion label to another one (e.g., a one-hot label). As shown in Fig. 7, this enables manual redirection when the predicted emotion from speech is complex or ambiguous.\nUser Study. In Tab. 6, we conduct a mean opinion score (MOS) based user study to compare the perceptual quality of each method (e.g., teeth clarity and naturalness of emotion). We generate 6 videos by using the baselines and FLOAT, and ask 15 participants to evaluate each generated video with five evaluation factors in the range of 1 to 5. As shown in Tab. 6, FLOAT outperforms the baselines."}, {"title": "6. Conclusion", "content": "We proposed FLOAT, a flow matching based audio-driven talking portrait generation model leveraging a learned motion latent space. We introduced a transformer-based vector field predictor, enabling temporally consistent motion generation. Additionally, we incorporated speech-driven emotion labels into the motion sampling process to improve the naturalness of the audio-driven talking motions. FLOAT addresses current core limitations of diffusion-based talking portrait video generation methods by reducing the sampling time through flow matching while achieving the remarkable sample quality. Extensive experiments verified that FLOAT achieves state-of-the-art performance in terms of visual quality, motion fidelity, and efficiency.\nDiscussion. We leave further discussion considering limitations, future work, and ethical considerations in the supplementary materials."}, {"title": "Supplementary Materials", "content": "In this supplement, we first provide more details on motion latent auto-encoder in Appendix A, regarding the model itself (Appendix A.1), methods for improving the fidelity of facial components (Appendix A.2), the training objective (Appendix A.3), and implementation details (Appendix A.4).\nIn Appendix B, we provide more details on FLOAT, regarding details on evaluation metrics (Appendix B.1), baselines (Appendix B.2), and ablation studies (Appendix B.3).\nIn Appendix C, we provide additional results, including comparison results (Appendix C.1), out-of-distribution results (Appendix C.2), and user study (Appendix C.3).\nFinally, we discuss ethical considerations, limitations, and future work in Appendix D.\nA. More on Motion Latent Auto-encoder\nIn this section, we provide more details on our motion latent auto-encoder, including its model architecture, dataset, and training strategy.\nA.1. Model\nWe provide a detailed model architecture of our motion latent auto-encoder in Fig. 11.\nIn Fig. 8a, Fig. 8b, Fig"}]}