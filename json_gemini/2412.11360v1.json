{"title": "Visual IRL for Human-Like Robotic Manipulation", "authors": ["Ehsan Asali", "Prashant Doshi"], "abstract": "We present a novel method for collaborative robots (cobots) to\nlearn manipulation tasks and perform them in a human-like manner.\nOur method falls under the learn-from-observation (LfO) paradigm,\nwhere robots learn to perform tasks by observing human actions,\nwhich facilitates quicker integration into industrial settings com-\npared to programming from scratch. We introduce Visual IRL that\nuses the RGB-D keypoints in each frame of the observed human task\nperformance directly as state features, which are input to inverse\nreinforcement learning (IRL). The inversely learned reward func-\ntion, which maps keypoints to reward values, is transferred from the\nhuman to the cobot using a novel neuro-symbolic dynamics model,\nwhich maps human kinematics to the cobot arm. This model allows\nsimilar end-effector positioning while minimizing joint adjustments,\naiming to preserve the natural dynamics of human motion in robotic\nmanipulation. In contrast with previous techniques that focus on\nend-effector placement only, our method maps multiple joint angles\nof the human arm to the corresponding cobot joints. Moreover, it\nuses an inverse kinematics model to then minimally adjust the joint\nangles, for accurate end-effector positioning. We evaluate the per-\nformance of this approach on two different realistic manipulation\ntasks. The first task is produce processing, which involves picking,\ninspecting, and placing onions based on whether they are blemished.\nThe second task is liquid pouring, where the robot picks up bot-\ntles, pours the contents into designated containers, and disposes of\nthe empty bottles. Our results demonstrate advances in human-like\nrobotic manipulation, leading to more human-robot compatibility in\nmanufacturing applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Several factors are known to contribute to successful and sustain-\nable human-robot collaborations [1, 21]. One of these is whether\nthe collaborative robot is likable. Robots tend to be likable when\nthey exhibit social intelligence and act in ways that make the hu-\nman partner comfortable. One of these ways is for the robot to use\nnaturalistic, human-like, motions in performing its tasks.\nToward achieving this goal, we adopt the learn-from-observation\n(LfO) paradigm, which is a human-inspired methodology where\nrobots learn to perform tasks by observing humans perform them.\nThis approach can facilitate quicker integration of collaborative\nrobots (cobot) in industrial settings without extensive programming\nfrom scratch. In this paper, we present a novel approach for cobots to\nlearn manipulation tasks and execute them in a naturalistic, human-\nlike manner. Our method leverages real-time human pose estimation\nand object detection on every RGB-D frame of the observed hu-\nman task performance. Three-dimensional keypoints from these\nframes are extracted, polished, and then used directly as state fea-\ntures input into an adversarial inverse reinforcement learning (AIRL)\ntechnique [16]. This technique learns a reward function that is in-\nterpreted as human preferences for the performed task. The learned\nreward function and corresponding behavior, which maps the pose\nkeypoints to actions in terms of human wrist joint angles, is trans-\nferred to a cobot using a novel neuro-symbolic dynamics model.\nThis model maps human kinematics to a cobot arm, allowing for\nsimilar end-effector positioning while minimizing joint adjustments,\nthus preserving the natural dynamics of human motion in robotic\ntasks.\nOur methodology offers two key contributions. First is the visual\nIRL that subscribes to the LfO paradigm and performs IRL directly\nwith human pose keypoints as state features obtained from a state-of-\nthe-art standard real-time pose estimation model such as YOLO [36].\nWe demonstrate that IRL can scale to a stream of such state features\nand yield an accurate reward function. This mitigates the need to\ntrain custom state-action recognition models such as SA-Net [32] or\nMVSA-Net [4] to process video frames and obtain high-level states\nand actions as input to IRL algorithms. Second is the neuro-symbolic\ndynamics mapping model. This model enables the transfer of human\narm motion dynamics to a cobot with differing degrees of freedom\n(DoF), maintaining the human-like characteristics of the motion.\nUnlike previous techniques that focus solely on end-effector place-\nment, our method maps multiple joint angles of the human arm to\ncorresponding cobot joints. After mapping the joints and calculating\nthe initial joint angles for the cobot, an inverse kinematics model is\nused to make adjustments for accurate end-effector positioning with\nminimum changes to the initial joint angles. This approach ensures\nthat the cobot performs the task while adopting the style and fluidity\nof human movement.\nIn the Experiments section, we examine this methodology for\nlearning two manipulation tasks with real-world applications and\nperforming each with a different cobot. One task is produce process-\ning which involves picking, inspecting, and placing onions at differ-\nent locations based on whether the onion is blemished or not [4, 29].\nRecordings of humans sorting onions in the lab are passed through\nthe method and used to train the 7 DoF Sawyer cobot to perform\nonion sorting with human-like motion. The second task is liquid\npouring which involves picking up colored bottles, pouring contents\ninto a corresponding container (based on the color of the bottles),\nand disposing of the empty bottles into a bin. A 6 DoF KUKA cobot\n(LBR iisy) is used to perform the task in a human-like manner. These\ntasks are chosen due to their relevance to manufacturing automation\nand inherent complexity due to variations in human judgment and\nmotion."}, {"title": "2 RELATED WORK", "content": "The Learn-from-Observation (LfO) paradigm enables robots to\nlearn skills by observing human demonstrations, reducing the need\nfor explicit programming. Advances like [4] and [32] introduced\ndeep neural networks for state-action recognition from RGB-D\nstreams, enhancing task transfer to robots. Reviews such as [14]\nand [26] categorize video-based learning methods, discussing strengths\nand visual data's role in LfO. One-shot imitation learning, explored\nin [40] and [15], leverages domain adaptation to learn tasks from\nsingle observations. Studies like [35] and [39] integrate visual ob-\nservation with neural networks, simplifying complex task learning\nfrom demonstrations.\nInverse Reinforcement Learning (IRL) aims to infer the under-\nlying reward function from observed expert behaviors, facilitating\nthe learning of policies that mimic these behaviors [2]. To enhance\nhuman-robot teaming, Decentralized Adversarial IRL (Dec-AIRL)\nutilizes decentralized demonstrations and adversarial training [29].\nAddressing occlusions and observer noise, MMAP-IRL employs\nmarginal MAP estimation for more reliable reward inference [33].\nLeveraging GANs, the method by Fu et al. [16] enhances the robust-\nness of learned rewards. In model-based IRL, Das et al. [12] integrate\nvisual perception with IRL to facilitate complex task learning from\nvisual demonstrations. Online IRL approaches, as explored by Arora\net al. [3], incorporate real-time observations for continuous adapta-\ntion. In another work, Zakka et al. [41] introduce Cross-embodiment\nIRL (XIRL), using temporal cycle consistency to transfer learned\nrewards across different embodiments.\nNeuro-Symbolic Models combine neural networks with sym-\nbolic reasoning to leverage the strengths of both methods. A scalable\nmethod by Bendinelli et al. employs large-scale pre-training to un-\ncover symbolic equations from data [6]. Enhancing transparency in\nlearning processes, Gopalan and Reichlin's approach focuses on con-\ntrollable neural symbolic regression [5]. The innovative use of sym-\nbolic building blocks instead of traditional activations by Martius and\nLampert allows for extracting formulas from trained networks [7].\nMerging neural predictions with symbolic search, Landajuela et al.'s\nunified framework achieves better generalization [19]. Discovering\nalgebraic formulas that explain learned models, Cranmer et al. [10]\nintegrate graph neural networks with symbolic regression. Finally,\nUdrescu and Tegmark's AI Feynman uses physics-inspired tech-\nniques to identify and simplify symbolic expressions, enhancing\nmodel interpretability [34].\nMapping human dynamics to robots involves capturing, ana-\nlyzing, and replicating human motion patterns to enable robots to\nperform tasks with human-like precision. Dube and Tapson [13]\ndeveloped a 10-DoF arm design to improve the kinematic range\nusing visual motion capture. Memmesheimer et al. [23] introduced\nSimitate, focusing on end-effector motion using human keypoints,\ndiffering from our full-arm approach. AVID translates human videos\ninto robotic actions with deep learning for multi-stage tasks [31]."}, {"title": "3 BACKGROUND", "content": "Inverse reinforcement learning (IRL) is concerned with determining\nthe reward function in a Markov decision process (MDP) [24] that\nbest explains the observed behavior of an expert. An MDP is repre-\nsented by the tuple (S, A, T, R, \u03b3, \u03c1\u03bf), where S is the set of states, A is\nthe set of actions, $T: S\u00d7A\u00d7S \u2192 [0, 1]$ defines the state transition\nprobabilities, $R: S\u00d7A \u2192 R$ is the reward function, $\u03b3 \u2208 [0, 1)$ is\nthe discount factor, and po is the initial state distribution. In the IRL\ncontext, the learner knows the states S, actions A, discount factor \u03b3,\ninitial state distribution po, and state transition probabilities T, but\nthe reward function R is unknown. Model-free approaches can forgo\nthe need for T. The policy \u03c0: S\u2192 A is a function that maps states\nto actions. Let XE represent the set of expert demonstrations; a com-\nplete trajectory X can be denoted as X = (s1, A1, S2, A2, . . ., ST, \u0430\u0442),\nwith X \u2208 XE.\nThe objective of IRL is to estimate the reward function R, which,\nwhen combined with the known components of the MDP (S, A, \u03a4, \u03b3, \u03c1\u03bf),\ninduces a policy \u03c0 that produces trajectories similar to the expert\ndemonstrations XE. These expert trajectories consist of a sequence\nof state-action pairs (st, at), where st \u2208 S and at \u2208 A for each time\nstep t. By observing the expert's transitions from state st to state st+1\nunder action at, the learner infers which rewards R(st, at) make the\nexpert's behavior optimal. The inferred policy n is thus expected to\nmaximize the cumulative discounted reward $\u2211_{t=1}^{T} y^tR(s_t, a_t)$ over\nthe trajectory, where y determines how future rewards are weighed\nrelative to immediate rewards.\nAdversarial Inverse Reinforcement Learning (AIRL) is an ap-\nproach that builds upon the framework of Generative Adversarial\nNetworks (GANs) to solve the IRL problem by simultaneously learn-\ning a policy and a reward function. AIRL is based on the maximum"}, {"title": "4 METHOD", "content": "This work presents a novel approach that addresses the challenge\nof teaching cobots to perform manipulation tasks in a human-like\nmanner. We first introduce Visual IRL which intends to learn the task\npreferences using human body keypoints and object location data.\nThen, we present the Neuro-Symbolic Dynamics Mapping model\nthat intelligently translates the human arm's motion dynamics to a\ncobot with a different DoF. This model enables the cobot to apply\nthe learned policy with joint motions similar to a human subject.\nIn order to learn the human's intentions as well as human kinematics,\nwe suggest a novel IRL method, called Visual IRL. Visual IRL uses\nhuman arm keypoints as part of its state and action parameters to\nlearn the human kinematics and takes advantage of AIRL to learn\nthe task preferences. Moreover, it utilizes the 3D location of the\nobject of interest to discover human intentions further.\nProblem Formulation. In the Visual Inverse Reinforcement Learn-\ning (Visual IRL) method, we model the problem as a single-agent\nMarkov Decision Process defined as MDP = (S, A, T, R) situated in\na Cartesian space. The state space (S) includes the end-effector lo-\ncation (ee floc), object location (onnloc), and object label prediction\n(onnpred), all in 3D space. The action space (A) comprises changes\nin the end-effector's 3D coordinates, represented as A(x, y, z). The\nreward function (R) assigns positive values for optimal manipu-\nlation actions. The transition function (T) models the probability\nof transitioning between states given specific actions, defined as\nT:S\u00d7S\u00d7A\u2192 [0, 1]. The goal is to learn both a policy (\u03c0) that"}, {"title": "4.1 Visual IRL", "content": "causal entropy IRL framework [42], which considers an entropy-\nregularized MDP. The goal is to find the optimal policy \u03c0* that\nmaximizes the expected entropy-regularized discounted reward:\n$\u03c0^* = arg max_\u03c0 \u0395_{\u03c4\u223c\u03c0}[\u03a3_{t=0}^{T}\u03b3^t (Re(s_t, a_t) + H(\u03c0(\u00b7|s_t)))]$ (1)\nwhere \u03c4 = (so, ao, . . ., ST, a\u0162) denotes a trajectory, and H(\u03c0(\u00b7|st))\nrepresents the policy's entropy at state st. AIRL frames the IRL prob-\nlem as a GAN optimization [17], with the discriminator De(s, a)\ndistinguishing between expert demonstrations and generated trajec-\ntories:\n$D_e(s, a) = \\frac{exp(f_0(s, a))}{exp(f_0(s, a)) + \u03c0(a|s)}$ (2)\nwhere fo (s, a) approximates the advantage function. At optimality,\nfo(s, a) = log \u03c0* (a|s) = A*(s, a). The reward function is updated\nbased on the discriminator's output:\n$R_e(s, a) = log D_e(s, a) \u2013 log(1 \u2013 D_e(s, a)).$ (3)\nThe policy n is optimized with respect to the learned reward func-\ntion Re using any policy optimization method, such as Trust Region\nPolicy Optimization (TRPO) [27]. The iterative process of AIRL\nensures the reward function is disentangled from the environment\ndynamics, leading to better generalization across different tasks and\nenvironments [16]."}, {"title": "4.2 Neuro-Symbolic Human-to-Robot Dynamics\nMapping", "content": "maps states containing human keypoints to actions involving the\nend-effector's movements and a reward function (R) that reflects the\nhuman's preferences, denoted as \u03c0\u0397: SH \u2192 AH and R: SXA \u2192 R,\nrespectively.\nAIRL Implementation. Although Section 3 outlined AIRL concep-\ntually, here we highlight how Visual IRL obtains the reward function\nR(s, a) and policy \u03c0 from human demonstrations. Specifically, we\ncollect expert trajectories (st, at) derived from pose keypoints and\nobject localization, and feed these into the AIRL framework. Within\nAIRL, a discriminator De(s, a) (Eq. 2) distinguishes expert from\ngenerated trajectories, thereby refining the learned reward:\n$R_e(s, a) = log D_e(s, a) \u2013 log(1 \u2013 D_e(s, a)).$ (4)\nSubsequently, we apply TRPO to optimize a policy \u03c0(als) that\nmaximizes the cumulative reward. Thus, IRL in our pipeline is not\nmerely replicating poses but inferring task-specific preferences from\ndemonstrations, ensuring that the cobot's learned behaviors align\nwith expert intentions and human-like motion dynamics.\nHuman Keypoint Detection. In order to learn the human arm mo-\ntion dynamics, the human body skeleton should be detected and\ntracked at every time step. In this regard, YOLOv8 pose estimation\nmodel is used as the base model for 2D human keypoint detection.\nThe 2D output coordinates get converted to 3D using the camera's\ninternal and external intrinsic information. While the two models\nare highly accurate in most cases, situations may arise where one or\nboth of them fail to detect joint locations due to occlusions, rapid\nmovements, or sensor noise. To address this issue, we suggest a\nkeypoint prediction model that can estimate the 3D coordinates of\nthe human arm and hip joints $(hip_x, hip_y, hip_z), (shd_x, shd_y, shd_z), (elb_x, elb_y, elb_z), (wrs_x, wrs_y, wrs_z)$, when\npose estimation predictions are unreliable. The input features in-\nclude the current (t) and previous (t - 1) 3D coordinates of the hip,\nshoulder, elbow, wrist, index finger, and thumb joints, represented\nas $(hip^t, shd^t, elb^t, wrs^t)$ and $(hip^{t-1}, shd^{t-1}, elb^{t-1}, wrs^{t-1})$, respectively.\nThe network architecture comprises two LSTM layers each con-\ntaining 64 units, followed by two fully connected layers with 256\nand 64 units using ReLU activations. The output layer predicts the\n3D coordinates for the joints, resulting in an output size of 12 includ-\ning the predicted 3D coordinates of the six input joints. The model\nis trained using the MSE loss function and the Adam optimizer with\na learning rate of 0.001.\nObject Localization and Prediction. This model aims to find the\n3D location of the object to be sorted. Initially, the custom YOLOv8\nobject detection model provides 2D bounding box coordinates and\nlabels for all of the objects of interest in the current frame. This 2D\ndata is converted to 3D using the camera's intrinsic parameters. Sim-\nilar to pose estimation models, the YOLOv8 object detection model\nmight lose track of objects due to occlusions or rapid movements\nof the human or robot. For such scenarios, we propose a model that\nuses the previous 3D location of the object (objloc\u00b9) and the previous\n(eeffto\u00b9) and current (eeffo) locations of the end-effector to predict\nthe current 3D location of the object (objloc):\n$obj^{loc} = f (obj^{loc}_{t-1}, eef^{loc}_{t-1}, eef^{loc}_{t}).$ (5)\nThe model's architecture includes an input layer processing se-\nquences of 3D coordinates, followed by two LSTM layers with 64\nand 32 units, and two fully connected layers with 256 and 64 units\nusing ReLU activations. The output layer provides adjusted 3D co-\nordinates of the object. The model ensures accurate localization by\nleveraging spatial and temporal data from previous predictions and\ncurrent detections, enhancing the reliability of the pipeline.\nAs part of our framework, we propose a Neuro-Symbolic Dynamics\nMapping method to effectively transfer human motion dynamics\nto a robot. This method ensures accurate end-effector placement\nwhile minimizing joint adjustments, thereby preserving the natural"}, {"title": "4.3 Human-Robot Dynamics Mapping", "content": "dynamics of human motion in robotic tasks such as pick-and-place\noperations. As an example, the process of mapping the human kine-\nmatics to Sawyer (which is a 7 DoF cobot) is shown in Fig. 2. This\nmethod involves multiple chronological steps, explained below in\ndetail in ascending order.\nHuman Joints Inverse Kinematics Model. To transfer the human\nkinematics to the cobot, the wrist joint can be used as the reference\npoint and other joints can be aligned accordingly. The human joints\ninverse kinematics model predicts the 3D coordinates of the human\nhip, shoulder, and elbow joints based on the 3D location of the wrist.\nThe model takes the 3D coordinates of the wrist at each time step\nand predicts the 3D coordinates of the other joints for the same time\nstep. Although the training data may contain multiple samples with\nsimilar wrist locations yet slightly different human poses, the inverse\nkinematics model averages these variations to determine a repre-\nsentative human pose. The model consists of three fully connected\nlayers with 256, 64, and 32 units using ReLU activations, followed\nby an output layer predicting the 3D coordinates of the three joints,\nresulting in a total of 9 output labels. The model is trained using the\nMean Squared Error (MSE) loss function and the Adam optimizer\nwith a learning rate of 0.001.\nSymbolic Mapping of Human Joints to Cobot Joints. This sym-\nbolic model maps human joint angles (hip, shoulder, elbow, and\nwrist) to the cobot's joint angles. The one-to-one mapping of human\njoints and the joints of the two cobots used in our experiments is\ndepicted in Fig. 3. The input to the model consists of human joint\nangles, and the output is the initial robot joint angles. This mapping\nprovides an initial estimate of the robot's joint configuration based\non human motion, which is the starting point for the subsequent\nneural models.\nCobot's Restricted Forward Kinematics Model. The cobot's re-\nstricted forward kinematics (FK) model aims to predict the 3D\nposition of the cobot's end-effector based on the angles of the hip,\nshoulder, elbow, and wrist joints. This model is restricted by assum-\ning that only the hip (hip), shoulder (shd), elbow (elb), and wrist\n(wrs) joints of the cobot are moving, while other joints remain sta-\ntionary (fixed at 0 degrees). The motivation here is to maximize the\nsimilarity of the cobot's arm posture to the human since the robot has\nextra joints. The input to the model consists of the cobot's hip, shoul-\nder, elbow, and wrist angles, and the output is the 3D position of the\nend-effector (x, y, z). The model architecture includes an input layer\naccepting the joint angles, followed by two fully connected layers\nwith 256 and 64 units using ReLU activations. The final output layer\npredicts the 3D coordinates of the end-effector. The model is trained\nusing the Mean Squared Error (MSE) loss function and the Adam\noptimizer with a learning rate of 0.001. The training data is collected\nby running the cobot on Gazebo and Rviz simulations, recording the\njoint movements and corresponding end-effector positions.\nCobot's Inverse Kinematics Model. This model is designed to\noptimize the joint angles of the robot's hip (hip), shoulder (shd),\nelbow (elb), and wrist (wrs) to ensure that the robot's end-effector\nreaches the desired target location while maintaining a human-like\npose. The model starts with an initial guess for the joint angles and\niteratively uses the trained forward kinematics model to estimate\nthe end-effector's position based on the updated angles. The inputs\nto the model are the initial robot's hip, shoulder, elbow, and wrist\nangles (hip, shd, elb, wrs), and the target end-effector position in\n3D space (x, y, z). The output of the model is the adjusted robot's\nhip, shoulder, elbow, and wrist angles (hip', shd', elb', wrs').\nThe optimization process involves minimizing the distance be-\ntween the estimated end-effector location and the desired location\nwhile also minimizing changes to the initial joint angles. The model\nis trained using the Mean Squared Error (MSE) loss function, and the\noptimization is performed using the Adam optimizer with a learning\nrate of 0.01. The optimization objective includes two components in\nthe loss function: position error and adjustment penalty. The position\nerror measures the difference between the predicted end-effector\nposition and the desired position, while the adjustment penalty mea-\nsures the deviation of the adjusted joint angles from the initial joint\nangles.\nThe loss function L can be formulated as follows:\n$L = ||p_{ee} - p_{ee^*}|| + \u03b1||\u03b8 \u2013 \u03b8_0||$ (6)\nwhere pee is the estimated end-effector position, pee* is the desired\nend-effector position, 0 represents the current joint angles, 00 repre-\nsents the initial joint angles, and a = 0.0005 is a weight parameter\nbalancing the importance of position accuracy and minimal adjust-\nment.\nThe optimization proceeds iteratively for a maximum of 10,000\niterations, with the learning rate decaying by a factor of 0.9 every\n1,000 steps. The process converges when the position error is below\na specified threshold (0.01 m). The model also employs gradient\nclipping with a maximum norm of 1.0 to stabilize the optimization\nprocess. The optimized joint angles (hip', shd', elb', wrs') are ob-\ntained when the model converges, ensuring the cobot's end-effector\nreaches the desired target location while maintaining a human-like\npose.\nRationale for Restricted Kinematics. Standard URDF-based for-\nward and inverse kinematics typically find any valid configuration\nmatching the end-effector pose, potentially activating joints that do\nnot mirror human arm motions. In contrast, our Restricted FK model\n(Section 4) fixes non-human-like joints, allowing only those (e.g.\nhip, shoulder, elbow, wrist) that correspond to human arm segments\nto move. Likewise, our specialized IK model jointly minimizes pose\nerror and deviations from the initial human-like angles. This ensures\nthat the resulting solution not only reaches the target position but also\npreserves the human motion dynamics captured from demonstra-\ntions. Such trainable, domain-specific kinematics models are flexible\nacross different robot DoFs, which standard URDF solvers alone do\nnot directly offer, especially when the human arm's joint mapping\ndoes not correspond one-to-one with a robot's entire kinematic chain."}, {"title": "5 EXPERIMENTS", "content": "We demonstrate the performance of the proposed model by evalu-\nating it in two applicable real-world robotic manipulation domains.\nThe targeted tasks involve the robotic automation of processing line\ntasks. We start by providing a brief description of the tasks followed\nby our experimentation procedures and results."}, {"title": "5.1 Domain Specifications", "content": "Domain Overview. We first evaluate our proposed model on a\nline sorting task where the physical cobot Sawyer (from Rethink\nRobotics) is tasked with observing and learning how a human sorts\nan arbitrary number of onions (i.e., separating blemished onions\nfrom unblemished ones). The human expert looks at the onions\non the conveyance and constantly tries to detect whether onions\nare blemished or not while sorting them. If an onion is detected\nas blemished while being on the conveyance, it will be picked and\ndiscarded into a bin positioned next to the conveyor belt. Otherwise,\nthe human picks up the onion and meticulously inspects it. If the\nonion is recognized as blemished after inspection, it will be discarded\ninto the bin. Conversely, if it is deemed unblemished, the expert\nreturns it to the corner of the conveyance and proceeds to inspect the\nnext onion until all onions are processed.\nThe second task used to evaluate our model is liquid pouring\nwhere the physical cobot LBR iisy (from KUKA Robotics) performs\nthe observation and learning of how a human expert pours the con-\ntents of different colored bottles into designated containers. In this\nsetup, we have an arbitrary number of blue and red bottles while\nthere are only two containers with blue and black colors. The con-\ntents of blue bottles should be transferred into the blue container\nwhile the liquid in red bottles should be poured into the black con-\ntainer. After depleting each bottle, regardless of its color, the empty\nbottle should be dropped into a bin. In this setup, the focus is on\nrobust pick-and-place style manipulations: grasping bottles, aligning\nthem with the designated containers, and pouring. The pipeline does\nnot explicitly model liquid flow or track pouring volume in real-time.\nInstead, once the cobot's end-effector aligns the bottle with the con-\ntainer, the actual pouring action is triggered by a simple tilt.\nDomain Configuration. In our setting, both cobots utilize an Intel\nRealSense L515 RGB-D camera positioned in front of the human\nexpert and the conveyor belt. The conversion from a 2D pixel lo-\ncation (x, y) to 3D world coordinates involves two steps. First, the"}, {"title": "5.2 Baseline", "content": "pixel coordinates are converted to camera coordinates using depth\ndata and camera intrinsics:\n$X = \\frac{(x - c_x).z}{f_x} Y = \\frac{(y-c_y).z}{f_y} Z = \\frac{Z}{1000}$\nThen, the camera 3D coordinates are transformed into the cobot's\n3D world coordinates (Ycobot, Zcobot, Xcobot), based on the relative\nspatial location of the camera with respect to the cobot's world origin.\nThe camera position remains stationary throughout the experiment\nto simplify the 2D to 3D coordinate conversion process.\nState and Action Parameters. In order to learn the human expert's\ntask preferences using Visual IRL and transfer this knowledge to\nthe cobot to perform the task in a human-like manner, we have\nformulated both tasks as an MDP problem.\nThe state can be adequately represented by three variables: the\npredicted label and the 3D spatial location of the object of inter-\nest and the 3D spatial location of the end-effector. The 3D spatial\nlocations are continuous values and the status of each object can\nbe labeled with respect to the task. For instance, in the line sort-\ning, the status of the onion is either blemished, unblemished, or\nunknown; while in the liquid pouring task, the status of the bottle is\neither red, blue, or unknown. The action is defined as the 3D spatial\ndisplacement of the end-effector in two consecutive time steps.\nMultiple objects (e.g. onions or bottles) usually appear in a single\nframe. To focus our object location prediction model on the object\ncurrently being manipulated, we establish a set of conditions to\nachieve the desired outcome. Before assessing these conditions,\nthe object detection results for the current frame are arranged in\nascending order based on their three-dimensional Y coordinates.\nSubsequently, each candidate bounding box and its predicted label\nare taken into consideration:\n\u2022 If no object is detected with classification confidence of 0.5 or\nhigher, the label of the object is marked as unknown, and minus\ninfinity is assigned to its 3D coordinates."}, {"title": "5.3 Evaluation Criteria", "content": "We assess our proposed method's ability to transfer the behavior to\nthe cobot in comparison to two well-known path planners, namely\nRRT and RRT-connect. The Rapidly-exploring Random Tree\n(RRT) algorithm [20] operates directly in the cobot's workspace.\nThe goal of RRT is to find a collision-free path by sampling random\npoints in the Cartesian space and incrementally building a tree rooted\nat the start position. Each new point is connected to the closest node\nin the tree, extending towards the target while avoiding obstacles. At\nevery time step, the target end-effector location is calculated using\nthe current end-effector location and the learned policy's action (end-\neffector displacement). The target location is then passed to RRT\nto find the desired joint angles to reach a certain threshold near the\ntarget. The RRT-connect algorithm [18] builds upon RRT by grow-\ning two trees simultaneously-one from the start and one from the\ngoal-aiming to connect them. This bidirectional approach allows\nfor faster exploration and pathfinding as the trees extend toward each\nother, leading to quicker convergence compared to standard RRT.\nRRT is used as the baseline for the onion-sorting task and RRT-\nconnect is the baseline for liquid pouring and their performance\nis compared with our proposed neuro-symbolic dynamics map-\nping model. We also compare two state-of-the-art object detection\n(YOLOv8 and Faster-RCNN) and two pose estimation (Mediapipe\nand YOLOv8) models to find the ones with the highest performance\nto be used as part of our methodology.\nThis subsection outlines the evaluation criteria used to assess the\nperformance of the proposed method. We employ several standard\nstatistical metrics including Mean Squared Error (MSE), Root Mean\nSquared Error (RMSE), Mean Absolute Error (MAE), and R-squared\n(R2) which are widely recognized in statistical analysis for evaluating\nthe performance of machine learning models [8].\nIn addition to these metrics, we use the following criteria to\ncompare our method with the baselines:\n\u2022 Learned Behavior Accuracy (LBA): Commonly used in\nIRL, LBA is computed as the number of demonstrated state-\naction pairs that match between using the true and learned\npolicies expressed as a percentage of the former which\nevaluates how accurately the model replicates observed\nbehaviors.[2]\n\u2022 Average Sorting Time: The mean time required to perform\nmanipulation for a single object, serving as an efficiency\nmetric.\n\u2022 Average End-Effector Distance: The mean distance trav-\neled by the robotic arm's end-effector during manipulation\nof a single object, reflecting operational efficiency.\n\u2022 Average Movement Jerkiness: The mean angular move-\nment of robotic joints during manipulation of a single object,\nwith lower values indicating smoother, more human-like\nmovements. Jerkiness is sampled every 100 milliseconds\nand summed for each sorting task."}, {"title": "5.4 Formative Evaluation", "content": "For each of the two tasks", "steps": "pose estimation and keypoint\nprediction. In order to select the most accurate pose estimation\nmodel, we focused on two state-of-the-art models Mediapipe\nand YOLOv8 and evaluated their performance on 3000 frames\nof a human expert performing the onion-sorting task. In our Neuro-\nSymbolic Dynamics Mapping model, four keypoints the right\nhip, right shoulder, right elbow"}]}