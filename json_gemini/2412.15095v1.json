{"title": "A Full Transformer-based Framework for Automatic Pain Estimation using Videos", "authors": ["Stefanos Gkikas", "Manolis Tsiknakis"], "abstract": "Abstract\u2014The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalization capability across all the primary pain estimation tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Pain, according to Williams and Craig [1], is \"a distressing experience associated with actual or potential tissue damage with sensory, emotional, cognitive and social components\". From a biological perspective, pain is an unfavorable sensation that originates from the peripheral nervous system. Its primary function is to activate sensory neurons and alert the organism to potentially harmful situations, thus serving as a vital mechanism for identifying and responding to threats [2]. The Global Burden of Disease (GBD) study refers that pain is the number one cause of years lived with disability (YLD), concerning not only individuals but also society as a whole, constituting clinical, economic, and social constraints [3]. The primary types of pain are acute and chronic. The major difference between them is related to the duration; when it is present for less than three months, the pain is considered acute and probably accompanied by physical damage, while chronic perseveres the recovery process [4]. People of all ages experience painful situations due to an accident, illness, or even during treatment, provoking a plethora of daily life challenges. Especially in chronic pain conditions, additional mental health problems, e.g., anxiety, depression, and sleep-related problems, commonly occur [5]. Furthermore, inadequate pain management often leads to negative collateral consequences associated with drug overuse, opioids, and addiction [6]. A crucial matter that needs focused attention is the welfare of vulnerable groups who may not be able to communicate directly or objectively. Their pain assessment is usually based on observing behavioral or physiological responses from caregivers or family members. This specific setting often leads to wrong or insufficient assessment for two main reasons: continuous monitoring is challenging without adopting technology-based solutions, and the estimation's precision is often minimal due to inadequate training or prejudices [7]. Further challenges arise with the elderly where either diminished manifestation ability or even unwilling communication behavior are presented [8]. In addition, an essential body of research [9][10] indicates significant variations of pain manifestation among people of different gender and age, suggesting that the pain assessment is an even more intricate process requiring increased consideration. The automatic pain estimation procedure is founded on utilizing behavioral and physiological modalities. The primary behavioral modalities include facial expressions, body-head movements, gestures, and vocalizations, while the physiological include electrocardiography, electromyography, and skin conductance responses.\nThe mainstream deep neural architectures in computer vision (CV) are the Convolutional Neural Networks (CNN). Especially in the research field of automatic pain assessment elaborating images/videos, CNNs are the fundamental component of every approach [5]. The domination of transformer architecture [11] in natural language processing (NLP), where their core element is the self-attention mechanism, inspired researchers to develop equivalent models for visual applications. The introduction of Vision Transformers (ViT) [12] led to the creation of a new paradigm of architecture in the computer vision domain. A plethora of new approaches has developed on the basis of ViT. Such an approach is the Transformer in Transformer (TNT) [13], which enhances the local feature representation by the further division of the patches into sub-patches. Despite the impressive results and flexibility of the transformer-based models, they scale poorly with the input size and increase the computational cost because of the self-attention layers which compare the input to every other input. Several efforts have been made to reduce the complexity and improve the efficiency of such architectures. The primary approach is the replacement of self-attention with cross-attention [14] or the incorporation of both [15].\nIn this study, we develop a framework consisting of a TNT model, which is utilized as the \"spatial feature extraction module\" applied to each video frame, and a transformer-based model with cross and self-attention blocks as the \"temporal feature extraction module\u201d applied to each feature sequence of videos. In this way, we can exploit the temporal dimension of videos and offer more reliable estimations about the continuous nature of the pain sensation. The remaining of this study is organized as follows: in Section"}, {"title": "II. RELATED WORK", "content": "Numerous research efforts have been made to estimate the pain level in humans utilizing videos. Zhi et Wan [16] trying to capture pain's dynamic nature, they developed Long Short-term Memory Networks with sparse coding (SLSTM). Tavakolian et al. [17] developed 3D CNNs with kernels of various temporal depths capturing short, mid, and long-range facial expressions. Similarly, in [18], the authors proposed a 3D CNN but combined it with self-attention structures to increase the importance of specific input dimensions. Thiam et al. [19] adopted two strategies to exploit video's temporal dimension. Initially, they encoded the video frames into motion history and optical flow images, and after, they designed a framework incorporating a CNN and a bidirectional LSTM (biLSTM). In a similar manner, the authors in [20] also encoded videos into single RGB images employing statistical spatio-temporal distillation (SSD) and followed by a Siamese network trained in a self-supervised setting. Werner et al. [21] followed a domain-specific feature approach, proposing a set of markers describing facial actions and their dynamics and classifying them with a deep random forest (RF) classifier, while Patania et al. [22] adopted Deep Graph Neural Networks (GNN) architectures and dense maps of fiducial points in order to detect pain. Finally, Xin et al. [23] presented a multi-task framework, estimating the person's identity beyond the pain level, comprising a CNN with an autoencoder attention module.\nRegarding transformer-based methods, the only study proposed in [24] where the authors developed a deep attention transformer framework that consists of a ResNet subnetwork extracting frame-based features and a transformer model capturing the temporal relationship among the frames."}, {"title": "III. METHODOLOGY", "content": "This section describes the employed database, the preprocessing methods, the design of our framework, as well as implementation details regarding the training procedure."}, {"title": "A. Preprocessing", "content": "Before feeding videos into our model for the pain estimation procedure, it was necessary to apply face detection and alignment for performance and computational efficiency improvements. We combined the well-known face detector MTCNN [25] with the Face Alignment Network (FAN) [26], which utilizes 3D landmarks. The 3D approach is essential to our problem since the head movements in several cases, especially in high-intensity pain, are increased, leading to erroneous alignment from 2D approaches. We also note that all the experiments were conducted utilizing frames of resolution 224 x 224 pixels."}, {"title": "B. Transformer-based Framework", "content": "Our framework consists of two main components; the \"spatial feature extraction module\", i.e., a TNT model, and the \"temporal feature extraction module\", i.e., a transformer with cross and self-attention blocks. In Figure 2, we illustrate our proposed framework, which consists of 24 million parameters and 4.2 giga floating point operations (GFLOPS).\n1) Spatial feature extraction module: Similarly to standard ViT, every given frame is initially split into n patches\n$J_k \\rightarrow [F_{k,1}, F_{k,2}, ...F_{k,n}] \\in []R^{n \\times p \\times p \\times 3}$, where p x p is the resolution of each patch (i.e., 16 \u00d7 16) and 3 is the number of color channels. Afterward, the patches are further divided into m sub-patches for the model to learn both global and local feature representations of the frame. Consequently, every input frame of a video is transformed into a sequence of patches and sub-patches:\n$J_k \\rightarrow [F_{k,n,1}, F_{k,n,2} ..., F_{k,n,m}]$,\nwhere $F_{k,n,m} \\in R^{s \\times s \\times 3}$ is the m-th sub-patch of n-th patch of k-th frame of each video, while s \u00d7 s is resolution of each sub-patch (i.e., 4 \u00d7 4). Next, the patches and the sub-patches with a linear projection are transformed into embeddings Z and Y. The following step is the position embedding, where the spatial information of each patch and sub-patch is retained. This procedure is based on the 1D learnable position encoding, where for each patch, the following position encodings is assigned:\n$Z_o \\rightarrow Z_o + E_{patch}$,\nwhere $E_{patch}$ are the patch position encodings. Respectively, for each sub-patch within a patch, a position encoding is added:\n$Y_o \\rightarrow Y_o + E_{sub-patch}$,\nwhere $E_{sub-patch}$ are the sub-patch position encodings and i = 1,2,...m is the index of a sub-patch within a patch. Next, the sub-patches are led to a transformer encoder called an \"Inner Transformer Encoder\", consisting of 2 multi-head self-attention blocks, which are essentially dot product attention. The attention is expressed as follows:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V$,\nwhere $Q \\in R^{M \\times D}$, $K \\in R^{M \\times C}$ and $V \\in R^{M \\times C}$ (M is input dimension, C and D are channel dimensions) are"}, {"title": "IV. EXPERIMENTS & RESULTS", "content": "In this section, we present the conducted experiments regarding pain estimation. We note that the experiments were performed in binary and multi-level classification settings. Specifically, (1) NP vs. P1, (2) NP vs. P2, (3) NP vs. P3, (4) NP vs. P4 respecting the binary classification tasks, and finally, (5) multi-level pain classification, utilizing all the available pain classes of the database. The evaluation protocol that we followed is the leave-one-subject-out (LOSO) cross-validation. Furthermore, the classification metrics adopted"}, {"title": "A. Pain Estimation", "content": "Regarding the classification results of the pain estimation tasks, we observe the following: on NP vs. P1, we achieved 65.95% accuracy, while the precision is close to it with 65.90%. Similarly, the F1 score is 65.04, and interestingly the recall (sensitivity) is 67.85%. On NP vs. P2, the accuracy increased to 66.87% as also the other performance metrics, especially the F1 score, which increased over 1.15% showing the improvement in the detection of true positive samples. On NP vs. P3, the increase in the performances is particularly noticeable. We attained 69.22% accuracy, while the sensitivity improved to 70.84%. The classification improvement is reasonable since the pain is characterized as severe at the P3 level, and the subjects' manifestations become more intense. On the task with the higher level of pain, i.e., NP vs. P4, the recall is 74.75%, while in terms of accuracy, we achieved 73.28%. It is evident that recognizing very severe pain is the most straightforward identification task considering that the pain threshold is on the tolerance limits, and most subjects demonstrate it clearly with their facial expressions. Finally, the range of performances is diminished in the last task, i.e., the multi-level classification, since estimating all levels simultaneously is a more challenging procedure. We attained 31.52% accuracy and recall of 29.94%, indicating that the ability to detect true positive samples in this task has more challenges.\nAt this point, we want to highlight that our framework regarding both the architecture and the training procedure remained identical across all tasks, binary and multi-level classification tasks. Our purpose was to study the generalization capabilities of our method for every possible scenario (within the limits of the database) similar to clinical settings."}, {"title": "B. Video Sampling", "content": "In this section, we study the effect of video sampling on automatic pain estimation. The experiments in IV-A were conducted utilizing all the available frames (i.e., 138) from each video. In the following experiments, we sample frames with a stride of 2, 3, and 4. Initially, utilizing all 138 frames leads to a video feature representation D with a size of"}, {"title": "C. Interpretation", "content": "An important area of research, especially in the deep learning-related fields, is the interpretability of the models to provide explanations for the decisions making. This is especially true regarding healthcare topics since the transparency improvement of these models is essential for their acceptance and adoption in the clinical domain. In this study, we adopted the method of [29] to create relevance maps displaying in which facial areas our model, i.e., the \u201cspatial feature extraction module\u201d, pays attention. Examples of the relevance maps are shown in Figure 4. We notice that in the initiation of a facial expression sequence, the model attends in \"arbitrary\" areas. As the pain progression continues, the attention becomes more precise to regions that manifest the painful occurrence. We want to point out that according to"}, {"title": "D. Comparison with existing methods", "content": "Finally, in this section, we compare our accomplished results employing the transformer-based framework (using all available frames per video) with studies that utilize the Part A of the BioVid database with all 87 subjects and follow the identical evaluation protocol, i.e., leave-one-subject-out (LOSO) cross-validation, in order to perform objective and accurate comparisons. Table IV shows the corresponding results, where there are three main groups of studies; i) studies conducting exclusive pain detection (NP vs. P4), ii) studies examing pain detection and multi-level pain estimation, and finally, iii) studies exploring all the major pain-related tasks. Our approach, comparing it with the studies that conducted experiments on every task, achieved the highest performances on both binary and multi-level pain estimations. Regarding the studies that were performed solely on pain detection or/and multi-level pain estimation, our method attained comparable or even better results, e.g., [19][20][22]. We observe that the performances come from the restricted in terms of experiments studies tend to be higher. In our view, however, the importance of researching and developing systems capable of performing adequately in every scenario is greater."}, {"title": "V. CONCLUSIONS", "content": "This study explored the application of the transformer-based architecture for automatic pain estimation using videos. We developed a framework that consisted exclusively of transformer models, exploiting both the spatial and temporal dimensions of the frame sequences. The conducted experiments revealed the efficacy of our framework in assessing pain and demonstrating the generalization capabilities to accomplish every pain estimation task with satisfactory classification results, especially in low-intensity pain where the facial expressions are subtle. Furthermore, we showed that our proposed framework is characterized by high efficiency and is able to perform in real-time settings. Another important aspect of our study is the creation of relevance maps demonstrating to which facial areas the model pays attention. We believe that more efforts from the affective-computing community are needed to improve the interpretability of the adopted deep-learning approaches.\nIn addition, we suggest that future studies include details regarding the computational cost of their approaches, e.g., throughput measurements, number of model parameters, or number of FLOPS, to assess their real-time application. Although it may not be the primary focus of the studies, it is still relevant. The comparison with other related methods demonstrated comparable or improved results depending on the corresponding training scenario. We believe that future research regarding the automatic pain estimation field needs to investigate all available tasks since they clinically provide essential information for pain management.\nIt is worth noting that our current approach may benefit from the utilization of more complex modules. Specifically, increasing the number of attention heads and blocks in the transformer models, or enlarging the extracted feature vectors, would result in a more comprehensive representation of the data. However, it should be acknowledged that implementing such modifications would also come with a significant increase in computational cost and time requirements."}]}