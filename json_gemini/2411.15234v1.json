{"title": "Adaptive Intelligence: leveraging insights from adaptive behavior in animals to build flexible AI systems", "authors": ["Mackenzie Weygandt Mathis"], "abstract": "Biological intelligence is inherently adaptive - animals continually adjust their actions based on environmental feed- back. However, creating adaptive artificial intelligence (AI) remains a major challenge. The next frontier is to go beyond traditional AI to develop \u201cadaptive intelligence,\u201d defined here as harnessing insights from biological intelligence to build agents that can learn online, generalize, and rapidly adapt to changes in their environment. Recent advances in neuroscience offer inspiration through studies that increasingly focus on how animals naturally learn and adapt their world models. In this Perspective, I will review the behavioral and neural foundations of adaptive biological intelligence, the parallel progress in AI, and explore brain-inspired approaches for building more adaptive algorithms.", "sections": [{"title": null, "content": "Our world is a series of predictions made in our mind. That is, to us, the world is a construct, built conceptually from a series of millions of predictions harbored in the neural code. When we walk on the beach, we accumulate photons with our eyes, sound waves with our ears, we feel the wind on our face, and the sand shifting under our feet, and all of this comes together to give us the perception of a warm, sunny day with ocean waves crashing on the shore. However, our brain sits in the dark confines of our skull and tens of billions of neurons communicate through electrical activity in a bath of chemicals. As incoming sensory information is processed in the brain at different times, the brain imposes a lag in or- der to compile it, thrusting our perception into the past tens of precious milliseconds behind our perceived reality. Never- theless, we need to act quickly, and our brains have evidently evolved solutions to overcome these delays.\nMoreover, we need to adapt to environments marked by un- predictability, continuous change, and infrequent repetition of decision-making scenarios. The uncertainty, variability, and complexity of these settings demand adaptive strategies, crucial for learning about local changes \u2013 such as the shifting sand under our feet. Dating back decades, researchers have developed new frameworks to understand (decompile) how this global prediction system may work (1-4). A core tenant has been that in order to overcome sensory-processing de- lays we must build internal models of the world. We use said models to then make predictions about the sensory and mo- tor consequences of our actions (5\u20138). But it remains largely unknown how the brain enables adaptive behavior, and how we can take inspiration from the brain to build more adaptive artificial systems (9, 10).\nDating at least as far back as Aristotle, there has been a philo- sophical and scientific question of how our brains build mod- els of the world (11). How is it that we can combine sen- sory information into perception? How long does this take? Are we always tens, or even hundreds of milliseconds behind reality? How could our brains make predictions about the"}, {"title": null, "content": "world in order to act faster? Why do we mostly notice when the world violates our predictions? \u2013 if the ocean were sud- denly quiet, we would instantly worry we had lost our hearing and not doubt that the ocean had completely changed. While this is familiar in everyday life, we also need to game-ify this approach to study it in the laboratory.\nWhat is the neural basis of this behavior? This historically has been difficult to answer, because we are always under- sampling the data and often want repeated measures. The mouse brain has an estimated 70 million neurons, yet even the most state-of-the-art technology allows for recording up to 1 million individual neurons at a time (12) (and on the order of only 3Hz resolution), while most labs are on the or- der of hundreds to thousands with two-photon microscopy or high-density arrays such as Neuropixels (13, 14). Thus, if one aims to study how a neuron encodes a given stimu- lus, you can repeatedly subject the animal to the stimulus and build encoding and decoding models (15) to gain insight into what the neural activity represents. However, if you want to understand how the brain adapts, there is an inherent problem with this approach. As Heraclitus infamously quipped, \"No man ever steps in the same river twice, for it's not the same river and he's not the same man\u201d; the neural code is changing dynamically so averaging across trials becomes, at best, an over simplification that would miss core-principles of neural computations.\nThus, to study the neural mechanisms underlying adaptive behavior we need three ingredients: (1) behavioral paradigms with measurable rapid-learning (within-session); (2) large- scale neural recordings; and (3) data analysis methods that consider time-series and can leverage many neurons, either within a single subject or across subjects, for single trial, continuous analysis. The first I will cover here, while the second I will not cover but rather refer the reader to reviews covering the rapid pace of technology development in two- photon imaging, flexible neural implants, and high-density recordings (16\u201318). For the third, I will briefly discuss algo-"}, {"title": "Adaptive animal & neural behavior", "content": "There has been a concurrent push for more naturalistic behav- iors, more complex behaviors, and tasks that enable within- session learning. Aside from new large-scale neural record- ing technology, a part of this is due to the advances in mark- erless tracking of behavior (reviewed in Mathis and Mathis (21)), which has made digitizing movements much more tractable. What behaviors are being developed? While I cannot cover all progress in the field, I want to highlight several new approaches. Freely moving paradigms for ro- dents have rapidly advanced to encompass complex stimuli such as moving (threatening) novel objects (22), real-time VR worlds (23), labyrinth mazes (19), extended multi-area home cages (24, 25), and 3D environments such as those used to study depth estimation (26). In several settings they have also built trial-based assays that smartly constrain these nat- uralistic settings in order to be able to directly probe learning across epochs.\nAn elegant thread of work across multiple groups aims to build tasks for zero- or few-shot learning in complex spa- tial worlds. For example, Rosenberg et al. (19) developed a task where mice must seek rewards in a labyrinth (Figure 1a). They found that mice can not only make up to 2000 decisions per hour, but after unsupervised exploration of the maze they could few-shot learn to find the water spout (in around 10"}, {"title": null, "content": "tries) (19). Notably, this is a learning rate that is 1000-fold higher than 2AFC tasks that are typically deployed in rodent studies of decision-making. Another example is showing that mice can zero-shot learn new sequences of rewards (such as go to location A, B, D, then C) (20). Mice were trained on multiple tasks with a shared underlying structure that orga- nized a sequence of goals, though the specific goal locations varied. Strikingly, the mice learned this common structure, allowing them to make zero-shot inferences on the first trial of new tasks (Figure 1b). Intriguingly, they also found neu- rons in medial prefrontal cortex that acted as task-structured memory buffers, namely, they tracked the progression along behaviorally-relevant steps (20).\nIn sensorimotor control there has been a long line of work on rapid learning. So-called motor adaptation studies has been developed where visual or proprioceptive information is per- turbed such that within-session, an animal must adapt. This has not only spurred the development of new neural analysis tools (15, 27-29), but a host of behavioral tasks. For exam- ple, visuomotor rotations have been used in humans and non- human primate studies to study how they can account for sen- sorimotor discrepancies in only a few hundred trials (27, 30\u2013 33). The same principle is used for changing environmen- tal dynamics of a manipulandum that causes deviations in a limb-movement tasks (7, 34-36). Notably, these tasks are specifically designed to measure both adaptive learning, and the formation of internal models. There is always a period of baseline control movements, perturbations, and a return to baseline-condition that allows researchers to behaviorally measure if an internal model was updated (Figure 1c)."}, {"title": null, "content": "Neurally, evidence shows that motor and sensory areas can change their tuning properties during the course of learn- ing (36-40). A study by Sun et al. (38) showed that dur- ing a motor adaptation task in macaques, motor cortex (M1) effectively index memories of the hand-force required for learning to adapt. Notably, the neural subspace most predic- tive of hand forces changed during the period before move- ment (preparatory), specifically during the learning epoch. In a neural dimension orthogonal to this force-predictive subspace, they identified a uniform shift across all move- ment directions, including those unaffected by learning. In- triguingly this uniform shift remained after exposure to the force field, reflecting an updated internal model. Other works also show new evidence of state-changes that persist across learning (8, 36), and temporally-resolved prediction errors (36, 41). Influential work on visual cortex (V1) has shown the existence of 'mismatch' neurons when the ex- pected visual feedback is disrupted (41), and similar predic- tion errors have been found in somatosensory (S1), motor (M1), and frontal areas of cortex (36, 40).\nSome of the best evidence for causally showing how neurons adapt has come from brain-machine-interface (BMI) studies that require the subject to directly alter neural firing in or- der to change something in the external world (like a cursor on a screen). Closed-loop systems, like calcium-based BMIs (caBMIs) or with electrical stimulation, have been used to study the timescale and subsets of neurons that can be used, as not all cell types have been found to be equally adapt- able. Fundamental work has revealed that even a few neurons can be leveraged for decoding, and for BMI-guided feed- back (27, 42). Notably, they often live in discrete subspaces of the neural dynamics. Vendrell-Llopis et al. (43) pushed this further to link these subspaces to cell-types. They trained mice to modulate the activity of either intratelencephalic (IT) or pyramidal tract (PT) neurons for reward. They found that mice learned to control PT neuron activity more quickly and effectively than IT neuron activity. This intriguingly could be related to the anatomical connectivity and differing inputs.\nRelated to cell types, the anatomy of the brain is important to consider, as this too could have direct implications for the future design of adaptive AI systems. Several areas in the brain have this incredible structure, - layers in the cerebral (neo-) and cerebellar cortex - which could be a critical part of new architectures for AI systems. There is a long-standing debate about representation emergence from data vs. archi- tecture constraints (48, 49), but it is increasingly clear that architecture is critically important and linked to function.\nFundamental work dating back to the 1970's is worth revis- iting. Vernon Mountcastle describes a framework of the or- ganizational principle of the neocortex (50). He delineates evidence that the major functional division in the neocortex is not whether an area is \"sensory\" or \"motor,\" but rather the vertical neocortical column constitutes the basic compu- tational unit, and the input-output pattern merely dictates the space of information it acts upon-namely, the \"auditory\","}, {"title": null, "content": "\"visual\" or \"motor neocortex\" has the same cellular scaffold"}, {"title": null, "content": "and should only considered a particular region based on the type of sensory input. Nonetheless, this architectural bias gives rise to computations on information with brain area- specific information.\nForm ties to function, and one emerging hot topic is how cortical circuits implement learning from prediction errors across cell types and layers. Building on the initial discovery of sensory prediction errors in cortex (41), additional works have begun to record from subtypes of neurons such as par- valbumin (PV), VIP, SST, and excitatory neurons in order to form a more complete model of how both excitatory and in- hibitory neurons across layers could implement the learning rule (44, 45, 51) (Figure 2a). This work in cortex follows ear- lier work that discovered reward prediction errors (RPEs) in the midbrain (within the ventral tegmental area) (46, 47, 52), and they are also part of an inhibitory-gated circuit, where GABAergic neurons play a critical role in the computation of RPES (53) (Figure 2b).\nRecent work aims to unify a view for how a hierarchical im- plementation of prediction errors (including RPEs) could aid in learning across sensory, cognitive, and motor systems (54). Tsai et al. (54) et al. found that during a cue-guided reward task, layer 2/3 somatosensory neurons showed enhanced re- sponses to reward-predictive stimuli, and this led to reduced reward-prediction errors and increased confidence in predic- tions. Following rule reversal, the lateral orbitofrontal cor- tex, via VIP interneurons, signaled context-prediction er- rors, reflecting a loss of confidence. The work suggests a hierarchical interaction of prediction errors across corti- cal regions, with top-down signals modulating sensory cor- tex activity Notably, prior work finds limited roles for only \"pure\" reward-based learning in motor learning, but it can be combined with sensory prediction errors to shape perfor- mance (7, 32, 55). I envision that future work will more deeply test how various learning signals are used concur- rently across systems. For example, tasks that have different types of prediction errors can be developed that might con- flict, forcing the subject to weight different errors in order to guide future actions.\nIn the context of learning and links to AI, another critical view on cell types and anatomy is neural development. For example, cells in the cortex migrate to their final resting loca- tion in different times. Moreover, winner-takes-all synapses form, such as at the neuro-muscular-junction of alpha motor neurons and muscles. There is also a complex and highly regulated developmental program in the spinal cord of tran- scription factors and hox genes the genetics lay out the global (rostral-caudal) and even local (dorsal-ventral zones with a level) connections. Moreover, minimally in cortex, early traveling, spontaneous waves form the basis of early connections between cortical neurons.\nIntriguingly, new work has highlighted how the transformer architecture naturally gives rise to waves, which are a crit-"}, {"title": null, "content": "ical feature in the brain (56). Waves are not only seen in early development (as mentioned), but also in steady-state"}, {"title": "Measuring changes in the neural code", "content": "New tools - i.e., the third ingredient are critical for the study of adaptive behavior. Computational neuroscience has made large advances in the past decade, fueled by the need to better understand ensembles of neurons (17, 57\u201359) and by leveraging the concurrent innovations in deep learning, which is reviewed in Hurwitz et al. (60) and Mathis et al. (15). In brief, algorithms such as variational autoencoders, transform- ers, contrastive learning-optimized neural networks, and dif- fusion models are already making big strides in our ability to combine data across subjects, measure model multi-region interactions, and 'decode the brain' (15).\nFor review, I want to briefly highlight several tools that enable time-series analysis in single neurons and neural populations, as these are critical for adaptive learning studies, and I sus- pect will become more popular tools for studying AI systems. One of the most impactful tools in the last 20 years has been the adoption of generalized linear models (GLMs) for neuro- scientific applications (61, 62). Here, a model is trained using a feature matrix (can be one or many features) with the goal of predicting the spikes of neurons. While GLMs can come in many flavors, as an example, to handle variability in behavior, models are trained and tested on different splits of the data (also for cross validation). Predictions from these test sets are combined, and a pseudo-R2 is calculated. This does re- quire that behavior is repeated, but also allows for flexibility; i.e., there is no direct trial-averaging or behavior-triggered averaging of the neural code.\nFor ensembles of neurons, several methods have been used, ranging from PCA for measuring neural trajectories (63, 64), contrastive learning paired with consistency metrics to mea- sure changes in the latent neural dynamics across learn- ing (29, 36), to nonlinear dynamical systems for task- and"}, {"title": null, "content": "task-irrelevant metrics (65, 66). These are more deeply cov- ered in reviews elsewhere (15, 60), but I will note that several approaches can be used for single-trial dynamics and have been especially impactful in motor learning and adaptation studies in non-human primates (59, 67). In particular, work by Azabou et al. (68) introduce a training framework and ar- chitecture designed to model population dynamics in large- scale neural recordings, which tokenizes individual spikes to capture fine temporal structures and constructs a latent rep- resentation of neural population activity. They trained this on nearly 100 hours of data and show excellent cross-task generalizing in primate reaching studies. One limitation of this approach is the requirement of labels at test-time, but in practice this is nearly always available.\nI predict we are just on the cusp of an influx of new advances in (neuro)science due to advances in AI. For example, large-language models (LLMs) are already starting to appear in behavioral neuroscience (69) for digitizing and quantifying actions in video-based data. New vision-language, hierarchi-cal, and multi-task foundation models are surely coming. For neural analysis, already several groups are actively working on building more unified encoders that can not only be used for brain-machine interfaces, but they themselves could serve as models of the brain (15, 29, 68, 70, 71). This is sure to give rise to new approaches to extract meaningful computa-tional principles from neural dynamics, and these new learn-ing rules have great potential to directly influence how we train, optimize, and deploy machine learning systems."}, {"title": "Training and learning in artificial systems", "content": "Today, most AI applications in production revolve around a train-test-deploy cycle, and therefore are inherently not adap- tive. One curates a dataset, which can be lab-project scale, or in the case of generative pretrained transformers (GPTs) nearly all the open-source content of the Internet, then split this into a train and test fraction. Most efforts benefit from leaving out-of-distribution (OOD) data\u00b9 in the test set for a realistic measure of how well the trained model generalizes."}, {"title": null, "content": "If generalization is not needed - perhaps you are training a specific model for gait analysis in a specific laboratory set- ting - then the train/test split is often a random subset of the original dataset. But for many applications, training \u201conce\" and deploying a robust model is ideal. So how do you build a robust, generalizable model that can handle changes in the world? While the remarkable progress of GPTs in LLMs and the discovery of scaling laws has massively accelerated progress in AI, we aim for something better more adaptive. What has been tried? This is where, at minimum, the sub-fields of continual learning (lifelong learning) and in-context learning come into view.\nContinual learning aims to address the challenge of devel- oping models that can learn incrementally from a contin- uous stream of data without forgetting previously acquired knowledge. Traditional machine learning models often face, catastrophic forgetting (72) when retrained on new data, but new advancements have introduced new techniques like elas- tic weight consolidation (73, 74), synaptic intelligence, and memory-replay (75, 76) to mitigate these issues.\nWhile not directly brain-inspired, Elastic Weight Consolida- tion (EWC) works by constraining certain parameters to re- duce interference between tasks, effectively mitigating catas- trophic forgetting (73). More recent work as been adapt- ing EWC to fine-tuned self-supervised models, which has yielded performance improvements on tasks that have biased datasets by maintaining knowledge of previous tasks better than traditional methods (74, 77). If we attempt to map this to neural dynamics, this might be related to the specialization of circuits. Namely, the \u201cparameters\u201d of certain areas could be fixed (such as primary receptors), while others could be more plastic (such as hippocampus and neocortex).\nOn the other hand, synaptic intelligence is a learning rule deeply inspired by brain plasticity (78). The authors devel- oped intelligent synapses that adaptively store information, minimizing the forgetting of previously learned tasks while"}, {"title": null, "content": "acquiring new ones. This approach mimics biological neural networks that balance plasticity and stability, enabling con- tinual learning in artificial neural networks. Notably, this method can perform as well as EWC but can be performed online. Now with modern scaleable compute and architec- tures, this type of learning rule could be pivotal for building more adaptive systems.\nAnother brain-inspired machine learning algorithm has been the development of memory-replay (79, 80), which mim- ics the memory systems in the hippocampus (Figure 3a). Memory-replay aims to have a memory bank of already- learned actions (or images or tokens, etc.) that can be \"replayed\" (interweaved) into the training data in order to limit catastrophic forgetting (Figure 3b). This became a popular technique in reinforcement learning, and more re- cently, in computer vision and in LLMs. This training ap- proach can also be used during active learning when new data (unlabeled, pseudo-labeled or labeled) can be used to fine- tune a model. If the input data continues to morph out-of-distribution (OOD) then using memory-replay can be lever- aged to be sure the prior performance can be achieved.\nSeveral memory-replay-based approaches in LLMs emerged after the release of ChatGPT in November 2022 (with it's impressive language abilities yet limited content win- dow, i.e., lack of memory), such as AmadeusGPT (69), MemGPT (82), and Voyager (83). AmadeusGPT introduced a short- and long-term memory where specific key-words could be quickly recalled in order to overcome token lim- its (i.e., the 4096 in GPT-3.5), while MemGPT used a vector database to construct a persistent memory to maintain context across interactions, and Voyager iteratively refined its skills by replaying feedback from past actions in games such as Minecraft.\nLastly, one of the most biologically inspired and grounded advances has been in the development of spiking neural net-works (SNNs) in the 1990's. Called the \"third generation"}, {"title": null, "content": "of neural networks\u201d (86), SNNs are unique in their ability to model a time-dependent spike process, making them not only biologically more plausible, but also highly energy effi- cient and can be used on edge-computing devices. Notably, SNNs naturally lend themselves to hardware accelerated de- vices and neuromorphic computing. Platforms such as Intel's Loihi, IBM's TrueNorth, and SpiNNaker provide hardware specifically designed to run SNNs efficiently (87). However, historically they have been difficult to train and scale, but that's rapidly changing and new methods to transfer informa- tion from ANNs to SNNs are emerging (88)."}, {"title": "How to build more adaptive Al", "content": "How can we take inspiration from adaptive behavior and the brain to build new approaches (89)? A core component of adaptive behavior is having already learned priors these aforementioned internal models \u2013 that can be dynamically called upon. These priors clearly require data-at-scale to form this \"data\" can be baked into neural circuits via the genome, or learned throughout life. An example of genetic priors would be the innate ability to suckle at birth across many species, or how a newborn horse to stand and walk in a few hours after birth, while learned would be a motor skill such as playing a musical instrument. Another data-source is the massive amount of unsupervised sensory and motor stim-"}, {"title": null, "content": "uli we rapidly accumulate across development. But how do we turn data into internal models?\nWhile many questions remain about how these models are implemented in neural dynamics, there is increasing evidence of core computations (such as prediction errors) across brain regions, and there are architectural constraints on these sys- tems that stem from brain-body dynamics. Brain regions are often based on primary sensors: vision, olfaction, touch, pro-prioception, audition, and so forth. Yet, as Mountcastle ar-gued, it's less about a \"sensory\u201d or \u201cmotor\u201d space, but rather a function of inputs and outputs, and by extension, inter-connectivity of regions. Thus, as I will argue below, we should take inspiration from building specialized nodes that have core computations in the right reference framework that are interconnected in order to act.\nOne current trend in machine learning is for large, unified Foundation Models that generalize and can be used for trans-fer learning in downstream tasks. The \u201cFoundation Model\" idea for natural language processing takes large-scale unla-beled data and learns a joint representation using tokeniza-tion and self-supervised learning (90) (Figure 4a). Other ap-proaches, such as Flamingo, generalize this for multi-modal data. Flamingo is a Visual Language Model (VLM) that integrates pretrained vision and language models, process-\""}, {"title": null, "content": "ing mixed visual-text data and handling images or videos as"}, {"title": null, "content": "inputs (84) (Figure 4b). This could be extended for more modalities, as recently done for robotics (91).\nWhile I believe this is exciting path for models that show better generalization, I believe we can push further to build smartly adaptive systems. We should take inspiration from our brain's specialization and build series of smartly intercon- nected specialist models that can then be adapted. Namely, just as all sensory/motor information gets converted to spikes a shared computing space and is an efficient digitization of analog signals \u2013 notably, this information is computed upon within specific brain regions. Just as a cortical column in primary visual cortex processes a particular region of the vi- sual field and has specific coding properties related to visual features (like texture or edges), and the cortical circuit in the sensorimotor region computes in egocentric 3D kinematic, force, or even muscle space (36, 38).\nMoreover, across the animal kingdom we see specialization in brains everywhere: from the remarkably high-resolution of hawk vision, to bat echolocation, to the twelve cones of the mantis shrimp visual system. They aren't sitting a single tan- gled mess of weights and nodes, but rather cleverly intercon-nected neural circuits that have specific tuning to collectively solve many tasks. While perhaps eventually a single model could reach artificial superhuman intelligence (ASI), we are far from this reality.\nThus, I argue we should also focus on building agents that are imbued with adaptive algorithms found in animal intel- ligence, and not only build unified monolithic Foundation Models. There is emerging work on artificial agents that I believe will make this possible. Agents that act as operation systems are one promising avenue. They can be a series of LLMs (which is currently being explored (69, 82)), but also a series of VLMs (92), or jointly optimized encoders that indi-vidually are first pretrained (think mini-Foundation Models, such as an encoder for animal behavior (76) or unified neural region-specific pretrained encoders (68)). Then, these series of models could leverage the \u2018learning' techniques discussed above, such as memory-replay or synaptic intelligence. This is also highly likely to drive innovations, as it is also not clear that having many, independent losses is optimal.\nHow could this work? Imagine we have a series of core-expert models (aka nodes): one for segmenting video, one for extracting poses, another for ingesting text plus images (VLMs), and one for extracting dynamics from neural spikes (Figure 4c). Each could be first experts in their specific do-main, then linked via a shared latent space (even in a com-mon spike-like framework) and optimized together. This could have a human-level language interaction agent (such as a LLM) where at inference-time the multi-modal input data could also be queried with natural language, but im-portantly, monitor the sub-modules (much like a prefrontal cortex). Then, we could dynamically \u201clock\" models that show robustness on out-of-distribution samples and robust-ness to adversarial attacks. These nodes don't then need to\""}, {"title": null, "content": "be trained (adapted) until they are deemed no longer robust. This proposal would be akin to the cortex-basal ganglia loop that has shown to be instrumental in learning and habit for- mation (93). Namely, these nodes could be in states of \"skill learning\" or \"habituation\" (frozen) and be constantly moni-tored for robustness (Figure 4c). When robust levels fall, then continual learning, memory-replay, pseudo-labeling or injec-tion of new high-quality labeled data could be added without taking the entire system offline. There has been a surge of exciting work on building OOD detection modules (94, 95) and explainable attribution methods in time-series and im-ages (96-100) that could make this an attractive path forward.\nAnother approach, closely linked to robustness detection, could be specifically adding in prediction errors into the LLMs. Effectively, prediction errors give a signed teach-ing signal when an OOD sample is detected. Early works on predictive-coding inspired networks already showed promise (101), and now with modern transformer architec-tures or hybrid CNN-transformers, perhaps we can leverage prediction errors in order to do not only in-context learning, but signal which nodes need a new model - akin to how the brain must decide to online change a motor command vs. update an internal model. Here, we can take inspiration from the motor system (102). A one-time error means \"in-context learn\" and change your ongoing motor command\u00b3, but a repetitive error means update your internal model (Fig-ure 1c). If the incoming perturbation cannot be solved with online learning or model adaptation, a new internal model would then need to be created (which takes more time and can be completed \u201coffline\u201d). The neural schema in prefrontal cor-tex discovered by El-Gaby et al. (20) could inspire new ap-proaches for adaptive memory-replay and new model build-ing as well (Figure 1b).\nLastly, taking the cellular diversity seriously in neural net-works might drive innovations. SNNs have both excitatory and inhibitory, and even neuro-modulatory units. Can we merge the power of large transformers and the scale of data animals receive, with the cleverness of cortical circuit and/or midbrain (Figure 2)? Perhaps gating mechanisms akin to transformer blocks (Figure 4b) plus with signed (excitatory or inhibitory) tokenization and masking (perhaps mimicking biological time-delays) can lead to new innovations in adap-tive artificial systems."}, {"title": "Closing Remarks", "content": "As we strive to move beyond traditional AI towards building truly adaptive intelligence, the key lies in integration of in-sights from biological systems. Inherent to these ideas is that these adaptive agent systems can be put into robotic hard-ware. These embodied agents can then be systematically tested in true sensory-motor systems. Understanding how an-imals naturally adapt and rapidly learn in changing environ-ments can guide the development of embodied AI agents that"}]}