{"title": "AASIST3: KAN-Enhanced AASIST Speech Deepfake Detection using SSL Features and Additional Regularization for the ASVspoof 2024 Challenge", "authors": ["Kirill Borodin", "Vasiliy Kudryavtsev", "Dmitrii Korzh", "Alexey Efimenko", "Grach Mkrtchian", "Mikhail Gorodnichev", "Oleg Y. Rogov"], "abstract": "Automatic Speaker Verification (ASV) systems, which identify speakers based on their voice characteristics, have numerous applications, such as user authentication in financial transactions, exclusive access control in smart devices, and forensic fraud detection. However, the advancement of deep learning algorithms has enabled the generation of synthetic audio through Text-to-Speech (TTS) and Voice Conversion (VC) systems, exposing ASV systems to potential vulnerabilities. To counteract this, we propose a novel architecture named AASIST3. By enhancing the existing AASIST framework with Kolmogorov-Arnold networks, additional layers, encoders, and pre-emphasis techniques, AASIST3 achieves a more than twofold improvement in performance. It demonstrates minDCF results of 0.5357 in the closed condition and 0.1414 in the open condition, significantly enhancing the detection of synthetic voices and improving ASV security.", "sections": [{"title": "1. Introduction", "content": "Automatic Speaker Verification (ASV) systems are designed to identify speakers based on their voice characteristics. These systems have a variety of applications, including in the financial sector for user authentication during transactions, in smart devices to ensure exclusive access for the owner to control their equipment, and in forensic analysis to detect fraud cases.\nHowever, the advent of deep learning algorithms has rendered ASV systems susceptible to many assaults. The public accessibility of Text-to-Speech (TTS) and Voice Conversion (VC) systems with pre-trained weights permits any user with access to computational resources, including cloud GPUs, to refine these models for potentially malevolent objectives.\nTo effectively counter such attacks, the implementation of anti-spoofing systems is imperative. The ASVSpoof community is engaged in active research in this field, as evidenced by the compilation of diverse data corpora for the development of both countermeasure (CM) systems and spoofing-aware speaker verification (SASV) systems. This research is documented in the following publications: [1, 2, 3, 4, 5]. Moreover, the Singfake dataset [6] was developed to detect AI-generated vocals within the musical domain. Additionally, the SVDD project has significantly contributed to advancing voice spoofing countermeasures.\nToday, many techniques are utilized to detect voice spoofing, including those based on Convolutional Neural Networks (CNN) [7, 8], ResNet-like architectures [9, 10, 11, 12], Time Delay Neural Networks (TDNN) [13, 14], and transformers [15]. The AASIST architecture [16] has demonstrated particular robustness, as confirmed by numerous studies. Various modifications have been proposed to enhance the generalization capability of AASIST, including the use of a Res2Net encoder [17], wav2vec [18], fusion of different audio representations [19], application of specific training schemes such as SAM [20], ASAM [21], SWL [22], as well as the use of alternative loss functions [23, 24].\nIn the present study, we propose an innovative architecture, AASIST3, developed on an AASIST base to detect speech deepfakes. The main modifications include:\n\u2022 The modification of attention layers in GAT, GraphPool, and HS-GAL utilizing KAN[25] is based on the primary PreLU activation function and learnable B-splines, allowing for the extraction of more relevant features.\n\u2022 The scaling of the model in width using the proposed KAN-GAL, KAN-GraphPool, and KAN-HS-GAL techniques enabled the extraction of more complex parameters, resulting in enhanced model performance.\n\u2022 The primary methods of data pre-processing employed were diverse augmentations and pre-emphasis, intending to obtain more meaningful discriminative frequency information."}, {"title": "2. Preliminaries", "content": null}, {"title": "2.1. Kolmogorov-Arnold Network", "content": "The Kolmogorov-Arnold theorem [26, 27, 28] postulates that any continuous multivariate function $f: [0,1]^n \\rightarrow R$ can be represented as a finite composition of continuous unary functions and a binary addition operation. More specifically:\n$f(x) = f (x_1, x_2,...,x_n) = \\sum_{q=0}^{2n} \\Phi_q (\\sum_{p=1}^{2n} \\Phi_{q,p}(x_p)),$ (1)\nwhere $\\Phi_{q,p}: [0, 1] \\rightarrow R$ and $I_q : R \\rightarrow R$\nAs all the functions to be learned by the model are one-dimensional, Liu et al. [25] proposed that each 1D function be parameterized as a B-spline curve and a basis function:\n$\\phi(x) = w_b b(x) + w_s spline(x),$ (2)\nwhere $b(x)$ represents the local basis function(eq. 3), while $w_b$ and $w_s$ denote trainable parameters that have been initialized following Kaiming initialization.\n$b(x) = PReLU(x) = max(0, x) + a min(0,x)$ (3)"}, {"title": "2.2. Audio preprocessing", "content": "In light of the hypothesis that high frequencies facilitate the model's ability to differentiate between bona fide and spoof utterances, we employed a pre-emphasis technique on the input signal:\n$X_l = X_l -0.97. X_{l-1},$ (12)\nwhere l equals 1, 2, 3, .., L, L represents the length of the audio signal, and 0.97 is the pre-emphasis factor. The pre-emphasis process suppresses low and enhances high frequencies, facilitating the model's ability to focus on more relevant features specific to spoofing or bona fide utterances."}, {"title": "2.3. SincConv frontend", "content": "Following AASIST[16], we use the non-trainable SincConv[29] to extract features from preprocessed audio. SincConv applies the function $g(n, f_1, f_2)$ to the speech signal chunks x(n) using the Hamming window function wn:\n$y(n) = x(n) g(n, f_1, f_2) * w(n)$ (13)\n$g(n, f_1, f_2) = 2 f_2 sinc(2\\pi f_2 n) \u2013 2f_1 sinc(2\\pi f_1\\eta)$ (14)\n$sinc(x) = \\frac{sin(x)}{X}$ (15)\n$w(n) = 0.54 - 0.46 cos(\\frac{2\\pi n}{L}).$ (16)\nwhere $f_1, f_2$ are fixed parameters equal to the minimum and maximum possible frequencies in the Mel-spectrogram of the passed signal"}, {"title": "2.4. Wav2Vec2 frontend", "content": "Wav2Vec2 [30], developed by Facebook AI, is a state-of-the-art method for converting audio to text. This model utilizes the Transformer architecture first introduced in [31]. The Wav2Vec2 [30] architecture consists of two main components:\nEncoder Layer: This layer transforms the input audio data into a sequence of hidden states. It consists of convolutional layers that transform the input audio data into a sequence of hidden states. Predictor Layer: This layer takes the sequence of hidden states from the encoder layer and predicts the next hidden state. It uses the Transformer architecture, which allows the model to consider context when predicting the next state. A key feature of Wav2Vec2 is that it is trained unsupervised, meaning it does not require labeled data for training. Instead, it uses a method called contrastive learning to learn audio representations. As a front-end component for a scientific paper, Wav2Vec2 can automatically transcribe audio to text, which can help analyze audio data or create text versions of audio recordings."}, {"title": "2.5. Encoder", "content": "The encoder comprises six convolutional units. Except for the initial unit, each subsequent unit comprises two convolution units. The initial unit, however, comprises a single convolution unit in conjunction with another unit. The convolution unit implements the following transformation:\n$ConvUnit(x) = Conv(SELU (BatchNorm(x)))$ (17)\nEach unit's input is added to the output following the second convolutional unit in that block and downsampled using a convolutional layer if necessary. Subsequently, MaxPooling is applied following this skip connection."}, {"title": "2.6. KAN-GAL", "content": "In our work, we were also inspired by AASIST, which is based on the premise that graphs are fully connected because it is impossible to determine the degree of importance of each node to a given task in advance. In contrast to RawGat [32], the activation functions were not employed due to the novel utilization of KANS.\nThe initial operation is to apply a dropout with a probability of 0.2. Subsequently, the attention mask is obtained by node-wise multiplication(denoted as \u201c\u00d7\u201d) of the nodes $h, h\\in R^{N,D}$ and N - the number of nodes, D - node dimensionality, and subsequent passing through the KAN layer. Following this, the hyperbolic tangent is applied. The resulting expression is then matrix multiplied by the attention weights $W_{att}$, which have been initialized using Xavier initialization. The resulting values are then divided by the temperature T, resulting in an attention map A consisting of the corresponding probabilities, which is obtained using the softmax function:\n$A = softmax (\\frac{tanh(KAN1 (h\\times h))W_{att}}{T} ).$ (18)\nThe resulting attention map is projected using KAN, and in parallel, it is multiplied by the matrix and projected. The resulting projections are then added together and normalized by batch:\n$KAN-GAL(h) = BatchNorm(KAN_2(Ah) + KAN_3(h)).$ (19)"}, {"title": "2.7. KAN-GraphPool", "content": "As described in the previous section, the initial operation is to apply a dropout, after which the resulting output passes through the KAN layer and is transformed by the sigmoid function, represented by the symbol \u03c3(\u00d7). The resulting value is then multiplied elementwise (denoted as \u201c\u22c5\u201d) by the original graph. The dimensionality is subsequently reduced using the rank function, which returns the k most significant nodes in the resulting graph:\n$KAN-GraphPool = rank((\u03c3(KAN(h))\u22c5 h), k).$ (20)"}, {"title": "2.8. \u039a\u0391\u039d-HS-GAL", "content": "The layer accepts three inputs: $h_t$, which has a node dimensionality of $D_t$ (temporal graph), $h_s$, which has a node dimensionality of $D_s$ (spatial graph), and S (stack node). Input graphs are projected into another latent space using KAN layers to equalize their dimensions and then merged to form a fully connected heterogeneous graph $h_{st}$ with node dimensionality $D_{st}$. A dropout with a probability of 0.2 is then applied to the resulting graph:\n$h_{st} = CONCAT(KAN_1 (h_t), KAN_2(h_s)).$ (21)\nThe primary attention map A is derived by multiplying each node in the graph $h_{st}$ by every other node, with the projection through the KAN layer undergoing a hyperbolic tangent transformation:\n$A = tanh(KAN_3(h_{st} \\times h_{st})).$ (22)\nTo derive a secondary attention map B, the initial attention map is partitioned into four matrices in accordance with the threshold $D_t$, defined as the number of nodes in the underlying graph $h_s$. Subsequently, these segments are multiplied by the weights $W_{11}, W_{12} and W_{22}$:\n$B = \\begin{cases}\\sum_{m=1}^{D_t+D_s} A_{ijm} W_{11m}, & \\forall i \\le D_t and j < D_t\\\\\\sum_{m=1}^{D_t+D_s} A_{ijm} W_{22m}, & \\forall i > D_t and j > D_t\\\\\\sum_{m=1}^{D_t+D_s} A_{ijm} W_{12m}, & otherwise.\\end{cases}$ (23)\nThe matrix is then divided by the temperature value T and passed through the Softmax function, thereby obtaining a probability map:\n$B = softmax (\\frac{B}{T})$ (24)\nTo produce the attention map for the stack node update, the heterogeneous graph $h_{st}$ is taken and multiplied by the Stack Node S node-wise. The resulting graph is then projected through the KAN layer, passed through the tangent, and matrix multiplied by the weights $W_m$. The value obtained is subsequently divided by the temperature and passed through softmax:\n$A_m = softmax (\\frac{tanh(KAN_4(h_{st} \\cdot S))}{T})$ (25)\nCombining two projections obtained using KAN layers to update a stack node is necessary. These are the projection of the matrix-multiplied attention map $A_m$ and graph $h_{st}$ and the projection of the stack node:\n$S = KAN_5(A_m h_{st}) + KAN_6(S)$ (26)\nThe update of $h_{st}$ is achieved by combining two projections derived from KAN layers: the projection of the matrix multiplied secondary attention map B and the heterogeneous graph $h_{st}$ and the projection of the graph $h_{st}$ itself. The expression obtained is then subjected to batch normalization:\n$h_{st} = BatchNorm(KAN_7(B h_{st}) + KAN_8(h_{st})).$ (27)\nThe resulting heterogeneous graph is then divided back into two components by multiplication with the mask matrices $M_1$ and $M_2$:\n$h_t = h_{st} M_t$ (28)\n$M_t = \\begin{pmatrix} I_t & I_t \\\\ O_s & O_s \\end{pmatrix}, I_t \\in R^{N\\times D_t}, O_s \\in R^{N\\times D_s}$ (29)\n$h_s = h_{st}M_s$ (30)\n$M_s = \\begin{pmatrix} O_t & O_t \\\\ I_s & I_s \\end{pmatrix}, O_t \\in R^{N\\times D_t}, I_s \\in R^{N\\times D_s}.$ (31)"}, {"title": "2.9. Models Architectures", "content": null}, {"title": "2.10. AASIST3", "content": "In the closed condition, the front-end is SincConv, whereas, in the open condition, it is Wav2Vec2 XLS-R[30] with additional linear or convolutional layers, which maintains the dimensionality.\nThe application of Max Pooling, Batch Normalization, and SELU preceded the encoder:\n$\\hat{x} = Encoder(SELU(BatchNorm(MaxPool(x))),$ (32)\nwhere x is an input pre-emphasized audio.\nSubsequently, the acquired features were divided into temporal and spatial components, after which positional embedding (PE) was incorporated. In this manner, graphs were formed, which were subsequently passed through a KAN-GAL and a KAN-GraphPool:\n$h_t = KAN-GraphPool(KAN-GAL(max(abs(\\hat{x}_t) + PE_t)))$ (33)\n$h_s = KAN-GraphPool(KAN-GAL(max(abs(\\hat{x}_s) + PE_s))).$ (34)\nThe resulting graphs and the previously initialized stack node were passed in parallel through four branches. The initial step is to apply KAN-HS-GAL in each branch:\n$\\begin{pmatrix} h_{t2} \\\\ h_{s2} \\\\ S_2 \\end{pmatrix} = KAN-HS-GAL \\begin{pmatrix} h_{t1} \\\\ h_{s1} \\\\ S_1 \\end{pmatrix}$ (35)\nThe graphs are then passed through KAN-GraphPool, and another KAN-HS-GAL is applied similarly:\n$\\begin{pmatrix} h_{t3} \\\\ h_{s3} \\\\ S_3 \\end{pmatrix} = KAN-HS-GAL \\begin{pmatrix} KAN-GraphPool(h_{t2}) \\\\ KAN-GraphPool(h_{s2}) \\\\ S_2 \\end{pmatrix}$ (36)"}, {"title": "2.11. Wav2Vec2-Conv-AASIST-KAN", "content": "In addition to the proposed AASIST3, we utilized the pre-trained Wav2Vec2 encoder for the feature and proceeded with 1D convolutions to provide the AASIST model with a KAN classification layer. It was motivated by inductive biases from a pre-trained speech encoder on a large-scale dataset in self-supervised (SSL) training, which is preferable for the open-set condition."}, {"title": "3. Experiments and Results", "content": null}, {"title": "3.1. Description of final approaches", "content": "For the Closed Condition, we considered our described AASIST3 model. The model was constrained to accept only four seconds of audio as input, which proved insufficient for the models to achieve a deeper comprehension of the audio as a whole. To address this limitation, the audio was fed into the model in four-second parts sequentially with a two-second overlap between them. We applied pre-emphasis for all audios with no augmentations.\nThe optimal models were identified upon testing the models on a closed test subset: one with two branches and one with four branches. Additionally, based on the hypothesis that SWL can enhance the results, the model incorporating SWL was utilized. The predictions of these models were averaged.\nFor the Open Condition, to provide the final prediction or probability that given audio x is bonafide, we averaged the predictions of two of our models trained differently to increase generalization ability.\n$\\hat{f} = \\frac{f_1(x) + f_2(x)}{2}$ (41)\nwhere\n$f_1(x) = \\sum_i f_1(x_i)$ (42)\n$f_2(x) = \\sum_j f_2(x_j),$ (43)\nand ${x_i}_{i=1}^n$ and ${x_j}_{j=1}^m$ are some parts of the original audio x, specifically, sequential parts with intersection (for example, 0-4s and 3-7s audio intervals) as in the submission for Closed Condition. The $f_1$ is AASIST3 but with a pre-trained Wav2Vec2 feature encoder. The $f_2$ is our second model Wav2Vec2+Conv+AASIST+KAN.\n$f_1$ was trained similarly as in closed condition only on the provided training set, whereas $f_2$ was trained on the union of the given training set plus additional bonafide audios from Mozilla Common Voice, train part if VoxCeleb2. For $f_2$ training, a combination of weighted Cross-Entropy, Focal [33], and LibAUCM [34, 35] losses with Adam optimizer. Augmentation methods such as RIR, environmental and Gaussian noises, VAD, and pitch shifting were randomly applied. Pre-emphasizing was used before augmentations."}, {"title": "3.2. Experiments with different frontends", "content": "Given the results presented by [19], it was hypothesized that combining multiple representations might improve the result. Combining the raw waveform with the CQT and Mel-spectrograms was attempted, but no improvement was seen.\nIn light of the findings in [36], we investigated using the $f_0$ subband independently and in conjunction with SincConv. In addition, given the evidence presented in the study referenced in [37], which indicated that Leaf outperformed SincConv, we compared its performance with our model's. However, none of the modifications resulted in a performance improvement. For open conditions, the best result was shown by Wav2Vec2 [30] pre-trained on XLSR-300, as front-end based on a transformer and convolutional neural networks, which allows suitable encoding of both temporal and spatial information in audio. Also, experiments with XEUS [38] did not provide better results compared to other front-ends."}, {"title": "3.3. Experiments with augmentations", "content": "This study employed a series of augmentations, including pitch shift, speed change, Gaussian and environmental noise, different room impulse responses, harmonic and percussive components, stretching, voice activity detection (VAD) application, and pre-emphasis. A random portion of the audio sample was extracted or padded during training. In some experiments, the pre-emphasis was used as an augmentation, in others it was applied to each audio. Additionally, we employed Attention Augmented Convolutional Networks[39] and Rawboost[40]. The findings indicate that pre-emphasis on each audio track represents an effective approach."}, {"title": "3.4. Experiments with different base KAN functions", "content": "A comparison of AReLU[41], PReLU, SELU, and RRELU showed that PReLU gave the most robust results."}, {"title": "3.5. Experiments with different KAN-based encoders", "content": "As one of the key ideas of our model is the use of KAN, we chose to explore the potential of KAN-based encoders, including KAN+tokenization[42], ReluConvKAN and WavKANConv[43], both in stand-alone form and combined with a sharpness-aware minimisation[20] mechanism. When evaluated using a validation set with SAM, WavKanConv showed favorable results, table 2\nOur experiments with different KAN encoders found that the model using the classic RawNet2-based encoder performed best."}, {"title": "3.6. Experiments with different KANS", "content": "A comparison was conducted between parametric B-spline functions and other types of polynomials, including Bessel polynomials, Chebyshev polynomials of the second kind, Gaussian radial basis functions, Fibonacci polynomials, radial-basis functions, and Jacobi polynomials. The results showed that the approximation with a B-spline of order 4 gave the most accurate results."}, {"title": "3.7. Experiments with AASIST modification", "content": "In order to test the effectiveness of the proposed methodology, several modifications were applied. These included the insertion of the third additional branch with channel-wise maximum, which was used to allow the extraction of more complex features. Furthermore, GraphPools were replaced by GALs to avoid removing a significant amount of information. The minimum was used instead of the maximum to form branches, and four branches with HS-GAL and SE[44] in the encoder were used. Six branches were also used with HS-GAL, and positional encoding was introduced instead of positional embedding[?]. Finally, a comparison was made between the results obtained with two branches and those obtained with the proposed methodology. The results show that the best result is obtained using four branches with HS-GAL."}, {"title": "3.8. Experiments with scaling of HS-GAL branches", "content": "Additional HS-gals with different temperature values were applied to two branches. As shown in the table 2, the obtained results did not exhibit a considerably enhanced degree of improvement.\nThe results allow us to conclude that scaling the model in width is more optimal than scaling it in depth."}, {"title": "3.9. Experiments with different encoders", "content": "Given that the original AASIST employs a RawNet2-based encoder, we postulated that a RawNet3-based encoder[45] would enhance the model. Concurrently, we anticipate that S2pecNet[19], as the authors have demonstrated, will improve the result through this integration of sound representations. Additionally, we explored the potential of WaveNet[46] as a front-end, but unfortunately, none of the experiments yielded a significant result."}, {"title": "3.9.1. Experiments with Res2Net-based encoders", "content": "Following AASIST2[17], we attempted to utilize Res2Net in various configurations with disparate learning rates. Concurrently, we evaluated SR LA RES2net[36] as a more sophisticated analog. The results of our investigation suggest that the application of Res2Net with the proposed AASIST configuration is not a viable approach."}, {"title": "3.9.2. Experiments with ResNet-based encoders", "content": "Utilizing alternative encoders and modifications to the Res2Net encoder yielded no perceptible improvement in results. Therefore, an investigation was conducted into alternative changes to the ResNet encoder. These included the utilization of the f0 subband instead of the SincConv, the use of two ResNet encoders for different segments of audio, with and without RawBoost, the integration of ELA [47], the substitution of BatchNorm with LayerNorm, the utilization of prelu as the activation function, and the integration of SE. The experimental results demonstrate that modifying the encoder does not improve outcomes."}, {"title": "3.10. Experiments with different loss functions", "content": "Furthermore, in line with AASIST2[17], AM-Softmax and its predecessor, ArcFace[48], were also tested. Based upon the results of the study [49], focal loss was also tested. Furthermore, we attempted to utilize generalized cross entropy[23], the effectiveness of which has been previously established for AASIST. Additionally, as in the original AASIST, we attempted to utilize weighted cross-entropy. Finally, we selected multitask losses, hypothesizing that the model would be capable of extracting more complex features. However, our findings indicated that regular cross-entropy was indeed efficacious. Consequently, the deployment of loss to address class imbalance in our model can be considered ineffective. However, for better generalization, our second model was trained using a combination of weighted cross-entropy, focal loss, and LibAUCM [34, 35] loss, which is implied for the x-risk minimization."}, {"title": "3.11. Experiments with different optimizers", "content": "The following optimizers were selected for our experiments: AdamW, Lion, NAdam, RAdam, and Adam. As illustrated in the table 2, the outcomes with AdamW and Lion exhibited a notable decline in performance. In addition, the results with RAdam were found to be unsatisfactory. The results on the development subset are presented in the table 2.\nThe findings indicate that Adam is the optimal optimizer for our model."}, {"title": "3.12. Experiments with different learning methods", "content": "To enhance the generalization capacity, we employed a variety of techniques, including SAM [20], ASAM [21], and SWL [22]. These were initially utilized in the original paper about AASIST and led to a notable enhancement in the quality of the models. Furthermore, a cosine annealing scheduler and a weighted random sampler are employed. As illustrated in the table 2, these strategies have also been demonstrated to be ineffective.\nIt was found that none of the proposed learning methods improved the result."}, {"title": "4. Conclusion", "content": "The rapid development of various deep learning algorithms has created new opportunities for generating synthetic audio using TTS and VC systems. This progress, however, has introduced a corresponding vulnerability in ASV systems, necessitating the development of a CM system to detect synthetic voices. In this paper, we proposed a novel architecture, AASIST3, which enhances the original AASIST framework by incorporating Kolmogorov-Arnold networks, additional layers, and pre-emphasis. Furthermore, we introduced modifications using B-spline features as training features inspired by previous enhancements in synthetic speech detection models. In addition, we utilized additional data, scores fusion, and a self-supervised pre-trained model as an encoder to achieve the best results. Our findings indicated that these modifications significantly improve model performance, achieving a more than twofold improvement over AASIST. The model demonstrated minDCF results of 0.5357 under closed conditions and 0.1414 under open conditions, affirming the effectiveness of our configuration."}]}