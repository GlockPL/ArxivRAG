{"title": "Large Language Models as Efficient Reward Function Searchers for Custom-Environment Multi-Objective Reinforcement Learning", "authors": ["Guanwen Xie", "Jingzehua Xu", "Yiyuan Yang", "Shuai Zhang"], "abstract": "Leveraging large language models (LLMs) for designing reward functions demonstrates significant potential. However, achieving effective design and improvement of reward functions in reinforcement learning (RL) tasks with complex custom environments and multiple requirements presents considerable challenges. In this paper, we enable LLMs to be effective white-box searchers, highlighting their advanced semantic understanding capabilities. Specifically, we generate reward components for each explicit user requirement and employ the reward critic to identify the correct code form. Then, LLMs assign weights to the reward components to balance their values and iteratively search and optimize these weights based on the context provided by the training log analyzer, while adaptively determining the search step size. We applied the framework to an underwater information collection RL task without direct human feedback or reward examples (zero-shot). The reward critic successfully correct the reward code with only one feedback for each requirement, effectively preventing irreparable errors that can occur when reward function feedback is provided in aggregate. The effective initialization of weights enables the acquisition of different reward functions within the Pareto solution set without weight search. Even in the case where a weight is 100 times off, fewer than four iterations are needed to obtain solutions that meet user requirements. The framework also works well with most prompts utilizing GPT-3.5 Turbo, since it does not require advanced numerical understanding or calculation.", "sections": [{"title": "I. INTRODUCTION", "content": "Due to their powerful capabilities in addressing objectives, reinforcement learning (RL) methods are increasingly applied in complex, multi-objective tasks. However, as the number of demands and performance optimization goals increases, the design of reward functions is becoming more challenging, requiring substantial effort to modify the form and coefficients of each component of the reward function. Adding to the challenge is that researchers' demands frequently change with scenarios and over time, and sometimes remain unclear [1], thereby imposing a substantial burden on achieving optimal performance.\nLarge language models (LLMs) learn from extensive text data [2], enabling them to exhibit strong problem-solving and content-generation capabilities when provided with well-designed prompts, even in the absence of prior knowledge, as well as achieving self-evolution through human feedback. The utilization of LLMs for designing functional code has yielded impressive performance in various tasks, such as dexterous robot control [3]\u2013[5] and Minecraft playing [6], [7], demonstrating significant potential in zero-shot scenarios with a limited number of self-evolution iterations. However, this improvement process relies on trial-and-error exploration. When a single, explicit objective (e.g., success rate) is present, the search space for function design and parameter tuning is limited, making it possible to gradually explore optimal solutions through iterations. However, for complex reward functions, in outputting the reward components and weights collectively, issues such as incorrect code and imbalanced weights may arise, which are challenging to address through training feedback. A related topic is LLM-driven white-box optimization [2], [8], [9], such as hyperparameter optimization (HPO) [8], [10], which abstracts all clearly defined coefficients and the function code itself into parameters, thereby enabling LLMs to conduct comprehensive analyses and improvements for well-defined machine learning tasks. This paradigm aligns with the observation that LLMs excel at summarizing and heuristically generating code in specific and clear task contexts, but they are less effective in tackling black-box optimization problems [11]. These considerations underscore the importance of providing clear and precise task descriptions while also exploring the vast search space rationally.\nIn this paper, we utilize task decomposition and white-box search, employing LLMs as effective searchers to fully leverage its semantic understanding capabilities. Unlike previous work on LLM-aided reward function design, we separate the process of reward code design and weight assignment into two stages, while dividing the multi-objective RL task into multiple explicit numerical goals. This division helps to avoid ambiguity in training feedback. For both stages, we employ a clear feedback and self-evolution search paradigm, using the reward critic to correct the reward components for each"}, {"title": "II. METHODS", "content": "A. High Quality Environment Description\nTask description is a common component of most subsequent prompts, comprising text descriptions, environment code, or APIs, which include variables and functions essential for designing the reward function, as well as user requirements. We decompose the user requirements into explicit performance demands (e.g., obstacle avoidance to achieve zero collision). Unclear task descriptions may lead to LLMs being unable to generate correct reward functions, which cannot be rectified through feedback. Therefore, we designed a meta-prompt to provide users with suggestions for enhancing the quality of the description, enabling LLMs to identify potential issues within the prompt, such as unclear structural organization and a lack of necessary information and explanations. We instruct LLMS to output in a fixed format to maximize the identification of potential issues.\nB. Reward Code Generator\nThe code generation process can utilize existing LLM-aided reward function design frameworks, but we generate reward components for each user requirement, rather than for the output as a whole. We also task LLMs with outputting explanations for the generated reward components, which may improve the correctness of the generated code. However, due to the lack of prior knowledge of custom environments, as well as the complexity of long contexts, the code initially generated by LLMs are likely to be incorrect. Therefore, we test each component separately and correct errors using a LLM-based reward critic. The reward critic follows instructions from a step-by-step guide, namely first listing possible reasons for code failure, then reviewing the environment code and the requirement, and finally outputting the correct function code. This process allows LLMs to analyze errors in the reward function clearly, thereby avoiding the ambiguity of overall feedback.\nC. Reward Weight Searcher\nMulti-objective reinforcement learning not only demands the correct form of reward components but also requires the correct scale of these components. We utilize LLMs as effective weight searchers under explicit task contexts.\nWeight searching requires a good starting point. We use specialized instructions that require LLMs to pre-calculate the approximate values of the rewards and adjust the weights to make reward values have the same scale, which provides a good starting point for the search, without deviating from the optimal solution too far. We name this process as reward weight initializer, and sample prompts are shown in Figure 2.\nThen, based on training results, LLMs propose weight modification suggestions. Existing methods often utilize a Python list-style training log to present these results. However,"}, {"title": "III. EXPERIMENTS", "content": "A. Task Description\nTo evaluate the proposed framework, we select a simplified task from our previous work [12], which utilizes RL to control multiple autonomous underwater vehicles (AUVs) for information collection. We task LLMs with designing reward functions without providing any reward examples (namely zero-shot), encompassing safety requirements (collision and border crossing avoidance), performance requirements (timely serving of the goal SN to minimize data overflow), and performance objectives (reduce energy consumption), as shown in the task description in Fig. 1. We refer to the original paper for the system models and simulation parameters. For determinacy, TD3 [13] is used as the RL algorithm instead of MAISAC, as in the original paper.\nFor experiments, we utilize gpt-4o-2024-08-06 (denoted as GPT-4o) as the default LLM due to its improved performance and reasonable API pricing. We also conduct experiments using the earlier version of OpenAI's LLM, gpt-3.5-turbo-0125 (denoted as GPT-3.5T). The LLM parameters are set to temperature=0.5 and Top P=1.\nMeanwhile, we design a baseline that takes the reward functions and numerical values as a whole. This approach is similar to Eureka [3]; however, during the reward function revision stage, it processes one or more reward functions along with their training logs using a training log analyzer as inputs, and generates K=5 outputs simultaneously. This approach remains consistent with the reward weight searcher, in contrast to generating a single reward function repeatedly in an i.i.d. manner as in Eureka. We denote this baseline as EUREKA-M.\nB. Main Results and Case Studies\nReward Critic can generate correct code stably and quickly. Due to the long context and possibly unclear task descriptions, the generated code may not be entirely correct. For example, we find that the reward function may have sparse reward terms, miscalculate distances between AUVS and the boundary, contain symbolic errors, among other issues. To address these issues, we provide feedback on the reward components to the reward critic unit for each requirement. We test each component separately and utilize a variation of the training log analyzer, which outputs [YES] and [NO] to indicate whether this component fulfills the requirement. For each requirement, only one feedback iteration to the reward critic is required to generate the correct code.\nTo investigate the advantages of the reward critic, we manually introduced an error into the reward function by reversing the penalty term for collision into a reward term, namely reversing the symbol `reward +=` to `reward -=`. The output of the reward critic and EUREKA-M is shown in Figure 3. The reward critic can identify the error, but EUREKA-M fails to do so, only modifying reward weights, even when prompted with \"This code contains errors.\" We also attempted to use the sparse term-only reward function\nfor EUREKA-M to modify, but unless explicitly prompted to generate a dense term, it only continues with weight modification. This suggests that the reward design process based on training feedback may be ineffective without explicit human input.\nAdditionally, the reward critic can overcome the negative effects caused by an incomplete environment description. We removed most of the code in the task description to replicate the experiment. At this point, the reward critic automatically generates the relevant variables and prompts the user to complete them, yielding results consistent with those obtained under a complete environment description.\nReward weight initialization and searching. We obtain initial groups of weights from the reward weight initializer, and the energy consumption and average data overflow times (lower is better) of these solutions are illustrated in Figure 4(a). Although three groups of generated weights do not meet user requirements, two groups (emphasizing timely response to target SNs and energy consumption, respectively) successfully achieve Pareto solutions. This means that no further search is required, or a more refined search may only be necessary for a specific point of the Pareto set.\nThen, we employ a reward weight searcher to iteratively adjust the weight coefficients (denoted as RWI initialized). We also perform ablation experiments by removing the value balance process of the weight initializer (i.e., eliminating the example values from the environment description and removing prompts requiring LLMs balance reward values, denoted as RWI w/o balance), and the training log analyzer (denoted as GPT-4o w/o TLA). Additionally, we task LLMs to search from a weight group generated from RWI initialized, but with the weight of the energy consumption penalty term increased by a factor of 100 (denoted as 100x off), to understand the search details. Table 1 displays the number of iterations required to meet user demands under different settings, while Figure 4(b) depicts the search step sizes during iteration.\nWhen the weight initialization does not consider the balance between reward components, the ratio between weights compared to RWI-initialized settings could differ by a factor of 1 to 50, while leading to a significant increase in the deviation of generated weights. Nevertheless, since the distance is not substantial, only 0-2 iterations are necessary to find a feasible solution.\nFor the 100x off experiment group, GPT-4o achieves minimum search iterations. Specifically, after one iteration, GPT-4o recognizes that the weight setting may be far from a feasible solution and thus tries to increase the search step size. However, when the training log analyzer is removed, the search process becomes less flexible. We also conduct the feedback evolution process using EUREKA-M, and the step size is lower than that of GPT-4o w/o TLA, and the modification suggestions sometimes contain errors.\nThe difference between utilizing GPT-4o and GPT-3.5T. Intuitively, GPT-3.5T and open-source LLMs exhibit weaker overall reasoning abilities compared to GPT-4o, and their numerical analysis and mathematical capabilities are also quite limited [14], which leads to poor performance in designing reward functions for robotic control [15]. This results in drawbacks, such as the reward function initializer not functioning with GPT-3.5T. In terms of answering content-generating prompts (such as code design and training log analysis), the quality of GPT-3.5T is slightly inferior to that of GPT-4o. Despite this, GPT-3.5T performs adequately. Still, the reward critic only needs one feedback to correct the code per requirement, while the reward weight searcher can generate accurate modification suggestions and surpass the performance of EUREKA-M, due to clear task definitions and textual feedback. However, as shown in Figure 4(b), due to its limited capabilities, the step sizes provided by GPT-3.5T lack flexibility, necessitating more search iterations. It is worth noting that GPT-3.5T's output sometimes overlooks certain requirements or experiences format degradation in extended contexts, necessitating stricter constraints on the output format specified in the input prompt."}, {"title": "IV. CONCLUSION AND DISCUSSION", "content": "In this paper, we decompose a multi-objective task into clear user requirements, enabling LLMs to function as zero-shot searchers that obtain clear feedback and effectively generate reward functions. LLMs generate reward components, which are subsequently corrected by the reward critic to prevent potential errors. Leveraging the enhanced numerical calculation capabilities of the latest GPT-4o, we initialize weights that balance the value of reward components, allowing us to obtain feasible Pareto solutions without the need for extensive searching. Furthermore, based on search history and non-numerical contexts provided by the training log analyzer, GPT-4o can flexibly adopt different search strategies and step sizes, thereby accelerating the search process. In addition, with the exception of the reward weight initializer, most prompts exhibit acceptable performance in GPT-3.5T. Future work may focus on developing clearer and more automated task descriptions and verifying the LLM-aided reward design process across a broader range of tasks."}]}