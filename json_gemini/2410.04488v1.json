{"title": "A Pluggable Common Sense-Enhanced Framework for Knowledge Graph Completion", "authors": ["Guanglin Niu", "Bo Li", "Siling Feng"], "abstract": "Knowledge graph completion (KGC) tasks aim to infer missing facts in a knowledge graph (KG) for many knowledge-intensive applications. However, existing embedding-based KGC approaches primarily rely on factual triples, potentially leading to outcomes inconsistent with common sense. Besides, generating explicit common sense is often impractical or costly for a KG. To address these challenges, we propose a pluggable common sense-enhanced KGC framework that incorporates both fact and common sense for KGC. This framework is adaptable to different KGs based on their entity concept richness and has the capability to automatically generate explicit or implicit common sense from factual triples. Furthermore, we introduce common sense-guided negative sampling and a coarse-to-fine inference approach for KGs with rich entity concepts. For KGs without concepts, we propose a dual scoring scheme involving a relation-aware concept embedding mechanism. Importantly, our approach can be integrated as a pluggable module for many knowledge graph embedding (KGE) models, facilitating joint common sense and fact-driven training and inference. The experiments illustrate that our framework exhibits good scalability and outperforms existing models across various KGC tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge graphs (KGs) such as Freebase [1], YAGO [2], WordNet [3], NELL [4], and DBpedia [5] store factual triples in the form of (head entity, relation, tail entity) (shortened as (h,r,t)), which can be applied to numerous knowledge-intensive tasks, including relation extraction [6], semantic search [7], dialogue systems [8], question answering [9], and recommender systems [10]. Particularly, many KGs contain ontologies that comprising multiple entity concepts along with their associated meta-relations.\nThe primary issue of employing KGs is their natural incompleteness, primarily attributed to noisy data and the limited performance of current information extraction models [11]. Knowledge graph completion (KGC) is a vital endeavor that aims to resolve this issue by inferring missing entities or relations in unobserved triples represented as (h, r, ?) or (?, r,t). To accomplish this, knowledge graph embedding (KGE) is a predominant technique to learn entity and relation embeddings that can be employed to assess the plausibility of unseen triple candidates through scoring mechanisms [12].\nTransE [13] is a widely-used KGE approach that models relations as translation operations from the head to tail entities in a triple (h,r,t), formulated as h + r = t. To enhance the representation ability, many variants of TransE have been developed, including TransH [14], TransR [15], RotatE [16], QuatE [17] and HAKE [18]. Tensor factorization-based approaches like RESCAL [19] and DistMult [20] utilize tensor products and diagonal matrices to capture complex interactions among latent factors. ComplEx [21] extends DistMult by representing entities and relations in the complex-valued space to model asymmetric relations effectively. Some KGE models exploit deep neural network to predict the plausibility of a triple, such as fully connected neural network-based model NTN [22], convolutional neural network-based approach ConvE [23], graph neural network-based method R-GCN [24], transformer-based model KG-BERT [25].\nDuring the inference stage, candidate triples are scored and sorted to produce inference results. However, this process may rank incorrect entities higher than correct ones due to uncertainty of KG embeddings. Some KGC techniques augment entity embeddings with external informantion, such as text descriptions [26], [27] or images [28], [29]. Whereas, the extra multi-modal information is usually unavailable. Conversely, humans always utilize common sense to directly evaluate the plausibility of facts. For instance, in Fig. 1, a KGE model believes the predicted tail entity California as the highest-ranked candidate but the corresponding concept State is inconsistent with the common sense (Person, Nationality, Country). Thus, taking advantage of common sense for KGC is a key idea of our work.\nIn contrast to factual triples, common sense is commonly represented as concepts together with their relations in the format of (head concept, relation, tail concept) in some popular common sense KGs like ConceptNet [30] and Microsoft Concept Graph [31]. However, common sense is costly the existing commonsense KGs only contain concepts without links to corresponding entities, which is not applicable to entity-centric KGC tasks. Although some KGE models leverage ontology for incorporating common sense-like information, such as TKRL [32] with hierarchical entity types and JOIE [33] introducing ontology layer embeddings, these ontology-based models cannot work on some KGs lacking entity concepts such as WordNet [3]. Therefore, generating common sense automatically from any KG remains a challenge of exploiting common sense for KGC.\nFollowing the open-world assumption [34], most KGE models employ a pairwise loss function and employ negative sampling process to negerate negative triples based on the local closed-world assumption [35]. However, uniform sampling [14] cannot judge the validity of negative triples and might generate low-quality or false-negative triples. As the instance shown in Fig. 1, the semantic gap between a low-quality negative triple (San Francisco, LocatedIn, Iphone) and a positive triple (San Francisco, LocatedIn, California) is too large, causing invalid training of KGE models. Although some mechanisms like KBGAN [36] and self-adversarial sampling [16] evaluate the quality of negative triples, the issue of false negatives persists. Therefore, generating high-quality triples is crucial for training any KGE model effectively.\nTo address the above pivotal challenges, a pluggable common sense-enhanced KGC framework is proposed. As illustrated in Fig. 2, our framework consists of an Explicit Common Sense-Enhanced (ECSE) model (in section III) and an Implicit Common Sense-Enhanced (ICSE) scheme (in section IV). Specifically, explicit common sense could be automatically generated with ontological concepts. On account of KGs lacking concepts, each factual triple could be extended to an implicit common sense-level triple. For instance, given a factual triple (David, WorkFor, Google Inc.), we could generate an explicit common sense triple (Person, WorkFor, Company), or an implicit one (David's concept, WorkFor, Google Inc.'s concept) in the absence of entity concepts. Based on explicit common sense, a common sense-guided high-quality negative sampling strategy is designed to construct high-quality negative triples to facilitate more effective training of KGE models. Furthermore, a novel coarse-to-fine inference mechanism is proposed to ensure that the predicted triples conform to common sense. On the other hand, we develop a relation-aware concept embedding mechanism to learn the representation of implicit common sense triples, and then score each candidate triple based on both the common sense-level and the factual triples.\nThe main contributions of this paper can be summarized as the following three-folds:\n\u2022 To the best of our knowledge, it is the first effort to introduce common sense into KGE in both training and inference stages, contributing to higher accuracy of KGC in a joint common sense and fact-driven fashion. More interestingly, our framework can be conveniently integrated as a pluggable module into existing KGE models.\n\u2022 We rigorously demonstrate that our ICSE model could represent both factual and common sense-level triples across various relation patterns and complex relations.\n\u2022 To evaluate the scalability and effectiveness of our proposed framework, we conduct extensive experiments compared with some typical KGC baseline models and negative sampling strategies in the scenarios with and without entity concepts to demonstrate the superiority of the proposed framework."}, {"title": "II. RELATED WORKS", "content": "A. Classical KGE Models\nIn comparison to rule learning-based models [37]\u2013[39] and multi-hop reasoning-based models [40]\u2013[42], KGE approaches demonstrate superior efficiency, robustness, and inference performance. At present, the representative KGE models could be classified into three main categories:\n(1) Translation-based models. One of the most typical KGE models TransE [13] regards the relation in a triple as the translation operation from the head to the tail entities. TransH [14] and TransR [15] extend TransE by defining a hyperplane and a space specific to each relation, addressing the issue of inferring complex relations namely one-to-many (1-N), many-to-one (N-1), and many-to-many (N-N). RotatE [16] represents symmetric relations by regarding the relation as a rotation operation. QuatE [17] represents entities and relations in quaternion space to enhance the representation. HAKE [18] learns entity and relation embeddings in the polar coordinate system to represent entities at different hierarchy levels.\n(2) Tensor factorization-based models. RESCAL [19] scores each triple via three-way matrix multiplication among two vectors of the entity pair and a matrix representing the relation. DistMult [20] simplifies RESCAL by representing each relation with a diagonal matrix. ComplEx [21] embeds entities and relations into complex space, and performs tensor decomposition with Hamiltonian multiplication. HolE [43] models the interaction between entities via vector circular correlation. DURA [44] designs an entity embedding regularizer for tensor factorization-based models.\n(3) Neural network-based models. NTN [22] and NAM [45] employ multi-layer perception while ConvE [23] and ConvKB [46] exploit convolutional neural networks to encode interactions among entities and relations. R-GCN [24], SACN [47], KBGAT [48] and DRGI [49] introduce graph neural networks to encode the neighborhood of entities.\nB. KGE Models Based on Auxiliary Information\nConventional KGE models exclusively focus on factual triples within KGs, neglecting a substantial amount of auxiliary information associated with entities and relations. TKRL [32] utilizes entity types but might introduce some noisy types. JOIE [33] jointly learns the embeddings from both the ontology and the instance graphs. Nevertheless, the ontology is inapplicable to inference because the relations in the ontology have few overlaps with those in the instance graph. Moreover, many KGs such as NELL [4], constructed through automatic or semi-automatic OpenIE techniques [50], only express the abstract concepts of entities. Additionally, some KGs like WordNet [3] even lack entity types, which limits the effectiveness of current type-based KGC models.\nC. Negative Sampling on KGs\nIn accordance with the open-world assumption (OWA) [34], training KGE models consistently employs a pair-wise loss function with both positive and negative triples. Existing negative sampling strategies for KGE models are designed based on the local-closed world assumption [53] and can be categorized into five groups: (1) Random uniform sampling: randomly substituting an entity or relation in the positive triple with another entity or relation, following a uniform distribution [14]. (2) Adversarial-based sampling: KBGAN [36] learns KG embeddings within an adversarial training framework [54]. This allows the discriminator to select high-quality negative triples. Inspired by KBGAN, Self-adversarial sampling [16] efficiently evaluates the quality of negative triples without a generator. (3) Domain constraint-based sampling: these negative sampling techniques corrupt entities according to the constraints derived from abstract domains [55] or concrete type information [56]. (4) Efficient sampling: NSCaching [57] incorporates a caching mechanism to efficiently sample negative triple candidates. (5) None-sampling: NS-KGE [58] eliminates negative triples by transforming the pair-wise loss function into a square loss."}, {"title": "III. EXPLICIT COMMON SENSE-GUIDED MODEL", "content": "For KGs containing rich entity concepts, explicit common sense can be automatically generated from the KG. Then, a common sense-guided negative sampling strategy is designed to improve the training process. Furthermore, we propose a coarse-to-fine inference mechanism that incorporates common sense-based candidate filtering and fact-based prediction.\nA. Automatic Common Sense Generation\nInspired by some well-known common sense graphs such as ConceptNet [30] and Microsoft Concept Graph [31] which represent common sense as concepts linked by their relations, we generate appropriate common sense by substituting entities in factual triples with concepts via an entity-to-concept converter as shown in Fig. 3. Particularly, according to the requirements for the usage of common sense in the following negative sampling and inference procedures, the automatically generated common sense here can be separated into the individual-form $C_1$ and the set-form $C_2$ as:\n$C_1 = (ch, r, ct), C_2 = (Ch,r,Ct)$\nwhere the individual-form common sense (ch,r,ct) consists of a head concept ch, a tail concept ct and an instance-level relation r between them. Furthermore, the set-form of common sense $C_2$ is derived by merging the common sense triples with the same relation in $C_1$ into a single common sense triple consisting of a relation and the accompanying head concept set and tail concept set.\nFor better understanding, take the instance in Fig. 3, the common sense of individual-form associated with the relation LocatedIn could be represented as (City, LocatedIn, State) and (Company, LocatedIn, State) while the common sense of set-form is ({City, Company}, LocatedIn, {State}). It is noteworthy that the individual-form is more accurate for representing common sense. On the contrary, the set-form common sense has a more diverse representation since the head concept and the tail concept are not unique. The detailed workflow of our automatic common sense generation mechanism is provided in Algorithm 1.\nB. Common Sense-Guided Negative Sampling\nGenerating high-quality negative triples is a crucial aspect of training robust KGE models. To achieve this, it is essential to simultaneously address three key factors: (1) Preventing the erroneous negative triples which would introduce noise. (2) Acquiring negative triples of superior quality, thereby enhancing the overall robustness of the embedding model. (3) Establishing a diverse array of negative triples, contributing to the comprehensive evaluation and refinement of KGE models.\nTo address the challenge of avoiding false-negative triples while concurrently enhancing the quality and diversity of negative triples, we take advantage of the common sense in set-form and leverage the complex properties of relations. We modify the traditional negative sampling strategies by developing two sampling principles as followings.\nComplex relation-aware sampling: this sampling strategy considers the complex characteristics of relations like N-1 relation BirthPlace. We define the unique entity and the non-unique entity according to the complex properties of relations, such as a non-unique tail entity and a unique head entity associated with the N-1 relation BirthPlace. By replacing the unique entity in a positive triple by any other entity, the reconstructed triple must be incorrect, ensuring that it is a true negative triple and resolving the issue of false-negative triples. On the contrary, a negative triple created by replacing non-unique entities requires extra criteria to evaluate its quality.\nCommon sense-enhanced sampling: unlike random sampling, this common sense-enhanced sampling approach creates negative triples that exhibit semantic similarity to positive triples, contributing to more performance gains of training KGE models. Specific to the negative triple candidates obtained by substituting the unique entity, the higher score indicates the higher quality, which could be assigned with larger weights. Besides, the higher-scored negative triples achieved by replacing the non-unique entity are more likely to be potential positive triples, and their weights should be lower to lessen the influence of training with false-negative triples.\nTo explain the advantages of our high-quality negative sampling, we present an example applied to an N-1 relation BirthPlace on the dataset DBpedia as shown in Fig. 4. The negative sampling process comprises two main steps:\n(1) Selecting concept candidates: given a positive triple (Neil Brady, BirthPlace, Montreal), we extract the corresponding set-form common sense ({Person, Artist, Writer, Athlete, Engineer}, BirthPlace, {Country, City, Island, Settlement,Town}) in regard to the relation BirthPlace. On account of the complex relation-aware sampling and the non-unique head entity, we identify all head concepts within the selected common sense as the head concept candidates. Besides, the concept City is selected as the tail concept candidate with regard to the unique entity Montreal.\n(2) Calculating the weights of negative triples: we sample entities belonging to the head or tail concept candidates to construct negative triple candidates such as (Ayn Rand, BirthPlace, Montreal). Then, we calculate the score of each negative triple candidate by KGE score function following the common sense-enhanced sampling principle. Specifically, the weight of a negative triple obtained by replacing the head entity Neil_Brady should be higher when the score of this negative triple is lower, thereby mitigating the false-negative issue. On the contrary, the weight of a negative triple generated by corrupting the tail entity Montreal is higher with the higher score, ensuring the better quality of the negative triple.\nParticularly, our framework is model-agnostic as to KGE, so we define a unified notation of score function $E_{ec} (h, r,t)$ to assess the plausibility of a triple (h,r,t). Here are three most typical score functions utilized in KGE models:\n(1) Translation-based score function, such as TransE:\n$E_{ec} (h,r,t) = - ||h+r-t||$\nwhere h, r and t signify the vector embeddings of head entity h, relation r and tail entity t, respectively.\n(2) Rotation-based score function, such as RotatE:\n$E_{ec} (h, r,t) = - ||h \\circ r-t||$\nin which $ \\circ $ denotes Hardmard product, and h, r, t indicate the vector embeddings in the complex space.\n(3) Tensor decomposition-based score function of ComplEx:\n$E_{ec}(h, r, t) = Re(h \\ diag (r) \\overline{t})$\nwhere diag(r) is a diagonal matrix in the complex space corresponding to r. Besides, h and t are the complex vectors of h and t. $\\overline{t}$ denotes the conjugation of t. Then, the weight of each reconstructed negative triple is obtained as followings:\n$w (h';, r,t) = 1 \u2212 p (h',;, r,t) = 1 - \\frac{exp (E_{ec} (h',;, r,t))}{\\Sigma_i exp (E_{ec} (h'i, r,t))}$\n$W (h,r,t) = p (h,r,t) = \\frac{exp(E_{ec} (h,r,t))}{\\Sigma_i exp (E_{ec} (h,r,t'))}$\nwhere $p (h';,r,t)$ and $p (h,r,t's)$ are the probability of the negative triples (h',r,t) and (h,r,t') being positive triples, respectively. $w (h';, r,t)$ and $w (h, r,t')$ indicate the weights of these two negative triples for training procedure.\nWe introduce a weighting scheme for negative triples originating from non-unique entity corruption, denoted as 1 \u2013 \u0440, mitigating the influence of false-negative triples. Conversely, in the context of negative triples derived from the unique entity, negative triples characterized by larger p values serve as indicators of higher quality that are assigned with higher weights. Thus, we allocate the highest weights to negative triple candidates involving the head entity Peter Cook and the tail entity London as shown in Fig. 4.\nSimilar to the above-mentioned common sense-guided negative sampling process for N-1 relations, the negative triples could also be generated for 1-1, 1-N, and N-N relations. Algorithm 2 shows the general procedure for our developed common sense-guided negative sampling strategy, adaptable to diverse properties of complex relations.\nC. Training and Coarse-to-Fine Inference\nFurthermore, we feed positive triples and weighted high-quality negative triples into a KGE model to learn entity and relation embeddings. Corresponding to the various score functions introduced above, we present two typical loss functions:\n$L = \\Sigma{max[0, \\gamma \u2013 E_{ec} (h, r,t) + w(h', r, t)E_{ec}(h',;, r, t)] + max[0, \\gamma \u2013 E_{ec}(h, r,t) + w(h,r,t)E_{ec}(h, r,ti)]}$\n$L = \u2212 log \\sigma(y + E_{ec} (h, r,t)) - \\Sigma{w(h, r,t) log \\sigma (-E_{ec}(h, r, t) \u2013 \\gamma) + w(h, r, t') log \\sigma(-E_{ec}(h, r, t';) - \\gamma)}$"}, {"title": "IV. IMPLICIT COMMON SENSE-GUIDED MODEL", "content": "With regard to the KGs that lack sufficient entity concepts, we extend the factual triples to be implicit common sense triples to guarantee the scalability of our framework. Consequently, we develop a relation-aware concept embedding module to learn embeddings of concepts and common sense-level relations. In order to obtain inference results, we score each candidate triple using a joint common sense and fact-driven fashion.\nA. Relation-Aware Concept Embedding\nEach factual triple (h, r, t) can be expanded into an implicit common sense triple (ch, r, ct), where the head concept ch and the tail concept ct should imply the abstract representations of entities adaptive to the specific relation r. For instance, in Fig. 5, the observed factual triples reveal that the entity LeBron James is associated with the ontological concepts of Athlete and Manager through intuitive inference. These ontological associations are specific to the relations PlayIn and Found, respectively. However, the entities LeBron James and Lionel Messi exhibit similar conceptual representations referred to the same relation PlayIn. Motivated by these observations, we propose a relation-aware concept embedding mechanism for modeling implicit common sense.\nPrimarily, we introduce the meta-concept embedding to represent each entity concept independent of any relation. Then, to accurately represent the entity concept that complies with the semantics of a specific relation, we design a relation-aware projection operator to transform the meta-concept embedding of an entity into the appropriate relation-aware concept embeddings adaptive to the associated relations. Then, we could model the implicit common sense triple with various common sense-specific score functions defined as follows:\n(1) Translation-based score function:\n$ch,r = M_r Ch, ct,r = M_r ct$\n$E_{cs}(h,r,t) = -||Ch,r + Cr \u2013 Ct,r||$\nin which $c_r$ denotes the vector embedding of the relation r in the real space, $c_h$ and $c_t$ indicate the meta-concept embeddings of the head entity h and the tail entity t, respectively. $c_{h,r}$ and $c_{t,r}$ are the concept embeddings of entities h and t adaptive to the relation r via the projection matrix $M_r$.\n(2) Rotation-based score function:\n$Ch,r Pr Ch, Ct,r = pr Ct$\n$Ecs(h,r,t) = - ||Ch,r \\Theta Cr Ct,r||$\nwhere $c_r, c_h$ and $c_t$ denotes the similar embeddings in Eq. 10-11 but are represented as the complex vectors. $c_{h,r}$ and $c_{t,r}$ are the concept embeddings of entities via the projection operator p in the complex vector space specific to the relation r.\n(3) Tensor decomposition-based score function:\n$Ch,r pro Ch, Ct,r = pr Ct$\n$E_{cs}(h, r,t) = Re(ch, diag (cr) \\overline{ct,r})$\nwhere diag(cr) represents the diagonal matrix embedding in the complex space of the relation r. pr, ch, ct, ch,r and ct,r are the same definitions in Eq. 12 and Eq. 13. Ct,r indicates the conjugation of ct,r.\nActually, a common sense triple reveals the abstract and generalized semantics of the intersection among a relation and its connected entity pair. For common sense triples, the head entities associated with the relation Found all refer to the concept Manager, whereas the tail entities are tied to the concept Company. Inspired by this natural property, we propose a concept similarity constraint mechanism to enhance the abstract features of concept embeddings. For two factual triples $(h_1,r,t_1)$ and $(h_2,r,t_2)$ with the same relation r, we expect that the concept embeddings in these two triples satisfy:\n$Ch1,r = Ch2,r, Ct1,r = Ct2,r$\nwhere $ch1,r$ and $ch2,r$ denote two head concepts while $ct1,r$ and $ct2,r$ represent two tail concepts associated with the relation r obtained by relation-aware concept embedding. Furthermore, regarding arbitrary factual triples $(h_1,r_1,t_1)$ and $(h_2, r_2, t_2)$, we define a score function for evaluating the similarity of concepts in these two triples as:\n$E_{sim} ((h_1, r_1, t_1), (h_2, r_2, t_2)) = -0.5 (||Ch1,r1 \u2013 Ch2,r2|| +||Ct1,r1 \u2013 Ct2,r2||)$\nin which $Ch1,r1$ and $ct1,r1$ are the head and tail concept embeddings relevant to the relation r1. $ch2,r2$ and $ct2,r2$ denote the head and tail concept embeddings adaptive to the relation r2, respectively. In this regard, according to the expectation defined in Eq. 16, the score function $E_{sim}$ would tend to a larger value when r1 and r2 are actually the same relations, which enhances the abstract feature of the entities to represent the semantics of concepts in implicit common sense.\nB. Joint Embedding Based on Common Sense and Fact\nThe model-agnostic nature of our framework signifies that we could directly leverage the score functions of many existing KGE models to represent a factual triple (h, r, t). To maintain consistency with the score functions in Eq. 10-15, the fact-specific score functions are clarified as:\n(1) Translation-based score function (TransE):\n$E_f (h,r,t) = - ||h+r-t||$\nin which h, t and r are vector embeddings in the real space of the entities h and t as well as the relation r.\n(2) Rotation-based score function (modified RotatE):\n$hr=h-whwr, tr=t-wr twr$\n$Ef(h, r,t) = -||hr r-tr ||$\nwhere h, t and r are complex vector embeddings of h, t and r. cher and ctr are the mapped entity embeddings obtained by a hyperplane with the normal vector wr associated with the relation r.\n(3) Tensor decomposition-based score function (ComplEx):\n$Ef(h, r,t) = Re(h diag (r) t)$\nwhere diag(r) is the diagonal matrix in the complex space for representing the relation r. h and t are the complex vector embeddings of h and t. t denotes the conjugation of t.\nBased on the previously defined score functions, we propose a joint learning approach tailored to both common sense triples and factual triples. The objective is to ensure that the learned entity embeddings capture unique characteristics, while concept embeddings emphasize abstract semantics. This is achieved by embedding entities in a higher-dimensional representation space compared to concepts. The fact-level embeddings of entities and relations, along with the common sense-level embeddings of concepts and relations, are achieved through a multi-task learning scheme. Besides, we employ a contrastive learning strategy to enhance the semantic similarity of concept embeddings within the same category. Consequently, the overall loss function is formulated as follows:\n$L = \\Sigma{ \\Sigma{(Lf+a_1L_{cs})} + a_2L_{sim}}$\nin which the overall loss function L consists of three components, namely fact-specific loss Lf, common sense-specific loss Lcs and concept embedding similarity regularization Lsim. The weights 1 and 2 serve as trade-off parameters to balance the three components. T denotes the set of all positive triples in the training set, while T' is the set of negative triples generated using self-adversarial negative sampling [16]. The loss functions Lf, Lcs, and Lsim, are defined as follows:\n$Lf = \u2212 log \\sigma (1 + Ef (h, r,t)) \u2013 log \\sigma (\u2212Ef (h',r,t') \u2013 1)$\n$Lcs = max[0, 2 \u2013 E_{cs} (h,r,t) + E_{cs} (h', r,t')]$\n$L_{sim} = \\Sigma{\\Sigma{max[0, 3 \u2013 E_{sim}((h,r,t),(hp, r, tp)) + E_{sim}((h, r,t), (hn, r', tn))]}}$\nin which Y1, Y2 and 3 denote the margins in Lf, Lcs and Lsim, respectively. o represents the sigmoid function. In Eq. 25, the positive triple (h,r,t) is regarded as an anchor instance. Meanwhile, (hp,r,tp) denotes a positive instance within the set of all triples sharing the same relation r with the anchor instance. (hn,r',tn) signifies a negative instance in the set Y' containing the triples without the relation r.\nThe embeddings of fact-level entities and relations together with common sense-level concepts and relations can be learned by optimizing the loss functions in Eq. 22-Eq. 25. During the inference stage, the plausibility of each triple candidate (h,r,ej) is assessed by a dual score function from both the fact and common sense perspectives, which is formulated as:\n$E_{ic} (h, r, ej) = E_f (h, r, ej) + a_1E_{cs} (h, r, ej)$\nThe intuition behind this scoring function is that a triple is more likely to be true if it is consistent with both the common sense and the factual knowledge. Then, all triple candidates are sorted in descending order based on their scores derived from Eq. 26, and the candidate triples with higher rankings are considered as the inference results. Importantly, ICSE approach can serve as a pluggable module to be conveniently integrated with any KGE model on any KG to exploit implicit common sense for improving KGC performance.\nC. Proof of Representing Various Relational Characteristics\nNaturally, there are four relational patterns in most KGs, namely symmetry, anti-symmetry, inversion, and composition. Additionally, relations exhibit complex properties including 1-1, 1-N, N-1, and N-N. For a clearer understanding, we offer definitions and examples of relational patterns and complex properties in Table I. In this paper, we analyze the capability of our approach in effectively representing these relational patterns and complex relations. This capability plays a pivotal role in the overall inference performance of a KGE model. A comparison between some existing models and our approach in handling various relational patterns and complex properties of relations is presented in Table II. Detailed proofs are provided in the following.\nLemma 1. Our model could represent the symmetry of relations specific to factual triples.\nProof. Based on the score function defined in Eq 20, two factual triples (h, r, t) and (t, r, h) associated with a symmetric relation r should satisfy the following constraints:\n$hor=tr, tror = hr$\nFrom Eq. 27, we can retrive that\n$horor = h$\nBased on Eq. 28, we deduce that ror = v(1), where the notation v(1) signifies a vector whose elements are all 1. Thus, we could represent the symmetric relation r for factual triples following this constraint of relation embedding.\nLemma 2. Our model could represent the anti-symmetry of relations specific to factual triples."}, {"title": "V. EXPERIMENTS", "content": "In this section", "Datasets": "Our experiments leverage six commonly-used benchmark datasets", "concepts": "FB15K [13", "61": "are two subsets of Freebase [1", "40": "is sampled from NELL [4", "62": "is extracted from DBpedia [5"}, {"concepts": "WN18 [13", "4": ".", "23": "is the subset of YAGO [2", "Baselines": "We compare our framework with several typical and state-of-the-art KGE baseline approaches", "13": "TransH [14", "15": "DistMult [20", "43": "SimplE [59", "21": "RotatE [16", "60": "and HAKE [18", "reasons": 1, "Protocol": "The experiments are implemented using PyTorch on a system running Ubuntu 16.04", "tuning": "dimensions of embeddings are selected from the set: {150", "range": {"from": {"14": ".", "metrics": 1, "categories": "n\u2022 r is a 1-1 relation if aht < 1.5 and ath < 1.5.\n\u2022 r is a 1-N relation if aht > 1.5 and ath < 1.5.\n\u2022 ris an N-1 relation if aht < 1.5 and ath > 1.5.\n\u2022 ris an N-N relation if aht > 1.5 and ath > 1.5.\nFollowing the inference stage of our framework"}, "metrics": "Following the inference stage of our framework"}, "metrics": "n\u2022 MR: mean rank of all"}]}