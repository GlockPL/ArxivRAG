{"title": "MetaSeg: MetaFormer-based Global Contexts-aware Network for Efficient\nSemantic Segmentation", "authors": ["Beoungwoo Kang", "Seunghun Moon", "Yubin Cho", "Hyunwoo Yu", "Suk-Ju Kang"], "abstract": "Beyond the Transformer, it is important to explore how\nto exploit the capacity of the MetaFormer, an architecture\nthat is fundamental to the performance improvements of\nthe Transformer. Previous studies have exploited it only\nfor the backbone network. Unlike previous studies, we ex-\nplore the capacity of the Metaformer architecture more ex-\ntensively in the semantic segmentation task. We propose a\npowerful semantic segmentation network, MetaSeg, which\nleverages the Metaformer architecture from the backbone\nto the decoder. Our MetaSeg shows that the MetaFormer\narchitecture plays a significant role in capturing the use-\nful contexts for the decoder as well as for the backbone.\nIn addition, recent segmentation methods have shown that\nusing a CNN-based backbone for extracting the spatial in-\nformation and a decoder for extracting the global informa-\ntion is more effective than using a transformer-based back-\nbone with a CNN-based decoder. This motivates us to adopt\nthe CNN-based backbone using the MetaFormer block and\ndesign our MetaFormer-based decoder, which consists of\na novel self-attention module to capture the global con-\ntexts. To consider both the global contexts extraction and\nthe computational efficiency of the self-attention for seman-\ntic segmentation, we propose a Channel Reduction Atten-\ntion (CRA) module that reduces the channel dimension of\nthe query and key into the one dimension. In this way, our\nproposed MetaSeg outperforms the previous state-of-the-art\nmethods with more efficient computational costs on pop-\nular semantic segmentation and a medical image segmen-\ntation benchmark, including ADE20K, Cityscapes, COCO-\nstuff, and Synapse. The code is available at https:\n//github.com/hyunwoo137/MetaSeg.", "sections": [{"title": "1. Introduction", "content": "Semantic segmentation, which classifies categories for\neach pixel, is a challenging task in computer vision. This\ntask has a wide range of applications [7, 8, 37], including\nautonomous driving and medical image segmentation.\nWith the great success of the vision transformer (ViT)\n[10] in the image classification, the transformer-based\nmethods have been introduced in the field of semantic seg-\nmentation. Most previous studies [30\u201332, 34, 36] mainly\nutilize the self-attention layer in the transformer block to\nachieve the superior performance. However, recent re-\nsearch [38] found that the abstracted architecture of the\ntransformer block (i.e., MetaFormer block), which consists\nof a token-mixer, channel MLPs and residual connections,\nplays a more significant role in achieving the competitive\nperformance than the specific token mixer (e.g. attention,\nspatial MLP). Therefore, the MetaFormer architecture has\nthe potential to be variably applied with different token mix-\ners depending on the specific purpose.\nFrom the MetaFormer architecture, some recent stud-\nies [16, 29] have derived their own methods. For example,"}, {"title": "2. Related Works", "content": "EfficientFormer [16] employs the MetaFormer architecture\nusing the self-attention as the token mixer to effectively cap-\nture the global semantic-aware features. InternImage [29]\nalso utilizes the MetaFormer with the deformable convo-\nlution as the token mixer to capture the contextual infor-\nmation. These methods have exploited the capacity of the\nMetaFormer architecture only for the encoder. However,\nunlike previous studies, we take advantage of the capac-\nity of the MetaFormer block more extensively for the se-\nmantic segmentation task. Therefore, we propose a novel\nand powerful segmentation network, MetaSeg, which uti-\nlizes the MetaFormer block up to the decoder to obtain the\nenhanced visual representation.\nIn addition, previous segmentation methods [17, 30, 34]\nused the transformer-based backbone with the CNN-based\ndecoder. However, recent studies [13, 39] have shown that\nusing the CNN-based backbone for extracting the local in-\nformation and the decoder for extracting the global infor-\nmation is more effective in improving the performance by\ncompensating for the globality in the local contexts. Based\non this observation, we adopt the CNN-based backbone\n(i.e., MSCAN [13]) that contains the MetaFormer block\nused the convolution as a token mixer, and design a novel\ntransformer-based decoder. Since it is important to con-\nsider the globality in the decoder to complement the CNN-\nbased encoder features, the proposed decoder leverages the\nMetaFormer block that uses the self-attention as a token\nmixer to capture the global contexts. However, the self-\nattention has a limitation of the considerable computational\ncosts due to the high-resolution features in the semantic seg-\nmentation task.\nTo address this issue, we propose a novel and effi-\ncient self-attention module, Channel Reduction Attention\n(CRA), which embeds the channel dimension of the query\nand key into the one dimension for each head in the self-\nattention operation. Conventional self-attention methods\n[10, 17, 30, 32, 34], which embed the channel dimension of\nthe query and key without the channel reduction, show great\nperformance but have high computational costs. Compared\nto these methods, our method leads to competitive perfor-\nmance with the computational reduction. This indicates\nthat our CRA can sufficiently consider the globality even\nwhen each query and key token is a scalar type, not a vector.\nTherefore, our CRA module is more efficient and effective\nthan the previous self-attention modules.", "2.1. MetaFormer-based architecture": "MetaFormer is an general architecture of the transformer\n[27] where the token mixer is not specified. Recent meth-\nods [25, 26, 38] have explored various types of token mixers\nwithin the MetaFormer architecture to encourage the per-\nformance. Mlp-Mixer [25] and ResMLP [26] utilized MLP-\nlike token mixers. PoolFormer [38] simply exploited pool-\ning as token mixers to verify the power of the MetaFormer\narchitecture. PVT [30], Swin [17], CvT [32], and Effi-\ncientFormer [16] adopted the self-attention as token mix-\ners to aggregate the global information. These studies have\nfocused on exploiting a variant token mixer based on the\nMetaFormer in the encoder. Therefore, we propose novel\nMetaFormer block which is leverage our Channel Reduc-\ntion Attention (CRA) module as a token mixer. In addition,\nunlike the previous methods that apply the MetaFormer ar-\nchitecture to the encoder, we propose novel approach that\nthe capacity of the MetaFormer architecture is extended to\nthe decoder to consider the globality that is helpful for im-\nproving the segmentation performance.", "2.2. Semantic segmentation": "As ViT [10] have achieved the great success on the image\nclassification task, self-attention based transformer back-\nbones have also been explored in the semantic segmenta-\ntion task. SETR [41] was the first to use ViT as a backbone"}, {"title": "3. Method", "content": "This section describes our MetaSeg architecture, an ef-\nficient and powerful segmentation network. Basically, we\nadopt the CNN-based encoder and MetaFormer-based de-\ncoder to aggregate the local and global information. We\nfirst explain the overall architecture, and then explain the\nencoder and decoder. Finally, we describe the Global Meta\nBlock (GMB) with the proposed Channel Reduction Atten-\ntion (CRA) that is an efficient self-attention module."}, {"title": "3.1. Overall Architecture", "content": "As shown in Fig. 2 (a), our MetaSeg is based on the\nMetaFormer block with a hierarchical backbone network\nof the four stages. We utilize the CNN-based encoder that\nadopts a series of convolutional layers as a token mixer. The\nencoder aggregates the local information from the input via\nthe token mixer. For the decoder, we design the novel CRA\nmodule as a token mixer to capture the global contexts with\nlow computational costs."}, {"title": "3.1.1 Hierarchical convolutional encoder", "content": "We adopt the CNN-based pyramid encoder to acquire\nmulti-scale features. Following previous encoder-decoder\nstructured segmentation networks, given an image $I \\in$\n$\\mathbb{R}^{H\\times W\\times 3}$ as an input, each stage of the encoder extracts\nthe down-sampled features $F_i \\in \\mathbb{R}^{\\frac{H}{2^{i+1}}\\times \\frac{W}{2^{i+1}}\\times C_i}$ where\n$i\\in \\{1,2,3,4\\}$ and $C_i$ denote the index of the encoder\nstage and the channel dimension. These features provide the\ncoarse to fine-grained features that leads to the performance\nimprovements of the semantic segmentation. Specifically,\nwe adopt MSCAN [13] as a encoder, which consists of\nMetaFormer blocks using a convolution-based token mixer."}, {"title": "3.1.2 Lightweight decoder", "content": "The decoder of our MetaSeg exploits the MetaFormer\narchitecture to improve the capture the global contexts that\nare not considered enough in the encoder. We discovered\nthat the MetaFormer block, with the self-attention module\nas a token mixer, exhibits exceptional capability in gather-\ning global contexts from the multi-scale features of the en-\ncoder. The decoder consists of following components: the"}, {"title": "3.2. Global Meta Block (GMB)", "content": "The proposed GMB leverages the MetaFormer block in\nthe decoder to further enhance the global contexts of the fea-\nture representations extracted by the encoder, which mainly\nfocuses on the local context. As illustrated in Fig. 2 (b),\nthe GMB adopts the MetaFormer block of two residual sub-\nblocks and employs a novel channel reduction self-attention\n(CRA) module as a token mixer. Our CRA module effec-\ntively captures global contexts of the features with efficient\ncomputational costs. The GMB is performed at each stage\nexcept the first stage (i.e., $i \\in \\{2, 3, 4\\}$). The overall opera-\ntion is defined as follows:\n$M_i = CRA(LN(F_i)) + F_i,$\n$F = MLP(LN(M_i)) + M_i,$\nwhere LN and MLP denote the layer normalization and the\nchannel MLP layer, respectively."}, {"title": "3.2.1 Channel Reduction Attention", "content": "We propose the Channel Reduction Attention (CRA)\nmodule as a novel token mixer utilized in the GMB to con-\nsider both the globality extraction and the computational\nefficiency of the self-attention for the semantic segmenta-\ntion. Our CRA is based on the multi-head self-attention.\nThe key and value are average pooled before the attention\noperation. As shown in Fig. 3, the channel dimensions\nof the query and key are embedded into the one dimen-\nsion to further reduce the computational costs. We found\nthat the channel squeezed query $Q \\in \\mathbb{R}^{Head\\times H_iW_i\\times 1}$ and\nkey $K \\in \\mathbb{R}^{Head\\times (H_iW_i/r_i^2)\\times 1}$ can sufficiently extract global\nsimilarities. The CRA operation is formulated as follows:\n$CRA(F_i) = Concat(Head_0, ..., Head_j)W^O,$\n$Q_i = F_iW_i^Q, K_i = AvgPool(F_i)W_i^K,$\n$V_i = AvgPool(F_i)W_i^V, Head_j = Att(Q_i, K_i, V_i),$\n$Att(Q_i, K_i, V_i) = Softmax(\\frac{Q_iK_i^T}{\\sqrt{d_k}})V_i,$\nwhere $W^Q, W^K \\in \\mathbb{R}^{C_i\\times 1}$, $W^V\\in \\mathbb{R}^{C_i}$, and $W^O \\in$\n$\\mathbb{R}^{C_i\\times C_i}$ are projection parameters. $j$ denotes the number"}, {"title": "3.2.2 Channel MLP", "content": "The channel MLP is used to consolidate the features pro-\ncessed with our token mixer. Channel MLP consists of the\ntwo 1\u00d71 convolution layers with a GELU activation layer.\nThe operation is defined as follows:\n$MLP(x) = Conv_{1\\times 1}(GELU(Conv_{1\\times 1}(x))),$\nwhere $Conv_{1x1}$ denotes the 1 \u00d7 1 convolution layer."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Experimental Settings", "content": "Datasets. We conducted experiments on four publicly\navailable datasets, ADE20K [42], Cityscapes [9], COCO-\nStuff [1], and Synapse [15]. ADE20K is a challenging\nscene parsing dataset composed of 20,210/2,000/3,352 im-\nages for training, validation, and testing with 150 semantic\ncategories. Cityscapes is an urban driving scene dataset that\ncontains 5,000 images finely annotated with 19 categories."}, {"title": "4.2. Comparison with State-of-the-Art Methods", "content": "ADE20K, Cityscapes, and COCO-Stuff datasets. In Ta-\nble 1, we compared our MetaSeg performance with previ-\nous state-of-the-art methods on ADE20K, Cityscapes, and\nCOCO-Stuff datasets. This comparison includes the num-\nber of the parameters, Floating Point Operations (FLOPs),\nand mIoU under both the single scale (SS) and multi-scale\n(MS) flip inference strategies. As shown in the Table 1,\nMetaSeg-T showed significant performance of 42.4% mIoU\nwith only 4.7M parameters and 5.5 GFLOPs for ADE20K.\nCompared to SegNeXt-T that uses the same backbone [13],\nour MetaSeg-T achieved 1.3% higher mIoU and 16.7%\nlower GFLOPs on ADE20K. Moreover, our MetaSeg-T\nshowed 0.3% and 1.0% higher mIoU with 5.2% and 16.7%\nlower GFLOPs on Cityscapes and COCO-Stuff, respec-\ntively. Our larger model, MetaSeg-B, also achieved com-\npetitive performance compared to previous state-of-the-art\nmodels. MetaSeg-B showed 48.5% mIoU with 12.9% less\ncomputations compared to SegNeXt-B on ADE20K. Fur-\nthermore, our MetaSeg-B achieved 82.7% and 45.8% mIoU\nwith 8.9% and 12.9% less GFLOPs on Cityscapes and\nCOCO-Stuff, respectively. These results demonstrated that\nour MetaSeg effectively captures the local to global con-\ntexts by leveraging the MetaFormer architecture up to the\ndecoder with an efficient token mixer, our CRA.", "Speed Benchmark Comparison.": "In Table 2, we present\nthe speed benchmark comparisons without any additional\naccelerating techniques. For fair comparison, we mea-\nsured Frames Per Second (FPS) of a whole single image\nof 1536\u00d7768 on Cityscapes using a single RTX3090 GPU.\nCompared to previous methods, our method achieved su-\nperior FPS with a higher mIoU score. This result demon-\nstrates that a decrease in FLOPs of our method can lead to"}, {"title": "4.3. Ablation Study", "content": "improvements in processing speed within the GPU.\nSynapse dataset. In Table 3, we compared our MetaSeg\nwith the previous methods on Synapse dataset using DSC\n(%). For a fair comparison, we utilized MetaSeg-B in the\nmedical image segmentation task by considering the sim-\nilar model size with the previous methods. As shown in\nTable 3, our MetaSeg-B sets the new state-of-the-art re-\nsult with 82.78% DSC. This result showed a 2.09% higher\nDSC compared to HiFormer [14]. This indicates that our\nMetaSeg is effective even for the medical image segmenta-\ntion task. Therefore, we demonstrated the high capabilities\nof our MetaSeg for application fields.", "Effectiveness of MetaSeg Decoder for Various CNN-based Backbones.": "In Table 4, we experimented with other\nCNN-based backbones to evaluate the effect of our MetaSeg\ndecoder. In semantic segmentation, ConvNeXt [18] adopts\nUperNet [33] as its decoder and MobileNetV2 [22] adopts\nDeepLabV3 [4] as its decoder. For these CNN-based back-\nbones, our decoder showed competitive performance with\nsignificant computational reduction of 86% and 93.9%.\nThis indicates that our MetaSeg decoder is an efficient and\neffective architecture for various CNN-based backbone by\nenhancing the visual representation from encoder features.", "Effectiveness of Global Meta Block.": "In Table 5, we\nverified the effectiveness of applying GMB in the de-\ncoder. We conducted experiments on various cases of ap-\nplying or non-applying GMB to each Stage{2,3,4}. Fol-\nlowing [13], we excluded the features from the first stage\nof the encoder in this experiment since they contain too\nmuch low-level information which degrades the segmenta-\ntion performance. The results show that applying GMB to"}, {"title": "Efficiency of Channel Reduction Attention.", "content": "Stage{2,3,4} is most effective structure compared to other\ncases. Especially, compared to Stage{3,4}, applying GMB\nto Stage{2,3,4} achieved 0.8% higher mIoU performance\neven though the parameters and GFLOPs are almost the\nsame. This result indicates that capturing the global con-\ntexts through the GMB from all features extracted by the\nencoder Stage{2,3,4} is effective in improving the seman-\ntic segmentation performance.", "Effectiveness of Global Modeling Token Mixer in Decoder.": "In Table 6, we conducted an experiment on apply-\ning various token mixers to our proposed meta block-based\ndecoder. Through this experiment, we verify which token\nmixer is the most effective and efficient structure for the de-\ncoder when using MSCAN-T, a CNN-based backbone. The\nglobal context modeling token mixer (e.g. SRA and our\nCRA) showed the better mIoU performance compared to\nthe local context modeling token mixer (e.g. pooling, depth-\nwise convolution and conventional convolution). This result\ndemonstrates the importance of considering the global con-\ntexts in the decoder when using a CNN-based backbone.", "In Table 7, we focus on the parameter size and computational costs\nof our channel reduction self-attention (CRA) and the spa-\ntial reduction self-attention (SRA) [30] to compare which\nmethod is more efficient in terms of capturing global con-\ntexts.": "SRA is a widely used self-attention method that re-\nduces the spatial resolution of the key-value by treating the\ntoken as a vector. In contrast, our CRA scalarizes each\nquery and key token by reducing the channel dimension of\nthe query and key into the one dimension. As shown in Ta-\nble 7, our CRA reduces the computations of the query-key\noperation by a factor of C times, leading to a total computa-\ntion reduction for the attention operation that is about twice"}, {"title": "5. Conclusion", "content": "This paper proposed MetaSeg, a novel and powerful se-\nmantic segmentation network that effectively captures the\nlocal to global contexts by leveraging the MetaFormer ar-\nchitecture up to the decoder. Our MetaSeg showed that\nthe capacity of the MetaFormer can be extended to the de-\ncoder as well as the backbone. In addition, we proposed a\nnovel attention module for efficient semantic segmentation,\nChannel Reduction Attention (CRA) module, which can ef-\nficiently consider the globality by reducing the channel di-\nmension of the query and key into the one dimension for\nlow computational costs in the self-attention operation. Ex-\nperiments demonstrated the effectiveness and efficiency of\nour method on three public semantic segmentation datasets\nand a medical image segmentation dataset for application."}]}