{"title": "MetaSeg: MetaFormer-based Global Contexts-aware Network for Efficient Semantic Segmentation", "authors": ["Beoungwoo Kang", "Seunghun Moon", "Yubin Cho", "Hyunwoo Yu", "Suk-Ju Kang"], "abstract": "Beyond the Transformer, it is important to explore how to exploit the capacity of the MetaFormer, an architecture that is fundamental to the performance improvements of the Transformer. Previous studies have exploited it only for the backbone network. Unlike previous studies, we explore the capacity of the Metaformer architecture more extensively in the semantic segmentation task. We propose a powerful semantic segmentation network, MetaSeg, which leverages the MetaFormer architecture from the backbone to the decoder. Our MetaSeg shows that the MetaFormer architecture plays a significant role in capturing the useful contexts for the decoder as well as for the backbone. In addition, recent segmentation methods have shown that using a CNN-based backbone for extracting the spatial information and a decoder for extracting the global information is more effective than using a transformer-based backbone with a CNN-based decoder. This motivates us to adopt the CNN-based backbone using the MetaFormer block and design our MetaFormer-based decoder, which consists of a novel self-attention module to capture the global contexts. To consider both the global contexts extraction and the computational efficiency of the self-attention for semantic segmentation, we propose a Channel Reduction Attention (CRA) module that reduces the channel dimension of the query and key into the one dimension. In this way, our proposed MetaSeg outperforms the previous state-of-the-art methods with more efficient computational costs on popular semantic segmentation and a medical image segmentation benchmark, including ADE20K, Cityscapes, COCO-stuff, and Synapse. The code is available at https://github.com/hyunwoo137/MetaSeg.", "sections": [{"title": "1. Introduction", "content": "Semantic segmentation, which classifies categories for each pixel, is a challenging task in computer vision. This task has a wide range of applications [7, 8, 37], including autonomous driving and medical image segmentation.\nWith the great success of the vision transformer (ViT) [10] in the image classification, the transformer-based methods have been introduced in the field of semantic seg- mentation. Most previous studies [30\u201332, 34, 36] mainly utilize the self-attention layer in the transformer block to achieve the superior performance. However, recent re- search [38] found that the abstracted architecture of the transformer block (i.e., MetaFormer block), which consists of a token-mixer, channel MLPs and residual connections, plays a more significant role in achieving the competitive performance than the specific token mixer (e.g. attention, spatial MLP). Therefore, the MetaFormer architecture has the potential to be variably applied with different token mix- ers depending on the specific purpose.\nFrom the MetaFormer architecture, some recent stud- ies [16, 29] have derived their own methods. For example,"}, {"title": "2. Related Works", "content": "EfficientFormer [16] employs the MetaFormer architecture using the self-attention as the token mixer to effectively cap- ture the global semantic-aware features. InternImage [29] also utilizes the MetaFormer with the deformable convo- lution as the token mixer to capture the contextual infor- mation. These methods have exploited the capacity of the MetaFormer architecture only for the encoder. However, unlike previous studies, we take advantage of the capac- ity of the MetaFormer block more extensively for the se- mantic segmentation task. Therefore, we propose a novel and powerful segmentation network, MetaSeg, which uti- lizes the MetaFormer block up to the decoder to obtain the enhanced visual representation.\nIn addition, previous segmentation methods [17, 30, 34] used the transformer-based backbone with the CNN-based decoder. However, recent studies [13, 39] have shown that using the CNN-based backbone for extracting the local in- formation and the decoder for extracting the global infor- mation is more effective in improving the performance by compensating for the globality in the local contexts. Based on this observation, we adopt the CNN-based backbone (i.e., MSCAN [13]) that contains the MetaFormer block used the convolution as a token mixer, and design a novel transformer-based decoder. Since it is important to con- sider the globality in the decoder to complement the CNN- based encoder features, the proposed decoder leverages the MetaFormer block that uses the self-attention as a token mixer to capture the global contexts. However, the self- attention has a limitation of the considerable computational costs due to the high-resolution features in the semantic seg- mentation task.\nTo address this issue, we propose a novel and effi- cient self-attention module, Channel Reduction Attention (CRA), which embeds the channel dimension of the query and key into the one dimension for each head in the self- attention operation. Conventional self-attention methods [10, 17, 30, 32, 34], which embed the channel dimension of the query and key without the channel reduction, show great performance but have high computational costs. Compared to these methods, our method leads to competitive perfor- mance with the computational reduction. This indicates that our CRA can sufficiently consider the globality even when each query and key token is a scalar type, not a vector. Therefore, our CRA module is more efficient and effective than the previous self-attention modules.\nTo demonstrate the effectiveness and efficiency of our method, we conduct experiments on the challenging seman- tic segmentation datasets: ADE20K [42], Cityscapes [9], and COCO-stuff [1]. To verify the ability for the applica- tion, we also conduct experiments on the medical image segmentation dataset: Synapse [15]. As shown in Fig. 1, our MetaSeg-T and MetaSeg-B surpass the previous state- of-the-art methods on three public semantic segmentation"}, {"title": "2.1. MetaFormer-based architecture", "content": "MetaFormer is an general architecture of the transformer [27] where the token mixer is not specified. Recent meth- ods [25, 26, 38] have explored various types of token mixers within the MetaFormer architecture to encourage the per- formance. Mlp-Mixer [25] and ResMLP [26] utilized MLP- like token mixers. PoolFormer [38] simply exploited pool- ing as token mixers to verify the power of the MetaFormer architecture. PVT [30], Swin [17], CvT [32], and Effi- cientFormer [16] adopted the self-attention as token mix- ers to aggregate the global information. These studies have focused on exploiting a variant token mixer based on the MetaFormer in the encoder. Therefore, we propose novel MetaFormer block which is leverage our Channel Reduc- tion Attention (CRA) module as a token mixer. In addition, unlike the previous methods that apply the MetaFormer ar- chitecture to the encoder, we propose novel approach that the capacity of the MetaFormer architecture is extended to the decoder to consider the globality that is helpful for im- proving the segmentation performance."}, {"title": "2.2. Semantic segmentation", "content": "As ViT [10] have achieved the great success on the image classification task, self-attention based transformer back- bones have also been explored in the semantic segmenta- tion task. SETR [41] was the first to use ViT as a backbone"}, {"title": "3. Method", "content": "on the segmentation task. PVT [30], Swin [17], CvT [32], and LeViT [12] studied the hierarchical transformer-based backbone to exploit the multi-scale features. Beyond intro- ducing transformer backbones for the segmentation, Seg- former [34] designed a light-weight transformer backbone and a MLP-based decoder to consider the computational ef- ficiency. More recent methods [13, 39] adopted the CNN- based backbone with the transformer-based decoder to ag- gregate the local to global information. TopFormer [39] en- coded the tokens by the MobileNetV2 [22], and then fed the tokens into the transformer blocks. In SegNeXt [13], the convolution-based encoder extracts the spatial informa- tion and the transformer-based decoder extracts the global context. These methods [13, 39] have demonstrated that us- ing the CNN-based backbone with the transformer-based- decoder is effective for the semantic segmentation. Accord- ing to these studies, we adopt the combination of the CNN- based backbone and transformer-based decoder.\nAdditionally, transformer-based segmentation methods [17, 30, 34] have considered the computational efficiency of the attention mechanism due to high-resolution features. Swin [17] proposed a shifted window self-attention by par- titioning the feature maps into the windows. Some recent methods [30, 34] adopted a spatial reduction attention that reduces the resolution of the key-value. In this paper, we introduce a novel self-attention module, Channel Reduction Attention (CRA), which reduces the channel dimension of the query and key into the one dimension for efficient com- putational costs of the self-attention.\nThis section describes our MetaSeg architecture, an ef- ficient and powerful segmentation network. Basically, we adopt the CNN-based encoder and MetaFormer-based de- coder to aggregate the local and global information. We first explain the overall architecture, and then explain the encoder and decoder. Finally, we describe the Global Meta Block (GMB) with the proposed Channel Reduction Atten- tion (CRA) that is an efficient self-attention module."}, {"title": "3.1. Overall Architecture", "content": "As shown in Fig. 2 (a), our MetaSeg is based on the MetaFormer block with a hierarchical backbone network of the four stages. We utilize the CNN-based encoder that adopts a series of convolutional layers as a token mixer. The encoder aggregates the local information from the input via the token mixer. For the decoder, we design the novel CRA module as a token mixer to capture the global contexts with low computational costs."}, {"title": "3.1.1 Hierarchical convolutional encoder", "content": "We adopt the CNN-based pyramid encoder to acquire multi-scale features. Following previous encoder-decoder structured segmentation networks, given an image $I \\in \\mathbb{R}^{H \\times W \\times 3}$ as an input, each stage of the encoder extracts the down-sampled features $F_i \\in \\mathbb{R}^{\\frac{H}{2^{i+1}} \\times \\frac{W}{2^{i+1}} \\times C_i}$ where $i \\in \\{1,2,3,4\\}$ and $C_i$ denote the index of the encoder stage and the channel dimension. These features provide the coarse to fine-grained features that leads to the performance improvements of the semantic segmentation. Specifically, we adopt MSCAN [13] as a encoder, which consists of MetaFormer blocks using a convolution-based token mixer."}, {"title": "3.1.2 Lightweight decoder", "content": "The decoder of our MetaSeg exploits the MetaFormer architecture to improve the capture the global contexts that are not considered enough in the encoder. We discovered that the MetaFormer block, with the self-attention module as a token mixer, exhibits exceptional capability in gather- ing global contexts from the multi-scale features of the en- coder. The decoder consists of following components: the"}, {"title": "3.2. Global Meta Block (GMB)", "content": "Global Meta Block (GMB), the up-sampling layer, the MLP layer and the prediction layer. The up-sampling layer ex- pands the feature resolution to $\\frac{H}{8} \\times \\frac{W}{8}$, unifying the size of outputs extracted from the GMB of each stage. We exclude the features of the first encoder stage since they contain too much low-level information and bring high computational costs. The MLP layer then concatenates the up-sampled features. Finally, the prediction layer predicts the segmen- tation mask. The overall procedure in decoder is as follows:\n$F_i = \\text{GMB}(F_i), i \\in \\{2,3,4\\}$\n$F_{\\text{up}\\_i} = \\text{UpSample}(\\frac{H}{8} \\times \\frac{W}{8})(F_i), C_{\\text{fuse}} = \\sum_{i=2}^4 C_i$\n$F = \\text{Linear}(C_{\\text{fuse}}, C_{\\text{MLP}})(\\text{Concat}(F_{\\text{up}\\_i})),$\n$Z = \\text{Linear}(C_{\\text{MLP}}, N_{\\text{cls}})(F),$\nwhere $\\text{Linear}(a, b)(\\cdot)$ denotes a linear layer with a size of $a$ as input dimensions and a size of $b$ as output dimensions. $C_{\\text{MLP}}$ denotes the channel dimension of the MLP. $N_{\\text{cls}}$ is defined as the number of classes.\nThe proposed GMB leverages the MetaFormer block in the decoder to further enhance the global contexts of the fea- ture representations extracted by the encoder, which mainly focuses on the local context. As illustrated in Fig. 2 (b), the GMB adopts the MetaFormer block of two residual sub- blocks and employs a novel channel reduction self-attention (CRA) module as a token mixer. Our CRA module effec- tively captures global contexts of the features with efficient computational costs. The GMB is performed at each stage except the first stage (i.e., $i \\in \\{2, 3, 4\\}$). The overall opera- tion is defined as follows:\n$M_i = \\text{CRA}(\\text{LN}(F_i)) + F_i,$\n$F = \\text{MLP}(\\text{LN}(M_i)) + M_i,$\nwhere LN and MLP denote the layer normalization and the channel MLP layer, respectively."}, {"title": "3.2.1 Channel Reduction Attention", "content": "We propose the Channel Reduction Attention (CRA) module as a novel token mixer utilized in the GMB to con- sider both the globality extraction and the computational efficiency of the self-attention for the semantic segmenta- tion. Our CRA is based on the multi-head self-attention. The key and value are average pooled before the attention operation. As shown in Fig. 3, the channel dimensions of the query and key are embedded into the one dimen- sion to further reduce the computational costs. We found that the channel squeezed query $Q \\in \\mathbb{R}^{\\text{Head} \\times H_iW \\times 1}$ and key $K \\in \\mathbb{R}^{\\text{Head} \\times (H_iW_i/r_i^2) \\times 1}$ can sufficiently extract global similarities. The CRA operation is formulated as follows:\n$\\text{CRA}(F_i) = \\text{Concat}(\\text{Head}_0, ..., \\text{Head}_j)W^O,$\n$Q_i = FW^Q, K_i = \\text{AvgPool}(F_i)W^K,$\n$V_i = \\text{AvgPool}(F_i)W^V, \\text{Head}_j = \\text{Att}(Q_i, K_i, V_i),$\n$\\text{Att}(Q_i, K_i, V_i) = \\text{Softmax}(QK^T)V_i,$\nwhere $W^Q, W^K \\in \\mathbb{R}^{C_i \\times 1}, W^V \\in \\mathbb{R}^{C_i}, and W^O \\in \\mathbb{R}^{C_i \\times C_i}$ are projection parameters. $j$ denotes the number of attention heads. AvgPool is the average pooling of scale $r_i \\in \\{2, 4, 8\\}$ at each stage, respectively. Compared to SRA [30] that is a previous efficient self-attention method, the computational complexity of our CRA is as:\n$N' = \\frac{N}{r_i \\times r_i}, \\Omega(\\text{SRA}) = (N')^2 C + (N')^2 C,$\n$\\Omega(\\text{CRA}) = (N')^2 1 + (N')^2 C,$\nwhere N denotes the number of pixel tokens. In eq.(4), the left and right terms indicate the computations of the query- key operation and the computations of the attention weight- value operation, respectively. By reducing the computation of the query-key operation by C times, our CRA reduces the total computation of the attention operation by about twice."}, {"title": "3.2.2 Channel MLP", "content": "The channel MLP is used to consolidate the features pro- cessed with our token mixer. Channel MLP consists of the two 1\u00d71 convolution layers with a GELU activation layer. The operation is defined as follows:\n$\\text{MLP}(x) = \\text{Conv}_{1 \\times 1}(\\text{GELU}(\\text{Conv}_{1 \\times 1}(x))),$"}, {"title": "4. Experiment", "content": "where $\\text{Conv}_{1 \\times 1}$ denotes the 1 \u00d7 1 convolution layer.\nIn Table 1, we compared our MetaSeg performance with previ- ous state-of-the-art methods on ADE20K, Cityscapes, and COCO-Stuff datasets. This comparison includes the num- ber of the parameters, Floating Point Operations (FLOPs), and mIoU under both the single scale (SS) and multi-scale (MS) flip inference strategies. As shown in the Table 1, MetaSeg-T showed significant performance of 42.4% mIoU with only 4.7M parameters and 5.5 GFLOPs for ADE20K. Compared to SegNeXt-T that uses the same backbone [13], our MetaSeg-T achieved 1.3% higher mIoU and 16.7% lower GFLOPs on ADE20K. Moreover, our MetaSeg-T showed 0.3% and 1.0% higher mIoU with 5.2% and 16.7% lower GFLOPs on Cityscapes and COCO-Stuff, respec- tively. Our larger model, MetaSeg-B, also achieved com- petitive performance compared to previous state-of-the-art models. MetaSeg-B showed 48.5% mIoU with 12.9% less computations compared to SegNeXt-B on ADE20K. Fur- thermore, our MetaSeg-B achieved 82.7% and 45.8% mIoU with 8.9% and 12.9% less GFLOPs on Cityscapes and COCO-Stuff, respectively. These results demonstrated that our MetaSeg effectively captures the local to global con- texts by leveraging the MetaFormer architecture up to the decoder with an efficient token mixer, our CRA.\nIn Table 2, we present the speed benchmark comparisons without any additional accelerating techniques. For fair comparison, we mea- sured Frames Per Second (FPS) of a whole single image of 1536 \u00d7768 on Cityscapes using a single RTX3090 GPU. Compared to previous methods, our method achieved su- perior FPS with a higher mIoU score. This result demon- strates that a decrease in FLOPs of our method can lead to improvements in processing speed within the GPU.\nIn Table 3, we compared our MetaSeg with the previous methods on Synapse dataset using DSC (%). For a fair comparison, we utilized MetaSeg-B in the medical image segmentation task by considering the sim- ilar model size with the previous methods. As shown in Table 3, our MetaSeg-B sets the new state-of-the-art re- sult with 82.78% DSC. This result showed a 2.09% higher DSC compared to HiFormer [14]. This indicates that our MetaSeg is effective even for the medical image segmenta- tion task. Therefore, we demonstrated the high capabilities of our MetaSeg for application fields."}, {"title": "4.3. Ablation Study", "content": "### 4.1. Experimental Settings\nWe propose the Channel Reduction Attention (CRA) module as a novel token mixer utilized in the GMB to con- sider both the globality extraction and the computational efficiency of the self-attention for the semantic segmenta- tion. Our CRA is based on the multi-head self-attention. The key and value are average pooled before the attention operation. As shown in Fig. 3, the channel dimensions of the query and key are embedded into the one dimen- sion to further reduce the computational costs. We found that the channel squeezed query $Q \\in \\mathbb{R}^{\\text{Head} \\times H_iW \\times 1}$ and key $K \\in \\mathbb{R}^{\\text{Head} \\times (H_iW_i/r_i^2) \\times 1}$ can sufficiently extract global similarities. The CRA operation is formulated as follows:\n$\\text{CRA}(F_i) = \\text{Concat}(\\text{Head}_0, ..., \\text{Head}_j)W^O,$\n$Q_i = FW^Q, K_i = \\text{AvgPool}(F_i)W^K,$\n$V_i = \\text{AvgPool}(F_i)W^V, \\text{Head}_j = \\text{Att}(Q_i, K_i, V_i),$\n$\\text{Att}(Q_i, K_i, V_i) = \\text{Softmax}(QK^T)V_i,$\nImplementation details. The mmsegmentation codebase was used to train our model on 4 RTX 3090 GPUs. We used MSCAN [13] as a backbone network. Our model with MSCAN-T and MSCAN-B backbones were each named MetaSeg-T, MetaSeg-B, and our decoder was randomly ini- tialized. For semantic segmentation evaluation, we adopted the mean Intersection over Union (mIoU) for ADE20K, Cityscapes, and COCO-Stuff datasets, and the Dice Simi- larity Score (DSC) for Synapse dataset. During the train- ing, we applied the commonly used data augmentation such as random horizontal flipping, random scaling from 0.5 to 2.0 ratios and random cropping with the size of 512\u00d7512, 1024\u00d71024, and 512\u00d7512 for ADE20K, Cityscapes, and COCO-Stuff datasets, respectively. For Synapse dataset, we used random rotation and flipping for data augmenta- tion with the size of 224x224. We trained our models us- ing AdamW optimizer for 160K iterations on ADE20K and Cityscapes, 160K iterations on COCO-Stuff, and 30K iter- ations on Synapse. The batch size was 16 for ADE20K and COCO-Stuff, 8 for Cityscapes, and 24 for Synapse. The poly LR schedule with a factor of 1.0 and an initial learning rate of 6e-5 were used.\nEffectiveness of MetaSeg Decoder for Various CNN- based Backbones. In Table 4, we experimented with other CNN-based backbones to evaluate the effect of our MetaSeg decoder. In semantic segmentation, ConvNeXt [18] adopts UperNet [33] as its decoder and MobileNetV2 [22] adopts DeepLabV3 [4] as its decoder. For these CNN-based back- bones, our decoder showed competitive performance with significant computational reduction of 86% and 93.9%. This indicates that our MetaSeg decoder is an efficient and effective architecture for various CNN-based backbone by enhancing the visual representation from encoder features.\nEffectiveness of Global Meta Block. In Table 5, we verified the effectiveness of applying GMB in the de- coder. We conducted experiments on various cases of ap- plying or non-applying GMB to each Stage{2,3,4}. Fol- lowing [13], we excluded the features from the first stage of the encoder in this experiment since they contain too much low-level information which degrades the segmenta- tion performance. The results show that applying GMB to"}, {"title": "4.2. Comparison with State-of-the-Art Methods", "content": "Datasets. We conducted experiments on four publicly available datasets, ADE20K [42], Cityscapes [9], COCO- Stuff [1], and Synapse [15]. ADE20K is a challenging scene parsing dataset composed of 20,210/2,000/3,352 im- ages for training, validation, and testing with 150 semantic categories. Cityscapes is an urban driving scene dataset that contains 5,000 images finely annotated with 19 categories. Stage{2,3,4} is most effective structure compared to other cases. Especially, compared to Stage{3,4}, applying GMB to Stage{2,3,4} achieved 0.8% higher mIoU performance even though the parameters and GFLOPs are almost the same. This result indicates that capturing the global con- texts through the GMB from all features extracted by the encoder Stage{2,3,4} is effective in improving the seman- tic segmentation performance.\nEffectiveness of Global Modeling Token Mixer in De- coder. In Table 6, we conducted an experiment on apply- ing various token mixers to our proposed meta block-based decoder. Through this experiment, we verify which token mixer is the most effective and efficient structure for the de- coder when using MSCAN-T, a CNN-based backbone. The global context modeling token mixer (e.g. SRA and our CRA) showed the better mIoU performance compared to the local context modeling token mixer (e.g. pooling, depth- wise convolution and conventional convolution). This result demonstrates the importance of considering the global con- texts in the decoder when using a CNN-based backbone.\nEfficiency of Channel Reduction Attention. In Table 7, we focus on the parameter size and computational costs of our channel reduction self-attention (CRA) and the spa- tial reduction self-attention (SRA) [30] to compare which method is more efficient in terms of capturing global con- texts. SRA is a widely used self-attention method that re- duces the spatial resolution of the key-value by treating the token as a vector. In contrast, our CRA scalarizes each query and key token by reducing the channel dimension of the query and key into the one dimension. As shown in Ta- ble 7, our CRA reduces the computations of the query-key operation by a factor of C times, leading to a total computa- tion reduction for the attention operation that is about twice"}, {"title": "5. Conclusion", "content": "This paper proposed MetaSeg, a novel and powerful se- mantic segmentation network that effectively captures the local to global contexts by leveraging the MetaFormer ar- chitecture up to the decoder. Our MetaSeg showed that the capacity of the MetaFormer can be extended to the de- coder as well as the backbone. In addition, we proposed a novel attention module for efficient semantic segmentation, Channel Reduction Attention (CRA) module, which can ef- ficiently consider the globality by reducing the channel di- mension of the query and key into the one dimension for low computational costs in the self-attention operation. Ex- periments demonstrated the effectiveness and efficiency of our method on three public semantic segmentation datasets and a medical image segmentation dataset for application."}]}