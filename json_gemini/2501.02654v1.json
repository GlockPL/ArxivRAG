{"title": "Tougher Text, Smarter Models:\nRaising the Bar for Adversarial Defence Benchmarks", "authors": ["Yang Wang", "Chenghua Lin"], "abstract": "Recent advancements in natural language processing have highlighted the vulnerability of deep learning models to adversarial attacks. While various defence mechanisms have been proposed, there is a lack of comprehensive benchmarks that evaluate these defences across diverse datasets, models, and tasks. In this work, we address this gap by presenting an extensive benchmark for textual adversarial defence that significantly expands upon previous work. Our benchmark incorporates a wide range of datasets, evaluates state-of-the-art defence mechanisms, and extends the assessment to include critical tasks such as single-sentence classification, similarity and paraphrase identification, natural language inference, and commonsense reasoning. This work not only serves as a valuable resource for researchers and practitioners in the field of adversarial robustness but also identifies key areas for future research in textual adversarial defence. By establishing a new standard for benchmarking in this domain, we aim to accelerate progress towards more robust and reliable natural language processing systems.", "sections": [{"title": "Introduction", "content": "Recent advancements in natural language processing (NLP) have led to impressive performance on various tasks, but also exposed the vulnerability of deep learning models to adversarial attacks (Wang et al., 2021; Han et al., 2022; Wang et al., 2022a; Ranjan et al., 2023; Zeng et al., 2023; Goyal et al., 2023; Shayegani et al., 2023; Huang et al., 2024). While numerous defence mechanisms have been proposed to counter these threats, there is a lack of comprehensive benchmarks to evaluate their effectiveness across diverse settings.\nThe advent of adversarial training (Goodfellow et al., 2014a) has demonstrated notable success in enhancing model robustness against small adversarial perturbations in computer vision. Traditional approaches adapt the training process to minimise empirical risk based on a robustness loss, as opposed to the standard loss applied to clean input samples (Madry et al., 2018). The robustness loss refers to the standard loss applied to the worst-case (i.e. loss-maximising) adversarial example for each training sample. In the context of NLP, however, adversarial training poses unique challenges due to the discrete nature of text. Specifically, the inner maximisation step required in the min-max formulation of adversarial training becomes computationally expensive (Yoo and Qi, 2021). To address this, various methods have been proposed in the literature, ranging from augmenting the training set with adversarial examples tailored to a specific model (Si et al., 2021; Dong et al., 2021; Zhou et al., 2021a), to more sophisticated optimisations in token-embedding space for the inner maximisation step (Zhu et al., 2020; Li and Qiu, 2021; Goyal et al., 2023).\nIn parallel, other studies focus on structure-free regularisation methods for adversarial robustness. Yang et al. (2023b) argue that encouraging higher entropy (i.e. uncertainty) in model outputs can enhance adversarial robustness. They emphasise the need to understand the inherent robustness properties of models, focusing on those that are flexible, simple, and not overly specialised for specific types of text adversarial attacks, as well as the interplay between a model's confidence and robustness. Building on this idea, they highlight that entropy regularisation techniques, such as label smoothing (Szegedy et al., 2015, 2016), can implicitly contribute to adversarial robustness by addressing model overconfidence. Similarly, Raina et al. (2024) proposed training-time temperature scaling as a defence mechanism. They empirically demonstrated that highly miscalibrated models (Guo et al., 2017) interfere with an adversarial attacker's ability to find meaningful search directions due to the little sensitivity in the predicted probabilities."}, {"title": "Background", "content": "The vulnerability of deep learning models to adversarial attacks has become a significant concern in NLP. This section provides an overview of adversarial attacks and defences in NLP, with a particular focus on flexible defence methods that can be adapted to various NLP tasks."}, {"title": "Adversarial Attacks", "content": "Adversarial attacks in NLP aim to manipulate input text in ways that preserve semantic meaning but cause model misclassification. Following notation in Raina and Gales (2023) the distance between the benign sample x and the adversarial example $\\hat{x}$ can be measured via a proxy function $G(x, \\hat{x}) < \\epsilon$, where $\\epsilon$ represents the maximum imperceptibility threshold. Goyal et al. (2023) categorised these attacks based on the attacker's knowledge (white-box vs. black-box), the perturbation level (character, word, or sentence-level), and the attack goal (targeted vs. untargeted). Common attack methods include word substitution (Ren et al., 2019; Zang et al., 2020; Li et al., 2020b; Garg and Ramakrishnan, 2020; Jin et al., 2020; Maheshwary et al., 2021; Waghela et al., 2024; Lu et al., 2024), character manipulation (Gao et al., 2018; Eger et al., 2019a,b; Pruthi et al., 2019; Liu et al., 2022a; Rocamora et al., 2024), and sentence paraphrasing (Ribeiro et al., 2018; Iyyer et al., 2018; Zhao et al., 2018; Li et al., 2020a, 2021a). Many of these popular attack methods are implemented in the TextAttack library (Morris et al., 2020)."}, {"title": "Adversarial Defences", "content": "In this section, we will discuss two different types of adversarial defence methods."}, {"title": "Adversarial Training-based Methods", "content": "Numerous defence methods have been proposed to counter adversarial threats. In computer vision, adversarial training (Goodfellow et al., 2014b) minimises empirical risk from worst-case adversarial examples, but its inner maximisation step is computationally expensive for NLP models. To address this, a group of adversarial training methods like PGD (Madry et al., 2018), FreeLB (Zhu et al., 2020), and TAVAT (Li and Qiu, 2021) accelerate optimisation by identifying adversarial examples in the token-embedding space.\nDespite their efficiency, the limited success of these methods is often attributed to perturbations in the embedding space, which may not adequately represent true adversarial examples in natural language. To mitigate this issue, approaches such as ASCC (Dong et al., 2021) and DNE (Zhou et al., 2021b) proposed a more meaningful embedding perturbation space, defining it as the convex hull of word synonyms. While these methods offer improved robustness, they require pre-computation of synonyms, limiting their adaptability and effectiveness against diverse adversarial attacks. In light of these challenges, we emphasise the need for synonyms-agnostic and structure-free defence strategies, which provide broader applicability across NLP tasks. In practical scenarios, defenders should not rely on prior knowledge of the adversary's mechanisms for generating synonyms, as this can limit the robustness of the defence."}, {"title": "Regularisation-based Methods", "content": "Regularisation-based methods have emerged as a more flexible and generalisable approach to adversarial defence in NLP, particularly because they do not rely on model structures or synonym sets, making them adaptable across a wide range of tasks. Methods such as Flooding-X (Liu et al., 2022c), adversarial label smoothing (Yang et al., 2023b), and temperature scaling (Raina et al., 2024) have demonstrated notable effectiveness in enhancing adversarial robustness.\nFlooding-X (Liu et al., 2022c) aims to prevent overconfidence in model predictions by maintaining the loss around a pre-defined \u201cflood\u201d level (Ishida et al., 2020), thereby mitigating the model's susceptibility to adversarial perturbations. Label smoothing (Szegedy et al., 2016), on the other hand, modifies the training objective by softening the hard labels, distributing a small amount of probability mass across all classes, which helps in reducing the model's confidence in incorrect predictions. Yang et al. (2023b) extensively studied standard label smoothing and its adversarial variant (Ren et al., 2022), and showed that label smoothing can improve robustness to textual adversarial attacks (both black-box and white-box) and mitigate overconfident errors on adversarial examples. Additionally, Raina et al. (2024) highlighted that the extreme class confidence exhibited by miscalibrated models (Guo et al., 2017) creates an illusion of robustness (IOR). To address this, they proposed training-time temperature scaling as a defence mechanism to improve true robustness against unseen attacks. Their empirical results showed that highly miscalibrated models impede adversarial attackers by reducing sensitivity in predicted probabilities, thereby limiting the attacker's ability to identify meaningful search directions.\nTogether, these regularisation-based methods provide a synonyms-agnostic and structure-free framework for adversarial defence, making them well-suited for diverse NLP tasks without requiring prior knowledge of adversarial strategies."}, {"title": "Experiments", "content": "Experiments are carried out on six NLP datasets (statistics summarised in Table 1), including different tasks: single-sentence classification, similarity and paraphrase identification, natural language inference, and commonsense reasoning."}, {"title": "Datasets", "content": "SST2 (Socher et al., 2013) is a binary sentiment classification task where each sample consists of a single sentence from movie reviews. The objective is to predict whether a given sentence expresses positive or negative sentiment. MR (Pang and Lee, 2005) is another binary sentiment classification dataset similar to SST-2, based on movie reviews. Each sentence is labelled as expressing either positive or negative sentiment. MRPC (Dolan and Brockett, 2005) is a binary classification dataset for similarity and paraphrase identification, where the task is to determine whether two sentences in a pair are semantically equivalent. SciTail (Khot et al., 2018) is a natural language inference (NLI) dataset designed to test a model's ability to recognise entailment. SIQA (Sap et al., 2019) is a commonsense reasoning dataset where the goal is to choose the most appropriate answer from three options to questions about everyday social situations. SIQA presents a challenge in understanding social dynamics and reasoning beyond explicit facts. CSQA (Talmor et al., 2019) is another multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers.\nThese datasets cover a range of tasks, including single-sentence classification, similarity and paraphrase identification, natural language inference, and commonsense reasoning, enabling comprehensive evaluation across multiple dimensions of language understanding. Each dataset was carefully selected to ensure diversity in task complexity and linguistic phenomena, providing a robust benchmark for assessing model performance in various natural language understanding (NLU) tasks."}, {"title": "Models", "content": "We follow existing adversarial robustness literature (Raina et al., 2024; Zhao et al., 2024; Moraffah et al., 2024) and use Transformer (Vaswani, 2017) encoders, which are state-of-the-art on many NLP tasks. Specifically, we consider the base variants"}, {"title": "Adversarial Defence Approaches", "content": "We consider seven defence baselines in our benchmark: PGD (Madry et al., 2018), FreeLB (Zhu et al., 2020), TAVAT (Li and Qiu, 2021), Flooding-X (Liu et al., 2022c), standard label smoothing (SLS) and adversarial label smoothing (ALS) (Yang et al., 2023b), and training-time temperature scaling optimisation (TTSO) (Raina et al., 2024). We further create a simple variant of the baseline TTSO that uses entropy-based temperature scaling during training, named TTSO++. This approach adjusts the temperature based on the entropy of the prediction distribution. High entropy indicates that the model is uncertain, so a lower temperature can be applied to sharpen the distribution. Conversely, low entropy (high certainty) can be smoothed by applying a higher temperature. The temperature $T$ is adjusted according to the entropy $H(\u00b7)$ of the softmax distribution $p$:\n$T = T_{base} + \\alpha \\cdot H(p)$\nwhere $H(p)$ is the entropy of the softmax probabilities $p$, $T_{base}$ is the base temperature, and $\\alpha$ is a scaling factor controlling how strongly the temperature reacts to uncertainty. By adding entropy $H(p)$ to the temperature scaling formula, we introduce dynamic confidence adjustment based on the model's uncertainty. Note that Balanya et al. (2024) also proposed an entropy-based temperature scaling method, but we introduce a simpler one that does not need any learnable parameters."}, {"title": "Evaluation Metrics", "content": "We follow the conventions in the literature (Li et al., 2021b; Liu et al., 2022c; Lee et al., 2022) to evaluate our benchmark. We leverage TextFooler (Jin et al., 2020) and TextBugger (Li et al., 2018) to attack the victim models and measure the empirical performance. Both attackers are implemented using the default settings from the TextAttack library (Morris et al., 2020). While we acknowledge the advancements in attack techniques, TextAttack currently provides limited support for newer methods and only includes adversarial attack methods developed prior to 2021. Similarly, another widely-used OpenAttack (Zeng et al., 2021) library only covers adversarial attack methods up to 2020. Therefore, we focused on three well-established, general-purpose attack methods that are widely recognised for evaluating adversarial robustness (Wang et al., 2022b; Yang et al., 2023a; Zhan et al., 2023; Hu et al., 2023; Yang et al., 2023b; Lu et al., 2024; Ji et al., 2024; Zhang et al., 2024; Zhao et al., 2024).\nTo quantify the impact of each adversarial attack, we follow prior works (Li et al., 2021b; Liu et al., 2022c; Hu et al., 2023) and report the following metrics: accuracy under attack (AUA), attack success rate (ASR), and the average number of queries (AVGQ) required to successfully attack a model."}, {"title": "Implementation Details", "content": "All experiments were conducted using the HuggingFace framework (Wolf et al., 2020) to leverage pre-trained model weights. For the adversarial training-based methods, including PGD, FreeLB, and TAVAT, we used the default hyper-parameters provided by the TextDefender library (Li et al., 2021b). The default hyper-parameters for each adversarial training baseline are: adversarial iterations of 5, adversarial learning rate of 0.03, adversarial initialisation magnitude of 0.05, adversarial maximum norm of 1, adversarial norm type of $l_2$. For experiments involving SLS and ALS, we performed a hyper-parameter search for the label smoothing coefficient from the set {0.1, 0.2, 0.3, 0.4, 0.5}. In experiments involving TTSO and TTSO++, we applied the same high temperature T = 10 to every instance and scaling factor $\\alpha$ = 0.5 by default. The learning rate was optimised by selecting the model that achieved the highest validation accuracy after fine-tuning for four epochs, with candidate values for the learning rate drawn from the set {1e \u2212 5, 2\u0435 \u2212 5, 5e \u2212 5}. For commonsense reasoning datasets, we follow Branco et al. (2021) to fine-tune the pre-trained model, converting the multiple-choice task into a sequence-ranking problem, as outlined in Liu et al. (2019a). We process the elements of input pairs separately, generating a score for each, with the maximum score corresponding to the selected answer. All experiments were executed on a single NVIDIA RTX 4090 GPU with 24GB of memory."}, {"title": "Results", "content": "Table 3 presents the experimental results trained with various defence methods. Notably, TTSO and TTSO++ consistently outperform other baselines, achieving superior AUA across diverse attacks (TextFooler and TextBugger) and model architectures (BERT, ROBERTa, and DeBERTa). This robustness can be attributed to their ability to counteract the Illusion of Robustness (IOR) by addressing model miscalibration (Raina et al., 2024), a key factor behind overconfidence in adversarial scenarios. Unlike token-level embedding perturbation techniques such as PGD, FreeLB, and TAVAT, which often lead to overfitting specific attack patterns without enhancing overall model uncertainty, TTSO and TTSO++ effectively recalibrate model confidence by softening predictions, setting a new benchmark for adversarial defence strategies.\nIn comparison, methods like SLS and ALS emerge as flexible and lightweight alternatives to adversarial training-based methods. While approaches such as PGD, FreeLB, or TAVAT require computationally expensive inner maximisation steps during training and sometimes degrade performance under adversarial conditions, SLS and ALS offer significant improvements in adversarial robustness with minimal additional complexity. As shown in Table 3, Flooding-X consistently underperforms compared to other baselines. This poor performance aligns with the findings of Zhu and Rao (2023), who found flooding techniques ineffective for adversarial robustness. By maintaining the loss above a threshold, we argue that Flooding-X will hinder the model's ability to minimise adversarial loss and learn intricate decision boundaries. Its non-targeted regularisation treats all examples uniformly, lacking the specificity needed to counter adversarial attacks. While aimed at improving generalisation, Flooding-X appears to compromise the nuanced feature learning required for robust adversarial performance."}, {"title": "Evaluate on Embedding-based Model", "content": "While BERT, ROBERTa, and DeBERTa are the most commonly used encoder-based models in prior studies (Raina et al., 2024; Zhao et al., 2024; Moraffah et al., 2024), we extend this evaluation by assessing adversarial robustness using a more recent state-of-the-art embedding-based model BGE-M3 (Chen et al., 2024). Results are summarised in Table 4. TTSO++ consistently achieves superior robustness performance, excelling in all metrics across all datasets and attack types."}, {"title": "Robustness in Commonsense Reasoning", "content": "Table 5 highlights the adversarial robustness performance of all baseline defence methods on commonsense reasoning tasks using RoBERTa-base. TTSO++ achieves the best overall performance, with the highest AUA and lowest ASR across both datasets, demonstrating its strong defence capabilities. Flooding-X, however, consistently underperforms, reaffirming its limitations in adversarial settings. Notably, token-level embedding perturbation methods such as PGD, FreeLB, and TAVAT exhibit marginal improvements over the baseline but fail to achieve robustness comparable to TTSO++."}, {"title": "Discussion", "content": "From Table 3, we observe that TTSO++ consistently outperforms TTSO across datasets and models in terms of all evaluation metrics. A key factor in this improvement lies in the nuanced difference between the temperature-scaling mechanisms of TTSO and TTSO++. TTSO applies a uniform temperature (Tbase = 10) to all instances during training, ensuring equal smoothing of logits across the dataset. While this strategy offers simplicity and improves model calibration, it is inherently limited. A fixed temperature does not account for variations in the difficulty of individual examples. For easy-to-classify examples (where the model is naturally confident), applying a slightly higher temperature can unnecessarily dampen the predictions, leading to a loss of useful certainty. Conversely, for hard-to-classify examples (where the model should be uncertain) or adversarial instances, applying a fixed high temperature may not be enough to capture the complexity of the example, leading to insufficient adjustment of the logits. In contrast, TTSO++ incorporates entropy-based temperature scaling, where the temperature is dynamically adjusted for each input instance based on the model's certainty. This approach leverages entropy as a proxy for uncertainty. Higher entropy (low certainty) leads to a higher temperature, while lower entropy (high certainty) results in a lower temperature. This adaptive mechanism allows TTSO++ to tailor the level of smoothing to the specific demands of each input, striking a better balance between preserving confidence for easy examples and enhancing robustness for challenging ones. As a result, TTSO++ achieves superior performance, where the ability to dynamically handle uncertain inputs is critical.\nThe effectiveness of TTSO++ is particularly evident in commonsense reasoning tasks like SIQA and CSQA (Table 5). Here, TTSO++ demonstrates the highest AUA and lowest ASR across all models and datasets. The instance-wise temperature scaling provides the model with the flexibility to adapt to diverse question-answering scenarios, effectively mitigating the impact of adversarial attacks. TTSO++ sets a new benchmark, offering superior adversarial robustness and generalisability across datasets and tasks."}, {"title": "Dynamic Confidence Adjustment", "content": "From Table 3, we observe that TTSO++ consistently outperforms TTSO across datasets and models in terms of all evaluation metrics. A key factor in this improvement lies in the nuanced difference between the temperature-scaling mechanisms of TTSO and TTSO++. TTSO applies a uniform temperature (Tbase = 10) to all instances during training, ensuring equal smoothing of logits across the dataset. While this strategy offers simplicity and improves model calibration, it is inherently limited. A fixed temperature does not account for variations in the difficulty of individual examples. For easy-to-classify examples (where the model is naturally confident), applying a slightly higher temperature can unnecessarily dampen the predictions, leading to a loss of useful certainty. Conversely, for hard-to-classify examples (where the model should be uncertain) or adversarial instances, applying a fixed high temperature may not be enough to capture the complexity of the example, leading to insufficient adjustment of the logits. In contrast, TTSO++ incorporates entropy-based temperature scaling, where the temperature is dynamically adjusted for each input instance based on the model's certainty. This approach leverages entropy as a proxy for uncertainty. Higher entropy (low certainty) leads to a higher temperature, while lower entropy (high certainty) results in a lower temperature. This adaptive mechanism allows TTSO++ to tailor the level of smoothing to the specific demands of each input, striking a better balance between preserving confidence for easy examples and enhancing robustness for challenging ones. As a result, TTSO++ achieves superior performance, where the ability to dynamically handle uncertain inputs is critical."}, {"title": "High Temperature Training", "content": "While tuning Tbase could potentially enhance performance against adversarial attacks, we opted for a fixed temperature to ensure consistency and simplicity in our experimental setup. The choice of 10 as the fixed temperature was empirically validated across a range of NLP tasks and demonstrated robust performance across clean and adversarial examples. By using a fixed temperature, we reduce the need for extensive hyper-parameter tuning, which can introduce additional computational overhead and potential overfitting to specific datasets or adversarial attacks.Figure 1 presents the changes in before- and after-attack accuracy of a model trained with the standard objective and various base temperatures (Tbase, as described in \u00a73.3) during training. While similar trends were observed across all models, we present the results specifically for RoBERTa-base in this section. The results indicate that higher temperatures during training generally enhance robustness against adversarial attacks. To quantify this, we use the average performance drop rate (APDR) (Zhu et al., 2023), which averages the performance drop rate\n$PDR = 1 - \\frac{\\sum_{(x,y) \\in D} M[f_{\\theta}(A(x)), y]}{\\sum_{(x,y) \\in D} M[f_{\\theta}(x), y]}$\nacross different adversarial attacks, where A is the adversarial attack applied to input text x, M[\u00b7] is the evaluation function, and f\u03b8(\u00b7) is the network. For classification task, M[\u00b7] is the indicator function 1 [\u0177, y] which equals to 1 when \u0177 = y, and 0 otherwise. Notably, on SST2 dataset, we observe a slight increase in APDR when the training temperature is set to 20. This suggests that excessively high temperatures may overly smooth the predicted probability distribution, making it harder for the model to effectively learn from the training data."}, {"title": "Runtime Analysis", "content": "Table 6 presents the runtime comparison of training the RoBERTa-base model on the SST2, MRPC, and SIQA datasets using different adversarial defence methods. The baseline model (no defence) has a normalised runtime of 1 across all datasets, serving as the standard for comparison."}, {"title": "Conclusion", "content": "In this work, we investigated adversarial defence techniques that are broadly applicable across diverse NLP tasks, focusing on synonym-agnostic and structure-free approaches. By establishing a comprehensive benchmark, we evaluated state-of-the-art adversarial defence strategies developed prior to 2024, extending the evaluation beyond traditional text classification to encompass single-sentence classification, similarity and paraphrase identification, natural language inference, and commonsense reasoning tasks.\nOur systematic exploration of regularisation-based methods revealed valuable insights into their potential for textual adversarial defence. Based on these findings, we proposed TTSO++, a simple yet effective variant of temperature scaling that leverages entropy-based adjustments during training. TTSO++ achieves state-of-the-art robustness under adversarial attacks while maintaining strong performance on clean examples. Its minimal computational overhead makes it highly practical for real-world applications. By extending adversarial evaluation to a broader spectrum of NLP tasks, we aim to inspire the development of more flexible, generalisable, and efficient defence mechanisms. We believe this study provides a robust foundation for future research, bridging the gap between task-specific defences and universally applicable solutions for adversarial robustness in NLP."}, {"title": "Limitations", "content": "Our study presents empirical results using state-of-the-art encoder-based Transformer models, which are widely regarded as the most appropriate for classification-based NLP tasks (Raina et al., 2024; Zhao et al., 2024). However, the rapidly growing field of LLMs opens new avenues for exploration. Future work could examine the susceptibility of decoder-based LLMs to adversarial attacks and evaluate the performance of the defence methods discussed in this paper in such settings. Additionally, while our research focuses on defence methods that can be uniformly applied across all benchmark datasets, it remains unexplored whether more specialised techniques, such as contrastive-based methods (Pan et al., 2022; he et al., 2023) or prompt-based methods (Xu and Wang, 2024; Yang et al., 2024), could be adapted to provide universal adversarial defences. Investigating these methods' applicability to a broader range of tasks could further enhance the scope of adversarial robustness research. Finally, we proposed TTSO++ as an improvement over the fixed-temperature TTSO method by introducing entropy-based temperature scaling. While TTSO++ demonstrates significant advancements, further optimisation of temperature scaling strategies could yield additional improvements. For example, dynamically adjusting the temperature based on training progression (e.g., curriculum-based or confidence-based scaling) may better align with the evolving complexity of the task during training. Future research could explore these methods to develop more adaptive and effective defences."}]}