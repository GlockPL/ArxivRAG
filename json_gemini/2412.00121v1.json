{"title": "Hybrid Discriminative Attribute-Object Embedding Network for Compositional Zero-Shot Learning", "authors": ["Yang Liu", "Xinshuo Wang", "Jiale Du", "Xinbo Gao", "Jungong Han"], "abstract": "Compositional Zero-Shot Learning (CZSL) recognizes new combinations by learning from known attribute-object pairs. However, the main challenge of this task lies in the complex interactions between attributes and object visual representations, which lead to significant differences in images. In addition, the long-tail label distribution in the real world makes the recognition task more complicated. To address these problems, we propose a novel method, named Hybrid Discriminative Attribute-Object Embedding (HDA-OE) network. To increase the variability of training data, HDA-OE introduces an attribute-driven data synthesis (ADDS) module. ADDS generates new samples with diverse attribute labels by combining multiple attributes of the same object. By expanding the attribute space in the dataset, the model is encouraged to learn and distinguish subtle differences between attributes. To further improve the discriminative ability of the model, HDA-OE introduces the subclass-driven discriminative embedding (SDDE) module, which enhances the subclass discriminative ability of the encoding by embedding subclass information in a fine-grained manner, helping to capture the complex dependencies between attributes and object visual features. The proposed model has been evaluated on three benchmark datasets, and the results verify its effectiveness and reliability.", "sections": [{"title": "1. Introduction", "content": "Humans can easily recognize new combinations of objects and attributes, like imagining a blue horse, by reasoning about different object aspects and generalizing knowledge to unseen combinations. In Compositional Zero-Shot Learning (CZSL) [23, 28, 33, 41], the goal is to predict unseen combinations of objects and attributes after learning from known classes and their descriptions. For instance, after learning about \"Red Goldfish\" and \"Yellow Koi\", the model can recognize a \"Red Koi\". This task is challenging due to variations in shapes, colors, and textures across different attribute-object combinations. Traditional methods [28-31] treat each attribute-object combination independently and classify each pair as a distinct category, disregarding the relationships between combinations. For example, Misra et al. [29] focus on distinguishing unrelated pairs but struggle with variability within a class, while Nagarajan et al. [31] separate attribute and object features, overlooking their interactions. These methods fail to capture subtle subclass distinctions and perform poorly on CZSL datasets. To address these issues, we introduce the Subclass"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Zero-Shot Learning", "content": "Zero-shot learning (ZSL) classifies objects in unseen categories by transferring knowledge from seen categories, using semantic information like attributes, text descriptions, or word embeddings, without relying solely on visual data [22, 43]. This approach is highly adaptable in recognizing new categories [42]. The first is embedding-based methods, such as Zhang et al. [46], which emphasize embedding spaces that ensure intra-class cohesion and inter-class separation. Other studies, such as Bi-VAEGAN [41], explored alternative embedding spaces to effectively connect visible and invisible elements. Techniques such as second-order pooling [15] and prototype learning [45] further refine image representations, while generative methods such as conditional VAEs decompose images into semantically meaningful components [16, 19]. In addition, graph convolutional networks (GCNs) [10, 13, 40] show good promise by using knowledge graphs to predict unseen categories and making improvements to alleviate issues such as Laplacian smoothing [10, 17]. Together, these approaches emphasize the importance of semantic feature integration, making ZSL a powerful tool for combinational and zero-shot learning tasks."}, {"title": "2.2. Compositional Zero-Shot Learning", "content": "Compositional Zero-Shot Learning (CZSL) [1, 9, 24, 29, 31] focuses on identifying unseen combinations of states and objects by examining various aspects of sample combinations. Existing methods fall into two categories: the first maps inputs to combination space for classification, using two classifiers to independently recognize object and state class prototypes. For instance, Chen et al. [3] proposed a tensor decomposition method to infer unseen object-state pairs using sparse class-specific SVM classifiers trained on visible components. Nagarajan et al. [31] suggested that the transformation of object features within the combination is a linear function of state features. Atzmon et al. [1] proposed a discriminative model to ensure conditional independence between state and object recognition. However, due to significant visual deviations between objects and states, these methods often struggle in practical applications. The second category focuses on learning a joint representation of state-object combinations for classification. Recently, Naeem et al. [30] introduced a GCN-based model to capture dependencies between objects and states, addressing CZSL challenges. SymNet [24] utilized the symmetric relationship between states and objects to filter out impossible combinations and improve prototype quality. A contrastive learning method [20] enhanced generalization for new combinations by isolating class prototypes, while Khan et al. [5] employed self-attention to capture component interdependencies, refining label embeddings for better differentiation."}, {"title": "2.3. Overcoming Training Data Limitations", "content": "To further improve model performance, many methods [9, 25, 49] focus on training data and address issues such as sample imbalance and data mixing between classes. For example, Redmon et al. [35] divided labels into levels based on their structural links, then augments the data at each level to preserve balance across categories. Zhou et al. [49] proposed an active incremental learning method to encourage the model to prioritize learning more difficult classes, thus mitigating the impact of simpler sample domains. Lin et al. [25] combined the difficulty of each sample with the objective function to adaptively assess sample difficulty during each iteration. Jiang et al. [9] evaluated the visual bias of two components to assess their imbalance and reweights the training process of CZSL using this imbalance information. Different from traditional methods, we construct a new dataset that is related to the original one but has certain differences. These two datasets are then combined to form the database required for our model training. This strategy not only introduces greater diversity into the training data but also exposes the model to a wider variety of images and attribute combinations during training."}, {"title": "3. Approach", "content": "The overall architecture is illustrated in Figure 2. We begin with the database construction, where a new hybrid database and its embeddings are generated by combining multiple datasets. Next, we present the feature extraction encoding, which decomposes encoded visual features using a traditional disentanglement framework. Following this, we elaborate on the implementation of the embedding expert module, which uses contrastive learning to align the generated virtual encoding with the original embedding, producing a virtual embedding with improved subclass discrimination."}, {"title": "3.1. Problem Definition", "content": "Let the set of possible attributes in the dataset be $A = \\{a_1, a_2,..., a_m\\}$, and the set of possible objects be $O = \\{o_1, o_2, ..., o_n\\}$. By combining attributes and objects, we can form all possible attribute-object pairs, creating a set $Y = A \\times O$, and the total number of compositions can be calculated as $|Y| = m \\cdot n$. In the CZSL setting, the set Y should be split into two disjoint parts, the visible component set $Y_s$ and the invisible component set $Y_u$, where $Y_s + Y_u \\leq Y$. During model training, we utilize samples from the visible class $Y_s$, denoted as $D_{tr} = \\{(X, Y)\\}$. Assume that $X$ is the set of images corresponding to Y, and $X_s$ corresponds to $Y_s$. For testing, we define two setups based on the range of the output label space: CW-CZSL (Closed-world Compositional Zero-shot Learning) and OW-CZSL (Open-world Compositional Zero-shot Learning). In CW-CZSL, the test set $D_t = \\{(X, Y)\\}$ comprises samples from the visible class $Y_s$, and all samples from the invisible class $Y_u$, $Y_t = Y_u \\cup Y_{st}$, where $Y_{st}$ belongs to $Y_s$. In OW-CZSL, the output space extends to all potential attribute-object pairs, i.e., $Y_t = Y$. This setup enables evaluating the model's ability to generalize to unseen categories, thereby enhancing its performance in real-world applications."}, {"title": "3.2. Baseline Framework", "content": "When presented with an input image x", "f_o$": "n\n$f_o = Norm(E_o(f_{cls})), f_a = Norm(E_a(f_{cls})),$"}]}