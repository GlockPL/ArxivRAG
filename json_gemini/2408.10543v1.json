{"title": "Diff-PCC: Diffusion-based Neural Compression for 3D\nPoint Clouds", "authors": ["Kai Liu", "Kang You", "Pan Gao"], "abstract": "Stable diffusion networks have emerged as a groundbreaking development for\ntheir ability to produce realistic and detailed visual content. This characteristic\nrenders them ideal decoders, capable of producing high-quality and aesthetically\npleasing reconstructions. In this paper, we introduce the first diffusion-based point\ncloud compression method, dubbed Diff-PCC, to leverage the expressive power of\nthe diffusion model for generative and aesthetically superior decoding. Different\nfrom the conventional autoencoder fashion, a dual-space latent representation\nis devised in this paper, in which a compressor composed of two independent\nencoding backbones is considered to extract expressive shape latents from distinct\nlatent spaces. At the decoding side, a diffusion-based generator is devised to\nproduce high-quality reconstructions by considering the shape latents as guidance\nto stochastically denoise the noisy point clouds. Experiments demonstrate that the\nproposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711\ndB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while\nattaining superior subjective quality. Source code will be made publicly available.", "sections": [{"title": "1 Introduction", "content": "Point clouds, composed of numerous discrete points with coordinates (x, y, z) and optional attributes,\noffer a flexible representation of diverse 3D shapes and are extensively applied in various fields such\nas autonomous driving [8], game rendering [36], robotics [7], and others. With the rapid advancement\nof point cloud acquisition technologies and 3D applications, effective point cloud compression\ntechniques have become indispensable to reduce transmission and storage costs."}, {"title": "1.1 Background", "content": "Prior to the widespread adoption of deep learning techniques, the most prominent traditional point\ncloud compression methods were the G-PCC [40] and V-PCC [41] proposed by the Moving Picture\nExperts Group(MPEG). G-PCC compresses point clouds by converting them into a compact tree\nstructure, whereas V-PCC projects point clouds onto a 2D plane for compression. In recent years,\nnumerous deep learning-based methods have been proposed [51, 46, 11, 12, 7, 31, 47, 14, 43],\nwhich primarily employ the Variational Autoencoder (VAE) [1, 2] architecture. By learning a prior\ndistribution of the data, the VAE projects the original input into a higher-dimensional latent space,\nand reconstructs the latent representation effectively using a posterior distribution. However, previous\nVAE-based point cloud compression architectures still face recognized limitations: 1) Assuming a\nsingle Gaussian distribution $N (\\mu, \\sigma^2)$ in the latent space may prove inadequate to capture the intricate"}, {"title": "1.2 Our Approach", "content": "Building on the preceding discussion, we introduce Diff-PCC, a novel lossy point cloud compression\nframework that leverages diffusion models to achieve superior rate-distortion performance with\nexceptional reconstruction quality. Specifically, to enhance the representation ability of simplistic\nGaussian priors in VAEs, this paper devises a dual-space latent representation that employs two\nindependent encoding backbones to extract complementary shape latents from distinct latent spaces.\nAt the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by\nconsidering the shape latents as guidance to stochastically denoise the noisy point clouds. Experiments\ndemonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g.,\n7.711 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining\nsuperior subjective quality."}, {"title": "1.3 Contribution", "content": "Main contributions of this paper are summarized as follows:\n\u2022 We propose Diff-PCC, a novel diffusion-based lossy point cloud compression framework.\nTo the best of our knowledge, this study presents the first exploration of diffusion-based\nneural compression for 3D point clouds.\n\u2022 We introduce a dual-space latent representation to enhance the representation ability of the\nconventional Gaussian priors in VAEs, enabling the Diff-PCC to extract expressive shape\nlatents and facilitate the following diffusion-based decoding process.\n\u2022 We devise an effective diffusion-based generator to produce high-quality noises by consider-\ning the shape latents as guidance to stochastically denoise the noisy point clouds."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Point Cloud Compression", "content": "Classic point cloud compression standards, such as G-PCC, employ octree[30] to compress point\ncloud geometric information. In recent years, inspired by deep learning methods in point cloud"}, {"title": "2.2 Diffusion Models for Point Cloud", "content": "Recently, diffusion models have ignited the image generation field[59, 17, 33], inspiring researchers\nto explore their potential in point cloud applications. DPM[20] pioneered the introduction of diffusion\nmodels in this domain. Starting from DPM, PVD[58] combines the strengths of point cloud and\nvoxel representations, establishing a baseline based on PVCNN. LION[48] employs two diffusion\nmodels to separately learn shape representations in latent space and point representations in 3D\nspace. Dit-3D[23] innovates by integrating transformers into DDPM, directly operating on voxelized\npoint clouds during the denoising process. PDR[21] employs diffusion model twice during the\nprocess of generating coarse point clouds and refined point clouds. Point-E[25] utilizes three diffusion\nmodels for the following processes: text-to-image generation, image-to-point cloud generation, and\npoint cloud upsampling. PointInfinity[13] utilizes cross-attention mechanism to decouple fixed-size\nshape latent and variable-size position latent, enabling the model to train on low-resolution point\nclouds while generating high-resolution point clouds during inference. DiffComplete[4] enhances\ncontrol over the denoising process by incorporating ControlNet[54], achieving new state-of-the-art\nperformances. These advancements demonstrate the promise of DMs in point cloud generation tasks,\nwhich motivates our exploring its applicability in point cloud compression. Our research objective is\nto explore the effective utilization of diffusion models for point cloud compression while preserving\nits critical structural features."}, {"title": "3 Method", "content": "Figure 1 illustrates the pipeline of the proposed Diff-PCC, which can also represent the general work-\nflow of diffusion-based neural compression. A concise review for Denoising Diffusion Probabilistic\nModels (DDPMs) and Neural Network (NN) based point cloud compression is first provided in\nSec. 3.1; The proposed Diff-PCC is detailed in Sec. 3.2."}, {"title": "3.1 Preliminaries", "content": "Denoising Diffusion Probabilistic Models (DDPMs) comprise two Markov chains of length T:\ndiffusion process and denoising process. Diffusion process adds noise to clean data xo, resulting in\na series of noisy samples {$x_1,x_2\u2026$}. When T is large enough, $x_T \\sim N(0, I)$. The denoising\nprocess is the reverse process, gradually removing the noise added during the diffusion process. We\nformulate them as follows:\n$q(x_1,\\dots, x_T|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1})$, where $q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$\n$P_\\theta(x_0,\\dots,x_{T-1}|x_T) = \\prod_{t=1}^T P_\\theta(x_{t-1}|x_{t})$, where $p_\\theta(x_{t-1}|x_{t}) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2I)$"}, {"title": "3.2 DIFF-PCC", "content": ""}, {"title": "3.2.1 Overview", "content": "As shown in Fig. 2, two key components, i.e., compressor and generator, are respectively utilized\nin the diffusion process and denoising process. In Diff-PCC, the diffusion process is identified as\nthe encoding, in which a compressor extracts latents from the point cloud and compresses latents\ninto bitstreams; at the decoding side, the generator accepts the latents as a condition and gradually\nrestoring point cloud shape from noisy samples."}, {"title": "3.2.2 Dual-Space Latent Encoding", "content": "Several research have demonstrated that a simplistic Gaussian distribution in the latent space may\nprove inadequate to capture the complex visual signals [57, 3, 6, 10]. Although previous works have\nproposed to solve these problems using different technologies such as non-gaussian prior [15] or\ncoupling between the prior and the data distribution [10], these techniques may not be able to directly\nemployed on neural compression tasks."}, {"title": "3.2.3 Diffusion-based Generator", "content": "The generator takes noisy point cloud xt at time t and necessary conditional information C as input.\nWe hope generator to learn positional distribution F of xt and fully integrate F with C to predict\nnoise et at time t. In this paper, we consider all information that could potentially guide the generator\nas conditional information, including time t, class label l, noise coefficient $\u03b2_t$, and decoded latent\nfeatures (y\u0131 and \u00ffh).\nDiffComplete [4] uses ControlNet [55] to achieve refined noise generation. However, the denoiser of\nDiffComplete is a 3D-Unet, adapted from its 2D version [16]. This structure is not suitable for our\nmethod, because we directly deal with points, instead of voxels. We embraced this idea and specially\ndesigned a hierarchical feature fusion mechanism to adapt to our method. Note that 3D-Unet can\ndirectly downsample features F through 3D convolution with a stride greater than one. It is very\ncomplex for point-based methods to achieve equivalent processing. Therefore, we did not replicate\nthe same structure as DiffComplete does, but directly used AdaLN to inject conditional information,\nformulated as:\n$AdaLN (F_{in}, C) = Norm(F_{in})Linear(C) + Linear(C)$\nwhere Fin denotes the original features in the Generator and C denotes the condition information.\nNow we detail the structure: First, we need to exact the shape latent of noise point cloud xt and we\nchoose PointNet for structural consistency. However, in the early stages of the denoising process,\nxt lacks a regular surface shape for the generator to learn. Therefore, we adopt the suggestion from\nPDR [23], adding positional encoding to each noise point so that the generator can understand the"}, {"title": "3.2.4 Training Objective", "content": "We follow the conventional rate-distortion trade-off as our loss function as follows:\n$L = D + \\lambda R$\nwhere D refers to the evaluated distortion; R represents bitrate as shown in Eq. 10; \u03bb serves as the\nbalance the distortion and bitrate. Specifically, a combined form of distortion D is used in this paper,\nwhich considers both intermediate noises ($(\u20ac,\\hat{\u20ac})$) and global shapes ($(x_0,\\hat{x_0})$):\n$D = D_{MSE}(\u20ac,\\hat{\u20ac}) + \\gamma D_{CD}(x_0, \\hat{x_0})$\nwhere $D_{MSE}$ denotes the Mean Squared Error (MSE) distance; $D_{CD}$ refers to the Chamfer Distance;\ny means the weighting factor. Here, the overall point cloud shape is additively supervised under the\nChamfer Distance $D_{CD}(x_0, \\hat{x_0})$ to provide a global optimization. The following function is utilized\nto predict the reconstructed point cloud $x_0$ in practice:\n$\\hat{x_0} = \\frac{1}{\\sqrt{\u03b1_t}} (x_t - \\sqrt{1 - \\overline{\u03b1_t}} \\epsilon_\\theta(x_t, t,c))$\nwhere at means the noise level; xt refers to the noisy point cloud at time step t; e denotes the\npredicted noise from the generator; c represent the conditional information we inject into the generator."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets Based on previous work, we used ShapeNet as our training set, sourced from [20]. This\ndataset contains 51,127 point clouds, across 55 categories, which we allocated in an 8:1:1 ratio for\ntraining, validation, and testing. Each point cloud has 15K points, and following the suggestions from\n[29], we randomly select 2K points from each for training. Additionally, we also used ModelNet10\nand ModelNet40 as our test sets, sourced from [44]. These datasets contain 10 categories and\n40 categories respectively, totaling 10,582 point clouds. During training and testing, we perform\nindividual normalization on the shape of each point cloud.\nBaselines & Metric We compare our method with the state-of-the-art non-learning-based method:\nG-PCC, and the latest learning-based methods from the past two years: IPDAE, PCT-PCC, Following"}, {"title": "4.2 Baseline Comparisons", "content": ""}, {"title": "Objective Quality Comparison", "content": "Table 1 shows the quantitative indicators using BD-Rate and BD-PSNR, and Fig. 3 demonstrates the\nrate-distortion curves of different methods. It can be seen that, under identical reconstruction quality\nconditions, our method achieves superior rate-distortion performance, conserving between 56% to\n99% of the bitstream compared to G-PCC. At the most minimal bit rates, point ot point PSNR of our\nproposed method surpasses that of G-PCC by 7.711 dB."}, {"title": "Subjective Quality Comparison", "content": "Fig 4 presents the ground truth and decoded point clouds from different methods. We choose three\npoint cloud:airplane, chair,and mug. to be tested across a comparable bits per pixel (bpp) range.\nThe comparative analysis reveals that at the lowest code rate, our method preserves the ground\ntruth's shape information to the greatest extent while simultaneously achieving the highest Peak\nSignal-to-Noise Ratio (PSNR)."}, {"title": "4.3 Ablation Studies", "content": "We conduct ablation studies to examine the impact of key components in the model. Specifically,\nwe investigate the effectiveness of low-frequency features, high-frequency features, and the loss\nfunction designed in Sec. 3.2.4. As shown in Table 2, utilizing solely low-frequency features to\nguide the reconstruction of the diffusion model results in a 20% reduction in the code rate, along\nwith a decrease in the reconstruction quality by 0.397dB. This indicates that high-frequency features\nplay an effective role in guiding the model during the reconstruction process. Conversely, discarding\nthe low-frequency features, which represent the shape of the point cloud, leads to a reduction in\nthe code rate and significantly diminishes the reconstruction quality. Therefore, we argue that the\nloss of the shape variable is not worth it. Lastly, we ascertain the impact of DCD(x0, 10), and the\nresults indicate that this loss marginally increases the bits per point (bpp) while diminishing the\nreconstruction quality."}, {"title": "5 Limitations", "content": "Although our method has achieved advanced rate distortion performance and excellent visual re-\nconstruction results, there are several limitations that warrant discussion. Firstly, the encoding and\ndecoding time are relatively long, which could potentially be improved by the acceleration techniques\nemployed in several explorations [18, 19]. Secondly, the model is currently limited to compressing"}, {"title": "6 Conclusion", "content": "We propose a diffusion-based point cloud compression method, dubbed Diff-PCC, to leverage the\nexpressive power of the diffusion model for generative and aesthetically superior decoding. We\nintroduce a dual-space latent representation to enhance the representation ability of the conventional\nGaussian priors in VAEs, enabling the Diff-PCC to extract expressive shape latents and facilitate\nthe following diffusion-based decoding process. At the decoding side, an effective diffusion-based\ngenerator produces high-quality reconstructions by considering the shape latents as guidance to\nstochastically denoise the noisy point clouds. The proposed method achieves state-of-the-art com-\npression performance while attaining superior subjective quality. Future works may include reducing\nthe coding complexity and extending to large-scale point cloud instances."}]}