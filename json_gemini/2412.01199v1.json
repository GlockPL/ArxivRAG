{"title": "TinyFusion: Diffusion Transformers Learned Shallow", "authors": ["Gongfan Fang", "Kunjun Li", "Xinyin Ma", "Xinchao Wang"], "abstract": "Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2\u00d7 speedup with an FID score of 2.86, outperforming competitors with comparable efficiency.", "sections": [{"title": "1. Introduction", "content": "Diffusion Transformers have emerged as a cornerstone architecture for generative tasks, achieving notable success in areas such as image [11, 26, 40] and video synthesis [25, 59]. This success has also led to the widespread availability of high-quality pre-trained models on the Internet, greatly accelerating and supporting the development of various downstream applications [5, 16, 53, 55]. However, pre-trained diffusion transformers usually come with considerable inference costs due to the huge parameter scale, which poses significant challenges for deployment. To resolve this problem, there has been growing interest from both the research community and industry in developing lightweight models [12, 23, 32, 58].\nThe efficiency of diffusion models is typically influenced by various factors, including the number of sampling steps [33, 43, 45, 46], operator design [7, 48, 52], computational precision [19, 30, 44], network width [3, 12] and depth [6, 23, 36]. In this work, we focus on model compression through depth pruning [36, 54], which removes entire layers from the network to reduce the latency. Depth pruning offers a significant advantage in practice: it can achieve a linear acceleration ratio relative to the compression rate on both parallel and non-parallel devices. For example, as will be demonstrated in this work, while 50% width pruning [12] only yields a 1.6x speedup, pruning 50% of the layers results in a 2x speedup. This makes depth pruning a flexible and practical method for model compression.\nThis work follows a standard depth pruning framework: unimportant layers are first removed, and the pruned model is then fine-tuned for performance recovery. In the literature, depth pruning techniques designed for diffusion transformers or general transformers primarily focus on heuristic approaches, such as carefully designed importance scores [6, 36] or manually configured pruning"}, {"title": "2. Related Works", "content": "Network Pruning and Depth Reduction. Network pruning is a widely used approach for compressing pre-trained diffusion models by eliminating redundant parameters [3, 12, 31, 51]. Diff-Pruning [12] introduces a gradient-based technique to streamline the width of UNet, followed by a simple fine-tuning to recover the performance. SparseDM [51] applies sparsity to pre-trained diffusion models via the Straight-Through Estimator (STE) [2], achieving a 50% reduction in MACs with only a 1.22 increase in FID on average. While width pruning and sparsity help reduce memory overhead, they often offer limited speed improvements, especially on parallel devices like GPUs. Consequently, depth reduction has gained significant attention in the past few years, as removing entire layers enables better speedup proportional to the pruning ratio [24, 27, 28, 36, 54, 56, 58]. Adaptive depth reduction techniques, such as MoD [41] and depth-aware transformers [10], have also been proposed. Despite these advances, most existing methods are still based on empirical or heuristic strategies, such as carefully designed importance criteria [36, 54], sensitivity analyses [18] or manually designed schemes [23], which often do not yield strong performance guarantee after fine-tuning.\nEfficient Diffusion Transformers. Developing efficient diffusion transformers has become an appealing focus within the community, where significant efforts have been made to enhance efficiency from various perspectives, including linear attention mechanisms [15, 48, 52], compact architectures [50], non-autoregressive transformers [4, 14, 38, 49], pruning [12, 23], quantization [19, 30, 44], feature"}, {"title": "3. Method", "content": "3.1. Shallow Generative Transformers by Pruning\nThis work aims to derive a shallow diffusion transformer by pruning a pre-trained model. For simplicity, all vectors in this paper are column vectors. Consider a $L$-layer transformer, parameterized by ${\\Phi = [\\Phi_1,\\Phi_2,\\dots, \\Phi_L]^T \\in \\mathbb{R}^{L \\times D}}$, where each element $\\Phi_i$ encompasses all learnable parameters of a transformer layer as a $D$-dim column vector, which includes the weights of both attention layers and MLPs. Depth pruning seeks to find a binary layer mask $M_{L \\times 1} = [m_1, m_2,\\dots, m_L]^T$, that removes a layer by:\n$\\qquad x_{i+1} = m_i\\phi_i(x_i) + (1 - m_i)x_i = \\begin{cases} \\phi_i(x_i), & \\text{if } m_i = 1, \\\\ x_i, & \\text{otherwise,} \\end{cases}$       (1)\nwhere the $x_i$ and $\\phi_i(x_i)$ refers to the input and output of layer $i$. To obtain the mask, a common paradigm in prior work is to minimize the loss $\\mathcal{L}$ after pruning, which can be formulated as $\\min_m \\mathbb{E}_x [\\mathcal{L}(x, \\Phi, m)]$. However, as we will show in the experiments, this objective \u2013 though widely adopted in discriminative tasks \u2013 may not be well-suited to pruning diffusion transformers. Instead, we are more interested in the recoverability of pruned models. To achieve this, we incorporate an additional weight update into the optimization problem and extend the objective by:\n$\\qquad \\min_{m} \\min_{\\Delta \\Phi} \\mathbb{E}_x [\\mathcal{L}(x, \\Phi + \\Delta \\Phi, m)]$\nRecoverability: Post-Fine-Tuning Performance             (2)\nwhere $\\Delta \\Phi = {\\Delta \\Phi_1, \\Delta \\Phi_2,\\dots, \\Delta \\Phi_M}$ represents appropriate update from fine-tuning. The objective formulated by Equation 2 poses two challenges: 1) The non-differentiable nature of layer selection prevents direct optimization using gradient descent; 2) The inner optimization over the retained layers makes it computationally intractable to explore the entire search space, as this process necessitates selecting a candidate model and fine-tuning it for evaluation. To address this, we propose TinyFusion that makes both the pruning and recoverability optimizable.\n3.2. Tiny Fusion: Learnable Depth Pruning\nA Probabilistic Perspective. This work models Equation 2 from a probabilistic standpoint. We hypothesize that the mask $m$ produced by \"ideal\" pruning methods (might be not unique) should follow a certain distribution. To model this, it is intuitive to associate every possible mask $m$ with a probability value $p(m)$, thus forming a categorical distribution. Without any prior knowledge, the assessment of pruning masks begins with a uniform distribution. However, directly sampling from this initial distribution is highly inefficient due to the vast search space. For instance, pruning a 28-layer model by 50% involves evaluating $\\binom{28}{14} = 40, 116, 600$ possible solutions. To overcome this challenge, this work introduces an advanced and learnable algorithm capable of using evaluation results as feedback to iteratively refine the mask distribution. The basic idea is that if certain masks exhibit positive results, then other masks with similar pattern may also be potential solutions and thus should have a higher likelihood of sampling in subsequent evaluations, allowing for a more focused search on promising solutions. However, the definition of \"similarity pattern\" is still unclear so far."}, {"title": "Sampling Local Structures.", "content": "In this work, we demonstrate that local structures, as illustrated in Figure 2, can serve as effective anchors for modeling the relationships between different masks. If a pruning mask leads to certain local structures and yields competitive results after fine-tuning, then other masks yielding the same local patterns are also likely to be positive solutions. This can be achieved by dividing the original model into $K$ non-overlapping blocks, represented as $\\mathbf{\\Phi} = [\\mathbf{\\Phi}_1, \\mathbf{\\Phi}_2,\\dots, \\mathbf{\\Phi}_K]^T$. For simplicity, we assume each block $\\mathbf{\\Phi}_k = [\\phi_{k1}, \\phi_{k2},\\dots, \\phi_{kM}]^T$ contains exactly $M$ layers, although they can have varied lengths. Instead of performing global layer pruning, we propose an $N:M$ scheme for local layer pruning, where, for each block $\\mathbf{\\Phi}_k$ with $M$ layers, $N$ layers are retained. This results in a set of local binary masks $\\mathbf{m} = [m_1, m_2, . . ., m_K]^T$. Similarly, the distribution of a local mask $m_k$ is modeled using a categorical distribution $p(m_k)$. We perform independent sampling of local binary masks and combine them for pruning, which presents the joint distribution:\n$\\qquad p(m) = p(m_1)\\cdot p(m_2)\\dots p(m_K)$  (3)\nIf some local distributions $p(m_k)$ exhibit high confidence in the corresponding blocks, the system will tend to sample those positive patterns frequently and keep active explorations in other local blocks. Based on this concept, we introduce differential sampling to make the above process learnable.\nDifferentiable Sampling. Considering the sampling process of a local mask $m_k$, which corresponds to a local block $\\mathbf{\\Phi}_k$ and is modeled by a categorical distribution $p(m_k)$. With the $N:M$ scheme, there are $\\binom{M}{N}$ possible masks. We construct a special matrix $\\mathbf{m}_{N:M}$ to enumerate all possible masks. For example, 2:3 layer pruning will lead to the candidate matrix $\\mathbf{m}_{2:3} = [[1, 1, 0], [1, 0, 1], [0, 1, 1]]$. In this case, each block will have three probabilities $p(m_k) = [p_{k1}, p_{k2}, p_{k3}]$. For simplicity, we omit $m_k$ and $k$ and use $p_i$ to represent the probability of sampling $i$-th element in $\\mathbf{m}_{N:M}$. A popular method to make a sampling process differentiable is Gumbel-Softmax [13, 17, 22]:\n$\\qquad \\mathbf{y} = \\text{one-hot} \\left( \\frac{\\exp((g_i + \\log p_i)/\\tau)}{\\sum_i \\exp((g_i + \\log p_i)/\\tau)} \\right)$       (4)\nwhere $g_i$ is random noise drawn from the Gumbel distribution $\\text{Gumbel}(0, 1)$ and $\\tau$ refers to the temperature term. The output $\\mathbf{y}$ is the index of the sampled mask. Here a Straight-Through Estimator [2] is applied to the one-hot operation, where the onehot operation is enabled during forward and is treated as an identity function during backward. Leveraging the one-hot index $\\mathbf{y}$ and the candidate set $\\mathbf{m}_{N:M}$ we can draw a mask $\\mathbf{m} \\sim p(m)$ through a simple index operation:\n$\\qquad \\mathbf{m} = \\mathbf{m}^{\\mathbf{y}}_{N:M}$              (5)"}, {"title": "Joint Optimization with Recoverability.", "content": "With differentiable sampling, we are able to update the underlying probability using gradient descent. The training objective in this work is to maximize the recoverability of sampled masks. We reformulate the objective in Equation 2 by incorporating the learnable distribution:\n$\\qquad \\min_{\\{p(m_k)\\}\\, \\Delta \\Phi} \\min_{\\mathbb{E}_{x, \\{m_k \\sim p(m_k)\\}}} [\\mathcal{L}(x, \\Phi + \\Delta \\Phi, \\{m_k}],$\nRecoverability: Post-Fine-Tuning Performance       (6)\nwhere $\\{p(m_k)\\} = \\{p(m_1),\\dots,p(m_K)\\}$ refer to the categorical distributions for different local blocks. Based on this formulation, we further investigate how to incorporate the fine-tuning information into the training. We propose a joint optimization of the distribution and a weight update $\\Delta \\Phi$. Our key idea is to introduce a co-optimized update $\\Delta \\Phi$ for joint training. A straightforward way to craft the update is to directly optimize the original network. However, the parameter scale in a diffusion transformer is usually huge, and a full optimization may make the training process costly and not that efficient. To this end, we show that Parameter-Efficient Fine-Tuning methods such as LoRA [21] can be a good choice to obtain the required $\\Delta \\Phi$. For a single linear matrix $W$ in $\\Phi$, we simulate the fine-tuned weights as:\n$\\qquad W_{\\text{fine-tuned}} = W + \\alpha \\Delta W = W + BA$,       (7)\nwhere $\\alpha$ is a scalar hyperparameter that scales the contribution of $\\Delta W$. Using LoRA significantly reduces the number of parameters, facilitating efficient exploration of different pruning decisions. As shown in Figure 3, we leverage the sampled binary mask value $m_i$ as the gate and forward the network with Equation 1, which suppresses the layer outputs if the sampled mask is 0 for the current layer. In addition, the previously mentioned STE will still provide non-zero gradients to the pruned layer, allowing it to be further updated. This is helpful in practice, since some layers"}]}