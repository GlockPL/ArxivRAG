[{"title": "3.4 Dataset Analysis", "authors": ["Zihui Gu", "Xingwu Sun", "Fengzong Lian", "Zhanhui Kang", "Cheng-Zhong Xu", "Ju Fan"], "abstract": "Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilities on instruction-following remains a challenge due to complexity and diversity of real-world user instructions. While existing evaluation methods focus on general skills, they suffer from two main shortcomings, i.e., lack of fine-grained task-level evaluation and reliance on singular instruction expression. To address these problems, this paper introduces DINGO, a fine-grained and diverse instruction-following evaluation dataset that has two main advantages: (1) DINGO is based on a manual-annotated, fine-grained and multi-level category tree with 130 nodes derived from real-world user requests; (2) DINGO includes diverse instructions, generated by both GPT-4 and human experts. Through extensive experiments, we demonstrate that DINGO can not only provide more challenging and comprehensive evaluation for LLMs, but also provide task-level fine-grained directions to further improve LLMs.", "sections": [{"title": "1 Introduction", "content": "Recently, Large language models (LLMs) exhibit surprising capabilities not previously seen in smaller models, which are often referred to as emergent abilities (Wei et al. 2022), including in-context learning, chain-of-thought, and instruction-following abilities. Among them, the instruction-following ability is crucial to the interaction between humans and LLMs (e.g., ChatGPT). Existing studies (OpenAI 2023; Chiang et al. 2023; Wang et al. 2023; Longpre et al. 2023) align LLMs with human instructions using supervised instruction-tuning or reinforcement learning from human feedback (RLHF), which enables LLMs to understand human instructions and make high-quality responses. Nonetheless, due to the complexity and diversity of human instructions, it remains a challenge to comprehensively evaluate the instruction-following ability of LLMs.\nExisting studies evaluate the instruction-following ability from the perspective of general skills. For example, InstructEval (Chia et al. 2023) assesses LLM's instruction-following ability based on three general abilities: problem-solving, writing, and alignment to human values. Flask (Ye et al. 2023) shifts the original coarse-grained scoring process to instance-wise skill scoring setup, and defines 4 primary abilities, divided into 12 specific skills, to assess the performance of LLMs. However, there are still two shortcomings in existing evaluation methods:\n\u2022 The lack of fine-grained task-level evaluation poses challenges in improving the instruction-following ability of LLMs. For example, the Factuality skill used in FLASK (Ye et al. 2023) includes many sub-tasks such as \"History Knowledge QA\" and \"Chemical Knowledge QA\". Consequently, even if we recognize that a particular LLM is deficient in this skill, it is challenging to pinpoint the exact aspects of the instruction-following ability that the LLM needs to be improved. Specifically, if the performance of the LLM is not satisfactory in \u201cChemical Knowledge QA\", it is not clear whether this is because the LLM's response contains non-standard chemical formulas. Similarly, if the LLM cannot perform well in \"History Knowledge QA\", it could potentially be because the key points are not clearly outlined in the LLM's response.\n\u2022 The expression of instructions tends to be singular, resulting in a gap between real-world user instructions and existing evaluation datasets. Existing datasets (Chia et al. 2023) often use previous NLP datasets as evaluation data for specific skills, such as employing DROP (Dua et al. 2019) to evaluate the Comprehension ability, and design a specific instruction template for the dataset. However, in real-world scenarios, users express their requests in various ways. Figure 1 shows several examples extracted from the ShareGPT website, a platform where users voluntarily share their interaction records with LLMs. As can be seen, the styles and attitudes of user instructions are very diverse: users may ask questions directly (i.e., Concise) or set specific roles to ask questions (i.e., Role-play). Therefore, it could be very beneficial to evaluate the LLM's instruction-following ability on these diverse instruction expressions.\nTo address the aforementioned shortcomings, in this paper, we present DINGO, a Diverse and Fine-grained Instruction-Following evaluation dataset. First, to support fine-grained instruction-following evaluation, we manually annotate a multi-level category tree with 130 nodes and 4 levels, based on the user instructions extracted from ShareGPT. This category tree encompasses tasks that users would want LLMs to complete in real-world scenarios, making it highly practical. Equipped with its multi-level structure, the category tree supports analyzing instruction-following ability at different granularities, and thus can address the shortcomings of LLM at task-level. Second, we prepare diverse instruction data for each category to comprehensively examine the instruction-following ability. Considering that user requests on ShareGPT have been used for instruction-tuning in many LLMs, such as vicuna (Chiang et al. 2023) and T\u00dcLU (Wang et al. 2023), we avoid data leakage by not directly using data from ShareGPT for evaluation. Instead, we employ GPT-4 to simulate various instruction styles, attitudes, and languages derived from ShareGPT, and generate diverse instruction data for each category. In addition, considering the weaknesses of LLMs in mathematics and logical reasoning, we utilize existing human-annotated datasets (e.g., GSM8K (Cobbe et al. 2021a)) as basic questions and guide GPT-4 to generate diverse instructions from the basic questions to ensure the instruction quality. For example, a math question from GSM8K, \"Ronnie was given 5 while Rissa was given thrice as much \" would be transformed by GPT-4 into a role-playing instruction form: \u201cAct as a patient math teacher to answer this question step by step: Ronnie was given 5 while Rissa was given thrice as much ... \". Based on the above methods, we, in total, collect 5026 diverse samples in DINGO to comprehensively evaluate the instruction-following ability of LLMs.\nBased on DINGO, we conduct extensive experiments to evaluate instruction-following of 10 different LLMs, and obtain the following findings. (1) Even if an instruction-tuned LLM performs well on coarse-grained categories, its performance on fine-grained categories may be diversified and, sometimes, it could even be worse than the base LLM without instruction fine-tuning. (2) Our dataset with diverse instructions presents more significant challenges to LLMs to generate responses that align with human preferences.\nOur contributions can be summarized as follows:\n\u2022 We publicly release a multi-level task category tree consisting of 130 nodes, designed to support instruction-following evaluations at various granularities.\n\u2022 We collect 5026 diverse and high-quality instructions based on real-world user instructions, presenting more significant challenges for LLMs in generating responses that align with human preferences.\n\u2022 We conduct a comprehensive evaluation on 10 representative LLMs, and the experimental results demonstrate that DINGO can support more extensive and challenging evaluation on the instruction-following ability, as well as provide fine-grained guidance to further improve LLMs. We release the DINGO dataset at Github\u00b9."}, {"title": "2 Background: Instruction-Following Ability of Large Language Models", "content": "Language models (LMs) are designed to comprehend and produce text that resembles human language (e.g., BERT (Devlin et al. 2019), GPT2 (Radford et al. 2019)). Recently, researchers have discovered that scaling LMs to large LMs (LLMs) (e.g., ChatGPT, GPT-4 (OpenAI 2023), LLaMA (Touvron et al. 2023a)) by increasing the model size or amount of training data can significantly enhance their downstream task abilities. Moreover, the existing studies also show that LLMs demonstrate surprising abilities that have not been seen in previous smaller LMs (Bubeck et al. 2023; Rae et al. 2021; Brown et al. 2020), such as in-context learning and instruction-following.\nInstruction-following is an important ability for LLMs to interact with real-world users. This means that the model can complete various tasks based on a wide range of natural language instructions provided by humans, including polishing articles (e.g.,Polish this email above in very urgent tone: {Email}.), solving math problems (e.g.,I need to calculate how long my gas canister will last for 360 burener.), providing travel plans (e.g., Plan a trip to Jindo for 2 nights and 3 days.), etc. LLMs can obtain the instruction-following ability in the following two ways: (1) supervised learning using instruction-following datasets (e.g., vicuna (Chiang et al. 2023)), and (2) reinforcement learning from Human Feedback(e.g., Llama2-chat (Touvron et al. 2023b)).\nIn this work, we aim to evaluate the capabilities of existing LLMs on instruction-following across a variety of tasks and various instruction expressions, and provide a comprehensive benchmark DINGO to promote in-depth analysis of the instruction-following ability of LLMs."}, {"title": "3 The DINGO Dataset", "content": "Our goal is to generate a fine-grained category tree and diverse instructions. To achieve this goal, we first collect real-world user instructions as seed data. Then, we manually classify the seed data to obtain a fine-grained category tree. Finally, based on seed data and category tree, we collect diverse instructions for each category by guiding GPT-4 (OpenAI 2023) to simulate various instruction styles, attitudes, and languages."}, {"title": "3.1 Seed Data Collection", "content": "To obtain real-world instruction-following data, we utilize public data from ShareGPT (https://sharegpt.com/), which is a platform for users to share their interactions with LLMs (e.g., GPT-4). Following previous work (Chiang et al. 2023; Wang et al. 2023), we use the \u2018html_cleaned\u201c version2 and truncate conversations with more than 2048 tokens. Based on this, we obtain 7265 seed samples from ShareGPT."}, {"title": "3.3 Instruction Data Collection", "content": "Generating diverse instructions is very challenging for human annotators, as it requires (1) the ability to transition between various instruction styles, attitudes, and languages, and (2) the capacity to produce a range of samples within a single category (e.g., \u201cGrammar-based Rewriting\u201d). Consequently, we propose employing the highly capable LLM, GPT-4 (OpenAI 2023), to simulate a variety of user types and generate diverse, high-quality instructions for each category. Please note that we do not directly incorporate instruction data from ShareGPT into our benchmark, because numerous LLMs (e.g., Vicuna (Chiang et al. 2023), T\u00dcLU (Wang et al. 2023)) have already utilized data from ShareGPT for supervised instruction-tuning. Therefore, we only use ShareGPT data as seed data to guide GPT-4. The data collection pipeline is depicted in Figure 2.\nFor any leaf category (e.g., \u201cMathematics and Reasoning ", "Mathematical Calculations\u201d\u2192 \u201cAlgebraic Equation Problems": "we consider the following three steps to collect the instruction-following data.\nIn the first step, the goal is to generate constraints to guide GPT-4 to simulate specific user types, thereby preventing generation of unrealistic instructions. To achieve this, we treat the seed data as a sample pool and randomly select two samples as demos of in-context learning, each associated with a particular instruction style (S), attitude (A), and language (L). We randomly sample target style, attitude, and language from these two demos to form constraints, compelling GPT-4 to learn from different instruction demos rather than excessively imitating one. For example, the constraints in Figure 2 is {S =\u201cReflective\u201d, A =\u201cPolite\", L =\u201cEnglish"}, ".", "Additionally, given that GPT-4 may struggle to generate high-quality mathematical or logical reasoning questions, we gather data from previous task-specific benchmarks as basic questions, which are then incorporated as part of the constraints. For example, we use the GSM8K (Cobbe et al. 2021a) dataset as a basic question source for the \u201cMathematics and Reasoning\u201d \u2192\u201cWord Problems\" category. More details of the existing dataset resources included in DINGO are listed in Table 3.\nThe goal of the second step is for GPT-4 to simulate a real-world user and generate high-quality instructions by adhering to the constraints. We use in-context learning to achieve this goal. As illustrated in Table 2, we combine the task description, the two demos obtained from the first step, and the target constraints as input context for GPT-4. As demonstrated, GPT-4 learns different expressions from two demos and transforms the basic question into specific instruction \"Act as a helpful math assistant to ...\" based on the Role-play and Command constraints. Following previous work (Wiegreffe et al. 2021; Yuan et al. 2023), we adopt the over-generate-then-filter approach to obtain higher quality instructions. Thus, in this step, we prompt GPT-4 to make two predictions based on the same input, generating two instruction candidates.\nIn the third step, the objective is to select faithful and diverse instructions. We consider two selection methods,"], "content": "Table 4 presents statistics of DINGO, which exhibits two main characteristics: (1) More fine-grained tasks are divided under each first-level category, such as \u201cBiology\u201d and \u201cChemistry\u201d within the \u201cKnowledge Utilization\u201d category. (2) Each sample may comprise multiple turns of questions, simulating the process of human interaction with LLMs.\nTo validate diversity within each category, we calculate the overlap degree of instructions in each category. Figure 3 illustrates the similarity distribution of instructions. For each instruction, we compute its highest ROUGE-L score with regard to other instructions in the same category. The results illustrate the diversity of instructions in DINGO."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nBaseline Models We select two representative types of LLMs: (1) Pre-trained only LLMs, including Llama (Touvron et al. 2023a) and Llama2 (Touvron et al. 2023b); and (2) Instruction-tuned LLMs, including vicuna-v1.3 (Chiang et al. 2023), vicuna-v1.5 (Chiang et al. 2023), Llama2-chat (Touvron et al. 2023b). Considering that vicuna-v1.3 is instruction-tuned from Llama and vicuna-v1.5 is instruction-tuned from Llama2, we refer to vicuna-v1.3 as vicuna and vicuna-v1.5 as vicuna2 in this paper to make the notations consistent with Llama and Llama2.\nEvaluation Method We employ the LLM-as-a-judge method to comprehensively evaluate LLM's responses (Zheng et al. 2023). LLM-as-a-judge is a technique to score the performance of LLMs by utilizing GPT-4. Researchers have discovered that GPT-4 can generate consistent scores and provide detailed justifications, which exhibit a high level of agreement with human experts. However, considering that GPT-4 has difficulty in accurately scoring math/code problems (Cobbe et al. 2021b), we include the standard answers for basic questions as a reference in the prompt given to GPT-4. Regarding the grading method, LLM-as-a-judge considers two types, pair-wise comparison and single-answer grading. However, considering that we need to compare the performance of multiple LLMs, we choose to use single-answer grading for more efficient evaluation. For different categories, we have manually annotated different scoring criteria to assist GPT-4 in generating scores that align with human preferences. For instance, in \u201cMathematics and Reasoning\" tasks, the primary considerations include the clarity of steps, the correctness of reasoning, and the appropriateness of natural language explanations. Meanwhile, for \"Knowledge Unilization\" tasks, the primary considerations is on the adequacy of key points and whether the answers contain hallucination.\nWe explore the agreement between these two grading methods and human experts in Section 4.2."}, {"title": "4.2 Experimental Results", "content": "How do the existing LLMs perform on DINGO? Figure 4-(a) shows the overall performance of ten LLMs on the first-level categories of DINGO. First, comparing pre-trained LLMs with instruction-tuned LLMs, such as Llama-13B and vicuna-13B, we can see that instruction-tuning significantly impacts alignment with human preferences. Second, comparing different instruction-tuned LLMs based on the same pre-trained LLMs, such as vicuna2-7B and Llama2-chat-7B, we find that Llama2-chat-7B has better instruction-following ability than Vicuna2-7B. This is mainly because Llama2-chat-7B utilizes an RLHF (reinforcement learning from human feedback) framework with two reward models for usefulness and safety to align with human preferences, enabling it to outperform the base LLM (i.e., Llama2-7B) under various user instructions. Finally, comparing LLMs of different sizes indicates that increasing the model size significantly improves the instruction-following ability of the pre-trained LLMs (such as Llama2-7B and Llama2-13B), but the impact on instruction-tuned LLMs (such as Llama2-chat-7B and Llama2-chat-13B) is comparatively weaker.\nCan instruction-tuning consistently achieve stable improvements in more fine-grained categories? Figure 4-(b) illustrates the performance across all sub-categories under \"Knowledge Utilization\u201d\u2192\u201cOpen-Book Questions", "Knowledge-Intensive Questions": "It can be seen that under a more fine-grained evaluation, the improvement brought by instruction-tuning is not consistent. For example, the instruction-following performance of vicuna2-7B after instruction-tuning does not improve compared to its base LLM Llama2-7B in the two sub-categories: \"Biology\" and \"Medicine\". This suggests that conducting a more fine-grained evaluation of LLMs' instruction-following ability is necessary, as high scores in coarse categories (e.g., \"Knowledge Utilization\") do not necessarily indicate stable performance in all finer subcategories (e.g., \"Biology\"). Therefore, DINGO helps guide instruction-tuned LLMs towards a more comprehensive direction for improvement, thus enhancing the interactive experiences for users with diverse requests.\nDo diverse instruction types pose a challenge to LLMs? To investigate whether the diverse instruction types in DINGO present a significant challenge to LLMs, we conduct an analysis on the categories containing basic questions. Specifically, we use four LLMs to respond to basic questions and instructions in DINGO across four subcategories. The experimental results are shown in Figure 5. It can be observed that the instruction following scores of the four LLMs on DINGO are lower than those on basic questions, indicating that the diverse instructions in DINGO are more challenging compared to standard questions. This also suggests that it is necessary to evaluate the LLMs' instruction following ability using more diverse instructions, as an LLM may perform well in one mode of expression but not in others, implying that the LLM's robustness to diverse instructions in real-world scenarios might be insufficient.\nAdditionally, to intuitively understand why the LLMs perform poorly on diverse instructions, we present two examples in Table 5. Example-1 indicates that when user instructions become more concise and require a concise output (i.e., \"Directly output the answer without any explanation.\"), LLMs still generate lengthy explanations that do not align with user instructions. Example-2 shows that when the instruction is in Challenge style (i.e., \"Please check each condition carefully!\"), the LLMs may go against the original correct answer in order to cater to human users, i.e., \"Therefore, the final answer is None of the above.\".\nWhat is the agreement between human judge scores and GPT-4 judge scores? To evaluate the agreement between GPT-4 and human experts, we choose 100 examples from DINGO and employ six human experts. Given a judge (i.e., either GPT-4 or human expert), we ask the judge to score the responses of the LLMs using two methods, (1) pairwise comparison and (2) single-answer grading. Pairwise comparison provides the judge a question and two potential answers, and asks the judge to decide which answer is more appropriate. Single-answer grading asks a juedge to assign a score to a specific answer. Figure 6 shows the agreement between GPT-4 and humans under the two scoring methods. With pairwise comparison, GPT-4 has higher agreement with human. However, pairwise comparison would incur high cost. On the other hand, single-answer grading is more efficient. Thus, we recommend single-answer grading for rough identification of model issues, and pairwise comparison for more detailed evaluations."}, {"title": "5 Related Work: Evaluation of LLMs", "content": "For benchmarking the effectiveness of LLMs, various evaluation frameworks have emerged. Frameworks such as HELM (Liang et al. 2022) and BIG-BENCH (bench authors 2023) focus on the effectiveness of LLMs on a wide range of NLP tasks, mainly evaluating the problem solving ability of the model, without paying attention to the LLM's instruction-following ability. Recently, some work has started to focus on the instruction-following ability of LLMs. For example, InstructEval (Chia et al. 2023) focuses on evaluating the ability of Instruction-Tuned LLMs on three aspects, including problem solving, writing, and alignment. Alpaca Farm (Dubois et al. 2023) and Chatbot Arena (Zheng et al. 2023) focus on evaluating the open-ended instruction-following ability of LLMs. However, there are two main differences between DINGO and the above studies: (1) a diverse set of instructions based on real-world scenarios, which can comprehensively evaluate the model's instruction-following performance. (2) a fine-grained task category tree, which can deeply analyze LLM's instruction-following ability on fine-grained task types and pinpoint the deficiencies for further improvement."}, {"title": "6 Conclusion", "content": "In this paper, we have presented a diverse and fine-grained instruction-following evaluation dataset DINGO. Based on a multi-level category tree with 130 nodes derived from real-world user requests, DINGO includes 5026 diverse instructions. Our experiments demonstrate that (1) while an instruction-tuned LLM may excel in broad categories, its performance can vary in fine-grained categories; (2) diverse instructions pose greater challenges for LLMs to generate responses that match human preferences."}]