{"title": "Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data", "authors": ["Zihui Gu", "Xingwu Sun", "Fengzong Lian", "Zhanhui Kang", "Cheng-Zhong Xu", "Ju Fan"], "abstract": "Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilities on instruction-following remains a challenge due to complexity and diversity of real-world user instructions. While existing evaluation methods focus on general skills, they suffer from two main shortcomings, i.e., lack of fine-grained task-level evaluation and reliance on singular instruction expression. To address these problems, this paper introduces DINGO, a fine-grained and diverse instruction-following evaluation dataset that has two main advantages: (1) DINGO is based on a manual-annotated, fine-grained and multi-level category tree with 130 nodes derived from real-world user requests; (2) DINGO includes diverse instructions, generated by both GPT-4 and human experts. Through extensive experiments, we demonstrate that DINGO can not only provide more challenging and comprehensive evaluation for LLMs, but also provide task-level fine-grained directions to further improve LLMs.", "sections": [{"title": "1 Introduction", "content": "Recently, Large language models (LLMs) exhibit surprising capabilities not previously seen in smaller models, which are often referred to as emergent abilities (Wei et al. 2022), including in-context learning, chain-of-thought, and instruction-following abilities. Among them, the instruction-following ability is crucial to the interaction between humans and LLMs (e.g., ChatGPT). Existing studies (OpenAI 2023; Chiang et al. 2023; Wang et al. 2023; Longpre et al. 2023) align LLMs with human instructions using supervised instruction-tuning or reinforcement learning from human feedback (RLHF), which enables LLMs to understand human instructions and make high-quality responses. Nonetheless, due to the complexity and diversity of human instructions, it remains a challenge to comprehensively evaluate the instruction-following ability of LLMs.\nExisting studies evaluate the instruction-following ability from the perspective of general skills. For example, InstructEval (Chia et al. 2023) assesses LLM's instruction-following ability based on three general abilities: problem-solving, writing, and alignment to human values. Flask (Ye et al. 2023) shifts the original coarse-grained scoring process to instance-wise skill scoring setup, and defines 4 primary abilities, divided into 12 specific skills, to assess the performance of LLMs. However, there are still two shortcomings in existing evaluation methods:\n\u2022 The lack of fine-grained task-level evaluation poses challenges in improving the instruction-following ability of LLMs. For example, the Factuality skill used in FLASK (Ye et al. 2023) includes many sub-tasks such as \"History Knowledge QA\" and \"Chemical Knowledge QA\". Consequently, even if we recognize that a particular LLM is deficient in this skill, it is challenging to pinpoint the exact aspects of the instruction-following ability that the LLM needs to be improved. Specifically, if the performance of the LLM is not satisfactory in \u201cChemical Knowledge QA\", it is not clear whether this is because the LLM's response contains non-standard chemical formulas. Similarly, if the LLM cannot perform well in \"History Knowledge QA\", it could potentially be because the key points are not clearly outlined in the LLM's response.\n\u2022 The expression of instructions tends to be singular, resulting in a gap between real-world user instructions and existing evaluation datasets. Existing datasets (Chia et al. 2023) often use previous NLP datasets as evaluation data for specific skills, such as employing DROP (Dua et al. 2019) to evaluate the Comprehension ability, and design a specific instruction template for the dataset. However, in real-world scenarios, users express their requests in various ways. Therefore, it could be very beneficial to evaluate the LLM's instruction-following ability on these diverse instruction expressions.\nTo address the aforementioned shortcomings, in this paper, we present DINGO, a Diverse and Fine-grained Instruction-Following evaluation dataset. First, to support fine-grained instruction-following evaluation, we manually annotate a multi-level category tree with 130 nodes and 4 levels, based on the user instructions extracted from ShareGPT. This category tree encompasses tasks that users would want LLMs to complete in real-world scenarios, making it highly practical. Equipped with its multi-level structure, the category tree supports analyzing instruction-following ability at different granularities, and thus can address the shortcomings of LLM at task-level. Second, we prepare diverse instruction data for each category to comprehensively examine the instruction-following ability. Considering that user requests on ShareGPT have been used for instruction-tuning in many LLMs, such as vicuna (Chiang et al. 2023) and T\u00dcLU (Wang et al. 2023), we avoid data leakage by not directly using data from ShareGPT for evaluation. Instead, we employ GPT-4 to simulate various instruction styles, attitudes, and languages derived from ShareGPT, and generate diverse instruction data for each category. In addition, considering the weaknesses of LLMs in mathematics and logical reasoning, we utilize existing human-annotated datasets (e.g., GSM8K (Cobbe et al. 2021a)) as basic questions and guide GPT-4 to generate diverse instructions from the basic questions to ensure the instruction quality. For example, a math question from GSM8K, \"Ronnie was given 5 while Rissa was given thrice as much \" would be transformed by GPT-4 into a role-playing instruction form: \u201cAct as a patient math teacher to answer this question step by step: Ronnie was given 5 while Rissa was given thrice as much ... \". Based on the above methods, we, in total, collect 5026 diverse samples in DINGO to comprehensively evaluate the instruction-following ability of LLMs.\nBased on DINGO, we conduct extensive experiments to evaluate instruction-following of 10 different LLMs, and obtain the following findings. (1) Even if an instruction-tuned LLM performs well on coarse-grained categories, its performance on fine-grained categories may be diversified and, sometimes, it could even be worse than the base LLM without instruction fine-tuning. (2) Our dataset with diverse instructions presents more significant challenges to LLMs to generate responses that align with human preferences.\nOur contributions can be summarized as follows:\n\u2022 We publicly release a multi-level task category tree consisting of 130 nodes, designed to support instruction-following evaluations at various granularities.\n\u2022 We collect 5026 diverse and high-quality instructions based on real-world user instructions, presenting more significant challenges for LLMs in generating responses that align with human preferences.\n\u2022 We conduct a comprehensive evaluation on 10 representative LLMs, and the experimental results demonstrate that DINGO can support more extensive and challenging evaluation on the instruction-following ability, as well as provide fine-grained guidance to further improve LLMs. We release the DINGO dataset at Github\u00b9."}, {"title": "2 Background: Instruction-Following Ability of Large Language Models", "content": "Language models (LMs) are designed to comprehend and produce text that resembles human language (e.g., BERT (Devlin et al. 2019), GPT2 (Radford et al. 2019)). Recently, researchers have discovered that scaling LMs to large LMs (LLMs) (e.g., ChatGPT, GPT-4 (OpenAI 2023), LLaMA (Touvron et al. 2023a)) by increasing the model size or amount of training data can significantly enhance their downstream task abilities. Moreover, the existing studies also show that LLMs demonstrate surprising abilities that have not been seen in previous smaller LMs (Bubeck et al. 2023; Rae et al. 2021; Brown et al. 2020), such as in-context learning and instruction-following.\nInstruction-following is an important ability for LLMs to interact with real-world users. This means that the model can complete various tasks based on a wide range of natural language instructions provided by humans, including polishing articles (e.g.,Polish this email above in very urgent tone: {Email}.), solving math problems (e.g.,I need to calculate how long my gas canister will last for 360 burener.), providing travel plans (e.g., Plan a trip to Jindo for 2 nights and 3 days.), etc. LLMs can obtain the instruction-following ability in the following two ways: (1) supervised learning using instruction-following datasets (e.g., vicuna (Chiang et al. 2023)), and (2) reinforcement learning from Human Feedback(e.g., Llama2-chat (Touvron et al. 2023b)).\nIn this work, we aim to evaluate the capabilities of existing LLMs on instruction-following across a variety of tasks and various instruction expressions, and provide a comprehensive benchmark DINGO to promote in-depth analysis of the instruction-following ability of LLMs."}, {"title": "3 The DINGO Dataset", "content": "Our goal is to generate a fine-grained category tree and diverse instructions. To achieve this goal, we first collect real-world user instructions as seed data. Then, we manually classify the seed data to obtain a fine-grained category tree. Finally, based on seed data and category tree, we collect diverse instructions for each category by guiding GPT-4 (OpenAI 2023) to simulate various instruction styles, attitudes, and languages."}, {"title": "3.1 Seed Data Collection", "content": "To obtain real-world instruction-following data, we utilize public data from ShareGPT (https://sharegpt.com/), which is a platform for users to share their interactions with LLMs (e.g., GPT-4). Following previous work (Chiang et al. 2023; Wang et al. 2023), we use the \u2018html_cleaned\u201c version2 and truncate conversations with more than 2048 tokens. Based on this, we obtain 7265 seed samples from ShareGPT."}, {"title": "3.2 Category Tree Annotation", "content": "Unlike previous work, we focus only on tasks that may appear in real-world user instructions, as this represents what users genuinely want the LLMs to achieve, providing a more practical evaluation of the instruction-following ability. Thus, we manually annotated the fine-grained task categories of the extracted instruction data from ShareGPT, primarily adhering to the traditional NLP task types commonly defined in previous research (Longpre et al. 2023; bench authors 2023; Zhao et al. 2023). For the convenience of conducting evaluations at different granularities, we design the categories as a multi-level tree structure, which facilitates efficient and in-depth analysis of the capabilities of LLMs. Statistically, our category tree comprises 4 levels, with the first level containing 6 categories, the second level containing 25 categories, the third level containing 65 categories, and the fourth level containing 34 categories. We present the first and second level categories in Table 1.\nAs the goal of this work is to evaluate the performance of LLMs on various instruction expressions, we also annotate the instruction style, attitude, and language for each instruction sample in the seed data, which are described as follows. For instruction style, we specify the following five types:\n\u2022 Inquisitive represents asking multiple questions on the same topic, or delving deeper into a particular question (See the first example in Figure 1).\n\u2022 Reflective represents asking multiple questions with the user's own thoughts and ideas (See the second example in Figure 1).\n\u2022 Challenge represents asking multiple questions, which are increasingly difficult (See the third example in Figure 1).\n\u2022 Role-play represents setting roles for both LLMs and users, and conducting questioning under this setting. (See the fourth example in Figure 1).\n\u2022 Concise represents asking a question directly and clearly. (See the fifth example in Figure 1).\nFor instruction attitude, we specify three types:\n\u2022 Polite represents asking questions using gentle words, such as \"Could you answer the question...\".\n\u2022 Command represents asking questions in a strong and imperative tone, such as \u201cSummarize this passage:\n\u2022 Impatient represents urging the LLM to respond to a certain aspect during the questioning process, such as \u201cAnswer this question directly: hurry up!\".\nMoreover, for languages, we list all the languages included in the conversation, as users often switch between languages during the conversation."}, {"title": "3.3 Instruction Data Collection", "content": "Generating diverse instructions is very challenging for human annotators, as it requires (1) the ability to transition between various instruction styles, attitudes, and languages, and (2) the capacity to produce a range of samples within a single category (e.g., \u201cGrammar-based Rewriting\u201d). Consequently, we propose employing the highly capable LLM, GPT-4 (OpenAI 2023), to simulate a variety of user types and generate diverse, high-quality instructions for each category. Please note that we do not directly incorporate instruction data from ShareGPT into our benchmark, because numerous LLMs (e.g., Vicuna (Chiang et al. 2023), T\u00dcLU (Wang et al. 2023)) have already utilized data from shareGPT for supervised instruction-tuning. Therefore, we only use ShareGPT data as seed data to guide GPT-4. The data collection pipeline is depicted in Figure 2.\nFor any leaf category (e.g., \u201cMathematics and Reasoning \"\u2192\"Mathematical Calculations\u201d\u2192 \u201cAlgebraic Equation Problems", "Reflective": "A =\u201cPolite", "English\"}. Additionally, given that GPT-4 may struggle to generate high-quality mathematical or logical reasoning questions, we gather data from previous task-specific benchmarks as basic questions, which are then incorporated as part of the constraints. For example, we use the GSM8K (Cobbe et al. 2021a) dataset as a basic question source for the \u201cMathematics and Reasoning": "Word Problems\\\" category. More details of the existing dataset resources included in DINGO are listed in Table 3.\nThe goal of the second step is for GPT-4 to simulate a real-world user and generate high-quality instructions by adhering to the constraints. We use in-context learning to achieve this goal. As illustrated in Table 2, we combine the task description, the two demos obtained from the first step, and the target constraints as input context for GPT-4. As demonstrated, GPT-4 learns different expressions from two demos and transforms the basic question into specific instruction \\\"Act as a helpful math assistant to ...\\\" based on the Role-play and Command constraints. Following previous work (Wiegreffe et al. 2021; Yuan et al. 2023), we adopt the over-generate-then-filter approach to obtain higher quality instructions. Thus, in this step, we prompt GPT-4 to make two predictions based on the same input, generating two instruction candidates.\nIn the third step, the objective is to select faithful and diverse instructions. We consider two selection methods, constraint-based pair-wise selection and similarity-based selection. Specifically, we first use GPT-4 to determine which of the two candidates adheres more closely to the constraints. We require GPT-4 to choose from three options, {first, second, tie}, and provide a rationale. Next, to ensure diversity, we calculate the similarity between the best candidate and the collected data in the dataset. Then, we only add the candidate to the dataset if the maximum ROUGE-L similarity is less than 0.6.\"\n    },\n    {\n      \"title\": \"3.4 Dataset Analysis\",\n      \"content\": \"Table 4 presents statistics of DINGO, which exhibits two main characteristics: (1) More fine-grained tasks are divided under each first-level category, such as \u201cBiology", "Chemistry": "ithin the \u201cKnowledge Utilization\u201d category. (2) Each sample may comprise multiple turns of questions, simulating the process of human interaction with LLMs.\nTo validate diversity within each category, we calculate the overlap degree of instructions in each category. Figure 3 illustrates the similarity distribution of instructions. For each instruction, we compute its highest ROUGE-L score with regard to other instructions in the same category. The results illustrate the diversity of instructions in DINGO."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nBaseline Models We select two representative types of LLMs: (1) Pre-trained only LLMs, including Llama (Touvron et al. 2023a) and Llama2 (Touvron et al. 2023b); and (2) Instruction-tuned LLMs, including vicuna-v1.3 (Chiang et al. 2023), vicuna-v1.5 (Chiang et al. 2023), Llama2-chat (Touvron et al. 2023b). Considering that vicuna-v1.3 is instruction-tuned from Llama and vicuna-v1.5 is instruction-tuned from Llama2, we refer to vicuna-v1.3 as vicuna and vicuna-v1.5 as vicuna2 in this paper to make the notations consistent with Llama and Llama2.\nEvaluation Method We employ the LLM-as-a-judge method to comprehensively evaluate LLM's re-sponses (Zheng et al. 2023). LLM-as-a-judge is a technique to score the performance of LLMs by utilizing GPT-4. Researchers have discovered that GPT-4 can generate consistent scores and provide detailed justifications, which exhibit a high level of agreement with human experts. However, considering that GPT-4 has difficulty in accurately scoring math/code problems (Cobbe et al. 2021b), we include the standard answers for basic questions as a reference in the prompt given to GPT-4. Regarding the grading method, LLM-as-a-judge considers two types, pair-wise comparison and single-answer grading. However, considering that we need to compare the performance of multiple LLMs, we choose to use single-answer grading for more efficient evaluation. For different categories, we have manually annotated different scoring criteria to assist GPT-4 in generating scores that align with human preferences. For instance, in \u201cMathematics and Reasoning\" tasks, the primary considerations include the clarity of steps, the correctness of reasoning, and the appropriateness of natural language explanations. Meanwhile, for \"Knowledge Unilization\" tasks, the primary considerations is on the adequacy of key points and whether the answers contain hallucination.\nWe explore the agreement between these two grading methods and human experts in Section 4.2."}, {"title": "4.2 Experimental Results", "content": "How do the existing LLMs perform on DINGO? Figure 4-(a) shows the overall performance of ten LLMs on the first-level categories of DINGO. First, comparing pre-trained LLMs with instruction-tuned LLMs, such as Llama-13B and vicuna-13B, we can see that instruction-tuning significantly impacts alignment with human preferences. Second, comparing different instruction-tuned LLMs based on the same pre-trained LLMs, such as vicuna2-7B and Llama2-chat-7B, we find that Llama2-chat-7B has better instruction-following ability than Vicuna2-7B. This is mainly because Llama2-chat-7B utilizes an RLHF (reinforcement learning from human feedback) framework with two reward models for usefulness and safety to align with human preferences, enabling it to outperform the base LLM (i.e., Llama2-7B) under various user instructions. Finally, comparing LLMs of different sizes indicates that increasing the model size significantly improves the instruction-following ability of the pre-trained LLMs (such as Llama2-7B and Llama2-13B), but the impact on instruction-tuned LLMs (such as Llama2-chat-7B and Llama2-chat-13B) is comparatively weaker.\nCan instruction-tuning consistently achieve stable improvements in more fine-grained categories? Figure 4-(b) illustrates the performance across all sub-categories under \"Knowledge Utilization\u201d\u2192\u201cOpen-Book Questions"}, {"title": "5 Related Work: Evaluation of LLMs", "content": "For benchmarking the effectiveness of LLMs, various evaluation frameworks have emerged. Frameworks such as HELM (Liang et al. 2022) and BIG-BENCH (bench authors 2023) focus on the effectiveness of LLMs on a wide range of NLP tasks, mainly evaluating the problem solving ability of the model, without paying attention to the LLM's instruction-following ability. Recently, some work has started to focus on the instruction-following ability of LLMs. For example, InstructEval (Chia et al. 2023) focuses on evaluating the ability of Instruction-Tuned LLMs on three aspects, including problem solving, writing, and alignment. Alpaca Farm (Dubois et al. 2023) and Chatbot Arena (Zheng et al. 2023) focus on evaluating the open-ended instruction-following ability of LLMs. However, there are two main differences between DINGO and the above studies: (1) a diverse set of instructions based on real-world scenarios, which can comprehensively evaluate the model's instruction-following performance. (2) a fine-grained task category tree, which can deeply analyze LLM's instruction-following ability on fine-grained task types and pinpoint the deficiencies for further improvement."}, {"title": "6 Conclusion", "content": "In this paper, we have presented a diverse and fine-grained instruction-following evaluation dataset DINGO. Based on a multi-level category tree with 130 nodes derived from real-world user requests, DINGO includes 5026 diverse instructions. Our experiments demonstrate that (1) while an instruction-tuned LLM may excel in broad categories, its performance can vary in fine-grained categories; (2) diverse instructions pose greater challenges for LLMs to generate responses that match human preferences."}]}