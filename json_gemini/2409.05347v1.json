{"title": "TriplePlay: Enhancing Federated Learning with CLIP for Non-IID Data and Resource Efficiency", "authors": ["Ahmed Imteaj", "Md Zarif Hossain", "Saika Zaman", "Abdur R. Shahid"], "abstract": "The rapid advancement and increasing complexity of pretrained models, exemplified by CLIP, offer significant opportunities as well as challenges for Federated Learning (FL), a critical component of privacy-preserving artificial intelligence. This research delves into the intricacies of integrating large foundation models like CLIP within FL frameworks to enhance privacy, efficiency, and adaptability across heterogeneous data landscapes. It specifically addresses the challenges posed by non-IID data distributions, the computational and communication overheads of leveraging such complex models, and the skewed representation of classes within datasets. We propose TriplePlay, a framework that integrates CLIP as an adapter to enhance FL's adaptability and performance across diverse data distributions. This approach addresses the long-tail distribution challenge to ensure fairness while reducing resource demands through quantization and low-rank adaptation techniques. Our simulation results demonstrate that TriplePlay effectively decreases GPU usage costs and speeds up the learning process, achieving convergence with reduced communication overhead.", "sections": [{"title": "I. INTRODUCTION", "content": "CLIP (Contrastive Language-Image Pre-Training) [1] is a neural network-based model developed by OpenAI, trained on an extensive and diverse dataset of images paired with textual descriptions. This approach enables CLIP to effectively understand and associate visual content with natural language. The evolution of pretrained models, from perceptron to complex architectures like CLIP, has led to an increase in their size and knowledge capacity. As FL cements its role as a key enabler of privacy-preserving artificial intelligence, it confronts challenges that stem from the inherent heterogeneity of data distributions across clients and the considerable resource demands of incorporating large foundation models like CLIP. These challenges, notably non-IID data and the substantial computational and communication costs, impede the seamless deployment and efficacy of FL in diverse operational environments characterized by a wide range of edge device capabilities. Despite its reputation for state-of-the-art image and text representations, CLIP remains relatively unexplored in FL, with prior efforts attempted to achieve efficient aggregation and local training but falling short in addressing computational costs and data distribution heterogeneity. Besides, in the realm of FL, the significance of data quality and distribution cannot be overstated as many datasets suffer from a common issue known as the 'long-tail distribution', where certain classes or categories are underrepresented, making it challenging for models to learn effectively from these minority samples. Besides, large model can cost high network bandwidth and encounter communication overhead. Considering these challenges, this paper investigates the impact of integrating a foundation model like CLIP as an adapter in FL on the adaptability and performance of FL systems across varied data distributions, to transcend existing limitations and fully exploit the potential of this powerful model. Recognizing CLIP's unexploited potential in FL-despite its proven capabilities in generating state-of-the-art image and text representations\u2014this research aims to bridge the gap by investigating strategies to overcome these obstacles. The challenges of data quality and the prevalent issue of long-tail distribution, where certain data classes are significantly underrepresented, further complicate model training and learning processes, underscoring the need for innovative solutions to ensure balanced and effective learning across all data spectrums. This paper sets out to explore CLIP's robust feature extraction capabilities, while addressing the computational constraints inherent in FL deployments. Moreover, this paper explores quantization and low rank adaptation to reduce the resource demands of large models in FL, focusing on decreasing the size and enhancing the efficiency of model exchanges between clients and the server. To this end, we provide scalable, efficient, and privacy-preserving machine learning solutions that capitalize on the strengths of advanced pretrained models and synthetic data creation, ultimately fostering personalized and equitable AI applications."}, {"title": "II. BACKGROUND STUDY", "content": "Federated Learning (FL) [2] is a decentralized approach that allows multiple devices or institutions to collaboratively train a machine learning model without directly sharing their data. Instead of pooling data into a central location, FL sends the model to local data sources, where it is trained independently on each dataset. The locally trained models are then aggregated into a global model, allowing the overall system to learn from distributed data while preserving privacy and reducing the need for data transfer. This approach has been continuously refined to address real-world machine learning challenges, particularly in scenarios where data is highly distributed and heterogeneous. A key challenge that FL addresses is the non-IID (non-independent and identically distributed) nature of client data, where the data on each client may differ significantly in distribution, leading to variability in model performance. As a result, FL has proven effective in environments with diverse data distributions, such as healthcare, finance, and mobile applications, where privacy and data ownership are paramount concerns [3]-[6]. However, the advent of data heterogeneity across clients introduced significant challenges, leading to skewed local models and impeded convergence rates. Several efforts [7]\u2013[11] were made to transpose existing domain generalization methodologies to the FL landscape. More recent developments have delved into the realms of generalization within FL, acknowledging its novelty and the inherent challenges it presents. Initial discussions on this topic aimed to delineate performance gaps, including out-of-sample and participation discrepancies, laying the groundwork for more focused solutions. Besides, efforts were made to transpose existing domain generalization methodologies to the FL landscape, with initiatives like FL Games [8] and FedSAM [9] leading the charge. These solutions tailor strategies like Nash equilibrium and Sharpness Aware Minimization (SAM) to foster invariant feature learning across clients and enhance model robustness [11]. Yet, despite these advancements, a critical observation is the limited applicability of these generalization strategies to larger models [12], [13] and their underutilization of knowledge from pretrained models [14]-[17]. This gap signifies challenges within the FL domain, hinting at the necessity for novel approaches that can fully leverage the breadth of knowledge encapsulated in large, pretrained models while ensuring robust generalization across diverse and distributed data landscapes."}, {"title": "B. Pretrained Vision-Language Models", "content": "The pretrained Models (e.g., Vision-Language models) are distinguished by their ability to concurrently learn from vast datasets containing both images and their corresponding textual descriptions. A notable feature of these models is their zero-shot prediction capabilities, where they can accurately infer information or classify images they have never encountered during training. One of the pioneering models in this area, CLIP [1] utilizes paired image and text encoders primarily for tasks like image classification and retrieval. Similarly, the ALIGN model [18] focuses on training visual and language representations using large collections of images paired with noisy alt-text data. Further developments have seen models like BLIP [19] which not only understand but also generate vision-language content, emphasizing the use of refined captions. FLAVA [20] represents a leap towards learning from both paired and unpaired images and text, integrating both multimodal and unimodal encoders for a comprehensive representation. SimVLM [21] simplifies the model training process through the use of large-scale weak supervision alongside a novel prefix language modeling objective. Extensions of the CLIP model to accommodate multilingual text encoding, such as AltCLIP [22], and domain-specific adaptations like FashionCLIP [23] and PLIP [24], showcase the versatility and adaptability of these models to various datasets and objectives. Other applications include domain-specific visual embedding generation [25]\u2013[30], semantic segmentation [31]\u2013[35], object detection through knowledge distillation [36]-[40], and the adaptation of CLIP for personalized supervised learning [41]\u2013[45]. This evolution of pretrained Vision-Language Models showcases the dynamic synergy between visual and textual data and paves the way for more efficient, and versatile AI systems capable of understanding and generating human-like perceptions of the world."}, {"title": "III. PROPOSED APPROACH", "content": "Our proposed approach focuses, TriplePlay on three key tasks in FL setting: prioritizing personalization and generalization, effectively managing underrepresented classes, and reducing resource consumption, which are detailed below:"}, {"title": "A. Prioritizing Personalization and Generalization:", "content": "To prioritize personalization and generalization in federated learning (FL), we propose a strategy that preserves valuable prior knowledge from pretrained models while adapting them to specific tasks efficiently. Fine-tuning entire networks with limited data can compromise their original abilities, especially in FL settings with resource constraints. Therefore, we focus on a simple attention-based adapter approach, integrated with the CLIP model, which allows for rapid task-specific adaptation with minimal resource overhead. The steps are detailed below:\n1) Pretrained CLIP Model: Start with a pretrained CLIP model denoted as $CLIP_{pre}$, which has been trained on a large and diverse dataset.\n2) Adapter Architecture: Design a simple attention-based adapter that can be added on top of $CLIP_{pre}$ to adapt it to specific tasks. The adapter consists of two main components:\nAttention Mechanism: This allows the adapter to focus on different parts of the input data (D), enabling task-specific adaptation:\n$Att(D) = softmax (Q. K^T) \u00b7 V$\nwhere Q, K, and V are the query, key, and value matrices, respectively, derived from the input data.\nFeedforward Network: After applying the attention mechanism, the output is passed through a feedforward network to further adapt the features. The feedforward network can be represented as:\n$FFN(Att(D)) = ReLU (W_1 \u00b7 Att(D) + b_1) \u00b7 W_2 + b_2$\nwhere $W_1, b_1, W_2,$ and $b_2$ are the weights and biases of the feedforward network.\n3) Adapter Integration: Integrate the adapter into the CLIP model by adding it as an additional layer. The adapted CLIP model can be represented as:\n$CLIP_{adapted}(D) = Adapter (CLIP_{pre}(D))$"}, {"title": "B. Handling Long-Tail Distribution", "content": "Generating synthetic data using GANs offers a promising solution for handling the long-tail distribution in foundation models. In datasets where certain classes are significantly underrepresented, GANs can be trained to generate synthetic samples that mimic the distribution of these minority classes (see Figure 1(b)). By augmenting the dataset with these synthetic samples, the model is exposed to a more balanced representation of all classes, allowing it to learn more effectively from the underrepresented classes. This approach enhances the model's ability to generalize and make equitable predictions across all classes, leading to improved performance on minority classes and a more robust foundation model.\nTo derive and understand the equation for training GANS, we consider the interaction between two models: the Generator (G) and the Discriminator (D). The generator aims to produce data that is indistinguishable from real data, while the discriminator aims to distinguish between real data and the data generated by G.\n1) Discriminator's Goal: The discriminator, D, aims to assign the correct labels to both real and generated data. It maximizes the probability of assigning the correct label to both real data (coming from the dataset) and fake data (produced by G). The first term, $E_{x\u223cp_{data}(x)} [log D(x)]$, represents the expected log probability that D correctly identifies real data as real. The higher this value, the better D is at recognizing real data.\n2) Generator's Goal: The generator, G, aims to produce data that D will mistakenly classify as real. This is represented by the second term, $E_{z\u223cp_z(z)} [log(1 \u2013 D(G(z)))]$, which is the expected log probability that D incorrectly classifies fake data (generated by G) as real. G tries to minimize this term, making D(G(z)) as close to 1 as possible, indicating D believes the fake data is real.\n3) The Min-Max Game: The min-max formulation $min_G max_D V (D, G)$ captures the adversarial nature of the training process. D maximizes V(D, G) by getting better at distinguishing real from fake, while G minimizes V(D, G) by improving its ability to generate data that appears real to D."}, {"title": "Explanation:", "content": "\u2022 The discriminator's optimization ($max_D$) increases its accuracy in distinguishing real data from fake. It does this by maximizing the probability of correctly identifying real data as real and fake data as fake.\n\u2022 The generator's optimization ($min_G$) aims to fool the discriminator by generating data that is indistinguishable from real data. It does this by minimizing the discriminator's ability to correctly label fake data as fake.\n\u2022 This adversarial process leads to a situation where G generates increasingly realistic data, and D becomes better at telling real from fake, until G's outputs are indistinguishable from actual data.\nThis iterative training process continues until a point of equilibrium is reached where D can no longer distinguish between real and generated data, meaning G has successfully learned to generate data resembling the real data distribution."}, {"title": "C. Reducing Resource Consumption and Communication Overhead:", "content": "We seek to address the dual challenges: communication overhead and resource consumption during large model exchange in FL training through the strategic application of quantization and QLoRa. Quantization decreases model size by converting parameters into lower-bit representations, thereby enhancing memory usage and computational speed. Besides, QLoRa improves the training process of quantized models on agents' devices, ensuring minimal accuracy loss and efficient learning from local data.\nIncorporating GAN-based synthetic data generation into the initial stages of feature extraction and adaptation, alongside the integration of QLoRa and quantization strategies for optimization, we redefine the methodology as follows:\n1) Advanced Feature Extraction with Synthetic Data Augmentation: For each participating agent in the FL system, a pretrained CLIP model is utilized at the outset to extract relevant features from textual and image data. Given an input-label pair (v, z), enhanced with GAN-generated synthetic data to address class imbalance and data underrepresentation, we utilize the pretrained CLIP model for comprehensive feature extraction:\n$V_{synth} = f_{vis} (GAN(v)), U_{synth} = f_{text}(GAN(z))$\nHere, GAN(v) and GAN(z) represent the synthetic visual and textual data generated to enrich the training dataset, ensuring a more balanced and diverse feature set for the learning process.\n2) Task-specific Feature Refinement with Adaptive and Synthetic Enhancements: An adaptive refinement mechanism h, further informed by the diversity and balance brought in by synthetic data, selectively enhances features crucial for task-specific needs. This process involves an attention mechanism applied to both original and synthetic features:\n$V' = h(V_{synth}) \\otimes V_{synth}, U' = h(U_{synth}) \\otimes U_{synth}$\n3) Optimized Feature Normalization and Interaction with QLoRa and Quantization: After refinement, features undergo an optimized normalization process incorporating QLoRa and quantization techniques for efficient computation and interaction:\n$V_{opt} = \\frac{V'}{||V' ||_{QLoRa}}, U_{opt} = \\frac{U'}{||U' ||_{QLoRa}}$\n$V_{quant} = s. quantize(V_{opt}, U_{opt}), U_{quant} = V^T_{quant}$\nHere, $|| \\cdot ||_{QLoRa}$ denotes the normalization process enhanced by QLoRa, and quantize(\u00b7) applies quantization for reduced model size and computational efficiency.\n4) Loss Computation with Enhanced Data Diversity: The loss for visual and textual predictions is computed against an expanded and diversified ground truth vector, \u017e, to reflect the inclusion of synthetic data:\n$l_{vis} = l(\\widehat{V}_{quant}, z_{synth}), l_{text} = l(\\widehat{U}_{quant}, z_{synth})$\n5) Federated Learning Optimization with Adaptive Parameters and Efficiency Enhancements: Optimizing for FL involves aggregating the efficiently compressed adapter parameters, $w_{opt}$, across clients, minimizing both computational and communication overhead:\n$w_{final}^{h,opt} = \\sum_{i=1}^N \\frac{m_i}{\\sum_{i=1}^N \\sum_{j=1}^{m_j}} -QLoRa(quantize(w_i^h))$\nThis final step incorporates the benefits of both QLoRa and quantization to ensure efficient learning and communication in the FL environment, leveraging the comprehensive feature set enhanced by synthetic data generation for a robust, adaptable, and efficient federated learning framework."}, {"title": "IV. EXPERIMENTAL ANALYSIS", "content": "We evaluate our proposed approach on PACS [46] and Office-Home [47] dataset. PACS consists of four domains (photo, art painting, cartoon, and sketch) with a total of 9, 991 images distributed across 7 object categories. On the other hand, Office-Home [47] is a classification benchmark dataset, containing approximately 15,500 images across 65 classes. Notably, in the PACS dataset, the 'Photo' class exhibits fewer samples than other classes, while in the Office-Home dataset, the 'Product' class contains less amount of data samples in comparison to other classes. Hence, we apply GAN to generate synthetic data for these underrepresented classes."}, {"title": "B. Result Analysis", "content": "In Fig. 2, we demonstrate the TriplePlay's proficiency in semantic understanding and precise text generation tailored to the specific visual context of each client. Each FL client's local visual data, ranging from the activity of a hospital patient to a zebra herd, is processed to produce accurate and contextually appropriate textual descriptions. This highlights the model's ability to understand and articulate fine-grained details from diverse visual inputs, ensuring personalized and relevant textual outputs based on the local datasets. In Figure 3 (left), the resource usage of FedCLIP versus TriplePlay is illustrated through a line graph that details the percentage of GPU utilization across a spectrum of communication rounds from 0 to 500. The FedCLIP line exhibits significant fluctuations, with utilization percentages oscillating between approximately 60% and 70%, indicating a variable demand on the GPU resources throughout the communication rounds. In stark contrast, our proposed approach, TriplePlay maintains a remarkably steady and lower GPU utilization, consistently around the 35% mark, indicating a more efficient and stable usage of GPU resources over time. This visual data clearly suggests that the TriplePlay method ensures a more uniform and possibly more efficient GPU usage profile compared to FedCLIP. Figure 3 (right) presents a comparative visualization of the accuracy trajectories between the FedCLIP methodology and our QLoRa Fine-tuning approach over 500 communication rounds. Starting below a 0.6 accuracy, FedCLIP gradually improves, eventually leveling off around the 0.7 mark as the rounds advance, showcasing a steady enhancement in performance with more communication rounds. Conversely, our proposed method, TriplePlay demonstrates a sharp increase in accuracy early on, crossing the 0.6 threshold within the initial 50 rounds. After this quick rise, while the accuracy continues to improve, it does so at a more moderate pace, surpassing the 0.7 level just beyond the 100th round."}, {"title": "V. CONCLUSION", "content": "This paper has set forth a framework to address the pressing challenges in FL, especially in the context of integrating large foundation models like CLIP and managing data diversity and resource constraints. The outcomes include improved adaptability of FL systems across varied data distributions, enhanced model performance, especially for minority classes, and a reduction in the computational and communication resources required. Ultimately, this research aims to push the boundaries of what is currently achievable in FL, paving the way for more inclusive, efficient, and privacy-preserving machine learning models that can adapt to and thrive within the complex, data-diverse landscapes of the real world."}]}