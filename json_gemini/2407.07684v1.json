{"title": "Towards Human-Like Driving: Active Inference in Autonomous Vehicle Control", "authors": ["Elahe Delavari", "John Moore", "Junho Hong", "Jaerock Kwon"], "abstract": "This paper presents a novel approach to Autonomous Vehicle (AV) control through the application of active inference, a theory derived from neuroscience that conceptualizes the brain as a predictive machine. Traditional autonomous driving systems rely heavily on Modular Pipelines, Imitation Learning, or Reinforcement Learning, each with inherent limitations in adaptability, generalization, and computational efficiency. Active inference addresses these challenges by minimizing prediction error (termed \"surprise\") through a dynamic model that balances perception and action. Our method integrates active inference with deep learning to manage lateral control in AVs, enabling them to perform lane following maneuvers within a simulated urban environment. We demonstrate that our model, despite its simplicity, effectively learns and generalizes from limited data without extensive retraining, significantly reducing computational demands. The proposed approach not only enhances the adaptability and performance of AVs in dynamic scenarios but also aligns closely with human-like driving behavior, leveraging a generative model to predict and adapt to environmental changes. Results from extensive experiments in the CARLA simulator show promising outcomes, outperforming traditional methods in terms of adaptability and efficiency, thereby advancing the potential of active inference in real-world autonomous driving applications.", "sections": [{"title": "I. INTRODUCTION", "content": "As the demand for fully Autonomous Vehicles (AVs) continues to push the boundaries of technology, the integration of sophisticated decision-making frameworks that can effectively mimic human cognitive processes becomes imperative. The development of autonomous driving systems has traditionally relied on methodologies such as Modular Pipeline (MP) [3], Imitation Learning (IL) [4] [5], Reinforcement Learning (RL) [6], and direct perception [7]. Each of these approaches, while contributing valuable insights and capabilities, also presents distinct limitations in scalability, adaptability, and integration complexity. For instance, MPs, although highly interpretable and modular, suffer from error propagation and rigidity. IL can mimic expert behavior but struggles with generalization to unseen scenarios. RL offers the ability to learn from interaction but at the cost of sample inefficiency and computational demands.\nAmong the various approaches explored, the principle of active inference emerges as a promising paradigm, inspired by theories of human brain function. Active inference suggests that the brain continuously predicts sensory inputs and minimizes the difference between expected and actual signals through perception and action. This framework, grounded in the Free Energy Principle (FEP) [1], suggests a robust method for handling the uncertainties inherent in real-world driving environments. Active inference [8], a theory originally from neuroscience, is applied across a spectrum of fields including psychology, cognitive science, artificial intelligence, robotics, and AVs. This theory conceptualizes the brain as a predictive machine that continuously constructs and updates a generative model-encompassing hidden states, observations, and actions-using Bayesian inference to mirror the actual generative processes of the world. The primary aim of active inference is to minimize the gap between these predictions and sensory inputs, a discrepancy known as \"surprise\" in the literature, which is considered the upper bound for Free Energy (FE) [2]. The prediction error, or surprise, can be minimized in two distinct ways: 1) by refining the internal model of the world often referred to as perception to better align with the sensory input, or 2) by taking actions that alter the environment in such a way that the sensory inputs become more congruent with the internal model held by the brain. In contrast to traditional methodologies, active inference offers a unified framework that seamlessly integrates perception and action, guided by a generative model that anticipates environmental changes. This approach enables an autonomous system to not just react to the environment but to engage with it in a predictive manner, learning a goal-directed task more human-like to mitigate the inherent problems of MP, IL, and RL.\nThis paper introduces a novel application of active inference to the lateral control of self-driving vehicles within a simulated urban environment. We propose a model that not only learns to predict the consequences of specific actions on the vehicle's trajectory but also adapts to new road conditions without extensive retraining. By leveraging a deep learning architecture integrated with active inference, we demonstrate the model's capability to perform driving maneuvers under varied urban structures.\nOur approach leverages the principles of active inference to achieve computationally efficient, and explainable. By focusing on minimizing FE through action-adjusting the vehicle's maneuvers to reduce prediction errors our model demonstrates enhanced adaptability and performance in different driving scenarios."}, {"title": "II. RELATED WORKS", "content": "There are different methods for controlling an AV in an environment. One of the most popular methods is MP in autonomous driving systems which generally consists of dedicated modules for perception, prediction, planning, and control. While this approach offers clear advantages in terms of interpretability and modularity, it also has several limitations as follows. (i) Errors in upstream modules (e.g., perception) can propagate downstream, leading to compounding errors in prediction, planning, and control. This can significantly degrade the overall system performance. (ii) Integrating multiple modules from different sources can be complex and challenging. Ensuring smooth communication and coordination between modules often requires significant engineering effort. (iii) MPs can be inflexible, making it difficult to adapt the system to new tasks or environments without significant re-engineering. This rigidity can limit the scalability and versatility of the system. (iv) Each module is typically optimized independently, which can lead to sub-optimal overall performance. Joint optimization across modules is challenging due to the different nature of tasks each module performs [9] [10].\nAnother method that is vastly used is Imitation Learning (IL) which has been a popular method for developing autonomous driving systems due to its ability to mimic expert behavior from demonstrations and a lot of work has been done in this domain [4] [5] [9] [11] [12]. Traditional IL methods, however, often struggle with generalizing to unseen states not covered by the expert's demonstrations. When the agent encounters scenarios outside the training data, it may fail to perform adequately, leading to safety and reliability issues [9] [13]. The IL performance heavily depends on the quality and quantity of expert demonstrations. Incomplete or sub-optimal demonstrations can lead to poor performance, as the agent can only replicate what it has seen during training. Unlike RL, IL does not promote environmental exploration or self-driven experimental learning, potentially restricting the agent's capacity to adjust to new or changing environments. [10]. Additionally, in complex scenarios such as urban driving, high-level decisions (e.g. choosing to turn left or right at an intersection) cannot be inferred from perceptual input alone, leading to ambiguity and oscillations in the agent's behavior. To overcome this, Codevilla et al. [9] introduced Conditional Imitation Learning (CIL), where the driving policy is conditioned on high-level commands. This approach allows the trained model to be guided by navigational commands during test time, thus enabling the vehicle to follow specific routes in urban environments. Their work showed that CIL could handle complex urban driving scenarios more effectively than traditional IL methods.\nDirect perception methods are also the other category of methods that are used. Sauer et al. [14] propose Conditional Affordance Learning (CAL), a direct perception approach for autonomous urban driving. This method maps video inputs to intermediate representations (affordances) that are then used by a control algorithm to maneuver the vehicle. CAL aims to combine the advantages of MPs and IL by using a neural network to learn low-dimensional, yet informative, affordances from high-level directional inputs. The approach handles complex urban driving scenarios, including navigating intersections, obeying traffic lights, and following speed limits. However, choosing the best set of affordances is challenging. Furthermore, the effectiveness of this method relies heavily on precise affordance predictions. Any inaccuracies or errors in predicting affordances may result in less than optimal or even hazardous driving decisions.\nUse of RL has shown promising results in training autonomous agents through trial and error [15] [16] [17]. However, it also has its own set of limitations. (i) RL methods typically necessitate extensive sample data to develop effective policies, which can be especially challenging in real-world settings where data acquisition is costly and time-intensive. (ii) Achieving a balance between exploration (testing new actions) and exploitation (leveraging known actions) presents a substantial hurdle in RL. Poor exploration strategies can lead to local optima and prevent the agent from discovering better policies. (iii) Designing appropriate reward functions is critical for RL but can be difficult and problem-specific. Poorly designed rewards can lead to unintended behaviors or sub-optimal performance. (iv) RL algorithms, particularly those that utilize deep learning, tend to be computationally intensive and demand significant computational resources. This requirement can restrict their use in environments where resources are limited. (v) Ensuring the stability and convergence of RL algorithms is challenging, especially in complex and dynamic environments. Instabilities during training can lead to unreliable and unpredictable agent behavior [9] [10].\nLiang et al. [17] introduce Controllable Imitative Reinforcement Learning (CIRL), which combines IL and RL to address the challenges of autonomous urban driving in complex, dynamic environments. CIRL utilizes a Deep Deterministic Policy Gradient (DDPG) approach to guide exploration in a constrained action space informed by human demonstrations. This method allows the driving agent to learn adaptive policies tailored to different control commands (e.g., follow, turn left, turn right, go straight). Despite using IL to guide initial exploration, CIRL still faces challenges associated with exploration in RL. The agent may struggle to discover optimal policies in highly dynamic and unpredictable environments without effective exploration strategies.\nTo adress these limitations of MPs,IL, direct perception, and RL, we propose to use active inference that provides a robust framework that dynamically balances exploration and exploitation, adapts to new and unseen states, and minimizes prediction errors by choosing the best possible action. This approach allows for more adaptive and reliable autonomous systems capable of handling the complexities and uncertainties of real-world environments.\nActive inference has been a growing area of interest in AV research. Friston et al. [18] established the theoretical foundations of active inference, proposing it as a process theory of the brain that minimizes prediction error or \"surprise\" by constantly updating its generative model or by inferring actions leading to less surprising states. This principle has been applied to various agents in simple simulated environments, but its application to realistic scenarios has been limited until recently. Catal et al. [19] introduced a novel approach by integrating deep learning with active inference for a real-world robot navigation task. They leveraged high-dimensional sensory data from camera frames to build complex generative models that operate without predefined state spaces. Their work demonstrated that deep active inference could successfully guide a mobile robot in a warehouse environment, showcasing the potential of this approach for real-world applications. However, they focused on a single, relatively controlled environment. de Tinguy et al. [20] presents a scalable hierarchical active inference model for autonomous navigation, exploration, and goal-oriented behavior inspired by human cognitive mapping strategies. The model integrates curiosity-driven exploration with goal-directed planning using visual observations and motion perception. It employs a multi-layered approach: from context to place to motion, allowing efficient navigation in new environments and rapid progress toward goals. They had a high success rate in goal-directed tasks with fewer steps required compared to other models. But, they only tested the validation of their method through simulations in a mini-grid environment. Nozari et al. [13] proposed a hybrid approach that combines active inference with IL to address the limitations of classical IL methods. Their framework uses a Dynamic Bayesian Network (DBN) to encode the expert's demonstrations and incorporates an active learning phase where the autonomous agent refines its policy based on new experiences. This method allows the agent to dynamically balance exploration and exploitation, improving its adaptability to complex and dynamic environments. However, their method uses DBNs which introduce significant computational overhead, which may not be feasible for all autonomous driving platforms, particularly those with limited processing power or energy constraints."}, {"title": "III. METHOD", "content": "Active inference is grounded in the FEP, which provides a mathematical framework for understanding how agents (biological or artificial) maintain a state of equilibrium with their environment. FEP is a fundamental principle that states organisms maintain their existence by minimizing their FE, which is a measure of surprise or prediction error. FE represents the difference between the predicted sensory inputs and the actual sensory inputs. Minimizing FE ensures that the organism's internal model remains consistent with its sensory experiences, promoting adaptive behavior. As the FE might be intractable an approximation of it is used which is called Variational Free Energy (VFE) and provides a tractable way to perform Bayesian inference by optimizing an approximate posterior distribution.\nThe VFE can be expressed as\n$F = D_{KL} (Q(S) || P(s | o)) \u2013 log P(o),$ (1)\nwhere $D_{KL}$ is the Kullback-Leibler divergence, Q(s) is the approximate posterior distribution over the hidden states s given the observations o, and P(s|o) is the true posterior distribution. The objective is to minimize the FE, ensuring that the approximate posterior Q(s) is close to the true posterior P(s|o), and reduces the surprise of observations under the model P(o). Active inference extends this concept by incorporating a generative model that allows the agent to predict future states and observations. This model includes prior beliefs about how the environment behaves, and the agent uses these predictions to minimize FE through perception (updating beliefs) and action (changing state to reduce surprise). The VFE is minimized through two processes:\n1) Perception (Inference): Updating beliefs to minimize prediction error.\n2) Action (Control): Selecting actions that minimize Expected Free Energy (EFE), which includes both epistemic (information-seeking) and pragmatic (goal-directed) aspects.\nIn this work we tried to minimize the VFE using the action, while considering the expected future state (future image for our case) constant based on the task we have in hand. This can be done with the minimization of Expected Free Energy (EFE). The EFE G at time t can be expressed as\n$G = E_{Q(o_t,s_t,a_t)} [log Q(s_t, a_t) \u2013 log P(o_t, s_t, a_t)],$ (2)\nwhere $E_{Q(o_t,s_t,a_t)}$ denotes the expectation under the approximate posterior distribution Q over the observations $o_t$, hidden states $s_t$, and actions $a_t$ at time t, $log Q(s_t, a_t)$ is the logarithm of the approximate posterior distribution over the hidden states $s_t$ and actions $a_t$ at time t, and $log P(o_t, s_t, a_t)$ is the logarithm of the true joint probability distribution over the observations $o_t$, hidden states $s_t$, and actions $a_t$ at time t. The equation essentially measures the expected divergence between the approximate posterior Q and the true joint probability distribution P over the variables of interest, guiding the agent's behavior to minimize this discrepancy, where the environment is considered as a Partially Observable Markov Decision Process (POMDP).\nIn autonomous driving, active inference involves the agent (the vehicle) continuously predicting its sensory inputs (visual observations of the road) and minimizing the prediction error through either perception (updating its internal model) or action (steering adjustments). We used a forward internal model [30] for the perception part and for the action we used sensory-motor simulation (use of covert action).\nPerception Model: For perception, we trained a forward internal model that contains all the information necessary for the agent to understand the environment in the context of the driving task. In this study, we assumed that the forward internal model, once learned offline, is comprehensive and does not require updating during the active inference agent training. This forward internal model is used to generate future images influenced by the steering angle. An overview of the forward internal model is depicted in Fig. 1. For learning forward internal model this paper uses a method inspired by [21]. As human beings, it is simple to predict how the environment will change based on the change of steering action as humans are really good at completely generating the missing information. However, it is not simple for a feed-forward neural network to predict details of the scene by providing one frame and the corresponding action when the future scene is too different from the current scene. The reason is that the network finds it hard to learn a good correlation between the current and the future scene even with the help of action. To the best of our knowledge, most literature available in future scene prediction based on action, considers normal driving scenarios [22] [23] or simple environments like Atari games [24] [25] in which the predicted images are still pretty much similar to the input image. Even for those cases, the models that are used cannot predict the future image without blurriness.\nFor the forward internal model, we used a U-Net Xception-style [31] model with modification for our task. We used the U-net [29] architecture due to multiple reasons. (i) UNet is known to performs well with relatively fewer training examples. (ii) It can efficiently extracts hierarchical features from input images as it uses convolutional layers in both the encoding and decoding paths. (iii) It uses skip connections which help in preserving spatial information from early layers. In our study, we observed increased blurriness when predicting future images for sharp steering angles, as the correlation between the input image and the prediction decreased. Since these predicted frames are crucial for solving the control problem, blurry images are not useful. Therefore, we cropped the image to focus only on the most informative parts relevant to decision-making, excluding areas unrelated to the driving task. We also considered using semantic segmented images and the road-only labels from these images. While semantic segmentation could potentially enhance the control task, it would introduce additional complexity and computational overhead that are unnecessary for low-level control tasks. Furthermore, using semantic segmentation to define preferred states (preferences [27]) presents challenges in incorporating environmental information unrelated to driving as this approach can make comparisons sensitive to changes in background scenes, such as variations between greenery or mountainous terrain along the road. So, we used road-only part of semantic segmented images for our control task.\nFor the data collection, a special strategy was used. As diverse data with various steering angles is needed to make the model learn about how steering affects the representation of the environment, the vehicle was driven in a zigzag pattern so that all different steering from -1 to 1 can be included in the data.\nWe used CARLA (Car Learning to Act) [26] simulator due to the realistic environment it provides for developing,"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We used a U-Net Xception-style model [28] in keras. The input image was encoded with the encoding part of the U-Net and then the action was added (which is only steering for in this case) by concatenating the action through Dense layers. For this task, the upsampling layers are changed to the conv2d_transpose layer as they can make the reconstructed image less blurry. In addition, as only the road part of the image is considered, grayscale images are used. So, one channel in both the input layer and output layer is used. The input and output images have the same size of 160 x 160 x 1. The RELU activation function is used for encoding and decoding layers and the ELU activation function is used for the part where the action was added. For the output layer, the sigmoid activation function was used. A learning rate of le-4 and batch size of 128 were used to train this model.\n2) Imitation Learning: We used the same size cropped images for training the BC agent as we used for the active inference agent. The model that was used for training the BC agent is shown in Table I. A learning rate of le-3 and batch size of 64 were used to train this model.\nIn this study, we adopt IL (more specifically Behavioral Cloning (BC) as our baseline method for several compelling reasons. (i) BC offers a straightforward approach to learning policies from expert demonstrations, requiring minimal additional infrastructure beyond the demonstration data itself. This simplicity not only facilitates rapid implementation but also serves as a foundational benchmark against which more complex algorithms can be evaluated. By establishing BC as our baseline, we aim to provide a clear reference point for assessing the efficacy of our active inference based method. (ii) As the BC only needs the camera images, we do not need to consider other sensors input and the comparison would be fair as both active inference method that we proposed in our work and the BC use the same sensor. (iii) Due to the use of the same sensor input (single camera) the size of the data for both methods can be the same.\nCARLA has different versions that provide different cities and structures. The stable version of CARLA which is 0.8.4 only supports two towns, but 0.9.X versions support more towns. As an example the version 0.9.14 that we have used in our work supports eight different towns. For the simulation, two different versions of the CARLA simulator were used. The version 0.9.14 is used for our test scenario which occurred in two different towns, Town04 and Town06. We compared the driving of our agent in Town04, Town06, and Town01 with an BC agent. For comparison, different tracks on Town01, Town04, and Town06 of CARLA simulator were randomly chosen and used. Here, three different tasks were considered for comparing different behaviors of the agents. For each of the tasks in Town04 and Town06 there are four different lanes that the car is tested in. The test in different lanes was done to see how robust the algorithm is against different lanes in the environment.\n1) Active Inference: We collected 36,000 images in Town06, and the data was augmented with the flipping and the change of the sign of regarding steering angle, which made"}, {"title": "V. RESULTS", "content": "Active inference proved to be a robust and adaptable agent for autonomous driving, as evidenced by the comprehensive analysis presented in Tables II and III. Across multiple tasks and urban environments, active inference consistently achieved superior performance compared to Behavioral Cloning (BC).In the tables deviation is the average value of the Euclidean distance between the vehicle's positions and the corresponding waypoints, and the success rate shows, across different tracks, how many tracks were successfully completed. For each track, four different experiments were conducted, and the success rate was computed based on the success of these experiments.\n1) Town01 Performance: In Town01, active inference excelled with lower deviations and a 100% success rate in all tasks. For instance, in the One-Turn task, active inference had an average deviation of 0.540, while BC faced difficulties, particularly in the Straight and Two Turns tasks with deviations of 0.978 and 3.418, respectively, and success rates of 0% and 25%.\n2) Town04 Performance: In Town04, active inference demonstrated consistent performance with lower deviations and higher success rates in all tasks compared to BC. For the Straight task, active inference maintained an average deviation of 0.812 units and a success rate of 100%, while BC had a deviation of 0.688 units with the same success rate. Although in the Straight task the average deviation was better for the BC agent, the difference is insignificant. In the One-Turn task, active inference achieved an average deviation of 0.944 units and a 100% success rate, significantly outperforming BC which had a deviation of 1.714 units and a 0% success rate.\n3) Town06 Performance (Training Town): In Town06, active inference maintained high performance with an average deviation of 0.573 units in the One-Turn task and a success rate of 100%. BC, on the other hand, had a deviation of 1.784 units for the same task but achieved a 100% success rate. For the Straight task, active inference recorded an average deviation of 0.701 units with a 75% success rate, whereas BC showed a higher deviation of 1.163 units with the same success rate.\nWe compared our results with different methods including MP, IL and RL which are introduced in CARLA simulator [26]. Moreover the results for papers CIRL [17], CAL [14], LBC [10] also shown in the Table IV. For these articles, Town01 is used as a training town and Town02 was used as a testing town. Although we simplified the problem, as this is a new method for learning and controlling the AV, we can assert that this new method can result in a comparable result as a more complicated method. Our method which follows the Active Inference principle is following a more human-like strategy with using the prior knowledge of how the world works by simulating the prior knowledge that humans can use as an imagination to see the effect of the action on the scene and deciding based on the desired future scene that they have in mind. Although based on the table, the result of our method for the straight task was better than IL and RL and slightly worse than the other methods, as we did not use any data from Town01 for training and these methods were using from this town for the training purposes, we can assert that our result is pretty good for this task. The results for Town02 shows that our agent driving is almost as good as CAL and MP methods and is better than the RL method. Although the result of our method is not as good as CIRL and LBC, the structure of our training is much more simpler than the one used in LBC as they use a two-stage training process that is more complex and resource-intensive. Also, CIRL uses a dual-stage learning process that introduces complexity in implementation and tuning. Balancing the IL and RL requires careful calibration to prevent issues like overfitting to the imitation data or insufficient generalization from the RL stage."}, {"title": "VI. CONLUSION", "content": "In this paper, we introduced a novel Active Inference-based method for controlling an autonomous vehicle, leveraging the Free Energy Principle (FEP) to dynamically adapt to different road types, including two-lane and multi-lane environments. Our method significantly enhances the adaptability and performance of AVs, achieving comparable results to more complex baseline methods in the CARLA benchmark.\nOur results demonstrate that the Active Inference agent, despite its simplicity, performs robustly across various urban environments and driving tasks. It consistently outperforms Behavioral Cloning (BC) in terms of lower average deviations and higher success rates in simulated environments. This highlights the effectiveness of our approach in mimicking human-like driving behavior through the continuous minimization of prediction errors.\nNotably, our method also showed competitive performance in the CoRL2017 benchmark, further validating its effectiveness. The success rates and deviation metrics indicate that our Active Inference-based agent can navigate complex scenarios effectively, maintaining human-like driving performance.\nFuture work will focus on extending the capabilities of our Active Inference-based agent to include tasks such as lane changing, handling dynamic objects in the driving scene, and adhering to traffic rules. Additionally, we aim to validate our approach using real-world data and explore its application to actual autonomous driving scenarios. By doing so, we hope to bridge the gap between simulation and real-world performance, ultimately advancing the field of autonomous vehicle control."}]}