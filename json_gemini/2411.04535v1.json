{"title": "Meta-Reasoning Improves Tool Use in Large Language Models", "authors": ["Lisa Alazraki", "Marek Rei"], "abstract": "External tools help large language models (LLMs) succeed at tasks where they would otherwise typically fail. In existing frameworks, LLMs learn tool use either by in-context demonstrations or via full model fine-tuning on annotated data. As these approaches do not easily scale, a recent trend is to abandon them in favor of lightweight, parameter-efficient tuning paradigms. These methods allow quickly alternating between the frozen LLM and its specialised fine-tuned version, by switching on or off a handful of additional custom parameters. Hence, we postulate that the generalization ability of the frozen model can be leveraged to improve tool selection. We present Tool selECTion via meta-reasONing (TECTON), a two-phase system that first reasons over a task using a custom fine-tuned LM head and outputs candidate tools. Then, with the custom head disabled, it meta-reasons (i.e., it reasons over the previous reasoning process) to make a final choice. We show that TECTON results in substantial gains \u2013 both in-distribution and out-of-distribution on a range of math reasoning datasets.", "sections": [{"title": "1 Introduction", "content": "Augmentation with external tools has proven effective at boosting the performance of large language models in knowledge-intensive tasks such as QA and math problem-solving (Hao et al., 2023; Paranjape et al., 2023; Parisi et al., 2022; Schick et al., 2023). Tools are self-contained programs or APIs whose execution timing and arguments are determined by the language model. To teach a model how to use tools, previous works adopt one of two main strategies: (1) tool demonstrations via in-context learning (Gao et al., 2023; Gupta and Kembhavi, 2023; Hsieh et al., 2023; Sur\u00eds et al., 2023), or (2) fine-tuning on a dataset where text samples are interleaved with tool annotations (Parisi et al., 2022; Schick et al., 2023; Tang et al., 2023). Hao et al. (2023) note that a major downside of in-context demonstrations is the finite size of the context itself, preventing the system from being able to scale up to a very large number of tools. The issue of scalability is not just theoretical: according to Patil et al. (2023), a general-purpose LLM may need access to thousands of tools to be able to carry out complex and diverse tasks (e.g., booking holidays or organizing conferences) from natural language instructions. In contrast, fine-tuned models are able to learn thousands of tools from annotated data (Patil et al., 2023). However, they are bound to the set of tools learned during training: adding further tools requires a new round of fine-tuning at significant computational cost (Hao et al., 2023). Recent works adopt parameter-efficient learning to reduce the cost of fine-tuning, thus facilitating potential future extensions of the tool set. Hao et al. (2023) develop ToolkenGPT by augmenting the output matrix of an LLM with additional special tokens, each representing a tool. Only the additional tokens are tuned while the rest of the weights remain frozen. Similarly, Wang et al. (2024) train tokens representing math operations to aid each reasoning step, using LoRA (Hu et al., 2022) to minimize the number of additional parameters. Qiao et al. (2024) implement a two-stage system called TRICE, where instruction tuning is followed by reinforcement learning from environmental feedback (RLEF), leveraging LoRA at both stages. An advantage of parameter-efficient paradigms is that they preserve most of the LLM's core capabilities acquired at pre-training (Ding et al., 2023; Han et al., 2024). These methods only tune a handful of additional parameters for a specific task, which can be selectively disabled to reinstate the original model. We argue that previous works exploring these techniques for tool generation do not fully leverage the generalization power of the underlying LLM. To address this issue, we propose a two-phase framework for Tool selECTion via meta-"}, {"title": "2 Method", "content": "We augment the language modeling head of a base model with additional token embeddings T to represent math operations, and train them via a standard language modeling objective. Existing works have shown the effectiveness of tuning additional tokens for math tasks (Hao et al., 2023; Wang et al., 2024) and general reasoning (Goyal et al., 2024; Herel and Mikolov, 2024). To take full advantage of both the specialized tokens and the generalist frozen LLM, we design a two-phase framework. The rest of the section describes this in detail."}, {"title": "2.1 Reasoning Phase", "content": "In the reasoning phase, we ask the LLM to solve a math problem step by step. Rather than greedily generate the most likely tool as in previous works (Hao et al., 2023; Wang et al., 2024), we leverage the augmented LM head to collect a set of candidate tools. To this end, we experiment with both temperature sampling and greedy decoding of mul-"}, {"title": "2.2 Meta-Reasoning Phase", "content": "In this phase, we let the LLM analyze its previous reasoning process and choose among the candidate tools. To this end, we use the frozen model without custom tokens. Pre-trained language models have been used for self-evaluation in previous literature (Alazraki et al., 2023; Shinn et al., 2023; Yao et al., 2023a). We experiment with two ways of eliciting meta-reasoning:\n1. We join each candidate tool with the previous text context, and present these as options for the LLM to score. We prefix each option with an uppercase letter label and select as the answer continuation the option whose label is assigned highest probability by the model, i.e., $\\arg \\max_{t_i \\in V_{\\text{sub}}} P(t_i | t_{<i} \\text{ with } t_{<i} \\in V)$, where $V_{\\text{sub}}$ denotes a subset of the vocabulary containing only the uppercase letter tokens that are in the label set. In this setup, which we call TECTON-SCORE, we limit the number of candidates to a maximum of four.\n2. We pass the candidate tools as hints and ask the model to generate an appropriate continuation of the answer. Here, the hints serve as mere guidance for the LLM (i.e., the model could choose to ignore all candidates and"}, {"title": "2.3 Bias Calibration", "content": "Upon running TECTON-SCORE without recalibration of the label probabilities, we find that the validation results are poor. Visual inspection of the samples reveals that the model assigns highest likelihood to the same label in most instances, as shown in Fig. 2. This is consistent with Zheng et al. (2024)'s finding that LLMs are prone to selection bias in multiple-choice tasks. To solve a similar problem, Duarte et al. (2024) measure their model's bias over the labels A, B, C, D using a set of neutral samples where a uniform distribution would be expected, and subtract that bias from each label's likelihood at inference time. Our math reasoning task does not lend itself to finding neutral samples, so we adopt a different strategy. Having created data samples of questions and options (by running the reasoning phase of TECTON on a validation set), we compute n! permutations of the n options while keeping the letter labels in the same position. We have the model score the labels for each permutation, and average over all permutations and all data samples to obtain an averaged biased distribution $B^{(n)}$. Since the number of options in our task is variable, we run this process independently for data samples with n = 2, n = 3 and n = 4 labels. At inference time, given a set of labels $L^{(n)}$ of size n, we retrieve the corresponding $B^{(n)}$ and compute the calibrated probability $p_i$ of each label $l_i$ in the"}, {"title": "set as", "content": "$p_i = p_i + \\frac{1}{n} - B^{(n)}_i$, where $p_i$ is the probability assigned by the model to label $l_i$ for the current sample, $B^{(n)}_i$ is the pre-computed biased probability of label $l_i$, and n is the total number of labels."}, {"title": "2.4 Retrieval of Tool Demonstrations", "content": "To aid answer generation in TECTON-GENERATE, we dynamically retrieve and add to the context few-shot exemplars demonstrating the candidate tools. We create the retrieval pool by extracting training samples and collecting candidate tools for each, by running the reasoning phase of TECTON. We construct each exemplar to simulate the inference task, as follows: (1) we append to the sample its set of candidate tools, and (2) we append to it the golden answer demonstrating how the correct tool is used to obtain the final solution. At inference time, we retrieve only the exemplars whose golden answers contain the tools currently in the candidate set. It is worth noting that dynamic retrieval was not included in TECTON-SCORE as it did not improve validation performance."}, {"title": "3 Experiments", "content": "Our system is model-agnostic and can be applied to any open-weights LLM. Here, we use Llama 3 8B Instruct (Dubey et al., 2024) (from here on referred to as Llama 3) as the base model in all experiments. Implementation details and hyperparameters are given in Appendix A."}, {"title": "3.2 Datasets", "content": "We train and evaluate TECTON on GSM8K-XL (Cobbe et al., 2021; Hao et al., 2023) and FuncQA (Hao et al., 2023). The test set of the latter is comprised of two distinct subsets: a 'one-hop' corpus containing problems solvable with one single operation (FuncQA-OH), and a \u2018multi-hop' one requiring multiple operations (FuncQA-MH). For GSM8K-XL we tune four additional tokens corresponding to the four basic operations, and extend these to 13 in the case of FuncQA. The complete set of tools for each dataset is shown in Appendix A. Additionally, we evaluate on out-of-distribution datasets that were not observed during fine-tuning. For this purpose we choose a range of math reasoning datasets: ASDiv (Miao et al., 2020), MAWPS"}, {"title": "3.3 Baselines", "content": "We implement recent tool-augmented models as baselines: TRICE (Qiao et al., 2024) and ToolkenGPT (Hao et al., 2023). These share a parameter-efficient approach with TECTON. Despite their relatively low computational cost, they have been shown to outperform strong systems: TRICE paired with Alpaca (Taori et al., 2023), Chat-GLM (Zeng et al., 2024) and Vicuna (Zheng et al., 2023) surpasses the much larger GPT-3.5 as well as tool learning via supervised fine-tuning. In addition to outperforming GPT-3.5, ToolkenGPT paired with LLaMA (Touvron et al., 2023) is more accurate than ReAct (Yao et al., 2023b). It should also be noted that LLMs are increasingly able to solve math problems and perform difficult arithmetic without tools, as shown by the relatively high accuracy (79.6%) obtained by Llama 3 on the non-enhanced version of GSM8K (Dubey et al., 2024). Therefore, we additionally compare against a vanilla version of Llama 3 as well as Chain-of-Thought (CoT) prompting (Wei et al., 2022) with exemplars extracted from the training set."}, {"title": "3.4 Results", "content": "Table 1 shows TECTON's gains on math reasoning. Both versions of the system achieve scores above baselines for in-distribution and out-of-distribution-data, with the exception of TECTON-GENERATE on GSM8K-XL, whose performance is slightly below that of ToolkenGPT. On GSM8K-XL, our best implementation scores 7.2 percentage points above TRICE and 2.3 above ToolkenGPT. When evaluated on in-distribution data, TEC-"}, {"title": "3.5 Ablations", "content": "To gain more insight into these results, we perform an ablation study on the meta-reasoning phase of TECTON, shown in Table 2. We ablate bias calibration from TECTON-SCORE and dynamic exemplar retrieval from TECTON-GENERATE. On average, TECTON-SCORE's ablated accuracy is 6.2 percentage points lower than the non-ablated version on in-distribution data, and 1.2 on unseen datasets. Additionally, TECTON-GENERATE's average performance drops by 7.2 on in-distribution data and 4.9 on OOD data. The most significant performance loss is on FuncQA: the ablated versions of TECTON-SCORE and TECTON-GENERATE see a decrease of 8.4 and 11.7 percentage points, respectively, on the one-hop test set. They also"}, {"title": "4 Conclusion", "content": "We introduce TECTON, a novel two-phase framework that first samples a set of candidate tools and then selects the optimal candidate via meta-reasoning. We implement two versions of the system and find that both achieve superior performance on math reasoning datasets, surpassing our strongest baseline by ~ 4% on in-distribution data and ~ 9% on unseen benchmarks, on average. These results confirm our hypothesis that a specialized, custom-tuned framework and a generalist pre-trained model can work together to improve tool use in challenging tasks."}, {"title": "Limitations", "content": "This paper solely focuses on math reasoning tasks. While this is consistent with established literature, there are other domains (e.g., knowledge-intensive QA, virtual environment navigation) that can benefit from the use of tools. Future work can investigate a wider range of tasks."}, {"title": "Ethical Considerations", "content": "We have verified that all datasets and software utilized in this paper allow for their use, distribution and modification. Our non-commercial purpose is consistent with all licenses. The distribution of our code and data is accompanied by the licenses and credits to the original authors."}, {"title": "A Model Implementation and Training", "content": "We do all training and inference on a single NVIDIA Tesla V100 GPU."}, {"title": "A.1 Tools", "content": "GSM8K-XL and FuncQA (Hao et al., 2023) are annotated with four and 13 tools respectively, each representing a math operation. We illustrate these in Table 3. Tools are trained as additional tokens added to the standard language modeling head of Llama 3, which comprises 128,256 token representations. Therefore, we extend these representations to 128,269 in the case of FuncQA and 128,260 for GSM8K-XL."}, {"title": "A.2 Details of the Training Process", "content": "The LM head of TECTON consists of the standard head of Llama 3 8B Instruct (Dubey et al., 2024) concatenated with an additional linear layer of size embedding_dimension \u00d7 number_of_tools. The tool token embeddings are randomly initialized and trained on math reasoning QA pairs from GSM8K-XL and FuncQA, following a standard language modeling objective. Both datasets are annotated with the token positions at which each tool should be generated. Note that the original datasets made available by Hao et al. (2023) are annotated according to SentencePiece tokenization (Kudo and Richardson, 2018). We edit the annotations to be compatible with Llama 3's Byte-Pair Encoding tokenizer (Sennrich et al., 2016). We tune two distinct sets of special tokens, one on each dataset, as each requires a different set of tools as shown in Table 3. We train at half precision (FP16) for ten epochs, using learning rates {1e-4, 1e-3}, batch size 1, and saving checkpoints at each epoch. We select the best checkpoint by measuring performance on a validation set."}, {"title": "A.3 Inference-time Hyperparameters", "content": "At inference, we decode with temperature t = 0, $p$ = 0.95, k = 5. In the reasoning phase, we apply logit bias to the tool tokens to promote their generation. We use logit bias 3.0 for GSM8K-XL"}, {"title": "B Tool Sampling Experiments", "content": "Temperature Sampling We experiment with temperature sampling and find that it is not an optimal strategy to gather candidate tools, as lower temperatures do not lead to enough diversity while higher ones generate irrelevant tools.\nTop-k Decoding We run ToolkenGPT (in its original implementation based on the first version of LLaMA) on the validation set of FuncQA. We find that, in samples where the system has generated an incorrect tool, the correct one \u2013 complete with correct arguments \u2013 is among the top five most likely tokens in over 60% of cases (we use greedy decoding as in the original paper). These cases include samples where the correct tool can be found by searching the top tokens at a different position from the one that has produced the incorrect tool. Overall, the system decodes the correct tool (regardless of the arguments it generates for it) in 76.9% of samples; this rises by over 10% to 87.2% when we consider the top k = 5 tokens at each decoding step (Fig. 3). Further increasing k to 10 incurs a higher computational cost without raising the proportion of decoded golden tools."}, {"title": "C Baselines Implementation", "content": "All our baselines use Llama 3 8B Instruct as the base model."}, {"title": "Llama 3 8B Instruct", "content": "We measure our base model's performance in the zero-shot setting. We experiment with CoT zero-shot prompting (prepending to the question the instruction Let's think step by step) but find that just using the raw question as input results in better performance."}, {"title": "Chain-of-Thought (CoT)", "content": "We extract from the training set six pairs of questions and answers demonstrating maths operations and their use. For comparability, we use the same exemplars as in the implementation of ToolkenGPT and TECTON. Note that Llama 3 8B Instruct achieves 79.6% accuracy (Dubey et al., 2024) on the non-enhanced version of GSM8K when prompted in few-shot CoT fashion. Our experiments show that it performs significantly worse (37.3%) on GSM8K-XL, highlighting the difficulty of the enhanced dataset."}, {"title": "TRICE", "content": "We train TRICE using the same hyperparameters as in the original implementation, with the exception of the batch size in the first phase of training, reduced to 128 with eight gradient accumulation steps due to the memory limitations of our hardware. Since we train on a single dataset (GSM8K-XL), our implementation corresponds to the setup referred to as TRICE-SPLIT in the original paper. Note that Qiao et al. (2024) implement TRICE on top of Alpaca (Taori et al., 2023), Chat-GLM (Zeng et al., 2024) and Vicuna (Zheng et al., 2023). For comparability with TECTON and the other baselines, here we use Llama 3 as the base model. We thus adjust TRICE's prompt templates and special tokens to be consistent with Llama 3's model card\u00b9."}, {"title": "ToolkenGPT", "content": "We implement ToolkenGPT with Llama 3 and train it on the same annotated datasets as we train TECTON, using the same sets of tools and the same hyperparameter combinations for direct comparability."}, {"title": "D Out-of-distribution Datasets", "content": "We enhance the test sets of ASDiv, MAWPS and SVAMP and obtain 'XL' versions of each. Our goal is to replace the numbers in each test sample with larger-magnitude ones in an automated manner, yet ensuring that the new numbers in each question"}, {"title": "E Prompts", "content": "In the reasoning phase, we prompt TECTON to generate an answer to a math reasoning question step by step, aided by in-context exemplars. Prompts 1 and 2 illustrate how we elicit tool choice in the meta-reasoning phase, for TECTON-SCORE and TECTON-GENERATE respectively. In both cases, we show the model in-context exemplars, followed by the current question and the current partial answer. The latter consists of the lines of text generated during the previous iterations of TECTON, if any. In Prompt 1, the placeholder [FIXED EXEMPLARS] is replaced with six in-context exemplars extracted from the training set. The number of options in each exemplar matches the number of available options in the current sample. In Prompt 2, we have [DYNAMIC EXEMPLARS] that are retrieved to demonstrate the tools currently in the candidate set. Here, the partial answer also includes the tokens generated during the current iteration, up to the sequence position where the first tool is found among the k top probability tokens. The hints are prepended to the partial answer in this setting."}]}