{"title": "A\u0130BAT: Artificial Intelligence/Instructions for Build, Assembly, and Test", "authors": ["Benjamin Nuernberger", "Anny Liu", "Heather Stefanini", "Richard Otis", "Amanda Towler", "R. Peter Dillon"], "abstract": "Abstract-Instructions for Build, Assembly, and Test (IBAT) refers to the process used whenever any operation is conducted on hardware, including tests, assembly, and maintenance. Currently, the generation of IBAT documents is time-intensive, as users must manually reference and transfer information from engineering diagrams and parts lists into IBAT instructions. With advances in machine learning and computer vision, however, it is possible to have an artificial intelligence (AI) model perform the partial filling of the IBAT template, freeing up engineer time for more highly skilled tasks. AiBAT is a novel system for assisting users in authoring IBATs. It works by first analyzing assembly drawing documents, extracting information and parsing it, and then filling in IBAT templates with the extracted information. Such assisted authoring has potential to save time and reduce cost. This paper presents an overview of the AiBAT system, including promising preliminary results and discussion on future work.", "sections": [{"title": "I. INTRODUCTION", "content": "At the National Aeronautics and Space Administration (NASA) Jet Propulsion Laboratory (JPL), the IBAT process is used to document how projects are fabricating, building, assembling, and testing their hardware [1], [2]. The IBAT process has been used extensively on missions such as the Mars Perseverance Rover [3], the Europa Clipper [4] mission, and many others. For example, IBATs are used when building printed wiring assemblies, such as the one shown in Figure 1. IBAT documents have three primary functions:\n1) providing a way for the user to plan operational steps to be performed on hardware,\n2) providing the instructions to follow during the execution of operations, and\n3) serving as a record of what was done.\nCurrently, the authoring of IBAT documents is time and labor intensive, as users must manually reference and transfer numerous amounts of information from engineering diagrams and parts lists into the IBAT template. With recent advances in AI, however, it is possible to have an AI model perform the partial filling of the IBAT template, freeing up engineer time for more highly skilled tasks.\nTo evaluate the viability of this approach, we developed a simple proof-of-concept system that automates locating and extracting information from assembly drawing documents and then assists in filling in IBAT templates from the Electronic Fabrication (EFAB) division at JPL. Due to their proprietary"}, {"title": "II. RELATED WORK", "content": "In this section, we described the related work in document understanding, assistive document authoring, and AI/ML for industrial use cases."}, {"title": "A. Document Understanding", "content": "The research field of automatically extracting information from documents via machine automated approaches has pre- viously been referred to as Document Analysis and Recog- nition [6]. More recently it has been discussed as Document Visual Question Answering (DocVQA) [7], Visual Document Understanding [8], and Visually Rich Document understand- ing [9], [10], [11]. On the one hand, the task of document understanding requires raw information extraction via optical character recognition (OCR) [12]; on the other hand, doc- ument understanding may also involve parsing tabular data, understanding figures, charts, and images, etc. Aballah et al. [13] describe how early methods typically relied on rule- based approaches, while the newest approaches have embraced the Transformer machine learning architecture [14]. Only recently have multimodal LLMs been applied to understanding engineering documentation [15]. As detailed in Section IV, we explored a variety of DocVQA models but ultimately relied on a custom rule-based approached for our prototype; future work will likely utilize the latest Transformer-based approaches."}, {"title": "B. Assistive Document Authoring", "content": "Assisting users in authoring documents comes in a variety of form factors. On the one hand, assistive document authoring may involve a highly structured approach of collecting data and filling in template documents. Achachlouei et al. [16], [17] present a comprehensive review of document automation techniques, noting the abundance of commercial software in the legal domain for this type of assistive authoring. On the other hand, assistive document authoring may also involve a less constrained approach, such as the system offering feedback and guidance or via the user asking the system to write text based on a simple prompt. In this regard, OpenAI has showcased the intriguing capabilities of LLMs to write essays to pass simulated exams, or to write descriptive text for images [5]. Products like Grammarly [18] have also gained much traction for generic assistive writing tasks.\nThe AiBAT system involves a semi-structured approach since the IBAT document (1) utilizes the inherent structure of assembly drawings (e.g., notes and tables and figures) and (2) may have a pre-defined template available for filling in data (as is the case with the EFAB division at JPL). In our current prototype implementation, we utilize \u201cgolden IBAT templates\u201d in our system; however, most IBAT processes do not have templates available and we leave generalizing the capability of the system to support this to future work."}, {"title": "C. AI/ML for Industrial Use Cases", "content": "There has recently been a large interest in applying AI for industrial use cases, especially in what is known as the 4th Industrial Revolution (or Industry 4.0) [19]. More recently, LLMs have been applied throughout the product development lifecycle [20], including design [15], [21] as well as with conversational assistants [22].\nJPL has recently investigated a variety of Industry 4.0 workflows. This includes immersive metaverse technologies, such as visualizing CAD models in augmented reality [23], im- mersively visualizing physics data for mission design [24], and using augmented reality for hardware maintenance [25], [26], [27]. We have also begun investigating a variety of use cases for generative AI for JPL, including science and industrial use cases [28], [29]. The AiBAT project can be considered an application of AI to this 4th Industrial Revolution in building spacecraft."}, {"title": "III. IBAT AUTHORING AND SYSTEM OVERVIEW", "content": "IBATs are a ubiquitous part of flight project work at JPL and used across the Lab (e.g., for EFAB, mechanical fabrication, environmental testing, etc.). Currently, IBATs are manually written by Subject Matter Experts (SMEs), often involving a repetitive, manual process of copying information from various sources (e.g., drawings, bill-of-materials, etc.) into the IBAT. For EFAB on Clipper [4], it is estimated to take 10-20 hours per IBAT draft and there are over 6,000 Clipper IBATs. Numerous SMEs report that a large portion of drafting an IBAT is the task of copying information, an ideal task for automation by AI/ML, which could free up SME cognitive load for more challenging tasks. The long-term impact is to reduce the time, effort, errors and, ultimately, cost associated with the IBAT creation process.\nFigure 2 describes the conceptual workflow of how we use LLMs to parse the assembly drawing note information and to insert the relevant information into an IBAT template. As previously noted, not all workflows utilize IBAT templates;"}, {"title": "IV. INFORMATION EXTRACTION", "content": "Due to export control restrictions, we were unable to test our specific data on the latest multimodal DocVQA models. How- ever, we conducted a preliminary investigation into how well similar, publicly available assembly documents could be un- derstood by some popular recent DocVQA models [30], [31], [32], [5]. Smaller and less accurate models failed quickly [30], [31]; however, the larger and more accurate models showed strong potential for this use case [32], [5]. We also found that commercial solutions showed very strong potential [33]. Due to export control restrictions and limited time, we decided to pursue a simple custom rule-based approach, as described in the next section."}, {"title": "B. Custom Approach", "content": "As noted previously, assembly drawing PDFs are considered the \"signed off\" authoritative documents between various organizations. While some PDFs may have selectable text that is directly extractable via PDF SDKs, there is no guarantee that a given assembly drawing PDF has that characteristic (in fact, one of our test PDFs fell into this category). Thus, we opted for the more general case of using OCR to extract the text from the assembly drawings.\nIn our custom approach, we first convert the assembly draw- ing PDF into a set of images using ImageMagick [34]. Then, to detect where the assembly notes are, we utilize Layout- Parser [35], using the Detectron2 architecture [36] and Faster R-CNN Model [37], trained on the TableBank dataset [38]. This effectively provides a cropped image of the assembly notes. We then apply a simple rule-based image processing algorithm to further crop out individual note images. We utilize OpenCV [39] to detect columns and rows based on simple rules such as ensuring that a certain percentage of consecutive pixels are white.\nAssembly drawings sometimes have flagged notes which are notes that have their number surrounded by a triangle shape; see Figure 4. We utilize two approaches to detect and remove these triangle shapes so that the final OCR can more accurately detect the note number. First, we utilize a contour detection approach [40], followed by a triangle approximation routine using the Ramer\u2013Douglas\u2013Peucker algorithm. This method works well for thick triangles. To remove the triangle, we simply draw a white triangle on top of any found triangles. The second approach, which we found to work better for thin triangles, is to perform a dilation and erosion operation. The dilation will remove any thin lines, while the subsequent erosion will effectively make remaining black pixels (text) be brought back to their original thickness. If there is a notable image difference between the original image and this processed image, we conclude that there must have been a triangle around the note number.\nFinally, OCR is performed on the final individually cropped (and triangle removed) note images using Tesseract [12], with the LSTM [41] configuration."}, {"title": "V. LLM NOTE PARSING AND FINAL STEP GENERATION", "content": "Due to the export controlled nature of most of our data, we opted to utilize on-premise LLMs for our prototyping [42], [43], [44]. We designed our system to be able to utilize various LLM frameworks, but mostly used llama.cpp [44] since it"}, {"title": "A. Note Parsing", "content": "In note parsing, we ask the LLM to output the following:\n\u2022 A list of actions, such as \"BOND\" or \"SOLDER\"\n\u2022 A list of information, such as statements referring to reference drawings or other documents\n\u2022 A list of entities, such as items, reference designators, tables, etc.\nAn example, abridged few-shot prompt is shown in Figure 5. We first write out some instructions to the LLM, as well as noting common actions and common reference designators. The few-shot examples then follow afterwards."}, {"title": "B. Final Step Generation", "content": "For final step generation, we utilize the parsed note and substeps of the IBAT template steps to output the final IBAT steps. For example, an \u201cUNDERFILL\u201d IBAT template step may include three substeps of (1) a text description of the action to perform; (2) a table with reference designator in- formation; and (3) details on the curing process. An example few-shot prompt is shown in Figure 6. Notice how we include an \"action\" in the IBAT template portion of the prompt to guide the LLM in what type of action to apply to the IBAT template. In some situations, we also provide a \"guidance\" field which helps guide the LLM to mitigate errors."}, {"title": "VI. EXPERIMENTAL EVALUATION", "content": "We tested our system on a set of 3 IBAT and assembly drawing pairs, which all use the same golden IBAT template:"}, {"title": "A. Information Extraction Results", "content": "Our note extraction approach achieved the following Char- acter Error Rates (avg, std):\n\u2022 Pair 1: 0.002577 (0.005249)\n\u2022 Pair 2: 0.001903 (0.005204)\n\u2022 Pair 3: 0.006926 (0.021729)\nAn example failure case is \"FOR U21\" detected as \"FORU21\"; with higher image resolution crops, we expect such errors to not be an issue in the future. The full extraction pipeline takes approximately 80s to complete on a MacBook M2 Max.\nOur triangle detection approach achieved the following accuracy results for correctly detecting if a note is a \u201cflagged\u201d note or not:\n\u2022 Pair 1: 100% (22/22)\n\u2022 Pair 2: 100% (18/18)\n\u2022 Pair 3: 95.5% (21/22)"}, {"title": "B. LLM Setup and Metrics", "content": "The following setup was used for LLMs in parsing notes and generating final IBAT steps. We utilized JPL's High Performance Computing (HPC) cluster GPU nodes which had two NVIDIA A100s per node, and used Mistral 7B [45] for our experimental evaluation.\nAccuracy was judged by a mechanical engineer SME, familiar with both the IBAT and assembly drawing documents, who categorized each LLM output (i.e., each parsed note or each substep) into the following result categories:\n1) R0: no errors\n2) R1: trivial error (e.g., whitespace difference, minor wording difference, etc.)\n3) R2: minor error (e.g., omitted relevant information, added unnecessary information, etc.)\n4) R3: major error (e.g., wrong info, misplaced info, etc.)"}, {"title": "C. Parsing of Notes Results", "content": "Automated LLM parsing of notes into steps, information, and entities took an average of 90s per assembly drawing."}, {"title": "D. Final IBAT Step Generation Results", "content": "To evaluate the final step generation, we utilized ground truth parsed notes, so as to analyze the accuracy of this part of our pipeline individually. Final step generation took an average of 40s per IBAT assembly drawing pair."}, {"title": "VII. DISCUSSION", "content": "First, we note that this initial prototype was only tested with three pairs of assembly drawings and IBATs, and that all pairs were taken from EFAB use cases. On the one hand, this limits the generalizability of the current AiBAT system; on the other hand, the overall workflow and system architecture are usable, making further handling of additional data doable.\nFor information extraction, our custom image processing approach was very accurate, and we were thus satisfied with it for the current prototype. However, future approaches could potentially move away from this custom approach to using multimodal foundation models that would likely be more generalizable to various assembly drawings beyond the EFAB ones.\nLLM note parsing results were mixed. While Pair 1 achieved strong %R01 results, Pair 2's results were poor. Most R3 results from Pair 2 were due to missing actions, information, or entities. Possible improvements may come from more ac- curate LLMs or better few-shot prompting. In addition, having additional data readily available can strengthen our few-shot prompts to generalize our system further to a variety of IBAT use cases. Using more sophisticated prompting techniques, including dynamically created prompts via retrieval augmented generation (RAG) [47], should also improve accuracy and generalizability. If necessary, fine-tuning of models may be done as well.\nFinally, LLM final step generation results showed extremely promising potential, with only a few R3 results and high %R01 scores. Ways to improve here are similiar to that of improving note parsing, such as using newer and more accurate LLMs, strengthening few-shot prompts, using more sophisticated prompting techniques, and fine-tuning models as necessary."}, {"title": "B. Risks", "content": "There are several risks with applying AI to assist in authoring IBATs. This includes the risks of confabulations, cybersecurity issues, and the possibility of insufficient data.\nFirst, the risk of confabulations (more popularly known as hallucinations) [48], [49] refer to the risk of the LLM to generate a false but plausible sounding response. In the context of build, assembly, and testing of spacecraft hardware, incorrect instructions could lead to hardware damage and personnel safety risks. While the likelihood of IBAT authoring errors is low with the current manual authoring process, the severity level can be high due to the sensitive nature of space- craft hardware. Thus, there are already safeguards in place in the existing IBAT process to reduce such risk, including via reviews by the IBAT author's organization as well as by the Quality Assurance (QA) organization. In this regard, the AiBAT system could still utilize these existing safeguards to migitate the risk of LLM confabulations. In addition, the time savings introduced by AIBAT would potentially allow for additional review time by SMEs to ensure IBAT correctness. Finally, future work should investigate how to determine if the AiBAT system is accurate enough for production deployment (e.g., should the %R01 metric scores be above a certain threshold?).\nAnother major risk is related to cybersecurity. If the AiBAT system eventually utilizes external LLM services, there is a risk of unauthorized access to data in-transit to and from the LLM service, as well to data at-rest if it is cached in the LLM service. To mitigate these risks, researchers have been studying various approaches to either encrypt or sanitize data before sending it to the LLM [50], [51].\nFinally, there is the risk that we may not have sufficient, quality data to deploy AiBAT more broadly. On the one hand, due to the nature of building custom spacecraft, sensors, and instruments, every project worked on at JPL is very different, which means that assembly drawings and IBATs may be very different. On the other hand, the low-level tasks remain similar (e.g., soldering) and thus we can still utilize previous assembly drawing notes and IBAT steps in few-shot prompts. However, every engineer inherently words things differently from other engineers; and in some cases, assembly drawing notes and/or IBAT steps may be poorly (or incorrectly) worded, in which case we would ideally not use those in few-shot prompt exam- ples for our system. In addition, sometimes IBAT documents get redlined or reworked, and this therefore reduces the amount of available, quality data. Overall, we currently believe this risk to be low, but we will have to reevaluate in future work by examining more assembly drawing and IBAT document pairs."}, {"title": "C. Cost", "content": "While the current AiBAT prototype runs on-premise, we put together a cost estimation for running this on Microsoft Azure's OpenAI platform\u00b9. Assuming GPT-40 costs from September 2024 ($0.005 per 1K prompt tokens, $0.015 per 1K completion tokens) [52], the cost to parse notes is approx- imately $0.039 per step (tokens: 7.2K prompt, 200 completion) and the cost to generate final steps is approximately $0.024 per step (tokens: 4.2K prompt, 200 completion). Assuming per IBAT/assembly-drawing we parse around 20 notes and convert around 10 steps, the total cost is approximately $1 per IBAT/assembly drawing pair. NASA JPL produces around 7,000 IBATs per year, which translates to around $7,000 per year in AiBAT LLM cost if this service were to be implemented at scale. On the one hand, the cost could go up due to more sophisticated prompt engineering, longer few-shot prompts, more expensive models, etc. On the other hand, the cost could go down due to algorithm optimizations, cheaper models, etc. We currently estimate a potential of $1.25M saved per year (minus $7K cost from AiBAT) if we could deploy"}, {"title": "D. Future Work", "content": "We note three main directions for future work. First, pro- grammatically or dynamically updating few-shot prompts has been shown to improve accuracy and may help generalize AiBAT further [53], [54]. Second, we plan to expand our testing beyond EFAB data to increase the generalizability of our system. Third, we plan to broaden the information extraction coverage to include tables, bill-of-materials, and figures. Finally, we note that accurate parsing of assembly drawing notes could open the way for automated authoring of immersive instructions, which has been explored previously for augmented reality [55]."}, {"title": "VIII. CONCLUSION", "content": "We presented AiBAT, a novel system that can extract information from assembly drawing documents, parse that information, and then utilize it to assistively author steps in the Instructions for Build, Assembly, and Test process. Results show strong potential for accurate assistive authoring of IBAT steps, which can lead to engineer time saved, ultimately freeing up their time for handling more cognitively demanding tasks."}]}