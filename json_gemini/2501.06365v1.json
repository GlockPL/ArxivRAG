{"title": "Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts", "authors": ["Elizabeth J. Schaefer", "Kirk Roberts"], "abstract": "This paper presents a pipeline for mitigating gender bias in large language models (LLMs) used in medical literature by neutralizing gendered occupational pronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to identify and modify pronouns tied to professions. We developed a BERT-based model, \u201cModern Occupational Bias Elimination with Refined Training,\u201d or \u201cMOBERT,\u201d trained on these neutralized abstracts, and compared its performance with \u201c1965Bert,\u201d trained on the original dataset. MOBERT achieved a 70% inclusive replacement rate, while 1965Bert reached only 4%. A further analysis of MOBERT revealed that pronoun replacement accuracy correlated with the frequency of occupational terms in the training data. We propose expanding the dataset and refining the pipeline to improve performance and ensure more equitable language modeling in medical applications.", "sections": [{"title": "Introduction", "content": "Background\nLarge language models (LLMs) are now widely used for a range of applications, from creating customer service chatbots to advertising that targets specific clients to predicting financial outcomes from potential economic indicators. LLMs have also increased in presence in the medical sector, ranging from accessible diagnostics to comprehensive literature retrieval, where they hold the promise of leading to a more informed level of care. Given the critical nature of these uses, it is essential to ensure that such LLMs remain free from biases that could potentially impact patient treatment and outcomes.\nDespite their potential, though, many LLMs have been shown to contain and perpetuate biases 1\u20135. The presence of these biases in LLMs is especially concerning in medical applications, where it can lead to incorrect diagnoses, inappropriate treatment recommendations, and ultimately, unequal healthcare. For example, an LLM fine-tuned on a dataset like PubMed might provide biased diagnostic suggestions if the underlying data contains gendered stereotypes. Gender biases and their effects have already been highlighted in a range of medical practice cases, for topics that include generalized surgical procedures, psychiatry, kidney transplantation, and intensive care treatment, among many other areas 6\u20139. Our research focuses specifically on occupational bias in conjunction with gendered pronouns, highlighting the underrepresentation and exclusion of women from traditionally male-dominated professions, a critical area given the concomitant distortion that can result from that in patient care decisions. Numerous instances of this bias are evident in the PubMed dataset. Instead of perpetuating these inequalities, a properly and rigorously trained LLM can mitigate and avoid such dangerous generalizations.\nIn this paper, we propose a novel approach to mitigate gender bias in LLMs used in medical contexts. To ensure that only relevant pronouns are neutralized without affecting critical medical details, our pipeline specifically targets pronouns that refer solely to occupations. This process preserves medically significant context, such as patient-specific information, while eliminating biased language tied to occupational stereotypes.\nOur method focuses on addressing biases in the training data before the fine-tuning stage of LLM development. By constructing and validating a robust query pipeline that identifies and neutralizes binarily-gendered pronouns linked to occupational terms in medical literature, we aim to create more reliable and fair models. This pipeline includes several key components: a lexicon for identifying gender-specific pronouns, a pronoun resolution query using Llama-3.1, and a classification query to identify occupation-specific subjects. The effectiveness of this approach is demonstrated through the development and evaluation of a BERT-based model, \u201cModern Occupational Bias Elimination with Refined Training,\u201d or \u201cMOBERT,\" trained on gender-neutralized abstracts from PubMed.\""}, {"title": "Related Work", "content": "The issue of bias in word embeddings and LLMs has been widely studied 10, with researchers highlighting how models trained on human-generated corpora often reflect and amplify societal stereotypes 11,12, as well as proposing both tech- nological and social solutions. Bolukbasi et al. 13 first demonstrated that word embeddings could capture and propagate gender biases, showing that terms like \u201cprogrammer\u201d were more closely associated with men than women. Subse- quent research provided further examples of such biases and explored their widespread implications in the field14\u201317.\nTo counter this effect, Bolukbasi et al. proposed a post-processing technique to debias word embeddings by project- ing gender-neutral words into a subspace orthogonal to a gender direction 13. However, their method had limitations, such as requiring a classifier to identify gender-neutral words, which could introduce errors and propagate bias if the classifier itself was flawed.\nBuilding on this, Zhao et al. 18 introduced Gender-Neutral Global Vectors (GN-GloVe), a method that embeds gender information into specific dimensions of word vectors while neutralizing others. This approach improved interpretabil- ity and allowed for more effective debiasing by focusing on protected attributes like gender. However, while GN-GloVe effectively reduced direct gender bias, it still left room for improvement in terms of indirect bias and applicability to contextualized word embeddings.\nMore recently, the focus has shifted to contextualized word embeddings, such as those used in LLMs like BERT and GPT. Studies by Basta et al.19 and Zhao et al.20 have explored gender bias in these models, finding that while contextualized embeddings reduce some biases present in static embeddings, they still retain significant levels of bias, especially in how they handle occupations and pronouns in context.\nIn line with these concerns, recent work has shifted attention toward non-binary and gender-neutral pronouns, as more individuals identify outside the binary gender framework. Although much of the previous research focused on binary gender categories, studies such as Hossain et al.21 have revealed that large language models struggle significantly with gender-neutral and neo-pronouns, like \u201cthey\u201d or \u201cxe.\u201d This highlights the broader issue of representation in train- ing corpora, where non-binary pronouns are often underrepresented, exacerbating the model's difficulty in handling inclusive language effectively.\nOur work builds on these foundations but diverges in its focus on preemptively addressing bias in the training data itself, particularly within the medical domain. By developing a pipeline that identifies and neutralizes gendered lan- guage before fine-tuning, we aim to create LLMs that are not only less biased but also more effective in delivering equitable healthcare outcomes. Unlike previous efforts that focused on post-processing or debiasing at the embedding level, our approach integrates bias mitigation into the model training process, addressing both direct and indirect biases more comprehensively."}, {"title": "Methods", "content": "Data\nOur data are taken from the MEDLINE 2019 baseline set of PubMed abstracts from 1965 through mid-2018, totaling approximately 29 million abstracts. To locate relevant abstracts for our study, we utilized two lexicons. The first lexicon searched for binarily-gendered pronouns, such as \u201chim,\u201d \u201cher,\u201d and \u201chimself,\u201d ensuring that only complete words were identified. This process reduced the initial set of 29 million abstracts to 687,000 relevant abstract instances.\nEach instance in our corpus represents the character offset of each pronoun found within an abstract. This means that if an abstract contains three gendered pronouns, it will appear in our baseline corpus three times, once for each pronoun occurrence. This approach is crucial for determining the specific pronoun resolution in each instance, as different resolutions may occur within a single abstract.\nAnnotation Process\nEach randomly selected pronoun mention, along with its containing abstract, was uploaded into LabelStudio as part of the broader corpus. The annotation process involved first identifying the noun phrase to which the pronoun referred (defined as \"pronoun resolution\") and then classifying that antecedent within the context of the abstract according to the established classification rules. Those rules were set as seen in Table 1."}, {"title": "Pronoun Resolution Query", "content": "Our pipeline initially employed a Llama-3.1-405b query to extract the subject associated with each pronoun (pronoun resolution). A randomly selected corpus of 500 pronoun instances was chosen from the relevant abstracts and each pronoun's respective antecedent was located and double-annotated with a Cohen's Kappa of 0.9000. Selected examples and the overall makeup of this corpus can be seen in both Figure 1 and Table 2. The Llama-3.1 query was then run on the non-annotated corpus and the pronoun instances and their respective antecedent query output were cross-referenced with the ground-truth annotations. The numerical outcome of this process is shown in Table 3."}, {"title": "Lexicon Validation", "content": "Once the accuracy of the pronoun resolution query is verified, a lexicon must be designed to identify occupational antecedents with greater frequency. The fidelity of the lexicon must also be evaluated in an iterative manner, through applying it to search for occupational terms within 500 antecedents identified by the pronoun resolution query, the process for which is described below.\nBased on observations made in the pronoun resolution query stage, it was found that a lexicon was needed that was specifically designed to identify occupational antecedents with greater frequency. In order to ensure the broad applicability of this lexicon to the relevant sample set, it was initially derived from the synset relations of \u201cprofessional\" in WordNet and then refined to include case-sensitive acronyms (e.g., \u201crn\u201d versus \u201cRN\u201d). To validate the lexicon for this use, we applied it to search for occupational terms within the 500 antecedents identified by the pronoun resolution query. The filtered results were then compared to the \u201coccupation\u201d labels in the ground-truth annotations. In our case, the application achieved a perfect recall score of 1.0000, confirming that the lexicon reliably identifies occupational antecedents. This validation supports its further use in refining and testing the pipeline on relevant data."}, {"title": "Classification Query", "content": "Using the validated lexicon described above, the results of our antecedent query can be successfully filtered. In this application, 250 pronoun instances were extracted, along with their corresponding antecedents, that included occupational terms. These 250 instances and the text of the abstract in which they appeared must also be examined and tested for accuracy. Those instances were double-annotated and reconciled with a Cohen's Kappa of 0.9470. After the annotations were completed, an additional Llama-3.1-405b query must then be employed to similarly label each antecedent in the context of its abstract. Precision, recall, and the F1 score were calculated between the generated labels and the ground-truth labels. The numerical outcome of this process is shown in Table 4 and an example of an antecedent versus classification query is given in Table 5. The primary focus of the classification query is identifying instances of occupations. The other categories were annotated to provide context for pronoun usage distribution, and offer potential for deeper analysis in future work."}, {"title": "Results: Application of the Pipeline 1965-1980", "content": "Pronoun Neutralization Case Study\nWe tested the effect of our pipeline on a corpus of the 379,000 PubMed abstracts from 1965-1980, hypothesizing that these texts would show a greater prevalence of singular gendered pronouns, based on a qualitative examination of a random sample set of the abstracts. After processing this corpus through our pipeline, the output consisted of all of the abstracts containing singular gendered pronouns referring to a labeled occupational antecedent, along with the pronoun character offsets. These were the pronouns deemed acceptable to neutralize as they are solely linked to an occupational term, and the abstract will not lose any medically necessary context (e.g., the gender of a trial participant).\nTo neutralize these pronouns, NLTK resources were downloaded and used for text tokenization and part-of-speech tag- ging. A pronoun-mapping dictionary was defined to convert gender-specific pronouns to gender-inclusive equivalents, with functions set up to identify, classify, and replace pronouns within the text. This dictionary also accounted for compound pronouns, such as \u201che or she\u201d or \u201chis/her,\" and grouped them for replacement accordingly. Gender-specific pronouns (both compounded and non-compounded) at the given offsets were replaced with the gender-inclusive coun- terparts \"they/them/theirs.\" Examples of phrases that would or would not be tapped for replacement, and the resulting modifications, are shown in Table 6.\nTo determine the success of this replacement, we trained two separate base uncased BERT models. The first model, named 1965Bert, was trained on the original, unmodified dataset of the 379,000 PubMed abstracts from 1965-1980. The second model, denoted \"Modern Occupational Bias Elimination with Refined Training,\u201d or \u201cMOBERT,", "physician,": "surgeon,\u201d \"doctor,", "practitioner,\u201d and \u201cnurse.": "oth models were trained for 3 epochs with a batch size of 4 per device, using a mixed precision (fp16) configuration across multiple GPUs. Training logs were saved at regular intervals, with models checkpointed every 10,000 steps.\nTo further assess the models, we conducted a masked language modeling test using 50 sentences from our initial annotated corpus of 500 abstracts, ensuring that each randomly selected sentence contained gendered pronouns from post-1980 texts. Importantly, the models were not trained on the data used in these tasks, ensuring an independent evaluation of their performance. The testing corpus was assembled by selecting ten sentences for each of the five most frequent occupational terms identified, resulting in 50 sentences. In each sentence, a [MASK] token was inserted in place of a pronoun, and the model was tasked with predicting the correct pronoun when given respective options of he/him/his, she/her/hers, and they/them/theirs."}, {"title": "Outcomes", "content": "We compared the results of this masking test between Bert-Base (the original untrained model), and microsoft/Biomed- NLP-PubMedBERT-base-uncased-abstract, 1965Bert, and MOBERT (all three of which are trained upon Bert-Base with their respective training data). Examples of this masking test and the corresponding outcomes can be seen in Table 7, with overall results shown in Table 8. Percentages are to indicate the share of sentences replaced with a gender inclusive pronoun of they/them/theirs. For example, if Bert-Base replaces 40% of masked pronoun instances with a gender inclusive pronoun, 1965Bert replaces 4% of those same instances with a gender inclusive pronoun.\nThe MOBERT results were further analyzed to determine a relationship between the frequency of the occupational term in the training data and the accuracy of replacement, as shown in Table 9."}, {"title": "Discussion", "content": "The application of our gender-neutralization pipeline to the 1965-1980 PubMed abstracts has demonstrated its potential to significantly reduce occupational gender bias in large language models. By introducing gender-neutral pronouns reconciled with occupational terms in 1,400 abstracts, we successfully trained a model, MOBERT, that demonstrated a 70% success rate in predicting inclusive pronouns in a masked language modeling task. This result far exceeds the 4% success rate of 1965Bert, a model trained on unmodified texts from the same period, and highlights the importance of correcting biased data at the training stage. MOBERT's performance also surpassed that of both the base model, Bert- Base, which exhibited a 40% success rate, and Full PubMedBert, a model trained on the complete PubMed dataset without gender-neutralization, which achieved only a 20% inclusive successive rate. These comparisons underscore the critical role of targeted intervention in mitigating bias in language models.\nOur results confirm the hypothesis that training models on biased, gendered data leads to a decline in their ability to generate gender-neutral pronoun predictions for occupations as subjects. The 1965-1980 dataset, which had a notable prevalence of gendered language, resulted in 1965Bert showing a marked decrease in its ability to predict inclusive pronouns. Conversely, MOBERT's improved performance illustrates that interventions like ours can substantially counteract this bias. This suggests that gender-neutralization efforts in model training can directly enhance a model's ability to use inclusive language, particularly in domains such as healthcare, where the presence of occupational gender bias could potentially skew diagnoses and treatment recommendations.\nDespite the overall success of MOBERT, our analysis did reveal some limitations. As seen in Table 8, the frequency of occupational terms in the dataset correlated strongly with the accuracy of pronoun replacement. For instance, terms like \"physician\" and \"surgeon,\" which appeared more frequently in the training data, saw a 100% accuracy in neutral pronoun predictions, while terms like \"nurse\" had a much lower replacement rate of 30%. This indicates that the more exposure the model had to gendered language associated with a particular occupation, the more effectively it could neutralize pronouns related to that term. This suggests that expanding the dataset to include a broader range of occupations and more balanced representation of male- and female-dominated roles could further improve the model's performance.\nAnother potential limitation arises from the process of language alteration itself. Although we carefully designed our pipeline to neutralize pronouns only in contexts where the occupational term was the antecedent, there remains a risk that some instances of gendered language with medically significant context may have been inadvertently modified. While we found no evidence of such errors in our testing, further refinements to the pipeline could include more sophisticated contextual analysis to ensure the protection of patient-specific or trial-related information.\nIn future work, we plan to expand our study to process the entire PubMed dataset, which will allow us to analyze a wider range of occupational terms and scenarios. This would provide a more comprehensive understanding of how LLMs can be trained to avoid occupational gender bias across a diverse range of medical contexts. Additionally, applying this pipeline to more recent texts would enable us to track changes in gender bias over time and assess whether such biases persist in more contemporary medical literature."}, {"title": "Conclusion", "content": "This work demonstrates the effectiveness of a gender-neutralization pipeline in reducing occupational gender bias in large language models trained on medical literature. By processing 379,000 PubMed abstracts from 1965-1980 and targeting gender-specific pronouns linked to professions, we improved MOBERT's success rate to 70% in predicting gender-neutral pronouns, compared to 4% for 1965Bert. This improvement highlights the importance of addressing bias during training. While promising, the study also reveals opportunities for improvement, such as expanding the dataset and refining the pipeline to ensure complete accuracy. These findings underscore the potential for creating more equitable and unbiased models in medical and other sensitive domains."}]}