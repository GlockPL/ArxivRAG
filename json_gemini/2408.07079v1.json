{"title": "Anatomical Foundation Models for Brain MRIS", "authors": ["Carlo Alberto Barbano", "Matteo Brunello", "Benoit Dufumier", "Marco Grangetto"], "abstract": "Deep Learning (DL) in neuroimaging has become increasingly relevant for de-\ntecting neurological conditions and neurodegenerative disorders. One of the most\npredominant biomarkers in neuroimaging is represented by brain age, which has\nbeen shown to be a good indicator for different conditions, such as Alzheimer's\nDisease. Using brain age for pretraining DL models in transfer learning settings\nhas also recently shown promising results, especially when dealing with data\nscarcity of different conditions. On the other hand, anatomical information of\nbrain MRIs (e.g. cortical thickness) can provide important information for learning\ngood representations that can be transferred to many downstream tasks. In this\nwork, we propose AnatCL, an anatomical foundation model for brain MRIs that\ni.) leverages anatomical information with a weakly contrastive learning approach\nand ii.) achieves state-of-the-art performances in many different downstream tasks.\nTo validate our approach we consider 12 different downstream tasks for diagnosis\nclassification, and prediction of 10 different clinical assessment scores.", "sections": [{"title": "1 Introduction", "content": "Magnetic Resonance Imaging (MRI) is the foundation of neuroimaging, providing detailed views of\nbrain structure and function that are crucial for diagnosing and understanding various neurological\nconditions. However, the complexity and high dimensionality of MRI data present significant\nchallenges for automated analysis. Traditional machine learning methods often require extensive\nmanual annotation, which is both time-consuming and expensive. Recently, contrastive deep learning\ntechniques have emerged as powerful tools for extracting meaningful information from complex data,\nshowing promise in improving the automated analysis of neuroimaging datasets.\nContrastive learning methods, particularly those leveraging supervised contrastive loss functions, have\nbeen effective in creating models that learn robust representation spaces. These methods typically\nutilize metadata, such as patient age, to guide the learning process, with the aim of aligning similar\ndata points (i.e. similar age) and distinguishing dissimilar ones in the representation space. While\nthis approach has yielded promising results, it may fall short of fully capturing the multifaceted\ninformation inherent in MRI data and the anatomical complexity of the brain.\nFor this reason, in this paper, we extend the capabilities of existing contrastive learning approaches\nby incorporating additional anatomical measures into the loss formulation. We propose AnatCL, a\ncontrastive learning framework that builds upon previous works that leverages patient age as proxy\nmetadata [12, 1] to also include information derived from anatomical measures (such as cortical\nthickness), computed on region of interests (ROIs) of common atlas such as the Desikan-Killiany\nparcellation [7] Specifically, we propose a modified contrastive learning loss function that integrates\npatient age with a targeted selection of three anatomical features: Mean Cortical Thickness, Gray"}, {"title": "2 Related Works", "content": "Machine learning on structural brain imaging. Current literature has been mainly devoted\nto segmentation tasks for brain anatomical data, such as brain tumor segmentation [2] or tissue\nsegmentation [10], in a supervised setting requiring annotated maps. The deep neural network\nU-Net [25] and its extension nnU-Net [18] were very successful at such tasks. Another body of\nliterature tackled phenotype and clinical prediction tasks from structural imaging, such as age\nprediction [14], Alzheimer's disease detection [31], cognitive impairment detection, or psychiatric\nconditions prediction [22, 4]. Current approaches mostly involve traditional machine learning models\nsuch as SVM or penalized linear models trained in a supervised fashion to predict the targeted\nphenotypes. Since most studies only include a few hundred subjects in their datasets, larger models\nintroducing more parameters (such as neural networks) do not necessarily translate into better\nperformance [11, 26, 16], probably because of over-fitting.\nSelf-supervised learning on brain imaging. Self-supervised learning has been popularized by its\nimpressive performance in Natural Language Processing (NLP) using auto-regressive models trained\non web-scale text corpus and mainly deployed on generation tasks [8, 3]. Since it does not require a\nsupervised signal for training, it is particularly appealing when large-scale data are available, and the\nannotation cost is prohibitive. Its applicability on neuroimage data is still unclear and very few studies\nhave tackled this problem so far [17]. [12] demonstrated that contrastive learning integrating subjects\nmeta-data can improve the classification of patients with psychiatric conditions. [29] demonstrated\nthat learning frameworks from NLP, such as sequence-to-sequence autoencoding, causal language\nmodeling, and masked language modeling, can improve mental state decoding from brain activity\nrecorded with functional MRI.\nContrastive Learning on brain imaging Recently, interests in contrastive learning approaches\nfor brain age prediction has risen. Among different works we can identify y-Aware [12] and"}, {"title": "3 Materials and Methods", "content": "3.1 Experimental data\nWe use a collection of T1-weighted MRI scans comprising 7,908 different individuals, for a total of\n21,155 images. The experimental data is gathered from different publicly available datasets, and we\ntarget 5 different neurological conditions: healthy samples (OpenBHB [13]), Alzheimer's Disease\n(ADNI [23] and OASIS-3 [20]), schizophrenia (SchizConnect [30]) and Autism Spectrum Disorder\n(ABIDE I [9]). The overall composition of the cohorts used in this study is presented in Fig. 1B.\nTab. 1 provides an overview of all the downstream tasks that we explore in this work.\nOpenBHB OpenBHB is a recently released dataset that aggregates healthy control (HC) samples\nfrom many public cohorts (ABIDE 1, ABIDE 2, CoRR, GSP, IXI, Localizer, MPI-Leipzig, NAR,\nNPC, RBP). Every scan comes from a different subject. We use OpenBHB as pretraining dataset\nfor our method. Besides the structural scans and patients information, OpenBHB also provides\n7 anatomical measures according to the Desikan-Kiliani parcellation [7] (cortical thickness mean\nand std, gray matter volume, surface area, integrated mean, gaussian curvature index and intrinsic\ncurvature index [13]).\nADNI and OASIS-3 We include all phases ADNI-1, ADNI-2, ADNI-GO and ADNI-3 in our\nstudy, amounting to 633 HC, 712 MCI and 409 AD patients. For OASIS3 we include 685 patients\nrespectively, containing 88 AD cases.\nSchizConnect We include anatomical MRIs for a total of 383 patients, divided in 180 HC, 102\nwith schizophrenia (broad), 74 with schizophrenia (strict), 11 schizoaffective patients and 9 with\nbipolar disorder.\nABIDE I We include anatomical MRIs for a total of 1102 patients, divided in 556 HC, 339 patients\nwith autism, 93 patients with Asperger's Syndrome, and 7 with pervasive developmental disorder not\notherwise specified (PDD-NOS).\nClinical assessment scores In addition to predict the above diagnosis, we also consider some\nrelevant clinical assessment included in the data, as reported in Tab. 1. For SchizConnect, we consider\nthe Abormal Involuntary Movement Scale (AIMS) evaluated in three assessments (overall, uppder\nbody, lower body), a Depression score based on the Calgary Scale, Handedness information, and\nGAIT measuremens with the Simpson-Angus-Scale (SAS). For ABIDE, we consider also handedness\nand three IQ scores, namely Full Scale IQ (FIQ), Visual IQ (VIQ) and Performance IQ (FIQ)\nmeasured with the Wechsler Abbreviated Scale (WASI).\n3.1.1 Data preparation and preprocessing\nAll the images underwent the same standard VBM preprocessing, using CAT12 [15], which includes\nnon-linear registration to the MNI template and gray matter (GM) extraction. The final spatial\nresolution is 1.5mm isotropic and the images are of size 121 x 145 x 121. The preprocessing is\nperformed using the brainprep package 2. After preprocessing, we consider in this work modulated\ngray matter (GM) images, as in [13]."}, {"content": "3.2 Background\nContrastive learning (CL) (either self-supervised as in SimCLR [5] or supervised as in SupCon [19])\nleverages discrete labels (i.e., categories) to define positive and negative samples. Starting from a"}, {"title": "3.3 Weakly contrastive learning", "content": "To tackle the issue of continuous labels, weakly contrastive approaches employ the notion of a\ndegree of \"positiveness\u201d between samples. This degree is usually defined by a kernel function\nw\u2081 = K(y - yi), where 0 \u2264 wi \u2264 1 is computed by a Gaussian or a Radial Basis Function (RBF)\nkernel and y, yi are the continuous attributes of interest. The goal of weakly contrastive learning is\nthus to learn a parametric function f : X \u2192 Sd that maps samples with a high degree of positiveness\n(wi ~ 1) close in the latent space and samples with a low degree (w\u2081 ~ 0) far away from each other.\ny-Aware [12] proposes to do so by aligning positive samples with a strength proportional to the\ndegree, i.e.:\n$L_{y-aware} = \\sum_{i}  w_{i} \\log{\\frac{\\exp(sim(f(x), f(x_i))}{\\sum_{t=1}^{N} \\exp(sim(f(x), f(x_t)))}}$"}, {"title": "3.4 Proposed method", "content": "In this section, we will discuss our approach to include multiple attributes in a weakly contrastive\nparadigm."}, {"title": "3.4.1 Anatomical Contrastive Learning with anatomical measures", "content": "We propose a novel formulation for weakly contrastive learning to employ multiple anatomical\nattributes derived from the MRI scan, that we call AnatCL. The features we consider are derived from\nthe standard Desikan-Kiliany parcellation [7], based on the information available in the OpenBHB\ndataset (see Sec. 3.1). Among the anatomical measurements, we consider cortical thickness (CT),\ngray matter volume (GMV), and surface area (SA) [13]. To include these measurements inside\nAnatCL, we propose two different formulation, global and local, based on how the similarity across\ntwo brain scans is computed.\nLocal Descriptor For each of the 68 regions, we obtain a local anatomical descriptor k \u2208 R3\nwhere k \u2208 {1,..., 68} composed by the three anatomical measurements considered for that specific\nregion. In order to compute the degree of positiveness wi across two samples x and xi, given their\ncorresponding anatomical descriptors \u03a8 = {1, ... \u03c868} and \u03a8\u2081 = {1, ... } we compute the\naverage cross-region similarity (which we call local descriptor) as:\n$w_{i} = \\frac{1}{68} \\sum_{k=1}^{68} K(\\gamma(\\psi_{k}), \\gamma(\\psi_{k}^{i}))$"}, {"title": "Global Descriptor", "content": "Taking a complementary approach, we now consider global anatomy descriptors\nwk \u2208 R68 for the entire brain with k \u2208 {1,2,3}, that contain the values across all 68 regions for\neach anatomical measurement. Similarly to above, in order to compute the degree of positiveness\nwi between two samples x and xi given their global anatomical descriptors \u03a9 = {w\u00b9, . . ., w\u00b3} and\n\u03a9\u2081 = {\u03c9,...,\u03c9?} we compute the cross-measurement similarity (global descriptor) as:\n$w_{i} = \\frac{1}{3} \\sum_{k=1}^{3} K(\\omega^{k}, \\omega_{i}^{k})$\nThis time we do not need to standardize wk and was K is evaluated between features of the same\ntype, so the scale of the value is comparable. Also in this case, for the sake of simplicity, we employ\na cosine similarity function for K."}, {"title": "3.4.2 Final objective function", "content": "For training, we also consider the available age information, thus our final loss formulation becomes:\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{age} + \\lambda_2 \\mathcal{L}_{AnatCL}$"}, {"title": "4 Results", "content": "In this section, we present the results of our proposed method in comparison to existing state-of-the-art\napproaches in several downstream task."}, {"title": "4.1 Experimental setup", "content": "Our experiments involved pretraining a model using the proposed AnatCL loss on the OpenBHB\ndataset, followed by testing the resulting model on various downstream tasks across different datasets.\nThe experimental settings for the two loss formulations, local (AnatCL-G3) and global (AnatCL-L3),\nare identical. We pretrain two ResNet-18 3D models using VBM-preprocessed images and their\ncorresponding Desikan measures with the proposed formulations. The training process employs\nthe Adam optimizer with a learning rate of 0.0001, and a decay rate of 0.9 applied after every 10\nepochs. The models are trained with a batch size of 32 for a total of 300 epochs. As values of \u51651\nand A2, for simplicity, we use 1. As standard practice in CL approaches [5, 19], the contrastive loss\nis computed on a fully-connected projection head following the encoder, composed of two layers.\nTo ensure robust evaluation, we perform cross-validation using 5 folds. The results are computed in\nterms of mean and standard deviation across the 5 folds.\nAfter the pre-training step we evaluate the models by testing their performance with a transfer learning\napproach: we extract the latent representations generated by the model using only the encoder of\nthe model (i.e., discarding the fully connected head). For each downstream task, we train different\nlinear classifiers on the extracted representations to assess the model's ability to learn meaningful and\ngeneralizable features.\nFor comparison with standard approaches, we also implement four different baselines: SimCLR [5]\n(completely self-supervised) and three supervised baselines: a standard model trained with the L1 loss,\ny-Aware [12] and ExpW [1] which has shown state-of-the-art results on the OpenBHB challenge [13].\nFor all these methods, we follow the same experimental setup described above.\nTo run our experiments, we employ a cluster of 4 NVIDIA V100 GPUs, with a single training taking\naround 10h."}, {"title": "4.2 Brain age prediction", "content": "Preliminary results in terms of brain age prediction and sex classification are evaluated on the\nOpenBHB dateset. The results are reported in Tab. 2. From the results we conclude that AnatCL is\nable to match and slightly surpass state-of-the-art performance on brain age prediction. It is worth\nnoting that we do not employ any bias-correction method as in [6, 21]. For sex classification, we do\nnot match ExpW [1], but we improve over the SimCLR and L1 baselines."}, {"title": "4.3 Alzheimer's desease and cognitive impariments", "content": "In Tab. 3 we report results for Alzheimer's Disease (AD) detection on ADNI and OASIS-3. While we\ndo not reach the best results overall, AnatCL can usually improve over the self-supervised baseline\n(SimCLR) and sometimes over either L1, y-Aware or ExpW."}, {"title": "4.4 Schizophrenia and ASD", "content": "We evaluate downstream performance on SchizConnect for detecting schizophrenia (broad and strict),\nschizoaffective and bipolar patients. Results are reported in Tab. 4. With AnatCL we achieve state-\nof-the-art performance on three out of four tasks, denoting that taking anatomical information into\naccount can prove useful for these psychiatric conditions. We also assess detection performance for"}, {"title": "4.5 Cognitive score / assessments", "content": "As final experiments, we turn our attention to predicting clinical assessments score from brain\nMRIs. To the best of our knowledge, this has not been explored in other works, and it could\nprovide useful insights on the relation between brain anatomy and behavioral phenotypes. The\n10 phenotypes considered can be distinguished based on the prediction task: AIMS, depression,\nhandedness and GAIT are classification tasks, while IQ scores (FIQ, VIQ and PIQ) are regression\ntasks. For handendess, we predict right-handed vs other (left-handed or ambi), for depression we\nclassify between absent vs mild and above, for AIMS we classify between none and minimal vs\nmild and above, for GAIT between normal vs everything else. For more detailed explanation of the\npossible value in the considered phenotypes we refer to the official documentation45.\nThe results are reported in Tab. 5. While we cannot conclude that any of the analysed method can\naccurately predict all clinical assessments from MRI scans, AnatCL overall achieves the best results\nthree out of ten times, which is more than any other baseline. Interestingly, AnatCL can better predict\nthe overall AIMS score and patients' handedness, hinting that brain anatomy may be linked with\nthese phenotypes."}, {"title": "5 Limitations, impact and ethical considerations", "content": "We believe that foundation models for neuroimaging may have a considerable impact on accurately\ndiagnosing neurological and psyhicatric diseases. With AnatCL we aim at laying the foundations\nfor this path. Currently, AnatCL is limited towards using a single data modality (structural MRI),\nconsidering limited anatomical features (only CT, GMV, and SA) and to a relatively small backbone\n(ResNet-18). Future research should focus on improving these issues, in order to obtain even more\naccurate predictions. Altough, deep learning techniques can be used in a variety of contexts, we do\nnot believe that AnatCL inherently poses any ethical issue. Furthermore, all the data employed in this\nwork is publicly available for researchers."}, {"title": "6 Conclusions", "content": "We propose AnatCL, a foundation model based on brain anatomy and trained with a weakly contrastive\nlearning approach. With thorough validation on 10 different downstream tasks, we show that\nincorporating anatomy information during training can results in more accurate predictions of\ndifferent neurological and psychiatry conditions, and also partially clinical assessment scores and\nphenotypes. We release the weights of the trained model for public use at omittedlink, empowering\nresearchers and practitioners worldwide to leverage AnatCL in many different applications."}]}