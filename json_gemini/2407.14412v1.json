{"title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs", "authors": ["Tang Li", "Mengmeng Ma", "Xi Peng"], "abstract": "Large pre-trained Vision-Language Models (VLMs) have become ubiquitous foundational components of other models and downstream tasks. Although powerful, our empirical results reveal that such models might not be able to identify fine-grained concepts. Specifically, the explanations of VLMs with respect to fine-grained concepts are entangled and mislocalized. To address this issue, we propose to DisEntAngle and Localize (DEAL) the concept-level explanations for VLMs without human annotations. The key idea is encouraging the concept-level explanations to be distinct while maintaining consistency with category-level explanations. We conduct extensive experiments and ablation studies on a wide range of benchmark datasets and vision-language models. Our empirical results demonstrate that the proposed method significantly improves the concept-level explanations of the model in terms of disentanglability and localizability. Surprisingly, the improved explainability alleviates the model's reliance on spurious correlations, which further benefits the prediction accuracy.\nKeywords: eXplainable Machine Learning Vision-Language Models", "sections": [{"title": "Introduction", "content": "Since the introduction of large pre-trained Vision-Language Models (VLMs),\ne.g., CLIP [50] and FLAVA [60], they become ubiquitous \u201cfoundations\u201d for other\nmodels [5,33] and downstream tasks [27,58]. Although these models exhibit\nsignificant capabilities, our empirical results reveal that they might not be able to\nidentify fine-grained concepts. As shown in Fig. 1, VLMs often associate related\nconcepts to the main object without further distinctions. This would yield severe\nconsequences in safety-critical downstream tasks. For example, in autonomous\nvehicles, a model that entangles shapes, colors, or contexts with specific road\nsigns, might misinterpret or fail to recognize temporary or region-specific signs.\nThis could lead to unsafe driving decisions, endangering passengers and other\nroad users. A question naturally arises: Can we develop VLMs that disentangle\nand localize fine-grained linguistic concepts on images?\n1 Our source code and pretrained weights: https://github.com/tangli-udel/DEAL."}, {"title": "Related Works", "content": "eXplainable Machine Learning (XML). The emerging field of XML aims\nto bring transparency to today's powerful but opaque deep learning models. XML\nusually consists of two categories. (1) Intrinsic methods, whose explanations are\ninherent to the model's design and training, provide explanations along with out-\nput, such as joint training [23,37] and prototyping [10] methods. However, such\ninterpretable methods usually sacrifice their prediction performance [10,54]. (2)\nPost hoc methods, which give insight into the learned associations of a model\nthat are not readily interpretable by design. Such methods usually leverage back-\npropagation or local approximation to offer saliency maps as explanations, e.g.,\nVanilla Gradient [59], Grad-CAM [57], LIME [51], and SHAP [35]. However, such\nsaliency-based explanations are inherently ambiguous and require ML expertise\nto understand [29,54]. For example, given a saliency map that highlights the\ncorrect object on the image, it remains unclear whether the model's prediction\nstems from the texture or color. In contrast, the proposed method offers human-\nunderstandable explanations while maintaining high prediction performance.\nConcept-based explanations. Instead of delving directly into raw features,\nthere have been existing attempts to correlate the internal representations of ma-\nchine learning models with high-level, semantically meaningful concepts familiar\nto humans. They either provide quantification of the influence of predefined con-\ncepts like \"stripes\" on the model's predictions, e.g., TCAV [24], Concept Bottle-\nneck [26], or manually translate the abstract feature maps learned by the model\ninto human-understandable terms, e.g., ProtoPNet [10], CRP [1]. However, their"}, {"title": "Methods", "content": "In this section, we first present how to prompt Large Language Models (LLMs)\nfor discriminative concepts, then provide the problem formulation, and then"}, {"title": "Prompting LLMs for Discriminative Concepts", "content": "The recent breakthroughs in LLMs demonstrate their capability to provide com-\nmonsense knowledge in language [41]. Therefore, LLMs are implicit knowledge\nbases that can be easily queried with natural language, even for non-experts. In-\nspired by recent works in language prompting [40,68], we can extract the desired\nconcepts for each category from LLMs efficiently for large-scale datasets.\nHowever, as shown in Fig. 3, our empirical results show that the concept\nquality in terms of discriminative capacity is highly sensitive to the prompts.\nTo address this issue, we leverage the In-context learning [8] capability of LLMs\nto provide Chain-of-Thought (CoT) [65] instructions. Specifically, we provide\nthe GPT-3.5 [45] model with exemplary queries and responses, followed by the\nsubsequent question:\nQ: What are the discriminative visual features with minimum overlap for\nidentifying a (CATEGORY) in an image?\nA: The discriminative visual features that identify a [CATEGORY] in an\nimage are:\nIn contrast to direct query, our concepts align with the typical human ratio-\nnale for predictions. Fig. 3 shows examples of our generated concepts for the\ncorresponding category, which are more visually distinctable."}, {"title": "Disentangling and Localizing Explanations", "content": "Problem formulation. The contrastive learning objective of typical VLMs,\ne.g., CLIP [50], is to learn a mapping function $f \\in F$ such that for each pair\n$(I,T) \\sim P(I, T)$, where $P(I, T)$ is the distribution of image-text pairs, the func-\ntion $f$ aligns the embeddings of $I$ and $T$ in a shared space if they are a correct\nmatch. This process is typically guided by a contrastive loss function $L_{contr} (.)$.\nAs we discussed in Secs 1 and 2, such learning objective is coarse-grained and ig-\nnores the distinction between fine-grained concept-level explanations. To address\nthis, we propose an effective approach to refine the learning process."}, {"title": null, "content": "Let $g([TEXT])$ be a post-hoc explanation method (e.g., Grad-CAM [57]) that\ncalculates the explanation heatmap for an image $I$ corresponding to the textual\ninput $[TEXT]$, based on their similarity in the embedding space measured by $f$.\nConsidering all $[CONCEPT]$ belong in to the same $[CATEGORY]$, we propose to\ndisentangle and localize (DEAL) concept-level explanations by optimizing:\n$\\min_{f\\in F} Risk(f) := E_{(I,T)\\sim P} [L_{contr}(f(I,T))]$\ns.t. $\\text{Dist}(g([CONCEPT]),g([CONCEPT]')) \\geq \\epsilon$,\n$\\text{Dist}(g([CONCEPT]), g([CATEGORY])) < \\delta$.\nContrast\n<Disentangle\nLocalize\n(1)\nIntuitively, our goal is to ensure that the explanation of each concept within a\ncategory is distinct, while the collective explanations for all concepts are consis-\ntent with the category's overall explanation. Next, we will explain the Contrast,\nDisentangle, and Localize in detail.\nContrastive learning objective. Following the standard contrastive learn-\ning methodology between image and text used in CLIP [50], we employ the In-\nfoNCE loss [42] as our backbone loss. Denote $\\{(I_i, T_i)\\}$, as a batch of $N$ paired\nimages and texts, $(v_i, t_i) = f(I_i, T_i)$ as the vision and text embeddings, $\\tau$ as a\ntemperature scaling factor, then we define our contrastive learning loss as:\n$L_{contr} = \\frac{1}{N} \\sum_{i=1}^N -log \\frac{exp(v_i t_i /\\tau)}{\\sum_{m=1}^N exp(v_i t_m /\\tau)}$\n(2)\nIn contrast to the typical practice that simply uses category as the text de-\nscription [16,66], we augment them with the corresponding concepts obtained in\nSec. 3.1. For example, we can apply a template \"An image of [CATEGORY] with\n[CONCEPT_1], [CONCEPT_2], ..., and [CONCEPT_K]\"\nDisentangling concept-level explanations. As we previously discussed in\nSecs. 1 and 2, the key challenge for learning correct concept-level explanations\nis the absence of human annotations. Let $\\{C_k\\}_{k=1}^K$ be the set of $K$ concepts\nfor the category of image $I$ as obtained in Sec. 3.1, then $g(C_k)$ calculates the\nexplanation heatmap for image $I$ with respect to concept $C_k$. Denote $Dist(., .)$ as\na distance metric, e.g., $l_1$ distance, we propose to disentangle the concept-level\nexplanations by imposing regularization on the model for each image using:\n$R_{disen} = \\sum_{k=1}^K \\sum_{j=1,j\\neq k}^K Dist(g(C_k), g(C_j))$\n(3)\nIntuitively, this regularization term constrains the model's explanations for each\nimage w.r.t. different concepts to have minimum overlap. Note that $g(\\cdot)$ is not\nrestrictive, including arbitrary gradient-based explanation implementation for\nVLMs, such as Chefer et al. [9] and GradCAM [57]. Different from existing meth-\nods that rely on category-level region annotations [28, 70], our method provides\nfiner-grained concept-level supervision without human annotations.\nLocalizing concept-level explanations. Our empirical results (Tab. 4)\nindicate that recklessly optimizing Eq. (3) could easily fall into local minimums."}, {"title": "Solving the Constrained Optimization", "content": "Solving such a constrained optimization problem in Sec. 3.2 often leads to a\nnon-convex problem, wherein methods like stochastic gradient descent (SGD)\ncannot guarantee constraint satisfaction. To address this problem, we leverage\nKarush-Kuhn-Tucker conditions [7] and introduce Lagrange multipliers $\\lambda$ and\n$\\gamma$ to convert the constrained problem into its unconstrained counterpart. Our\noverall learning objective is formulated as follows:\n$\\min_{f\\in F} {Risk(f) := E_{(I,T)\\sim P(I,T)} [L_{contr}(f(I,T))] + \\lambda R_{disen} + \\gamma R_{local}}$\n(5)\nThe entire training pipeline is summarized in Alg. 1. The proposed method\nhas the following merits. (1) In contrast to the pertaining objective of VLMs\nwhich is limited to coarse-grained alignment [21,50,62], the proposed learning"}, {"title": "Experiments", "content": "To best validate the explanation quality and prediction performance, we con-\nduct a series of experiments to compare the proposed DEAL method with other\nlearning methods on different vision backbones. The experimental results prove\nthat our method achieves superior concept-level explanation disentanglability\nand localizability on a wide scope of benchmark datasets. In contrast to most\ninterpretability methods that come with a compromise on the benchmark per-\nformance, our method benefits the model's prediction."}, {"title": "Datasets and Implementation Details", "content": "Datasets. We conduct experiments on five typical image recognition datasets\nthat have been used for evaluating the CLIP [50] model. (1) ImageNet [14]\ndataset stands as one of the typical benchmarks in the field of computer vision.\nThe dataset spans 1,000 object classes and 1,281,167 training images from the\nWordNet hierarchy, ensuring a vast and varied semantic landscape. (2) CUB [63],\nshort for Caltech-UCSD Birds-200-2011, is a highly specialized dataset designed\nto distinguish between subcategories of birds. The dataset contains 11,788 im-\nages of 200 bird species. (3) Food-101 [6] dataset is designed for the task of food\nrecognition. The dataset comprises 101,000 images split across 101 food cate-\ngories. (4) Oxford-Pets [44] dataset includes a set of 7,349 images divided across\n37 distinct pet categories, including a balanced collection of 12 dog breeds and\n25 cat breeds. (5) EuroSAT [22] dataset is derived from Sentinel-2 satellite im-\nages, including a diverse range of European land use and land cover categories.\nThe dataset contains 27,000 geo-referenced samples of 10 different classes.\nImplementation details. For all datasets, we use the implementations of\nCLIP [50] for different vision backbones. Due to the computational cost of train-\ning CLIP from scratch, we focused on finetuning experiments using ViT-B/32\nand ResNet-50 backbones. We fine-tune the full model leveraging the proposed\nmethod, and compare the results with zero-shot and representative fine-tuning\nstrategies of the CLIP model, including full model and vision-encoder-only fine-\ntuning. For all the training, we use our augmented text of the category as the\ntext description. We split the training data of each dataset into 80% and 20%\nsplits, and use the larger splits for training, and the smaller splits for validation\nand model selection. We evaluate the model performance on the testing data\nprovided by each dataset. All models are trained using Adam [25] optimizer un-\ntil convergence. We crop the images of random size and aspect ratio, resizing"}, {"title": "Evaluation Metrics", "content": "Concept-level Explanation Disentanglability. The disentanglability of\nconcept-level explanations is evaluated through the $R_{disen}$ regularization term\nthat is also applied in our model's training phase. While this term was optimized\non training data, the disentanglability of concept-level explanations on the hold-\nout test data remains essential for evaluating the success of our goal. For better\ncomparison, its values are normalized to [0, 1]. A higher metric score is expected\nfor concept-level explanations that exhibit better disentanglement.\nConcept-level Explanation Localizability. Due to the absence of region\nannotations for concepts, we use the explanation fidelity [46] w.r.t. the concepts\nto measure their localizability. This metric quantifies the increase in predictive\nscore probability with the sequential inclusion of pixels, sorted in descending\norder of their importance as derived from the explanation [4]. Different from\nthe conventional category-level version, our metric measures the fidelity at the\nconcept level using the predictive score of concepts. We calculate the area under"}, {"title": "Comparison with State-of-the-Art VLMs", "content": "Takeaway 1: DEAL can disentangle and localize fine-grained concepts.\nAs aforementioned, the existing learning objectives of VLMs are limited to the"}, {"title": "Comparison with Fine-tuned Models", "content": "Takeaway 2: The disentanglability of DEAL can boost prediction accu-\nracy. To better evaluate the effectiveness of the proposed constraints, we com-"}, {"title": "Evaluation using Ground Truth Object Part Annotations", "content": "Takeaway 3: DEAL can accurately localize fine-grained object parts.\nObject parts represent a specific subset of fine-grained concepts that can eval-\nuate the model's localization capability. We evaluate our model on the CUB-\nPart [55] and PartImageNet [20] datasets that provide ground truth masks of\nparts, such as \"beak\" and \"head\", for 299 and 24,000 images. As shown in Tab. 3,\nour model significantly improves the localization of explanations for these con-\ncepts by 5.20% and 5.52% for two different vision backbones on CUB-Part, and\n3.94% and 4.85% on PartImageNet in terms of mIoU score."}, {"title": "Ablation Study", "content": "In this section, we perform ablation studies to investigate the key components\nof our method proposed in Eq. (3) and Eq. (4). The empirical results are shown\nin Tab. 4. Furthermore, as shown in Tab. 5, we compare the scalability between\nour method and the baseline fine-tuning method.\nAblation on disentanglement constraint. The \"DEAL w/o disen.\" rep-\nresents a variant of our method which only optimizes the consistency between\nthe aggregation of concept-level explanations and the corresponding category-\nlevel explanation. Without constraints on the disentanglement, the concept-level\nexplanations tend to be similar to easily satisfy the consistency constraint. This\nleads to entangled concept-level explanations, which also dramatically decrease\nthe prediction performance by 10.4%.\nAblation on localization constraint. The \"DEAL w/o local.\" represents\na variant of our method which only optimizes for the disentanglement of ex-\nplanations. As we discussed in Sec. 3, recklessly disentangling concept-level ex-\nplanation might easily fall into a local minimum. As shown in Tab 4, although"}, {"title": "Conclusion", "content": "In this paper, we present comprehensive studies to show that the training ob-\njective of existing VLMs ignores the distinction between fine-grained concepts.\nTheir explanations with respect to these concepts are entangled and mislocalized.\nTo address this problem, we propose to Disentangle and Localize (DEAL) the\nconcept-level explanations for VLMs. Our method fully utilizes the discrepancy\nand consistency between concept- and category-level explanations to provide\nsupervisory signals for concept learning without human annotations. Extensive\nexperiments demonstrate the superiority of our method in terms of the disen-\ntanglability and localizability of concept-level explanations."}]}