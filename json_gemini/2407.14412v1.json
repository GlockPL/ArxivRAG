{"title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs", "authors": ["Tang Li", "Mengmeng Ma", "Xi Peng"], "abstract": "Large pre-trained Vision-Language Models (VLMs) have become ubiquitous foundational components of other models and downstream tasks. Although powerful, our empirical results reveal that such models might not be able to identify fine-grained concepts. Specifically, the explanations of VLMs with respect to fine-grained concepts are entangled and mislocalized. To address this issue, we propose to DisEntAngle and Localize (DEAL) the concept-level explanations for VLMs without human annotations. The key idea is encouraging the concept-level explanations to be distinct while maintaining consistency with category-level explanations. We conduct extensive experiments and ablation studies on a wide range of benchmark datasets and vision-language models. Our empirical results demonstrate that the proposed method significantly improves the concept-level explanations of the model in terms of disentanglability and localizability. Surprisingly, the improved explainability alleviates the model's reliance on spurious correlations, which further benefits the prediction accuracy.\nKeywords: eXplainable Machine Learning Vision-Language Models", "sections": [{"title": "1 Introduction", "content": "Since the introduction of large pre-trained Vision-Language Models (VLMs), e.g., CLIP [50] and FLAVA [60], they become ubiquitous \u201cfoundations\u201d for other models [5,33] and downstream tasks [27,58]. Although these models exhibit significant capabilities, our empirical results reveal that they might not be able to identify fine-grained concepts. As shown in Fig. 1, VLMs often associate related concepts to the main object without further distinctions. This would yield severe consequences in safety-critical downstream tasks. For example, in autonomous vehicles, a model that entangles shapes, colors, or contexts with specific road signs, might misinterpret or fail to recognize temporary or region-specific signs. This could lead to unsafe driving decisions, endangering passengers and other road users. A question naturally arises: Can we develop VLMs that disentangle and localize fine-grained linguistic concepts on images?"}, {"title": "2 Related Works", "content": "eXplainable Machine Learning (XML). The emerging field of XML aims to bring transparency to today's powerful but opaque deep learning models. XML usually consists of two categories. (1) Intrinsic methods, whose explanations are inherent to the model's design and training, provide explanations along with output, such as joint training [23,37] and prototyping [10] methods. However, such interpretable methods usually sacrifice their prediction performance [10,54]. (2) Post hoc methods, which give insight into the learned associations of a model that are not readily interpretable by design. Such methods usually leverage backpropagation or local approximation to offer saliency maps as explanations, e.g., Vanilla Gradient [59], Grad-CAM [57], LIME [51], and SHAP [35]. However, such saliency-based explanations are inherently ambiguous and require ML expertise to understand [29,54]. For example, given a saliency map that highlights the correct object on the image, it remains unclear whether the model's prediction stems from the texture or color. In contrast, the proposed method offers human-understandable explanations while maintaining high prediction performance.\nConcept-based explanations. Instead of delving directly into raw features, there have been existing attempts to correlate the internal representations of machine learning models with high-level, semantically meaningful concepts familiar to humans. They either provide quantification of the influence of predefined concepts like \"stripes\" on the model's predictions, e.g., TCAV [24], Concept Bottleneck [26], or manually translate the abstract feature maps learned by the model into human-understandable terms, e.g., ProtoPNet [10], CRP [1]. However, their"}, {"title": "3 Methods", "content": "In this section, we first present how to prompt Large Language Models (LLMs) for discriminative concepts, then provide the problem formulation, and then"}, {"title": "3.1 Prompting LLMs for Discriminative Concepts", "content": "The recent breakthroughs in LLMs demonstrate their capability to provide commonsense knowledge in language [41]. Therefore, LLMs are implicit knowledge bases that can be easily queried with natural language, even for non-experts. Inspired by recent works in language prompting [40,68], we can extract the desired concepts for each category from LLMs efficiently for large-scale datasets.\nHowever, as shown in Fig. 3, our empirical results show that the concept quality in terms of discriminative capacity is highly sensitive to the prompts. To address this issue, we leverage the In-context learning [8] capability of LLMs to provide Chain-of-Thought (CoT) [65] instructions. Specifically, we provide the GPT-3.5 [45] model with exemplary queries and responses, followed by the subsequent question:\nQ: What are the discriminative visual features with minimum overlap for identifying a (CATEGORY) in an image?\nA: The discriminative visual features that identify a [CATEGORY] in an image are:\nIn contrast to direct query, our concepts align with the typical human rationale for predictions. Fig. 3 shows examples of our generated concepts for the corresponding category, which are more visually distinctable."}, {"title": "3.2 Disentangling and Localizing Explanations", "content": "Problem formulation. The contrastive learning objective of typical VLMs, e.g., CLIP [50], is to learn a mapping function f \u2208 F such that for each pair (I,T) ~ P(I, T), where P(I, T) is the distribution of image-text pairs, the function f aligns the embeddings of I and T in a shared space if they are a correct match. This process is typically guided by a contrastive loss function \\(L_{contr}(.)\\). As we discussed in Secs 1 and 2, such learning objective is coarse-grained and ignores the distinction between fine-grained concept-level explanations. To address this, we propose an effective approach to refine the learning process."}, {"title": "4 Experiments", "content": "To best validate the explanation quality and prediction performance, we conduct a series of experiments to compare the proposed DEAL method with other learning methods on different vision backbones. The experimental results prove that our method achieves superior concept-level explanation disentanglability and localizability on a wide scope of benchmark datasets. In contrast to most interpretability methods that come with a compromise on the benchmark performance, our method benefits the model's prediction."}, {"title": "4.1 Datasets and Implementation Details", "content": "Datasets. We conduct experiments on five typical image recognition datasets that have been used for evaluating the CLIP [50] model. (1) ImageNet [14] dataset stands as one of the typical benchmarks in the field of computer vision. The dataset spans 1,000 object classes and 1,281,167 training images from the WordNet hierarchy, ensuring a vast and varied semantic landscape. (2) CUB [63], short for Caltech-UCSD Birds-200-2011, is a highly specialized dataset designed to distinguish between subcategories of birds. The dataset contains 11,788 images of 200 bird species. (3) Food-101 [6] dataset is designed for the task of food recognition. The dataset comprises 101,000 images split across 101 food categories. (4) Oxford-Pets [44] dataset includes a set of 7,349 images divided across 37 distinct pet categories, including a balanced collection of 12 dog breeds and 25 cat breeds. (5) EuroSAT [22] dataset is derived from Sentinel-2 satellite images, including a diverse range of European land use and land cover categories. The dataset contains 27,000 geo-referenced samples of 10 different classes.\nImplementation details. For all datasets, we use the implementations of CLIP [50] for different vision backbones. Due to the computational cost of training CLIP from scratch, we focused on finetuning experiments using ViT-B/32 and ResNet-50 backbones. We fine-tune the full model leveraging the proposed method, and compare the results with zero-shot and representative fine-tuning strategies of the CLIP model, including full model and vision-encoder-only fine-tuning. For all the training, we use our augmented text of the category as the text description. We split the training data of each dataset into 80% and 20% splits, and use the larger splits for training, and the smaller splits for validation and model selection. We evaluate the model performance on the testing data provided by each dataset. All models are trained using Adam [25] optimizer until convergence. We crop the images of random size and aspect ratio, resizing"}, {"title": "4.2 Evaluation Metrics", "content": "Concept-level Explanation Disentanglability. The disentanglability of concept-level explanations is evaluated through the \\(R_{disen}\\) regularization term that is also applied in our model's training phase. While this term was optimized on training data, the disentanglability of concept-level explanations on the hold-out test data remains essential for evaluating the success of our goal. For better comparison, its values are normalized to [0, 1]. A higher metric score is expected for concept-level explanations that exhibit better disentanglement.\nConcept-level Explanation Localizability. Due to the absence of region annotations for concepts, we use the explanation fidelity [46] w.r.t. the concepts to measure their localizability. This metric quantifies the increase in predictive score probability with the sequential inclusion of pixels, sorted in descending order of their importance as derived from the explanation [4]. Different from the conventional category-level version, our metric measures the fidelity at the concept level using the predictive score of concepts. We calculate the area under"}, {"title": "4.3 Comparison with State-of-the-Art VLMs", "content": "Takeaway 1: DEAL can disentangle and localize fine-grained concepts. As aforementioned, the existing learning objectives of VLMs are limited to the"}, {"title": "4.4 Comparison with Fine-tuned Models", "content": "Takeaway 2: The disentanglability of DEAL can boost prediction accuracy. To better evaluate the effectiveness of the proposed constraints, we com-"}, {"title": "4.5 Evaluation using Ground Truth Object Part Annotations", "content": "Takeaway 3: DEAL can accurately localize fine-grained object parts. Object parts represent a specific subset of fine-grained concepts that can evaluate the model's localization capability. We evaluate our model on the CUB-Part [55] and PartImageNet [20] datasets that provide ground truth masks of parts, such as \"beak\" and \"head\", for 299 and 24,000 images. As shown in Tab. 3, our model significantly improves the localization of explanations for these concepts by 5.20% and 5.52% for two different vision backbones on CUB-Part, and 3.94% and 4.85% on PartImageNet in terms of mIoU score."}, {"title": "4.6 Ablation Study", "content": "In this section, we perform ablation studies to investigate the key components of our method proposed in Eq. (3) and Eq. (4). The empirical results are shown in Tab. 4. Furthermore, as shown in Tab. 5, we compare the scalability between our method and the baseline fine-tuning method.\nAblation on disentanglement constraint. The \"DEAL w/o disen.\" represents a variant of our method which only optimizes the consistency between the aggregation of concept-level explanations and the corresponding category-level explanation. Without constraints on the disentanglement, the concept-level explanations tend to be similar to easily satisfy the consistency constraint. This leads to entangled concept-level explanations, which also dramatically decrease the prediction performance by 10.4%.\nAblation on localization constraint. The \"DEAL w/o local.\" represents a variant of our method which only optimizes for the disentanglement of explanations. As we discussed in Sec. 3, recklessly disentangling concept-level explanation might easily fall into a local minimum. As shown in Tab 4, although"}, {"title": "5 Conclusion", "content": "In this paper, we present comprehensive studies to show that the training objective of existing VLMs ignores the distinction between fine-grained concepts. Their explanations with respect to these concepts are entangled and mislocalized. To address this problem, we propose to Disentangle and Localize (DEAL) the concept-level explanations for VLMs. Our method fully utilizes the discrepancy and consistency between concept- and category-level explanations to provide supervisory signals for concept learning without human annotations. Extensive experiments demonstrate the superiority of our method in terms of the disentanglability and localizability of concept-level explanations."}]}