{"title": "TabICL: A Tabular Foundation Model for In-Context Learning on Large Data", "authors": ["Jingang Qu", "David Holzm\u00fcller", "Ga\u00ebl Varoquaux", "Marine Le Morvan"], "abstract": "The long-standing dominance of gradient-boosted\ndecision trees on tabular data is currently chal-\nlenged by tabular foundation models using In-\nContext Learning (ICL): setting the training data\nas context for the test data and predicting in a\nsingle forward pass without parameter updates.\nWhile the very recent TabPFNv2 foundation\nmodel (2025) excels on tables with up to 10K\nsamples, its alternating column- and row-wise\nattentions make handling large training sets com-\nputationally prohibitive. So, can ICL be effec-\ntively scaled and deliver a benefit for larger ta-\nbles? We introduce TabICL, a tabular foundation\nmodel for classification, pretrained on synthetic\ndatasets with up to 60K samples and capable of\nhandling 500K samples on affordable resources.\nThis is enabled by a novel two-stage architecture:\na column-then-row attention mechanism to build\nfixed-dimensional embeddings of rows, followed\nby a transformer for efficient ICL. Across 200\nclassification datasets from the TALENT bench-\nmark, TabICL is on par with TabPFNv2 while\nbeing systematically faster (up to 10 times), and\nsignificantly outperforms all other approaches.\nOn 56 datasets with over 10K samples, TabICL\nsurpasses both TabPFNv2 and CatBoost, demon-\nstrating the potential of ICL for large data. Infer-\nence code and pre-trained models are available at\nhttps://github.com/soda-inria/tabicl.", "sections": [{"title": "1. Introduction", "content": "Tabular data, structured in rows and columns, is widely used\nin industries like healthcare (Cartella et al., 2021) and fi-\nnance (Johnson et al., 2016), where tabular classification\nproblems underpin numerous real-world applications. In\nother data modalities, foundation models -particularly Large\nLanguage Models (LLMs) (Zhou et al., 2024)- have signifi-\ncantly advanced the ability to tackle new tasks and few-shot\nlearning. This is largely due to their remarkable in-context\nlearning (ICL) capabilities (Brown et al., 2020), which en-\nable them to capture patterns in prompts without requiring\nparameter updates. This success combined with the perva-\nsiveness of tables have spurred interest in tabular foundation\nmodels (van Breugel & van der Schaar, 2024).\nWhile LLMs are primarily designed to model natural lan-\nguage, attempts to fine-tune them for tabular data have\nemerged (Hegselmann et al., 2023; Fang et al., 2024, and\nreferences therein). These efforts rely on table serialization,\nwhich is the process of converting table rows into text or sen-\ntences which can then be tokenized. Gardner et al. (2024)\nfine-tuned a Llama 3-8B on a corpus of serialized tables and\ndemonstrated the effectiveness of this approach compared\nto tree-based models in the few-shot setting. However, such\nlanguage models-based approaches are limited by the size\nof their context window to fit large serialized tables (e.g.\nup to 32 or 64 shots in Gardner et al. 2024). Moreover, it\nis unclear whether LLMs can effectively handle numerical\nvalues (Thawani et al., 2021). Finally, as evidence shows\nthat LLMs are pretrained on many popular datasets (Bordt\net al., 2024), their evaluation on tabular prediction tasks\nshould also be conducted with care.\nTaking a markedly different approach, Hollmann et al.\n(2022) introduced TabPFN, a transformer-based tabular\nfoundation model for classification tasks, pretrained on syn-\nthetic tabular datasets only. A key feature of TabPFN is\nICL with tables, which eliminates the need for tokenization\nand enables efficient handling of small tables with up to\n1K samples and 100 features. Very recently, the same au-\nthors introduced TabPFNv2 (2025), an improved version\nthat significantly outperforms tree-based and neural network\ncompetitors on small-to-medium datasets with up to 10K\nsamples and 500 features. The great promise of tabular ICL\nhas spurred a new line of research (see Section 2.3), yet the\nquadratic cost of self attention is a threat to scalability of\nall these. TabPFNv2 uses a two-way attention mechanism\nalternating between column-wise and row-wise attentions,\nwhich limits its scalability for large datasets. In real-world\nscenarios, where industrial datasets can contain millions of\nsamples (Rubachev et al., 2024), the high computational\nand memory demands of TabPFNv2 hinder its practicality."}, {"title": "TabICL: A Tabular Foundation Model for Large Data", "content": "In this paper, we introduce TabICL, a scalable and efficient\ntabular foundation model designed for classification tasks.\nPretrained on synthetic datasets with up to 60K samples,\nTabICL can effectively handle datasets with up to 500K\nsamples and 500 features, significantly expanding the scal-\nability boundaries of ICL for tables and cementing it as a\nfoundational technique for tabular foundation models.\nTo handle tables of arbitrary sizes with architectural changes,\nTabICL treats individual cells as basic elements: each col-\numn is seen as a set of cell values capturing feature-specific\ndistribution and semantics, and each row consists of inter-\ndependent feature values providing a holistic view of each\nsample. TabICL employs a two-stage architecture to achieve\nefficient ICL for tabular data. First, it encodes rows (ex-\ncluding target labels) into dense vector embeddings. Each\nembedding is designed to be capable of capturing the en-\ntire table information. This stage effectively collapses the\ncolumn dimension to substantially reduce computational\ncomplexity and memory footprint for subsequent ICL. Sec-\nond, it combines these compact yet informative embeddings\nwith corresponding labels and then performs ICL. There-\nfore, the core of TabICL lies in its embedding strategy of\nthe first stage, which is supposed to transform rows into\nsemantically rich embeddings.\nWords and phrases in text often carry clear semantics, and\nare thus naturally associated to informative embeddings\n(Mikolov et al., 2013). However, tabular data lacks such an\ninherent structure: cell values can be ambiguous without\nmetadata such as column names or data types. To address\nthis challenge, TabICL adopts a well-constrained embed-\nding strategy combining (1) distribution-aware column-wise\nfeature embedding to capture statistical regularities within\neach column and (2) attention-based row-wise interaction to\nmodel dependencies across columns, thereby constructing\nsemantically grounded representations for tabular data.\nFeature embedding involves mapping scalar cell values into\nhigh-dimensional vectors for each feature, serving as a crit-\nical factor in model performance (Gorishniy et al., 2022).\nSince features often exhibit vastly different distributions,\nprevious approaches typically use feature-specific embed-\nding modules without parameter sharing, which, however,\nlimits cross-table transferability. In this work, we refor-\nmulate feature embedding as a set-input problem, where a\npermutation-invariant set of cell values acts as input, and\nthe output comprises corresponding one-to-one embeddings.\nTo achieve this, we leverage the Set Transformer (Lee et al.,\n2019), a model specifically designed to process sets through\nefficient induced self-attention. It excels in tasks such as\nidentifying extrema and counting unique elements, enabling\nthe discovery of distribution-related metadata within each\ncolumn and enhancing the ability to distinguish between\nfeatures of different data types."}, {"title": "TabICL: A Tabular Foundation Model for Large Data", "content": "The feature embeddings are then processed per-row by an-\nother transformer, and aggregated into a single vector using\nlearnable [CLS] tokens. This effectively captures complex\nfeature interactions and accommodates a varying number of\nfeatures. Overall, this column-then-row attention-based em-\nbedding achieves efficient sparse attention across all cells by\nleveraging the column/row inherent structure of tabular data\nas a strong inductive bias. Finally, the resulting row-wise\nembeddings are handled by a final transformer for ICL.\nIn addition to the above innovations, we introduce several\nimprovements: (1) We refine pretraining synthetic datasets\nof TabPFN by adding a new tree-based data-generating\nmodel to incorporate the inductive biases of tree-based mod-\nels (Grinsztajn et al., 2022; den Breejen et al., 2024); (2) We\nadopt curriculum learning by gradually scaling the pretrain-\ning dataset size from 1K to 60K; (3) To address classification\nproblems with over 10 classes (the pretraining limit), we use\nhierarchical classification (Silla & Freitas, 2011), breaking\nthem into a hierarchical structure of subproblems with < 10\nclasses. The increase in the number of tasks is largely offset\nby the fast ICL-powered predictions of TabICL.\nTo summarize our contributions: (1) We present TabICL,\na novel scalable tabular foundation model for classifica-\ntion tasks that can accommodate any number of samples,\nfeatures, and classes. In practice, TabICL handles up to\n500K samples and 500 features with around 20GB of GPU\nmemory; (2) We introduce a distribution-aware feature em-\nbedding approach that handles features with diverse prop-\nerties in a unified manner, unlocking new possibilities for\ncross-table transferability; (3) TabICL performs tasks in a\nsingle forward pass and is orders of magnitude faster than\ntabular methods requiring hyper-parameter tuning while\nstill providing better performance in most cases. TabICL\nis also consistently faster than TabPFNv2 (up to 10 times),\nwith efficiency gains increasing as dataset size grows; (4)\nWe evaluate TabICL on the TALENT benchmark (Ye et al.,\n2025), comprising 200 classification datasets across various\ndomains and sizes (up to 150K samples). TabICL performs\ncomparably to TabPFNv2 on medium-sized datasets and\nsignificantly outperforms all other methods. On the 55 large\ndatasets with over 10K samples, TabICL surpasses both\nTabPFNv2 and CatBoost (Dorogush et al., 2018). These\nresults demonstrate the potential of ICL for large data."}, {"title": "2. Related Work", "content": "In recent years, deep learning (DL) has been transformed\nby the emergence of foundation models, which are pre-\ntrained on massive, diverse datasets and serve as versatile\nbackbones for downstream tasks. These transformer-based\nmodels enable In-Context Learning (ICL): performing tasks"}, {"title": "2.3. TabPFN and its Offsprings", "content": "TabPFN, short for Tabular Prior-Data Fitted Network, is\na tabular foundation model. It is a transformer pretrained\non extensive synthetic datasets to perform tabular classifi-\ncation tasks through ICL. TabPFN interprets ICL from a\nBayesian perspective as an approximate posterior predic-\ntive distribution over synthetic datasets. Several variants\naim to enhance its scalability, including distilling training\ndata into a compact learned context via prompt tuning (Ma\net al., 2024b; Feuer et al., 2024), selecting the most rele-\nvant subset of training data for each test sample (Xu et al.,\n2024; Thomas et al., 2024; Koshil et al., 2024), replacing\nquadratic with linear attention (Zeng et al., 2024), and gener-\nating small task-specific neural networks via an ICL-based\nhypernetwork (M\u00fcller et al., 2023). However, most vari-\nants do not structurally improve TabPFN but instead act as\nprompt engineering to reduce in-context samples.\nOther approaches try to improve the quality of pre-training\ndata, such as TabForestPFN (den Breejen et al., 2024) in-\ncorporating tree-based synthetic datasets and TabDPT (Ma\net al., 2024a) curating and using real-world datasets.\nVery recently (January 2025), TabPFNv2 was released and\nlargely improved TabPFN in terms of both prediction per-\nformance and scalability. Our contributed model, TabICL,"}, {"title": "3. The TabICL Architecture", "content": "We consider a tabular classification task with an input space\nX \u2208 \u211d^(nm) and a target space Y \u2208 [1,\u06f0\u06f0\u06f0, C']. Given a train-\ning dataset of input-output pairs Dtrain = {(x^(i)train, y^(i)train)}^me_(i=1) me\nand test samples Xtest = {x^(i)test}1, our goal is to predict\nthe class probabilities p(\u00b7|Xtest, Dtrain)."}, {"title": "3.1. High-level Structure: Embedding then ICL", "content": "TabICL comprises two key modules: a tabular embedding\nmodule followed by an ICL module, with labels utilized\nexclusively in the ICL module. The tabular embedding mod-\nule encodes table rows into dense vector representations\nwhile taking the inherent column-row structure into account.\nThis module consists of two components: distribution-aware\ncolumn-wise embedding, which captures statistical charac-\nteristics of individual columns, and context-aware row-wise\ninteraction, which models dependencies between features.\nThe ICL module subsequently processes both training and\ntest embeddings through a transformer, enabling the predic-\ntion of the entire test set in a single forward pass through\nICL. The overall architecture is depicted in Figure 1."}, {"title": "3.2. Distribution-aware Column-wise Embedding", "content": "The column-wise embedding (or feature embedding) maps\neach scalar cell in a column cj \u2208 \u211d^n into a d-dimensional\nembedding. Unlike typical approaches that assign a separate\nlinear layer to each column (Gorishniy et al., 2022), we\nembed all columns through a shareable Set Transformer\nTFcol, as illustrated in Figure 2b and formulated as follows:\n$W, B = TFcol(Cj) \\in \\mathbb{R}^{n \\times d}$\n$e_j = W c_j + B  \\in \\mathbb{R}^{n \\times d}$\nNote that each cell in a column is assigned its own weight\nand bias. Essentially, feature embedding can be framed as\na set-input problem, where TFcol serves as a hypernetwork\ntaking as input a permutation-invariant set of cell values and\ngenerating distribution-aware weights and biases.\nThe operations within TFcol unfold as follows:\n$U = Lin(c) \\in \\mathbb{R}^{n \\times d}$\n$M = MAB1 (V1, Utrain, Utrain) \\in \\mathbb{R}^{k \\times d}$\nISAB\n$V = MAB2(U, M, M) \\in \\mathbb{R}^{n \\times d}$\n$W, B = Lin(V) \\in \\mathbb{R}^{n \\times d}$\nwhere Lin represents a linear layer, MAB denotes multi-\nhead attention block, and IASB stands for induced self-\nattention block (Figure 2a), introduced by (Lee et al., 2019)."}, {"title": "TabICL: A Tabular Foundation Model for Large Data", "content": "A column c (omitting the subscript j for simplicity) is pro-\njected into a d-dimensional space (d = 128) via a linear\nlayer, processed by ISAB consisting of MAB1 and MAB2,\nand passed through linear layers to generate W and B.\nISAB reduces self-attention complexity to O(n) while pre-\nserving the ability to capture global information through a\ntwo-stage attention mechanism. In MAB\u2081, inducing vectors\nV\u2081 act as queries and attend to training samples Utrain to\ngenerate induced representations M. In MAB2, inputs U\n(including both training and test samples) serve as queries\nand attend back to M, enabling global information to prop-\nagate across all training samples. We use k = 128 inducing\nvectors, 4 attention heads in the MABs, and 3 ISAB blocks\n(only one ISAB block is shown in Equations (4) and (5) for\nclarity). Crucially, only train samples serve as keys and val-\nues in MAB1. This ensures that the outputs of ISAB depend\nsolely on training data, thereby preventing data leakage.\nTo get insights into the information captured in the learned\nfeature embeddings, we visualize M \u2208 \u211d^(k\u00d7d) (Equation (4))\nproduced by the final ISAB block of TFcol. We summarize\nM by summing along the first dimension (i.e., aggregating\nthe induced representations of all inducing vectors) to obtain\na single vector per column, and then apply Principal Com-\nponent Analysis. Figure 3 reveals that columns with similar\nskewness (resp. kurtosis) tend to cluster together, i.e., non-\nsymmetric distributions are differentiated from symmetrical\nones (Figure 3 left), and heavy-tailed distributions are dif-\nferentiated from light-tailed ones. This suggests that TFcol\nencodes distributional properties in a structured manner,\nand cells in a column are probably embedded to reflect their\nstatistical role (e.g., min, max, mean, mode). Therefore,\nthe learned feature embeddings should distinguish features\nbased on their unique distributional properties, effectively\nserving as feature identifiers. This contrasts with methods\nthat rely on semantic column names (Kim et al., 2024) or\nlearn feature identifier vectors (Kossen et al., 2021)."}, {"title": "3.3. Context-aware Row-wise Interaction", "content": "After obtaining all feature embeddings E = [e1,\u2026, em] \u2208\n\u211d^(n\u00d7m\u00d7d), a 3-layer transformer with 8 attention heads, de-\nnoted by TFrow, processes E for inter-feature interactions.\nTo aggregate the embeddings into a single vector, four learn-\nable [CLS] tokens are prepended to each row of E, and their\nfinal outputs are concatenated together. We use four tokens\nto provide richer representations with a total embedding\nsize of 4 x d = 512 for subsequent ICL, while maintaining\na lower embedding size (d = 128) for TFcol and TFrow to\nreduce memory consumption.\nA defining characteristic of tabular data is that columns\ndo not have a natural ordering. Ideally, tabular methods\nshould be invariant to permutations of columns. Unlike\nTabPFN (v1), TabICL naturally incorporates this invariance"}, {"title": "TabICL: A Tabular Foundation Model for Large Data", "content": "through the row-wise attention. However, we experimen-\ntally observed that TFrow can suffer from a representation\ncollapse issue. As mentioned earlier, features are identified\nby their distributional properties after column-wise embed-\nding. Consequently, features originating from similar dis-\ntributions thus become less distinguishable. In the extreme\ncase where all features are drawn from the same distribu-\ntion, TFrow cannot differentiate a sample from any of its\ncolumn-permuted versions, leading to nearly identical repre-\nsentations for originally distinct samples, i.e., representation\ncollapse. This phenomenon is exemplified by the balance\nscale dataset (Siegler, 1976), as shown in Figure 4. In\nthis dataset, all features follow the same discrete distribution\nand can take only 5 values, making collapse highly probable.\nAfter processing with TFrow, we observe that many samples\ncollapse to the same representation (rightmost plot), despite\nbeing originally distinct (leftmost plot).\nTo break the symmetry between identically distributed fea-\ntures, we incorporate rotary positional embedding (ROPE,\nSu et al., 2023) in TFrow. ROPE is widely adopted in recent\nLLMs (Dubey et al., 2024) and directly encodes relative\npositional information into the attention mechanism by ro-\ntating the query and key vectors. The rotation angle is\ndetermined by the position p in the sequence and the di-\nmension index i, defined as $\\theta_i = p/(base^{2i/d})$, where d is\nthe embedding dimension and base is the frequency scaling\nfactor. More details can be found in Appendix C."}, {"title": "3.4. Dataset-wise In-Context Learning", "content": "After converting all samples into embeddings H \u2208 \u211d^(n\u00d74d),\ntraining labels are mapped to the same space as H using\none-hot encoding. The embeddings of X and y for the\ntraining set are added to create the final training embeddings\nHtrain. We then process Htrain and Htest using a 12-layer\nTransformer with 4 attention heads, denoted by TFicl. The\nembeddings in Htrain can attend to one another while those\nin Htest can only attend to Htrain. Finally, a two-layer MLP"}, {"title": "TabICL: A Tabular Foundation Model for Large Data", "content": "converts the outputs of Htest into class probabilities for the\ntest samples."}, {"title": "4. Pretraining and Inference", "content": ""}, {"title": "4.1. Improved Pretraining Synthetic Datasets", "content": "TabICL is pre-trained exclusively on synthetic datasets. To\nensure realistic dependencies among variables, we gener-\nate these datasets using structural causal models (SCMs),\nfollowing the approach proposed in TabPFN (v1). We first\nsample a directed acyclic graph (DAG) to define depen-\ndencies, following the structure of a fully connected MLP,\nwhere each neuron corresponds to a variable. Each feature\nc is then modeled as a function f of its parent variables\nPa(c) in the graph, with added independent noise \u03f5, i.e.,\nc = f(Pa(c)) + \u03f5. Compared to previous work, we enrich\nthe dataset generation in two ways: (i) we introduce tree-\nbased SCMs to benefit from their inductive biases, and (ii)\nincrease the diversity of modeling functions f."}, {"title": "TabICL: A Tabular Foundation Model for Large Data", "content": "Tree-based generation As tree-based models excel on\ntabular data, we introduce tree-based SCMs to leverage\ntheir ability to model complex interactions and hierarchi-\ncal dependencies between variables. We define f using an\nXGBoost regression model, as it is widely favored by practi-\ntioners and supports multi-output regression. At each layer\nof the graph, an XGBoost model is trained on fake targets\ndrawn from Gaussian noise, taking the values of the parent\nvariables as input. The obtained predictions then become\nthe values of the child variables. To balance data generation,\nwe combine SCMs (70%) with tree-based SCMs (30%).\nAppendix B gives details and examples of generated data.\nDiversifying activation functions In TabPFN (v1),\nf is defined as a random affine mapping (a lin-\near layer) with an activation function chosen from\n[Identity, Tanh, Leaky ReLU, ELU]. We enrich this set with\n15 additional activation functions to enhance the diversity\nof non-linear dependencies, introducing for example non-\nmonotone or discontinuous functions. We also include ran-\ndom activation functions sampled from Gaussian processes\nwith random kernels. Gaussian Process functions have been\nused for synthetic data generation for time series foundation\nmodels (Ansari et al., 2024), but not as activation functions\nand with different types of kernels. Finally, we added an\noption to use different activation functions across layers\nand applied standardization followed by random rescaling\nbefore each activation function. Figure B.1 visualizes the\nemployed activation functions."}, {"title": "4.2. Curriculum Learning for Large-scale Pretraining", "content": "Similar to pretraining LLMs on shorter sentences before\nmoving to longer ones, we gradually increase the size of\nsynthetic datasets (i.e., the number of samples) while adjust-\ning the micro batch size NB used for gradient accumulation\nto accommodate memory constraints, as follows:\n1. NB = 4 with a fixed size of 1,024 for 100K steps;\n2. NB = 1 with the size randomly drawn from a log-\nuniform distribution between 1K and 40K over 2K\nsteps. Activation checkpointing is enabled for datasets\nexceeding 10K samples, and we accordingly reduce\nthe number of features to avoid out-of-memory issues;\n3. NB = 1 with the size uniformly sampled between 40K\nand 60K for 50 steps, training only TFicl while freezing\nall other components.\nEach step consists of 512 datasets with the number of\nfeatures (< 100) and classes (< 10) randomly sampled.\nFlashAttention and automatic mixed precision are applied\nglobally. The pretraining took 2 weeks on three A100 GPUs\nwith 40GB memory using PyTorch (10, 3, and 1 days for\nstage 1, 2, and 3, respectively). Appendix D.1 gives more\npretraining details."}, {"title": "4.3. Hierarchical Class-extension Strategy", "content": "We tackle many-class classification problems (> 10 classes)\nthrough hierarchical classification (Silla & Freitas, 2011).\nSpecifically, we recursively and evenly partition classes\ninto subgroups of up to 10 classes, forming a multi-level\nclassification tree. A classification problem with k classes\nrequires a hierarchy with depth r = \u2308log10k\u2309. Each node in\nthe tree corresponds to a sub-task that predicts probabilities\nfor its subgroup. During inference, the final probability for\na given class is obtained by multiplying the probabilities\nacross all relevant nodes from root to leaf.\nAs noted earlier, labels are used exclusively in the final ICL\nblock. Consequently, the hierarchical tree is constructed dur-\ning dataset-wise ICL, with all sub-tasks sharing the learned\nrow embeddings H and the same TFicl for ICL predictions.\nThis kind of sharing greatly enhances the efficiency of\nTabICL in hierarchical classification scenarios."}, {"title": "4.4. Memory-efficient Inference", "content": "Using FlashAttention, which offers linear memory complex-\nity with respect to sequence length, we observed that the\npeak activation memory can be effectively modeled by a\npolynomial regression during inference: a1 \u00d7 batch_size +\na2 \u00d7 seq len + a3 \u00d7 batch_size \u00d7 seq_len + a4. This enables\nus to dynamically adjust the batch size based on sequence\nlength and available GPU memory. The batch dimension\nserves different roles upon the context: it represents the\nnumber of columns for column-wise embedding, the sample\nsize for row-wise interaction, and the number of datasets\nfor dataset-wise ICL. Inspired by Rajbhandari et al. (2021),\nintermediate activations can be offloaded to CPU and disk"}, {"title": "TabICL: A Tabular Foundation Model for Large Data", "content": "as needed to further reduce memory consumption. These\noptimizations enable TabICL to handle datasets with 100K\nsamples and 500 features using only 5 GB of GPU mem-\nory and 32 GB of RAM. This suffices for most real-world\napplications. See Appendix D.2 for details."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Benchmark", "content": "We evaluate TabICL on the benchmark by Ye et al. (2025),\nreferred to as \u201cTALENT\u201d here. It comprises 200 classifi-\ncation datasets (120 binary and 80 multiclass datasets) and\ncomes with results for over 30 baselines. We will first an-\nalyze the 188 datasets with at most 10 classes, since these\ncan be handled natively by TabICL and TabPFNv2.\nDatasets are split into 64% training, 16% validation, and\n20% test data. While most models rely on the validation set\nfor early stopping and hyperparameter tuning, TabICL and\nTabPFNv2 do not. Nonetheless, we opted to train TabICL\nand TabPFNv2 using only the training data, which places\nthem at a disadvantage. On the flip side, both TabICL and\nTabPFNv2 leverage ensembles, unlike the other deep learn-\ning models. A fair comparison would require training the\nother models as an ensemble as well. However, this is com-\nputationally expensive, was not part of the original bench-\nmark, and is not implemented in these experiments. Both\nTabICL and TabPFNv2 average over 32 predictions obtained\nby randomly shuffling columns and classes and using dif-\nferent pre-processors. TabICL applies z-normalization with\nor without power transformation to all input features. See\nHollmann et al. (2025) for details on TabPFNv2. To avoid\nout-of-memory issues, we subsample training sets to 30K\nsamples for TabPFNv2, while TabICL could predict on all\ndatasets without subsampling. In addition, we disable auto-\nmatic mixed precision across all methods during inference\nto ensure reproducibility and consistent time measurements."}, {"title": "5.2. Results", "content": "TabICL obtains state-of-the-art accuracy. Figure 5\nshows accuracies relative to the tuned MLP for all models.\nTabICL obtains the best median relative accuracy across all\ndatasets while being much faster than traditional state-of-\nthe-art models: the geometric mean training+inference time\nper 1K samples for TabICL is 1.1 seconds, while tuning\nCatBoost on a CPU takes around 3 minutes, and RealMLP\nand ModernNCA on a GPU take around 7 minutes. Looking\nat the ranks of each method based on the obtained accura-\ncies, the critical difference diagram in Figure E.1 shows that\nTabICL and TabPFN2 outperform competitors by a wide\nmargin, while the difference between the two of them is not\nstatistically significant."}, {"title": "TabICL enables ICL for large datasets.", "content": "While\nTabPFNv2 achieves excellent performance on datasets up\nto 10K samples, it has only been pre-trained with up to\n2048 training samples and can fail on datasets above 30K\nsamples due to its memory usage. Figure 7 shows that un-"}, {"title": "Speedup over TabPFNv2.", "content": "Figure 6 shows that TabICL\nis 1.5x faster than TabPFNv2 on small datasets and 3-10\u00d7\nfaster on large datasets. This is facilitated by the hybrid ar-\nchitecture of TabICL, using fewer of the expensive row-wise\nand column-wise attention layers with smaller embedding\ndimension before ICL on tokenized rows. On a dataset with\n10,000 samples and 100 features, TabICL takes roughly 20\nseconds versus 1 minute 40 seconds for TabPFNv2, while\nfor a dataset of 1000 samples and 10 features, TabICL takes\n1 second compared to 2 seconds for TabPFNv2. In Fig-\nure A.1, we fit simple scaling laws to predict the runtime of\nTabICL and TabPFN2 based on the number of samples and\nfeatures. These scaling laws show that the average speedup\nof TabICL remains around five-fold for large datasets."}, {"title": "TabICL produces reliable probabilities.", "content": "In many prac-\ntical situations, the quality of the estimated probabilities\nis crucial for decision-making. Therefore, we also report\nthe log loss (a.k.a. cross-entropy loss), which is a proper\nscoring rule and therefore rewards accurate prediction of\nprobabilities (Gneiting & Raftery, 2007). Since TabICL\ndoes not leverage hyperparameter tuning, its predictions are\nnot specifically optimized towards accuracy. The critical\ndifference diagrams in Appendix E show that TabICL and\nTabPFN2 significantly outperform accuracy-tuned competi-\ntors on the log loss, though the difference between the two\nis not significant, indicating that both produce more reliable\nprobability estimates than the other models."}, {"title": "TabICL remains effective with more than 10 classes.", "content": "On datasets with more than 10 classes, we apply the hier-\narchical classification strategy from Section 4.3 to TabICL.\nThanks to its use of label-independent row embeddings that\ncan be shared between all sub-classifiers, TabICL scales\nefficiently to a large number of classes. Figure 8 shows that\nTabICL still achieves the second best result on these datasets\nin terms of mean normalized accuracy, while TabPFNv2\ncannot natively handle more than 10 classes."}, {"title": "6. Conclusion", "content": "We introduce TabICL, a novel tabular foundation model\nthat extends the scalability of existing tabular foundation\nmodels by an order of magnitude. Evaluated on datasets\nwith up to 100K training samples, it delivers excellent per-\nformance without hyperparameter tuning, making it nearly\ntwo orders of magnitude faster than other tabular methods\nrequiring hyper-parameter tuning. TabICL achieves this\nthrough a hybrid architecture and memory-saving optimiza-\ntions. Compared to the newly released and leading tabular\nfoundation model TabPFNv2, our TabICL achieves com-\nparable performance while being more scalable and faster.\nLimitations Like other foundation models, TabICL suf-\nfers from slow inference speed, although TabPFNv2 has\ndemonstrated that this issue can be alleviated to some extent\nthrough caching. Currently"}]}