{"title": "Rethinking Code Refinement: Learning to Judge Code Efficiency", "authors": ["Minju Seo", "Jinheon Baek", "Sung Ju Hwang"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating codes. Due to these capabilities, many recent methods are proposed to automatically refine the codes with LLMs. However, we should rethink that the refined codes (from LLMs and even humans) are not always more efficient than their original versions. On the other hand, running two different versions of codes and comparing them every time is not ideal and time-consuming. Therefore, in this work, we propose a novel method based on the code language model that is trained to judge the efficiency between two different codes (generated across humans and machines) by either classifying the superior one or predicting the relative improvement. We validate our method on multiple programming languages with multiple refinement steps, demonstrating that the proposed method can effectively distinguish between more and less efficient versions of code.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown significant success across a wide range of tasks, extending from natural language understanding to programming-related activities (Brown et al., 2020; Chen et al., 2021; Achiam et al., 2023; Touvron et al., 2023; Rozi\u00e8re et al., 2023; Abdin et al., 2024). Specifically, thanks to their capabilities in understanding and generating codes, LLMs are able to allow developers to save time, reduce errors, and boost their productivity (Shen et al., 2022). For example, several recent studies have utilized LLMs to optimize and refine the existing code bases (Madaan et al., 2023; Chen et al., 2023b). Also, more recent work has been proposed to iteratively refine the generated codes from LLMs by judging them with LLMs (Zelikman et al., 2023). Another noteworthy approach involves using LLMs to check the functional correctness of the generated codes from LLMs (Dong et al., 2023a).\nHowever, despite huge advancements made in the field of LLMs for code generation, the aforementioned studies assume that the codes generated and refined from LLMs are more efficient than their originals. However, as shown in Figure 1, our observations contradict this assumption, showing that LLM-generated and -refined codes do not always perform better. To handle this issue, while one may calculate the efficiencies of codes both before after modifications by actually executing them, this process introduces unnecessary inefficiencies, and may be very costly and time-consuming.\nIn this work, to overcome those challenges, we introduce a new task of judging the efficiency of the refined code over its original version. In addition, not only LLMs but also human coders may degrade the efficiency of codes during refinement; thus, our task of judging the efficiency between two codes involves all the possible pairs of code modification sources, including human-human, human-machine, and machine-machine. Then, to address this new task, we propose a new model (based on the code LM) that is trained to compare efficiencies between two codes. Specifically, given a code pair (one is original and the other is refined from it), the code LM is trained to classify which one is more efficient or predict how much the refined code is efficient.\nWe experimentally validate the effectiveness of our efficiency judgement model on multiple code refinement scenarios with multiple programming languages. The results show that our judgement model can identify the more efficient code among two different versions, substantially outperforming baselines. Our contributions are as follows:\n\u2022 We point out that the refined codes from LLMs or humans do not always improve their efficiency.\n\u2022 We introduce a novel approach that judges the efficiency of two different versions of codes.\n\u2022 We validate our model on multiple code refinement scenarios, demonstrating its effectiveness."}, {"title": "2 Related Work", "content": "Large Language Models for Code Large Language Models (LLMs), trained on extensive corpora with multi-billion parameters, exhibit remarkable performance across a broad spectrum of tasks involving both text and code (Brown et al., 2020; Li et al., 2022; Rozi\u00e8re et al., 2023; Li et al., 2023; Achiam et al., 2023; Abdin et al., 2024; Guo et al., 2024). These models, particularly those trained on code-specific datasets, have opened up a new era in software development by not only assisting with basic programming tasks but also enabling more complex activities such as code generation (Chen et al., 2023a; Nijkamp et al., 2023), translation (Yin and Neubig, 2018; Rozi\u00e8re et al., 2020; Cassano et al., 2023), and refinement (Yu et al., 2023; Shirafuji et al., 2023). As such, these models are increasingly integrated into development environments, optimizing workflows, and reducing the time for development (Shen et al., 2022; Dong et al., 2023b).\nLLMs for Code Refinement Beyond basic code generation, LLMs are widely used to refine the existing code bases. One of the early work in code refinement aims to detect and fix bugs in the codes by pre-training a transformer-based model on English and source code, and then fine-tuning it on commits (relevant to the part fixing bugs and improving performance) (Garg et al., 2022). In a similar vein, another work proposes to refine the code with a sequence-to-sequence model that is trained to transform the original code to its optimized version of the code (Chen et al., 2023b). Additionally, recent work showcases that LLMs are able to recursively improve their own generated codes, progressively enhancing their outputs (Zelikman et al., 2023). Further, Madaan et al. (2023) demonstrate that LLMs with sophisticated prompting strategies (to adapt LLM for code optimization) can surpass human-level performance in code optimization tasks. However, despite these substantial achievements, prior studies have primarily focused on code enhancement with limited attention to the actual efficiency of the refined code. Meanwhile, we focus on a different angle, proposing to judge the efficiency of the refined codes in advance (without executing them) based on our observation that not all the refined codes have better efficiency.\nLLM-Powered Code Evaluation The objective of our work which aims to evaluate the efficiency between two codes based on LLMs has a similarity to work on LLMs for code evaluation. Early work on it uses either a term-matching-based approach (similar to BLEU) or an embedding-based approach (whose representations are obtained from language models), to compare two codes (Ren et al., 2020; Zhou et al., 2023). However, as collecting ground-truth answers for every evaluation is difficult, recent work has shifted towards using LLMs to judge the quality of the generated code, such as its utility or functional correctness, without the need for comparisons to the reference code (Dong et al., 2023a; Zhuo, 2024). Yet, unlike these approaches evaluating the single instance of the code other than the efficiency, we aim to compare efficiency in the setting where the code pair is given."}, {"title": "3 Method", "content": "We first provide a general description of code refinement, and then introduce our approach."}, {"title": "3.1 Code Refinement", "content": "Let us assume that the existing code base is defined as c, which consists of a sequence of tokens as follows: c = {C1, ..., Cn}. Then, the objective of code refinement (in this work) is to transform the existing code base c into its improved version c' = {\\hat{c_1}, ..., \\hat{c_m}}, where the execution time for c' should be faster than c, formalized as follows:"}, {"title": "3.2 Judging Code Efficiency", "content": "To overcome the aforementioned limitation, we aim to predict the efficiency of the modified code over its original code, without actually executing them. This can be formulated by either the classification problem (where we classify the superior code) or the regression problem (where we predict the relative improvement of the modified code over the original code), given a pair of original and modified codes. Also, note that we operationalize classification and regression problems with CodeLLMs, due to their capabilities in understanding codes.\nSpecifically, given the code pair (e.g., c and c'), we concatenate it and provide it with the CodeLLM, formalized as follows: 0 = CodeLLM([c, c']) where [\u00b7] is the concatenation operation, and o is the prediction output. Then, for the classification problem, we formulate it as the next token prediction task: Lc = -log p(o'|[c, c'], o' \u2208 {A, B}), where o' is ground truth that belongs to one of A or B, in which A represents the improvement over the original code and vice versa for B. Similarly, for the regression problem, we train the model to predict the relative improvement of the refined code over its original code by minimizing this prediction value with the actual relative improvement."}, {"title": "4 Experiment", "content": "We now describe experimental setups and results. We provide our code at https://github.com/going-doer/judge_code_efficiency, for reproducibility."}, {"title": "4.1 Experimental Setups", "content": "Dataset To validate the efficacy of our approach to judge code efficiency, we should collect pairs of two different versions of codes before and after modifications. Here, we consider three different scenarios of code editing, and, for the cases where humans refine the code, we use a dataset of code edits made by humans from Madaan et al. (2023). For the other scenarios where the machine improves the human- or machine-generated codes, we prompt the Code LLM (namely DeepSeek-Coder-Instruct-7B) (Guo et al., 2024) to refine the given codes for better efficiency. Specifically, starting with the codes generated by humans from the existing dataset, we generate the machine-refined codes with the Code LLM. In addition, from those machine-generated codes, we similarly prompt the Code LLM to improve them. Through these steps, we can obtain pairs of human-human, human-machine, and machine-machine code versions.\nBaselines and Our Model In this work, as we tackle a novel problem of judging code efficiency, there are no direct baselines available to compare. Therefore, we turn to compare our approach against the basic models powered by LLMs. Specifically, given a code pair, we perform zero-shot and few-"}, {"title": "4.2 Experimental Results", "content": "Main Results We report the main results in Table 1, and, from this, we observe that our method consistently outperforms all baseline models across all settings. Surprisingly, we find that our approach is substantially superior to GPT-3.5 and GPT-40, demonstrating the continued limitations of even larger models in judging the code efficiency, which further supports the efficacy of our training strategy for it. In addition, our model is particularly effective in scenarios where there is a clear difference in code efficiency \u2014 specifically, a difference exceeding 10% (the easy setting). To examine the performance of our model more granularly based on varying degrees of efficiency differences between code pairs, we bucketize the code pairs based on their efficiency differences. As shown in Figure 2, the performance of our model increases when the difference between two codes becomes larger.\nResults with Varying Refinement Scenarios It is worth noting that our code refinement scenario is categorized as human-human, human-machine,"}, {"title": "5 Conclusion", "content": "In this work, we pointed out that the codes refined by humans or machines are sometimes inferior than originals, and to tackle this, we introduced a novel approach to identify the more efficient code given a pair of codes before and after modifications. We validated our method on multiple code editing scenarios involving both humans and machines, showcasing its substantial efficacy despite its simplicity."}, {"title": "Limitation", "content": "There are some areas that future work may improve upon. First, we perform experiments with two widely used programming languages, such as Python and C++, and it may be promising to consider other languages, particularly those used less frequently. In addition, in terms of measuring the code efficiency, we consider its execution time, meanwhile, there may be additional factors to consider, such as memory usage, I/O operations, and the underlying OS environment. Future work may incorporate these factors to perform a more holistic assessment of program efficiency. Lastly, beyond predicting the efficient code, future work may explore its interpretability, providing the reasons why certain codes are more efficient than others at a more fine-grained level (e.g., lines of codes)."}, {"title": "Ethic Statement", "content": "We believe this work does not have particular concerns about ethics. This is because, it strictly focuses on the technological aspect of comparing code efficiency, which does not engage in the unethical use of LLMs for manipulating software codes, user data, or any other sensitive information."}]}