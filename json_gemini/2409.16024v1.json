{"title": "Bridging Environments and Language with Rendering Functions and Vision-Language Models", "authors": ["Th\u00e9o Cachet", "Christopher R. Dance", "Olivier Sigaud"], "abstract": "Vision-language models (VLMs) have tremendous potential for grounding language, and thus enabling language-conditioned agents (LCAs) to perform diverse tasks specified with text. This has motivated the study of LCAs based on reinforcement learning (RL) with rewards given by rendering images of an environment and evaluating those images with VLMs. If single-task RL is employed, such approaches are limited by the cost and time required to train a policy for each new task. Multi-task RL (MTRL) is a natural alternative, but requires a carefully designed corpus of training tasks and does not always generalize reliably to new tasks. Therefore, this paper introduces a novel decomposition of the problem of building an LCA: first find an environment configuration that has a high VLM score for text describing a task; then use a (pretrained) goal-conditioned policy to reach that configuration. We also explore several enhancements to the speed and quality of VLM-based LCAs, notably, the use of distilled models, and the evaluation of configurations from multiple viewpoints to resolve the ambiguities inherent in a single 2D view. We demonstrate our approach on the Humanoid environment, showing that it results in LCAs that outperform MTRL baselines in zero-shot generalization, without requiring any textual task descriptions or other forms of environment-specific annotation during training.", "sections": [{"title": "1. Introduction", "content": "The widespread adoption of large language models (LLMs) and text-to-image models in modern society demonstrates the convenience of natural language interaction with AI models (Bubeck et al., 2023; Rombach et al., 2022). This motivates the study of language-conditioned agents (LCAs) that can execute diverse commands, specified with text, in a given environment (Colas et al., 2020; Zhou et al., 2023). Large-scale internet-scraped text and image data is a key enabler of current LLMs and text-to-image models (Schuhmann et al., 2022; Gadre et al., 2023; Penedo et al., 2023). However, data of comparable scale relating text with environments does not exist and human annotation is costly, leading to the question: how can we learn LCAs given the scarcity of annotated data?\nRecently, foundation models (FMs) have emerged as promising tools to tackle this challenge. LLMs have been used to orchestrate predefined motion primitives (Huang et al., 2022; Zeng et al., 2022; Ichter et al., 2023), but the learning of such primitives also suffers from the lack of annotated data. Another approach is to use LLMs or vision-language models (VLMs) to define reward functions for a given text, either by source-code generation (Yu et al., 2023; Perez et al., 2023; Ma et al., 2023), or by image-text similarity evaluation (Mahmoudieh et al., 2022; Tam et al., 2022; Cui et al., 2022; Rocamonde et al., 2023; Adeniji et al., 2023). One may couple such rewards with single-task reinforcement learning (STRL), but such approaches are limited by the need to train a policy for each new task, making them too slow for real-time application. Training multi-task reinforcement learning (MTRL) agents is a natural alternative (Fan et al., 2022), which we explore in this paper. However, it is not obvious how to construct a training corpus of textual task descriptions that enables MTRL agents to generalize reliably to new tasks (Cobbe et al., 2020; Yu et al., 2020).\nTherefore, this paper explores a novel decomposition of the problem of building LCAs into VLM-based text-to-goal generation and goal-reaching, as shown in Figure 1: we find a configuration such that rendered images of the environment in this configuration have high VLM scores for a given text; then we use a goal-conditioned reinforcement learning (GCRL) agent to reach that configuration\u00b9. In this paper, configurations capture enough of the state to render"}, {"title": "2. Related Work", "content": "Central to our work is the challenge of grounding language: the process of linking language with an agent's observations and actions (Harnad, 1990). In this section, we first discuss the use of environment-specific annotations, LLMs and VLMs for grounding language and building LCAs. Then, we focus on text-to-goal generation methods.\nAnnotation. A natural approach to grounding is to gather textual annotations for an environment. For instance, state descriptions have been used to learn language-conditioned goal generators (Colas et al., 2020) and language-conditioned reward functions (Bahdanau et al., 2018; Nair et al., 2022). Additionally, state-sequence descriptions can be coupled with imitation learning (Stepputtis et al., 2020; Lynch & Sermanet, 2021; Chen et al., 2023) or with inverse reinforcement learning (Fu et al., 2019; Zhou & Small, 2021) to create LCAs. To reduce the cost of human annotation, some works generate annotations algorithmically (Jiang et al., 2019; Hill et al., 2020; Stooke et al., 2021), but so far such works are limited to simple tasks such as \u201cplace X near Y\u201d.\nFoundation models (FMs). Another way to circumvent costly human annotation is to exploit FMs. Given a textual task, LLMs have been used to write the source code of reward functions (Yu et al., 2023; Perez et al., 2023; Ma et al., 2023) or to orchestrate predefined skills (Huang et al., 2022; Ichter et al., 2023). LLMs have also been coupled with VLMs to leverage visual information (Zeng et al., 2022; Huang et al., 2023b; Ajay et al., 2023). However LLMs require environment-specific prompting and are prone to hallucination (Huang et al., 2023a).\nOther works have also explored language grounding using VLMs. For instance, VLMs can be coupled with rendering functions to derive reward functions from natural language (Mahmoudieh et al., 2022; Fan et al., 2022; Rocamonde et al., 2023; Baumli et al., 2023), to pretrain language-conditioned policies (Adeniji et al., 2023), to derive extrinsic reward functions for exploration (Tam et al., 2022), and to detect task-completion (Du et al., 2023). Unfortunately, VLM-based reward functions are costly to evaluate, and they are highly oscillatory (\u2018noisy'), as illustrated in Figure 3 of Adeniji et al. (2023), leading to slow and unreliable RL. Our approach also relies on VLMs, but we circumvent these difficulties by only using VLMs to find configurations with high VLM scores, as we now discuss.\nText-to-goal methods. Text-to-goal methods identify (sets of) states that align with a given textual description. These states may then be fed to goal-conditioned policies (Colas et al., 2020; Akakzia et al., 2021) or used to construct hybrid controllers (Raman et al., 2013), and thus"}, {"title": "3. Methods", "content": "We address the overall problem of finding a language-conditioned policy: given text describing a (potentially previously unseen) task to be performed in an environment, the policy should result in configurations of that environment that correspond well to the given text. Our approach is to decompose this into two subproblems: finding configurations of the environment with high VLM scores for a given text; and designing goal-conditioned policies to reach such configurations. This decomposition has two main advantages over end-to-end training of LCAs. First, it is easy to generate a huge number of configurations to train a goal-conditioned policy, helping with generalization to new tasks, and avoiding the need to collect a large training set of task descriptions. Second, as grounding is independent of low-level control in our approach, it is possible to determine whether failures of the LCA are due to poor alignment of the goal with the task description, or to failure of the low-level controller to reach the goal.\nThe section begins with definitions relating configurations with VLM scores, before presenting methods for optimizing such scores that enable a range of speed-quality tradeoffs. Then we discuss the combination of text-to-goal methods with goal-conditioned policies, to craft zero-shot LCAs."}, {"title": "3.1. Definitions", "content": "Our approach selects configurations of an environment using rendering functions and VLMs, as we now explain."}, {"title": "Environment.", "content": "The environment is modelled as a controlled Markov process\n$\\mathcal{M} = (S, A, \\rho, P)$,\nwith state space $S$, action space $A$, initial state distribution $\\rho$ and transition distribution $P$. The controlled Markov process may be augmented with a reward function $R : S \u00d7 A \u2192 \\mathbb{R}$ to define a Markov decision process (MDP). In this work, we define various reward functions, using distances to goal configurations, or VLM scores for a given text (in our baselines)."}, {"title": "Configurations.", "content": "Each state of $S$ is associated with a specific configuration $q \u2208 Q$, which captures the state dimensions relevant to rendering images of the environment. For instance, a configuration might consist of the angles or positions of an environment's bodies, but not their velocities. As we use gradient methods to optimize configurations, we assume the configuration space $Q$ can be represented as a subset of a real Euclidean space. Typically, not all configurations are admissible: there may be inequality constraints corresponding to the requirement that objects do not interpenetrate (unilateral constraints) or to safety requirements. We denote the admissible subset of configurations by $Q_a$."}, {"title": "Rendering functions.", "content": "A rendering function $f_{\\text{render}} : Q \u2192 \\mathcal{I}$ maps configurations to images of the environment. Suitable rendering functions can be found in MuJoCo (Todorov et al., 2012) and OpenGL (Neider et al., 1993)."}, {"title": "VLMs.", "content": "A VLM, such as CLIP (Radford et al., 2021), typically consists of\n1. An image encoder $f_{\\text{image}} : \\mathcal{I} \u2192 \\mathbb{R}^d$ that maps images to an embedding space of dimension $d$; and\n2. A text encoder $f_{\\text{text}} : T \u2192 \\mathbb{R}^d$ that maps texts to the same embedding space. The space of texts $T$ consists of finite strings on a finite vocabulary.\nWe assume that the outputs of these encoders are normalized so that $||f_{\\text{image}}(\u00b7)|| = 1$ on $ \\mathcal{I}$ and $||f_{\\text{text}}(\u00b7)|| = 1$ on $T$. The image-text similarity score (or VLM score) for a given image $I \u2208  \\mathcal{I}$ and text $x \u2208  T$ is then defined as the cosine similarity of their embeddings:\n$S_{it}(I,x) := f_{\\text{image}} (I) \u00b7 f_{\\text{text}}(x)$."}, {"title": "Configuration-text score.", "content": "Composing a rendering function with the image-text similarity score enables us to define a configuration-text similarity score for a given configuration $q \u2208 Q$ and text $x \u2208  T$:\n$S_{qt}^{sv}(q, x) := S_{it} (f_{\\text{render}} (q), x)$,\nwhere 'sv' emphasizes that this is for a single view. To resolve ambiguities inherent in a single 2D view, we propose to extend this definition to a multiview configuration-text similarity score by averaging the similarity scores for multiple rendering functions $f_{\\text{render}}^1, .., f_{\\text{render}}^m$:\n$S_{qt}(q, x) := \\frac{1}{m} \\sum_{k=1}^{m} S_{it} (f_{\\text{render}}^k (q), x)$\n$= f_{\\text{config}}(q) \u00b7 f_{\\text{text}}(x)$,\nwhere the configuration embedding $f_{\\text{config}} : Q \u2192 \\mathbb{R}^d$ is defined by\n$f_{\\text{config}}(q) := \\frac{1}{m} \\sum_{k=1}^{m} f_{\\text{image}} (f_{\\text{render}}^k (q)).$"}, {"title": "Distilled model.", "content": "Given multiple rendering functions and billion-parameter VLMs, evaluating the configuration-text similarity score can be costly. Therefore, we propose to distill the configuration embedding into a neural network\n$f_{\\text{config}}(q) \u2248 \\hat{f}_{\\text{config}}(q)$.\nNot only is the distilled model $\\hat{f}_{\\text{config}}$ faster than $f_{\\text{config}}$, it is also readily differentiated with respect to the configuration: this proves useful when optimizing scores with respect to the configuration. In contrast, the gradients of $f_{\\text{render}}$ and hence $f_{\\text{config}}$ may not be defined; and even if they are, the embedding $f_{\\text{config}}$ may be highly oscillatory (see Figure 23 in the Appendix), making its gradients of questionable utility.\nWe use the distilled model in three ways: to sample a diverse dataset for retrieval of configurations; to finetune the resulting configurations; and to train STRL and MTRL baselines."}, {"title": "3.2. Text-to-Goal Generation Methods", "content": "Given a text $x$, we wish to find an admissible configuration $q^\u2217$ that maximizes the configuration-text similarity score:\n$q^\u2217 \u2208 \\underset{q\u2208Qa}{\\text{argmax }} \\hat{f}_{\\text{config}}(q) \u00b7 f_{\\text{text}}(x)$.\nAs the similarity score can be highly multimodal and costly to evaluate, we adopt a three-step approach to optimizing it for a given text, which involves: (i) retrieving high-scoring configurations from a dataset of precomputed configuration embeddings; (ii) starting from those configurations, performing gradient ascent on an approximate version of the similarity score, based on the distilled model of the configuration embedding; and (iii) selecting from the resulting configurations using the exact similarity score, based on the VLM. These steps are explained in detail below. Optionally, one might stop immediately after step (i), returning the single best configuration in the dataset; or after step (ii), returning the best configuration according to the approximate score."}, {"title": "3.2.1. RETRIEVING CONFIGURATIONS", "content": "As the configuration embedding $f_{\\text{config}}$ is independent of the text, one way to mitigate the cost of optimizing the configuration-text similarity is to work with a dataset of configurations $D := {q1, q2, ..., qn} \u2282 Qa$ with precomputed configuration embeddings $f_{\\text{config}}(q)$ for $q \u2208 D$. Given a text $x$, one may then retrieve a configuration with a high configuration-text similarity score, simply by taking dot products with those precomputed embeddings:\n$q_{\\text{retrieved}} \u2208 \\underset{q\u2208D}{\\text{argmax }} f_{\\text{config}}(q) \u00b7 f_{\\text{text}}(x)$.\nThe effectiveness of this retrieval method hinges on the choice of the retrieval dataset $D$. To ensure high-scoring configurations for any text, dataset $D$ must encompass diverse configurations. We therefore propose the following three dataset-generation methods aiming for diversity, while ensuring admissibility of the resulting configurations (full details are given in Appendix B):\nRandom policy dataset. This dataset consists of configurations resulting from a random policy interacting with the environment. To encourage diversity, the random policy samples actions uniformly over the action space.\nUniform sampling dataset. This dataset is generated by uniformly sampling from the configuration space $Q$ and projecting them onto admissible configurations $Qa$.\nEmbedding-diversity dataset. This dataset is created by focusing on the diversity of configuration embeddings, using the distilled model (5). We optimize a set of configurations by minimizing the maximum cosine similarity between embeddings of distinct configurations with the following loss:\n$L(q_1,..., q_n) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\underset{j\u2208[n] \\{i\\}}{\\text{max }} f_{\\text{config}}(q_j) \u00b7 f_{\\text{config}}(q_i).$"}, {"title": "3.2.2. FINETUNING CONFIGURATIONS", "content": "Even a huge retrieval dataset may lack configurations aligned with a given text. Therefore, we propose a method to finetune retrieved configurations. As the exact configuration-text similarity score is costly and may not be differentiable, we attempt to maximize a surrogate $S_{qt}$ for that score, based on the distilled model (5):\n$\\hat{S}_{qt} (q, x) := \\hat{f}_{\\text{config}}(q) \u00b7 f_{\\text{text}}(x)$.\nBy the product rule, the gradient of this surrogate score is\n$\u2207_q \\hat{S}_{qt} (q, x) = \u2207_q \\hat{f}_{\\text{config}}(q) \u00b7 f_{\\text{text}}(x)$.\nAs some configurations may not be admissible, we assume a projection operator $P_{Qa} : Q \u2192 Qa$ is available, which maps configurations to nearby admissible configurations. In our experiments, we implement $P_{Qa}$ with one step of the MuJoCo physics engine. (This step does no dynamics, it just recovers from any interpenetrations.) We use this projection to perform projected gradient ascent, with the update\n$q^{(j+1)} = P_{Qa} (q^{(j)} + \u03b1 \u00b7 \u2207_q \\hat{S}_{qt}(q^{(j)}, x))$,\nwhere $\u03b1$ is the learning rate. The full procedure is presented in Appendix E. In practice, both the gradient and projection calculations are highly parallelizable, allowing concurrent optimization of multiple solutions."}, {"title": "3.3. Zero-Shot LCA", "content": "To craft an LCA, we feed a text $x$ to one of the text-to-goal methods described above, resulting in a goal configuration $q_{goal}$. Then we feed that goal to a task-agnostic goal-conditioned policy $\u03c0$, which takes actions distributed as $a \u223c \u03c0(\u00b7|s, q_{goal})$ when in state $s$. We train this policy on a collection of goals with GCRL (Colas et al., 2022; Liu et al., 2022), using a reward based on the Euclidean distance between the current configuration and a goal configuration $q$:\n$R(s, a|q) = ||\\varphi_Q(s) \u2212 q|| \u2212 ||\\varphi_Q(P(s, a)) \u2212 q||$,\nwhere $\u03c6_Q : S \u2192 Q$ is the mapping from state to configuration, with configurations seen as Euclidean vectors, and $P$ gives the next state (assuming transitions distributed as $P$ are deterministic). At test time, we simply condition the GCRL agent on a goal configuration to execute the task, without additional training or pre-processing."}, {"title": "4. Experiments", "content": "In this section, we evaluate our text-to-goal methods and LCAs to address the following questions: (Q1) How does the choice of dataset-sampling method affect the performance of VLM-based configuration retrieval? (Q2) What is the impact of using multiple viewpoints to assess configurations in 3D environments? (Q3) What is the speed-quality trade-off of the proposed text-to-goal methods? (Q4) How do the proposed LCAs compare with STRL and MTRL baselines?"}, {"title": "4.1. Evaluation Benchmark", "content": "Environment and rendering functions. We evaluate our approach on the Humanoid environment from OpenAI's Gym framework (Brockman et al., 2016), which we augment with feet and with a cube, enabling stable standing and agent-object interaction. We create three MuJoCo rendering functions front view, right view and left view"}, {"title": "4.2. Dataset-Sampling Evaluation (Q1)", "content": "We sample three datasets of $2.5 \u00d7 10^6$ configurations using the three dataset-sampling methods described in Section 3.2.1 and precompute their configuration embeddings (4) with the front-view rendering function and the EVA-02-E-14+ VLM (Sun et al., 2023). Then for all tasks, we retrieve the configuration (6) with the highest configuration-text score from each dataset.\nTable 1 presents the configuration-text scores of the retrieved configurations as assessed by various VLMs, averaged over all tasks. We observe that the configurations retrieved from the embedding-diversity dataset attain the highest scores for all VLMs. Therefore we use that dataset-sampling method in the remaining experiments. Further results comparing different dataset sizes and using multiview scores are discussed in Appendix B."}, {"title": "4.3. Multiview Configuration-Text Score (Q2)", "content": "We now study the impact of using multiple views to retrieve configurations, as in equation (3).\nTable 2 compares configurations retrieved from the embedding-diversity dataset using three views (at 0\u00b0, \u00b145\u00b0) with configurations retrieved using the front view (0\u00b0) only. These two sets of configurations are evaluated with single-view configuration-text scores for different viewpoints. We include a mid-left view at -22.5\u00b0 to assess generalization to rendering functions distinct from those used for retrieval. When evaluated on the front view, the configurations retrieved using the front view only attain higher scores than those retrieved using three views. But when evaluated from other views, their scores decrease significantly. In contrast, the configurations retrieved using three views exhibit little variation in score across the viewpoints tested.\n provides some insight into the lack of robustness of single-view retrieval, illustrating six tasks for which a high single-view configuration-text score is misleading. We identify the following three reasons for these failures:\nOcclusion. At times, some components hide other components. For instance, in the top three rows of column 1 of , the right leg of the humanoid is behind its body. This is not apparent in the front view (top row), but it becomes apparent in the left and right views. Using three views to evaluate configurations mitigates this issue.\nDistance ambiguity. Based on a single view, the distance between objects in a scene is ambiguous, even for a human. In columns 3 and 4, the humanoid and the cube appear close to each other in the front view (top row), but the side views (second and third rows) reveal that they are actually far apart.\nStability. While the top row of columns 5 and 6 appears to show configurations that correspond well to the specified tasks, the left and right views reveal that the positions are unstable. While our text-to-goal method does not explicitly aim for stability, stability emerges from optimizing the multiview configuration-text score.\nWe conclude that evaluating configurations using multiple views mitigates some of the problems arising from the lack"}, {"title": "4.4. Configuration Finetuning (Q3, Quality)", "content": "We now study the optimization of retrieved configurations. First, we use the configurations from the embedding-diversity dataset and their precomputed embeddings to train a distilled model (as detailed in Appendix D). Then for each task, we use that model to finetune the 256 top-scoring configurations (based on the multiview score) from the embedding-diversity dataset (as detailed in Appendix E).\nWe compare two options after finetuning: either we return the configuration with the highest approximate score (7) based on the distilled model; or we evaluate all finetuned configurations with EVA02-E-14+ and select the one with the highest exact multiview score (3). The choice of hyperparameters for finetuning is discussed in Appendix E: one reason for using 256 configurations is that it corresponds to one batch of VLM evaluations.\nTable 3 presents results of finetuning, indicating that the proposed finetuning methods further improve the configuration-text alignment of retrieved configurations, even when evaluated by other VLMs.  illustrates retrieved configurations and their finetuned counterparts, showing that even"}, {"title": "4.5. Time Efficiency (Q3, Speed)", "content": "Evaluating 256 three-view configuration embeddings with our distilled model takes $4.8 \u00d7 10^{\u22124}$ s, whereas computing those embeddings with EVA02-E-14+ takes 21.1 s (for the computing infrastructure described in Appendix F). Thus, the distilled model reduces the computation time of configuration embeddings by 40 000\u00d7.\nThe timings of our text-to-goal methods are as follows: embedding the text takes 0.013 s, retrieving high-scoring configurations with precomputed embeddings (by brute force, from a dataset of $2.5 \u00d7 10^6$ samples) takes 0.015 s; and the optional stages of finetuning and selecting the highest-scoring configuration (by applying EVA02-E-14+ to three views, corresponding to three batches) take 2.8 s and 21.1 s"}, {"title": "4.6. Language-Conditioned Agents (Q4)", "content": "We now detail our baselines and the GCRL policy. Then we compare the resulting LCAs.\nObservation and action spaces. Each of the LCAs receives a 343-dimensional observation vector. This includes the current position, orientation and velocity of each component of the Humanoid and the cube, along with the timestep (as detailed in Appendix A). For MTRL, this observation is augmented with the text embedding of the task. For GCRL, it is augmented with the goal configuration. The agents take 19-dimensional actions, corresponding to actuator torques.\nMTRL and STRL baselines. Previous works (Mahmoudieh et al., 2022; Rocamonde et al., 2023; Fan et al., 2022; Adeniji et al., 2023) use the single-view configuration-text score to define the reward function\n$R_{\\text{past work}}(s, a) = S_{qt}(\u03c6_Q(s), x)$\nfor action $a$, in state $s$, with text $x$. We propose three improvements. First, we use three-view configuration-text scores, which improve robustness as shown in Section 4.3. Second, we define the VLM reward as the time difference of configuration-text scores:\n$R_x(s, a) = S_{qt}(\u03c6_Q(P(s, a)), x) \u2212 S_{qt}(\u03c6_Q(s), x)$.\nThis reward encourages policies to attain high final scores, rather than to maintain high average scores as discussed in Appendix G. Finally, we define the approximate VLM reward, using the distilled model (the same one that we used for finetuning):\n$\\hat{R}_x(s, a) = \\hat{S}_{qt}(\u03c6_Q(P(s, a)), x) \u2212 \\hat{S}_{qt}(\u03c6_Q(s), x)$.\nThis reward is up to 40000\u00d7 faster to evaluate and less oscillatory than $R_x$.\nUnlike previous work, which only trains STRL agents, we also train MTRL agents. Both STRL and MTRL agents use reward $\\hat{R}_x$. The MTRL agents learn a multi-task policy $\u03c0(a|s, f_{\\text{text}}(x))$, which is conditioned on the VLM text-embedding. This enables zero-shot execution of new tasks, so it is directly comparable with the proposed LCA based on text-to-goal generation. To compare the performance of MTRL on test and training tasks, we randomly partition the 256 tasks into two sets of 128, and train one MTRL agent on each set.\nGCRL agent. We train the GCRL agent to maximize reward (9) using goals sampled uniformly from the embedding-diversity dataset. At test time, we condition the agent on configurations retrieved and potentially finetuned using the methods of Section 3.2.\nArchitectures and training. The MTRL, STRL and GCRL agents have identical architectures (as far as possible given their different inputs). All are trained with proximal policy optimization (Schulman et al., 2017) with similar hyperparameters. Full details are given in Appendix F."}, {"title": "5. Limitations and Further Work", "content": "We would like to extend the range of tasks that may be performed to include compositional relationships between a diverse set of objects. As the current generation of VLMs is limited in its ability to assess such relationships, as discussed in Appendix H.2, we await a new generation of VLMs to enable such work. Also, our current approach is limited to static configurations: in future, it will be interesting to extend the approach to dynamic tasks."}, {"title": "Impact Statement", "content": "Positive impacts. The development of language-conditioned agents (LCAs) will further fluidify human-technology interaction. Given the convenience of language as a medium for humans to express desires and commands, this will likely result in an increase in such interactions. The possibility of steering technologies easily will empower every potential user, including those without technical expertise. LCAs are particularly promising as they might enhance the independence of individuals with mobility limitations, such as elderly or paraplegic persons. They may also be valuable for fine-grained control of robots in hazardous environments, such as radioactive or extraterrestrial terrains.\nNegative impacts. The proposed technology can be used to enable language-conditioned design and interaction with virtual environments, such as games, potentially making"}, {"title": "A. Environment, Cameras and Configurations", "content": "In this section, we first provide details about the Humanoid-plus-cube environment, then we describe the camera settings used to render that environment, and finally we detail each dimension of the environment's configuration space."}, {"title": "A.1. Environment Details", "content": "We use the Humanoid environment from OpenAI's Gym framework (Brockman et al., 2016), originally designed for locomotion, which we modify as follows:\nThe original Humanoid does not have feet. This is problematic, as we expect the agent to balance in diverse poses. We therefore augment the XML definition of the Humanoid model with feet, borrowed from the Humanoid model in the DeepMind Control Suite (Tassa et al., 2018). This modification adds two dimensions to the action space (for ankle control) and four dimensions to the state space (ankle-foot angles and angular velocities).\nWe add a cube to the environment, with sides of length 0.15 m. This allows agent-object interactions, other than agent-floor interactions. This adds 14 dimensions to the state space (7 dimensions for the cube position and orientation and 7 for the corresponding velocities). The cube is modelled as a homogeneous volume with mass 0.5 kg.\nEpisodes terminate after 100 steps. We therefore include the episode time-step in the state. To do otherwise would negatively impact policy performance (Pardo et al., 2018).\nWe delete some of the original 376 state dimensions, as they are not relevant. Notably, the 84-dimensional vector of external forces is always zero when using MuJoCo > 2.0.\nIn summary, this results in a Humanoid-plus-cube environment, with action space $[-1, 1]^{19}$ and an observation space of 343 dimensions.\nInitial state distribution. The original Humanoid environment specifies a default state, in which the Humanoid is standing up, facing forward, with all velocities equal to zero. We extend this default state by setting the ankle angles such that the feet are at 6.5\u00b0 to the floor (toes pointing up), setting the Cartesian coordinates of the cube to be (\u20130.15, 0.4, 0.15), and setting its orientation to be the quarternion (0, 0, 0, 1), so that its sides are parallel to the axes. All corresponding velocities are zero. As in the original Humanoid environment, we sample the initial state of each episode by adding uniform noise on [\u20130.01, 0.01] independently to all positions and velocities of this default state."}, {"title": "A.2. Camera Settings", "content": null}, {"title": "A.3. Configurations", "content": "In our modified Humanoid environment, the configuration space Q has 31 dimensions, as listed in Table 4. These encompass the torso height, the relative positions of its joints, the Cartesian coordinates of the cube, and a quaternion giving the cube's orientation."}, {"title": "B. Dataset Sampling and Retrieval", "content": "This section gives a detailed explanation of the configuration-dataset sampling methods introduced in Section 3.2.1 of the main paper. Then it discusses the effect of dataset size on the quality of the retrieved configurations, and on the time and memory required for retrieval."}, {"title": "B.1. Random Policy Dataset", "content": "This dataset is constructed by collecting all configurations encountered by a random policy. Episodes start in a state sampled from the initial state distribution detailed in Section A, with one exception: we sampled the initial height of the cube uniformly on the interval [0.15, 1], as such configurations are relevant but unlikely to be encountered without this change. The policy then performs actions that are uniformly sampled in the action space at each time step."}, {"title": "B.2. Uniform Sampling Dataset", "content": "This dataset is constructed by uniformly sampling the configuration space Q", "bounds": "nConfiguration dimension 0 represents the height of the Humanoid\u2019s torso, which is not bounded in the Humanoid XML file. Instead, we sample this component from the interval [0.1, 1.45", "1": "."}, {"1": "."}, {"1": "to avoid cube-floor interpenetration"}]}