{"title": "Hi-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting", "authors": ["Boying Li", "Zhixi Cai", "Yuan-Fang Li", "Ian Reid", "Hamid Rezatofighi"], "abstract": "We propose Hi-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a novel hierarchical categorical representation, which enables accurate global 3D semantic mapping, scaling-up capability, and explicit semantic label prediction in the 3D world. The parameter usage in semantic SLAM systems increases significantly with the growing complexity of the environment, making it particularly challenging and costly for scene understanding. To address this problem, we introduce a novel hierarchical representation that encodes semantic information in a compact form into 3D Gaussian Splatting, leveraging the capabilities of large language models (LLMs). We further introduce a novel semantic loss designed to optimize hierarchical semantic information through both inter-level and cross-level optimization. Furthermore, we enhance the whole SLAM system, resulting in improved tracking and mapping performance. Our Hi-SLAM outperforms existing dense SLAM methods in both mapping and tracking accuracy, while achieving a 2x operation speed-up. Additionally, it exhibits competitive performance in rendering semantic segmentation in small synthetic scenes, with significantly reduced storage and training time requirements. Rendering FPS impressively reaches 2,000 with semantic information and 3,000 without it. Most notably, it showcases the capability of handling the complex real-world scene with more than 500 semantic classes, highlighting its valuable scaling-up capability.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual Simultaneous Localization and Mapping (SLAM) is a critical technique for ego-motion estimation and scene perception, widely employed in multiple robotics tasks for drones [1], self-driving cars [2], as well as in applications such as Augmented Reality (AR) and Virtual Reality (VR) [3]. Semantic information, which provides high-level knowledge about the environment, is fundamental for comprehensive scene understanding and essential for intelligent robots to perform complex tasks. Recent advancements in image segmentation and map representations have significantly enhanced the performance of Semantic Visual SLAM [4], [5].\nRecently, 3D Gaussian Splatting has emerged as a popular 3D world representation [6], [7], [8] due to its rapid rendering and optimization capabilities, attributed to the highly parallelized rasterization of 3D primitives. Specifically, 3D Gaussian Splatting effectively models the continuous distributions of geometric parameters using Gaussian distribution. This capability not only enhances performance but also facilitates efficient optimization, which is especially advantageous for SLAM tasks. SLAM problem involves a complex optimization space, encompassing both camera poses and global map optimizations at the same time. The adoption of 3D Gaussian Splatting has led to the development of several SLAM systems [9], [10], [11], [12], [13], demonstrating promising performance in geometric understanding of unknown environments. However, the lack of semantic information in these approaches limits their ability to fully comprehend the global environment, restricting their potential in downstream tasks such as visual navigation, planning, and autonomous driving.\nThus, it is highly desirable to extend the original 3D Gaussian Splatting with semantic capabilities while preserving its advantageous probabilistic representation. A straightforward approach would be to augment 3D points with a discrete semantic label and parameterize its distribution with a categorical discrete distribution, i.e., a flat Softmax embedding representation. However, 3D Gaussian Splatting is already a storage-intensive representation [14], [15], requiring a large number of 3D primitives with multiple parameters to achieve realistic rendering. Adding semantic distribution parameters would result in significantly increased storage demands and processing time, growing linearly with the number of semantic classes. This makes it particularly impractical for complex scene understanding. Recent works formulate semantic classes using non-distributional approaches to handle this complexity. The work [16] directly learns a 3-channel RGB visualization for semantic maps instead of the true semantic information understanding. Another work [17] uses a flat semantic representation with supervision from pre-trained foundation models, yet it can only produces a 3D semantic feature map, rather than an explicit 3D semantic label map.\nUnlike flat representations, semantic information naturally organises into a hierarchical structure of classes, as illustrated in Fig. 1. This hierarchical relationship can be effectively represented as a tree structure, allowing for efficient encoding of extensive information with a relatively small number of nodes, i.e., a compact code. For instance, a binary tree with a depth of 10 can cover 210 classes, enabling the representation of 1,024 classes using just 20 codes (i.e., 2 \u00d7 10, through 2-dimensional Softmax coding for each level).\nBuilding on this concept, we propose Hi-SLAM, a Semantic Gaussian Splatting SLAM leveraging the hierarchical categorical representation for semantic information. Specifically, taking both semantic and geometric attributes into consideration, a well-designed tree is established with the help of Large Language Models (LLMs), which significantly reduces memory usage and training time, effectively compressing data while preserving its physical meaning. Additionally, we introduce a hierarchical loss for the proposed representation, incorporating both inter-level and cross-level optimizations. This strategy facilitates a coarse-to-fine understanding of scenes, which aligns well with real-world applications, particularly those involving observations from distant to nearby views. Furthermore, we enhance and refine the Gaussian SLAM to improve both performance and running speed. The main contributions of this paper include:\n1) We propose a novel hierarchical representation that encodes semantic information by considering both geometric and semantic aspects, with assistance from LLMs. This tree coding effectively compacts the semantic information while preserving its physical hierarchical structure.\n2) We introduce a novel optimization loss for the semantic hierarchical representation, incorporating both inter-level and cross-level optimizations, ensuring comprehensive refinement across all levels of the hierarchical coding.\n3) We update several modules to enhance the SLAM system by incorporating semantic information and fully leveraging the rendering capabilities of Gaussian Splatting. We conduct experiments on both synthetic and real-world datasets. The results demonstrate that our SLAM system outperforms existing methods in localization and mapping performance while achieving faster speeds. In small synthetic scenes, our method achieves competitive performance in rendering semantic segmentation. In complex real-world scenes, our approach, for the first time, demonstrates a valuable scaling-up capability, successfully handling more than 500 semantic classes- an important step toward the semantic understanding of complex environments."}, {"title": "II. RELATED WORK", "content": "a) 3D Gaussian Splatting SLAM: 3D Gaussian Splatting has emerged as a promising 3D representation recently. With the usage of 3D Gaussian Splatting, SplaTAM [9] leverages silhouette guidance for pose estimation and map reconstruction in RGBD SLAM systems. Gaussian Splatting SLAM [10] implements both monocular and RGBD SLAM using 3D Gaussian Splatting. 3D Gaussian Splatting has demonstrated its strong capabilities across various Gaussian Splatting SLAM tasks [11], [12], [13]. However, integrating semantic understanding into SLAM tasks makes optimization particularly challenging, as it combines three high-dimensional optimization problems with different value ranges and convergence characteristics that to be optimized jointly. In this paper, we leverage hierarchical coding for semantic information and employ a suitable optimization strategy to ensure effective optimization.\nb) Neural Implicit Semantic SLAM: Semantic SLAM has been a longstanding research topic in the field of computer vision and robotics [4], [18], [19], [20], [21]. Many works [5], [22], [23] have utilized the neural implicit representation for semantic mapping and localization tasks. DNS-SLAM [5] leverages 2D semantic priors combined with a coarse-to-fine geometry representation to integrate semantic information into the established map. SNI-SLAM [22] incorporates appearance, geometry, and semantic features into a collaborative feature space to enhance the robustness of the entire SLAM system. However, these methods are constrained by the limitations of neural implicit map representations, which is known to suffer from slow convergence, which leads to inefficiency and performance degradation when combined with semantic objectives [24], [25]. In contrast, Gaussian Splatting offers advantages with its fast rendering performance and high-density reconstruction quality.\nc) Gaussian Splatting Semantic SLAM: With the recent emergence of 3D Gaussian Splatting, the work [16] integrated additional RGB 3-channels to learn semantic visualization map, rather than true semantic understanding. SemGauss-SLAM [17] employs a flat semantic representation, supervised by a large pre-trained foundation model. However, these methods neglect the natural hierarchical characteristics of the real world. Furthermore, the reliance on large foundation models increases the complexity of the neural network and its computational demands, with performance heavily dependent on the embeddings from these pre-trained models. In this paper, we introduce a simple yet effective hierarchical representation for semantic understanding, eliminating the dependency on foundation models, enabling a coarse-to-fine semantic understanding for the unknown environments."}, {"title": "III. METHOD", "content": "A. Hierarchical representation\nTree Parametrization. We propose a hierarchical tree representation to encode semantic information, represented as G = (V,E). The node_set V = Uf=0{v} comprises all classes, where {v} represents the set of nodes at the 1-th level of the tree. The edge set E = UM=0{em} captures the subordination relationships, encompassing both semantic attribution and geometric prior knowledge. Similarly we use the subscript m to indicate the level of the tree. In this way, the i-th semantic class g\u00b9, regarded as a single leaf node in the tree view, can be expressed hierarchically as:\n gleaf = {vi, em | 1 = 0,1,...,L; m = 0,1,...,M},\nwhich corresponds to the root-to-leaf path: gleaf = v0M0v1M-2v-1MV. Take a leaf node class 'Wall' as an example, a 4-level tree coding can be as: {vwall: Background} \u2192 {vwall : Structure} \u2192 {vwall: Plane} \u2192 {vwall : Wall}. Among these nodes, the relationships such as 'include' and 'possessing' are represented by the edge information em:\u2192. In this way, any semantic concept can be coded in a progressive, hierarchical manner, incorporating both semantic and geometric perspectives. Moreover, the standard flat representation can be seen as a single-level tree coding from the hierarchical viewpoint.\nLLM-based Tree Generation. We utilize Large Language Models (LLMs), specifically GPT-40-mini [26], to generate the hierarchical tree representation due to its efficient performance. Specifically, the set of original semantic classes are input into the LLMs to cluster them into coarser-level classes. This process is repeated layer by layer from leaf to root, ultimately forming the complete hierarchical tree. However, when dealing with a large number of classes in a complex environment, the results are often unsatisfactory because the LLM tends to cluster only a subset of the input classes, leaving out many classes and incorrectly including numerous unseen classes in the hierarchy.\nTo address this issue, we employ a loop-based critic operation, including an LLM followed by a validator. Specifically, during the clustering process from the l-level to the (1-1)-level, the l-level semantic classes {v} are used as the initial prompt input to the LLM, awaiting clustering. The LLM then generates the clustering result {v} \u2192 {v{-1}. By comparing the LLM's results and the prompt input, the validator will identify three components: the successfully grouped nodes {v}, the unseen classes {v} and the omitted semantic nodes {v} The successfully grouped nodes {v} will be retained, while the unseen classes {v} will be removed. Next, the omitted nodes {v} are used as the input prompt for the LLM to do the clustering in the subsequent iteration. Additionally, the clustering nodes {v/_1} generated in the previous iteration are also provided to the LLM as a reference, suggesting that {v} can either be clustered into the previously generated clusters or form new groups. This procedure loops until {v} = 0, indicating that no classes have been omitted. In this way, we obtain all the clustering results from the l-level to the (1 \u2013 1)-level. The proposed loop-based critic operation progresses from the leaf-to-root levels, ending when fewer than @ clusters can be generated by the LLM, where we set 0 = 4. Worth mentioning is that the tree generation is performed offline before the SLAM operation.\nTree Encoding. For each 3D Gaussian primitive, its semantic embedding h is composed of the embedding h' of each level:\nh = f(h') \u2208R^, h\u2208R\", 1= 0,1,...,L \nwhere we use I to represents 1-th level of the tree and f stands for the concatenation operation. As shown in Fig. 2, the overall dimension of the hierarchical embedding is the sum of the dimensions across_all_levels_N = \u03a3\u03af=on', where the dimension ni of each embedding h\u00b9 depends on the maximum number of nodes at the 1-th level.\nB. Hierarchical loss\nTo fully optimize the hierarchical semantic coding effectively, we propose the hierarchical loss as follows:\nLSemantic = @1 LInter + W2LCross\nwhere Linter and Lcross stands for the Inter-level loss and Cross-level loss respectively. We use w\u2081 and 2 to balance the weights between each loss. The Inter-level loss Linter is employed within each level:\nL\nLInter = \u2211 Lce (softmax(h'), P\u00b9) \nwhere Lce represents the cross-entropy loss, and P\u00b9 stands for the semantic ground truth for the 1-th level. In contrast, the Cross-level loss is computed based on the entire hierarchical coding. First, a linear layer F shared between all Gaussian primitives is used to transform the hierarchical embeddings into flat coding. Following is a softmax(F(h)) operation to convert the embeddings into probabilities. The Cross-level loss Lcross is then defined as follows:\nLcross = LCE (softmax(F(h)),P)\nwhere P represents the semantic ground truth.\nC. Gaussian Splatting Semantic Mapping and Tracking\nThe pipeline of our Hi-SLAM is illustrated in Fig. 2. We will detail the submodules in this subsection.\nSemantic 3D Gaussian representation. We adopt Gaussian primitives with hierarchical semantic embedding for the scene representation. Each semantic Gaussian is represented as the combination of color c, the center position \u03bc, the radius r, the opacity o, and its semantic embedding h. And the influence of each Gaussian according to the standard Gaussian equation is G = o exp(-11-12), where X stands for the 3D point.\n2r2\nFollowing [6], each semantic 3D Gaussian primitive is projected to the 2D image space using the tile-based differentiable a-compositing rendering. The semantic map is rasterized as follows:\nn\ni-1\nH= \u03a3 ShiG(X)T; with T\u2081 = \u03a0(1 \u2013 G(X))\ni=1\nj=1\nThe rendered color image C, depth image D, and the silhouette image S are defined as follows:\nn\nn\nn\nC = CG(X), D = d;G(X)T, S = \u2211 G(X);\ni=1\ni=1\ni=1\nIn contrast to previous work [9], which employs separate forward and backward rendering modules for different parameters, we adopt unified forward and backward modules that handle all parameters, including semantic, color, depth, and silhouette images.\nTracking. The tracking step aims to estimate each frame's pose. We adopt constant velocity model to initialize the pose of every incoming frame, following a pose optimization while fixing the global map, using the rendering color and depth losses:\nLTrack = M (W1LDepth +W2LColor)\nwhere LDepth and LColor stands for the L1-loss for the rendered depth and color information. We use weights w1 and w2 to balance the two losses and the optimization is only performed on the silhouette-visible image M = (S > \u03b4).\nMapping. The global map information, including the semantic information, is optimized in the mapping procedure with fixed camera poses. The optimization losses include the depth, color, and the semantic losses:\nLMap = W3MLDepth + W4LColor+W5LSemantic\nwhere Lsemantic is the proposed semantic loss introduced in Section III-B, and LColor is the weighted sum of SSIM color loss and L1-Loss. And we use w3, W4, and w5 for balancing different terms.\""}, {"title": "IV. EXPERIMENTS", "content": "A. Experiment settings\nThe experiments are conducted on both synthetic and real-world datasets, including 6 scenes from ScanNet [28] and 8 sequences from Replica [27]. Following the evaluation metrics used in previous SLAM works [9], [29], we leverage ATE RMSE (cm) to assess SLAM tracking accuracy. For mapping performance, we use Depth L1 (cm) to evaluate accuracy. To assess image rendering quality, we adopt PSNR (dB), SSIM, and LPIPS metrics. Similar to previous methods [17], [5], [22], due to the lack of direct metrics for evaluating 3D semantic understanding in 3D Gaussian Splatting representations, we rely on 2D semantic segmentation performance, measured by mIoU (mean Intersection over Union across all classes), to reflect global semantic information. To demonstrate the improved efficiency, we also measure the running time of the proposed SLAM method. We compare our method against state-of-the-art dense visual SLAM approaches, including both NeRF-based and 3D Gaussian SLAM methods, to highlight its effectiveness. Additionally, we include state-of-the-art semantic SLAM techniques, covering both NeRF-based and Gaussian-based methods, to showcase our hierarchical semantic understanding and scaling-up capability. The experiments are conducted in the Nvidia L40S GPU. For experimental settings, the semantic embedding of each Gaussian primitive is initialized randomly. We set semantic optimization loss weights @\u2081 and 2 to1.0 and 0.0, respectively, for the first \u03b7 iterations, where \u03b7 is set to 15. Afterwards, W\u2081 and 2 are adjusted to 1.0 and 5.0, respectively. This means that we first use the Inter-level loss to initialize the hierarchical coding, followed by incorporating the Cross-level loss to refine the embedding. For tracking loss, we set 8 = 0.99, w\u2081 = 1.0, w2 = 0.5, respectively. For mapping, we set w3 = 1.0, w4 = 0.5, w5 = 0.2, respectively.\nB. SLAM Performance\nTracking Accuracy. We present the tracking performance on the Replica [27] and ScanNet [28] datasets in Tab. I and Tab. II, respectively. On the Replica dataset, our proposed method surpasses all current approaches. For the ScanNet dataset, the performance of all methods is lower than on the synthetic dataset due to the noisy, sparse depth sensor input and the limited color image quality caused by motion blur. We evaluate all six sequences, showing that our method performs comparably to state-of-the-art methods [29].\nMapping Performance. In Tab. III, we evaluate the mapping performance using the L1 depth loss in Replica [27]. The results show that our method surpasses all existing approaches, demonstrating superior mapping capabilities.\nRendering Quality. Similar to Point-SLAM [34] and NICE-SLAM [29], we evaluate rendering quality on input views from 8 sequences of the Replica dataset [27]. The evaluation uses average PSNR, SSIM, and LPIPS metrics. Our methods achieves superior performance (Hi-SLAM: PSNR\u2191:\n35.70, SSIM\u2191: 0.980, LPIPS \u2193: 0.067) compared to the state-of-the-art approaches, where the best performances being: (SpltaTAM [9]: PSNR\u2191: 34.11, SSIM\u2191: 0.968, LPIPS \u2193: 0.102), and (SemGauss-SLAM [17]: PSNR \u2191: 35.03, SSIM\u2191: 0.982, LPIPS\u2193: 0.062). Due to space limitations, detailed results for rendering performance across all methods are provided in future supplementary materials.\nRunning time. Running times for all methods are shown in Tab. IV. Compared to state-of-the-art dense visual SLAM approaches, our method (Ours*) achieves up to 2.4x faster tracking and 2.2\u00d7 faster mapping than the SOTA performance [9]. When incorporating semantic information, our method remains efficient, leveraging hierarchical semantic coding to achieve nearly 3x faster tracking and 1.2\u00d7 faster mapping compared with the semantic SLAM with flat semantic coding. Notably, our Hi-SLAM achieves a rendering speed of 2000 FPS. For Hi-SLAM without semantic information, the rendering speed increases to 3000 FPS.\nC. Hierarchical semantic understanding\nWe conduct semantic understanding experiments in synthetic dataset Replica [27] to demonstrate the comprehensive performance of our proposed method. Replica [27] is a synthetic indoor dataset comprising a total of 102 semantic classes with high-quality semantic ground truth.\nWe establish a five-level tree to encode these original classes hierarchically. The semantic rendering performance is illustrated in Fig. 3, where the first five rows show the progression from level-0 to level-4, moving from coarse to fine understanding. The coarsest semantic rendering, i.e., level-0 which shown in the first row, includes segmentation covering 4 broad classes: Background, Object, Other, and Void. In contrast, the finest level encompasses all 102 original semantic classes. For example, the hierarchical understanding of the class 'Stool' progresses from Object \u2192 Furniture \u2192"}, {"title": "V. CONCLUSIONS", "content": "We present Hi-SLAM, a novel semantic 3D Gaussian Splatting SLAM method with a hierarchical categorical representation, which can generate global 3D semantic map with scaling-up capability and explicit semantic semantic label prediction. Specifically, we propose a novel hierarchical representation to encode semantic message in a compact form, further forming it into 3D Gaussian Splatting, with the assistance of LLMs. Additionally, we present a novel semantic loss for the hierarchical semantic information optimization, including both inter-level and cross-level optimization. Furthermore, we refine the whole SLAM system. Experiments demonstrate that Hi-SLAM surpasses existing dense SLAM methods in both tracking and mapping performance with faster speed, and it can significantly reduces storage and training requirements. The method exhibits exceptional rendering performance, achieving up to 2,000/3,000 FPS with/without semantic information. Most notably, Hi-SLAM showcases its ability to manage complex real-world scenes containing over 500 semantic classes, underlining its robust scalability and effectiveness in large-scale applications."}]}