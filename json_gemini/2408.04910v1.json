{"title": "Unleashing Artificial Cognition: Integrating Multiple AI Systems", "authors": ["Muntasir Adnan", "Buddhi Gamage", "Zhiwei Xu", "Damith Herath", "Carlos Noschang Kuhn"], "abstract": "In this study, we present an innovative fusion of language models and query analysis techniques to unlock\ncognition in artificial intelligence. Our system seamlessly integrates a Chess engine with a language model,\nenabling it to predict moves and provide strategic explanations. Leveraging a vector database through retrievable\nanswer generation, our OpenSIAI system elucidates its decision-making process, bridging the gap between raw\ncomputation and human-like understanding. Our choice of Chess as the demonstration environment underscores\nthe versatility of our approach. Beyond Chess, our system holds promise for diverse applications, from medical\ndiagnostics to financial forecasting.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) systems have achieved remarkable feats in specialized areas such as image recognition\nand natural language processing [13, 19, 36]. Despite these advancements, individual AI models typically excel\nin isolated tasks and lack general cognition abilities, leading to Artificial General Intelligence (AGI) [25].\nThis fragmentation restricts their potential for broader and more generalized applications requiring seamless\ninteraction of multiple cognitive functions.\nHuman cognition is marked by adaptability, creativity, and emotional intelligence, guided by goals, norms,\nand social and ethical considerations [24]. In contrast, artificial cognition involves simulating these processes\nin machines, enabling them to perform tasks autonomously [21]. Studies have highlighted the strengths and\nlimitations of human and artificial cognition, emphasizing the need for understanding these differences for\neffective human-AI collaboration [14].\nThe Turing Test, introduced by Alan Turing [26], posits that a machine can be considered intelligent if it\ncan carry on a conversation indistinguishable from a human. Despite its historical significance, the Turing\nTest has notable limitations. It is anthropocentric, assuming human-like conversation as the definitive marker\nof intelligence, thereby excluding other forms of intelligence like complex problem-solving or creative pattern\nrecognition. Critics, including Turing, have argued that pre-programmed responses could deceive the interrogator,\nundermining the test's ability to assess cognitive abilities [9, 23]. Additionally, the Turing Test lacks granularity\nin evaluating cognition, as it does not assess various cognitive abilities such as attention, memory, learning, and\nreasoning, nor does it compare Al's cognitive stages to human levels [20].\nEvaluating cognition in AI involves assessing the system's ability to perform tasks requiring intelligence and\nadaptation to various situations. This includes simulating human-like cognitive processes to enable socially\nintelligent and adaptive interactions with humans [15, 27, 29]. By incorporating specific tasks that assess the\nmentioned cognitive qualities, we aim to create a more comprehensive assessment strategy for AI cognition,\noffering insights into the strengths and weaknesses of AI systems.\nIn the book \"Cognitive Robotics\", Cangelosi and Asada [1] discuss eight cognitive abilities, drawing on the\nwork of [15], who examine seven essential cognitive abilities: perception, attention mechanisms, action selection,\nmemory, learning, reasoning, and meta-reasoning. Vernon et al. [28] add anticipation to this list.\nFollowing these ideas, in this study centred around chess, we identified five cognitive qualities relevant to chess\nplayers for making decisions during gameplay. The cognitive qualities we focus on are:\n\u2022 Perception. The ability to interpret and understand sensory information from the environment.\n\u2022 Memory. The capability to store, retain, and retrieve information.\n\u2022 Attention. The skill of focusing on relevant stimuli while filtering out distractions.\n\u2022 Reasoning. The ability to draw logical inferences and conclusions from available information.\n\u2022 Anticipation. The capability to predict future events or outcomes based on current information and past\nexperiences.\nThis paper focuses on developing the initial requirements for an AI system to achieve higher cognition levels\nin a closed environment. We present a systematic way to evaluate the cognitive capabilities of our integrated\nsystem. We show that individual models may exhibit cognitive qualities independently, and their integration can\nlead to the emergence of cognitive behaviours comparable to humans."}, {"title": "2 Methodology", "content": "Our proposed system integrates multiple Al models and tools, each specialising in different aforementioned\ncognitive qualities. By integrating these tools, we aim to enable the system to perform complex tasks that\nrequire the interplay of multiple cognitive functions, thus exhibiting cognition.\nWhile constituting a mainstream and demonstrably effective set, the employed techniques are acknowledged to\nbe limited. More advanced fine-tuning, Retrieval-Augmented Generation (RAG) [16], and Retrieval-Augmented\nFine-Tuning (RAFT) [35], may offer [8] further performance enhancements. Nonetheless, this study combines\nthese mainstream technologies to assess the feasibility and potential for introducing human-like cognitive\ncapabilities within an AI system. The proposed system encompasses a range of services that the agents can\ndecide to employ. To evaluate its efficacy, the system is designed to experiment with several Large Language\nModels (LLMs). The system is comprised of the following components:\n\u2022 A query analyser service.\n\u2022 Base LLM or a fine-tuned LLM using Parameter-Efficient Fine-Tuning's (PEFT) [34] and Low-Rank\nAdaptation (LoRA) [11].\n\u2022 An external knowledge source facilitated by a Faiss vector database and RAG capability.\n\u2022 A chess engine service powered by Stockfish.\n\u2022 A vector database update service that allows real-time information updates.\nThe components mentioned above work together to achieve cognitive qualities within the system. Figure 1\nillustrates this collaboration in detail, depicting the system architecture.\nFine-tuning. For fine-tuning, we leverage an instruction tuning [30, 31] methodology. The base model is Mistral\n7B, chosen for its balance of performance, efficiency and size.\nTo promote slow and deliberative reasoning in a small student model, we employ a teacher-student learning\nparadigm [18]. OpenAI's GPT-40 served as the teacher model, and we interacted with it using specific system\nprompts designed to elicit a deliberate step-by-step reasoning approach while generating responses. These outputs\nform a core component of the training data. Conversely, during fine-tuning the student model, a generic system\nprompt is employed. This generic prompt, used for future inference across all models, would allow the student\nmodel to leverage its learned reasoning skills as its default behaviour. This is because the pre-trained teacher\nmodel, due to its size and capacity, may provide answers directly, while the small student model will require\na more deliberative approach to provide similar answers [18]. This distinction in prompt engineering transfers\nthe generalised reasoning abilities from the teacher model to the student model.\nAdditionally, a dataset of chess games with detailed and step-by-step reasoning annotations for each move,\nakin to a \"chain-of-thought\" approach [32], is constructed [35]. This custom chess reasoning dataset is further\nsupplemented by the publicly available Kaggle Lichess dataset to provide broader coverage of chess scenarios.\nThe finetuning training dataset included the following tasks:\n\u2022 Chess Move analysis. Explain the rationale for a chess move prediction with reference analysis from\nOpenAI.\n\u2022 Next Move Prediction. Predict the best next move played by a human opponent with samples from Kaggle\nLichess.\n\u2022 Game Winner Prediction. Predict the winner of a chess game based on the current state with samples\nfrom Kaggle Lichess and OpenAI.\n\u2022 Piece Capture Analysis. Analyse potential captures based on a list of moves in Algebraic Notation using\ncustom scripts.\n\u2022 FEN Parsing and Reasoning. Reason from a position represented in Forsyth-Edwards Notation (FEN)\nformat [7] using custom scripts.\n\u2022 Retrieval-Augmented Fine-Tuning dataset. Analyse the provided contexts and use the most relevant\ncontext to generate an answer, following work of [35]\nThe diversity of the training data aims to enhance the model's ability to \"think\" in various chess scenarios and\ndevelop transferable reasoning capabilities. Additionally, we introduce new knowledge to the model through\nfine-tuning, enabling it to understand and utilise FEN. Although the model successfully demonstrates this new\ncapability, the new knowledge increases its propensity for hallucinations when confronted with zero-shot tasks\n[8].\nTo optimize computational efficiency during fine-tuning, we employ several techniques. The base Mistral 7B\nmodel is quantized to a 4-bit representation [4], significantly reducing the computational complexity regarding\nmemory and running time. Furthermore, instead of fully fine-tuning the base model, we leverage Low-Rank\nAdaptation (LoRA) to fine-tune a smaller adapter module. This approach further reduces the computational\noverhead associated with fine-tuning. The quantization configuration is consistent across all models utilising the\nHugging Face Transformers library [33].\nFor inference, we use the finetuned PEFT model, which involves adding the fine-tuned LoRA adapters to the\nquantized base model. Interestingly, we observe behavioural discrepancies between the fine-tuned model when\nadding the LoRA adapter (as a PEFT model) and fully merging the adapter into the base Mistral 7B model.\nWhile the PEFT model performs as expected during inference, a more detailed investigation into this discrepancy\nis warranted for future studies.\nFinally, to assess the effectiveness of the proposed fine-tuning methodology for comparisons, the AI system is\nadditionally evaluated with base models from other LLM families, including GPT-40, GPT-3.5 Turbo, Gemma\n7B Instruct, and Mistral 7B Instruct. The results of this comparative analysis will be presented in the experiments.\nRetrieval-Augmented Generation (RAG). We adopt RAG [16] in our framework to leverage external knowledge\nsources stored in a Faiss vector data store [6]. An embedding model plays a crucial role in transforming both\ntextual information from the knowledge source and user queries into high-dimensional vectors. This enables\nefficient similarity search using distance measurement [5] between the query vector and vectors representing\ndata points in the Faiss database, where the smallest distance indicates the most similar context to the query\nvector. The similarity_search_with_score from LangChain [2] facilitates the retrieval of the most\nrelevant contexts based on these distances.\nA pre-defined similarity threshold filters retrieve information to extract highly relevant knowledge for reasoning.\nThis is crucial as we aimed to assess the system's ability to exhibit reasoning and attention across diverse\nscenarios, potentially extending beyond the immediate domain of chess.\nAll base LLMs used in the system leverage the same embedding model embedding_model_name retrieved\nfrom the Hugging Face library for seamless integration with the Faiss vector store, which works seamlessly for\nboth pipelines using LangChain and the Llama indexing [17] library. However, combining the fine-tuned model\nrequires slightly more advanced techniques since Llama Indexing currently lacks support for PEFT models.\nAdditionally, the fine-tuned model necessitates further training to leverage retrieved contexts effectively. To\naddress these concerns, we use RAFT and provide our strategy below.\nRetrieval-Augmented Fine-Tuning (RAFT). The base models perform optimally within the RAG framework,\neffectively utilising the provided contexts. However, the fine-tuned model exhibits confusion when presented with\nthe extracted contexts from the documents. Although the model demonstrates its capability to tackle various\nproblem-solving tasks, including novel queries for piece-capture and game-analysis problems in a zero-shot\nmanner, it struggles to deal with entirely novel contexts, leading to hallucinations in response. Consequently,\nfurther fine-tuning is necessary to instruct the model on effectively leveraging the provided contexts [35].\nWe adopt the methodology outlined in [35], where the authors train an LLM to concentrate on the most relevant\ncontext by inducing a slow reasoning analysis of all given contexts, similar to the Chain of Thought (CoT)\napproach. The resultant dataset comprises a user query, relevant data points, and a CoT-styled analysis of the\ncontexts as the answer, thus instructing the model on how to analyse and utilise the provided contexts. We also\ninclude training examples where the model is expected to disregard all provided contexts upon analysis and\ninstead rely on its pre-trained knowledge to respond to the query. An example of this dataset is provided below.\nWho are Tardo and Peo in {DISQUALIFIED} by Charles L. Fontenay?\nTo determine who Tardo and Peo are in {DISQUALIFIED} by Charles L. Fontenay, we can\nlook at the provided context.\n1. The context mentions that Tardo, the Solar Council's Planetary Aid agent, and his\ncompanion, Peo, were taken to the castle. This indicates that Tardo is the Solar\nCouncil's Planetary Aid agent, and Peo is his companion.\nTherefore, based on the context provided, Tardo is the Solar Council's Planetary Aid\nagent, and Peo is his companion.\n<ANSWER>: Tardo is the Solar Council's Planetary Aid agent, and Peo is his companion.\nWhile this approach successfully introduces the fine-tuned model to consider the retrieved contexts, we believe\nthis specific methodology warrants a dedicated study to enhance the RAG capability among various problem-\nsolving tasks for large language models (LLMs).\nChess Engine Service. The chess engine service is an integral component of our system, leveraging the"}, {"title": "2.2 Scoring Mechanism", "content": "This section explores how to assess an AI System's cognitive qualities in a closed environment; in this case,\nwe are using Chess as the closed environment.\nTo quantify the cognition capability of our AI system, we design a scoring mechanism with the aforementioned\n5 qualities from the perspectives of the agent's environment understanding, information processing, and solution\nprovision. To this end, we have developed a Question-and-Answer (Q&A) testing system, wherein the list of\nquestions was meticulously curated to consider each quality. It is important to note that the Q&A dataset used\nin this study for the particular chess domain is not exhaustive; the questions can be generalized to other task\ndomains and the AI system's level of cognition [20].\nThe evaluation of each quality is based on statistics of sufficient test samples. We provide the details and their\ncorresponding assessment criteria below. These criteria form the basis of our evaluation, ensuring a thorough\nand systematic assessment of Al's cognitive capabilities. Standard Algebraic Chess Notation [10] is used to\nrepresent the chessboard and assess cognitive aspects.\nPerception. To assess perception, we simulate a chessboard state by providing a sequence of moves into the\nsystem. We then query the system with questions that evaluate its understanding of the current board state,\nincluding:\n\u2022 Understand chess piece position on a given FEN.\n\u2022 Compute the number of captured pieces in Algebraic Chess Notation.\n\u2022 Provide step-by-step analysis of pieces captured, the number of pieces left in total, and the number of\npieces left for each player in Algebraic Chess Notation.\nFor the capture analysis questions, we reward partial understanding of the board. If a model manages to predict\nthe number of captures partially, we will penalize only for the incorrect predictions which include skipping\npiece capture or overestimating the number of captures. If the number of captures in the FEN is $N_c$ and the\nprediction from the model is $N_m$, the formula for scoring on this query is\nCapture Analysis: $S_{capture} = 1 - \\frac{|N_c - N_m|}{N_c}$ (1)\nThen, we get the overall perception score by using the following formula where the FEN perception score is\n$S_{FEN}$, capture analysis score is $S_{capture}$, and piece analysis score is $S_{piece}$,\nPerception Score: $S_{perception} = \\frac{S_{FEN} + S_{capture} + S_{piece}}{\\text{number of questions}}$ (2)\nMemory. The system's memory is evaluated using questions that assess its general chess knowledge. Fur-\nthermore, the Al system is augmented with RAG [16] and has access to two chess books, simulating external"}, {"title": "3 Experiments", "content": "In this section, we present a full evaluation and analysis of the cognition performance of the proposed OpenSIAI\nSystem, which is featured by 5 cognition qualities: perception, memory, attention, reasoning, and anticipation.\nWe provide the quality scores on 5 LLMs in Fig. 2. Our proposed AI system integrates LLMs with 3 services: a\nchess engine for best move prediction using Stockfish, a vector database for dynamic information retrieval, and\nretrieval-augmented generation on documents. The main LLMs for evaluation are GPT-40, GPT-3.5 Turbo (for\nanticipation), Gemma 7B Instruct, Mistral 7B Instruct, and fine-tuned Mistral 7B. All GPU-related experiments\nare conducted on NVIDIA GeForce RTX 3090, and our OpenSIAI system will be available upon publication.\nBest-move Prediction for Chess Game. The best-move prediction aims to predict the best next move for\na given chess FEN or a sequence of moves. Our system incorporates the strong open-source chess engine,\nStockfish, with interaction with LLMs to analyse the move decision, yielding nearly perfect predictions to the\nground truth labels obtained from [3]. In Fig. 2, Gemma 7B Instruct, Mistral 7B Instruct, and fine-tuned Mistral\nare unable to predict any correct best moves, indicating their deficiency in the game reasoning.\nWhile GPT-40 alone exhibits an unexpectedly high prediction accuracy with a 32.5% success rate, Fig. 2, it falls\nshort in strategic reasoning, particularly in determining check and checkmate situations. In contrast, our system\ndemonstrates superior performance by accurately predicting the optimal moves in 40 chess games, underscoring\nthe significant benefit of integrating our chess engine service.\nVector Database for Dynamic Information Retrieval. Similar to one-shot learning, our system can retrieve\nup-to-date context by adding certain information with an updated timestamp to the inbuilt vector database, which\nis managed by using the Facebook AI similarity search tool. Information to be added can be sourced from a\ndocument or a sentence prefix with _update_store_.\nRAG on Documents. In our system, the query into LLMs will first be used to retrieve relevant context\nfrom the vector database, followed by prompt generation under a tuned prompt template to trigger the LLM\nengine for text generation. This retrieved context will be filtered out if the score of its cosine similarity to the\nquery is under a given threshold, 0.7 in our case, to avoid misleading information in the query to LLMs. For\ninformation retrieval from an external document, our system uses LangChain to split the document into chunks\nwith a chunk size 1,000 and overlap size 100, and then encodes them with an embedding model to update the\nvector database. The success rates achieved by GPT-40, Gemma 7B Instruct, and Mistral 7B Instruct are 80%,\n70%, and 77.5% respectively."}, {"title": "3.2 Evaluation on System Cognition Capability", "content": "We evaluate the cognition ability of our OpenSIAI system with 5 qualities in Sec. 1. The\nevaluation scale of each quality is provided below.\nWhile the perception can be represented as the system's understanding of a scenario, we\nprovide 3 types of questions: 40 questions for parsing chess piece position, 20 questions for parsing chess\npiece status, and 8 questions for identifying chess piece captures.\nThe system's memory is evaluated on 3 types of questions: 40 questions for retrieving LLM's\nbase knowledge, 8 questions for the memory of previous scenarios in a chess game, and 6 questions for\nretrieving context from the updated system vector database.\nWe evaluate 40 RAG questions that are extracted or raised from 3 books. The correct attention\nshould be localized on the page providing the correct answer to the question.\nThe reasoning capability is evaluated through human annotation, assessing LLM's analysis\nof the suggested move by the chess engine service, for a given FEN. A dataset of 40 chess puzzles is"}, {"title": "4 Future Work", "content": "In our current system, the query analyser utilises keyword detection to route user queries to appropriate services\nand employs a Chain of Thought (CoT) query service to convert user queries into CoT queries through keyword\nmatching. Despite its effectiveness in improving the system's cognition ability, this method has limitations\nregarding flexibility and scalability. To overcome these challenges, we propose enhancing the query analyser by\nreplacing the keyword detection mechanism with a fine-tuned LLM or a classification model. This will facilitate\nmore accurate and context-aware routing of user queries to relevant services. Furthermore, we intend to automate\nand refine the CoT query generation process by fine-tuning an LLM to produce CoT queries, thereby enhancing\nthe system's reasoning capabilities [12]. To further increase system reliability and scalability, we will integrate\nmultiple fine-tuned LLMs replacing the single LLM at the core, each specialized for different tasks relevant\nto the domain, and deploy them in a distributed system architecture. This will enable the AI system to handle\nmore queries without sacrificing its cognition performance, scale more efficiently by reducing the computational\ncomplexity, and facilitate easier maintenance, updates, and addition of external services. Moreover, with the"}, {"title": "5 Conclusion", "content": "In this study, we have showcased the efficacy of integrating multiple Al systems with LLMs to augment the\ncognition abilities of digital assistants. Our proposed architecture is resilient and intuitive, allowing for seamless\nincorporation into broader systems. One of the core innovations lies in the query analyser for specific services,\nthereby enhancing the LLM's role as an interactive intermediary between the user and the system. Furthermore,\nwe have illustrated that while LLMs may exhibit limited predictive capabilities, they are competent at interpreting\nthe responses from predictive tools. This enhanced system holds the potential to assist a wide spectrum of pro-\nfessionals, including financial advisors, lawyers, programmers, etc. In summary, the proposed system integrates\nmultiple AI models and services to create a cohesive framework to demonstrate comprehensive cognitive abilities.\nBy integrating these components, our system aims to bridge the gap between raw computational abilities and\nhuman-like cognitive processes, setting the stage for future advancements in AGI."}, {"title": "Author Contributions", "content": "Muntasir Adnan and Zhiwei Xu collaborated on implementing the code base, designing experiments, evalu-\nating results, and writing the manuscript. Buddhi Gamage focused on experiment design, result evaluation,\nand manuscript writing. Damith Herath conducted a thorough review of the manuscript. Carols Noschang\nKuhn developed the initial concept for this project and contributed to experiment design, research question\ndevelopment, data evaluation, and manuscript writing."}]}