{"title": "Unleashing Artificial Cognition: Integrating Multiple Al Systems", "authors": ["Muntasir Adnan", "Buddhi Gamage", "Zhiwei Xu", "Damith Herath", "Carlos Noschang Kuhn"], "abstract": "In this study, we present an innovative fusion of language models and query analysis techniques to unlock cognition in artificial intelligence. Our system seamlessly integrates a Chess engine with a language model, enabling it to predict moves and provide strategic explanations. Leveraging a vector database through retrievable answer generation, our OpenSIAI system elucidates its decision-making process, bridging the gap between raw computation and human-like understanding. Our choice of Chess as the demonstration environment underscores the versatility of our approach. Beyond Chess, our system holds promise for diverse applications, from medical diagnostics to financial forecasting.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) systems have achieved remarkable feats in specialized areas such as image recognition and natural language processing [13, 19, 36]. Despite these advancements, individual AI models typically excel in isolated tasks and lack general cognition abilities, leading to Artificial General Intelligence (AGI) [25]. This fragmentation restricts their potential for broader and more generalized applications requiring seamless interaction of multiple cognitive functions.\nHuman cognition is marked by adaptability, creativity, and emotional intelligence, guided by goals, norms, and social and ethical considerations [24]. In contrast, artificial cognition involves simulating these processes in machines, enabling them to perform tasks autonomously [21]. Studies have highlighted the strengths and limitations of human and artificial cognition, emphasizing the need for understanding these differences for effective human-AI collaboration [14].\nThe Turing Test, introduced by Alan Turing [26], posits that a machine can be considered intelligent if it can carry on a conversation indistinguishable from a human. Despite its historical significance, the Turing Test has notable limitations. It is anthropocentric, assuming human-like conversation as the definitive marker of intelligence, thereby excluding other forms of intelligence like complex problem-solving or creative pattern recognition. Critics, including Turing, have argued that pre-programmed responses could deceive the interrogator, undermining the test's ability to assess cognitive abilities [9, 23]. Additionally, the Turing Test lacks granularity"}, {"title": "2 Methodology", "content": "in evaluating cognition, as it does not assess various cognitive abilities such as attention, memory, learning, and reasoning, nor does it compare Al's cognitive stages to human levels [20].\nEvaluating cognition in AI involves assessing the system's ability to perform tasks requiring intelligence and adaptation to various situations. This includes simulating human-like cognitive processes to enable socially intelligent and adaptive interactions with humans [15, 27, 29]. By incorporating specific tasks that assess the mentioned cognitive qualities, we aim to create a more comprehensive assessment strategy for AI cognition, offering insights into the strengths and weaknesses of AI systems.\nIn the book \"Cognitive Robotics\", Cangelosi and Asada [1] discuss eight cognitive abilities, drawing on the work of [15], who examine seven essential cognitive abilities: perception, attention mechanisms, action selection, memory, learning, reasoning, and meta-reasoning. Vernon et al. [28] add anticipation to this list.\nFollowing these ideas, in this study centred around chess, we identified five cognitive qualities relevant to chess players for making decisions during gameplay. The cognitive qualities we focus on are:\n\u2022 Perception. The ability to interpret and understand sensory information from the environment.\n\u2022 Memory. The capability to store, retain, and retrieve information.\n\u2022 Attention. The skill of focusing on relevant stimuli while filtering out distractions.\n\u2022 Reasoning. The ability to draw logical inferences and conclusions from available information.\n\u2022 Anticipation. The capability to predict future events or outcomes based on current information and past experiences.\nThis paper focuses on developing the initial requirements for an AI system to achieve higher cognition levels in a closed environment. We present a systematic way to evaluate the cognitive capabilities of our integrated system. We show that individual models may exhibit cognitive qualities independently, and their integration can lead to the emergence of cognitive behaviours comparable to humans."}, {"title": "2.1 Proposed System for Demonstrating Cognitive Abilities", "content": "Our proposed system integrates multiple Al models and tools, each specialising in different aforementioned cognitive qualities. By integrating these tools, we aim to enable the system to perform complex tasks that require the interplay of multiple cognitive functions, thus exhibiting cognition.\nWhile constituting a mainstream and demonstrably effective set, the employed techniques are acknowledged to be limited. More advanced fine-tuning, Retrieval-Augmented Generation (RAG) [16], and Retrieval-Augmented Fine-Tuning (RAFT) [35], may offer [8] further performance enhancements. Nonetheless, this study combines these mainstream technologies to assess the feasibility and potential for introducing human-like cognitive capabilities within an AI system. The proposed system encompasses a range of services that the agents can decide to employ. To evaluate its efficacy, the system is designed to experiment with several Large Language Models (LLMs). The system is comprised of the following components:\n\u2022 A query analyser service.\n\u2022 Base LLM or a fine-tuned LLM using Parameter-Efficient Fine-Tuning's (PEFT) [34] and Low-Rank Adaptation (LoRA) [11].\n\u2022 An external knowledge source facilitated by a Faiss vector database and RAG capability.\n\u2022 A chess engine service powered by Stockfish.\n\u2022 A vector database update service that allows real-time information updates.\nThe components mentioned above work together to achieve cognitive qualities within the system. Figure 1 illustrates this collaboration in detail, depicting the system architecture.\nFine-tuning. For fine-tuning, we leverage an instruction tuning [30, 31] methodology. The base model is Mistral 7B, chosen for its balance of performance, efficiency and size.\nTo promote slow and deliberative reasoning in a small student model, we employ a teacher-student learning paradigm [18]. OpenAI's GPT-40 served as the teacher model, and we interacted with it using specific system"}, {"title": "2.2 Scoring Mechanism", "content": "This section explores how to assess an AI System's cognitive qualities in a closed environment; in this case, we are using Chess as the closed environment.\nTo quantify the cognition capability of our AI system, we design a scoring mechanism with the aforementioned 5 qualities from the perspectives of the agent's environment understanding, information processing, and solution provision. To this end, we have developed a Question-and-Answer (Q&A) testing system, wherein the list of questions was meticulously curated to consider each quality. It is important to note that the Q&A dataset used in this study for the particular chess domain is not exhaustive; the questions can be generalized to other task domains and the AI system's level of cognition [20].\nThe evaluation of each quality is based on statistics of sufficient test samples. We provide the details and their corresponding assessment criteria below. These criteria form the basis of our evaluation, ensuring a thorough and systematic assessment of Al's cognitive capabilities. Standard Algebraic Chess Notation [10] is used to represent the chessboard and assess cognitive aspects.\nPerception. To assess perception, we simulate a chessboard state by providing a sequence of moves into the system. We then query the system with questions that evaluate its understanding of the current board state, including:\n\u2022 Understand chess piece position on a given FEN.\n\u2022 Compute the number of captured pieces in Algebraic Chess Notation.\n\u2022 Provide step-by-step analysis of pieces captured, the number of pieces left in total, and the number of pieces left for each player in Algebraic Chess Notation.\nFor the capture analysis questions, we reward partial understanding of the board. If a model manages to predict the number of captures partially, we will penalize only for the incorrect predictions which include skipping piece capture or overestimating the number of captures. If the number of captures in the FEN is $N_c$ and the prediction from the model is $N_m$, the formula for scoring on this query is\nCapture Analysis: $S_{capture} = 1 - \\frac{|N_c - N_m|}{N_c}$ (1)\nThen, we get the overall perception score by using the following formula where the FEN perception score is $S_{FEN}$, capture analysis score is $S_{capture}$, and piece analysis score is $S_{piece}$,\nPerception Score: $S_{perception} = \\frac{S_{FEN} + S_{capture} + S_{piece}}{\\text{number of questions}}$ (2)\nMemory. The system's memory is evaluated using questions that assess its general chess knowledge. Fur-thermore, the Al system is augmented with RAG [16] and has access to two chess books, simulating external"}, {"title": "3 Experiments", "content": "In this section, we present a full evaluation and analysis of the cognition performance of the proposed OpenSIAI System, which is featured by 5 cognition qualities: perception, memory, attention, reasoning, and anticipation. We provide the quality scores on 5 LLMs in Fig. 2. Our proposed AI system integrates LLMs with 3 services: a chess engine for best move prediction using Stockfish, a vector database for dynamic information retrieval, and retrieval-augmented generation on documents. The main LLMs for evaluation are GPT-40, GPT-3.5 Turbo (for anticipation), Gemma 7B Instruct, Mistral 7B Instruct, and fine-tuned Mistral 7B. All GPU-related experiments are conducted on NVIDIA GeForce RTX 3090, and our OpenSIAI system will be available upon publication."}, {"title": "3.1 Evaluation on System Services", "content": "a) Best-move Prediction for Chess Game. The best-move prediction aims to predict the best next move for a given chess FEN or a sequence of moves. Our system incorporates the strong open-source chess engine, Stockfish, with interaction with LLMs to analyse the move decision, yielding nearly perfect predictions to the ground truth labels obtained from [3]. In Fig. 2, Gemma 7B Instruct, Mistral 7B Instruct, and fine-tuned Mistral are unable to predict any correct best moves, indicating their deficiency in the game reasoning.\nWhile GPT-40 alone exhibits an unexpectedly high prediction accuracy with a 32.5% success rate, Fig. 2, it falls short in strategic reasoning, particularly in determining check and checkmate situations. In contrast, our system demonstrates superior performance by accurately predicting the optimal moves in 40 chess games, underscoring the significant benefit of integrating our chess engine service.\nb) Vector Database for Dynamic Information Retrieval. Similar to one-shot learning, our system can retrieve up-to-date context by adding certain information with an updated timestamp to the inbuilt vector database, which is managed by using the Facebook AI similarity search tool. Information to be added can be sourced from a document or a sentence prefix with _update_store_.\nc) RAG on Documents. In our system, the query into LLMs will first be used to retrieve relevant context from the vector database, followed by prompt generation under a tuned prompt template to trigger the LLM engine for text generation. This retrieved context will be filtered out if the score of its cosine similarity to the query is under a given threshold, 0.7 in our case, to avoid misleading information in the query to LLMs. For information retrieval from an external document, our system uses LangChain to split the document into chunks with a chunk size 1,000 and overlap size 100, and then encodes them with an embedding model to update the vector database. The success rates achieved by GPT-40, Gemma 7B Instruct, and Mistral 7B Instruct are 80%, 70%, and 77.5% respectively."}, {"title": "3.2 Evaluation on System Cognition Capability", "content": "a) Evaluation Scale. We evaluate the cognition ability of our OpenSIAI system with 5 qualities in Sec. 1. The evaluation scale of each quality is provided below.\n\u2022 Perception. While the perception can be represented as the system's understanding of a scenario, we provide 3 types of questions: 40 questions for parsing chess piece position, 20 questions for parsing chess piece status, and 8 questions for identifying chess piece captures.\n\u2022 Memory. The system's memory is evaluated on 3 types of questions: 40 questions for retrieving LLM's base knowledge, 8 questions for the memory of previous scenarios in a chess game, and 6 questions for retrieving context from the updated system vector database.\n\u2022 Attention. We evaluate 40 RAG questions that are extracted or raised from 3 books. The correct attention should be localized on the page providing the correct answer to the question.\n\u2022 Reasoning. The reasoning capability is evaluated through human annotation, assessing LLM's analysis of the suggested move by the chess engine service, for a given FEN. A dataset of 40 chess puzzles is"}, {"title": "4 Future Work", "content": "In our current system, the query analyser utilises keyword detection to route user queries to appropriate services and employs a Chain of Thought (CoT) query service to convert user queries into CoT queries through keyword matching. Despite its effectiveness in improving the system's cognition ability, this method has limitations regarding flexibility and scalability. To overcome these challenges, we propose enhancing the query analyser by replacing the keyword detection mechanism with a fine-tuned LLM or a classification model. This will facilitate more accurate and context-aware routing of user queries to relevant services. Furthermore, we intend to automate and refine the CoT query generation process by fine-tuning an LLM to produce CoT queries, thereby enhancing the system's reasoning capabilities [12]. To further increase system reliability and scalability, we will integrate multiple fine-tuned LLMs replacing the single LLM at the core, each specialized for different tasks relevant to the domain, and deploy them in a distributed system architecture. This will enable the AI system to handle more queries without sacrificing its cognition performance, scale more efficiently by reducing the computational complexity, and facilitate easier maintenance, updates, and addition of external services. Moreover, with the"}, {"title": "5 Conclusion", "content": "In this study, we have showcased the efficacy of integrating multiple Al systems with LLMs to augment the cognition abilities of digital assistants. Our proposed architecture is resilient and intuitive, allowing for seamless incorporation into broader systems. One of the core innovations lies in the query analyser for specific services, thereby enhancing the LLM's role as an interactive intermediary between the user and the system. Furthermore, we have illustrated that while LLMs may exhibit limited predictive capabilities, they are competent at interpreting the responses from predictive tools. This enhanced system holds the potential to assist a wide spectrum of pro-fessionals, including financial advisors, lawyers, programmers, etc. In summary, the proposed system integrates multiple AI models and services to create a cohesive framework to demonstrate comprehensive cognitive abilities. By integrating these components, our system aims to bridge the gap between raw computational abilities and human-like cognitive processes, setting the stage for future advancements in AGI."}]}