{"title": "RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing", "authors": ["Bo Ai", "Stephen Tian", "Haochen Shi", "Yixuan Wang", "Cheston Tan", "Yunzhu Li", "Jiajun Wu"], "abstract": "Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Imagine packing an item into a nearly full suitcase. As humans, we typically first form a visual representation of the scene and then make attempts to insert the object, feeling the compliance of the objects already inside to decide where and how to insert the new object. If a particular region feels soft, we can then apply additional force to make space and squeeze the new object in. This process is natural for us humans but very challenging for current robotic systems.\nWhat would it take to produce adept packing capabilities in robots? Firstly, a robot needs to understand how its actions will affect the objects in the scene and how those objects will interact with each other. Dynamics models of the world predict exactly this: how the state of the world will change based on a robot's action. However, most physics-based dynamics models (e.g., physical simulators), assume full-state information and typically exhibit significant sim-to-real gaps, especially in unstructured scenes involving deformable objects.\nAt the same time, tasks such as dense packing present significant challenges due to severe occlusions among objects, creating partially observable scenarios where vision alone is insufficient to determine the properties of an object, such as its softness, or assess whether there is space for additional objects. For effective operation, the robot must integrate information from its actions and the corresponding tactile sensing into its planning procedure. However, the optimal method for incorporating tactile sensing information into dynamic models is unclear. Na\u00efvely integrating tactile sensing into a model's state space can perform poorly because the intricate contacts make tactile modeling a challenging problem, as we will also show empirically later on.\nTo tackle these challenges, in this work, we propose to 1) learn dynamics directly from real physical interaction data using powerful deep function approximators, 2) equip our robotic system with a compliant vision-based Soft-Bubble tactile sensor [22], and 3) develop a learning-based method for effective estimation of latent physics information from tactile feedback in interaction histories.\nBecause learning dynamics in raw pixel observation space can be challenging due to the problem's high dimensionality, we instead model scenes using keypoint particles [37, 29, 49, 48, 50]. Finding and tracking meaningful keypoint representa- tions of densely packed scenes over time is itself challenging due to the proximity of objects and inter-occlusions. In this work, we extend an optimization-based point tracking system to preprocess raw observation data into keypoints.\nWe use the Soft-Bubble tactile sensor [22], which is ideal for tasks like dense packing, as it can safely sustain stress from the handheld object in all directions and provides high-resolution percepts of the contact force via an embedded RGB-D camera.\nFinally, we propose an effective way to incorporate tactile information into our system by learning a separate state estimation module that incorporates tactile information from prior interactions and infers latent physics vectors that contain information that may be helpful for future prediction. This allows us to learn tactile-informed dynamics.\nWe call this system comprising keypoint-based perception, latent physics vector and state estimation from tactile in- formation, dynamics prediction, and model-based planning RoboPack. We deploy RoboPack on two real-world settings_ a tool-use manipulation and a dense packing task. These tasks involve multi-object interactions with complex dynamics that cannot be determined from vision alone. Furthermore, these settings are exceptionally challenging because, unlike prior work that only estimates the physical properties of the object held in hand, our tasks also require estimating the physical properties of objects with which the robot interacts indirectly through the handheld object.\nWe find that our method can successfully leverage histo- ries of visuo-tactile information to improve prediction, with models trained on just 30 minutes of real-world interaction data per task on average. Through empirical evaluation, we demonstrate that RoboPack outperforms previous works on dynamics learning, an ablation without tactile information, and physics simulator-based methods in dynamics prediction and downstream robotic tasks. We further analyze the properties of the learned latent physics vectors and their relationship with interaction history length."}, {"title": "II. RELATED WORK", "content": "Simulators developed to model rigid and non-rigid bodies approximate real-world physics, often creating a significant sim-to-real gap [57, 17, 41]. To address this, we use a graph neural network (GNN)-based dynamics model trained directly on real-world robot interaction data, aligning with data-driven approaches for learning physical dynamics [42, 36]. Recent works have demonstrated inspiring results in learning the complex dynamics of objects such as clothes [34], ropes [5], and fluid [26], with various representations including low- dimensional parameterized shapes [38], keypoints [30], latent vectors [24], and neural radiance fields [31]. RoboPack, in- spired by previous works [29, 47, 2], focuses on the struc- tural modeling of objects with minimal assumptions about underlying physics. This approach overcomes the limitations of physics simulators by directly learning from real-world dy- namics. Prior work on GNN-based dynamics learning [48, 49, 50, 55, 6] heavily relies on visual observations for predicting object dynamics, failing to capture unobserved latent vari- ables that affect real-world dynamics, such as object physical properties. To address this challenge, our method incorporates tactile sensing into dynamics learning and leverages history information for state estimation, offering a robust solution to overcome the constraints of vision-only models.\nReinforcement learning (RL) aims to derive policies di- rectly from interactions. Our method contrasts with model- free RL approaches [40, 32, 12, 19, 27], by incorporating an explicit dynamics model, enhancing interpretability and including structured priors for improved generalization. Our work is closer to model-based RL [16, 13, 42, 46, 39, 62] in that we combine learned world models with planning via trajectory optimization. In particular, we learn world models in an offline manner from pre-collected interaction data, avoiding risky trial-and-error interactions in the real world. However, our approach is different from existing offline model-based RL [45, 59, 9, 54, 15] as it leverages multiple sensing modalities, i.e., tactile and visual perception. This multi-modal approach provides a more comprehensive understanding of both global geometry and the intricate local physical interactions between the robot gripper and objects. Moreover, our method addresses challenges in scenarios where visual observations are not always available. It uses tactile observation histories to esti- mate partially observable states, enabling online adaptation to different dynamics. This integration of offline model learning, multi-modal perception, and online adaptation equips our system with adaptive control behaviors for complex tasks.\nTactile sensing plays an important role in both human and robot perception [7]. Among all categories of tactile sensors, vision-based sensors such as [60, 8, 25, 33] can achieve accurate 3D shape perception of their sensing surfaces. In our work, we use the Soft-Bubble tactile sensor [22] which"}, {"title": "III. METHOD", "content": "The objective of RoboPack is to manipulate objects with unknown physical properties in environments with heavy occlusions like dense packing. To formulate this problem, we define the observation space as O, the state space as S, and the action space as A. Our goal is to learn a state estimator g that maps O to S and a transition function \\mathcal{T}: S \\times A \\rightarrow S.\nTo efficiently learn dynamics from real-world multi-object interaction data, we would like to extract lower-dimensional representations of observations like keypoints. Furthermore, we require a mechanism to fuse tactile interaction histories into these representations without full tactile future prediction. Finally, to solve real robotic tasks, we need to leverage our learned model to plan robot actions.\nThus, our system has four main components: perception, state estimation, dynamics prediction, and model-predictive control, discussed in Section III-B, III-C, III-D, and III-E respectively. They are used together in the following way:\nFirst, the perception system extracts particles from the scene as a visual representation $o^{vis}$ and encodes tactile readings into latent embeddings $o^{tact}$ attached to those particles.\nSecondly, the state estimator g infers object states s from any prior interactions, which includes a single visual frame $o^{vis}_{t}$, the subsequent tactile observations $o^{obat}$ and the corresponding robot actions $a_{1:t-1}$:\n$$s_t = g(o^{vis}_{t}, \\{o^{vis}_{t},\\ o^{obat}, o^{tact}, a_{1:t-1}\\}) .$$\nWe define our state as a tuple of object particles and an object- level latent physics vector, which capture the geometry and physics properties of objects respectively. In the following paragraphs, we describe how our method performs state es- timation using history information.\nAt time 0 < t < T, our state estimator g infers all states for t = 1, ..., T autoregressively. Given the estimated previous state $S_{t-1}$ and the tactile feedback at the previous and the cur- rent state $o^{tact}_{t}$, we construct a graph $G_{t-1} = (V_{t-1}, E_{t-1})$ with $V_{t-1}$ as vertices and $E_{t-1}$ as edges. For each node, $V_{i,t-1} = (X_{i,t-1}, C_{it-1})$, where $X_{i,t-1}$ is the particle position i at time t - 1, and $c_{t-1}$ are particle attributes. The particle attributes contain (1) the previous and current tactile readings, $o^{tact}_{t-1}$, and (2) the latent physics vector of the object that the particle belongs to, $M_{i,t-1}$, where $M_i$ is the object index corresponds to the i-th particle, $1 \\lt M \\lt Z$ and Z is the maximum number of objects in the scene. Formally, $c_{i,t-1} = (\\{S_{t-1}, o^{tact:t}\\})$ Note that here we implicitly assume that M is constant (i.e., objects only exhibit elastic and plastic deformations but not break apart), which generally holds for a large number of common manipulation tasks. Moreover, edges between pairs of particles are denoted $e_k = (u_k, v_k)$, where $u_k$ and $v_k$ are the receiver and sender particle indices respectively, and $1 \\lt u_k,v_k \\leq |V_{t-1}|$ where k is the edge index. We construct graphs by connecting any nodes within a certain radius of each other.\nGiven the graph, we first use a node encoder $f^{enc}_{v}$ and an edge encoder $f^{enc}_{e}$ to obtain node and edge features, respectively:\n$$h_{i,t-1} = f^{enc}_{v}(v_{i,t-1}), h_{k,t-1} = f^{enc}_{e}(e_{k,t-1}).$$\nThen, the features are propagated through the edges in multiple steps, during which node effects are processed by neighboring nodes through learned MLPs. We summarize this procedure as $f^{dec}_{e \\rightarrow v}$, which outputs an aggregated effect feature for each node called i:\n$$\\phi_{i,t-1} = f^{dec}_{e \\rightarrow v}(h_{i,t-1}, \\sum_{k=1,...,|E_{t-1}|} h_{k,t-1}) .$$\nwhere $N_i$ is a set of relations with particle i as the receiver. Next, the model predicts node (particle) positions and updates the latent physics vector:\n$$o^{vis}_{i,t} = f^{dec}_{v}(h_{i,t-1}, \\phi_{i,t-1}) i=1,...,|V_{t-1}|,$$\n$$\\xi_{\\eta,t}, m_t = f^{dec}_{l} (h_{i,t-1}, \\sum_{i:M_i=\\eta} \\phi_{i,t-1}, m_{t-1}) \\eta=1,...,Z .$$"}, {"title": "D. Dynamics Prediction", "content": "After the state estimator produces an estimated state $\\hat{S}_T = (o^{vis}, \\xi_T)$ from the T-step history, our dynamics model pre- dicts into the future to evaluate potential action plans. The dynamics predictor f is constructed similarly to the state estimator g, with two key differences: (i) it does not use tactile observations as input, and (ii) it is conditioned on frozen physics parameters estimated by g. Figure 3 illustrates this process. The forward prediction happens recursively: For a step t > T, we construct a graph in the same way as in Section III-C, but excluding tactile observations from the particle attributes, i.e., $c_t = \\{f_t\\}$. Then, the dynamics predictor infers the particle positions at the next step $\\hat{o}_{t}$ as formulated in Equations 5-7. The final state prediction is then $\\hat{S}_{t+1} = (o^{vis}_{t}).$ Note that the estimated physics parameters are not modified by the dynamics predictor.\nTraining procedure and objective. We train the state estimator and dynamics predictor jointly end-to-end on tra- jectories of sequential interaction data containing observations and robot actions. For a training trajectory of length H, the state estimator estimates the first T states, and the dynamics predictor predicts all remaining states. The estimation and pre- diction are all computed autoregressively. The loss is computed only on visual observations:\n$$L = \\frac{1}{H-1} \\sum_{t=0}^{H-1} || o^{vis}_t - o^{vis}_{t} ||_2^2 .$$\nPrevious works [48, 50, 49] use the earth mover's distance (EMD) or chamfer distance (CD) as the training loss, but these provide noisier gradients because EMD requires estimating point-to-point correspondence and CD is prone to outliers. Instead, we use mean squared error (MSE) as the objective, enabled by the point-to-point correspondences from our 3D point tracking (Section III-B). The details of the architecture and training procedure of the state estimator and dynamics predictor are in Appendix A-B.\nNote that the learning of the latent physics information is not explicitly supervised. The model is allowed to identify any latent parameters that enhance its ability to accurately estimate the current state and predict future outcomes. We provide an analysis on the learned physics parameters in Section V."}, {"title": "E. Model-Predictive Control", "content": "With the learned state estimator and dynamics predictor, we perform planning toward a particular goal by optimizing a cost function on predicted states over potential future actions. Concretely, we use Model Predictive Path Integral (MPPI) to perform this optimization [58].\nPlanning begins with sampling actions from an initial dis- tribution performing forward prediction with the dynamics models. The cost is then computed on predicted states. Based on the estimated costs, we re-weight the action samples by importance sampling and update the distribution parameters. The process repeats for multiple interactions and we select the optimal execution plan."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We set up our system on a Franka Emika Panda 7-DoF robotic arm. We use four Intel RealSense D415 cameras surrounding the robot and a pair of Soft-Bubble sensors for tactile feedback. We use 3D-printed connectors to attach the Soft-Bubble sensors to the robot. Each Soft-Bubble has a built- in RealSense D405 RGB-D camera. The RGB data are post- processed with an optical flow computation to approximate the force distribution over the bubble surface [22]. Our hardware setup is depicted in Figure 4.\nWe demonstrate our method on two tasks where the robot needs to handle objects with unknown physical properties and significant visual occlusion: manipulating a box with an in-hand tool and dense packing.\nThis task focuses on ma- nipulating rigid objects with varying mass distributions using an in-hand rod. The objective is to push a box to a goal pose with the minimum number of pushes. The robot has access to tactile feedback at all steps but only visual observations in between pushes, which corresponds to the real-world feedback loop frequency. The task is much more challenging than usual pushing tasks because (i) the boxes have different dynamics yet the same visual appearance; (ii) the robot has little visual feedback to identify box configurations; and (iii) the in-hand object can rotate and slip due to the highly compliant Soft-Bubble grippers. This is why we emphasize that our task is non-prehensile. This leads to rather complex physics interactions. To achieve effective planning, the robot needs to identify the box's properties from the tactile interaction history and adjust its predictions of the rod and box poses.\nWe experiment with four boxes, each equipped with varying calibration weights attached to their inner layers to control their dynamics. We train our model on three of these boxes with identical visual appearances. During evaluation, we test our method on all four boxes including an additional one with a distinct visual appearance and mass distribution.\nThis goal of this task is to place an additional object in an already densely packed box. Due to heavy occlusions during task execution, the robot does not have access to meaningful visual feedback during robot execution other than the initial frame, but again tactile signals are always observed. To place the object into the occupied box, the robot needs to identify potentially deformable regions with tactile information and make space for the object via pushing actions. The robot needs to avoid inserting into infeasible regions to prevent hardware and object damage. We specify the box that contains the object as the goal and the robot can insert the object at any position as long as it fits inside.\nTo test the generalizability of learned models, we create train and test object sets (Figure 5). The test objects differ from the training objects in of visual appearance, surface geometry, and physical properties. During evaluation, we consider sce- narios with only training objects and those with half or more of objects from the test set.\nTo generate diverse and safe interaction behaviors, we use human teleoperation for data collection. In the Non- Prehensile Box Pushing task, for each weight configuration, we gather random interaction data for around 15 minutes. By \"random\", we refer to the absence of labels typically present in demonstration data. During these interactions, the end-effector approaches the box from various angles and contact locations, yielding diverse outcomes including translation and rotation, as well as relative movements between the in-hand object and the bubble gripper. The dataset contains approximately 12000 total frames of interaction.\nFor dense packing, we collect approximately 20 minutes of teleoperated random interaction data with five unique objects, randomizing the initial configurations of the objects at the beginning of each interaction episode. Each episode includes various attempts at packing an object into the box and includes pushing and deforming objects, as well as in-hand slipping of the in-hand object in some trials. The dataset contains approximately 6000 total frames of interaction."}, {"title": "D. Action Space", "content": "Though our dynamics model is orthogonal to the action space, suitable action abstractions are important for efficient planning and execution.\nTo reduce the planning horizon and number of optimized parameters, we sample macro-actions during planning, which are defined as a linear push and represented by $i, \\theta, \\alpha$, where i refers to the box particle index for end effector contact, $\\theta$ denotes the angle of the push trajectory relative to the x-axis, and $\\alpha$ represents the fraction of the distance covered before end effector-box contact along the entire push length. For dynamics prediction, the macro action is decomposed into smaller motions.\nAs this task involves a large state space, we constrain the action space for planning efficiency. We first identify the outer objects in the box and compute feasible starting positions of actions nudging each object, determined by the geometric center of the object and its approximate radius. Then we sample straight-line push actions of varying lengths from each contact point towards the respective object centers. Similarly, the long push action is divided into small movements for dynamics prediction."}, {"title": "E. Planning Cost Functions", "content": "We specify the goal state as a point cloud and use MSE as the cost function.\nWe specify a 2D goal region by uni- formly sampling points in the area underneath the tray. We use a cost function that (i) penalizes the objects in the box from being pushed out of the boundary, (ii) encourages the robot to make space for placing the in-hand object by maximizing the distance from target to object points, and (iii) rewards exploring different starting action positions. Mathematically, the loss function is\n$$J(\\hat{o}_{t}, o_{g}, a_{t}) = \\sum_{x \\in \\hat{o}_{t}} min_{y \\in o_{g}} ||x - y||^2 - \\sum_{y \\in o_{g}} min_{x \\in \\hat{o}_{t}} ||x - y||^2 + r*1[||a_{0,t}||^2=0],$$\nwhere $\\hat{o}_{t}$ is the predicted object particles in the box, $o_{g}$ is the target point cloud, $||a_{0,t}||^2$ is the size of the first action, which is zero if it does not plan to switch to a different contact row, r is a negative constant, and 1 is an indicator function."}, {"title": "V. EXPERIMENTS", "content": "In this section, we investigate the following questions.\ni. Does integrating tactile sensing information from prior interactions improve future prediction accuracy?\nii. Do the latent representations learned by tactile dynamics models discover meaningful properties such as the phys- ical properties of objects?\niii. Does our tactile-informed model-predictive control framework enable robots to solve tasks involving objects of unknown physical properties?\nWe first introduce our baselines and then present empirical results in the subsequent subsections.\nWe compare our approach against three prior methods and baselines, including ablated versions of our model, previous work on dynamics learning, and a physics-based simulator:\ni. To study the effects of using tactile sensing in state estimation and dynamics prediction, we evaluate this ablation of our method, which zeroes out tactile input to the model.\nii. This approach differs from ours in that it treats the observations, i.e., visual and tactile observations ($o^{vis}$, $o^{tact}$), directly as the state representa- tion, whereas RoboPack assumes partial observability of the underlying state and performs explicit state estima- tion. This can be viewed as an adaptation of previous work [29, 48, 50, 49] to include an additional tactile observation component. With this baseline, we seek to study different state representations and our strategy of separating state estimation from dynamics prediction.\niii. We also compare our method to using a physics-based simulator for dynamics pre- diction after performing system identification of explicit physical parameters. We use heuristics to convert ob- served point clouds into body positions and orientations in the 2D physics simulator Pymunk [3]. For system identification, we estimate the mass, center of gravity, and friction parameters from the initial and current visual observations with covariance matrix adaptation [14].\nThe considered methods, including our approach, share some conceptual components with prior offline model-based reinforcement learning (RL) methods (Section II-B), although with very different concrete instantiations. Each method either learns the full environment dynamics, or in the case of Physics- based simulator, performs system identification from a static dataset. All compared methods use the dynamics models to perform model-predictive control via sampling-based plan- ning. Specifically, RoboPack (no tactile) can be framed as a model-based RL method (e.g., [59, 11, 9]) that uses only sparse visual observations for model learning. On the other hand, RoboCook + tactile treats visual and tactile observations as the state, overlooking the partially observable nature of the task. Our upcoming results demonstrate that our integration of multi-modal perception and physical parameter estimation leads to superior performance in challenging task domains."}, {"title": "B. Evaluating Dynamics Prediction", "content": "Results are summarized in Table I. On the Non-Prehensile Box Pushing task, RoboPack is significantly better than al- ternative methods in all metrics. Compared to RoboPack (no tactile), RoboPack can better estimate the mass distribution of the boxes, which is crucial in predicting the translation and rotation accurately. In contrast, when using tactile and visual observations directly as the state representation (RoboCook + tactile), the performance is even worse than RoboPack without tactile information. We hypothesize that this is because the model has very high errors in learning to predict future tactile readings because of the intricate local interactions between the Soft-Bubble grippers and the object. The difficulty in learning to predict tactile reading may distract the model from learning to predict visual observations accurately.\nComparing RoboPack to a physics-based simulator baseline, we find that the simulator performs poorly on dynamics prediction for a few potential reasons, including (i) limited visual feedback for performing system ID, and (ii) the sim- ulator's parameter space may not capture the full range of real-world dynamics given the complex interactions between the compliant bubble and in-hand tool and rotating tool and the box. To illustrate the difference in model predictions, qualitative results are presented in Figure 6.\nFor the Dense Packing task, our model outperforms the best baseline on the pushing task, RoboPack (no tactile). We note that in this task, object movements are minimal and object deformation is the major source of particle motions. Metrics such as EMD and CD that emphasize global shape and distri- bution but are insensitive to subtle positional changes cannot differentiate the two methods in a statistically significant way. However, for the MSE loss, which measures prediction error for every point, RoboPack is significantly better than the baseline, indicating its ability to capture fine details of object deformation. This subtle performance difference between the two methods in dynamics prediction turns out to have a significant effect on real-world planning (Section V-D)."}, {"title": "C. Analysis of Learned Physics Parameters", "content": "In this subsection, we seek to provide some quantitative and qualitative analyses of the latent representation learned by the state estimator. As it gives more direct control of object properties, we use our dataset collected for the Non-Prehensile Box Pushing task for the analysis.\nTo understand if the representation contains information about box types, we first attempt to train a linear classifier to"}, {"title": "D. Benchmarking Real-World Planning Performance", "content": "Next, we evaluate the performance of our approach in solving real-world robotic planning tasks.\nFor Non-Prehensile Box Pushing, we present quantitative results in Figure 9 and Table II. We can see that our method both achieves lower final error as measured by point-wise MSE (Table II) and makes progress toward goals more quickly (Fig- ure 9) than other methods. The gap in performance between our model and RoboPack (no tactile) demonstrates the benefits of using tactile sensing in this task. While the physics-based simulator achieves the strongest performance of the baselines, it is not able to achieve as precise control as our method, taking more pushes to finish the task yet ending with higher MSE loss. We hypothesize this is because it can only infer dynamics of limited complexity via properties such as friction or mass center/moment. It also requires significant manual designs to construct the simulation for each task. Finally, RoboCook + tactile has the poorest control performance, consistent with its high dynamics prediction error on the test set. We hypothesize that the poor performance of this method is due to the difficulty of learning to predict future tactile observations, which are high-dimensional and sensitive to precise contact details.\nFor the Dense Packing task, we would ideally compare our method against the baseline with the best results on non-prehensile box pushing: the physics-based simulator. However, this is impractical for this task, because it is infeasible to obtain corresponding object models for the diverse and complex objects in this task or to estimate objects' explicit physics parameters without visual feedback. Thus, we compare against the best among the remaining baselines instead, i.e., RoboPack (no tactile). We test on scenarios containing only training objects (Seen Objects) as well as scenarios where half or more of the objects are from the test set (Unseen Objects). Results on both settings, shown in Table III, indicate that our method is more effective in identifying objects that are deformable or pushable, which consequently enables the robot to insert the object at feasible locations. Examples of our experiments are illustrated in Figure 8. Despite our method having only seen rectangular boxes and plastic bags in the training set,"}, {"title": "VI. DISCUSSION", "content": "We presented RoboPack, a framework for learning tactile- informed dynamics models for manipulating objects in multi-object scenes with varied physical properties. By integrating information from prior interactions from a compliant visual tactile sensor, our method adaptively updates estimated latent physics parameters, resulting in improved physical prediction and downstream planning performance on two challenging manipulation tasks, Non-Prehensile Box Pushing and Dense Packing. We hope that this is a step towards robots that can seamlessly integrate information with multiple modalities from their environments to guide their decision-making.\nIn this paper we demonstrated our approach on two specific tasks, but our framework is generally applicable to robotic manipulation tasks using visual and tactile perception. To extend it to other tasks, one needs to adapt the cost function and planning module to the task setup, but the perception, state estimation, and dynamics prediction components are general and task-agnostic. For future work, we seek to develop dy- namics models that can efficiently process higher-fidelity par- ticles to model fine-grained object deformations. Integrating alternative trajectory optimization methods with our learned differentiable neural dynamics models is another promising direction. Finally, incorporating additional physics priors into the dynamics model could further improve generalization."}, {"title": "APPENDIX A MODEL ARCHITECTURE AND TRAINING", "content": "Both the encoder and decoder are two-layer MLPs with hidden dimension 32 and ReLU activations. The encoder maps the raw point-wise tactile signal to latent space, then the decoder maps it back to the original dimension. The autoencoder is trained with MSE loss using the following hyper-parameters:\nWe use the same hyperparameters to train dynamics models for the nonprehensile box pushing and dense packing tasks, which are shown in Table V For graph construction, we connect any points within a radius of 0.15. We train the state estimator and dynamics model jointly, using sequences of length 25. To prevent the model from overfitting to a specific history length, which could vary at deployment time, we use the first k steps in a sequence as the history, k~ Uniform(0, 24). To stabilize training, we restrict the magnitude of the rotation component of predicted rigid transformations for a single step to be at most 30 degrees, which is much larger than any rotation that occurs in our datasets. Model training converges within 25 and 8 hours on the two tasks respectively with one NVIDIA RTX A5000 GPU.\nFor baselines RoboPack (no tactile) and RoboCook + tactile, we performed a hyper-parameter sweep and the optimal train- ing parameters are the same as RoboPack described above.\nWe use the same hyperparameters for the nonprehensile box pushing and dense packing tasks."}, {"title": "APPENDIX B HARDWARE SETUP", "content": "The hardware setup is depicted in Figure 4 in the main text.\nRobot. We use a Franka Emika Panda robot arm, controlled using the Deoxys open-source controller library [63]. In our experiments, we use the OSC_POSITION and OSC_YAW controllers provided by the Deoxys library.\nSensors. We attach the Soft-Bubble sensors to the Franka Panda gripper using custom-designed 3D-printed adapters. We inflate both Soft-Bubble sensors to a width of 45mm measured from the largest distance sensor frame to the rubber sensor surface. While there can be slight variations in the exact amount of air in the sensor due to measurement error, we do not find this to be a significant cause of domain shift for learned models, likely because the signals that are used as input to our model are largely calculated using differences between the current reading and a reference frame captured when the gripper does not make contact with any object that we reset upon each inflation. While we contribute a novel method for integrating tactile information into the particle- based scene representation, the computation of raw tactile features described in Section III-B2 are computed by the Soft- Bubble sensor API [22] and is not part of our contribution."}, {"title": "APPENDIX C PLANNING IMPLEMENTATION DETAILS", "content": "We provide hyperparameters for the MPPI optimizer that is used for planning with learned dynamics models in Table VI. We use the same planning hyperparameters for baselines as we do our method.\nFor the parameters denoted by *, we use the nota- tion from Nagabandi et al. [42]. As introduced in Section III-E, K refers to the number of steps in the best plan found that is actually executed on the real robot before replanning. For box pushing it is the entire plan, while for dense packing it is 45 out of 80 steps."}, {"title": "APPENDIX D SYSTEM WORKFLOW", "content": "To present the offline training and online planning processes more clearly, a system diagram is provided in Figure 10."}, {"title": "APPENDIX E EXPERIMENTS", "content": "For the non-prehensile box pushing task, we use boxes that have the same geometry different weight configurations to test the ability of each model to adapt its prediction based on the physical characteristics of each scenario. Specifically, we empty cardboard boxes of dimensions 18 \u00d7 9.5 \u00d7 3.8 cm, and then add metal weights in the following configurations:\n*   Box 1: Two 100g weights placed at opposing corners of the box.\n*   Box 2: One 200g weight placed at the geometric center of the box.\n*   Box 3: No additional weight added.\n*   Box 4 (unseen during training): This is the original unmodified box, which contains roughly uniformly dis- tributed food items. The items are not affixed to the inner sides of the box, and there could be relative movement between the box and its contents if force is applied."}, {"title": "APPENDIX F TRACKING MODULE DETAILS", "content": "As described in Section III-B", "terms": "n$$L_{depth}(P) = \\frac{1}{|P|} \\sum_{p\\in P} max(0, depth_{interp}(p)-depth_{proj}(p)$$\nwhere $depth_{interp}(p)$ is the depth estimation from inter- polating information from multi-view depth observations, and $depth_{proj}(p"}]}