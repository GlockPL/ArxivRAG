{"title": "RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing", "authors": ["Bo Ai", "Stephen Tian", "Haochen Shi", "Yixuan Wang", "Cheston Tan", "Yunzhu Li", "Jiajun Wu"], "abstract": "Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Imagine packing an item into a nearly full suitcase. As humans, we typically first form a visual representation of the scene and then make attempts to insert the object, feeling the compliance of the objects already inside to decide where and how to insert the new object. If a particular region feels soft, we can then apply additional force to make space and squeeze the new object in. This process is natural for us humans but very challenging for current robotic systems.\nWhat would it take to produce adept packing capabilities in robots? Firstly, a robot needs to understand how its actions will affect the objects in the scene and how those objects will interact with each other. Dynamics models of the world predict exactly this: how the state of the world will change based on a robot's action. However, most physics-based dynamics models (e.g., physical simulators), assume full-state information and typically exhibit significant sim-to-real gaps, especially in unstructured scenes involving deformable objects.\nAt the same time, tasks such as dense packing present significant challenges due to severe occlusions among objects, creating partially observable scenarios where vision alone is insufficient to determine the properties of an object, such as its softness, or assess whether there is space for additional objects. For effective operation, the robot must integrate information from its actions and the corresponding tactile sensing into"}, {"title": "II. RELATED WORK", "content": "its planning procedure. However, the optimal method for incorporating tactile sensing information into dynamic models is unclear. Na\u00efvely integrating tactile sensing into a model's state space can perform poorly because the intricate contacts make tactile modeling a challenging problem, as we will also show empirically later on.\nTo tackle these challenges, in this work, we propose to 1) learn dynamics directly from real physical interaction data using powerful deep function approximators, 2) equip our robotic system with a compliant vision-based Soft-Bubble tactile sensor [22], and 3) develop a learning-based method for effective estimation of latent physics information from tactile feedback in interaction histories.\nBecause learning dynamics in raw pixel observation space can be challenging due to the problem's high dimensionality, we instead model scenes using keypoint particles [37, 29, 49, 48, 50]. Finding and tracking meaningful keypoint representa-tions of densely packed scenes over time is itself challenging due to the proximity of objects and inter-occlusions. In this work, we extend an optimization-based point tracking system to preprocess raw observation data into keypoints.\nWe use the Soft-Bubble tactile sensor [22], which is ideal for tasks like dense packing, as it can safely sustain stress from the handheld object in all directions and provides high-resolution percepts of the contact force via an embedded RGB-D camera.\nFinally, we propose an effective way to incorporate tactile information into our system by learning a separate state estimation module that incorporates tactile information from prior interactions and infers latent physics vectors that contain information that may be helpful for future prediction. This allows us to learn tactile-informed dynamics.\nWe call this system comprising keypoint-based perception, latent physics vector and state estimation from tactile in-formation, dynamics prediction, and model-based planning RoboPack. We deploy RoboPack on two real-world settings_ a tool-use manipulation and a dense packing task. These tasks involve multi-object interactions with complex dynamics that cannot be determined from vision alone. Furthermore, these settings are exceptionally challenging because, unlike prior work that only estimates the physical properties of the object held in hand, our tasks also require estimating the physical properties of objects with which the robot interacts indirectly through the handheld object.\nWe find that our method can successfully leverage histo-ries of visuo-tactile information to improve prediction, with models trained on just 30 minutes of real-world interaction data per task on average. Through empirical evaluation, we demonstrate that RoboPack outperforms previous works on dynamics learning, an ablation without tactile information, and physics simulator-based methods in dynamics prediction and downstream robotic tasks. We further analyze the properties of the learned latent physics vectors and their relationship with interaction history length.", "subsections": [{"title": "A. Learning Dynamics Models", "content": "Simulators developed to model rigid and non-rigid bodies approximate real-world physics, often creating a significant sim-to-real gap [57, 17, 41]. To address this, we use a graph neural network (GNN)-based dynamics model trained directly on real-world robot interaction data, aligning with data-driven approaches for learning physical dynamics [42, 36]. Recent works have demonstrated inspiring results in learning the complex dynamics of objects such as clothes [34], ropes [5], and fluid [26], with various representations including low-dimensional parameterized shapes [38], keypoints [30], latent vectors [24], and neural radiance fields [31]. RoboPack, in-spired by previous works [29, 47, 2], focuses on the struc-tural modeling of objects with minimal assumptions about underlying physics. This approach overcomes the limitations of physics simulators by directly learning from real-world dy-namics. Prior work on GNN-based dynamics learning [48, 49, 50, 55, 6] heavily relies on visual observations for predicting object dynamics, failing to capture unobserved latent vari-ables that affect real-world dynamics, such as object physical properties. To address this challenge, our method incorporates tactile sensing into dynamics learning and leverages history information for state estimation, offering a robust solution to overcome the constraints of vision-only models."}, {"title": "B. Model-Free and Model-Based Reinforcement Learning", "content": "Reinforcement learning (RL) aims to derive policies di-rectly from interactions. Our method contrasts with model-free RL approaches [40, 32, 12, 19, 27], by incorporating an explicit dynamics model, enhancing interpretability and including structured priors for improved generalization. Our work is closer to model-based RL [16, 13, 42, 46, 39, 62] in that we combine learned world models with planning via trajectory optimization. In particular, we learn world models in an offline manner from pre-collected interaction data, avoiding risky trial-and-error interactions in the real world. However, our approach is different from existing offline model-based RL [45, 59, 9, 54, 15] as it leverages multiple sensing modalities, i.e., tactile and visual perception. This multi-modal approach provides a more comprehensive understanding of both global geometry and the intricate local physical interactions between the robot gripper and objects. Moreover, our method addresses challenges in scenarios where visual observations are not always available. It uses tactile observation histories to esti-mate partially observable states, enabling online adaptation to different dynamics. This integration of offline model learning, multi-modal perception, and online adaptation equips our system with adaptive control behaviors for complex tasks."}, {"title": "C. Tactile Sensing for Robotic Manipulation", "content": "Tactile sensing plays an important role in both human and robot perception [7]. Among all categories of tactile sensors, vision-based sensors such as [60, 8, 25, 33] can achieve accurate 3D shape perception of their sensing surfaces. In our work, we use the Soft-Bubble tactile sensor [22] which"}]}, {"title": "III. METHOD", "content": "The objective of RoboPack is to manipulate objects with unknown physical properties in environments with heavy occlusions like dense packing. To formulate this problem, we define the observation space as O, the state space as S, and the action space as A. Our goal is to learn a state estimator g that maps O to S and a transition function \\(T: S \\times A \\rightarrow S\\).\nTo efficiently learn dynamics from real-world multi-object interaction data, we would like to extract lower-dimensional representations of observations like keypoints. Furthermore, we require a mechanism to fuse tactile interaction histories into these representations without full tactile future prediction. Finally, to solve real robotic tasks, we need to leverage our learned model to plan robot actions.\nThus, our system has four main components: perception, state estimation, dynamics prediction, and model-predictive control, discussed in Section III-B, III-C, III-D, and III-E respectively. They are used together in the following way:\nFirst, the perception system extracts particles from the scene as a visual representation \\(o^{vis}\\) and encodes tactile readings into latent embeddings \\(o^{tact}\\) attached to those particles.\nSecondly, the state estimator g infers object states s from any prior interactions, which includes a single visual frame \\(o_t^{vis}\\), the subsequent tactile observations \\(o^{tact}, o^{obat}\\) and the corre-sponding robot actions \\(a_{1:t-1}\\):\n\\[s_t = g(o^{vis}, o^{vis}, o^{obat} o^{tact}, a_{1:t-1}).  \\tag{1}\\]"}, {"title": "D. Dynamics Prediction", "content": "After the state estimator produces an estimated state \\(\\hat{s}_T = (o^{vis}, \\xi_T)\\) from the T-step history, our dynamics model pre-dicts into the future to evaluate potential action plans. The dynamics predictor f is constructed similarly to the state estimator g, with two key differences: (i) it does not use tactile observations as input, and (ii) it is conditioned on frozen physics parameters estimated by g. Figure 3 illustrates this process. The forward prediction happens recursively: For a step t > T, we construct a graph in the same way as in Section III-C, but excluding tactile observations from the particle attributes, i.e., \\(c_t = f_t\\). Then, the dynamics predictor infers the particle positions at the next step \\(\\hat{o}_1\\) as formulated in Equations 5-7. The final state prediction is then \\(\\hat{s}_{t+1} =(o^{vis}, \\xi_{t})\\). Note that the estimated physics parameters are not modified by the dynamics predictor.\nTraining procedure and objective. We train the state estimator and dynamics predictor jointly end-to-end on tra-jectories of sequential interaction data containing observations and robot actions. For a training trajectory of length H, the state estimator estimates the first T states, and the dynamics predictor predicts all remaining states. The estimation and pre-diction are all computed autoregressively. The loss is computed only on visual observations:\n\\[\\mathcal{L} = \\frac{1}{H-1}\\sum_{t=0}^{H-1} ||o_t^{vis} - \\hat{o}_t^{vis}||^2. \\tag{9}\\]\nPrevious works [48, 50, 49] use the earth mover's distance (EMD) or chamfer distance (CD) as the training loss, but these provide noisier gradients because EMD requires estimating point-to-point correspondence and CD is prone to outliers. Instead, we use mean squared error (MSE) as the objective, enabled by the point-to-point correspondences from our 3D point tracking (Section III-B). The details of the architecture and training procedure of the state estimator and dynamics predictor are in Appendix A-B.\nNote that the learning of the latent physics information is not explicitly supervised. The model is allowed to identify any latent parameters that enhance its ability to accurately estimate the current state and predict future outcomes. We provide an analysis on the learned physics parameters in Section V."}, {"title": "E. Model-Predictive Control", "content": "With the learned state estimator and dynamics predictor, we perform planning toward a particular goal by optimizing a cost function on predicted states over potential future actions. Concretely, we use Model Predictive Path Integral (MPPI) to perform this optimization [58].\nPlanning begins with sampling actions from an initial dis-tribution performing forward prediction with the dynamics models. The cost is then computed on predicted states. Based on the estimated costs, we re-weight the action samples by importance sampling and update the distribution parameters. The process repeats for multiple interactions and we select the optimal execution plan."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "A. Physical Setup\nWe set up our system on a Franka Emika Panda 7-DoF robotic arm. We use four Intel RealSense D415 cameras surrounding the robot and a pair of Soft-Bubble sensors for tactile feedback. We use 3D-printed connectors to attach the Soft-Bubble sensors to the robot. Each Soft-Bubble has a built-in RealSense D405 RGB-D camera. The RGB data are post-processed with an optical flow computation to approximate the force distribution over the bubble surface [22]. Our hardware setup is depicted in Figure 4.\nB. Task Description\nWe demonstrate our method on two tasks where the robot needs to handle objects with unknown physical properties and significant visual occlusion: manipulating a box with an in-hand tool and dense packing.\n1) Non-Prehensile Box Pushing: This task focuses on ma-nipulating rigid objects with varying mass distributions using an in-hand rod. The objective is to push a box to a goal pose with the minimum number of pushes. The robot has access to tactile feedback at all steps but only visual observations in between pushes, which corresponds to the real-world feedback loop frequency. The task is much more challenging than usual pushing tasks because (i) the boxes have different dynamics"}, {"title": "V. EXPERIMENTS", "content": "In this section, we investigate the following questions.\ni. Does integrating tactile sensing information from prior interactions improve future prediction accuracy?\nii. Do the latent representations learned by tactile dynamics models discover meaningful properties such as the phys-ical properties of objects?\niii. Does our tactile-informed model-predictive control framework enable robots to solve tasks involving objects of unknown physical properties?\nWe first introduce our baselines and then present empirical results in the subsequent subsections.\nA. Baselines and Prior Methods\nWe compare our approach against three prior methods and baselines, including ablated versions of our model, previous work on dynamics learning, and a physics-based simulator:\ni. RoboPack (no tactile): To study the effects of using tac-tile sensing in state estimation and dynamics prediction, we evaluate this ablation of our method, which zeroes out tactile input to the model.\nii. RoboCook + tactile: This approach differs from ours in that it treats the observations, i.e., visual and tactile observations (\\(o^{vis}, o^{tact}\\)), directly as the state representa-tion, whereas RoboPack assumes partial observability of the underlying state and performs explicit state estima-tion. This can be viewed as an adaptation of previous work [29, 48, 50, 49] to include an additional tactile observation component. With this baseline, we seek to study different state representations and our strategy of separating state estimation from dynamics prediction.\niii. Physics-based simulator: We also compare our method to using a physics-based simulator for dynamics pre-diction after performing system identification of explicit physical parameters. We use heuristics to convert ob-served point clouds into body positions and orientations in the 2D physics simulator Pymunk [3]. For system identification, we estimate the mass, center of gravity, and friction parameters from the initial and current visual observations with covariance matrix adaptation [14].\nThe considered methods, including our approach, share some conceptual components with prior offline model-based reinforcement learning (RL) methods (Section II-B), although with very different concrete instantiations. Each method either learns the full environment dynamics, or in the case of Physics-based simulator, performs system identification from a static dataset. All compared methods use the dynamics models to perform model-predictive control via sampling-based plan-ning. Specifically, RoboPack (no tactile) can be framed as a model-based RL method (e.g., [59, 11, 9]) that uses only sparse visual observations for model learning. On the other hand, RoboCook + tactile treats visual and tactile observations as the state, overlooking the partially observable nature of the task. Our upcoming results demonstrate that our integration of multi-modal perception and physical parameter estimation leads to superior performance in challenging task domains."}, {"title": "VI. DISCUSSION", "content": "We presented RoboPack, a framework for learning tactile-informed dynamics models for manipulating objects in multi-object scenes with varied physical properties. By integrating information from prior interactions from a compliant visual tactile sensor, our method adaptively updates estimated latent physics parameters, resulting in improved physical prediction and downstream planning performance on two challenging manipulation tasks, Non-Prehensile Box Pushing and Dense Packing. We hope that this is a step towards robots that can seamlessly integrate information with multiple modalities from their environments to guide their decision-making.\nIn this paper we demonstrated our approach on two specific tasks, but our framework is generally applicable to robotic manipulation tasks using visual and tactile perception. To extend it to other tasks, one needs to adapt the cost function and planning module to the task setup, but the perception, state estimation, and dynamics prediction components are general and task-agnostic. For future work, we seek to develop dy-namics models that can efficiently process higher-fidelity par-ticles to model fine-grained object deformations. Integrating alternative trajectory optimization methods with our learned differentiable neural dynamics models is another promising direction. Finally, incorporating additional physics priors into the dynamics model could further improve generalization."}, {"title": "APPENDIX A MODEL ARCHITECTURE AND TRAINING", "content": "A. Tactile Autoencoder\nBoth the encoder and decoder are two-layer MLPs with hidden dimension 32 and ReLU activations. The encoder maps the raw point-wise tactile signal to latent space, then the decoder maps it back to the original dimension. The autoencoder is trained with MSE loss using the following hyper-parameters:\nB. State Estimator and Dynamics Predictor\nWe use the same hyperparameters to train dynamics models for the nonprehensile box pushing and dense packing tasks, which are shown in Table V For graph construction, we connect any points within a radius of 0.15. We train the state estimator and dynamics model jointly, using sequences of length 25. To prevent the model from overfitting to a specific history length, which could vary at deployment time, we use the first k steps in a sequence as the history, k~ Uniform(0, 24). To stabilize training, we restrict the magnitude of the rotation component of predicted rigid transformations for a single step to be at most 30 degrees, which is much larger than any rotation that occurs in our datasets. Model training converges within 25 and 8 hours on the two tasks respectively with one NVIDIA RTX A5000 GPU.\nFor baselines RoboPack (no tactile) and RoboCook + tactile, we performed a hyper-parameter sweep and the optimal train-ing parameters are the same as RoboPack described above."}, {"title": "APPENDIX B HARDWARE SETUP", "content": "The hardware setup is depicted in Figure 4 in the main text.\nRobot. We use a Franka Emika Panda robot arm, controlled using the Deoxys open-source controller library [63]. In our experiments, we use the OSC_POSITION and OSC_YAW controllers provided by the Deoxys library.\nSensors. We attach the Soft-Bubble sensors to the Franka Panda gripper using custom-designed 3D-printed adapters. We inflate both Soft-Bubble sensors to a width of 45mm measured from the largest distance sensor frame to the rubber sensor surface. While there can be slight variations in the exact amount of air in the sensor due to measurement error, we do not find this to be a significant cause of domain shift for learned models, likely because the signals that are used as input to our model are largely calculated using differences between the current reading and a reference frame captured when the gripper does not make contact with any object that we reset upon each inflation. While we contribute a novel method for integrating tactile information into the particle-based scene representation, the computation of raw tactile features described in Section III-B2 are computed by the Soft-Bubble sensor API [22] and is not part of our contribution."}, {"title": "APPENDIX C PLANNING IMPLEMENTATION DETAILS", "content": "We provide hyperparameters for the MPPI optimizer that is used for planning with learned dynamics models in Table VI. We use the same planning hyperparameters for baselines as we do our method.\nFor the parameters denoted by *, we use the nota-tion from Nagabandi et al. [42]. As introduced in Section III-E, K refers to the number of steps in the best plan found that is actually executed on the real robot before replanning. For box pushing it is the entire plan, while for dense packing it is 45 out of 80 steps."}, {"title": "APPENDIX D SYSTEM WORKFLOW", "content": "To present the offline training and online planning processes more clearly, a system diagram is provided in Figure 10."}, {"title": "APPENDIX E EXPERIMENTS", "content": "A. Box Configurations\nFor the non-prehensile box pushing task, we use boxes that have the same geometry different weight configurations to test"}, {"title": "APPENDIX F TRACKING MODULE DETAILS", "content": "As described in Section III-B, after sampling initial sets of points for each object \\(P_{init}\\), we formulate point tracking as optimization for the points at each step p. Specifically, the new points are computed as a 3D transformation of the points output at the previous step, represented by a rigid rotation \\(R \\in \\mathbb{R}^3\\), translation \\(T \\in \\mathbb{R}^3\\) and optional per-axis shearing \\(S \\in \\mathbb{R}^3\\). The transform is a composition of rotation by R, scaling by S, and translation by T in that order. We abuse notation to sometimes use p for ease of reading, but pis a function of the actual optimized parameters R, S, T. Thus the optimization objective has the following loss terms:\n1) Distance to surface.\n\\[L_{depth}(P) = \\frac{1}{P \\epsilon p} max(0, depth_{interp}(p)-depth_{proj}(p))\\]\nwhere \\(depth_{interp}(p)\\) is the depth estimation from inter-polating information from multi-view depth observations, and \\(depth_{proj}(p)\\) is the expected depth at each point when projected into each camera frame.\n2) Semantic alignment.\n\\[L_{align} (P) \\frac{1}{P} \\sum_{p \\epsilon p} min(||dinov2(P_{init})-dinov2(p)||^2,30);\\]\nwhere \\(dinov2(p)\\) represents the multi-view interpolated DinoV2 feature at the 3D point represented by p, and again \\(P_{init}\\) is the position of the point in the first frame (not necessarily immediately prior frame) of tracking.\n3) Motion regularization.\n\\[L_{reg}(R,T, S) = w_{reg}^T ||R||^2 + w_{reg}^R ||T||^2 + w_{reg}^S ||S||^2.\\]\nMotion regularization prevents tracked points from ex-hibiting high frequency jitter when the objects they are tracking do not move.\n4) Mask consistency. We introduce a mask consistency loss.\nIntuitively, this loss tries to ensure that each pixel within a 2D mask for an object from a particular camera view should have a tracked point for that object that is close to that pixel when projected into that view.\nLet the set of all views be V and the set of object masks in a particular view v be M(v). Then the total number of masks points N is \\(N = \\sum_{v \\epsilon V} \\sum_{obj \\epsilon M(v)} |obj|\\).\nConcretely, this can be written as:\n\\[L_{mask}(p) = \\frac{1}{N} \\sum_{v \\epsilon V} \\sum_{obj \\epsilon M(v)} \\frac{1}{P \\epsilon p_{obj}} min_{p \\epsilon p} ||pix-proj (p, v) ||\\]\nwhere proj(p, v) is the 2D projection of 3D point p into the image space of viewpoint v.\nThe overall objective is computed by weighting and combining these terms:\n\\[L_{tracking} = W_{depth}L_{depth} + W_{align}L_{align} + W_{reg}L_{reg} + W_{mask}L_{mask}.\\]\nThe weights for each term as well as optimizer parameters are enumerated in Table VIII. The transformed points with the best loss after the total number of gradient steps is complete is output as the result."}]}