{"title": "ND-SDF: LEARNING NORMAL DEFLECTION FIELDS\nFOR HIGH-FIDELITY INDOOR RECONSTRUCTION", "authors": ["Ziyu Tang", "Weicai Ye", "Yifan Wang", "Di Huang", "Hujun Bao", "Tong He", "Guofeng Zhang"], "abstract": "Neural implicit reconstruction via volume rendering has demonstrated its effec-\ntiveness in recovering dense 3D surfaces. However, it is non-trivial to simultane-\nously recover meticulous geometry and preserve smoothness across regions with\ndiffering characteristics. To address this issue, previous methods typically employ\ngeometric priors, which are often constrained by the performance of the prior\nmodels. In this paper, we propose ND-SDF, which learns a Normal Ddeflection\nfield to represent the angular deviation between the scene normal and the prior\nnormal. Unlike previous methods that uniformly apply geometric priors on all\nsamples, introducing significant bias in accuracy, our proposed normal deflection\nfield dynamically learns and adapts the utilization of samples based on their spe-\ncific characteristics, thereby improving both the accuracy and effectiveness of the\nmodel. Our method not only obtains smooth weakly textured regions such as\nwalls and floors but also preserves the geometric details of complex structures. In\naddition, we introduce a novel ray sampling strategy based on the deflection an-\ngle to facilitate the unbiased rendering process, which significantly improves the\nquality and accuracy of intricate surfaces, especially on thin structures. Consistent\nimprovements on various challenging datasets demonstrate the superiority of our\nmethod.", "sections": [{"title": "INTRODUCTION", "content": "3D surface reconstruction Chen et al. (2024); Ye et al. (2024b;a); Wang et al. (2024); Ye et al. (2022;\n2023b); Liu et al. (2021); Li et al. (2020) aims to recover watertight, dense 3D geometry from multi\nview images Hartley & Zisserman (2003), representing a significant research area in computer vision\nand graphics. The recovered surfaces have proven invaluable for many downstream tasks, including\nrobotic navigation, AR/VR, and smart cities.\nRecently, coordinate-based networks Mildenhall et al. (2021); Barron et al. (2022); M\u00fcller et al.\n(2022); Martin-Brualla et al. (2021); Zhang et al. (2020); Ye et al. (2023a); Huang et al. (2024);\nMing et al. (2022) are drawing increasing attention, due to their remarkable performance in the task\nof novel view synthesis. Inspired by the implicit representation of the scene, many subsequent works\nintroduced SDF Park et al. (2019) or occupancy Li et al. (2022) to parameterize the 3D geometry.\nHowever, recovering high-fidelity surfaces remains challenging, as relying solely on color images\nfor supervision often results in an underconstrained problem, especially for regions like textureless\nwalls and ceilings. Previous methods Yu et al. (2022b); Wang et al. (2022) have attempted to mitigate\nthis issue by incorporating auxiliary supervision. For instance, MonoSDF employs monocular cues\nfrom a pretrained model as pseudo-ground truth to supervise the model, which partially alleviates the\nproblem in textureless areas. However, the large errors due to domain gaps in monocular cues and\nthe inconsistencies in view-dependent prior guidance often lead to visible detail loss and erroneous\nsurfaces, as depicted in Figure 3 of the experimental section.\nNoting that prior models are highly accurate in simpler regions like floors and walls but struggle\nwith complex geometries, we introduce ND-SDF for high-fidelity indoor reconstruction (Figure 1).\nOur core innovation is the construction of a deflection field that adaptively learns the geometric\ndeviations between the actual scene geometry and the prior geometry derived from normal priors.\nThe inherent deviation is defined as the angular difference between the true scene normals and the\nprior normals. By aligning the deflected scene normals with the normal cues, our model can adap-\ntively learn the deviation, which encourages accurate recovery of intricate structures maintaining\nfine details without being misled by erroneous priors.\nIn addition, we propose a novel adaptive deflection angle prior loss that leverages prior normals\nfor differential supervision of high and low-frequency areas, achieving an optimal balance between\nsmoothness and detail. Our observations indicate that large angle deviations are primarily concen-\ntrated in thin and fine-grained structures. Building on this insight, we introduce deflection-angle\nguided optimization to proactively facilitate the recovery of detailed structures. Furthermore, we\naddress a significant bias issue and propose deflection angle-guided unbiased rendering to improve\nthe reconstruction of small or thin structures, as shown in Figure 1. In summary, we present the\nfollowing contributions:\n\u2022 We propose a novel scene attribute field, named normal deflection field, which adaptively\nlearns the deviations between the scene and normal priors. This method aids us in dis-\ntinguishing between detailed and textureless areas. Therefore, we can restore more fine\nstructures while ensuring the smoothness and integrity of the scene.\n\u2022 Empirically assuming that prior models incur larger errors in complex areas, we employ the\ndeflection angle to discriminate between high and low-frequency regions. Consequently,\nwe propose a novel adaptive deflection angle prior loss that dynamically adjusts the utiliza-\ntion of distinct cues, thereby achieving a balance between complex structures and smooth\nsurfaces. Furthermore, we utilize the deflection angle to guide ray sampling and photomet-\nric optimization, facilitating the restoration of finer-grained structures.\n\u2022 To address the inherent bias issue of neural surface rendering, we integrate the unbiasing\nmethod Zhang et al. (2023) to implement an adaptive, unbiased rendering strategy. This\napproach facilitates the recovery of extremely thin structures without compromising scene\nfidelity. Our method outperforms previous approaches significantly in indoor reconstruc-\ntion evaluations, and this superiority is validated through extensive ablation experiments."}, {"title": "RELATED WORK", "content": "Neural Surface Representation of 3D scenes Recently, the representation of 3D scenes using\nneural fields has gained popularity due to their expressiveness and simplicity. NeRF-type ap-\nproaches Mildenhall et al. (2021); Barron et al. (2022); M\u00fcller et al. (2022); Fridovich-Keil et al.\n(2022); Sun et al. (2022) have proposed encoding coordinate-based density and appearance of scenes\nby utilizing simple multilayer perceptrons (MLPs) and explicit structures such as voxel grids, result-\ning in photorealistic novel view synthesis. However, these approaches fail to accurately recover\nsurfaces due to the lack of constraints on density. To address this issue, subsequent works, such\nas VolSDF Yariv et al. (2021) and Neus Wang et al. (2021), implicitly represent signed distance\nfunctions (SDFs) and employ a SDF-density conversion function to encourage precise surface re-\nconstruction. Other methods Li et al. (2023); Rosu & Behnke (2023); Wang et al. (2023); Yariv\net al. (2023); Fu et al. (2022); Zhang et al. (2023) have also been proposed, introducing different\nrepresentations and optimization techniques to further enhance reconstruction quality and efficiency.\nNevertheless, these aforementioned methods struggle to handle indoor scenes with a large number of\nlow-frequency areas (e.g., walls and floors), as photometric loss becomes unreliable in such regions.\nNeural reconstruction in Indoor Environments Due to the complex layouts of indoor scenes,\nadditional auxiliary data are required for reasonable reconstruction. Manhattan-SDF Guo et al.\n(2022) employs the Manhattan assumption and semantic priors to jointly regularize textureless re-\ngions such as walls and floors. HelixSurf Liang et al. (2023) achieves intertwined regularization\niteratively by combining neural implicit surface learning with PatchMatch-based multi-view stereo\n(MVS) Barnes et al. (2009) robustly and efficiently. Other works Yu et al. (2022b); Wang et al.\n(2022) propose utilizing monocular priors from pretrained models to achieve smooth and complete\nreconstruction. However, directly applying normal or depth priors Yu et al. (2022b) may lead to\nundesirable reconstruction results due to the unreliable nature of these priors. NeuRis Wang et al.\n(2022) filters out unreliable priors based on the assumption that areas with rich 2D visual features\nare more error-prone. H2O-SDF Park et al. (2024) simply utilizes prior uncertainty to re-weight the\nnormal prior loss. Nevertheless, both of these methods exhibit weak generalization capabilities. For\nexample, the visual feature hypothesis Wang et al. (2022) struggles to differentiate between rich-\ntextured planar areas. Similarly, prior-guided re-weighting Park et al. (2024) is further constrained\nby the domain gap between the prior model and the scene. On the other hand, DebSDF Xiao et al.\n(2024) effectively designs an uncertainty field based on a probabilistic model to guide the loss func-\ntion, leading to more robust and accurate reconstructions. In contrast to methods with weaker gen-\neralization capabilities and those relying on probabilistic models, our proposed method, ND-SDF,\nlearns a Normal Deflection field that enables the dynamic adaptation of priors based on their char-\nacteristics. This field directly models geometrically meaningful SO(3) residuals, resulting in highly\ndetailed surface reconstructions while maintaining smoothness and robustness."}, {"title": "METHOD", "content": "The primary objective of our Normal Deviation Signed Distance Function (ND-SDF) is to recon-\nstruct dense surfaces from calibrated multi-view images, with a primary focus on indoor scenes with\nestablished priors. To achieve this, we introduce a novel component known as the normal deflection\nfield. This field is designed to quantify and learn the deviations between actual scene geometries\nand their corresponding normal priors. By integrating this field, our method can adaptively supervise\nboth high and low-frequency regions in the scene. This adaptive supervision is crucial for preserving\nfine details while ensuring overall surface smoothness.\nThe operational framework of our approach is depicted in Figure 2. In subsequent sections, we delve\ninto a comprehensive discussion on the implementation of the normal deflection field. Additionally,\nwe explore various strategies that leverage this field to enhance the fidelity of surface details, thereby\npromoting a more accurate surface reconstruction."}, {"title": "PRELIMINARIES", "content": "Volume Rendering NeRF assumes that a ray r(t) = o + tv is emitted from viewpoint o in direction\nv, where t denotes the distance from the viewpoint. N points are sampled on the ray, i.e. x ="}, {"title": "NORMAL DEFLECTION FIELD", "content": "A significant challenge in indoor 3D reconstruction is achieving a balance between the smoothness\nof surfaces and the intricacy of complex structures. Traditional approaches relying solely on pho-\ntometric loss have proven inadequate for accurately capturing smooth areas, particularly in texture-\ndeficient regions. These methods often necessitate auxiliary data, such as normal priors, to enhance\nthe reconstruction quality in textureless areas. However, the uniform application of normal priors\nacross different scene types can impair the recovery of complex structures. This is primarily due\nto the variable reliability of these priors in diverse regions, where they may not accurately reflect\nthe underlying geometry. To address this issue, we propose the development of a Normal Deflec-\ntion field. This field is designed to dynamically represent the deviation between the actual scene\nnormals and the provided normal priors. By doing so, it effectively circumvents the potential mis-\nguidance from inconsistent priors, thereby enabling a more reliable and nuanced reconstruction of\nboth smooth and complex indoor structures.\nWe choose quaternion as the deflection form, which is a lightweight rotation representation. The\nquaternion can be parameterized by a single MLP $f_d$:\n$q_i = f_d(x_i, v_i, n_i, z_i)$,\nwhere $q_i = (q_0^i, q_1^i, q_2^i, q_3^i)$ is the deflection quaternion(normalized default) of sampled $x_i$. Quaternion at the surface intersection point is synthesized using volume rendering technology, similar to\nhow NeRF synthesizes colors:\n$Q(r) = \\sum_{i=1}^N T_i \\alpha_i q_i$\nWe deflect the rendered normal using the composed deflection quaternion Q(r):\n$\\tilde{N}^d(r) = Q(r) \\otimes \\tilde{N}(r) \\otimes Q^{-1}(r)$,\nwhere $\\tilde{N}^d$ denotes the deflected rendered normal, $Q^{-1}$ denotes the inverse of Q, also known as the\nconjugate, and $\\otimes$ is a quaternion multiplication operation. Since a quaternion can be represented in\ntrigonometric form, i.e. $Q = cos\\frac{\\theta}{2}+sin\\frac{\\theta}{2}(u^1i, u^2j, u^3k)$, this operation is to rotate the rendered\nnormal around the quaternion axis $u = (u^1, u^2, u^3)$ by $\\theta$ degrees.\nThe deviation between the scene and priors is learned by minimizing the difference between the\ndeflected rendered normal and the prior normal. Thus, we define the deflected normal loss:\n$L_{normal}^d = \\sum_{r \\in R} ||N^d (r) - N(r) || + ||1 - N^d (r) N(r) ||_1$,"}, {"title": "ADAPTIVE DEFLECTION ANGLE PRIOR LOSS", "content": "The learned deviation is defined as the deflection angle($\\theta$) of the rendered normal:\n$\\Delta \\theta = arccos (\\tilde{N}(r). \\tilde{N}^d(r))$,"}, {"title": "DEFLECTION ANGLE GUIDED OPTIMIZATION", "content": "Through the utilization of the proposed adaptive prior loss, we dynamically adjust the utilization of\nvarious priors, significantly enhancing the quality of reconstruction. However, this alone is insuf-\nficient for generating more complex structures. Fundamentally, merely learning the deviation does\nnot endow the capability to reconstruct additional details. Recognizing that large deflection angles\nindicate complex areas, we introduce three deflection angle guided optimization methods designed\nto facilitate the recovery of thinner and more fine-grained structures.\nDeflection angle guided sampling. Textureless areas, such as walls, constitute a significant por-\ntion of indoor scenes. These areas converge rapidly by utilizing priors. To prevent continuous\noversampling of well-learned smooth regions, we proactively sample more rays in complex areas to\ncapture finer details. The sampling process is guided by the deflection angles (see supplementary\nfor details).\nDeflection angle guided photometric loss. To further promote the recovery of details, we impose\nadditional photometric loss on complex areas indicated by the deflection angles. We re-weight the\noriginal color loss guided by the deflection angle, resulting in the re-weighted color loss:\n$L_{color} = \\sum_{r \\in R} W_{color} (\\Delta \\theta (r))||\\hat{C}(r) - C(r) ||_1$,\nwhere $\\Delta \\theta (r)$ denotes the deflection angle of sampled ray r, and $W_{color}$ is the re-weight function\nguided by $\\Delta \\theta$.\nDeflection angle guided unbiased rendering. Accurately reconstructing thin structures, such as\nchair legs, remains challenging. This limitation arises from inherent bias issues in SDF-induced vol-\nume rendering, manifesting in two ways: (1) According to TUVR Zhang et al. (2023), the derivative\nof the rendering weight $\\frac{\\partial \\sigma}{\\partial t}(t)$ is influenced by various angle differences between ray directions and\nscene normals. This results in a non-maximum value of $\\sigma$ at the surface intersection point, reducing\nsurface quality. (2) For rays passing close to the object, the Laplace CDF assigns high density to\npoints near the surface of the ray, leading to incorrect rendering depths and normals. This issue\ncauses thin structures to gradually disappear during training. Therefore, it is crucial to apply an\nunbiased rendering method during reconstruction.\nFollowing TUVR Zhang et al. (2023), we transform SDF to density using an unbiased function:\n$\\sigma(r(t_i)) = \\alpha \\Psi_{\\beta} (\\frac{-f_g(r(t_i))}{|f_g(r(t_i))|})$,\nwhere r(ti) = o + tiv represents a sampled point on the ray, and $f_g$ is the geometry network, which\npredicts the SDF value. This transformation effectively alleviates bias issues. However, we observe\nthat simply applying it results in the surface failing to converge. Hence, we only partially apply it to\nthin structures indicated by the learned deflection angles."}, {"title": "OPTIMIZATION", "content": "The overall loss function is defined as:\n$L = L_{color} + \\lambda_1 L_{eik} + \\lambda_2 L_{curv} + \\lambda_3 L_{normal}^d + \\lambda_4 L_{depth}^{ad}$,\nThe weights $\\lambda_1 ... \\lambda_4$ are utilized to balance the importance of these loss terms."}, {"title": "EXPERIMENT", "content": "Datasets We conducted experiments on four indoor datasets: ScanNet Dai et al. (2017),\nReplica Straub et al. (2019), TanksandTemples Knapitsch et al. (2017), and ScanNet++ Yeshwanth\net al. (2023). ScanNet contains 1513 indoor scenes captured with an iPad, processed using the\nBundleFusion algorithm to obtain camera poses and surface reconstruction. Replica is a synthetic\ndataset comprising 18 indoor scenes. Each scene features dense geometry and high dynamic range\ntextures. TanksandTemples is a large-scale 3D reconstruction dataset including high-resolution out\ndoor and indoor environments. We followed the splits from MonoSDF and applied the same eval\nuation settings. We also conduct experiments on ScanNet++, which includes 460 indoor scenes\ncaptured using laser scanners These scenes offer high-quality dense reconstructions and images.\nFor testing, we selected six scenes from ScanNet++.\nImplementation details Our method was implemented using PyTorch Paszke et al. (2019). The\nimage resolution for all scenes is 384\u00d7384. We obtained normal and depth cues using Omnidata.\nMulti-resolution hash grids were utilized for scene representation. Both the geometry network and\ncolor network consisted of two layers, each with 256 nodes. Our network was optimized using\nAdamW Loshchilov & Hutter (2017) optimizer with a learning rate of 1e-3. The weights for loss\nterms were: $\\lambda_1 = 0.05, \\lambda_2 = 0.0005, \\lambda_3 = 0.025, \\lambda_4 = 0.05$. Upon adequate initialization of\nthe deflection field, we initiated deflection angle guided sampling, photometric optimization, and\nunbiased rendering. All experiments were conducted on an NVIDIA TESLA A100 PCIe 40GB,\nwith each iteration sampling 4\u00d71024 rays, totaling 128,000 training steps. The total training time\nwas about 12 hours. Our hash encoding resolution spanned from 25 to 211 across 16 levels, with the\ninitial activation level set to 8 and activation steps to 2000.\nMetrics In line with prior research, we employ six standard metrics to evaluate the reconstructed\nmeshes: Accuracy, Completeness, Chamfer Distance, Precision, Recall, and F-score. Additionally,\nnormal consistency is utilized for evaluating the Replica dataset.\nBaselines We compare with the following methods: (1) Classic MVS COLMAP Schonberger &\nFrahm (2016); Sch\u00f6nberger et al. (2016); (2) Neural implicit methods including VolSDF and Neus;\n(3) Methods utilizing auxiliary data including MonoSDF and NeuRIS; (4) Other indoor reconstruc\ntion methods like HelixSurf Liang et al. (2023)."}, {"title": "COMPARISONS", "content": "ScanNet: In Figure 3, we present qualitative results and in Table 1, quantitative results are displayed.\nThese quantitative results are averaged over the selected four scenes. ND-SDF surpasses previous\nbest-performing works, achieving state-of-the-art performance. Specifically, we achieve the highest\nF-score, which serves as a credible indicator reflecting reconstruction accuracy by considering both\nAccuracy and Completeness. When compared with HelixSurf and MonoSDF (as shown in Figure 3),\nour approach accurately captures thin structures such as chair legs and lamp rings. This underscores\nthe superiority of our proposed deflection methods, which significantly enhance the recovery of thin\nand fine-grained structures indoors."}, {"title": "ABLATION STUDY", "content": "The learned deviations play a crucial role in locating high-frequency regions. Consequently, we\nemploy deflection angles for sampling, photometric optimization, and unbiased rendering to facil-\nitate the recovery of thin and fine-grained structures. We conducted detailed ablation experiments\nto assess the effectiveness of these modules, resulting in five post-ablation models: Base, ModelA,\nModelB, ModelC, and Ours. For quantitative results and comprehensive definitions, please refer to\nTable 6. Figure 4 visually illustrates how the proposed components enhance overall performance."}, {"title": "ABLATION OF DIFFERENT MODULES", "content": "The learned deviations play a crucial role in locating high-frequency regions. Consequently, we\nemploy deflection angles for sampling, photometric optimization, and unbiased rendering to facil-\nitate the recovery of thin and fine-grained structures. We conducted detailed ablation experiments\nto assess the effectiveness of these modules, resulting in five post-ablation models: Base, ModelA,\nModelB, ModelC, and Ours. For quantitative results and comprehensive definitions, please refer to\nTable 6. Figure 4 visually illustrates how the proposed components enhance overall performance."}, {"title": "Analysis of Normal Deflection Field", "content": "The Base model strictly adheres to monocular cues, leading\nto significant detail loss due to misguided priors in complex areas. In ModelA, we introduced the\ndeflection field and applied the deflected normal loss to learn deviations. The improvement in the\nF-score to 0.632 (as shown in Table 6) confirms the superiority of the deviation learning approach.\nHowever, we observed wrinkles (as depicted in Figure 4) in textureless regions like walls. This\nissue arises from the lack of constraints on deflection, resulting in inefficient utilization of priors in\nsmooth areas. In ModelB, introducing an adaptive deflection angle prior loss significantly enhanced\nreconstruction quality, balancing both smoothness and detail."}, {"title": "Analysis of deflection angle guided optimization", "content": "While ModelB substantially improves recon-\nstruction quality compared to the Base, it still struggles with finer structures. In ModelC, we in-\ntroduced deflection angle guided sampling and color loss to emphasize surface details. Finally, we\nincorporated deflection angle guided unbiased rendering to facilitate the recovery of thin structures.\nFigure 4 demonstrates that Ours recovers a wide range of complex and fine structures, including\nskull models, wires, and iron swabs, surpassing the Base model. As indicated in Table 6, Ours\nachieved the highest f-score, proving the effectiveness of the proposed modules for restoring high-\nfidelity surfaces."}, {"title": "ABLATION OF DIFFERENT PRIOR MODELS", "content": "Our method learns deviations between the scene and normal priors predicted by a specific pretrained\nmodel. It is essential to explore its effectiveness across different cues. To achieve this, we employed\nthe currently state-of-the-art method Geowizard Fu et al. (2024) to generate both depth and normal\ncues. The quantitative results, as summarized in Table 5, demonstrate that our method significantly\nenhances surface quality regardless of the cues used. This experiment demonstrates the superiority\nof our method. It invariably improves reconstruction accuracy in scenarios where prior knowledge\nis used."}, {"title": "CONCLUSION", "content": "We have presented ND-SDF, a novel approach that learns deviations between the scene and normal\npriors for high-fidelity indoor surface reconstruction. We propose an adaptive deflection angle prior\nloss to dynamically supervise areas with varying characteristics. By identifying high-frequency\nregions based on deflection angles, we employ angle guided optimization to generate thin and fine-\ngrained structures. Our method recovers a substantial amount of complex structures, as demon-\nstrated by extensive qualitative and quantitative results that confirm its superiority. ND-SDF effi-\nciently adapts to unreliable priors in challenging areas, significantly addressing the issue of detail\nloss caused by large prior errors for regions with fine structures.\nLimitation ND-SDF is confined to reconstruction scenarios involving priors. Our approach exhibits\nlimited capability in handling areas with less coverage. Due to inherent ambiguities in observations,\nthe deflection field may learn incorrect structures and settle into local optima. We kindly refer the\nreader to the supplementary material for more analysis and results. In the future, we may explore\nmulti-view consistency constraints to enhance reconstruction quality."}, {"title": "IMPLEMENTATION DETAILS", "content": "ARCHITECTURE\nWhen considering the inputs to the deflection network, two scenarios are contemplated: (1) No\ndeflection is required when the prior is entirely accurate. In this case, the quaternion axis aligns\nwith the scene normals, thereby closely associating deflection with the scene geometry. (2) Given\nthat the prior model is view-dependent, the quaternion for alignment with the prior must also be\nview-dependent. Consequently, we define:\n$q = f_d(x, v, n, z)$,\nwhere $q = (q_0, q_1, q_2, q_3)$ represents the deflection quaternion. As detailed in Section 3.2, the\nquaternion can be expressed in trigonometric form as $q = cos(\\theta/2)+sin(\\theta/2) (u^1i + u^2j + u^3k)$.\nWithin the deflection network, the rotation axis is initialized along the x-axis, i.e., $u = (1, 0, 0)$, and\nthe rotation angle $\\theta/2$ is set to $\\pi/2$. By default, the network normalizes the output quaternions.\nWe observe that the structure of the deflection network mirrors that of the color network Yariv\net al. (2020), denoted as $c = f(x, v, n, z)$. However, considering that the color network solely\nmodels scene-related radiance, while the deflection network encapsulates a greater extent of the\nprior deviation, we separate these two networks."}, {"title": "PROGRESSIVE WARM-UP", "content": "In the early stages of training, deflection fields would introduce negative effects such as noise. Before\nthe rough formation of scene geometry, these deflection quaternions are random and erroneous. If\nunconstrained, deflecting the true normals during this initial phase can lead to reconstruction errors,\nsuch as surface protrusions."}, {"title": "MORE DETAILS", "content": "ADAPTIVE DEFLECTION ANGLE PRIOR LOSS\nIn Section 3.3, we introduce two modulation functions, namely $g^d$ and $g$ (Eq. 12), which adjust the\nweight of the deflected and original normal loss terms based on the deflection angle.\nHere provide the specific form for both functions. For convenience, we employ a shifted logistic\nfunction to define them:"}, {"title": "DEFLECTION ANGLE GUIDED OPTIMIZATION", "content": "In Section 3.4, we introduce three deflection angle guided optimization methods, including ray sam-\npling, photometric optimization, and unbiased rendering.\nTo achieve ray sampling guided by the deflection angle, we dynamically maintain a deflection angle\nmap ($\\Delta \\theta$) per image during the training process. In the sampling phase, we first calculate the\nper-pixel sampling probability based on the deflection angle map. Then use the probability map\nfor inverse sampling, meaning that areas with greater deflection magnitude tend to have more rays\nsampled.\nWe employ a scaled and shifted logistic function to calculate the per-pixel sampling probability:\n$p(r_i) = \\frac{t_1}{(1 + e^{-s_1 (\\Delta \\theta(r_i) - \\theta_1)})}$\nwhere $\\Delta \\theta$ represents the deflection angle map, and $p(r_i) \\in [1, t_1 + 1]$, with the steepness controlled\nby $s_1$. We then normalize the probability map for inverse sampling using the following equation:\n$\\hat{p}(r_i) = \\frac{p(r_i)}{\\sum p(r_i)}$\nFor the angle guided color loss, we similarly employ a scaled and shifted logistic function to re\nweight the original color loss term guided by the deflection angle (Eq. 14):\n$W_{color} (\\Delta \\theta(r)) = 1 + \\frac{t_2}{(1 + e^{-s_2 (\\Delta \\theta(r) - \\theta_2)})}$"}, {"title": "EXTENSIVE ABLATION STUDIES", "content": "ABLATION OF PARTIAL UNBIASED RENDERING\nIn Section 3.5, we introduce an unbiased function. We partially apply this technique to thin structures\nindicated by the learned deflection angles, without doing so may result in an inability to converge\nand a decrease in reconstruction quality.\nWe attribute this issue to the inconsistency in perspective inherent in the transform function. Ac-\ncording to TUVR Zhang et al. (2023):\n$f(r(t_i)) = \\nabla f_g(o + t_i v) \\cdot v = n_i \\cdot v$,\nwhere v represents the ray direction and $n_i$ denotes the normal at point $x_i$ (also $r(t_i)$). We find that\n$\\frac{\\partial f_g}{\\partial t}(r(t_i))$ is equivalent to the cosine of ray and normal.\nConsider any other ray in space, denoted as $r'(t) = o' + t v'$. Assuming that this ray also intersects\nat $x_i$ and samples the same point such that $x_i = o' + t_j v' = x_i$, the volumetric density of $x_i$ under\nthe transform function is given by:\n$\\sigma(r'(t_j)) = \\alpha \\Psi_{\\beta} (\\frac{-f_g(r'(t_j))}{|f(r'(t_j))|})$\nwhere $f_g(r(t_i)) = f_g(r(t_j))$ because $x_i = x_j$ and $n_i = n_j$, but $|\\frac{\\partial f_g}{\\partial t}(r(t_i))| \\neq |\\frac{\\partial f_g}{\\partial t}(r'(t_j))|$ because\n$n_i v \\neq n_j v'$. The inconsistency in perspective leads to ambiguity in the volume density at the\nsame sampling point, causing complex indoor surfaces to fail to converge. Therefore, to reduce this"}, {"title": "SOCIETAL IMPACT", "content": "Our method contributes to high-fidelity indoor surface reconstruction. On the positive side, pre-\ncise indoor surface reconstruction can revolutionize fields such as architecture, virtual reality, and\nrobotics. It enables architects to visualize and optimize designs, enhances immersive experiences in\nvirtual environments, and aids in navigation for autonomous robots. However, there are also chal-\nlenges. Misuse of this technology could invade privacy, as detailed indoor reconstructions might\nreveal sensitive information. Striking a balance between innovation and responsible deployment\nwill be crucial for maximizing the positive impact while minimizing potential harm."}]}