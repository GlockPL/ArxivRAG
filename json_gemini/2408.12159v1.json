{"title": "Search-Based LLMs for Code Optimization", "authors": ["Shuzheng Gao", "Cuiyun Gao", "Wenchao Gu", "Michael R. Lyu"], "abstract": "The code written by developers usually suffers from efficiency problems and contain various performance bugs. These inefficiencies necessitate the research of automated refactoring methods for code optimization. Early research in code optimization employs rule-based methods and focuses on specific inefficiency issues, which are labor-intensive and suffer from the low coverage issue. Recent work regards the task as a sequence generation problem, and resorts to deep learning (DL) techniques such as large language models (LLMs). These methods typically prompt LLMs to directly generate optimized code. Although these methods show state-of-the-art performance, such one-step generation paradigm is hard to achieve an optimal solution. First, complex optimization methods such as combinatorial ones are hard to be captured by LLMs. Second, the one-step generation paradigm poses challenge in precisely infusing the knowledge required for effective code optimization within LLMs, resulting in under-optimized code.\nTo address these problems, we propose to model this task from the search perspective, and propose a search-based LLMs framework named SBLLM that enables iterative refinement and discovery of improved optimization methods. SBLLM synergistically integrate LLMs with evolutionary search and consists of three key components: 1) an execution-based representative sample selection part that evaluates the fitness of each existing optimized code and prioritizes promising ones to pilot the generation of improved code; 2) an adaptive optimization pattern retrieval part that infuses targeted optimization patterns into the model for guiding LLMs towards rectifying and progressively enhancing their optimization methods; and 3) a genetic operator-inspired chain-of-thought prompting part that aids LLMs in combining different optimization methods and generating improved optimization methods. Our evaluation of SBLLM on a dataset of Python and C++ code demonstrates its effectiveness in improving code efficiency. Specifically, the results indicate that SBLLM can improve program execution efficiency by up to 209.59% and consistently outperform all baseline methods by 8.75% ~ 28.06% and 1.15% ~ 9.56% with different LLMs in terms of top-5 speedup rate on Python and C++, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "As referred in the ISO/IEC 25010 software quality guidelines, the computational efficiency of the software is a critical cornerstone of system performance and user satisfaction [1], [2]. Inefficient code snippets can induce increased system latency, computational resources waste, and lead to poor user experience, which is referred to as performance bugs [3], [4]. Existing studies have demonstrated that these inefficiencies are widely existed in software and are hard to detect and repair [4], [5]. Consequently, the task of code optimization, which aims to automatically refactor the code, simplify its complexity and enhance performance metrics, has attracted researchers' attention in recent years.\nEarly research in code optimization primarily focuses on rule-based methods, which mainly target specific types of inefficiencies such as software misconfigurations [6] and loop inefficiencies [3]. These methods heavily rely on pre-defined rules created by experts, which are labor-intensive and suffer from the low coverage problem [2], [7]. Recent advancements in deep learning (DL) such as large language models (LLMs) have inspired a burgeoning body of research. These techniques learn code optimization patterns from data, broadening the array of inefficiency types that can be addressed. For example, RapGen [8] introduces a retrieval-augmented generation approach with LLMs to generate the optimized code in a zero-shot manner, surpassing the performance of previous fine-tuned small-sized neural models [2]. Another recent work, PIE [9] establishes a benchmark with test cases and evaluates the performance of various prompting methods such as in-context learning (ICL) [10] and chain of thought (COT) [11].\nDespite the success of LLM-based approaches, the adopted one-step generation paradigm tends to largely limit the performance of code optimization for two main reasons. First, it is challenging for LLMs to capture the complex optimization methods in one attempt. Code optimization involves a range of optimization levels [12], from incremental improvements like removing some unnecessary computation to more substantial optimizations that reshape the whole algorithm. Moreover, optimization patterns are combinatorial in nature [13], [14], implying that a single snippet may consist of multiple segments with the potential for different optimization. Second, it is difficult to precisely integrate the essential knowledge required for effective code optimization into LLMs. Although some studies [8], [15] have attempted to improve LLMs by retrieving similar code snippets, these approaches only consider the input similarity and ignores the characteristic of code optimization such as its combinatorial nature, which may result in the generation of under-optimized code.\nTo mitigate the above challenges, we propose to model the code optimization task from the search perspective instead of the typical generation viewpoint. Specifically, the task of refactoring a given code snippet to be more efficient can be formulated as a search problem, where the objective is to find the most efficient optimization method among a vast set of potential methods created by extensive code transformation"}, {"title": "II. PROPOSED FRAMEWORK", "content": "Fig. 1 presents the overview of the proposed framework SBLLM. SBLLM follows the evolutionary search paradigm that first generates initial solutions and then iteratively selects the fittest candidates while breeding new ones until termination criteria are met.\nTo start, SBLLM acquires the initial seed optimized code of the given slow code $S_t$ using existing optimization techniques. 1) In the execution-based representative sample selection part, SBLLM evaluates the fitness of current optimized code, and selects the representative samples that contain distinct and effective optimization methods by a re-ranking mechanism. 2) In the adaptive optimization pattern retrieval part, SBLLM retrieves code optimization patterns from the pattern base based on both the slow code and the selected representative samples, aiming to guide LLMs towards rectifying and progressively enhancing their optimization methods. 3) In the genetic operator-inspired chain-of-thought prompting part, SBLLM constructs a prompt that leverages crossover and mutation operations to facilitate LLMs in combining existing optimization methods and developing improved optimized code for $S_t$. The above procedure can be conducted multiple iterations until it no longer yields further optimization in the code efficiency or reaches the maximum iteration.\nTo enable LLMs iteratively refine the optimized code, we propose to evaluate the fitness of each sample and provide the selected representative ones to LLMs. As shown in Fig. 1, the execution-based representative sample selection module"}, {"title": "A. Overview", "content": "B. Execution-based Representative Sample Selection"}, {"title": "C. Adaptive Optimization Pattern Retrieval", "content": "The adaptive optimization pattern retrieval module aims at providing LLMs with effective optimization patterns to facilitate the generation of improved optimized code. The retrieved optimization patterns are expected to provide hints to LLMs towards generating correct and more efficient code. We propose to involve both the input slow code $s_t$ and selected representative samples $RS$ to adaptively retrieve effective optimization patterns for LLMs. Specifically, the retrieval part considers both the optimization methods that are semantically similar to $RS$ for rectifying potential errors, and those different from $RS$ for drawing inspiration from unexploited optimization methods. As shown in Fig. 1, the adaptive optimization pattern retrieval module mainly contains two steps, including (a) fine-grained pattern parsing and (b) representative sample-based pattern retrieval.\nTo facilitate accurate re- retrieval of similar and different patterns, SBLLM first parses each optimization pair in the training dataset and extracts patterns with fine-grained optimization information to construct the pattern base. Each optimization pair consists of a non-optimized code snippet $s$ and its optimized version $f$. To preserve the general optimization information and eliminate project-specific influences, SBLLM parses them into ASTS and obtains their abstracted code $s_a$ and $f_a$. Then it identifies the fine-grained optimization information with the \"difflib\" package [26] by isolating the abstracted deleted statements $d_s$ from $s_a$, as well as the abstracted added statements $d_f$ from $f_a$. Based on the above process, we obtain the pattern base with fine-grained optimization information including $s_a$, $f_a$, $d_s$, and $d_f$, which is then used to assist the retrieval process.\nThe representative sample-based pattern retrieval method aims at guiding LLMs in refining the current optimized code $RS$ in two ways. Specifically, as shown in Fig. 2, current optimized code may contain incorrect and insufficient optimizations. To address these issues, we propose to adaptively retrieve separate patterns that are semantically similar to and different from $RS$, respectively.\nFirst, as shown in Fig. 2 (a) \u2460, the current optimized code intends to use the Eratosthenes Sieve algorithm to reduce the time complexity. However, as LLMs do not learn this optimization method well, the optimized code contains incorrect initialization, which will lead to an out-of-index error. The similar pattern presented in Fig. 2 (a) \u2461 shows a correct implementation of this algorithm that can provide hints for LLMs to rectify the errors. However, the pattern is hard to be retrieved based solely on its semantic similarity"}, {"title": "D. Genetic Operator-inspired Chain-of-thought Prompting", "content": "The part aims at guiding LLMs in integrating different optimization methods from representative samples $RS$ and retrieved patterns $P$, and subsequently generating refined optimized code. Specifically, we propose to aid LLMs with the evolutionary algorithm's generic operators, and introduce the genetic operator-inspired chain-of-thought (GO-COT) prompt. Genetic operators [29] are inspired by biological evolution principles, and comprise crossover and mutation to synthesize"}, {"title": "E. Evolutionary Optimization", "content": "Based on the aforementioned processes, SBLLM iteratively generates improved optimized code. As shown in Algorithm 2, at the end of each iteration, the newly generated code will then be integrated with the representative samples in the prompt for the next iteration generation. The iterative refinement process continues until it reaches the convergence condition. Specifically, if the code has been correctly optimized and the representative samples $RS$ remain unchanged with those in the last iteration, we terminate the process. This is based on the intuition that the prompt in this iteration will be the same as the one in the last iteration and LLMs are less likely to generate better code."}, {"title": "III. EXPERIMENTAL SETUP", "content": "In the evaluation, we focus on the following four research questions:\nRQ1: How effective is SBLLM in improving code efficiency?\nRQ2: What is the fine-grained performance of code generated by SBLLM across different optimization levels?\nRQ3: What are the contributions of different modules in SBLLM?\nRQ4: What is the impact of different hyper-parameters on the performance of SBLLM?\nTo study RQ1, we conduct a comprehensive evaluation of SBLLM by comparing with four representative baseline methods across four popular LLMs, aiming to provide a thorough assessment across language models with different parameter sizes and capabilities. For RQ2, we delve into the analysis of the proportion of generated code across different accuracy and speedup rate levels, including cases where the code is not correct, correct but not faster than slow code, faster than slow code but not faster than human reference and faster than human reference. Additionally, we investigate how much code generated by SBLLM could be faster than reference code derived by human developers. For RQ3, we remove different parts in SBLLM to assess their individual contributions. For RQ4, we explore the influence of various hyperparameters by varying the number of representative samples in the prompt and the number of maximum iterations."}, {"title": "A. Research Questions", "content": "B. Datasets\nIn this work, we evaluate SBLLM on the widely-used PIE [9] dataset which contains two programming languages (i.e., Python and C++). The two popular programming lan- guages are critical for code optimization evaluation. Python is a dynamic language known for its slow execution speed [30]. In contrast, C++ is a statically typed, compiled language renowned for its high performance, especially when lever- aging the O3 optimization option. By applying the proposed optimization, we can validate whether the optimized methods generated by SBLLM are trivial and can be achieved through compiler optimization techniques. The PIE dataset is derived from CodeNET [31], which is curated from an online judge system and contains 3,474 programming problems. Here we craft the public and private test cases for each problem by using the input-output examples in the program description in CodeNET and the test cases provided by AlphaCode [24], respectively. On average, for each problem, we obtain 2.8 public test cases to obtain feedback for SBLLM and 95.9 private test cases to evaluate the correctness and efficiency of generated code. Each entry in PIE contains a triplet (problem id, slow code, fast code) written by the same programmer. The Python subset of the PIE dataset comprises 36,857 training samples, 1,940 valid samples, and 986 test samples; while the C++ comprises 77,967 training samples, 2,544 valid samples, and 994 test samples."}, {"title": "C. Baselines", "content": "To provide a comprehensive evaluation, we experiment on four popular LLMs and compare SBLLM with four representative prompt methods, with details as below.\nFor LLMs, we evaluate the performance of SBLLM on both open-source and closed-source models, including CodeLlama, Gemini, ChatGPT, and GPT-4. CodeLlama [20] is a family of open-source large-scale code language models developed by Meta. We use the 34B instruct-tuned version (i.e., CodeLlama-34b-Instruct-hf) for experiments. ChatGPT [22] and GPT-4 [23] are two popular LLMs developed by OpenAI which show versatile abilities across different fields such as code generation. They are closed-source model and we access it through APIs (i.e., gpt-3.5-turbo-0613 and gpt-4-1106-preview). Gemini [21] is a recent powerful closed-source LLM developed by Google which shows comparable ability with GPT-4. We also access it based on its official API (i.e., gemini-pro).\nAs for the prompt methods, we follow previous work [9] and involve four representative methods including instruction prompting, in-context learning (ICL), retrieval-augment generation (RAG), and chain-of-though (COT). Instruction prompting directly prompts LLMs to generate optimized code without providing other information. In-context learning adds some examples (input-output pair) before the query sample to help the model understand this task. Following prior work [32], we randomly sample four pairs from the training set to create the examples for ICL. As for retrieval-augment generation (RAG) method, instead of random selection, it retrieves different samples from the training set for different query samples. Specifically, we employ BM25 to select the code from the training set with the highest similarity to the query sample. Lastly, in the chain-of-thought (COT) prompt, we follow [8], [9] and employ prompts that instruct the LLM to first explain how to optimize the program before producing the optimized code. We use the same examples as ICL for COT and manually craft the explanations to aid the LLM in reasoning through COT. The detailed prompts for all baseline methods are provided in our replication package [28]."}, {"title": "D. Metrics", "content": "To evaluate the correctness and efficiency of optimized code, we follow previous work [9] and measure the following metrics:\nPercent Optimized (OPT): OPT denotes the fraction of code in the test set that demonstrate improvement through a given method. A program must be at least 10% faster and correct (i.e., pass all test cases) to contribute, i.e., $\\frac{T(s)-T(o)}{T(o)} \\geq 10\\%$ and $A(o) = 1$, where $T(\\cdot)$ and $A(\\cdot)$ represent the execution time and accuracy and $o$ and $s$ denote the optimized and slow code, respectively.\nSpeedup Rate (SP): SP measures the improvement in running time. We first calculate the speedup rate of each generated code and then report the average results on the whole test set. If a generated code is either incorrect or slower than the original slow code, we assign a speedup of 1.0 to that example, as the worst-case scenario assumes the original program has a speedup of 1.0. Formally, SP is calculated as follows:\n$SP = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{T(s_i)}{T(o_i)}$ if $A(o_i) = 1 \\land T(o_i) \\leq T(s_i)$ else 1\nwhere $n$ is the size of test set."}, {"title": "E. Implementation Details", "content": "For the hyperparameters of all LLMs, following the previous work [9], we set the temperature to 0.7 for all experiments and generate five results by random sampling. For all baseline methods, the optimized code will be re-ranked based on the output probability predicted by the LLMs. For SBLLM, after the whole optimization process, we re-rank the optimized code obtained in the last iteration using the selection method in Algorithm 1. As for the hyper-parameters of SBLLM, we set the number of selected representative samples $N_s$ to three and the maximum iteration number to four. The impact of different numbers is discussed in Section IV-D. For GPT-4, due to the cost limitation, we randomly sample 10% data in the test set for experiment. Following previous work [9], we execute each slow and generated program 25 times, and report the average execution results excluding the first run. For the execution environment, we execute Python programs with Python 3.9.12 and compile all C++ programs with GCC version 9.4.0 and C++17 as well as the O3 optimization flag. All experiments are conducted on a Linux server (64-bit Ubuntu 20.04) with one 112-core Intel Xeon Platinum 8276 CPU@ 2.20GHz, four NVIDIA A100-40GB GPUs, and 1TB RAM."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "To assess the effectiveness of SBLLM in improving code efficiency, we compare SBLLM with four representative prompt methods on four popular LLMs. Table I presents the performance of SBLLM along with baseline methods on Python and C++. For each metric, we denote the performance using the Top@k metric, where $k$ represents the number of generated code snippets considered.\nSBLLM demonstrates considerable improvements over the baseline methods across both languages and all LLMs. For example, when compared to the strongest baseline method, COT, SBLLM achieves an average improvement of 5.11% and 2.87% in OPT@5 on Python and C++, respectively. These results demonstrate the effectiveness of SBLLM in identifying efficient optimization methods within the vast search space.\nBesides, by comparing the improvements across different LLMs, we observe that SBLLM achieves higher enhancements on more powerful LLMs such as ChatGPT and GPT-4. For instance, on Python, SBLLM enhances the OPT@1 of CodeLlama and GPT-4 by 2.37% and 9.19%, respectively. This discrepancy can be attributed to the limited instruction-following capability and context size of CodeLlama, which might make it not fully comprehend the instructions and"}, {"title": "A. RQ1: Comparison with Baselines", "content": "Answer to RQ1: SBLLM successfully optimizes the most code snippets compared to all baselines across different LLMs. It boosts program execution efficiency by up to 209.59% and 154.76%, outperforming the top-5 speedup rate of all baselines by 8.75% ~ 28.06% and 1.15% ~ 9.56% on Python and C++, respectively."}, {"title": "B. RQ2: Fine-grained Performance Analysis", "content": "In this RQ, we study the fine-grained proportion of generated code across different levels of accuracy and speedup rates. To achieve this, we classify the generated code snippets"}, {"title": "C. RQ3: Ablation Study", "content": "To investigate the individual contribution of different components in SBLLM, we conduct an ablation study and present the results in Table II. Due to the space limitation, we only present the results on ChatGPT, with results for other LLMs presented on our GitHub repository [28]."}, {"title": "D. RQ4: Parameter Analysis", "content": "In this section, we investigate the influence of two parameters, namely the number of selected examples ($N_s$) and the maximum number of iterations, on the performance of SBLLM. Similar to Section IV-C, the results presented in this"}, {"title": "Answer to RQ2: SBLLM excels in generating code with the lowest error rate in Python and achieves the highest rate of efficiency surpassing human-written reference across both programming languages.", "content": "Answer to RQ3: All components in SBLLM contribute to the performance. Removing the execution-based representative sample selection, adaptive optimization pattern retrieval, or GO-COT leads to substantial performance decreases."}, {"title": "V. DISCUSSION", "content": "SBLLM synergistically integrates evolutionary search into LLMs. The first advantage of SBLLM is its novel"}, {"title": "Answer to RQ4: SBLLM achieves the best performance with three representative samples. For the number of iterations, the performance of SBLLM improves with more iterations initially, but excessively large iteration numbers may cause performance degradation.", "content": "A. What makes SBLLM work?"}, {"title": "VI. RELATED WORK", "content": "Early research in code optimization techniques tends to employ rule-based methods and focus on specific inefficiency types such as software misconfigurations and loop inefficien- ciess [3], [6], [40], [41]. Recently, deep learning-based meth- ods are introduced and achieve promising results. DeepDev- PERF [2] is a pre-trained model that suggests performance improvements in C# code. Chen et al. [42] introduce a varia- tional auto-encoder that identifies effective code edits for per- formance. RAPGen [8] leverages OpenAI Codex [39] to fix C# code inefficiencies issue in zero-shot. It surpasses DeepDev- PERF in precision without extra training. Supersonic [7] develops a seq2seq model for code optimization using diff- based output representation. PIE [9] is a recent benchmarks that explore using LLMs for improving code performance. It evaluates various prompting methods and shows that these methods can significantly speed up code execution. Different from the above work that typically focuses on directly gener- ating optimized code, SBLLM aims at iterative refining and boosting the initial results directly generated by LLMs in a search-based manner."}, {"title": "A. Code Optimization", "content": "B. Large Language Models for Software Engineering\nRecently, the advent of LLMs has significantly advanced various software engineering tasks [38], [43], [44]. A lot of research is dedicated to effectively harnessing the capability of LLMs by fine-tuning or prompt engineering for software engineering tasks [45]\u2013[47]. For example, WizardCoder [48] fine-tunes LLMs with complex instructions for code generation. Xia et al. [34] leverage LLMs for program repair by using the cloze-stype prediction. TypeGEN [49] prompts LLMs with static analysis results and COT prompts for type inference. CHATRepair [50] iteratively evaluates programs on test cases and feeds the error messages to LLMs for further patch generation. Self-edit [51] utilizes compiler error messages to enhance the correctness of code generation."}, {"title": "B. Threats to Validity", "content": "We identify three main threats to the validity of our study:\n1) The selection of languages. In this paper, we conduct experiments on the PIE dataset containing two widely-used programming languages Python and C++. While there are other datasets that contain different programming languages, such as the C# dataset in DeepDev-PERF [2], we don't use this dataset since it does not contain test cases. In the future, we will create test cases for this dataset and conduct experiments.\n2) The selection of LLMs. Another threat is the baselines we utilized in our evaluation. We evaluate SBLLM on four popular and representative LLMs and prompting methods. While there are other existing LLMs [38], [39], our proposed SBLLM is model-agnostic and can be easily generalized to different LLMs. Furthermore, our method focuses on how to further boost the initial results directly generated by LLMs. Therefore, our research is orthogonal to the work that solely generates code such as fine-tuning and one-step prompting methods. In future work, we plan to further evaluate the effectiveness of SBLLM on other LLMs.\n3) Potential data leakage. In this paper, we conduct ex- periments utilizing the APIs of ChatGPT, GPT-4, and Gemini. However, as these models are closed-source, their training data are not publicly accessible, giving rise to concerns regarding potential data leakage. However, our experiments reveal that directly prompting the LLMs to optimize the code cannot yield promising results. Therefore, we believe that the optimization code generated by SBLLM is not simply from memorizing the training data."}, {"title": "C. SBSE and Large Language Models", "content": "CodaMOSA [52] leverages LLMs to provide example test cases for under-covered functions when search-based testing hits a coverage stall. Tawosi et al. [53] use available search-based methods to optimize the number and combination of few-shot examples for LLMs in the story point estimation task. Brownlee et al. [54] introduce a method that employs LLMs as mutation operators for genetic improvement. Similarly, Kang and Yoo [55] propose to leverage the capabilities of LLMs in code comprehension and generation for creating objective- tailored mutants. Dakhama et al. [56] improve search-based fuzzing by using ChatGPT to parameterise C programs for gem5 testing."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose SBLLM, a search-based LLMs framework for code optimization. SBLLM synergistically integrates LLMs with evolutionary search, encompassing three components: an execution-based representative sample selection mechanism, an adaptive optimization pattern retrieval method, and a genetic operator-inspired chain-of-thought prompting method. Extensive experiments on four popular LLMs show that SBLLM can effectively guide LLMs towards identifying efficient optimization methods in the vast search space. In the future, we plan to apply our search-based LLMS framework to other tasks in software engineering such as program repair. Our source code and detailed experimental results are available at [28]."}]}