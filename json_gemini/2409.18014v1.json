{"title": "Role-RL: Online Long-Context Processing with Role Reinforcement\nLearning for Distinct LLMs in Their Optimal Roles", "authors": ["Lewei He", "Tianyu Shi", "Pengran Huang", "Bingzhi Chen", "Qianglong Chen", "Jiahui Pan"], "abstract": "Large language models (LLMs) with long-\ncontext processing are still challenging because\nof their implementation complexity, training effi-\nciency and data sparsity. To address this issue, a\nnew paradigm named Online Long-context Pro-\ncessing (OLP) is proposed when we process a\ndocument of unlimited length, which typically\noccurs in the information reception and organi-\nzation of diverse streaming media such as au-\ntomated news reporting, live e-commerce, and\nviral short videos. Moreover, a dilemma was of-\nten encountered when we tried to select the most\nsuitable LLM from a large number of LLMs\namidst explosive growth aiming for outstand-\ning performance, affordable prices, and short re-\nsponse delays. In view of this, we also develop\nRole Reinforcement Learning (Role-RL) to auto-\nmatically deploy different LLMs in their respec-\ntive roles within the OLP pipeline according to\ntheir actual performance. Extensive experiments\nare conducted on our OLP-MINI dataset and\nit is found that OLP with Role-RL framework\nachieves OLP benchmark with an average recall\nrate of 93.2% and the LLM cost saved by 79.4%.\nThe code and dataset are publicly available at:\nhttps://anonymous.4open.science/r/Role-RL.", "sections": [{"title": "Introduction", "content": "In the academic and technological spheres, large lan-\nguage models (LLMs) have emerged as pivotal tools\nfor improving production efficiency and advancing our\nunderstanding of human language. These models lever-\nage deep learning techniques, particularly the power\nof transformers, to capture the intricacies of syntax,\nsemantics, and context within both oral and written\ncommunications.\nAs computational linguistics makes a significant\nleap forward, a growing number of LLMs are quickly\ncoming to the forefront. However, the associated prob-\nlems also become more pronounced, that is, choosing\nthe most suitable LLM for a given task. Studies have\nshown that different LLMs have different strengths,\nweaknesses, and even personalities due to differences\nin training datasets (Serapio-Garc\u00eda et al. 2023). For\nexample, generative model T5 has superior perfor-\nmance to discriminative model Bert on conversational\nQA tasks (Abbasiantaeb et al. 2024). For zero-shot\ncoding assignments, InCoder-1b scores twice as much\nas Code Llama-7b, but the situation reverses in 1-\nshot tasks, with Code Llama-7b scoring considerably\nhigher (Fan et al. 2024). In terms of comparative rea-\nsoning, multimodal LLM LLaVA-1.6 has better perfor-\nmance in spatiality comparison tasks whereas GPT-4V\nis stronger in temporality comparison tasks (Kil et al.\n2024). In terms of personality, GPT-Neo is more extro-\nverted than GPT-3.5, while GPT-3.5 exhibits a higher\ndegree of agreeableness (Lee et al. 2024)(Jiang et al.\n2024). It was also found that ChatGPT embodies the\nENTJ personality type in the MBTI framework, char-\nacterized by self-confidence, decisiveness, and natural\nleadership abilities, whereas OpenLlama7b aligns with\nthe INFJ type, known for its deep insight into people\nand a strong adherence to personal values (Pan and\nZeng 2023). Hence, it is essential to carefully choose\nthe most appropriate LLMs on the basis of their unique\ncharacteristics and the role requirements they are in-\ntended to fulfill.\nAnother issue regarding LLM is the ability to pro-\ncess long contexts, especially in the scenario of stream-\ning media transcripts of unlimited length, as shown in\nFigure 1. Streaming media refers to an emerging form\nof media content that is continuously delivered with-\nout requiring a complete download before playback,\nwhich is becoming increasingly relied upon by the\npublic (Falkowski-Gilski and Uhl 2020). A survey of\nyoung people between 16 and 24 years of age revealed\nthat 55.1% of the respondents spent 2-3 hours, 16.3%\nspent 3-4 hours, and 12.2% spent more than 4 hours\non streaming media every day (Deshpande et al. 2020).\nThis means that an end user consumes approximately\n27k words per day if a speed of 2.5 words per sec-\nond is assumed (Yuan, Liberman, and Cieri 2006) for\nlanguage-intensive content such as live e-commerce,\nwhich is comparable to a novella with 20k-40k words.\nFrom the perspective of end users, it is helpful if an\nexcerpt of transcripts is generated in real-time to help"}, {"title": "Related work", "content": "LLM on long-context tasks is a popular research topic\nand many techniques were developed to improve the\nlong-context processing ability. Infinite-LLM (Lin\net al. 2024) manages dynamic context lengths ef-\nficiently through a distributed attention mechanism\ncalled DistAttention and a pooled GPU memory strat-\negy, enabling support for extensive context lengths\nup to 2000K tokens and demonstrating a 1.35-3.4x\nthroughput improvement. MEGALODON (Ma et al.\n2024) features timestep normalization and normalized\nattention for efficient long context handling, outper-\nforming Transformers in efficiency and accuracy es-\npecially in pretraining and downstream tasks. Lon-\ngROPE (Ding et al. 2024) expands LLM context to\n2048k tokens with an efficient search algorithm that\nexploits non-uniformities in positional interpolation\nand a progressive extension strategy. SelfExtend (Jin\net al. 2024) enhances LLMs' long context handling\nvia a bi-level attention mechanism which maps large\nrelative positions to known ones with a simple floor\ndivision operation.\nMeanwhile, long-context processing by agent coop-\neration is another direction. CoA (Zhang et al. 2024)\nimproves LLMs in long-context tasks by aggregat-\ning information and reasoning across models with en-\nhanced performance in QA, summarization, and code\ncompletion. GraphReader (Li et al. 2024) structures\nlong texts into a graph for autonomous exploration,\noutperforming other models in long-context QA and\nshowing robustness in handling very long texts. LON-\nGAGENT (Zhao et al. 2024) scales LLMs to manage\nup to 128K tokens by dividing contexts, assigning\nagents, and resolving conflicts, with an average im-\nprovement of 19.53% in single-document. PEARL\n(Sun et al. 2023) enhances reasoning over long docu-\nments by segmenting the process into action mining,\nplan formulation, and execution, showing effective-\nness on a subset of the QUALITY dataset."}, {"title": "Novel agent frameworks", "content": "Beyond agent frameworks tailored for the long-context\ntask, numerous insights and methodologies can be\ngleaned from a variety of multipurpose frameworks,\nfor example, to elect an LLM leader (Guo et al. 2024)\nand add agents into the system dynamically and au-\ntomatically (Talebirad and Nadiri 2023). In addition,"}, {"title": "Problem formulation", "content": "A transcript of unlimited length allows for an un-\nlimited number of potential chunks, represented by\n{C1, C2, ...}. In the chunk i, there are several topics\n{t1, t2,..., tm}. For the topic j, we are concerned\nabout n aspects {aj,1, aj,2, ..., aj,n} which are pre-\ndefined and usually kept the same for all topics. If\nwe assume the first passage related to topic j is pj,1\nand the last is pj,\u22121, for topic j we have a set of pas-\nsages pj = {Pj,1, ..., Pj,\u22121}. In order to organize pj\nacross different aspects, we aim to find pj \u2229 aj,k with\nk = 1 ~ n for the n aspects of the topic j. Similarly,\nthe process also applies to each and every other topic\nwithin a given transcript."}, {"title": "Online Long-context Processing (OLP)", "content": "Streaming media has become more and more impor-\ntant in our daily lives. However, it is excessively la-\nborious for humans to distill useful information from\nstreaming media, and this task also does not fit well\ninto conventional long-context processing due to its\nunlimited length of the context and the demand for\nreal-time output with online processing ability. In or-\nder to address this issue and organize the transcripts\nof streaming media according to the required specifi-\ncations with minimal repetitions, losses and halluci-\nnations, we propose Online Long-context Processing\n(OLP) pipeline. As shown in Figure 2, OLP splits the\nstreaming long context into chunks automatically ac-\ncording to its length and semantic information, and\nprocesses each chunk into structured text with core\ntopics and related aspects of supportive information\nlisted below. It consists of Topic finder, Topic loca-\ntor, Relationship checker, Content organizer, Format\nchecker, and Chunk splitter, which cooperate closely\nto classify each passage of the streaming long context\ninto the aspects that we focus on.\nTopic finder recognizes the topics that we are inter-\nested in, for instance products for sale, news events,\nand short video storylines. Topic locator locates the\ncontent correlated to each topic, usually by the passage\nindex that is distributed for each passage as its unique\nID. Relationship checker examines the topics that are\nrecognized in the first step based on their semantic and\nlocation information, and decides whether to delete\na topic or merge two relevant topics together. With\nthe confirmed topics and locations, Content organizer\nretrieves the passages relevant to each topic except the\nlast topic in a chunk, and reorganizes the passages into\na structured layout topic by topic with supportive pas-\nsages below each key aspect. Finally, Chunk splitter\ndivides the current chunk between the last topic and\nother topics, and passes the passages of the last topic\nto the next chunk in order to avoid the scenario where\na single topic is divided into two chunks with two dif-\nferent titles allocated. As such, OLP guarantees that\nthe topics are recognized properly and the passages in\neach aspect of a topic are excerpted exhaustively."}, {"title": "Role Reinforcement Learning (Role-RL)", "content": "A dilemma was often encountered when we sought the\noptimal LLM from already numerous and still grow-\ning LLMs with outstanding performance, affordable\nprice and short response delay. Lots of experimental\nworks were required to test the LLMs one by one, espe-\ncially in scenarios like OLP where the roles are highly\nspecialized with very distinct difficulty levels and re-\nsponse delay requests. Worse still, these labor works\nare permanent since the role requirements may evolve\nand there are always new LLMs coming out to be com-\npared with the LLMs already in position. Therefore,\nRole-RL is developed by this work to select the LLMs\nautomatically and place them into the most suitable\nroles with optimized overall performance of an LLM\nnetwork.\nQ-learning and reward. Ideally, Role-RL should\nmaintain an LLM that successfully completed the pre-\nvious round, but assign a more potent LLM if the\nprevious attempt was unsuccessful, as shown in Figure\n3. Therefore in the reinforcement learning, the possi-\nble actions A are the LLMs in the pool to select from,\nand the state St is the LLM used in the previous round\nwith a success or failure resulted. Given the discrete\nactions and states in a limited number, Q-learning is\nadopted as the base of Role-RL due to its robustness\nand efficiency. The reward function in Role-RL can\nbe tailored to specific objectives and conditions, but\ntypically it is expressed by\n$R = v - k_{1}c - k_{2}t$                                                                                                                    (1)\nwhere v stands for the reward due to the accuracy of\nthe answer which is judged by the board members,\nc for the LLM cost, t for the response delay, and $k_1$"}, {"title": "Update of board member and LLM pool.", "content": "Since\nthe accuracy of an LLM answer is judged by the board\nmembers, they should be elected from the LLMs with\nrelatively high inference ability to increase the confi-\ndence level of their judgments. Moreover, the election\nof board members should also be automatic without\ngenerating too much burden on the system. In order\nto achieve this, Markov chains are extracted from the\nfailure states in the Q-table, since only stronger LLMs\nreceive positive rewards when an LLM fails, which in\nturn implies an inherent ranking order of those LLMs.\nTaking Figure 4 as an example, the Markov chain\nin Role A ends at LLM 3, indicating that LLM 3 is\nenough to solve all the tasks in Role A. However in\nRole B and Role C, the Markov chains go through\nLLM 3 and end at LLM 5 and LLM 4 respectively,\nwhich implies that LLM 5 and LLM 4 are stronger\nthan LLM 3 and thus selected as the board members\nto judge the answers of all the LLMs. Moreover, the\nboard members should be updated according to the\ncurrent Markov chains and Q-tables, and the update of\nboard members should be smooth to reduce any fluctu-\nations imposed on the system. Therefore, a maximum\nchange limit denoted by \u0394\u03c9 is applied to the voting\nweight of each board member, and the final reward is\ncalculated by a weighted sum of the rewards from all\nboard members. That is, an LLM is excluded from the\nadvisory board if its weight is equal to 0. Furthermore,\nwith the evolution of LLMs, the LLM pool should be\ncontinually updated with new models that offer en-\nhanced capabilities at a lower cost, and the LLM pool\ncan either be expanded or remain the same size by\nreplacing the LLM with the lowest success rate."}, {"title": "Greedy-update and cross-update strategies.", "content": "As\nshown in Figure 5, a greedy-update strategy is devel-\noped to use the data more effectively and avoid a cold\nstart during the deployment of Role-RL. In Round\n0, every LLM in the pool takes the role in turn and\nobtains respective reward Ri,o and state Si,0 where i\nranges from 0 to the maximum LLM index n. Then\none LLM serves as the previous LLM (e.g. LLM 0)\nand all other LLMs (e.g. LLM 1 ~ LLM n) serve as\nthe current LLM in turn to update the Q-table with\nthe previous state (i.e. S0,0) and current rewards (i.e."}, {"title": "Datasets", "content": "The dataset for OLP can be transformed from any\nlong-context datasets by adding a unique index at the\nfront of each passage for topic locating. We also con-\ntribute our dataset OLP-MINI which consists of 300\nEnglish live-commerce transcripts from TikTok and\n300 Chinese live-commerce transcripts from Alibaba.\nThe statistics of transcript length is shown in Figure\n6, with an average length of 8758 words for English\ntranscripts and 7304 characters for Chinese transcripts."}, {"title": "Experiment", "content": "The experiments are conducted on the OLP-MINI\ndataset with four aspects listed for each topic, namely\n\"Opening\", \"Product description\", \"Price\" and \"Or-"}, {"title": "Main Results", "content": "The scoreboards for each LLM, each role, and each\ntask judged by Gemini-1.5-Pro and Claude-3-Opus are\nshown in Figure 7, with a success noted by a blue \u201c1\u201d,\na failure noted by a red \"-1\", and the different scores\nbetween two scoreboards marked by dotted circles.\nTask 2 is relatively simpler with more correct answers,\nand the first two LLMs, especially Claude-3-Opus,"}]}