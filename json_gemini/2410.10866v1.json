{"title": "CODEUNLEARN: AMORTIZED ZERO-SHOT MACHINE\nUNLEARNING IN LANGUAGE MODELS USING DIS-\nCRETE CONCEPT", "authors": ["YuXuan Wu", "Bonaventure F. P. Dossou", "Dianbo Liu"], "abstract": "Large Language Models (LLMs) offer extensive knowledge across various do-\nmains, but they may inadvertently memorize sensitive, unauthorized, or malicious\ndata, such as personal information in the medical and financial sectors. Machine\nunlearning methods aim to remove specific information from models after train-\ning to address this. However, current approaches require additional model training\nor struggle to effectively erase particular data points and their associated context\ndue to LLMs' complex, dense, and continuous nature. In this study, we propose\na novel amortized unlearning approach using codebook features and Sparse Au-\ntoencoders (SAEs). By leveraging a bottleneck to decompose the activation space\nand regulate information flow, our method efficiently unlearns targeted informa-\ntion while preserving the model's performance on unrelated data. To the best of\nour knowledge, this is the first work that successfully enables unlearning specific\ntopics with contextual relevance in an LLM, marking a significant step towards\nreal-world applications of machine unlearning.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language Models (LLMs) have been widely used in various applications, generating text re-\nsponses that attempt to create the equivalent of human conversations OpenAI et al. (2024). These\nmodels leverage vast scientific literature to facilitate and accelerate interdisciplinary research Taylor\net al. (2022) while drawing upon large datasets of human-generated content to provide professional\nadvice. However, in many cases, such data is a double-edged sword. Including personal informa-\ntion or sensitive scientific knowledge can be beneficial or, conversely, harmful. For instance, Soice\net al. (2023) discusses how LLMs, when used by non-experts, can enable the creation of biological\nagents, posing both potential benefits and significant risks.\nIn response to these concerns, machine unlearning has emerged as a promising research area focused\non selectively removing specific data points or information from a trained model. This approach\nhelps mitigate the misuse of sensitive data and addresses privacy concerns. Existing solutions, such\nas Sharded, Isolated, Sliced, and Aggregated (SISA) training Bourtoule et al. (2020), primarily in-\nvolve partitioning the training data into disjoint shards and retraining models on these individual\nshards. Although effective in certain scenarios, these methods are often time-consuming, resource-\nintensive, and lack scalability when applied to large models like LLMs. Moreover, traditional ap-\nproaches typically require specialized data structures or full retraining, making them impractical for\ndynamic or complex tasks.\nGiven these limitations, there is an increasing demand for zero-shot unlearning methods, which aim\nto remove specific information without retraining or specialized data structures. Unlike traditional\nunlearning techniques that rely on retraining portions of the model, zero-shot unlearning seeks to\ndirectly eliminate the influence of specific data points or pieces of information from the model's\nlearned representation-without additional computational steps or parameter adjustments. More-"}, {"title": "2 RELATED WORK", "content": "Machine unlearning methodologies have been developed to tackle the challenges of efficiently re-\nmoving data from trained models. Among the early influential frameworks is the Sharded, Isolated,\nSliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into inde-\npendent shards. By retraining only the specific shards containing the data to be unlearned, SISA\nreduces the computational burden. Extensions of this approach include Ginart et al. (2019), which\napplies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests.\nSchelter et al. (2021) further extended the concept to decision trees, minimizing retraining through\nhierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to\nforget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data\nfrom recommendation systems.\nWhile these methods are effective for structured models, they struggle to scale to large, complex\nmodels like Language Models. Additionally, the retraining costs, though reduced, remain signifi-\ncant, and the reliance on specific architectures limits their generalizability to more dynamic tasks.\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as\na teacher and trains a student model to mimic it on retained data while 'forgetting' specific informa-\ntion. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence\nfunctions, providing closed-form updates to model parameters for more efficient data removal.\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an al-\nternative by measuring the effect of individual data points on a model's predictions and adjusting\nparameters accordingly, providing more direct methods for unlearning.\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without\nretraining, making them highly efficient for large models. Shah et al. (2024) introduced a method\nfor editing model computations to 'forget' specific information. While this is effective for tasks\nlike token classification, it may struggle with the more complex context and semantics in LLMs,\nunderscoring the need for scalable, adaptable unlearning techniques tailored to these models."}, {"title": "3 METHODOLOGY", "content": "To address the challenges of zero-shot machine unlearning, we propose a novel approach that lever-\nages codebook features to bottleneck latent representations within a language model, enabling the\ntargeted unlearning of specific knowledge by altering related codebook embeddings. Initially intro-\nduced by Tamkin et al. (2023), codebook features efficiently compress the activation space of neural\nnetworks by introducing a sparse discrete bottleneck. This bottleneck can be further optimized to\nisolate the codes most relevant to specific topics in the input, offering deeper insight and control over\nthe model's response and interpretation. By utilizing this discrete latent representation, we can more\neffectively identify and remove the specific information encoded in the codebook corresponding to\nthe input's targeted knowledge.\nThe following section details our approach to employing codebook features to efficiently identify\nand unlearn specific areas of related information in a zero-shot manner. This process ensures that\nthe model can no longer effectively handle prompts that contain the target information to unlearn."}, {"title": "3.1 CODEBOOK FEATURES", "content": "The core concept behind employing codebook features is to transform the original activations from\na hidden layer into a representation regulated by a codebook. Let $a \\in \\mathbb{R}^F$ represent the activation\nvector from a hidden layer, where F denotes the dimensionality of the activations. We use a code-\nbook $C = \\{C_k\\}_{K=1} \\in \\mathbb{R}^{K \\times F}$, where K represents the number of code vectors. The codebook offers\na compressed, discrete representation of the original activations. To perform this transformation, we\ncalculate the cosine similarity between the activation a and each code vector $c_k$ in the codebook:\n$\\operatorname{cosineSim}(a, c_k) = \\frac{a \\cdot C_k}{\\|a\\| \\|C_k\\|},$ (1)\nfor each code vector $c_k$ in the codebook. We then identify the top S (where $S \\geq 1$) most similar code\nvectors corresponding to the activation a. The index set $\\Omega$ of these top S code vectors is defined as:\n$\\Omega = \\operatorname{Tops}\\left(\\{k \\mid k \\in \\{1, \\ldots, K\\}, \\operatorname{cosineSim}(a, c_k)\\}\\right).$ (2)"}, {"title": "3.2 CODEBOOK SETTINGS", "content": "Multiple Codebooks In prior work Tamkin et al. (2023), multiple codebooks were applied to each\nattention head, with the outputs concatenated across heads. Each attention head operates with its own\ncodebook, selecting codes independently. The chosen codes from each head are then concatenated to\nproduce the final output for that attention layer, effectively allowing the model to represent a broader\nset of features through the combination of different codebooks. Using multiple codebooks across at-\ntention heads can lead to a superposition effect, as described by Elhage et al. (2022). Superposition\nrefers to the phenomenon where linear representations can encode more features than the dimen-\nsions, effectively allowing the neural network to simulate more extensive networks. In this case,\ncombining multiple codebooks across attention heads allows for a significantly more comprehen-\nsive set of activations to be represented, even when using only the top S = 1 codebooks. However,\ntracking which individual codebooks contribute to specific activation patterns becomes challenging.\nRather than relying on the output of a single codebook, the overall representation emerges from the\ncombined outputs of all the codebooks.\nSingle Codebook As shown in Section 3, to maintain interpretability, we focus on using a single\ncodebook, positioning it after the multi-head attention layer and residual connection to prevent in-\nformation leakage. However, in a single codebook setup, selecting only S = 1 leads to a significant\ndrop in model performance, as a single codebook feature is insufficient to capture the complexity of\nthe activation space. In Cai (2024), the author rigorously demonstrates that treating word vectors as\nmappings allows a finite vocabulary to achieve infinite approximation through composition. Based\non this insight, we employ S > 1 in our approach. While this may slightly affect code discretization\nand information clarity, it strikes a balance between model performance and interpretability."}, {"title": "3.3 CODEBOOK WITH SPARSE AUTOENCODERS", "content": "Our goal is to decompose the activation space into sparse, interpretable features rather than recon-\nstructing the original input. To accomplish this, we incorporate the Sparse Autoencoder (SAE) con-\ncept. The SAE applies a linear transformation encoder with a ReLU activation function to project\nthe activations into a higher-dimensional space, effectively decomposing features. A linear transfor-\nmation decoder is employed used to reconstruct the activations.\nIn line with the SAE structure, we introduce a linear transformation encoder with ReLU before\nthe codebook and a linear transformation decoder after the codebook. This setup provides two\nsignificant benefits for machine unlearning:\n\u2022 Security through ReLU: The ReLU activation function ensures that the extracted features\nare non-linear and sparse, making it more difficult to recover or reconstruct the original\ninput from the features. This acts as a safeguard, reducing the likelihood of information\nleakage. By enforcing sparsity and non-linearity, ReLU provides greater control over fea-\nture representation, allowing us to obscure specific activations and protect data integrity\nduring machine-unlearning processes.\n\u2022 Decentralization of Information: Sparsity promotes the decentralization of encoded in-\nformation, which helps isolate and unlearn specific patterns or features without disrupting\nthe rest of the model. This targeted approach allows for more precise unlearning of sensi-\ntive or undesired information."}, {"title": "3.4 TRAINING THE CODEBOOK", "content": "Reconstruction Loss As with the Sparse Autoencoder (SAE) and codebook models, we utilize\nthe Mean Squared Error (MSE) loss as the primary loss function. The MSE loss can be expressed\nas:\n$L_{M S E}=\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|a_{i}-\\hat{a}_{i}\\right\\|^{2},$ (9)\nwhere N is the number of samples, $a_i$ is the original activation, and $\\hat{a}_i$ is the reconstructed activation\nobtained from the decoder.\nAdditionally, to promote sparsity and enforce more distinct and sparse internal feature representa-\ntions within each codebook vector, we introduce an $L_1$ penalty term on the codebook activations.\nThis encourages the model to represent each code with sparser and more well-separated internal\nfeatures. The overall loss function incorporating this sparsity constraint is defined as:\n$L_{\\text {Codebook }}=\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|a_{i}-\\hat{a}_{i}\\right\\|^{2}+\\lambda \\sum_{k \\in \\Omega} \\sum_{f=1}^{F}\\left|c_{k f}\\right|,$ (10)\nwhere $\\Omega$ represents the set of indices for the top S most similar code vectors, $c_k$ refers to the k-\nth codebook vector, F denotes the dimensionality of the code vectors, and $\\lambda$ is a regularization\ncoefficient that controls the strength of the $L_1$ penalty term. In our experiments, we set $\\lambda$ to $1 \\times 10^{-6}$\nto balance sparsity with reconstruction accuracy.\nJoint Training for Machine Unlearning Both the SAE and codebook features are used to re-\nconstruct the input a, but this presents a critical issue in the context of machine unlearning: one\ncould easily remove the codebook layer, reverting the model to its original state, which negates the\nunlearning process. To address this, it is vital to ensure that the model is trained so that the down-\nstream components are entirely dependent on the output of the codebook. At the same time, the\nupstream layers must learn to generate activations that conform to the codebook's representations.\nThis joint training approach ensures that the entire model relies on the codebook's representation,\nmaking it harder to bypass or remove without degrading performance. The joint loss function for\nthis training process is defined as:\n$L_{\\text {joint }}=L_{\\text {Codebook }}+L_{C E},$ (11)\nwhere $L_{\\text {Codebook }}$ refers to the reconstruction loss for the codebook, and $L_{C E}$ represents the Cross-\nEntropy loss for the original language modeling or task-specific objective."}, {"title": "3.5 CODE RETRIEVAL", "content": "As shown in Figure 2, after training, the codebook encodes a set of representative codes $C=\n\\{C_k\\}_{K=1} \\in \\mathbb{R}^{K \\times F}$ that are sparse and represent different features. To perform unlearning, we\nretrieve the codes activated for specific inputs and identify which codes are enriched for a particular\ntopic. The model can effectively unlearn the associated information by deleting the corresponding\nenriched codes from the codebook. The key steps involve retrieving these relevant codes for each\ninput and determining their relationship to the target topic.\nBecause of the nature of the attention mechanism, the activation of these codes also depends on the\nsurrounding context. This means we are not just identifying individual words that activate specific\ncodes but retrieving codes that represent the broader topic within the input context. To unlearn a\nspecific topic T, consider a dataset $D_T$ with samples related to topic T, alongside with the remaining\nirrelevant data set $D_R$. We create a control dataset $D_{\\bar{T}}$ by replacing words associated with T in $D_T$\nwith unrelated words, ensuring the context remains consistent. By comparing the code activations\nbetween $D_T$ and $D_{\\bar{T}}$, we can identify and search for the codes linked to topic T.\nFor each code $c_k$ activated in the dataset, we compute its frequency in both datasets by considering\nthe top $S'$ activated codes:\n$f_k(D_T) = \\frac{1}{N_T} \\sum_{i=1}^{N_T} I(k \\in \\Omega_T(a_i)),$ (12)\n$f_k(D_{\\bar{T}}) = \\frac{1}{N_{\\bar{T}}} \\sum_{j=1}^{N_{\\bar{T}}} I(k \\in \\Omega_{\\bar{T}}(a_j)),$ (13)\nwhere $\\Omega_T(a_i)$ represents the set of indices of the top $S'$ activated codes for activation $a_i$ in dataset\n$D_T$, and $\\Omega_{\\bar{T}}(a_j)$ is similarly defined for $D_{\\bar{T}}$. $N_T$ and $N_{\\bar{T}}$ denote the sample sizes of $D_T$ and $D_{\\bar{T}}$,\nrespectively. I is the indicator function that checks whether code k is in the set of activated codes.\nThe hyperparameter $S'$ controls the number of top activated codes considered, thereby influencing\nthe number of codes to be removed.\nTo quantify the enrichment of code $c_k$ for topic T, we use the following formula:\n$R(c_k, T) = \\log_2\\left(\\frac{f_k(D_T) + \\epsilon}{f_k(D_{\\bar{T}}) + \\epsilon}\\right),$ (14)\nwhere e is a small constant added to avoid division by zero. When R(ck, T) is positive, it indicates\nthat the code $c_k$ is enriched in dataset $D_T$ relative to $D_{\\bar{T}}$. However, if the frequency of ck in $D_"}, {"title": "3.6 METRICS", "content": "In our work, we not solely assess the absolute drop in performance within the topic or non-topic\ndatasets but also need to compare the relative decline between them. Instead, to fairly compare\nthe models and the datasets, we used normalized percentage improvement to evaluate the perfor-\nmance of the unlearning procedure. The performance improvement percentage is set to 0 for the\nzero-shot model and 1 for the codebook model, which is the upper bound. In contrast, the per-\nformance drop percentage is set to 1 for the zero-shot model and 0 for the codebook model. We\nuse four evaluation metrics to assess the effectiveness of the unlearning procedure and the overall\nquality of the remaining information in the output. These metrics include: We use four evaluation\nmetrics to assess the impact of the unlearning procedure on translation quality and semantic preser-\nvation: BLEUPapineni et al. (2002), METEORBanerjee & Lavie (2005), BERTScoreZhang et al.\n(2020), and Bart-ScoreYuan et al. (2021). BLEU offers a general accuracy measure, and METEOR\nbuilds on BLEU by considering synonymy and word order, often providing a more sensitive quality\nassessment. BERTScore leverages contextual embeddings to evaluate semantic similarity, crucial\nfor detecting whether unlearning procedures change the sentence's meaning. Bart-Score evaluates\nfluency and informativeness using pre-trained BART models, with scores reflecting log-likelihood,\nso close to zero indicates better quality. BERTScore and Bart-Score offer insight into more subtle\nchanges, and percentage change trends are prioritized for a comprehensive analysis."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "We applied the codebook features combined with SAE on a large language model(LLM) and trained\nit on tasks that exhibit clear distinctions between correct and incorrect answers. After training, we\nunlearned the model on several specific topics to measure the degradation in performance on the\nunlearned issues while ensuring minimal impact on the other topics. An example of the unlearning\neffect on the topic of 'love' is shown in Table 1. The results illustrate that as more codes related\nto the target topic were deleted, the model's translation became less accurate in representing the\noriginal meaning. For instance:\nThe translation introduces minor inaccuracies in the case of S' = 8 (16 codes deleted). As the num-\nber of deleted codes increases to S' = 72 (133 codes deleted), the translation significantly deviates"}, {"title": "5 CONCLUSION", "content": "In this work, we introduced CodeUnlearn, a novel framework for zero-shot machine unlearning in\nLarge Language Models (LLMs). Leveraging codebook features and Sparse Autoencoders (SAEs),\nwe devised a method that effectively isolates and removes specific knowledge, ensuring that the\ntargeted data and its contextual associations are erased from the model. Unlike previous methods,\nwhich required retraining or were limited to classification tasks, CodeUnlearn operates amortized\nand zero-shot, providing an efficient and scalable solution for unlearning in complex, generative\nmodels like LLMs. Our approach uses a discrete concept representation to regulate the flow of\ninformation in a language model, enabling the unlearning of specific topics while preserving overall\nmodel performance on unrelated tasks. The results show that CodeUnlearn successfully mitigates\nthe model's ability to reproduce the unlearned information without requiring additional training,\nachieving substantial unlearning effectiveness and maintaining interpretability."}]}