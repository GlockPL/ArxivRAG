{"title": "Interpretability as Compression: Reconsidering SAE Explanations of Neural Activations with MDL-SAEs", "authors": ["Kola Ayonrinde", "Michael T. Pearce", "Lee Sharkey"], "abstract": "Sparse Autoencoders (SAEs) have emerged as a useful tool for interpreting the internal representations of neural networks. However, naively optimising SAEs for reconstruction loss and sparsity results in a preference for SAEs that are extremely wide and sparse. We present an information-theoretic framework for interpreting SAEs as lossy compression algorithms for communicating explanations of neural activations. We appeal to the Minimal Description Length (MDL) principle to motivate explanations of activations which are both accurate and concise. We further argue that interpretable SAEs require an additional property, \u201cindependent additivity\": features should be able to be understood separately. We demonstrate an example of applying our MDL-inspired framework by training SAEs on MNIST handwritten digits and find that SAE features representing significant line segments are optimal, as opposed to SAEs with features for memorised digits from the dataset or small digit fragments. We argue that using MDL rather than sparsity may avoid potential pitfalls with naively maximising sparsity such as undesirable feature splitting and that this framework naturally suggests new hierarchical SAE architectures which provide more concise explanations.", "sections": [{"title": "1 Introduction", "content": "Sparse Autoencoders (SAEs) (Le, 2013; Makhzani and Frey, 2013) were developed to learn a dictionary of sparsely activating features describing a dataset. They have recently become popular tools for interpreting the internal activations of large foundation language models, often finding human-understandable features (Sharkey et al., 2022; Huben et al., 2024; Bricken et al., 2023b).\nInterpretability, in particular human-understandability, is difficult to optimise for since ratings-from humans or auto-interpretability methods (Bills et al., 2023)\u2014are not differentiable at training time and often cannot be efficiently obtained. Researchers often use sparsity, the number of nonzero feature activations as measured by the Lo norm, as a proxy for interpretability. SAEs are typically trained with an additional L\u2081 penalty in their loss function to promote sparsity.\nWe adopt an information theoretic view of SAEs, inspired by Gr\u00fcnwald (2007), which views SAEs as explanatory tools that compress neural activations into communicable explanations. This view suggests that sparsity may appear as a special case of a larger objective: minimising the description length of the explanations. This operationalises Occam's razor for selecting explanations: all else equal, prefer the more concise explanation.\nWe introduce this information theoretic view by describing how SAEs can be used in a communication protocol to transmit neural activations. We then argue that interpretability requires explanations to have the property of independent additivity, which allows individual features to be interpreted separately and discuss SAE architectures that are compatible with this property. We find that sparsity"}, {"title": "2 SAEs are communicable explanations", "content": "SAEs aim to provide explanations of neural activations in terms of \"features\"2. Here we reformulate SAEs as solving a communication problem: suppose that we would like to transmit the neural activations x to a friend with some tolerance \u025b, either in terms of the reconstruction error or change in the downstream cross-entropy loss. Using the SAE as an encoding mechanism, we can approximate the representation of the activations in two parts. First, we send them the SAE encodings of the activations z = Enc(x). Second, we send them a decoder network Dec(\u00b7) that recompiles these activations back to (some close approximation of) the neural activations, \u00ee = Dec(z).\nThis is closely analogous to two-part coding schemes (Gr\u00fcnwald, 2007) for transmitting a program via its source code and a compiler program that converts the source code into an executable format. Together the SAE activations and the decoder provide an explanation of the neural activations, based on the definition below."}, {"title": "Definition 2.1", "content": "An explanation e of some phenomena p is a statement e(p) for which knowing e(p) gives some information about p. An explanation is typically a natural language statement\u00b3."}, {"title": "Definition 2.2", "content": "The Description Length (DL) of some explanation e is given as |e|, where |\u00b7 | is the metric denoting the number of bits needed to send the explanation through a communication channel.\nThe description length (DL) of an explanation is the number of bits needed to transmit the explanation. For an SAE, this would be DL = |z|bits + |Dec(\u00b7)|bits. The first term is O(n) and the second term is O(1) in the dataset size so the first term dominates in the large data regime.\nOccam's Razor: All else equal, an explanation e\u2081 is preferred to explanation e2 if DL(e1) < DL(e2). Intuitively, the simpler explanation is the better one. We can operationalise this as the Minimal Description Length (MDL) Principle for model selection: Choose the model with the shortest description length which solves the task. It has been observed that lower description length models often generalise better (MacKay, 2003)."}, {"title": "Definition 2.3", "content": "We define the Minimal Description Length (MDL) as $MDL_{\\varepsilon}(x) = \\min DL(SAE)$ where $Loss(x,\\hat{x}) < \\varepsilon$ and $\\hat{x} = SAE(x)$. We say an SAE is \u025b-MDL-optimal if it obtains this minimum."}, {"title": "3 Interpretability requires independent additivity", "content": "Following Occam's razor we prefer simpler explanations, as measured by description length. But SAEs are not intended to simply give compressed explanations. They are also intended to give explanations that are interpretable and ideally human-understandable.\nSAE features can be interpreted either as causal results of the model inputs (which we can see by analyzing feature activation patterns) or they can be interpreted as causes of the model outputs (which we can see through conducting interventions on the features and seeing the downstream effects). In both cases, we want to be able to understand each SAE feature independently, without needing to control for the activations of the other features. If all the feature activations are causally entangled-as is the case for the dense neural activations themselves\u2014then they are not interpretable. Note that for D features there are O(D2) pairs of features and \u03a3\u039a (P) possible sets of features which is much too large for humans to hold in working memory. So for feature explanations to be human-understandable we cannot have the all the features being entangled such that understanding a single concept requires understanding arbitrary feature interactions.\nHence, for interpretability, we need to be able to understand features independently of each other such that understanding a collection of features together is equivalent to understanding all the features separately. We call this property independent additivity, defined below."}, {"title": "Definition 3.1", "content": "Independent Additivity: An explanation e based on a vector of feature activations $\\bf{z} = \\sum_i z_i$ is independently additive if $e(\\bf{z}) \\approx \\sum_i e(z_i)$. We say that a set of features $z_i$ are independently additive if they can be understood independently of each other and the explanation of the sum of the features is the sum of the explanations of the features.\nWe see that if our SAE features are independently additive, we can also use this property for interventions and counterfactuals too. For example, if we intervene on a single feature (e.g. using it as a steering vector), we can understand the effect of this intervention without needing to understand the other features.\nThe independent additivity condition is directly analogous to the \"composition as addition\" property of the Linear Representation Hypothesis (LRH) discussed in Olah (2024). Independent additivity relates to the SAE features being composable via addition with respect to the explanation - this is a property of the SAE Decoder. In the Linear Representation Hypothesis however, Composition as Addition is"}, {"title": "4 SAEs should be sparse, but not too sparse", "content": "Naively we might see SAEs as decompressing neural activations which contain densely packed features in superposition. To see that SAEs are producing compressed explanations of activations we must note that the inherent feature sparsity means that it is more efficient to communicate SAE latent features rather than neural activations even though the dimension of the latent dimension is higher.\nThe description length for a set of SAE activations (under independent additivity) with distribution p(z) is given by H(p) = \u2211zez-p(z) log2 p(z). For exposition, consider a simpler formulation where we directly consider the bits needed without prior knowledge of the distributions. For a set of feature activations with Lo nonzero elements out of D dictionary features, an upper bound on the description length is\n$DL \\leq L_0(B + log_2 D)$ (1)\nwhere B is the effective precision of each float and log2 D is the number of bits required to specify which features are active. To achieve the same loss, higher sparsity (lower Lo) typically requires a larger dictionary, so there's an inherent trade-off between decreasing LO and decreasing the dictionary size in order to reduce description length.\nAs an illustrative example, we compare reasonable hyperparameters for GPT-2 SAEs to dense/narrow and sparse/wide extreme hyperparameters:\n\u2022 Reasonable SAES: Bloom (2024)'s open-source SAEs for GPT-2 layer 8 have Lo = 65, D = 25,000. Given B = 7 bits per nonzero float (8-bit quantization with the sign fixed to positive), the description length per input token is 1405 bits.\n\u2022 Dense Activations: A dense representation that still satisfies independent additivity would be to send the neural activations directly instead of training an SAE. GPT-2 has a model size of d = 768, the description length is simply DL = B d = 5376 bits per token.\n\u2022 One-hot encodings: At the sparse extreme, our dictionary has a row for each neural activation in the dataset, so Lo = 1 and D = (vocab size)seq len. GPT-2 has a vocab size of 50,257 and the SAEs are trained 128 token sequences. All together this gives DL = 13,993 bits per token.\nAlthough the comparison is slightly unfair because the SAE is lossy (93% variance explained) and the other cases are lossless, these calculations demonstrate that reasonable SAEs are indeed compressed\nIn practice, we typically expect feature trees to be shallow structures which capture causal relationships between highly related features. A particularly interesting example of this structure is a group-sparse autoencoder where linear subspaces are densely activated together."}, {"title": "5 MDL-SAEs find interpretable and composable features for MNIST", "content": "Lee (2001) describe the classical method for using the Minimal Description Length (MDL) criteria for model selection. Here we choose between model hyperparameters (in particular the SAE width and expected Lo) for the optimal SAE. Our algorithm for finding the MDL-SAE solution and details for this case study are given in Appendix C.\nWe trained SAEs on the MNIST dataset of handwritten digits (LeCun et al., 1998) and find the set of hyperparameters resulting in the same test MSE. We see three basic regimes:\n\u2022 High Lo, narrow SAE width (C, D in fig. 3): Here, the description length (DL) is linear with Lo, suggesting that the DL is dominated by the number of bits needed to represent the Lo nonzero floats. The features appear as small sections of digits that could be relevant to many digits (C) or start to look like dense features that one might obtain from PCA (D).\n\u2022 Low Lo, wide SAE width (A in fig. 3): Though Lo is small, the DL is large because as the SAE becomes wider, additional bits are required to specify which activations are nonzero. The features appear closer to being full digits, i.e. similar to samples from the dataset.\n\u2022 The MDL solution (B in fig. 3): There's a balance between the two contributions to the description length. The features appear like longer line segments or strokes for digits, but could apply to multiple digits.\nIn this example, the MDL solution finds a meaningful decomposition of digits into stroke-like features. More dense SAEs find less interpretable point-like features, while sparser SAEs find features that resemble examples from the dataset and fail to decompose the digits into reusable and composable features."}, {"title": "6 Optimising for MDL can reduce undesirable feature splitting", "content": "In large language models, SAEs with larger dictionaries learn finer-grained versions of features learned in smaller SAEs, a phenomenon known as \"feature splitting\" (Bricken et al., 2023b). Feature splitting that introduces a novel conceptual distinction is desirable but some feature splitting-for"}, {"title": "7 Hierarchical features allow for more efficient coding schemes", "content": "Often features are semantically or causally related and this should allow for more efficient coding schemes. For example, consider the hierarchical concepts \"Animal\" (A) and \"Bird\" (B). Since all birds are animals, the \"Animal\" feature will always be active when the \"Bird\" feature is active. A conventional SAE would represent these as separate feature vectors, one for \"Bird\" (B) and one for \"Generic Animal\" (A^\u00acB), that are never active together, as shown in fig. 5. This setup has a low Lo, equal to the probability of \"Animal\", PA, since something is a bird, a generic animal, or neither.\nAn alternative approach would be to define a variable length coding scheme (Salomon, 2007). For example, one might consider first sending the activation for \"Animal\" (A) and only if \"Animal\" is active, sending the activation for \"Animal is a Bird\" (B|A). Now the description length is given as $DL = H(p_A) + p_A H(p_{B|A})$ which is always fewer bits compared to the conventional SAE with $DL = H(p_A - p_B) + H(p_B)$, (see the phase diagram in fig. 5). The overall Lo however is higher because sometimes two activations are nonzero at the same time, so $L_0 = p_A + p_{B|A}$."}, {"title": "8 Related Work", "content": "Bricken et al. (2023a) also consider how information measures relate to SAEs and find that \"bounces\" in entropy correspond to dictionary sizes with the correct number of features in synthetic experiments. We find a similar bounce in description length in a non-synthetic experiment. We go further by studying several examples where minimal description length gives more intuitive features and discuss more description-efficient SAE architectures.\nOur setting is inspired by rate-distortion theory (Shannon, 1948) and the Rate-Distortion-Perception Tradeoff (Blau and Michaeli, 2019), which notes the surprising result that distortion (e.g. squared-error distortion) is often at odds with perceptual quality and suggest that the divergence d(px, px) more accurately represents perception as judged by humans (though the exact divergence which most closely matches human intuition is still an ongoing area of research).\nAs in Ramirez and Sapiro (2012), we use the MDL approach for the Model Selection Problem using the criteria that the best model for the data is the model that captures the most useful structure from the data. The more structure or \"regularity\" a model captures, the shorter the description of the data, X, will be under that model (by avoiding redundancy in the description). Therefore, MDL will select the best model as the one that produces the most efficient description of the data. Chan et al. (2024) use Mechanistic Interpretability techniques to generate compact formal guarantees (i.e. proofs) of model performance and also note a deep connection between interpretability and compression.\nDhillon et al. (2011) use the information theoretic MDL principle to motivate their Multiple Inclusion Criterion (MIC) for learning sparse models. Their setup is similar to ours but their method relies on sequential greedy-sampling rather than a parallel approach, which performs slower than the SAE methods on modern hardware but is otherwise a promising approach. They present applications where human interpretability is a key driver of the reason for a sparse solution and we present additional motivations for sparsity as plausibly aligning with human interpretability.\nSharkey (2024) motivates a high-level framework for Mechanistic Interpretability involving three stages: mathematical description (breaking the network down into functional parts), semantic descrip- tion (labelling the functional parts) and validation (using the semantic description to make predictions about model behaviour and evaluating these predictions). Here we focus on the mathematical descrip-"}, {"title": "9 Conclusion", "content": "In this work, we have presented an information-theoretic perspective on Sparse Autoencoders as explainers for neural network activations. Using the MDL principle, we provide some theoretical motivation for existing SAE architectures and hyperparameters. We also hypothesise a mechanism for, and criteria to describe, the commonly observed phenomena of feature splitting. In the cases where feature splitting can be seen as undesirable for downstream applications, we hope that, using this theoretical framework, the prevalence of undesirable feature splitting could be decreased in practical modelling settings.\nA limitation of this work as presented is that the MDL priniciple is treated as strategy for model selection: to choose a model out of a collection of multiple models. However, training multiple models with a hyperparameter sweep may be computationally expensive. Future work could look to include the entropy term in the loss function and optimise for it directly through either a straight-thought estimation approach or with a Bayesian prior.\nOur work suggests a path to a formal link between existing interpretability methods and information- theoretic principles such as the Rate-Distortion-Perception trade-off and two-part MDL coding schemes. We would be excited about work which further connects concise explanations of learned representations to well-explored problems in compressed sensing.\nHistorically, evaluating SAEs for interpretability has been difficult without human interpretability ratings studies, which can be labour intensive and expensive. We propose that operationalising interpretability via description length can help in creating principled evaluations for interpretability, requiring less subjective and expensive SAE metrics.\nWe would be excited about future work which explores to what extent variants in SAE architectures can decrease the MDL of communicated latent feature activations. In particular, we suggest that exploiting causal structure inherent in the data distribution may be important to efficient coding."}, {"title": "A Details on determining the MDL-SAE", "content": "A.1 Algorithm\n1. Specify a tolerance level, &, for the loss function. The tolerance & is the maximum allowed value for the loss, either the reconstruction loss (MSE for the SAE) or the model's cross- entropy loss when intervening on the model to swap in the SAE reconstructions in place of the clean activations. For small datasets using a reconstruction, the test loss should be used.\n2. Train a set of SAEs within the loss tolerance. It may be possible to simplify this task by allowing the sparsity parameter to also be learned.\n3. Find the effective precision needed for floats. The description length depends on the float quantisation. We typically reduce the float precision until the change in loss results in the reconstruction tolerance level is exceeded.\n4. Calculate description lengths. With the quantised latent activations, the entropy can be computed from the (discretized) probability distribution, {pa}, for each feature i, as\n$H = \\sum_{i,\\alpha} -p^i_\\alpha log_2 p^i_\\alpha$\n5. Select the SAE that minimizes the description length i.e. the &-MDL-optimal SAE.\nA.2 Details for MNIST case study\nFor MNIST, we trained BatchTopK SAEs (Bussmann et al., 2024), typically for 1000+ epochs until the test reconstruction loss converged or stopping early in cases of overfitting. Our desired MSE tolerance was 0.0150. Discretizing the floats to roughly 5 bits per nonzero float gave an average change in MSE of \u2248 0.0001, which was roughly the scale over which MSE varied for the hyperparameters used.\nGao et al. (2024) find that as the SAE width increases, there's a point where the number of dead features starts to rise. In our experiments, we noticed that this point seems to be at a similar point to where the description length starts to increase as well, although we did not test this systematically and this property may be somewhat dataset dependent."}]}