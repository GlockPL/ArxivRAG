{"title": "Cost-Aware Uncertainty Reduction in Schema Matching with GPT-4: The Prompt-Matcher Framework", "authors": ["Longyu Feng", "Huahang Li", "Chen Jason Zhang"], "abstract": "Schema matching is the process of identifying correspondences between the elements of two given schemata, essential for database management systems, data integration, and data warehousing. The inherent uncertainty of current schema matching algorithms leads to the generation of a set of candidate matches. Storing these results necessitates the use of databases and systems capable of handling probabilistic queries. This complicates the querying process and increases the associated storage costs. Commonly employed methods include leveraging crowd-sourcing or employing machine learning ranking models. Motivated by GPT-4's outstanding performance, we explore its potential to reduce uncertainty. Our proposal is to supplant the role of crowd-workers with GPT-4 for querying the set of candidate matches. To get more precise correspondence verification responses from GPT-4, We have crafted Semantic-match and Abbreviation-match prompt for GPT-4, achieving state-of-the-art results on two benchmark datasets: DeepMDatasets 100% (+0.0) and Fabricated-Datasets 91.8% (+2.2) recall rate. To optimise budget utilisation, we have devised a cost-aware solution. Within the constraints of the budget, our solution delivers favourable outcomes with minimal time expenditure.\nWe introduce a novel framework, Prompt-Matcher, to reduce the uncertainty in the process of integration of multiple automatic schema matching algorithms and the selection of complex parameterization. It assists users in diminishing the uncertainty associated with candidate schema match results and in optimally ranking the most promising matches. We formally define the Correspondence Selection Problem (CSP), aiming to optimise the \"revenue\" within the confines of the GPT-4 budget. We demonstrate that CSP is NP-Hard and propose an (1 - 1/e)-approximation algorithm with minimal time expenditure. Ultimately, we demonstrate the efficacy of Prompt-Matcher through rigorous experiments.", "sections": [{"title": "I. INTRODUCTION", "content": ""}, {"title": "A. Background and Motivation", "content": "Schema matching is pivotal for identifying equivalent elements within diverse schemata across heterogeneous data repositories. A plethora of applications, such as database management systems, data migration, warehousing, mining, and knowledge discovery, rely on the deployment of schema matching algorithms. Advancements in deep learning and the advent of large language models have substantially increased the demand for sophisticated data integration and management solutions.\nAs mentioned above, candidate result set is useful and common in schema match. It is helpful for users to get a better result of schema match. However, the utilization of candidate result sets necessitates the integration with databases and systems that can accommodate probabilistic queries [12], [13]. This requirement introduces additional complexity to the querying process and can result in increased storage costs. Our goal is to make informed decisions at the earliest stages possible, aiming to mitigate or eliminate the propagation of uncertainty. [14] highlight that human insights significantly contribute to the reduction of uncertainty in schema matching. A prevalent strategy to diminish uncertainty is through human experts providing a curated list of top-k results. [15] employs crowdsourcing techniques to alleviate uncertainty. While [15] demonstrates commendable performance, crowdsourcing can be time-consuming. In practical applications, the need for automated algorithms remains imperative. In the realm of automation, [16], [17] have explored the use of learn-to-rank methods to reprioritize the candidate result sets. Their work advances the field by potentially eliminating the need for human experts as the ultimate decision-makers in schema matching. However, these learn-to-rank approaches are contingent upon the availability of sufficient labeled data for model training, which can be challenging to obtain in certain practical scenarios.\nWe investigate the potential of GPT-4, equipped with meticulously crafted prompts, to diminish uncertainty in the Candidate Result Set (CRS) and to elevate the optimal outcome to the top ranking position. In [18], it was discovered that GPT-4, when paired with tailored prompts, achieved superior scores over elite crowdsourcing workers, consequently saving a research team an estimated $500,000 and 20,000 hours. [19] illustrate that ChatGPT surpasses crowd-workers in multiple annotation tasks, offering a cost-effective alternative at approximately one-fifth the expense of MTurk, a renowned crowdsourcing platform. [20] present evidence that GPT-4 exhibits more accurate performance than both experts and crowd workers in the textual analysis task of discerning the political leanings of Twitter users. Motivated by the findings of [15] and the remarkable capabilities of GPT-4, we delve into employing GPT-4 for verifying correspondences within the CRS of schema matching. As noted in [15], the method also possesses the capability to rank candidate results concurrently with the reduction of uncertainty, with closer proximity to the optimal result correlating with a higher rank. By replacing crowdsourcing workers with GPT-4, we aim to reduce both cost and time while maintaining effectiveness. In contrast to learn-to-rank methods, GPT-4 operates effectively without the prerequisite of fine-tuning on pre-labeled data. These distinct advantages render GPT-4 a promising candidate for exploration within this domain."}, {"title": "B. Challenges and Contributions", "content": "It is widely acknowledged that GPT-4 exhibits superior performance on simpler tasks compared to those requiring greater complexity. Directly verifying the accuracy of an entire candidate result is often too complex to maintain high precision. A more effective approach is to decompose a single candidate result into its constituent correspondences and verify each individually using GPT-4. This strategy undoubtedly requires the creation of a mechanism to effectively map between individual attribute correspondences and their potential matches. Fortunately, foundational work in this area has been laid, as outlined in [11], [15], following a relatively straightforward procedure: assuming that schema matching options are mutually exclusive, the likelihood of each correspondence can be determined by summing the probabilities of the candidate results that support it. The path forward is evident. However, we face three principal challenges.\n1) Prompt Engineering To optimize GPT-4's performance, it is essential to devise appropriate prompts that guide its responses effectively. While there are a few studies on GPT-4 focused on entity resolution [21], [22], which is a related field, there appears to be no existing research specifically addressing element matching tasks within schema matching. Prompt engineering is instrumental in harnessing the capabilities of Large Language Models (LLMs). Therefore, crafting a novel prompt is imperative for the success of our task.\n2) Noisy Answers Despite our prompts getting state-of-the-art (SOTA) results for GPT-4, there remain instances where the model provides incorrect answers. The verification answers provided by GPT-4 should not be construed as definitive facts or the ground truth. Consequently, it would be imprudent to rely solely on GPT-4's verification answers to directly filter the candidate result set.\n3) Budget Constraint Users, when determining their budget allocations, must consider how to achieve the most significant reduction in uncertainty possible. Prioritizing the verification of correspondences with the highest potential impact on uncertainty reduction can significantly enhance the likelihood of identifying the most accurate match. Given that the prompt-related costs are invariant within a dataset, our strategy specifically targets the selection of correspondences that promise the largest decrease in uncertainty, all within the scope of a predefined budget.\nWe have crafted two specialized prompts for GPT-4 tailored for application on the DeepMDatasets and the fabricated dataset, both of which are renowned public benchmarks for schema matching. The DeepMDatasets encompass a collection of real-world datasets such as amazon_google_exp,"}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Uncertain Schema Match", "content": "Uncertainty is common and significant in data integration systems. [11], [23], [15], [24], [6] have researched on the uncertainty of schema match over the past 20 years. In [6], authors thought that schema matchers are inherently uncertain and it is unrealistic to expect a single matcher to identify the correct mapping for any possible concept in a set. [5] also gets a similar conclusion through its evaluation experiments on schema match datasets. In effort to increase the robustness of individual matchers in the face of matching uncertainty, researchers have turned to schema matcher ensembles[25], [26], [27], [28].\nIn [11], authors assume that a schema match system will be built on a probabilistic data model and introduce the concept of probabilistic schema mappings and analyze their formal foundations. In [15], crowdsourcing platform is used as a basic fact provider, where workers determine whether a correspondence should exist in the schema match result or not. Within a budget, the uncertainty of possible matchings can be reduced to a lowest level. In [17], [16], learn-to-rank methods are introduced to rerank the top-K matchings.\nWe find that crowdsourcing workers answer answer 'CCQs' in [15], whose noisy answers are used to reduce the uncertainty of possible matchings. According to [19], [18], [20], GPT-4 or even ChatGPT outperforms the crowd workers. Additionally, GPT-4 is more automatic, efficient and cheaper. It is valuable to explore how to use GPT-4 instead of workers in crowdsourcing platforms to reduce the uncertainty of candidate result set of schema match."}, {"title": "B. Large Language Models (LLMs)", "content": "The introduction of Transformer models by [29] marked a paradigm shift in the development of language models. This architecture effectively captures long-range dependencies between input and output sequences, addressing the limitations inherent in previous models. The capabilities of Transformer models are exemplified in both generative and embedding models, such as the GPT model family [30], [31], [32] and BERT [33]. Since the advent of ChatGPT, the terms \"Large Language Model\" (LLM) and \"general purpose pretrained language models\" have become virtually synonymous.\nGenerative language models, such as GPT-4 and Claude2, are designed to produce human-like text and can be used to perform various tasks without additional training [30], [32], [34], [35]. Embedding models like BERT and Ada [36] offer contextual embeddings that significantly enhance performance across a wide range of NLP tasks [37], [38]. Additionally, text representation through embeddings facilitates efficient and accurate passage retrieval based on semantic similarity [36], [39]."}, {"title": "C. Prompt Engineering Techniques", "content": "Prompt engineering has become an essential technique for augmenting the capabilities of large language models (LLMs). This method utilizes task-specific instructions, known as prompts, to improve model performance without altering the core model parameters. Instead of modifying these parameters, prompts enable the seamless integration of pre-trained models into downstream tasks by eliciting the desired behaviors based solely on the provided instructions.\nPrompt engineering can be used to guide model response better. The current methods of prompt engineering encompasses a wide range of techniques, from foundational methods such as zero-shot [31] and few-shot [40] prompting to more complex approaches like 'Chain-of-Thought'[41], 'Retrieval Augmented Generation' [42], 'Active-Prompt' [43] and 'Expert Prompting' [44]. More information can be found at [45], [46], [47]."}, {"title": "D. Data Matching and Schema Matching", "content": "Data matching, the process of determining the equivalence of two data elements, plays a pivotal role in data integration. Unicorn[48] align matching semantics of multiple tasks, including entity matching, entity linking, entity alignment, column type annotation, string matching, schema matching, and ontology matching. Unicorn gets multiple SOTA results in the most of 20 datasets. In the two datasets of schema matching, it both achieve the SOTA,DeepMDatasets 100% and Fabricated-datasets 89.6% recall.\nIn [5], schema matchers are classified into Attribute Overlap Matcher, Value Overlap Matcher, Semantic Overlap Matcher, Data Type Matcher, Distribution Matcher and Embeddings Matcher. Different matchers have their own considerations on fuzzy matching. Most of schema match tools use not only one Matcher. The results of experiments also prove that there is not a single matcher which is always better than others. Therefore, ensemble algorithm is useful in schema match area. [49] combines Word Embedding Featurizer, Lexical Featurizer with Bert Featurizer to generate top-K matchings suggestions for each unmapped source attribute. Then, users would select one as the label of candidate pairs, which will be used to train the Bert model.\nThere are some works about inference-only methods of LLMs on data matching task. Agent-OM[50] is a novel agent-powered large language model (LLM)-based design paradigm for ontology matching(OM). It leverages two Siamese agents for retrieval and matching, along with simple prompt-based OM tools, to address the challenges in using LLMs for OM. ReMatch[51] is a novel schema matching method that leverages retrieval-enhanced large language models (LLMs) to address the challenges of textual and semantic heterogeneity, as well as schema size differences, without requiring predefined mappings, model training, or access to source schema data. ReMatch represents schema elements as structured documents and retrieves semantically relevant matches, creating prompts for LLMs to generate ranked lists of potential matches."}, {"title": "III. PROMPT-MATCHER", "content": "In this section, as fig 1 and 2 show, we generally introduce Prompt-Matcher and propose the problem formulation."}, {"title": "A. Problem Formulation", "content": "As we all know, GPT-4 is better at answering simple question than complex question. Prompt-Matcher is designed to make a verification prompt with a single correspondence. We formally define Candidate Result Set, Correspondence and the relevant concepts. Let CRS denote a candidate result set. Table I illustrates an example of a candidate result set, displaying a single schema matching result along with its corresponding probability per row. Let sch\u2081 and sch2 represent two specified schemas in the schema matching task, and let ATT\u2081 and ATT2 denote the sets of attribute names associated with these schemas.\nDefinition 1. Correspondence\nOne correspondence, denoted as c, can be defined as a pair of attribute sets, $(A_{sch1}, A_{sch2})$, where $A_{sch1}$ and $A_{sch2}$ represent two subsets of attributes from schema sch\u2081 and sch2 respectively.\nDefinition 2. Single Candidate Result\nSingle Candidate Result $S_i = {c_1, c_2,...,c_j}$ denotes a set of correspondences that satisfy the constraint that no attribute is associated with more than one correspondence.\nDefinition 3. Candidate Result Set\nCandidate Result Set is denoted by CRS = {$S_1, ..., S_n$}, a set of single candidate results. The probability distribution of CRS is {P($S_1$), ..., P($S_n$)}, generated by its probability assignment function P.\nDefinition 4. Correspondence Set and Probability The set of correspondences, denoted as C = {$c_1, ..., c_{|C|}$}, is defined as a set of all the correspondences in CRS. Let P($S_i$) and P($c_i$) denote the probabilities of $S_i$ and $c_i$, respectively. We have\n$C = {c_i | S_i \\in CRS \\; and \\; c_i \\in S_i}$\n$P(c_i) = \\sum_{S \\in CRS} P(S_i), \\; if \\; c_i \\in S_i $   (1)\nDefinition 5. Correspondence Verification Task Let c represent a single correspondence in CRS. The correspondence verification task is to utilize the GPT-4 to verify the correctness of c and response in the range of {True, False}.\nDefinition 6. View A View V is a truth-valued form of CRS. Single Candidate Result can be looked as a group of truth-valued answers or named a view vi, for all the Correspondence Verification Tasks of Correspondence Set in CRS.\nAs illustrated in Table II, an example of View V is presented. This View is derived from CRS in Table I, in accordance with Definition 6. Transforming CRS into View make it easier to understand the relationship between CRS and Correspondence Verification Task. For n Correspondence Verification Tasks, there are 2n possible truth-value answer groups or views. For vi \u2208 V, the probability of vi being the ground truth is represented as P(gt(V) = vi). We denote the probability P(gt(V) = vi) as P(vi) for the abbreviation. If a correspondence c is 'true' in one v, we say that v is a positive model of c, which is denoted as v |= c. Otherwise, if v is a negative model for c, then v does not satisfy c, denoted as v |\u22a8 c. Moreover, the following assertions hold for each c\u2208 C, and one example is demonstrated in Table I.\n$P(c) = \\sum_{v \\in V \\\\ v|=c} P(v)$\n$P(\\neg c) = 1 - \\sum_{v \\in V \\\\ v|=c} P(v) = \\sum_{v \\in V \\\\ v|\\neg=c} P(v)$   (2)\nNote that, the following equation is not necessarily true for \u03bd\u2208 V,\n$P(c) = \\prod_{v \\in V} P(l_i)$    (3)\nwhere\n$l_i =  \\\\ \\neg c_i  \\\\ otherwise.$   (4)\nEquation 3 above may not hold true, as the views in V are independent. In the example of Table II, it can be observed that the probability P(v1) does not satisfy Equation 3.\nDefinition 7. Uncertainty Metric of Candidate Result Set\nGiven a CRS and its correspondence set C, Let the View of CRS is V, the uncertainty of CRS can be represented by Shannon Entropy of V, denoted by H(V)\n$H(V) = -\\sum_{v \\in V} P(v) log P(v)$,   (5)"}, {"title": "B. Prompt Engineering for schema matching", "content": "In this part, our goal is to get as accurate answers of correspondence verification as possible from GPT-4. In [48], authors transform DeepMDatasets and Fabricated-Datasets into a proper form for data matching task and provide comprehensive evaluations. In our experiments, we choose the DeepMDatasets and Fabricated-Datasets processed by [48].\nDeepMDatasets We design semantic-match prompt for DeepMDatasets, as Figure 3 shows. The recall rate of GPT-4 with this prompt is 100%. There are 7 schemata in DeepMDatasets, which are collected from various industries, including amazon_google_exp, beeradvo_ratebeer, dblp_acm, dblp_scholar, fodors_zagats, itunes_amazon and walmart_amazon. To make the results more reliable, we tested this prompt with GPT-4 not only in test dataset (27), but also train dataset (210) and validation dataset (72). GPT-4 with this prompt get 100% recall rates on all datasets (309).\nSemantic-match prompt is following the 'ICIO' prompt framework, which includes four parts, Instruction, Context, Input data and Output indicator. We try different prompts to test the accuracy on DeepMDatasets. Finally, we find Semantic-match is very suitable for the datasets similar DeepMDatasets. It includes three parts,\n1) task instructions, is \"Determine whether the two attributes match with each other in schema match\". It is to tell GPT-4 the context of task and instruct GPT-4 the details of task.\n2) input data, the attributes of DeepMDatasets own clear semantics. Therefore, we take the names of target schema attributes and sources schema attributes as input.\n3) output indicator, we instruct the GPT-4 to answer follwoing our requirement.\nSemantic-match get quite good result. We think the key to semantic-match prompt is to express the instruction clearly and accurately. Additionally, GPT-4 has strong ability and many prompts we try on DeepMDatasets get over 98% recall rate.\nFabricated-Datasets First, we try semantic-match prompt on fabricated-datasets. However, it gets just 73% recall rate on Fabricated-Datasets. We observe that the abbreviations and the whole attribute names can not be correctly matched with each other. So we take the rules for generating abbreviations as the tips. Moreover, we design a Value Exchange Task to instruct GPT-4 how to verify the correctness of correspondence.\nAs Figure 3 shows, we add four tips in abbreviation-match prompt. \"(1) (2) (3)\" are tips about rules of abbreviation generation. They explicitly pass the rules and knowledge to GPT-4. \"(4)\" defines a new task, Value Exchange Verification, which verifies the correctness of match through exchanging the values and attribute names. In our experiment, We filled the 'values_1' and 'values_2' with the first three value instances of the attribute. Actually, the inspiration of \"(4) Value Exchange Verification\" is from the traditional schema matcher, Value Overlap Matcher [5]. Finally, we use Abbreviation-match prompt for GPT-4 to get a SOTA result, 91.8% recall, in Fabricated-Dataset. We call the Semantic-match Prompt with (1)(2)(3)(4) tips Abbreviation-match Prompt.\nIn [48], authors compare the performances of many different models on DeepMDatasets and Fabricated-Datasets. As Table IV shows, GPT-4 with abbrevation-match prompt has best recall on both two datasets. Abbreviation-match Prompt only make two mistakes in 309 instances of the DeepMDataset. DeepMDataset consists of 7 schema match datasets from various industries, including amazon google exp, beeradvo ratebeer, dblp acm, dblp scholar, fodors zagats, itunes amazon and walmart amazon. Fabricated-Dataset is a more challenging dataset created by [5]. Abbreviation-match prompt gets 99.35% and 100% recall rate on DeepMDatasets and Fabricated-Datasets. This proves that it is a better general prompt.\nFor a specific schema match dataset, the best option is to design a proprietary prompt. However, we think Semantic-match prompt and Abbreviation-match prompt can be a general choice."}, {"title": "C. Reducing Uncertainty With Noisy Answer", "content": "We take the candidate result set in Table I as an example to explain the method to reduce uncertainty with noisy answers. Initially, we select c2 = [Email, EmailAddress] as the designated correspondence set. We make the reasonable assumption that the statement \"I:in EmployeeInfo and Employee c2=[Email, Email Address] is right correspondence\" is a response from ChatGPT with a confidence of 80%. Then, the confidence of the View will be updated using the following equation:\n$Pr(v1|e: I \\;from LLM) = \\frac{Pr(v1)Pr(e|v\u2081)}{Pr(e)} = \\frac{Pr(v1)Pr(LLM \\;is\\;correct)}{Pr(I)Pr(C \\;is\\;correct) + (\\neg Pr(I))Pr(C \\;is\\;incorrect)} = \\frac{0.55 * 0.80}{0.80 * 0.80 +0.20 * 0.20} = 0.647$ (6)\nSimilarly, Pr(v2|e) = 0.294 and Pr(v3|e) = 0.059. The uncertainty H(V) has been reduced from 0.997 to 0.351. In this way, the reduction of uncertainty are achieved and none of candidate results is eliminated, which avoids the result being lost by mistake.\nDefinition 8. Query Set And Answer Set For a correspondence set C = {c1, ..., cn}, a query set is T \u2286 C, which is selected to ask GPT-4. An answer set is AT = {ac\u2081|ci \u2208 T}, where the value for ac, is either \"true\" or \"false\" for c\u2208T.\nLemma 1. Computation of answer set probability Given a correspondence set C, a query set T \u2286 C, the accuracy of GPT-4 is Pr and a view is v, the conditional probability of receiving AT from GPT-4 given a view v is\n$P(A|v) = \\prod_{c\\in T^{+}(v,AT)} Pr . \\prod_{c\\in T^{-}(v,AT)} (1-Pr)$   (7)\n$ = Pr^{|T^{+}(v,AT)|} .(1 - Pr)^{|T^{-}(v,AT)|}$\nwhere $T^{+}(v, AT)$ and $T^{-}(v, AT)$ are the consistent set and inconsistent set of v and AT , respectively,\n$T^{+}(v, AT) = {c|c \\in T \\land ((v |= c, AT |= c) \\lor (v |\\neg= c, AT |\\neg= c))}$\n$T^{-}(v, AT) = {c|c \\in T \\land ((v |= c, AT |\\neg= \\neg c) \\lor (v |\\neg= c, AT |= c))}$\nand the probability of AT is\n$P(AT) = \\sum_{v \\in V} (P(v) . P(AT|v))$\n$ = \\sum_{v \\in V} P(v) .\\prod_{c \\in T^{+}(v,AT)} Pr .\\prod_{c \\in T^{-}(v,AT)} (1-Pr)$\n$= \\sum_{v \\in V} P(v) .Pr^{|T^{+}(v,AT)|} .(1 - Pr)^{|T^{-}(v,AT)|}$   (9)\nThe sets $T^{+}(v, AT)$ and $T^{-}(v, AT)$ mentioned above respectively provide consistent and inconsistent information between the view v and the answer set AT. For every correspondence c\u2208 C, if its truth value in v is consistent with the answer in AT, then it will be added to the set $T^{+}(v, AT)$. Otherwise, if the consistency is not maintained, such as when v |= ci but aci = False, the correspondence ci will be included in the set $T^{-}(v, AT)$. Note that for correspondences that are not included in the query set T, the answer set AT will not provide any information regarding them. Hence, they are not in either $T^{+}(v, AT)$ or $T^{-}(v, AT)$. Given a view v and an answer set AT, we have,\n$T^{+}(v, AT) \\cap T^{-}(v, AT) = \\emptyset$\n$T^{+}(v, AT) \\cup T^{-}(v, AT) = T$    (10)\nThe above Lemma 1 presents a method for the computation of the probability of an answer set AT. The confidence of answers from GPT-4 are utilized to update the probability distribution of the View. Note that if a correspondence c \u2209 T, there will be no answer for it. For an answer set AT and a correspondence c\u2208 C, we denote AT |= c if ac in AT is 'Yes', and AT |\u22a8 c if ac in AT is 'No'. Different from a View, an answer set is not a complete assignment over C. Therefore, AT |\u22a8 c does not implies AT |= \u00acc.\nUpdating Probability Distribution Supposing that GPT-4 has confidence Pr of the correspondence verification task in V. For one query set T, suppose the answer set is AT, the probability of view v \u2208 V will be updated to\n$P(v|AT) = \\frac{P(v) . P(AT|v)}{P(AT)}$\n$P(v)\\cdot \\prod_{c \\in T^{+}(v,AT)} Pr .\\prod_{c \\in T^{-}(v,AT)} (1 - Pr)$\n$=\\frac{}{\\sum_{v' \\in V} P(v') .\\prod_{c \\in T^{+}(v',AT)} Pr .\\prod_{c \\in T^{-}(v',AT)} (1 - Pr)}$    (11)"}, {"title": "D. Correspondence Selection Problem", "content": "With the Abbreviation-match Prompt and Semantic-match Prompt above, we try to establish a loop to update the probability distribution of the Candidate Result Set with nosiy answers from GPT-4. In order to find the optimal correspondence set to verify, we take uncertainty reduction expectation to estimate the possible \"revenue\u201d of one correspondence set.\nPossible Answer Families For one given query set T, we denote the set of all the possible answer families from GPT-4 by AST.\nDefinition 9. Entropy of the answer Families For a given correspondence set C, a given query set T, the entropy of the answer families H(AST) is\n$H(AST) = -\\sum_{AT \\in EAST} P(AT) log P(AT)$   (12)\nDefinition 10. Uncertainty Reduction Expectation For a correspondence set C and its View V, a query set T, the expected uncertainty of V after getting answers from GPT-4 for T is\n$\\Delta H(V) = H(V) - H(V|AST)$\n$= H(V) + \\sum_{AT \\in AST}P(AT)H(V|AT)$  (13)"}, {"title": "E. Greedy Selection Algorithm", "content": "The challenge of the correspondence selection problem stems from its NP-hard nature, which necessitates an exponential complexity to obtain the optimal solution. The time complexity of brute solution becomes impractical when dealing with large-scale data. Based on the monotone submodularity of the Objective Function, we use Algorithm 4 of [53] as base to establish an greedy algorithm with partial enumeration. This kind of approximate algorithm was first proposed by [54].\nPartial Enumeration algorithm visits all the query set T, whose size is |T| \u2264 2 and find the set T\u2081 maximizing the objective function H(V|AST).\nGreedy Strategy algorithm visits all the query set |T| = 3, and make them as start points to perform greedy strategy until the budget is used up. Greedy Strategy is to select a most cost-effective correspondence at each time, which is like the line 8 in table ??."}, {"title": "F. The Whole Loop", "content": "Prompt-Matcher is a framework that helps people to reduce the uncertainty of candidate result set and rank the best result to the front too. Prompt-Matcher is established on the loop of correspondence selection, the verification of GPT-4 with prompt and the probability distribution updating of View, as figure 2 shows. At each iteration, prompt-matcher get the more reliable verified answers from GPT-4 to reassign the probabilities to various schema match results.\nInitialization B is initialized by total_budget/k, which is used as the budget of each round. k is the number of rounds in the loop, which is a parameter decided by users. To test the performance of prompt-matcher, we choose one budget larger than the cost of all correspondences and larger than three times of the mean cost. In fact, the budget can be set at any value. Due to greedy algorithm with partial enumeration will brutely search when the number of query correspondence set is less than 3, we recommend to avoid setting a large budget at each round. The large budget at one round will result in a quite high time cost.\nNote that greedy algorithm with partial enumeration can provide a (1-1/e) approximate solution at each round. Then, the solution of K rounds will also be 1 \u2013 1/e. In fact, if there are some remaining budget at each round, it will passed to next round. So the solution of our greedy selection will have higher ratio than (1 \u2013 1/e)."}, {"title": "IV. EXPERIMENT RESULTS", "content": "We conducted 16 datasets, which are fabricated from 4 datasets(musician, assays, prospect, miller2) in [5]. As Figure 4 shows, we collect candidate result set from 7 schema matching algorithms with multiple groups of parameters. We can see the threshold of each schema match algorithm in Table VIII. We do contrast experiments between random algorithm and greedy selection. Our codes and the details of experiment can be found at [55]. we take MRR(Mean Reciprocal Rank) as the evaluation metric. MRR can represent the location of optimal result. The equation of computation is MRR =  where rank is the location of the optimal result. We take the F1-score of the correspondences in one candidate result as its rank metric.\nThe effect of Prompt-Matcher In figure 5, 6, 7, 8, we can find that Prompt-Matcher with random selection or greedy selection can rank the best schema match result to the first place in the 12/16 (75%) experiments. Further, 14/16 (87.5%) experiments can rank the best result to the second place. The final ranking place is highly related to the accuracy of GPT-4. Although we achieve the SOTA result, 91.8% accuracy and recall in the fabricated datasets, it can not ensure Prompt-Matcher can rank the best schema match result to the first place. As figure 9 shows, the experiment of miller2_both_50_30_ac5_ev dataset has a bad result. We check the accuracy of GPT-4 on this dataset. The result is 21/37 (56.7%), which is far less than 91.8%. GPT-4 with Abbreviation-match prompt has poor performance on this dataset. So it is quite necessary to evaluate the accuracy of GPT-4 on the dataset. If the evaluation result of GPT-4 on your dataset is quite good, top-3 or 5 can cover the best schema match result. Another bad case is the miller2_horizontal_50_ac4_av. We find that the accuracy of GPT-4 on this dataset is quite high, 54/55 (98.1%). However, the f1 scores of the candidate results in CRS are really low. The highest one is 0.169, which is a bad performance. When the results in CRS are all far from the ground truth, prompt-matcher can not get a good result.\nGreedy selection is more effective than random selection within the same budget As depicted in Figures 5, 6, 7, and 8, the MRR metric for random selection exhibits a slower rate of increase compared to our greedy selection algorithm as the budget expands. The results show that our greedy selection can work better than random selection when the budget is not enough. With the exception of the first three line charts in Figure 7, the performance of the greedy selection approach is at least as good as that of random selection. Moreover, the optimal outcome of the greedy selection experiment is closer to the top-performing result at equivalent budget costs. Prompt-Matcher, integrated with a greedy selection strategy for the Correspondence Selection Problem (CSP), proves to be effective. It expedites the ranking of the optimal schema match result by reducing uncertainty within the Candidate Result Set (CRS). As illustrated in Figure 10, greedy selection facilitates a more rapid reduction in uncertainty compared to random selection across the majority of the 16 experiments. This phenomenon is consistent with the phenomenon in figure 5, 7, 8, 6. Prompt-Matcher with greedy selection can make a faster uncertainty reduction of CRS, and rank the best schema match result to the front faster. The brute algorithm is capable of obtaining an optimal solution to the Correspondence Selection Problem (CSP). However, when compared to the brute force algorithm, the advantages of greedy selection merit consideration, particularly in terms of efficiency.\nGreedy selection need less time cost than brute algorithm We conducted a comparative analysis of the time complexity between our \"greedy selection with partial enumeration\" algorithm and the brute force approach. As depicted in Table IX, the brute force algorithm is significantly more time-consuming than our proposed method. Moreover, the time complexity of the brute force algorithm exhibits a rapid increase as the budget expands, as illustrated in the table. This is attributable to its exponential time complexity. Consequently, as the scale increases, the time cost of the brute force algorithm escalates, surpassing that of greedy selection with partial enumeration.\nIn most cases, prompt-matcher is quite effective to rank the best schema match result to the front of candidate result set. When the budget can not cover all correspondences, Greedy selection can result in a better uncertainty reduction and ranking effect than random selection. When time cost is not concerned and budget is insufficient, brute algorithm is suitable."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose the Prompt-Matcher to apply GPT-4 to reduce uncertainty of candidate result set and rank the best result to the front. Technically, we explore the method to design prompt of GPT-4 on two"}]}