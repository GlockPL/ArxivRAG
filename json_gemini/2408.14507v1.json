{"title": "Cost-Aware Uncertainty Reduction in Schema Matching with GPT-4: The Prompt-Matcher Framework", "authors": ["Longyu Feng", "Huahang Li", "Chen Jason Zhang"], "abstract": "Schema matching is the process of identifying correspondences between the elements of two given schemata, essential for database management systems, data integration, and data warehousing. The inherent uncertainty of current schema matching algorithms leads to the generation of a set of candidate matches. Storing these results necessitates the use of databases and systems capable of handling probabilistic queries. This complicates the querying process and increases the associated storage costs. Commonly employed methods include leveraging crowd-sourcing or employing machine learning ranking models. Motivated by GPT-4's outstanding performance, we explore its potential to reduce uncertainty. Our proposal is to supplant the role of crowd-workers with GPT-4 for querying the set of candidate matches. To get more precise correspondence verification responses from GPT-4, We have crafted Semantic-match and Abbreviation-match prompt for GPT-4, achieving state-of-the-art results on two benchmark datasets: DeepMDatasets 100% (+0.0) and Fabricated-Datasets 91.8% (+2.2) recall rate. To optimise budget utilisation, we have devised a cost-aware solution. Within the constraints of the budget, our solution delivers favourable outcomes with minimal time expenditure.\nWe introduce a novel framework, Prompt-Matcher, to reduce the uncertainty in the process of integration of multiple automatic schema matching algorithms and the selection of complex parameterization. It assists users in diminishing the uncertainty associated with candidate schema match results and in optimally ranking the most promising matches. We formally define the Correspondence Selection Problem (CSP), aiming to optimise the \"revenue\" within the confines of the GPT-4 budget. We demonstrate that CSP is NP-Hard and propose an (1 - 1/e)-approximation algorithm with minimal time expenditure. Ultimately, we demonstrate the efficacy of Prompt-Matcher through rigorous experiments.", "sections": [{"title": "I. INTRODUCTION", "content": "Schema matching is pivotal for identifying equivalent el-ements within diverse schemata across heterogeneous data repositories. A plethora of applications, such as database management systems, data migration, warehousing, mining, and knowledge discovery, rely on the deployment of schema matching algorithms. Advancements in deep learning and the advent of large language models have substantially increased the demand for sophisticated data integration and management solutions.\nA multitude of semi-automated and automated schema matching algorithms have been developed and extensively researched over the years [1], [2], [3], [4]. Different Schema match tools pay attention to different areas, including lin-guistic, structural, and instance-based cues. Although these tools get good performance in some datasets, different schema match tools usually have their own advantages and disad-vantages. There is not a single schema matching method that consistently performs better than others[5]. Most schema match algorithms require complex parameterization in order to perform well. In real-world scenarios, it is anticipated that the performance of most algorithms will be lower, given that their parameters are often not finely tuned[5]. Furthermore, some schema matching tools generate a set of candidate results to users [6], [7], [8], [9]. consequently, these schema match tools can prevent the loss of potential results. Some researchers proposed algorithms that not only generate a candidate result set, but also establish a search space in uncertain scenarios, as well as the allocation of probabilities in the context of probabilistic schema matching [10], [11] ,as illustrated in the top section of Table I.\nAs mentioned above, candidate result set is useful and common in schema match. It is helpful for users to get a better result of schema match. However, the utilization of candidate result sets necessitates the integration with databases and systems that can accommodate probabilistic queries [12], [13]. This requirement introduces additional complexity to the querying process and can result in increased storage costs. Our goal is to make informed decisions at the earliest stages possible, aiming to mitigate or eliminate the propagation of uncertainty. [14] highlight that human insights significantly contribute to the reduction of uncertainty in schema matching. A prevalent strategy to diminish uncertainty is through human experts providing a curated list of top-k results. [15] employs crowdsourcing techniques to alleviate uncertainty. While [15] demonstrates commendable performance, crowdsourcing can be time-consuming. In practical applications, the need for automated algorithms remains imperative. In the realm of automation, [16], [17] have explored the use of learn-to-rank methods to reprioritize the candidate result sets. Their work advances the field by potentially eliminating the need for human experts as the ultimate decision-makers in schema matching. However, these learn-to-rank approaches are contin-gent upon the availability of sufficient labeled data for model training, which can be challenging to obtain in certain practical scenarios.\nWe investigate the potential of GPT-4, equipped with metic-ulously crafted prompts, to diminish uncertainty in the Can-didate Result Set (CRS) and to elevate the optimal outcome to the top ranking position. In [18], it was discovered that GPT-4, when paired with tailored prompts, achieved superior scores over elite crowdsourcing workers, consequently saving a research team an estimated $500,000 and 20,000 hours. [19] illustrate that ChatGPT surpasses crowd-workers in mul-tiple annotation tasks, offering a cost-effective alternative at approximately one-fifth the expense of MTurk, a renowned crowdsourcing platform. [20] present evidence that GPT-4 exhibits more accurate performance than both experts and crowd workers in the textual analysis task of discerning the political leanings of Twitter users. Motivated by the findings of [15] and the remarkable capabilities of GPT-4, we delve into employing GPT-4 for verifying correspondences within the CRS of schema matching. As noted in [15], the method also possesses the capability to rank candidate results concurrently with the reduction of uncertainty, with closer proximity to the optimal result correlating with a higher rank. By replacing crowdsourcing workers with GPT-4, we aim to reduce both cost and time while maintaining effectiveness. In contrast to learn-to-rank methods, GPT-4 operates effectively without the prerequisite of fine-tuning on pre-labeled data. These distinct advantages render GPT-4 a promising candidate for exploration within this domain."}, {"title": "B. Challenges and Contributions", "content": "It is widely acknowledged that GPT-4 exhibits superior performance on simpler tasks compared to those requiring greater complexity. Directly verifying the accuracy of an entire candidate result is often too complex to maintain high precision. A more effective approach is to decompose a single candidate result into its constituent correspondences and verify each individually using GPT-4. This strategy undoubtedly requires the creation of a mechanism to effectively map be-tween individual attribute correspondences and their potential matches. Fortunately, foundational work in this area has been laid, as outlined in [11], [15], following a relatively straightfor-ward procedure: assuming that schema matching options are mutually exclusive, the likelihood of each correspondence can be determined by summing the probabilities of the candidate results that support it. The path forward is evident. However, we face three principal challenges.\n1) Prompt Engineering To optimize GPT-1's performance, it is essential to devise appropriate prompts that guide its responses effectively. While there are a few studies on GPT-4 focused on entity resolution [21], [22], which is a related field, there appears to be no existing research specifically addressing element matching tasks within schema matching. Prompt engineering is instrumental in harnessing the capabilities of Large Language Models (LLMs). Therefore, crafting a novel prompt is imperative for the success of our task.\n2) Noisy Answers Despite our prompts getting state-of-the-art (SOTA) results for GPT-4, there remain instances where the model provides incorrect answers. The ver-ification answers provided by GPT-4 should not be construed as definitive facts or the ground truth. Conse-quently, it would be imprudent to rely solely on GPT-4's verification answers to directly filter the candidate result set.\n3) Budget Constraint Users, when determining their bud-get allocations, must consider how to achieve the most significant reduction in uncertainty possible. Prioritizing the verification of correspondences with the highest potential impact on uncertainty reduction can signifi-cantly enhance the likelihood of identifying the most accurate match. Given that the prompt-related costs are invariant within a dataset, our strategy specifically targets the selection of correspondences that promise the largest decrease in uncertainty, all within the scope of a predefined budget.\nWe have crafted two specialized prompts for GPT-4 tailored for application on the DeepMDatasets and the fabricated dataset, both of which are renowned public benchmarks for schema matching. The DeepMDatasets encompass a col-lection of real-world datasets such as amazon_google_exp,"}, {"title": "II. RELATED WORK", "content": "Uncertainty is common and significant in data integration systems. [11], [23], [15], [24], [6] have researched on the uncertainty of schema match over the past 20 years. In [6], authors thought that schema matchers are inherently uncertain and it is unrealistic to expect a single matcher to identify the correct mapping for any possible concept in a set. [5] also gets a similar conclusion through its evaluation experiments on schema match datasets. In effort to increase the robustness of individual matchers in the face of matching uncertainty, researchers have turned to schema matcher ensembles[25], [26], [27], [28].\nIn [11], authors assume that a schema match system will be built on a probabilistic data model and introduce the concept of probabilistic schema mappings and analyze their formal foundations. In [15], crowdsourcing platform is used as a basic fact provider, where workers determine whether a correspondence should exist in the schema match result or not. Within a budget, the uncertainty of possible matchings can be reduced to a lowest level. In [17], [16], learn-to-rank methods are introduced to rerank the top-K matchings.\nWe find that crowdsourcing workers answer answer 'CCQs' in [15], whose noisy answers are used to reduce the uncertainty of possible matchings. According to [19], [18], [20], GPT-4 or even ChatGPT outperforms the crowd workers. Additionally, GPT-4 is more automatic, efficient and cheaper. It is valuable to explore how to use GPT-4 instead of workers in crowdsourc-ing platforms to reduce the uncertainty of candidate result set of schema match."}, {"title": "B. Large Language Models (LLMs)", "content": "The introduction of Transformer models by [29] marked a paradigm shift in the development of language models. This architecture effectively captures long-range dependencies between input and output sequences, addressing the limitations inherent in previous models. The capabilities of Transformer models are exemplified in both generative and embedding models, such as the GPT model family [30], [31], [32] and BERT [33]. Since the advent of ChatGPT, the terms \"Large Language Model\" (LLM) and \"general purpose pretrained language models\" have become virtually synonymous.\nGenerative language models, such as GPT-4 and Claude2, are designed to produce human-like text and can be used to perform various tasks without additional training [30], [32], [34], [35]. Embedding models like BERT and Ada [36] offer contextual embeddings that significantly enhance performance across a wide range of NLP tasks [37], [38]. Additionally, text representation through embeddings facilitates efficient and accurate passage retrieval based on semantic similarity [36], [39]."}, {"title": "C. Prompt Engineering Techniques", "content": "Prompt engineering has become an essential technique for augmenting the capabilities of large language models (LLMs). This method utilizes task-specific instructions, known as prompts, to improve model performance without altering the core model parameters. Instead of modifying these parameters, prompts enable the seamless integration of pre-trained models into downstream tasks by eliciting the desired behaviors based solely on the provided instructions.\nPrompt engineering can be used to guide model response better. The current methods of prompt engineering encom-passes a wide range of techniques, from foundational methods such as zero-shot [31] and few-shot [40] prompting to more complex approaches like 'Chain-of-Thought'[41], 'Retrieval Augmented Generation' [42], 'Active-Prompt' [43] and 'Expert Prompting' [44]. More information can be found at [45], [46], [47]."}, {"title": "D. Data Matching and Schema Matching", "content": "Data matching, the process of determining the equivalence of two data elements, plays a pivotal role in data integra-tion. Unicorn[48] align matching semantics of multiple tasks, including entity matching, entity linking, entity alignment, column type annotation, string matching, schema matching, and ontology matching. Unicorn gets multiple SOTA results in the most of 20 datasets. In the two datasets of schema matching, it both achieve the SOTA,DeepMDatasets 100% and Fabricated-datasets 89.6% recall.\nIn [5], schema matchers are classified into Attribute Overlap Matcher, Value Overlap Matcher, Semantic Overlap Matcher, Data Type Matcher, Distribution Matcher and Embeddings Matcher. Different matchers have their own considerations on fuzzy matching. Most of schema match tools use not only one Matcher. The results of experiments also prove that there is not a single matcher which is always better than others. Therefore, ensemble algorithm is useful in schema match area. [49] combines Word Embedding Featurizer, Lexical Featurizer with Bert Featurizer to generate top-K matchings suggestions for each unmapped source attribute. Then, users would select one as the label of candidate pairs, which will be used to train the Bert model.\nThere are some works about inference-only methods of LLMs on data matching task. Agent-OM[50] is a novel agent-powered large language model (LLM)-based design paradigm for ontology matching(OM). It leverages two Siamese agents for retrieval and matching, along with simple prompt-based OM tools, to address the challenges in using LLMs for OM. ReMatch[51] is a novel schema matching method that lever-ages retrieval-enhanced large language models (LLMs) to ad-dress the challenges of textual and semantic heterogeneity, as well as schema size differences, without requiring predefined mappings, model training, or access to source schema data. ReMatch represents schema elements as structured documents and retrieves semantically relevant matches, creating prompts for LLMs to generate ranked lists of potential matches."}, {"title": "III. PROMPT-MATCHER", "content": "As we all know, GPT-4 is better at answering simple question than complex question. Prompt-Matcher is designed to make a verification prompt with a single correspondence. We formally define Candidate Result Set, Correspondence and the relevant concepts. Let CRS denote a candidate result set. Table I illustrates an example of a candidate result set, displaying a single schema matching result along with its cor-responding probability per row. Let $sch_1$ and $sch_2$ represent two specified schemas in the schema matching task, and let $ATT_1$ and $ATT_2$ denote the sets of attribute names associated with these schemas."}, {"title": "Definition 1. Correspondence", "content": "One correspondence, denoted as c, can be defined as a pair of attribute sets, ($A_{sch_1}$, $A_{sch_2}$), where $A_{sch_1}$ and $A_{sch_2}$ represent two subsets of attributes from schema $sch_1$ and $sch_2$ respectively."}, {"title": "Definition 2. Single Candidate Result", "content": "Single Candidate Result $S_i$ = {$c_1$,$c_2$,...,$c_j$} denotes a set of correspondences that satisfy the constraint that no attribute is associated with more than one correspondence."}, {"title": "Definition 3. Candidate Result Set", "content": "Candidate Result Set is denoted by CRS = {$S_1$, ..., $S_n$}, a set of single candidate results. The probability distribution of CRS is {$P(S_1)$, ..., $P(S_n)$}, generated by its probability assignment function P."}, {"title": "Definition 4. Correspondence Set and Probability", "content": "The set of correspondences, denoted as C = {$c_1$, ..., $c_{|C|}$}, is defined as a set of all the correspondences in CRS. Let $P(S_i)$ and $P(c_i)$ denote the probabilities of $S_i$ and $c_i$, respectively. We have\n$C = \\{c_i | S_i \\in CRS \\text{ and } c_i \\in S_i \\}\n$P(c_i) = \\sum_{S_i \\in CRS}P(S_i)$, if $c_i \\in S_i$\n(1)"}, {"title": "Definition 5. Correspondence Verification Task", "content": "Let c represent a single correspondence in CRS. The correspondence verification task is to utilize the GPT-4 to verify the correct-ness of c and response in the range of {True, False}."}, {"title": "Definition 6. View", "content": "A View V is a truth-valued form of CRS. Single Candidate Result can be looked as a group of truth-valued answers or named a view $v_i$, for all the Correspondence Verification Tasks of Correspondence Set in CRS.\nAs illustrated in Table II, an example of View V is presented. This View is derived from CRS in Table I, in accordance with Definition 6. Transforming CRS into View make it easier to understand the relationship between CRS and Correspondence Verification Task. For n Correspondence Verification Tasks, there are $2^n$ possible truth-value answer groups or views. For $v_i \\in V$, the probability of $v_i$ being the ground truth is represented as P(gt(V) = $v_i$). We denote the probability P(gt(V) = $v_i$) as P($v_i$) for the abbreviation. If a correspondence c is 'true' in one v, we say that v is a positive model of c, which is denoted as v = c. Otherwise, if v is a negative model for c, then v does not satisfy c, denoted as v != c. Moreover, the following assertions hold for each c$\\in$ C, and one example is demonstrated in Table I.\n$P(c) = \\sum_{v \\in V \\\\ v \\models c}P(v)$\n$P(\\neg c) = 1 - \\sum_{v \\in V \\\\ v \\models c}P(v) = \\sum_{v \\in V \\\\ v \\not\\models c}P(v)$\n(2)\nNote that, the following equation is not necessarily true for v$\\in$V,\n$P(c) = \\prod_{v \\in V} P(l_i)$\n(3)\nwhere\n$l_i = \\begin{cases}\nc_i & \\text{if } v \\models c_i, \\\\\n\\neg c_i & \\text{otherwise}.\n\\end{cases}$\n(4)\nEquation 3 above may not hold true, as the views in V are independent. In the example of Table II, it can be observed that the probability P(v1) does not satisfy Equation 3."}, {"title": "Definition 7. Uncertainty Metric of Candidate Result Set", "content": "Given a CRS and its correspondence set C, Let the View of CRS is V, the uncertainty of CRS can be represented by Shannon Entropy of V, denoted by H(V)\nH(V) = -$\\sum_{v \\in V} P(v) \\log P(v)$,\n(5)"}, {"title": "B. Prompt Engineering for schema matching", "content": "In this part, our goal is to get as accurate answers of correspondence verification as possible from GPT-4. In [48], authors transform DeepMDatasets and Fabricated-Datasets into a proper form for data matching task and provide com-prehensive evaluations. In our experiments, we choose the DeepMDatasets and Fabricated-Datasets processed by [48].\nDeepMDatasets We design semantic-match prompt for DeepMDatasets, as Figure 3 shows. The recall rate of GPT-4 with this prompt is 100%. There are 7 schemata in DeepMDatasets, which are collected from various in-dustries, including amazon_google_exp, beeradvo_ratebeer, dblp_acm, dblp_scholar, fodors_zagats, itunes_amazon and walmart_amazon. To make the results more reliable, we tested this prompt with GPT-4 not only in test dataset (27), but also train dataset (210) and validation dataset (72). GPT-4 with this prompt get 100% recall rates on all datasets (309).\nSemantic-match prompt is following the 'ICIO' prompt framework, which includes four parts, Instruction, Context, In-put data and Output indicator. We try different prompts to test the accuracy on DeepMDatasets. Finally, we find Semantic-match is very suitable for the datasets similar DeepMDatasets. It includes three parts,\n1) task instructions, is \"Determine whether the two at-tributes match with each other in schema match\". It is to tell GPT-4 the context of task and instruct GPT-4 the details of task.\n2) input data, the attributes of DeepMDatasets own clear semantics. Therefore, we take the names of target schema attributes and sources schema attributes as input.\n3) output indicator, we instruct the GPT-4 to answer follwoing our requirement.\nSemantic-match get quite good result. We think the key to semantic-match prompt is to express the instruction clearly and accurately. Additionally, GPT-4 has strong ability and many prompts we try on DeepMDatasets get over 98% recall rate.\nFabricated-Datasets First, we try semantic-match prompt on fabricated-datasets. However, it gets just 73% recall rate on Fabricated-Datasets. We observe that the abbreviations and the whole attribute names can not be correctly matched with each other. So we take the rules for generating abbreviations as the tips. Moreover, we design a Value Exchange Task to instruct GPT-4 how to verify the correctness of correspondence.\nAs Figure 3 shows, we add four tips in abbreviation-match prompt. \"(1) (2) (3)\" are tips about rules of abbreviation generation. They explicitly pass the rules and knowledge to GPT-4. \"(4)\" defines a new task, Value Exchange Verification, which verifies the correctness of match through exchanging the values and attribute names. In our experiment, We filled the 'values_1' and 'values_2' with the first three value instances of the attribute. Actually, the inspiration of \"(4) Value Exchange Verification\" is from the traditional schema matcher, Value Overlap Matcher [5]. Finally, we use Abbreviation-match prompt for GPT-4 to get a SOTA result, 91.8% recall, in Fabricated-Dataset. We call the Semantic-match Prompt with (1)(2)(3)(4) tips Abbreviation-match Prompt.\nIn [48], authors compare the performances of many dif-ferent models on DeepMDatasets and Fabricated-Datasets. As Table IV shows, GPT-4 with abbrevation-match prompt has best recall on both two datasets. Abbreviation-match Prompt only make two mistakes in 309 instances of the DeepMDataset. DeepMDataset consists of 7 schema match datasets from various industries, including amazon google exp, beeradvo ratebeer, dblp acm, dblp scholar, fodors zagats, itunes amazon and walmart amazon. Fabricated-Dataset is a more challenging dataset created by [5]. Abbreviation-match"}, {"title": "C. Reducing Uncertainty With Noisy Answer", "content": "We take the candidate result set in Table I as an example to explain the method to reduce uncertainty with noisy an-swers. Initially, we select c2 = [Email, EmailAddress] as the designated correspondence set. We make the reasonable assumption that the statement \"I:in EmployeeInfo and Em-ployee c2=[Email, Email Address] is right correspondence\" is a response from ChatGPT with a confidence of 80%. Then, the confidence of the View will be updated using the following equation:\nPr(v1|e: I from LLM) = Pr(v1)Pr(e|v1)/Pr(e)\n= Pr(v1)Pr(LLM is correct)\nPr(I)Pr(C is correct) + (\u00acPr(I))Pr(C is incorrect)\n= 0.55 * 0.80/ 0.80 * 0.80 +0.20 * 0.20\n= 0.647\n(6)\nSimilarly, Pr(v2|e) = 0.294 and Pr(v3|e) = 0.059. The uncertainty H(V) has been reduced from 0.997 to 0.351. In this way, the reduction of uncertainty are achieved and none of candidate results is eliminated, which avoids the result being lost by mistake."}, {"title": "Definition 8. Query Set And Answer Set", "content": "For a correspon-dence set C = {$c_1$, ..., $c_n$}, a query set is T$\\subseteq$ C, which is selected to ask GPT-4. An answer set is AT = {a_{ci}$|c_i \\in$ T}, where the value for a_{ci}, is either \"true\" or \"false\" for c$\\in$T."}, {"title": "Lemma 1. Computation of answer set probability", "content": "Given a correspondence set C, a query set T$\\subseteq$C, the accuracy of GPT-4 is $P_r$ and a view is v, the conditional probability of receiving AT from GPT-4 given a view v is\nP(AT|v) = $\\prod_{c \\in T^+(v,AT)} P_r \\cdot \\prod_{c \\in T^-(v,AT)} (1 - P_r)$\n= $P_r^{|T^+(v,AT)|}\\cdot (1 - P_r)^{|T^-(v,AT)|}$\n(7)\nwhere $T^+(v, AT)$ and $T^-(v, AT)$ are the consistent set and inconsistent set of v and $AT$, respectively,\nT+(v, AT) = {c|c$\\in$ T $\\land$ ((v = c, AT |= c) \\\\ v (v = \u00abc, AT = \u00abc))}\nT-(v, AT) = {c|c$\\in$ T $\\land$ ((v |= c, AT |\u251c= \u00abc c) \\\\ v (v = \u00abc, AT = c))}\nand the probability of AT is\nP(AT) = $\\sum_{v \\in V} (P(v) \\cdot P(AT|v))$\n= $\\sum_{v \\in V} P(v) \\cdot \\prod_{c \\in T^+(v,AT)} P_r \\cdot \\prod_{c \\in T^-(v,AT)} (1 - P_r)$\n= $\\sum_{v \\in V} P(v) \\cdot P_r^{|T^+(v,AT)|}\\cdot (1 - P_r)^{|T^-(v,AT)|}$\n(8)\n(9)\nThe sets T+(v, AT) and T\u2212(v, AT) mentioned above respectively provide consistent and inconsistent information between the view v and the answer set AT. For every correspondence c$\\in$ C, if its truth value in v is consistent with the answer in AT, then it will be added to the set T+(v, AT). Otherwise, if the consistency is not maintained, such as when v Ci but aci = False, the correspondence ci will be included in the set T\u2212(v, AT). Note that for correspondences that are not included in the query set T, the answer set AT will not provide any information regarding them. Hence, they are not in either T+(v, AT) or T\u2212(v, AT). Given a view v and an answer set AT, we have,\nT\u207a (v, AT) \u2229 T\u207b (v, AT) = \u00d8\nT\u207a (v, AT) \u222a T\u207b (v, AT) = T\n(10)\nThe above Lemma 1 presents a method for the computation of the probability of an answer set AT. The confidence of answers from GPT-4 are utilized to update the probability distribution of the View. Note that if a correspondence c$\\notin$ T, there will be no answer for it. For an answer set AT and a correspondence c$\\in$ C, we denote AT = c if ac in AT is 'Yes', and AT = \u00abc if a\u025b in AT is 'No'. Different from a View, an answer set is not a complete assignment over C. Therefore, AT \u2260 c does not implies AT = \u00abc.\nUpdating Probability Distribution Supposing that GPT-4 has confidence $P_r$ of the correspondence verification task in V. For one query set T, suppose the answer set is AT, the probability of view v$\\in$ V will be updated to\nP(v|AT) = P(v) \u00b7 P(AT|v) / P(AT)\n=\nP(v)\u00b7 $\\prod_{c \\in T^+(v,AT)} P_r \\cdot \\prod_{c \\in T^-(v,AT)} (1 - P_r)$\n$\\sum_{v' \\in V}$ P(v')\u00b7 $\\prod_{c \\in T^+(v',AT)} P_r \\cdot \\prod_{c \\in T^-(v',AT)} (1 - P_r)$\n(11)"}, {"title": "D. Correspondence Selection Problem", "content": "With the Abbreviation-match Prompt and Semantic-match Prompt above, we try to establish a loop to update the probabil-ity distribution of the Candidate Result Set with nosiy answers from GPT-4. In order to find the optimal correspondence set to verify, we take uncertainty reduction expectation to estimate the possible \"revenue\u201d of one correspondence set.\nPossible Answer Families For one given query set T, we denote the set of all the possible answer families from GPT-4 by AST."}, {"title": "Definition 9. Entropy of the answer Families", "content": "For a given correspondence set C, a given query set T, the entropy of the answer families H(AST) is\nH(AST) = -$\\sum_{AT \\in AST} P(AT) \\log P(AT)$\n(12)"}, {"title": "Definition 10. Uncertainty Reduction Expectation", "content": "For a correspondence set C and its View V, a query set T, the expected uncertainty of V after getting answers from GPT-4 for T is\n\u0394H(V) = H(V) \u2013 H(V|AST)\n= H(V) + $\\sum_{AT \\in AST} P(AT)H(V|AT)$\n(13)\nwhere the H(V|AT) is the conditional uncertainty expec-tation with the answer set $AT$."}, {"title": "Lemma 2. Objective Function.", "content": "For one View V, the query set is T. The answer families is AST. The Objective Function is\n-H(V|AST) = $\\sum_{AT \\in AST} \\sum_{v \\in V} P(AT)P(v|AT) \\log P(v|AT)$\n(14)\nProof. For a query set T, the expected uncertainty reduction of the View is \u0394\u0397(V|AST). We have\n\u2206H(V) = H(V) \u2013 H(V|AST)\n(15)\nSince H(V) is constant at each round, the Objective Func-tion can be simplified to -H(V|AST).\nThen,\n-H(V|AST) = $\\sum_{AT \\in AST} P(AT)H(V|AT)$\n= $\\sum_{AT \\in AST} \\sum_{v \\in V} P(AT)P(v|AT) \\log P(v|AT)$\n(16)\nProof ends."}, {"title": "Definition 11. Cost Metric.", "content": "For the GPT-4, the number of input and output tokens will decide the price. Due to prompt remains unchanged, we set the added tokens number of one correspondence verification as its cost."}, {"title": "Definition 12. Correspondence Selection Problem.", "content": "Given a View V, a budget B, and a correspondence set C with its cost function W(), our objective is to maximize -H(V|AST) by selecting a query set T* while ensuring that the cost of T* does not exceed B. We assume GPT-4 has an accuracy rate of Pr to finish the correspondence verification task.\nT* = $max_{T \\subseteq V,W(T) \\leq B} -H(V|AST)$\n(17)"}, {"title": "NP-Hard", "content": "Based on the definition of the correspondence selection problem, we prove that the problem is NP-Hard.\nTo establish the NP-hardness of the Correspondence Se-lection problem, it is sufficient to prove the NP-completeness of its decision version. We designate the decision version as DCS (Decision Correspondence Selection). In the context of DCS, there exists a view set denoted as V, along with a cor-respondence set denoted as C. These sets are associated with the correspondence cost function w() and the correspondence value function value(). Given a budget B and a value H, the objective of DCS is to determine the existence of a subset T from a set C that satisfies the conditions value(T) > H and w(T) \u2264 B.\nTo establish the NP-completeness of the DCS problem, it is sufficient to prove the NP-completeness of a specific instance of DCS. Initially, the accuracy rate is set to 1.0. For a subset T of C, we let w(T) = value(T), and B = H = $\\sum_{c \\in C}w(c)$. If we set X = {w(c), \u2200c \u2208 C}, we can deduce that the answer to the set partitioning problem of X is \u201cyes\u201d if and only if the answer to the DCS is \"yes\". Since the set partitioning problem has been proven to be NP-complete by [52], it is reasonable to infer that the DCS problem is also NP-complete. Given that the decision version is less challenging, it can be inferred that the correspondence selection problem is an NP-hard problem."}, {"title": "E. Greedy Selection Algorithm", "content": "The challenge of the correspondence selection problem stems from its NP-hard nature, which necessitates an exponen-tial complexity to obtain the optimal solution. The time com-plexity of brute solution becomes impractical when dealing with large-scale data. Based on the monotone submodularity of the Objective Function, we use Algorithm 4 of [53] as base to establish an greedy algorithm with partial enumeration. This kind of approximate algorithm was first proposed by [54].\nPartial Enumeration algorithm visits all the query set T, whose size is |T| \u2264 2 and find the set T\u2081 maximizing the objective function H(V|AST).\nGreedy Strategy algorithm visits all the query set |T| = 3, and make them as start points to perform greedy strategy until the budget is used up. Greedy Strategy is to select a most cost-effective correspondence at each time, which is like the line 8 in table ??."}, {"title": "F. The Whole Loop", "content": "Prompt-Matcher is a framework that helps people to reduce the uncertainty of candidate result set and rank the best result to the front too. Prompt-Matcher is established on the loop of correspondence selection, the verification of GPT-4 with prompt and the probability distribution updating of View, as figure 2 shows. At each iteration, prompt-matcher get the more reliable verified answers from GPT-4 to reassign the probabilities to various schema match results.\nInitialization B is initialized by total_budget/k, which is used as the budget of each round. k is the number of rounds in the loop, which is a parameter decided by users."}]}