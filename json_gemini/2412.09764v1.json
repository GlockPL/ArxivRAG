{"title": "Memory Layers at Scale", "authors": ["Vincent-Pierre Berges", "Barlas O\u011fuz", "Daniel Haziza", "Wen-tau Yih", "Luke Zettlemoyer", "Gargi Gosh"], "abstract": "Memory layers use a trainable key-value lookup mechanism to add extra parameters to a model without increasing FLOPs. Conceptually, sparsely activated memory layers complement compute-heavy dense feed-forward layers, providing dedicated capacity to store and retrieve information cheaply. This work takes memory layers beyond proof-of-concept, proving their utility at contemporary scale. On downstream tasks, language models augmented with our improved memory layer outperform dense models with more than twice the computation budget, as well as mixture-of-expert models when matched for both compute and parameters. We find gains are especially pronounced for factual tasks. We provide a fully parallelizable memory layer implementation, demonstrating scaling laws with up to 128B memory parameters, pretrained to 1 trillion tokens, comparing to base models with up to 8B parameters.", "sections": [{"title": "1 Introduction", "content": "Pretrained language models encode vast amounts of information in their parameters (Roberts et al., 2020), and they can recall and use this information more accurately with increasing scale (Brown et al., 2020). For dense deep neural networks, which encode information primarily as weights of linear matrix transforms, this scaling of parameter size is directly coupled to an increase in computational and energy requirements. It is unclear if this is the most efficient solution to all information storage needs of language models. An important subset of information that language models need to learn are simple associations.\nLLMs learn birthdays of celebrities, capital cities of countries, or how one concept might relate to another. While feed-forward networks can in principle (given sufficient scale) learn any function (Hornik et al., 1989), including lookup tables of associations, using an associative memory for this purpose would be both more efficient and more natural.\nSuch memory layers can be implemented with a simple and cheap key-value lookup mechanism where both keys and values are encoded as embeddings (Weston et al., 2015). Earlier works introduced end-to-end trainable memory layers (Sukhbaatar et al., 2015) and incorporated them as part of neural computational systems (Graves et al., 2014). Despite early enthusiasm however, memory layers have not been studied and scaled sufficiently to be useful in modern AI architectures. There are distinctive challenges one encounters when attempting to scale memory layers, which we touch upon in section 3. In contrast to dense layers which are predominantly FLOP-bound, memory layers with their sparse activation pattern are almost entirely memory bandwidth bound. Such components are rarely used in modern architectures and have not been optimised for hardware accelerators. In addition to, and partly as a result of this, little research was done to improve their performance. Instead, the field focused on alternatives such as mixture-of-experts (Shazeer et al., 2017), which more closely resemble dense networks and are thus easier to scale.\nIn this work, we show that memory layers, when improved and scaled sufficiently, can be used to augment dense neural networks to great benefit. We do so by replacing the feed-forward network (FFN) of one or more transformer layers with memory layers (we leave other layers unchanged). These benefits are consistent across a range of base model sizes (ranging from 134 million to 8 billion parameters), and memory capacities (up to 128 billion parameters). This represents a two orders of magnitude leap in memory capacity compared to previous memory layers reported in the literature. Our results (section 4) indicate that memory layers improve the factual accuracy of language models by over 100% as measured by factual QA benchmarks, while also improving significantly on coding (HumanEval, MBPP) and general knowledge (Hellaswag, MMLU). In many cases, memory augmented models can match the performance of dense models that have been trained on 4x more compute. They also outperform mixture-of-experts architectures with matching compute and parameter size, especially on factual tasks. Given these findings, we strongly advocate that memory layers should be integrated into all next generation AI architectures."}, {"title": "2 Related work", "content": "Language model scaling laws (Kaplan et al., 2020) study the empirical performance of language models as they are scaled in compute, data, and parameter size. Scaling laws are typically formulated in terms of training/test log likelihood, which is generally believed to correlate well with downstream performance. Scaling plots on downstream tasks are also not without precedent (Brown et al., 2020), but have sometimes been shown to exhibit non-linear behaviour and phase transitions (Wei et al., 2022; Ganguli et al., 2022). Nevertheless, given a well behaved metric (such as task likelihood loss), most tasks exhibit smooth improvements with scaling (Schaeffer et al., 2023).\nKaplan et al. (2020) showed that performance scales log-linearly with compute and parameter size across a wide range of architecture hyper-parameters, such as model depth and width. It has been difficult to find architectures which substantially deviate from these laws. Mixture-of-experts (MOE) (Shazeer et al., 2017; Lepikhin et al., 2020) is a notable exception. MOE adds extra parameters to the model without increasing the computation budget. While scaling laws for MOE also mostly focus on training perplexity, gains transfer well to downstream applications, as evidenced by the popularity of MOE architectures in recent state-of-the-art model families (Jiang et al., 2024; OpenAI et al., 2024; Team et al., 2024). Nevertheless, scaling laws for specific task families and capabilities like factuality remain understudied.\nLike MOE, memory augmented models also aim to augment the parameter space of the model without adding significant computational cost. Memory networks were proposed initially in (Weston et al., 2015), and with end-to-end training in (Sukhbaatar et al., 2015). Neural Turing Machines (Graves et al., 2014, 2016) combine external trainable memory with other components to build a neural trainable computer. Product-key networks (Lample et al., 2019) were introduced to make the memory lookup more efficient and scalable. The recent PEER (He, 2024) builds on this work, replacing vector values with rank-one matrices, forming a bridge between memory architectures and MOE."}, {"title": "3 Memory Augmented Architectures", "content": "Trainable memory layers work similarly to the ubiquitous attention mechanism (Bahdanau et al., 2016). Given a query $q \\in \\mathbb{R}^{n}$, a set of keys $K\\in \\mathbb{R}^{N\\times n}$ and values $V\\in \\mathbb{R}^{N\\times n}$, the output is a soft combination of values, weighted according to the similarity between $q$ and the corresponding keys. Two major differences separate memory layers from attention layers as they are typically used (Vaswani et al., 2023). First, the keys and values in memory layers are trainable parameters, as opposed to activations. Second, memory layers typically have larger scale in terms of the number of keys and values, making sparse lookup and updates a necessity. For example in this work, we scale the number of key-value pairs to several millions. In this case, only the top-k most similar keys and corresponding values take part in the output. A simple memory layer can be described by the following equations:\n$I = SelectTopkIndices(Kq), \\qquad s = Softmax(K_I q), \\qquad y = s V_I$ (1)\nHere I is a set of indices, $s \\in \\mathbb{R}$, $K_I, V_I \\in \\mathbb{R}^{k\\times n}$, and the output $y \\in \\mathbb{R}^{n}$. Each token embedding (for us, the output of the previous attention layer) goes through this memory lookup independently, similar to the FFN operation that we replace."}, {"title": "3.1 Scaling memory layers", "content": "Being light on compute, and heavy on memory, memory layers have distinct scaling challenges. We detail some of these challenges and how we address them in this section."}, {"title": "3.1.1 Product-key lookup", "content": "One bottleneck which arises when scaling memory layers is the query-key retrieval mechanism. A naive nearest-neighbour search requires comparing each query-key pair, which quickly becomes prohibitive for large memories. While fast approximate vector similarity techniques (Johnson et al., 2019) could be used here, it's a challenge to incorporate them when the keys are being continually trained and need to be re-indexed. Instead, we adopt trainable product-quantized keys from (Lample et al., 2019). Product keys work by having two sets of keys instead of one, where $K_1, K_2 \\in \\mathbb{R}^{\\sqrt{N}\\times \\frac{n}{2}}$. The full set of keys of size $N \\times n$, which is never instantiated, consists of the product of these two sets. The top-k lookup on the full set of keys can be efficiently done by searching the much smaller set of half-keys first, saving compute and memory. To perform the lookup, we first split the query as $q_1,q_2 \\in \\mathbb{R}^{\\frac{n}{2}}$. Let $I_1, I_2$ and $s_1, s_2$ be the top-k indices and scores obtained from the respective key sets $K_1, K_2$. Since there are only $\\sqrt{N}$ keys in each set, this operation is efficient. The overall indices and scores can be found by taking $argmax_{i_1\\in I_1, i_2 \\in I_2} s_1[i_1] + s_2[i_2]$."}, {"title": "3.1.2 Parallel memory", "content": "Memory layers are naturally memory-intensive, mostly due to the large number of trainable parameters and associated optimizer states. To implement them at the scale of several millions of keys, we parallelize the embedding lookup and aggregation across multiple GPUs. The memory values are sharded across the embedding dimension. At each step, the indices are gathered from the process group, each worker does a lookup and then aggregates the portion of embeddings in its own shard. After this, each worker gathers the partial embeddings corresponding to its own portion of the indices. We take care to keep activation memory manageable at this stage, by making sure each GPU only gets its own portion, and does not need to instantiate the entire embedding output. The process is illustrated in figure 2. The implementation is independent of other model parallelism schemes such as tensor, context or pipeline parallelism, and operates on its own process group."}, {"title": "3.1.3 Shared memory", "content": "Deep networks encode information at different levels of abstraction across different layers. Adding memory to multiple layers may help the model use its memory in more versatile ways. In contrast to previous work (Lample et al., 2019), we use a shared pool of memory parameters across all memory layers, thus keeping parameter count the same and maximizing parameter sharing. We find that multiple memory layers increase performance significantly over having a single layer with the same total parameter count, up to a certain number of layers (in our case, 3). Beyond this point, replacing further FFN layers degrades performance, showing sparse and dense layers are both needed and likely complementary (see section 5.4)."}, {"title": "3.1.4 Performance and stability improvements", "content": "The main operations in the memory layer is to compute the weighted sum of the top-k embeddings: it is implemented in PyTorch's EmbeddingBag operation. As the number of floating-point operations is negligible, we expect this operation to be solely limited by the GPU memory bandwidth, but find multiple inefficiencies in PyTorch's implementation in practice. We implemented new and more efficient CUDA kernels for this operation. Our forward pass optimizes memory accesses and achieves 3TB/s of memory bandwidth, which is close to our H100 specification of 3.35TB/s (compared to less than 400GB/s with PyTorch's implementation). The backward pass is more complicated as multiple output gradients have to be propagated to the same weight gradient. We benchmarked multiple strategies: accumulation via atomic additions (\"atomics\"), row-level atomic lock where we amortize the cost of memory lock over the embedding dimension (\"lock\"), and atomic-free (\"reverse_indices\"). The latter approach requires some preprocessing to inverse the token_id to embedding_id mapping, so that each row in the embedding gradient can know which token will contribute to it. Typically, while the \"atomics\" approach is already up to 5x faster than the existing PyTorch operator, we found that the \"reverse_indices\" and \"lock\" approaches can be faster when the embedding dimension exceeds 128, as long as the embeddings are roughly balanced. Overall, our custom kernels make the embedding bag operation end-to-end 6x faster compared to PyTorch's EmbeddingBag for our use cases.\nWe improve training performance of the memory layer by introducing input-dependent gating with a silu non-linearity (Hendrycks and Gimpel, 2023). The output in equation (1) then becomes\n$output = (y \\odot silu(x^T W_1))^T W_2$ (2)\nwhere $silu(x) = x \\cdot sigmoid(x)$ and $\\odot$ is the element-wise multiplication(see also figure 3). We find that for large memory layers, training can become unstable, especially for small base models. We use qk-normalization (Team, 2024) when needed to alleviate this issue."}, {"title": "4 Experimental setup", "content": "For our base model architecture, we follow closely the Llama series of dense transformers (Touvron et al., 2023; Dubey et al., 2024), which also serve as our dense baselines. We augment the base models by replacing one or more of the feed-forward layers with a shared memory layer. For scaling law experiments, we pick base model sizes of 134m, 373m, 720m, and 1.3b parameters. For these models, we use the Llama2 tokenizer with 32k tokens, and train to 1T tokens with a pretraining data mix that is similar to that of Llama2 (Touvron et al., 2023). For experiments at the 8B base model scale, we use the Llama3 (Dubey et al., 2024) configuration and tokenizer (128k tokens), and a better optimized data mix similar to Llama3."}, {"title": "4.1 Baselines", "content": "In addition to the dense baselines, we also compare to other parameter augmentations including mixture-of-experts (MOE) (Shazeer et al., 2017) and the more recent PEER (He, 2024) model. In MOE, each FFN layer is composed of multiple \"experts\", only a subset of which participate in the computation for each input. The PEER model is conceptually similar to a memory layer, but instead of retrieving a single value embedding, it retrieves a pair of embeddings, which combine into a rank-1 matrix. Several of these are assembled together into a dynamic feed-forward layer. PEER works similarly to memory layers in practice, but requires twice the number of parameters for the same number of keys. Like memory layers, these methods increase the number of parameters in the model without significantly increasing FLOPs. We pick the number of experts in MOE and the number of keys in PEER to match the number of parameters of our memory-augmented models as closely as possible. MOE models are trained with expert choice (Zhou et al., 2022), and evaluated with top-1 routing. PEER layers share the same configuration and hyper-parameters as our memory layer implementation."}, {"title": "4.2 Evaluation benchmarks", "content": "Our evaluations cover factual question answering (NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018)), scientific and common sense world knowledge (MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2019)) and coding (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)). We try to report the most commonly used accuracy metrics (exact match or F1 score for QA benchmarks, pass-at-1 for coding). For some bencmarks, the performance of small models can be very low, and accuracy numbers noisy. Therefore we use negative log-likelihood (nll) of the correct answer for model ablations."}, {"title": "5 Scaling results", "content": "We compare Memory models to baselines in a compute-controlled setting."}, {"title": "5.1 With fixed memory size", "content": "First, we fix the size of the memory, and therefore the number of extra parameters, and compare with the dense baseline, as well as roughly parameter matched MOE and PEER models. Models with the same base model configuration have negligible differences in FLOPs. For Memory models, we fix the number of half keys to $2^{10}$, and thus the number of memory values to $2^{20}$ (roughly 1 million). For the PEER baseline, we pick the number of half-keys to be 768, resulting in slightly more total parameters than Memory. For MOE models, we pick the lowest number of experts such that the parameter count exceeds that of Memory. This corresponds to 16, 8, 6, and 4 experts for the 134m, 373m, 720m and 1.3b sizes respectively.\nThe vanilla Memory model has a single memory layer, which we pick to replace the middle FFN layer of the transformer. Our improved Memory+ model has 3 memory layers, placed centered with a stride of 4 for the 134m models and 8 for the others. Additionally it includes a custom swilu non-linearity, and optimized key dimension (set to equal half of the value dim). As noted earlier, memory layers share parameters, thus have identical memory footprint to a single memory layer."}, {"title": "5.2 Scaling memory size with a fixed base model", "content": "Next, we investigate scaling behaviour with respect to the memory size for a fixed base model. In figure 1, we see that factual QA performance for a Memory+ model keeps increasing predictably with increasing memory size. At 64 million keys (128 billion memory parameters), a 1.3b Memory model approaches the performance of the Llama2 7B model, that has been trained on 2x more tokens using 10x more FLOPs. (see also table 2)."}, {"title": "5.3 Results at 8B scale", "content": "Finally, we scale our Memory+ model with an 8B base model and 40962 memory values (64B memory parameters). We use the Llama3 8B (Dubey et al., 2024) architecture and tokenizer, and train on a data mix similar to Llama3 (Dubey et al., 2024). We report results at 200 billion and 1 trillion tokens of training in table 2. On an expanded set of benchmarks, including general scientific and world knowledge and coding, we see that memory augmented models significantly outperform dense baselines. The gains are more pronounced earlier in training (200B tokens), suggesting that memory helps models learn facts faster. At only 1 trillion tokens of training, our Memory+ model approaches the performance of Llama3.1 8B, which was trained on 15 trillion tokens."}, {"title": "5.4 Model ablations", "content": "In this section, we present results which motivate our modelling choices for the Memory+ architecture.\nMemory layer placement Since the memory pool is shared, we can replace more FFN layers with memory layers without increasing either the memory or the compute budget. We see that as we add more memory layers, performance initially increases. However, as we're effectively removing dense parameters from the model for each added memory layer, eventually the model performance degrades, revealing a sweet spot at around 3 memory layers (table 3, left). Moreover, we experiment with the placement of these layers, modifying the centring and spacing. We find that centred placements with larger strides are better, and we adopt this for our Memory+ architecture."}, {"title": "6 Implications and shortcomings of the work", "content": "Scaling of dense transformer models has dominated progress in the AI field in the last 6 years. As this scaling is nearing its physical and resource limits, it's useful to consider alternatives which might be equally scalable without being as compute and energy intensive. Memory layers with their sparse activations nicely complement dense networks, providing increased capacity for knowledge acquisition while being light on compute. They can be efficiently scaled, and provide practitioners with an attractive new direction to trade-off memory with compute.\nWhile the memory layer implementation presented here is orders of magnitude more scalable than previous works, there still remains a substantial engineering task to make them efficient enough for large scale production uses. Dense architectures have been optimized for and co-evolved with modern GPU architectures for decades. While we believe it's in principle possible to make memory layers as fast, or even faster than regular FFN layers on current hardware, we acknowledge that this needs non-trivial effort.\nWe have so far presented only high level empirical evidence that memory layers improve factuality of models. However, we believe the sparse updates made possible by memory layers might have deep implications to how models learn and store information. In particular, we hope that new learning methods can be developed to push the effectiveness of these layers even further, enabling less forgetting, fewer hallucinations, and continual learning."}]}