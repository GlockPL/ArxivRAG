{"title": "Large Language Model Enhanced Knowledge Representation Learning: A Survey", "authors": ["Xin Wang", "Zirui Chen", "Haofen Wang", "Leong Hou U", "Zhao Li", "Wenbin Guo"], "abstract": "The integration of Large Language Models (LLMs) with Knowledge Representation Learning (KRL) signifies a pivotal advancement in the field of artificial intelligence, enhancing the ability to capture and utilize complex knowledge structures. This synergy leverages the advanced linguistic and contextual understanding capabilities of LLMs to improve the accuracy, adaptability, and efficacy of KRL, thereby expanding its applications and potential. Despite the increasing volume of research focused on embedding LLMs within the domain of knowledge representation, a thorough review that examines the fundamental components and processes of these enhanced models is conspicuously absent. Our survey addresses this by categorizing these models based on three distinct Transformer architectures, and by analyzing experimental data from various KRL downstream tasks to evaluate the strengths and weaknesses of each approach. Finally, we identify and explore potential future research directions in this emerging yet underexplored domain, proposing pathways for continued progress.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (e.g., BERT [18], LLaMA [59]) which represents a direction of ever-increasing model sizes pre-trained on larger corpora, have demonstrated powerful capabilities in solving natural language processing (NLP) tasks, including question answering [99], text generation [100] and document understanding [101]. There are no clear and static thresholds regarding the model sizes. Early LLMs (e.g., BERT, ROBERTa) adopt an encoder architecture and show capabilities in text representation learning and natural language understanding. In recent years, more focus has been given to larger encoder-decoder [102] or decoder-only [103] architectures. As the model size scales up, such LLMs have also shown reasoning ability and even more advanced emergent ability [104], exposing a strong potential for Artificial General Intelligence (AGI).\nThis inflection point, with the arrival of LLMs, marks a paradigm shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. As a popular approach for explicit knowledge representation, KGs are now widely investigated for the combination with Transformer-based LLMs, including pre-trained masked language models (PLMs) like BERT and RoBERTa, and more recent generative LLMs like the GPT series and LLaMA. Some works use LLMs to augment knowledge graph representation learning. In this survey, considering three directions, encoder-based methods, encoder-decoder-based method, and decoder-based methods. We present a better understanding of the shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge.\nCao et al. [22] and Biswas et al. [40] discuss recent advancements in knowledge graph representation learning, yet they inadequately address aspects related to the integration with large models. Pan et al. [42] and Pan et al. [43] explore the combination of knowledge graphs and large models, specifically addressing LLM4KG and KG4LLM; however, they provide limited coverage of representation learning. Consequently, there is currently no comprehensive review article dedicated to outlining the latest developments in the field of knowledge graph representation learning.\nThe notable contributions of this survey are summarized as follows:\n\u2022 Categorization of KRL. We systematically summarize the knowledge representation learning where large language models can be adopted into: encoder-based, encoder-decoder-based, and decoder-based methods.\n\u2022 Systematic Review of Techniques. We provide the most comprehensive overview of large language models on knowledge graph representation learning techniques. For different methods, we summarize the representative models, provide detailed illustrations, and make necessary comparisons."}, {"title": "2 Preliminaries", "content": "This section provides formal definitions and relevant notational conventions (as shown in Tab. 2) used in this survey."}, {"title": "2.1 Knowledge Graph", "content": "A KG G is a labelled directed graph, which can be viewed as a set of knowledge triples T \u2286 E \u00d7 R \u00d7 (E \u222a L), where E is the set of nodes, corresponding to entities (or resources), R is the set of relation types (or properties) of the entities, and L is the set of literals. An entity represents a real-world object or an abstract concept. Often the labels of entities and relations are chosen to be URIs or IRIS (Internationalised Resource Identifiers).\nGiven a KG G, we call (en, r, et) \u2208 T a triple, where en \u2208 E is the subject, r \u2208 R is the relation, and et \u2208 E \u222a L is the object. The subject is also called the head entity, and an object et \u2208 E may be referred to as the tail entity. Triples with literals as objects, i.e., et \u2208 L are known as attributive triples. In this survey, we use the notation (en, r, et), with angle brackets, to indicate a triple.\nDepending on the nature of the objects in a triple, one may distinguish two main kinds of relations:\nObject Relation (or Property), in which an entity is linked to another entity. For instance, in the triple (dbr:Daniel_Craig, dbo:birthPlace, dbr:Cheshire), dbr:Daniel_Craig and dbr:Cheshire are head and tail entities, respectively, and dbo:birthPlace is an Object Relation (or Property).\nData Type Relation (or Property), in which the entity is linked to a literal. For instance, we find the date \"1868-03-02\" in the triple (dbr:Daniel_Craig, dbo:birthDate, \"1868-03-02\"), and therefore the relation dbo:birthDate is a Data Type Relation (or Property)."}, {"title": "2.2 Large Language Model", "content": "Large language models (LLM) are built upon the pretraining technique, which aims to train a general model using large amounts of data and tasks that can be fine-tuned easily in different downstream applications. The concept of pretraining originates from transfer learning in computer vision (CV) tasks. Recognizing the effectiveness of pretraining in CV, researchers have extended pretraining techniques to other domains, including natural language processing (NLP).\nWhen applied to NLP, well-trained language models capture rich knowledge beneficial for downstream tasks, such as understanding long-term dependencies, hierarchical relationships, and more. One of the significant advantages of pretraining in NLP is that training data can be derived from any unlabeled text corpus, providing an essentially unlimited amount of training data.\nEarly pretraining methods in NLP, such as the Neural Language Model (NLM) and Word2vec, were static, meaning they could not adapt effectively to different semantic environments. Static models learn fixed word representations, which limits their ability to handle context variations. To overcome this limitation, dynamic pretraining techniques were developed, leading to Pretrained Language Model (PLM) like BERT (Bidirectional Encoder Representations from Transformers) and XLNet. These models dynamically adjust word representations based on context, significantly enhancing their adaptability and performance across various tasks.\nBERT models the conditional probability of a word given its bidirectional context, also named masked language modeling (MLM) objective:\n$L_{si} \\mathbb{E}_{S} \\sum_{s \\in S} \\log p(s_{i} | s_{1}, ..., s_{i-1}, s_{i+1}, ..., s_{Ns})$\nwhere S is a sentence sampled from the corpus D, si is the i-th word in the sentence, and Ns is the length of the sentence. BERT utilizes the Transformer architecture with attention mechanisms as the core building block. In the vanilla Transformer, the attention mechanism is defined as:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\nwhere Q, K, V \u2208 RNs\u00d7dk are the query, key, and value vectors for each word in the sentence, respectively. Following BERT, other masked language models are proposed, such as RoBERTa, ALBERT, and ELECTRA, with similar architectures and objectives of text representation.\nAlthough the original Transformer paper was experimented on machine translation, it was not until the release of GPT-2 that language generation (aka. causal language modeling) became impactful on downstream tasks."}, {"title": "2.3 Knowledge Representation Learning", "content": "The models for knowledge representation learning can be categorized into three main types: Linear Models, Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs).\nSimple linear models can be used to generate text-based representations. Description-embodied knowledge representation learning (DKRL) and Joint(BOW) use continuous bag-of-words (CBOW) to encode keywords extracted for entity descriptions. The vectors of these keywords are summed up to form text-based entity representations. However, CBOW has limitations as it treats all keywords equally and neglects word order in the description. Veira et al. leverage Wikipedia entries for entity descriptions and generate relation-specific weighted word vectors (WWV) for entities. WWV assigns different importance to words in the description based on frequency and their relationship with the relation of the word. Matrix A records the number of occurrences of each word in the description, while matrix B captures relevance.\nConvolutional neural networks (CNNs) are effective for encoding entity descriptions and textual mentions, learning deep expressive features from textual information. DKRL(CNN) assumes that word order in entity descriptions involves implicit relations between entities that KGs may omit. It uses a five-layer CNN to discover these implicit relations. The vector embeddings of words, excluding stop words, are input to the CNN, with max-pooling and mean-pooling operations used to manage parameter space and filter noise. The non-linear output layer constructs the text-based entity representation.\nRecurrent neural networks (RNNs) capture long-term relational dependencies. Sequential Text-embodied Knowledge Representation Learning (STKRL) extracts reference sentences for each entity from a corpus, treating entity representation as a multi-instance learning problem. A position-based RNN/LSTM encoder generates sentence-level representations, assigning importance based on cosine similarity between sentence representations and structure-based embeddings. Entity Descriptions-Guided Embedding (EDGE) uses BiLSTM to encode entity descriptions, handling context and word sequences effectively. Pre-trained word embeddings from word2vec are input to BiLSTM, refined iteratively with structure-based embeddings to enhance representation quality."}, {"title": "3 LLM Enhanced KRL Methods", "content": "In this section, we delve into the advanced methodologies for enhancing Knowledge Representation Learning using large language models. The section is structured into three main sections, each focusing on a different category of LLM-based methods. Section 3.1 explores Encoder-based methods, which utilize powerful encoders to generate dense vector representations of knowledge entities. Section 3.2 discusses Encoder-Decoder-based methods that leverage both encoding and decoding processes to capture and generate complex relations within knowledge graphs. Finally, Section 3.3 examines Decoder-based methods, emphasizing the generation and prediction capabilities of decoders to infer and expand knowledge representations."}, {"title": "3.1 Encoder-Based Methods", "content": "These works employ the encoder of Transformer to transform knowledge graph elements into textual sequences that are then processed to evaluate triple plausibility or to complete knowledge graphs. The contextual capabilities are central to assessing relations and entity attributes in the graph structure."}, {"title": "3.1.1 Multi-Level Encoder", "content": "The typical approach is illustrated in Fig. 4. This category primarily focuses on employing BERT or similar encoder-based pre-trained language models to convert knowledge graph elements into textual sequences, which are then used to assess the plausibility of triples. The common method is to use BERT deep contextual embeddings to enhance the representation of entities and relations, thereby improving accuracy in predicting plausible triples.\n$Input = [CLS] + Tokenize(h) + [SEP] + Tokenize(r) + [SEP] + Tokenize(t) + [SEP]$\nwhere [CLS] and [SEP] are special tokens used by BERT, and Tokenize(.) represents the tokenization process.\nThe classification task is to determine if the concatenated sequence correctly represents a valid triple from the KG. A logistic sigmoid function computes the probability of the triple being valid:\n$S_{i} = \\sigma (CW^{T})$\nHere, C is the final hidden state of the [CLS] token, W is a learnable parameter matrix, and \u03c3 denotes the sigmoid function.\nThe training uses a cross-entropy loss function to handle both positive and negative examples of triples:\n$L = -\\sum_{\\tau \\in D^{+} \\cup D^{-}} (y_\\tau \\log(s_{\\tau, 0}) + (1 - y_\\tau) \\log (s_{\\tau, 1}))$\nwhere D+ and D\u2212 represent the sets of positive and negative triples, respectively, and y\u03c4 is the label (0 or 1) of the triple.\nThe use of BERT allows the model to leverage contextualized text embeddings, improving the quality of predictions compared to traditional methods that might only use structural information of the KG.\nPretrain-KGE [74] enriches traditional Knowledge Graph Embeddings (KRLS) using BERT pre-trained embeddings, aiming to leverage linguistic context for better entity and relation representation. The method is beneficial in contexts where semantic richness is critical, but it relies heavily on the quality and scope of the training data.\nStAR [88] introduces a hybrid model combining textual encoding with graph embedding techniques, addressing the limitations of both approaches by modeling graph elements and their spatial relations. It benefits from enhanced representational efficiency but might struggle with complexity and scalability due to the dual nature of the encoding.\nMEM-KGC [63] utilizes a Masked Language Model approach for predicting masked entities in a knowledge graph, enhancing embeddings for better performance on KGC tasks. Its strength lies in handling unseen entities during testing, though it might introduce biases from the pre-training data used in the MLM.\nThe advantage of this approach lies in leveraging the robust linguistic capabilities of BERT, which can capture complex patterns in text data that are beneficial for knowledge graph completion. However, the reliance on textual representation may limit the model's ability to capture purely structural relationships inherent in knowledge graphs, potentially leading to performance bottlenecks when dealing with entities that have poor textual descriptions."}, {"title": "3.1.2 Score Function Variation", "content": "This category focuses on utilizing pre-trained language models across multiple tasks simultaneously, such as link prediction, entity resolution, and relation classification. Techniques often involve fine-tuning a shared language model on KG-specific data, employing strategies that allow the model to leverage learned representations more effectively. The process of the typical method is depicted in Fig. 5.\nPapers in this subcategory explore the enhancement of knowledge graph embeddings through multi-task learning frameworks and fine-tuning processes. These approaches often integrate additional tasks like relation prediction and relevance ranking with the primary link prediction task, aiming to create a more holistic learning scenario that captures various aspects of knowledge graphs.\nEach KG triplet (h,r,t) is represented by natural language descriptions of the head h, relation r, and tail t. These descriptions are tokenized into sequences Th, Tr, and Tt. A pre-trained language model (LM) encodes these sequences. The output embeddings for the head, relation, and tail are obtained by mean pooling the LM outputs for respective tokens. The semantic embedding process uses the formula:\n$h = MeanPool(LM(T_h))$\n$r = MeanPool(LM(T_r))$\n$t = MeanPool(LM(T_t))$\nwhere Th, Tr, Tt are token sequences for the head, relation, and tail, respectively.\nAfter semantic embedding, a structured loss is optimized to align the embeddings with the KG structure. This loss is designed to make the embedding of a true triplet (h, r,t) such that h + r approximates t."}, {"title": "3.1.3 Negative Strategy", "content": "As shown in Fig. 6, some integrate textual information encoded by LLMs with structured knowledge graph embeddings. This fusion approach enhances the ability to handle both the semantic nuances captured through text and the explicit relational data represented in knowledge graphs.\nResearch in this area focuses on hybrid models that combine the strengths of both textual and structural data. Methods typically involve using language models for initial encoding followed by integration with graph embedding techniques, which help in capturing both textual nuances and relational structures within the graph.\nEach entity v and relation e in the graph G is embedded into a 4D-dimensional hypercomplex space, capturing both the structural and textual information:\n$v = s_v + x_{v_1} i + y_{v_2} j + z_{v_3} k$\n$e = s_e + x_{e_1} i + y_{e_2} j + z_{e_3} k$\nwhere sv and se represent the structural embeddings, and xv1, yv2, zv3, etc., are embeddings derived from textual information.\nThe compatibility of a triplet (v, e, w) in G is scored using a function that measures the alignment of the Dihedron product of the embeddings:\n$f(v, e, w) = Re((v \\otimes e) \\overline{w})$\nwhere \u2297 denotes the Dihedron product, and $\\overline{v \\otimes e}$ represents the conjugate of the product.\nThe method stands out by efficiently integrating diverse information sources and leveraging the mathematical properties of hypercomplex spaces to enhance the semantic richness of knowledge graph embeddings.\nCODEX [28] guides traditional KRL models using embeddings derived from LLMs, enhancing the link prediction capabilities. It benefits from the textual richness of LLMs but depends heavily on the initial quality of the language model embeddings.\nOpen World KGC [72] presents a unified learning framework that combines MLM with multi-task learning to integrate new entities into KGs effectively. It excels in handling unconnected entities but might struggle with data sparsity issues.\nLovelace et al. [61] optimizes PLMs for KGC by decoupling querying and candidate retrieval embeddings, using supervised and unsupervised techniques. It enhances embedding suitability for KGC, but maintaining scalability and efficiency can be challenging.\nThe integration of textual and structural embeddings allows for a richer representation of knowledge graphs, potentially improving accuracy in tasks such as link prediction and entity resolution. Nevertheless, the complexity of these models can lead to challenges in training stability and increased computational demands."}, {"title": "3.2 Encoder-Decoder-Based Methods", "content": "In this category, LLMs are employed as sequence-to-sequence generators in knowledge graph completion. The models receive textual sequences of query triples and directly generate the text of the tail entity, employing the full generative capabilities of language models to predict missing elements of the graph."}, {"title": "3.2.1 Multi-Level Encoder", "content": "Recent works explore advanced contextual embedding strategies and contrastive learning methods to refine knowledge graph completion techniques. The use of rich contextual embeddings from language models combined with innovative training techniques like contrastive loss helps in distinguishing between correct and incorrect triples more effectively. Fig. 7 outlines the steps involved in the conventional method.\nThis subgroup applies contextual embeddings and contrastive learning techniques to knowledge graph completion. Contextual methods leverage language models to generate rich, nuanced embeddings, while contrastive approaches focus on distinguishing between correct and incorrect triples through advanced sampling strategies and loss functions.\nFor discrimination-based methods, the input sequence for a KG triplet (v, e, w) is prepared as:\n$X_{pair} = [CLS] x_v [SEP] x_e [SEP]$\n$X_{tail} = [CLS] x_w [SEP]$\nwhere xv, xe, xw are the text representations of the head entity, relation, and tail entity, respectively.\nFor generation-based methods, the scoring function for a KG triplet is defined using the output probabilities of the sequence model:\nThis represents the likelihood of generating the tail entity sequence given the head entity and relation sequences.\nThis method represents a significant step forward in KRL research by combining the strengths of PLMs with graph representation learning, thereby improving the effectiveness of knowledge extraction and reasoning tasks.\nKG-GPT2 [49] employs GPT-2 for KGC, treating triples as sentences for sequence classification. It leverages GPT-2's contextual understanding but may not always capture the structural nuances of KGs effectively.\nGenKGC [40] transitions KGC to a Seq2Seq generation problem using PLMs, focusing on efficient entity generation. It reduces inference times but may sacrifice accuracy for speed.\nCD [20] distills context-rich information from LLMs into KGC models, enhancing them with detailed text-based contexts. It improves contextual understanding but requires significant computational resources for the distillation process."}, {"title": "3.2.2 Soft Prompt", "content": "This group utilizes sequence-to-sequence and Transformer architectures to perform tasks such as predicting missing entities in triples or completing parts of the knowledge graph. The flexibility of Seq2Seq models is particularly useful for tasks where structured inputs need to be transformed into textual or other structured outputs. The sequence of actions in the typical method is demonstrated in Fig. 8.\nSeq2Seq and Transformer-based models in this category utilize sequence-to-sequence frameworks to generate missing entities or relations in a knowledge graph directly. These models are pre-trained on large text corpora and fine-tuned on knowledge graph data, leveraging their ability to generate coherent and contextually appropriate text sequences.\nEach entity, relation, and triplet in the KG is treated as a text sequence. This allows for the use of textual prompts in LLMs for prediction tasks.\n$Triple = (v, e, w)$\nwhere v is the head entity, e is the relation, and w is the tail entity. The text sequence for a triple would typically look like \"Is v e w?\"\nFor triple classification task, which involves determining if a given triple (v, e, w) is correct (valid) or not, the prompt example is:\n$Prompt: \"Is this true: v e w?\"$\nFor triple classification task, Given v and w, predict e,\n$Prompt: \"What is the relationship between v and w?\"$\nFor entity prediction task, depending on the missing entity, prompts are created to predict either v or w, the prompt for predicting tail entity w is:\n$Prompt: v e$\nThe prompt for predicting head entity v is:\n$Prompt: \"Who/What e w?\"$\nFinally, uses instruction tuning with prompts and expected responses to fine-tune the models. This aligns the predictions with the factual correctness required for KG tasks.\nKGT5 [52] is pre-trained and fine-tuned on Knowledge Graph Completion (KGC) and Knowledge Graph Question Answering (KGQA) tasks. The model employs regularization to maintain link prediction accuracy. The advantage of this method is its flexibility in handling both KGC and KGQA, making it a versatile solution. However, the complexity of training and fine-tuning a Seq2Seq model can be computationally expensive and time-consuming.\nKG-S2S [51] adopts a Seq2Seq generative framework to manage various types of KGC tasks by translating graph structures into flat text representations. This approach allows for unified processing but might suffer from a loss of structural information, which is critical for accurate graph completion.\nCSPromp-KG [32] leverages Conditional Soft Prompts to guide pre-trained language models in balancing the integration of structural and textual information for KGC. The method enhances model performance by ensuring a more nuanced understanding of graph structures. The major drawback is the potential overfitting to specific knowledge graph structures, which may limit the generalizability.\nThe strength of Seq2Seq models lies in their flexibility and effectiveness in generating textual output that adheres to the structural rules of knowledge graphs. However, these models can be prone to generating implausible links if not properly constrained or if the training data is not representative enough of the test scenarios."}, {"title": "3.3 Decoder-Based Methods", "content": "In these studies, LLMs are adapted to enhance reasoning and inference capabilities over knowledge graphs, particularly for complex question answering and fact verification tasks. Techniques include adapting attention mechanisms to focus on relevant subgraphs and employing reasoning strategies that leverage structured graph data."}, {"title": "3.3.1 Multi-Level Encoder", "content": "Fig. ?? illustrates the workflow of this approach.\nThis category includes models that enhance the capabilities of LLMs in complex reasoning and question answering tasks over knowledge graphs. Techniques involve structuring the input to LLMs in a way that mimics reasoning or using retrieval-augmented generation to enhance the context available for decision-making.\nThe approach starts by converting a relevant subgraph Gq from the knowledge graph into a sequence using a breadth-first search (BFS)-based serialization. This serialized form is then processed by the PLM. The serialized subgraph Gq is denoted as:\n$SG_q = {v, e, w, ...}$\nwhere v, e, and w are nodes and edges in the subgraph.\nAs for subgraph-aware self-attention, the mechanism is designed to mimic the processing of a GNN, enabling the model to understand the structure of the subgraph and reason over it. It utilizes attention mechanisms to model the relations and interactions between different entities (nodes) and relations (edges) within the subgraph. The attention process can be represented as:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}} + MV)$\nHere, Q, K, and V are query, key, and value matrices, respectively, and M is a mask matrix that directs attention flow to maintain structural integrity.\nThe PLM is adapted to handle the serialized subgraph and question data simultaneously, facilitating deep interaction and knowledge sharing between the question context and the graph structure. This integration enables the PLM to perform reasoning tasks directly on the subgraph structure, harnessing both the rich semantic understanding of the PLM and the structured reasoning capabilities typically associated with GNNs.\nThe PLM undergoes adaptation tuning using a dataset created from synthesized questions and subgraphs. This adaptation helps the PLM better understand and process the unique format of KG data. The fine-tuning phase optimizes the PLM on downstream question answering tasks over KGs, improving accuracy and efficiency in real-world applications.\nReSKGC [81] introduces a retrieval-enhanced Seq2Seq model that uses a retrieval module to enhance the generation of missing entities by fetching semantically relevant triplets. This method improves accuracy and relevance but relies heavily on the effectiveness of the retrieval process, which can be a bottleneck if not optimized properly.\nStructGPT [88] employs an Iterative Reading-then-Reasoning framework to enhance LLMs\u2019reasoning capabilities over structured data like knowledge graphs. It significantly improves reasoning performance, but the iterative process can be resource-intensive and slow, particularly with complex queries.\nThese models excel in tasks requiring deep reasoning and can handle nuanced queries effectively by leveraging both the structured information from knowledge graphs and the unstructured textual data processed by LLMs. The main drawback is their potential computational inefficiency and the complexity of integrating structured reasoning with generative language models."}, {"title": "3.3.2 Prompt Engineering", "content": "Research in this area focuses on integrating comprehensive contextual information and structured knowledge into the training and operation of LLMs. This enables the models to better understand and generate responses based on both the textual descriptions and the underlying graph structures of the knowledge graph. The conventional method flowchart can be seen in Fig. 9.\nMethods under this heading focus on integrating extensive contextual information and structured knowledge graph data within the processing capabilities of LLMs. This often involves using knowledge prompts or other forms of guided input to enhance the LLM's understanding of the graph structure and semantics.\nComplex questions are decomposed into simpler sub-questions based on predefined templates. This is achieved through a slot-filling mechanism driven by LLMs, formalized as:\n${{S_{i,t}}_{t=1}^{T} = LLM(Q_i)}$\nwhere Qi is the initial complex question, and Si,t represents the decomposed sub-questions.\nFor each decomposed sub-question, the framework retrieves relevant entities and facts from the KG using predefined logical chains that correspond to the sub-questions, the retrieval process is summarized as:\n${{C_t = \\cup {(s, r, o)_l | (s, r, o)_l \u2208 G}}_{l=1}^{L_i}}$\nwhere (s, r, o)l are the triples retrieved based on the logical chains, with s, r, and o denoting the subject, relation, and object, respectively.\nThe candidate reasoning stage involves selecting the correct answers from the retrieved candidates by evaluating their relevance to the sub-questions, expressed using the following function:\n$A_i = LLM(Q_{i,t} | C_t)$\nwhere A are the answers derived for sub-question Qi,t, based on candidates Ct.\nFinally, the responses from all sub-questions are aggregated to construct a comprehensive answer to the original question, which formalized as:\n$Response = Generate({{A_l}_{l=1}^{L_i}})$\nwhere the function Generate compiles the individual answers into a final coherent response.\nKOPA [56] introduces the Knowledge Prefix Adapter (KoPA), which enhances the integration of structural KG information into LLMs for better structural-aware reasoning. While it improves reasoning accuracy, the adaptation of LLMs to specific knowledge graph structures could limit the model versatility across different KGs.\nCP-KGC [31] explores the use of zero-shot large language models to enhance text-based KGC without fine-tuning. The advantage is the minimal training requirement, but the reliance on zero-shot capabilities may result in less precise or contextually inappropriate text generation.\nKICGPT [53] integrates a triple-based knowledge graph completion retriever with a large language model to address information scarcity in long-tail entities. This method effectively utilizes in-context learning strategies, but its success heavily depends on the quality and relevance of the retrieved knowledge.\nThese approaches allow LLMs to leverage their pre-trained knowledge effectively by guiding them with structured inputs, enhancing their performance on knowledge-intensive tasks. However, the success of these methods heavily depends on the quality and the extent of the integration between text and graph data, which can be challenging to optimize."}, {"title": "4 Experiments and Evaluations", "content": "In this section, we present a comprehensive suite of experiments and evaluations to assess the performance and effectiveness of the various LLM-enhanced KRL methods discussed previously. We utilize a variety of datasets and metrics to provide a thorough analysis, ensuring that our findings are robust and generalizable. The section is structured to cover the datasets used, the evaluation metrics employed, and the downstream tasks for which the models were tested."}, {"title": "4.1 Datasets", "content": "Firstly, we provide detailed information about the datasets utilized in the KRL experiments. These datasets span multiple tasks, including entity typing, relation classification, triple classification, and link prediction, offering a broad evaluation scope. We outline the key statistics for each dataset, such as the number of entities, relations, and instances in the training, development, and test sets, as well as the specific tasks they are designed for. This comprehensive overview sets the stage for understanding the context and challenges addressed in the subsequent experiments.\n\u2022 FIGER. The FIGER dataset is designed for fine-grained entity recognition. It provides annotations for a wide range of entity types, offering a comprehensive resource for training models to recognize detailed and specific categories beyond the usual named entities like person, organization, and location.\n\u2022 Open Entity. Open Entity is a dataset used for entity typing in an open-domain setting. It provides annotations for entities within free text, helping to improve the classification of entities into a broad set of predefined categories, enhancing the ability to understand and process natural language.\n\u2022 FewRel. FewRel (Few-shot Relation Extraction) is a benchmark dataset for evaluating the performance of models on relation classification tasks with limited examples. It includes various relation types and supports the development of models that can learn to classify relations from a few instances.\n\u2022 TACRED. TACRED (TAC Relation Extraction Dataset) is a large-scale relation extraction dataset created by Stanford. It includes sentences annotated with a wide range of relation types, providing a rich resource for training and evaluating models on the task of extracting relations between entities in text.\n\u2022 WN11. WN11 is a subset of the WordNet knowledge graph used for knowledge graph completion tasks. It focuses on 11 relation types and includes a set of entities and their relations, facilitating research in link prediction and knowledge base inference.\n\u2022 FB13. FB13 is derived from the Freebase knowledge graph and is used for relation extraction and knowledge base completion tasks. It includes 13 relation types and provides a benchmark for evaluating the ability of models to predict missing links in a knowledge graph.\n\u2022 FB15K. FB15K is a widely used benchmark dataset derived from Freebase. It includes a large set of entities and relations, supporting research in knowledge graph completion, entity resolution, and link prediction.\n\u2022 FB15k-237. FB15k-237 is a subset of the FB15K dataset, created to address issues of redundancy and test leakage in the original dataset. It excludes inverse relations, making it a more challenging and realistic benchmark for knowledge graph completion tasks.\n\u2022 WN18. WN18 is a benchmark dataset based on the WordNet knowledge graph. It contains a subset of entities and relations, often used for evaluating models on the task of link prediction in knowledge graphs.\n\u2022 WN9. WN9 is another subset of the WordNet knowledge graph, designed for tasks such as knowledge graph completion and link prediction. It includes a specific set of entities and relations to facilitate focused research in these areas.\n\u2022 WN18RR. WN18RR (WN18 Reversed Relations) is a refined version of the WN18 dataset, created to address issues with inverse relations that were present in the original dataset. It is used for benchmarking link prediction and knowledge graph completion models.\n\u2022 UMLS. The Unified Medical Language System (UMLS) is a comprehensive dataset used in biomedical and healthcare research. It integrates various health and biomedical vocabularies, providing a rich resource for developing models in medical text mining and knowledge extraction.\n\u2022 NELL-One. NELL-One is a few-shot learning dataset derived from the Never-Ending Language Learning (NELL) system. It supports research in few-shot relation learning by providing a challenging benchmark with a variety of relation types annotated from web text.\n\u2022 Wikidata5M. Wikidata5M is a large-scale dataset derived from Wikidata, containing millions of entities and their relations. It is used for various tasks, including knowledge graph completion, entity linking, and question answering, providing a comprehensive resource for natural language understanding.\n\u2022 Nations. The Nations dataset is a small-scale dataset used for relation prediction and knowledge graph completion. It includes entities representing countries and various relations among them, offering a simplified benchmark for evaluating models.\n\u2022 CoDEx. CoDEx (Comprehensive Knowledge Graph Completion and Explanation) is a suite of knowledge graph completion datasets designed to provide diverse and challenging benchmarks. It includes multiple subsets with different relation types and complexities, supporting research in explainable AI and knowledge graph reasoning."}, {"title": "4.2 Metrics", "content": "Here", "as": "n$Accuracy = \\frac{TP + TN"}, {"as": "n$Precision = \\frac{TP"}, {"as": "n$Recall = \\frac{TP"}, {"as": "n$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n\u2022 Mean Rank. Mean Rank ("}]}