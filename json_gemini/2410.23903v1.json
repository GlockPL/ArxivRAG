{"title": "Neural Network Verification with PyRAT", "authors": ["Augustin Lemesle", "Julien Lehmann", "Tristan Le Gall"], "abstract": "As AI systems are becoming more and more popular and used in various critical domains (health, transport, energy, ...), the need to provide guarantees and trust of their safety is undeniable. To this end, we present PYRAT, a tool based on abstract interpretation to verify the safety and the robustness of neural networks. In this paper, we describe the different abstractions used by PYRAT to find the reachable states of a neural network starting from its input as well as the main features of the tool to provide fast and accurate analysis of neural networks. PYRAT has already been used in several collaborations to ensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing its performance.", "sections": [{"title": "Introduction", "content": "There is no doubt that Artificial Intelligence (AI) has taken over an important part of our lives and its recent popularisation with large language models has anchored this change even more in our current landscape. The use of AI is becoming more and more widespread reaching new sectors such as health, aeronautics, energy, etc., where it can bring tremendous benefits but could also cause environmental, economic, or human damage, in critical or high-risk systems. In fact, numerous issues are still being uncovered around the use of AI, ranging from its lack of robustness in the face of adversarial attacks [1, 2], to the confidentiality and privacy of the data used, the fairness of the decisions, etc.\nFaced with these threats and the exponential growth of its use, regulations have started to emerge with the European AI Act. Not waiting for regulations, tools have already been developed to respond to and mitigate these threats by providing various guarantees from the data collection phase to AI training and validation of the AI. Our interest here lies in this last phase, the formal validation of an AI system, and more specifically a neural network, to allow its use in a high-risk system.\nNeural networks are a subset of AI models and are also one of the most popular and widely used ar- chitectures. They include a large variety of architectures (DNN, CNN, GNN, Residual, Transformers, etc.) applicable to various use cases such as image classification, regression tasks, detection, control & command, etc. Similarly, to a classical software, these neural networks, when implemented in critical systems, should be tested and validated in order to provide strong guarantees regarding their safety. While there are many approaches, such as empirically testing the robustness to small input variations, adversarial attacks or even metamorphic transformations [3], we focus here on using formal methods to provide strong mathematical guarantees on the safety of a neural network.\nTo this end, PYRAT, which stands for Python Reachability Assessement Tool, has been under develop- ment at CEA since 2019, in the hope of simplifying the validation process of neural network. It provides an simple interface and an easy step to formally prove the safety of neural network, which can often be seen as an arduous task. To easily interface with most of the neural network training frameworks (Tensorflow, PyTorch, etc.) PYRAT is written in Python and handles several standard AI formats (ONNX, NNet, Keras, etc.). PYRAT also provides several possibilities and interfaces to evaluate the safety of a network through standard safety property files (VNN-LIB or similar format) or through several Python APIs.\nPYRAT has been time-tested in several national and international projects as well as in multiple col- laboration with industrial partners. It has also participated in the international neural network verification competition, the VNN-Comp, in 2023 and 2024 [4] achieving third and second place respectively."}, {"title": "Related tools", "content": "Even before the widespread use of machine learning, classical programs had to be formally verified and/or certified to avoid any erroneous behavior when used in critical systems. Thus, many verification techniques used for AI were initially developed to increase trust in critical systems, including model-checking [5, 6], which represents the program as a state transition system and checks whether the model built satisfies a formal property, often using Satisfiability Modulo Theory (SMT), or abstract interpretation [7, 8], which abstracts the semantics of a program to compute an approximation of what the program does (e.g. an over-approximation of the reachability set).\nAs the need arose, these methods were later adapted to the verification of neural networks with specific modifications, e.g. no loops, no memory to allocate, but instead linear and non-linear operation on huge matrices or tensors. The first tools developed for this purpose were SMT-based methods [9, 10, 11], which rely on SMT formulations and solvers to compute the feasibility of an input-output property on small neural networks. Instead of using SMT, some tools transformed the verification task into a constrained optimisation problem in the Mixed Integer Programming (MIP) setting [12, 13]. Both of these techniques are known to be computationally expensive.\nTo avoid these costly computations, methods based on abstract interpretation have been developed [14, 15]. In general, tools using these methods represent the inputs they want to verify as a set which is then passed along the neural network and it yields a set that over-approximates the output which is used to evaluate the property. The formalism used to represent the set along the neural network is called an abstract domain. The accuracy of an analysis by abstract interpretation depends heavily on the abstract domain chosen. If the result of the analysis is too imprecise, one way to improve the precision is to combine several abstract domains [8, 16].\nWe briefly present a non-exhaustive list of tools aimed at the formal verification of neural networks with abstract interpretation and we refer to Urban and Min\u00e9 [17] for a more comprehensive survey. Many of these tools, including PYRAT, accept the Open Neural Network Exchange (ONNX) format to describe the neural network and the VNN-LIB format (based on SMT-LIB) to describe the properties to be verified and have participated to at least one instance of the annual VNN-Comp. They are marked with (*).\n\u2022 (*)ALPHA-BETA-CROWN [18, 19, 20] uses the CROWN-domain which is mathematically equivalent to the polytope domain with a tailored abstractions for ReLU and other non-linear functions, and branch and bound verification strategy. This tool won the yearly VNN-Comp from 2021 to 2024.\n\u2022 (*)MN-BAB [21] is a dual solver built on the DeepPoly domain [22] leveraging multi-neural relaxation of the ReLU along with branch and bound verification strategy.\n\u2022 (*)NNV [23, 24] and (*)NNENUM [25, 26] use the star set domain that uses linear programming to optimize ReLU abstractions for the Zonotopes domain\n\u2022 LIBRA [16] also uses abstract interpretation, although for checking fairness properties (unlike PYRAT and other tools that analyse reachability and safety properties). Like PYRAT, LIBRA can combine several abstract domains to increase the precision of its analysis. Its domains include intervals, as well as domains from the APRON [27] library or domains reimplemented from SYMBOLIC [28], DEEPPOLY [22], and NEURIFY [29].\n\u2022 SAVER [30, 31] verifies properties on support vector machines (SVMs), focusing on robustness or vul- nerability properties of classifiers. Its domains include Intervals and Zonotopes.\nPYRAT also relies on abstract interpretation techniques to verify the safety of neural networks. It im- plements several abstract domains such as intervals, zonotopes, zonotopes paired with inequality constraints or a DeepPoly reimplementation. These domains can be combined and optimised by PYRAT with multiple strategies such as branch and bound."}, {"title": "Key Notions", "content": null}, {"title": "Neural Networks", "content": "Neural networks are functions $N: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ which are generally defined in terms of layered architectures alternating between linear and non-linear operations.\nWe represent a neural network as a directed graph of layers resulting in a composition of the operations corresponding to each layer. For example, a simple feedforward neural network with an activation function between each layers without any loop or branching in the network as shown in Figure 1.\nIt can also be defined as the composition of each layer of the network with $x \\in \\mathbb{R}^{n}$ the input and $y \\in \\mathbb{R}^{m}$ the output of the network:\n$y = N(x) = \\sigma_{2}(l_{2}(\\sigma_{1}(l_{1}(x))))$.\nTypically, the activation functions $\\sigma_{1}, \\sigma_{2}$ and $\\sigma_{3}$ can be the Rectified Linear Unit (ReLU), the Sigmoid or the Tanh functions which are defined below. While the layers $l_{1}, l_{2}$ and $l_{3}$ can be matrix multiplication or convolution layers.\n$\\operatorname{ReLU}(x) = \\max(x, 0)$\n$\\operatorname{Sigmoid}(x) = \\frac{1}{1+ e^{-x}}$\n$\\operatorname{Tanh}(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = 2 \\times \\operatorname{Sigmoid}(2 x)-1$\nMore complex architectures can include some residual connections between layers to model more complex behaviors, as represented in Figure 2.\nThe representation in Figure 2 is equal to the following formula:\n$y = N(x) = \\sigma_{2}(l_{2}(\\sigma_{1}(l_{1}(x))))) + \\sigma_{1}(l_{1}(x))$.\nOverall, we classify the layers of a neural network into three categories which will lead to different treatment in PYRAT:\n\u2022 Linear layers such as affine or convolutional layers.\n\u2022 Non-linear layers such as the various activation functions but also layers implementing multiplication or division between different layers (Mul, Div, Pow, ...).\n\u2022 The layers that do not directly influence the values of the data but may be only modify their shape such as concatenation, flattening or reshaping layers."}, {"title": "Reachability Analysis", "content": "Let $N: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ be a neural network. We define the notion of safety property by a set of constraints on the inputs and outputs of $N$. Through set inclusion, we express the satisfiability of this safety property: given the input set $X \\subseteq \\mathbb{R}^{n}$ and the output set $Y \\subseteq \\mathbb{R}^{m}$ defined by the safety property, the safety property is satisfied if and only if $N(X) \\subseteq Y$.\nIdeally, we would like to define a safety property such that for all inputs $x \\in \\mathbb{R}^{n}$ of a given neural network $N$, the output $y$ is correctly classified by $N$. However, in practice, we cannot define the correct classification for all inputs since this is precisely the task we are trying to solve using a neural network. Therefore, we can only reduce the scope of what we aim to verify by defining more general properties.\nThe first type of safety property here consists of properties on the behavior of the models. For example, given a certain input space $X$, we know that the output will always be $y$. These properties are well suited for control-command neural networks such as ACAS-Xu [32] where the data are structured around some physical values and their semantic is clear. For ACAS-Xu, Katz et al. [33] defined 10 such safety properties on the neural networks used. For example, property 1 is expressed by:\n\u2022 Description: If the intruder is distant and is significantly slower than the ownship, the score of a Clear-of-Conflict advisory will always be below a certain fixed threshold.\n\u2022 Input constraints: $p \\geq 55947.691, V_{\\text {own }} \\geq 1145, V_{\\text {int }} \\leq 60$\n\u2022 Desired output property: the score for Clear-of-Conflict is at most 1500.\nMore generally for the ACAS-Xu, as we possess a fully defined specifications through a look up table (LUT), the safety property can be formalised like in Damour et al. [34] by saying that the decision of the neural network should be contained in the decision of the LUT:\n$\\forall x \\subseteq \\mathbb{R}^{5}, N(x) \\subseteq L U T(x)$\nOn the other hand, with unstructured data such as images or sound, it is harder to define a semantically meaningful set as the input space that we want to verify, e.g. \"there is a pedestrian on the image\". Defining what a pedestrian is on an image is an open question and approaches such as using simulators or image generation may answer this in the future [35]. For now, we focus on a more local approach to the safety on such data. For example, considering a classification task, we can define a local robustness property as follows:\nLocal robustness: Given an input $x_{o} \\in \\mathbb{R}^{n}$ and a modification size $\\epsilon>0$, we want to prove or invalidate the following property\n$\\forall x^{\\prime} \\in B_{\\| \\cdot \\|_{\\infty}}\\left(x_{0}, \\epsilon\\right), \\arg \\max N(x_{0})_{i}=\\arg \\max N\\left(x^{\\prime}\\right)_{i}$\nwhere $B_{\\| \\cdot \\|_{\\infty}}\\left(x_{0}, \\epsilon\\right):=\\left\\{z \\mid\\left\\|x_{0}-z\\right\\|_{\\infty} \\leq \\epsilon\\right\\}$.\nIn this example, we have a set $X := B_{\\| \\cdot \\|_{\\infty}}\\left(x_{0}, \\epsilon\\right)$ and a set $Y := \\left\\{y^{\\prime} \\mid \\arg \\max _{i} N\\left(x_{0}\\right)_{i}=\\arg \\max _{i} y_{i}^{\\prime}\\right\\}$, and the local robustness property holds if $N(X) \\subseteq Y$. The local robustness property can be extended to a regression task, i.e. without the argmax by adding a threshold on the distance between the two outputs.\nComputing $N(X)$ for any given input set $X \\subset \\mathbb{R}^{n}$ is called a reachability analysis, and its exact compu- tation is generally hard. For example, when the activation function of $N$ are only ReLU, then computing exactly $N(X)$ is NP-complete [11]. Therefore, PYRAT uses abstract interpretation to compute an over- approximation of the reachability set $N(X)$."}, {"title": "Abstract Interpretation", "content": "Abstract Interpretation [7, 8] is a theoretical framework that can be applied to the verification of computer programs and neural networks. Its principle is to abstract the semantics of a program and thus to compute an over-approximation of its reachability set by the following method:\n\u2022 Choose an abstract domain $A^{\\#}$, and two operators $\\alpha: \\mathcal{P}\\left(\\mathbb{R}^{d}\\right) \\rightarrow A^{\\#}$ and $\\gamma: A^{\\#} \\rightarrow \\mathcal{P}\\left(\\mathbb{R}^{d}\\right)$ forming a Galois connection [8]"}, {"title": "Abstract domains & abstract transformers", "content": "In this section, we present several domains to abstract $\\mathcal{P}(\\mathbb{R}^{d})$ and the operation commonly found in neural networks."}, {"title": "Boxes and Interval Arithmetic", "content": "One of the easiest ways to abstract any set $X \\in \\mathcal{P}(\\mathbb{R}^{d})$ is to abstract each dimension individually (we consider the projection of $X$ on dimension $i: X_{i}=\\operatorname{proj}_{i}(X)$). We thus obtain a box $\\alpha(X)=\\left(x_{1}, \\ldots, x_{d}\\right)$ where, for each dimension $i, x_{i}=\\left[\\underline{x}_{i}, \\overline{x}_{i}\\right]$ is an interval with $\\underline{x}_{i}=\\min X_{i}$ and $\\overline{x}_{i}=\\max X_{i}$. This is the first abstract domain implemented in PYRAT.\nFor each dimension, we can then do interval arithmetic to compute the result of different operations\u00b9 [40]:\n\u00b9in this section, we only present intervals with finite (real number) bounds and refer to Hickey et al. [38], Dawood [39] for the extension to intervals with floating-point or infinite bounds\naddition: $[a, b]+[c, d]$-> $[a+c, b+d]$\nopposite: $[a, b]$-> $[-b, -a]$\nscalar multiplication X$[a, b]$: X$[a, b]$ -> $\\begin{cases}{[\\lambda a, \\lambda b] \\text { if } \\lambda \\geq 0} \\\\ {[\\lambda b, \\lambda a] \\text { if not }}\\end{cases}$\ngeneral multiplication $[a, b] X[c, d]$: $[a, b] X[c, d]$-> $[\\min (a c, a d, b c, b d), \\max (a c, a d, b c, b d)]$\ninverse:  $[1 / \\overline{d}, 1 / \\underline{c}] \\text { if } \\underline{c}<0 \\leq \\overline{d} \\ [1 / \\overline{c}, 1 / \\underline{d}] \\text { if } 0<\\underline{c}<\\overline{d} \\ [1 / \\underline{c}, 1 / \\overline{d}] \\text { if } \\underline{c}<\\overline{d}<0$\nThe subtraction (resp. division) operators are obtained by adding the opposite (resp. by multiplying by the inverse) of the interval given as a second operand. We can combine the three first operations (addition, opposite and scalar multiplication) to compute the multiplication of an abstraction $X \\in \\mathcal{P}(\\mathbb{R}^{d})$ by a weight matrix $W \\in \\mathbb{R}^{p \\times d}$ by decomposing $W=W^{+}+W^{-}$ between positive and negative weights and then computing $W X=W^{+} X+W^{-} X$. In PYRAT, this is used to optimise the computation done on the Box domain for large matrix multiplication, often found in neural networks.\nMoreover, note that the abstraction of any increasing function (resp. decreasing function) $f$ is simply $f([a, b])=[f(a), f(b)]$ (resp. $f([a, b])=[f(b), f(a)])$. For example, the ReLU function defined as $f(x)=\\max (0, x)$ is increasing, therefore its abstraction for an interval $[a, b]$ is:\n$\\operatorname{ReLU}([a, b])=\\max (0,[a, b])=[\\max (0, a), \\max (0, b)]$\nNevertheless, it is also possible to abstract periodic functions or non monotonic functions such as cos, sin or sigmoid using interval arithmetic.\nOverall, the Box domain is very efficient as it allows to compute reachable bounds for any operation with the same complexity as that operation (on average only 2 operations are needed, one for the lower bound and one for the upper bound of the interval). However, this domain often lacks precision, e.g. if $x \\in[-1,1]$ then a simple operation like $x-x=[-1,1]-[-1,1]=[-2,2] \\neq 0$ will produce a wider result than expected. Due to the nature of the Box domain, they do not capture the relations between the different variables or inputs of a problem and so may fail to simplify such equations. In the context of neural networks, there are numerous relations between the inputs due to the fully connected or convolutional layers. Therefore, more complex abstract domains that capture the relations between variables such as zonotopes will be presented."}, {"title": "Zonotopes", "content": "A zonotope $x$ in $\\mathbb{R}^{d}$, with $d \\in \\mathbb{N}$ the dimension of the zonotope, is a special case of a convex polytope [41]; with the particularity of having a central symmetry. It is formally defined as the weighted Minkwoski sum over a set of $m$ noise symbols $(\\epsilon_{1}, \\ldots \\epsilon_{m}) \\in[-1,1]^{m}$, in other words, for every dimension $1 \\leq i \\leq d$ we have :\n$x_{i}=\\left\\{\\sum_{j=1}^{m} a_{i, j} \\epsilon_{j}+\\beta_{i} \\mid \\epsilon \\in[-1,1]^{m}\\right\\}$\nwhere $a_{i, j}$ are real numbers, called generators, and $\\beta_{i}$ is a real number, called center. This sum can be represented in vector form $\\alpha \\epsilon+\\beta$ where $\\alpha$ is a matrix of dimension $d \\times m$ and $\\epsilon$ is a vector of dimension $m ; \\beta$ is also a vector of dimension $d$. Since the zonotope $\\mathcal{X}$ is solely characterized by the matrix $\\alpha$ and the vector $\\beta$, we use $(\\alpha, \\beta)$ to represent it.\nUnlike boxes, there is no general algorithm to abstract any set $X \\in \\mathcal{P}(\\mathbb{R}^{d})$ directly into a zonotope. However, if anything else fails, there is always the possibility to first abstract $X$ by a box and then create a zonotope from the box. For each dimension $i$, given a box $\\left[l_{i}, u_{i}\\right]$, we can create the zonotope as such:\n$x_{i}=\\frac{u_{i}-l_{i}}{2} \\epsilon_{i}+\\frac{u_{i}+l_{i}}{2}$\nwith $\\epsilon \\in[-1,1]^{d}, d$ new noise symbols.\nMoreover, it is easy to obtain a box containing all the possible values taken by the zonotope, through a concretisation operation. For each dimension $i$, the bounds of the box will be given by:\n$\\left[\\underline{x}_{i}, \\overline{x}_{i}\\right]=\\left[\\min x_{i}, \\max x_{i}\\right]=\\left[\\beta_{i}-\\left|a_{i}\\right|, \\beta_{i}+\\left|a_{i}\\right|\\right] \\text { with }\\left|a_{i}\\right|=\\sum_{j=1}^{m} a_{i, j}$\nConcretising the zonotope into a box allows to compare the box obtained to the safe space defined by the property and thus check whether it holds or not.\nFor most operations present in a neural network, the arithmetic of zonotope is simple as it relies on affine arithmetic [42]. Indeed most of the neural network operations are affine functions for which a zonotope can perfectly capture their results. The table below shows the addition, the opposite and the scalar multiplication on a zonotope.\naddition: $(\\alpha, \\beta)+(\\alpha^{\\prime}, \\beta^{\\prime})$ -> $(\\alpha+\\alpha^{\\prime}, \\beta+\\beta^{\\prime})$\nopposite: $(\\alpha, \\beta)$ -> $(-\\alpha, -\\beta)$\nscalar multiplication: $\\lambda X(\\alpha, \\beta)$ -> $(\\lambda \\times \\alpha, \\lambda \\times \\beta)$\nFor non-linear operations, such as the activation functions or multiplication or division, we need to use an abstraction of the operation to handle them as the affine arithmetic cannot represent these operations otherwise. Thus we are constructing an over-approximation of the reachable output of the layer. We present quickly the abstraction for the ReLU, Sigmoid and Cast functions as shown in Figure 3. The abstraction are constructed neuron-wise, i.e. independently for each element or neuron of a given layer. In that sense, we can consider a single dimension input zonotope $x$ living in $[l, u] \\subset \\mathbb{R}$ for which we want to compute the output zonotope $y$ through a non-linear activation function. In turn, this can be generalised to a multi-dimension zonotopes."}, {"title": "ReLU", "content": "$\\operatorname{ReLU}(x)=\\max (x, 0)$\nThe ReLU function is thus a piecewise linear function, if $x<0$ then $\\operatorname{ReLU}(x)=0$ otherwise $\\operatorname{ReLU}(x)=x$. As such, it is not a linear function for all values of $l$ and $u$ but is linear if $x$ is not in the interval $[l, u]$. Thus we consider three cases for the ReLU abstraction. The first two cases consider a stable or linear ReLU, where the interval is either fully positive or fully negative. If $u<0$, then we have $y=0$ and if $l>0$, the zonotope is fully positive and $y=x$.\nThe last case is the general case if $l<0<u$ where the ReLU is said to be unstable. As it not linear, we need to provide a linear abstraction of the ReLU as in DeepZ [43]. This abstraction $\\operatorname{ReLU}^{\\#}$ is parameterised by $a=\\frac{u}{u-l}$ and $b=-\\frac{u l}{u-l}$ and requires adding a new noise symbol to the zonotope $\\epsilon_{m+1}$:\n$y=\\operatorname{ReLU}^{\\#}(\\mathcal{X})=\\alpha X+\\frac{b}{2} \\epsilon_{m+1}=\\left(\\left(\\begin{array}{c}\\alpha \\times \\alpha\\end{array}\\right), \\alpha \\times \\beta+\\frac{b}{2}\\right)$"}, {"title": "Sigmoid", "content": "$\\operatorname{Sigmoid}(x)=\\frac{1}{1+e^{-x}}$\nFor the Sigmoid function, we build on the work presented in [44] to construct our abstraction. First, we compute $x_{-}$(respectively $x_{+}$) the point where the Sigmoid is the furthest below (respectively above) the slope between $\\operatorname{Sigmoid}(l)$ and $\\operatorname{Sigmoid}(u)$. If $l>0$, we define $x_{-}:=1$ and, similarly, if $u<0, x_{+}:=u$. With these points, we can obtain the distance from the slope to the Sigmoid and thus create our zonotope abstraction:\n$y=\\operatorname{sigmoid}^{\\#}(x)=\\alpha x+\\frac{\\delta}{2} \\epsilon_{m+1}=\\left(\\left(\\begin{array}{c}\\alpha \\times \\alpha\\end{array}\\right), \\alpha \\times \\beta+\\frac{\\delta}{2}\\right)$\nwith\n$a = \\frac{\\operatorname{Sigmoid}(u)-\\operatorname{Sigmoid}(l)}{u-l}$\n$b = -a l+\\operatorname{Sigmoid}(l)+\\frac{\\operatorname{Sigmoid}\\left(x_{+}\\right)+\\operatorname{Sigmoid}\\left(x_{-}\\right)-a\\left(x_{+}+x_{-}\\right)}{2}$\n$\\delta = \\frac{\\operatorname{Sigmoid}\\left(x_{+}\\right)-\\operatorname{Sigmoid}\\left(x_{-}\\right)-a\\left(x_{+}-x_{-}\\right)}{2}$\nHere a new noise symbol $\\epsilon_{m+1}$ will always be introduced for all values of $l$ and $u$ as the function is not piece-wise linear.\nCast The Cast layer and its variants, such as Ceil or Floor, are operations used to change the data type from float32 to int8 for example or to replace the activation function in some networks. These operations are not linear and thus need an abstraction. We only detail the abstraction of the Cast operation here but Floor and Ceil are similar. Here, we have two cases for this abstraction due to its piece-wise linear nature:"}, {"title": "Complexity reduction for Zonotopes", "content": "As the number of abstraction tend to grow in large networks, the number of noise symbols introduced in the network can also grow significantly in the analysis. These symbols can in turn slow down the analysis a great deal as the complexity of the operations on zonotopes is at least $O(m d)$ with $d$ the number of dimension of the zonotope and $m$ the number of noise symbol in the zonotope. Multiple heuristics to reduce the number of noise symbols have been implemented in PYRAT; for example merging some noise symbols in order not to exceed a set number of noise symbols. Criteria such as the one presented in Kashiwagi [45] can be used to choose the noise symbols to merge. Moreover as we merge these noise symbols, we create an over-approximation of their values so that the analysis with PYRAT remains correct although it does loose some precision. Different parameters allow the user to tune this reduction efficiently depending on the network used."}, {"title": "Constrained Zonotopes", "content": "Building on the work in Scott et al. [46], we extend our zonotope domain by adding linear constraints shared over the $d$ dimensions of the zonotope. These constraints allow for more precise abstractions of non- linear functions, reducing the over-approximation introduced during an analysis. At the same time, these constraints can be used to split functions into multiple parts for greater precision in a branch and bound approach as this will be detailed in section 5.3. Instead of the equality constraints used in Scott et al. [46], we use inequality constraints as the equality constraints are not meaningful in the context of our abstractions.\nMore formally a constrained zonotope $x=\\left(x_{1}, \\ldots, x_{d}\\right) \\in \\mathbb{R}^{d}$ with $K$ constraints and $m$ noise symbols is defined by:\n$x_{i}=\\left\\{\\sum_{j=1}^{m} a_{i, j} \\epsilon_{j}+\\beta_{i} \\mid\\begin{array}{c}\\epsilon \\in[-1,1]^{m} \\\\ \\forall k \\in\\{1, \\ldots, K\\}, \\sum_{j=1}^{m} A_{k, j} \\epsilon_{j}+b_{k} \\geq 0\\end{array}\\right\\}$\nwhere $A_{k, j} \\in \\mathbb{R}$ and $b_{k} \\in \\mathbb{R}$. Note that $A$ and $b$ are not indexed by $i$, implying that the constraint are shared for all $i$. At the start of the analysis, the constrained zonotope is unconstrained with $K=0$ and we can add constraints at different layers.\nAn example of this is shown in Figure 4 with the ReLU abstraction when using the constrained zonotope domain. The linear abstraction remains unchanged (in green) but two constraints (in purple) are added $y \\geq x$ and $y \\geq 0$ reducing the approximation introduced. These constraints are added neuron-wise for each unstable neuron, as such for each ReLU layer we add $2 p$ constraints with $0<p<d$ the number of unstable ReLU neurons."}, {"title": "Hybrid Zonotopes", "content": "Introduced in the field of closed loop dynamical systems by Bird et al. [49] and then extended to neural networks by Zhang et al. [50] and Ortiz et al. [51], an hybrid zonotope can be seen as the union of constrained zonotopes. Over $d$ dimensions, we define an hybrid zonotope $x_{i}$ for all dimension $i \\in\\{1, \\ldots, d\\}$ with $K$ constraints similarly to other zonotopes with $m_{c}$ continuous noise symbols and $m_{b}$ binary noise symbols:\n$x_{i}=\\left\\{\\sum_{j=1}^{m_{c}} \\alpha_{i, j} \\epsilon_{j}+\\sum_{j=1}^{m_{b}} \\gamma_{i, j} \\epsilon_{j}^{\\prime}+\\beta_{i} \\mid\\begin{array}{c}\\epsilon \\in[-1,1]^{m_{c}} \\\\ \\epsilon^{\\prime} \\in\\{-1,1\\}^{m_{b}} \\\\ \\forall k \\in\\{1, \\ldots, K\\}, \\sum_{j=1}^{m_{c}} a_{k, j} \\epsilon_{j}+\\sum_{j=1}^{m_{b}} b_{k, j} \\epsilon_{j}^{\\prime}+c_{k}=0\\end{array}\\right\\}$\nwhere $\\gamma_{i, j} \\in \\mathbb{R}$ is called binary generators and $a_{k, j}, b_{k, j}, c_{k} \\in \\mathbb{R}$.\nThe union of constrained zonotopes is here defined through the binary noise symbols that can only take the discrete values of -1 or 1. The hybrid zonotope is thus the union of $2^{m_{b}}$ constrained zonotopes. The constraints used here are equality constraints as they allow to define the intersection between two constrained zonotope (defined by [46]) which is needed for the hybrid zonotopes.\nThe arithmetic is the same as classical zonotope. Nevertheless, an hybrid zonotope has the advantage of being able to exactly represent any piece-wise linear function such as the ReLU or the cast function. With them, we can thus obtain an exact representation of any neural network for which the activation function are piece-wise linear (which comprise a good part of the currently used networks)."}, {"title": "Other domains", "content": "PYRAT also implements a variety of other domains such as polytopes from Singh et al. [22], polynomial zonotopes [52] or small sets. PYRAT also provides interfaces to easily extend its support to new domains either for quick proof of concept or full implementation."}, {"title": "PyRAT", "content": "In this section, we will present the main functionalities of PYRAT as a tool to verify neural networks. More specifically, we will detail how PYRAT applies abstract domains to perform a reachability analysis and proves safety properties."}, {"title": "Network and property parsing", "content": "The first step performed in PYRAT before any analysis is the parsing of the inputs, i.e. the neural network to analyse and the property to verify. For the neural network, PYRAT supports the ONNX standard as its main input but it can also parse, though with a more limited support, Keras/Tensorflow or PyTorch models. These networks are transformed in PYRAT's own internal representation which will then serve to apply all the operations in the network on our abstract domains.\nFor the parsing of the property, PYRAT supports the VNN-LIB\u00b2 property specification language which is itself based on the SMT-LIB format. At the same time, PYRAT supports its own specification format in textual mode or through a Python API as shown in Figure 6.\n\u00b2https://www.vnnlib.org/"}, {"title": "Reachability analysis in PYRAT", "content": "After these steps of parsing and preprocessing, PYRAT will create the abstract domains from the input box specified in the property. It will then apply all the operations of the network on the input box and the"}, {"title": "Branch and bound", "content": "As described in Section 5.2, individual analysis in PYRAT can result in \"Unknown\" evaluation. This happens because the abstract domains are too loose to represent precisely the output, for this reason, we use branch and bound techniques which rely on multiple analyses to increase the precision.\nThis method comes from the field of mathematical optimization [54"}]}