{"title": "CONTRASTIVE LANGUAGE PROMPTING TO EASE\nFALSE POSITIVES IN MEDICAL ANOMALY DETECTION", "authors": ["YeongHyeon Park", "Myung Jin Kim", "Hyeong Seok Kim"], "abstract": "A pre-trained visual-language model, contrastive language-\nimage pre-training (CLIP), successfully accomplishes various\ndownstream tasks with text prompts, such as finding im-\nages or localizing regions within the image. Despite CLIP's\nstrong multi-modal data capabilities, it remains limited in\nspecialized environments, such as medical applications. For\nthis purpose, many CLIP variants-i.e., BioMedCLIP, and\nMedCLIP-SAMv2-have emerged, but false positives re-\nlated to normal regions persist. Thus, we aim to present a\nsimple yet important goal of reducing false positives in medi-\ncal anomaly detection. We introduce a Contrastive Language\nPrompting (CLAP) method that leverages both positive and\nnegative text prompts. This straightforward approach identi-\nfies potential lesion regions by visual attention to the positive\nprompts in the given image. To reduce false positives, we\nattenuate attention on normal regions using negative prompts.\nExtensive experiments with the BMAD dataset, including\nsix biomedical benchmarks, demonstrate that CLAP method\nenhances anomaly detection performance. Our future plans\ninclude developing an automated fine prompting method for\nmore practical usage.", "sections": [{"title": "1. INTRODUCTION", "content": "Recent advancements in multi-modal models, particularly\nvisual-language models (VLMs), have revolutionized var-\nious downstream tasks, such as image retrieval, caption-\ning, and object localization. Among these, Contrastive\nLanguage-Image Pre-training (CLIP) [3] has demonstrated\nremarkable performance by leveraging natural language\nprompts to interpret visual data. This capability enables\nCLIP to handle a wide array of tasks without specialized\nfine-tuning. However, its application to highly specialized\ndomains, such as medical imaging, has uncovered limita-\ntions.\nIn the medical field, accurate anomaly detection is crucial\nfor early diagnosis and treatment. Nevertheless, general-\npurpose models like CLIP often struggle with the intricacies\nof medical images, which contain subtle and features unique\nto medical imaging essential for identifying pathological\nregions. To improve performance in biomedical domains,\nvarious adaptations of CLIP, such as BiomedCLIP [1] and\nMedCLIP-SAMv2 [4], have been proposed to improve per-\nformance in biomedical domains. They shows surprising\nenhancement of medical reasoning compared to ordinary\nmodels, but the issue of false positives\u2014incorrectly identify-\ning normal regions as anomalous-remains prevalent. These\nfalse positives can lead to unnecessary medical procedures,\nincreasing the burden on healthcare systems and potentially\nharming patients.\nTo address this issue, we propose a novel method called\nContrastive Language Prompting (CLAP), which introduces\na more refined way of leveraging natural language prompts\nfor medical anomaly detection. By leveraging both positive\nand negative prompts, our method aims to find out lesions\naccurately with CLIP attention. Positive prompts guide the\nCLIP attention toward potential lesion regions, while nega-\ntive prompts help attenuate the attention on normal regions,\nthereby reducing the occurrence of strong attention to false\npositives. This approach not only provides a more improved\nunderstanding of the medical image but also aligns with the\ndemands for reliability in medical diagnostics by artificial in-\ntelligence.\nWe can just determine whether disease or not based on\nthe CLIP attention. Toward a more accurate diagnosis, we\nemploy an unsupervised anomaly detection (UAD) method\nthat features a reconstruction-by-inpainting strategy [2]. For\nthis, we obfuscate strong attention regions, over \u03bc + 0.6740\n(Q3) [5] valued regions, by considering suspected disease re-\ngions. Then, we attempt to reconstruct obfuscated regions\ninto normal patterns by U-Net which is trained with normal\nsamples only. Finally, we determine the final disease based\non the reconstruction error obtained.\nTo evaluate the legitimacy of our proposal, we perform\nextensive experiments using BMAD dataset [6]. This dataset\nprovides six benchmarks for five anatomies. Visual compar-\nisons demonstrate that CLAP successfully overcomes issues\nof strong attention in non-lesion regions. In addition, we\nimproved UAD performance compared to existing methods.\nThrough this work, we aim to bridge the gap between general-\npurpose VLMs and the specific needs of medical anomaly de-\ntection. We conclude by discussing the potential for automat-\ning language prompt construction to further improve the us-\nability of this approach in real-world clinical settings."}, {"title": "2. METHODS", "content": "In this section, we introduce a VLM-leveraged medical UAD\nmethod. The proposed method as shown in Fig. 2 consists\nof two major components as follows. (1) Obtain an atten-\ntion map that indicates a suspected lesion area by zero-shot\ninference of VLM. For this, we propose a method that over-\ncomes an issue of false positive attention in a single positive\nlanguage prompting. (2) We present a UAD method using\na reconstruction-by-inpainting strategy. The UAD model at-\ntempts to reconstruct a suspected lesion area covered by an\nattention mask. Then, it can be determined whether lesion\nor not based on the reconstruction error. In-depth details are\nintroduced sequentially."}, {"title": "2.1. Contrastive language prompting method", "content": "The VLM takes each image I as a visual prompt and each\ntext string T as a language prompt. All CLIP variants are\nconsisted with image encoder \u03a6\u2081 and text encoder \u0424\u0442. In\nthis study, we exploit BiomedCLIP [1] which has special fea-\nture extraction capability on the biomedical domain by the\nfine-tuning process. Specifically, we simply inherit an atten-\ntion map generation method of MedCLIP-SAMv2 [4] sum-\nmarized as (1). Note, MI is the mutual information operation\nand \u03b2 is a hyperparameter to balance each term.\n$A = MI(Z_{1}, Z_{T}; \\theta_{1}, \\theta_{T}) - \\beta \\times MI(Z_{1}, I; \\theta_{1}, \\theta_{T})$\nw.r.t. $Z_{1} = \\Phi_{1}(I;\\theta_{1}), Z_{T} = \\Phi_{T}(T; \\theta_{T})$\n(1)\nThe results of a positive language prompt, denoted as\nApositive, are shown in Fig. 1. This prompt conveys comma-\nseparated words, recommended words by ChatGPT, that\ncan be used to refer to lesions (e.g., \u201cGlioma, Astrocytoma,\nOligodendroglioma ...\"). There are many strong attention\nregions, dubbed false positives, in Apositive even if the in-\nput image was normal. To mitigate the false positive issue on\nApositive, we additionally check the results of negative prompts\nas shown in Anegative. Those results show not very strong at-\ntention on non-negative regions (false negatives). Moreover,\nthe true negative regions are mostly highlighted that can be\nused to suppress the false positives of Apositive.\nIn this study, we propose CLAP, a straightforward ap-\nproach. CLAP shows a very intuitive approach to take at-\ntention map as \u2018ACLAP = Apositive - Anegative\u2019. This simple\nmethod can be further refined with a parametric function and\ndeep neural networks. The purpose of this study is to take a\nfirst step toward reducing false positive attention when utiliz-\ning VLM for the biomedical domain. Therefore, more refined\nmethods will be dealt with in the next study.\""}, {"title": "2.2. Reconstruction-by-inpainting for UAD", "content": "UAD approaches based on the U-Net aim to block abnormal\nfeature transmission from the encoder to the decoder through\nskip connections [2]. Since it is not known in advance where\nand how large the anomalous pattern exists in the given im-\nage, cutting out the anomalous region by masking is difficult\nto prevent abnormal feature transmission.\nTo address this, an attention map-based saliency obfus-\ncation method was developed [2]. We adopt this method in\nthis study, and the overall scheme is shown in Fig. 2 (b).\nThe saliency region S, suspected to be abnormal, is identified\nusing our CLAP method according to (2), where each pixel\nvalue exceeding \u03bc + 0.6740 (Q3) [5] is flagged. Here, \u03bc and\n\u03c3 are the mean and standard deviation of ACLAP.\n$S = where(A_{CLAP} > \\mu + 0.674\\sigma)$\n(2)\nThis region will be obfuscated by a mosaic process to pre-\nvent the UAD model from receiving the abnormal pattern.\nThe U-Net is trained to reconstruct the partially obfuscated\nimage to its original form, a process known as reconstruction-\nby-inpainting. Only normal samples are used for training, en-\nsuring the model generalizes well to normal patterns while\nstruggling with abnormal pattern reconstruction.\nDuring inference, the U-Net attempts to reconstruct the\nsaliency-obfuscated image. A reconstruction error map is ob-\ntained based on the error between the reconstruction result\nand the original input image before obfuscation. We can per-\nform image-level diagnosis using the maximum value of re-\nconstruction error map."}, {"title": "3. EXPERIMENTS", "content": "We conduct experiments on the BMAD dataset [6], which\ncomprises brain MRI, liver CT, retinal OCT, chest X-ray, and\nlymph node histopathology images. The training set com-\nprises only anomaly-free samples, whereas the test and val-\nidation sets include both anomaly-free and anomalous sam-\nples."}, {"title": "3.2. Qualitative results", "content": "Qualitative results are shown in Fig. 1 and Fig. 3. Fig. 1\npresents attention maps obtained with individual positive and\nnegative prompts, compared to our method. Using only the"}, {"title": "3.3. Quantitative results", "content": "To quantitatively assess the UAD performance, we report AU-\nROC values in Table 2. The primary objective of this study is\nto enhance the true positive detection capability of Biomed-\nCLIP [1] in biomedical anomaly detection tasks.\nAccordingly, we compare the attention performance of\nthe non-biomedical model DINO [7] with that of Biomed-\nCLIP using PLP, positive language prompting, and our CLAP\nmethod. The performance of basic DINO, marked with EAR,\nand PLP is almost the same. This means that DINO atten-\ntion can catch the lesion to some extent without biomedical\nknowledge. At the same time, PLP has biomedical knowledge\nbut contains false attention alarms. CLAP not only improves\nperformance over PLP by attenuating false attention but also\nachieves better overall performance compared to EAR. No-\ntably, CLAP performs particularly well on subsets of images\nwith small, irregular patterns, such as those in 'RESC' and\n'CAMELYON16'."}, {"title": "4. CONCLUSION", "content": "In this work, we introduced a novel approach, Contrastive\nLanguage Prompting (CLAP), aimed at reducing false posi-\ntives in medical anomaly detection using VLMs like Biomed-\nCLIP [1]. By utilizing both positive and negative prompts,\nour method enhances the identification of lesion regions\nwhile suppressing attention on normal areas, addressing a\nkey limitation in previous models that only leveraged posi-\ntive prompts. Extensive experiments with the BMAD dataset\ndemonstrate that CLAP improves anomaly detection accu-\nracy across various medical image types, outperforming both\nDINO [7] and single-prompt methods. Furthermore, we inte-\ngrated CLAP with a reconstruction-by-inpainting U-Net ap-\nproach, enhancing its diagnostic utility. Moving forward, we\naim to refine this technique by automating language prompt\ngeneration to further support real-world clinical applications,\nreducing manual intervention and enhancing the scalability\nof this method in diverse medical scenarios."}]}