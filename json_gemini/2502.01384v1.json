{"title": "Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods", "authors": ["Oussama Zekri", "Nicolas Boull\u00e9"], "abstract": "Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO.", "sections": [{"title": "1. Introduction", "content": "Diffusion models have become efficient generative modeling tools in various tasks, including image and video generation (Song et al., 2021; Ho et al., 2020). Although most of the applications of diffusion models depend on a continuous state space (such as images), recent works extended these models to discrete settings, enabling their use in language modeling and other discrete generative tasks (Sun et al., 2023; Campbell et al., 2022; Austin et al., 2021; Benton et al., 2024). Moreover, several studies showed that these models can be competitive with autoregressive models, such as GPT (Brown et al., 2020) or Llama (Touvron et al., 2023), while allowing for more flexible generation as opposed to next-token prediction (Lou et al., 2024; Sahoo et al., 2024; Shi et al., 2024). These discrete diffusion models hold great promise if they can be scaled up to natural language processing tasks.\nHowever, fine-tuning discrete diffusion models remains a challenging task. Different approaches, such as classifier guidance (Ho & Salimans, 2021; Nisonoff et al., 2025; Gruver et al., 2024) or steering (Rector-Brooks et al., 2025) often suffer from scalability issues or intractable training objectives. In this work, we focus on fine-tuning based on reinforcement learning (RL), where the aim is to maximize an objective by modifying the weights of a pre-trained model. The limitations of the existing methods mentioned earlier highlight the need for novel and efficient methodologies to address the unique challenges of discrete diffusion models.\nMore specifically, sampling from a categorical distribution, which is the type of distribution arising in the discrete setting, is a non-differentiable procedure that cannot be handled by gradient-based optimization algorithms. To bypass this issue, a recent work by Wang et al. 2025 proposes to fine-tune discrete diffusion models through direct backpropagation of rewards with the Gumbel-Softmax trick (Jang et al., 2017). While this is an interesting approach, it cannot handle non-differentiable rewards. Moreover, when working at scale, the size of a reward model that one wants to differentiate with backpropagation may quickly become memory-intensive.\nFinally, methods that achieved state-of-the-art results in Reinforcement Learning from Human Feedback (RLHF) are policy gradient method, namely Proximal Policy Optimization (PPO) (Schulman et al., 2017) or Group Relative Policy Optimization (GRPO) (Shao et al., 2024) due to their stability, efficiency, unbiased gradient estimates, and mechanisms like trust region constraints to handle noisy feedback. These methods deal with the zero-order derivative of the reward oracle, in contrast to (Wang et al., 2025) that uses the first-order derivative of the reward oracle.\nIn this work, we focus on developing policy gradient methods specifically tailored to discrete diffusion models to improve performance and robustness, and introduce a Score Entropy Policy Optimization (SEPO), for fine-tuning discrete diffusion models. Unlike (Wang et al., 2025), our approach does not require the reward function R to be differentiable, which expands the possibilities for fine-tuning beyond Direct Reward Backpropagation. Our method provides a unified framework for optimizing discrete diffusion models."}, {"title": "1.1. Main contributions", "content": "Our contributions are summarized as follows:\n1) We provide an explicit characterization of policy gradient algorithms for discrete diffusion models in the concrete score matching framework. This allows the use of non-differentiable rewards in discrete fine-tuning tasks without steering and guidance mechanisms.\n2) We propose an efficient and scalable algorithm based on policy gradient methods (Schulman et al., 2017; Shao et al., 2024), called Score Entropy Policy Optimization (SEPO), for discrete diffusion. We also introduce a gradient flow alternative that improves sample quality at a higher complexity cost.\n3) We perform numerical experiments on DNA fine-tuning and natural language tasks to demonstrate the performance of our methods."}, {"title": "2. Background and preliminaries", "content": ""}, {"title": "2.1. Related works", "content": "Inference-time techniques. Inference-time techniques are simple yet effective as they require no fine-tuning or training when reward functions are available. Recent studies (Singhal et al., 2025; Ma et al., 2025) showed that they can achieve competitive performance by scaling computational resources. Although inference-time techniques offer distinct advantages, they typically result in longer inference times compared to fine-tuned models. The key considerations for these techniques include computational efficiency and differentiability of the reward (Uehara et al., 2025).\nPolicy gradients algorithms. Policy gradient algorithms are a key class of reinforcement learning methods that optimize parameterized policies by directly maximizing expected returns. Modern implementations include Proximal Policy Optimization (Schulman et al., 2017) or Group Relative Policy Optimization (Shao et al., 2024). These algorithms are highly sensitive to policy design since the architecture impacts expressiveness, optimization stability, and exploration.\nFine-tuning diffusion models with Reinforcement Learning. In the case of continuous diffusion models, fine-tuning via policy gradients has been proposed (Fan et al., 2024; Li et al., 2024; Black et al., 2024; Ren et al., 2025). In a more recent study, (Marion et al., 2024) implements REINFORCE algorithm (Williams, 1992) for continuous diffusion models in a single-loop algorithm, avoiding nested optimization. However, extending these approaches to discrete diffusion models is more challenging. This work adapts these studies to the discrete case and extends them to general policy gradient algorithms."}, {"title": "2.2. Discrete Diffusion", "content": "In discrete diffusion models, the dynamics of a single particle is described by a continuous-time Markov chain (CTMC), denoted as a stochastic process $(x_t)_{0 < t \\leq T}$ operating on a finite space $X = \\{1,...,a_m\\}^n$. Here, $(a_i)_{1 < i < m}$ represents the possible states that form a vocabulary of size m, and n is the length of the sequences, which is a fixed number known as context window or block size. Typically, it describes sequences of tokens or image pixel values. While the size $d := |X| = m^n$ of X is exponential in n, deep neural networks like transformers (Vaswani, 2017) were shown to perform and generalize well on these incredibly large state spaces (Zekri et al., 2024)."}, {"title": "Forward process", "content": "At any given time t, the distribution of a particle $x_t$ is given by $p_t$, which lies within the probability simplex $\\Delta_d \\subset \\mathbb{R}^d$. The forward process is a noising process that maps the initial data distribution $p_0 := p_{data}$ to some final noisy distribution $p_T := p_{ref}$, which is easy to sample. During the noising forward process, the particle's probability transitions between states are given by a rate matrix $Q_t \\in \\mathbb{R}^{d \\times d}$, indexed by X, through the following equation:\n$\\frac{dp_t}{dt} = Q_t p_t, \\quad t \\in [0,T].$ \n(1)\nThe time reversal of this equation is known as (Kelly, 2011),\n$\\frac{dp_{T-t}}{dt} = Q_{T-t} p_{T-t}, \\quad t \\in [0,T],$ \n(2)\nwhere for x, y \u2208 X,\n$Q_t(x, y) = \\begin{cases} \\frac{p_t(y)}{p_t(x)} Q_t(x, y), & x \\neq y, \\\\ -\\sum_{z \\neq x} Q_t(x,z), & x = y. \\end{cases}$"}, {"title": "Concrete score matching", "content": "Lou et al. 2024 recently showed that one can approximate Eq. (2) via concrete score matching (Meng et al., 2022). This is done by learning the concrete score as $s_\\theta(x, t)_y \\approx p_t(x)/p_t(y)$ with a sequence-to-sequence neural network $s_\\theta$ parameterized by $\\theta \\in \\mathbb{R}^P$. We emphasize that this setup includes the simplified approaches detailed in (Sahoo et al., 2024; Shi et al., 2024). The resulting process is described by the following equation:\n$\\frac{dq_t}{dt} = Q_{T-t} q_t, \\quad t \\in [0,T],$ \n(3)\nwhere the denoising process $q_t \\approx p_{T-t}$ maps $q_\\infty := p_{ref}$ to $q_T := \\pi(\\theta)$, and $\\theta$ is learned to achieve $\\pi(\\theta) \\approx p_{data}$. The matrix $Q_t$ is defined for x, y \u2208 X as\n$Q_t(x, y) = \\begin{cases} s_\\theta(x,t)_y Q_t(x, y), & \\text{if } x \\neq y, \\\\ - \\sum_{z \\neq x} Q_t(z, y), & \\text{if } x = y. \\end{cases}$\nNote that, in practice, the quantity $s_\\theta(x, t)_y$ is available for all y \u2208 X at Hamming distance (Hamming, 1950) one of x, i.e., the states y that differ from x by exactly one token. This represents only O(mn) ratios, instead of $O(m^{2n})$ (Campbell et al., 2022; Lou et al., 2024)."}, {"title": "Sampling strategies", "content": "Sampling discrete diffusion models involves selecting efficient strategies to simulate the backward equation (3), while balancing computational cost and sample quality. Among other strategies for CTMCs, sampling can be done via the tau-leaping algorithm (Gillespie, 2001), which implements an Euler step at each position i simultaneously and independently:\n$q_{t}(x^{t-\\Delta t}|x) = q_{t}(x) + \\Delta t Q_{T-t}(x, x^{t-\\Delta t})$ \n(4)\nDiscrete diffusion models can also be used to perform flexible conditional sampling (Lou et al., 2024). Unlike unconditional sampling, which samples $q_t(x_{t-\\Delta t}|x_t)$, we incorporate auxiliary data c by modifying the probability to be sampled to $q_t(x_{t-\\Delta t}|x_t, c)$. Finally, the number of reverse diffusion steps, T, directly impacts computational efficiency and sample fidelity, with larger T providing more accurate approximations of the target distribution at a higher computational cost."}, {"title": "2.3. Fine-tuning with Reinforcement Learning", "content": "After the pretraining phase, a discrete diffusion model with learned parameter $\\theta_{pre}$ aims to approximate $p_{data}$, in the sense $\\pi(\\theta_{pre}) \\approx p_{data}$. Our goal is to fine-tune the target distribution $\\pi(\\theta)$ to increase a reward function $R : X \\rightarrow \\mathbb{R}$, without having access to $p_{data}$.\nMinimization problem. We focus on optimization problems over implicitly parameterized distributions. For a given family of functions $(F_t)_{t \\in [0,T]} : \\Delta_d \\rightarrow \\mathbb{R}$, we aim to minimize the loss function defined as\n$l_t(\\theta) := -F_t(q_t), \\quad t \\in [0,T],$ \n(5)\nover $\\theta \\in \\mathbb{R}^P$. Classical choices of $F_t$ include $F_t(q_t) = \\mathbb{E}_{x \\sim q_t} [R_t(x)]$, where $R_t = 0$ for $t < T$ and $R_T = R$, to maximize a reward function $R : X \\rightarrow \\mathbb{R}$, or $F_t(q_t) = -KL(q_t || q_{pre})$ to minimize the KL divergence of $q_t$ from a distribution $q_{pre}$. As detailed in (Uehara et al., 2024), a typical fine-tuning algorithm for diffusion models combines these two terms as follows,\n$l_t(\\theta) = - \\mathbb{E}_{x \\sim q_t} [R_t(x)] + \\alpha KL(q_t || q_{pre}),$ \n(6)\nwhere $\\alpha > 0$ is a weighting factor. Following standard choices in the fine-tuning diffusion models with reinforcement learning literature (Black et al., 2024; Fan et al., 2024; Clark et al., 2024; Uehara et al., 2024), we assume that $R_t = 0$ for $t < T$ and $R_T = R$ in the rest of this paper. Therefore, the first term on the right side of Eq. (6) is nonzero and equal to $l_R := -\\mathbb{E}_{x \\sim \\pi(\\theta)} [R(x)]$ when $t = T$.\nLoss reward gradient. To apply first-order optimization methods, one needs to compute the gradient $\\nabla_\\theta l_R(\\theta)$. Since X is a finite space of size d, we have\n$\\nabla_\\theta l_R(\\theta) = - \\nabla_\\theta (F_T(\\pi(\\theta))) = -f^\\top \\nabla_\\theta \\pi(\\theta),$ \n(7)\nwhere $f \\in \\mathbb{R}^d$ is the vector of first variations $F_T(\\pi(\\theta))$ (see Appendix D.1). Importantly, we note that $f(p)(x) = R(x)$"}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Policy gradients for concrete score", "content": "The gradient of the target distribution $\\nabla_\\theta \\pi(\\theta)$, which appears in Eq. (7), can be calculated based on to its relationship with the concrete score $s_\\theta$ as $s_\\theta(x, \\theta)_y = \\pi_y(\\theta)/\\pi_x(\\theta)$ for x \u2208 X. The following theorem shows that one can first compute $\\nabla_\\theta \\pi(\\theta)$, and then $\\nabla_\\theta l_R(\\theta)$ through a discrete analogue of the REINFORCE algorithm (Williams, 1992).\nTheorem 3.1 (Discrete REINFORCE trick). With the notations introduced in Section 2, applying the discrete REINFORCE algorithm to the concrete score changes Eq. (7) to:\n$\\nabla_\\theta l_R(\\theta) = \\sum_{x \\in X} \\pi_x(\\theta) R(x) \\sum_{y \\in X \\atop y\\neq x} \\pi_y(\\theta) \\nabla_\\theta \\log s_\\theta(x, \\theta)_y.$"}, {"title": "Monte-Carlo estimation of the outer sum", "content": "The summand in Theorem 3.1 involves the unknown distributions $\\pi_x(\\theta)$ and $\\pi_y(\\theta)$. While the outer sum can be estimated via Monte Carlo sampling, the inner sum is weighted by $\\pi_y(\\theta)$. As noted in (Lou et al., 2024), a single x \u2208 X provides access to every component of the concrete score $s_\\theta(x, \\theta)_y$, for y \u2260 x, and then to $\\pi_y(\\theta)$ since it this is the only missing quantity in the tau-leaping sampling scheme Eq. (4). It is then possible to compute the gradient as\n$\\nabla_\\theta l_R(\\theta) = \\mathbb{E}_{x \\sim \\pi(\\theta)} [R(x) g(x, \\theta)],$\nwhere $g(x, \\theta) := \\sum_{y \\in X \\atop y\\neq x} \\pi_y(\\theta) \\nabla_\\theta \\log s_\\theta(x, \\theta)_y.$\nImportance sampling. Although this defines an unbiased gradient, the REINFORCE algorithm is known to have high variance and to not restrict large policy updates. To address the latter limitation and estimate g(x, $\\theta$), we build upon the core ideas introduced by Trust Region Policy Optimization (TRPO) (Schulman et al., 2015). Instead of sampling from $\\pi(\\theta)$ one can leverage importance sampling through an old policy $\\pi(\\theta_{old})$, and constraint the KL divergence between the old and the current policy as follows:\n$\\nabla_\\theta l_R(\\theta) = \\mathbb{E}_{x \\sim \\pi(\\theta_{old})} \\Big[ \\frac{\\pi_x(\\theta)}{\\pi_x(\\theta_{old})} R(x) g(x, \\theta)\\Big].$ \n(8)\nOnce adapted for concrete score, this formulation leads us to the following result."}, {"title": "Theorem 3.2 (Importance sampling gradient)", "content": "With the notations introduced in Section 2, applying TRPO to Eq. (7) yields:\n$\\nabla_\\theta l_R(\\theta) = \\mathbb{E}_{x \\sim \\pi(\\theta_{old})} [R(x) h(x, \\theta)],$\n(9)\nwhere\n$h(x, \\theta) = \\sum_{y \\in X \\atop y\\neq x} \\pi_y(\\theta) \\frac{s_{\\theta_{old}}(x, \\theta)_y}{s_{\\theta}(x, \\theta)_y} \\frac{\\pi_x(\\theta)}{\\pi_y(\\theta_{old})} \\nabla_\\theta \\log s_\\theta(x, \\theta)_y.$\nThe quantity h(x,\u03b8) is expressed in this way in Eq. (8) to emphasize how the loss will be computed in practice. While being the founding step of state-of-the-art policy gradient algorithms, TRPO requires solving a constrained optimization problem at each step. However, thanks to Theorem 3.2, we can now build powerful, stable, scalable, and easy-to-implement policy gradient algorithms."}, {"title": "3.2. SEPO: Score Entropy Policy Optimization", "content": "Our algorithm relies on the ideas introduced in (Schulman et al., 2017; Shao et al., 2024), but can be adapted to any policy gradient algorithm built on REINFORCE or TRPO. Inspired from these algorithms, we clip the following ratio that appears in the inner sum of Theorem 3.2:\n$\\tau_{x,y} = \\frac{\\pi_y(\\theta) s_{\\theta_{old}}(x, \\theta)_y}{\\pi_y(\\theta_{old}) s_{\\theta}(x, \\theta)_y}$\nto the interval $[1-\\epsilon, 1+\\epsilon]$ for some hyperparameter $\\epsilon > 0$. Another advantage of discrete diffusion models is their great generation flexibility. It is then be possible to apply our algorithm conditionally (via a training dataset, typically in RHLF) or unconditionally for fine-tuning. Hence, in the conditional form, Eq. (8) becomes\n$\\mathbb{E}_{z \\sim D} \\mathbb{E}_{x \\sim \\pi(\\theta_{old})/z} \\Big[ R(x) \\frac{\\pi_x(\\theta)}{\\pi_x(\\theta_{old})} g(x, \\theta)\\Big]$\nInstead of using directly the reward R(x), we compute an advantage A(x) to reduce the variance of the Monte-Carlo estimations. This quantifies how much better an action is compared to the expected return at a given state. A common approach in PPO (Schulman et al., 2017) is to learn a value network to approximate the reward, and then define the advantage as A(x) = R(x) - V(x). For GRPO (Shao et al., 2024), the advantage is the standardized reward over each group. Specifically, for a group of outputs $x = \\{x_1,...,x_G\\}$ the advantages are defined as\n$A(x_i) = \\frac{R(x_i) - mean(R(x))}{std(R(x))}, \\quad i \\in \\{1, ..., G\\}.$"}, {"title": "Remark 3.1", "content": "The loss function takes the form\n$\\mathcal{L}_A(\\theta) = \\mathbb{E}_{x \\sim \\pi(\\theta_{old})} \\Big[\\sum_{y \\in X \\atop y\\neq x} W_{x,y} \\log s_{\\theta}(x, \\theta)_y\\Big],$ \n(10)\nwhere $w_{x,y} = \\pi_y(\\theta)r_{x,y}$ is a coefficient and the log concrete score $\\log s_\\theta(x, \\theta)_y$ is the only term with an attached gradient. PPO, GRPO, and other methods can be constructed by modifying the coefficient $w_{x,y}$. In Appendix B, we present a unified framework encompassing methods that can be derived from SEPO.\nOptionally for t \u2208 [0,T], a $KL(q_t || q_{pre})$ term can also be added to the loss, as in Eq. (6). Although this is not absolutely necessary, as clipping already implicitly regularizes with a $KL(\\pi(\\theta) || \\pi(\\theta_{old}))$ term (Schulman et al., 2017; Fan et al., 2024), the derivation is given in Appendix D.2, for completeness. This leads to the Score Entropy Policy Optimization (SEPO) algorithm described in Algorithm 1."}, {"title": "Algorithm 1 SEPO", "content": "1: Require: CTMC $Q^\\mathcal{Q}$, iteration S, policy optimization iteration K\n2: Set $\\theta_0$ and $\\theta_{old}$ to $\\theta_{pre}$\n3: for s \u2208 [1,..., S] do\n4: Sample from $\\pi(\\theta_{old})$ with $Q^{\\theta_{old}}$\n5: Compute the reward and the advantage\n6: Optimize $\\theta_s$ with $l_A$ for K epochs\n7: Set $\\theta_{old}$ to $\\theta_s$\n8: end for\n9: Output: $\\theta_{S+1}$\nSEPO iteratively samples from the target distribution via a CTMC (Line 4) and optimizes $\\theta_s$ using an optimization objective (Line 6), refining the policy with policy gradients. Specifically:\n\u2022 Line 4: This step generates samples from the target distribution $\\pi(\\theta_{old})$ using the CTMC $Q^{\\theta_{old}}$. This can be done in O(1) time complexity by leveraging the queuing trick introduced in (Marion et al., 2024, Alg. 3), at a higher memory cost.\n\u2022 Line 6: This step updates the parameters \u03b8 using a policy optimization algorithm based on the objective $l_A$ (see Eq. (10)). This means performing K iterations of gradient ascent (or descent) on the policy loss function to improve the policy $\\pi(\\theta_s)$ using the previously collected samples and computed advantages."}, {"title": "3.3. Sampling through gradient flow", "content": "Bilevel problem. We use sampling to reach the limiting process of the backward distribution $\\pi(\\theta)$. This procedure can be interpreted as optimizing a functional $G : \\Delta_d \\times \\mathbb{R}^P \\rightarrow \\mathbb{R}$ over the probability simplex $\\Delta_d \\subset \\mathbb{R}^d$ as\n$\\pi(\\theta) = \\text{argmin}_{p \\in \\Delta_d} G(p, \\theta).$\nWhen $\\pi(\\theta)$ is the limiting distribution of an infinite time process (e.g., Langevin diffusion in the continuous case, Langevin 1908; Pavliotis 2014), one can recast Eq. (5) as a bilevel optimization problem. This has been proposed by Marion et al. 2024 in the continuous diffusion case and allows to efficiently alternate between optimizing one-step of the inner problem and one step of the outer problem.\nGradient flow interpretation. In our case, $\\pi(\\theta)$ is reached with finite-time horizon, in T steps of sampling. However, it is possible to reach $\\pi(\\theta)$ in infinite-time horizon by sampling from a specific time-homogeneous CTMC. The choice of the functional $G(p, \\theta) = KL(p || \\pi(\\theta))$ leads to a gradient flow interpretation of sampling via a specific CTMC.\nLemma 3.3 (Gradient flow). Sampling from the following ordinary differential equation\n$\\frac{dp_t}{dt} = Q^\\infty p_t,  \\text{ where } Q^\\infty := Q_0 + Q_0,$\nimplements a gradient flow for KL(\u00b7 ||Pdata) in \u2206d, with respect to a Wassertein-like metric.\nCorrector steps. Of course, $s_\\theta$ is not perfectly learned in practice, and we just have access to the rate matrix $Q^{\\mathfrak{C},\\theta} := Q_0 + Q$. But this gives us insight into the choice of our sampling strategy, especially with predictor-corrector techniques for discrete diffusion, introduced in (Campbell et al., 2022) and developed in (Zhao et al., 2024). We will then sample from the time-homogeneous CTMC of rate $Q^{\\mathfrak{C},\\theta}$ to reach $\\pi(\\theta)$ with infinite-time horizon. Note that this does not require computing an integral compared to the time-inhomogeneous case. We are then optimizing a functional in Wassertein space through sampling (Marion et al., 2024; Bonet et al., 2024).\nSampling from $Q^\\mathfrak{C}$ affects Line 4 of Algorithm 1. In practice, the sample quality can be improved by adding corrector steps with $Q^\\mathfrak{C} = Q_t + Q_t$, as proposed in (Campbell et al., 2022). Once the process has run for T steps, multiple sampling iterations from $Q^\\mathfrak{C}$ can be performed.\nLinear system characterization. In this case, $\\nabla_\\theta \\pi(\\theta)$ in Eq. (7) will be obtained by solving a linear system, using the implicit function theorem (see Appendix C.3) on $\\nabla l_R$, through a corrected version denoted $\\nabla \\pi(\\theta)$. While both the evaluation of the derivatives and the inversion of this"}, {"title": "linear system characterization", "content": "In this case, $\\nabla_\\theta \\pi(\\theta)$ in Eq. (7) will be obtained by solving a linear system, using the implicit function theorem (see Appendix C.3) on $\\nabla l_R$, through a corrected version denoted $\\nabla \\pi(\\theta)$. While both the evaluation of the derivatives and the inversion of thislinear system can be done automatically (Blondel et al., 2022), it is costly given the dimensionality d. Instead, we provide the exact linear system as well as a closed form of the inverse in Proposition 3.4.\nProposition 3.4. For each \u03b7 > 0, $\\nabla \\pi(\\theta)$ is the solution to a linear system of the form\n$A_{\\eta} X = B \\in \\mathbb{R}^{d \\times P},$\nwhere $A_{\\eta}$ is a rank-1 update to the d \u00d7 d identity matrix, whose inverse can be explicitly computed using the Sherman\u2013Morrison formula.\nNote that this affects Line 6 of Algorithm 1, where $\\nabla_\\theta \\pi(\\theta)$ in Eq. (7) is replaced by $\\nabla \\pi(\\theta)$."}, {"title": "3.4. Convergence bounds", "content": "From a high-level point of view, Algorithm 1 alternates between sampling and optimization steps. We can then view Algorithm 1 as the following coupled equations:\n$dq_s = Q q_s ds,$\n$\\frac{d\\theta_s}{ds} = -\\beta_s \\Gamma (q_s, \\theta_s) ds,$\n(11)\nfor 0 < s < S. The gradient used on line 6 of Algorithm 1 depends both on $q_s$ and $\\theta_s$, and we refer to it as \u0393 (so that $\\nabla l_A(\\theta_s) = \\Gamma(\\pi(\\theta_s), \\theta_s)$). To simplify the analysis, the evolution of both $q_s$ and $\\theta_s$ is done in continuous time flow, for some s \u2208 [0, S], with S > 0. Let $||\\cdot||$ denote the Euclidean norm on $\\mathbb{R}^P$. We reintroduce assumptions on $\\pi(\\theta)$ and \u0393 made in (Marion et al., 2024).\nAssumption 3.2. There exists $C \\geq 0$ such that for all x \u2208 X and $\\theta \\in \\mathbb{R}^P$, $|\\nabla_\\theta \\pi_x(\\theta)| \\leq C$. There exists $\\epsilon > 0$ such that for all x \u2208 X and $\\theta \\in \\mathbb{R}^P$, $\\pi_x(\\theta) > \\epsilon$.\nThis assumption states that the gradient of the target distribution is bounded. The second part is similar to the ambiguity of the language often considered when studying models acting on spaces like X (Zekri et al., 2024; Hu et al., 2024; Xie et al., 2021).\nAssumption 3.3. There exists $C \\geq 0$ such that for all p, q \u2208 \u2206d, \u03b8 \u2208 RP, ||\u0393(p, \u03b8) \u2212 \u0393(q, \u03b8) || \u2264 Cr\u221aKL(p||q).\nThis assumption essentially states that the gradient \u0393 is Lipschitz continuous with respect to the KL divergence on \u2206d.\nWith all these elements in place, we establish the convergence of the average objective gradients."}, {"title": "Theorem 3.5 (Convergence of Algorithm 1)", "content": "Let S > 0 and $\\theta_s$ be the solution to Eq. (11) with $\\beta_s = \\min(1, \\frac{1}{\\sqrt{s}})$, for s \u2208 [0, S]. Under Assumptions 3.2 and 3.3, we have\n$\\frac{1}{S} \\int_{0}^{S} \\mathbb{E}[||\\nabla l_A(\\theta_s)||^2]ds = O\\Big(\\frac{1}{\\sqrt{S}}\\Big),$\nas S\u2192\u221e."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. DNA sequence modeling", "content": "In this first experiment, we employ the pretrained model of (Wang et al., 2025), a masked discrete diffusion model (Sahoo et al., 2024) pretrained on ~ 700k DNA sequences of the Gosai dataset (Gosai et al., 2023).\nWe illustrate the upper bound of Theorem 3.5 in Fig. 2, where we used a reward function designed to enhance CG content in DNA sequences. Details about the non-differentiable reward function can be found in Appendix E. We applied SEPO GRPO with a group size G = 50, with 10 gradient flow corrector sampling steps at the end."}, {"title": "4.2. Discrete diffusion language modeling", "content": ""}, {"title": "4.2.1. Training", "content": "We implement SEPO in an Actor-Critic PPO style to fine-tuning SEDD Medium Absorb (Lou et al., 2024), a discrete diffusion model with 320M non-embedding parameters, pretrained on OpenWebText (Gokaslan et al., 2019)."}, {"title": "4.2.2. EVALUATION", "content": "We use the 153 prompts from the Awesome ChatGPT Prompts dataset (Ak\u0131n, 2023). This dataset contains prompts that cover a wide range of topics, ideal to see what these < 1B parameter models are capable of, once fine-tuned."}, {"title": "5. Conclusion", "content": "We introduced SEPO, a novel approach for fine-tuning discrete diffusion models using policy gradient methods. By extending previous works that applied these methods on continuous spaces, we developed a unified framework that adapts this methodology to the discrete case. Experimental results demonstrate the effectiveness of our approach in optimizing discrete diffusion models while addressing key challenges such as non-differentiability and combinatorial complexity. Future work includes further refining gradient estimation techniques and exploring applications in structured generative modeling."}, {"title": "D. Mathematical supplements", "content": ""}, {"title": "D.1. First Variation in the discrete setup on Ad", "content": "Overview In the discrete setup, when considering a functional F(p) defined on the probability simplex Ad, the first variation quantifies the sensitivity of F to perturbations in the probability distribution p = {p1,...,pa}. For a small perturbation p \u2192 p + en, the first variation is given by\n$\\delta F(p; \\eta) = \\lim_{\\epsilon \\rightarrow 0} \\frac{F(p + \\epsilon \\eta) - F(p)}{\\epsilon}.$\nIn practice, the first variation can often be expressed as a weighted sum over the components of n, as\n$\\delta F(p; \\eta) = \\sum_{i=1}^d \\eta_i \\frac{\\partial F}{\\partial p_i},$\nwhere $\\frac{\\partial F}{\\partial p_i}$ denotes the partial derivative of F with respect to pi, which is the quantity of interest. The next two paragraphs details the derivation for the two functionals considered in this paper"}]}