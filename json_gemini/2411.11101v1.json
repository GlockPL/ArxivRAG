{"title": "Different Horses for Different Courses: Comparing Bias Mitigation Algorithms in ML", "authors": ["Prakhar Ganesh", "Usman Gohar", "Lu Cheng", "Golnoosh Farnadi"], "abstract": "With fairness concerns gaining significant attention in Machine Learning (ML), several bias mitigation techniques have been proposed, often compared against each other to find the best method. These benchmarking efforts tend to use a common setup for evaluation under the assumption that providing a uniform environment ensures a fair comparison. However, bias mitigation techniques are sensitive to hyperparameter choices, random seeds, feature selection, etc., meaning that comparison on just one setting can unfairly favour certain algorithms. In this work, we show significant variance in fairness achieved by several algorithms and the influence of the learning pipeline on fairness scores. We highlight that most bias mitigation techniques can achieve comparable performance, given the freedom to perform hyperparameter optimization, suggesting that the choice of the evaluation parameters rather than the mitigation technique itself-can sometimes create the perceived superiority of one method over another. We hope our work encourages future research on how various choices in the lifecycle of developing an algorithm impact fairness, and trends that guide the selection of appropriate algorithms.", "sections": [{"title": "Introduction", "content": "Over the past decade, concerns about fairness and discrimination in Machine Learning (ML) systems have emerged as critical issues, driving extensive research into the development of fair ML practices, including mitigation algorithms and fairness criteria [Mehrabi et al., 2021, Gohar and Cheng, 2023, Barocas et al., 2023]. This has led to emerging global AI regulation focused on mitigating discrimina- tion in AI/ML systems, mandating the reporting of fairness metrics of algorithms in compliance with various anti-discrimination laws such as the disparate impact doctrine [Justice., 2023].\nHowever, despite the regulatory efforts, recent research has increasingly shown that the fair ML pipeline suffers from instability and high variance in fairness measures, which can mask the underlying unfairness while creating an illusion of fairness [Black et al., 2023]. For instance, recent work has pointed out how fairness measures vary across different training runs or between training and deployment, challenging the effectiveness, reliability, and utility of existing methods [Baldini et al., 2021, Black and Fredrikson, 2021, Friedler et al., 2019, Ganesh et al., 2023]. Additionally, the multitude of mitigation techniques and fairness metrics further complicate accurate benchmarking. Therefore, from both a regulatory perspective and best practices, such variances must be taken into account to accurately represent the performance of these systems and fairness intervention methods."}, {"title": "Related Work", "content": "Researchers have developed a range of mitigation techniques and notions to address unfairness in ML systems, targeting different stages of the ML pipeline, including pre-processing, in-processing, and post-processing methods [Mehrabi et al., 2021, Pessach and Shmueli, 2022, Gohar et al., 2024]. Pre-processing balances data distribution across protected groups, reducing variance, while post- processing modifies model outputs without accessing internal algorithms, ideal for black-box models. In contrast, in-processing methods impose fairness constraints on the model or modify the objective function to mitigate bias. There is inherent randomness in the training process, which, while vital for convergence and generalization [Noh et al., 2017], can be a source of high-fairness variance. Moreover, these techniques also have control parameters (for example, regularization weight) that must be optimized for the training data, further introducing variance [Bottou, 2012]. In this work, we limit our focus on in-processing techniques due to the high variance exhibited by these methods [Baldini et al., 2021, Black and Fredrikson, 2021, Friedler et al., 2019, Ganesh et al., 2023, Perrone et al., 2021]."}, {"title": "Variance in Bias Mitigation", "content": "In this section, we argue that benchmarking under different settings can reveal trends that are lost when sticking to only a single standardized hyperparameter setting or aggregating results across multiple datasets, as done by Han et al. [2023]. Thus, a more nuanced approach to benchmarking bias mitigation techniques is needed to capture the strengths and limitations of various algorithms."}, {"title": "Experiment setup", "content": "For our experiments, we borrow from Han et al. [2023], using their open-source code 2. We focus on the seven tabular datasets and the seven bias mitigation algorithms used in their benchmark (not including the standard empirical risk minimization without fairness constraints). In addition to varying the random seed for training and the control parameter for bias mitigation as done in the benchmark, we also vary the batch size, learning rate, and model architecture to explore several different hyperparameter settings. More details on the experiment setup are delegated to Appendix A."}, {"title": "Case Study: Adult Dataset", "content": "We start with a case study on the Adult dataset [Becker and Kohavi, 1996] and show the changing trends across different hyperparameters. We plot fairness as demographic parity and utility as accuracy, studying the fairness-utility tradeoff across various settings in Figure 2. Additional discussions on other datasets and fairness metrics are present in Appendix B.\nWe first observe the absence of a clear winner among bias mitigation algorithms; no single method consistently and significantly outperforms the others. While HSIC achieves better tradeoffs under the hyperparameter setting used by Han et al. [2023], other techniques such as PRemover and DiffDP perform equally well \u2013 or sometimes even better \u2013 under other hyperparameter settings."}, {"title": "Changing Trends Across Datasets", "content": "In the previous section, we observed different techniques perform better than others under changing settings within a single dataset. We now extend this observation to multiple datasets to show that even the trends across different datasets can vary significantly, and the choice of combining results from all datasets, as done by Han et al. [2023], can obscure these trends. We record the fairness (demographic parity) and utility (accuracy) across different datasets and bias mitigation algorithms in Figure 3. Results for other fairness metrics are present in Appendix C.\nWe begin again with HSIC, which Han et al. [2023] identifies as the algorithm offering the best tradeoff overall. While HSIC does well on most datasets, it is noticeably not the unanimous top choice for both the COMPAS and German datasets. Interestingly, both of these are small datasets with smaller batch sizes used for training. Given that HSIC relies on pairwise similarity in a batch to estimate dependence, its superiority with larger batch sizes but its lackluster performance with smaller batch sizes is not surprising. Most of the datasets analyzed by Han et al. [2023] were big datasets with large batch sizes. Thus, combining results from multiple datasets overshadowed the trends present only in smaller datasets, effectively hiding HSIC's shortcomings."}, {"title": "Comparisons Beyond the Fairness-Utility Tradeoff", "content": "In the previous section, we discussed the limitations of a generic comparative analysis of bias mitigation algorithms that don't take into account the nuances of the entire learning process. Building on this, we now turn to a practical concern: choosing the appropriate algorithm.\nWe begin this section by first showing that given the opportunity to perform hyperparameter opti- mization, various mitigation algorithms can provide competitive models. We then discuss how, given the lack of appropriate differentiation between these algorithms in their fairness-utility tradeoff, the selection of the appropriate algorithm can prioritize other factors, like algorithm runtime, complexity, potential robustness, theoretical guarantees, etc. Consequently, selecting the appropriate algorithm and the resulting model would involve balancing these additional considerations, rather than solely focusing on just the best fairness-utility tradeoff."}, {"title": "Most Mitigation Algorithms are Competitive", "content": "As we saw in Figure 2 (and Appendix B), different algorithms do well under varying settings. In several real-world applications, many of these choices are flexible, and hyperparameter optimization plays an important role in model selection. Thus, when comparing different algorithms, it is important to focus on evaluating the best-performing models from each algorithm, as these are the models that would be deployed if those algorithms were used.\nTo perform this comparison, we only filter the models at the Pareto front for various algorithms after searching through different hyperparameters and random seeds collected in Figure 4. Trends for other fairness metrics are present in Appendix D. We find that several algorithms can provide competitive tradeoffs for almost every dataset. For instance, DiffDP, PRemover, and HSIC demonstrate excellent fairness-utility tradeoffs for the Adult dataset, while all seven bias mitigation algorithms exhibit competitive tradeoffs on the German dataset. With multiple algorithms showing similar tradeoffs, it becomes evident that simply evaluating fairness-utility tradeoffs is insufficient when choosing the most suitable bias mitigation technique. We explore these considerations further in the next section."}, {"title": "Choosing the Right Mitigation Technique", "content": "When several bias mitigation algorithms provide similar tradeoffs, selecting one can be challenging. In such cases, additional factors must be considered, such as the specific requirements of the task, the deployment environment, the stakeholders' expectations, etc. Here, we provide some examples of comparisons beyond the fairness-utility tradeoff that can help choose an appropriate algorithm.\nRuntime: An algorithm's runtime can be a crucial factor when comparing bias mitigation tech- niques. Even minor differences in runtime might become relevant when multiple runs of the same algorithm are needed, for instance, to perform hyperparameter optimization. Our results, detailed in Table 1, reveal interesting trends in training runtime across various algorithms. We find the algorithms HSIC, LAFTR, PRemover, DiffEOdd, and DiffEOpp to be quite expensive, while in contrast, algorithms DiffDP and AdvDebias offer runtime comparable to the standard empirical risk minimization. Considering the competitive tradeoffs achieved by DiffDP, in addition to the lower runtime, it emerges as an appropriate choice for settings where computational efficiency is critical, surpassing other well-performing but slower methods like HSIC and PRemover.\nTheoretical Guarantees and Procedural Requirements: Another important consideration when selecting the appropriate algorithm is the theoretical guarantees that some techniques can offer. For instance, while adding regularizers to the training objective can be useful, it does not provide any form of guarantee for the model's final fairness scores. In contrast, methods like HSIC and LAFTR can provide theoretical bounds on the fairness of the final model, albeit limited to only simpler models [Li et al., 2022, Madras et al., 2018].\nFurthermore, the deployed models may need to comply with specific procedural requirements, which can influence the choice of the mitigation algorithm. For instance, one might need to choose between algorithms focusing on outcome fairness (such as DiffDP, DiffEOpp, DiffEOdd) versus those focusing on process fairness (such as HSIC, LAFTR, PRemover, AdvDebias). The specific requirements of the application can dictate the choice of the algorithm, looking beyond the tradeoffs it can provide.\nMultiplicity and Arbitrariness: Model multiplicity refers to the existence of a set of good models, which have similar performance but differ in their predictions for individuals [Marx et al., 2020, Black et al., 2022]. Existing works have shown that bias mitigation can exacerbate multiplicity concerns, leading to arbitrariness in individual-level predictions [Long et al., 2024]. However, the degree of multiplicity introduced can vary depending on the mitigation algorithm used. Following Long et al. [2024], we define the set of competing models as models with similar accuracy under ERM and record multiplicity using ambiguity [Marx et al., 2020], which is the fraction of data points whose predictions change across different models within the set of good models, in Table 2."}, {"title": "Discussion", "content": "In this paper, we underscore the limitations of current fairness benchmarking practices that rely on uniform evaluation setups. We demonstrate that hyperparameter optimization can yield similar performance across different bias mitigation techniques, raising questions about the effectiveness of existing benchmarks and the criteria for selecting appropriate fairness algorithms.\nContext-dependent evaluation. We argue that the current one-dimensional approach to fairness evaluation may be insufficient. Given the high variability in fairness scores, relying on a single run or, conversely, simply aggregating multiple training runs, both common practices across different dimensions, may not always provide an appropriate comparison of bias mitigation techniques.\nFor example, when models are too large and retraining is impractical, choosing fairness interventions that prioritize stability and consistent scores may be more appropriate. On the other hand, if sufficient computational resources exist to explore hyperparameter options, selecting the best-performing model might be more valid. Additionally, explainability, runtime, and scalability constraints can significantly impact the choice of fairness assessments. Ultimately, the method of comparing algorithms depends on the context. However, in all cases, it is crucial to consider the variability introduced by hyperparameter tuning.\nFuture work. Our experiments were limited to in-processing techniques in bias mitigation. In the future, we plan to explore a broader range of methods, including pre and post-processing. Moreover, we have not explored the potential presence of consistent fairness trends for different hyperparameter choices covered in the experiments. It would be interesting to investigate whether we can identify patterns that guide our decisions to choose better hyperparameter settings for various bias mitigation algorithms. Finally, while evidence in the literature would suggest similar trends exist even with hyperparameters in other parts of the pipeline, for instance, data processing [Simson et al., 2024], our empirical results are limited to hyperparameter choices during training. Further work on a large-scale study of the impact of various choices in the lifetime of an algorithm design is needed."}, {"title": "Additional Details on Experiment Setup", "content": "As we directly borrow the experiment setup from Han et al. [2023], we redirect the reader to their work and the FFB benchmark code 3 for details on the underlying setup. In this section, we briefly mention the datasets and algorithms used in the benchmark, and the new additions and changes we made to their setup."}, {"title": "Datasets", "content": "We use 7 different tabular datasets for our experiments. This includes the Adult dataset [Becker and Kohavi, 1996], COMPAS dataset [Larson et al., 2016], German dataset [Hofmann, 1994], Bank Marketing dataset [Moro et al., 2014], KDD Census dataset [cen, 2000], and ACS dataset with tasks Income and Employment [Ding et al., 2021]. We use the sensitive attribute Race for all datasets, except the Bank Marketing dataset and the German dataset, where we use Age as the sensitive attribute."}, {"title": "Bias Mitigation Algorithms", "content": "We use 7 different bias mitigation algorithms in our setup. This includes DiffDP, DiffEOdd, DiffEOpp, PRemover [Kamishima et al., 2012], HSIC [Baharlouei et al., Gretton et al., 2005, Li et al., 2022], AdvDebias [Adel et al., 2019, Beutel et al., 2017, Edwards and Storkey, 2016, Louppe et al., 2017, Zhang et al., 2018], and LAFTR [Madras et al., 2018]."}, {"title": "Hyperparameters", "content": "We use the Adam optimizer, with no weight decay and a step learning rate scheduler for training. We train the model for 150 epochs and record the fairness and accuracy scores at the final epoch.\nWe use three different values of the control parameter for each algorithm, as defined in Table 3."}, {"title": "Additional Results for Trends Under Changing Hyperparameters", "content": "We present additional results for comparing trends under different hyperparameters in the Adult dataset for fairness definitions of equalized odds (Figure 5) and equal opportunity (Figure 6). We also present additional results for comparing trends in other datasets like Bank Marketing dataset (Figure 7), COMPAS dataset (Figure 8), German dataset (Figure 9), KDDCensus dataset (Figure 10), ACS-Income dataset (Figure 11) and ACS-Employment dataset (Figure 12)."}, {"title": "Additional Results for Changing Trends Across Datasets", "content": "We present additional results for comparing trends across multiple datasets, under fairness definition as equalized odds (Figure 13) and equal opportunity (Figure 14). Similar to the observations in the main paper, we find distinct trends across different datasets and no clear single bias mitigation algorithm that excels across all datasets."}]}