{"title": "Melody Is All You Need For Music Generation", "authors": ["Shaopeng Wei", "Manzhen Wei", "Haoyu Wang", "Yu Zhao", "Gang Kou"], "abstract": "We present the Melody Guided Music Generation (MMGen) model, the first novel approach using melody to guide the music generation that, despite a pretty simple method and extremely limited resources, achieves excellent performance. Specifically, we first align the melody with audio waveforms and their associated descriptions using the multimodal alignment module. Subsequently, we condition the diffusion module on the learned melody representations. This allows MMGen to generate music that matches the style of the provided audio while also producing music that reflects the content of the given text description. To address the scarcity of high-quality data, we construct a multi-modal dataset, MusicSet, which includes melody, text, and audio, and will be made publicly available. We conduct extensive experiments which demonstrate the superiority of the proposed model both in terms of experimental metrics and actual performance quality.", "sections": [{"title": "1 Introduction", "content": "Music generation has been a popular topic in artificial intelligence for decades , and its significance has grown in recent times due to promising applications in generating personalized background music for short videos on platforms like TikTok, YouTube Shorts, and Meta Reels. Additionally, it serves various other scenarios, including dance, virtual reality, and gaming.\nRecently, the development of Large Language Models (LLMs) and diffusion models has transformed many fields, including music generation. The ability to generate audio files that can be played directly from given text descriptions has garnered significant attention. A typical approach involves encoding the input text description with a large language model, which is then used as a condition in a diffusion model to generate an audio representation. Subsequently, a VAE module decodes this audio representation into a playable audio waveform. However, music, as a quintessential form of art, requires harmony both in the overall composition and within each fragment. Previous methods relying solely on text descriptions to condition the music generation process ensure semantic accuracy but inherently fail to maintain harmony across all fragments. Consequently, these methods face issues such as monotony, noise in generated fragments, and a lack of fluency.\nIn this paper, we claim that using melody as a guide is an effective solution to address previous challenges, resulting in improved music generation performance. Specifically, we propose aligning music waveform, text descriptions, and melody into the same semantic space, where melody consists of symbols that concretely reflect the rhythm and harmony of a piece of music. This approach allows us to fully leverage the expertise inherent in melody while preserving semantic information. Additionally, we find that the melody enhance the alignment of text description and audio waveform, rather than impeding it.\nAfter alignment, these materials are fed into the diffusion model, which can accept any combination of text, audio waveform, or melody as input. These inputs serve as foundational semantic information and expertise for the subsequent process. We construct a vector database based on the learned representations of music, consisting of three subsets: audio, text, and melody. A hierarchical retrieval method is then employed to obtain relevant materials as conditions for the diffusion model. We further propose a novel metric, the intersection rate, to evaluate the performance of the search algorithm and demonstrate its relevance to generation performance. Finally, the generated music representation is processed through a decoder that includes a Variational AutoEncoder and a Vocoder to produce a playable music waveform.\nWith melody guiding the process, the generated music not only reflects the semantic information of the given text but also mitigates issues like monotony and noise, ensuring harmony both across and within each fragment. Extensive experiments demonstrate that the proposed MMGen, trained with MusicCaps-a public dataset consisting of only 5,000 samples-achieves surprisingly comparable performance to models trained on over 1 million music samples. Furthermore, MMGen, when provided with more data, surpasses current open-source models in both experimental metrics and actual performance quality.\nAdditionally, we have processed a multi-modal music generation dataset, MusicSet, which consists of over 100,000 samples, each associated with melody, audio waveform, and text description. We are making this dataset publicly accessible to the research community.\nIn conclusion, the contributions of this paper are threefold as following:\n\u2022 We propose utilizing melody to guide the music generation process. To the best of our knowledge, this is the first attempt to leverage melody for generating music waveforms.\n\u2022 We propose the alignment module to align the melody with audio and text description. We find that the newly added melody dimension not only aligns effectively with the music waveform and text description but also enhances the existing alignment between the two.\n\u2022 We propose the generation module that uses retrieved instructive melody as conditions to guide the music generation process. To support this, we introduce, intersection rate, a new method for evaluating the quality of the retrieved material and demonstrate its effectiveness in predicting the final performance of the diffusion model.\n\u2022 We collect and process a large multi-modal music generation dataset, MusicSet, consisting of more than 500k samples, with each of them incorporating high quality music waveform, ample and accurate description, and unique melody. We open access of the dataset, with the hope of promoting related research on music generation.\n\u2022 We implement extensive experiments, which demonstrate the excellent performances of MMGen and effectiveness of its sub-modules on both public music datasets and the human evaluation experiment."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Text-to-Music Generation", "content": "The Text-to-Music (TTM) task aims to generate music, including both melody and audio waveforms, from a given text description. It is a specialized sub-domain of Text-to-Audio (TTA) generation, which demands not only semantic alignment with the input description but also the production of harmonious melodies. Current research in TTM primarily focuses on enhancing the quality of generated music by improving the alignment between the textual description and the resulting audio.\nCurrent research primarily focuses on two technical approaches: generating musical score and generating audio files. ChatMusician treats music as a language, using ABC notation to generate and understand symbolic music without multimodal systems or tokenizers. Similarly, SongComposer generates lyrics and melodies with a tuple format that aligns text and music by combining lyrics with musical note attributes. In previous work, MusicGen proposes a method for generating audio waveforms using compressed discreate music reprersentation based on residual codebooks. Similarly, MusicLM frames the conditional music generation process as a hierarchical sequence-to-sequence modeling task, ensuring that the generated music maintains coherence.\nIn order to ensure that the generated music aligns with the textual information, Theme Transformer introduces a new gated parallel attention mechanism to ensure that the Transformer treats the given conditioning sequence as thematic material that must appear multiple times in the generated music. The generation of musical melodies entails not only the arrangement of note symbols but also the comprehension of the relationships between them, such as harmony, melody, and rhythm. The model proposed in the aforementioned work fails to effectively capture these intricate musical semantics, resulting in generated music that lacks both musicality and coherence. In addition, mapping textual information audio files into a shared vector space, enabling the model to align textual semantic information with musical melody information, remains an unresolved challenge."}, {"title": "2.2 Diffusion Models", "content": "In recent year, diffusion models have made significant progress in generative tasks involving images, text, audio, and video. However, as the number of diffusion steps increases during the training process, both the training and inference costs of diffusion models are require more computational resources. To address this challenge, Rombach et al. proposed Latent Diffusion Model, which performs the diffusion process in the latent representation space, significantly reducing computational complexity while maintaining high-quality results. Additionally, cross-attention was employed to enable conditional generation for multimodal tasks. Traditional diffusion models, when handing text-to-audio generation tasks, operate directly in the data space of raw audio, resulting in high computational costs, AudioLDM proposes a framework that introduces the diffusion process in the latent space, incorporates a cross-attention mechanism and employs CLAP to align text and audio. Compared to AudioLDM, MusicLDM proposes new data augmentation strategies to retrain the CLAP model on music datasets specifically for music generation tasks, with the goal of enhancing the model's understanding and generation capabilities for music. Noise2Music employs two diffusion models: a generator model, which creates an intermediate representation from text prompts, and a cascader model, which refines it into high-resolution audio. TANGO is a text-to-audio method based on latent diffusion models. Unlike AudioLDM, TANGO employs FLAN-T5 instead of CLAP, enhancing its ability to generate accurate audio from textual descriptions."}, {"title": "2.3 Retrieval-Augmented Generation", "content": "The advancements in model algorithms, the growth of foundational models, and the acquisition of high-quality datasets have propelled the evolution of AI-generated content. Despite significant successes, the training of models still faces obstacles such as updating knowledge, handling long-tail data, reducing data leakage, and managing the high costs of training and inference. In the field of image generation, Blattmann et al. proposed to construct an independent image database and retrieval strategy, retrieving a set of nearest neighbors from an external database for each training instance, and conditioning the generative model on these information samples. KNN-diffusion proposed training a relatively small text-to-image diffusion model without using any text, and generating out-of-distribution images by swapping the retrieval database during inference. Yuan et al. proposed a retrieval-augmented TTA framework, introducing retrieved information from datasets to enhance text-to-audio generation performance. However, due to low quality of music-text pairs in existing music datasets and the lack of a dataset for retrieving melody text corresponding to music, there is currently no related work applying the retrieval-augmented framework to music generation."}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Retrieval-Augmented Generation", "content": "Given the query q, the RAG aims to retrieval most relevant materials from database $D_{retrieval}$, thereby enhancing the performance of model. The retrieval process is as following:\n$r^* = \\underset{r \\in D_{retrieval}}{arg \\; topK \\; Sim(q, r)},$"}, {"title": "3.2 Diffusion Model", "content": "Diffusion models have two fundamental process: (1) forward process injects Gaussian noise into a given sample $x_0 \\sim q(x)$ with variance schedule $\\beta_t \\in (0,1)]_{t=1}^T$ so that the sample $x_T$ will subject to a standard isotropic Gaussian distribution after T steps finally; (2) reverse process aims to denoising the damaged sample by predicting the noise $\\epsilon_t$ in each step t and finally restore the original music. Specifically, the forward process is as following:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t I),$\n$q(x_t|x_0) = N(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)\\epsilon),$"}, {"title": "4 Method", "content": "As illustrated in Figure 1, the proposed method consists of three core components: the multimodal alignment module, the generation module, and the decoder. The multimodal alignment module is designed to align three dimensions-waveform, melody, and text description. The generation module then creates a latent music representation based on the learned alignments from the multimodal alignment module. Finally, the decoder module transforms this latent representation into a playable audio waveform."}, {"title": "4.1 Multimodal Alignment", "content": ""}, {"title": "4.1.1 Melody Construction", "content": "We choose the widely used Musical Instrument Digital Interface (MIDI) as the digital format for melody, which records the start time, duration, pitch, and velocity of each note in a piece of music. However, most music tracks do not have an associated MIDI file. Additionally, current open-source music datasets are primarily collections of music waveforms, with only a small portion including corresponding text descriptions, which are often limited to labels or short phrases. Consequently, finding a dataset that contains waveform files, text descriptions, and MIDI files for aligning these three modalities is particularly challenging.\nWe therefore utilize Basic Pitch [3] to extract melody from music waveform files and save them as MIDI files. Inspired by Ding et al. , we discard velocity symbols from the MIDI data, retaining only pitch and duration information. Each note in the MIDI file is transformed into triplets like < pitchname, duration, rest >, where duration refers to how long a pitch lasts and rest indicates the time before the next pitch begins. There are 128 types of pitch. Following Ding et al. , we divide the continuous duration range up to 6.3 seconds into 512 bins. An example of a transformed melody might look like: | < F#3 >,< 125 >,< 79 > | < B#3 >, < 129 >, < 17 > |, where symbols like < F#3 > represent the pitch token, < 129 > is the duration token, and | acts as a separator token."}, {"title": "4.1.2 Alignment", "content": "Unlike CLIP and CLAP, we propose a novel multimodal alignment module that aligns music waveform, melody, and text description simultaneously. This results in three key alignment subparts: (1) music waveform and melody; (2) melody and text description; (3) music waveform and text description. We refer to CLAP for the implementation of the first and third alignment tasks. Specifically, to process the waveform, we first convert each waveform into a mel-spectrogram, which helps to extract the main information from the waveform. We then use HTS-AT to encode the mel-spectrogram into audio representations. For the text description, we directly use ROBERTa to encode the input text into text representations. We also employ a melody encoder to transform the melody into melody representations. However, unlike the waveform and text processing steps, we utilize a small, randomly initialized melody encoder to process the pitch and duration, rather than a large pretrained model. After encoding, we apply a pooling strategy to convert sequences of varying lengths of melody tokens into a uniform shape, which is then fed into Multilayer Perceptrons (MLP) to generate the updated melody representation.\nWe then align the three modality into same vector space using a contrastive loss function, as follows:\n$\\mathcal{L} = \\frac{1}{6} (\\mathcal{L}_{ma} + \\mathcal{L}_{am} + \\mathcal{L}_{at} + \\mathcal{L}_{ta} + \\mathcal{L}_{tm} + \\mathcal{L}_{mt}),$\nThe calculation of the subpart losses follows a similar structure. For example, the contrastive loss between text and melody is defined as follows:\n$\\mathcal{L}_{tm} = \\frac{1}{2N} \\sum_{i=1}^N (log \\frac{exp(x_i^T x_i^t / \\tau)}{\\sum_{j=1}^N exp(x_i^T x_j^t / \\tau)}),$where $\\tau$ is a learnable temperature parameter. $x^t, x^m$ are representations of text and melody, respectively. N is the batch size. We implement contrastive learning within each batch and update the multimodal alignment module by batch gradient descent."}, {"title": "4.2 Generation", "content": ""}, {"title": "4.2.1 Melody Retrieval", "content": "At this stage, we have achieved the alignment of representations pertaining to music, melody, and text through the application of a multimodal alignment module. To put it succinctly, these representations now reside within a shared semantic space, enabling each modality to retrieve corresponding representations with analogous semantic information from the other two modalities.\nThe processes of text-to-music generation and music-to-music generation described in this paper share significant similarities; therefore, the following exposition will focus on the text-to-music generation process as a representative example. To initiate the procedure, the multimodal alignment module is employed to transmute a substantial corpus of melodies into vector representations for constructing a vector database. We explore both conventional retrieval algorithms and hierarchical indexing methodologies. Specifically, we utilize techniques such as the Inverted File Index (IVF) and the Hierarchical Navigable Small World Graph (HNSW). Hierarchical indexing strategies commence by automatically partitioning the vector representations into distinct groups, each representing a unique category of music based on semantic attributes such as genre. Following this segmentation, the system conducts a search within these groups to identify the most relevant K candidates, which are then presented as the ultimate retrieval results. This hierarchical indexing approach significantly enhances the robustness of the indexing outcome, thereby improving the precision of the retrieval process.\nIn this paper, given a textual representation, it serves as a query to index the most relevant melody (i.e.,K = 1) for the subsequent generation process. Furthermore, we introduce a novel metric termed Intersection Rate (ISR) to evaluate the accuracy of indexing performance in an unsupervised manner. The ISR is utilized to select the optimal retrieval algorithm along with its corresponding hyperparameters based on achieving the highest ISR value. The ISR is defined as follows:\n$ISR_{1,2}(q, D_1, D_2, arg_{retrieval}) = \\frac{|R_1 \\cap R_2|}{K},$\nwhere q denotes the given query, and K denotes the size of retrieval set using a searching strategy $arg_{retrieval}$. $R_1$ and $R_2$ denote the retrieval results from database $D_1$ and database $D_2$, respectively. Noted that $D_1$ and $D_2$ contain same content with different modality, such as melody database and corresponding music waveform database."}, {"title": "4.2.2 Conditional Generation", "content": "We put the retrievaled melody as well as input query together as the conditon of diffusion process as following:\nc = MLP([q, r]).\nAs a result, the denoising process with conditon c is as following:\nq(x_{t-1}|x_t, x_0) = N(x_{t-1}; \\tilde{\\mu}_t (x_t, x_0, c), \\beta_t I),\nAnd the predicted noise is as following:\n$\\epsilon(x_t, t, c) = (w + 1)\\epsilon_\\theta (x_t, t, c) - w\\epsilon_\\theta (x_t, t),$\nwhere w denotes the guidance weight."}, {"title": "4.3 Decoder", "content": "The decoder module is designed to generate a playable music waveform based on the previously generated music latent representation. This module comprises two primary components: a Variational Autoencoder (VAE) and a Vocoder. The VAE consists of an encoder and a decoder. The VAE encoder transforms the mel-spectrogram $M \\in R^{T\\times F}$ into a latent representation $X \\in R^{C \\times \\tau}$, where $\\tau$ represents the compression level, and C denotes the number of latent channels. Drawing inspiration from the architecture used in AudioLDM, we adopt the same VAE design and training loss functions, which include reconstruction loss, adversarial loss, and a Gaussian constraint loss. During the inference phase, the VAE decoder is utilized to synthesize mel-spectrograms from the music latent representations that have been learned by the generation module described in Section 4.2. Subsequently, to convert the mel-spectrogram into a playable music waveform, we employ the HiFi-GAN vocoder . This process restores the spectral information into a time-domain signal that can be played as audible music."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Dataset", "content": "We utilize the well-known open-access music datasets MusicBench and MusicCaps to evaluate the proposed MMGen model. To further enhance the performance of MM-Gen, we also attempt to first pre-train the model on MusicSet, a newly proposed music dataset in this paper, before fine-tuning MMGen on the aforementioned MusicBench and MusicCaps datasets.\nMusicCaps This dataset consists of 5,521 music examples, each of which is accompanied by an aspect list and a text caption written by musicians. The aspect list comprises short sentences such as \"high-pitched female vocal melody\". This dataset is actually processed based on the AudioSet dataset.\nMusicbench This dataset contains 52,768 music fragments, each with a corresponding text caption. Similarly, it expands from the MusicCaps dataset. However, we were only able to obtain 42,000 clips of music waveforms from the official link.\nMusicSet We processed a new music generation dataset based on the MTG music dataset, combining it with the processed MusicBench and MusicCaps datasets to form a new multimodal music dataset, named MusicSet. MusicSet consists of 160,000 samples, each associated with a music waveform, text description, and melody. The MTG part includes 10,000 high-quality samples and 100,000 normal samples. We filtered and retained samples that had more than five text labels. To enrich the text descriptions, we utilized the MPT-7B-Chat model to generate more detailed descriptions by querying the model based on the initial text labels. For the audio processing, we segmented the original audio waveforms into multiple 10-second clips, excluding the first and last 10 percent of each audio. This resulted in multiple descriptions and audio segments corresponding to each original audio file. Training samples were constructed by randomly pairing these text descriptions with audio clips, forming the normal part of the samples. To further enhance the quality of the text descriptions, we randomly selected one audio segment from each original music waveform and combined all corresponding text descriptions into a single, comprehensive description. We then used the Deepseek model to rewrite these longer descriptions by having the model refer to examples found in MusicCaps. Through this process, we constructed a high-quality multimodal dataset containing 10,000 samples."}, {"title": "5.2 Baselines", "content": "We compare the MMGen model with the following well-known strong methods, including both text-to-audio models and text-to-music models:\n\u2022 AudioLDM : This is one of the most notable music generation models in recent years, which takes advantage of CLAP, a latent diffusion model, a VAE, and a Vocoder for text-to-audio generation.\n\u2022 TANGO : This work adopts an instruction-tuned LLM as the text encoder and a diffusion model for text-to-audio generation, rather than using CLAP to align text and audio.\n\u2022 Mustango : This work proposes a Music-Domain-Knowledge-Informed UNet guidance module to steer the generated music, capturing music-specific factors for text-to-music generation.\n\u2022 AudioLDM2 : This work proposes transforming any audio into a general representation based on AudioMAE and then generating audio, music, or speech using a latent diffusion model, VAE, and Vocoder."}, {"title": "5.3 Experimental Setting", "content": "We implement all models using the official codes for the baseline models. For the alignment module of MMGen, we choose Adam as the optimizer, setting the learning rate to $1\\times10^{-5}$, the batch size to 48, and the number of epochs to 90. For the generation and decoder modules of MMGen, we adopt AdamW as the optimizer, with a learning rate of $1\\times10^{-4}$, a batch size of 96, a denoising step of 100, an unconditional guide weight of 3, a compression level of 4, and a total of 60,000 training steps. The alignment module is trained on one NVIDIA RTX 4090 24GB GPU, while the generation module is trained on one NVIDIA A800 80GB GPU. We use the officially released checkpoint of AudioLDM-M and fine-tune it on the MusicBench and MusicCaps datasets, respectively. For TANGO, we refer to the results tested by MUSTANGO. For MUSTANGO, since it is fine-tuned on MusicBench and MusicCaps, we directly test it using the official checkpoint on the same test set as MMGen. The training dataset of AudioLDM 2 includes AudioSet, which incorporates MusicBench and MusicCaps. Therefore, we also directly test it without fine-tuning."}, {"title": "5.3.1 Evaluation Protocol", "content": "We evaluate all models on the MusicBench dataset and the MusicCaps dataset. Both datasets are divided into training, validation, and testing sets at an 8:1:1 ratio through random selection. We use the Fr\u00e9chet Audio Distance (FAD), Inception Score (IS), and Kullback-Leibler (KL) divergence as evaluation metrics. FAD evaluates the similarity between original music and generated music, utilizing VGGish as the music classifier. IS measures both the quality and diversity of the generated samples. KL divergence measures the distribution distance between the generated samples and the original samples."}, {"title": "5.4 Music Generation Results", "content": "As shown in Table 1, the proposed MMGen outperforms all baselines on most indicators. Specifically, MMGen consistently achieves the best FAD, surpassing the strongest baseline model, Mustango, by 0.38 on MusicBench and AudioLDM-M by 1.06 on MusicCaps. It also achieves comparable performances on KL and IS. Notably, MMGen is trained on MusicBench with 50,000 music tracks and on MusicCaps with 5,000 music tracks, which amounts to approximately 132 hours of audio. In contrast, AudioLDM 2-Full is trained with 29,510 hours of samples. What's more, due to the intersection of the fine-tune datasets (i.e., MusicCaps and AudioCaps), TANGO and AudioLDM may have been exposed to part of the test set of MusicCaps in our experimental setting. Despite this, MMGen still achieves comparable performance on the KL divergence and IS indicators, which further demonstrates the effectiveness of MMGen.\nIn conclusion, the proposed MMGen utilizes the least number of model parameters and training data while achieving either the best or comparable performances to the current strongest baselines. Specifically, it achieves comparable performance using only 5,000 samples, whereas AudioLDM 2-Full uses more than one million samples. Furthermore, MMGen outperforms AudioLDM 2-Full using just 50,000 training samples. We attribute this improvement to the newly proposed melody guidance mechanism. First, we align the melody with text and audio, and then use the melody to guide the generation process. This approach ensures that the music generated by MMGen is harmonious across and within each fragment, resulting in less noise and more appropriate rhythm."}, {"title": "5.5 Multimodal Alignment", "content": "As demonstrated in Table 2, we fine-tune the alignment based on the same checkpoint provided by AudioLDM on MusicBench and MusicCaps, respectively. We evaluate the alignment performance using Recall (R@1, R@5, and R@10) and Mean Average Precision (mAP). It is evident that the proposed multimodal alignment module significantly outperforms CLAP on both datasets and across all metrics. For instance, the multimodal alignment module of MMGen achieves 0.7414 on R1, representing an improvement of 21.82% compared with CLAP. Moreover, the multimodal alignment module also demonstrates excellent performance in the alignment of other modalities."}, {"title": "5.6 Intersection Rate", "content": ""}, {"title": "5.7 Human Evaluation", "content": ""}, {"title": "5.8 Ablation Study", "content": ""}, {"title": "5.9 Visualization Analysis", "content": ""}, {"title": "5.10 Case Study", "content": ""}, {"title": "5.11 Parameter Analysis", "content": ""}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel music generation model that leverages melody to guide the generation process. The model consists of a multimodal alignment module, a diffusion module, and a decoder. Additionally, we process a multimodal music generation dataset, MusicSet, which contains more than 160,000 music tracks, each associated with a melody, text description, and music waveform. We conduct extensive experiments that validate the effectiveness of the proposed MMGen and its subcomponents. From an application perspective, future directions include enhancing the performance of music continuation, music inpainting, long music generation, singing music, and generating music for short videos."}]}