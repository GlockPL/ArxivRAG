{"title": "Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification", "authors": ["Peipei Wei", "Dimitris Dimitriadis", "Yan Xu", "Mingwei Shen"], "abstract": "We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human-crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.", "sections": [{"title": "Introduction", "content": "In recent years, transformer-based language models with attention mechanisms have deeply revolutionized the field of NLP. Particularly, decoder-only transformer language models, such as GPT-series models, demonstrate impressive emerging capabilities after scaling up the pre-training corpora and model sizes-capabilities not seen in their smaller predecessors such as BERT-based models (Zheng et al. 2023). One of these capabilities is In-Context Learning (ICL) (Brown et al. 2020). Equipped with knowledge acquired during the pre-training stage, these large language models (LLMs) are able to perform various tasks with only task instructions and a few demonstrations, without any parameter updates. Despite their surprisingly good zero-shot and few-shot performance on a wide range of tasks such as general QA, reasoning, and text generation, their performance still significantly lags behind fine-tuned models for text classification (Sun et al. 2023b).\nOn the other hand, these fine-tuned models heavily depend on human annotations, which are not only costly and time-consuming but also sometimes unavailable. Accordingly, leveraging zero-shot or few-shot ICL capabilities of LLMs for text classification has become an important research topic. However, ICL relies on prompt engineering and human expertise in designing demonstration questions, intermediate reasoning steps, and final answers for LLMs to generalize to a variety of unseen queries. Additionally, increasing the number of demonstrations in few-shot settings leads to increased inference costs and may exceed the maximum input length imposed by LLMs.\nWhen humans work on complicated tasks, they usually follow Standard Operating Procedures (SOPs) to ensure that anyone with varying degrees of domain and task-specific knowledge can perform the task with consistently high quality. These SOPs are written by domain experts who have gained expertise by analyzing numerous concrete examples and extracting common principles from them. Inspired by this, we ask: can we mimic the same procedure to generate task-specific principles based on analysis of a handful of demonstrations and then feed them back to LLMs to help mitigate the limitation of lack of task-specific knowledge in ICL?\nPrevious studies show that adding complex class descriptions as additional inputs to a pre-trained transformer backbone via cross-encoder architecture can significantly boost classification performance under zero-shot and few-shot settings (De Silva et al. 2023). Intuitively, injecting more knowledge-intensive principles should also help improve LLMs' ICL performance.\nIn this paper, we present PRINCIPLE-BASED PROMPTING for zero-shot text classification. It utilizes a multi-agent collaboration framework to auto-generate principles for each classification task. First, it employs multiple LLM agents to generate candidate principles from demonstrations with or without labels. In the prompts, it explicitly instructs LLMs to extract key principles that can distinguish each class based on analysis of provided demonstrations. Then, all LLM agents send their principle candidates to a central agent for finalization, which selects the best principles for downstream classification tasks. Our approach demonstrates substantial performance gains over other strong baseline ICL approaches, such as Chain-of-Thought (CoT) (Wei"}, {"title": "Related work", "content": "Demonstration and label relationship Supervised ML models rely heavily on drawing mappings between representations of training examples and their label information to make predictions on unseen examples. Surprisingly, early research on ICL shows that ground truth in demonstration-label mapping is not as important, as showing demonstrations with random labels only leads to minimal performance drops on a range of classification tasks (Min et al. 2022). However, later research points out the limitations of this study and arrives at a different conclusion: the correct correspondence between examples and labels is essential to ensure ICL performance (Kossen, Rainforth, and Gal 2023). The previous biased conclusion could be attributed to the use of binary (accuracy) instead of probabilistic metrics, relatively weaker LLMs that are mostly under 20B parameters, and focus on only one few-shot setting (16 demos). Thus, although LLMs predominantly rely on knowledge acquired during pre-training to perform downstream tasks, they indeed can learn new tasks from in-context information, which motivates this work to find an alternative approach to providing more effective context information for LLM ICL than the commonly used demonstration-based approach. In our experiments, we also conduct ablation studies to explore the importance of label information on the quality of principles generated.\nNumber of demonstrations Supervised ML algorithms are data-hungry and require a substantial amount of labeled training data to ensure model performance. Under ICL few-shot settings, previous work shows that adding more than one demonstration might not be necessary due to only marginal performance improvements (Chen et al. 2023). As Chen suggests, this indicates that the use of demonstrations is inefficient and the information provided by randomly selected demonstrations is most likely redundant. In some cases, multiple demonstrations can even hurt performance due to misguidance or negative interference among them (Chen et al. 2023). This leads to our research question: under the same input length constraint, can we design more concise but knowledge-intensive contexts as alternatives to few-shot demonstrations to better guide LLMs in performing downstream classification tasks? We also conduct ablation studies to explore the importance of the number of demonstrations on the quality of principles generated.\nSingle-Agent vs. Multi-Agent LLM Framework Text classification, as one of the most fundamental NLP tasks, appears to be straightforward in the sense that LLMs only need to output one or more class labels from a predefined label space. However, it can actually be quite complicated and even more challenging due to the implicit nature of the reasoning process in comparison to other tasks. Most research on LLM ICL attempts to enhance model performance either by decomposing complex tasks into multiple steps or by providing LLMs with relevant domain-and task-specific data as additional context, such as the Retrieval Augmented Generation (RAG) approach. For instance, Chain-of-Thought (CoT) prompting first prompts the LLM to break problem-solving into multiple steps and then derives the final answer by following a step-by-step thought process (Wei et al. 2022). Focusing on QA questions, step-back prompting (Zheng et al. 2023) runs inference on the same LLM twice by first asking LLMs to provide abstract principles or concepts to help resolve the original question before answering it. To improve LLMs' performance on text classification, for each data point, Clue And Reasoning Prompting (CARP) (Sun et al. 2023b) includes multiple steps in a single prompt by asking the same LLM to first find superficial clues (e.g., keywords, tones, semantic relations, references, etc.) based on which final decisions are made after reasoning steps. CARP also leverages knowledge acquired through supervised fine-tuning on labeled datasets to search for more effective demonstrations for ICL.\nRecently, the multi-agent framework has gained popularity and has been shown to greatly improve LLMs' performance on complicated tasks such as long-context QA, multi-hop QA, math, and reasoning (Shridhar, Stolfo, and Sachan 2022; Wang et al. 2022). For instance, the multi-agent debate framework can improve LLMs' reasoning capability, factuality, and inter-consistency in mathematical and multiple-choice commonsense reasoning tasks, as well as output quality in open-ended generation tasks, in comparison to their"}, {"title": "Methods", "content": "PRINCIPLE-BASED PROMPTING is motivated by the observation that when performing classification tasks, human beings usually start to build their mental models after reviewing a few concrete examples by summarizing common key principles. Humans tend to rely on abstracted principles since we have limited memory capacity to remember overwhelmingly large amounts of detailed data points. The more comprehensive these principles are to include different scenarios, the more helpful they should be for performing the same task on unseen data. As we see later that in our two internal datasets (Product Classification 1 and Product Classification 2, PC1 and PC2), we have principles manually drafted by domain experts for each task to help ensure annotation quality. In the experiments section, we also investigate whether text classification via ICL with principles generated by our multi-agent framework can outperform their human-generated counterparts. We implement our PRINCIPLE-BASED PROMPTING strategy via a multi-agent LLM framework. It consists of three major steps, each of which can be completed by one or multiple LLM agents (see Figure 1).\nPrinciple Generation Before tackling the classification problem, we first ask the multi-agent LLMs to analyze a few randomly sampled demonstrations with or without label information on their own. Then, we ask them to generate principles to distinguish each class based on their analysis. Since principles are generated at the task level, additional inference costs only occur for each principle generated, which is almost negligible in comparison to the inference costs for entire datasets.\nIn this step, we experiment with a diverse set of six different LLMs, ranging from open to closed models in various model sizes: two open-source LLMs from Huggingface:\nPrinciple Consolidation After the Principle Generation step, we discard the analysis and extract the principles only. These 36 principle candidates are then sent to a finalizer agent to provide the optimal principle for performing the target classification task. We implement three methods based on the paradigm of how these principle candidates are utilized to derive the final principle:\n(1) Listwise ranking by the finalizer agent: We directly ask each LLM to rank the top five principles given the entire list of candidate principles based on their helpfulness for performing the target classification task. Previous research shows that ICL is sensitive to permutation of in-context examples (i.e., selection and ordering) (Wu et al. 2022). Accordingly, we randomize the list of principles presented to LLMs in two different orders, with and without demonstrations (n=2) to illustrate how the target task is defined, yielding 2 x 2 = 4 different prompts for each LLM agent. We aggregate the top five ranked principles from each LLM agent and select the top 1 principle for each dataset based on majority voting. We use all LLMs mentioned above except FLAN-T5-XXL (Chung et al. 2024) and FLAN-UL2 (Tay et al. 2022) because they exceed the input token length limits of 512 or 2048 if we put all the candidate principles in one single prompt. This requires 4 x 4 = 16 inference costs from various multi-agent LLMs. See the Appendix for prompt examples for principle ranking and Table 7 as an example of the final principle selected.\n(2) Consolidation by the finalizer agent: The listwise ranking method tries to make agents compete with each other and select the best principle based on their helpfulness to the downstream classification task. In contrast, the consolidation method acknowledges that a single agent might not be able to provide the optimal principle for the task and instead tries to establish a comprehensive principle by integrating and summarizing key points from all principles while resolving conflicting information. Since this method requires the LLM agent to possess reasoning capabilities, we select Claude 3.5 Sonnet as the finalizer agent based on the overall high quality of principles generated in the previous step. See Appendix for prompt examples for principle con-"}, {"title": "Experiments", "content": "We test our PRINCIPLE-BASED ICL approach and baselines on five text classification datasets: three are public"}, {"title": "Discussion and Conclusion", "content": "We introduce PRINCIPLE-BASED PROMPTING, implemented via a multi-agent framework, as a simple yet generic strategy to elicit deep reasoning capabilities of LLMs by providing them with principles to perform downstream classification tasks. We show its superior performance over single-agent frameworks, including vanilla prompting and other strong ICL strategies such as CoT (Wei et al. 2022), CARP (Sun et al. 2023b), and stepback prompting (Zheng et al. 2023). One of the key differences between our work and previous works that attempt to scaffold LLMs with self-elicited clues or ask high-level concepts and principles before tackling the problem lies in our approach: instead of prompting LLMs to extract abstract principles or superficial clues to answer a single question, we perform knowledge distillation at the task level by providing multiple demonstrations with or without labels and instructing LLMs to extract common patterns (principles) based on their analysis. Our intuition is that analyzing how to solve the same task under different scenarios can help generate general knowledge that is abstracted away from details and thus easily applicable to unseen data with different distributions. The principles generated this way are knowledge-intensive and task-specific, and thus more efficient than those generated by purely relying on LLMs' general world knowledge obtained during the pretraining stage.\nBecause principle generation is performed at the task level, we show that by implementing the principle-based approach via a multi-agent consolidation framework, we can achieve significant performance improvement with only minimal additional inference costs for text classification tasks.\nThe competitive performance of our principle-based approach compared to few-shot ICL settings indicates that naively adding more demonstrations is not an efficient way to teach LLMs the input-label mapping relationship on new tasks. On one hand, sub-optimal sampling of demonstrations might provide a biased perspective for tackling the task, thus becoming insufficient to perform well on more complex or challenging examples. On the other hand, adding more demonstrations can potentially introduce more noise, as the vast amount of details contained in demonstrations is not only challenging for LLMs to comprehend but also distracting, since some details might be irrelevant for performing the classification task at hand. Accordingly, performance could be negatively impacted, as we observe in Table 2. In contrast, our PRINCIPLE-BASED approach abstracts away all these irrelevant details based on analysis across multiple demonstrations and presents only the most salient instructions for LLMs to focus on. It can serve as an alternative to the popular few-shot ICL approach for performing classification tasks, especially when inference costs and input token length are constraints imposed by certain LLMs.\nAdditionally, our multi-agent framework for principle generation is generic and can be applied to any use cases that require synthetic text generation. It can automatically generate highly relevant and knowledge-intensive documents (e.g. SOPs) with only a handful of examples, regardless of availability of labeling resources. Although traditional Retrieval Augmented Generation (RAG) usually performs retrieval of relevant documents from existing data stores, our approach can automatically generate highly relevant documents or SOPs for any tasks. The comparable or even better classification performance of LLMs shown in Table 1 using principles that are LLM-generated in comparison to human-generated counterparts suggests a promising direction to automate SOP generation without compromising on the quality of SOPs generated. As future research, it would be also interesting to see how our PRINCIPLE-BASED approach can be integrated with RAG.\nWhile our principle-based approach provides an effective and efficient ICL solution for text classification under zero-shot settings, we acknowledge several limitations. First, it might not work well for classification problems with many labels since generating principles that cover all classes might lead to very lengthy content to be included as contexts. In this case, we could potentially generate principles for each class individually and use a retriever to fetch corresponding principles for top-k classes before performing downstream classification. Additionally, we only explore open-source models such as FLAN-T5-XXL and FLAN-UL2 as classifier agents due to inference cost constraints. In future work, we would like to investigate whether the same performance gains can be replicated with black-box LLMs such as GPT-4. Lastly, while we mainly focus on zero-shot settings of our principle-based approach, it would also be interesting to explore whether adding concrete examples that are specifically analyzed and explained based on these principles would further improve model performance. We leave these research questions for future work."}]}