{"title": "GTA: A Benchmark for General Tool Agents", "authors": ["Jize Wang", "Zerun Ma", "Yining Li", "Songyang Zhang", "Cailian Chen", "Kai Chen", "Xinyi Le"], "abstract": "Significant focus has been placed on integrating large language models (LLMs) with various tools in developing general-purpose agents. This poses a challenge to LLMs' tool-use capabilities. However, there are evident gaps between existing tool-use evaluations and real-world scenarios. Current evaluations often use AI-generated queries, single-step tasks, dummy tools, and text-only interactions, failing to reveal the agents' real-world problem-solving abilities effectively. To address this, we propose GTA, a benchmark for General Tool Agents, featuring three main aspects: (i) Real user queries: human-written queries with simple real-world objectives but implicit tool-use, requiring the LLM to reason the suitable tools and plan the solution steps. (ii) Real deployed tools: an evaluation platform equipped with tools across perception, operation, logic, and creativity categories to evaluate the agents' actual task execution performance. (iii) Real multimodal inputs: authentic image files, such as spatial scenes, web page screenshots, tables, code snippets, and printed/handwritten materials, used as the query contexts to align with real-world scenarios closely. We design 229 real-world tasks and executable tool chains to evaluate mainstream LLMs. Our findings show that real-world user queries are challenging for existing LLMs, with GPT-4 completing less than 50% of the tasks and most LLMs achieving below 25%. This evaluation reveals the bottlenecks in the tool-use capabilities of current LLMs in real-world scenarios, which provides future direction for advancing general-purpose tool agents.", "sections": [{"title": "1 Introduction", "content": "Integrating tools with large language models (LLMs) has attracted broad research interest as a potential approach towards general AI assistants. Notable works include LangChain [5], AutoGPT [7], and ChatGPT Plugins [18]. These systems decompose workflow into two interactive parts: planning and execution, respectively handled by LLM controllers and callable tools. Solving complex real-world tasks requires multiple types of tools, including perception, operation, logic, and creativity, posing great challenges to LLMs' tool-use proficiency. Consequently, evaluating the models' tool-use capabilities for real-world tasks is crucial for enhancing the effectiveness of agent systems.\nDespite the progress on benchmarking the tool-use capability of LLMs made by recent works, especially on collecting massive APIs and AI-generated user queries to enable scalable testing, there remain noticeable gaps regarding real-world scenarios, as shown in Table 1. First, AI-generated user queries, limited by the generative model, often result in overly brief or monotonous solutions. This is unsuitable for evaluating the reasoning and planning capability of agent systems, as shown in Table 2. Second, existing tool-use benchmarks mainly focus on text-formed user-agent interaction, lacking assessment of multimodal capabilities, thus falling short of aligning with real-world scenarios effectively. Third, existing tool-use evaluation approaches build up virtual tools. They can only evaluate isolated steps in the tool invocation chains, thus unable to reflect the agents' capability to end-to-end accomplish complex tasks.\nTo ensure the evaluation closely reflects real-world scenarios, we consider the authenticity of user queries, tools, and interaction modalities. We propose a comprehensive tool-use evaluation with real-world user queries. The primary features of the evaluation are:\ni. Real user queries. The user queries are designed by humans, rather than generated by AI, to reflect real-world tasks accurately. These queries describe tasks with clear objectives, but the tool-use steps are implicit. Thus, the LLM must employ reasoning to deduce the suitable tools required to address the given tasks. In this way, we avoid the drawbacks of using AI-generated queries in which the tool invocation steps are often explicitly hinted at. Moreover, each query requires multiple steps to resolve, necessitating the model to plan the sequence of tool invocations.\nii. Real deployed tools. We provide an evaluation platform deployed with tools across various categories, such as perception, operation, logic, and creativity. All tools are executable rather than simulated by text description. For each task, a detailed and executable ground truth tool chain is"}, {"title": "3 GTA Benchmark", "content": "In this section, we describe the design and content of GTA. The whole dataset construction pipeline is shown in Figure 2. We first present the composition of each sample in the dataset in Section 3.1. The construction method of queries and tool chains are depicted in Section 3.2 and Section 3.3, respectively. We then present the dataset's statistics in Section 3.4."}, {"title": "3.1 Dataset Formulation", "content": "Given a set of tools \\(T_o = \\{t_i\\}_{i=1}^N\\), a sample in GTA is composed of five parts (F, Q, T, C, A). Among these parts, F is a set of files containing one or two images. Q is a query based on F. It is a real-world scenario based problem of simple form but needs to be solved through multiple steps with tools in T. Which tools need to be used, and in what steps, are not explicitly included in the query. They require reasoning and planning by the LLM, which serves as a central controller. This procedure is given in the reference tool chain \\(C = \\{s_i\\}_{i=1}^m\\). The tool chain contains m steps. Each step is \\(s_i = (t_i, a_i, r_i)\\), where \\(t_i\\) is the tool used in step i. \\(a_i\\) and \\(r_i\\) indicate arguments and return values. \\(T = \\cup_{i=1}^m\\{t_i\\} \\subseteq T_o\\) notes the set of tools involved in this query. A is the final answer yielded by the LLM after reasoning with tools.\nIn our setting, \\(T_o\\) contains 14 tools across four categories, including perception, operation, logic, and creativity. The full list of tools is shown in Figure 1, and more detailed information can be found in Appendix B.1. The queries Q are classified into three types: subjective, objective, and image generation. Examples of the three types of queries are shown in Appendix B.2. For a subjective query \\(Q_s\\), the final answer A is usually some descriptive text. It is not unique, but the general idea is the same. In this case, A contains a list of three reference answers. For an objective query \\(Q_o\\), A is a uniquely determined number or phrase. For an image generation query \\(Q_g\\), we only measure the tool call procedure and do not evaluate the generated image. In this situation, A = 0."}, {"title": "3.2 Query Construction", "content": "To construct (F, Q, T), we first gather human-designed queries that meet three main principles: i) Given \\(T\\subseteq T_o\\), the task (F, Q) can be solved with the capabilities enabled by tools in T. ii) To evaluate LLMs' reasoning and planning abilities, the tool invocation steps should not be explicitly stated in the queries. iii) The queries are meaningful and based on real-world scenarios. Satisfying all the principles simultaneously is challenging. It requires F, Q, and T to match each other in a sensible and logical way. We use a query construction pipeline based on exemplar expansion, as shown in the first part of Figure 2. We first give some initial exemplars with diverse scenarios and tool combinations. Then we instruct annotators to create more queries based on the exemplars.\nExemplar designed by experts. We first design some initial questions as exemplars, which are provided in Appendix C.1. These example questions are of diverse scenarios and contain different tool combinations. Every sample should comprise six components: F (image files), Q (queries), T (involved tools), S (solution steps), A (answers), and E (evidence). Image files F could be obtained from the internet and their URLs must be recorded. F could also be a photo taken or a diagram drawn by the annotators. The query Q needs to avoid obvious references to a specific tool. For example, the query please describe the image for me is unqualified since it obviously refers to the tool ImageDescription. The components S, A, and E, will not appear in the final dataset but are utilized to assist annotators in meeting the annotation requirements. S represents the steps required to solve the problem. Annotators should note down the steps, ensuring their number exceeds two. The answer A of objective queries should be given to guarantee there is a unique answer. To ensure the uniqueness, the answer should not be dependent on the images generated in previous steps. For example, the question what kind of animal is in the picture should not be asked after generate an image of an animal, as the answer is uncertain. For queries utilizing the GoogleSearch tool, & should include the answer's URL and a screenshot pinpointing the answer's location to verify the query's searchability with the tool.\nDiversified expansion by annotators. After the initial exemplars are given, we instruct annotators to create more samples based on each exemplar. We adopt a diversified expansion strategy for the annotators to expand the questions based on the exemplars. The general idea is to keep the tool set T of the template unchanged or slightly modify it. Then annotators brainstorm scenarios different from the template. Further information on the diversified expansion approach is detailed in Appendix C.2. For each sample, we have crafted a manual expansion example to serve as guidance for the annotators. After the expansion process, we perform a quality check and manually filter out the questions that do not satisfy the expansion requirements. The instruction documents for annotators are reported in Appendix C.3."}, {"title": "3.3 Tool Chain Construction", "content": "Based on the (F, Q, T) samples constructed in Section 3.2, we instruct three annotators majoring in computer science to manually construct the corresponding tool chain C and the final answer A. We design a JSON file structure, containing the query-related tool list, image paths, and ReAct [30] style dialog sequences. The dialog sequences include the user query, the executable tool chain, and the final answer. Initially, (T, F, Q) are put into the associated sections for tools, images, and user queries. Subsequently, we deploy all tools in T. The annotators utilize the tools according to the reference steps S and get the outcomes. They record this process in the tool chain section of the dialog sequences, alongside the final answer. Since we do not evaluate the tools' efficacy, when a tool fails to provide accurate recognition for a query (for instance, OCR inaccuracies in text recognition within diagrams), we discard the query. Through the above process, we ensure the feasibility of the questions, the executability of the tool chains, as well as the precision of the final answers. The structure of the tool chain is provided in Appendix C.4."}, {"title": "3.4 Dataset Statistics", "content": "GTA comprises a total of 229 questions, with the basic dataset statistics presented in Table 3. The dataset involves 252 images and 14 distinct tools. It includes 156 objective, 16 subjective, and 57 image-generation queries. The number of tools involved in each question varies from 1 to 4, with most questions using 2 or 3 tools. The steps to resolve the questions range from 2 to 8, with most questions requiring 2 to 4 steps, as depicted in Figure 3(a). The detailed frequency distribution of different tool combinations is listed in Figure 3(b). P, O, L, C are short for Perception, Operation, Logic, Creativity, respectively. Perception+Logic and Perception+Operation are the most frequently appearing tool combination types."}, {"title": "4 Evaluation and Analysis", "content": ""}, {"title": "4.1 Experiment Settings", "content": "We evaluate 16 LLMs on GTA. For API-based models, we select GPT-3.5 [19], GPT-4 [1], GPT-4o, Claude3 [2], and Mistral-large [8]. For open-source models, we select LLaMA3 [14] series, Qwen1.5 [3] series, Mistral [8], Mixtral [9], Yi [31] series, Deepseek [4] series. Experiments are conducted using NVIDIA A100 GPU within OpenCompass [6] evaluation platform. We adopt Lagent [25] as the agent framework. ReAct [30] is used as the tool invocation prompt schema. More experiment information can be found in Appendix D.1 and D.2.\nWe evaluate the models in two modes. Step-by-step mode is designed to evaluate the model's fine-grained tool-use capabilities. In this mode, the model is provided with the initial n steps of the reference tool chain as prompts, with the expectation to predict the action in step n + 1. This method does not involve the actual use of the tool, and the prediction of each step does not depend on the model's preceding outputs. This enables an alignment comparison between the model's output with each step of the ground truth tool chain. End-to-end mode is designed to reflect the tool agent's actual task executing performance. In this mode, the model actually calls the tools and solves the problem by itself. Each step relies on the preceding step's output. We compare the tools selected and the execution result with the ground-truth tool set and the ground-truth result under this mode."}, {"title": "4.2 Evaluation Metrics", "content": "We design fine-grained metrics spanning from the LLM's tool invocation process to execution results. To evaluate the tool invocation process, we devise four metrics under step-by-step mode: InstAcc, ToolAcc, ArgAcc, and SummAcc. InstAcc is instruction-following accuracy, which quantifies the percentage of steps executed without errors. ToolAcc measures the accuracy of tool selection. ArgAcc accesses the accuracy of argument name prediction. SummAcc reflects how accurately the model can summarize the final answers considering all previous tool-use steps. For end-to-end mode, we use AnsAcc to measure the accuracy of the execution result. Besides, we calculate the F1 scores of tool selection in perception, operation, logic, and creativity categories. The four F1 scores compare the model's tool selection with the ground truth tool set, measuring its tool selection ability.\nIn calculating the metric AnsAcc, we exclude image generation queries and focus solely on queries with pure text answers, including objective and subjective queries. For objective queries, the ground truth contains both a whitelist and a blacklist of phrases. An answer is considered correct if it includes all terms from the whitelist and excludes all terms from the blacklist. In the case of subjective queries, the ground truth contains three manually labeled responses from distinct annotators. We compute the cosine similarity (ranging from 0 to 1) between the model's prediction and each of the three ground truth answers, ultimately considering the highest score obtained."}, {"title": "4.3 Main Results", "content": "Real-world tool-use tasks are challenging for existing LLMs. Current LLMs are struggling to accurately invoke tools to solve these real-world tasks. As shown in Table 4, the best-performing models, GPT-4 and GPT-4o can only correctly solve fewer than 50% of the problems, while the rest of the models solve less than 25%. This shows that real-world problems with implicit steps, real tool invocations, and multimodal contextual inputs impose high requirements on the tool-use capabilities of LLMs. Regarding model performance comparisons, API-based models outperform open-source ones. Among open-source models, Qwen-72b has the highest result accuracy. Larger models within the same series perform better than their smaller counterparts, but larger models from different series do not necessarily outperform the smaller ones, as shown in Figure 4. For example, the AnsAcc of LLaMA3-70b is higher than that of LLaMA3-8b, but lower than Qwen-7b. Some prediction examples of different models can be found in Appendix D.3.\nThe four metrics in the step-by-step mode follow the buckets effect. From the results, we observe that the overall performance of the system is affected by the lowest metric. To verify this observation, we calculate the Pearson correlation coefficients between four metrics (InstAcc, ToolAcc, ArgAcc, SummAcc) and AnsAcc, the result is shown in Figure 5. We find that the correlation coefficient for ArgAcc with AnsAcc is the highest. ArgAcc is low for most models, indicating that the four metrics follow the buckets effect. For example, the scores of LLaMA3-70b in InstAcc, ToolAcc, and SummAcc are higher than those of Qwen1.5-14b, but its ArgAcc is lower than Qwen1.5-14b, resulting in a lower final answer accuracy. The scores of GPT-4o in InstAcc and ToolAcc are higher than GPT-4, but its weaker argument prediction capability leads to a lower accuracy rate in the final result. The reason for the buckets effect is that under our evaluation framework, the model needs to follow user instructions, invoke tools multiple times in the correct format, and summarize the answer based on the returned results. Any error in this process can lead to an incorrect conclusion. Currently, argument prediction is the weakest capability for most models, suggesting that to enhance their general tool-use capabilities, researchers can focus on argument prediction capabilities. This concerns both the value and the format correctness of an argument.\nDifferent series of LLMs exhibit distinct behavioral patterns. We count the number of successful and failed tool calls, illustrated in Figure 6. Successful means there are not any errors in the tool call. GPT-4o has the highest number of successful tool calls, while GPT4 has the highest successful tool call rate. We find that models from different series exhibit distinct behavioral tendencies. Yi and Deepseek series tend to be aggressive, leaning towards invoking tools frequently but lacks sufficient instruction-following ability to invoke tools in a correct format. The Qwen series is conservative, preferring to invoke tools less often, yet it has stronger instruction-following capabilities than most other open-source models, resulting in a higher success rate of tool calls. The GPT series is neutral, tending to invoke tools moderately and possessing robust instruction-following abilities, which leads to the highest final answer accuracy. This suggests that to improve the performance of Yi or Deepseek, focus should be given to enhancing their instruction-following ability. Conversely, to enhance the Qwen series, reducing its conservative behavior to tool invocation could be beneficial.\nModels favor either format errors or argument format errors, not both equally. We count the percentage of error types when calling tools, including format error, argument format error, and N/A (other errors, mainly containing the tools' internal error). Most models exhibit a clear tendency toward either format errors or argument format errors, rather than making both types of mistakes in nearly equal numbers. For example, Claude-3's errors are predominantly argument format-related, amounting to 82.86%, while format errors account for a mere 4.29%. This indicates that Claude-3 can follow the tool-call format well, but fails to pass the argument in a correct format."}, {"title": "5 Conclusion", "content": "We propose GTA, a real-world tool-use benchmark for general-purpose agents. The user queries are human-designed, step-implicit, and settled in real-world scenarios. Multimodal contextual inputs are provided. We build an evaluation platform equipped with executable tools in the categories of perception, operation, logic, and creation. Fine-grained metrics are designed for the tool-use capabilities of LLMs in real-world scenarios. We evaluate the tool-use capabilities of 16 LLMs. The evaluation results show that GTA is challenging for current LLMs, with advanced models like GPT-4 struggling with these real-world tasks, completing less than 50% of them. Based on our findings, we give takeaways and further suggestions on tool-use capability improvement for different models. We believe that the GTA benchmark will advance further research in identifying the model's tool-use capabilities and contribute to realizing general-purpose tool agents."}, {"title": "6 Limitations", "content": "Our benchmark lacks language diversity since all queries are in English. Multilingual queries can be added in future work to assess the capability of tool agents in non-English environments. Moreover, to achieve high data quality, both the user queries and the tool chains are human-written. So the cost of a data piece is higher than that of AI-generated counterparts."}, {"title": "A.1 Motivation", "content": "\u2022 For what purpose was the dataset created?\nWe create GTA (a benchmark for General Tool Agents) to evaluate the general tool-use ability of LLMs in real-world scenarios. The benchmark has human-written queries with simple real-world objectives but implicit tool-use, an evaluation platform equipped with executable tools across diverse categories, and authentic image files as context input. These features bridge the gap between existing benchmarks and real-world tool-use scenarios.\n\u2022 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe authors of this paper.\n\u2022 Who funded the creation of the dataset?\nThis project is supported by the National Key R&D Program of China (No. 2022ZD0161600)."}, {"title": "A.2 Composition", "content": "\u2022 What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\nEach instance in GTA is in the JSON format. It contains natural language queries, image file inputs, tool descriptions, a reference tool chain, and a final answer.\n\u2022 How many instances are there in total (of each type, if appropriate)?\nThere are 229 instances in GTA, with 252 image files.\n\u2022 Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\nWe will provide all instances in our GitHub repository for GTA.\n\u2022 What data does each instance consist of?\nEach instance contains a natural language query, image file inputs, tool descriptions, a reference tool chain, and a final answer.\n\u2022 Is there a label or target associated with each instance?\nThe correct tool chain and final answer is provided for each query.\n\u2022 Is any information missing from individual instances?\nNo.\n\u2022 Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\nNo.\n\u2022 Are there recommended data splits (e.g., training, development/validation, testing)?\nThe whole dataset is a test set.\n\u2022 Are there any errors, sources of noise, or redundancies in the dataset?\nThe dataset are created and verified by human. The noise may come from human error in writing.\n\u2022 Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\nThe dataset is self-contained."}, {"title": "A.3 Collection Process", "content": "\u2022 How was the data associated with each instance acquired?\nThe queries are all human designed. The image inputs are collected from the Internet or created by annotators (such as diagrams drawn by annotators).\n\u2022 What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)?\nWe use Google Images to collect image inputs. Queries are written by human.\n\u2022 Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The data are created by researchers and student annotators. The annotators were paid about $ 40 per day.\n\u2022 Over what timeframe was the data collected?\nThe data were constructed in 2023 and 2024.\n\u2022 Were any ethical review processes conducted (e.g., by an institutional review board)?\nYes. All images within GTA are available for academic use. During the collection process, we instruct annotators to document the original URL of each image. Subsequently, we manually review these URLs, eliminating images that are not suitable for academic use. Moreover, should any authors request the removal of their images from GTA, we will promptly comply."}, {"title": "A.4 Preprocessing/cleaning/labeling", "content": "\u2022 Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?\nThe dataset is created by human from scratch, and verified manually.\n\u2022 Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?\nThere is no raw data, since the dataset is created from scratch, rather than a cleaned version of existing data.\n\u2022 Is the software that was used to preprocess/clean/label the data available?\nExcel and VSCode are used for create the data."}, {"title": "A.5 Uses", "content": "\u2022 Has the dataset been used for any tasks already?\nNo.\n\u2022 Is there a repository that links to any or all papers or systems that use the dataset?\nNo.\n\u2022 What (other) tasks could the dataset be used for?\nGTA is used for evaluating the general tool-use ability of LLMs in real-world scenarios."}, {"title": "A.6 Distribution", "content": "\u2022 Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?\nNo.\n\u2022 How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?\nThe dataset will be released at https://github.com/open-compass/GTA.\n\u2022 Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\nThe dataset is released under the Apache License.\n\u2022 Have any third parties imposed IP-based or other restrictions on the data associated with the instances?\nNo.\n\u2022 Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?\nNo."}, {"title": "A.7 Maintenance", "content": "\u2022 Who will be supporting/hosting/maintaining the dataset?\nThe authors of this paper.\n\u2022 How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\nPlease contact with authors through emails in the paper.\n\u2022 Is there an erratum?\nNo.\n\u2022 Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\nYes, users can propose issues and the dataset will be updated on Github.\n\u2022 Will older versions of the dataset continue to be supported/hosted/maintained?\nPrimarily, we plan to maintain only the most recent version of the dataset. However, under certain circumstances, such as significant updates to our dataset or the need for validation of previous research work using older versions, we will exceptionally preserve previous versions of the dataset for up to one year.\n\u2022 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\nContact the authors of the paper."}, {"title": "B.1 Tool Definition", "content": "The detailed definition of 14 tools across perception, operation, logic, and creativity categories are shown in Table 6."}, {"title": "B.2 Examples of Three Query Types", "content": "The examples of objective queries \\(Q_o\\), subjective queries \\(Q_s\\), and image generation queries \\(Q_g\\) are shown in Figure 7 to 12, Figure 13 to 16, and Figure 17 to 21, respectively. We provide the complete data sample, which is in the JSON format, including the involved tools, files, query, tool chain, and the final answer. To facilitate automatic evaluation, we design different final answer format for the three query types. For objective queries, the final answer contains both a whitelist and a blacklist of phrases, as shown in Figure 12. An answer is considered correct if it includes all terms from the whitelist and excludes all terms from the blacklist. In the case of subjective queries, the final answer contains three manually labeled responses from distinct annotators, as shown in Figure 16. We compute the cosine similarity (ranging from 0 to 1) between the model's prediction and each of the three ground truth answers, ultimately considering the highest score obtained. For image"}, {"title": "C.2 Diversified Expansion Approach", "content": "To ensure expansion diversity, we instruct annotators to design new questions according to the diversified expansion approach. Rules of the approach are shown in Figure 37. We also provide an example, shown in Figure 38.\nFor each exemplar, adopt the three following approaches.\nApproach One: Keep the tools in the exemplar unchanged, change the question scenarios and design 6 new samples. These scenarios should be different from each other. An expansion example is provided for each exemplar.\nApproach Two: Replace one of the tools in the exemplar and design questions based on the new involved tool set. Design 2 new samples in this way.\nApproach Three: Increase or decrease the tools in the exemplar and design 2 new samples in this way according to the new involved tool set. The detailed rules are as follows:\ni. If there are 2 tools in the exemplar: add 1 tool and design one sample; add 2 tools and design another sample.\nii. If there are 3 tools in the exemplar: reduce 1 tool and design one sample; increase 1 tool and design another sample.\niii. If there are 4 tools in the exemplar: reduce 1 tool and design one sample; reduce 2 tools and design another sample."}, {"title": "C.3 Instruction for Annotators", "content": "The detailed instruction for annotators during the query construction stage is provided in Figure 39. The instruction during the tool chain construction stage is provided in Figure 40.\nGeneral Goal:\n\u2022 Design questions that require calling tools and go through multiple steps to solve. Each question should be based on one or two image files.\n\u2022 We provide the tool list (B.1) and query exemplars (C.1). Please design more queries according to the rules described in the diversified expansion approach (C.2).\nEach sample should fulfill the following requirements:\n1. Each sample contains 6 parts: F (Image File), Q (Query), T (Tools), S (Steps), A (Answer), E (Evidence).\n2. Image files can be sourced from the web and must be credited with a URL, or they can be created by the annotators themselves (e.g., through photography, drawing, etc.).\n3. Q is the query posed based on the image. T is the tool needed to solve the problem. S is the steps to be taken to solve the problem. A is the answer to the question. The role of E is described in 8.\n4. S needs to contain two or more steps.\n5. Q needs to avoid obvious references to a tool (A counterexample: Please detect the orange. This statement clearly refers to the tool DetectGivenObject).\n6. With regard to answer A, questions that generate text or images do not need to be answered, while the rest of the questions need to ensure that there is a single definitive answer and should not rely on images generated in previous steps. For example, the question what kind of animal is in the picture should not be asked after generate an image of an animal, as the answer is uncertain.\n7. Q and A need to be in English. If there is text in the pictures, it can only be in English.\n8. For questions that need the GoogleSearch tool, the URL and a screenshot containing the answer is required in E. Other questions are not required to provide E.\n9. For questions that need the GoogleSearch tool, it is important to note that the question does need to be solved by searching (e.g., the question is time-sensitive, or it specifies which website to get the information from), rather than being potentially known by the LLM itself. (Counter example: Tsinghua University is located in which city in China? Positive example: What is the QS ranking of Tsinghua University in 2023? Counter example: What is the recipe for Mapo Tofu? Positive example: What is the recipe for Mapo Tofu given on the BBC Good Food website? Counter example: How long is Trump's term in office? Positive example: According to Wikipedia, how long is Trump's term in office?)\n10. Questions that need the GoogleSearch tool are often time-sensitive. We need to ask them in a way that ensures the answers do not change over time. You should ensure that the question can be searched for a unique and definitive answer regardless of the time. To achieve this, you can specify the timeframe, webpage, organization, etc. to be searched for in your question. (Counter example: What is the QS ranking of Tsinghua University? Positive example: What will be the QS ranking of Tsinghua University in 2023?) Please record the URL and a screenshot containing the answer in E."}, {"title": "C.4 Illustration of Executable Tool Chains", "content": "An illustration on each part of the tool chain is shown in Figure 41. It is in the JSON format. It contains the involved tool list, file list, and dialog list. There are three roles in the dialog list: user, assistant, and tool. In the user's dialog, the query content is recorded. In the assistant's dialog, the correct tool call including the tool name and arguments is recorded. In the tool's dialog, the tool's return value is recorded. You can refer to Figure 7 to 12, Figure 13 to 16, and Figure 17 to 21 for JSON-format tool chain examples."}, {"title": "D.1 Build an LLM-Based Agent System", "content": "We build the LLM-based agent system using Lagent 2 framework. It equips an LLM with some action & planning schema, using action executor to let it interact with external tools. To build such an agent system, we should consider three parts: LLM, action & planning schema, and tools. In our experiment, we use ReAct as the action & planning schema. As for tools, we have implemented the 14 tools using AgentLego 3, which is a platform supporting tool serving and remote accessing. When evaluating different LLMs, we replace different LLMs into the Lagent framework, and evaluate this system on the Opencompass 4 evaluation platform."}, {"title": "D.2 ReAct-Style Prompts", "content": "The ReAct-style prompt template using for the agent system is shown in Figure 42. A prompt example is shown in Figure 43.\nCALL_PROTOCOL_EN = \"\"\"You are a assistant who can utilize external tools. {tool_description}\nTo use a tool, please use the following format:\n{\\thought}Think what you need to solve, do you need to use tools?\n{\\action}the tool name, should be one of {action_names}]\n{\\action_input}the input to the action\n\"\"\"\nThe response after utilizing tools should using the following format:\n{\\response}the results after call the tool.\nIf you already know the answer, or you do not need to use tools, please using the following format to reply:\n{\\thought}the thought process to get the final answer\n{\\finish}final answer\nBegin!\"\"\""}, {"title": "D.3 Prediction Examples of Different Models", "content": "In order to visualize the tool-calling capabilities of each model", "Mistral.\nQuery": "json\n{"}, {"title": "GTA: A Benchmark for General Tool Agents", "authors": ["Jize Wang", "Zerun Ma", "Yining Li", "Songyang Zhang", "Cailian Chen", "Kai Chen", "Xinyi Le"], "abstract": "Significant focus has been placed on integrating large language models (LLMs) with various tools in developing general-purpose agents. This poses a challenge to LLMs' tool-use capabilities. However, there are evident gaps between existing tool-use evaluations and real-world scenarios. Current evaluations often use AI-generated queries, single-step tasks, dummy tools, and text-only interactions, failing to reveal the agents' real-world problem-solving abilities effectively. To address this, we propose GTA, a benchmark for General Tool Agents, featuring three main aspects: (i) Real user queries: human-written queries with simple real-world objectives but implicit tool-use, requiring the LLM to reason the suitable tools and plan the solution steps. (ii) Real deployed tools: an evaluation platform equipped with tools across perception, operation, logic, and creativity categories to evaluate the agents' actual task execution performance. (iii) Real multimodal inputs: authentic image files, such as spatial scenes, web page screenshots, tables, code snippets, and printed/handwritten materials, used as the query contexts to align with real-world scenarios closely. We design 229 real-world tasks and executable tool chains to evaluate mainstream LLMs. Our findings show that real-world user queries are challenging for existing LLMs, with GPT-4 completing less than 50% of the tasks and most LLMs achieving below 25%. This evaluation reveals the bottlenecks in the tool-use capabilities of current LLMs in real-world scenarios, which provides future direction for advancing general-purpose tool agents.", "sections": [{"title": "1 Introduction", "content": "Integrating tools with large language models (LLMs) has attracted broad research interest as a potential approach towards general AI assistants. Notable works include LangChain [5], AutoGPT [7], and ChatGPT Plugins [18]. These systems decompose workflow into two interactive parts: planning and execution, respectively handled by LLM controllers and callable tools. Solving complex real-world tasks requires multiple types of tools, including perception, operation, logic, and creativity, posing great challenges to LLMs' tool-use proficiency. Consequently, evaluating the models' tool-use capabilities for real-world tasks is crucial for enhancing the effectiveness of agent systems.\nDespite the progress on benchmarking the tool-use capability of LLMs made by recent works, especially on collecting massive APIs and AI-generated user queries to enable scalable testing, there remain noticeable gaps regarding real-world scenarios, as shown in Table 1. First, AI-generated user queries, limited by the generative model, often result in overly brief or monotonous solutions. This is unsuitable for evaluating the reasoning and planning capability of agent systems, as shown in Table 2. Second, existing tool-use benchmarks mainly focus on text-formed user-agent interaction, lacking assessment of multimodal capabilities, thus falling short of aligning with real-world scenarios effectively. Third, existing tool-use evaluation approaches build up virtual tools. They can only evaluate isolated steps in the tool invocation chains, thus unable to reflect the agents' capability to end-to-end accomplish complex tasks.\nTo ensure the evaluation closely reflects real-world scenarios, we consider the authenticity of user queries, tools, and interaction modalities. We propose a comprehensive tool-use evaluation with real-world user queries. The primary features of the evaluation are:\ni. Real user queries. The user queries are designed by humans, rather than generated by AI, to reflect real-world tasks accurately. These queries describe tasks with clear objectives, but the tool-use steps are implicit. Thus, the LLM must employ reasoning to deduce the suitable tools required to address the given tasks. In this way, we avoid the drawbacks of using AI-generated queries in which the tool invocation steps are often explicitly hinted at. Moreover, each query requires multiple steps to resolve, necessitating the model to plan the sequence of tool invocations.\nii. Real deployed tools. We provide an evaluation platform deployed with tools across various categories, such as perception, operation, logic, and creativity. All tools are executable rather than simulated by text description. For each task, a detailed and executable ground truth tool chain is"}, {"title": "3 GTA Benchmark", "content": "In this section, we describe the design and content of GTA. The whole dataset construction pipeline is shown in Figure 2. We first present the composition of each sample in the dataset in Section 3.1. The construction method of queries and tool chains are depicted in Section 3.2 and Section 3.3, respectively. We then present the dataset's statistics in Section 3.4."}, {"title": "3.1 Dataset Formulation", "content": "Given a set of tools \\(T_o = \\{t_i\\}_{i=1}^N\\), a sample in GTA is composed of five parts (F, Q, T, C, A). Among these parts, F is a set of files containing one or two images. Q is a query based on F. It is a real-world scenario based problem of simple form but needs to be solved through multiple steps with tools in T. Which tools need to be used, and in what steps, are not explicitly included in the query. They require reasoning and planning by the LLM, which serves as a central controller. This procedure is given in the reference tool chain \\(C = \\{s_i\\}_{i=1}^m\\). The tool chain contains m steps. Each step is \\(s_i = (t_i, a_i, r_i)\\), where \\(t_i\\) is the tool used in step i. \\(a_i\\) and \\(r_i\\) indicate arguments and return values. \\(T = \\cup_{i=1}^m\\{t_i\\} \\subseteq T_o\\) notes the set of tools involved in this query. A is the final answer yielded by the LLM after reasoning with tools.\nIn our setting, \\(T_o\\) contains 14 tools across four categories, including perception, operation, logic, and creativity. The full list of tools is shown in Figure 1, and more detailed information can be found in Appendix B.1. The queries Q are classified into three types: subjective, objective, and image generation. Examples of the three types of queries are shown in Appendix B.2. For a subjective query \\(Q_s\\), the final answer A is usually some descriptive text. It is not unique, but the general idea is the same. In this case, A contains a list of three reference answers. For an objective query \\(Q_o\\), A is a uniquely determined number or phrase. For an image generation query \\(Q_g\\), we only measure the tool call procedure and do not evaluate the generated image. In this situation, A = 0."}, {"title": "3.2 Query Construction", "content": "To construct (F, Q, T), we first gather human-designed queries that meet three main principles: i) Given \\(T\\subseteq T_o\\), the task (F, Q) can be solved with the capabilities enabled by tools in T. ii) To evaluate LLMs' reasoning and planning abilities, the tool invocation steps should not be explicitly stated in the queries. iii) The queries are meaningful and based on real-world scenarios. Satisfying all the principles simultaneously is challenging. It requires F, Q, and T to match each other in a sensible and logical way. We use a query construction pipeline based on exemplar expansion, as shown in the first part of Figure 2. We first give some initial exemplars with diverse scenarios and tool combinations. Then we instruct annotators to create more queries based on the exemplars.\nExemplar designed by experts. We first design some initial questions as exemplars, which are provided in Appendix C.1. These example questions are of diverse scenarios and contain different tool combinations. Every sample should comprise six components: F (image files), Q (queries), T (involved tools), S (solution steps), A (answers), and E (evidence). Image files F could be obtained from the internet and their URLs must be recorded. F could also be a photo taken or a diagram drawn by the annotators. The query Q needs to avoid obvious references to a specific tool. For example, the query please describe the image for me is unqualified since it obviously refers to the tool ImageDescription. The components S, A, and E, will not appear in the final dataset but are utilized to assist annotators in meeting the annotation requirements. S represents the steps required to solve the problem. Annotators should note down the steps, ensuring their number exceeds two. The answer A of objective queries should be given to guarantee there is a unique answer. To ensure the uniqueness, the answer should not be dependent on the images generated in previous steps. For example, the question what kind of animal is in the picture should not be asked after generate an image of an animal, as the answer is uncertain. For queries utilizing the GoogleSearch tool, & should include the answer's URL and a screenshot pinpointing the answer's location to verify the query's searchability with the tool.\nDiversified expansion by annotators. After the initial exemplars are given, we instruct annotators to create more samples based on each exemplar. We adopt a diversified expansion strategy for the annotators to expand the questions based on the exemplars. The general idea is to keep the tool set T of the template unchanged or slightly modify it. Then annotators brainstorm scenarios different from the template. Further information on the diversified expansion approach is detailed in Appendix C.2. For each sample, we have crafted a manual expansion example to serve as guidance for the annotators. After the expansion process, we perform a quality check and manually filter out the questions that do not satisfy the expansion requirements. The instruction documents for annotators are reported in Appendix C.3."}, {"title": "3.3 Tool Chain Construction", "content": "Based on the (F, Q, T) samples constructed in Section 3.2, we instruct three annotators majoring in computer science to manually construct the corresponding tool chain C and the final answer A. We design a JSON file structure, containing the query-related tool list, image paths, and ReAct [30] style dialog sequences. The dialog sequences include the user query, the executable tool chain, and the final answer. Initially, (T, F, Q) are put into the associated sections for tools, images, and user queries. Subsequently, we deploy all tools in T. The annotators utilize the tools according to the reference steps S and get the outcomes. They record this process in the tool chain section of the dialog sequences, alongside the final answer. Since we do not evaluate the tools' efficacy, when a tool fails to provide accurate recognition for a query (for instance, OCR inaccuracies in text recognition within diagrams), we discard the query. Through the above process, we ensure the feasibility of the questions, the executability of the tool chains, as well as the precision of the final answers. The structure of the tool chain is provided in Appendix C.4."}, {"title": "3.4 Dataset Statistics", "content": "GTA comprises a total of 229 questions, with the basic dataset statistics presented in Table 3. The dataset involves 252 images and 14 distinct tools. It includes 156 objective, 16 subjective, and 57 image-generation queries. The number of tools involved in each question varies from 1 to 4, with most questions using 2 or 3 tools. The steps to resolve the questions range from 2 to 8, with most questions requiring 2 to 4 steps, as depicted in Figure 3(a). The detailed frequency distribution of different tool combinations is listed in Figure 3(b). P, O, L, C are short for Perception, Operation, Logic, Creativity, respectively. Perception+Logic and Perception+Operation are the most frequently appearing tool combination types."}, {"title": "4 Evaluation and Analysis", "content": ""}, {"title": "4.1 Experiment Settings", "content": "We evaluate 16 LLMs on GTA. For API-based models, we select GPT-3.5 [19], GPT-4 [1], GPT-4o, Claude3 [2], and Mistral-large [8]. For open-source models, we select LLaMA3 [14] series, Qwen1.5 [3] series, Mistral [8], Mixtral [9], Yi [31] series, Deepseek [4] series. Experiments are conducted using NVIDIA A100 GPU within OpenCompass [6] evaluation platform. We adopt Lagent [25] as the agent framework. ReAct [30] is used as the tool invocation prompt schema. More experiment information can be found in Appendix D.1 and D.2.\nWe evaluate the models in two modes. Step-by-step mode is designed to evaluate the model's fine-grained tool-use capabilities. In this mode, the model is provided with the initial n steps of the reference tool chain as prompts, with the expectation to predict the action in step n + 1. This method does not involve the actual use of the tool, and the prediction of each step does not depend on the model's preceding outputs. This enables an alignment comparison between the model's output with each step of the ground truth tool chain. End-to-end mode is designed to reflect the tool agent's actual task executing performance. In this mode, the model actually calls the tools and solves the problem by itself. Each step relies on the preceding step's output. We compare the tools selected and the execution result with the ground-truth tool set and the ground-truth result under this mode."}, {"title": "4.2 Evaluation Metrics", "content": "We design fine-grained metrics spanning from the LLM's tool invocation process to execution results. To evaluate the tool invocation process, we devise four metrics under step-by-step mode: InstAcc, ToolAcc, ArgAcc, and SummAcc. InstAcc is instruction-following accuracy, which quantifies the percentage of steps executed without errors. ToolAcc measures the accuracy of tool selection. ArgAcc accesses the accuracy of argument name prediction. SummAcc reflects how accurately the model can summarize the final answers considering all previous tool-use steps. For end-to-end mode, we use AnsAcc to measure the accuracy of the execution result. Besides, we calculate the F1 scores of tool selection in perception, operation, logic, and creativity categories. The four F1 scores compare the model's tool selection with the ground truth tool set, measuring its tool selection ability.\nIn calculating the metric AnsAcc, we exclude image generation queries and focus solely on queries with pure text answers, including objective and subjective queries. For objective queries, the ground truth contains both a whitelist and a blacklist of phrases. An answer is considered correct if it includes all terms from the whitelist and excludes all terms from the blacklist. In the case of subjective queries, the ground truth contains three manually labeled responses from distinct annotators. We compute the cosine similarity (ranging from 0 to 1) between the model's prediction and each of the three ground truth answers, ultimately considering the highest score obtained."}, {"title": "4.3 Main Results", "content": "Real-world tool-use tasks are challenging for existing LLMs. Current LLMs are struggling to accurately invoke tools to solve these real-world tasks. As shown in Table 4, the best-performing models, GPT-4 and GPT-4o can only correctly solve fewer than 50% of the problems, while the rest of the models solve less than 25%. This shows that real-world problems with implicit steps, real tool invocations, and multimodal contextual inputs impose high requirements on the tool-use capabilities of LLMs. Regarding model performance comparisons, API-based models outperform open-source ones. Among open-source models, Qwen-72b has the highest result accuracy. Larger models within the same series perform better than their smaller counterparts, but larger models from different series do not necessarily outperform the smaller ones, as shown in Figure 4. For example, the AnsAcc of LLaMA3-70b is higher than that of LLaMA3-8b, but lower than Qwen-7b. Some prediction examples of different models can be found in Appendix D.3.\nThe four metrics in the step-by-step mode follow the buckets effect. From the results, we observe that the overall performance of the system is affected by the lowest metric. To verify this observation, we calculate the Pearson correlation coefficients between four metrics (InstAcc, ToolAcc, ArgAcc, SummAcc) and AnsAcc, the result is shown in Figure 5. We find that the correlation coefficient for ArgAcc with AnsAcc is the highest. ArgAcc is low for most models, indicating that the four metrics follow the buckets effect. For example, the scores of LLaMA3-70b in InstAcc, ToolAcc, and SummAcc are higher than those of Qwen1.5-14b, but its ArgAcc is lower than Qwen1.5-14b, resulting in a lower final answer accuracy. The scores of GPT-4o in InstAcc and ToolAcc are higher than GPT-4, but its weaker argument prediction capability leads to a lower accuracy rate in the final result. The reason for the buckets effect is that under our evaluation framework, the model needs to follow user instructions, invoke tools multiple times in the correct format, and summarize the answer based on the returned results. Any error in this process can lead to an incorrect conclusion. Currently, argument prediction is the weakest capability for most models, suggesting that to enhance their general tool-use capabilities, researchers can focus on argument prediction capabilities. This concerns both the value and the format correctness of an argument.\nDifferent series of LLMs exhibit distinct behavioral patterns. We count the number of successful and failed tool calls, illustrated in Figure 6. Successful means there are not any errors in the tool call. GPT-4o has the highest number of successful tool calls, while GPT4 has the highest successful tool call rate. We find that models from different series exhibit distinct behavioral tendencies. Yi and Deepseek series tend to be aggressive, leaning towards invoking tools frequently but lacks sufficient instruction-following ability to invoke tools in a correct format. The Qwen series is conservative, preferring to invoke tools less often, yet it has stronger instruction-following capabilities than most other open-source models, resulting in a higher success rate of tool calls. The GPT series is neutral, tending to invoke tools moderately and possessing robust instruction-following abilities, which leads to the highest final answer accuracy. This suggests that to improve the performance of Yi or Deepseek, focus should be given to enhancing their instruction-following ability. Conversely, to enhance the Qwen series, reducing its conservative behavior to tool invocation could be beneficial.\nModels favor either format errors or argument format errors, not both equally. We count the percentage of error types when calling tools, including format error, argument format error, and N/A (other errors, mainly containing the tools' internal error). Most models exhibit a clear tendency toward either format errors or argument format errors, rather than making both types of mistakes in nearly equal numbers. For example, Claude-3's errors are predominantly argument format-related, amounting to 82.86%, while format errors account for a mere 4.29%. This indicates that Claude-3 can follow the tool-call format well, but fails to pass the argument in a correct format."}, {"title": "5 Conclusion", "content": "We propose GTA, a real-world tool-use benchmark for general-purpose agents. The user queries are human-designed, step-implicit, and settled in real-world scenarios. Multimodal contextual inputs are provided. We build an evaluation platform equipped with executable tools in the categories of perception, operation, logic, and creation. Fine-grained metrics are designed for the tool-use capabilities of LLMs in real-world scenarios. We evaluate the tool-use capabilities of 16 LLMs. The evaluation results show that GTA is challenging for current LLMs, with advanced models like GPT-4 struggling with these real-world tasks, completing less than 50% of them. Based on our findings, we give takeaways and further suggestions on tool-use capability improvement for different models. We believe that the GTA benchmark will advance further research in identifying the model's tool-use capabilities and contribute to realizing general-purpose tool agents."}, {"title": "6 Limitations", "content": "Our benchmark lacks language diversity since all queries are in English. Multilingual queries can be added in future work to assess the capability of tool agents in non-English environments. Moreover, to achieve high data quality, both the user queries and the tool chains are human-written. So the cost of a data piece is higher than that of AI-generated counterparts."}, {"title": "A.1 Motivation", "content": "\u2022 For what purpose was the dataset created?\nWe create GTA (a benchmark for General Tool Agents) to evaluate the general tool-use ability of LLMs in real-world scenarios. The benchmark has human-written queries with simple real-world objectives but implicit tool-use, an evaluation platform equipped with executable tools across diverse categories, and authentic image files as context input. These features bridge the gap between existing benchmarks and real-world tool-use scenarios.\n\u2022 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe authors of this paper.\n\u2022 Who funded the creation of the dataset?\nThis project is supported by the National Key R&D Program of China (No. 2022ZD0161600)."}, {"title": "A.2 Composition", "content": "\u2022 What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\nEach instance in GTA is in the JSON format. It contains natural language queries, image file inputs, tool descriptions, a reference tool chain, and a final answer.\n\u2022 How many instances are there in total (of each type, if appropriate)?\nThere are 229 instances in GTA, with 252 image files.\n\u2022 Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\nWe will provide all instances in our GitHub repository for GTA.\n\u2022 What data does each instance consist of?\nEach instance contains a natural language query, image file inputs, tool descriptions, a reference tool chain, and a final answer.\n\u2022 Is there a label or target associated with each instance?\nThe correct tool chain and final answer is provided for each query.\n\u2022 Is any information missing from individual instances?\nNo.\n\u2022 Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\nNo.\n\u2022 Are there recommended data splits (e.g., training, development/validation, testing)?\nThe whole dataset is a test set.\n\u2022 Are there any errors, sources of noise, or redundancies in the dataset?\nThe dataset are created and verified by human. The noise may come from human error in writing.\n\u2022 Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\nThe dataset is self-contained."}, {"title": "A.3 Collection Process", "content": "\u2022 How was the data associated with each instance acquired?\nThe queries are all human designed. The image inputs are collected from the Internet or created by annotators (such as diagrams drawn by annotators).\n\u2022 What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)?\nWe use Google Images to collect image inputs. Queries are written by human.\n\u2022 Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The data are created by researchers and student annotators. The annotators were paid about $ 40 per day.\n\u2022 Over what timeframe was the data collected?\nThe data were constructed in 2023 and 2024.\n\u2022 Were any ethical review processes conducted (e.g., by an institutional review board)?\nYes. All images within GTA are available for academic use. During the collection process, we instruct annotators to document the original URL of each image. Subsequently, we manually review these URLs, eliminating images that are not suitable for academic use. Moreover, should any authors request the removal of their images from GTA, we will promptly comply."}, {"title": "A.4 Preprocessing/cleaning/labeling", "content": "\u2022 Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?\nThe dataset is created by human from scratch, and verified manually.\n\u2022 Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?\nThere is no raw data, since the dataset is created from scratch, rather than a cleaned version of existing data.\n\u2022 Is the software that was used to preprocess/clean/label the data available?\nExcel and VSCode are used for create the data."}, {"title": "A.5 Uses", "content": "\u2022 Has the dataset been used for any tasks already?\nNo.\n\u2022 Is there a repository that links to any or all papers or systems that use the dataset?\nNo.\n\u2022 What (other) tasks could the dataset be used for?\nGTA is used for evaluating the general tool-use ability of LLMs in real-world scenarios."}, {"title": "A.6 Distribution", "content": "\u2022 Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?\nNo.\n\u2022 How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?\nThe dataset will be released at https://github.com/open-compass/GTA.\n\u2022 Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\nThe dataset is released under the Apache License.\n\u2022 Have any third parties imposed IP-based or other restrictions on the data associated with the instances?\nNo.\n\u2022 Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?\nNo."}, {"title": "A.7 Maintenance", "content": "\u2022 Who will be supporting/hosting/maintaining the dataset?\nThe authors of this paper.\n\u2022 How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\nPlease contact with authors through emails in the paper.\n\u2022 Is there an erratum?\nNo.\n\u2022 Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\nYes, users can propose issues and the dataset will be updated on Github.\n\u2022 Will older versions of the dataset continue to be supported/hosted/maintained?\nPrimarily, we plan to maintain only the most recent version of the dataset. However, under certain circumstances, such as significant updates to our dataset or the need for validation of previous research work using older versions, we will exceptionally preserve previous versions of the dataset for up to one year.\n\u2022 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\nContact the authors of the paper."}, {"title": "B.1 Tool Definition", "content": "The detailed definition of 14 tools across perception, operation, logic, and creativity categories are shown in Table 6."}, {"title": "B.2 Examples of Three Query Types", "content": "The examples of objective queries \\(Q_o\\), subjective queries \\(Q_s\\), and image generation queries \\(Q_g\\) are shown in Figure 7 to 12, Figure 13 to 16, and Figure 17 to 21, respectively. We provide the complete data sample, which is in the JSON format, including the involved tools, files, query, tool chain, and the final answer. To facilitate automatic evaluation, we design different final answer format for the three query types. For objective queries, the final answer contains both a whitelist and a blacklist of phrases, as shown in Figure 12. An answer is considered correct if it includes all terms from the whitelist and excludes all terms from the blacklist. In the case of subjective queries, the final answer contains three manually labeled responses from distinct annotators, as shown in Figure 16. We compute the cosine similarity (ranging from 0 to 1) between the model's prediction and each of the three ground truth answers, ultimately considering the highest score obtained. For image"}, {"title": "C.2 Diversified Expansion Approach", "content": "To ensure expansion diversity, we instruct annotators to design new questions according to the diversified expansion approach. Rules of the approach are shown in Figure 37. We also provide an example, shown in Figure 38.\nFor each exemplar, adopt the three following approaches.\nApproach One: Keep the tools in the exemplar unchanged, change the question scenarios and design 6 new samples. These scenarios should be different from each other. An expansion example is provided for each exemplar.\nApproach Two: Replace one of the tools in the exemplar and design questions based on the new involved tool set. Design 2 new samples in this way.\nApproach Three: Increase or decrease the tools in the exemplar and design 2 new samples in this way according to the new involved tool set. The detailed rules are as follows:\ni. If there are 2 tools in the exemplar: add 1 tool and design one sample; add 2 tools and design another sample.\nii. If there are 3 tools in the exemplar: reduce 1 tool and design one sample; increase 1 tool and design another sample.\niii. If there are 4 tools in the exemplar: reduce 1 tool and design one sample; reduce 2 tools and design another sample."}, {"title": "C.3 Instruction for Annotators", "content": "The detailed instruction for annotators during the query construction stage is provided in Figure 39. The instruction during the tool chain construction stage is provided in Figure 40.\nGeneral Goal:\n\u2022 Design questions that require calling tools and go through multiple steps to solve. Each question should be based on one or two image files.\n\u2022 We provide the tool list (B.1) and query exemplars (C.1). Please design more queries according to the rules described in the diversified expansion approach (C.2).\nEach sample should fulfill the following requirements:\n1. Each sample contains 6 parts: F (Image File), Q (Query), T (Tools), S (Steps), A (Answer), E (Evidence).\n2. Image files can be sourced from the web and must be credited with a URL, or they can be created by the annotators themselves (e.g., through photography, drawing, etc.).\n3. Q is the query posed based on the image. T is the tool needed to solve the problem. S is the steps to be taken to solve the problem. A is the answer to the question. The role of E is described in 8.\n4. S needs to contain two or more steps.\n5. Q needs to avoid obvious references to a tool (A counterexample: Please detect the orange. This statement clearly refers to the tool DetectGivenObject).\n6. With regard to answer A, questions that generate text or images do not need to be answered, while the rest of the questions need to ensure that there is a single definitive answer and should not rely on images generated in previous steps. For example, the question what kind of animal is in the picture should not be asked after generate an image of an animal, as the answer is uncertain.\n7. Q and A need to be in English. If there is text in the pictures, it can only be in English.\n8. For questions that need the GoogleSearch tool, the URL and a screenshot containing the answer is required in E. Other questions are not required to provide E.\n9. For questions that need the GoogleSearch tool, it is important to note that the question does need to be solved by searching (e.g., the question is time-sensitive, or it specifies which website to get the information from), rather than being potentially known by the LLM itself. (Counter example: Tsinghua University is located in which city in China? Positive example: What is the QS ranking of Tsinghua University in 2023? Counter example: What is the recipe for Mapo Tofu? Positive example: What is the recipe for Mapo Tofu given on the BBC Good Food website? Counter example: How long is Trump's term in office? Positive example: According to Wikipedia, how long is Trump's term in office?)\n10. Questions that need the GoogleSearch tool are often time-sensitive. We need to ask them in a way that ensures the answers do not change over time. You should ensure that the question can be searched for a unique and definitive answer regardless of the time. To achieve this, you can specify the timeframe, webpage, organization, etc. to be searched for in your question. (Counter example: What is the QS ranking of Tsinghua University? Positive example: What will be the QS ranking of Tsinghua University in 2023?) Please record the URL and a screenshot containing the answer in E."}, {"title": "C.4 Illustration of Executable Tool Chains", "content": "An illustration on each part of the tool chain is shown in Figure 41. It is in the JSON format. It contains the involved tool list, file list, and dialog list. There are three roles in the dialog list: user, assistant, and tool. In the user's dialog, the query content is recorded. In the assistant's dialog, the correct tool call including the tool name and arguments is recorded. In the tool's dialog, the tool's return value is recorded. You can refer to Figure 7 to 12, Figure 13 to 16, and Figure 17 to 21 for JSON-format tool chain examples."}, {"title": "D.1 Build an LLM-Based Agent System", "content": "We build the LLM-based agent system using Lagent 2 framework. It equips an LLM with some action & planning schema, using action executor to let it interact with external tools. To build such an agent system, we should consider three parts: LLM, action & planning schema, and tools. In our experiment, we use ReAct as the action & planning schema. As for tools, we have implemented the 14 tools using AgentLego 3, which is a platform supporting tool serving and remote accessing. When evaluating different LLMs, we replace different LLMs into the Lagent framework, and evaluate this system on the Opencompass 4 evaluation platform."}, {"title": "D.2 ReAct-Style Prompts", "content": "The ReAct-style prompt template using for the agent system is shown in Figure 42. A prompt example is shown in Figure 43.\nCALL_PROTOCOL_EN = \"\"\"You are a assistant who can utilize external tools. {tool_description}\nTo use a tool, please use the following format:\n{\\thought}Think what you need to solve, do you need to use tools?\n{\\action}the tool name, should be one of {action_names}]\n{\\action_input}the input to the action\n\"\"\"\nThe response after utilizing tools should using the following format:\n{\\response}the results after call the tool.\nIf you already know the answer, or you do not need to use tools, please using the following format to reply:\n{\\thought}the thought process to get the final answer\n{\\finish}final answer\nBegin!\"\"\""}, {"title": "D.3 Prediction Examples of Different Models", "content": "In order to visualize the tool-calling capabilities of each model", "Mistral.\nQuery": ""}]}]}