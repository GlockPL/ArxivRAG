{"title": "Deep reinforcement learning with time-scale invariant memory", "authors": ["Md Rysul Kabir", "James Mochizuki-Freeman", "Zoran Tiganj"], "abstract": "The ability to estimate temporal relationships is critical for\nboth animals and artificial agents. Cognitive science and neu-\nroscience provide remarkable insights into behavioral and\nneural aspects of temporal credit assignment. In particular,\nscale invariance of learning dynamics, observed in behav-\nior and supported by neural data, is one of the key princi-\nples that governs animal perception: proportional rescaling\nof temporal relationships does not alter the overall learning\nefficiency. Here we integrate a computational neuroscience\nmodel of scale invariant memory into deep reinforcement\nlearning (RL) agents. We first provide a theoretical analysis\nand then demonstrate through experiments that such agents\ncan learn robustly across a wide range of temporal scales, un-\nlike agents built with commonly used recurrent memory ar-\nchitectures such as LSTM. This result illustrates that incorpo-\nrating computational principles from neuroscience and cog-\nnitive science into deep neural networks can enhance adapt-\nability to complex temporal dynamics, mirroring some of the\ncore properties of human learning.", "sections": [{"title": "Introduction", "content": "Learning temporal relationships between cause and effect is\ncritical for successfully obtaining rewards and avoiding pun-\nishments in a natural environment. Humans and many other\nanimals can estimate the temporal duration of events and use\nthat estimate as an integral component of decision-making.\nFurthermore, the ability to do this rapidly and flexibly across\na wide range of temporal scales has been critical for sur-\nvival. While machine learning systems also possess the ca-\npacity to represent the elapsed time, typically via recurrent\nconnections, they often struggle with learning temporal rela-\ntionships, especially when those are extended over multiple\nscales.\nThe mammalian ability to estimate time and learn exhibits\nscale invariance across a wide range of temporal scales,\nspanning from seconds to several minutes (Buhusi and Meck\n2005; Gibbon 1977; Buhusi et al. 2009; Balci and Free-\nstone 2020; Gallistel and Gibbon 2000; Balsam and Gal-\nlistel 2009; Gallistel and Shahan 2024). A scale invariant\nsystem has a linear relationship between the mean esti-\nmated time and the actual time, with a constant coefficient\nof variation. In other words, the relative uncertainty or er-\nror in time estimation remains constant as the duration in-\ncreases. This is known as the Weber-Fechner law (Fechner\n1860/1912), which states that the just noticeable difference\nbetween two stimuli is proportional to the magnitude of the\nstimuli. Hence the ratio between these two quantities is con-\nstant, implying that the perceived magnitude is on a log-\narithmic scale. This law is foundational for understanding\nmammalian perceptions and spans virtually all perceptual\ndomains except angles (Gibbon 1977; Wilkes 2015). Scale\ninvariance in learning is demonstrated in classical condition-\ning where animals observe a salient stimulus followed by a\nreward. When the temporal distance between stimulus and\nreward is rescaled by the same factor as between two stim-\nuli, the number of trials the animal needs to learn the value\nof the stimulus remains unchanged. While previous studies\nhave demonstrated this effect on the order of about a minute\n(Gallistel and Gibbon 2000; Balsam and Gallistel 2009), re-\ncent work has demonstrated that it holds for at least 16 min-\nutes (Gallistel and Shahan 2024). Contrary to biological or-\nganisms, the performance of machine learning systems is\ntypically not scale invariant. Machine learning systems tend\nto perform well only at a limited set of scales and require\nadjustments of hyperparameters such as learning rate, tem-\nporal resolution and temporal discounting to learn problems\nat different scales.\nA number of neuroscience studies have investigated the\nneural underpinning of temporal representations in tasks\nsuch as interval timing and temporal bisection (Emmons\net al. 2017; Matell and Meck 2004; Kim et al. 2013,\n2017; Parker et al. 2014; Mello, Soares, and Paton 2015;\nNarayanan 2016; Gouv\u00eaa et al. 2015; Tiganj et al. 2017;\nDonnelly et al. 2015). Neural activity observed in several\nbrain regions, including the hippocampus, prefrontal cor-\ntex and striatum, was often characterized by one of two\ntraits: (1) ramping/decaying activity, where neurons mono-\ntonically increase/decrease their firing rate as a function of\ntime following some salient stimulus (such as the onset of\nthe interval timing cue); and (2) sequential activity, where\nneurons activate sequentially following some salient stimu-\nlus, with each neuron elevating its firing rate for a distinct\nperiod of time. These sequentially activated neurons, also\nknown as time cells (as a temporal analog of place cells\n(O'Keefe 1976)) have been observed in the hippocampus"}, {"title": "", "content": "and the prefrontal cortex in several studies over the recent\nyears (Pastalkova et al. 2008; MacDonald et al. 2011; Salz\net al. 2016; Cruzado et al. 2020; Eichenbaum 2014; \u041c\u0430\u0441-\nDonald and Tonegawa 2021). Sequential neural activity is\nalso scale invariant: the width of the temporal windows in\ntime cells increases with the peak time and the ratio of the\npeak times of the adjacent cells is constant (geometric pro-\ngression), indicating uniform spacing along a logarithmic\naxis (Cao et al. 2022).\nIn this study, we draw on insights from neuroscience and\ncognitive science to enable more flexible learning in deep\nreinforcement learning (RL) agents by incorporating scale\ninvariant temporal memory. The memory model is based\non previous work in computational neuroscience (Shankar\nand Howard 2012, 2013), which has been used to explain a\nbroad range of phenomena in neuroscience (Howard et al.\n2014), including the emergence of time and place cells, as\nwell as findings from cognitive psychology memory experi-\nments, such as free recall and judgment of recency (Howard\net al. 2015; Tiganj, Tang, and Howard 2021). We integrate\nscale invariant memory into RL agents and evaluate their\nperformance across tasks designed to span a wide range of\ntemporal scales. We compare the performance and neural ac-\ntivity of proposed agents to agents built with standard mem-\nory architectures, such as LSTM (Hochreiter and Schmid-\nhuber 1997) and simple RNN. Through theoretical analysis\nand experiments, we demonstrate that agents with scale in-\nvariant memory effectively generalize and maintain strong\nperformance across rescaled temporal relationships."}, {"title": "Prior work", "content": "Previous computational work proposed that ramping/decay-\ning activity and time cells provide a representation essen-\ntial for timing and learning temporal relationships (Balci and\nSimen 2016; Howard et al. 2014). This has also been studied\nin the RL context (Petter, Gershman, and Meck 2018; Nam-\nboodiri 2022; Ludvig, Sutton, and Kehoe 2008). A num-\nber of computational neuroscience studies have closely ex-\namined and attempted to model neural activity during in-\nterval timing and similar time-related tasks (Grossberg and\nSchmajuk 1989; Wang et al. 2018; Jazayeri and Shadlen\n2015; Cueva et al. 2020; P\u00e9rez and Merchant 2018; Raphan,\nDorokhin, and Delamater 2019). Together, these models\nprovide remarkable insights into neural mechanisms of tem-\nporal learning from computational neuroscience and cogni-\ntive science perspectives. Our efforts focus on examining\ntemporal learning in artificial RL agents trained using error\nbackpropagation. We believe this effort complements com-\nputational neuroscience efforts for building neural models\nof time-scale invariant learning since we specifically evalu-\nate agents based on a computational neuroscience model of\nmemory.\nDeverett et al. (2019) studied interval reproduction in\ndeep RL agents and provided valuable insights about the ca-\npabilities of these agents. Our work extends this approach, as\nwe closely examine the neural activity of the artificial agents\nand conduct experiments at different temporal scales. Previ-\nous work has also shown that under particular circumstances\n(mnemonic demand), neural activity in deep RL agents re-\nsembles the activity of time cells (Lin and Richards 2021).\nOther types of brain-like neural activity have also been ob-\nserved in deep learning systems (Sorscher et al. 2019; Scha-\neffer, Khona, and Fiete 2022), which in some cases led to\nimproved performance of deep RL agents (Banino et al.\n2018). Furthermore, recent computational models of hip-\npocampal activity have been linked to modern deep learn-\ning architectures, such as transformers (Whittington, War-\nren, and Behrens 2021). Our work complements these ef-\nforts in brain-inspired AI by comparing neural activity in\nbiological and artificial systems and evaluating whether sys-\ntems whose activity is closer to biological counterparts in-\ndeed perform better.\nNeural networks that use temporal convolution to con-\nstruct an offline version of scale invariant memory have been\nproposed in Jacques et al. (2021, 2022), but these have not\nbeen explored in the context of online temporal learning\nor deep RL. An approach similar to ours has been used to\ndevelop systems with scale invariant temporal discounting\n(Tano, Dayan, and Pouget 2020; Masset et al. 2023; Mo-\nmennejad and Howard 2018; Tiganj et al. 2019). A simi-\nlar approach has also been used to build systems that can\nlearn to represent variables such as numerosity and position\n(Maini et al. 2023; Mochizuki-Freeman, Kabir, and Tiganj\n2024). These approaches are complementary to ours since\nwe focus on scale invariance in temporal memory of deep\nRL agents."}, {"title": "Environments", "content": "The interval timing environment was inspired by a neuro-\nscience study where rats had to discriminate between two\ngroups of intervals, long and short, each having three possi-\nble durations (Kim et al. 2013) (Fig. 1A). The interval be-\ngins when the agent crosses the start line (indicated by the\nred line in Fig. 1B). The duration of the interval is randomly\nchosen from six equally probable durations. After the end of\nthe interval period, the central bridge of the T-maze drops,\nand the agent freely navigates towards the end of the track.\nWhen the agent reaches the end, it has to choose one of the\ngoal locations (indicated by the blue lines in Fig. 1B) de-\npending on whether the interval was from the long or short\ngroup of intervals. We used two versions of this environ-\nment: one with 3D realistic visual observations developed\nusing the open-source PyBullet physics engine (Coumans\nand Bai 2021), and the other that consists of a 1D observa-\ntion space. The purpose of the simple 1D environment was\nto facilitate interpretability of the results, while the 3D real-\nistic environment was used to evaluate the scalability of the\nproposed approach.\nIn the 3D environment, at each time step, the agent re-\nceives a three-dimensional pixel observation of shape 60 \u00d7\n60 \u00d7 3 and performs any of the three possible actions: left,\nright and forward. The forward action moves the agent\nstraight towards the end of the track at a fixed speed of\n1cm/step. Each left and right action realigns the agent in\n7.5\u00b0 increments to a max/min of \u00b115\u00b0 along the straight"}, {"title": "", "content": "portion of the T-maze and without a bound at the end. Once\nthe agent reaches the goal positions, it receives a reward of\neither 1.0 for a correct or 0 for an incorrect choice. It also\nreceives a reward of -0.1 if it tries to turn beyond 15\u00b0 in\neither direction when on the straight track, or if it collides\nwith the back wall of the environment.\nIn the simple version of the environment (top row in\nFig. 3B), the observation space is one-dimensional such that\na \u03b4 pulse is introduced after the fixation period to signal the\nbeginning of an interval, followed by a second \u03b4 pulse that\nmarks the end of the interval. The interval is followed by a\ndelay period, after which the agent chooses either a \"left\"\nor \"right\" action, similar to the 3D environment. In this and\nsubsequent environments, the agent receives a reward of 1.0\nfor a correct choice and -1.0 for an incorrect choice.\nInterval discrimination\nThis simple environment was based on a duration discrimi-\nnation task (Genovesio, Tsujimoto, and Wise 2009). In each\ntrial, two stimuli are presented sequentially with a delay pe-\nriod separating them. The agent is required to determine\nwhich of the two stimuli has a longer duration. The obser-\nvation space is one-dimensional. Consistent with the 1D in-\nterval timing task, the onset and termination of each interval\nare signaled by a \u03b4 pulse.\nDelayed-match-to-sample\nWe used an existing delayed-match-to-sample environment\nfrom NeuroGym (Molano-Mazon et al. 2022). This environ-\nment was inspired by a common memory task (Miller, Er-\nickson, and Desimone 1996) in which a sample stimulus\nis followed by a delay period and a test stimulus. To re-\nceive a reward, the participant needs to determine whether\nthe test stimulus matches the sample stimulus. The observa-\ntion space for this environment is a vector comprising three\nfeatures.\nInterval reproduction\nThis simple environment is inspired by the interval repro-\nduction task described by Deverett et al. (2019). In each trial,\ntwo stimuli are presented sequentially, separated by an inter-\nval that the agent must later reproduce. During the reproduc-\ntion phase, which occurs after the second stimulus, the agent\nmust perform an action to replicate the interval within a 20%\ntolerance to receive a reward. In line with the other 1D tasks,\nthe onset and termination of the interval are marked by a \u03b4\npulse."}, {"title": "Model", "content": "Our recurrent deep learning architecture consists of three\nmajor parts: encoder (used only for the 3D interval\ntiming environment), core, and agent. The encoder consists\nof three consecutive convolutional layers. The resulting out-\nput from the last convolutional layer is flattened and passed\nthrough a fully connected layer. This encoded representation\nis then fed into the core, the recurrent memory of our ar-\nchitecture. We used three different kinds of recurrent cores:\nRNN, LSTM, and the cognitively inspired RNN, described\nin detail below, which we abbreviate as CogRNN. Lastly, the\noutput from the core is passed through two attention layers\n(for the 3D interval timing environment) or a dense layer (for\nother environments unless specified otherwise). This output\nis then fed into the agent part of the network, which has two\nbranches: (i) policy network and (ii) value network. At every\ntime step, the agent chooses an action depending on the out-\nput of the policy network. Outputs from the policy and value\nnetworks are used to calculate losses, which are then used in\nbackpropagation for gradient-based parameter updates."}, {"title": "Scale invariant memory network", "content": "Building on models from computational and cognitive neu-\nroscience (Shankar and Howard 2012; Howard et al. 2014),\nwe designed a neural network architecture that maintains a\nscale invariant memory. Specifically, this network constructs\nan approximation of a real-domain Laplace transform of the\ntemporal history of the input signal and then constructs an\napproximate inverse of the history \u2013 giving rise to an inter-\nnal timeline of the past along a log-compressed axis charac-\nterized by sequentially activated neurons (Fig. 3A).\nTo construct a scale invariant memory we use a network\ncomposed of two layers. The input coming from the en-\ncoder, which we label as f, is fed into a recurrent layer (F)\nwith the weights analytically computed to approximate the\nreal-domain Laplace transform of the temporal history of the\ninput. The output of the recurrent layer is mapped through a\nlinear layer with analytically computed weights implement-\ning the inverse Laplace transform f (Fig. 3A). Below we de-\nscribe this procedure step by step, first the continuous-time\nformulation and then discrete, neural network implementa-\ntion.\nContinuous-time formulation Given a one-dimensional\ninput signal f(t), we define a modified version of the\nLaplace transform F(s; t):\n$F(s;t) = \\int_0^t e^{-s(t-t')} f(t')dt'.$\nThis modified version differs from the standard Laplace\ntransform only in the variable s. Instead of s being a com-\nplex value composed of real and imaginary parts, we re-\nstricts to a positive real value. This modification sim-\nplifies the neural network implementation while giving us\nthe computational benefits of the standard Laplace trans-\nform, as illustrated below. Note that F is also a function\nof time t. This implies that at every moment, we construct\nthe Laplace transform of the input function up to time t:\n$f(0 \\leq t' < t) \\hookrightarrow F(s;t)$.\nTo construct the temporal history of the input, we need to\ninvert the Laplace transform. The inverse, which we denote\nas f(\u03c4;t), can be computed using Post's inversion formula\n(Post 1930):\n$f(\\tau; t) = \\mathcal{L}^{-1}F(s;t) = \\lim_{k \\to \\infty} \\frac{(-1)^k}{k!} (\\frac{k}{t})^{k+1} \\frac{d^k}{ds^k} F(s;t),$\nwhere \u03c4 := k/s and k \u2192 \u221e (see Tano, Dayan, and Pouget\n2020; Horv\u00e1th et al. 2020) for alternative approaches to\ncomputing the inverse transform).\nNeural networks implementation To describe a neural\nnetwork approximation of the Laplace transform, we first\nrewrite Eq. 1 in a differential form:\n$\\frac{dF(s;t)}{dt} = -sF(s;t) + f(t).$\nThe impulse response (response to input f(t) = \u03b4(0)) of\nF(s;t) decays exponentially as a function of time t with\ndecay rate s: e\u2212st (the second row in Fig. 3A). Note that\nthis is a linear transform, so F(s;t) will be a convolution\nbetween f(t) and the impulse response.\nWe implement an approximation of the modified Laplace\nand inverse Laplace transform as a two-layer neural network\nwith analytically computed weights. The first layer imple-\nments the modified Laplace transform through an RNN. The\nsecond layer implements the inverse Laplace transform as a\ndense layer with weights analytically calculated to compute\na k-th order derivative with respect to s.\nWhile in the Laplace domain s is a continuous variable,\nhere we redefine s as a vector of N elements. We can now\nwrite a discrete-time approximation of Eq. 3 as an RNN with\na diagonal connectivity matrix:\n$F_{s;t} = LF_{s;t-1} + f_t,$\nwhere L is an N \u00d7 N recurrent matrix L := e\u2212S\u2206t imple-\nmenting the discrete Laplace transform operator and S is a\ndiagonal matrix composed of s values. At every time step\nt, Fs;t is an N-element vector. For brevity of notation, we"}, {"title": "", "content": "assume that the duration of a discrete-time step \u2206t = 1.\nFollowing Eq. 2, a discrete approximation of the inverse\nLaplace transform, f\u03c4;t, can be implemented as a dense layer\non top of Fs;t. The connectivity matrix of the dense layer is\nL\u22121(see Maini et al. (2023) for the derivation of the exact\nmatrix form of L\u22121).\nTo interpret f\u03c4;t and to select s values in an informed way,\nwe compute the impulse response of f\u03c4;t. For input f(t) =\n\u03b4(0), the activity of ft is:\n$f_{\\tau;t} = \\frac{1}{t} \\frac{k^{k+1}}{k!} \\left( \\frac{t}{\\tau} \\right)^{k+1} e^{-\\frac{kt}{\\tau}}.$\nThe impulse responses of units in f\u03c4;t is a set of uni-\nmodal basis functions (Fig. 3A). To better characterize their\nproperties, we first find the peak time by taking a partial\nderivative with respect to t, equate it with 0 and solve for\nt\u2217: df\u03c4;t/dt = 0 \u2192 t\u2217 = \u03c4. Therefore, each unit in f\u03c4;t peaks\nat \u03c4.\nTo further characterize our approximation, we express the\nwidth of the unimodal basis functions of the impulse re-\nsponse of fit through the coefficient of variation c: c =\n$1/\\sqrt{k + 1}$. Importantly, c does not depend on t and \u03c4, imply-\ning that the width of the unimodal basis functions increases\nlinearly with their peak time. Therefore, when observed as a\nfunction of log(t), the width of the unimodal basis functions\nis constant.\nWe choose values of \u03c4 as log-spaced between some min-\nimum \u03c4min and maximum \u03c4max as specified in the next\nsection. Note that fixing the values of \u03c4 and choosing k\nalso fixes values of s since s = k/\u03c4, so s is not a train-\nable parameter. Because of the log-spacing and because c\ndoes not depend on t and \u03c4, when analyzed as a function\nof log(t), the unimodal basis functions are equidistant and\nequally wide, providing uniform support over the log(t) axis\n(Fig 3C). Because the described system is linear, any input\nfunction is represented as a convolution with a set of these\nbasis functions. As we demonstrate next, this produces a log-\ncompressed memory that is scale invariant."}, {"title": "", "content": "Invariance to temporal rescaling Following Eq. (2) and\nEq. (5) we note that f(\u03c4;t) is scale invariant in the sense\nthat rescaling f(\u03c4;t) \u2192 f(\u03c4; at) can be undone by set-\nting \u03c4\u2217 \u2192 \u03c4\u2217/a. Choosing \u03c4\u2217 to be log-spaced (\u03c4\u2217 =\n(1+c)\u22121\u03c4min, with c > 0) makes the rescaling of \u03c4\u2217 equiv-\nalent to translation: \u03c4i = \u03c4i+\u2206 where \u2206 = log1+ca. This\nimplies that temporal rescaling will cause a translation of the\nsequentially activated units (Fig. 3B,C).\nThis approach can be used to either build agents whose\ntemporal memory representation covaries with the temporal\nscale (i.e., temporal rescaling is converted into translation as\ndescribed above) or agents with temporal memory that is in-\nvariant of the temporal scale (i.e., if time in the environment\nrescales, the agent will have identical memory representa-\ntion). When the objective is to build agents that are invari-\nant to temporal rescaling, we apply convolution and pooling\nover f. The output of convolution and pooling is translation\ninvariant, making the network invariant to temporal rescal-\ning (top row in Fig. 5A). We note that since rescaling is con-\nverted into translation, the presence of edge effects impacts\nthe output of convolution and pooling, thereby making the\ninvariance imperfect.\nIntuition To provide an intuitive understanding of the\nscale invariant memory, we refer to Fig. 3B. The top row\nshows three input signals each composed of two \u03b4 pulses.\nEach of the three signals is a rescaled version of another,\ni.e. f1(t) = f2(a1t) = f3(a2t) where ai is the scaling\nfactor (a1 = 2 and a2 = 4 in our example). The second\nrow shows activity of neurons in the scale invariant mem-\nory of the above signals (this is the state of the memory at\ntime t = 250). Neurons in a scale invariant memory are\nactivated sequentially as a function of internal representa-\ntion of time. Each circle in the plot represents activity of a\nsingle neuron and the x-axis corresponds to the log-spaced\npeak times (hence we refer to it as an internal representation\ntime). Together, these neurons code a log-compressed mem-\nory of the input. This memory representation converts func-\ntions of time t into functions of log(t). If time is rescaled by\nfactor a, the resulting representation will be shifted (trans-\nlated) by log(a): f(at) \u2192 f(log(at)) = f(log(a)+log(t)).\nTherefore, the memory representation is covariant with re-\nspect to rescaling. The strength of such a representation is\nthat it makes computational problems that occur at different\nscales equally difficult. When plotted as a function of neu-\nron index, rather than peak time (top row in Fig. 3C), the\nthree signals corresponding to three temporal scales are now\ntranslated, rather than rescaled, versions of one another. If\nthe x-axis is shifted by log(a), the three signals overlap (bot-\ntom row in Fig. 3C).\nRL Agent\nWe used deep RL agents based on a synchronous version\nof A3C (Mnih et al. 2016) that uses advantage to calculate\ngradient-based policy updates. However, as the direct cal-\nculation of advantage may lead to high variance and longer\ntraining times, we have used an exponentially weighted es-\ntimator of advantage called generalized advantage estimator\n(GAE) (Schulman et al. 2016).\nHyperparameters and training\nOur setup for the 3D interval timing environment used the\nthree following convolutional layers: 32 kernels of size 8 \u00d7 8\n(stride 2), 16 kernels of size 4 x 4 (stride 1), and 32 ker-\nnels of size 8 \u00d7 8 (stride 2). The fully connected layer af-\nter the convolution layer has 64 nodes. Outputs of all lay-\ners had ReLU activation. Following the encoder, the RL\nagent had either an LSTM network with 256 hidden units,\nor the CogRNN architecture with 8 log-spaced units having\n\u03c4min = 1, \u03c4max = 1000, and k = 8. We also introduced\ntwo layered multihead-attention (Vaswani et al. 2017) over"}, {"title": "", "content": "with 8 heads and dmodel 128. Parameters related to\nthe RL algorithm were a discount factor (\u03b3) of 0.98 and a\ndecay (\u03bb) factor of 0.95. We used the Adam optimizer with\n\u03b21 = 0.9, \u03b22 = 0.999, and \u03b5 = 1e-8. We also trained the RL\nagents with varying learning rates, including 0.001, 0.0001\nand 0.00001, and selected the best-performing learning rates\nfor each agent. We explored the impact of the hyperparame-\nters (learning rate and entropy coefficient) and provided re-\nsults as a part of the Supplemental Information.\nFor simple environments, we used LSTM networks with\n128 hidden size and the same CogRNN network without\nthe attention network. We updated the model parameters af-\nter each trial instead of backpropagating the gradients after\nhorizon number of steps.\nResults\nAgents with scale invariant memory learn equally\nwell at different temporal scales\nWe first demonstrate that when temporal relationships be-\ntween task-relevant variables rescale, CogRNN agents learn\nat about the same rate. This is because the log compression\ncauses the memory representation to shift rather than rescale\n(Fig. 3B), making the learning problem equally difficult at\nevery scale where the temporal relationships fall between\nthe hyperparameters \u03c4min and \u03c4max.\nWe trained agents based on CogRNN, LSTM and RNN\nin each of the five environments. To evaluate robustness\nto rescaling in four environments (interval timing 1D and\n3D, interval discrimination and delayed-match-to-sample)\nwe conducted the training with different durations of task-\nrelevant temporal intervals. We controlled the duration using\na step size parameter, which varied from 10 to 100. In inter-\nval timing for instance, with a step size of 10, the intervals\nranged from 300 to 480 steps, while with a step size of 100,\nthey ranged from 30 to 48 steps. All other parameters of the\nenvironment and of the agents remained unchanged across\ndifferent scales.\nThe mean reward as a function of the number of trials in\nthose four environments is shown in Fig. 4 for CogRNN and\nLSTM agents. Agents based on CogRNN reached high per-\nformance in all tasks. Critically, the speed of learning was\nsimilar at different temporal scales. While LSTM and RNN\nagents were able to learn in all environments except the 3D\ninterval timing environment, they did so with different learn-\ning speeds for different temporal scales.\nPsychometric curves shown in Fig. S2 provide another\nperformance measure. The y-axis on the psychometric\ncurves represents the probability of selecting the long in-\nterval. For an agent that performs perfectly, that probability\nwould be zero for the three short intervals and one for the\nthree long intervals. Consistent with the results in Fig. 4,\nCogRNN agents performed better than others. Similar to rat\nbehavior, agents made the most mistakes for the most diffi-\ncult time intervals (36 and 40 steps)."}, {"title": "", "content": "The results of training on the interval reproduction task\nare shown in Fig. S11, Fig. S12, Table S1, and Table S2.\nThis task differs conceptually from the other four, since we\ndid not train agents on rescaled versions of the environ-\nments. Instead, we trained on different interval durations,\nsimilar to Deverett et al. (2019). The results indicate that\nCogRNN agents were able to improve performance simul-\ntaneously at all training and validation intervals, demon-\nstrating the capabilities of multi-scale learning. On the other\nhand, LSTM agents learned quickly only at short intervals.\nIf LSTM agents were trained much longer, they could possi-\nbly learn the task at all scales, but critically, their learning is\nscale-dependent. We also highlight that training of CogRNN\nagents took roughly half the time as training of LSTM agents\n(since CogRNN has fewer trainable parameters).\nCombining scale invariant memory with\ntranslation-invariance results in invariance to\ntemporal rescaling\nWe now consider a case where the entire environment, in-\ncluding the duration of stimuli and the intervals between the\nstimuli, is rescaled in time (i.e., the elapse of time in the en-\nvironment is sped up or slowed down, rather than just rescal-\ning the time between task-relevant variables as in the previ-\nous case). Then combining scale invariant memory with con-\nvolution and maxpool results in agents which observation\nspace is invariant (no longer covariant) of the temporal scale.\nIf trained at one scale, such agents will follow the same pol-\nicy when presented with the environment at a different scale\nand obtain the same performance as on the scale that was\nused for training (aside from edge effects mentioned in the\nModel section). The agents do not need to know the scale\nas long as the temporal relationships again fall between the\nhyperparameters \u03c4min and \u03c4max.\nWe demonstrate this property by training agents in the 1D\ninterval timing environment shown in Fig. 3B. For simplic-\nity, instead of A2C, we used the REINFORCE algorithm\n(Williams 1992). Agents with scale invariant memory fol-\nlowed by convolution and pooling were able to learn the\ntask after about 100k trials. We then rescaled the time in\nthe observation space to 2x and 4x the initial scale. With-\nout additional training, our agents reached perfect perfor-\nmance (Fig. 5B). This was not the case for agents trained\nwith RNNs, as they failed to generalize across different tem-\nporal scales.\nNeural activity is scale invariant and resembles\ntime cells recorded from mammalian brains\nMammalian neural activity during tasks such as interval tim-\ning and delayed match to sample is characterized by neurons\nwith monotonically growing/decaying firing rates and neu-\nrons that exhibit sequential activation (time cells) (Jin, Fu-\njii, and Graybiel 2009; Tiganj et al. 2017, 2018). Consistent\nwith the Weber-Fechner law, the width of the temporal win-\ndows in time cells increases with the peak time (Cao et al.\n2022).\nTo test whether these properties from biological neurons\nare present in the artificial agents, we selected a representa-"}, {"title": "", "content": "tive agent from RNN, LSTM and CogRNN cores. To clean\nthe data, for each of the three agents, we first removed neu-\nrons that were persistently active (i.e. that had constant ac-\ntivity, therefore not encoding any temporal information) and\nneurons that were completely silent. We then identified neu-\nrons that had monotonically growing/decaying activity, find-\ning such neurons in all three agents (Fig. S3).\nNext, we identified neurons with transient activation\nsometime during the timed interval (neurons that resemble\ntime cells) and again found them in all three agents (Fig. 6).\nFor CogRNN, the activation pattern resembles the scale in-\nvariant impulse responses shown in Fig. 3A.\nTo better understand the activation profiles, we fitted each\ntime cell with a Gaussian distribution and estimated standard\ndeviation. For a representation to be scale invariant, the re-\nlationship between the peak time and"}]}