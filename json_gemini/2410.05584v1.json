{"title": "RETHINKING REWARD MODEL EVALUATION: ARE WE BARKING UP THE WRONG TREE?", "authors": ["Xueru Wen", "Jie Lou", "Yaojie Lu", "Hongyu Lin", "Xing Yu", "Xinyu Lu", "Ben He", "Xianpei Han", "Debing Zhang", "Le Sun"], "abstract": "Reward Models (RMs) are crucial for aligning language models with human pref-erences. Currently, the evaluation of RMs depends on measuring accuracy againsta validation set of manually annotated preference data. Although this methodis straightforward and widely adopted, the relationship between RM accuracyand downstream policy performance remains under-explored. In this work, weconduct experiments in a synthetic setting to investigate how differences in RMmeasured by accuracy translate into gaps in optimized policy performance. Ourfindings reveal that while there is a weak positive correlation between accuracyand downstream performance, policies optimized towards RMs with similar accu-racy can exhibit quite different performance. Moreover, we discover that the wayof measuring accuracy significantly impacts its ability to predict the final policyperformance. Through the lens of Regressional Goodhart's effect, we identify theexistence of exogenous variables impacting the relationship between RM qualitymeasured by accuracy and policy model capability. This underscores the inade-quacy of relying solely on accuracy to reflect their impact on policy optimization.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning from Human Feedback (RLHF) (Ibarz et al., 2018; Ouyang et al., 2022)has emerged as a prominent paradigm for aligning Large Language Models (Yang et al., 2024;Dubey et al., 2024). The reward model (Ng & Russell, 2000; Brown & Niekum, 2019; Palan et al.,2019) plays a crucial role in this process by substituting human preferences for model optimization.However, building an RM that fully captures human preferences is highly challenging (Armstrong& Mindermann, 2019; Skalse & Abate, 2023; Lambert et al., 2023). Therefore, the RM can bean imperfect proxy for ideal preferences and cause downstream performance deterioration whenoptimized against it, known as reward model overoptimization (Gao et al., 2022). This phenomenon,as a result of Goodhart's law (Karwowski et al., 2023), presents a critical challenge to the RLHF.\nThe inherent difficulty of constructing an ideal RM require a careful evaluation process to captureits potential negative impace in policy optimization. To date, common practice for evaluating theRM includes directly assessing the optimized policy (Hendrycks et al., 2021; Li et al., 2023) andcomputing accuracy on a fixed dataset (Lambert et al., 2024). The former, while it can be seenas a final metric in practice, is limited by the cost of optimization. More importantly, it fails todistinguish whether the causes of undesirable behavior arise from the policy optimization process orthe reward learning process. The latter, while straightforward, remains the question of whether suchevaluation accurately predicts the performance of the downstream optimized policy."}, {"title": "2 PRELIMINARY", "content": "Let X be the set of prompts, and Yx be the set of possible responses. A policy \\(\\pi\\) is a languagemodel that generates responses y to the prompt x at probability \\(\\pi(y|x)\\). The golden reward functionassesses the response and gives a score based on their quality r* : X \u00d7 Yx \u2192 R. In practice, thegolden reward function r represents the complicated human preference and is generally inaccessible."}, {"title": "3 DOES THE RM ERROR MEASURED BY ACCURACY CORRELATE WITH THE POLICY REGRET?", "content": "Current research typically assesses reward model errors by computing accuracy on a fixed test set.While this method is straightforward and widely used, few studies have explored whether the accuracy metric correlates with policy regret. In this section, we examine the relationship betweenmeasured accuracy and policy regret empirically.\nFinding 1: RM evaluation accuracy is positively related to policy regret, but even with similar RMaccuracy, policies can exhibit different levels of regret.\nWe first investigate the correlation between the accuracy and the policy regret. Original promptsand responses from RewardBench are used to measure accuracy, while the prompts are used fordownstream optimization. We assess three traditional correlation coefficients to evaluate how wellaccuracy metrics benchmark reward models. Additionally, we include the Mean Reciprocal Rank(Voorhees & Tice, 2000) to examine if accuracy can identify the best reward model among multiplecandidates. As presented in Table 1, there is a positive relationship between accuracy and policyregret. However, trends depicted in Figure 3a and Figure 3b illustrate that policy regret can varyconsiderably even within similar accuracy ranges. Lastly, we observe that accuracy generally correlates more strongly with regret in BoN than in PPO. This is expected, as BoN is a more localizedand stable optimization algorithm, making it more predictable by reward model error."}, {"title": "4 HOW TO BETTER MEASURE RM ERROR FOR POLICY REGRET PREDICTION?", "content": "In the previous section, we examine the positive correlation between accuracy and policy regret.However, there appears to be room for enhancing this correlation, which leads us to the question:how to better quantify the RM error? In this section, we first investigate the influence of prompt andresponse distribution. Moreover, we explore a straightforward yet effective strategy, i.e., extendingresponses per prompt. Finally, we validate it under different constraints."}, {"title": "5 WHAT'S THE RELATIONSHIP BETWEEN RM ERROR AND POLICY REGRET?", "content": "In this section, we investigate the relationship between RM error and policy regret. The translationfrom RM error to the policy regret can be seen as the result of the reward model overoptimization.Therefore, to accurately predict policy regret, the quantified RM error should be able to reveal thepotential overoptimization. We first theoretically explore this issue. Then we empirically examinethe relationship and analyze the observed optimization dynamics.\nFinding 5: There could be exogenous variables impacting the relationship between RM qualitymeasured by accuracy and policy model optimization.\nThe translation from RM error to the policy regret can seem as the result of Goodhart's Laws (Good-hart, 1984). It says that optimizing a less effective metric rather than the golden metric leads to thefailure of the system. Such phenomenon is also commonly termed reward model overoptimization(Gao et al., 2022) under the context of RLHF. Among various kinds of Goodhart's effect leadingto the reward model overoptimization, the Regressional Goodhart's effect (Manheim & Garrabrant,"}, {"title": "6 RELATED WORKS", "content": "Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022; Ouyang et al., 2022;Stiennon et al., 2022) has been a common strategy for the alignment of Large Language Models"}, {"title": "7 CONCLUSION", "content": "Our study highlights the limitations of evaluating the reward model solely based on accuracy for pre-dicting downstream policy performance. Although there exists a weak positive correlation betweenaccuracy and policy performance, reward models with similar accuracy can result in varying policyoutcomes. Moreover, the way of measuring accuracy will significantly impact the prediction perfor-mance. Additionally, we identify the presence of exogenous variables influencing the optimizationprocess. Overall, we should take a more cautious attitude about the RM performance indicated byaccuracy and develop more advanced techniques for RM evaluation."}, {"title": "8 APPENDIX", "content": ""}, {"title": "8.1 DATASET CONSTRUCTION", "content": "We build up a RM training dataset by mixing the following open-sourced datasets:\n\u2022 Nectar (Zhu et al., 2023), a high-quality 7-wise comparison dataset generated by GPT-4 ranking.\n\u2022 Capybara-7K-binarized (Argilla, 2024), a binarized prefernce dataset built with distilabel atopthe Capybara (Daniele & Suphavadeeprasit, 2023).\n\u2022 Orca-pairs (Intel, 2023), a dataset contains 12k examples from OpenOrca dataset (Lian et al.,2023).\n\u2022 UltraFeedback (Cui et al., 2023), a large-scale, fine-grained, diverse preference dataset.\n\u2022 PKU-SafeRLHF (Ji et al., 2024), a high-quality dataset consisting of 83.4K preference entries,which is annotated across two dimensions: harmlessness and helpfulness.\n\u2022 MTBench-human (Zheng et al., 2023), a dataset contains 3.3K expert-level pairwise humanpreferences for model responses generated by 6 models in response to 80 MT-bench questions."}, {"title": "8.2 PARAPHRASE PROMPTS", "content": "We employed two strategies for rewriting prompts: one that alters expression without changingsemantics, and another that rewrites the prompt into a similar but related prompt. The prompt inTable 5 is used for the paraphrase strategy, and the prompt in Table 6 is used for the rewrite strategy."}, {"title": "8.3 REWARD ERROR METRICS", "content": "Given a prompt x and N = 5 different responses Y = [Y1, \u2026\u2026\u2026, YN], their corresponding rewardscores and ranks are represented as r(Y) and R(Y). Since each prompt has multiple annotatedresponses, many ranking evaluation metrics can be applied to our setting. These include the metricslisted in Table 4, which we will detail one by one."}, {"title": "Pearson corr.", "content": "Pearson correlation coefficient can be computed as follows:\n\\(P_{r(Y),r^* (Y)} = \\frac{cov (r(Y), r^* (Y))}{\\sigma_{r(Y)}\\sigma_{r^*(Y)}}\\)\nwhere cov is the covariance, \\(\\sigma_{r(Y)}\\) is the standard deviation of the proxy rewards given to the re-sponses r(Y) and \\(\\sigma_{r^*(Y)}\\) is the standard deviation of the golden rewards given to the responses"}, {"title": "Spearman corr.", "content": "Spearman's rank correlation coefficient follows the basically same formula as thePearson correlation coefficient, only with the reward scores replaced by the ranks:\n\\(S_{R(Y), R^* (Y)} = \\frac{cov (R(Y), R^* (Y))}{\\sigma_{R(Y)}\\sigma_{R^*(Y)}}\\)"}, {"title": "Kendall \u03c4 corr.", "content": "Kendall rank correlation coefficient is computed as follows:\n\\(T_{R(Y), R^* (Y)} = \\frac{n_c - n_d}{N(N-1)/2}\\)\nwhere nc and nd stands for the number of concordant and disconcordant pair betweenR(Y) and R*(Y), respectively. The concordant pair (yi, yj) means that their rank satisfy(R(Yi) \u2013 R(Yj))(R*(Yi) \u2013 R*(Yj)) > 0. In practice, we employ \u0442\u0432 which handles the ties forthe rare case that some responses get the same reward scores. The final metric is the average of thecoefficient of each prompt."}, {"title": "Accuracy", "content": "The accuracy metrics are mostly the same as in the typical case that there are tworesponses per response. The main difference is that if there are N responses per response, we canthen form \\(C_N^2\\) different pairs for comparison."}, {"title": "Bo5", "content": "The best-of-5 metric can be seen as a special case of NDR for N = 5, which computes:\n\\(\\frac{r^* (argmax_{y_i} (r(Y)) - \\mu_{r^*(Y)}}{max[r^*(Y)] - \\mu_{r^*(Y)}}\\)\nThis metric represents the improvement in reward values obtained using the proxy reward scorecompared to those achievable with the original golden reward model. The final metric is the averageof the coefficient of each prompt."}, {"title": "ECE", "content": "Expected Calibration Error (ECE) is calculated by averaging the absolute differences be-tween predicted probabilities and observed frequencies, typically across a set of bins partitioningthe prediction space:\n\\(ECE = \\sum_{m=1}^M \\frac{|B_m|}{n} |acc(B_m) - conf(B_m)|\\)\nwhere B represents the bins that split pairs by reward margins and M stands for the number ofbins. acc is the accuracy of pairs in each bin. conf computes the expected accuracy inferred fromreward score margins by the Bradley-Terry Model (Bradley & Terry, 1952). Expected calibrationerror indicate the alignment of reward models' confidence. We follow the same strategy for formpreference pairs as in the Accuracy."}, {"title": "MRR", "content": "Mean reciprocal rank is a traditional Information Retrieval metric that can be transported toour setting smoothly. We first define reciprocal rank as the golden rank of the response that receivesthe highest reward score.. Then we take the average overall prompts:\n\\(MRR = E_{x \\in X} \\frac{1}{R^*(arg \\max_{Y_i} [R(Y)])}\\)"}, {"title": "NDCG", "content": "Normalized discounted cumulative gain is another typical Information Retrieval metricthat is transported here. This computes compute:\n\\(NDCG = \\frac{\\sum_{i=1}^N (2^{R^* (Y_i)} - 1)/log_2(N - R(Y_i) + 1)}{\\sum_{i=1}^N (2^{R^* (Y_i)} - 1)/log_2(N - R^*(Y_i) + 1)}\\)\nThe main difference from the typical usage in the field of Information Retrieval is that we replacerelevance score reli with the golden rank R(Yi). These metrics can be seen as a smooth version ofMRR. The final metric is the average of the coefficient of each prompt."}, {"title": "\u00a7corr.", "content": "\u00a7 correlation coefficient (Chatterjee, 2020) is relatively new metric for evaluating the rankcorrelation. Compared to traditional rank coefficients like Spearman corr., this coefficient is moreeffective to compute. It first rearrange the data as [r* (Y(1)), (Y(1)], ..., [r* (Y(N)), (Y(N))] suchthat r* (Y(1) \u2264 r* (Y(2) \u2264 ... \u2264 r*(Y(v)), and then compute:\n\\(\\xi_\u03bd (R(Y), R^* (Y)) = 1 \u2013 \\frac{3 \\sum_{i=1}^{N-1} |R(i+1) \u2013 R(i)|}{N^2-1}\\)"}, {"title": "8.4 PARAMETRIC EQUATION BETWEEN ACCURACY AND REGRET", "content": "Based on the assumption that only Regressional Goodhart takes effect and golden reward scorer* ~ N(0, \u03c3\u00b2), the noise z ~ N(0, \u03c3\u00b2), we can then derive the relationship between the accuracyand the regret. Based on the Regression Goodhart's, the proxy reward r can represented as:\nr = r* + z\nThe process of constructing an RM test set can be viewed as performing two independent samplesfrom the distribution of the golden reward score. Therefore, the scores obtained from the two sam-ples can be represented as r\u2081 ~ N(0, \u03c3\u00b2) and r2 ~ N(0, \u03c3\u00b2). Subsequently, the difference betweentwo samples' golden reward scores also follows a normal distribution: r* ~ N(0,207). Then the proxy reward model score difference can be written as:\nr_ = r* - 21 + 22\nwhere 21 and 22 is the noise introduced in the two times of sampling. The distribution of noisedifference is also normal distribution 21\n22 ~ N(0, 2\u03c3\u00b2). The accuracy metrics can be translatedinto:\ndacc = P(r-> 0, r* > 0) + P(r_ < 0,r* < 0)\n= P(r_ > 0, r* > 0)\n= 1 - 2P(21 - 22 < 0,r* > 0)\n\\(=1-2 \\int_{x=0}^{+\\infty} P(x = r^*)P(z_1 - z_2 < x)dx\\)\n\\(= 1- \\int_{x=0}^{+\\infty} \\frac{1}{\\sigma\\sqrt{\\pi}} e^{\\frac{x^2}{4\u03c3^2}} \\Phi(\\frac{x}{\\sqrt{2\u03c3^2}})dx\\)\nAs for the d\u03c0, we refer the result from (Gao et al., 2022) that:\n\\(E [X|X + Z = c] = E[X] + (c \u2013 E[X] \u2013 E[Z]) \\frac{Var(X)}{Var(X) + Var(Z)}\\)\nwhere X and Z are independent absolutely continuous random variables with X normally dis-tributed and Z normally distributed. In our case, X can be replaced by r* and X + Z can bereplaced by r:\n\\(d_\u03c0 = \\frac{\u03c3_r^2}{\u03c3_r^2 + \u03c3_e^2}\\)"}]}