{"title": "RATIONALIZATION MODELS FOR TEXT-TO-SQL", "authors": ["Gaetano Rossiello", "Nhan Pham", "Michael Glass", "Junkyu Lee", "Dharmashankar Subramanian"], "abstract": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward construct-ing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A ra-tionalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To eval-uate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query genera-tion improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.", "sections": [{"title": "INTRODUCTION", "content": "Recent advances in LLMs have demonstrated breakthroughs in the text-to-SQL task, a long-standing challenge in computer science (Hendrix et al., 1978; Copestake & Jones, 1990). Modern text-to-SQL systems follow a pipeline: schema linking, SQL generation via LLMs, and post-processing (repair, verification, selection). (Gao et al., 2024b; Pourreza et al., 2024; Maamari et al., 2024; Talaei et al., 2024). Nevertheless, the performance of the best-performing systems is still far from human-level performance, especially in enterprise use cases (Li et al., 2023; Lei et al., 2024).\nIn parallel to complex natural language reasoning tasks, question decomposition approaches have shown to be a promising direction for complex text-to-SQL tasks (Pourreza & Rafiei, 2023; Dong et al., 2023; Gao et al., 2024a; Wang et al., 2025). Rather than generating the desired SQL in a single pass, decomposition approaches break down the input natural language question into a sequence of simpler sub-questions and employ the step-by-step reasoning approach. The CoT style in-context learning methods often help LLMs to generate more accurate results and intermediate thoughts pro-vide human-readable explanations. Moreover, CoT steps are essential for self-improvement (Light-man et al., 2024; DeepSeek-AI et al., 2025) and for enhancing the reasoning capabilities of smaller models by fine-tuning them through knowledge distillation (Zelikman et al., 2022; Huang et al., 2023; Magister et al., 2023; Li et al., 2024).\nHowever, publicly available text-to-SQL datasets often lack step-by-step decompositions, and man-ual annotation can be expensive. Therefore, we propose a framework for generating rationales in CoT style for the text-to-SQL task, which comes with unique challenges. We start with existing text-to-SQL training sets, provide a few human annotations, and incorporate dynamic few-shot sample selection to bootstrap valid rationales and fine-tune rationalization models.\nOur preliminary evaluation on the BIRD benchmark shows that fine-tuning smaller models with intermediate rationales consistently enhances performance, especially on moderate to challenging problems. Additionally, these fine-tuned models generate clear explanations of the query-building process and offer actionable plans for implementing corrective measures at each step."}, {"title": "METHODOLOGY", "content": "Figure 1(a) provides an overview of the synthetic data generation method. The process involves a subject matter expert who defines the representation of rationales, a text-to-SQL dataset containing"}, {"title": "SQL-BUILDING RATIONALE REPRESENTATION", "content": "The first stage involves manually annotating a few seed examples with the desired rationale repre-sentation. There are various approaches to generating CoT steps, including manual (Wei et al., 2022; Fu et al., 2023), automatic (Kojima et al., 2022; Chen et al., 2023), and semi-automatic (Shum et al., 2023; Lu et al., 2023; Ye & Durrett, 2023) prompting techniques. Our approach aims to harness the benefits of all three methods by combining them effectively. First, we employ automatic prompt-ing with the zero-shot instruction (see Appendix A.1) to generate an initial draft of SQL steps. We then refine this draft by incorporating a preliminary step that outlines the plan and structuring it using Markdown (see Appendix A.2). Markdown facilitates easy parsing of the generated output, allowing for the extraction of individual intermediate SQL statements and enhancing visualization for qualitative assessment.\nWe develop the rationale by emulating the cognitive process of incrementally constructing an SQL query, joining tables, and adding conditions step-by-step. Figure 1(b) illustrates an example of such a CoT representation. Given a question, the first step involves rephrasing it, outlining the decom-position steps, and identifying the relevant tables and columns while justifying their selection. It benefits the user by providing an interpretable overall solution plan and aids the LLM by offering focused context to enhance in-context learning. Then, the SQL-building process continues through a sequence of steps with increasing complexity, providing a textual headline along with the cor-responding intermediate, executable SQL. The SQL at the final step represents the solution to the given input question."}, {"title": "KNOWLEDGE DISTILLATION WITH DYNAMIC FEW-SHOT", "content": "Manually curated seed examples are stored in a repository of validated SQL CoTs, to boostrap a dynamic few-shot rationale generation process. This iterative approach is applied to each (question, gold SQL) pair within a given text-to-SQL training set. For each training instance, the gold SQL serves as a query to rank the most similar examples from the validated SQL CoTs. To implement the example ranking function, we construct a sparse vector space model, where distinct SQL-reserved keywords define the feature space. Each validated CoT's final SQL statement is extracted and trans-formed into a sparse vector representing SQL keyword frequency. The gold SQL is projected into the same vector space and used as a query vector to rank SQL CoT examples based on cosine simi-larity. Finally, the top-n most similar examples, annotated with SQL-building rationales, are selected as few-shot examples and incorporated into the prompt. This strategy facilitates the selection of val-idated CoT examples with similar SQL structures, such as nested queries, aggregation functions, or multi-joins, drawing inspiration from a case-based reasoning approach."}, {"title": "RATIONALIZATION MODEL FINE-TUNING", "content": "Despite the dynamic few-shot process with the teacher model significantly enhancing the coverage of validated SQL CoT annotations (see Section 3.1), some training instances may still contain invalid, synthetically generated rationales. Following STaR (Zelikman et al., 2022), we distinguish rationales from rationalizations. Rationales are CoTs produced without knowing the target answer, while rationalizations are produced as step-by-step explanations of already known answers. To achieve complete annotation coverage, we fine-tune a rationalization model and apply it to the remaining training examples that lack correct generated rationales. Validated CoTs from the previous dynamic few-shot stage provide the training dataset for the rationalization model. The filtered DB schema, the user question, and the gold SQL are the input, and the fully validated SQL-building CoT is the output.\nThe rationalization model is designed not to generate an SQL solution for a given user request but to produce step-by-step reasoning by incorporating both the question and the correct SQL query into the input prompt. As in the previous stage, the execution of the final SQL query derived from the generated CoT is compared with the execution of the gold-standard SQL. Our findings indicate that a text-to-SQL rationalization model fine-tuned in this manner can help identify incorrect (question, gold SQL) training pairs. In rare cases where the result sets do not match, the provided ground truth may contain inconsistencies that can be easily detected and corrected.\nFurthermore, a fine-tuned rationalization model can be utilized to generate SQL-building rationales for other text-to-SQL datasets, even across different domains, without needing to repeat the entire process from scratch. This presents a promising direction for future research."}, {"title": "EXPERIMENTS", "content": "To evaluate the quality of the generated SQL CoTs, we conducted a controlled experiment using the train and dev sets of the BIRD (Li et al., 2023) dataset. This dataset is particularly well-suited to our study, as its train set (containing question-SQL pairs) does not include query decomposition steps. Furthermore, the dev set is organized into three main categories -simple, moderate, and challenging SQL statements- enabling a fine-grained, independent evaluation of model performance across different difficulty levels.\nWe begin by cleaning the training set, removing instances where the gold SQL is not executable, either due to syntax errors, timeouts, or empty result sets. After the cleaning process, we collect 8,807 text-to-SQL training pairs. The original dev set, consisting of 1,534 instances, is retained. We use the open-source Llama 3.1 LLM family (Dubey et al., 2024), specifically the 70B version as the teacher for the dynamic few-shot process, and the 8B version as the student to fine-tune both the rationalization model and the text-to-SQL models, with and without CoTs."}, {"title": "SQL RATIONALE TRAINING COVERAGE", "content": "The coverage of validated CoT at each stage is shown in Table 3.1. After manually annotating two random few-shot examples, the teacher model produces correct CoT for 53.18% of the train-"}, {"title": "IMPACT OF SQL RATIONALES IN FINE-TUNING TEXT-TO-SQL MODELS", "content": "Due to the iterative nature of dynamic few-shot learning, training instance pairs can generate multi-ple valid synthetic CoTs. We created six different training sets for the text-to-SQL task, split into two groups based on the number of instances. These sets use the same input prompts but differ in their targets: the gold SQL without CoT, which serves as a baseline, and selections of the shortest/longest CoTs for each train instance. The goal is to evaluate the impact on the execution accuracy of Llama 3.1 8B, fine-tuned with and without query decomposition steps, both before and after the application of the rationalization model. The results of the fine-tuned models applied to the BIRD development set are presented in Table 3.2. Using 73.68% of the training set, after the dynamic few-shot process, a model fine-tuned without CoT demonstrates overall better performance, particularly on the simple questions, which are more frequent. However, the model fine-tuned with the short/long CoT demon-strates a greater ability to handle moderate and challenging questions, respectively. After applying the rationalization model with full coverage of the training set, the text-to-SQL model, fine-tuned on the longest CoT, outperforms in all three categories, improving overall performance by +1.24%, with a notable increase of +4.86% on the challenging queries."}, {"title": "CONCLUSION", "content": "In this work, we presented a framework designed to facilitate the annotation of text-to-SQL datasets with CoT rationales, which consist of intermediate SQL-building steps. The process requires mini-mal manual annotations, using only a few examples to define the rationale representation. A teacher model is then employed to distill a large volume of validated synthetic CoTs, which are used to boot-strap a rationalization model that enables full training coverage with CoT. We introduced a sample selection method based on SQL similarity, supporting a dynamic few-shot approach. Our compar-ison demonstrates that long CoTs enhance the accuracy of text-to-SQL tasks while providing an interpretable, user-friendly output that clarifies the reasoning process behind query construction."}, {"title": "APPENDIX", "content": ""}, {"title": "TEXT-TO-SQL PROMPT REPRESENTATION (SOURCE)", "content": "[SCHEMA]\nCREATE TABLE prof (\nprof_id INTEGER PRIMARY KEY,\nfirst_name TEXT,\nlast_name TEXT,\npopularity INTEGER,\n);\nprof.prof_id\nprof.first_name: 'Bernhard', 'Hattie', 'Mateo'\nprof.last_name: 'Conkay', 'Cunningham', 'Ewenson'\nprof.popularity: 2, 3\nCREATE TABLE registration (\nstudent_id INTEGER,\n);\nregistration.student_id\nCREATE TABLE ra (\nstudent_id INTEGER,\ncapability INTEGER,\nprof_id INTEGER,\n);\nra.student_id\nra.capability: 2, 4, 5\nra.prof_id\nCREATE TABLE student (\nstudent_id INTEGER PRIMARY KEY,\nf_name TEXT,\n);\nstudent.student_id\nstudent.f_name: 'Ahmed', 'Alvera', 'Amerigo'\nselected: prof.popularity, prof.prof_id, ra.student_id, None, ra.\nprof_id, ra.capability, student.student_id, prof.last_name,\nstudent.f_name, prof.first_name\njoin: ra.prof_id, prof.prof_id, None, student.student_id, ra.\nstudent_id, registration.student_id, ra.capability, prof.\npopularity\ncondition: ra.capability, prof.popularity, None, prof.prof_id, ra.\nprof_id, ra.student_id, student.student_id, prof.last_name\norder: prof.popularity, None, ra.student_id, ra.prof_id, prof.\nprof_id, student.student_id, ra.capability\ngroup: None, ra.prof_id, prof.prof_id, prof.popularity, ra.\nstudent_id, ra.capability, prof.last_name"}, {"title": "SQL RATIONALE REPRESENTATION (TARGET)", "content": "**Step 1: Identify the required tables and columns**\nFrom the question, we need to find the number of students with\nresearch capability of 5 among professors with the highest\npopularity. This implies we need to:\n1. Find the highest popularity of professors.\n2. Filter professors with the highest popularity.\n3. Join the 'ra' table to get the students advised by these\nprofessors.\n4. Filter students with research capability of 5.\n5. Count the number of students.\nRequired tables:\n* \u2018prof` (contains professor information)\n* 'ra (maps students to professors)\n* 'student' (contains student information)\nRequired columns:\n* `prof.popularity` (to find the highest popularity)\n* `ra.capability` (to filter students with research capability of\n5)\n* 'ra.student_id' (to count the number of students)\n**Step 2: Find the highest popularity of professors**\n'''sql\nSELECT MAX(popularity) AS max_popularity\nFROM prof;"}]}