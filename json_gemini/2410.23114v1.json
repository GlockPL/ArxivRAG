{"title": "UNIFIED TRIPLET-LEVEL HALLUCINATION EVALUATION FOR LARGE VISION-LANGUAGE MODELS", "authors": ["Junjie Wu", "Tsz Ting Chung", "Kai Chen", "Dit-Yan Yeung"], "abstract": "Despite the outstanding performance in vision-language reasoning, Large Vision-Language Models (LVLMs) might generate hallucinated contents that do not exist in the given image. Most existing LVLM hallucination benchmarks are constrained to evaluate the object-related hallucinations. However, the potential hallucination on the relations between two objects, i.e., relation hallucination, still lacks investigation. To remedy that, in this paper we design a unified framework to measure object and relation hallucination in LVLMs simultaneously. The core idea of our framework is to conduct hallucination evaluation on (object, relation, object) triplets extracted from LVLMs' responses, and thus, could be easily generalized to different vision-language tasks. Based on our framework, we further introduce Tri-HE, a novel Triplet-level Hallucination Evaluation benchmark which can be used to study both object and relation hallucination at the same time. We conduct comprehensive evaluations on Tri-HE and observe that the relation hallucination issue is even more serious than object hallucination among existing LVLMs, highlighting a previously neglected problem towards reliable LVLMs. Moreover, based on our findings, we design a simple yet effective training-free approach to mitigate hallucinations for LVLMs, with which, we exceed all open-sourced counterparts on Tri-HE, achieving comparable performance with the powerful GPT-4V. Our dataset and code for the reproduction of our experiments are available publicly at https://github.com/wujunjie1998/Tri-HE.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (LVLMs) [1, 2, 3, 4] have attracted significant attention. Despite the superior perfor-mances, existing works primarily focus on enhancing the helpfulness of LVLMs without careful consideration of thereliability of responses generated by LVLMs. However, it has already been observed by recent literature that LVLMssuffer from severe hallucination [5, 6, 7, 8, 9], i.e., LVLMs might generate contents that do not exist in the given image,probably due to insufficient training during visual instruction tuning. A typical example is provided in Figure 1a, wherethe LLaVA [2] model considers the location to be busy, simply because LLaVA recognizes that it is a train station withseveral people existing.\nWith the prevalence of LVLMs, enormous works have started to explore the evaluation and analysis of LVLMhallucination. However, two problems are observed: 1) Hallucination category: most existing works focus on object-related hallucination [5, 6, 10] (i.e., LVLMs describing an object not existing in the given image) while ignoring thepossibility that even when two objects are successfully recognized, LVLMs might still mess up with their relationshipswhen conducting commonsense reasoning. As illustrated in the example in Figure 1a, LLaVA successfully recognizesthe \"people\" and the train station \"area\", yet predicts their relation to be \u201cwalking around\u201d that cannot be directlyobtained from the given image. Therefore, a unified definition and taxonomy is necessary to integrate different kinds ofLVLM hallucination.\n2) Hallucination discrimination: existing works [5, 6, 8, 11] rely on self-discrimination (e.g., Yes/No questions) totestify LVLMs' awareness of objects and relations in the given image. When prompted with an instruction \u201cIs there a{content} in the image?\", hallucination is determinied only when LVLMs answer \"Yes.\" for a non-existing content.\""}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Large Vision-Language Models (LVLMs)", "content": "The powerful capability exhibited by Large Language Models (LLMs) has facilitated the extension of LLMs towardsthe multi-modal domain. LLMs are empowered to understand and reason about both images and text by aligningrepresentations from visual encoders to pre-trained language models, followed by visual instruction tuning. LLaVA [14,2] proposes to use a simple projection layer to integrate the visual representations into textual encoders, which isfurther enhanced in Shikra [15] by incorporating referential dialogue tasks. Instead, BLIP [16] proposes the Q-Formerarchitecture to extract useful information from the visual representations, which is also used by MiniGPT-4 [3] andInstructBLIP [1]. MoCLE [17] further introduces the Mixture-of-Experts architecture into LVLMs to deal with the data"}, {"title": "2.2 Hallucination Evaluation in LVLMs", "content": "With the prevalence of LVLMs, a growing number of studies have been conducted on their hallucination issues [9, 18,19, 20, 21, 6, 8, 22]. Previous hallucination evaluation works can be categorized into two groups: 1) solely evaluatingobject hallucinations or do not distinguish different hallucinations [23, 24, 6, 10], which neglects other hallucinationtypes like relation hallucination and is thus not comprehensive. The other type of works use \u201cyes/no\u201d questionsto evaluate LVLM's relation/object hallucinations [5, 25, 8, 11]. However, these benchmarks require transforminggeneral vision-language tasks into \u201cyes/no\u201d formats, limiting their applicability. Also, different LVLMs may havedifferent ability in answering such\u201cyes/no\u201d questions since they are pre-trained on different data, which may bias theevaluation results. To remedy this research gap, our paper proposes a triplet-level evaluation framework that can providefine-grained object and relation hallucinations for responses to any vision-language tasks, with an evaluation benchmarkTri-HE that incorporates questions requiring more complicated commonsense reasoning.\nIt is worth noting that a concurrent benchmark, Reefknot [26], also evaluates relation hallucinations at the triplet level.However, compared to Tri-HE, Reefknot has several limitations: 1) It fully relies on existing datasets, which may lead todata contamination, whereas Tri-HE consists of questions synthesized by GPT-4V. 2) Reefknot evaluates only a limitednumber of relation types, while Tri-HE covers a broader range, enabling more comprehensive evaluation results asshown in \u00a75. 3) Reefknot employs only one entailment-based hallucination discriminator, whereas Tri-HE offers moreaccurate hallucination detection using GPT-4, which can simultaneously identify both object and relation hallucinations."}, {"title": "3 Tri-HE Framework Formulation", "content": "Inspired by the relation extraction [27] tasks in NLP, in this section, we propose a unified framework to evaluate bothobject and relation hallucinations via the object-relation triplets (i.e., (Object\u2081, Relation, Object2)). Here the objectsand relations can either be a word or a phrase with attributes. We start by defining object and relation hallucinations viatriplets in \u00a73.1, based on which, we define our evaluation metrics and pipeline in \u00a73.2 and \u00a73.3 separately."}, {"title": "3.1 Definitions", "content": "As shown in Figure la, given an input image I, a question Q related to I with its ground truth answer A, and the answerA predicted by a LVLM $A_{\\theta}(\\cdot|Q, I)$ parameterized with $\\theta$, we first define,\n\u2022 $G = (V, E)$ as the scene graph of I, where V and E refer to all the objects existing in I and all the possiblerelations among existing objects, respectively.\n\u2022 $G' = (V' \\subseteq V, E' \\subseteq E)$ as the knowledge graph that includes all the required objects and relations to answerQ.\n\u2022 $G_{\\Theta} = (V_{\\Theta}, E_{\\Theta})$ as the knowledge graph extracted from $A_{\\Theta}$, where $V_{\\Theta}$ and $E_{\\Theta}$ include all the objects and all thepossible relations among objects mentioned in $A_{\\Theta}$.\nNote that here all graphs can be converted to a set of triplets (i.e., $G = \\{(v_1, e, v_2)\\}$, where $v_1, v_2 \\in V$ and $e \\in E$). Acommon nightmare in previous LVLM hallucination literature lies in the ambiguous discrimination between predictionhallucinations and errors [28]. To obtain unbiased hallucination evaluation results, we separate them depending onwhether or not the wrongly generated objects or relations exist in the given image I. Specifically, given a triplet$(v_1, e, v_2) \\in G_{\\Theta}$, we have the following definitions,\n\u2022 Object hallucination: if $v_1 \\notin V$ or $v_2 \\notin V$, suggesting $A_{\\Theta}$ includes an object not within I. For example, thetriplet (location, suggests, popular spot for socializing) in Figure la encounter an object hallucination since the object \"popular spot for socializing\u201d cannot be obtained from V.\n\u2022 Relation hallucination: if $v_1, v_2 \\in V$ yet $e \\notin E$, suggesting that $A_{\\Theta}$ correctly recognizes two related objectsfrom I but pair them with a non-existing relation. For example, the triplet (people, walking around, area)in Figure la has a relation hallucination since the relation \"walking around\u201d cannot be obtained from G,despite that the objects are all in V.\n\u2022 Prediction error: if $v_1, v_2 \\in V$ and $e \\in E$ yet $(v_1, e, v_2) \\notin G$, suggesting $A_{\\Theta}$ correctly recognizes objectsand relations from I, yet pairs in a wrong way."}, {"title": "3.2 Evaluation Metrics", "content": "With the above definition in hand, given the knowledge graph $G_{\\Theta}$ extracted from a model response $A_{\\Theta}$, we calculatehallucination rates of $A_{\\Theta}$ as the proportion of hallucinated triplets in $G_{\\Theta}$. Most previous works (e.g., POPE [5])directly evaluate the hallucination rate at the object-level with respect to the total number of predicted objects, yet maketheir results not comparable among LVLMs, since different LVLMs might refer to different numbers of objects intheir responses. To address this issue, we instead opt to calculate the hallucination rate in the question- and image-level.Specifically, we calculate two types of hallucination rates, including the question-level hallucination rate (HalluQ) andimage-level hallucination rate (HalluI), as defined in the following,\nHalluQ $(\\{Q\\}) = \\frac{1}{|\\{Q\\}|} \\sum_{Q'\\in\\{Q\\}} (\\frac{\\# \\text{HT in } G_{\\Theta}}{\\# \\text{TT in } G_{\\Theta}}) \\times 100\\%$,\t\t(1)\nHalluI $(\\{I\\}) = \\frac{1}{|\\{I\\}|} \\sum_{I'\\in\\{I\\}} \\text{HalluQ} (\\{Q_{I'}\\}) \\times 100\\%$, (2)\nwhere HT is Hallucinated Triplets, TT is Total Triplets, $\\{Q\\}$ and $\\{I\\}$ are the sets of questions and images that LVLMsare evaluated on, respectively, and $\\{Q_{I'}\\} \\subseteq \\{Q\\}$ suggest the subsets of questions related to the image $I'$. For bothmetrics, lower values demonstrate fewer hallucinations. Since the total number of questions and images is maintainedthe same for all evaluated LVLMs, HalluQ$(\\cdot)$ and HalluI$(\\cdot)$ are indeed comparable and unbiased."}, {"title": "3.3 Evaluation Pipeline", "content": "With the definitions and evaluation metrics provided in \u00a73.1 and \u00a73.2, the remaining problems contain two parts: 1)how to extract the knowledge graph $G_{\\Theta}$ from LVLM responses $A_{\\Theta}$, and 2) how to judge a triplet in $G_{\\Theta}$ is hallucinatedor not. The overview of our pipeline is illustrated in Figure la.\nKnowledge Graph Extraction. Given an LVLM response $A_{\\Theta}$ with the corresponding question Q and image I, weextract the knowledge graph $G_{\\Theta}$ from $A_{\\Theta}$ via prompting GPT-4. Check our prompt for knowledge graph extractionin Figure 6. Afterward, we propose two different strategies to judge whether a triplet $(v_1, e, v_2) \\in G_{\\Theta}$ includeshallucination based on the ground truth answer A and the image scene graph G, as described in the following.\nNLI Judge. The first strategy is implemented with a natural language inference (NLI) [29] model 1. Specifically,given an extracted triplet, we first calculate its cosine similarity scores with all triplets in the image scene graph G andonly retain those ground truth (GT) triplets with similarity scores greater than 0.5 to refine the information that will beused for the NLI model. If no triplets in G meet this criterion, only the top three GT triplets with the highest similarityscores will be kept, which are then taken as ground truth inputs for the NLI model to make predictions. If the NLIscore between the extracted triplet and ground truth triplets is lower than 0.6, suggesting the extracted triplet cannot beinduced based on GT triplets, and therefore, resulting in a hallucination."}, {"title": "4 Tri-HE Construction", "content": "Following the formulation in \u00a73, in this section, we provide a detailed discussion on how to construct our Tri-HEbenchmark for a unified triplet-level evaluation of both hallucinations in LVLMs.\nDataset. We build Tri-HE upon the GQA dataset [32], which requires LVLMs to conduct commonsense reasoningbased on a given image. The provision of scene graph annotations aligns with our formulation of hallucination evaluationat the triplet-level.\nData collection. However, several scene graphs in GQA are only equipped with parts of the relations among objectsappearing in the image, making necessary information required for question answering absent. To address that, we firstemploy a filtering process to identify images with adequate distinct relations in the scene graph annotation. Specifically,we start by selecting 300 images with more than 5 relations from the corresponding GQA scene graph annotations.To enlarge the question-answer pairs for each GQA image, we utilize GPT-4V 3 to generate new questions requiringcommonsense reasoning based on the input image. By leveraging the prompt provided in Figure 7, we prompt GPT-4Vto generate ten questions for each image along with their corresponding answers and relation triplets that illustrate thereasoning process. Afterward, we manually verify the correctness of generated questions, answers, and triplets, whileonly questions indeed requiring reasoning are maintained. Moreover, the verified triplets are added to the original scenegraphs collected in GQA.\nStatistics. The overall statistics for Tri-HE are summarized in Table 1. As described in Figure 2, each image inTri-HE is linked to a scene graph and a set of question-answer pairs that require reasoning, accompanied by groundtruth triplet annotations. Note that since the quality of each question in Tri-HE is manually verified, expanding its sizerequires significant resources and poses challenges. Nonetheless, the number of images and questions in Tri-HE is"}, {"title": "5 Results on the Tri-HE Benchmark", "content": ""}, {"title": "5.1 Evaluated LVLMS", "content": "We selected six open-source LVLMs for evaluation, including the LLaVA series [14, 2], MiniGPT-4 [3], InstructBLIP [1],Shikra [15], and InternLM-XComposer2 (abbrev., InternLM2) [33]. For all evaluated LVLMs, we selected the 7Bvariants to ensure fair comparison. Additionally, we test the recent popular Llama-3.2-Vision-Instruct model (abbrev.,LLaMA-3.2) [34] and used its smallest version (11B). The official prompt templates and inference configurations wereused for all LVLMs. All experiments are conducted on two Nvidia A100 GPUs."}, {"title": "5.2 Main Result", "content": "LVLM comparison. Table 2 compares hallucination rates of different LVLMs on our Tri-HE benchmark. As can beseen, all the evaluated LVLMs suffer from generating hallucinations with at least 38% hallucination rates. Among theseLVLMs, InternLM2 [33] obtains the best overall performances, suggesting that its strategy to train with both text-imageand textual-only instruction data simultaneously helps better align its visual encoder and LLM, and thus, reduces itshallucination rates. Moreover, compared to LLaVA [2], Shikra [15] has consistently lower hallucination rates, which isbuilt upon LLaVA's structure with extra grounding capability introduced, indicating that introducing extra groundingcould help LVLMs reduce hallucination. Additionally, LLaMA-3.2 achieves the lowest relation hallucination rates,suggesting that a strong textual backbone can help mitigate relation hallucination. However, it exhibits a weaker abilityto accurately identify objects, impacting its object and overall hallucination rates. Since LLaMA-3.2 does not outperformother LVLMs with even more parameters, we do not adopt it in the remaining.\nRelation hallucination is more severe. Except for MiniGPT-4 and LLaMA-3.2, all the LVLMs generatemore relation hallucinations than object hallucinations. A possible explanation is that existing LVLMs lack reasoning abilities,which makes them easily confused and mess up the relations among objects. This further suggests that focusing onobject hallucination [5] is not enough for a throughout analysis of the LVLM reliability and a unified and comprehensivestudy like our proposed triplet-level evaluation is necessary.\nEvaluation pipeline. In addition, we observe that GPT-4 judge can provide clearer and more reasonable discriminationbetween models compared to NLI judge. We provide a more comprehensive investigation into the differences betweenthese two judges later in \u00a75.3. Besides, the evaluation results under both HalluI and HalluQ metrics demonstratethe same trend, proving the robustness of our proposed triplet-level hallucination evaluation setting under differentevaluation granularities.\nPerformance of GPT-4V. Except for the open-sourced LVLMs, we also explore the performance of state-of-the-artcommercial LVLMs [13, 35]. Due to the experiment budget, here we only evaluate the GPT-4V [13] model with 25randomly selected samples in Tri-HE. Specifically, we directly prompting GPT-4V to gather its answers and calculatethe corresponding hallucination rates, following the same setting in Table 2. We also report the performances ofopen-sourced LVLMs on the 25 selected images. As can be seen in Figure 1b, GPT-4V demonstrates its superior"}, {"title": "5.3 Analysis", "content": "Investigating automatic hallucination judgments with human judgments. In \u00a73, we propose to measure hallucina-tion on triplet-level and design two automatic hallucination judges. Here, we further illustrate the effectiveness of thetriplet-level evaluation setting by studying its correlation with human judgments. To conduct fine-grained hallucinationanalysis, previous works [36, 37] split a model response into sub-sentences first, on which their hallucination mea-surements are conducted. We regard this method as a baseline for comparison. Specifically, we sample a subset of 20images from Tri-HE and invite human annotators to score five-point-scale hallucination rates of the responses of all theLVLMs in \u00a75.1. Check Table 9 for the detailed annotation guidelines. The human annotators achieve a Krippendorff'salpha score [38] of 0.66, indicating a high inter-agreement.\nResults are shown in Table 3. We find that triplet-level hallucination rates have higher correlations with humanjudgments with both NLI and GPT-4 judges, indicating that identifying hallucination on triplets can lead to a moreaccurate, human-preferred evaluation for model responses. Moreover, we notice that the GPT-4 judge achieves ahigher correlation to human judgments compared to the NLI counterpart, revealing GPT-4's superior ability to findhallucinations, which is also consistent with our observation in \u00a75.2.\nInvestigating relation hallucination with object information. As concluded from \u00a75.2, existing LVLMs tend togenerate both object and relation hallucinations in their replies, while the relation hallucination rates are even higher.Since different LVLMs have pairs of objects (v1, v2) that they are familiar with (e.g., high-frequency object pairs in theinstruction data they are fine-tuned on) and might generate correct relations on these objects easily, we suppose that therelation hallucination problem might mostly be located in less-frequent object pairs. To verify this assumption, weextract all object pairs for each LVLM from their respective Go generated from responses on Tri-HE, and rank thesepairs based on their frequency 4. Then, we calculate each LVLM's relation hallucination rates on their most frequentobject pairs.\nAs in Table 4, all the LVLMs have significantly lower relation hallucination rates on frequent object pairs they arefamiliar with, suggesting that they know the possible relations among objects and understand how to choose a relationappropriately. Check more analyses regarding the impact of input length on hallucination rates in Appendix A."}, {"title": "5.4 Hallucination Mitigation", "content": "Regarding the reasons of LVLM hallucination, previous studies [36, 39] have identified modality misalignment asa key factor. Inspired by ECSO [40], we propose a training-free LVLM hallucination mitigation method via self-alignment [12, 40, 41]. Specifically, we disable the direct visual access of LVLMs to alleviate hallucinations resulting"}, {"title": "6 Conclusion", "content": "Starting from a unified definition of hallucinations, we propose a novel triplet-level LVLM hallucination evaluationframework for both object and relation hallucinations. Then we introduce Tri-HE, a novel triplet-level LVLM hallu-cination evaluation benchmark, with which, we conduct a throughout analysis of the discrepancy among object and"}, {"title": "Limitations", "content": "Due to the experiment budget, in Figure 1b, we evaluate GPT-4V on a randomly selected subset of 25 images; in Table 3,we conduct human annotation on another randomly selected subset of 20 images. Although we do not interfere with thedata selection process, there may still exist a few biases in the corresponding results. Also, we do not have the budget toevaluate more LVLMs on Tri-HE. We will try to provide more comprehensive results in future works once we get moreexperimental budgets."}, {"title": "Appendix", "content": ""}, {"title": "A More Analyses", "content": "Investigating hallucination rates with response length. Previous studies on LVLM hallucination evaluation suggestthat the length of model responses may influence the extent of hallucination [5, 39], as some LVLMs tend to produceshorter, safer outputs. However, directly instructing an LVLM to generate a response of a specific length is challenging.To address this, we instead truncate the responses to the first K tokens and compute hallucination rates, varying K toassess its impact on the results.\nAs shown in Figure 4, while the exact hallucination rates vary, the ranking of different LVLMs remains consistent as thenumber of tokens increases from 10. Overall, as fewer tokens provide insufficient data for triplet extraction, this findingsupports the robustness of our proposed triplet-level evaluation across LVLMs with varying response lengths."}, {"title": "B More on Hallucination Mitigation", "content": "Ablation study. We conducted an ablation study on a general description (i.e., \u201cdescribe the image\"). As shownin Table 6, the combination of both triplet description prompting and disability of visual input contributed to theeffectiveness of our hallucination mitigation approach.\nMitigation on LLaVA-1.5. To provide a comprehensive evaluation, we conducted additional experiments usinganother LVLM (i.e., LLAVA-1.5) to mitigate hallucinations. The results presented in Table 7 indicate that both thegeneral description and triplet description methods show an improved reduction in hallucinations, further highlightingthe effectiveness of our mitigation approach.\nBaseline Comparison. Current methods for mitigating hallucinations typically involve retraining, integrating externaldetection models, and devising decoding strategies. Compared to existing works, our approach is a plug-and-playmethod that neither requires costly retraining nor relies on external models. To make a more fair comparison, we\""}, {"title": "C NLI Threshold Selection", "content": "We randomly selected question instances from 10 images and reviewed the set of filtered triplets that were returned.The similarity score threshold was adjusted to 0.5 for the most reasonable returned triplets. These triplets laterconcatenate together as the ground truth required for generating NLI judgments. In determining if a generated tripletwas hallucinated, we further review the NLI judgment results in different thresholds, ultimately deciding on a thresholdof 0.6."}, {"title": "D Object Pairs Extraction and Ranking", "content": "In this section, we detailedly describe how we obtain object pairs and their rankings from LVLM responses. Supposewe have all an LVLM's responses to all questions in Tri-HE, i.e., Go, we first extract all the object pairs (v1, v2) fromGo. Then for each object, we replace it by the name of its synset using WordNet to reduce the total types of objects.Afterward, we could calculate the frequency of each object pair and rank them based on their frequency. This rankingwill then be used to calculate the first 20% frequent object pairs in Table 4."}, {"title": "E More on Prompts", "content": ""}, {"title": "E.1 Prompt for triplets extraction with GPT-4", "content": "The prompt for extracting triplets in the answer generated by LVLMs is illustrated in Figure 6."}, {"title": "E.2 Prompt for question generation with GPT-4V", "content": "The prompt for generating questions, answers, and corresponding triplets with GPT-4V is shown in Figure 7."}, {"title": "E.3 Prompt for GPT-4 Judge", "content": "The prompt for our proposed GPT-4 judge method is illustrated in Figure 8.\nGiven a list of reference triplets (\"object1\", \"relation\", \"object2\") extracted from the scene graph of an image,along with a list of objects observed in this image, your task is:\nTask 1. Determine if a claim triplet (\"object1\", \"relation\", \"object2\") is directly supported by any sin-gle triplet in the reference, or can be logically inferred from multiple reference triplets and the list of objects.Follow these steps when finishing the task:\n1. Answer \"yes\" if the claim appears in the reference.\n2. Answer \"yes\" if the claim can be logically inferred from one or more triplets in the reference. Consider:\na. General Inferences: Assess common associations or implications.\nb. Conditional Phrases: Note phrases like \"could be\", \"might\", \"suggests\", which allow broader inferences.\nc. Equivalence of Objects: In your judgment, treat objects of the same kind as equal. For example, \"woman\",\"man\" should be considered under the general category of \"person\".\nd. Support from Object List: If the claim is not directly supported or inferable from the triplets, assess whetherthe list of objects provides additional evidence to support or infer the claim.\n3. Answer \"no\" if the claim neither directly matches any triplet in the reference nor can be reasonably inferredfrom the triplets and the object list.\nTask 2: Error categorization.\nIf your answer to the previous task is \"no\", determine whether the not supported/inferred part in the claim is\"object1\" or \"object2\" or \"relation\"."}, {"title": "E.4 Prompt for LVLMs in Evaluation", "content": "The prompt for generating responses from LVLMs for evaluation is the combination of a question and the correspondingimage."}, {"title": "F Human Annotation Guideline", "content": "The detailed guidelines of our human evaluation tasks are shown in Table 9. Noting that two types of inferences in themodel responses are regarded as hallucinations during human annotation:\n1. Unreasonable inferences (inferences that violate commonsense knowledge).\n2. Inferences that are correct, yet cannot be correctly inferred from the image."}, {"title": "G More Discussion", "content": "Future works. Currently, the proposed triplet-level evaluation is primarily deployed on LVLMs, whose extensionto diffusion models [43, 44, 45, 46, 47, 48] is feasible, while for the hallucination mitigation proposed in \u00a75.4 can befurther enhanced by utilizing stronger vision encoder [49, 50, 51, 52] and visual tools (e.g., object detectors [53, 54]) tobetter extract visual information for LLM reasoning."}]}