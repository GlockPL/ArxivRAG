{"title": "Mapping Galaxy Images Across Ultraviolet, Visible and Infrared Bands Using Generative Deep Learning", "authors": ["YOUSSEF ZAAZOU", "ALEX BIHLO", "TERRENCE S. TRICCO"], "abstract": "We demonstrate that generative deep learning can translate galaxy observations across ultraviolet, visible, and infrared photometric bands. Leveraging mock observations from the Illustris simulations, we develop and validate a supervised image-to-image model capable of performing both band inter-polation and extrapolation. The resulting trained models exhibit high fidelity in generating outputs, as verified by both general image comparison metrics (MAE, SSIM, PSNR) and specialized astro-nomical metrics (GINI coefficient, M20). Moreover, we show that our model can be used to predict real-world observations, using data from the DECaLS survey as a case study. These findings highlight the potential of generative learning to augment astronomical datasets, enabling efficient exploration of multi-band information in regions where observations are incomplete. This work opens new pathways for optimizing mission planning, guiding high-resolution follow-ups, and enhancing our understanding of galaxy morphology and evolution.", "sections": [{"title": "1. INTRODUCTION", "content": "Galaxies can appear quite different when observed across various photometric bands due to the wavelength-dependent nature of the light they emit (Kouroumpatzakis et al. 2023). Each photometric band captures a specific range of wavelengths, highlighting different physical properties of galaxies (Van den Bergh 1998). In shorter wavelength bands, like the ultraviolet (U) band, galaxies reveal the presence of hot, young stars and regions of active star formation. Meanwhile, in optical bands (e.g., G, R), the light primarily comes from older, cooler stars. At longer wavelengths, such as in infrared bands, the emission is dominated by dust and the cooler, more evolved stellar populations.\nStudying galaxies across multiple photometric bands is essential because each band reveals different aspects of a galaxy's physical properties, offering a more complete picture of its structure, composition, and evolution. By combining observations from various bands, it is possible to trace a galaxy's star formation history, stellar populations, dust content, and even interactions with its environment. Observations made in multiple wavelengths require fewer inferences and lead to stronger conclusions being drawn about the observed galaxies. This makes multi-band astronomy critical for developing a detailed understanding of a galaxy's life cycle and the processes driving its evolution. For further details, see Kouroumpatzakis et al. (2023) and Van den Bergh (1998)."}, {"title": "1.1. Image-To-Image Translation", "content": "Image-to-image translation refers to a class of computer vision modeling techniques that aims to transform an image from one domain into another while preserving essential visual content (Pang et al. 2021). This process has gained"}, {"title": "1.2. Deep Learning Applications in Astronomy", "content": "Traditional data analysis methods in astronomy, particularly for studying galaxies, have been vastly outpaced by the enormous volume of data generated by modern telescopes and surveys, as outlined by Zhang & Zhao (2015). These traditional approaches often involved painstaking manual inspection and interpretation, making them time-consuming and less efficient. An example of this would be the Galaxy Zoo project, see Willett et al. (2013), where crowdsourcing was used to assign morphological labels to images of galaxies. However, deep learning algorithms have revolutionized this process by automating many tasks once performed manually. For example, deep learning has proven instrumental in classifying galaxies by identifying their various types and structures as in De La Calleja & Fuentes (2004). This automation has significantly accelerated the analysis process and increased reliability, allowing astronomers to handle larger datasets and uncover new insights more effectively. For a comprehensive dive into machine learning and deep learning applications in astronomy, see Fluke & Jacobs (2019).\nInitially, discriminative learning was favored over generative learning because it was more straightforward and computationally feasible given the technology and resources available. Discriminative models, which focus on classifying data and predicting labels by learning the boundaries between different classes, were simpler to implement and required less computational power relative to generative models. Recently, however, generative learning has become popular in astronomy due to significant advancements in computational power and the development of sophisticated algorithms. Examples of generative learning in astronomy can be seen in Spindler et al. (2021) and Smith et al. (2022). However, these approaches are for unconditioned generative modeling where the output is not conditioned on any input. Image-to-image models are a subset of generative models that differ from their unconditioned counterparts in conditioning the output image on a particular input image.\nImage-to-image modeling has seen some applications in astrophysics in recent years. Initially, image-to-image mod-eling aimed at reconstructing noise signatures and denoising input images. Lin et al. (2021) was the first attempt to apply image-to-image techniques to reconstruct noise signatures and perform translation between sky surveys. Vo-jtekova et al. (2020) proposed a U-net-based approach for image denoising and enhancement. Similarly, Liu et al. (2023) presents a denoising method that differs from Vojtekova et al. (2020) by requiring no labeled data and in-stead relying on self-supervision (Quan et al. 2020) to train their models. More recently, Kinakh et al. (2024) have trained and compared several image-to-image models, including both GAN and DDPM based models, to translate Hubble Space Telescope (HST) data into James Webb Space Telescope (JWST) imagery. However, their approach is for indiscriminate patches of sky and does not isolate any one class of astronomical phenomena (e.g., galaxies, stars, nebulae)."}, {"title": "1.3. Our research", "content": "In this paper, we explore the problem of translating across different photometric bands (Bessell 2005) for Galaxy observations using a custom generative image-to-image machine learning model. The challenge of translating images of galaxies across photometric bands using image-to-image modeling remains relatively unexplored. This offers a promising area for further research and exploration. Despite the great successes of image-to-image machine learning"}, {"title": "2. PROPOSED APPROACH", "content": ""}, {"title": "2.1. Dataset", "content": "Two distinct datasets are used throughout this work. The first dataset is a collection of mock observations obtained from the Illustris simulations. Most of our testing and prototyping has been conducted using the Illustris mock observations. The second data set is a collection of real observations of galaxies taken from the Galaxy10 DECALS dataset courtesy of the Python package astroNN by Leung & Bovy (2018). This dataset provides a proof of concept to demonstrate that our models can be trained on and make inferences from real data."}, {"title": "2.1.1. Illustris Dataset", "content": "The Illustris mock observation catalog (Torrey et al. 2015) is a synthetic dataset generated from the Illustris cos-mological simulations (Vogelsberger et al. 2014), which model the formation and evolution of galaxies over cosmic time. The Illustris project simulates a wide range of physical processes that govern galaxy formation, including dark matter dynamics, gas cooling, star formation, supernova feedback, and black hole accretion. These simulations cap-ture the complex interplay between baryonic matter and dark matter, allowing for the detailed modeling of galaxy"}, {"title": "2.1.2. Galaxy 10 Decals Dataset", "content": "The Galaxy10 DECals dataset, as obtained through the astroNN Python package from Leung & Bovy (2018), contains over 17 thousand colored galaxy images available in G, R, and Z bands. The observations are collected from the DESI Legacy Imaging Surveys or DECaLS (Dey et al. 2019). The images are normalized to scale the image pixel values from [0, 255] to [0, 1], but otherwise the images are not altered or preprocessed in any other way."}, {"title": "2.2. Metrics", "content": "Various metrics are used to assess the quality of the output images as compared to target images. Some of these metrics are general image comparison metrics, in the sense that they can be applied to any set of input images. Others are specialized metrics specifically designed for analyzing galaxy morphology. We use the following set of metrics:\n\u2022 MAE: The mean absolute error between output images and target images. This averages the absolute per pixel differences.\n\u2022 SSIM: The Structural Similarity Index, or SSIM (Brunet et al. 2011), measures the distance between a pair of input images. Unlike the pointwise MAE, the SSIM can account for local structural differences. The SSIM has a range of [-1.0, 1.0] with values closer to 1.0 indicating high agreement between the images, 0.0 indicates no relation and values closer to -1.0 indicate disagreement between the inputs.\n\u2022 PSNR: The Peak Signal-to-Noise Ratio (PSNR) is a widely used metric for assessing the similarity between two images, particularly in the context of image compression and reconstruction. It measures the ratio between the maximum possible pixel value (signal) and the power of the noise (difference) between the original and the reconstructed image. PSNR is expressed in decibels (dB), with higher values indicating greater similarity, as they imply lower error between the images. A PSNR value around 30-50 dB is generally considered to imply that the reconstructed image is of high fidelity, with the reconstructed image almost indistinguishable from the original to the human eye for PSNR around 40 dB.\n\u2022 GINI and M20: The Gini and M20 measurements, first presented in Lotz et al. (2004), are used to characterize the morphological features of galaxies. The Gini coffecient is a statistical measure originally used to quantify income inequality, but in galaxy morphology, it quantifies the distribution of light among a galaxy's pixels. The M20, on the other hand, is a measure of the relative contribution of the brightest 20 percent of a galaxy's light to its overall light distribution. Specifically, it quantifies the spatial distribution of the brightest regions within a"}, {"title": "2.3. Inference Techniques", "content": "Most sky surveys rarely capture information in one band. For example, in conducting translations from the 2MASS'S K band observation (Skrutskie et al. 2006), information from 2MASS's H band observations could be leveraged with minimal additional extraction and preprocessing overhead. To fully exploit all the information available from any given survey, we introduce band extrapolation. With extrapolation, a sequence of bands is extended to infer bands further along the sequence. For instance, we could select bands G and R as input and train a model to extend this sequence in either direction by inferring lower frequency bands (such as Z and K), or higher frequency bands (such as U or NUV).\nWe also introduce band interpolation, where instead of extending a sequence of observations, intermediate observa-tions are generated at specific bands delineated by both input bands. For example, for inputs such as U and K, we could generate G, R, and Z observations. To clarify, each output band requires a new instance of the model. This leads to the training of multiple models to any combinations of input bands to different output bands. This applies to both interpolation and extrapolation approaches.\nThere are inherent pros and cons for each approach. The extrapolation models are at a disadvantage compared to the interpolation models, as they must extend in wavelength away from the input bands. Interpolation is able to use information from both high and low wavelength bands, by comparison. However, band interpolation will often require that the data will be gathered from multiple surveys, making it logistically more challenging."}, {"title": "2.4. Generator Training", "content": "The inputs of the model are denoted as Xph where ph gives the photometric band such that Xu and XK denote inputs from the U and K photometric bands respectively. The output of the model will be denoted by \u0176 where Y is to be substituted with the target output band. In this manner, a model accepting inputs XNUV and XK having outputs R is a model trained to interpolate the R band from corresponding NUV and K inputs. In the previous example, the ground truth labels would be denoted as R, and this notation will carry forward in subsequent sections.\nWe elected to employ a fully supervised training process to train our generator models. Our models can be considered a complex non-linear transformation with the model weights controlling the transformation applied to the input(s). Training our models consists of finding the model weights that minimize the distance between the output and the ground truth label. This distance is referred to as the loss function.\nOur loss function is the combination of the L\u2081 (mean absolute) error and the inverse SSIM loss given by,\n$L_{SSIM} = 1 - SSIM(Y, \\hat{Y}),$ \nwhere $SSIM(Y, \\hat{Y})$ corresponds to the SSIM between the ground truth and reconstructed images. This loss function was determined through a series of trial and error. The full objective to be minimized during training is\n$L = C_1 + \\lambda L_{SSIM},$ \nwhere $\\lambda$ is a regularization parameter greater than 0. We experimented with setting $\\lambda = 0$, but found that setting $\\lambda = 1$ yielded higher quality results. Investigating the effects of $\\lambda$ values greater than 1 would be of interest."}, {"title": "2.5. Generator Architecture", "content": "We employ a ResNet-like architecture (He et al. 2016), primarily composed of residual blocks. This architecture emphasizes residual learning, making it easier for the model to learn identity mappings if needed. The generator itself follows the architecture of the generator networks in the CycleGAN implementation (Zhu et al. 2017). However, after performing hyperparameter tuning, we found the default hyperparameters in CycleGAN did not perform best compared to other configurations. In summary, we decreased the number of downsampling and upsampling blocks to increase the resolution of the latent residual layers along with increasing the number of residual blocks from 6 to 9. For more details on model selection and training, see Appendix A. Note that we have elected not to use a diffusion-based architecture due to the probabilistic nature of the image generation process, which may yield several outputs for any one input."}, {"title": "3. RESULTS", "content": ""}, {"title": "3.1. Interpolation", "content": "We now investigate the performance of three models tasked with band interpolation based on the same input. The input bands are the NUV and K band which correspond, in wavelength, to GALAX's NUV and 2MASS's K band respectively. We note significant differences between both input bands since we have selected the bands to be quite far apart to provide the models with the opportunity to showcase their ability to handle non-trivial transformations.\nWe trained three models to map to G, R, and Z bands corresponding in wavelength to SDSS bands. The reason for selecting those bands is to investigate the theoretical augmentation of SDSS's library using GALAX and 2MASS observations. A paired set of observations taken from both GALAX and 2MASS could be used to augment the SDSS database significantly. The task of obtaining and preprocessing such a paired data set is not one we have investigated, but is theoretically possible and would potentially produce high-quality images to augment SDSS observations with minimal inference cost.\nThe inputs and the interpolated results can be seen in Figure (1). The generated outputs are nearly identical to the ground truth data. This is due to the model being able to take advantage of the information present in both bands. The model can detect areas of high star formation from the NUV band while also having access to the distribution of the majority of the stellar matter through the K band. Leveraging information from both bands allows it to make predictions that are more precise than is possible with only one band's input."}, {"title": "3.2. Extrapolation", "content": "This section investigates the performance of three models tasked with band extrapolation based on similar inputs. The input bands are the G and R bands, which correspond, in wavelength, to SDSS's G and R bands. The two input bands are proximal in wavelength but still provide the model with sufficient information as to the progression of band observations. Further experimentation is needed to investigate the effect of the input bands' proximity. A different band combination (such as G and Z) may provide the model with better information.\nWe trained three models to map to U, NUV, and FUV bands corresponding in wavelength to the SDSS U band and the GALAX NUV and FUV bands, respectively. All three output bands are lower in wavelength than the input bands, which presents the models with more challenging transformations. Lower wavelength inferences are more demanding than their higher wavelength counterparts because of the inherent complexity in regions containing high star formation rates. Compared to band interpolation, this task is, practically, more straightforward since it is highly probable that all of the input data can be obtained from a single survey.\nThe inputs and the extrapolated results can be seen in Figure (3). As with the band interpolation, the generated images are nearly identical to the ground truth data. The model is able to recreate structure present in the ground truth data with a very high degree of accuracy. Of particular note is the models' ability to preserve their accuracy"}, {"title": "3.3. DECALS Observations", "content": "We train our model architecture on real observational data from DECALS (see the data description in Section 2.1.2). A single model is trained to interpolate the R band based on input G and Z bands, respectively. Training on DECALS data is highly significant, as it represents our model's capacity to learn and generate from real data, rather than mock observational data from the Illustris simulations. While the wavelength difference between the input bands and output band is smaller than the results of the models presented in Section (3.1), this concession is necessary to gauge how noise and various background artifacts in real observational data can complicate the training process.\nFigure (5) shows the input images (G and Z bands), the ground truth and generated R band images, along with the residuals between the generated and ground truth images. A significant level of agreement between the ground truth data and the generated images is evident. Of note is that the signatures of the central bulge are much more similar to the ground truth images compared to either of the inputs. Similarly, the outline of the spiral arms is in line with the ground truth images. The residuals contain mostly noise and are free from any significant signs of structure. The model is recreating the galaxy structure present in the ground truth data."}, {"title": "4. CONCLUSION", "content": "In this study, we have demonstrated the feasibility and effectiveness of generative deep learning models for translating galaxy observations across photometric bands. Our model is a convolutional neural network with a ResNet-like architecture that uses a combination of MAE loss and SSIM score as a custom loss function. The model is a supervised image-to-image model that can perform interpolation or extrapolation of photometric bands from input band sources. Our experiments use mock galaxy observations from Illustris simulation data and real-world observational data from DECALS.\nThe main contributions of this paper include: (i) outlining a straightforward image-to-image model that relies on supervised training using a custom training loss; (ii) demonstrating that our image-to-image model successfully performs a variety of mappings between different photometric bands; and (iii) demonstrating that the Illustris mock observation catalog can be used to develop and prototype image-to-image models (and that successes with the Illustris dataset carry over into observed datasets). Our main contribution is a proof of concept that image-to-image models can be used to translate observations of galaxies across different photometric bands."}, {"title": "APPENDIX", "content": ""}, {"title": "A. TRAINING DETAILS", "content": "We trained our each of our models for 128 epochs on a V100 GPU which took approximately 2 hours for each training instance. The model uses around 12 MB of memory and, once trained, can be used to make around 1000 inferences in about 5 minutes (using the same V100 GPU), making it very practical to deploy and run."}, {"title": "A.1. Datasets", "content": "Following the preprocessing discussed in Section (2.1), the images in both the Illustris and DECALS datasets were resized to a resolution of 128 \u00d7 128. The Illustris training dataset is comprised of 2000 samples (a sample is a set inputs along with an associated ground truth label), while the validation dataset contains 500 samples. The DECALS training dataset contains 8000 samples while the validation dataset contains 2000 samples.\nThe reason for the DECALS datasets being larger than the Illustris datasets is that the DECALS samples contain more noise and background artifacts which makes learning from the DECALS dataset more challenging compared to the Illustris dataset. To compensate for this, we selected a larger number of samples for the DECALS dataset compared to the Illustris dataset."}, {"title": "A.2. Generator Hyperparameters", "content": "We have based our model architecture on the model presented in Zhu et al. (2017), which proposed a generator architecture with a ResNet backbone as in He et al. (2016). The architecture is composed of down-sampling blocks followed by residual blocks, first presented in He et al. (2016), and finally up-sampling blocks. The down-sampling blocks reduce the spatial dimensions of the input image while capturing high-level features. This is achieved using strided convolutions, which compress the image's resolution, allowing the network to focus on broader patterns and context. The residual block preserves the input's spatial information while transforming features. It consists of convolutional layers with skip connections that add the input directly to the output, facilitating gradient flow during training. The up-sampling block restores the compressed spatial dimensions back to the original size using transposed convolutions. This process reconstructs finer details in the output image. For more information on different types of convolutions used in deep learning, see Dumoulin & Visin (2016).\nZhu et al. (2017) proposed that for images of size 128 \u00d7 128, the generator architecture be comprised of 2 down-sampling blocks followed by a latent space containing 6 residual blocks and 2 up-sampling blocks. We found that a different hyperparameter configuration works better for our purposes (note that hyperparameters are the configuration parameters of the model, and are not learned during training). Our generators utilize only 1 down-sampling and up-sampling block while increasing the number of residual blocks from 6 to 9. Our reasoning for this configuration working better is that it contains a wider bottleneck (in other words less down-sampling or compression of the input) and relies more on residual learning. A small bottleneck does not transmit high-frequency features, such as spirals and bars, but only transmits low-frequency features such as overall shape and orientation. Given that the most significant differences between photometric bands are mainly high-frequency features, it follows that the more aggressive the bottleneck, the less adept our network is in resolving these high-frequency features."}]}