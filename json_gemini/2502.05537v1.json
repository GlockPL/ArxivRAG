{"title": "SEQUENTIAL STOCHASTIC COMBINATORIAL OPTIMIZATION Using HIERARCHICAL REINFORCEMENT LEARNING", "authors": ["Xinsong Feng", "Zihan Yu", "Yanhai Xiong", "Haipeng Chen"], "abstract": "Reinforcement learning (RL) has emerged as a promising tool for combinatorial optimization (CO) problems due to its ability to learn fast, effective, and generalizable solutions. Nonetheless, existing works mostly focus on one-shot deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied despite its broad applications such as adaptive influence maximization (IM) and infectious disease intervention. In this paper, we study the SSCO problem where we first decide the budget (e.g., number of seed nodes in adaptive IM) allocation for all time steps, and then select a set of nodes for each time step. The few existing studies on SSCO simplify the problems by assuming a uniformly distributed budget allocation over the time horizon, yielding suboptimal solutions. We propose a generic hierarchical RL (HRL) framework called wake-sleep option (WS-option), a two-layer option-based framework that simultaneously decides adaptive budget allocation on the higher layer and node selection on the lower layer. WS-option starts with a coherent formulation of the two-layer Markov decision processes (MDPs), capturing the interdependencies between the two layers of decisions. Building on this, WS-option employs several innovative designs to balance the model's training stability and computational efficiency, preventing the vicious cyclic interference issue between the two layers. Empirical results show that WS-option exhibits significantly improved effectiveness and generalizability compared to traditional methods. Moreover, the learned model can be generalized to larger graphs, which significantly reduces the overhead of computational resources.", "sections": [{"title": "INTRODUCTION", "content": "Combinatorial optimization (CO) problems cover a broad spectrum of application domains, such as social networks [45, 2], public health [17, 29], transportation [7], telecommunications [8], and scheduling [40, 55]. Hence, much attention has been drawn from both the theory and application communities toward solving CO problems. Recently, there have been studies using reinforcement learning (RL) to learn generalizable heuristic policies for CO problems on graphs [24, 39, 14, 5, 26, 6]. The RL policy is trained on a set of previously seen training graphs and generalizes to unseen test graphs of similar structure. This policy is usually represented using graph embedding methods such as structure2vec (S2V) [12] and graph convolutional networks (GCNs) [25]. However, most existing studies focus on one-shot deterministic CO, while research on another broad class of CO problems, which contain stochasticity and must be solved sequentially over multiple time steps, remains limited.\nProblem Statement In this paper, we study the sequential stochastic CO (SSCO) problem where we first decide the budget allocation for all the time steps, and then select a combination of nodes for each time step. Given a graph $G = (V, E)$, where $V$ and $E$ represent the set of vertices and edges on the graph, respectively, the number of time steps $T$, and the total budget $K$ for selecting nodes, the task is to decide 1) an allocation $K_1, K_2, ..., K_T$ of budget over the time horizon, together with 2) a"}, {"title": "RELATED WORK", "content": "RL for CO RL has been widely used to solve CO problems [35]. Nazari et al. [39] propose a generic RL framework for solving the vehicle routing problem (VRP), outperforming traditional heuristic algorithms. Bello et al. [4] introduce neural combinatorial optimization, combining neural networks and RL to solve the TSP problem. Kool et al. [26] develop an attention-based model using the REINFORCE algorithm, achieving superior results on various CO problems like TSP, VRP, and the Orienteering Problem (OP). Khalil et al. [24] combine RL with graph embedding to construct solutions for CO problems step by step. Chen & Tian [11] present a method where neural networks iteratively improve combinatorial solutions through learned local modifications. Deudon et al. [14] introduce a method where policy gradient techniques are used to train neural networks to develop heuristics for solving the TSP. Emami & Ranka [15] propose Sinkhorn policy gradient methods to learn permutation matrices for CO problems. Cappart et al. [9] combine decision diagrams with deep RL to enhance optimization bounds for combinatorial problems. Lu et al. [34] introduce an iterative approach that leverages REINFORCE to enhance solutions for VRPs. Recent methods have further integrated search algorithms, such as active search [20], Monte Carlo tree search [16], and beam search [28], to enhance the solution qualities of RL algorithms during inference time. While these algorithms show promising results, they typically tackle one-shot deterministic CO problems without sequential decision-making, limiting their effectiveness for more complex scenarios.\nGraph embedding Graph embedding techniques are crucial for RL-based solutions to CO problems, representing graph structures in continuous vector spaces to preserve structural properties, facilitate downstream RL tasks, and generalize to unseen (potentially larger) graphs. Node2Vec [18] uses biased random walks to explore neighborhoods efficiently, which maps nodes to low-dimensional vector spaces while preserving the neighbor structures. Deepwalk [43], which is inspired by language models, uses truncated random walks to obtain local node information and learn potential representations. Significant progress has also been made with graph neural networks (GNN) [44, 58, 53], such as GCN [25], Graph Attention Networks (GAT) [49] and Graph Transformer Networks [57]. Xu et al. [54] evaluate the expressiveness of GNNs, comparing them to the Weisfeiler-Lehman test in capturing graph structures. Li et al. [31] analyze the effectiveness and mechanisms of GCNs in semi-supervised learning. Ying et al. [56] introduce a method to interpret GNNs by identifying the crucial subgraphs and features that influence their predictions. Nonetheless, determining the most beneficial graph embedding technique for RL tasks remains an important research question.\nHierarchical reinforcement learning HRL introduces a hierarchical structure to RL, decomposing complex tasks into simpler subtasks to improve learning efficiency. The option framework [47] defines HRL with options as temporally extended actions, which enables agents to learn at different levels of temporal abstraction. Dayan & Hinton [13] introduce a multi-level RL approach based on subgoals, where high-level managers set tasks for sub-managers to achieve efficiently. Nachum et al. [38] propose a data-efficient HRL method using off-line experiences to train higher-level and lower-level layers, and the results demonstrate its ability to learn complex behaviors. Bacon et al. [3] develop a method to automatically learn policy, termination function, and intra-option policy in HRL using intra-option policy gradient and termination gradient, without extra rewards or subgoals. Vezhnevets et al. [50] divide agent behavior into two levels: the Manager, which sets goals in a latent space, and the Worker, which executes these goals with primitive actions. Kulkarni et al. [27] combine hierarchical structures and intrinsic motivation to improve learning in complex tasks. Andrychowicz et al. [1] introduce a technique called HER that enhances RL by learning from failures as if they were successes with different goals, and Levy et al. [30] integrate HER with HRL to enhance learning at multiple levels. Despite their success, these methods often do not address the specific challenges of SSCO problems, especially the need to handle sequential, combinatorial state-action spaces, and dynamic environments. Furthermore, they typically do not integrate graph-based representations, which could significantly enhance their effectiveness when dealing with graph-based CO problems."}, {"title": "THE WS-OPTION FRAMEWORK", "content": "This section will show how to learn budget allocation and node selection using our proposed WS- option framework. We begin with a brief introduction to the option framework, followed by the architecture used in our approach. Subsequently, we will formulate the MDPs for both hierarchical layers and adopt value-based methods as the backbone RL approach. In particular, to balance training"}, {"title": "PRELIMINARY: THE OPTION FRAMEWORK", "content": "The option framework, proposed by [47], is an implementation of temporal abstraction in RL. It provides a straightforward way to describe temporally extended actions, known as options, which are higher-level actions composed of several primitive actions. Formally, an option $o$ is defined by the tuple $(\u0399, \u03c0^{\u0399\u0399}, \u03b2)$, where\n\u2022 Initiation set $I$: a set of states where the option can be initialized. We will assume that any state satisfies $s \u2208 I$ in our problems.\n\u2022 Intra-option policy $\u03c0^{\u00b9\u00b9}$: a policy that maps states to actions under a given option, controlling the lower-level actions taken under that option. We use $\u03c0^{\u00b9\u00b9}$ as the lower-level node selection policy.\n\u2022 Termination function $\u03b2$: a mapping $\u03b2 : S \u2192 [0, 1]$ that specifies the probability of terminating the option in a given state, where $S$ is the state space.\nWhen an agent operates within the option framework, it follows the process: 1) Option selection \u2013 the agent selects an option based on its option policy given a state. 2) Option execution \u2013 once the option is determined, the agent follows the intra-option policy $\u03c0^{\u00b9\u00b9}$ until the option terminates. 3) Option termination \u2013 when the termination condition is met, the agent terminates the current option and proceeds to the next option selection.\nIn our study, we adopt a slightly different approach to defining options. While at the higher layer, the option $o$ we select represents the budget $K_t$ allocated at the current time step $t$, we simplify the information representation when passing the option to the lower layer. Specifically, we use option $o_t^1 = 1$ to indicate that the lower layer should continue selecting nodes, and option $o_t^1 = 0$ to indicate that it should stop selecting nodes and interact with the environment. It is important to note that to handle cases where the budget allocated by the higher layer is 0 (i.e., $o_t = o_t^1 = 0$), we introduce a null action $a_{t,\u00f8}$ for the lower layer. The null action $a_{t,\u00f8}$ represents directly interacting with the environment and moving to the next time step. In fact, we achieved very good results by introducing this null action."}, {"title": "HIERARCHICAL MDP FORMULATION", "content": "Here, we define the MDPs for the two layers of our hierarchical model (c.f. Figure 1). For ease of understanding, we will take the adaptive IM problem as a running example. More details of both problems are shown in Appendix C.2 and C.3, respectively."}, {"title": "HIGHER LAYER MDP", "content": "State The state $s^I = (X, g) \u2208 S$ includes two types of information: node features $X$ (e.g., inactive or active in adaptive IM problem) and global information $g$ (e.g., the remaining time steps $T$, and the remaining budget $K_T$).\nOption The option $o_t^I$ represents the budget $0 < K_t < K_T$ allocated to the current time step $t$.\nState transition The state transition in this layer is stochastic, which is a fundamental characteristic of SSCO problems. Given the current state $s_t^I$ and the action $a_t^{aI}$ from the lower layer, the state $s_{t+1}^I$ is updated according to the following function:\n$s_{t+1}^I = f(s_t^I, a_t^{II}) + \u03b7(s_t^I, a_t^{II}),$ (2)\nwhere $f(s_t^I, a_t^{II})$ is a deterministic function that updates the state $s_t^I$ based on the current action $a_t^{aI}$, while $\u03b7(s_t^I, a_t^{II})$ represents the stochastic component, which introduces randomness into the state transition, thereby capturing the stochastic nature of the environment. For example, in the adaptive IM problem, there is no deterministic component. Instead, the entire state transition is governed by the stochastic process $\u03b7(s_t^I, a_t^{II})$, which can be interpreted as the \"activation process\u201d within this problem.\nReward The total reward depends on the problem of interest. In adaptive IM, it is the total number of influenced nodes (e.g., active and removed nodes). Then, the immediate reward $r^I$ is the increase in the total reward."}, {"title": "LOWER LAYER MDP", "content": "State The state $s^{II}$ is also defined as $s^{II} = {X, g}$.\nOption The option $o^{II}$ can be either 0 or 1, where \"1\" represents continuing to select nodes, and \"0\" represents not selecting any node and interacting with the environment.\nAction Two kinds of actions are defined. Given the current time step $t$, the action $a_{t,v}^{II}$ is the node selected for a sub time step $j$ (e.g., $o_{t,j}^{II} = 1$) or just the null action $a_{t,\u00f8}$ (e.g., $o_{t,j}^{II} = 0$). And the $a_t^{I}$ is all the nodes selected in the current time step or the null action $a_{t,0}$ if no nodes are selected.\nState transition The state transition can occur either in a deterministic or stochastic way. If option $o_{t,\u00f8}^I$ is 1 and the action is node $v$, we update the state of node $v$ deterministically (e.g., activating node $v$). If option $o_{t,\u00f8}^I$ is 0, the agent interacts with the environment as described in the higher layer MDP.\nReward The reward in the lower layer significantly affects the convergence of the model. We first define the reward $r_{t,\u00f8}^{II}$ for the null action $a_{t,\u00f8}$, which means directly interacting with the environment at time step $t$. $r_{t,\u00f8}^{II}$ depends on the specific problem, and the intuition is to determine the expected reward when moving to the next day if we do not select any node in the current state. For propagation problems, we can choose $r_{t,\u00f8}^{II} = r - k_{eff}$, where $k_{eff}$ represents the effective number of nodes in action $a_t^I$ (e.g., number of inactive nodes in $a_t^I$ that can yield actual gain. For problems without a propagation process (e.g., TSP, RP), we can simply set this reward to 0. Next, we define the marginal reward $m_{t,v}$ for node $v$. We denote the selected nodes in the current sub time step $j$ as $A_t = {a_{t,v_1}, \u2026\u2026\u2026, a_{t,v_{j-1}}}$. The marginal reward is obtained as follows\n$m_{t,v} = R_t(s_t^I, A_t \u222a {a_{t,v}}) \u2013 R_t(s_t^I, A_t),$ (3)\nwhere $R_t (S, A)$ is the immediate reward from taking action $A$ in state $S$. In our experiments, this reward is estimated by averaging the results of multiple (typically 10) simulations. As a result, the reward for selecting node $v$ is given by\n$r_{t,v}^{II} = \\frac{m_{t,v}}{\\sum_{u \u2208 a_t^I} m_{t,u}} (r - r_{t,\u00f8}^{II}).$ (4)\nIn this manner, the total reward at each time step for the lower layer will remain consistent with the higher layer, enhancing the model's stability."}, {"title": "WS-OPTION", "content": "Due to the high interdependencies between the learned Q-values across the two policy layers, along with advantages such as reduced sample complexity, we employ a value-based RL for both layers. In our framework, the policies of both layers are e-greedy."}, {"title": "WAKE-SLEEP TRAINING", "content": "The primary challenge in HRL lies in ensuring stable training and model convergence. Notably, training both layers together can lead to instability, as the policies of each layer are interdependent and can interfere with one another. To address this issue, we devise a wake-sleep approach to enhance training stability. As we will show in Section 3.3.3, this proposed wake-sleep training procedure is well-aligned with our theoretical analysis. In Figure 2(a), we present the Q value learned by traditional HRL, where both layers are trained simultaneously from the beginning, and our proposed WS-option. Then, we will provide the details of this wake-sleep training procedure.\nIn the sleep stage of this training paradigm, the Q-function of the higher layer is initially frozen and an average budget allocation strategy is used instead. While more advanced strategies could be considered, we choose the simplest one to allow the lower layer to approach convergence under the current high-layer policy. The lower layer's Q-function is sufficiently trained along with the average budget allocation strategy in this stage. Meanwhile, the high layer's Q-function will be pre-trained offline using the trajectories of the average strategy. In the subsequent wake stage, both layers are trained online simultaneously, allowing for the fine-tuning and optimization of their interdependent policies, which ultimately lead to convergence."}, {"title": "LAYER-WISE LEARNING METHOD SELECTION", "content": "MC methods provide unbiased estimates by calculating the expected value via a complete trajectory, offering reliable and accurate results when the sample size is sufficient. In contrast, TD methods may reduce the accuracy of the Q estimation if errors occur during the intermediate process, known as error propagation. Additionally, in practice, TD methods often result in the overestimation of Q values. Hence, using MC methods to provide more reliable Q value estimates for the higher layer can enhance the model stability. Our experimental results show that this trick, combined with wake-sleep training, is one of the key components that makes the HRL training successful. Figure 2(b) shows the Q values learned using MC and TD methods. The Q values from TD methods are monotonic, while those from MC methods are concave.\nOn the downside, MC methods are usually more data-hungry because they have to wait until the end of a trajectory to get a training sample. Given that the lower-layer Q-learning (which is essentially a subtask that uses RL to solve one-shot CO problems) has been demonstrated to be reliable [24, 39, 14, 5, 26, 6], we choose the off policy TD-learning (Q-learning) framework for the lower-layer Q-function update. TD-learning has a faster convergence speed compared to MC methods. When training in the sleep stage, the rapid feedback of the TD method can help the lower layer converge faster, so that the whole model will be better initialized with more optimized parameters. This in turn will accelerate the co-training in the wake stage."}, {"title": "CONVERGENCE ANALYSIS", "content": "For completeness of our study and to offer intuition for our framework, we present a concise theoretical analysis of the algorithm's convergence. We first discuss the convergence of the intra-"}, {"title": "EXPERIMENTAL RESULTS", "content": "In this article, we examine two broad classes of problems: the propagation problem (high stochasticity) and the route planning problem (low stochasticity). For the propagation problem, we use an adaptive version of the classic IM problem. For the route planning (RP) problem, we consider a model tailored for travel route planning. Notably, instead of showing the standard deviation of test results, we perform two-sample t-tests to show the effectiveness of our framework in SSCO scenarios."}, {"title": "COMPARISON WITH BASELINES", "content": "Baselines To evaluate the effectiveness of our approach, we compare it against appropriate baselines for each problem, measuring the cumulative reward obtained by various methods. Each baseline is named in the form of A-B, where A is the higher-layer algorithm, and B is the lower-layer algorithm. For the AIM problem, we use the baselines mentioned in Tong et al. [48]: average policy, static policy, and normal policy at the high level. The static policy divides the time period T into d cycles, allocating the same resources only on the first day of each cycle. The normal policy allocates all resources at the beginning. At the lower level, we use a degree-based greedy strategy and a well-designed score-based strategy. The score for each node $v$ is defined as $s_v = \\sum_{u \u2208 N_{in}(v)} p(u, v)$, where $N_{in}(v)$ denotes the\nset of inactive nodes among the neighbors of $v$, and $p(u, v)$ is the probability that $u$ can successfully activate $v$. This method takes into account the characteristics of the problem and uses expected values to score each node, showing strong performance in our experiments. For the RP problem, we use genetic algorithms (GA) and greedy algorithms. We train and test on graphs of the same size here. The results are shown in Tables 1-2, where T and K are chosen according to Tong et al. [48]."}, {"title": "ASSESSING THE LEARNED POLICIES", "content": "We now assess the learned budget allocation and node selection policies using the AIM problem. By fixing one layer's policy, we compare the cumulative rewards obtained by only the other layer, enabling us to independently assess the policy of each layer.\nTo evaluate the higher layer, we fix the lower layer using its learned policy and vary the budget allocation policy. Conversely, to evaluate the performance of the lower layer, we fix the higher layer using the average budget allocation policy and vary the node selection policy. As shown in Table 3, our algorithm performs well even when applied to a single layer. Both the node selection and"}, {"title": "GENERALIZATION TO LARGER GRAPHS", "content": "Most excitingly, our algorithm generalizes well to larger graphs. This means that models trained on small graphs can be effectively applied to larger ones. The reason for this generalization ability lies in the framework we use, which does not include any parameters related to graph size or number of nodes. This allows us to use the same parameters for graphs of different sizes."}, {"title": "CONCLUSION", "content": "We propose an option-based HRL framework for solving complex SSCO problems. The core of our approach is to use the wake-sleep training procedure and layer-wise learning method selection to make the system more stable without interfering with each other, as well as to facilitate our learning process by giving a clear definition of MDPs of two layers. Through extensive experimental evaluations, we demonstrate the strong performance of our algorithm in solving the SSCO problem, its ability to learn effective budget allocation and node selection policies, and its strong generalization to larger graphs. However, a notable limitation of our algorithm is that each new problem needs a new graph embedding technique, which adds complexity to the framework's implementation across different scenarios. We leave the investigation of model-agnostic graph embedding techniques for RL-based solutions to SSCO as future work, potentially borrowing ideas of recent advancements on graph foundation models [33]."}, {"title": "THEORETICAL INSIGHTS", "content": null}, {"title": "HIERARCHICAL LEARNING FOR BI-LEVEL OPTIMIZATION", "content": "Based on the problem formulation in equation 1, we provide a high-level insight into the nature of this problem, which motivates the use of HRL. Specifically, the SSCO problem can be reformulated as a bi-level optimization problem:\n$\\max_{\\pi^I} J(\\pi^I, \\pi^{II*}(\\pi^I))$ (5)\nsubject to\n$\\pi^{II*}(\\pi^I) \\in arg \\max_{\\pi^{II}} J(\\pi^I, \\pi^{II})$ (6)\nwhere $J = \\sum_{t=1}^T r_t(S_t)$, $\\pi^I = \\{K_t\\}$, and $\\pi^{II} = \\{S_t\\}$. Here, $\\pi^I$ is constrained by equation 1b, and $\\pi^{II}$ satisfies the constraints in equation 1c.\nThe formulation highlights that the SSCO problem inherently exhibits a bi-level optimization structure. The higher-level policy $\\pi^I$ determines the budget allocation strategy (e.g., $\\{K_t\\}$), while the lower-level policy $\\pi^{II}$ selects the corresponding subsets (e.g., $\\{S_t\\}$) based on the higher-level decisions.\nGiven this hierarchical dependency, it is natural and intuitive to employ a hierarchical learning algorithm such as HRL to address such problems effectively.\nIn contrast, traditional single-agent RL methods struggle with this bi-level optimization problem due to the following limitations:\n\u2022 Hierarchical Dependency: Single-agent RL lacks the structure to handle the interdependent objective, as the lower-level policy $\\pi^{II}$ directly depends on higher-level decisions $\\pi^I$.\n\u2022 Exploration Efficiency: HRL separates exploration across two tractable levels, improving learning effiency compared to single-agent RL's flat exploration approach, which is one of the HRL's greatest advantages [52].\n\u2022 Scalability and Interpretability: Optimizing the joint action space of both levels in single-agent RL is computationally intractable. HRL, by decoupling the problem into manageable subproblems, offers a scalable, interpretable, and modular framework for solving complex hierarchical tasks."}, {"title": "INTUITION BEHIND WAKE-SLEEP OPTION FRAMEWORK", "content": "The core idea for solving this bi-level optimization problem is to decompose it into two separate single-level optimization problems.\nWhen the lower layer provides a conditionally optimal solution, denoted as $\\pi^{II*}(\\pi^I)$, the bi-level problem reduces to a single-layer problem, which corresponds to the objective defined in equation 5. In this context, the task is just to find the optimal policy for the higher layer. Conversely, when the higher-layer policy is fixed, the problem becomes solving the sub-optimization objective in equation 6, which is to find the corresponding conditionally optimal lower-layer policy. Building on this decomposition, the framework iteratively refines the higher-layer policy by alternating between the two optimization processes. Specifically, a higher-layer policy is first provided, and the corresponding conditionally optimal lower-layer policy is determined. This iterative procedure continues until the global optimal solution is achieved.\nHowever, in the context of the SSCO problem, the role of lower layer is to perform node selection, while the higher layer only adjusts the termination condition. Due to the nature of this task, we claim that the conditionally optimal lower-layer policy remains similar across different higher-layer policies $\\pi^I$. Given this, the wake-sleep procedure is employed. During the sleep stage, the lower layer determines a near-optimal policy. Subsequently, the two layers are trained jointly during the wake stage. Since the lower layer has already converged to a close-to-optimal policy, the joint training further refines both the higher-layer policy and the lower-layer policy, ensuring that the global optimal solution is achieved."}, {"title": "SIMPLIFICATION OF OPTIONS IN THE LOWER LAYER", "content": "In this section, we justify our simplification of options and provide a deeper understanding of the option framework within our HRL approach. First, we revisit the definition of an option $o$ in 3.1. In the traditional option framework, an option $o$ is defined as a tuple $(I, \u03c0^{\u0399\u0399}, \u03b2)$, where:\n\u2022 $I$ is the initiation set, which specify the states where the option can be initialized.\n\u2022 $\u03c0^{\u00b9\u00b9}$ is the lower-policy that the agent follows while the option is being executed.\n\u2022 $\u03b2$ is the termination condition that determines when the option should terminate.\nOptions allow the agent to execute temporally extended actions, thereby enabling more efficient exploration and decision-making by abstracting lower-level actions into higher-level policies."}, {"title": "WHY SIMPLIFY OPTIONS?", "content": "In our specific bi-level optimization problem, the primary role of the option in the lower layer is to determine when the lower-level policy $\u03c0^{\u00b9\u00b9}$ should terminate its node selection process. Specifically, the higher-level budget allocation $K_t$ implicitly decides the number of nodes the lower layer should select before terminating. Given this context, we simplify options due to the following reasons.\nRedundant Information and Enhanced Interpretability The numerical value of the budget $K_t$ does not provide additional useful information beyond determining the number of node selections. Therefore, conveying the exact budget value to the lower layer is unnecessary and does not contribute to the termination decision. By representing options as binary indicators, we clarify whether the lower layer should continue or terminate, enhancing the interpretability of the policy.\nImproved Learning Efficiency Reducing the option to a binary indicator allows the lower layer to focus solely on a straightforward termination condition, without needing to interpret specific budget values. This simplification minimizes the complexity of the learning task, leading to faster convergence and more stable learning processes.\nTherefore, we simplify the option passed to the lower layer to a binary indicator:\n\u2022 $o_{t,1}^{II} = 1$: Indicates that the lower layer should continue selecting nodes.\n\u2022 $o_{t,1}^{II} = 0$: Indicates that the lower layer should stop selecting nodes and interact with the environment."}, {"title": "JUSTIFICATION OF BUDGET IRRELEVANCE", "content": "It is important to note that the specific numerical value of the budget $K_t$ does not inherently affect the termination condition beyond determining the number of node selections. The budget serves as a constraint that limits the range of the lower layer's actions but does not need to be explicitly taken as part of the option. By abstracting the budget into a binary termination signal, we retain the essential functionality required for effective decision-making without introducing additional complexity.\nOverall, this design choice strikes a balance between simplicity and functionality, leveraging the hierarchical structure of our problem to enhance learning efficiency and policy interpretability."}, {"title": "LOWER-LAYER REWARD DESIGN", "content": "The reward structure in the lower layer is a critical component of the WS-option framework, as it directly influence both the stability and convergence of the model. Proper design of the reward ensures that the hierarchical optimization aligns seamlessly across layers, mitigating potential instability caused by sparse or inconsistent feedback signals."}, {"title": "CHALLENGES IN LOWER-LAYER REWARD DESIGN", "content": "The task of the lower layer is to select nodes based on the budget allocated by the higher layer. However, if the lower layer were to select all $K_t$ nodes in a single step, the size of the action"}, {"title": "NORMALIZATION OF MARGINAL REWARD", "content": "We normalize marginal rewards to ensure that the total reward of the lower layer for each time step t is the same as the reward of the higher layer, which is\n$\\sum_{u\u2208a_t^{II}} r_{t,u}^{II} = r = R(s_t^I, o_t^I, a_t^{II}).$ (7)\nAt each time step, the reward $R(s_t^I, o_t^I, a_t^{II})$ is determined jointly by both the higher-layer option $o_t^I$ and the lower-layer action $a_t^{II}$. Since both layers share the same reward and objective, it is essential that the rewards remain aligned.\nWhen the lower-layer actions are decomposed into sequential sub-actions (e.g., selecting one node at a time), the individual rewards for each sub-action must sum up to the total reward $R(s_t^I, o_t^I, a_t^{II})$. Without normalization, the reward distribution across these sub-actions could deviate, leading to inconsistencies between the lower layer's objective and the higher layer's global reward signal. As a result, by normalizing the marginal reward, we ensure: 1) reward consistency. The total reward for the lower layer matches the original reward defined by the hierarchical framework, maintaining coherence between the two layers; and then 2) objective alignment. Both the higher layer and lower layer optimize the same objective, preserving the shared purpose of achieving the overall framework's goals."}, {"title": "CONVERGENCE ANALYSIS", "content": null}, {"title": "PROOF OF THEOREM 1", "content": "Theorem. (Intra-option policy convergence). In our WS-option framework, given any Markov transition $(s_{t,\u03c4}, o_{t,\u03c4}, a_{t,\u03c4}, r_{t,\u03c4}, s_{t,\u03c4+1}, o_{t,\u03c4+1})$, the Q-value function $q^{II}(s_{t,\u03c4}, o_{t,\u03c4}, a_{t,\u03c4})$ converges to the optimal Q-value function $q^{II*}(s_{t,\u03c4}, o_{t,\u03c4}, a_{t,\u03c4})$ with probability 1, assuming that the higher-layer policy is fixed.\nProof. The update formula of intra-option Q-value in tabular form is as follows:\nq^{II}_{n+1} (s_{t,\u03c4}, o_{t,\u03c4}, a_{t,\u03c4}) =\\begin{cases} \\alpha r_{t,\u03c4} + \u03b3 \\text{max} q^{II} (s_{t,\u03c4+1}, o_{t,\u03c4+1}, a) & \\text{If } o_{t,\u03c4} > 0 \\text{ and } a_{t, \\neq} a_{t,\u03d5}, \\\\ alpha r_{t,\u03c4} + \u03b3 q^{II} (s_{t+1}, o_{t+1}) & \\text{otherwise}. \\end{cases} (8)\nAccording to Theorem 1 from [21], we need to show the update operator is a contraction mapping. For $o_{t,\u03c4} > 0$ and $a_{t, \\neq} a_{t,\u03d5}$,\n$\\mid q^{II}_{n+1} (s_{t,\u03c4}, o_{t,\u03c4}, a_{t,\u03c4}) \u2013 q^{II*}_{n+1} (s_{t,\u03c4}, o_{t,\u03c4}, a_{t,\u03c4}) \\mid$ = \\mid \u03b1 $r_{t,\u03c4} + \u03b3 \\text{max} q^{II} (s_{t,\u03c4+1}, o_{t,\u03c4+1}, a) - $ \u03b1 $r_{t,\u03c4} - \u03b3 \\text{max} q^{II*} (s_{t,\u03c4+1}, o_{t,\u03c4+1}, a) \\mid$\n =  \u03b3 \\mid \\text{max} q^{II} (s_{t,\u03c4+1}, o_{t,\u03c4+1}, a) \u2013  \\text{max} q^{II*}(s_{t,\u03c4+1}, o_{t,\u03c4+1}, a) \\mid\n$\u2264 \u03b3 \\text{max} \\mid q^{II} (s_{t,\u03c4+1}, o_{t,\u03c4+1}, a) \u2013 q^{II*}(s_{t,\u03c4+1}, o_{t,\u03c4+1}, a) \\mid$ (9)\nFor the other case,\n$ \\mid q^{II}_{n+1} (s_{t,\u03c4}, o_{t,\u03c4}, a_{t,\u03c4}) \u2013 q^{II*}_{n+1}(s_{t,\u03c4}, o_{t,\u03c4}, a_{t,\u03c4}) \\mid = \\mid \u03b1 r_{t,\u03c4} + \u03b3 q^{II} (s_{t+1}, o_{t+1}) - \u03b1 r_{t,\u03c4} - \u03b3 q^{II*} (s_{t+1}, o_{t+1}) \\mid $"}, {"title": "PROOF OF THEOREM 2"}]}