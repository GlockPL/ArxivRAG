{"title": "VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Chongling Rao", "Yushuo Guan", "Jiaheng Liu", "Fuzheng Zhang", "Chengru Song", "Qiang Liu", "Di Zhang", "Tieniu Tan"], "abstract": "The training of controllable text-to-video (T2V) models relies heavily on the alignment between videos and captions, yet little existing research connects video caption evaluation with T2V generation assessment. This paper introduces VidCapBench, a video caption evaluation scheme specifically designed for T2V generation, agnostic to any particular caption format. VidCapBench employs a data annotation pipeline, combining expert model labeling and human refinement, to associate each collected video with key information spanning video aesthetics, content, motion, and physical laws. VidCapBench then partitions these key information attributes into automatically assessable and manually assessable subsets, catering to both the rapid evaluation needs of agile development and the accuracy requirements of thorough validation. By evaluating numerous state-of-the-art captioning models, we demonstrate the superior stability and comprehensiveness of VidCapBench compared to existing video captioning evaluation approaches. Verification with off-the-shelf T2V models reveals a significant positive correlation between scores on VidCapBench and the T2V quality evaluation metrics, indicating that VidCapBench can provide valuable guidance for training T2V models.", "sections": [{"title": "1 Introduction", "content": "Controllable text-to-video (T2V) generation leverages text prompts to guide video synthesis (Team, 2024; Zhou et al., 2024b), enabling the instant visualization of designs and facilitating applications in creative content and entertainment. Advances in generative model's backbones (Blattmann et al., 2023; Esser et al., 2024; Peebles and Xie, 2023; Weng et al., 2024) further innovate the video generation process to adhere to textual instructions, exhibit aesthetic appeal, and conform to physical laws. Video captioning, the crucial supporting infrastructure to T2V generation, has also progressed. Coarse-grained or detail-lacking captions significantly hinder both the comprehension and reconstruction of visual information (Jin et al., 2024; Cheng et al., 2024). Hence, prevalent T2V models are devoted to strengthening the alignment between the generated content and the detailed prompts/captions (Kim et al., 2023; Liu et al., 2024b). With the objective to optimize this alignment, T2V models present high fidelity in subjects' motion (Wei et al., 2024b; Wang et al., 2024e; Zhou et al., 2024a), temporal changes (Guo et al., 2024; Yang et al., 2023; Xiong et al., 2024), and event progression (He et al., 2024b; Wang et al., 2024a). Meanwhile, the quality of datasets used for training captioning models (Hong et al., 2024; Zhang et al., 2023) has also improved considerably, such as OpenVid (Nan et al., 2024) and ShareGPT4V (Chen et al., 2025). To direct the optimization of T2V models, video captions should be accurate, comprehensive, diverse, concise, and abundant. While caption formats vary across different models (Ju et al., 2024; Zheng et al., 2024), the core elements in captions emphasized by T2V models seem to be converging. A practical evaluation of video captions for T2V generation must address three main challenges.\nAlignment with T2V evaluation: The evaluation should assess whether a video caption adequately covers aesthetics, content, motion, and physical laws, aligning with the key metrics of T2V generation. Diversity and stability: The diversity of evaluation data and the stability of the evaluation approach influence the accurate assessment of caption quality. Impact on T2V generation: The correlation between caption evaluation and T2V performance remains unexplored, lacking evidence on how captions influence the generated videos.\nTo address these challenges, we introduce VidCapBench, the first evaluation benchmark designed for assessing video captions in controllable T2V generation, as depicted in Figure 1. Prioritizing the video diversity, VidCapBench comprises 643 richly annotated video clips. These videos are annotated with critical aspects relevant to T2V generation, and we construct a discerning set of question-answer pairs decoupled from specific caption formats. The workflow of VidCapBench is transferable to arbitrary in-house datasets for the more targeted evaluation. The main contributions of this paper are summarized as follows:\n\u2022 We introduce VidCapBench, a novel benchmark designed to facilitate comprehensive and stable evaluation of video captions across multiple dimensions relevant to T2V generation.\n\u2022 We propose a two-stage evaluation method: rapid automated evaluation on a stable-to-judge subset provides quick feedback for developers, while introducing accurate human evaluation on the remaining subset offers crucial guidance.\n\u2022 Our experiments demonstrate that most open-source captioning models perform inferior to proprietary models like GPT-40. Applying captions to several production-ready T2V models reveals a strong positive correlation between the performance on VidCapBench and the quality of generated videos, validating the effectiveness of our proposed evaluation approach."}, {"title": "2 Related Work", "content": "Video captioning. The goal of video captioning is to describe a video across several key aspects, aiding understanding (Doveh et al., 2023), retrieval (Ma et al., 2024), and motion control (Wang et al., 2024d). In T2V generation, accurate and detailed video captions can enhance semantic alignment during model training (Polyak et al., 2024). Naive captioning models adopt free-form descriptions (Chen et al., 2024a; Wang et al., 2024c). To enhance controllability, MiraData (Ju et al., 2024), VDC (Chai et al., 2024), and Vript (Yang et al., 2024b) emphasize specific aspects like subjects, background, and shots, significantly benefiting T2V generation. Other methods describe videos from an event perspective (Wang et al., 2024a; He et al., 2024b), capturing temporal information more effectively. Despite advancements in caption controlling (Wang et al., 2023; Hua et al., 2024), evaluations with omissions may lead to a seesaw effect where gains in one dimension come at the cost of others, limiting the utility of the captioning model.\nEvaluation methods for video captioning. The advancement of T2V generation has spurred the development of evaluation approaches for video captioning. Traditional approaches (Xu et al., 2017, 2016) for short captions rely on legacy metrics like CIDEr and BLEU. For dense captions, inspired by image captioning evaluation (Liu et al., 2024a; Prabhu et al., 2024; Tu et al., 2024), many approaches employ question answering (QA) followed by natural language inference (NLI) with a critic model. Existing evaluation schemes of video captions are often tied to specific caption formats and suffer from instability in automatic evaluation. In this context, VidCapBench emerges as a more robust solution, offering a comprehensive and stable evaluation framework that aligns with the controllable T2V evaluation (Rawte et al., 2024; Huang et al., 2024; He et al., 2024a), providing better guidance for T2V model training."}, {"title": "3 VidCapBench", "content": "In this section, we introduce the design and curation of VidCapBench."}, {"title": "3.1 Preliminaries", "content": "Caption evaluation is typically performed through human or machine evaluation.\nHuman evaluation. Human evaluation demands annotators to assess captions based on predefined criteria. Experienced annotators deliver accurate and consistent evaluations, along with analysis of erroneous cases, which helps training T2V models. Currently, human annotation primarily employs two methods. In 5-point Likert scale, annotators rate captions on a 5-point scale (1 being worst, 5 being best) based on ground truth. Regarding candidate comparison, pairwise samples are presented to annotators to determine the ratio of good-same-bad and finally conclude with the ELO ratings.\nMachine evaluation. Human evaluation can be inconsistent among inexperienced annotators and is generally slower and more expensive. Conversely, automatic machine evaluation of video captions is faster and can provide some guidance for training. Mainstream machine evaluation often utilizes GPT-4 as a judge, which can be divided into two categories:\n\u2022 Predefined-QA paradigm: Multiple key information points are annotated for each video by QA pairs. Captions are evaluated by posing questions to the judge model, awarding points only for correct answers. Natural Language Inference (NLI) is used to categorize answers as \u201cEntailed", "Neutral": "or", "Contradictory": "n\u2022 Retrieval-based paradigm: This approach generates a series of yes/no questions about entities based on a given caption (Cho et al., 2023). A judge model then answers these questions using the original video as context. Descriptions corresponding to questions answered with \u201cno\u201d are considered hallucinatory. Note that this approach may incur high computational costs due to the repeated video question-answering process."}, {"title": "3.2 Benchmarking Video Captions", "content": "To establish a comprehensive evaluation framework for video captions, VidCapBench tackles two fundamental inquiries: what criteria should be employed to align the caption evaluation with T2V generation, and how to ensure a stable and reliable evaluation process."}, {"title": "Alignment with T2V evaluation", "content": "An effective T2V model is expected to produce videos with high visual fidelity, coherent object representation, precise semantic alignment with the input textual description, and realistic detail enhancement. Correspondingly, VidCapBench evaluates video captions across the following dimensions:\n\u2022 Video aesthetics (VA) encompasses the artistic and technical aspects of video creation, from filming techniques to post-production.\n\u2022 Video content (VC) refers to the narrative content presented in the video.\n\u2022 Video motion (VM) covers movements of foreground subjects and background objects.\n\u2022 Physical laws (PL) allow for more realistic or dramatic visual expression, even though creativity can somewhat bend them.\nEach dimension is further subdivided into specific sub-categories to ensure comprehensive and systematic evaluation coverage.\nStability of evaluation. Both the judge model's capabilities and the difficulty of evaluating the QA pairs influence the stability of machine evaluation. Here, we focus on the latter. Taking the VDC benchmark as an example, which contains roughly 100 QA pairs per video, we evaluate five models three times with GPT-40 under different random seeds. To analyze the evaluation stability of the QA pairs, we examine the number of times that the evaluation results are consistent across all three trials in the five models. Experimental results reveal that only 41% of the questions receive consistent evaluations, while 13% exhibit agreement at most twice out of five. Furthermore, we compare the accuracy (Acc) of five captioning models on a subset of all-agreed QA pairs with that on the full VDC benchmark. As shown in Table 2, the performance on the selected subset demonstrates significant discrepancies compared to that on the full benchmark, which highlights the unreliability of evaluating all QA pairs solely through automated methods. Instead, a more refined approach is warranted, wherein QA pairs should be categorized into two groups: (1) those suitable for automated evaluation due to their high machine evaluation consistency, and (2) the remaining, more challenging QA pairs that necessitate human intervention for nuanced differentiation."}, {"title": "Metrics of caption evaluation", "content": "Considering efficiency, cost, and stability, VidCapBench employs a predefined-QA paradigm to evaluate video captioning models. Due to the complexity of video content, answers may involve multiple adjectives, nouns, or verbs. Therefore, the judge model is required to categorize responses into four classes: Wrong (nw), indicating explicit mention of the relevant content but with factual errors; Neutral (nn), indicating no mention of the relevant content; Partially correct (np), indicating mention of relevant and correct information, but with incomplete descriptions; and Correct (nc), indicating complete alignment between the caption and the ground-truth answer. Based on these classifications, we compute the following four metrics to comprehensively assess the performance of captioning models:\n\u2022 Accuracy (Acc): The proportion of responses marked as Correct, defined as $\\frac{n_p + n_c}{n_p + n_c + n_w + n_n}$, reflecting the model's ability to cover comprehensive details of the video. Notably, models generating longer captions may have advantages.\n\u2022 Precision (Pre): Calculated as $\\frac{n_p + n_c}{n_p + n_c + n_w}$, representing the proportion of mentioned content that is at least partially correct.\n\u2022 Coverage (Cov): Calculated as $\\frac{n_p + n_c + n_w}{n_p + n_c + n_w + n_n}$, representing the proportion of addressed content relative to the total content covered by the QA pairs of the video.\n\u2022 Conciseness (Con): Measured by the contribution of each text token to Acc, defined as Acc/\u03c4, where \u03c4 represents the token number of the corresponding captions, as determined by a T5 model (Raffel et al., 2020)."}, {"title": "3.3 Data Curation", "content": "Open-source video datasets are often delicately curated and valuable for detailed analysis. Hence, we sample videos from several prominent open-source datasets. However, recognizing that these videos may have been extensively captioned and incorporated in many training datasets, we augment our data collection with additional copyright-free videos from YouTube and public user-generated content (UGC) platforms, extracting segments to ensure a portion of our data remains unexposed to prior training or processing. Figure 2 illustrates the pipeline of curating VidCapBench."}, {"title": "3.3.1 Videos Collection", "content": "We focus on a set of subjects in our analysis: person, animal, plant, food, common object, landscape, vehicle, building, specific intellectual property (IP), and no subject. Uniform involvement is expected across these categories. We perform an initial filtering based on captions provided by the Omega-MultiModal project , employing Qwen2-VL-72B (Wang et al., 2024b) for classification, retaining approximately 3,000 videos per category. These videos are then segmented into 3-15 second clips using PySceneDetect. Videos made of static images are removed using an optical flow tool. Subsequently, we sample 16 frames from each video and perform the following operations in parallel: Pose estimation (Khirodkar et al., 2025) to detect human presence and pose variations; Object detection and grounding (Liu et al., 2025) over the initial keywords to identify the amount of the target subject; Object tracking (Zhang et al., 2022) to track object motion, labeling objects as static or consistently trackable; Image segmentation (Ravi et al., 2024) to filter excessively complex scenes; Optical character recognition (OCR) (Wei et al., 2024a) to identify and minimize the presence of text. These generated labels inform the subsequent balanced sampling. Furthermore, we employ optical flow models, a custom-trained artistic style classifier, and a set of custom-trained character attribute classifiers for additional video labeling. Finally, we randomly select videos and manually verify the presence of each label at least three times, ensuring uniform distribution across labels."}, {"title": "3.3.2 Keypoints Generation", "content": "Based on the focused aspects mentioned in Section 3.2, we employ GPT-4o to generate 40 QA pairs for each video, supplemented by 10 additional question-category combinations generated via expert classifiers. Subsequently, we generate three different captions for each video using Gemini with varying random seeds. The first caption is evaluated three times using GPT-4o, while the subsequent two captions are evaluated once each. Questions exhibiting inconsistent judgments across the three assessments of the first caption, along with those receiving consistently negative evaluations across all five assessments, are flagged as potentially problematic. These problematic questions, likely due to their ambiguity or lack of clarity, are manually reviewed. Any factual inaccuracies within the QA pairs are corrected, and the revised pairs are subsequently re-evaluated to determine their suitability. Finally, to maintain a balanced distribution of key aspects across the dataset, additional human annotations are performed to address any dimension imbalances introduced by deletions and modifications."}, {"title": "3.4 QA Pairs Split", "content": "As discussed in Section 3.2, the difficulty of evaluating the QA pairs has a significant impact on the reliability of machine evaluation. Therefore, we split the total QA pairs based on their evaluation consistency using the same strategy in Section 3.2. In order to achieve a balance between accuracy and efficiency, we identify and segregate QA pairs that fail to receive consistent evaluations within the dimensions of video motion and physical laws, which are more crucial to T2V generation. Consequently, a total of 1,150 QA pairs are classified as VidCapBench-HE, which necessitates human intervention for accurate evaluation. The remaining QA pairs are designated as VidCapBench-AE, which can be evaluated automatically."}, {"title": "4 Experiments", "content": "4.1 Experimental setup\nCaptioning models. A variety of vision language models that demonstrate strong captioning capabilities are evaluated. When available, their official prompts are utilized; otherwise, the generic prompt \"Describe the video in detail\" is employed. Greedy decoding is applied across all models to minimize the influence of stochasticity.\nEnvironment. All experiments are conducted on A800-80GB GPUs using bfloat16 precision. To ensure a fair evaluation, each result is averaged over three independent runs, utilizing GPT-40 with random seeds set to 0, 1, and 2, respectively. Frame rate, resolution, and video decoding follow official recommendations where provided. Otherwise, the \"decord\" library is used to extract 16 frames for captioning, ensuring a minimum frame rate of 1 fps on VidCapBench. Fifteen experienced annotators, familiar with VidCapBench, provide reliable annotations for the generated captions."}, {"title": "4.2 Performance on VidCapBench", "content": "We first analyze model performance on the VidCapBench-AE.  However, our analysis reveals a notable tendency for GPT-40, as well as InternVL2-76B, to produce redundant outputs. In contrast, Tarsier-34B generates relatively concise captions, which contributes to its superior Con score, while also maintaining remarkable results in other metrics. Additionally, models specifically trained on large-scale dense captioning data, such as CogVLM2-Caption and Aria, manifest a distinct performance advantage.\nThe evaluation results also indicate that some models exhibit specialized proficiency in specific dimensions while showing limitations in others. A typical example is Pixtral-12B, which excels in Video Aesthetics and Physical Laws but underperforms in Video Motion and Video Content. While open-source models generally lag behind proprietary models in Video Motion, they exhibit comparable capabilities in Physical Laws. Notably, certain open-source models, particularly Pixtral-12B and Tarsier-34B, even surpass GPT-40 in the Acc score within this dimension.\nRegarding evaluation stability, VidCapBench demonstrates superior consistency compared to VDC, which exhibits significant variability across its three evaluation runs. The enhanced stability in VidCapBench contributes to more reliable T2V guidance, establishing it as a more robust evaluation framework for video captioning tasks."}, {"title": "4.3 Fine-grained Analysis of VidCapBench", "content": "This section focuses on five representative captioning models: GPT-4o, Gemini, Qwen2-VL-72B, CogVLM2-Caption, and Tarsier-34B. We conduct a fine-grained analysis of their performance to demonstrate the validity of VidCapBench.\nHuman evaluation consistency. To validate the reliability of automatic evaluation on VidCapBench-AE and the necessity of human intervention in VidCapBench-HE, we investigate the discrepancies between automatic and human evaluation on both subsets. Specifically, human annotators are engaged to assess model performance on VidCapBench-HE, and their judgments are compared with those derived from automatic evaluations. Additionally, we randomly sample 1,150 QA pairs from VidCapBench-AE for human annotation and subsequent comparative analysis. The results reveal strong consistency between human and automatic evaluations for all five models on VidCapBench-AE. Conversely, substantial inconsistencies emerge on VidCapBench-HE. These findings highlight the reliability of automatic evaluation on a stable-evaluation QA subset and the critical role of human annotation in accurately evaluating video captions in certain contexts.\nImpact of caption format. The above experiments do not specify caption formats. In practice, different T2V models demand distinct caption formats. Therefore, we evaluate the impact of different caption formats, including MiraData, Vript, Hunyuan-Video, and DREAM-1K, on the performance of Gemini and Qwen2-VL-72B. Different formats primarily affect caption length. Shorter captions, such as those in the DREAM-1K format, often lack comprehensive details of videos, resulting in lower Acc and Cov scores. However, the Pre score, which measures the proportion of partially correct answers, remains stable across formats, which underscores the robustness of VidCapBench in evaluating captions of varying formats."}, {"title": "4.4 Training-free T2V Verification", "content": "Ideally, we would train multiple identical T2V models from scratch using extensive datasets generated by corresponding captioning models and then evaluate the video quality produced by each caption variant. However, considering the influence of data distribution and convergence behavior, such a lengthy validation pipeline might not yield clear and focused conclusions. Therefore, leveraging the high semantic alignment capabilities of advanced T2V models, we adopt a training-free verification approach. Specifically, we directly feed captions generated towards videos in VidCapBench into three open-source T2V models: CogVideoX-5B, LTX-Video, and HunyuanVideo. The captions are generated using GPT-40, Gemini, Qwen2-VL-72B, CogVLM2-Caption, and Tarsier-34B. Figure 4 provides examples of the generated videos.\nWe conduct automated evaluations on the generated videos across four dimensions: semantic relevance, quantified using CLIP score derived from CLIP-L (Radford et al., 2021), which assesses both the model's textual alignment and the caption's suitability for the T2V task; aesthetic quality and structural integrity, evaluated using inter-frame PSNR and SSIM; and fidelity to the original video, measured using FVD. The correlations between the four T2V evaluation metrics and the Acc on VidCapBench are illustrated in Figure 5. The upper panels demonstrate a strong positive correlation between the Acc on VidCapBench-AE and automated T2V quality assessments, with an average Pearson correlation coefficient of 0.89, which substantiates the effectiveness of automated video caption assessments on VidCapBench-AE. Furthermore, the lower panels reveal an enhanced correlation for the Acc on VidCapBench full set, achieving a higher mean Pearson correlation coefficient of 0.92, highlighting the significant impact of human intervention on VidCapBench-HE.\nWe further conduct human evaluations on the videos generated from captions produced by GPT-40, Gemini, and CogVLM2, scoring aspects in visual aesthetics, subject consistency, action relevance, and logical coherence on a scale of 1 to 5. Each annotator is assigned approximately 130 sets of videos (9 videos per set), ensuring that each video receives three independent evaluations. To maintain objectivity and minimize bias, annotators are presented with an original video alongside nine T2V generations in a randomized and anonymized order. The results, reveal a similar pattern to the findings in VidCapBench. Specifically, GPT-40 and Gemini exhibit comparable performance, both significantly outperforming CogVLM2 across the four dimensions, which further validates the alignments between caption evaluations on VidCapBench and the T2V qualities.\nThe above findings demonstrate that, for a production-ready T2V model, the quality of captions as assessed in VidCapBench is highly correlated with the quality of the generated videos. Consequently, improving caption quality emerges as a crucial strategy for enhancing T2V model performance, regardless of whether training-based or training-free methods are employed."}, {"title": "5 Conclusion", "content": "VidCapBench introduces a comprehensive evaluation framework for video captioning in T2V generation, assessing across four key dimensions: video aesthetics, video subject, video motion, and physical laws. To cater to different evaluation needs, VidCapBench comprises two subsets: one designed for automated evaluation prioritizing speed, and the other for human evaluation prioritizing accuracy. Compared to existing benchmarks, VidCapBench exhibits greater stability and reliability. Furthermore, the strong correlation between scores on VidCapBench and T2V quality metrics demonstrates its potential for guiding T2V training processes."}, {"title": "Limitations", "content": "While VidCapBench provides a stable and reliable framework for evaluating captioning models in the aspect of T2V generation, it has certain limitations. Specifically, VidCapBench focuses primarily on captioning tasks, thereby excluding the assessment of other model capabilities, such as mathematical reasoning."}, {"title": "Ethical Considerations", "content": "Regarding the ethical considerations, it is worth noting that some T2V models may generate biased or harmful content, which could perpetuate stereotypes or misinformation. We strongly emphasize the importance of responsible use and encourage developers to implement robust safeguards, including bias detection mechanisms and content moderation systems, to mitigate these risks."}, {"title": "Appendix", "content": "A Detailed Settings of Experiments\nIn this section, we provide the detailed settings for the experiments."}, {"title": "A.1 Captioning Models", "content": "In this paper, we select the following models as representatives of mainstream caption technologies.\n\u2022 Llava-Next-Video: This model represents a significant advancement within the native Llava family. Its principal advancements encompass the integration of AnyRes and a more diverse dataset, making it a strong representative of the Llava family. It demonstrates impressive zero-shot performance on video understanding tasks.\n\u2022 LongVA: This model improves the long context capability via zero-shot transfer from language to vision, which can process 2,000 frames or over 200K visual tokens.\n\u2022 mPLUG-Owl3: This approach leverages the cross-attention mechanism to fuse the vision modality and language modality, somewhat like the Flamingo and llama3V architecture.\n\u2022 InternVL2: A family of vision language models that consumes a large amount of instruction data.\n\u2022 Qwen2-VL: A family of vision language models that employs 3D ROPE and NaViT, getting rid of the resized aspect ratio of the video frames.\n\u2022 Pixtral: A family of vision language models that employs 2D ROPE, a prominent representative on 12B-scale and 124B-scale.\n\u2022 CogVLM2-Caption: A captioning model linked to CogVideoX, also a typical caption-related SFT model from existing vision language models.\n\u2022 Aria: Representative of the MoE-based vision language models.\n\u2022 Tarsier: A captioning model that is designed to describe the events in a video."}, {"title": "A.2 Licensing", "content": "The benchmarks and captioning models used in this paper are solely for academic purposes, as permitted by their respective licenses below.\nBenchmarks license. DREAM-1K and VDC are licensed under the Apache-2.0 License.\nCaptioning models license. Llava-Next-Video, LongVA, mPLUG-Owl3, Qwen2-VL, Pixtral, Aria, CogVLM2-Caption, and Tarsier adopt the Apache-2.0 License. InternVL-2 is under the MIT License."}, {"title": "A.3 Prompts for Caption Generation", "content": "The prompts to generate captions for all models are depicted in Figure 6-10. We have verified that all models can follow the instructions and provide the captions with the correct format."}, {"title": "A.4 Prompts for Judgment", "content": "The prompts for the judge model is listed in Figure 11 and Figure 12."}, {"title": "B.1 Model Performance", "content": "We examine the performance of various models on the VDC and DREAM-1K benchmarks, summarized in Table 7. VDC demonstrates significant instability across its three evaluation runs, decreasing the reliability of T2V guidance. While repeated evaluations could mitigate uncertainty, the large volume of QA pairs in VDC imposes substantial constraints on both time and computational resources. Moreover, the VDCScore is quite close among different models, further complicating the accurate assessment of model performance. DREAM-1K, on the other hand, primarily focuses on event-centric descriptions without analysis of other crucial aspects, categorized solely by video type. Similar to the VDC benchmark, DREAM-1K also exhibits significant instability across its three evaluation runs."}, {"title": "B.2 Stability of VDC", "content": "Table 8 presents the stability of caption judgment for five vision language models (GPT-40, Gemini, Qwen2-VL-72B, CogVLM2-Caption, Tarsier-"}, {"title": "B.3 Unstable Cases in VDC", "content": "We probe into the unstable QA pairs in the VDC benchmark. We carefully analyzed the cases that were labeled \"unstable\", and found that the unclear questions and the leading questions are two common problems, as demonstrated by Figure 13 and Figure 14, respectively. Meanwhile, there are also many questions that are difficult to answer, or difficult to locate the direction of answer. We believe that these forms of questions are not appropriate to involve judge models for judgment."}, {"title": "C In-depth Study of VidCapBench", "content": "In this section, we attempt to provide some statistics and visualization of VidCapBench."}, {"title": "C.1 Detailed Dimensions of Evaluation", "content": "The focused dimensions in VidCapBench are defined as follows.\nVideo aesthetics (VA) encompasses the artistic and technical aspects of video creation, from filming techniques to post-production, which includes: Composition - Arrangement of objects and characters within the frame; Color - Mainly temperature and saturation; Lighting - Either natural or artificial lighting; Cinematography - Regarding lenses and camera movements; Style - Focusing on visual presentation and narrative techniques.\nVideo content (VC) refers to the narrative content presented in the video. Subjects - The primary person(s) or object(s) of focus within the frame, including characteristics, attributes, relationships, positions, and poses. Background - The non-focal elements of the video, providing visual support and spatial context. Events - Specific activities or plot points that drive the narrative.\nVideo motion (VM) encompasses all movement and motions, including: Body movements - Dynamic activities performed by subjects, reflecting posture, interactions with the environment, or other subjects. Object motion - Changes in object position over time; Attribute changes - Alterations in physical form, chemical properties, or motion state of objects (e.g., explosions, ripples, shattering); Environmental motion - Movement within the background (e.g., natural phenomena, water movement); Spatiotemporal transformations - Techniques that alter the perception of time and space (e.g., slow motion, time-lapse).\nPhysical laws (PL) allow for more realistic or dramatic visual expression, even though creativity can somewhat bend them. Specifically, VidCapBench focuses: Counter-intuitive scenarios - Identifying and describing scenarios that defy typical physical expectations; Geometry - Understanding and describing the spatial relationships between objects and the impact of camera perspective. Common Sense - Applying everyday knowledge and intuition to interpret events in a scene."}, {"title": "C.2 Statistics by Dimension", "content": "Figure 2(c) depicts the distribution of QA pairs in VidCapBench across dimensions. We further present statistics of the videos and the annotated keypoints in VidCapBench. Figure 15(a) illustrates the subject distribution. Given the prominence of humans and animals as subjects in T2V generation, we incorporated data with humans as the primary subject in 30% of instances and animals in 11%, with the remaining categories evenly distributed. Figure 15(b) presents the distribution of video styles: 80% real-world footage, 10% animation/anime, and the remaining 10% evenly allocated to other artistic styles. Figure 15(c) depicts the distribution of the number of subjects. We included 10% of videos without explicit subjects, 45% with 1-2 subjects, and 45% with 3 or more. Figure 15(d) shows the video duration distribution, with 40% of videos being under 10 seconds."}, {"title": "C.3 Tools in Data Pipeline", "content": "\u2022 Pose Estimation: We use sapiens-pose-1b to detect human presence and pose variations within the sampled frames.\n\u2022 Object Detection and Grounding: We apply GroundingDINO-base to the sampled frames based on initial classification keywords, ensuring the target subject appeared in at least 10 frames with sufficient relative size, and record the number of detected objects.\n\u2022 Object Tracking: Using bytetrack_x_mot20 to track object motion, we label objects as static or consistently trackable.\n\u2022 Image Segmentation: We use sam2.1-hiera-large for image segmentation. Considering practical T2V generation requirements, we record the number of segmented regions to filter excessively complex scenes (retaining 16-48 segments in practical applications).\n\u2022 Optical Character Recognition: We use a lightweight OCR model, namely GOT-OCR-2.0 to detect text within the frames. We aim to minimize the presence of text, particularly subtitles, prioritizing naturally occurring characters."}, {"title": "C.4 Statistics by Video Source", "content": "Table 9 presents statistics for videos from three sources within VidCapBench. To ensure diversity, we purposefully select videos with varying screen orientation, resolution, and motion types, thereby encompassing a wide spectrum of video characteristics and content typologies. Specifically, open-source videos (sourced from ActivityNet, DREAM-1K, NEXT-QA, MovieStory101, and Vript-HAL) primarily comprise well-defined and single-event scenarios. Videos from YouTube consist mainly of edited videos, exhibiting greater stylistic diversity. UGC videos, collected from short-video platforms, predominantly reflect everyday life."}]}