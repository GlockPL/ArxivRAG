{"title": "Towards Linguistic Neural Representation Learning and Sentence Retrieval from Electroencephalogram Recordings", "authors": ["Jinzhao Zhou", "Yiqun Duan", "Ziyi Zhao", "Yu-Cheng Chang", "Yu-Kai Wang", "Thomas Do", "Chin-Teng Lin"], "abstract": "Decoding linguistic information from non-invasive brain signals using EEG has gained increasing research attention due to its vast applicational potential. Recently, a number of works have adopted a generative-based framework to decode electroencephalogram (EEG) signals into sentences by utilizing the power generative capacity of pretrained large language models (LLMs). However, this approach has several drawbacks that hinder the further development of linguistic applications for brain-computer interfaces (BCIs). Specifically, the ability of the EEG encoder to learn semantic information from EEG data remains questionable, and the LLM decoder's tendency to generate sentences based on its training memory can be hard to avoid. These issues necessitate a novel approach for converting EEG signals into sentences. In this paper, we propose a novel two-step pipeline that addresses these limitations and enhances the validity of linguistic EEG decoding research. We first confirm that word-level semantic information can be learned from EEG data recorded during natural reading by training a Conformer encoder via a masked contrastive objective for word-level classification. To achieve sentence decoding results, we employ a training-free retrieval method to retrieve sentences based on the predictions from the EEG encoder. Extensive experiments and ablation studies were conducted in this paper for a comprehensive evaluation of the proposed approach. Our evaluation results demonstrate that our EEG encoder achieves up to 55.15% top-20 classification accuracy with unseen EEG signals. Visualization of the top prediction candidates reveals that our model effectively groups EEG segments into semantic categories with similar meanings, thereby validating its ability to learn patterns from unspoken EEG recordings. Additionally, using the predicted classification results, our retrieval method attains a recall@5 of up to 55.55% and a BLEU-1 score of 30.44% for sentence-level evaluation. Despite the exploratory nature of this work, these results suggest that our method holds promise for providing more reliable solutions for converting EEG signals into text.", "sections": [{"title": "1 INTRODUCTION", "content": "Decoding linguistic information from brain signals has traditionally relied on intracranial approaches, which offer promising prospects for restoring communication abilities in individuals with paralysis or spinal cord injuries [29, 31, 48]. In contrast, the use of non-invasive brain signals such as EEG in linguistic decoding has only recently begun to attract research attention, due to their superior temporal resolution, portability, and safety [2, 3, 19].\nFor its ability to measure surface neural activity with high temporal resolution and detect a diverser range rhythmic patterns, EEG signals can capture electrical activity in the sensorimotor cortex, which is known to produce \u00b5 rhythms rich in information during speech production [39]. This inherent connection between EEG signals and speech has led to various successful approaches in decoding EEG into linguistic units such as syllables [4], phonemes [10, 11], and words [20, 30], despite limitations due to the scale of available datasets and subject variability.\nOn the other hand, decoding sentences from EEG signals during unspoken reading tasks presents several unique challenges. Firstly, unspoken speech elicits less discriminative brain activity compared to spoken speech, making it harder to distinguish between different neural responses [34]. Second, there exists significant data sparsity, as the number of semantic categories is large while the dataset size remains relatively small [32]. Third, the noisiness of thought during reading further complicates the decoding task. For instance, participants may not focus on every word equally, often paying less attention to grammatical words and more to words that contain crucial or interesting information from the sentence [15]. Existing methodologies for decoding reading sentences from EEG signals have predominantly relied on a framework that pairs an EEG encoder with a pretrained large language model (LLM) decoder, training and decoding sentences by the machine translation approach [18, 46]. However, recent analyses suggest that when training the EEG encoder with an LLM using a machine translation objective, the encoder's ability to genuinely learn to capture semantic EEG patterns remains unclear. Instead, the overpowered LLM decoder may generate sentences simply based on its training memory regardless of the EEG input [23]. These empirical findings underscore the necessity to validate the efficacy of learning EEG encoders from EEG reading tasks and highlight the limitations of using overly powerful pretrained LLMs for converting EEG signals into sentences.\nTo overcome the aforementioned limitations, we aim to develop a novel approach for EEG-to-sentence conversion, which seeks to eliminate the bias introduced by the training memory of an overpowered LLM while enabling the assessment of the semantic"}, {"title": "2 RELATED WORKS", "content": "Linguistic unit or word decoding from brain signals Due to EEG's capacity to capture neural activities associated with speech production, pioneering words on linguistic decoding using EEG mainly focus on the decoding of linguistic units such as syllables or phenomes [11, 17, 43]. For instance, [7] proposed to extract autoregressive coefficients as features for imagined syllable classification with a k-nearest neighbor (KNN) classifier. [13] leveraged the Hilbert spectrum to extract features and classify the syllables using a Bayesian classifier.\nTo decode higher-level semantics, numerous studies have dedicated efforts to word-level classification using EEG signal [5, 20, 33, 33, 45, 51]. However, most of these studies have trained and evaluated their models on a very limited dataset, typically comprising only 4 to 10 words or a narrow set of directional words. As a result, recent research has sought to expand the output vocabulary scope to a more practical scale, either at the word-level [28] or the pre-word level [49]. [16] used a large-scale word-level EEG dataset collected during listening, they enhance word-level classification accuracy through contrastive learning to align E/MEG signals with speech.\nEnd-to-End Decoding from EEG to sentence The recent trend in EEG-based sentence decoding on the other hand predominantly employs end-to-end machine translation approaches. For instance, EEG-to-Text [46] pioneered open-vocabulary decoding of EEG signals into sentences, establishing an initial performance benchmark. In their work a Transformer-based EEG encoder is used to transform EEG signals into EEG representations while a pre-trained LLM model takes these EEG representations as input and generate sentences. Building upon this, DeWave [15] advanced decoding performance by introducing discrete codex and achieved text decoding directly from raw EEG waves. Subsequent innovations such as BELT [9, 52] and Curriculum Contrastive [18] introduced contrastive learning to enhance encoding quality. Additionally, NuSpeech [50] leveraged the end-to-end speech decoding model Whisper [36] to achieve commendable performance.\nHowever, these end-to-end methods are prone to issues where a newly initialized EEG encoder combined with a powerful pre-trained language decoder leads to the decoder merely memorizing and generating the training text without truly utilizing information from the EEG modality. Consequently, this may result in the EEG encoder failing to learn to capture EEG patterns. Diverging from these LLM-based approaches, our work first develops an effective EEG encoder for word-level classification and explores the feasibility of using a training-free, unbiased retrieval method to achieve sentence-level output. This approach eliminates potential limitations such as implicit teacher forcing evaluation or test sentence leakage."}, {"title": "3 EEG-TO-TEXT RETRIEVAL", "content": "In this section, we present our ETER approach, a two-step EEG-to-text retrieval method that identifies the most relevant sentence a participant reads based on word-level EEG classification results. The general pipeline of our approach is depicted in Figure 1. \u03a4\u03bf achieve word-level decoding, we developed a Conformer-based EEG encoder. To learn semantic EEG representation, we guide the"}, {"title": "3.1 Preprocessing", "content": "To perform word-level EEG representation learning and classification, we first preprocess the dataset's vocabulary. Grammatical words such as \"the,\" \"a,\" \"an,\" and \"is\" constitute a significant portion (40-60%) of English text in general [24, 26]. From a sample balance perspective, these grammatical words dominate the training and testing samples, potentially leading the EEG encoder or classifier to overemphasize on these words, which do not contain critical information about the sentence. Furthermore, previous neurobiological studies in reading comprehension has identified that \u201csemantic strong\u201d words elicit higher and more distinguishable neural patterns compared to \u201csemantic moderate\u201d words [25]. Therefore, during preprocessing, we remove EEG-word pairs containing these grammatical words from the dataset.\nAdditionally, we perform word lemmatization on the remaining vocabulary. The lemmatization step serves two purposes. First, we hypothesize that during reading comprehension, different forms of the same word will elicit similar neural patterns, as they convey the same meaning. So the EEG signals for these similar words can be seen as the same category. Second, this lemmatization process also increases the sample size for each word in the vocabulary and reduces the sparsity of the word-level training dataset.\nFor preprocessing the EEG signals, they are first transformed into word-level EEG embeddings using frequency-domain transformation following the same preprocessing pipeline in previous works [22, 47]. First, the EEG recordings are segmented according to the eye-tracking fixation on each word. Then, the segmented EEG signals are denoised and band-pass filtered into eight frequency bands: theta1 (4-6Hz), theta2 (6.5-8Hz), alpha1 (8.5-10Hz), alpha2 (10.5-13Hz), beta1 (13.5-18Hz), beta2 (18.5-30Hz), gamma1 (30.5-40Hz), and gamma2 (40-49.5Hz). The Hilbert transform is then applied to each channel. Finally, word-level EEG embeddings are obtained by averaging the frequency band power within each frequency band. In the remainder of this paper, we denote the word-level EEG embedding as e. For the corresponding word of the EEG embedding, we use the embedding layer of a distilled BERT model [14] to convert it into word representation, denoted by w for brevity. To enhance word-level EEG representation learning and classification performance, we apply standard normalization to the word-level EEG embeddings. Specifically, we compute the mean and standard deviation of e for each subject and use these values for applying standard normalization. Empirically, we found that this normalization stabilizes the training process and improves performance, likely by suppressing noise and reducing inter-subject variations to some extent."}, {"title": "3.2 EEG Encoder", "content": "We train an EEG encoder for encoding and classifying EEG signals. We first tokenize e into frequency tokens and then feed them to a Conformer encoder. The Conformer encoder outputs the same number of tokens as input, we use a global pooling layer to aggregate the information across all frequency bands into the final EEG representation h."}, {"title": "3.2.1 Frequency-wise EEG tokenization", "content": "After preprocessing, the word-level EEG embedding has the shape of $e \\in R^{N \\times D}$. Here, N denotes the number of channels, and D is the number of frequency bands (in our case D = 8). To tokenzie the EEG, we split e into non-overlapping frequency bands across all channels ${e^{(i)}}_{i=1,...,D}$. Since these EEG channels are distributed spatially on a participant's head so we employ spatial operations here to capture and aggregate frequency responses in a specific scalp area. As depicted in Figure 3, we use a spatial encoder to transform $e^{(i)}$ into EEG token. The spatial encoder consist of a lightweight convoutional network. The spatial encoder comprises a lightweight convolutional network and a positional embedding layer. The convolutional network processes the channel dimension to produce embeddings that consolidate spatial information from specific frequency bands. Concurrently, the positional embedding layer is used to encode the positional information of the frequency bands, indicating which frequency range is contained within the input $e^{(i)}$."}, {"title": "3.2.2 Conformer for EEG encoding", "content": "The detailed architecture of our EEG encoder is depicted in Figure 3. We use the conformer blocks [21] to build our EEG encoder for capturing both spectrum dependencies across EEG frequency bands and spatial relationships among channels [42, 52]. To aggregate the encoded EEG representations across all frequency bands, we used a global adaptive pooling layer to the output of the last confomer block and outputs h as the final EEG representation for each word.\nIn a Conformer block, two feed-forward networks (FFN\u00b9 and FFN\u00b2), a multi-head self-attention (MHA) module, a convolution (CN) module are stacked together using residual connections. We applied a 1/2 weigh for the two FFN layers. The convolution module is depicted in Fig. 4, which is in turn comprised of two pointwise convolution layers and a depthwise convolution layer. The first pointwise convolution layer of the convolution module uses the gated linear unit (GLU) as the activation function. A batch normalization layer and a swish activation function were also used after the depthwise convolution layer. Overall, the Comformer blocks take the EEG embeddings e as input and output the continuous EEG representation h."}, {"title": "3.2.3 Masked contrastive training", "content": "To train the EEG encoder, we employ the masked contrastive learning objective, as depicted in Figure 5. This self-supervised approach aligns EEG representations h with word representations w, enabling the EEG encoder to extract semantic information from EEG signals. This alignment ensures that EEG representations are not only closely related to its groundtruth word category but also to words with similar meanings. To further enhance the robustness of the EEG representations, we apply random masking to the input EEG tokens with a masking ratio \u03b7. Notably, we do not apply masking to the word embeddings to avoid introducing unnecessary noise into the learning process. The masked contrastive training loss function is defined by $L_{ct}$ as follows:\n$L_{ct} = \\frac{1}{M} \\sum_{i=1}^{M} -log \\frac{exp sim(h_{i}, w_{i}) / \\tau}{\\sum_{j=1}^{M} exp sim(h_{i}, w_{j}) / \\tau}$ (1)\n, where M is the training sample size of the dataset, \u03c4 is the temperature parameter that scales the logits, and sim( , ) denotes the dot product similarity measure. We employ a frozen, pretrained BERT model [14] as the text encoder to generate word representations and guide the learning of EEG representations. In our experiments, we empirically determined that a mask ratio of \u03b7 = 0.1 and a temperature parameter of \u03c4 = 0.3 yield optimal classification performance."}, {"title": "3.2.4 Word-level classification", "content": "While training an EEG encoder with a self-supervised objective provides a robust foundation for learning semantic representations, it alone is insufficient for effective EEG classification. To address this limitation, we introduce a supervised learning phase that augments the self-supervised training with an additional classification head. We use a fully-connected layer with softmax activation function as the classification head using the EEG representation h. This layer maps the language-aligned EEG representations to specific word categories, leveraging supervised loss $L_{sup}$ (Equation 2) to refine the encoder's predictions.\n$L_{sup} = -\\frac{1}{M} \\sum_{i=1}^{M} y_{i} log(\\hat{p}(y_{i} | h_{i}))$ (2)\n, M denotes the number of training samples, yi represents the one-hot encoded target word for the i-th sample, hatyi is the predicted word. In addition to the fully-connected layer classifier, we apply regularization techniques such as dropout and weight decay to prevent overfitting and ensure that the model generalizes well to unseen EEG samples."}, {"title": "3.3 Sentence Retrieval Method", "content": "In this section, we introduce the retrieval method designed to achieve EEG-to-text conversion based on the results from our word-level classification model. Our word-level EEG encoding and classification approach, as introduced previously, provides a solid and transparent measure of how well the encoder captures linguistic patterns from EEG data by allowing the direct evaluation using accuracy metrics. However, achieving high top-1 accuracy in linguistic EEG classifications remains a significant challenge under a large vocabulary as reported in previous works [6, 12, 38].\nTo address this limitation, we leverage a characteristic that emerged from our masked contrastive learning approach. After training, our model can generate top-k word predictions with similar meanings from the input EEG signals. This capability is crucial as it mitigates the challenges of achieving precise top-1 classification by aggregating semantically related words. This aggregation enhances the robustness and accuracy of our retrieval method, allowing for more reliable decoding of EEG signals into meaningful sentences. We denote the group of top-k prediction words as a keyword set (KS), denoted by Sk. Here, k denotes the number of top prediction words. Building upon this, we design our retrieval method to leverage the Sk from each \u201cEEG word\u201d to identify the most relevant sentence from the reading corpus. We denot the sequence of KSs in a sentence as $S = \\{S_{i}\\}_{i=1,\u2026,L}$, where L denotes the number of KS predicted for the sentence."}, {"title": "3.3.1 Beam search retrieval method", "content": "We depict the proposed beam search retrieval method (BSR) in Figure 6. The BSR method is designed to leverage a large search space that considers all k candidates in Sk, while reducing exponential memory consumption. BSR begins by constructing keyword combination queries from the first n KSs. Each query contains one candidate from a Sk, and will be scored according to its relevance to sentences in the dataset corpus. The scoring method will be explained in Section 3.3.2. The score for each query measures the relevance of this query to the dataset corpus. After scoring, we apply re-ranking to the queries and only keep the best m combination queries for the next evaluation round. In the next round, the (n + 1)th KS will be added to the queries to produce further combination queries. This iterative method ensures that at each step, we maintain the most promising combinations, incrementally building up to the final sentence retrieval. Mathematically, this iterative beam search process can be described as follows:\n$q^{0} := \\{\u00d8\\}$\n$q^{l} = argmax_{q' \\in B_{l}} H(q', C)$\n$|q'| = m$ (3)\n, here q\u00ba denotes the initial combination query set before the interactive search. It is an empty set as there is no relevant query is kept at the start. q\u02e1 denotes the retained combination queries after the lth iteration. We use Bl to denote the new combinations obtained when adding the lth KS (Sl) in this iteration. H(q', C) denotes a scoring method between the combinations q' and sentences from the dataset corpus C. We set |q'| = m to limits the beam width of the searching. We calculate the candidate query set at l > 0 by:\n$B_{l} = \\{q \\circ y | q \\subseteq q^{l-1}, y \\in KS_{l}\\}$ (4)\n, where \u25e6 denotes the concatenation operation. We borrow the process depicted in Figure 6 as an illustrative example. Assume we have a total of L = 3 KS in the sentence. Figure 6(a) shows all KSs from stage 1. In this example, none of the KS predicted the ground truth word as its top-1 prediction. However, the correct word can be found within the top-k prediction set. Figure 6(b) illustrates the ground truth words and ground truth reading sentence for reference. The BSR method, as shown in Figure Figure 6(c), compares a number of combinations to the dataset corpus, distinguishing relevant combinations from irrelevant ones. In our example, the relevant combinations are [may, become, star], [during, time, work], and [during, time, much]. Using these relevant combinations, our model is able to identify the closest sentences from the dataset, including the ground truth sentence \"During this time, he worked...\" and returns this as the retrieval result."}, {"title": "3.3.2 Scoring Method", "content": "We use the Aho-Corasick algorithm [1] as the training-free scoring method. In particular, the Aho-Corasick algorithm efficciently finds all occurences of the combination query within a sentence from the corpus by constructing a finite state machine. Thus, we denote the calculation of H(q, C) by:\n$H(q, C) = \\sum_{top-m} max |q \\cap c|, c \\in C, q \\in q,$ (5)\n, where |q \u2229 c| denotes the number of occurrences of a query within a sentence c. We score a query using its average occurrence match with the sentence to allow the tolerance of \"wrong keywords\" in the query."}, {"title": "4 EXPERIMENT", "content": "In this study, we utilize the Zurich Cognitive Language Processing Corpus (ZuCo) dataset [22] for training and evaluating the proposed method. The ZuCo dataset contains EEG data recorded during unspoken reading tasks involving 12 participants. It includes"}, {"title": "4.1 Dataset", "content": "data from 105 EEG channels, with EEG waves denoised and filtered into eight frequency bands after segmentation. For our experiments, we use data from reading comprehension tasks, specifically Task 1 and Task 3 to evaluate the performance of our ETER method. Task 1 focuses on sentiment comprehension from movie reviews [41], while Task 3 involves understanding and extracting entities' relation from Wikipedia biography articles. As discussed in Section 3.1, we removed all EEG-word pairs containing grammatical words from the dataset and performed lemmatization on the remaining words, merging words with the same lemmatized root form. Additionally, we observed a sharp decrease in sample numbers for words outside the top-100 most frequently occurring words in the remaining EEG-word pairs. As depicted in Figure 7, most long-tailed cases have fewer than 30 samples in the entire dataset, with some extreme cases having only one sample. This imbalance problem results in significant sparsity in the training dataset. Making it impossible to develop any effective word-level models on the full vocabulary of the dataset. To address this issue, we selected only the 100 most frequently appearing words from the dataset for training our EEG encoder. Although this selection may limit the system's ability to scale, it provides relatively stable performance and serves as a reliable solution for our current needs. We have also conducted experiments involving a larger vocabulary in ablation studies, which will be discussed in Section 4.6.2."}, {"title": "4.2 Mectrics", "content": "To ensure a thorough evaluation of our approach, we utilize a range of evaluation metrics for both the EEG classifier and the retrieval method. Firstly, we evaluate the effectiveness of the EEG encoder through classification accuracy assessment. In the context of sentence retrieval, we employ the retrieval mectics including recall@5 and precision@5 metrics to evaluate the ability of our system to retrieve relevant sentence based on the results from EEG classification. Additionally, we calculate the BLEU metric [8] to quantify the relevance between the retrieved sentences and the target sentence."}, {"title": "4.3 Implementation Details", "content": "We train a Conformer encoder with 2 Conformer blocks. We set the embedding dimension to 512 with 8 attention heads with the feed-forward dimension size of 1024. During training, we set the coefficient for training loss as \u03b1 = 0.5 and \u03b2 = 0.5 respectively. We optimize the parameters of the Conformer models using AdamW optimizer with an initial learning rate of 1e-4 and a weight decay of 0.05. The learning rate warms up over the first 500 steps to 1e-2 and linearly decays to 1e-6. In all experiments, we set the batch size to 256 and train the model for 100 epochs. Training is performed on a single A40 GPU with 48 GBs of memory."}, {"title": "4.4 Word-level classification performance", "content": "We train and evaluate our EEG encoder and its ablative versions using the ZuCo dataset to demonstrate its ability to learn semantic representations from unspoken EEG signals. For the baseline, we use a random model that predicts a uniform distribution over the EEG segments. Our initial model is a conformer EEG encoder trained solely with the supervised learning loss Lsup, without subject-baseline removal. We then assess the performance gains by incorporating subject-baseline removal (+bm.) and masked contrastive loss (+MCT). As shown in Table 1, our model predicts the correct word from EEG with a top-20 accuracy of 55.15% and a top-10 accuracy of 36.4%. This indicates that for more than half of the unseen EEG samples across different subjects, the ground truth words rank significantly higher than others within a 100-word vocabulary. Compared to the random baseline, our model achieves nearly three times higher accuracy. Furthermore, we observe that the addition of baseline removal and masked contrastive training improves the top-20 accuracy by 7.86% and 7.62%, respectively. These results highlight the incremental improvement provided by these methods in learning linguistic EEG patterns during reading."}, {"title": "4.5 Sentence-level retrieval performance", "content": "We evaluate the performance of the second-stage retrieval method using the sentiment movie review corpus from Task 1 and the Wikipedia biography corpus from Task 3. For these evaluations, we impose constraints on the number of available words within the sentences, requiring at least 5 or 7 KSs, as shown in Table 2. We compare the proposed BSR method using various scoring methods including the Aho-Corasick method, Levenshtein distance, and Term Frequency-Inverse Document Frequency (TF-IDF). When using Levenshtein distance, we compute the edit distance between the query and the compared sentences while for the TF-IDF method, we calculate the cosine similarity between the bag-of-word representations of the query and the comparison sentence retrieved from the corpus. As presented in Table 2, our experiment demonstrates the superior performance of using the Aho-Corasick-based scoring with our BSR method to accurately retrieve relevant sentences from the corpus based on input keyword sets. In the sentiment movie review corpus, our method achieves a recall@5 metric of 37.5% for sentences containing over 5 keyword sets. For sentences containing over 7 keyword sets in both corpora, we achieve a recall@5 of over 50%. Moreover, our method demonstrates the highest performance in retrieving relevant sentences, as evidenced by the BLEU metrics, surpassing a BLEU-1 score of 40% on both reading corpora for sentences with over 7 keyword sets. Since These results are achieved without requiring any training in the retrieval method, it showcase the plausibility of the proposed ETER method for EEG-to-text conversion.\nAside from the quantitative results, Table 3 presents a qualitative assessment of the proposed ETER method. For qualitative comparison with a generative LLM decoder, we additionally fine-tuned a T5 model [37] to generate ground truth sentences using lists of keywords as input. We show that our approach effectively retrieved top-ranking sentences in the first example case. In contrast, the T5 model produced sentences outside the training dataset, which is largely based on its pre-training memory. In the last example, although our model failed to find the correct sentence, it still managed to successfully identify keywords like 'best', enabling retrieval of similarly sentiment sentences from the corpus. This underscores the efficacy of our retrieval-based method in transcribing EEG signals into text given an imperfect word-level EEG classifier."}, {"title": "4.6 Ablations and Discussions", "content": "We delve deeper into assessing the effectiveness of different encoder architectures using the same training paradigm as the proposed method. Specifically, we interchange Conformer blocks with Transformer blocks [44] or Emformer blocks [40], in our EEG encoder architecture and evaluate their word-level classification performance. Results are presented in Table 4. To begin with, both the Conformer and Emformer, with their ability to capture local patterns across channels, exhibit notably superior performance compared to the general Transformer encoder. This observation underscores the significance of leveraging structures that exploit local patterns, justifying our design choice of EEG encoder. Notably, the Conformer encoder achieves better performance than the Emformer-based encoder in our assessments, indicating that the convolutional layers in the conformer architecture allow the model to learn hierarchical features, which can be crucial for understanding complex signals such as EEG. Additionally, our results show that the introduction of a reconstructive term does not yield a consistence enhancement to the performance, further validating our choice of a masked contrastive learning scheme without a reconstructed decoder."}, {"title": "4.6.2 Vocabulary size", "content": "The ablation results on vocabulary size is depicted in Figure 8. This result highlight the scalability of our proposed method across varying vocabulary sizes. Notably, our approach consistently outperforms competing models, demonstrating robustness even with smaller vocabulary sizes. However, as the vocabulary size increases to include more than 200 words, we can observe a significant decline in decoding performance. This decline is primarily due to the inherent imbalance and increased scarcity in the dataset as shown in Figure 7, where words with lower frequencies lack sufficient training data. Despite these challenges, our proposed Conformer model maintains competitive performance, achieving a top-10 accuracy of 28.4% with a 200-word vocabulary. This result compares favorably to recent classifications of listening EEG data, which reported a top-10 accuracy of 31.4 \u00b1 1.59% with a 203-word vocabulary size [12]. This comparison underscores the efficacy of our approach, particularly with the exclusion of"}, {"title": "4.6.3 Visualization of word-level results", "content": "Figure 9 illustrates the top-10 keyword set predicted by the encoder on unseen EEG samples from the test set. After training, our model shows a strong capability of encoding EEG signals to similar concepts. For example, when predicting 'university', our model also considers 'graduate', 'school', and 'college' to be in the same keyword group, indicating the model has assigned these concepts into a close semantic representation space. We consider the clustering of meaningful concepts to support that our encoder has learned useful representation from the brain signal and has aligned these linguistic representations with language modality in the subspace."}, {"title": "4.6.4 Retrieval method and number of keyword sets", "content": "Lastly, we compare the performance of our proposed beam search retrieval method with a more straightforward greedy retrieval (GS) method and observe the retrieval performance of retrieval methods using a different number of KSs as input. The GS method is designed to use only the top-1 prediction from each keyword set for retrieving sentences from the corpus. The comparison results, presented in Figure 10, show that both strategies perform well on the training set as the number of keyword sets within a sentence increases. This observation suggests the potential of extracting full sentences from EEG signals using a high-accuracy EEG encoder. However, the GS method's performance significantly decreases on the test set due to its limited capacity to explore a broader array of queries, constrained by the EEG encoder's moderate top-1 prediction performance. This finding underscores the rationale for implementing a beam search-based retrieval strategy. On the other hand, we are aware that our EEG encoder is currently constrained to the top 100 highest-frequency words, limiting the number of keywords that can be decoded from a sentence and, consequently, the application scenarios of our method. Despite this limitation, the proposed retrieval method shows improved performance when more keywords are given, even if some noise is present. These findings suggest that our method is promising and has potential for future development, particularly if the vocabulary constraint can be addressed to allow for a broader range of keywords and more accurate sentence retrieval."}, {"title": "5 CONCLUSION", "content": "This paper demonstrates the potential of combining an EEG encoder with a retrieval method to convert EEG signals into sentences, introducing a pioneering approach termed EEG-to-text reteival(ETER)."}, {"title": "This novel method", "content": "employs a transparent EEG encoder, to learn semantic patterns from EEG data. By extracting keyword sets from unseen EEG segments, ETER enables the sentence retriever to identify the most relevant sentences from a corpus. Both quantitative and qualitative evaluations affirm the efficacy of our approach in acquiring meaningful semantic representations and retrieving relevant sentences. Our extensive experiments and ablation studies validate the approach's ability to learn patterns from unspoken EEG recordings both quantitatively and qualitatively, demonstrating that our method holds promise for providing more reliable solutions for converting EEG signals into text. Despite the achieved results, we recognize the substantial room for future improvement. Given the exploratory nature of this research, we only employed a simple retrieval method and tested it on a limited vocabulary set. Our future work will focus on exploring more diverse datasets to continuously improve the EEG encoder design and enhance retrieval methods to accommodate larger vocabularies, thereby improving sentence retrieval accuracy on a larger scale. Additionally, collecting more EEG data at the word level will be pursued to further advance research in linguistic EEG decoding."}]}