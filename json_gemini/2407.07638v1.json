{"title": "Tuning Vision-Language Models with Candidate\nLabels by Prompt Alignment", "authors": ["Zhifang Zhang", "Beibei Li"], "abstract": "Vision-language models (VLMs) can learn high-quality rep-\nresentations from a large-scale training dataset of image-text pairs. Prompt\nlearning is a popular approach to fine-tuning VLM to adapt them to\ndownstream tasks. Despite the satisfying performance, a major limita-\ntion of prompt learning is the demand for labelled data. In real-world\nscenarios, we may only obtain candidate labels (where the true label is in-\ncluded) instead of the true labels due to data privacy or sensitivity issues.\nIn this paper, we provide the first study on prompt learning with candi-\ndate labels for VLMs. We empirically demonstrate that prompt learning\nis more advantageous than other fine-tuning methods, for handling candi-\ndate labels. Nonetheless, its performance drops when the label ambiguity\nincreases. In order to improve its robustness, we propose a simple yet ef-\nfective framework that better leverages the prior knowledge of VLMs to\nguide the learning process with candidate labels. Specifically, our frame-\nwork disambiguates candidate labels by aligning the model output with\nthe mixed class posterior jointly predicted by both the learnable and the\nhandcrafted prompt. Besides, our framework can be equipped with vari-\nous off-the-shelf training objectives for learning with candidate labels to\nfurther improve their performance. Extensive experiments demonstrate\nthe effectiveness of our proposed framework.", "sections": [{"title": "1 Introduction", "content": "Large-scale vision-language models (VLMs) such as CLIP [38], ALIGN [19], and\nCoca [51] have become excellent base models in multiple domains, most of which\nemploy a dual-encoder architecture to align the natural images with descriptive\ntexts. Remarkably, this special training manner has endowed VLMs with superior\nzero-shot transfer performance on visual recognition tasks. In specific, during\nthe inference, the pre-trained text encoder receives inputs in the form of man-\ncrafted prompts, e.g., \u201ca photo of <CLS>.\". Subsequently, all the generated\ntextual embeddings are matched with the visual embedding obtained from the\nimage encoder to predict the image category. However, the powerful zero-shot\nability of VLMs was shown to be heavily dependent on the choice of handcrafted\""}, {"title": "2 Related Work", "content": "Vision-language models with fine-tuning. Recently, significant advance-\nments have been made in the field of vision-language models (VLMs) [19,38,51,52].\nUnlike models learning from uni-modal supervision, VLMs align visual and tex-\ntual signals to learn rich representation from massive image-text pairs in the\npre-training stage. One of the keys to the excellent performance of VLMs is the\ntremendous amount of training data. CLIP [38] trains both the text and im-\nage encoder simultaneously using a contrastive loss on a dataset of 400 million\nimage-text pairs. ALIGN [19] leverages a noisy pre-training dataset of 1.8 billion\npairs to train the model. However, although VLMs have shown promising perfor-\nmance in generalizing to new concepts without auxiliary information, their large\namount of parameters makes it impractical to fine-tune the entire model [25].\nBesides, full-parameter fine-tuning will also make large models prone to overfit-\nting to downstream tasks and catastrophic forgetting [32]. To address the above\nissues, multiple transfer learning methods [57,12,38,55] are proposed to adapt\nVLMs to downstream tasks effectively and efficiently. Linear probe [38] freezes\nthe pre-trained VLM and trains a linear classifier on top of the image encoder.\nCLIP-adapter [12] introduces an adapter with residual-style feature blending\nwith the pre-trained features. Notably, the success of most of the fine-tuning\nmethods heavily depends on the acquisition of precisely labelled data."}, {"title": "Prompt learning", "content": "Among all fine-tuning methods that adapt VLMs to a new\ntask, one typical approach is prompt learning. It treats the text prompt, e.g.,\n\"a photo of a <CLS>\" as continuous learnable parameters, and optimizes the\nprompt with multiple vision tasks, including image classification [57,56,22,50,20],\ndense prediction [39,13,26], etc. CoOp [57] is the first work that migrated prompt\nlearning to vision tasks, which fine-tunes CLIP by optimizing the parameters of\nthe learnable textual prompt (also called soft prompt) while keeping the class\ntoken fixed. MaPLe [22] adds multi-modal prompts and learns them mutually\nto align both representation spaces dynamically. Moreover, recent studies have\nrevealed the advantages of prompt learning when fine-tuning with weakly super-\nvised or unsupervised data [46,33,17,16]. Wu et al. [46] demonstrates that the\nprocess of prompt learning is robust to label noise. UPL [17] utilizes handcrafted\nprompts to generate pseudo labels on unlabelled data and adopts the confident\nexamples per class to tune the learnable prompt."}, {"title": "Partial-label learning", "content": "Partial-label learning (PLL) allows each training ex-\nample to be annotated with a candidate set, containing the true label. Two main-\nstream strategies have been developed to address this problem: the averaged-\nbased strategy and the identification-based strategy. Average-based strategy dis-\nambiguates the candidate labels by treating them equally [18,4,54]. Identification-\nbased strategy regards the true label as a latent variable and selects the most\nlikely label for training [21,9,34,10], which is prone to error accumulation if the\nwrong label is selected initially [49]. As deep learning thrives, many PLL algo-\nrithms have been proposed for training with deep neural network [11,30,29,53].\nFeng et al. [11] assumes the generation process of partial labels and derives\nclassifier-consistent and risk-consistent methods from a theoretical perspective.\nPRODEN [30] updates the model parameters and identifies the true labels seam-\nlessly. PiCO [44] divides the learning process into representation learning by\ncontrastive loss and label disambiguation by prototype and pseudo target up-\ndating. In particular, as the scale of the parameters and training data of modern\ndeep neural expands [6,2,23], the deep learning community has embraced a new\ntraining paradigm of pre-training and fine-tuning. Nonetheless, to the best of our\nknowledge, little research has explored PLL under this new training paradigm."}, {"title": "3 Preliminaries of Prompt Learning", "content": "In this work, we show that prompt learning is a preferred choice when tun-\ning VLMs with candidate labels and propose a novel framework to augment\nits performance. Primarily, we conduct the experiments and build our approach\nupon a prevailing method in prompt learning: CoOp [57], which tunes the tex-\ntual prompt based on a well-known VLM called CLIP [38]. In this section, we\nbriefly revisit the details of CLIP and CoOp.\nCLIP. CLIP (Contrastive Language-Image Pre-training) is a cutting-edge VLM\nthat learns joint image-text representations through contrastive learning. It con-\nsists of both an image encoder and a text encoder. The image encoder encodes\nvisual signals, which can be constructed by ResNet [15] or ViT [7]. The text"}, {"title": "CoOp.", "content": "Although CLIP is amazing at zero-shot transfer, its performance is\nsensitive to differently designed prompts. In order to overcome the inefficiency\nof the handcrafted prompt designs, CoOp (Context Optimization) [57] replaces\nthe fixed sentence: \"a photo of a <CLS>.\" with learnable vectors that have the\nsame dimension as the word embedding of CLIP. These learnable vectors will be\noptimized with a few labelled examples. In the remaining sections, we will study\nCoOp as a typical case of prompt learning for VLMs.\nM\nTo be specific, assume CoOp introduces M learnable vectors {vk}k=1 and\nC fixed class tokens {ci}i=1. Together, they are usually concatenated to form\nthe full prompt si = {v1, v2, ..., vM, ci} for class i. Let the normalized image\nembedding be f\", then the class posterior is estimated as:\n\\(p(y = i|x) = \\frac{exp(sim(f_u, \\text{TextProj}(s_i))/\\tau)}{\\sum_{j=1}^{C}exp(sim(f_u, \\text{TextProj}(s_j))/\\tau)}\\)\nUltimately, the learnable context vectors {vk}M\nk=1 are optimized on a dataset\nD = {(xi, yi)i=1} with the cross-entropy loss:\n\\(L_{true} = -\\mathbb{E}_{(x,y)\\in D} [\\log p(y|x)].\\)\nNotably, optimizing this training objective of CoOp requires examples of\ntrue labels. But in many realistic scenarios, the true label is not accessible due\nto multiple reasons. Instead, we can obtain a candidate label set Yi that contains\nthe true label yi, i.e., yi \u2208 Yi. Unfortunately, we cannot utilize the candidate\nlabel set to optimize the above training objective. Therefore, the next section\nwill study prompt learning with candidate labels.\""}, {"title": "4 Prompt Learning with Candidate Labels", "content": "Prompt learning has been shown to be effective when fine-tuning VLMs, provided\na few examples with perfect labels. Nevertheless, it remains unclear how prompt\nlearning performs with candidate labels.\nIn this section, we first empirically demonstrate the superiority of prompt\nlearning with PLL training objectives. Then, we explain why prompt learning\nperforms better with candidate labels. Finally, based on the observations above,\nwe propose a framework that dynamically aligns the mixed class posterior with\nthe model's output to enhance the performance. Figure 2 depicts our framework."}, {"title": "4.1 Pilot Experiments", "content": "This part explores prompt learning with PLL training objectives and presents\nour key findings. The most important observation is that prompt learning signifi-\ncantly outperforms linear probe with ambiguous supervision. Linear probe [42,38]\nis a strong baseline in few-shot learning, which trains a linear classifier on top of\nthe pre-trained frozen image encoder of VLMs. In the following experiments, we\nwill use linear probe and prompt learning to fine-tune the pre-trained model with\ndifferent types of PLL training objectives to learn from the candidate labels."}, {"title": "4.2 Reasons for the Robustness of Prompt Learning", "content": "In order to investigate the reasons behind the excellent performance of prompt\nlearning, the test accuracy of three models is evaluated by epoch on datasets of\nUCF101 and Caltech101 when q = 0.3, as shown in Figure 4. The models are:\nPrompt learning with random initialization.\nPrompt learning with a handcrafted initialization (i.e., handcrafted prompt).\nLinear probe\nTo be clear, the handcrafted prompts of the second variant are chosen from the\nresult of the prompt engineering from CLIP [38].\nFrom Figure 4, we can see that, for prompt learning, the label ambiguity will\nnot sabotage the generalization ability that the VLM initially possesses. More-\nover, with a proper PLL training objective, the model can quickly adapt to these\ntraining data. Furthermore, even the performance of the model with a randomly\ninitialized prompt can rapidly return to zero-shot performance. Conversely, due\nto the lack of zero-shot ability, linear probe struggles with the candidate labels\nat first and cannot learn well from the candidate labels.\nIn contrast to linear probe, the special position of the tuning parameter\nof prompt learning keeps the zero-shot ability of VLMs. Even if the tunable\nprompt is initialized randomly, due to the regularization effect of the fixed class\ntoken, the model is more robust to the noisy false-positive labels [46] and will\nachieve the zero-shot ability to the downstream data through only a few epochs.\nRemarkably, the zero-shot ability is essential in candidate label disambiguation\nbecause it enables the model to select a more likely true label initially, which will\nmitigate the error accumulation problem [49] in PLL. The error accumulation\nproblem means that since most PLL methods leverage the model's prediction\nfrom the previous iteration, the bias will be accumulated and pose a challenge for\nadjusting mistakes once a false-positive label is identified as the true label, thus\nseverely hurting the generalization of the model. For linear probe, it can only\nrandomly guess which label is correct at first and will significantly suffer from"}, {"title": "4.3 The Proposed Framework", "content": "Our framework is proposed to further improve the performance of prompt learn-\ning with candidate labels. It provides a simple but significant regularization that\naligns the mixed prediction of the handcrafted and soft prompt with the current\nmodel output using weighted cross-entropy loss. It is shown in Figure 2.\nPrompt Alignment Regularization. Let X be the input space, and y =\n{1,2,..., C} be the label space. The i-th learnable prompt si = {v1, v2, \u2026\u2026\u2026, vM, ci},\nwhere {vm}m=1 denotes M learnable tokens and ci is the the word embedding\nfor the i-th class name. Similarly, the i-th manually crafted prompt is denoted\nas hi. To clarify, fi(x; si) and gi(x; hi) are the softmax outputs of the i-th label,\nas predicted by the learnable prompt and handcrafted prompt separately. When\n(x, Y) is drawn from the partialized dataset, the alignment loss is calculated as:\n\\(L_{align}(x, Y) = -\\sum_{i \\in Y}^{C} P_i \\log f_i(x; s),\\)\nwhere Pi is mixed linearly with the class posteriors predicted by both prompts:\n\\(P_i = \\alpha p(y = i | x; s) + (1 - \\alpha) p(y = i | x; h).\\)\nSince the non-candidate labels can never be the ground-truth label, the class\nposteriors are recalculated as:\n\\(p(y = i | x; s) =\\begin{cases} f_i(x; s), & i \\in Y, \\\\ 0, & i \\notin Y. \\end{cases}\\)\n\\(p(y = i | x; h) =\\begin{cases} g_i(x; h), & i \\in Y, \\\\ 0, & i \\notin Y. \\end{cases}\\)\nThis regularization term can be adapted to any PLL training objective as:\n\\(L_{total}(X, Y) = L_{PLL}(X, Y) + \\beta L_{align} (X, Y).\\)\nDynamic Mixing Strategy. In Equation (5), we use a balancing factor \u03b1\nto mix the handcrafted and soft prompt predictions. However, a fixed balancing\nfactor may be sub-optimal since the performance of the soft prompt will surpass\nthe handcrafted prompt as training goes on. With a fixed \u03b1, the performance\nof the model will be affected by the low-quality predictions of the handcrafted"}, {"title": "5 Experiments", "content": "5.1 Experimental Setting\nDatasets. We adopt 10 image recognition datasets: ImageNet [5] and Cal-\ntech101 [8] for generic object classification; OxfordPets [36], StanfordCars [24],\nFlowers102 [35], FGVCAircraft [31], and Food101 [1] for fine-grained classifica-\ntion; SUN397 [48] for scene recognition; UCF101 [40] for action classification;\nDTD [3] for texture classification. In order to evaluate the model\u2019s performance\nfor partial-label learning, as the same in Section 4.1, we define the level of label\nambiguity q as the uniform probability of flipping negative labels \u0177i \u2260 yi to\nfalse-positive labels inside the candidate label set Y\u00ec: q = Pr(Yi \u2208 Yi \u0177i \u2260 yi).\nIn addition, we use a 16-shot fine-tuning strategy, randomly choosing 16 images\nper class from the partialized dataset.\nImplementation Details. Our implementation is based on Pytorch [37]. We\napply prompt learning on a pre-trained CLIP whose backbone of the image en-\ncoder is ResNet-50 [15]. The total number of the learnable prompt tokens is\n16, and the fixed class tokens are at the end of the prompt. Models are trained\nwith a batch size of 32 and 50 total epochs for each method and dataset, ex-\ncept for ImageNet, which sets the batch size to 256. The optimizer is SGD with\na cosine decay schedule annealing the learning rate to 0.00001. The learning\nrate for prompt learning and our framework is initialized to be 0.002. Follow-\ning CoOp[57], the learnable vectors are initialized from a zero-mean Gaussian\ndistribution with a standard deviation equal to 0.02. As for the handcrafted\nprompts of our framework, we follow the result of prompt engineering of CLIP\n[38]. For the hyperparameters of our method, we set \u03b1 = 0.5, \u03c4 = 25, \u03b2 = 1.\nMoreover, if a confidence matrix is required in the PLL method, it will be ini-\ntialized with the model output before training. We conduct all experiments on\neight NVIDIA RTX 3090 GPUs and report the average test accuracy and the\nstandard deviation of 4 experiments while keeping the seeds fixed."}, {"title": "5.2 Main Results", "content": "PLL Training Objectives. We prove the effectiveness of our framework\nby incorporating our framework with six state-of-the-art PLL methods on ten"}, {"title": "5.3 Further Analysis", "content": "In this part, we conduct extensive experiments to assess the effectiveness and\nstability of our framework by comparing test accuracy with the PLL training\nobjective of PiCO in 3 representative datasets: DTD, FGVCAircraft and Cal-\ntech101.\nInfluence of Different Handcrafted Prompts. Because our framework\nincorporates handcrafted prompts, it is crucial to determine the performance\nof our framework with differently crafted prompts. In Table 2, we design some\nprompts with zero-shot performance lower than the default handcrafted prompts\n[38] and evaluate our framework with these prompts at different levels of label"}, {"title": "6 Conclusion", "content": "This work, for the first time, investigated the scenario when fine-tuning vision-\nlanguage models (VLMs) with candidate labels. Throughout a series of exper-\niments, we empirically demonstrated that prompt learning is superior to other\nfine-tuning methods when fine-tuning VLMs with candidate labels. However, as\nthe ambiguity of candidate labels increases, the performance of prompt learning\nis degraded. To alleviate this issue, we proposed a framework that disambiguates"}]}