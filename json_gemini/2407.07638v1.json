{"title": "Tuning Vision-Language Models with Candidate Labels by Prompt Alignment", "authors": ["Zhifang Zhang", "Beibei Lil"], "abstract": "Vision-language models (VLMs) can learn high-quality rep-resentations from a large-scale training dataset of image-text pairs. Prompt learning is a popular approach to fine-tuning VLM to adapt them to downstream tasks. Despite the satisfying performance, a major limita-tion of prompt learning is the demand for labelled data. In real-world scenarios, we may only obtain candidate labels (where the true label is in-cluded) instead of the true labels due to data privacy or sensitivity issues. In this paper, we provide the first study on prompt learning with candi-date labels for VLMs. We empirically demonstrate that prompt learning is more advantageous than other fine-tuning methods, for handling candi-date labels. Nonetheless, its performance drops when the label ambiguity increases. In order to improve its robustness, we propose a simple yet ef-fective framework that better leverages the prior knowledge of VLMs to guide the learning process with candidate labels. Specifically, our frame-work disambiguates candidate labels by aligning the model output with the mixed class posterior jointly predicted by both the learnable and the handcrafted prompt. Besides, our framework can be equipped with vari-ous off-the-shelf training objectives for learning with candidate labels to further improve their performance. Extensive experiments demonstrate the effectiveness of our proposed framework.", "sections": [{"title": "1 Introduction", "content": "Large-scale vision-language models (VLMs) such as CLIP [38], ALIGN [19], and Coca [51] have become excellent base models in multiple domains, most of which employ a dual-encoder architecture to align the natural images with descriptive texts. Remarkably, this special training manner has endowed VLMs with superior zero-shot transfer performance on visual recognition tasks. In specific, during the inference, the pre-trained text encoder receives inputs in the form of man-crafted prompts, e.g., \u201ca photo of <CLS>.\u201d. Subsequently, all the generated textual embeddings are matched with the visual embedding obtained from the image encoder to predict the image category. However, the powerful zero-shot ability of VLMs was shown to be heavily dependent on the choice of handcraftedprompts, which needs substantial efforts and professional domain knowledge to design [57]. To avoid the manual design of the prompts, prompt learning [57] is proposed, which treats the textual prompt as additional learnable parameters and tunes them while keeping all the parameters of the pre-trained model fixed. Later, the concept of the prompt is extended to visual prompt [20] and multi-modal prompt [22] in VLMs. Overall, there has been increasing attention paid to prompt learning due to its potential to perform significantly better than zero-shot transfer with a few sets of labelled data.While prompt learning has demonstrated effectiveness and efficiency in few-shot supervised learning, the true labels must be provided for the training data used in prompt learning. This is a significant defect and will limit the usage of prompt learning in various real-world scenarios because we may be unable to collect accurate labels due to security issues or labelling difficulties. Fortunately, obtaining a set of candidate labels that includes the true label in these situations is easier. For example, as shown in Figure 1, it is challenging to determine which is the true label from \u2018Hawk', 'Eagle', and 'Falcon', hence all these three labels can be considered as candidate labels during the annotation process of this bird recognition task. As we see, learning with only candidate labels (also widely known as partial-label learning [44,47,30,11]) is practically significant, which also has arisen in many vital applications such as web mining [28], online annotation [41] and ecoinformatics [27]. Nevertheless, existing PLL methods primarily focus on training a model from scratch, and the effectiveness of PLL in the new training paradigm called prompt learning remains unconfirmed. To bridge this research gap, we, for the first time, explore the validity and potential approaches for prompt learning with candidate labels.This paper provides empirical evidence that prompt learning combined with the prevailing PLL training objectives exhibits superior performance when learn-ing with only candidate labels. Moreover, we conjecture the reason behind the robustness of prompt learning to candidate labels lies in its special position of learnable parameters, which endows the model with strong zero-shot ability, thus mitigating the error accumulation problem [49] in PLL. However, as exper-imentally suggested, candidate labels will degrade the model's performance as"}, {"title": "2 Related Work", "content": "Vision-language models with fine-tuning. Recently, significant advance-ments have been made in the field of vision-language models (VLMs) [19,38,51,52]. Unlike models learning from uni-modal supervision, VLMs align visual and tex-tual signals to learn rich representation from massive image-text pairs in the pre-training stage. One of the keys to the excellent performance of VLMs is the tremendous amount of training data. CLIP [38] trains both the text and im-age encoder simultaneously using a contrastive loss on a dataset of 400 million image-text pairs. ALIGN [19] leverages a noisy pre-training dataset of 1.8 billion pairs to train the model. However, although VLMs have shown promising perfor-mance in generalizing to new concepts without auxiliary information, their large amount of parameters makes it impractical to fine-tune the entire model [25]. Besides, full-parameter fine-tuning will also make large models prone to overfit-ting to downstream tasks and catastrophic forgetting [32]. To address the above issues, multiple transfer learning methods [57,12,38,55] are proposed to adapt VLMs to downstream tasks effectively and efficiently. Linear probe [38] freezes the pre-trained VLM and trains a linear classifier on top of the image encoder. CLIP-adapter [12] introduces an adapter with residual-style feature blending with the pre-trained features. Notably, the success of most of the fine-tuning methods heavily depends on the acquisition of precisely labelled data.Prompt learning. Among all fine-tuning methods that adapt VLMs to a new task, one typical approach is prompt learning. It treats the text prompt, e.g., \"a photo of a <CLS>\" as continuous learnable parameters, and optimizes the prompt with multiple vision tasks, including image classification [57,56,22,50,20], dense prediction [39,13,26], etc. CoOp [57] is the first work that migrated prompt learning to vision tasks, which fine-tunes CLIP by optimizing the parameters of the learnable textual prompt (also called soft prompt) while keeping the class token fixed. MaPLe [22] adds multi-modal prompts and learns them mutually to align both representation spaces dynamically. Moreover, recent studies have revealed the advantages of prompt learning when fine-tuning with weakly super-vised or unsupervised data [46,33,17,16]. Wu et al. [46] demonstrates that the process of prompt learning is robust to label noise. UPL [17] utilizes handcrafted prompts to generate pseudo labels on unlabelled data and adopts the confident examples per class to tune the learnable prompt.Partial-label learning. Partial-label learning (PLL) allows each training ex-ample to be annotated with a candidate set, containing the true label. Two main-stream strategies have been developed to address this problem: the averaged-based strategy and the identification-based strategy. Average-based strategy dis-ambiguates the candidate labels by treating them equally [18,4,54]. Identification-based strategy regards the true label as a latent variable and selects the most likely label for training [21,9,34,10], which is prone to error accumulation if the wrong label is selected initially [49]. As deep learning thrives, many PLL algo-rithms have been proposed for training with deep neural network [11,30,29,53]. Feng et al. [11] assumes the generation process of partial labels and derives classifier-consistent and risk-consistent methods from a theoretical perspective. PRODEN [30] updates the model parameters and identifies the true labels seam-lessly. PiCO [44] divides the learning process into representation learning by contrastive loss and label disambiguation by prototype and pseudo target up-dating. In particular, as the scale of the parameters and training data of modern deep neural expands [6,2,23], the deep learning community has embraced a new training paradigm of pre-training and fine-tuning. Nonetheless, to the best of our knowledge, little research has explored PLL under this new training paradigm."}, {"title": "3 Preliminaries of Prompt Learning", "content": "In this work, we show that prompt learning is a preferred choice when tun-ing VLMs with candidate labels and propose a novel framework to augment its performance. Primarily, we conduct the experiments and build our approach upon a prevailing method in prompt learning: CoOp [57], which tunes the tex-tual prompt based on a well-known VLM called CLIP [38]. In this section, we briefly revisit the details of CLIP and CoOp.CLIP. CLIP (Contrastive Language-Image Pre-training) is a cutting-edge VLM that learns joint image-text representations through contrastive learning. It con-sists of both an image encoder and a text encoder. The image encoder encodes visual signals, which can be constructed by ResNet [15] or ViT [7]. The textencoder encodes textual signals, which is constructed by Transformer [43]. By projecting data from both modalities into a shared representation space, CLIP aligns them with a contrastive loss function. The contrastive loss encourages pairs of image-text embeddings to be pushed closer, effectively learning a pow-erful multi-modal understanding mechanism. With this pre-training approach, CLIP achieves remarkable performance across various visual tasks, particularly zero-shot image classification. In the zero-shot image classification setting, an image x is encoded into a normalized feature f_x. Likewise, on the text encoder side, a handcrafted prompt of the form \"a photo of a <CLS>.\" is concatenated with the prior class tokens to generate the textual input. After being encoded by the text encoder, assuming the total number of classes is C, a group of normalized text embeddings \\{f_i^t\\}_{i=1}^C is obtained. Finally, the class posterior is estimated as:\nP_{zs}(y = i|x) = \\frac{exp(sim(f_x^u, f_i^t)/\\tau)}{\\sum_{j=1}^C exp(sim(f_x^u, f_j^t)/\\tau)}\t,(1)\nwhere sim(\u00b7, \u00b7) denotes the cosine similarity, and \u03c4 is a temperature factor that controls the range of the output logits.\nCoOp. Although CLIP is amazing at zero-shot transfer, its performance is sensitive to differently designed prompts. In order to overcome the inefficiency of the handcrafted prompt designs, CoOp (Context Optimization) [57] replaces the fixed sentence: \"a photo of a <CLS>.\" with learnable vectors that have the same dimension as the word embedding of CLIP. These learnable vectors will be optimized with a few labelled examples. In the remaining sections, we will study CoOp as a typical case of prompt learning for VLMs.\nM\nTo be specific, assume CoOp introduces M learnable vectors {v_k}_{k=1}^M and C fixed class tokens \\{c_i\\}_{i=1}^C. Together, they are usually concatenated to form the full prompt s_i = \\{v_1, v_2, ..., v_M, c_i\\} for class i. Let the normalized image embedding be f_x^u, then the class posterior is estimated as:\np(y = i|x) = \\frac{exp(sim(f_x^u, TextProj(s_i))/\\tau)}{\\sum_{j=1}^C exp(sim(f_x^u, TextProj(s_j))/\\tau)}\t,(2)\nM\nUltimately, the learnable context vectors {v_k\\}_{k=1}^M are optimized on a dataset D = \\{(x_i, y_i)\\}_{i=1}^N with the cross-entropy loss:\nL_{true} = -E_{(x,y)\\in D} [log p(y|x)].\t(3)\nNotably, optimizing this training objective of CoOp requires examples of true labels. But in many realistic scenarios, the true label is not accessible due to multiple reasons. Instead, we can obtain a candidate label set Y_i that contains the true label y_i, i.e., y_i \u2208 Y_i. Unfortunately, we cannot utilize the candidate label set to optimize the above training objective. Therefore, the next section will study prompt learning with candidate labels."}, {"title": "4 Prompt Learning with Candidate Labels", "content": "Prompt learning has been shown to be effective when fine-tuning VLMs, provided a few examples with perfect labels. Nevertheless, it remains unclear how prompt learning performs with candidate labels.\nIn this section, we first empirically demonstrate the superiority of prompt learning with PLL training objectives. Then, we explain why prompt learning performs better with candidate labels. Finally, based on the observations above, we propose a framework that dynamically aligns the mixed class posterior with the model's output to enhance the performance. Figure 2 depicts our framework.\n4.1 Pilot Experiments\nThis part explores prompt learning with PLL training objectives and presents our key findings. The most important observation is that prompt learning signifi-cantly outperforms linear probe with ambiguous supervision. Linear probe [42,38] is a strong baseline in few-shot learning, which trains a linear classifier on top of the pre-trained frozen image encoder of VLMs. In the following experiments, we will use linear probe and prompt learning to fine-tune the pre-trained model with different types of PLL training objectives to learn from the candidate labels."}, {"title": "4.2 Reasons for the Robustness of Prompt Learning", "content": "In order to investigate the reasons behind the excellent performance of prompt learning, the test accuracy of three models is evaluated by epoch on datasets of UCF101 and Caltech101 when q = 0.3, as shown in Figure 4. The models are:\nPrompt learning with random initialization.\nPrompt learning with a handcrafted initialization (i.e., handcrafted prompt).\nLinear probe\nTo be clear, the handcrafted prompts of the second variant are chosen from the result of the prompt engineering from CLIP [38].\nFrom Figure 4, we can see that, for prompt learning, the label ambiguity will not sabotage the generalization ability that the VLM initially possesses. More-over, with a proper PLL training objective, the model can quickly adapt to these training data. Furthermore, even the performance of the model with a randomly initialized prompt can rapidly return to zero-shot performance. Conversely, due to the lack of zero-shot ability, linear probe struggles with the candidate labels at first and cannot learn well from the candidate labels.\nIn contrast to linear probe, the special position of the tuning parameter of prompt learning keeps the zero-shot ability of VLMs. Even if the tunable prompt is initialized randomly, due to the regularization effect of the fixed class token, the model is more robust to the noisy false-positive labels [46] and will achieve the zero-shot ability to the downstream data through only a few epochs. Remarkably, the zero-shot ability is essential in candidate label disambiguation because it enables the model to select a more likely true label initially, which will mitigate the error accumulation problem [49] in PLL. The error accumulation problem means that since most PLL methods leverage the model's prediction from the previous iteration, the bias will be accumulated and pose a challenge for adjusting mistakes once a false-positive label is identified as the true label, thus severely hurting the generalization of the model. For linear probe, it can only randomly guess which label is correct at first and will significantly suffer from"}, {"title": "4.3 The Proposed Framework", "content": "Our framework is proposed to further improve the performance of prompt learn-ing with candidate labels. It provides a simple but significant regularization that aligns the mixed prediction of the handcrafted and soft prompt with the current model output using weighted cross-entropy loss. It is shown in Figure 2.\nPrompt Alignment Regularization. Let X be the input space, and y = \\{1,2,..., C\\} be the label space. The i-th learnable prompt s_i = \\{v_1, v_2, ..., v_M, c_i\\}, where \\{v_m\\}_{m=1}^M denotes M learnable tokens and c_i is the the word embedding for the i-th class name. Similarly, the i-th manually crafted prompt is denoted as h_i. To clarify, f_i(x; s) and g_i(x; h) are the softmax outputs of the i-th label, as predicted by the learnable prompt and handcrafted prompt separately. When (x, Y) is drawn from the partialized dataset, the alignment loss is calculated as:\nL_{align}(x, Y) = - \\sum_{i \\in Y} p_i log f_i(x; s),\t(4)\nwhere p_i is mixed linearly with the class posteriors predicted by both prompts:\np_i = \\alpha p(y = i | x; s) + (1 - \\alpha) p(y = i | x; h).\t(5)\nSince the non-candidate labels can never be the ground-truth label, the class posteriors are recalculated as:\np(y = i | x; s) = \\begin{cases}\nf_i(x;s), &i\\in Y,\\\\\n0, &i \\notin Y.\n\\end{cases}\t(6)\np(y = i | x; h) = \\begin{cases}\ng_i(x; h), &i\\in Y,\\\\\n0, &i \\notin Y.\n\\end{cases}\t(7)\nThis regularization term can be adapted to any PLL training objective as:\nL_{total}(X, Y) = L_{PLL}(X, Y) + \\beta L_{align} (X, Y).\t(8)\nDynamic Mixing Strategy. In Equation (5), we use a balancing factor \u03b1 to mix the handcrafted and soft prompt predictions. However, a fixed balancing factor may be sub-optimal since the performance of the soft prompt will surpass the handcrafted prompt as training goes on. With a fixed \u03b1, the performance of the model will be affected by the low-quality predictions of the handcrafted"}, {"title": "5 Experiments", "content": "5.1 Experimental Setting\nDatasets. We adopt 10 image recognition datasets: ImageNet [5] and Cal-tech101 [8] for generic object classification; OxfordPets [36], StanfordCars [24], Flowers102 [35], FGVCAircraft [31], and Food101 [1] for fine-grained classifica-tion; SUN397 [48] for scene recognition; UCF101 [40] for action classification; DTD [3] for texture classification. In order to evaluate the model's performance for partial-label learning, as the same in Section 4.1, we define the level of label ambiguity q as the uniform probability of flipping negative labels \\hat{y_i} \u2260 y_i to false-positive labels inside the candidate label set Y_i: q = Pr(Y_i \u2208 Y_i | \\hat{y_i} \u2260 y_i). In addition, we use a 16-shot fine-tuning strategy, randomly choosing 16 images per class from the partialized dataset.\nImplementation Details. Our implementation is based on Pytorch [37]. We apply prompt learning on a pre-trained CLIP whose backbone of the image en-coder is ResNet-50 [15]. The total number of the learnable prompt tokens is 16, and the fixed class tokens are at the end of the prompt. Models are trained with a batch size of 32 and 50 total epochs for each method and dataset, ex-cept for ImageNet, which sets the batch size to 256. The optimizer is SGD with a cosine decay schedule annealing the learning rate to 0.00001. The learning rate for prompt learning and our framework is initialized to be 0.002. Follow-ing CoOp[57], the learnable vectors are initialized from a zero-mean Gaussian distribution with a standard deviation equal to 0.02. As for the handcrafted prompts of our framework, we follow the result of prompt engineering of CLIP [38]. For the hyperparameters of our method, we set \u03b1 = 0.5, T = 25, \u03b2 = 1. Moreover, if a confidence matrix is required in the PLL method, it will be ini-tialized with the model output before training. We conduct all experiments on eight NVIDIA RTX 3090 GPUs and report the average test accuracy and the standard deviation of 4 experiments while keeping the seeds fixed.\n5.2 Main Results\nPLL Training Objectives. We prove the effectiveness of our framework by incorporating our framework with six state-of-the-art PLL methods on ten"}, {"title": "5.3 Further Analysis", "content": "In this part", "datasets": "DTD, FGVCAircraft and Cal-tech101.\nInfluence of Different Handcrafted Prompts. Because our framework incorporates handcrafted prompts, it is crucial to determine the performance of our framework with differently crafted prompts. In Table 2, we design some prompts with zero-shot performance lower than the default handcrafted prompts [38"}]}