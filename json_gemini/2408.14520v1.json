{"title": "Towards Graph Prompt Learning: A Survey and Beyond", "authors": ["Qingqing Long", "Yuchen Yan", "Peiyan Zhang", "Chen Fang", "Wentao Cui", "Zhiyuan Ning", "Meng Xiao", "Ning Cao", "Xiao Luo", "Lingjun Xu", "Shiyue Jiang", "Zheng Fang", "Chong Chen", "Xian-Sheng Hua", "Yuanchun Zhou"], "abstract": "Large-scale \"pre-train and prompt learning\" paradigms have demonstrated remarkable adaptability, enabling broad applications across diverse domains such as question answering, image recognition, and multimodal retrieval. This approach fully leverages the potential of large-scale pre-trained models, reducing downstream data requirements and computational costs while enhancing model applicability across various tasks. Graphs, as versatile data structures that capture relationships between entities, play pivotal roles in fields such as social network analysis, recommender systems, and biological graphs. Despite the success of pre-train and prompt learning paradigms in Natural Language Processing (NLP) and Computer Vision (CV), their application in graph domains remains nascent. In graph-structured data, not only do the node and edge features often have disparate distributions, but the topological structures also differ significantly. This diversity in graph data can lead to incompatible patterns or gaps between pre-training and fine-tuning on downstream graphs. We aim to bridge this gap by summarizing methods for alleviating these disparities. This includes exploring prompt design methodologies, comparing related techniques, assessing application scenarios and datasets, and identifying unresolved problems and challenges. This survey categorizes over 100 relevant works in this field, summarizing general design principles and the latest applications, including text-attributed graphs, molecules, proteins, and recommendation systems. Through this extensive review, we provide a foundational understanding of graph prompt learning, aiming to impact not only the graph mining community but also the broader Artificial General Intelligence (AGI) community. This survey underscores the potential of graph prompt learning for future research and practical applications, paving the way for advancements in this promising area.", "sections": [{"title": "INTRODUCTION", "content": "In recent decades, the rapid advancement of artificial intelligence (AI) has facilitated its widespread application across various domains [1]\u2013[3]. Graphs, as a distinctive data structure capable of describing general relationships between entities, have found extensive use in numerous real-world scenarios, including social network analysis [4]\u2013[6], recommendation systems [7]\u2013[10], and the chemical industry [11], [12]. To uncover the latent patterns within graph data and support downstream tasks, graph representation learning has emerged as a primary focus in the field of graph analysis [13], [14]. Given the non-Euclidean and high-dimensional nature of graph data, direct analysis of raw graphs poses significant challenges [15]-[17]. Graph representation learning addresses this issue by compressing graph data into low-dimensional Euclidean space, where vectors effectively represent the structure and properties of graphs [18], [19].\nTraditional methods for graph representation learning primarily embed nodes into low-dimensional spaces based on the graph adjacency matrix [20], [21]. With the evolution of deep learning, Graph Neural Networks (GNNs) have introduced novel solutions for graph representation learning [22], [23]. The core concept behind GNNs is message passing, which integrates local structural features into node attributes [13], [24]. Despite their success in various applications, GNNs predominantly rely on a supervised training paradigm, which depends on extensive labeled datasets to learn patterns between data samples and annotations for specific tasks [25]. However, data annotation is both time-consuming and often requires expert knowledge, posing challenges to scalability for large datasets [16]. Moreover, supervised learning approaches are susceptible to overfitting, which can adversely impact downstream performance [26].\nRecently, self-supervised learning has achieved significant success in fields such as Natural Language Processing (NLP) [27], Computer Vision (CV) [28], and Recommender Systems (RecSys) [29]. Inspired by these advancements, there has been increasing interest in adapting the \"pre-training and fine-tuning\" paradigm to the graph domain [30], [31]. This approach leverages task-agnostic information to create pretext tasks for model initialization, followed by fine-tuning on target tasks with fewer labeled samples [26]. The knowledge gained from extensive pretext datasets enhances the model's generalization capabilities for specific target problems [32]. Within the \"pre-train, fine-tune\u201d framework, researchers focus on designing pretext task objectives and aligning them with target problems [33].\nGraph prompt learning, an innovative extension of prompt engineering, applies the principles of prompt-based learning-widely studied in NLP-to the graph domain [27], [34]. Prompt engineering involves augmenting the input of large-scale pre-trained models with task-specific hints, i.e., prompts [28]. These prompts could be manually crafted natural language instructions, automatically generated in-structions, or vector representations, which are referred to as discrete (hard) prompts and continuous (soft) prompts, respectively. This method has gained prominence due to its ability to adapt pre-trained models to new tasks with minimal labeled data and without extensive parameter tuning.\nThe primary advantage of graph prompt learning lies in its ability to efficiently leverage large pre-trained models for graph-related tasks. By providing task-specific prompts, these models can be guided to perform new tasks using their existing knowledge base. This approach significantly reduces the need for extensive labeled data and computational resources typically required for fine-tuning. Furthermore, it allows a single pre-trained model to be utilized for a wide range of downstream tasks, enhancing its applicability in real-world scenarios.\nWhile prompt engineering has been extensively studied and applied in NLP and CV, its application in graph learning remains relatively unexplored. This survey aims to bridge this gap by providing a comprehensive overview of graph prompt learning. We classify the various prompting methods into categories based on their design and application, and we discuss the unique challenges and opportunities presented by this approach in the graph domain.\nThe remainder of this paper is organized as follows. In Section 4, we summarize the basic prompt design methodologies, including token design principles, task alignment mechanisms, and prompt tuning methods. Section 5 introduces general methods of homogeneous and heterogeneous graph prompting and summarizes their downstream applications, such as in molecular analysis, recommendation systems, and large language models (LLMs). Section 6 reviews benchmark datasets and evaluation methods, categorizing datasets into homophilous and heterophilous graphs and discussing specific evaluation criteria for tasks such as node classification, graph classification, and link prediction. We also highlight challenges related to dataset descriptions and the need for standardized evaluation methods. In Section 7, we discuss the unresolved problems and future work, addressing the limitations of current graph prompt methods and proposing directions for deeper theoretical understandings. Through this survey, we aim to provide a foundational understanding of graph prompt learning, highlighting its potential for future research and applications. By addressing the current limitations and exploring new methodologies, we hope to advance the field and unlock new possibilities for graph representation learning.\nIn comparison with existing surveys, this survey has three inclusive traits: (1) Novel Taxonomy: This work systematically examines the research works in the field of graph prompt learning and proposes a new classification. We first outline the design principles into three parts, i.e., prompting tokenization, alignment strategy between pretraining and downstream tasks, and prompt tunning strategy. We then discuss these principles in the context of both homogeneous and heterogeneous graphs. Finally, we summarize the applications of graph prompt learning, which include text-attributed graphs, molecules, proteins, and recommendation systems. This is the first time such a classification has been proposed, providing a new perspective on the organization and categorization of graph prompt learning techniques. (2) Up-to-date Summarization: Graph prompt learning is a newly proposed concept, emerging around 2023. Thus, we have collected the most recent and relevant papers from 2023 to 2024, ensuring that our survey reflects the latest developments and state-of-the-art research in this rapidly evolving field. (3) Thorough Discussions: Based on the latest research, this survey offers a thorough discussion encompassing the background, design principles, benchmark datasets, and the latest applications of graph prompt learning. We showcase the latest and significant breakthroughs, analyze the current challenges, and provide insights to help readers grasp key concepts and principles, facilitating a deep understanding.\nThe broader impact of our work is twofold: (1) To Graph Community. Our survey serves as a valuable resource for machine learning researchers focusing on graph-"}, {"title": "TAXONOMY", "content": "This section introduces the terms and notations related to Graph Prompt Learning that are used throughout the paper."}, {"title": "2.1 Terminology", "content": "In this section, we outline essential terms and their descriptions. Instead of providing formal definitions, we offer general explanations for clarity.\nGraph Prompt: A directive or cue included in the input of a pre-trained graph model to assist in executing specific tasks. These prompts can take the form of task-oriented instructions, handcrafted natural language directives, or vector-based representations.\nPrompting Method: A strategy used to embed prompts into the input, aiming to guide the model's actions and improve its performance.\nPrompt Tuning: The process of optimizing prompts to maximize the model's performance on specific tasks. This involves adjusting the prompts based on task requirements and model responses.\nGraph Neural Networks: A class of neural networks designed to work with graph-structured data. GNNs use message passing to integrate local structural features into node attributes.\nSelf-Supervised Learning: A learning paradigm where the model learns to predict part of the data from other parts of the data. It does not require labeled data, which makes it suitable for large-scale, unlabeled datasets.\nPre-Training and Fine-Tuning: A dual-phase training approach where a model is first pre-trained on an extensive, general-purpose dataset to acquire broad features. Subsequently, the model is fine-tuned on a more focused, task-specific dataset to tailor its capabilities to particular tasks."}, {"title": "2.2 Notations", "content": "We introduce the mathematical notations used throughout the paper in this section. All formulations in this work adhere to these notations unless otherwise specified. Consider an attributed graph $G = (V, E, X)$, in which $V = V_1, V_2, ..., U_{|V|}$ represents the set of nodes, $E C V \\times V$ represents the set of edges, and $X$ denotes the feature matrix. Each node $v_i$ in the graph has an associated attribute vector $x_i \\in \\mathbb{R}^F$, where $F$ represents the dimension of the attribute vector. The collection of attribute vectors for all nodes forms the feature matrix $X \\in \\mathbb{R}^{|V|\\times F}$. The edges can also be represented by an adjacency matrix $A \\in \\mathbb{R}^{|V|\\times |U|}$, where $A_{ij} = 1$ if $(v_i, v_j) \\in E$ and $A_{ij} = 0$ otherwise. Thus, the graph can alternatively be represented as $G = (A, X)$."}, {"title": "2.3 Literature Overview", "content": "This paper will be organized as follows:\nIn Section 3, we explore the foundational aspects of graph prompt learning. We begin by discussing the essential token designations within the graph domain, which encompass node token design, structure token design, and task token design. We then examine how these tokens are embedded into graph-related tasks, addressing both node-level alignment and graph-level alignment. Subsequently, we delve into advanced fine-tuning strategies for prompt learning, including pre-training models and integrating meta-learning techniques to enhance performance and adaptability.\nSection 4 provides an extensive review of existing graph prompt learning methodologies, categorized into homogeneous and heterogeneous prompt learning. For homogeneous prompt learning, we investigate three critical dimensions: i) the integration of prompts as model inputs, ii) the enhancement of model outputs facilitated by prompts, and iii) the incorporation of supplementary information beyond mere attributes and structure into graph prompts. In the context of heterogeneous graph prompt methods, we focus on i) the design and application of prompts for heterogeneous tokens and message-passing processes, and ii) the impact of these prompts on improving the efficacy of heterogeneous graph tasks.\nIn Section 5, we delve into the diverse applications of graph prompt technology. We primarily address three prominent areas: i) the application of prompts in molecular graph analysis, where prompt learning aids in understanding complex molecular interactions; ii) the application of prompts in Text-Attributed Graphs, enhancing the integration and analysis of textual and structural data; and iii) the application of prompts in recommender systems, where prompts contribute to more personalized and accurate recommendation algorithms.\nSection 6 is devoted to summarizing the common benchmarks, datasets, and experimental metrics used in the evaluation of graph prompt learning techniques. We provide a detailed comparison of various evaluation criteria tailored to different graph-related tasks, highlighting their strengths and limitations.\nIn Section 7, we address several challenging problems currently faced in the field of graph prompt learning. We also outline potential future research directions, emphasizing the need for innovative solutions and continued advancements to overcome existing limitations and to further enhance the applicability and robustness of graph prompt learning methodologies. Finally, we conclude our survey in Section 8."}, {"title": "CONNECTION TO RELATED TECHNIQUES", "content": "This section presents a comparative analysis of graph prompt learning against relevant existing technologies to elucidate its distinctive features and advantages. We conduct two primary comparisons: (1) graph prompt learning versus graph pre-training/fine-tuning; (2) graph prompt learning in relation to graph few-shot learning. These comparisons aim to provide a comprehensive understanding of graph prompt learning\u2019s position within the current technological landscape."}, {"title": "3.1 Comparision with Graph Pre-training/Fine-tuning", "content": "Graph pre-training [35], [36] leverages large-scale graph datasets to learn generalizable representations, capturing intrinsic graph properties and topological patterns [37]. This unsupervised learning phase yields models with enhanced adaptability across diverse graph domains. The subsequent fine-tuning process optimizes these pre-trained models for specific tasks, exploiting the acquired structural knowledge to improve performance. This transfer learning approach enables efficient adaptation to novel graph problems, reducing the need for extensive task-specific data and training. The two-stage paradigm-pre-training followed by fine-tuning-significantly enhances the effectiveness of graph neural networks (GNNs) in various graph-based applications, from molecular modeling to social network analysis.\nTaking the node classification task as an example, let $g_{\\theta}$ denote the pre-trained GNNs, and $G$ denote the graph of downstream tasks, i.e., $G = \\{V,E,X,A\\}$ represent a graph with n nodes, where: $V = \\{v_i\\}_{i=1}^n$ is the node set, $E \\subseteq V \\times V$ defines the edge set, $X \\in \\mathbb{R}^{n \\times d}$ denotes the d dimensional node feature matrix, $A \\in \\mathbb{R}^{n \\times n}$ is the adjacency matrix such that $A_{ij} = 1$ if $(v_j, v_j) \\in E$, and 0 otherwise. And let $[i]$ denote the node index of node $v_i$, and $D$ denote the label distribution of downstream tasks, i.e., $D = \\{(v_1, Y_1), ..., (v_n, Y_n)\\}$. Then the pre-training and finetuning paradigm [38] optimizes the parameters of the pretrained model $g_{\\theta}$ and the learnable projection head $g_{\\phi}$ to maximize the likelihood of predicting labels $V_i$ for the downstream tasks. The paradigm is formally defined as:\n$\\theta^*, \\phi^* = \\arg \\max_{\\theta,\\phi} \\sum_{i=1}^{n} log P(V_i|g_{\\phi}(g_{\\theta}(X, A)[i]))$. (1)\nIn contrast, the graph prompt learning paradigm [39], [40] maintains fixed parameters for the pre-trained GNN model. The target of graph promoting is to obtain task-specific graph prompts $T$. Before the optimization of prompt learning, the node embeddings $X$ and edge embeddings $E$ of the downstream graphs are initialized by the pre-trained model $g_{\\theta}$, and the initialization process is formulated as:\n$X,E = g_{\\theta}(X, A)$. (2)\nThen, general graph templates will be added to the input graph that include two learnable components, i.e., feature (node-level) prompts and adjacency (edge/motif-level) prompts:\n$G^* : (X, \\hat{E}) = T(X,E)$. (3)"}, {"title": "3.2 Comparision with Graph Few-shot Learning", "content": "Few-shot learning focuses on enabling models to learn new tasks with very limited amounts of labeled data, typically referred to as \"shots\u201d [41]. Few-shot learning on graphs aims to generalize well on graph-structured data, even when only a few examples of certain nodes, edges, or subgraphs are available for training [42], [43]. To address few-shot learning problems over graphs, meta-learning methods are usually adopted [44]. The application of meta-learning in graph learning involves training a model on various tasks, such that it can leverage prior knowledge from various tasks to rapidly adapt to new, unseen tasks with only a few labeled examples. Specifically using the node classification task as an example, consider a set of tasks $T = \\{T_1, T_2, ..., T_n\\}$ drawn from a distribution over tasks $p(T)$. Each task $T_i$ is associated with a graph $G_i = (V_i, &i)$, in which $V_i$ is the set of nodes and $E_i$ is the set of edges. Moreover, each task $T_i$ involves a node classification problem with its own training set $D_i^{train}$ and validation set $D_i^{val}$. The objective during meta-training is to learn a model $M_\\theta$ that minimizes the loss on $D_i^{val}$ after being fine-tuned on $D_i^{train}$. The meta-learning objective can be formalized as:\n$\\mathcal{L} = \\sum_{T \\in T} \\mathcal{L}(D^{val}; \\theta'), \\\\ \\theta' = \\text{Update} (\\theta, D^{train} )$. (5)\nwhere the Update operation represents the adaptation process (e.g., a few gradient steps), and $\\mathcal{L}$ denotes the loss objective (e.g., cross-entropy for classification). Then, the adaptation process involves adjusting the model parameters based on a few examples from the new task, leveraging the knowledge gained during the meta-training phase. Specifically, when given a new task $T_{new}$ with its own graph $G_{new} = (V_{new}, E_{new})$ and a small labeled dataset $D_{new}^{train}$, the model $M_\\theta$ is firstly fine-tuned on $D_{new}^{train}$ to obtain adapted parameters $\\theta'_{new}$ and the adapted model $M_{\\theta'}^{new}$ can now be evaluated on a separate test set $D_{new}^{test}$ for the new task.\nGraph prompt learning, on the other hand, seeks to repurpose existing models to new tasks through the clever design of prompts, which guide the model\u2019s understanding and application of its pre-existing knowledge to the task at hand with minimal data. Unlike meta learning, graph prompt learning does not need to specifically design other models or special learning paradigms to generalize well from a small number of examples."}, {"title": "DESIGN PRINCIPLE", "content": "In this section, we summarize the basic prompt design methodologies for a better understanding of the working principles and mechanisms behind graph prompting. We first summarize the token design principles. Then we sum up the general task alignment mechanisms of the inherent gap between the pre-trained and downstream tasks. Finally, the prompt tuning methods are introduced."}, {"title": "4.1 Prompting Tokenization", "content": "The concept of tokens is derived from NLP, which is proposed to segment text into meaningful units. The to-kens capture the semantic and syntactic structure of the text. Tokenization techniques [45], [46], encompass various levels, including character-level, word-level, subword-level, and sentence-level fragments, etc. By decomposing long texts into smaller, more manageable units, the tokenization process enables language models to process and generate text more efficiently, reducing computational complexity and memory requirements. Graph prompting learning also has learnable tokens designed for graph-structured data, which decomposes complex large graphs into smaller, meaningful subgraphs or graph features, thereby enabling more efficient handling of graphs. Unlike text or sentences, graph-structured data cannot be directly segmented based on spaces or punctuation marks. Based on the information it captures across different levels of graphs, tokenization for prompting learning can be classified into three types, i.e., node tokenization, structure tokenization, and task tokenization."}, {"title": "4.1.1 Node Tokenization", "content": "During the pretraining phase, the pre-trained model generally adopts an unsupervised loss, which mainly focuses on the structural characteristics of pre-trained large-scale graphs. In this context, the model's ability to adapt to node features is relatively poor, necessitating node-level prompts for rapid adaptation to downstream tasks. Let $P_{node} = \\{p_1, p_2, ,p_{|P_{node}|} \\}$ denote the learnable node-level $|P_{node}|$ prompt tokens, where $p_i \\in \\mathbb{R}^{1\\times d}$ and d denotes the dimensionality of the node features. In practice, we observe $|P_{node}| < |V|$ in graph prompt learning tactics. Based on these token vectors, the node-level tokenization process can be expressed by the following formulation,\n$\\hat{X_i} = X_i \\bigoplus \\big( X_i \\bigotimes \\big( \\sum_{k=1}^{|P|} w_{ik}p_k \\big) \\big)$, (6)\nwhere $\\bigoplus$ generally denotes the add operation, $\\bigotimes$ denotes the multiplication operation, and $w_{ik}$ denotes the adaptive weight for different types of nodes. The node tokenization process aims at replacing the input node features $x_i$ with the prompted features and sending $\\hat{x_i}$ for downstream processing."}, {"title": "4.1.2 Structure Tokenization", "content": "Many works proved that there is a structural discrepancy between the pretraining and downstream dataset [39], [47], [48]. For example, the graph structure is extremely different between a pertaining citation network and downstream molecular graphs [48]. The molecular graphs are more dense and have a larger average node degree, while the citation graphs have a small node degree. Compared with node tokenization, structure tokenization provides a perspective to capture global information of a target node $v_i$. The structure token is also called the semantic or contextual token, as the graph structure always resides and preserves rich self-information and contextual information by neighboring node features and connections [49]\u2013[51], which play distinct roles in various downstream tasks. Let $p$ denote a structure-related structure prompt around the center node $v_i$, which can be one type of graph meth-paths, motifs, subgraphs, etc [52], [53]. Then the $P_{struc.} = \\{p_1, p_2,\u2026,p_{|P_{struc.}|} \\}$ denotes the set of structure prompts. Let $z_i$ denote the node representation of $v_i$ aggregated with structure information. We summarize the general process based on $P_{Struc.}$,\n$Z_i = \\sigma \\bigg(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j} Z_j \\bigg)$. (7)\n$\\alpha_{i,j} = \\frac{\\exp \\big( \\sigma \\big( p^T . [z_i \\bigoplus z_j] \\big) \\big)}{\\sum_{k\\in \\mathcal{N}(i)} \\exp \\big( \\sigma \\big( p^T . [z_i \\bigoplus z_k] \\big) \\big)}$,\nwhere $\\bigoplus$ denotes the multiplication or concatenation operation. GPPT [53] designs subgraph-centered structure prompts based on the social influence theory [54] to leverage the expressive neighborhood information. SAP [55] connects several prompt tokens to the original graph by cross-links. HetGPT [52] proposes the meta-path based structure prompting tokens. IGAP [47] further designs a recessive prompt learning scheme from the spectral space. Inspired by cluster-based graph pooling, SUPT [56] proposes clustering prompts and node-selection graph pooling prompting scheme, for better representation of graph structures."}, {"title": "4.1.3 Task Tokenization", "content": "During the pretraining phase, information from downstream tasks is not utilized. To rapidly adapt to the characteristics of downstream tasks and achieve quick fitting for specific scenarios, it is necessary to design task tokens. Task tokens are designed to suit the various downstream tasks. Let $P_{task} = \\{p_1,p_2,\u2026,p_{|P_{task}|} \\}$ denote task-level $|P_{task}|$ prompt tokens. Take the node classification task as an example, let $|Y|$ denote the number of node classes, we have $|P_{task} \\leq |Y|$. GPPT [53], SGL-PT [57], and GPF [58] adopt the task tokens to align the task gaps for better performance."}, {"title": "4.2 Task Alignment Strategies", "content": "Task alignment methodologies are proposed to reduce the inherent gap between the pre-trained and downstream tasks. Based on the task levels, they can be classified into node-level and graph-level alignment strategies."}, {"title": "4.2.1 Node Level Alignment", "content": "Node-level alignment aims to learn node-level and structural/context-level prompts for a node classification task. The alignment process can be summarized into two categories, i.e., cross-entropy-based and graph reconstruction-based. Cross-entropy-based ones are primarily designed to address the gap of node features for node classification tasks, while others are designed to remit the structural gap for link prediction tasks. The general process of these methods can be generally formulated as,\n$\\mathcal{L} = - \\sum \\log(y_i^T \\hat{x}_i)$. (8)\nGPPT [53], ULTRA-DP [59], SAP [55] and DeepGPT [60] are representative cross-entropy-based models. GraphPrompt [39] introduces contrastive learning. VNT [61] and GPF [58] employ learnable perturbations to the input graph to modulate the pre-trained node embeddings. HetGPT [52] employs heterogeneous feature prompts through an attention mechanism for node alignments.\nGraph reconstruction-based methods take the link prediction task as the objective, which is in line with the classical learning process of reconstruction loss. Positive nodes $\\{v^+\\}$ and negative nodes $\\{v^-\\}$ around a given center node $v_i$ are randomly sampled, then the triplets the objective is adopted to increase the similarity between graph instance $v_i$ and $v_i^+$, while decreasing that between $v_i$ and $v_i^-$,\n$\\mathcal{L} = - \\sum_{(v_i,v^+)} \\log \\frac{\\exp(z_p(v_i)^T z_p(v^+))}{\\sum_{v^u \\in \\mathcal{N}(v_i)} \\exp(z_p(v_i)^T z_p(v^u))}$. (9)\nwhere $\\bigoplus$ denotes the similarity measurement, $\\mathcal{N}(v_i)$ denotes the k-hop neighbors of $v_i$. PGCL [62], SGL-PT [57] are typical models. SGL-PT integrates class-wise prototypes based on Eq. (9) to represent essential features corresponding to labels, which encourages the similarity scores between examples of a class and its corresponding prototype larger than other pro-totypes. PGCL adds a fusion operation between the graph\u2019s semantic and contextual views. In addition, contrastive learning is also adopted to enhance the differentiation."}, {"title": "4.2.2 Graph Level Alignment", "content": "Graph-level alignment aims to learn graph-level prompts for a graph or subgraph-related classification task. In this context, the objectives of task head tuning and prompt tuning are aligned, and can be formulated as follows:\n$\\mathcal{L} = - \\sum \\log \\frac{\\exp(z^T \\hat{z_y})}{\\sum_{c \\in \\mathcal{Y}} \\exp(z^T \\hat{z_c})}$. (10)\nwhere $\\bigoplus$ denotes the similarity measurement, $\\hat{z_\\cdot}$ denotes the prompted graph latent representation. Based on this framework, All-in-One [40] proposes graph-level contrastive learning prompting. GPF [58] tunes the prompt token and the task head via maximizing the likelihood of ground truth labels."}, {"title": "4.3 Prompt Tuning Strategies", "content": "Prompt tuning methods are proposed to reduce the inherent gap between the pre-trained and downstream tasks. Prompt Tuning Methods can be generally categorized into pretext-based and task-oriented methods."}, {"title": "4.3.1 Tuning in Line with Pertaining Objectives", "content": "If the upstream and downstream tasks and data character-istics are similar, the same loss function used during the pretraining phase can be applied to further optimize the downstream tasks. Take the node classification task as an example, the tuning process can be formalized as,\n$\\mathcal{L}_{pretrain} = - log P \\big( \\mathcal{Y}_{pretrain} | \\mathcal{F} (\\mathcal{X}_{pretrain}, \\mathcal{E}_{pretrain}) \\big)$,\\\n$\\mathcal{L}_{tunning} = - log P \\big( \\mathcal{Y}_{down} | \\mathcal{F} (\\mathcal{X}_{down}, \\mathcal{E}_{down}, \\mathcal{P}_{node}, \\mathcal{P}_{structure}, \\mathcal{P}_{task})\\big)$, (11)\nwhere $\\mathcal{F}$ denotes the learnable graph mapping functions, and $\\mathcal{P}$ denotes the learnable prompt templates in tunning processes. They adopt the same $\\mathcal{F}$ in pertaining process for downstream tunning. GPPT [53] and VNT [61] utilize the same loss objective for both link prediction and node classification tasks. GraphPrompt [39] follows the same node and graph classification objective loss in its pertaining stage. HetGPT [52] and SAP [55] adopt the contrastive loss for nodes to optimize the prompt tokens since their pre-training procedure has the same contrastive task as well. ULTRA-DP [59] designs edge prediction and neighboring prediction loss corresponding with the pre-trained tasks. PRODIGY [63] conducts the neighbor matching constraint in the tunning stage, which is in line with its pre-trained tasks."}, {"title": "4.3.2 Task-oriented Tuning", "content": "Due to the significant differences between the objectives of downstream tasks and the unsupervised pretraining objectives, many models have proposed customized task-directed tuning methods. To better adaptation of various tasks, type-specific feature tokens, structure-aware graph tokens, and task-related tokens are designed. In general, these works always adopt the multi-task architecture to capture more complex features and substructures,\n$\\mathcal{L} = \\mathcal{L}_{node}(\\mathcal{X}, \\mathcal{P}_{node})$\n$\\mathcal{L}_{graph} (\\mathcal{X}, \\mathcal{P}_{structure}, \\mathcal{P}_{task})$, (12)\nwhere $\\mathcal{L}_{node}$ and $\\mathcal{L}_{graph}$ denotes the node-level and graph-level oriented objective functions. Specifically, GPF [64] focus on a graph classification task. Therefore, it requires us to optimize the prompt token and the task head via maximizing the likelihood of ground truth labels based on graph embeddings generated from a pre-trained model. HetGPT enhances feature tokens by enforcing them highly related to different node type, and a multi-view neighborhood aggregation scheme to capture the heterogeneous substructures. SGL-PT [57] proposes multi-task tuning objectives, to address the complex requirements of downstream tasks. All-in-One [40] generates an induced graph for each node, aligning the graph label with the target node label, and considers this graph an individual task."}, {"title": "4.4 Homogeneous Graph Prompt", "content": "In this section, we will present studies that integrate prompt learning into homogeneous graphs, characterized by having only one type of nodes and edges. As the basis of graph prompt learning, these works focus more on the differences between graphs and other data types, such as non-Euclidean characteristics. The efforts are dedicated to designing pre-training tasks and prompting architectures suitable for graph structures, and proposing relevant optimization methods."}, {"title": "4.4.1 Prompting the Input Graph", "content": "In the first series of works, prompts are inserted into the original graph data, generating a prompted graph which will be fed into the pre-trained GNN model.\nVNT [61] inserts a set of virtual nodes into the graph, which function as soft prompts tailored to a few-shot node classification (FSNC) task. The original feature of virtual nodes $P \\in \\mathbb{R}^{P\\times d}$ are learnable, and are fed into the pre-trained Graph Transformer (GT) alongside the real nodes in the graph to generate prompted node representations:\n$[E'\\|Z'] = GTL'[E^{l-1}\\|Z^{l-1}], l=1,...,L$, (13)\nwhere $\\mathcal{V}$ and $P$ respectively represent the number of real nodes and virtual nodes. $E^0$ is the output of the embedding layer of GT for real nodes, and GTL are GT layers. $\\mathcal{P}$ are finetuned together with a classifier with cross entropy classification objective. With few parameters to be optimized, these virtual prompted nodes effectively harness the rich knowledge contained in GT, and help push node representations from the same classes closer. To enhance the transferability between FSNC tasks and get more informative prompts, VNT introduces the GPPE module, which utilizes an attention mechanism to gather information from prompts associated with other tasks.\nIn addition to introducing virtual prompt nodes, All in One [40] further give consideration to the edge connections between nodes. For two virtual nodes, the edge between them are learned adaptively. Specifically, for two virtual nodes, the edge connecting them is learned adaptively. Moreover, the presence of an edge connecting a virtual node and a real node is determined based on their similarity:\n$w_{ik} = \\sigma(p_k^T . x)$, if $\\sigma(p_k^T x) > \\delta$, (14)\nwhere $\\delta$ is a threshold. Then, the feature of real node $v_i$ is updated with aggregation of virtual nodes:\n$X_i = X_i + \\sum_{i=1}^{P} w_{ik} p_k$. (15)\nPrompt tuning is accomplished through a meta-learning mechanism. A similar idea is proposed in PSP [55], which consistently utilize structural information throughout both pre-training and prompt adjustment with class prototype nodes and learnable edges."}, {"title": "4.4.2 Prompting the Model Output", "content": "While using prompts as graphs is useful for aligning with graph data", "53": "consolidates the pre-training and downstream node classification tasks into a unified framework of masked edge prediction. To accomplish this", "tokens": "task tokens and structure tokens. Each task token is a vector representing a node label. Meanwhile", "class": "n$f_{prompt}(V_i, y) = [T_{task}(y), T_{structure}(V_i)"}]}