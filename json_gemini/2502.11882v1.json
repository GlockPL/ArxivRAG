{"title": "Leveraging Dual Process Theory in Language Agent Framework\nfor Real-time Simultaneous Human-AI Collaboration", "authors": ["Shao Zhang", "Xihuai Wang", "Wenhao Zhang", "Chaoran Li", "Junru Song", "Tingyu Li", "Lin Qiu", "Xuezhi Cao", "Xunliang Cai", "Wen Yao", "Weinan Zhang", "Xinbing Wang", "Ying Wen"], "abstract": "Agents built on large language models (LLMs)\nhave excelled in turn-by-turn human-AI collab-\noration but struggle with simultaneous tasks\nrequiring real-time interaction. Latency is-\nsues and the challenge of inferring variable\nhuman strategies hinder their ability to make\nautonomous decisions without explicit instruc-\ntions. Through experiments with current in-\ndependent System 1 and System 2 methods,\nwe validate the necessity of using Dual Pro-\ncess Theory (DPT) in real-time tasks. We\npropose DPT-Agent, a novel language agent\nframework that integrates System 1 and Sys-\ntem 2 for efficient real-time simultaneous\nhuman-AI collaboration. DPT-Agent's Sys-\ntem 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and control-\nlable decision-making. DPT-Agent's System\n2 integrates Theory of Mind (ToM) and asyn-\nchronous reflection to infer human intentions\nand perform reasoning-based autonomous de-\ncisions. We demonstrate the effectiveness of\nDPT-Agent through further experiments with\nrule-based agents and human collaborators,\nshowing significant improvements over main-\nstream LLM-based frameworks. To the best\nof our knowledge, DPT-Agent is the first lan-\nguage agent framework that achieves success-\nful real-time simultaneous human-AI collabora-\ntion autonomously. Code of DPT-Agent can be\nfound in https://github.com/sjtu-marl/\nDPT-Agent.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolution-\nized generalization capabilities and interaction\nmethods, driving the application of human-AI col-\nlaboration in real-world tasks. LLM-based agents\nhave already been successfully applied to many\ncollaborative tasks with humans, such as writ-\ning (Wan et al., 2024) and coding (Prather et al.,\n2024), where humans and the agents interact turn-\nby-turn. However, many collaborative tasks in\nshared workspaces require entities involved in the\ncollaboration to cooperate simultaneously in the\nenvironment (Salikutluk et al., 2024; Dourish and\nBellotti, 1992). Unlike turn-by-turn collaborative\ntasks, the simultaneous collaboration tasks that are\ntime-sensitive require real-time responses to part-\nners and interaction with the environment (Shao\net al., 2024; Gong et al., 2024), as well as reasoning\nabout dynamically changing human partners' strate-\ngies and environments (Wang et al., 2024). Such\nsimultaneous human-AI collaboration tasks present\ntwo challenges for LLM-based agents: real-time\nresponsiveness and autonomous collaboration\nadapted to humans.\nThe real-time responsiveness issues faced by\nLLMs in inference time have been widely dis-\ncussed. Larger models with stronger reasoning\ncapabilities often suffer from significant latency\n(Zhou et al., 2024), making it difficult for them\nto respond quickly to dynamic changes in human\ninteractions and environments in highly real-time\nscenarios. The combination of fast and slow think-\ning using System 1 and System 2 based on Dual\nProcess Theory (DPT) (Kahneman, 2011; Evans\nand Stanovich, 2013) has already been applied to\naddress real-time issues via the combination of\nlarge and small models in language agent frame-\nworks (Liu et al., 2024b). However, this method\nstill cannot resolve the contradiction between la-\ntency and performance fundamentally, as it uses\nsmall models as System 1.\nThe agent frameworks designed for collaborat-\ning with humans also face challenges of insufficient\nautonomy and difficulty in adapting to human strat-\negy variability. Agents in the shared workspace\ntasks are regarded as independent collaborators\njoining the partnership (Dafoe et al., 2021). How-\never, most collaborative agent frameworks still re-\nquire human input to output actions or strategies\n(Liu et al., 2024b; Guan et al., 2023), failing to\ncollaborate with humans autonomously. Further-\nmore, humans in shared workspace tasks might per-\nceive and engage with agents like how they interact\nwith human partners for fostering collaboration\nlike inferring agents' intentions to adjust strategies\n(Zhang et al., 2024b), which further enhances the\nchallenge of simultaneous human-AI collaboration.\nResearchers also point out that LLMs are still lim-\nited in their ability to adapt to dynamic human strat-\negy changes (Zhang et al., 2024c), making it dif-\nficult to transition reasoning into decision-making\nfor effective adaptation (Riemer et al., 2024).\nTo address these challenges, we propose\nDPT-Agent, which leverages Dual Process Theory\n(DPT) to integrate FSM-based System 1 and LLM-\ndriven System 2, as shown in Figure 1. Based on the\nintuitive thinking and fast decision-making char-\nacteristics of System 1, we use a Finite-state Ma-\nchine (FSM) for low-level action decision-making\nand execution, while employing a code-as-policy\n(Liang et al., 2023) approach to enable System 2's\nslow thinking to guide and control fast decisions.\nFor slow thinking (System 2), we design a Theory\nof Mind (ToM) mechanism for actively inferring\nhuman intentions and reflecting on environmental\nfeedback based on how humans infer the partners\nand situations in shared workspace collaboration\n(Krych-Appelbaum et al., 2007). We also further\nimprove the performance of the reflection mecha-\nnism with an asynchronous design to achieve better\nefficiency in self-evolution.\nBuilding on the shared workspace task environ-\nment which is a hard version of Overcooked from\nZhang et al. (2024b), we further develop a real-time\nsimultaneous human-AI collaboration environment\nwith new layouts and conduct multiple experiments\nin single agent setup, with rule-based agent and real\nhumans. We aim to understand: 1) DPT-Agent's\ncapability in real-time tasks, 2) DPT-Agent's ca-\npability in collaboration, and 3) DPT-Agent's per-\nformance in collaboration with humans simultane-\nously.\nIn the experiments collaborating with rule-based\nagents, DPT-Agent outperforms strong language\nagent frameworks. When collaborating with real\nhumans, DPT-Agent also outperforms these base-\nlines in both subjective and objective results, show-\ning the significant improvement brought by asyn-\nchronous reflection and ToM module to infer hu-\nmans.\nIn summary, our contributions are as follows:\n\u2022 We experimentally analyze LLMs indepen-\ndently as System 1 and System 2 in real-time\ntasks, highlighting the challenge of the trade-\noff between performance and latency.\n\u2022 We propose DPT-Agent that integrates FSM-\nbased System 1 for fast and intuitive decision-\nmaking and LLM-driven System 2 for deliber-\nate and analytical reasoning, effectively bal-\nancing latency and performance.\n\u2022 We conduct extensive experiments with rule-\nbased agents and human participants, demon-\nstrating that DPT-Agent outperforms existing\nlanguage agent frameworks in real-time simul-\ntaneous human-AI collaboration.\nTo the best of our knowledge, DPT-Agent is the\nfirst agent framework that can achieve successful\nreal-time simultaneous human-AI collaboration au-\ntonomously in the hard version of Overcooked,\nwhich is one step closer to real-world application."}, {"title": "2 Related Works", "content": "Dual Process Theory (DPT). Dual Process The-\nory (DPT) (Evans and Stanovich, 2013) refers to\nhuman cognition operates through two distinct sys-\ntems: System 1, which is fast, automatic, and in-\ntuitive, and System 2, which is slower, deliberate,\nand analytical (Kahneman, 2011). DPT explains\nhow humans think during the perception-decision\nprocess. The ability to effectively integrate Sys-\ntem 1 and System 2 helps humans accomplish com-\nplex perception and decision-making tasks. Nu-\nmerous LLM-based reasoning frameworks also uti-\nlized DPT to facilitate human-related interactions\nlike dialogue (He et al., 2024) and mitigate latency\nissues via using a small model as System 1 (Liu\net al., 2024b). Many current agent frameworks\nuse System 2-based approaches to assist with plan-\nning and decision-making (Yu et al., 2024; Zhang\net al., 2024c), such as chain-of-thought (CoT) (Wei\net al., 2022), ReAct (Yao et al., 2022), and Reflex-\nion (Shinn et al., 2024). DPT-Agent is inspired by\nDPT, further alleviating latency issues in System 1\nand endowing the agent with greater autonomy and\nadaptability to humans in the design of System 2.\nSimultaneous Human-AI Collaboration. Most\ntasks related to LLMs in human-AI collaboration\nresearch pose lower demands on real-time respon-\nsiveness, such as task-oriented dialogue systems\n(Yi et al., 2024) and word-guessing (Ashktorab\net al., 2021), where players take actions turn-by-\nturn. However, collaborative tasks in the real world\nare often simultaneous, requiring real-time reason-\ning, which presents latency challenges for many\nLLM-based frameworks (Liang et al., 2023). An-\nother significant challenge of simultaneous collabo-\nrative tasks is adapting to humans, who are unfamil-\niar partners not encountered during training (Wang\net al., 2024; Li et al., 2023; Carroll et al., 2019;\nZhang et al., 2024a). Theory of Mind (ToM) (Rabi-\nnowitz et al., 2018; Baron-Cohen et al., 1985) has\nbeen introduced to enhance reasoning in human-\nAI collaborative scenarios (Wester et al., 2024).\nHowever, studies have pointed out that LLMs fail\nto achieve functional ToM (Riemer et al., 2024),\nwhere reasoning cannot be effectively implemented\nin decision-making processes. To adapt to humans,\nDPT-Agent integrates DPT and ToM to support the\nentire process from perception to reasoning and\ndecision-making, achieving functional ToM while\nensuring real-time performance."}, {"title": "3 Why We Need Dual Process Theory?", "content": "To understand the necessity of DPT in real-time\nsimultaneous human-AI collaboration, we first ex-\namine the real-time responsiveness and task com-\npletion capabilities of using large language models\n(LLMs) independently as System 1 and System 2\nagents.\nIn the Overcooked environment (Zhang et al.,\n2024b), we employ a single-agent setup, Counter\nCircuit (shown on the left in Figure 4), to compare\nthe performance of typical LLM-based System 1-\nonly agents using mainstream LLMs of varying\nsizes with that of an FSM-based agent. Addition-\nally, we include the DeepSeek-R1 series reasoning\nmodel (Guo et al., 2025) and OpenAI's o3-mini,\nwhich incorporate System 2 capabilities with long\nCoT as agents for this task.\nTo evaluate the performance of the System 1-\nonly agents in real-time task completion, we as-\nsess action output latency, task score, and score\nefficiency. Each model is evaluated over 20 runs,\nusing the same game introduction prompt (Ap-\npendix B), instruction prompt (Appendix C.1), and\noutput prompt (Appendix D.1).\nAs shown in Figure 2, with detailed data pro-\nvided in Appendix G, as independent System 1,\nmodels with fewer than 20B parameters excel in\nlatency but often have near-zero score efficiency,\nindicating fast responses but ineffective actions.\nSince missed orders lead to score deductions, some\nhigh-score-efficiency models with high latency still\nscore below zero. The models that can balance\ncapability in generating scoring actions with low\nlatency perform better. When the reasoning mod-\nels use long CoT as the System 2, despite their\nstronger reasoning capabilities, their performance\npresents even lower score efficiency and overall\nscores compared to many smaller models function-\ning as System 1. Additionally, all agents perform\nworse than the FSM agent.\nThese results show that LLM-based indepen-\ndent System 1 and System 2 agents struggle with\nlow-latency models lacking capability and high-\ncapability models suffering from excessive latency.\nThis phenomenon highlights the need for a frame-\nwork to integrate System 1 and System 2, balancing\ncapability and latency in real-time tasks."}, {"title": "4 DPT-Agent Framework", "content": "To enable real-time responsiveness and seam-\nless autonomous collaboration that aligns with\nhuman cognitive processes, we propose Dual\nProcess Theory Agent framework (DPT-Agent).\nDPT-Agent integrates both System 1, which facili-\ntates fast, intuitive decision-making, and System 2,\nwhich supports deliberate, analytical reasoning.\nFormulation. We model real-time simultaneous\nhuman-AI collaboration as a two-agent decentral-\nized Markov decision process (DEC-MDP) (Bern-\nstein et al., 2002). The framework is defined by the\ntuple (S, {A}, {Ah}, p, P, r) where S is the state\nspace, A\u00b2 and Ah denote the agent's and human's\naction spaces, $p : S \\rightarrow [0,1]$ is the initial state\ndistribution, $P : S \u00d7 A \u00d7 S \\rightarrow [0, 1]$ governs tran-\nsitions with $A = A^i \u00d7 A^h$ as the joint action space,\nand $r : S\u00d7A \\rightarrow R$ is the reward function. At\neach timestep t, the agent executes $a \u2208 A^i$ while\nthe human performs $a^h \u2208 A^h$ simultaneously, in-\nducing the joint action $a_t = (a_t^i, a_t^h)$ that drives\nstate transitions through $P(s_{t+1}|s_t, a_t)$. We further\ndevelop modular formulations for DPT-Agent in\nthe following sections."}, {"title": "4.1 System 2: Deliberate and Analytical\nReasoning", "content": "When facing complex situations, humans often rely\non System 2 to process large amounts of infor-\nmation to aid decision-making. Inspired by this\nprocess, we designed System 2 for DPT-Agent, in-\ntegrating environmental feedback for Theory of\nMind and self-evolution-based inference, which\naims to enable advanced reasoning and planning\nwhile dynamically adapting to human partners. We\nalso refine the reflection mechanism (Shinn et al.,\n2024) by using asynchronous reflection to facilitate\nefficient and flexible self-evolution of strategies."}, {"title": "4.1.1 Theory of Mind for Inferring Human", "content": "Equipped with the Theory of Mind (ToM) capa-\nbility, individuals can infer others' mental states\nas beliefs by analyzing their actions and commu-\nnication history, allowing them to understand and\nanticipate their behaviors (Premack and Woodruff,\n1978). In the context of ToM, belief refers to an in-\ndividual's perception of events, which subsequently\nshapes their actions (Baron-Cohen et al., 1985; Ra-\nbinowitz et al., 2018; Wen et al., 2019). We develop\na Theory of Mind module that enables the agent\nto construct a belief about the human, encompass-\ning aspects such as tendencies, conventions, and\nplans, based on observed human behaviors. The\nbelief output from the ToM module influences the\nstrategy by guiding both the strategy reflection in\nSystem 2 and the decision-making in System 1.\nTo formulate the ToM process, we denote\nthe history from time-step 0 to time-step t of\nthe game that the agent perceives as $\\tau_{0:t} =$\n$\\{(s_0, a_0^i, a_0^h, r_0), ..., (s_t, a_t^i, a_t^h, r_t)\\}$. The Theory\nof Mind module takes in the history $\\tau_{0:t}$, summa-\nrizes the history, infers the conventions and tenden-\ncies of the human, and explains how the agent's\npolicy can be adjusted to coordinate better with the\nhuman player. The Theory of Mind module outputs\nthe belief in natural language, as shown in Figure 3.\nThe n-th ToM process execution can be formal-\nized as $b^n = LLM(\\tau_{0:t_n}, b^{n-1})$, where $b^{n-1}$ is the\n1-th generated belief and $t_n$ is the time-step\nwhen the n-th belief inference is performed."}, {"title": "4.1.2 Asynchronous Reflection for\nSelf-evolution", "content": "The Asynchronous Reflection module enables the\nagent to improve its policy in such a long-horizon\ninteraction process for higher performance. We\ndesign the \"Behavior Guideline,\" where the agent\nmaintains and iteratively updates language guide-\nlines for the self-evolution of the current policy,\nbased on the generated belief about the human part-\nner and the game history. The Asynchronous Re-\nflection module proceeds asynchronously with the\ndecision-making process and allows real-time re-\nsponsiveness to be handled by System 1, enabling\nthe reflection process to focus on thinking with-\nout worrying about decision delays, thus facilitat-\ning more thorough self-evolution. The m-th Re-\nflection process execution can be formalized as\n$g^m = LLM(\\tau_{0:t_m}, b^n, g^{m-1})$, where $b^n$ is the lat-\nest inferred belief about human, $g^{m}$ is the \"Behav-\nior Guideline\" that is updated m times.\nGiven the modular formulation of the ToM and\nAsynchronous Reflection modules, we derive the\nformulation of the whole System 2 process as a\npolicy $\\pi^{S2} : T\u00d7 B \u00d7 G \\rightarrow B \u00d7 G$, where $T =$\n$\\{\\tau_{0:t} = (s_0, a_0,...) | s_t \u2208 S, a_t \u2208 A, t = 0, . . . \\}$ is the space of the game history. The System 2\npolicy $\\pi^{S2}$ iteratively updates the belief about\nthe human player and the behavior guidelines\ngiven the game history, which can be denoted as\n$b^n, g^m = LLM(\\tau_{0,max(t_n,t_m)}, b^{n-1}, g^{m-1})$."}, {"title": "4.2 System 1: Fast and Intuitive Decision\nMaking", "content": "In time-sensitive tasks, humans typically rely on\nSystem 1 to make intuitive decisions without engag-\ning in complex reasoning and keep asynchronous\nreasoning while taking action. Inspired by this\nprocess, we implement System 1 in DPT-Agent by\ncombining a code-as-policy generator and Finite-\nstate Machine (FSM) to enable intuitive and rapid\ndecision-making. The code-as-policy approach\nalso establishes a decision pipeline from System\n2 to System 1, which allows System 2 to influence\nand refine actions."}, {"title": "4.2.1 Code-as-Policy Generator", "content": "To enhance the performance of the agent, we de-\nsigned the code-as-policy generator to effectively\nbridge the gap between System 2's guidelines and\ninferred beliefs, and System I's rapid decision-\nmaking. By incorporating System 2's reasoning\ninto the decision pipeline, we ensure that the agent\ncan leverage System 2's reasoning abilities to gradu-\nally transform System 2's inferences into actionable\ndecisions within an episode.\nThe Code-as-policy generator takes in the his-\ntory, guidelines and inferred beliefs, and outputs ex-\necutable code that consists of task-completing rules\nand modifies the logic of the Finite-state Machine,\nwhich is detailed in section 4.2.2. This process al-\nlows System 1 to refine its intuitive responses with\nthoughts derived from System 2, thus enhancing\nthe agent's overall decision-making capabilities in\ndynamic environments.\nThe policy generation process of Code-as-policy\ngenerator at time-step t can be formalized as $c_t =$\n$LLM (T_{t\u2212x:t}, b_n, g_m)$, where $b_n$ and $g_m$ represents\nthe latest belief about human and the latest guide-\nlines respectively, and A is the interval the Code-as-\npolicy generator executes."}, {"title": "4.2.2 Finite-state Machine & Action Executor", "content": "To implement rapid response in system 1, we adopt\nthe Finite-state Machine (FSM) method (Russell\nand Norvig, 2016), which is a widely used com-\nputational model that enables structured and ef-\nficient decision-making by transitioning between\npre-defined states based on inputs. In DPT-Agent,\nwe leverage FSM to facilitate fast and intuitive\ndecision-making by defining each state as a spe-\ncific agent context or situation. State transitions\nare triggered by environment dynamics, allowing\nthe agent to adapt efficiently without relying on\nexternal LLM responses.\nWhen LLM generates code-as-policy, the ex-\necutable code changes the pre-defined logics of\nFSM and thus facilitates the adaption to human\nand performance improvement. The FSM takes in\nthe code-as-policy and game states, and outputs\nmacro actions, denoted as $m_a$, which are high-\nlevel combinations of atomic actions for specific\ntargets. For example, in Overcooked, macro ac-\ntions include food ingredients preparation, food\nassembling and food serving. The generated macro\nactions are sent to an action executor for conversion\ninto atomic actions that can be directly executed\nin the environment. The action executor employs\nscript policies, ensuring smooth and efficient exe-\ncution. Upon receiving a macro action, the action\nexecutor selects an appropriate execution plan and\nperforms path planning to determine the necessary\natomic actions using the $A^*$ algorithm (Hart et al.,\n1968). The Detailed design and implementation\nof the FSM is provided in Appendix A.1. These\nprocesses can be formalized as $m_{at} = FSM(c_t, s_t)$\nand $a_t = Executor (m_{at})$\nGiven the formulation of these modules, we de-\nrive the formulation of the whole System 1 pro-\ncess as $\\pi^{S1} : S \u00d7 B \u00d7 G \\rightarrow A$. At time-step\nt, $\\pi^{S1}$ generates executable atomic action $a_t =$\nExecutor(FSM(LLM($\\tau_{t\u2212x:t}, b_n, g_m$), $s_t$))."}, {"title": "5 Experimental Design", "content": "In this section, we introduce the new real-time si-\nmultaneous human-AI collaboration environment\nand tasks we designed based on Zhang et al.\n(2024b) and our experimental setup. Specifically,\nwe aim to understand: 1) DPT-Agent's capability\nin real-time tasks, 2) DPT-Agent's capability in col-\nlaboration, and 3) DPT-Agent's performance when\ncollaborating with humans simultaneously."}, {"title": "5.1 Overcooked Challenge for Real-time\nSimultaneous Human-AI Collaboration", "content": "To effectively evaluate the performance of\nDPT-Agent in real-time simultaneous human-AI\ncollaboration, we implement the real-time shared\nworkspace environment proposed by Zhang et al.\n(2024b), using a challenging version of Over-\ncooked based on the original Overcooked game\n(Carroll et al., 2019; Strouse et al., 2021; Li et al.,\n2023, 2024; Yu et al., 2023; Wu et al., 2021). In our\nexperiments, we introduce a new layout. As shown\nin Figure 4, we adopt the basic layout, referred to\nas New Counter Circuit, from Zhang et al. (2024b)\nand design a new layout, named New Asymmetric\nAdvantages, building on the original Overcooked\nAI environment (Carroll et al., 2019). The imple-\nmentation is based on the gym-cooking environ-\nment (Wu et al., 2021). In the real-time settings,\neach timestep corresponds to 0.25 seconds in the\nreal world. Time-sensitive elements within the en-\nvironment, such as overcooked beef and expiring\norders, underscore the importance of timely task\nexecution. Additionally, layout conflicts and the\ncomplexity of the burger-making process empha-\nsize the critical role of collaboration. Further de-\ntails about the environment and tasks can be found\nin Appendix A."}, {"title": "5.2 Experimental Setup", "content": "Based on the Overcooked challenge, we set up\nthree series of experiments to validate the effective-\nness of DPT-Agent using the commonly adopted\nReAct (Yao et al., 2022) and Reflexion (Shinn et al.,\n2024) framework. We first compare DPT-Agent\nwith baselines in a single agent setting to under-\nstand the DPT-Agent's capability of a real-time\ntask. Next, we use three specialized rule-based\nagents as partners to evaluate the simultaneous col-\nlaboration capability of DPT-Agent. Finally, we\nconduct human-involved experiments to compare\nbaseline frameworks with DPT-Agent in collabora-\ntion with real humans. The baseline frameworks\nin experiments are implemented in a manner that\nensures a fair comparison via using the same out-\nput way of code-as-policy with DPT-Agent. Based\non this implementation, the ReAct and Reflexion\nbecome System 1 + System 2 frameworks. The im-\nplementation details can be found in Appendices B\nto D. All the open-source models used in experi-\nments are deployed locally with NVIDIA A800-\nSXM4-80GB and NVIDIA H100-80GB-HBM3 for\nthe best latency performance. Model deployment\ndetails can be found in Appendices G and H. For\nclose-source models, we use the original API. All\nthe models' temperature is set to 0. The whole\nexperiment cost 517.5 A800 GPU hours, 228 H100\nGPU hours and $735 in API in total. For reliabil-\nity, all the experiments are repeated 20 runs and\nreported as the inter-quartile mean and the standard\nerror. The details of the metrics used in experi-\nments can be found in Appendix E.\nCapability in Real-time Task. We first consider\nthe real-time performance and task completion ca-"}, {"title": "6 Results", "content": "In this section, we present the results of experi-\nments and analyze DPT-Agent's effectiveness in\nreal-time simultaneous human-AI collaboration."}, {"title": "6.1 Capability in Real-time Task", "content": "As shown in Figures 5(a) and 5(b) (detailed data\nin Appendix G), under ReAct and Reflexion frame-\nwork, the score efficiency of most models has sig-\nnificantly improved compared with when they func-\ntioned as independent System 1 (Figure 2). How-\never, the score of many models has declined with\nan increase in latency due to more complex System\n2 reasoning. Low-latency models, like Qwen2.5-\n14b, still struggle with capability issues, failing to\nachieve higher final scores despite good score ef-\nficiency. Further comparison of the performance\nof DPT-Agent in Figure 5(c) reveals that inference\nmodels with high latency and larger models get\na significant improvement. DPT-Agent can help\nthese high latency models convert the high score ef-\nficiency and reasoning capability to scores, which\ndemonstrates the effectiveness of DPT-Agent in\nreal-time tasks."}, {"title": "6.2 Capability in Simultaneous Collaboration", "content": "As shown in Table 1, DPT-Agent achieved the best\nperformance across the majority of models, espe-\ncially on the widely recognized general-purpose\nSOTA models like GPT-40. This phenomenon\naligns with the conclusions from the experiments\nin single-agent settings, where larger models can\novercome the latency limitations and achieve better\nperformance with the help of DPT-Agent. Addi-\ntionally, when facing rule-based agents that can\nonly perform a single task, DPT-Agent can main-\ntain a high contribution rate. For some models like\nLlama3.3-70b, DPT-Agent w/o ToM outperforms\nthe complete DPT-Agent, which may be closely\nrelated to the model's ToM capabilities. We pro-"}, {"title": "6.3 Experiments with Real Humans", "content": "After data validation, we have 68 valid data points\nin total: 36 of Map 1 and 32 of Map 2. The data\nvalidation details are in Appendix I. As shown in\nTable 2, DPT-Agent achieves the highest scores in\nboth Map 1 and Map 2 when collaborating with\nhumans. DPT-Agent w/o ToM also outperforms\nReAct and Reflexion, confirming the effectiveness\nof asynchronous reflection. Moreover, the ToM\nmodule also brought a significant score improve-\nment in collaborating with humans, confirming that\nincorporating human belief reasoning into System\n2 can foster better collaboration. Regarding hu-\nman perception (Table 3), DPT-Agent ranks highest\nin Map 1, with the most participants recognizing\nits collaborative abilities. Interestingly, in Map 2,\nDPT-Agent w/o ToM surpasses DPT-Agent in both\ncooperation and preference ranking with a higher\nagent contribution rate, which may refer to the hu-\nman preference for partners who work more."}, {"title": "7 Discussion and Future Works", "content": "The experiment results illustrate the complex inter-\nplay between latency, capability, and collaboration\nin real-time tasks. DPT-Agent shows the capability\nto address this issue by effectively balancing la-\ntency and capability, enabling high-latency models\nto convert score efficiency and reasoning capability\ninto better outcomes. Moreover, the significant im-\nprovement observed when incorporating ToM into\nDPT-Agent during human collaboration confirms\nthe value of human-like reasoning in enhancing\ntask performance. This insight emphasizes the im-\nportance of integrating cognitive abilities, like ToM,\nto optimize human-agent interactions in real-world\napplications. Interestingly, the absence of ToM in\nDPT-Agent outperformed the complete DPT-Agent\nin some cases, suggesting the models' lack of ToM\ncapabilities, which might influence the effective-\nness of DPT-Agent. For future work, the integra-\ntion approach of DPT-Agent with FSM holds great\npotential for integrating LLMs into existing FSMs\nin the real world, offering the possibility of sup-\nporting more simultaneous human-AI collaboration\nscenarios to achieve stronger capabilities and pro-\nmote better cooperation."}, {"title": "8 Conclusion", "content": "In this paper, we propose DPT-Agent, a language\nagent framework for the challenges of real-time re-\nsponsiveness and autonomous adaption to humans\nin real-time simultaneous human-AI collaboration\ntasks. Inspired by DPT, DPT-Agent combines\nFSM-based System 1 for rapid decision-making\nwith a System 2 driven by LLMs for deeper reason-\ning. The single-agent experiments and experiments\nwith rule-based agents highlight that DPT-Agent\nhas the capability in real-time tasks and simulta-\nneous collaboration. Moreover, the performance\nof DPT-Agent in human experiments marks a sig-\nnificant advancement, offering a more autonomous\nand adaptive framework in simultaneous human-\nAl collaboration. We open-source both the method\nand the environment to foster future research and\nadvancements in simultaneous human-AI collabo-\nration. To the best of our knowledge, DPT-Agent\nis the first agent framework to achieve autonomous\nand simultaneous collaboration with humans in real\ntime, making it a major step forward in language\nagents for human-AI collaboration."}, {"title": "Limitations", "content": "DPT-Agent has already made breakthrough\nprogress in the task of simultaneous Human-AI\ncollaboration, providing a solid foundation for de-\nsigning more complex agent frameworks in the\nfuture. However, DPT-Agent still has significant\nroom for improvement. First, providing guidance\nand code-as-policy to DPT-Agent FSM-driven Sys-\ntem 1 remains a major challenge for many models\nwith weaker capabilities, especially small models.\nMany models are still limited by errors in their\noutput, which cannot be verified and thus lead to\ninvalid policies. Secondly, using lambda functions\nto control the FSM still has a certain lack of flex-\nibility. However, given the current limitations of\nmodel capabilities"}]}