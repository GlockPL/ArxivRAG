{"title": "Adaptive Data Exploitation in Deep Reinforcement Learning", "authors": ["Mingqi Yuan", "Bo Li", "Xin Jin", "Wenjun Zeng"], "abstract": "We introduce ADEPT: Adaptive Data Exploitation, a simple yet powerful framework to enhance the data efficiency and generalization in deep reinforcement learning (RL). Specifically, ADEPT adaptively manages the use of sampled data across different learning stages via multi-armed bandit (MAB) algorithms, optimizing data utilization while mitigating overfitting. Moreover, ADEPT can significantly reduce the computational overhead and accelerate a wide range of RL algorithms. We test ADEPT on benchmarks including Procgen, MiniGrid, and PyBullet. Extensive simulation demonstrates that ADEPT can achieve superior performance with remarkable computational efficiency, offering a practical solution to data-efficient RL. Our code is available at https://github.com/yuanmingqi/ADEPT.", "sections": [{"title": "1. Introduction", "content": "Deep reinforcement learning (RL) has achieved remarkable success in diverse domains such as complex games, algorithm innovation , large language model (LLM) , and chip design . However, promoting data efficiency and generalization remains a long-standing challenge in deep RL, especially when learning from complex environments with high-dimensional observations (e.g., images). The agents often require millions of interactions with their environment, resulting in substantial computational overhead and restricting applicability in scenarios where data collection is costly or impractical. Moreover, the learned policy frequently struggles to adapt to dynamic environments in which minor variations in tasks or conditions may significantly degrade its performance.\nTo tackle the data efficiency problem, research has primarily focused on two key aspects: data acquisition and data exploitation. Data acquisition aims to maximize the quality and diversity of the data collected during interactions with the environment, while data exploitation seeks to optimize the utility of this data to enhance learning efficiency. Prominent techniques include data augmentation , efficient experience replay, distributed training , and environment acceleration . For example, proposed RND that utilizes the prediction error against a fixed network as intrinsic rewards, enabling structured and efficient exploration. In contrast, RAD applies simple transformations to input observations during training, improving both data efficiency and generalization. On the exploitation side, developed prioritized experience replay (PER), which enhances learning efficiency by sampling high-priority experiences more frequently during training. However, these methods struggle to balance data efficiency and computational efficiency."}, {"title": "Adaptive Data Exploitation in Deep Reinforcement Learning", "content": "Techniques like data augmentation often introduce auxiliary models, storage, and optimization objectives, which significantly increase the computational overhead . Moreover, they often lack guarantees for optimizing long-term returns. For instance, intrinsic reward approaches suffer from the policy-invariant problem, where the exploration incentivized by intrinsic rewards may diverge from the optimal policy.\nBeyond data efficiency, generalization represents another critical challenge in deep RL, as agents often overfit to their training environments . To address this issue, proposed a phasic policy gradient (PPG), which explores decoupling the representation learning of policy and value networks. PPG also incorporates an auxiliary learning phase to distill the value function and constrain the policy, thereby enhancing generalization. Similarly, proposed decoupled advantage actor-critic (DAAC), which also leverages decoupled policy and value networks but eliminates the need for auxiliary learning. Both PPG and DAAC can achieve superior generalization performance on the Procgen benchmark. However, they rely on sophisticated hyperparameter tuning (e.g., the number of update epochs of the two separated networks), which limits their adaptability across diverse scenarios. Moreover, they lack mechanisms to dynamically adjust to different learning stages, further constraining their applicability.\nInspired by the discussions above, we aim to enhance the data efficiency and generalization of RL agents while minimizing computational overhead. To that end, we propose a novel framework entitled ADEPT: Adaptive Data Exploitation, which incorporates three scheduling algorithms to assist RL algorithms. Our main contributions are summarized as follows:\n\u2022 We propose to adaptively control the utilization of the sampled data across different tasks and learning stages. This scheduling process is formulated as a multi-armed bandit (MAB) problem, in which a set of extent values represents the arms. ADEPT can automatically select the optimal extent value based on the estimated task return, significantly maximizing the data efficiency while reducing the computational costs;\n\u2022 By adaptively adjusting data utilization, ADEPT can effectively prevent the RL agent from overfitting and enhance its generalization ability. In particular, ADEPT has a simple architecture and requires no additional learning processes, which can facilitate a wide range of RL algorithms;\n\u2022 Finally, we evaluate ADEPT on Procgen (sixteen procedurally-generated environments), MiniGrid (envi-"}, {"title": "Adaptive Data Exploitation in Deep Reinforcement Learning", "content": "ronments with sparse rewards), and PyBullet (robotics environments with continuous action space). Extensive simulation results demonstrate that ADEPT can achieve superior performance with remarkable computational efficiency."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Data Efficiency in RL", "content": "Observation augmentation and intrinsic rewards have emerged as promising approaches to promoting data efficiency in RL. proposed data-regularized Q (DrQ) that leverages standard image transformations to perturb input observations and regularize the learned value function. DrQ enables robust learning directly from pixels without using auxiliary losses or pre-training, which can be combined with various model-free RL algorithms. further extended DrQ and proposed DrQ-v2, which is the first model-free RL algorithm that solves complex humanoid locomotion tasks directly from pixel observations. In contrast, proposed RISE that maximizes the R\u00e9nyi entropy of the state visitation distribution and transforms the estimated sample mean into particle-based intrinsic rewards. RISE can achieve significant exploration diversity without using any auxiliary models.\nIn this paper, we improve the data efficiency from the perspective of exploitation. Our method provides a systematic guarantee for optimizing long-term returns and significantly reduces computational costs."}, {"title": "2.2. Generalization in RL", "content": "Achieving robust generalization is a fundamental challenge in RL. To that end, extensive research has focused on techniques such as regularization, data augmentation, representation learning, representation decoupling, causal modeling , exploration , and gradient strategies . For instance, proposed delayed-critic policy gradient (DCPG), which mitigates overfitting and enhances observational generalization by optimizing the value network less frequently but with larger datasets. In contrast, proposes a conflict-aware gradient agreement augmentation (CG2A) framework that combines data augmentation techniques with gradient harmonization. CG2A improves generalization and sample efficiency by resolving gradient conflicts and managing high-variance gradients."}, {"title": "Adaptive Data Exploitation in Deep Reinforcement Learning", "content": "align with the dynamic nature of learning, particularly in procedurally-generated environments. (ii) A dynamic NUE value can reduce reliance on specific data and preserve the agent's plasticity throughout the training process, preventing overfitting and thereby improving generalization. (iii) Among the three key phases in Figure 2(a), the model update phase incurs the highest computational overhead. By minimizing unnecessary updates through adaptive NUE tuning, we can significantly reduce the overall computational overhead. This approach allows us to concentrate computational resources on the most impactful updates, leading to a more efficient training process.\nDenote by $K = \\{K_1, K_2, . . ., K_n\\}$ the set of NUE values, the value selection at different learning stages can be formulated as a MAB problem. Each value is considered an arm, and the objective is to maximize the long-term return evaluated by the task reward function. In the following, we introduce three specific algorithms to solve the defined MAB problem."}, {"title": "4.1. Upper Confidence Bound", "content": "We first leverage the upper confidence bound (UCB)  to solve the defined MAB problem, which is a representative and effective method. Specifically, UCB selects actions by the following policy:\n$K_t = \\underset{K \\in K}{\\operatorname{argmax}} Q_t(K) + c \\sqrt{\\frac{\\log t}{N_t(K)}}$  (2)\nwhere $K_t$ is the NUE value selected at time step t, $N_t(K)$ is the number of times that K has been chose before time step t, and c is the exploration coefficient. Before the t-th update, we select a K using Eq. (2), which will used for the policy updates. Then, the counter is updated by $N_t(K) = N_t(K)+1$. Next, we collect rollouts with the new policy and update the Q-function using a sliding window average of the past mean returns obtained by the agent after being updated using K:\n$Q_t(K) = \\frac{1}{W} \\sum_{i=1}^{W} \\varphi\\[i]$  (3)\nwhere $V\\phi$ is the average estimated task return of the episode.\nUCB encourages the exploration of less-used K values while progressively focusing on the most promising options. We refer to this algorithm as ADEPT(U)."}, {"title": "4.2. Gaussian Thompson Sampling", "content": "Furthermore, we introduce Gaussian Thompson sampling (GTS) to solve the same MAB problem by modeling the return distribution of each K as a Gaussian"}, {"title": "Algorithm 1 Adaptive Data Exploitation (UCB)", "content": "1: Initialize the policy network $\\pi_{\\theta}$ and value network $V_{\\phi}$;\n2: Initialize a set K of NUE values, an exploration coefficient c, a window length W for estimating the Q-functions;\n3: $\\forall K \\in K$, let\n4: $\\qquad N(K) = 1$, $Q(K) = 0$, $R(K) = FIFO(W)$;\n5: for each episode e do\n6: $\\qquad$ Sample rollouts using the policy network $\\pi_{\\theta}$;\n7: $\\qquad$ Perform the generalized advantage estimation (GAE) to get the estimated returns;\n8: $\\qquad$ Select $K_e$ using Eq. (2);\n9: $\\qquad$ Update policy network and value network;\n10: $\\qquad$ Compute the mean return $V_{\\phi}$ obtained by the new policy;\n11: $\\qquad$ Add $V_{\\phi}$ to the queue R($K_e$) using the first-in-first-out rule;\n12: $\\qquad Q(K_e) \\leftarrow \\frac{1}{|R(K_e)|} \\sum_{V_{\\phi} \\in R(K_e)} V_{\\phi}$;\n13: $\\qquad N(K_e) \\leftarrow N(K_e) + 1$;\n14: end for\ndistribution. At each time step, GTS samples from the distributions corresponding to all NUE candidates and selects the one with the highest sampled value:\n$K_t = \\underset{K \\in K}{\\operatorname{argmax}} N \\left(\\mu_t(K), \\sigma_t^2(K)\\right),$  (4)\nwhere $\\mu_t(K)$ and $\\sigma_t^2(K)$ are the mean and variance of the reward distribution for K. Then the parameters are updated incrementally as follows:\n$\\mu_{t+1}(K) = \\mu_t(K) + \\eta \\cdot \\frac{Q_t(K) - \\mu_t(K)}{N_t(K) + 1}$,\n$\\sigma_{t+1}^2(K) = \\frac{N_t(K) \\sigma_t^2(K) + (Q_t(K) - \\mu_t(K))^2}{N_t(K) + 1}$,  (5)\nwhere $\\eta$ is a step size.\nGTS allows for a more flexible exploration pattern that adapts dynamically to new information compared to the fixed confidence bound strategy in UCB. We refer to this algorithm as ADEPT(G)."}, {"title": "4.3. Round-Robin Scheduling", "content": "Finally, we employ Round-Robin scheduling (RRS) to ensure that each candidate in K is selected in a cyclical order, giving equal opportunity to all options without bias. This strategy is widely used in various domains, such as network packet scheduling, load balancing in distributed systems, and time-sharing in resource management."}, {"title": "Adaptive Data Exploitation in Deep Reinforcement Learning", "content": "The selection at time step t follows:\n$K_t = K \\left[\\left(t \\bmod |K|\\right) + 1\\right]$,  (6)\nwhere |K| is the cardinality of the set K. This algorithm is referred to as ADEPT(R)."}, {"title": "5. Experiments", "content": "In this section, we design the experiments to investigate the following questions:\n\u2022 Q1: Can ADEPT improve data efficiency as compared to using fixed NUE values? (See Figure 1, 3, 7, 8, 22, and 23)\n\u2022 Q2: Can ADEPT reduce the overall computational overhead? (See Figure 1, 3, 22, and 23)\n\u2022 Q3: Can ADEPT achieve higher generalization performance in procedurally-generated environments? (See Figure 4)\n\u2022 Q4: What are the detailed decision processes of ADEPT? (See Figure 5, 24, 25, 26, and 27)\n\u2022 Q5: How does ADEPT behave in sparse-rewards environments and continuous control tasks? (See Figure 7 and 8)"}, {"title": "5.1. Setup", "content": "We first evaluate the ADEPT using the Procgen benchmark , which contains sixteen procedurally-generated environments. We select Procgen for two reasons. First, Procgen is similar to the arcade learning environment (ALE) benchmark that requires the agent to learn motor control directly from images and presents a clear challenge to the agent's data efficiency. Moreover, Procgen provides procedurally generated levels to evaluate the agent's generalization ability with a well-designed protocol. All the environments use a discrete fifteen-dimensional action space and generate (64, 64, 3) RGB observations. We use the easy mode and train the agents on 200 levels before testing them on the full distribution of levels. Furthermore, we introduce the MiniGrid and PyBullet to test ADEPT in sparse-rewards environments and continuous control tasks.\nAlgorithmic Baselines. For the algorithm baselines, we select the proximal policy optimization (PPO) and data regularized actor-critic (DrAC) as the candidates. PPO is a representative algorithm that produces considerable performance on most existing RL benchmarks, while DrAC integrates"}, {"title": "Adaptive Data Exploitation in Deep Reinforcement Learning", "content": "data augmentation techniques into AC algorithms and significantly improves the agent's generalization ability in procedurally-generated environments. The details of the selected algorithmic baselines can be found in Appendix A.\nNUE Candidates.  reported the overall best hyperparameters for the two algorithms for the Procgen benchmark. Both PPO and DrAC update their parameters for 3 epochs after each episode, so we conduct experiments with K = {3,2,1} and K = {5,3,2,1}. For the first set, we aim to assess whether ADEPT can enhance RL algorithms while reducing computational overhead. In contrast, the second set allows us to explore whether a broader range of NUE values further improves performance. For MiniGrid and PyBullet experiments, please refer to Appendix B.\nEvaluation Metrics. We evaluate the data efficiency and generalization of each method using three metrics: (i) the average floating point operations (FLOPS) over 16 environments and all the runs, and the calculation process is depicted in Appendix G, (ii) the aggregated mean scores on 200 levels, and (iii) the aggregated mean, median, interquartile mean (IQM), and optimality gap (OG) on the full distribution of levels. Note that the score of each method on each environment is computed as the average episode returns over 100 episodes and 5 random seeds.\nMore details about the experimental setup and hyperparameters selection can be found in Appendix B."}, {"title": "5.2. Results Analysis", "content": "The following results analysis is performed based on the predefined research questions. We provide the detailed training curves of all the methods and configurations in Appendix C.\nData efficiency comparison. Figure 3 illustrates the data efficiency and computational overhead comparison between vanilla PPO, DrAC, and their combinations with three ADEPT algorithms in eight environments, with the full comparison provided in Appendix D. By alternating NUE values from K, PPO+ADEPT(R) achieves close or higher performance than the vanilla PPO agent, especially in the BigFish, Chaser, Dodgeball, and Plunder environments. The average computational overhead of PPO+ADEPT(R) is 70% of the vanilla PPO agent. Similarly, PPO+ADEPT(U) outperforms the vanilla PPO agent in 14 environments and achieves the highest performance in 6 environments. Meanwhile, it produces the minimum computational overhead in 11 environments. In contrast, PPO+ADEPT(G) also achieves the highest performance in 6 environments and obtains the highest computational efficiency in 4 environments. Therefore, PPO+ADEPT(U) can achieve more extreme overhead compression than PPO+ADEPT(G).\nFor the DrAC algorithm, DrAC+ADEPT(U) achieves the highest performance in 7 environments and obtains the highest computational efficiency in 6 environments. Specifically, DrAC+ADEPT(U) takes 69.1% of the overhead to achieve the same or higher score in multiple environments against the vanilla DrAC agent, such as BossFight, Chaser, and CoinRun. In contrast, DrAC+ADEPT(G) achieves the highest data efficiency in 5 environments and takes"}, {"title": "Adaptive Data Exploitation in Deep Reinforcement Learning", "content": "68.7% of the overhead of the vanilla DrAC agent. Finally, DrAC+ADEPT(R) also excels in 2 environments. The experiment results of PPO and DrAC demonstrate that ADEPT can significantly improve the data efficiency of RL algorithms and reduce the computational overhead.\nAnalysis of the decision process. Next, we analyze the detailed decision processes of ADEPT. Figure 5 illustrates"}, {"title": "Generalization performance on Procgen", "content": "Generalization performance on Procgen. We further evaluate the generalization performance of PPO, DrAC, and their combinations with ADEPT on the Procgen benchmark. Figure 4 illustrates four aggregated evaluation met-"}, {"title": "Ablation studies", "content": "Ablation studies. We also conducted a number of ablation experiments to study the importance of hyperparameters used in ADEPT, and the results are provided in Appendix F.1. It indicates that c = 5.0 and W = 10 are"}, {"title": "Data efficiency on MiniGrid", "content": "Data efficiency on MiniGrid. Additionally, we evaluate ADEPT on the MiniGrid benchmark with sparse-rewards and goal-oriented environments. Specifically, we conduct experiments using DoorKey-6\u00d76, LavaGapS7, and Empty-16\u00d716. Figure 8 illustrates the aggregated learning curves of the vanilla PPO agent, PPO+ADEPT(R), and PPO+ADEPT(U) using various sets of NUE candidates. It is obvious that ADEPT takes fewer environment steps to solve the tasks, highlighting its capability to accelerate RL algorithms in both dense and sparse-reward settings. More experimental details are provided in Appendix B."}, {"title": "Performance on continuous control tasks", "content": "Performance on continuous control tasks. Finally, we evaluate ADEPT on the PyBullet benchmark with continu-"}, {"title": "6. Discussion", "content": "In this paper, we investigated the problem of improving data efficiency and generalization in deep RL and proposed a novel framework entitled ADEPT. By adaptively managing the data utilization across different learning stages, ADEPT can optimize data efficiency and significantly reduce computational overhead. In addition, ADEPT substantially enhances generalization in procedurally-generated environments. We evaluate ADEPT on Procgen, MiniGrid, and PyBullet benchmarks. Extensive simulation results demonstrate that ADEPT can effectively enhance RL algorithms with simple architecture, providing a practical solution to data-efficiency RL.\nStill, there are currently remaining limitations to this work. Specifically, the decision-making process in ADEPT relies on the task return predicted by the value network, and inaccurate predictions will directly affect the scheduling quality. Furthermore, oscillatory scheduling may lead to underfitting in the value network, potentially degrading overall performance. Additionally, in the case of ADEPT(G), we assume the arm reward follows a normal distribution, which may not generalize well to all scenarios. Future work will focus on mitigating these issues by improving the robustness of value network predictions and exploring more generalized reward modeling techniques, further solidifying the applicability and reliability of ADEPT."}, {"title": "Impact Statement", "content": "This paper introduces the ADEPT framework, which aims to advance deep reinforcement learning (RL) by improving data efficiency, enhancing generalization, and minimizing computational overhead. While AI has achieved remarkable success across various domains, the training processes are resource-intensive, consuming large amounts of electricity annually. This substantial energy demand contributes to increased carbon emissions, further exacerbating climate change, and results in higher operational costs for institutions involved in AI research and development. ADEPT provides a practical and scalable solution to data-efficient and computation-efficient RL, with the potential to reduce energy consumption and carbon emissions. By addressing these issues, ADEPT contributes to energy conservation and supports global sustainability efforts, promoting environmental protection while advancing AI capabilities."}, {"title": "A. Algorithmic Baselines", "content": ""}, {"title": "A.1. PPO", "content": "Proximal policy optimization (PPO)  is an on-policy algorithm that is designed to improve the stability and sample efficiency of policy gradient methods, which uses a clipped surrogate objective function to avoid large policy updates.\nThe policy loss is defined as:\n$\\mathcal{L}_{\\pi}(\\theta) = -\\mathbb{E}_{\\tau \\sim \\pi}[\\min (p_t(\\theta) A_t, \\text{clip} (p_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t)]$,  (7)\nwhere\n$p_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$,  (8)\nand e is a clipping range coefficient.\nMeanwhile, the value network is trained to minimize the error between the predicted return and a target of discounted returns computed with generalized advantage estimation (GAE):\n$\\mathcal{L}_V(\\phi) = \\mathbb{E}_{\\tau \\sim \\pi} [(V_{\\phi}(s) - V_{\\phi}^{\\text{target}})^2]$.  (9)"}, {"title": "A.2. DrAC", "content": "Data-regularized actor-critic (DrAC)  is proposed to address the challenge of generalization in procedurally-generated environments by introducing data augmentation during training. Moreover, DrAC utilizes two regularization terms to constrain the agent's policy and value function to be invariant to various state transformations.\nThe policy network is trained to minimize two parts of losses:\n$\\mathcal{L}(\\theta) = \\mathcal{L}^{\\text{PPO}}_\\pi(\\theta) + G_{\\pi}(\\theta)$,  (10)\nwhere\n$G_{\\pi}(\\theta) = D_{\\text{KL}}(\\pi_{\\theta}(a|s) || \\pi_{\\theta}(a|f(s))$,  (11)\nand $D_{\\text{KL}}$ is the Kullback-Leibler divergence and f is a mapping that satisfies\n$V_{\\theta}(s) = V_{\\theta}(f(s)), \\pi_{\\theta}(a|s) = \\pi_{\\theta}(a|f(s))$.  (12)\nSimilarly, the value network is also trained using two parts of losses:\n$\\mathcal{L}_V = \\mathcal{L}_V^{\\text{PPO}}(\\phi) + [V_{\\phi}(s) - V_{\\phi}(f(s))]^2$.  (13)"}, {"title": "B. Experimental Setup", "content": ""}, {"title": "B.1. Procgen", "content": "PPO+ADEPT. In this part, we leverage the implementation of CleanRL  for the PPO algorithm. Table 1 illustrates the PPO hyperparameters, which remain fixed throughout all the experiments.\nSince the reported overall best K in  is 3, the candidates of NUE are set as K = {3,2,1} for all the experiments. For ADEPT(U), we ran a hyperparameter search over the exploration coefficient $c\\in \\{0.1, 1.0, 5.0\\}$ and the size of the sliding window used to compute the Q-values $W \\in [10, 50, 100]$. We found that c = 5.0, W = 10 are the best hyperparameters overall. Similarly, for ADEPT(G), we ran a hyperparameter search over the learning rate $\\eta \\in \\{0.1, 0.5, 1.0\\}$ and the size of the sliding window used to compute the Q-values $W \\in [10, 50, 100]$, and $\\alpha$ = 1.0, W = 50 are the overall best hyperparameters. These values are used to obtain the results reported in the paper."}, {"title": "DrAC+ADEPT", "content": "DrAC+ADEPT. In this part, we use the official implementation  of DrAC for the experiments, and Table 1 lists the shared and fixed hyperparameters. For each Procgen environment, Table 2 lists the best augmentation method of DrAC as reported in . The candidates of NUE are also set as K = {3,2,1} for all the experiments. For ADEPT(U) and ADEPT(G), we run the same hyperparameter search as the experiments of PPO+ADEPT"}, {"title": "B.2. MiniGrid", "content": "B.2. MiniGrid\nIn this part, we use the implementation of  for the PPO agent. Since the reported K = 4, we evaluate PPO+ADEPT(R) and PPO+ADEPT(U) using three NUE sets: {4,2,1}, {6, 4}, and {8, 4}. For ADEPT(U), the exploration coefficient c is set as 1.0, and the size of the sliding window is set as 50. Finally, Table 3 illustrates the PPO hyperparameters, which remain fixed throughout all the experiments."}, {"title": "B.3. PyBullet", "content": "Finally, we perform the experiments on the PyBullet benchmark using the PPO implementation of . Since  reported the best K = 10, we set the NUE candidates as K = {10,5, 1} and K = {10,5}. Then we test the PPO agent with three ADEPT algorithms. Moreover, we run a similar hyperparameter search as the Procgen experiments and report the best results of each method. Table 3 illustrates the PPO hyperparameters that remain fixed throughout all the experiments."}, {"title": "C. Learning Curves", "content": ""}, {"title": "C.1. PPO+ADEPT(R)+Procgen", "content": ""}, {"title": "C.2. PPO+ADEPT(U)+Procgen", "content": ""}, {"title": "C.3. PPO+ADEPT(G)+Procgen", "content": ""}, {"title": "C.4. DrAC+ADEPT(R)+Procgen", "content": ""}, {"title": "C.5. DrAC+ADEPT(U)+Procgen", "content": ""}, {"title": "C.6. DrAC+ADEPT(G)+Procgen", "content": ""}, {"title": "D. Data Efficiency Comparison", "content": ""}, {"title": "E. Detailed Decision Processes", "content": ""}, {"title": "E.1. PPO+ADEPT(U)+Procgen", "content": ""}, {"title": "E.2. PPO+ADEPT(G)+Procgen", "content": ""}, {"title": "E.3. DrAC+ADEPT(U)+Procgen", "content": ""}, {"title": "E.4. DrAC+ADEPT(G)+Procgen", "content": ""}, {"title": "F. Ablation Studies", "content": ""}, {"title": "F.1. Hyperparameter Search", "content": ""}, {"title": "F.1.1. PPO+ADEPT(U)+PROCGEN", "content": ""}, {"title": "F.1.2. PPO+ADEPT(G)+PROCGEN", "content": ""}, {"title": "F.1.3. DRAC+ADEPT(U)+PROCGEN", "content": ""}, {"title": "F.1.4. DRAC+ADEPT(G)+PROCGEN", "content": ""}, {"title": "F.2. Different NUE Sets", "content": ""}, {"title": "F.2.1. PPO+ADEPT(R)+PROCGEN", "content": ""}, {"title": "F.2.2. PPO+ADEPT(U)+PROCGEN", "content": ""}, {"title": "F.2.3. PPO+ADEPT(G)+PROCGEN", "content": ""}, {"title": "F.2.4. DRAC+ADEPT(R)+PROCGEN", "content": ""}, {"title": "F.2.5. DRAC+ADEPT(U)+PROCGEN", "content": ""}, {"title": "F.2.6. DRAC+ADEPT(G)+PROCGEN", "content": ""}, {"title": "G. Calculation of Computational Overhead", "content": "To compare the computational efficiency of the baseline algorithms and ADEPT, we utilize the floating point operations (FLOPS) as the KPI. Moreover, we only count the computational overhead of the network-involved operations, such as the data sampling and model update. In the Procgen experiments, all the methods use an identical architecture for the policy and the value network, which can be found in . We leverage an open-source tool entitled PyTorch-OpCounter  to calculate its FLOPS, and the result is denoted as $O_{bs1}$ for batch size=1.\nFor the sampling phase, the computational overhead is\n$O_{sampling} = (N_{episode length} + 1) * N_{environments} * O_{bs1}$  (14)\nwhere +1 is for predicting the returns of the next observations at the end of the episode, as shown in the PPO implementation of CleanRL .\nFor the model update phase, the computational overhead is\n$O_{update} = O_{forward} + O_{backward}$,  (15)\nwhere\n$O_{forward} = O_{bs1} * B * N_{batches} * N_{update epochs}$,  (16)\nand\n$O_{backward} = O_{forward} * 2$.  (17)\nHere, B is the batch size, and the overhead ratio of a forward pass to a backward pass is 1:2 as suggested by .\nFinally, the total computational overhead is\n$O_{total} = (O_{sampling} + O_{update}) * N_{episodes}$  (18)\nIn the Procgen experiments, we have\n$O_{bs1}$ = 528384FLOPS\n$N_{environments}$ = 64\n$N_{episode length}$ = 256\n$N_{episodes}$ = 1525\nB = 2048\n$N_{batches}$ = 32  (19)"}]}