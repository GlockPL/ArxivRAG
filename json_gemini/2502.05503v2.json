{"title": "A Physical Coherence Benchmark for Evaluating Video Generation Models via Optical Flow-guided Frame Prediction", "authors": ["Yongfan Chen", "Xiuwen Zhu", "Tianyu Li", "Hao Chen", "Chunhua Shen"], "abstract": "Recent advances in video generation models demonstrate their potential as world simulators, but they often struggle with videos deviating from physical laws, a key concern overlooked by most text-to-video benchmarks. We introduce a benchmark designed specifically to assess the Physical Coherence of generated videos, PhyCoBench. Our benchmark includes 120 prompts covering 7 categories of physical principles, capturing key physical laws observable in video content. We evaluated four state-of-the-art (SoTA) T2V models on PhyCoBench and conducted manual assessments. Additionally, we propose an automated evaluation model: PhyCoPredictor, a diffusion model that generates optical flow and video frames in a cascade manner. Through a consistency evaluation comparing automated and manual sorting, the experimental results show that PhyCoPredictor currently aligns most closely with human evaluation. Therefore, it can effectively evaluate the physical coherence of videos, providing insights for future model optimization. Our benchmark, including physical coherence prompts, the automatic evaluation tool PhyCoPredictor, and the generated video dataset, has been released on GitHub at https://github.com/Jeckinchen/PhyCoBench.", "sections": [{"title": "1. Introduction", "content": "With the rapid advancement of video generation models [6, 7, 13, 24, 42, 43], the quality of generated videos has continuously improved, demonstrating their potential to become world simulators. However, ensuring the physical coherence of video content remains challenging, which is one of the major concerns for current video generation models. Physical coherence refers to the extent to which the motion in a video follows physical laws observed in real-world scenarios. However, most video generation benchmarks [21, 22, 26] do not evaluate physical coherence, allowing visually appealing but physically implausible content to receive high scores. Recently, VIDEOPHY [3] attempted to address this issue using video-language models to answer \"Does this video follow the physical laws?\" The logits output can be used to assess the model's decision tendency, but this score is not equivalent to a metric that indicates how well a video conforms to physical laws.\nTo comprehensively evaluate whether generated videos adhere to physical laws, we introduce a novel video generation benchmark called PhyCoBench. PhyCoBench aims to encompass observable physical phenomena, including Newton's laws of motion, conservation principles (energy, momentum), collisions, rotational motion, static equilibrium, elasticity, vibration, and fluid dynamics. To capture these physical phenomena in specific human or object interactions, we construct a text prompt benchmark set across three categories: (1) simulated physical experiments, (2) common physical phenomena in everyday life, and (3) object movements in sports activities. The primary motion types examined in these test cases can be categorized into seven major groups: (1) gravity, (2) collision, (3) vibration, (4) friction, (5) fluid dynamics, (6) projectile motion, and (7) rotation. We present PhyCoBench, a benchmark specifically designed to evaluate the physical consistency of generated videos. Our benchmark covers seven categories of physical principles: gravity, collision, vibration, friction, rotation, projectile motion, and fluid dynamics,capturing the majority of physical laws that are readily observable in video content. A total of 120 prompts are provided, each exemplifying the corresponding physical principles across these categories.\nWe create a benchmark set of 120 prompts for these seven categories. The content of these prompts draws inspiration from various motion recognition datasets, which are expanded using large language models (LLMs) [45] and further refined by human experts. Based on these prompts, we use state-of-the-art video generation models to produce corresponding videos. Our goal is to evaluate these models' ability to generate physically consistent content.\nFurthermore, we propose an optical flow-guided video frame prediction model. In video generation tasks, the appearance and texture of objects can distract the model from focusing on physical laws, making it difficult for the generated video to follow these laws. To address this, we guide the generation process using optical flow, which contains only motion information. This allows the model to focus on object motion trajectories, thereby enhancing its adherence to physical laws.\nSimilar to video anomaly detection tasks, evaluating the physical coherence of generated videos requires detecting anomalies within the video. However, this task presents two main challenges: (1) anomalies are diverse and complex, making them difficult to define and quantify; (2) existing datasets lack negative samples, hindering the model's ability to learn prior knowledge of anomalies. To address these issues, we take inspiration from previous video anomaly detection approaches [25, 28, 30, 46, 47] and propose a frame prediction model called PhyCoPredictor to detect anomalies by predicting future frames. Specifically, our model takes an initial frame and a text prompt as inputs, utilizes a latent diffusion model to predict future optical flow, and uses the predicted flow to guide a text-conditioned video diffusion model for future frame prediction. After extensive training, our model can effectively predict the optical flow and visual content in dynamic scenes, allowing comparison with existing video generation models to evaluate physical coherence.\nOverall, our contributions are as follows:\n\u2022 We construct a comprehensive benchmark set of text prompts for physical scenarios, which covers a wide range of common motion scenes.\n\u2022 We propose an optical flow-guided model, named PhyCoPredictor, for video frame prediction that effectively predicts motion information in dynamic scenes.\n\u2022 Using the proposed prompts and model, we develop a benchmark, named PhyCoBench, for evaluating the physical coherence capabilities of video generation models.\n\u2022 Consistency evaluation shows that PhyCoPredictor can effectively assess T2V model's ability to generate videos that satisfy physical laws."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Reconstruction and Prediction-Based Video Anomaly Detection", "content": "The fundamental principle of reconstruction and prediction-based video anomaly detection methods is to to train models on a large number of normal videos, enabling them to reconstruct or predict normal frames. When an abnormal frame is present in the input video, it can be detected by comparing it with the model's output. For example, based on the previous frame, [25] predicts the next frame using FlowNet[10] and GANs[11]. [30] predicts both the next frame and the optical flow between consecutive frames. [47] generates high-quality future frames using GANs. [28] reconstructs optical flow using a memory module and employs a conditional VAE to predict future frames. [46] selects multiple keyframes from t frames to reconstruct the original sequence.\nIn this paper, we draw inspiration from the approaches mentioned above and use a frame prediction model to detect whether generated videos exhibit physical coherence. Instead, we employ a latent diffusion model-based approach, which makes our model more flexible and efficient."}, {"title": "2.2. Video Generation Model", "content": "The field of video generation has been developing rapidly [5-7, 12-14, 19, 20, 23, 24, 36, 39, 43, 44, 49, 51], with numerous studies on text-to-video[6, 7, 24, 42] and image-to-video generation [8, 13, 43]. VDM [20] is the first to introduce a diffusion-based approach for video generation, laying the foundation for subsequent developments in this direction. Subsequently, works like MagicVideo [51], Video LDM [5], and LVDM [15] introduce latent diffusion approaches that generate videos in latent space. SVD[4] standardize the pre-training process for video generation models in three steps: text-to-image pre-training, video pre-training on a large-scale low-resolution dataset, and fine-tuning on a small-scale high-quality high-resolution dataset. Sora [27] advance the diffusion Transformer models by replacing the U-Net architecture in the latent diffusion model (LDM) with a Transformer while directly compressing videos using a video encoder.\nThe rapid progress in video generation models has placed higher demands on video generation benchmarks. However, existing benchmarks still struggle to comprehensively evaluate model capabilities and the quality of generated videos."}, {"title": "2.3. Automatic Metrics for Video Generation", "content": "Current popular benchmarks primarily use the following metrics: IS [34], FID [17], FVD [38], SSIM [40], CLIP-Score [16], and their variants. EvalCrafter [26] includes 500 prompts, derived from both real user data and data generated with the assistance of large language models (LLMs), and uses 17 metrics to evaluate the capabilities of text-to-video generation models. Vbench [21] assesses video generation quality from 16 dimensions, covering 24 categories with a total of 1,746 prompts. T2VBench [22] contains 1,680 prompts to specifically evaluate text-to-video models in terms of temporal dynamics across 16 temporal dimensions. VIDEOPHY [3] is a recently proposed text-to-video benchmark, consisting of 9,300 generated videos that are manually labeled. It evaluates video reasonableness based on human judgment of whether the videos conform to physical commonsense.\nWhile these benchmarks evaluate video generation quality from multiple aspects, they still lack a simple and effective way to assess the physical coherence of video generation models. Therefore, we propose a new benchmark to fill this gap."}, {"title": "3. Physical Coherence Benchmark", "content": null}, {"title": "3.1. Prompts", "content": "To comprehensively evaluate the physical coherence of text-to-video generation models, we propose a benchmark set containing 120 prompts, categorized into seven groups: (1) gravity, (2) collision, (3) vibration, (4) friction, (5) fluid dynamics, (6) projectile motion, and (7) rotation. Some examples are shown in Table 1. We reference definitions"}, {"title": "3.2. Human Evaluation Results", "content": "We evaluated four text-to-video generation models (Keling1.5 [1], Gen-3 Alpha [33], Dream Machine [2], and OpenSora-STDiT-v3 [50]) by generating videos based on our 120 prompts. Some of the generated video results are shown in Figure 3. It is evident that the generated videos do not consistently adhere to physical consistency. For the same prompt, the quality of the videos varies significantly across the four models. This indicates that there is still a substantial gap between different models in terms of physical consistency. Therefore, there is an increased need for a more accurate and in-depth evaluation of model performance in this dimension."}, {"title": "4. Automatic Evaluator", "content": "Our optical flow-guided video frame prediction model, named PhyCoPredictor, comprises two Latent Diffusion Model (LDM) modules. First, after obtaining the video generated by a text-to-video model, we take the first frame of the input video along with the corresponding text prompt. The first LDM is used to predict the future optical flow from this initial frame. Then, the predicted future optical flow, combined with the initial frame and text prompt, serves as a guiding condition for the second LDM, which generates a multi-frame video starting from that frame. The model's training pipeline is shown in Figure 7. Our ultimate goal is to determine whether the generated video maintains physical coherence by detecting physical inconsistencies or anomalies. To achieve this, we use FlowFormer++ [35] to compute the optical flow of the generated video as a reference flow, and compare the reference flow, the generated video, the predicted flow, and the predicted video. If the discrepancies are significant, it indicates that the generated video contains anomalies and does not meet physical coherence."}, {"title": "4.1. Preliminary: Latent Diffusion Model", "content": "Our model belongs to the class of generative diffusion models [18]. Diffusion models define both a forward diffusion process and a reverse denoising process. The forward diffusion process gradually adds noise to the data $x_0 \\sim p(x)$, which resulting in Gaussian noise $x_T \\sim N(0, I)$, while the reverse denoising process restores the original data by progressively removing noise. The forward process $q(x_t | x_0, t)$ consists of T timesteps, during which at each timestep, $x_{t-1}$ is gradually noised to obtain $x_t$. The denoising process $p_\\theta(x_{t-1} | x_t, t)$ uses a denoising network $\\epsilon_\\theta(x_t, t)$ to predict a less noisy version $x_{t-1}$ from the noisy input $x_t$. The objective function of the denoising network is\n$\\min_\\theta E_{t,x \\sim p, \\epsilon \\sim N(0,1)} ||\\epsilon - \\epsilon_\\theta(x_t, t)||^2$,\nwhere $\\epsilon$ represents the true noise, and $\\theta$ is the set of learnable parameters of the network. After training, the model can employ the reverse denoising process to recover the original data $x_0$ from random Gaussian noise $x_T$.\nTo reduce computational complexity, the Latent Diffusion Model (LDM) [32] was proposed, which performs noise addition and denoising in the latent space. Many recent works on diffusion models are based on the LDM architecture, including our work. In this paper, our frame prediction model is built upon an open-source image-to-video LDM framework called DynamiCrafter [43]. For LDM, the input $x_0$ is encoded to obtain the latent variable $z_0 = E(x)$. The forward noise addition process $q(z_t | z_0, t)$ and the reverse denoising process $p_\\theta(z_{t-1} | z_t, t)$ are performed in the latent space, and the final output of the model is obtained by decoding with a decoder, $\\hat{x} = D(z)$."}, {"title": "4.2. PhyCoPredictor", "content": null}, {"title": "4.2.1. Latent Flow Diffusion Module", "content": "To enhance the performance of the frame prediction model, we drew inspiration from [31] by using optical flow as a condition to guide the frame prediction process. The optical flow mentioned here is the latent flow generated by LDM. We input the text prompt and the first frame $x \\in \\mathbb{R}^{1 \\times 3 \\times H \\times W}$ of the video into the model, and after encoding through the VAE encoder, we replicate it N times to obtain the latent variable $z_f \\in \\mathbb{R}^{N \\times 4 \\times h \\times w}$, which contains the visual information of the first frame. Next, we compute the optical flow for N frames using FlowFormer++ and downsample it to the latent space, yielding the flow $f \\in \\mathbb{R}^{N \\times 2 \\times h \\times w}$, where $f_0$ is the optical flow calculated between the first frame and itself, and $f_i (i \\neq 0)$ represents the flow calculated between the (i - 1)-th and i-th frames.\nTo align the dimensions of the latent variable and the optical flow, we designed a Latent Adapter consisting of an MLP layer and an activation layer, which downsamples the feature dimension of $z_f$ from 2 to 4. Next, the noised latent flow $\\tilde{f}$ is concatenated with $z_f$, and the result is fed into a 3D U-Net. After the denoising process, we obtain the generated optical flow $\\hat{f} \\in \\mathbb{R}^{N \\times 2 \\times h \\times w}$. Our loss function is\n$L_{flow} = ||f - \\hat{f}||^2$,"}, {"title": "4.2.2. Latent Video Diffusion Module", "content": "Our network architecture for predicting video frames in latent space is built upon DynamiCrafter [43]. To more accurately predict the motion trajectories of objects in physically dynamic scenes, we chose to use optical flow as a condition to guide the frame prediction process. During the training phase, we utilize FlowFormer++ [35] to obtain optical flow and employ a Flow Adapter to upsample the feature dimension from 2 to 4 to align with the latent space. Our Flow Adapter consists of a 3D convolution layer. We then add the upsampled flow to the latent variable $z_f$, resulting in $z_{con} \\in \\mathbb{R}^{N \\times 4 \\times h \\times w}$, which fuses both motion and visual information. The input consists of a text prompt and video $v \\in \\mathbb{R}^{N \\times 3 \\times H \\times W}$. The video v is encoded by a VAE encoder to produce $z_0 \\in \\mathbb{R}^{N \\times 4 \\times h \\times w}$. After adding noise to $z_0$, it is concatenated with $z_{con}$ and fed into a 3D U-Net. Similar to DynamiCrafter, we use a CLIP image encoder to encode the first frame of the video, convert it into visual conditions through an image context network, and control the generation result through cross-attention alongside the text conditions provided by the text prompt. During this process, our loss function is\n$L_{video} = ||v - \\hat{v}||^2$,\nwhere $\\hat{v}$ represents the video predicted by the model.\nDuring the inference phase, the optical flow is provided by the previously mentioned Latent Flow Diffusion Module (and needs to be upsampled from 32\u00d732 to 40\u00d764) to predict the motion trends of future video frames."}, {"title": "4.2.3. Training Setup", "content": "In our model, apart from the two 3D U-Nets and two adapters, all other components are frozen. The training process is divided into two stages: In the first stage, we train the Latent Adapter and the 3D U-Net in the Latent Flow Diffusion Module. We initially use LLM[45] to filter out relatively static data based on the captions from Openvid[29], and then train these two components from scratch using the remaining data. We sample 16 frames from the videos to ensure that the model focuses on dynamic content. The batch size is set to 4, and the training runs for a total of 100k steps on 64 L20 GPUs. Subsequently, we use Motion Data for an additional 30k steps of training, still with a batch size of 4 and on 64 L20 GPUs. The Motion Data is a dynamic scene dataset selected from UCF101[37], Physics101[41], Penn Action[48], and HAA500[9], annotated with captions using Multimodal Language Model(MLM). All training in the first stage is conducted at a resolution of 256x256, with the corresponding latent space dimensions being 32\u00d732.\nIn the second stage, we train the Flow Adapter and the 3D U-Net in the Latent Video Diffusion Module. The U-Net in this stage is initialized with pre-trained weights from the DynamiCrafter model at a resolution of 320\u00d7512, with the latent space dimensions being 40\u00d764. The training also uses Motion Data, with a batch size of 4 for a total of 30k steps."}, {"title": "4.3. Automatic Evaluation Process", "content": "We used FlowFormer++[35] to compute the optical flow of the generated videos and sampled them to N frames to obtain the original optical flow, while also sampling the generated videos to N frames to obtain the original video. Subsequently, we input the first frame of the original video and the corresponding prompt into the trained frame prediction model to obtain the predicted optical flow and predicted video. To predict optical flow and video frames, we propose an optical flow-guided frame prediction model called PhyCoPredictor. The inference process of PhyCoPredictor is illustrated in Figure 6.\nOur model is trained extensively on typical motion scenarios to learn motion and visual priors. As a result, it generates predictions that are more likely to exhibit physically consistent behavior when forecasting optical flow and future frames. In cases where the original video does not satisfy physical coherence, anomalies can be detected by comparing the original optical flow and video with the predicted optical flow and video. We evaluate the performance of the four models by calculating both optical flow loss and video loss. Finally, we evaluate the performance of each model using a scoring metric defined as:\n$\\text{score} = MSE(f, \\hat{f}) + 2 \\times MSE(v, \\hat{v})$,\nwhere $f$ and $\\hat{f}$ denote the original and predicted optical flow, respectively, while v and $\\hat{v}$ represent the original and predicted video frames, respectively. A higher score indicates better physical coherence of the generated video."}, {"title": "4.4. Evaluation Results", "content": null}, {"title": "4.4.1. Quantitative Evaluation", "content": "Based on our proposed text prompt benchmark set, we evaluated the text-to-video generation performance of four models: Keling1.5, Gen-3 Alpha, Dream Machine, and OpenSora-STDiT-v3. For each model, we generated a video using each prompt. We then manually ranked the performance of the four models for each prompt. Additionally, we used Dynamicrafter and our proposed optical flow-guided frame prediction model to score and rank the four models. We calculated Kendall's Tau-b coefficient and Spearman's Rank Correlation coefficient to compare the model ranking results with the manual evaluations. The results in Table 2 indicate that our model's rankings align more closely with human assessments, and that incorporating optical flow guidance provides a better evaluation of the physical coherence of generated videos."}, {"title": "4.4.2. Qualitative Evaluation", "content": "To further demonstrate the effectiveness of our model, we conducted a visual comparison of video predictions using DynamiCrafter and our optical flow-guided frame prediction model. The visualization results in Figure 8 show that, with the addition of optical flow guidance, PhyCoPredictor produces more natural and realistic motion trajectories in the predicted video frames. For example, in scenes involving falling objects, it can accurately predict the falling trajectory and even the rebound trajectory after the fall. In contrast, DynamiCrafter struggles to effectively predict the motion trajectories of objects. In the scene depicting leaves falling naturally, PhyCoPredictor can accurately predict the leaves drifting down and swaying in the wind, while DynamiCrafter incorrectly predicts the leaves moving upward. Additionally, PhyCoPredictor can capture the complex motion of a rolling stone, whereas DynamiCrafter performs poorly in this scenario."}, {"title": "5. Conclusion", "content": "The rapid development of the video generation field has raised higher demands for video benchmarks, necessitating more comprehensive and effective methods for evaluating video quality. In this paper, we introduce PhyCoBench, a benchmark specifically focused on the dimension of physical coherence in videos. We have carefully designed a prompt set that comprehensively covers various physical scenarios. We also utilize an optical flow-guided frame prediction model, PhyCoPredictor, which effectively predicts the motion trajectories of objects. Based on this model and the prompt set, we can effectively assess whether the content of a video adheres to physical coherence. We believe that PhyCoBench makes a significant contribution to the fields of video generation and evaluation, and will aid in enhancing the capabilities of future video generation models."}]}