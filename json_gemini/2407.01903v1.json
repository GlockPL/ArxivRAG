{"title": "Text-Aware Diffusion for Policy Learning", "authors": ["Calvin Luo", "Mandy He", "Zilai Zeng", "Chen Sun"], "abstract": "Training an agent to achieve particular goals or perform desired behaviors is often accomplished through reinforcement learning, especially in the absence of expert demonstrations. However, supporting novel goals or behaviors through reinforcement learning requires the ad-hoc design of appropriate reward functions, which quickly becomes intractable. To address this challenge, we propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a pretrained, frozen text-conditioned diffusion model to compute dense zero-shot reward signals for text-aligned policy learning. We hypothesize that large-scale pretrained generative models encode rich priors that can supervise a policy to behave not only in a text-aligned manner, but also in alignment with a notion of naturalness summarized from internet-scale training data. In our experiments, we demonstrate that TADPoLe is able to learn policies for novel goal-achievement and continuous locomotion behaviors specified by natural language, in both Humanoid and Dog environments. The behaviors are learned zero-shot without ground-truth rewards or expert demonstrations, and are qualitatively more natural according to human evaluation. We further show that TADPoLe performs competitively when applied to robotic manipulation tasks in the Meta-World environment.", "sections": [{"title": "1 Introduction", "content": "Can we train reinforcement learning agents that drive humanoids in a virtual environment [40] to stably stand? How about standing with hands on hips, kneeling, or doing splits? While state-of-the-art algorithms have shown success on the former scenario (e.g. [12]), the latter remains challenging due to the need for carefully (and often manually) crafted reward functions to specify the desired behaviors. The dependence on ad-hoc designed reward functions renders inscalable the learning of ever-increasing amounts of novel behaviors, which are required in applications ranging from character animation [2] to robotic manipulation [43].\nOur work looks towards natural language as a powerful interface through which humans can flexibly specify desired goals or behaviors of interest. We therefore investigate how to construct a zero-shot text-conditioned reward signal, replacing the need for ad-hoc designs, through which text-aligned policies can be learned. We present Text-Aware Diffusion for Policy Learning (TADPoLe), which utilizes a large-scale pretrained, frozen text-conditioned diffusion model to generate a dense reward signal for policy learning. We hypothesize that generative diffusion models, which are pretrained on internet-scale datasets to produce text-aligned, natural-looking images [32, 29] and videos [3, 8, 13], can be utilized to automatically craft a multimodal reward signal that encourages an agent to behave both faithfully with respect to text conditioning and naturally with respect to human perception. Our method is novel in its reward computation, as well as its utilization of a domain-agnostic generative model, rather than one trained from environment-specific or task-specific video demonstrations, as used in prior work [39, 24, 5, 7, 20, 19]."}, {"title": "2 Related Work", "content": "Diffusion models [34, 35, 14, 36] have recently demonstrated amazing generative modeling capa- bilities, particularly in the domain of text-conditioned image generation [4, 29, 32, 31]. Notably, guidance [36, 4, 15] has been shown to be a critical component in producing visual outputs aligned with textual data, enabling the generation of images that accurately match a desired text caption, especially when the models are scaled to utilize large foundation models such as CLIP [27] or T5 [28], and trained on massive image-text datasets [33]. Our work is inspired by DreamFusion [25], where a"}, {"title": "3 Method", "content": "We propose Text-Aware Diffusion for Policy Learning (TADPoLe) to learn text-aligned policies by leveraging frozen, pretrained text-conditioned diffusion models. An overview of the framework can be found in Figure 3."}, {"title": "3.1 Text-Aware Diffusion for Policy Learning", "content": "We first describe how TADPoLe produces text-conditioned rewards from image observations. At each timestep t, reward \\(r_t\\) is computed as a score between rendered subsequent image \\(o_{t+1}\\) and the provided text caption describing the behavior of interest, denoted by y, using a frozen, pretrained text- to-image diffusion model. We begin by corrupting the rendered image \\(o_{t+1}\\) with a sampled Gaussian source noise vector \\(\\epsilon_0 \\sim N(\\epsilon; 0, I)\\) to produce noisy observation \\(\\tilde{o}_{t+1}\\), and use the diffusion model to make an unconditional prediction \\(\\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise})\\) as well as a conditional prediction \\(\\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise}, y)\\). Here \\(\\hat{\\epsilon}_{\\phi}\\) is a neural network that predicts the source noise given \\(\\tilde{o}_{t+1}\\), the level of noise corruption \\(t_{noise}\\), and optionally the text prompt y. We then compute the mean squared error (MSE) between the two predictions as a reward signal \\(r_t\\) to be maximized:\n\\(r_t^{align} = ||\\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise}, y) - \\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise}) ||_2.\\)\nAs investigated in Appendix B.3, we empirically observe that \\(r_t^{align}\\) plays a crucial role on the success of TADPoLe. We hypothesize that for an appropriately-selected noise corruption level \\(t_{noise}\\), this term measures the alignment between the environmental observation and the text prompt. Intuitively, for unconditional prediction \\(\\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise})\\), the model is incentivized only to bring the noisy input to any arbitrary cleaner image, and makes minimal edits by moving it towards the closest clean mode in data space. On the other hand, if the model recognizes visual features in the noisy image aligned with the text prompt, conditional prediction \\(\\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise}, y)\\) is incentivized to do \u201cextra work\u201d and bring it closer to the specific mode described by the text conditioning. We thus expect the MSE to be larger for well-aligned text conditioning. For an unaligned text prompt, the model may have more difficulty in recognizing relevant visual features in the corrupted image, and therefore generally has a lower computed \\(r_t^{align}\\) signal. Therefore maximizing \\(r_t^{align}\\) is a tractable proxy for maximizing the alignment between the rendered observation \\(o_{t+1}\\) and the provided text prompt y.\nWe also wish to encourage behaviors that are natural to human perception (e.g. a humanoid should walk similar to how a typical pedestrian would walk). We approximate the naturalness of a behavior by how accurately the diffusion model is able to predict the exact source noise vector that was applied. Intuitively, if it voluntarily predicts the exact noise vector with informative text conditioning, thereby perfectly reconstructing the query image, then the diffusion model believes the original rendered frame is reasonably natural (according to the priors captured by the diffusion model). We would therefore like to minimize \\(||\\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise}, y) - \\epsilon_0||_2\\). We would also like this term to be comparatively closer to the source noise vector than the unconditional prediction is, further reaffirming the benefit of the text conditioning. We therefore seek to also maximize a comparative reconstruction term as below:\n\\(r_t^{rec} = ||\\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise}) - \\epsilon_0||_2 - ||\\hat{\\epsilon}_{\\phi}(\\tilde{o}_{t+1}; t_{noise}, y) - \\epsilon_0||_2,\\)"}, {"title": "3.2 TADPoLe with Text-to-Video Diffusion Models", "content": "Conceptually, there exist fundamental limitations to using a text-to-image model to provide a reward signal. As each image is evaluated statically and independently, we are unable to expect the text- to-image diffusion model to be able to accurately understand and supervise an agent in learning notions of speed, or in some cases, direction, as such concepts require evaluating multiple consecutive timesteps to deduce. We therefore propose Video-TADPoLe, where a dense text-conditioned reward signal is calculated over sliding windows of consecutive frames through a pretrained text-to-video diffusion model. We extend and generalize the reward formulation from TADPoLe thusly.\nWe can compute reward terms for arbitrary start index i and end index j inclusive, for i < j, by considering the sequence of subsequently rendered frames o[i+1:j+1]. We once again utilize source noise vector \\(\\epsilon_0 \\sim N(\\epsilon; 0, I_{j-i+1})\\) to produce noisy observation \\(\\tilde{o}[i+1:j+1]\\). Then, we can compute a batch of alignment reward terms through one inference step of the text-to-video diffusion model as:\n\\(r^{align}[i:j] = ||\\hat{\\epsilon}_{\\phi}(\\tilde{o}[i+1:j+1]; t_{noise}, y) - \\hat{\\epsilon}_{\\phi}(\\tilde{o}[i+1:j+1]; t_{noise}) ||_2,\\)\nand a batch of reconstruction reward terms as:\n\\(r^{rec}[i:j] = ||\\hat{\\epsilon}_{\\phi}(\\tilde{o}[i+1:j+1]; t_{noise}) - \\epsilon_0||_2 - ||\\hat{\\epsilon}_{\\phi}(\\tilde{o}[i+1:j+1]; t_{noise}, y) - \\epsilon_0||_2.\\)\nFor a desired context window of size n, we then calculate the reward at each timestep t utilizing each context window that involves achieved observation \\(o_{t+1}\\):\n\\(r_t = \\sum_{i=1}^{n} symlog \\bigg( w_1 * r^{align}[t-i+1:t-i+n] \\bigg) + symlog \\bigg( w_2 * r^{rec}[t-i+1:t-i+n] \\bigg).\\)\nIntuitively, we seek to calculate an overall reward for an action based off how well the resulting rendered frame aligns with text-conditioning at the beginning of a motion sequence, the end of one, and arbitrarily inbetween. For window size n = 1, this recreates TADPoLe behavior, but using a text-to-video model; for n > 1, we make the computation tractable through dynamic programming."}, {"title": "4 Experiments", "content": "We now demonstrate the effectiveness of TADPoLe on goal achievement, continuous locomotion, and robotic manipulation tasks. All results are achieved without access to in-domain demonstrations."}, {"title": "4.1 Experimental Setup and Evaluation", "content": "Benchmarks: We present our main results using the Dog and Humanoid environments from the DeepMind Control Suite [40], and robotic manipulation tasks from Meta-World [43]. Dog and Humanoid are known to be challenging due to their large action space, complex transition dynamics, and lack of task-specific priors (such as termination conditions). We update the environments by modifying the terrain rendered by MuJoCo [38] to have green grass and blue sky. We also limit the number of environment timesteps to be 300, which is sufficient to demonstrate successful learning of a behavior, rather than the default 1000. The agent's initialized joint configurations were also fixed, as we focus on learning text-conditioned capabilities rather than robustness to initialization conditions. Meta-World is initially designed for multi-task and meta reinforcement learning, and is later adopted to evaluate language-conditioned imitation learning algorithms [23, 22, 37]. We select a suite of tasks which are balanced for diversity and complexity, and each task is paired with a text prompt (see tasks and their corresponding prompts in Appendix C). Following prior design [17] for Meta-World, we also add a sparse success signal to the dense text-conditioned reward signals.\nImplementation: We use TD-MPC [12] as the reinforcement learning algorithm for all tasks. It is the first documented model to solve Dog tasks when ground truth rewards are available for walking. We fix the hyperparameters to the default ones recommended by the TD-MPC authors (see Table A3 in Appendix) for all experiments unless otherwise mentioned. We train Humanoid and Dog agents for 2M steps, and Meta-World agents for 700K steps. For Meta-World experiments, we scale the sparse success signal by 2. Visualizations and quantitative evaluations are reported using the last checkpoint achieved at the end of training. We use StableDiffusion 2.1 [31] as the text-to-image diffusion model (~1.3B parameters), and AnimatedDiff [10] v2 (~1.5B parameters) as the text-to-video diffusion model. AnimatedDiff is implemented on top of StableDiffusion 1.5. We fix the reward weights W\u2081 = 2000 and w\u2082 = 200 based on Humanoid standing and walking performance, and study their impact in Appendix B.3. Selection of noise level is discussed in Appendix A. All experiments are performed on a single NVIDIA V100 GPU.\nBaselines: We compare TADPoLe against other text-to-reward approaches, including VLM-RM [30], LIV [20], and Text2Reward [42], on top of the same underlying TD-MPC architecture, hyperpa- rameters, and optimization scheme for fair comparison. For LIV, we use their provided CLIP-based checkpoint finetuned on robotic demonstration videos. For VLM-RM, we utilize the ViT-bigG-14 CLIP checkpoint (~1.3B parameters), reported as the best performing in their work. We follow a prompt template provided in the Text2Reward paper to generate reward functions for the Dog and Humanoid agent, interfaced through vanilla ChatGPT using GPT-3.5. Whereas VLM-RM and LIV provide a multimodal reward signal, and are more directly comparable to TADPoLe, it is of note that Text2Reward generates a text-conditioned reward function purely as the output of a pretrained language model. However, Text2Reward does have access to underlying sensor data such as speed and direction in real-time, whereas the visual interface approaches, including TADPoLe, do not.\nEvaluation Protocols: We benchmark all text-conditioned methods with a corresponding standard- ized prompt for fair comparison, and report both quantitative as well as qualitative comparisons. We use cumulative ground-truth rewards as quantitative evaluation metrics for Dog and Humanoid when it is available. We note this is a naturally unfavorable comparison for methods that provide a text-conditioned reward signal purely through a visual interface, as the reward the agent receives has no access to the underlying sensors (such as ones that measure speed and energy usage) that the ground-truth reward function uses to evaluate performance. For example, the ground-truth reward function may have an arbitrary threshold on a speed sensor that needs to be hit to constitute successful \u201cwalking\", and a separate threshold for \"running\"; however the detailed characteristics of and even existence of such a sensor, as well as any thresholds surrounding it, are hidden for policies supervised only through vision and language feedback. Nonetheless, it offers a standardized, numerical compari- son across all methods. For Meta-World, we report the \u201csuccess rate\u201d evaluation metric, computed as the proportion of evaluation rollouts in which the agent successfully completes the given task."}, {"title": "4.2 Goal Achievement", "content": "For text-conditioned goal-achievement, the objective is to learn a policy to consistently achieve a particular pose described by a text prompt; as the emphasis is for every frame to match a fixed goal pose rather than performing continuous motion, it is natural to apply TADPoLe with text-to-image diffusion models. We set the noise level \\(t_{noise} \\sim U(400, 500)\\).\nIn the Humanoid environment, there is a ground-truth reward function that measures standing performance, as a function of the straightness of the agent's spine. We therefore compare all text- conditioned methods using the provided reward function as a quantitative metric, with a standard prompt of \"a person standing\u201d; these results are shown in the first row of Table 1. TADPoLe and VLM-RM achieve competitive quantitative performance with an agent trained on the ground-truth reward function. The following rows show that according to the user study, TADPoLe consistently achieves text-aligned behaviors beyond making the humanoid stand, whereas other approaches often fail. Table 2 shows that users consistently found TADPoLe to produce more natural-looking motions and poses when compared head-to-head with VLM-RM."}, {"title": "4.3 Continuous Locomotion", "content": "We further explore the ability of TADPoLe to learn continuous locomotion behaviors conditioned on natural language specifications. Such tasks are often difficult to learn purely from static external description, as there is no canonical pose or goal frame that if reached, would denote successful achievement of the task. This is challenging for approaches that statically select a canonical goal- frame to achieve, such as CLIP or LIV, and we propose Video-TADPoLe, which leverages large-scale pretrained text-to-video generative models, as a promising direction forward.\nWe utilize a noise level \\(t_{noise} \\sim U(500, 600)\\) in our continuous locomotion experiments. We perform a search over context windows of size n = {1, 2, 4, 8}, and report the best configuration per task. We observe that when the context window is too high (e.g. 8 or higher), the agent has consistently lower performance, and that although the agent learns coherent motion and repeats it, the pose is less text-aligned. For fair comparison against a text-video alignment model trained in a contrastive manner, we extend VLM-RM to ViCLIP-RM, where a ViCLIP-L-14 checkpoint [41] finetuned from ViT-L-14 CLIP is used to compute dense, text-conditioned rewards. At each timestep t, we compute dense rewards as cosine similarity between the encoded representations of video observation up to t + 1 and the text prompt. We ask ViCLIP to encode 8 video frames at a time, which is adopted by its authors for zero-shot experiments.\nFor the Humanoid task, we find that Video-TADPoLe achieves the best results amongst methods trained purely from visual and/or language feedback as in Table 3. On the other hand, ViCLIP-RM indeed learns to take steps, but does so sideways while maintaining an unnaturally lop-sided pose. VLM-RM also keeps the agent upright, but refrains from learning substantial movement, whereas LIV and Text2Reward fail to learn meaningful behavior.\nFor the Dog task, we notice that both VLM-RM as well as ViCLIP-RM collapse; they both learn to strike a particular pose and maintain it for perpetuity. Text2Reward, which does not have access to"}, {"title": "4.4 Robotic Manipulation", "content": "We further investigate how well TADPoLe can be applied to learn robotic manipulation tasks through dense text-conditioned feedback. We do so by replacing the manually-designed ground-truth dense reward for each Meta-World task with TADPoLe's text-conditioned reward. Since TADPoLe aims to leverage domain-agnostic diffusion models for policy learning, we focus our evaluation to compare with baseline methods that also do not utilize in-domain (expert) demonstrations for the robotic manipulation tasks. We note that most of the prior methods which report performance on Meta-World rely on (often expert-produced) video demonstrations from a similar domain or the target environment directly for representation learning [23], reward learning [16], or both [20]. They are thus not directly comparable to TADPoLe.\nWe perform thorough comparisons between TADPoLe and VLM-RM by evaluating them on a diverse set of selected Meta-World tasks. Both models reuse the setup in Section 4.2 without modification, with training performed for 700k steps. In Table 4, we report the final success rate for each manipulation task averaged over 30 evaluation rollouts. We highlight that TADPoLe achieves high success rates across a variety of tasks, and significantly exceeds VLM-RM in terms of average overall performance. We take this as a positive signal that TADPoLe can meaningfully provide dense text-conditioned rewards that replace dense ground-truth hand-designed feedback. We also highlight how TADPoLe is able to successfully supervise the learning of policies within the synthetic-looking visual environment of Meta-World without finetuning the pretrained text-to-image diffusion model, despite the visual attributes (such as the appearance of the robotic arm, or the quality of the renderings) being quite dissimilar from the style of images StableDiffusion was trained on."}, {"title": "5 Conclusion and Future Work", "content": "We present Text-Aware Diffusion for Policy Learning (TADPoLe), a framework that optimizes a policy according to a provided natural language prompt through a pretrained text-conditioned diffusion model. TADPoLe enables novel behaviors to be learned in a zero-shot manner purely from text conditioning, and also offers a promising angle to train policies to behave in accordance with natural priors summarized from large-scale pretraining. TADPoLe can be applied across visual environments and different robotic states without modification, and we experimentally demonstrate that TADPoLe is able to learn to novel goal-achievement as well as continuous locomotion behaviors conditioned only on text, across Humanoid, Dog, and Meta-World environments.\nLimitations: An observed limitation of TADPoLe is that it is difficult to explicitly control the weight each individual word of an input text prompt has on the reward provided to the agent. For certain"}, {"title": "A Intuition Regarding A Reasonable Noise Level Range", "content": "We desire a reward signal that is high when the text prompt is well-aligned with the frame rendered following a well-selected action. However, we find that using too high or too low a noise level will cause the reward computation to ignore or discount the provided text prompt.\nFor a sufficiently low noise value, the conditional and unconditional noise prediction is similar for a particular text prompt and rendered frame that already has high cross-modal alignment. Being already close in data space to the desired mode described by the text prompt, the unconditional prediction will also cheaply seek to denoise the input towards the original input. Therefore, too low a noise value may cause the computed reward signal to decrease as cross-modal alignment increases, which runs counter to what is desirable. This is supported by looking at the left tails of the two graphs depicted in Figure A1, where the difference in computed TADPoLe reward between well-aligned paired inputs and misaligned paired inputs is small.\nSimilarly, choosing a sufficiently high noise value may also cause unconditional and conditional predictions to be similar. For a given noisy input with virtually all of the spatial structure perturbed, the pretrained denoising model intuitively makes denoising predictions that fill in general structure to the image irrespective of text-conditioning. This is intuitively similar to the behavior of an unconditional prediction. Once again, we can visually observe this in the right tails of both graphs in Figure A1; for high noise values, both misaligned and well-aligned paired inputs have similar computed TADPoLe reward.\nIntuitively, TADPoLe is able to meaningfully quantify text-conditioned alignment with rendered observations into a reward signal for a noise level range approximately in the middle. For a balanced level of noise corruption on a text-aligned rendered observation, there is enough existing visual structure to help the denoising model make a meaningful text-conditioned prediction close to the original input, whereas an unconditional prediction may not do so accurately. In Figure A1, we observe that for a noise range in the middle, TADPoLe is able to substantially favor the well-aligned pair over the misaligned pair, respecting changes in both text prompt as well as rendered observations. We verify in our experiments that using a noise level between 400 and 500 works well for TADPoLe, across both Dog and Humanoid environments."}, {"title": "A.1 Noise Level for Video-TADPOLe", "content": "We discover that Video-TADPoLe achieves better performance with a higher noise level than TAD- PoLe, and use a range of 500 to 600 in our experiments. We hypothesize that being able to observe multiple frames at once enables the space-time U-Net to exploit uncorrupted (or lesser-corrupted) portions from adjacent frames across the context window to inform how each individual frame should be denoised coherently. Because each frame in a context window has a distinct source noise applied to it, structural information can be leaked from randomly uncorrupted portions of the other frames in the context window when making predictions. Therefore, to make the denoising prediction more challenging, thereby forcing it to rely on the text-conditioning and learned motion priors rather than"}, {"title": "B Detailed Hyperparameters", "content": "In our experiments, we place less emphasis on the underlying architecture and optimization scheme, instead comparing our method against other text-conditioned reward functions. We keep the same reinforcement learning architecture, optimization scheme, and text-to-image/text-to-video generative model in all environments, unless otherwise noted. We provide detailed hyperparameters about these existing components below."}, {"title": "B.1 Pretrained Text-Conditioned Diffusion Models", "content": "We utilize a StableDiffusion v2.1 checkpoint for our TADPoLe experiments. We provide the sizes of the components in Table A1. For Video-TADPoLe experiments, we use a pretrained AnimateDiff checkpoint, and provide relevant details in Table A2. We do not update or modify either checkpoint during our training, utilizing them purely for inference."}, {"title": "B.2 TD-MPC", "content": "We include the default hyperparameters from the TD-MPC implementation in Table A3 for complete- ness. We do not modify the default recommended settings for both Humanoid and Dog environments, as well as the Meta-World experiments."}, {"title": "B.3 Selecting w\u2081 and W2", "content": "We select hyperparameters w\u2081 and w\u2082, which scale the alignment term \\(r^{align}\\) and the reconstruction term \\(r^{rec}\\) respectively, firstly such that the resulting computed reward r has value roughly between 0 and 1 at any arbitrary timestep. This is visualized in Figure A1, where over all noise levels, the computed reward stays roughly between 0 and 1. Because the same pretrained text-conditioned diffusion model is used across all environments without modification, these hyperparameters can be generally reused to achieve the same kind of reward scale across environments. We found that the values of w\u2081 = 2000 and w\u2082 = 200 through a light hyperparameter sweep, reported in Table A4, and indeed verify that they work well without modification across both Humanoid and Dog environments in our main experiments."}, {"title": "C Meta-World Tasks", "content": "We visualize the complete suite of selected Meta-World tasks in Figure A2 along with the official names of the tasks. In Table A5 we also list the corresponding prompts consistently utilized across all text-conditioned approaches. In Table A6, we list the average success for all 12 robotic manipulation tasks. As we add a sparse \"success\" signal to text-conditioned approaches such as VLM-RM and TADPoLe, following prior experimental design [17] for Meta-World, we also list the performance of utilizing the sparse signal only.\nWe first notice that no approach is able to solve two selected tasks, \u201cPeg Insert Side\u201d and \u201cShelf Place\u201d; these were therefore omitted in Table 4. Furthermore, we observe that utilizing a sparse reward signal only appears to have strong default performance, and achieves a higher overall average success rate than TADPoLe or VLM-RM. This showcases the inherent power of a sparse reward in solving Meta-World tasks. However, there are two additional takeaways; firstly, VLM-RM surprisingly achieves a substantial decrease in performance from the default sparse reward signal, highlighting TADPoLe as a more preferable dense text-conditioned reward provider. Secondly, TADPoLe is able to solve more overall tasks than other approaches. In particular, TADPoLe can solve the \u201cDoor Open\u201d task, which is completely unable to be solved by VLM-RM or using a sparse reward only. We therefore highlight TADPoLe as a promising text-conditioned reward signal in replacing ground-truth hand-designed feedback, particularly as the text-to-image diffusion model utillized was pretrained in a general manner, without finetuning explicitly on Meta-World demonstrations."}, {"title": "D Training Curves", "content": "In our experiments, we compare against other methods that provide text-conditioned rewards. As each of these methods are formulated differently, the difference in scales naturally prevent easy direct comparison. However, plotting the training curves for TADPoLe does yield insights into its speed of convergence, and investigating the visual performance at intermediate steps can be interesting. In Figure A3 we showcase the TADPoLe training curves for a variety of novel text-conditioned policies. We highlight the performance at steps 500k, 1M, 1.5M, and 2M in red; we further visualize the policy achieved at these intermediate steps by showcasing the last frame of the achieved video.\nIn the case of the policy learned for the prompt \u201ca person standing with hands on hips", "a person kneeling\", the policy first learns to kneel on all fours; by the end, the policy learns to successfully kneel on one knee.\"\n    },\n    {\n      \"title\"": "E Motivating Visualizations"}, {"content": "For further intuition on TADPoLe's behavior, we visualize the complete denoising results for a query video achieved through a Dog policy and corrupted with some level of noise, conditioned on a consistent text prompt of \"a dog walking\". Note that this is purely for visualization purposes; in practice, when training policies, we do not visually denoise over multiple steps but instead extract a reward signal from components of a one-step denoising prediction. However, the multi-step denoising visualizations can offer us a glimpse of intuition into the behavior of the pretrained diffusion model and the properties of its predicted components at one single denoising step.\nWe first find that StableDiffusion, used in TADPoLe, is able to reconstruct a well-aligned policy rollout (a video of the Dog agent actually walking) relatively accurately frame-by-frame, for a noise level of 500 (Figure A5). However, the predictions can differ substantially for a misaligned policy rollout (a video of the Dog agent falling over rather than walking); for frames where the dog has collapsed on the floor, the StableDiffusion model still tries to respect the specified input prompt and attempts to predict dogs standing upright or walking, as depicted in Figure A6, resulting in a more noticeable difference between the achieved frame and predicted frame. This difference can be exploited to distinguish between well-aligned and misaligned policy rollouts, lending confidence to StableDiffusion as a supervisory signal for text-conditioned policy learning. For a higher noise level, StableDiffusion quickly forms hallucinatory final predictions (depicted in Figures A7, A8), but preserves more of the structure of the original video when the achieved video and provided text prompt is aligned.\nWe also visualize reconstruction predictions using a text-to-video model, namely an AnimateDiff v2 checkpoint, in Figures A9 and A10. We find that the motion prior is indeed meaningful, enabling it to reconstruct the Dog with a coherent texture and pose over time. In Figures A11, A12 we highlight how a text-to-video model is more robust in reconstructing the achieved video by the policy at high noise levels compared with StableDiffusion (Figures A7, A8). This supports the hypothesis outlined in Section A.1."}]}