{"title": "Bridging Critical Gaps in Convergent Learning: How Representational Alignment Evolves Across Layers, Training, and Distribution Shifts", "authors": ["Chaitanya Kapoor", "Sudhanshu Srivastava", "Meenakshi Khosla"], "abstract": "Understanding convergent learning-the extent to which artificial and biological neural networks develop similar representations-is crucial for neuroscience and AI, as it reveals shared learning principles and guides brain-like model design. While several studies have noted convergence in early and late layers of vision networks, key gaps remain. First, much existing work relies on a limited set of metrics, overlooking transformation invariances required for proper alignment. We compare three metrics that ignore specific irrelevant transformations: linear regression (ignoring affine transformations), Procrustes (ignoring rotations and reflections), and permutation/soft-matching (ignoring unit order). Notably, orthogonal transformations align representations nearly as effectively as more flexible linear ones, and although permutation scores are lower, they significantly exceed chance, indicating a robust representational basis. A second critical gap lies in understanding when alignment emerges during training. Contrary to expectations that convergence builds gradually with task-specific learning, our findings reveal that nearly all convergence occurs within the first epoch-long before networks achieve optimal performance. This suggests that shared input statistics, architectural biases, or early training dynamics drive convergence rather than the final task solution. Finally, prior studies have not systematically examined how changes in input statistics affect alignment. Our work shows that out-of-distribution (OOD) inputs consistently amplify differences in later layers, while early layers remain aligned for both in-distribution and OOD inputs, suggesting that this alignment is driven by generalizable features stable across distribution shifts. These findings fill critical gaps in our understanding of representational convergence, with implications for neuroscience and AI.", "sections": [{"title": "Introduction", "content": "Deep Neural Networks (DNNs) are becoming increasingly popular in neuroscience for predicting neural responses ((Yamins et al., 2014), (Yamins & DiCarlo, 2016), (Khaligh-Razavi & Kriegeskorte, 2014)) or as models for reverse-engineering algorithms of neural computation ((Schrimpf et al., 2018), (Schrimpf et al., 2020), (Cichy et al., 2016)). This congruence invokes the necessity to gain a deep understanding of how DNNs learn to represent information. A core question in this domain is whether independently trained networks converge on similar internal representations and if so, under what conditions and along which dimensions this convergence unfolds. Comparative analysis of model representations helps reverse-engineer neural networks by linking architectural components, training objectives, and data inputs to emergent behavior. For neuroscientific modeling using deep neural networks, a foundational question lies in ascertaining which aspects of their representations vary across different architectural choices, and which aspects if any\u2014are universal across such choices. Thus, studying this representational convergence has far-reaching implications.\nOver the past decade, there has been growing recognition that similar representations can emerge across diverse models, even when these models differ in architecture, training procedures, or data modalities. Early work demonstrated that independent training runs of the same architecture often develop a core set of features that align well across networks. For example, early layers in convolutional networks consistently learn Gabor-like filters, a phenomenon observed across a range of architectures and tasks (Yosinski et al., 2014). Efforts to quantify these similarities have employed techniques such as singular vector canonical correlation analysis (SVCCA) (Raghu et al., 2017) and centered kernel alignment (\u0421\u041a\u0410) (\u041aornblith et al., 2019), which have underscored the high degree of alignment in early layers and noted convergence even in later stages of the network. These findings provide empirical grounding for theories of representational convergence, hinting at the existence of universal principles governing learning principles that may also shed light on how biological neural circuits process information.\nMore recent studies have extended this line of inquiry, demonstrating that as models become more capable\u2014whether through increased scale, multitask training, or cross-modal learning their representations converge not only within a single modality but also across modalities. For instance, the Platonic Representation Hypothesis argues that as models solve a larger number of tasks and scale up in capacity, they are pressured into discovering a universal, modality-agnostic representation of the underlying reality (Huh et al., 2024). Yet, many open questions remain about the conditions under which different networks converge on similar representations and the implications of such convergence.\nTo address these gaps, in our work, we examine representational alignment along three key axes:\nAcross layers: One important axis of inquiry is representational alignment across layers. Prior work has shown that early layers tend to extract general, low-level features such as edge detectors in vision networks-while deeper layers develop more task-specific representations. Studies by (Kornblith et al., 2019), (Mehrer et al., 2020), and (Li et al., 2015) have quantified these changes using methods like canonical correlation analysis (CCA) and CKA. However, these approaches often rely on single metrics that obscure the minimal transformations needed to align representations. Understanding the precise nature of these transformations is critical for dissecting how representations in different networks relate to each other (e.g., are they similar in information content, representational geometry, or even at the level of single-neuron tuning?). It is also unknown whether the hierarchical (layer-wise correspondence) results hold true for other metrics with more restricted invariances than affine transformations.\nAcross training: A second key question concerns the evolution of representational convergence over the course of training. Most studies compare networks at their converged state. Yet, a deeper understanding requires examining not only the final representations, but also the learning trajectories that lead there. Conventionally, it is assumed that as different networks optimize on a task, their internal representations become more similar, driven by the final task solution. This assumption is the basis for the contravariance principle (Cao & Yamins, 2021), which posits that when a network is pushed to achieve a challenging task, there is less room for variation in the final solution, forcing representations to converge. However, the question of when representational convergence occurs during training remains underexplored. Understanding this dynamic can illuminate the roles of initialization, early data statistics, architectural biases, learning dynamics, and the final task solution in shaping alignment. Previous studies have shown a \u201clower layers learn first\u201d (Raghu et al., 2017) behavior by comparing layers across time on CIFAR-10 using SVCCA, but little else is known about the dynamics of convergence, especially for more complex vision networks and using other metrics.\nAcross distribution shifts: Although many deep neural networks exhibit highly human-like responses to in-distribution stimuli, there is mounting evidence that their responses can diverge dramatically under out-of-distribution (OOD) conditions (Prasad et al., 2022; Geirhos et al., 2021; Geirhos, Temme, et al., 2018). Despite this, the effect of OOD inputs on model-to-model representational convergence remains poorly understood. Exploring this axis is crucial for rigorously testing the universality of learned representations."}, {"title": "Key contributions", "content": "In this work, we address these gaps by systematically investigating representational convergence along these three axes. First, we employ three alignment metrics\u2014linear regression, Procrustes analysis, and permutation-based methods each with different restrictions on the freedom of the mapping function, to identify the minimal set of transformations needed to align representations across networks reasonably well for each layer. This approach allows us to dissect how representations relate to each other, whether in terms of information content, representational geometry, or single-neuron tuning. Second, we examine the temporal dynamics of convergence during training, revealing that nearly all alignment occurs within the first epoch, challenging the assumption that convergence is tied to task-specific learning. Finally, we explore how changes in input statistics affect representational alignment across layers, demonstrating how OOD inputs differentially affect later versus early layers."}, {"title": "Problem Statement", "content": "We consider two representations,\n\\(X_i \\in \\mathbb{R}^{M \\times N_x}\\) and \\(X_j \\in \\mathbb{R}^{M \\times N_y}\\),\nobtained from different models over M unique stimuli, where \\(N_x\\) and \\(N_y\\) denote the number of neurons (or units) in each representation, respectively. To systematically identify the minimal transformations needed for alignment, we use three metrics that quantify similarities between networks while ignoring nuisance transformations. These metrics are ordered to reflect progressively more permissive mapping functions (from strict to flexible):\n1.  Permutation Score (and its extension, the Soft-Matching Score): This metric treats the order of units as arbitrary, reflecting similarity in representational form-i.e., how the representations are assigned at the level of individual neurons. In this case, we seek a mapping matrix M that minimizes\n\\(\\min_M ||X_i - MX_j||_2^2\\).\nFor the permutation score, when \\(N_x = N_y\\), M is constrained to be a permutation matrix:\n\\(M \\in P(N)\\)."}, {"title": "Results", "content": "How convergence varies with network depth.\nWhen comparing representational convergence across the network hierarchy for different seeds of the same architecture, we observe that convergence is strongest in the earliest layers and gradually diminishes in deeper layers (Fig. 1). This pattern is consistent across all three metrics and across networks trained on different datasets (CIFAR100 and ImageNet). The high alignment in early layers likely arises because they capture fundamental, low-frequency features (e.g., edges, corners, contrast) that are universal across representations (Rahaman et al., 2019; Bau et al., 2017; Zeiler & Fergus, 2014). In contrast, deeper layers, while still showing significant alignment (> 0.5), exhibit greater variability due to their sensitivity to specific training conditions and noise. This trend also holds when comparing networks with different architectures, underscoring the robustness of hierarchical convergence across diverse models.\nMinimal transformations needed to align representations. Across all layers, we find that alignment scores increase as the mapping functions become more permissive (Permutation \u2192 Procrustes \u2192 Linear), as expected. However, linear mappings provide only modest improvements over Procrustes correlations, indicating that rotational transformations are sufficient to capture the majority of alignment information. This suggests that the added flexibility of linear mappings such as scaling and shearing\u2014does not substantially enhance alignment beyond what is achieved with Procrustes transformations. Importantly, because Procrustes is symmetric, this result highlights that alignment reflects a deeper similarity in the geometric structure of representations, rather than merely the ability of one representation to predict another.\nSimple permutations achieve significant alignment. Despite the strict constraints imposed by permutation-based alignment, Permutation scores achieve surprisingly high alignment levels, indicating a strong one-to-one correspondence between individual neurons across network instances. This suggests that convergent learning extends down to the level of single neurons, even without allowing for more flexible transformations.\nTo further probe this result and assess the depth of convergent learning, we tested the sensitivity of permutation alignment to changes in the representational basis. Specifically, we applied a random rotation matrix \\(Q \\in \\mathbb{R}^{n \\times n}\\) to the converged basis of a neural representation, where n is the number of neurons in a given layer. The rotation matrix was sampled from a Haar distribution via a QR decomposition, ensuring that all orthogonal matrices were equally likely. We then recomputed the Permutation score after applying this rotation.\nWe conducted this analysis by taking response matrices from two identical DCNNs (initialized with different random seeds) at a given convolutional layer, \\(\\{X_1, X_2\\} \\in \\mathbb{R}^{m \\times n}\\), where m represents the number of stimuli. We applied the random rotation Q to one network's responses and computed the resulting permutation-based correlation score, \\(S_{perm}(X_1Q, X_2)\\). This process was repeated across all convolutional layers, with the alignment differences summarized in Table 1.\nThese rotations consistently reduced alignment, with a drop between ~ 6-51% on CIFAR-100 and ~ 10-202% on ImageNet for ResNet18 across all layers. This significant decrease highlights that the learned representations are not rotationally invariant and that the specific basis in which features are encoded is meaningfully preserved across networks. In other words, convergent learning aligns not just the overall representational structure but also the specific axes along which features are encoded. This observation echoes recent findings by (Khosla et al., 2024), who report the existence of privileged axes in biological systems as well as the penultimate layer representations of trained artificial networks."}, {"title": "Evolution of convergence over training", "content": "We next explore how representational convergence evolves during the training process. Specifically, we compute Procrustes alignment scores between pairs of networks over the first 10 epochs of training for both CIFAR100 and ImageNet.\nAs shown in Fig. 2, a striking pattern emerges: the majority of representational convergence happens within the first epoch-long before networks approach optimal task performance. This rapid early convergence suggests that factors independent of task optimization drive much of the representational alignment. Shared input statistics, architectural biases, and early training dynamics seem to play a dominant role in shaping the learned representations, overshadowing the influence of the final task-specific solution.\nThis observation challenges prevailing hypotheses that attribute convergence to constraints imposed by the task, such as the contravariance principle (Cao & Yamins, 2021) or the convergence via task generality hypothesis (Huh et al., 2024). These theories propose that networks converge because they are steered toward a limited subspace of solutions capable of achieving high task performance. However, the early emergence of representational alignment-well before networks achieve such performance-implies that convergence is not primarily driven by task-related pressures but is instead rooted in the network's inductive biases, statistical properties of the input data and early training dynamics.\nInterestingly, this result aligns with findings from another study on the early phase of training (Frankle et al., 2020), which demonstrated that substantial changes in network representations occur within the first few hundred iterations, even before meaningful task learning begins. Their work showed that perturbations to network weights in this early phase such as re-initialization or weight shuffling can significantly degrade performance, suggesting that early representations are already highly structured and data-dependent. Moreover, they found that pre-training using only the input distribution p(x) (e.g., through self-supervised tasks) could approximate the changes seen in early supervised training, albeit requiring substantially longer pre-training. This also lends credence to the idea that shared input statistics, rather than task-specific labels, play a critical role in early convergence.\nFor the earliest convolutional layers, we find that training has minimal impact on representational similarity and can even reduce it in some cases. This phenomenon arises because early layers compute a largely linear function of the input, even in untrained networks, allowing them to align well with simple linear transformations. As training progresses, these layers may undergo minor adjustments that slightly disrupt this initial alignment, though they remain highly similar overall. This result highlights that the convergence of early-layer representations is not driven by the specifics of the training process."}, {"title": "Convergence across distribution shifts", "content": "In the previous sections, we examined representational alignment under the same input distributions used for training. However, a critical question remains: does representational convergence persist under distribution shifts? To explore this, we analyzed the internal representations of ImageNet-trained DCNNs when exposed to out-of-distribution (OOD) stimuli. We used 17 OOD datasets from (Geirhos, Rubisch, et al., 2018), all sharing the same 16 coarse labels as ImageNet (Deng et al., 2009), allowing for a controlled comparison of representational alignment under varying distributional shifts. The specifics of each OOD dataset has been described in Sec. Out-Of-Distribution Dataset.\nWe computed representational alignment across these datasets using the Procrustes metric and observed a consistent pattern: OOD inputs amplify differences in the later layers of the networks, while early layers maintain comparable alignment levels between in-distribution and OOD stimuli (Fig. 5). We hypothesize that this pattern arises as a result of early layers capturing basic, universal features (e.g., edges, corners, textures) that remain nearly identical across distributions, whereas later layers encode more task-specific features that are more sensitive to distributional shifts, thus amplifying the divergence between models.\nMoreover, we found a strong correlation between representational alignment in later layers and the networks' classification accuracy on the OOD datasets. Datasets where models maintained higher accuracy showed stronger alignment, whereas datasets with lower accuracy exhibited weaker alignment. This correlation was notably weaker in early layers but increased progressively with network depth across all architectures (Fig. 6). We extend these analyses to other vision networks in Fig. A1.\nThese results have several important implications. First, the stability of early-layer alignment across distributions suggests that these layers encode generalizable features that are consistent across both network initializations and input distributions. This highlights their role as a shared foundation for higher-level processing, which becomes more specialized and sensitive to distribution shifts in later layers. Second, these findings inform model-brain comparisons. Prior studies have shown that diverse architectures and learning objectives can yield similar brain predictivity (Conwell et al., 2024). However, the observed amplification of representational divergence in later layers under OOD conditions suggests that using OOD stimuli could be an effective strategy for distinguishing between models and identifying more brain-like models.\nAn important potential consequence of this result is that if the alignment between models in early layers is largely constant across data distributions, it might be possible to fine-tune models only in later layers to improve their OOD generalization capabilities."}, {"title": "Discussion", "content": "This study fills critical gaps in our understanding of convergent learning, offering a comprehensive analysis of how representational alignment between independently trained networks varies across network depth, training, and distribution shifts. We systematically explored how different alignment metrics with varying levels of transformation invariance-capture representational similarities, providing a more nuanced view of convergent learning than previous work.\nDespite these insights, the study has important limitations. While we show that alignment emerges early during training, we do not quantify precisely when within the first epoch this convergence occurs. Further, different networks may learn at varying rates so epoch one for one network may not reflect an analogous state for another network. Comparing networks epoch-wise may overlook this difference in learning speeds. Nonetheless, since alignment at the end of the first epoch already matches the alignment seen at convergence, the broader conclusions about early representational alignment remain robust.\nAnother limitation stems from the alignment metrics themselves. Although our metrics reveal that alignment stabilizes quickly and does not improve significantly over training, this could reflect the limitations of the metrics rather than the absence of representational changes. Prior work (Bo et al., 2024) has demonstrated that certain alignment metrics may fail to capture subtle shifts in representations that are critical for task performance. It remains possible that more sensitive or alternative metrics could reveal gradual representational alignments over training that are invisible to the methods employed here.\nFinally, our method for computing alignment focuses on the center pixel of each feature map, enforcing a strict spatial correspondence between representations. This approach may underestimate alignment, especially in cases where two filters detect the same feature at slightly shifted spatial locations. While more flexible approaches such as incorporating spatial shifts into alignment computations could offer a more accurate view, they are computationally intensive and were beyond the scope of this work. However, prior research has shown that optimal spatial shifts in many convolutional layers are typically close to zero (Williams et al., 2021), supporting the validity of this approximation. Developing scalable methods that account for such spatial variability remains an important direction for future research."}]}