{"title": "Abstracted Gaussian Prototypes for One-Shot Concept Learning", "authors": ["Chelsea Zou", "Kenneth J. Kurtz"], "abstract": "We introduce a cluster-based generative image segmentation framework to encode higher-level representations of visual concepts based on one-shot learning inspired by the Omniglot Challenge. The inferred parameters of each component of a Gaussian Mixture Model (GMM) represent a distinct topological subpart of a visual concept. Sampling new data from these parameters generates augmented subparts to build a more robust prototype for each concept, i.e., the Abstracted Gaussian Prototype (AGP). This framework addresses one-shot classification tasks using a cognitively-inspired similarity metric and addresses one-shot generative tasks through a novel AGP-VAE pipeline employing variational autoencoders (VAEs) to generate new class variants. Results from human judges reveal that the generative pipeline produces novel examples and classes of visual concepts that are broadly indistinguishable from those made by humans. The proposed framework leads to impressive but not state-of-the-art classification accuracy; thus, the contribution is two-fold: 1) the system is uniquely low in theoretical and computational complexity and operates in a completely standalone manner compared while existing approaches draw heavily on pre-training or knowledge engineering; and 2) in contrast with competing neural network models, the AGP approach addresses the importance of breadth of task capability emphasized in the Omniglot challenge (i.e., successful performance on generative tasks). These two points are critical as we advance toward an understanding of how learning/reasoning systems can produce viable, robust, and flexible concepts based on literally nothing more than a single example.", "sections": [{"title": "1. Introduction", "content": "The ability of humans to acquire novel concepts after minimal exposure to examples is an important constituent of general intelligence. Humans have the remarkable ability to quickly abstract concepts and extrapolate from one or few provided examples (Lake et al., 2015), thereby allowing for efficient and adaptable learning and reasoning. On the contrary, most current machine learning (ML) architectures require large amounts of data to learn, massive numbers of parameters (e.g., GPT-3: 175B, AlexNet: 62.3M, VGG16: 138M), and in many cases pre-training or access to external data or trained models (Hendrycks et al., 2019; Han et al., 2021; L'heureux et al., 2017; Zhou, 2016). Hence, a key computational challenge is to understand how an intelligent system with minimal complexity and without reliance on external supports can acquire new concepts from highly restricted training data (Chollet, 2019).\nIn this paper, we approach the challenges of one-shot learning (Wang et al., 2020; Kadam & Vaidya, 2020) by developing a framework that can perform the classification and generative tasks defined by the Omniglot challenge of handwritten characters a testbed designed to promote the study of human-like intelligence in artificial systems (Lake et al., 2015; 2019). In the classification task, a single image of a novel character is presented and the aim is to correctly identify another instance of that character from a choice set of characters. In the generative tasks, the goal is to create new variants of characters that are indistinguishable from human drawings. While the classification task has received much attention, there has been limited success in the attempt to achieve both types of tasks with the same model (despite an emphasis on exactly this breadth of functionality in the Omniglot challenge).\nThe Omniglot challenge is a highlight of the emerging field of computational cognition. Unlike traditional machine learning research, the goal is not just to achieve high classification performance on novel test items given a set of labeled training data. The Omniglot challenge differs in ways that derive from perceived weakness in dominant ML approaches relative to human intelligence: 1) the ability to form a concept from a single training item rather than expansive training sets; and 2) the ability to form flexible, robust, and transparent/trustable concepts. Accordingly, the challenge focuses on one-shot learning under a broad, multi-task interpretation of inductive concept learning that asks proposed systems not only to classify new items, but also to successfully perform a set of generative tasks. The latter includes inventing viable new instances of an individual character in an alphabet, inventing viable new characters"}, {"title": "2. Prototypes in Human Concept Learning", "content": "Concepts are mental representations acquired through inductive learning processes that are used to categorize and reason about objects, events, and relational situations (Gregory, 2002). Psychologists have emphasized that concepts are formed and represented through the integration and combination of simpler parts known as features or attributes (Schyns et al., 1998; Farhadi et al., 2009). In the concepts and categories literature in cognitive psychology, a prominent account is prototype theory in which the mental representations of categories consist of a stored summary of the central tendency of feature values across observed members of a category (Rosch, 1973; Hampton, 2006; Posner & Keele, 1968; Minda & Smith, 2011). The prototype of a concept is a statistical average or an abstraction of all instances observed as members of a category. Categorization then operates as a matter of finding the best match among candidate prototypes which supports graded membership and classification decisions based on family resemblance rather than logical rules (Rosch & Lloyd, 1978; Mervis & Rosch, 1981). This approach has contrasted sharply in the psychological literature with exemplar-based similarity approaches that eschew explicit abstraction in favor of storing labeled instances (Nosofsky, 1988; 1986). The main advantage of prototype theory is its broad computational and psychological plausibility: prototypes are economical in terms of representational and processing requirements, and they capture the intuition that the generic meaning underlying a category is represented explicitly and independently of its members.\nIn our framework, we adapt a form of prototype theory to address the Omniglot challenge. Prototypes are not necessarily formed by storing the centroid of category members but can instead be realized implicitly through statistical learning or neural network architectures like simple linear associators or auto-associative systems which adapt their weights to capture the statistical regularities of a domain (Biehl et al., 2016; Rosenblatt, 1958; Ruck et al., 1990). To be clear, prototypes are nearly always a matter of abstracting a summary representation or model from individual cases. By contrast, our approach to one-shot learning transforms a single example into a prototype by taking a particular configuration of pixels as the basis for a set of probabilistic clusters that imply an underlying distribution with a generative capacity. The crux of our approach lies in the formation of an ensemble of augmented subparts achieved by using the parameters inferred from the Gaussian components of the GMMs. The abstracted Gaussian prototype is an abstraction from a single case in order to create its own subparts via clustering, establish the distributional central tendency and variability of its subparts, inherently capture spatial relational properties between the subparts, and further populate the prototype with distribution-consistent generated pixels. The result of this process can be seen as similar to a summary representation of the central tendency across varying training examples with the additional quasi-structural aspect of an underlying model that captures the form and location of subparts (without any explicit propositional or symbolic representation).\nJust as the prototype theory of human categorization relies on similarity to the stored prototype in order to determine likelihood of category membership, we invoke a psychological similarity metric for classification relative to AGPs. Tversky's (1977) model of similarity proposed that individuals assess similarity by considering the number of featural differences and commonalities between items along with an additional design principle of highlighting (via greater weighting) the importance of the differences. The Tversky index is given by:\n$T(A, B) = \\frac{|A \\cap B|}{|A \\cap B + \\alpha|A\\backslash B| + \\beta|B\\backslash A|}$ (1)\nwhere | A\u2229 B is the size of the intersection of sets A and"}, {"title": "3. Related Works", "content": "3.0.1. ONE-SHOT CLASSIFICATION:\nMany neural-based models have been successful at one or few-shot classification tasks (Finn et al., 2017; Santoro et al., 2016; Salakhutdinov et al., 2012). For instance, Siamese Neural Networks involve twin sub-networks sharing the same parameters that are trained to learn embeddings capturing the similarity or dissimilarity between pairs of instances (Koch et al., 2015; Chicco, 2021). Another approach through Prototypical Networks learn a representative prototype for each class based on the mean of the embeddings in the latent space and classify according to these distances (Snell et al., 2017). Similarly, Matching Networks work by employing an attention mechanism on embeddings of the labeled set of instances to forecast classes for unlabeled data (Vinyals et al., 2016). However, all of these neural-based classification approaches require an initial training phase for the network to learn a general understanding of the task. Furthermore, they are incapable of addressing generative tasks. Our proposed approach, on the other hand, offers a direct way for both classification and generative tasks to learn specific concepts from one, and only one, shot without background training on other data.\n3.0.2. ONE-SHOT GENERATION:\nOne recent approach introduces GenDA for one-shot generative domain adaptation using pre-trained Generative Adversarial Networks (Yang et al., 2023; Goodfellow et al., 2014). GenDA designs an attribute classifier that guides the generator to optimally capture representative attributes from a single target image, in turn, synthesizing high-quality variant images. However, this approach relies on source models that are pre-trained on large-scale datasets such as FFHQ and Artistic-Faces dataset.\n3.0.3. BAYESIAN MODELS:\nSignificant progress has been made to address both one-shot classification and generative tasks through Bayesian implementations, such as the Object Category Model (Fei-Fei et al., 2006) involving parametric representations of objects and prior knowledge when faced with minimal training examples. These Bayesian principles are manifested in the approach proposed by the original authors for the Omniglot dataset (Lake et al., 2011). In this system, a stroke model learns part-based representations from previous characters to help infer the sequence of latent strokes in new characters. An extension of this work introduces Bayesian Program Learning (BPL), which learns a dictionary of sub-strokes and probabilistically generates new characters by constructing them compositionally from constituent parts and their spatial relationships (Lake et al., 2015). BPL and the stroke model, however, requires the model to learn from stroke-data trajectories in order to extract and store a dictionary of primitive parses at the sub-stroke level. While this model first requires information from live-drawings, the approach may be unfeasible when temporally labeled sequential stroke data is not accessible. In contrast, our approach uses generative image segmentation to directly infer the sub-strokes of the characters using GMMs which allows our model to learn purely from a single raw image.\nIn essence, existing approaches rely on either pre-training and/or built-in representational systems that fall outside of the scope of our stricter interpretation of the challenge; and many do not address the breadth of tasks in Omniglot. For instance, (Rezende et al., 2016) involves pre-training as part of its methodology to achieve one-shot generalization, which is a common approach in deep generative models to understand and generalize from very few examples. In (Edwards & Storkey, 2016), the concept of the neural statistician implies learning statistical representations from data, which also involve pre-training to capture the statistical properties across different contexts or datasets. In (Liang et al., 2022b), given its neuro-symbolic approach, this paper uses built-in knowledge/representational languages which involves more explicit and expensive forms of knowledge representation. Finally, in (Giannone & Winther, 2022; Antoniou et al., 2017; Boutin et al., 2022), these papers use highly complex methodologies and low transparency architectures like GANS and hierarchical approaches that attempt to model data at multiple levels of abstraction. While the performance of these works offer solutions to few shot learning, our work attempts to capture a different focus. Our system is trained on nothing except the presented single characters for each task."}, {"title": "4. Background", "content": "In this section, we provide the mathematical background underlying GMMs and VAES.\n4.1. Gaussian Mixture Models\nA GMM is a probabilistic clustering model that assumes the data is generated from a combination of multiple Gaussian distributions (Duda et al., 1973). Each Gaussian component"}, {"title": "5. Approach", "content": "In this section, we describe and formalize our approach to the classification and generative tasks in the Omniglot Challenge. The Omniglot dataset consists of 1623 hand-written characters taken from 50 different alphabets, with 20 examples for each class (Lake et al., 2019). All code, question sets, and data can be accessed at https://github.com/bosonphoton/ AbstractedGaussianPrototypes\n5.1. Classification Task\nIn a one-shot classification task, there is a set of N classes, denoted as C = {C1, C2, ..., CN }, and one available instance"}, {"title": "5.1.1. ABSTRACTED GAUSSIAN PROTOTYPE (AGP) GENERATION", "content": "Each instance of a concept is provided as a binary image of pixels. Under the probabilistic framework of a GMM, let us define each sampled pixel as the realization of a random variable, characterized by its PDF corresponding to the inferred Gaussian component. Each instance in X, along with q, is first segmented into its unique component subparts using a GMM, where G = {91, 92, ..., gk} represents the set of different subparts in each instance and k is a hyperparameter controlling the number of components. Here, G represents the mixture of Gaussian components of each instance, which allows the GMM to sample from the fitted distribution for each component gi and generate new augmented subparts pi. We define the ensemble of these subparts as the prototype P of the class, where P = {P1, P2,..., Pk}."}, {"title": "5.1.2. SIMILARITY METRIC", "content": "The similarity metric is computed between the query prototype Pq and each prototype of the available instances Pi. Each prototype P is the entire set of its 2D image coordinates, generated by the sampled components in a GMM. To simplify the notation, let A be the set of all image coordinates for Pq, and B be the set of all image coordinates for Pi. We base our similarity metric on the Tversky index with the following equation:\n$S(A, B) = |A\\cap B \u2013 \\beta |A\\Delta B|$ (12)\nwhere A\u2206B = (A\\B)U(B\\A) is the symmetric difference representing the non-intersections and \u03b2 > 1 is a weight"}, {"title": "5.2. Generative Tasks", "content": "The primary generative tasks in the Omniglot challenge are as follows:\n1. Generating new exemplars of a particular class.\n2. Generating new classes consistent with a particular alphabet given a starting set.\n3. Generating entirely new classes (unconstrained).\nFor Task (1), only a single instance is used for the entire task. For Task (2), one instance per class from a given alphabet is allowed. Following (Lake et al., 2019), we use ten different"}, {"title": "5.2.1. AGP TRAINING SET", "content": "The approach from section 5.1.1 is used to generate more AGPs for each concept to synthetically increase the size of a training set \u03a6 = {\u03a61, \u03a62, ..., \u03a6\u039d}. This training set consists of a larger set of AGPs where \u0424\u2081 = {P1, P2, ..., PD}, containing D new variants for each class, specified with a different value of k components. An equal number of variants is generated for each value of k, so that there are D/K variants for k \u2208 K. In our pipeline, we generate D = 500 AGPs per class with K = {6,7,8,9 10} components."}, {"title": "5.2.2. VAE INTERPOLATION", "content": "After generating the prototype training sets \u03a6\u2081 for each class i \u2208 N, the next step is to create continuous variations amongst these prototypes. To accomplish this, a VAE is trained across \u03a6 = {\u03a61, \u03a62, ..., \u03a6} which is the enumer-"}, {"title": "5.2.3. \u03a4\u039f\u03a1OLOGICAL SKELETON REFINEMENT", "content": "The final step in this pipeline is a post-processing technique based on the work of (Lee et al., 1994; Zhang, 1997) on topological skeletons. Skeletonization is used often in image processing and computer vision to reduce the thickness of binary objects to one-pixel-wide representations while preserving the topological properties of objects. This step refines the reconstructed output images generated by the VAE and emphasizes the stroke-like properties of Omniglot characters. After each reconstructed image from the VAE is skeletonized, the final result is a collection of generated variants of characters for each task."}, {"title": "6. Results", "content": "6.1. Classification Tasks\nThe classification accuracy of the proposed approach is evaluated based on the number of correct responses averaged over 1000 trials in both 5-way and 20-way one-shot tasks. We test this in the context of unconstrained classes independent of alphabets, as well as a more challenging within-alphabet classification task. We compared our one-shot classification approach using the Tversky-inspired similarity metric to a baseline euclidean distance metric calculated by the standard mean squared error (MSE). Given our simple binary image stimuli, we found MSE as the most relevant baseline over other metrics like cosine similarity, which evaluates similarity between high-dimensional embeddings and feature vectors, and structural similarity index (SSIM), which unnecessarily considers texture, luminance, and contrast not present in line-based binary strokes. Results are"}, {"title": "6.2. Generative Tasks", "content": "A \"visual Turing test\", as described in (Lake et al., 2013) is used to assess the quality of the generative outputs of the model. In this test, a set of characters produced by a human is displayed next to a set produced by the model. Human judges then try to identify which set was drawn by a human, and which set was generated by the model, see Figure 2 and Appendix A. Our generative approach is evaluated based on the identification accuracy of the 20 human judges recruited online. The ideal performance is 50 percent, indicating that the judges cannot distinguish between characters produced by the human and the model, and the worst-case performance is 100 percent. Ten question sets with four instances from the human and four instances from our model were created for each of the three tasks (total of 30 sets). Additionally, we asked follow up questions after displaying each set of images to probe whether the machine's outputs could potentially surpass the quality of human generated characters. These questions were phrased as the following: (1) \u201cWhich set represents a better job of making four new examples of the given character?\u201d, (2) \u201cWhich set represents a better job of making four new characters that fit the given alphabet?\u201d, and (3) \u201cWhich set represents a better job of creating four new characters?\u201d\nGenerating New Concepts from Type For the evaluation of generating new characters belonging to an alphabet, the identification accuracy across judges was (M = 52.00%, SD = 14.73%. Min = 40.00%, Max = 90%). The preference for the machine-made was (M = 49.00%, SD = 15.18%, Min = 20.00%, Max = 80.00%).\nGenerating New Exemplars For the set of images corresponding to generating new exemplars of a particular class, the average identification accuracy across judges was (M = 57.50%, SD = 14.82%. Min = 30.00%, Max = 80%). The preference for machine-made in this specific task was (M = 59.50%, SD = 12.76%, Min = 30.00%, Max = 80.00%).\nGenerating New Concepts (Unconstrained) For the final task of generating entirely new concepts independent of alphabet, the identification accuracy across judges was (M = 47.50%, SD = 12.01%. Min = 30.00%, Max = 70%). The preference for the machine-made in this task was (M = 57.50%, SD = 12.93%. Min = 30.00%, Max = 80.00%).\nOverall Results for Generative Tasks The overall identifi-"}, {"title": "7. Conclusion", "content": "We present a novel approach for addressing the Omniglot challenge using Abstracted Gaussian Prototypes in conjunction with a VAE for generative tasks. AGPs leverage GMMs to build representative prototypes for each concept by abstracting the subparts of the single available instance for each class. The AGP functions as a generative image segmentation method applied to cluster the pixels of an image which offering a simple yet powerful method for encoding higher level representations that capture a quasi-structural model of 'what and where' for visual concepts from minimal data. One-shot classification is achieved by applying Tversky's set-theoretic similarity metric to compare AGPs. A novel AGP-VAE pipeline employs VAEs to utilize AGPs with different component numbers to generate diverse and compelling (i.e., consistent, yet original) variants that judges found indistinguishable from human drawings.\nWhile our approach presents promising contributions to one-shot learning, there are several limitations to be acknowledged. First, we acknowledge that these are not state-of-the-art results for one-shot classification; however, we are not aware of another approach that performs this well under the strict interpretation of one-shot learning as being fully from scratch. The classification performance should also be considered in the context of the model's excellent performance on the generative tasks in the challenge. In terms of potential wider impact of our approach, our method currently only handles line-based images with binary (on/off) pixels. The scalability to more complex natural images (i.e., ImageNet, CIFAR, etc.) would require new approaches to handle more features such as color. Furthermore, computational challenges from AGP generations and Tversky calculations can arise when working with non-binary images. Future research directions should aim to address these limitations and further refine the AGP framework for broader and more robust applicability in the field of one-shot learning.\nIn sum, we present AGPs as a novel approach that achieves high performance on one-shot classification task and proves impressively adaptability to the generative tasks of the Omniglot challenge. Critically, this degree and breadth of success is achieved without high model complexity, without slow and demanding computation, without lack of transparency, without the need for external pre-training, and without invoking a complex symbol system for explicit structural recoding. Ideal takeaways from this work include: (1) the value of the computational cognition framework for productive crosstalk between cognitive science and ML, (2) the potential for approaches that are intermediate in nature between the poles of the statistical and symbolic frameworks, (3) the future potential of the design principles of the abstracted Gaussian prototype and the AGP-VAE pipeline for 'truly' one-shot learning and generative tasks without having to first learn how to learn."}]}