{"title": "Global Outlier Detection in a Federated Learning Setting with Isolation Forest", "authors": ["Daniele Malpetti", "Laura Azzimonti"], "abstract": "We present a novel strategy for detecting global outliers in a federated learning setting, targeting in particular cross-silo scenarios. Our approach involves the use of two servers and the transmission of masked local data from clients to one of the servers. The masking of the data prevents the disclosure of sensitive information while still permitting the identification of outliers. Moreover, to further safeguard privacy, a permutation mechanism is implemented so that the server does not know which client owns any masked data point. The server performs outlier detection on the masked data, using either Isolation Forest or its extended version, and then communicates outlier information back to the clients, allowing them to identify and remove outliers in their local datasets before starting any subsequent federated model training. This approach provides comparable results to a centralized execution of Isolation Forest algorithms on plain data.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated learning (FL) is a machine learning paradigm where multiple parties collaborate to train a shared machine learning model without centralizing data at a single location [1]. During model training, data holders refrain from directly exchanging raw data; instead, they share model parameters such as gradients, weights, or other forms of processed information. This distributed learning paradigm is typically facilitated by a coordinating server, often referred to as the aggregator, which collects local contributions from data holders, commonly known as clients, and aggregates them to create a global model. Model training may adopt iterative schemes where an updated global model is sent to the clients at each iteration.\nThe applicability of FL spans diverse contexts, each driven by distinct needs. For instance, one common scenario involves a limited number of data-holding entities collaborating to train a global model without directly sharing their data, often for privacy reasons. This situation is known as the cross-silo scenario and is notably observed in highly regulated domains like biomedicine, where FL is expected to become a prevalent technology [2]. Conversely, another scenario involves a multitude of edge devices acting as clients. This is referred to as the cross-device scenario and is common in IoT deployments.\nIn this case, FL primarily aims to reduce the time and cost associated with centralized data transfer, while also addressing privacy needs.\nIt is worth noting that even though FL permits data owners to retain sovereignty over the usage of their own data, it does not inherently guarantee security. In several cases, information can be reconstructed about the data used in training from models or model parameters [3], [4]. Therefore, to ensure further privacy and security, various techniques such homomorphic encryption [5], secure multiparty computation protocols [6], or differential privacy [7] are commonly employed.\nSimilar to other machine learning models, FL training is susceptible to outliers or anomalies in data, which can detrimentally impact model performance. Furthermore, in a federated setting, outliers can be classified as local outliers, which are outliers for a given client, and global outliers, which are outliers overall. Across several domains, it is common to find examples of data points that are local outliers but not global outliers. For example, in the medical field, a given medical condition may be common in one region and rare in another [8]. Therefore, in a study conducted at a center located in a low-prevalence region, individuals suffering from that condition may appear as local outliers. However, if the center participates in a FL multicenter study including centers in areas where the condition is more common, those individuals would not appear as global outliers. In most cases, for the training of FL models, a consortium would be interested in discarding global outliers and retaining local ones.\nIn centralized environments, various strategies have been developed over the years for outlier detection using a wide range of techniques [9], [10]. These include statistical methods like z-score and modified z-score, distance-based algorithms such as k-nearest neighbors, density-based approaches like Local Outlier Factor (LOF) [11], tree-based models like Isolation Forest [12] and its variants, as well as deep learning approaches [13].\nDespite the ubiquity of outlier detection, only a limited number of solutions tailored for federated contexts exist. Furthermore, such solutions predominantly focus on the IoT cross-device setting, where anomaly detection is intrinsic, typically signaling device malfunctions or intrusions in IoT networks. Few solutions have been specifically designed for the cross-silo setting, where outlier detection serves mainly as a preprocessing step aimed at identifying and removing outliers to enhance the quality of subsequently trained FL models.\nIn this article, we introduce a methodology focused on outlier detection within a FL framework, using the Isolation Forest (IF) algorithm [12] or its Extended Isolation Forest (EIF) [14] variant. The method is designed for a cross-silo scenario, where two servers are present, and where clients hold data described by the same variables (i.e., horizontally partitioned data). In our approach, the principal server receives a masked version of the data that preserves the \"isolationness\" of outliers, conducts outlier detection on these masked data, and communicates results to the clients. Notably, thanks to a permutation procedure operated with the help of the auxiliary server, the principal server does not know to which clients the identified outliers belong.\nThe article is structured as follows: Section II examines current methods for outlier detection in a federated context, Section III provides a concise overview of the main algorithms and techniques used in our solution, Section IV presents our methodology, and Section V outlines the experiments conducted and presents the results. In Section VI, we delve into key aspects of our solution, particularly focusing on its security implications. Finally, Section VII offers concluding remarks and discusses potential extensions of the method to other contexts."}, {"title": "II. RELATED WORK", "content": "Federated outlier detection is increasingly leveraged in Internet of Things (IoT) systems in order to ensure reliable and timely identification of anomalies, while maintaining data privacy and network efficiency. Indeed, a federated approach not only preserves the privacy of individual devices but also reduces the need for extensive data transfer, minimizing latency and bandwidth usage. In particular, federated deep learning techniques, based, e.g., on long short-term memory (LSTM), gated recurrent units (GRUs) and convolutional neu-ral networks (CNNs), have been recently proposed to predict intrusions in IoT networks [15]\u2013[17] or device failures [18].\nA recent work, also targeted at the IoT domain, implements a federated version of IF [19], where a global isolation tree is built from local encrypted contributions after the use of differential privacy locally.\nAs observed in the introduction, outside of the IoT domain, there are very few examples of methods specifically developed for cross-silo scenarios, where outlier detection mainly con-stitutes a preprocessing step for the subsequent training of a FL model. A notable example is a privacy-preserving version of LOF [20], which was developed prior to the introduction of the term FL."}, {"title": "III. BACKGROUND", "content": "In this section, we provide a brief overview of the main characteristics of both IF and EIF, as well as a short review of the homomorphic encryption Paillier cryptosystem, which we use a few times in our work. For an in-depth analysis and comparison of the IF and EIF, we refer the reader to [21]. It is worth mentioning that, in addition to EIF, there exist several other extensions or improvements of the IF algorithm, such as SA-IForest [22], E-IForest [23], and LSHIForest [24]. For the sake of simplicity, in this work, we decided to focus on the original algorithm and on its most well-known extension, as they both remain widely used."}, {"title": "A. Isolation Forest", "content": "The IF algorithm is based on the principle that anomalies (outliers) are easier to isolate than normal data points (inliers). The algorithm consists of two different phases: a training phase, which builds the forest, and a scoring phase, which assigns each data point an outlier score.\nDuring the training phase, a forest of t binary trees is created, with each tree using a different set of $\\psi$ data points randomly selected from the entire dataset. For every tree, the algorithm starts with all the $\\psi$ data points in a root node, and then randomly chooses a direction for splitting (i.e., horizontal or vertical), creating a split within the range defined by the minimum and maximum values assumed by the data points. This creates two child nodes in the tree, where the data points are stored. The same procedure is repeated iteratively for each of the nodes. The splitting of a given node stops if there is a single data point in the node or if a maximum tree depth parameter ($\\log_2(\\psi)$) is reached. Nodes without any child nodes are called external nodes, whereas the others are called internal nodes.\nDuring the scoring phase, every tree in the forest evaluates each data point in the dataset, assigning each data point to an external node within that tree. Subsequently, the distance (path length) between the external node containing the data point and the root node is computed for each data point and for each tree. These distances are then averaged across all trees for each data point, yielding an average path length that characterizes each data point. Outlier scores are then calculated based on the average path lengths, with shorter average path lengths resulting in higher outlier scores."}, {"title": "B. Extended Isolation Forest", "content": "There exist scenarios where the IF algorithm fails to produce satisfactory results, particularly when the data has symmetries like rotational symmetries, which are not reflected in the outlier scores. To address these limitations, the Extended Isolation Forest (EIF) was developed. The difference between IF and EIF resides in the training phase, whereas the scoring phase is identical. In EIF, the splits are not performed by means of horizontal or vertical hyperplanes, but by selecting a random hyperplane from the set of all possible ones. It is worth noting that, despite the more general approach, EIF does not systematically outperform IF [21]."}, {"title": "C. Paillier cryptosystem", "content": "We briefly recall the main properties of a cryptosystem, with a focus on the Paillier cryptosystem. Every cryptosystem comprises three fundamental algorithms:\n\u2022 The key generation algorithm $(sk, pk) = Gen(keysize)$, which generates a secret key sk and a public key pk based on a security parameter keysize.\n\u2022 The encryption algorithm $[x]_{pk} = Enc(x, pk)$, which maps a plaintext x to a ciphertext $[X]_{pk}$ using the public key pk.\n\u2022 The decryption algorithm $x = Dec([x]_{pk}, sk)$, which reverses the encryption process using the secret key sk to recover the original plaintext x.\nPlease, note that in the following we will always explicitly indicate the key used for encrypting a given plaintext. This is essential because our pseudocodes involve the use of multiple key pairs, and not indicating the key could lead to ambiguities.\nThe Paillier cryptosystem constitutes an additive partially homomorphic cryptosystem. Let $x_1$ and $x_2$ be two plaintexts. Homomorphic addition is achieved through the product of two ciphertexts, namely it is defined as $[x_1]_{pk} \\cdot [x_2]_{pk} := [x_1]_{pk} \\cdot [x_2]_{pk}$, and ensures that $[x_1]_{pk} \\cdot [x_2]_{pk} = [x_1 + x_2]_{pk}$. Moreover, homomorphic multiplication by a plaintext is achieved through exponentiation, is defined as $x_1^{[x_2]_{pk}} := [x_2]_{pk}^{x_1}$, and ensures that $x_1^{[x_2]_{pk}} = [x_1x_2]_{pk}$."}, {"title": "IV. METHODOLOGY", "content": "We propose a methodology centered around two servers: the principal server P and the auxiliary server A. The distinction between these servers lies in the amount of information to which they have access as well as in their roles. The principal server P receives a masked version of client data and carries out the main task of outlier identification, and communicates results to clients. Through the knowledge of outlier scores, this server is aware of the presence and extent of outliers in the data. Conversely, the auxiliary server A only receives noise or encrypted information and does not have visibility on any information regarding the data apart from knowing the total number of data points involved in the process. Its presence primarily has the role of enhancing the security of the process. Alongside these servers, there are m clients, indexed from 0 to m \u2212 1, with the i-th client denoted as $C^i$. In the process, the two servers communicate with all the clients and between themselves, whereas no direct communication channel among clients is in place. For this reason, the indexing of clients is purely a mathematical notation, not associated with any actual order among of them: client $C^i$ does not know the identity of client $C^{i-1}$ nor of client $C^{i+1}$, and each of the two servers uses its own indexing system.\nWe operate under the assumption that both servers act as semi-honest parties, adhering to the working protocol without collusion or malicious intent to disrupt or poison the data. However, they may attempt to infer information about the original data from the information that they receive during the process. Regarding the clients, we assume their adherence to the protocol without attempts to undermine it, but we can partially relax the assumption of non-collusion, as elaborated in Section VI.\nOur methodology consists of several preliminary steps followed by a main process. The preliminary steps focus on generating a set of integer values that will govern the main process phase, and on generating a set of matrices, mainly used to transform or mask the data. All of these operations are based exclusively on the use of randomly chosen values or metadata. In the main process, the actual data processing occurs, including the task of outlier detection. The full workflow is summarized in Algorithm 3, where references to the subsections describing the specific steps are also provided.\nIn this section, we use $\\delta_{ij}$ to denote the Kronecker delta and the shorthand notation $Z_k = \\{0, 1, 2, ..., k \u2212 1\\}$ to denote the set of integers modulo k. The notation $A = \\{A_i\\}_{i\\in Z_k}$ represents an ordered set of k elements, indexed from 0 to k-1, where $A_i$ is the i-th element of the set, and $\\|A\\|$ is the cardinality of the set. Additionally, $W_{j.}$ denotes the j-th row of a matrix W, with rows indexed starting from 0 rather than 1. Note that, for the sake of clarity, we use apices only for indices associated to clients: e.g. $N^i$ denotes the local sample size of client $C^i$."}, {"title": "A. Preliminary steps - integers generation", "content": "Initially, each client $C^i$ generates a key pair $(pk^i, sk^i) = Gen(keysize)$ within Paillier cryptosystem, shares the public key $pk^i$ with all the members of the consortium, and keeps the secret key $sk^i$ private. Please note that even though the Paillier cryptosystem is a key element of the method, it is used only a limited number of times and on a limited number of integers in order to minimize the computational overhead.\nNext, the clients collectively agree on an integer number $\\Xi$, unknown to the two servers. This integer is of great importance as it will serve as a global seed for conducting operations that involve random generations, ensuring consistency across all clients. The consensus on $\\Xi$ is achieved through Algorithm 1, based on the Paillier cryptosystem, which clients execute together with the auxiliary server A.\nSubsequently, the clients, together with either of the two servers, jointly calculate the total number of data points involved in the process, N, which then becomes public to all parties. It is worth noting that our methodology does not require any server to know the individual number of data points held by each client; only the total number of data points is necessary for the method to function. This is a key aspect of the approach, as knowing the number of data points a client holds before and after the process would reveal how many outliers the client discarded. This step can be executed using any secure sum protocol according to the preference of the parties involved. Algorithm 1 can also be used for this purpose by using the local sample sizes $\\{N^i\\}_{i\\in Z_m}$ as inputs instead of randomly generated numbers, and it does so without the need of establishing direct communication among clients.\nIn the subsequent step, clients collaborate with the auxiliary server A so that each client $C^i$ is assigned a set of non-consecutive integers, denoted as $\\tilde{Z}^i$, which satisfies $\\|\\tilde{Z}^i\\| = N^i \\forall i \\in Z_m$, $\\tilde{Z}^i \\cap \\tilde{Z}^j = \\emptyset \\forall i, j \\in Z_m$ such that $i \\neq j$, and $\\cup_{i\\in Z_m} \\tilde{Z}^i = Z_N$. This is achieved following Algorithm 2, which also makes use of the Paillier cryptosystem. The algorithm first assigns to each client $C^i$ a starting point $s^i$, such that $s^i + N^i = s^{i+1} (mod N)$, with A introducing an offset H > 0 so that $s^0 \\neq 0$. Please recall that, as already observed, $C^i$ does not know the identity of the client who receives $s^{i+1}$, as the indexing is only known to A. Then, each client permutes the full set of integers in $Z_N$ using a previously agreed-upon permutation function, with the global seed $\\Xi$, ensuring that they all permute the numbers in the same way. Each client $C^i$ selects the integers in positions $\\{s^i mod N, ..., (s^i + N^i \u2013 1) mod N\\}$ in the permuted set, thus creating the sets $\\tilde{Z}^i$. These sets consist of non-consecutive integers, are non-intersecting, and their union covers the entire set of integers $Z_N$. The sets $\\tilde{Z}$ will be used by clients to conduct operations on different rows of an N-row matrix, ensuring both that a client does not use a block of consecutive rows and that each client uses distinct rows.\nAt the conclusion of these steps, the servers have knowledge of N, and the i-th client $C^i$ of N, $\\Xi$, $\\tilde{Z}^i$."}, {"title": "B. Preliminary steps matrices generation", "content": "All clients generate the same real invertible matrix M locally, out of the product of three matrices. Specifically, each client uses the integer $\\Xi$ as a seed to generate both an orthogonal matrix Q and a diagonal invertible matrix S, and the seed $\\Xi$+1 to generate another orthogonal matrix Q'. The matrix M is then calculated as M = QSQ'. Various tools are available for generating orthogonal matrices, such as the pracma [25] package in R and the scipy.stats module in Python. For the invertible diagonal matrix S, values are generated uniformly in the interval (1,T), where T is a specified threshold greater than 1. The lower bound 1 is chosen so that the matrix is numerically far from singularity. We call M the masking transformation, as clients will use it as a multiplicative mask for their data.\nNext, each client creates a noise matrix $R^i$ locally, with as many rows as the total number of data points N and as many columns as the number of variables D. Clients will sum their data points to rows of $R^i$, using it as an additive mask. To create $R^i$, clients sample matrix elements independently from a Gaussian distribution $N(0, \\sigma^2)$, with $\\sigma^2 = 10^{12}$. In this, we follow the same approach as in [26]. There, the authors use a framework [27] with a two-server topology analogous to the one presented in this article and use Gaussian noise to mask data too. As the authors mention, the choice of $\\sigma^2 = 10^{12}$ should be reasonable in a variety of situations and could be modified according to specific needs.\nAt the conclusion of these steps, the i-th client $C^i$ has generated the matrices M and $R^i$."}, {"title": "C. Main process data masking and transfer", "content": "Once all the preliminary steps have been completed, the parties can start the main process, which makes use of all the integers and matrices generated in the preliminary steps.\nAs a first step, each client multiplies their local data $X^i$ by the matrix M, obtaining $X^i = X^iM$. It is important to note that the matrices are applied to the right of $X^i$, since data points are described by rows of the data matrices. Then, each client creates a matrix $W^i$ by summing the rows of $X^i$ to specific rows of $R^i$, namely to those with indices correspond-ing to the integers in $\\tilde{Z}^i$. As clear from the observations in Section IV-A, each client sums their own data to different rows of their noise matrix. Then, clients send $R^i$ to the auxiliary server A and $W^i$ to the principal server P. A calculates $R = \\sum_{i\\in Z_m} R^i$, whereas P calculates $W = \\sum_{i\\in Z_m} W^i$.\nAt this point, the server A sends the aggregated noise matrix R to P. The server P can denoise their data and obtain the masked data matrix $X_{masked} = W - R$. This is a matrix where each row corresponds to a data point, and all data points have undergone the transformation M (which is unknown to P).\nNotably, thanks to the permutation operated with Algorithm 2, P does not know to which client belongs any data point. Moreover, data points belonging to same client are (in general) not stored in adjacent rows of the matrix."}, {"title": "D. Main process outlier detection and results communication", "content": "The principal server P can now apply an outlier detection method, such as IF or EIF, to the masked data matrix. As an output of this step, the server obtains an outlier score for each data point involved in the process (even though, thanks to the permutation in Algorithm 2 it does not know to which client a given outlier score is associated).\nIf the consortium agrees, P could send the entire vector of scores to all clients, so that each client gains knowl-edge of the overall outlier landscape. The clients could then evaluate the outlier scores of their data points (since they know at which elements of the vector they are stored) and autonomously decide whether to discard them. Otherwise, in a more conservative approach, P could make the choice of which data points to qualify as outliers, so that they get discarded. It could then send to the clients only the equations of hyperplanes characterizing the regions of the \"masked space\" where outliers lie. In this way, each client could check whether any of their points lies in such a region and, if so, discard them."}, {"title": "V. EXPERIMENTS", "content": "We tested our approach by conducting outlier detection on several different well-known datasets, which we downloaded from [28]. These datasets include entries manually labeled as true outliers and are commonly used as benchmarks for testing and comparing outlier detection methods. They were selected to provide a good variety of scenarios, including varying sizes, numbers of variables, and percentages of outliers."}, {"title": "VI. DISCUSSION", "content": "From a geometrical point of view, the masking matrix M = QSQ' represents a composite transformation encoding two rotations and a scaling. However, this is not a special characteristic of M, as any real matrix can be decomposed into a product commonly denoted as UDVT, where U and V are real orthogonal matrices and $\\sum$ is a real non-negative diagonal matrix. This is, in fact, the Singular Value Decomposition (SVD) of the matrix, whose existence is guaranteed by an existence theorem. The reason why we decided to build the matrix Mas a product of matrices instead of randomly generating it element-wise is that, in our approach, we can generate it in a controlled manner. In particular, by choosing the elements of S in the interval (1,T), we can ensure that M is far from singularity. However, the QR decomposition required for generating the random orthogonal matrices is the operation with the most significant impact on the method's runtime, with a time complexity of O(n\u00b3) [30].\nIt is interesting to observe how the different elementary transformations composing the masking transformation con-tribute to obfuscating different aspects of the data (while not altering the isolation status of outliers). Anisotropic scaling alters the absolute and relative distances among data points, as well as densities, correlations, norms of data vectors, and the singular values of the data matrix (which rotation preserves), while rotation alters the rankings between data points with respect to a given axis of the coordinate system (which anisotropic scaling preserves). Moreover, the rotations and scaling operated by the masking transformation essentially generalize the split directions in IF, thus making its use substantially equivalent to the use of EIF, and explaining what observed in Figure 2 regarding their equivalence in the multiparty approach."}, {"title": "B. Collusion among parties", "content": "There are several possible collusion scenarios that we shall discuss, involving different kinds of parties and with different levels of criticality.\nIn the case where up to m \u2212 2 clients collude, they cannot univocally identify the owner of the data point associated with a given outlier score. However, if m \u2212 1 clients collude, they can trace which outlier scores belong to the m-th client.\nIf the auxiliary server colludes with one or more clients, the clients do not gain any additional information. In fact, thanks to the homomorphic encryption used in Algorithm 2 for creating the starting points $\\{s^i\\}_{i\\in Z_m}$, the auxiliary server does not know the starting points of the different clients, and therefore it cannot associate an outlier score to the owner of the associated data point. Similarly, if the two servers collude, the principal server does not gain any additional information.\nThe most critical situation arises if the principal server colludes with one or more clients. In such a case, the colluding parties can reconstruct the entire data matrix. However, thanks to the use of homomorphic encryption for assigning starting points, they cannot determine to which specific client the individual data points belong. The assumption of a trusted principal server is therefore the most important one, and constitutes the main limitation of the method. However, there is a wide range of situations where this assumption can be con-sidered reasonable (e.g., principal server belonging to a well-known and reputed academic institution, non-governmental organization or foundation)."}, {"title": "C. Strategies for further privacy", "content": "To enhance privacy further, clients could implement strate-gies that modify their local datasets without compromising the identification of outliers. For instance, clients could execute IF locally to assess the extent to which their data points can be considered as local outliers. Subsequently, they could focus on data points with low outlier scores and employ strategies to modify them. For example, they might use downsampling, uni-formly excluding data points with outlier scores below a given threshold. Alternatively, they could introduce new instances by leveraging techniques such as SMOTE [31]. SMOTE can effectively generate synthetic instances that are interpolations of the original ones, thus adding data points without altering the isolation pattern of true outliers. A coupled use of IF and SMOTE has already been proposed in [32], even though with a different scope. Clients could even consider fabricating fake outliers to obfuscate the true number of outliers from the server. However, we caution against this approach due to the potential risks it poses to the integrity of the process."}, {"title": "VII. CONCLUSION AND OUTLOOK", "content": "We have presented a technique for identifying global out-liers in a Federated Learning setting using a two-server approach. The clients provide one of the two servers with masked data, where the masking preserves the ability to identify outliers while maintaining privacy. The server ap-plies an outlier detection method, such as Isolation Forest or Extended Isolation Forest, and then communicates the results to the clients either by providing outlier scores or indicating regions of the masked space from which data points should be discarded. Our tests on various datasets show that the performance is comparable to traditional methods applied to unmasked data at a single site.\nIt would be interesting to test the proposed masking scheme with other outlier detection algorithms. We initially tested it with the Isolation Forest algorithm (and its extended ver-sion), as it seemed the most natural choice for a masking transformation enforcing a scaling and two rotations. While we expect the method not to provide satisfactory results when combined with density-based algorithms (as density is altered by the transformation), it could potentially yield good results with other classes of algorithms. Moreover, it would be interesting to evaluate whether the same masking scheme permits achieving other tasks, such as batch effect detection, as we expect the \"batchness\" of data to be preserved by the transformation."}]}