{"title": "SELF-SATISFIED: AN END-TO-END FRAMEWORK FOR SAT GENERATION AND PREDICTION", "authors": ["CHRISTOPHER R. SERRANO", "JONATHAN GALLAGHER", "KENJI YAMADA", "ALEXEI KOPYLOV", "MICHAEL A. WARREN"], "abstract": "The boolean satisfiability (SAT) problem asks whether there exists an assignment of boolean values to the variables of an arbitrary boolean formula making the formula evaluate to True. It is well-known that all NP-problems can be coded as SAT problems and therefore SAT is important both practically and theoretically. From both of these perspectives, better understanding the patterns and structure implicit in SAT data is of significant value. In this paper, we describe several advances that we believe will help open the door to such understanding: we introduce hardware accelerated algorithms for fast SAT problem generation, a geometric SAT encoding that enables the use of transformer architectures typically applied to vision tasks, and a simple yet effective technique we term head slicing for reducing sequence length representation inside transformer architectures. These advances allow us to scale our approach to SAT problems with thousands of variables and tens of thousands of clauses. We validate our architecture, termed Satisfiability Transformer (SaT), on the SAT prediction task with data from the SAT Competition (SATComp) 2022 problem sets. Prior related work either leveraged a pure machine learning approach, but could not handle SATComp-sized problems, or was hybrid in the sense of integrating a machine learning component in a standard SAT solving tool. Our pure machine learning approach achieves prediction accuracies comparable to recent work, but on problems that are an order of magnitude larger than previously demonstrated. A fundamental aspect of our work concerns the very nature of SAT data and its suitability for training machine learning models.", "sections": [{"title": "1. INTRODUCTION", "content": "Boolean satisfiability (SAT) is a paradigmatic example of an NP-complete problem [2] that has been widely studied for both its theoretical importance in complexity theory and its practical applications. From the perspective of machine learning, SAT also presents a compelling challenge. Even in its simplest form as a learning problem, so-called SAT prediction, which amounts to a binary classification task, extraordinary richness of the data presents a number of intriguing technical obstacles to be overcome. First, because SAT is NP-complete, generating labeled data for hard problem instances is non-trivial. Indeed, it is well known in the complexity theory literature that an efficient generator for guaranteed hard problems would yield a proof that P = NP (see e.g. [18, 16]). Second, finding ways to scale SAT prediction to large problems, of the kind typically encountered in industry, using a pure machine learning approach requires a careful choice of architecture."}, {"title": "2. BACKGROUND AND RELATED WORK", "content": ""}, {"title": "2.1. Boolean Satisfiability.", "content": "In this section we define the problems SAT and UNSAT, describe why generating instances, consisting of a problem together with its correct label, of each might appear hard, why generating hard instances of each is hard, and previous work that creates datasets of SAT and UNSAT instances.\nAny expression built from the constants 0, 1, the operations $V$, $\\land$, $\\neg$ and variables is a boolean formula. A boolean formula $\\varphi(x_1,...,x_n)$ is satisfiable in case there are choices $a_1,...,a_n \\in {0,1}$, also known as a variable assignment, such that $\\varphi(a_1,...,a_n) = 1$. For example $\\varphi(x,y) := x \\lor \\neg y$ is satisfiable since (1, 1) is a witness in the sense that $1 \\lor \\neg 1 = 1$. A formula $\\varphi(x_1,...,x_n)$ is unsatisfiable in case for every choice $a_1,...,a_n$, $\\varphi(a_1,...,a_n) = 0$. For example $\\varphi(x) := \\neg(\\neg x \\lor \\neg \\neg x)$ is unsatisfiable.\nBoolean formulae are often represented in a simplified form known as conjunctive normal form (CNF). CNF formulae have the form $\\bigwedge_{i=1}^{m} (\\bigvee_{j=1}^{m_i} l_{ij})$ where each so-called literal $l_{ij}$ is either a variable or the negation of a variable. The disjunctions are called clauses (resp. k-clauses when $m_i = k$). This is a k-CNF if each $m_i = k$.\nThe Tseitin transformation [32] associates a CNF to any boolean formula with only polynomial overhead in a way that preserves satisfiability. The SAT classification task is the classification task of determining whether an arbitrary CNF is satisfiable or unsatisfiable 1, and is also the task we wish to predict using machine learning.\nSAT is a well known NP-complete problem [2]: given a CNF $\\phi$ and an assignment $a$, there is a polynomial time algorithm that checks if $\\phi(a) = 1$ and any NP problem can be transformed into SAT in polynomial time. UNSAT is a well known co-NP-complete problem: this means that given a CNF $\\phi$ and an assignment $a$, there is a polynomial time algorithm that checks if $a$ is a counterexample to purported unsatisfiability of $\\phi$ i.e. $\\phi(a) = 1$, and that the complement of any NP problem can be transformed into UNSAT in polynomial time. It is known that NP $\\supseteq P \\subseteq co-NP$; however, it is not known if either of the inclusions are strict nor is it known if NP and co-NP are distinct, and the challenge to characterize the strictness or lack thereof"}, {"title": "2.2. Applications of machine learning to SAT.", "content": "One of the first and most prominent applications of neural networks to SAT prediction was NeuroSAT [30], which used a graph neural network (GNN) architecture together with a CPU-bound data generation method that leverages an external SAT solver. Unlike NeuroSAT, we use a transformer architecture and a novel data generation method that is not tied to an external solver. Earlier work [5] similarly formulates SAT as a machine learning classification problem, though unlike our methodology it requires hand-crafted features extracted by the SATzilla [37] algorithm portfolio for SAT solving. Our approach, which learns features and heuristics directly from the data, automates the crafting of problem features and heuristics removing the need for experts in the field to invent such heuristics by hand. Other work such as NeuroCore [28] and NeuroComb [33] present hybrid approaches combining neural networks with traditional SAT solving algorithms. While these approaches are of interest, we are here concerned with the pure machine learning approach and the SAT prediction (not SAT solving) problem.\nRecent related work is that of Polu and Sutskever [25] in which a transformer language model [26] is applied to the task of automated theorem proving in the Metamath [21] formal environment. However, this work is focused on tactic-level, as opposed to low-level (our case), theorem proving and therefore does not apply to the kinds of constraint satisfaction tasks we address. Recent work similarly applies transformer architectures, in combination with a GNN embedding, to SAT prediction [31]. However, the approach in ibid. is limited to problems with a maximum of 60"}, {"title": "2.3. Transformer architectures for input sequence length reduction.", "content": "A transformer block typically maps a sequence of N input tokens to N output tokens. The self-attention portion of the transformer block performs pairwise comparisons between the tokens of the input sequence, requiring O(N\u00b2) memory and time complexity. This computational bottleneck has spawned research into techniques that reduce the length of the output sequence, either by modifying the self-attention algorithm directly [36, 3], via a learned heuristic for down-sampling output tokens [15, 17], or via down-sampling policies learned through evolutionary algorithms [17] or reinforcement learning [38]. Convolutional neural network (CNN) based vision architectures have long made use of pooling to down-sample feature maps [11], and similar approaches have been applied to Vision Transformer architectures to reduce patch size [12, 24], including to reduce the sequence length [23, 34]. These approaches, analogous to pooling in CNNs, rely on the notion of locality inherent in image processing to down-sample by combining local features. Our work makes no assumptions of locality and does not rely on notions of spatial relationships in the input sequence, making it more general."}, {"title": "3. METHODS", "content": ""}, {"title": "3.1. Variable space encoding.", "content": "We represent boolean formula in a geometric encoding we term the variable space encoding that allows us to relate boolean formulae to a computer vision problem and leverage recent developments in Vision Transformer architectures. We canonically represent any CNF in v variables and m clauses as an m \u00d7 v matrix, $\\gamma$, over {-1,0,1}. $L_{ij}$ is 0 if $x_j$ does not appear in the $i^{th}$ clause, is 1 if it appears positively and is -1 if it appears negatively.\nSimilar encodings of CNF have appeared frequently in the literature. If one turns all the occurrences of -1 to 1, then $\\gamma$ is the adjacency matrix for the bipartite literal incidence graph used in [35, 22]. It has also been directly studied as the labeled adjacency matrix for the bipartite factor graph of the CNF [8] 2. The novelty in our approach to this encoding is to regard it geometrically, by associating an image to such a matrix where each of -1,0,1 is mapped to a distinct color, turning each cell in the m \u00d7 v into a pixel. We can thus bring state-of-the-art Transformer-based Computer Vision models [7] to the task of predicting SAT/UNSAT by treating each row or column as a patch of the \"image\". It also gives rise to a visual presentation of the meaning of satisfiability."}, {"title": "3.2. Training Data: Labeled CNF generation.", "content": "Creating boolean formulae is easy; however, as explained earlier, creating boolean formulae with labels indicating whether they are satisfiable or unsatisfiable, comes with fundamental challenges. One cannot rely on generating formulae and then testing their satisfiability ex post facto. We require generating on the order of billions of labeled pairs of CNF, that have broad variability, and with variables and clauses scaling to the 1k and 10k levels"}, {"title": "3.3. Satisfiability Transformer (SaT).", "content": "We develop the Satisfiability Transformer (SaT) with inspiration from the Vision Transformer (ViT) [7], and adopt a similar"}, {"title": "3.4. Head slicing.", "content": "Inspired by the [class] token of BERT [6] and ViT [7], we additionally prepend a set of learnable [concept] tokens to each input sequence $x$ that we utilize for down-sampling in subsequent layers. Formally, for an input sequence $x = (x_1,x_2,...,x_n)$ where each $x_i$ is a vector embedding of dimension d such that $x_i \\in R^d$, we prepend a set $head$, containing one [class] token and m-many learnable [concept] tokens $c$, $head = ([class],c_1,c_2, \u2026\u2026\u2026,c_m) \\in R^d$, to produce"}, {"title": "4. RESULTS", "content": "It is common in the literature on machine learning for SAT prediction to both train and validate on data produced by the same generator, since prior work targets around 60 variables maximum and there does not exist a representative corpus of small, labeled CNF with so few variables (see e.g. [30, 31]). Because we are capable of training on much larger problems, we are able to validate against problems from the SATComp 2022 dataset, thereby obtaining a better assessment of how well our model generalizes and also ensuring it has not learned to merely exploit artificial problem features introduced by the data generator."}, {"title": "5. LIMITATIONS AND FUTURE WORK", "content": "In this work we have introduced the Satisfiability Transformer for SAT/UNSAT problem classification and validated it on a subset of the SATComp (SATComp) 2022 problem sets. While we have demonstrated a pure machine learning solution that can handle SATComp-sized problems an order of magnitude larger than previous work, evaluating all of SATComp would require scaling our approach another order of magnitude and na\u00efvely doing so would require a significant hardware commitment.\nThis work has also only demonstrated SAT/UNSAT classification, whereas generation of satisfying variable assignments in the case of SAT, and the UNSAT core in the case of UNSAT is not something that we consider here. Predicting the satisfying variable assignment and UNSAT core could be tackled with only minor modifications to our current architecture and training scheme, as our data generation"}, {"title": "6. CONCLUSION", "content": "We believe that by tackling hard problems such as SAT many insights into both machine learning practice and the nature of these problems can be gained. SAT and UNSAT are problems about a synthetic language with precise syntax and semantics where we can actually provide ground truth about the statements involved that are not open to interpretation. Many current problems involve some amount of reasoning; isolating the learnability of this reasoning may lead to new insights on bounds of what can be learned about reasoning in natural domains. The ideas we have presented include immediate benefits: new approaches to fast, hardware accelerated data generation for synthetic languages; new approaches to using computer vision models in domains related to optimization; and an end-to-end framework for learning hard problems in memory-efficient ways."}, {"title": "APPENDIX A. SAT GENERATOR PROOFS", "content": "In this section, we provide a proof of Proposition 1 and we also provide some exhibition of additional properties enjoyed by the SAT generator.\nWe stay that a literal l is underlied by a variable $x$ if $l = x$ or $l = \\neg x$.\nProposition (Proposition 1). The generator from Algorithm 1 produces only satisfiable formulae.\nBefore beginning the proof, we say that a literal $l$ agrees with an assignment $\\alpha = (\\alpha_0,..., \\alpha_n)$ if l(a) = True. Let the variable underlying l be $x_j$, the agreement of $l$ with $a$ can be made on cases; if $l = x_j$ is a positive variable then $a_j = 1$, and if $l = \\neg x_j$ is a negative variable then $a_j = 0$.\nProof. Suppose Algorithm 1 is run with inputs n, m, var, lits-clause, polarities, and we obtain $\\phi$ as output. To show that $\\phi$ is satisfiable, we must exhibit an assignment a with $\\phi(a)$ = True. Since $\\phi$ is a CNF, it then suffices to show that for each clause $C_i$, $C_i(a)$ = True. Then it suffices to find some literal $l_{io}$ of $c_i$ which agrees with a for each i.\nNote that asst generated in the first step of the algorithm gives us such an assignment, by changing all the occurrences of \u22121 to 0. To see this, the inner for loop sets the $j^{th}$ variable of $c_i$ to agree with $a$; if the $j^{th}$ element of seq is a 1. Then we are guaranteed that one literal agrees with $a$ as long as seq contains at least one 1; however, seq is chosen from a distribution of non-empty strings that contain at least one 1.\nNote that the satisfiable instance generator, Algorithm 1, is parameterized over four distributions; the vars distribution determines how likely each variable is to appear in the formula, the lits-clause distribution determines the likelihood of literals-per-clause in the formula, the polarities distribution determines a bias over 0,1 on the satisfying assignment that the formula will satisfy, and the polarity-bias determines a bias over how many literals in the clause will agree with the satisfying assignment (at least one must). These then allow generating formulae with a variety of distributional phenomena, for example completeness.\nThen,\nLemma 1. Suppose vars is the uniform distribution on m characters, lits-clause is the constantly k distribution, polarities is the Bernoulli distribution with parameter p = 0.5 and polarity-bias is the uniform distribution on the $2^k \u22121$ sequences of {0,1}$^k$ that contain at least one 1. Then GenerateSAT(n, m, vars, lits-clause, polarities, polarity-bias) will eventually generate every n-clause, m variable, satisfiable k-CNF."}, {"title": "APPENDIX B. UNSAT GENERATOR PROOFS", "content": "We call a clause full when each of the n variables appears in it.\nLemma. Suppose ResSearch is called on prob\u2081, vars, bloom, with prob\u2081 prob\u2082 unsatisfiable and where prob\u2081 has no full clauses. Denote by prob'\u2081 the returned problem. Then prob'\u2081 prob\u2082 is unsatisfiable.\nHere we use that a formula $\\phi$ is unsatisfiable iff $\\phi \\Rightarrow \\perp$.\nProof. Suppose we call ResSearch on $\\phi$, vars, bloom with $\\phi$ containing no full clauses, and obtain the result $\\phi'$. Since $\\phi$ has no full-clauses, every clause will split under ResSearch.\n$\\phi'$ is a formula in 2m clauses when $\\phi$ is a formula in m clauses. Denote the $i^{th}$ clause of $\\phi$, $\\phi'$ by $c_i$, $c'_i$ respectively. Then we will show that for i $\\in$ {1,...,m}, $c_{2i} \\wedge c_{2i-1} \\Rightarrow c_i$; hence $\\phi' \\Rightarrow \\phi$."}, {"title": "APPENDIX C. HYPERPARAMETERS", "content": "C.1. SaT training. We experimented with various hyperparameters during training, though we held many of the model hyperparameters constant across problem sizes. In particular, we use a learning rate of 1e-4, an embedding dimension of 32, 8 attention heads, prepend 31 [concept] tokens and 1 [class] token, and always perform head slicing after the first layer and keep only the first 32 output tokens. Our hyperparameters for SaT are informed by our desire to run our data generation and model training regime on a single NVIDIA Tesla V100S GPU, though in practice we distribute training across 4 GPUs and average the gradients in order to update the model. We realize our data generators and SaT in PyTorch and utilize the Distributed Data Parallel module for multi-GPU training.\nFor the problem size encountered during training, we select the number of variables uniformly from 4 up to a maximum number of variables being trained on. The minimum number of variables is chosen so that with the minimum clause to variable ratio, we can always generate unsatisfiable problems. The number of clauses selected depends on hyperparameters that determine the clause to variable ratio.\nThe algorithm for producing a satisfiable formula can be modified slightly to turn any formula into a new formula f' with the same literal incidence graph that is guaranteed to be satisfied by a chosen assignment. We realize this modification by skipping directly to the assignment of polarities, taking var-indices to be determined by the literal incidence graph of the starting problem. There is also a more biased algorithm for turning a formula into a satisfiable one with the same literal incidence graph. In this algorithm, we select one literal per clause and, if needed, change it so that it agrees with the assignment. Yet another related method proceeds in the"}]}