{"title": "EMBANet: A Flexible Efficient Multi-branch Attention Network", "authors": ["Keke Zu", "Hu Zhang", "Jian Lu", "Lei Zhang", "Chen Xu"], "abstract": "This work presents a novel module, namely multi-branch concat (MBC), to process the input tensor and obtain the multi-scale feature map. The proposed MBC module brings new degrees of freedom (DoF) for the design of attention networks by allowing the type of transformation operators and the number of branches to be flexibly adjusted. Two important transformation operators, multiplex and split, are considered in this work, both of which can represent multi-scale features at a more granular level and increase the range of receptive fields. By integrating the MBC and attention module, a multi-branch attention (MBA) module is consequently developed to capture the channel-wise interaction of feature maps for establishing the long-range channel dependency. By substituting the 3x3 convolutions in the bottleneck blocks of the ResNet with the proposed MBA, a novel block namely efficient multi-branch attention (EMBA) is obtained, which can be easily plugged into the state-of-the-art backbone CNN models. Furthermore, a new backbone network called EMBANet is established by stacking the EMBA blocks. The proposed EMBANet is extensively evaluated on representative computer vision tasks including: classification, detection, and segmentation. And it demonstrates consistently superior performance over the popular backbones.", "sections": [{"title": "1. Introduction", "content": "The ability of the multi-scale feature representation is essential for various vision tasks, such as image classification [1, 2], object detection [3, 4, 5, 6], instance segmentation [7, 8], semantic segmentation [9, 10], and scene parsing [11, 12]. Multi-scale features are widely used in conventional network designs, as illustrated by [13, 14, 15, 16, 17, 18, 19, 20, 21]. The efficiency of the multi-scale ability of convolutional neural networks (CNNs) can be subsequently improved in various ways, such as by utilizing features with different resolutions (e.g., [22, 23, 24, 25]), by using shortcut dense connections (e.g., DenseNet [18]), or by using convolutional layers with different kernel sizes (e.g., InceptionNets [15, 16, 26]). In particular, the popular backbone Res2Net [1] uses the 3x3 convolution to connect channels in a cascading way, and generate a feature map that is rich in multi-scale spatial information.\nDifferentiated from prior art, a novel module, namely Multi-branch and Concat (MBC), is proposed to process the input tensor. The proposed MBC-based architecture allows the type of transformation operators and the number of branches to be adaptively adjusted according to practical applications. There are various implementation choices on different types of transformation operators for MBC. In this work, we mainly focus on two important types namely Multiplex and Split, and thus two variations for implementing the MBC are developed: Multiplex and Concat (MUC) and Split and Concat (SPC). As illustrated by the two variations, the proposed MBC brings new degrees of freedom (DoF) to the design of attention networks by scaling or compressing the channel dimension of an input tensor and controlling the number of branches. Consequently, an effective module namely Multi-branch Attention (MBA) is developed by integrating the proposed MBC and classical attention module. As shown in Fig. 1, a novel block namely Efficient Multi-branch Attention (EMBA) is obtained by substituting the 3x3 convolutions in the bottleneck blocks of the ResNet with the developed MBA module. Finally, a network called EMBANet is established by stacking these EMBA blocks, as illustrated by Fig. 2. The main contributions of this work are listed below:\n\u2022 A novel module named Multi-branch and Concat (MBC) is proposed by allowing the type of transformation operators and the number of branches to be flexibly adjusted, which brings new degrees of freedom (DoF) to the design of attention networks.\n\u2022 Two important transformation operators, multiplex and split, are developed to implement the MBC.\n\u2022 A novel Multi-branch Attention (MBA) module, which can effectively extract multiscale spatial information at a more granular level and establish a long-range channel dependency, is developed based on the MBC module.\n\u2022 Seven variations are demonstrated based on the proposed architecture of EMBANet and a large DoF is achieved."}, {"title": "2. Related Work", "content": "2.1. Attention Mechanism\nThe attention mechanism is used to strengthen the allocation of the most informative feature expressions while suppressing the less useful ones, and thus makes the model attend to important regions within a context in an adaptive way. The Squeeze-and-Excitation (SE) attention in [27] can capture channel correlations by selectively modulating the scale of the channel. The CBAM in [28] can enrich the attention map by adding max-pooled features for the channel attention with large-size kernels. Motivated by the CBAM, the GSoP in [29] proposes a second-order pooling method to extract richer feature aggregation. More recently, the Non-Local block [30] is proposed to build a dense spatial feature map and capture the long-range dependency via non-local operations. Based on the Non-Local block, the Double Attention Network(A\u00b2Net) [31] introduces a novel relation function to embed attention with spatial information into the feature map. Sequentially, the SKNet in [32] employs a dynamic selection attention mechanism that allows each neuron to adaptively adjust its receptive field size based on multiple scales of the input feature map. The ResNeSt [33] proposes a similar Split-Attention block that enables attention across groups of the feature map. The FcaNet [34] proposes a novel multi-spectral channel attention mechanism that realizes the preprocessing in the frequency domain. The GCNet [35] introduces a simple spatial attention module, and thus a long-range channel dependency is established. The ECANet [36] develops a one-dimensional convolution layer to reduce the redundancy of fully connected layers. The DANet [11] adaptively integrates local features with their global dependencies by summing these two attention modules from different branches. The methods discussed above either focus on the design of more sophisticated attention modules, which inevitably incur a higher computational cost, or they are incapable of establishing a long-range channel dependency. In this work, a novel module namely multi-branch attention (MBA) is proposed to further integrate the local and global attention information. And a more flexible architecture to design and implement attention networks is offered.\n2.2. Multi-scale Feature Representations\nThe ability to represent multi-scale features is critical for various vision tasks and recent advances in CNN design have demonstrated that performance can be improved by a stronger multi-scale feature representation ability [13, 14, 16, 17, 18, 19, 20, 21].\n2.2.1. Image Classification\nBy introducing a 1\u00d71 convolution, the Network in Network (NIN) [37] inserts multi-layer perceptrons as micro-networks into the large network and the model discriminability of local patches is enhanced within the receptive field. By utilizing parallel filters with different kernel sizes, the GoogLeNet [16] can further enhance the multi-scale representation capability. By stacking more filters in each parallel path in the GoogLeNet, Inception networks [26, 15] can further expand the receptive field. By employing the shortcut connection, the ResNet [17] and DenseNet [18] can make objects processed on a very wide range of scales and their networks can really go deeper. The DPN [38] inherits the advantages of ResNet and DenseNet, its hierarchical tree structure enables the network to obtain even stronger multi-scale representation abilities at individual layers. The DyNet [25] employs the dynamic convolution method to adaptively generate convolution kernels based on image content. The PyConv [2] enlarges the receptive field by varying the size and depth of kernels at different levels. Besides, the PyConv processes the input tensor by using an increased kernel size in parallel to capture details of different levels. By constructing hierarchical residual-like connections within one single residual block, the Res2Net [1] represents multi-scale features at a more granular level and increases the range of receptive fields for each layer. The HRNet [39, 40] introduces high-resolution representations and keeps the interaction of feature maps with different resolutions in a parallel way. Our preliminary work was implemented as the EPSANet [41] by using the Efficient Pyramid Squeeze Attention (EPSA) block. In this work, we are no longer limited to improving the specific attention model, but creatively propose a multi-branch architecture from the new perspective of DoF. It is worth noting that the EPSANet can be considered as one of embodiments of the proposed EMBANet.\n2.2.2. Image Segmentation\nThe ESPNet [42] uses the spatial pyramid structure of dilated convolutions to re-sample the feature maps for learning representations from a large receptive field. The DeepLabv3 [7] applies several parallel atrous convolutions with different rates (called Atrous Spatial Pyramid Pooling, or ASPP), to capture the contextual information at multiple scales. The PSPNet [12] exploits the capability of global context information by performing pyramid pooling operations at different grid scales, to incorporate suitable global features.\n2.2.3. Object Detection\nThe Feature Pyramid Network (FPN) [4] develops a top-down architecture with lateral connections to build high-level semantic feature maps at all scales. While, the DetectoRS [3] uses a recursive feature pyramid module to incorporate extra feedback connections from FPNs into the bottom-up backbone layers. And the switchable atrous convolution is used to convolve the features with different atrous rates and gather results with switch functions.\nOur proposed EMBA block enables the variation of receptive fields in channel dimensions to capture detailed information and global features. Thus, multi-scale features at a more granular level are extracted. The proposed EMBA block can be used as a plug-and-play block to be directly integrated with CNNs and boost their performance."}, {"title": "3. Method", "content": "3.1. Revisting Channel Attention\nThe channel attention mechanism allows the network to selectively weight the importance of each channel and thus generate a more informative output. An SE block [27] consists of two parts: squeeze and excitation, which are respectively designed for encoding global information and recalibrating the channel-wise relationship adaptively. Generally, channel-wise statistics can be generated by using global average pooling, which is used to embed global spatial information into a channel descriptor. The input feature map is denoted by the quantity $X \\in \\mathbb{R}^{C \\times H \\times W}$, where H, W, C represent the height, width, and the number of input channels respectively. The global average pooling operator can be calculated as\n$g_{c} = \\frac{1}{H W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{c}(i, j)$\t\t(1)\nThe attention weight of the c-th channel in the SE block can be written as\n$\\omega_{c} = \\sigma(W_{1} \\delta(W_{0}(g_{c})))$ (2)\nwhere the symbol $\\delta$ represents the Rectified Linear Unit (ReLU) operation as in [43], $W_{0} \\in \\mathbb{R}^{\\frac{C}{r} \\times C}$ and $W_{1} \\in \\mathbb{R}^{C \\times \\frac{C}{r}}$ are the fully-connected (FC) layers. With two FC layers are employed, the linear information among channels can be combined more efficiently. And it is helpful for the interaction of information from high and low channel dimensions. The symbol $\\sigma$ represents the excitation function, and the Sigmoid function is commonly used in attention networks. With an excitation function, weights are assigned to channels after the channel interaction and thus the information can be extracted more efficiently. The above introduced process of generating channel attention weights is called the SE module in [27], which is illustrated by Fig. 3.\n3.2. Multi-branch and Concat Module\nTo fully exploit the multi-scale information from the input feature map effectively in a more flexible way, a novel module namely Multi-branch and Concat (MBC) is proposed in this work. As illustrated by Fig. 4, the MBC module can be used as a paradigm of scale transformation and brings new DoF to the attention networks. By allowing the type of projection operators and the number of branches be flexibly adjusted, the MBC module can be implemented in a variety of variants for various application scenarios and tasks. In this work, we mainly focus on two important types of scale transformation operators: Multiplex and Split. Correspondingly, two variations of the MBC module, namely Multiplex and Concat (MUC) and Split and Concat module (SPC), are developed. Both of them can further explore the distribution of multi-scale information on channel dimensions. The MUC applies a multiplexing operator to the channel dimension of the input tensor, and then divides it into multiple S branches. The input channel dimension is C for each branch, which is the same as the input tensor. By doing this, more abundant positional information from the input tensor is obtained. And the generalization ability of the model can be effectively improved. Next, a multi-scale pyramid convolution structure is used to squeeze the channel dimension of the input tensor and integrate the information at different scales. For the SPC module, however, it applies a splitting operator to reorganize the input feature map more efficiently. Specifically, the input tensor is split into S groups in channel dimension, with $\\frac{C}{S}$ channels in each split feature map. Similarly, the multi-scale pyramid convolution structure is used to integrate information at different scales on each channel-wise feature map. By doing this, neighboring scales of context features can be more precisely incorporated and the channel redundancy is reduced by a large margin. In summary, the MUC-based network is more suitable for large backbone CNNs because of its strong generalization ability, and the SPC-based network is more suitable for lightweight networks because it can efficiently reduce the channel redundancy.\n3.2.1. Multiplex and Concat Module\nAs illustrated by Fig. 5, an input feature map $X \\in \\mathbb{R}^{C \\times H \\times W}$ is multiplexed S times in channel dimension, and then divided into S branches. For each branch, it has C number of input channels as the original input tensor, and the output feature map of each branch has the channel dimension as $C^{\\prime} = \\frac{C}{S}$. Note that the number of input channels C should be divisible by S. With the MUC design, we can obtain more abundant spatial information from the input tensor. For each branch, it learns the multi-scale spatial information independently and establishes a cross-channel interaction locally. However, an increase in the number of parameters will result from the increase in kernel sizes. In order to process the input tensor at different kernel scales without increasing the computational cost, a method of group convolution is developed and applied to the convolutional kernels in parallel. Furthermore, we design a novel criterion for choosing the group size without increasing the number of parameters. The relationship between the multi-scale kernel and the group size can be written as\n$\\begin{aligned}G=\\begin{cases} \\frac{K-1}{2}  & K>3 \\\\ 1 & K=3 \\end{cases}\\end{aligned}$ (3)\nwhere the quantity K is the kernel size, and G is the group size. The above equation has been verified by our ablation experiments. Finally, the multi-scale feature map generation function is given by\n$F_{i} = Conv(K_{i} \\times K_{i}, G_{i})(X) \\quad i = 0, 1, 2\\dots S - 1$ (4)\nwhere the quantity $K_{i} = 2\\times(i+1) +1$ is the i-th kernel size, $G_{i}=\\begin{cases} \\frac{K_{i}-1}{2}  & K>3 \\\\ 1 & K_{i}=3 \\end{cases}$\n is the i-th group size and $F_{i} \\in \\mathbb{R}^{C^{\\prime} \\times H \\times W}$ denotes the feature map with different scales. Thus the whole multi-scale preprocessed feature map can be obtained in a concatenating way as\n$F = Cat([F_{0}, F_{1},\\dots, F_{S-1}])$\t\t(5)\n3.2.2. Split and Concat Module\nAs shown in Fig. 6, an input feature map $X \\in \\mathbb{R}^{C \\times H \\times W}$ is split into S parts, denoted by $[X_{0}, X_{1},\\dots, X_{S-1}]$ along with the channel dimension. For each split part, it has $C^{\\prime} = \\frac{C}{S}$ number of channels, and the i-th feature map is $X_{i} \\in \\mathbb{R}^{C^{\\prime} \\times H \\times W}$ with i = 0,1,\\dots, S \u2212 1. It also learns the multi-scale spatial information independently and establishes a cross-channel interaction in a local manner. With this splitting way, we can process the input tensor at multiple scales in parallel. Thus a feature map that contains a single type of kernel can be obtained. The relationship between the group and the multi-scale kernel size is established according to (3). Finally, the function for multi-scale feature map generation is given by\n$F_{i} = Conv(K_{i} \\times K_{i}, G_{i})(X_{i}) \\quad i = 0, 1, 2\\dots S - 1$ (6)\nwhere the quantity $K_{i} = 2\\times(i+1) +1$ is the i-th kernel size, $G_{i} =\\begin{cases} \\frac{K_{i}-1}{2}  & K>3 \\\\ 1 & K_{i}=3 \\end{cases}$\nis the i-th group size and $F_{i} \\in \\mathbb{R}^{C^{\\prime} \\times H \\times W}$ denotes the feature map with different scales. Thus the whole multi-scale preprocessed feature map can be obtained by a concatenating way as\n$F = Cat([F_{0}, F_{1},\\dots, F_{S-1}])$\t\t(7)\n3.3. Multi-branch Attention\nAs illustrated by Fig. 7, a novel multi-branch attention (MBA) module is developed by integrating the MBC and attention module. Correspondingly, the MBA module inherits the advantages of the MBC module and provides a more flexible way of selecting attention modules. Specifically, the MBA module is mainly implemented in four steps. First, the multi-scale feature map on channel-wise is obtained by implementing the proposed MBC module. Second, channel-wise attention vectors are obtained by the attention module for extracting the attention weight of the feature map at different scales. Third, the re-calibrated weight of a multi-scale channel is obtained by using Softmax to re-calibrate the channel-wise attention vector. Fourth, the operation of an element-wise product is applied to the re-calibrated weight and the corresponding feature map. Finally, a refined feature map that is richer in multi-scale feature information is obtained as the output. Therefore, a better interaction between local and global channel attention is achieved by the proposed MBA module. After processing by the MBC module, the multi-scale feature map $F \\in \\mathbb{R}^{C \\times H \\times W}$ is obtained. Then, we extract the channel attention weight information from it to generate attention weights with different scales. Mathematically, the attention weight vector is represented as\n$Z_{i} = \\phi(F_{i}), \\quad i = 0, 1, 2 \\dots S - 1$ (8)\nwhere the quantity $Z_{i} \\in \\mathbb{R}^{C^{\\prime} \\times 1 \\times 1}$ is the attention weight, the symbol $\\phi$ denotes an attention module, which is set as the SE module by default. The attention module is used to obtain the attention weight from the input feature map with different scales. By doing this, the proposed MBA module can fuse context information at different scales and produce better pixel-level attention for high-level feature maps. Further, in order to realize the interaction of attention information and fuse the cross-dimension vectors without destroying the original channel attention vector, the whole multi-scale channel attention vector is obtained in a concatenating way as\n$Z = Z_{0} \\oplus Z_{1} \\oplus \\dots \\oplus Z_{S-1}$ (9)\nwhere the symbol $\\oplus$ denotes the concat operator, $Z_{i}$ is the attention value from $F_{i}$, and Z is the multi-scale attention weight vector. A soft attention $att_{i}$ as below is used across channels to adaptively select different spatial scales, which is guided by the compact feature descriptor $Z_{i}$.\n$att_{i} = Softmax(Z_{i}) = \\frac{exp(Z_{i})}{\\sum_{i=0}^{S-1}exp(Z_{i})}$\t\t(10)\nwhere the Softmax function is used to obtain the re-calibrated weight $att_{i}$ of the multi-scale channel, which contains the location information and the attention weight in channel. By doing this, the interaction between local and global channel attention is established. Next, the channel attention of feature re-calibration is fused in a concatenation way, and thus the whole channel attention vector can be obtained as\n$att = att_{0} \\oplus att_{1} \\oplus \\dots \\oplus att_{S-1}$ (11)\nwhere att represents the multi-scale channel weight after the attention interaction. Then, we multiply the re-calibrated weight of multi-scale channel attention $att_{i}$\nwith the feature map of the corresponding scale $F_{i}$ as\n$Y_{i} = F_{i} \\otimes att_{i} \\quad i = 1, 2, 3, ... S-1$ (12)\nwhere $\\otimes$ represents the channel-wise multiplication, $Y_{i}$ refers to the feature map that with the obtained multi-scale channel-wise attention weight. The concatenation operator is more effective than the summation because it can integrally maintain the feature representation without destroying the information of the original feature map. Finally, the process to obtain the refined output can be written as\n$Out = Cat([Y_{0}, Y_{1},\\dots, Y_{S-1}])$ (13)\n3.4. Network Design\nAs illustrated by Fig.1, a novel block, namely Efficient Multi-branch Attention (EMBA), is developed by substituting the 3x3 convolution in the bottleneck blocks of ResNet with the MBA module at corresponding positions. Then, the multi-scale spatial information and the cross-channel attention are integrated into the EMBA block by the proposed MBA module. Thus, the EMBA block can extract multi-scale spatial information at a more granular level and develop a long-range channel dependency. Next, a novel backbone network EMBANet is further established by stacking the EMBA blocks. The proposed EMBANet inherits the advantages of the EMBA block, and thus it has strong multi-scale representation capabilities and can adaptively recalibrate the cross-dimension channel-wise weight. Correspondingly, two versions of the proposed EMBANet, the EMBANet(Small) and EMBANet(Large), are developed accordingly. For the proposed EMBANet(Small), the kernel and group sizes are respectively set as (3, 5, 7, 9) and (1, 4, 8, 16) in the MBC module. On the other hand, the proposed EMBANet(Large) has a higher group size as (32, 32, 32, 32). Because two variants of MBC, i.e. the MUC and SPC are proposed in this work, four networks EMBANet-M(Small), EMBANet-M(Large), EMBANet-S(Small), EMBANet-S(Large) are developed, where the letter 'M' denotes the multiplexing operator and 'S' denotes the splitting operator. That's, the EMBANet-M and EMBANet-S are equipped with the MUC and SPC module, respectively.\n3.5. Discussion\nThe proposed MBA module is developed by integrating the MBC and atten-tion modules. The proposed MBA module can be flexibly adjusted according to practical tasks by changing the types of transformation operator and the number of branches. From a technical perspective, the proposed MBA module extends the prior art and serves as a unified framework for various attention methods. For example, the famous backbone Res2Net in [1] can be interpreted as a special case of the MBA, where its transformation operator can be implemented by connecting the 3x3 convolution in a cascaded way between channels. Similarly, the SKNet [32] can be regarded as another instance of the MBA by only setting the number of branches to 2. For conventional channel attention modules like ECA or Fca, it can be interpreted as an instance example of the MBA by simply setting the number of branches as 1 and replacing the SE module at corresponding positions. In fact, there are many other models that can be regarded as variants of the MBA. Therefore, the proposed MBA module not only naturally generalizes the pre-processing of scale transformations in the channel-wise dimension but also provides a more flexible design for attention networks. To the best of our knowledge, this is the first attempt to investigate multi-scale feature extraction and enrich the design of the backbone from the DoF perspective in terms of types of transformation operators, attention modules, and the number of branches.\nIn this paper, we mainly focus on the hand-designed module for CNN ar-chitecture. In the future, we will further consider using the method of neural architecture search (NAS) to realize the dynamic balance of MBA sub-modules and find an optimal configuration strategy on different sub-tasks."}, {"title": "4. Experiments", "content": "4.1. Implementation Details\nIn this work, the Pytorch framework is employed to implement experiments. For image classification tasks, the ResNet [17] is used as the backbone and ImageNet as the dataset [13]. Similar to the prior art, the training configuration is set according to the reference in [1, 27, 34], the standard data augmentation scheme is implemented, and the size of the input tensor is cropped to 224 \u00d7 224 by randomly horizontal flipping and normalization. The optimization was performed by using stochastic gradient descent (SGD) with a weight decay of 1e-4, momentum of 0.9, and a mini-batch size of 256. The label-smoothing regularization [44] is used with the coefficient value of 0.1 during training. The learning rate was initially set as 0.1 and decreased by a factor of 10 after every 30 epochs for 100 in total. For object detection tasks, the default setting is that the shorter side of the input image is resized to 800, the SGD is used with a weight decay of 1e-4, momentum of 0.9, and the batch size is 2 per GPU within 12 epochs. The learning rate is 0.01 and decreased by a factor of 10 at the 8th and 11th epochs, respectively. For instance segmentation tasks, the settings of the training configuration and dataset are similar to those are used for object detection. Finally, all detectors are implemented by the MMDetection toolkit, and all models are trained on 8 Titan RTX GPUs.\n4.2. The Degrees of Freedom\nTo demonstrate the benefits of the DoF from the proposed MBA-based framework, we performed a series of dynamic transformations on the ImageNet dataset. Two widely used CNNs, ResNet-50[17] and ResNet-101 [17], are em-ployed as backbone models. The proposed MBA framework can be flexibly equipped with various attention modules such as SE, ECA, and Fca. In par-ticular, the EMBANet-M(Large)V2 and EMBANet-M(Large)V3 are developed by employing the ECA and Fca respectively. As illustrated in Table 1, the proposed MBA-based networks outperform their counterparts in terms of Top-1 accuracy for the ResNet-50 backbone based models by 1.94%, 1.33%, and 0.45% respectively. For the ResNet-101 based models, the proposed MBA-based networks can outperform their counterparts by 1.76%, 1.06%, and 0.23% in terms of Top-1 accuracy, respectively. It should be noted that the proposed EMBANet-M(Large)V3 outperforms the SENet in terms of Top-1 accuracy by more than 2% on both the ResNet-50 and ResNet-101 based backbones. The results demonstrated the overall flexibility and effectiveness of the proposed MBA module. Although the experiments are based on the EMBANet-M(Large), similar results can be obtained for other proposed networks.\n4.3. ImageNet\nIn this section, the performance of the proposed MBA based networks is compared with several state-of-the-art attention methods, such as the SENet [27], \u0421\u0412\u0410\u041c [28], A2-Net [31], GCNet [35], ECANet [36], FcaNet [34], and SKNet [32] on ImageNet dataset [46]. The ResNet-50 is employed as the backbone network in these experiments.\nAs shown by Table 2, the proposed EMBANet-M(Large) outperforms the SENet, ECANet, and FcaNet in terms of Top-1 accuracy by approximately 1.94%, 1.17% and 0.13% respectively, while maintaining almost the same computational cost. The proposed EMBANet-M(Large)V3 improves the Res2Net and SKNet in terms of Top-1 accuracy by about 0.98% and 1.42% respectively. Meanwhile, with almost the same complexity, the proposed EMBANet-M(Large)V3 out-performs the Res2Net+SE and the FcaNet by 0.53% and 0.45% in terms of Top-1 accuracy respectively. The proposed EMBANet-S(Small) outperforms the SENet-50 by 0.62%, while requiring 41.8% fewer parameters and a 41.4% lower computational cost. Furthermore, with nearly the same complexity, the pro-posed EMBANet-S(Large) outperforms the ECANet by 0.87% in terms of Top-1 accuracy. In summary, the proposed EMBANet-S type networks can reduce channel redundancy and combine channels effectively while still maintaining the quality of multi-scale feature representations, which is more suitable for a lightweight network structure. And the proposed EMBANet-M type networks can provide considerable performance gains for various vision tasks.\n4.4. CIFAR\nWe conduct experiments on the CIFAR-100 dataset [47], which contains 50k training images and 10k testing images from 100 classes. We employ the ResNet-50 as the backbone, and only replace the original basic blocks with our proposed MBA module while keeping the other configurations unchanged. As shown by Table 3, the proposed EMBANet-S(Small) achieves a large margin of 1.35% and 1.11% higher Top-1 accuracy over ResNet-50 and SENet-50, and reduces the number of parameters by 38.9% and 44.8% respectively. The proposed EMBANet-M(Small) further improves Top-1 accuracy by 1.92% and 2.16% as compared to the SENet and the ResNet respectively, while using much fewer parameters. In summary, efficient and effective performance can be achieved simultaneously by both the proposed EMBANet-M and EMBANet-S types of networks."}, {"title": "5. Ablation Analysis", "content": "5.1. Going Deeper and Wider with EMBANet\nDeeper and wider networks have demonstrated a stronger representation capability [17, 20, 15] for vision tasks. As presented in Table 4, we mainly compare the performance of the proposed three variants that are based on the EMBANet-M architecture with several classical and more complicated CNNs, such as the Res2Net [1], Inception-v3 [26], ResNeXt [20], SKNet [32], and DenseNet [18]. The proposed three variants employ ResNet-101 as the backbone network. To implement a more fair comparison, we duplicated the SKNet-101 with the same configuration and dataset. As shown in Table 4, the proposed EMBANet-M(Large) outperforms the DenseNet-264 (k=32), DenseNet-161 (k=48) and Inception-v3 in terms of Top-1 accuracy, respectively, by about 1.53%, 1.73% and 1.93%. What's more, the proposed EMBANet-M(Large)V2 achieves 1.51% and 0.91% gains in terms of Top-1 accuracy as compared to ResNet-200 (the one with deeper layers) and ResNeXt-101 (the one employing more convolutional filters and expensive group convolutions), respectively. The proposed EMBANet-M(Large)V3 outperforms more competitive models such as Res2Net-101 and SKNet-101 in terms of Top-1 accuracy by 0.68% and 1.03%, respectively. The above results demonstrate that the proposed EMBANet-M type networks can effectively strengthen multi-scale feature extraction and compact the internal representations in the channel dimension. Meanwhile, the proposed EMBANet-M type networks can perform favourably against state-of-the-art CNNs which are even deeper and wider in network architecture. These results also verified that the proposed MBA module is very flexible and can be integrated with deeper models when necessary to achieve better performance.\n5.2. Going Lightweight with EMBANet\nAs illustrated by Fig. 8, we create a new lightweight module called depthwise-MBA (DWMBA) by substituting the 3x3 deep separable convolution at corre-sponding positions in MobileNetV2[48] with the proposed MBA. In particular, the proposed DWMBA module consists of the proposed SPC module and the SE attention. The proposed SPC module turns each convolution with different scales into a pyramid structure, such as 3x3, 5x5, 7x7, and 9x9 convolutions, into a depth-wise separable convolution. Correspondingly, a novel lightweight network named EMBANet-L is further established by stacking the lightweight blocks in the MobileNetV2 style. During the training process, we use the standard SGD optimizer with the decay and momentum parameter set as 0.9 for all models. The weight decay rate is set to 4 \u00d7 10-5. A cosine learning schedule with an initial learning rate of 0.05 is adopted, and the batch size is 256. We use MobileNet V2 [48] as our backbone, and all models are trained with 200 epochs. For data augmentation, we use the same methods as in MobileNetV2. As shown in Table 5, our proposed EMBANet-L improves Top-1 accuracy by approximately 1.4% and 0.98% over the original MobileNetV2 and the updated MobileNetV2 with SENet, respectively. Based on the above results, the efficiency and effectiveness of the proposed MBA module for lightweight CNN architectures is verified.\n5.3. The Impacts of the Re-calibrating Function\nIn order to re-calibrate the channel-wise attention vector more effectively, we assessed the choice of the excitation function for the re-weight mechanism. We mainly consider two commonly used options: Softmax and Sigmoid. For the SPC and MUC modules, we compared the performance of using Softmax with that of using the Sigmoid operation on the CIFAR-100 dataset. As shown in Table 6, the performance of the Softmax is better than that of the Sigmoid.\n5.4. The Impacts of the SPC and the MUC module\nAs shown by Fig 7, the proposed EMBANet consists of the MBC and the SE attention modules. In order to explicitly verify that benefits resulted from the MBC module, we conducted this experiment and employed the EMBANet-S(Small) and the EMBANet-M(Small) as baselines on the CIFAR-100dataset. As illustrated by Table 7, the word 'SPC' means to remove the SE attention module and only keeping the SPC, the word 'MUC' means to remove the SE attention module and only keeping the MUC, the word 'SE' means that the number of branches in the MBC module is set to 1, which can be interpreted as removing the MBC module. Besides, the word 'SPC+SE' means that both the SPC and SE attention modules are equipped, and the word 'MUC+SE' means that both the MUC and SE attention modules are equipped. The experimental results verified that the proposed MBC is more effective than the SE module in terms of performance improvement. Thus, the proposed MBC-based framework brings flexibility and effectiveness to attention network design."}, {"title": "6. Object Detection", "content": "In order to demonstrate the generalization ability of our proposed EM-BANet on downstream vision tasks, experiments on object detection are im-plemented. The ResNet-50 along with FPN[4] is used as the backbone model."}, {"title": "7. Instance Segmentation", "content": "Instance segmentation is a combination of object detection and semantic segmentation. It requires not only objects of various sizes in an image be correctly detected but also each object be precisely segmented. For the instance segmentation task, our experiments are implemented by using the Mask R-CNN on the MS COCO dataset. As illustrated by Table 10, the proposed EMBANet-M(Large)-50 outperforms the SENet by approximately 1.7% and 1.6% on AP, AP50 respectively. The proposed EMBANet-S(Large) also achieves a large margin of 1.1%, 1.0% and 0.5% on AP as compared to ECANet, GCNet and FcaNet respectively. The visualization results are further illustrated in Fig. 9 and 10. In summary, test results on the instance segmentation illustrated the strong migration ability of our proposed architecture."}, {"title": "8. Class Activation Mapping", "content": "Because the MUC-based network can offer better performance for various computer vision tasks, we employed the EMBANet-M(Small) as the baseline in this experiment to demonstrate the multi-scale ability of the MBC module. Specifically, we visualized the class activation mapping (CAM) of the EMBANet-M(Small) by using Grad-CAM [52]. The Grad-CAM is commonly used to localize discriminatory regions for image classification and can also be used to visualize the gradients of top-class prediction with respect to the input image as a colored overlay. As illustrated by Fig. 11, the proposed EMBANet-M(Small) has more concentrated activation maps on small and medium objects and can generate fine-level feature representations. For intensive objects, the EMBANet-M(Small) has more accurate positioning and division than that of the SENet. The visualization results verified that the MBC-based network can capture richer and more discriminatory contextual information for a particular target class."}, {"title": "9. Conclusion and Future Work", "content": "A novel multi-scale feature extraction framework, namely Multi-branch and Concat (MBC), is proposed in this work. The proposed MBC brings extra DoF and thus makes the attention network design more flexible and scalable. To implement the MBC, two important variations, the Multiplex and Concat (MUC) and the Split and Concat (SPC), are developed. Both can bring performance gains from different perspectives across various computer vision tasks. The MUC, which can achieve state-of-the-art performance in tasks of image classification and object detection, is more suitable for a large backbone. The SPC, which is more efficient as compared to existing attention methods, is more suitable for a mobile network. The proposed MBC can also learn a richer multi-scale feature representation of the input tensor at a more granular level. Correspondingly, a unified multi-scale attention network architecture called EMBANet has been further developed based on the MBC module. Extensive experiments on tasks of image classification, object detection, and image segmentation demonstrate that the proposed EMBANet can outperform CNN- and Attention-based models in terms of effectiveness and efficiency. In the future, we will design a novel search method to automatically find the optimal DoF for attention modules and detailed structures."}]}