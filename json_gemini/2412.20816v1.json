{"title": "Length-Aware DETR for Robust Moment Retrieval", "authors": ["Seojeong Park", "Jiho Choi", "Kyungjune Baek", "Hyunjung Shim"], "abstract": "Video Moment Retrieval (MR) aims to localize moments\nwithin a video based on a given natural language query.\nGiven the prevalent use of platforms like YouTube for in-\nformation retrieval, the demand for MR techniques is sig-\nficantly growing. Recent DETR-based models have made\nnotable advances in performance but still struggle with ac-\ncurately localizing short moments. Through data analysis,\nwe identified limited feature diversity in short moments,\nwhich motivated the development of MomentMix. Moment-\nMix employs two augmentation strategies: ForegroundMix\nand BackgroundMix, each enhancing the feature represen-\ntations of the foreground and background, respectively. Ad-\nditionally, our analysis of prediction bias revealed that\nshort moments particularly struggle with accurately pre-\ndicting their center positions of moments. To address this,\nwe propose a Length-Aware Decoder, which conditions\nlength through a novel bipartite matching process. Our\nextensive studies demonstrate the efficacy of our length-\naware approach, especially in localizing short moments,\nleading to improved overall performance. Our method sur-\npasses state-of-the-art DETR-based methods on benchmark\ndatasets, achieving the highest R1 and mAP on QVHigh-\nlights and the highest R1@0.7 on TACoS and Charades-\nSTA (such as a 2.46% gain in R1@0.7 and a 2.57% gain in\nmAP average for QVHighlights). The code is available at\nhttps://github.com/sjpark5800/LA-DETR.", "sections": [{"title": "1. Introduction", "content": "As vast amounts of video content are created and shared\non the internet daily [7], the need for effective filtering\nhas become more critical. Text-based search algorithms [3]\nhave emerged as one of the most effective solutions, en-\nabling rapid and accurate retrieval of videos that match user\nqueries. To improve user experience and search efficiency,\nmoment retrieval (MR) [1, 11] has gained significant atten-\ntion. MR identifies the specific moments within a video that\nbest align with a given query. Specifically, this task involves\nlocalizing the start and end points in the video relevant to the\ntextual query, offering a more fine-grained understanding of\nvideo content.\nFor the MR task, previous approaches leveraging DETR\nfor its efficiency and flexibility [19, 26, 32, 38, 44] have pro-\nposed methods to improve video-text feature representation,\nachieving impressive performance. However, our empirical\nfindings indicate that these DETR-based models suffer from\na significant drop in performance when handling short mo-\nments as highlighted in Table 1. For example, UVCOM [38]\nshows an average mAP of 49.04 for middle-length moments\n(10-30 seconds), while achieving only 12.65 for short mo-\nments (less than 10 seconds), revealing a substantial gap. As\nillustrated in Figure 1, moments are uniformly distributed\nacross different lengths. However, due to the inherently\nshort duration of short moments, they appear more fre-\nquently within a single video sample. Performance metrics\nare calculated by averaging the performance across all mo-\nments within a sample and then averaging these sample per-\nformances to obtain the overall performance. Therefore, the\noverall performance does not sufficiently reveal the perfor-\nmance on short moments, leading to a lack of attention to\nthe challenges associated with short moment retrieval.\nRetrieving short moments within videos is a crucial task\nbecause videos often contain a significant amount of re-\ndundant or irrelevant information, while essential content\nis frequently condensed in short moments. This aligns with\nthe importance of MR, where improving the accuracy of\nshort-moment retrieval enables the precise extraction of the\nmost relevant information. Such improvements can signifi-\ncantly reduce the time and effort required for video explo-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Moment Retrieval", "content": "The moment retrieval (MR) task involves identifying the\nstart and end timestamps of a specific moment within a\nvideo given a text query. Traditional methods approach\nMR through either two-stage [2, 9, 13, 45, 47] or one-\nstage [5, 14, 22, 27, 34, 41, 46] framework, which often\nrely on cumbersome processes such as proposal genera-\ntion, non-maximum suppression (NMS), and human priors.\nA significant paradigm shift was introduced with Moment-\nDETR [19], which framed MR as a direct set prediction\ntask using Detection Transformer (DETR) [4]. This ap-\nproach eliminates these dependencies, thereby making MR"}, {"title": "2.2. Mixing-based Augmentation", "content": "Mixing-based augmentations have been explored in both\nimage and video tasks, each adapting spatial or temporal\nmixing based on task requirements. In image classification,\nMixup [42] and CutMix [39] create new image samples\nby interpolating or combining patches, promoting diverse\nfeature representations. Copy-Paste [12] augments data for\ndetection and segmentation by inserting objects from one\nimage into another, increasing object and scene variety.\nIn video understanding tasks, VideoMix [40] inserts ran-\ndomly selected video cuboid patches from one video into\nanother, thereby introducing both spatial and temporal di-\nversity. Similarly, VIPriors [16] extends traditional image-\nbased mixing augmentations to the temporal dimension,\nwhich strengthens temporal feature representations and im-\nproves model robustness against temporal fluctuations.\nHowever, these approaches primarily focus on modify-"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Motivation", "content": "Background. Suppose that a video consists of N\u2082 clips,\n{v_i}_{i=0}^{N_v}, and a text query of N\u209c words, {t_i}_{i=1}^{N_t}. The ob-\njective of moment retrieval (MR) is to predict a set of N\u2098\nmoments, {m_i}_{i=1}^{N_m}, corresponding to video clips relevant\nto the text query. Each moment m\u1d62 is defined by its cen-\nter coordinate c\u1d62 and length (span) \u03c3\u1d62, representing a con-\ntiguous subset of video clips. In this paper, we classify mo-\nments based on the following criteria: 1) A temporal mo-\nment within the video is defined as foreground if it is rel-\nevant to the text query, and as background if it is not. 2)\nMoments are categorized as short (less than 10 seconds),\nmiddle (10 to 30 seconds), or long (over 30 seconds) based\non their temporal duration, consistent with the classification\nused in the previous method [19].\nPerformance limit of DETR-based methods. Recent ap-\nproaches actively employ DETR for the MR task and\nachieve impressive performances. We selected representa-\ntive DETR-based models and analyzed their performance\naccording to the length of target moments. Despite achiev-\ning strong performance, these models exhibited significant\nperformance drops in retrieving short moments, as shown in\nTable 1. Specifically, QD-DETR, TR-DETR, and UVCOM\nexperienced mAP declines of - 79.8%, 78.0%, and 72.4%\nfor short moments. These results indicate a consistently sig-"}, {"title": "3.2. MomentMix: Leveraging ForegroundMix and\nBackgroundMix", "content": "We propose MomentMix, a data augmentation strategy de-\nsigned to address the issue of low feature diversity for\nshort moments. MomentMix consists of two components:\n(1) ForegroundMix, which enhances the diversity of fore-\nground features, and (2) BackgroundMix, which increases\nthe variety of background features. To the best of our knowl-\nedge, this is the first data augmentation approach specifi-\ncally tailored for video moment retrieval.\nForegroundMix augmentation. The goal of Foreground-\nMix is to increase the visual diversity of foreground fea-\ntures in short moments, enabling more generalized predic-\ntion. To achieve this, we randomly extract and mix rich fore-\nground features from longer samples to create augmented\nshort moments. Visual features within a single video natu-\nrally exhibit higher similarity compared to those from dif-\nferent videos. By exploiting features from other video clips,\nour method allows the model to generalize diverse contexts,\nensuring it can detect short moments reliably even in chal-\nlenging or ambiguous frames.\nGiven an existing video training sample $\\mathcal{X} = \\{V_i\\}_{i=0}^{N_v}$\nthat contains a long foreground (moment) $f_{source} = \\{V_i\\}_{i=s}^e$,\nthis foreground can be divided into sub-foregrounds\n$f_1, f_2,..., f_n$ as follows:\n$f_{source} = \\bigcup_{i=1}^n f_i, \\text{ where } f_i \\cap f_j = \\emptyset \\text{ for all } i \\neq j$. (1)\nHere, $n = \\frac{\\text{len}(f_{source})}{\\epsilon_{cut}}$, where $\\epsilon_{cut}$ is a hyperparameter de-\ntermining the extent to which each sub-foreground is short-\nened relative to the original long foreground."}, {"title": "BackgroundMix augmentation.", "content": "The goal of Background-\nMix is to improve the diversity of the visual background\nfeatures, thereby strengthening the association between\nforeground visual features and the text query. To achieve\nthis, we keep the original foreground features while replac-\ning the background with features from different videos. This\nmethod provides the model with richer training signals, al-\nlowing it to learn various boundaries more effectively.\nGiven the k-th video training sample X\u1d4f, consists of\n$N_f^k$ foreground segments $f^k = \\{f_i^k\\}_{i=1}^{N_f^k}$ and $N_b^k$ back-\nground segments $b^k = \\{b_i^k\\}_{i=1}^{N_b^k}$. All segments within the\nvideo are defined as follows:\n$a^k = f^k \\cup b^k = \\{a_i^k\\}_{i=1}^{N^k}, \\text{ where } N^k = N_f^k + N_b^k$.(5)\nTo increase feature diversity, we replace each back-\nground segment $b_i^k$ of the k-th sample with a randomly\ncropped segment from a different training sample X\u1d50\n(m \u2260 k). Specifically, for each $b_i^k$, a segment $a_i^m$ is ran-\ndomly selected from X\u1d50 and cropped to match the duration\nof $b_i^k$. The replacement is performed as follows:\n$b_i^k \\leftarrow \\text{ Crop}(a_i^m, |b_i^k|)$\nThis approach ensures that while the backgrounds of the\nk-th sample are augmented with diverse background fea-\ntures, the original foreground remains intact."}, {"title": "3.3. Length-Aware Decoder", "content": "In our previous analysis, we identified that the model\nstruggles to accurately predict both the center and the\nlength of short moments. To address this issue, we pro-\npose a length-aware decoder that conditions the moment\nlength, enabling the model to focus more effectively on cen-\nter prediction. We categorize moment lengths into distinct\nclasses such as short, middle, and long-by analyzing a\ncumulative mAP graph and identifying inflection points as\nboundaries. (Detailed information can be found in the sup-\nplementary materials.) The decoder queries are trained us-\ning a length-wise matching approach based on these length\ncategories. This categorization creates length-wise expert\nqueries that better handle the specific characteristics of dif-\nferent moment lengths.\nDecoder queries with class-pattern. We define N\ud835\udcb8 as the\nnumber of length classes for assigning roles to decoder\nqueries. Drawing inspiration from Anchor-DETR [36], we\ninterpret pattern in pattern embedding as a length category\nand create class-pattern embeddings Q\ud835\udcb8:\n$Q_c = \\text{Embedding}(N_c, d) \\in \\mathbb{R}^{N_c \\times d}$.(6)\nBy replicating each class-pattern embedding Nq times (the\nnumber of queries per length), we obtain class-specific\nqueries $Q \\in \\mathbb{R}^{N_c N_q \\times d}$. This approach ensures that decoder\nqueries share the same class embedding within each length\ncategory, enabling each query to perform roles tailored to\nits specific length class.\nLength-wise matching. To create the length-wise exper-\ntise within class-pattern embeddings, we revised the bipar-\ntite matching approach to operate on a per-class basis. This\nmethod ensures that class-specific queries are matched and\ntrained only with ground truth moments of the correspond-\ning length class. By categorizing ground truth moments into\nlength classes and performing length-class-wise matching,"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Datasets. We utilized three datasets, QVHIGHLIGHTS [19],\nCHARADES-STA [10], and TACOS, [30]) for evaluation.\nQVHIGHLIGHTS consists of over 10k YouTube videos cov-\nering various topics such as everyday activities, travel, so-\ncial activities, and political activities. It contains moments\nof various lengths distributed evenly and allows for testing\nour intended aspects effectively, as multiple moments ap-\npear within a single video. Considering the diversity and\ncomplexity of the dataset, it covers the most realistic and\nchallenging scenario. CHARADES-STA focuses on daily in-\ndoor activities, comprising 9,848 videos with 16,128 anno-\ntated queries. The lengths of moments are mostly below 20\nseconds. TACOS primarily features activities in the cook-\ning domain, consisting of 127 videos with 18,818 queries.\nThe video lengths vary from very short to nearly 800 sec-\nonds, with most moments being shorter than 30 seconds.\nUnlike QVHIGHLIGHTS and TACOS, which encompass\nmoments of varying lengths, CHARADES-STA is predom-\ninantly composed of shorter samples (under 30 seconds),\nthus it does not fully align with the problem we aim to ad-\ndress. Nevertheless, we include it in our evaluation to assess\nthe model's generalization performance.\nEvaluation metrics. Following the metrics of existing\nmethods, we use mean average precision (mAP) with Inter-\nsection of Union (IoU) thresholds of 0.5 and 0.75, as well\nas the average mAP over multiple IoU thresholds [0.5: 0.05:\n0.95]. Additionally, we report the standard metric Recall@1\n(R1) metric, commonly used in single-moment retrieval,\nwith IoU thresholds of 0.5 and 0.7. Also, we report the av-\nerage R1 over multiple IoU thresholds [0.5: 0.05: 0.95].\nImplementation details. We divided the classes based on\nthe point where the change in performance was most signif-\nicant in the validation results of each baseline model. To\nachieve this, we plotted the cumulative mAP graph with\nrespect to length and identified the inflection points. The\nthresholds for class division were then determined by cal-\nculating the k-means centers of these inflection points. As\na result, we set the thresholds for QVHIGHLIGHTS as [12,\n36, 65, inf], for CHARADES-STA as [5.67, 14, inf], and for\nTACOS as [10, 19, 38, inf], using UVCOM as baseline.\nIn ForegroundMix, we set \u03f5cut = 5 for QVHIGHLIGHTS\nand TACOS and \u03f5cut = 10 for CHARADES-STA to create\nshorter moments. In Length-Aware Decoder, the number of\nqueries per class Nq was set to 10.\nFor a fair comparison, we utilize the same features that\nprevious works used. On QVHIGHLIGHTS and TACOS, the\nvideo features are extracted from SlowFast [8] and CLIP vi-\nsual encoder [29]. On CHARADES-STA, we use two feature\ntypes as in previous works. The first type is the video fea-\ntures from SlowFast and CLIP visual encoder, and text fea-\ntures extracted from the CLIP text encoder. The second type\nis video features extracted from VGG [31] and text features\nextracted from GloVe [28].\nThe model is trained for 200 epochs on all datasets with\nlearning rates 1e-4. The batch size is 32 for QVHIGH-\nLIGHTS, 8 for CHARADES-STA, and 16 for TACOS, fol-\nlowing previous methods. We kept all the baseline parame-\nters."}, {"title": "4.2. Results", "content": "We applied our method to QD-DETR [26], a common\nbaseline in many studies. However, since our method can\nbe easily added to other models, we further validated our\nmethod on three recent methods (TR-DETR [32], and\nUVCOM [38]) to demonstrate its effectiveness. We com-"}, {"title": "4.3. Ablation Studies and Discussions", "content": "Component analysis. In Table 5, we examined the im-\npact of MomentMix and the Length-Aware Decoder on en-hancing performance for short moments, observing overall\ngains. While each component individually improves perfor-\nmance, their combined application leads to even greater im-\nprovements. This suggests that our two components, Mo-\nmentMix and Length-Aware Decoder, each contribute ef-\nfectively without redundancy, making their combined use\nthe effective approach for tackling the challenge of short-moment retrieval.\nEvaluation in few-shot scenarios. To validate the effec-\ntiveness of MomentMix as a data augmentation technique,\nwe conducted experiments using 50%, 20%, and 10% of"}, {"title": "5. Limitation and Conclusion", "content": "Limitation. While we have incorporated length-awareness\nin the decoder, further investigation into enhancing the en-\ncoder is necessary. Moreover, our novel data augmentation\nmethod for MR improves performance but also increases"}, {"title": "Conclusion.", "content": "In this study, we addressed the limitations of\nshort-moment retrieval in existing DETR-based approaches\nfrom both data and model perspectives. To overcome the\ndata-centric issue of limited feature diversity in short mo-\nments, we introduced MomentMix, which leverages two\nmix-based data augmentation strategies: ForegroundMix\nand BackgroundMix. These strategies enhance the feature\nrepresentations of both foreground and background ele-\nments. On the model side, we identified inaccuracies in cen-\nter predictions for short moments and proposed a Length-\nAware Decoder with a novel bipartite matching process\nconditioned on moment length. This approach leverages\nlength expert queries to improve center prediction accuracy.\nExtensive experiments demonstrate that our method sur-\npasses state-of-the-art DETR-based moment retrieval mod-\nels in terms of R1 and mAP on benchmark datasets. Further-\nmore, our methodology can be seamlessly integrated with\nother DETR-based models, paving the way for future ad-\nvancements in the field."}, {"title": "A. Additional Ablation Studies", "content": "Effect of MomentMix. We propose MomentMix, a novel\nmixing-based augmentation for moment retrieval (MR). To\nvalidate its effectiveness, we compared MomentMix with\nthree naive augmentations: (1) Random Crop: randomly\ncrops foreground regions. (2) Random Drop: masks a ran-\ndom a% of frame features by setting them to zero. (3) Gaus-\nsian Noise: adds Gaussian noise with mean=0 and std=\u1e9e to\nall frame features.\nAs shown in Table A2, while Group-DETR improves R1, it\nsuffers a significant drop in mAP. In contrast, LAD achieves\nsubstantial gains in both R1 and mAP, effectively address-\ning performance degradation in short moments through its\nlength-aware mechanisms. These results underscore LAD\nas a more effective and task-specific approach for MR.\nEffect of number of queries. Our method employs 40\nqueries, with 10 queries allocated to each length class. In\ncomparison, QD-DETR and TR-DETR originally use 10\nqueries, while UVCOM uses 30. To ensure that the ob-\nserved performance improvements with our method are not\nsimply due to the increased number of queries, we re-\ntrained the baselines with 40 queries for a fair comparison.\nThe results in Table A1 show that while Random Crop\nslightly enhances performance on short moments, it de-\ngrades mAP for other lengths, diminishing overall effec-\ntiveness. Similarly, Random Drop and Gaussian Noise fail\nto produce meaningful performance gains. In contrast, Mo-\nmentMix achieved notable improvements for short mo-\nments while also enhancing performance across all lengths,\nleading to substantial gains in overall metrics. These results\ndemonstrate the effectiveness of MomentMix augmentation\ncompared with other naive augmentations.\nguarantee performance gains and can even lead to perfor-\nmance drops, as observed with TR-DETR. This indicates\nthat the performance gains achieved by our method are not\ntrivial or merely due to an increased number of queries.\nEffect of cut criteria in ForegroundMix. We propose\nForegroundMix, which cuts a long foreground into shorter\nsub-foregrounds, shuffles them, and generates new short-moment data. We analyze the effect of \u03f5cut, which deter-\nmines sub-foreground shortening relative to the original\nlong foreground, with QD-DETR as the baseline.\nAs shown in Table A4, smaller values of \u03f5cut (more aggres-\nsive cutting and greater shortening) lead to improved per-\nformance on shorter moments. Regardless of the value, \u03f5cut\nconsistently enhances overall performance. Since our pri-\nmary objective is to improve short-moment performance,\nwe adopt the smallest value, \u03f5cut = 5, as our default setting."}, {"title": "B. Moment Length Class Selection", "content": "Defining length class. To define multiple length classes, we\nselect corresponding length thresholds using the cumulative\nmAP graph with respect to length, as shown in Figure A1.\nWe chose cumulative mAP because it effectively highlights\nlengths where the model underperforms. Initially, we com-\npute the cumulative mAP for each moment length based\non an existing moment retrieval baseline, UVCOM. Sub-\nsequently, we identify the inflection points on the graph and\ncluster them using K-means. These clustered points deter-\nmine the length class thresholds.\nPerformance comparison based on the number of\nclasses. The number of classes, Nc, is determined by the\nvalue of k in K-means. To determine the optimal k, we ex-\nperimented with different class numbers with QD-DETR as\nthe baseline. As shown in Table A5, using four classes re-\nsulted in the highest length-awareness in the model.\nEffect of consistent class definition. We investigated the\nperformance when using fixed thresholds [10, 30, 70, inf],\ndeviating from the aforementioned approach. As depicted\nin Tab. A6, this also led to improved performance across all\nthe datasets. This results demonstrates robust performance\nregardless of class definitions and holds the promise for fur-\nther enhancement through precise tuning tailored to dataset\ncharacteristics."}, {"title": "C. Evaluation with Diverse Feature Types", "content": "We conducted experiments using various feature types to\ndemonstrate that our methods\u2014MomentMix augmentation\nand the Length-Aware Decoder\u2014are robust and not limited\nto specific features.\nEvaluation with additional audio features. Following\nprior work, we incorporated additional audio features ex-\ntracted from PANNs [17", "37": "a recent foundational model for multimodal video under-\nstanding, for both video and text modalities. We re-trained\nthe baseline UVCOM and our method using these richer\nand more powerful features. As shown in Table A8, de-"}]}