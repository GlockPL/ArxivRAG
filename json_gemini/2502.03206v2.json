{"title": "A Unified and General Humanoid Whole-Body Controller for Fine-Grained Locomotion", "authors": ["Yufei Xue", "Wentao Dong", "Minghuan Liu", "Weinan Zhang", "Jiangmiao Pang"], "abstract": "Locomotion is a fundamental skill for humanoid robots. However, most existing works made locomotion a single, tedious, unextendable, and passive movement. This limits the kinematic capabilities of humanoid robots. In contrast, humans possess versatile athletic abilities-running, jumping, hopping, and finely adjusting walking parameters such as frequency and foot height. In this paper, we investigate solutions to bring such versatility into humanoid locomotion and thereby propose HUGWBC: a unified and general humanoid whole-body controller for fine-grained locomotion. By designing a general command space in the aspect of tasks and behaviors, along with advanced techniques like symmetrical loss and intervention training for learning a whole-body humanoid controlling policy in simulation, HUGWBC enables real-world humanoid robots to produce various natural gaits, including walking (running), jumping, standing, and hopping, with customizable parameters such as frequency, foot swing height, further combined with different body height, waist rotation, and body pitch, all in one single policy. Beyond locomotion, HUGWBC also supports real-time interventions from external upper-body controllers like teleoperation, enabling loco-manipulation while maintaining precise control under any locomotive behavior. Extensive experiments validate the high tracking accuracy and robustness of HUGWBC with/without upper-body intervention for all commands, and we further provide an in-depth analysis of how the various commands affect humanoid movement and offer insights into the relationships between these commands. To our knowledge, HUGWBC is the first humanoid whole-body controller that supports such fine-grained locomotion behaviors with high robustness and flexibility.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent progress in humanoid robots has shown impressive results in achieving complex tasks, and the huge potential to become a general robot platform [4, 50, 3, 38]. It is a funda- mental skill to support various humanoid motions, enabling them to navigate environments and perform tasks with agility and adaptability. However, most current humanoid locomotion systems, although showing impressive results in motion-based control [18, 20, 10, 3] and mobile manipulation [26], pay limited attention to producing versatile and fine-grained gait styles, leading to single, tedious, unextendable, and passive movements. Think about us humans, we have versatile athletic abilities, such as running, jumping, and even hopping. Even when only walking, we can fine-tune our frequencies, strides, and foot heights. Bringing such versatility into humanoid locomotion is challenging, but is the key to exploring the edge of humanoid robots' abilities. To resolve the challenge and build a unified and general humanoid whole-body controller,\nin this work, we propose HUGWBC, namely, Humanoid's Unified and General Whole-Body Control. HUGWBC is designed for generating fine-grained locomotion with dynamic, customizable control, enabling the robot to perform gaits such as walking, running, jumping, and hopping. Furthermore, HUGWBC provides the flexibility to adjust foot behavior parameters foot swing height and gait frequency, and allows combining posture parameters such as body height, waist rotation, and body pitch. To achieve this, HUGWBC includes a general command space designed for humanoid locomotion, along with advanced training techniques to learn versatile gaits within one single policy (except the hopping gait) using reinforcement learning in simulation, which can be directly transferred onto real robots.\nPositioned as a fundamental controller for humanoid robots to perform a wider range of tasks in diverse real-world scenarios, HUGWBC introduces intervention training and supports real-time external control signals of the upper body, like teleoperation, allowing for highly robust loco-manipulation, while maintaining precise locomotion control. An overview of the framework is illustrated in Fig. 2.\nIn experiments, we show HUGWBC preserves high tracking accuracy on eight different commands under four different gaits; we also ablate the necessity of invention sampling in improving the stability and robustness. We further provide a detailed analysis of how commands combination works, shedding light on the intricate relationships between these commands and how they can be leveraged to optimize movement performance. Through this work, we aim to significantly broaden the scope of humanoid locomotion capabilities, pushing the boundaries of what is possible with current robotic systems.\nThe key contributions are summarized as follows:\n\u2022 An extended general command space with advanced training techniques designed for fine-grained humanoid locomotion.\n\u2022 Accurate tracking for eight different commands under four different gaits in one single policy (except the hopping gait).\n\u2022 A fundamental humanoid controller that supports external upper-body intervention and enables a wider range of loco-manipulation tasks."}, {"title": "II. BACKGROUND", "content": "To support various high-level functionalities and allow the humanoid robot to perform complicated tasks, a fundamental whole-body controller is essential. Formally, given a set of continuous commands C and observations O, our objective is to develop a control function that maps these inputs to appropriate control signals. Model-based approaches represent one solution paradigm, typically decomposing the control function into planning and tracking modules [13, 32]. The planning module generates optimal trajectories and contact sequences based on O and C, while the tracking module translates these into control laws, specifying joint positions, velocities, and torques. However, these methods face computational challenges due to the complex dynamics of humanoid robots and the discrete nature of whole-body contact points. Learning-based methods offer an alternative approach by directly learning a policy function $a = \\pi(o, c)$ that maps observations O and commands C to joint-level actions. These actions typically represent offsets to target joint positions across three categories: upper-body, lower-body (legs), and hands. The final target position combines the nominal position with these learned offsets, which is then tracked using a proportional derivative (PD) controller with fixed gains."}, {"title": "B. Command Tracking as Reinforcement Learning", "content": "To achieve a generalized and powerful whole-body control behavior for humanoid robots, we learn a policy function by constructing a command-tracking problem. In detail, we want the learned policy \u03c0 to control the robots to match the provided commands c. To this end, we turn to the help of reinforcement learning (RL), where we define the reward functions r typically by distances d or similarities s of the command c and the observed robot state $s_c$ corresponding to that command:\n$r(o, a, c) = -d(c, s_c)$ or $r(o, a, c) = s(c, s_c)$.\nUnder the formulation of RL, the policy is trained to maximize the rewards, corresponding to matching these commands."}, {"title": "C. Simulation Training and Real-World Transfer", "content": "Many recent works, especially those of legged robots, take advantage of RL training a robust robot-control policy with a large set of parallel environments in simulation and directly deploying into the real world [5, 23, 18, 19]. Due to the partial observability of the real robot, whose onboard sensors can only provide limited and noisy observations, it is difficult to learn a deployable policy from them directly. Therefore, researchers have developed a set of sim-to-real techniques to resolve the challenge. Among them, one of the most commonly used techniques is asymmetric training [35, 33], which is proposed as a one-stage solution for sim-to-real training.\nIn this paper, we adopt an asymmetric actor-critic (AAC) framework proposed for quadruped locomotion [6]. In this framework, the critic network has access to all privileged information, while the actor network only receives data available from onboard sensors, with a separate encoder to estimate the key privileged information (e.g., the linear velocity, robot's body height, and robot's feet swing height). The training paradigm incorporates the RL objective (including a value loss $\\mathcal{L}_{value}$ and a policy loss $\\mathcal{L}_{policy}$) with an estimation loss [33, 24, 6] $\\mathcal{L}_{est}$ to train the encoder:\n$\\mathcal{L}_{AAC} = \\mathcal{L}_{value} + \\mathcal{L}_{policy} + \\mathcal{L}_{est}$.\nIn this work, we take AAC as our default training framework, but the proposed techniques are not limited to it."}, {"title": "III. HUGWBC", "content": null}, {"title": "A. A General Command-Space for Humanoid Locomotion", "content": "We define the command space of the humanoid whole- body controller $\\mathcal{C} = \\mathcal{K} \\times \\mathcal{B}$ by two sets of commands, the task commands $\\mathcal{K}$ and the behavior commands $\\mathcal{B}$. The task commands determine a goal for the robot to reach, typically for movement, while the behavior commands construct the specific behavior pattern of the humanoid robots. In this work, we specify the task command as the target velocity $v_t \\in \\mathbb{R}^3$ (including the longitudinal and horizontal linear velocities $v_{t,x}, v_{t,y}$ and the angular velocity $\\omega_t$) at each time step t. As for the behavior command, we define the behavior command $b_t$ as a vector:\n$[f_t, l_t, h_t, p_t, w_t, \\Psi_t, \\Phi_{t,1}, \\Phi_{t, 2}, O_{t,stance}]_{gait}^{foot \\ \\ posture}$.\nwhere $f_t \\in \\mathbb{R}$ is the gait frequency and $l_t \\in \\mathbb{R}$ is the maximum foot swing height, both of which can be explained as foot behaviors. Besides, $h_t \\in \\mathbb{R}$ represents the body height, $p_t \\in \\mathbb{R}$ is the body pitch angle, and $w_t \\in \\mathbb{R}$ is the waist yaw rotation. These commands can be regarded as controlling the posture behavior. Beyond the commands above, we further introduce distinct gaits, such as walking, standing, jumping, and hopping. To do so, we refer to legged gait control [42, 30] and define $\\phi_i \\in [0,1)$, i = 1, 2 as two time-varying periodic phase variables to represent the humanoid gaits, on behalf of two legs (feet). These two phase variables can either be set as constants, or be computed by the phase offset $\\psi$ and the gait frequency $f_t \\in \\mathbb{R}$ at each time:\n$\\Phi_{t+1,1} = (\\Phi_{t,1} + f_t \\times d_t)$,\n$\\Phi_{t+1,2} = (\\Phi_{t+1,1} + \\psi)$,\nwhere $d_t$ is a discrete time step. When following the compu- tation of Eq. 4, each i loops in a range of [0, 1), resulting in repeated phase cycles. $O_{stance} \\in [0,1]$ is the duty cycle, which divides the gait cycle into two stages: stance (i.e., foot in contact with the ground) when $\\phi_i < O_{stance}$, and swinging (i.e., foot in the air) otherwise. $f$ is the stepping frequency, determining the wall time of each gait cycle.\nHumanoid gait control. We consider four distinct standard gaits in this project, i.e., walking, jumping, standing, hopping, and we note that the running gaits can be derived from the walking gaits via high-velocity and small duty cycle commands, which promotes the prolonged flight of both feet. By constructing the behavior commands above, we can adjust the phase offset psi, the duty cycle $O_{stance}$, and the phase variable $\\phi_i$ for each leg to control the humanoid robots in versatile and gaits. In this work, as we only consider standard gaits, so set the phase offset $\\psi = 0.5$ for walking gaits, since the phase difference between the left and right foot is half a cycle; regarding jumping gaits, the phase of the left and right foot is the same, thus we set $\\psi$ to 0. As for the standing and the hopping gaits, a certain foot of the robot is always in two states of contact or non-contact with the ground, which motivates a constant $\\phi_i$ (thus psi is not working). In particular, for the standing gait, we set $\\phi_i = 0.25$ for both feet; and for the hopping gait, we set $\\phi_i = 0.75$ for the flying leg, and $\\phi_i$ of another leg steps with frequency f. An illustrative explanation is shown in Fig. 3. The $O_{stance}$ determines the time ratio of stance and swinging during a gait cycle, and a smaller $O_{stance}$ will promote longer leg flight time. In this work, we set a constant $O_{stance} = 0.5$ for all supported gaits in all time steps.\nWe highlight that HUGWBC trained one single policy for the standing, walking, and jumping gaits, and an independent policy for the hopping gait."}, {"title": "B. Detailed Observation", "content": "In our asymmetric actor-critic framework, the observation for the critic network of obtains all information related to the environment, including proprioceptive observations $o_{pro}$, privileged observations $o_{pri}$, terrain observations $o_{ter}$, commands $c_t$, and an indicator signal I(t). Regarding the actor network, its available observation $o_t$ only contains history of proprioceptive observations within last k steps $o_{his} = (o_{t-k+1},\u00b7\u00b7\u00b7, o_{pro})$,\ncommands $c_t$, and the indicator signal I(t). The proprioceptive $o_{pro} \\in \\mathbb{R}^{63}$ consists of angular velocity and gravity projection in the robot's base frame, joint position, joint velocity, and previous policy output $a_{t-1}$. The privileged observations $o_{pri} \\in \\mathbb{R}^{24}$ contain the linear velocity, the base height error, foot clearance, friction coefficient of the ground, feet contact forces, and collision detection of the link (trunk, hip, thigh, shank, shoulder, and arm). The terrain observations $o_{ter} \\in \\mathbb{R}^{221}$ are samplings of terrain elevation points around the robot.\nCommands. The commands $c_t = [v_t, b_t]$ includes the task command (i.e., target velocity $v_t$ in this work) and the extended behavior command $b_t \\in \\mathbb{R}^{9}$, where we extend the behavior command $b_t$ defined above through replacing the phase variables $\\phi_i$, i = 1,2 with two additional clock functions $[Cl_L(t), Cl_R(t)] = [sin(2\\pi \\phi_{t,1}), sin(2\\pi \\phi_{t,2})]$ representing the contact of both feet, where $\\phi_{t,i}$, i = 1,2 are the homogenized phase variables defined in Eq. (7). Note that the sine function $sin(2\\pi \\phi_{t,i})$, i = 1,2 is a gait cycle contact indicator function, designed for a smoother transition between swinging and stance phases."}, {"title": "C. Reward Design for Policy Learning", "content": "Our humanoid whole-body controller is obtained through an asymmetric actor-critic training paradigm via reinforcement learning (RL). To learn a policy with general and diverse behaviors, we design a set of reward functions, which mainly consist of three parts: task rewards, behavior rewards, and regularization rewards.\nThe task rewards are meant to track any task command k, in this work, it is the target velocity v, including the linear and angular velocities. The regularization rewards take into account the performance of physical hardware and impose constraints on the smoothness and safety of the locomotion. These are commonly used in previous works [39]. In this work, since we want to build a general whole-body controller to support fine-grained locomotion behaviors for humanoid robots, we introduce a set of behavior rewards to encourage the robots to track any behavioral commands b, shown below.\nFor most behavior commands, including body height h, body pitch p, and waist rotation w, we simply formulated the rewards with mean squared error (MSE):\n$r^{ema}_{t} = -||t_{arget}^{cmd} - b_t||^2$.\nBeyond these simple tracking rewards, we further introduce periodic contact-swing rewards $r_{contact}$ [42, 30] and the foot trajectory reward $r_{swing}$ to help to generate complicated gaits.\nThe periodic contact-swing reward $r_{contact}$ is designed for fine-grained adjustments between swinging and stance in different gaits, according to $\\phi_i$. Since humanoid gaits can be expressed as different combinations of contact sequences, like foot contact forces and velocities, we define the periodic contact-swing rewards $r_{contact}$ over them to generate desired contact patterns.\nTo this end, we first define the expected contact probability function $C(\\Phi_{t,i})$ for leg i $\\in$ {1,2} at each time step t as:\n$C(\\Phi_{t,i}) = \\Phi((\\Phi_{\\tau,i}/\\sigma))[1 - \\Phi((\\Phi_{t,i} \u2013 0.5)/\\sigma)]$\n$+ \\Phi((\\Phi_{t,i} \u2212 1)/\\sigma)[1 \u2013 \\Phi((\\Phi_{t,i} \u2013 1.5)/\\sigma)]$,\n$\\Phi_i = \\begin{cases}0.5 \\times \\frac{\\Phi_i}{O_{stance}}, & \\Phi_i < O_{stance}\\\\\n0.5 + 0.5 \\times \\frac{\\Phi_i - O_{stance}}{1 - O_{stance}}, & \\Phi_i > O_{stance}\\end{cases}$,\nwhere $\\delta_i \\in [0,1]$ is a homogenized phase variable that maps the $\\phi_i$ of the stance and swinging phases to intervals [0,0.5] and [0.5, 1] according to $O_{stance}$, as computed in Eq. (7). $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution $N(0,1)$. The standard deviation allows for the relaxation of switching points ($\\phi_i = 0, 0.5$) to switching interval ($\\phi_i \\in [-3\\sigma, 3\\sigma], [0.5 \u2013 3\\sigma, 0.5 + 3\\sigma]$) (see Fig. 4 for detailed explanation). $C'(\\Phi_{t,i})$ is the probability of leg i coming into contact with the ground. Specifically, when $\\Phi_{t,i} \\in [0,0.5]$, the first term of $C'(\\Phi_{t,i})$ is dominant; otherwise the second term becomes dominant. Based on $C(\\Phi_{t,i})$, we then construct the periodic contact-swing rewards $r_{contact}$ to encourage humanoid robots to learn specific contact modes and generate various humanoid gaits:\n$r_{t}^{contact} = \\sum_{i=1}^2 C(\\Phi_{t,i}) [1 \u2013 exp (||u_{t,xy}^{foot, i}||^2/5)]\n+ [1 \u2013 C(\\Phi_{t,i})] [1 - exp (|| f^{foot, i} ||^2/50)]$,\nwhere $f^{foot}$ denotes the foot contact force and $u_{t,xy}^{foot}$ is the foot velocity. Note that during the stance phase, this reward function penalizes the foot velocities and ignore the foot contact force; on the other hand, during the swing phase, it penalizes the foot contact force and ignore the foot velocity."}, {"title": null, "content": "Except for the contact in fine-grained gait control, we also require the foot to smoothly reach the highest point and fall down, ensuring a precise and controllable swing. We introduce the foot trajectory reward $r_{swing}$ to achieve this:\n$r_t^{swing} = -\\sum_{i=1}^2[1-C(\\Phi_{t,i})] ||l_{target, i} \u2013 l^{foot,i}_{t}||^2$.\nNote that in Eq. (9), $l_{t}^{foot,i}$ denotes the actual swing height of foot i, $C(\\Phi_{t,i})$ is the expected contact probability function. $l_{target}$, is the target swing height, derived from a desired foot trajectory, as discussed below.\nA desired foot trajectory should typically require the fulfill- ment of three key criteria: 1) zero foot velocity and acceleration during the stance phase; 2) zero foot velocity and acceleration at the end of the swing phase; and 3) continuity of both foot velocity and acceleration during the transition between the two phases. This is beneficial for enhancing motion stability and reducing energy consumption. In this work, we follow the experience in robot kinetics and quadruped robots [7, 43], and incorporate the quintic polynomial trajectory to compute the target swing height $l_{target, i}$, at each control step:\nl^{target, i} =  \\begin{cases}\nl_t \\sum_{k=0}^5 a_k (0.25 \u2013 \\Phi_{t, i} \u2013 0.75)^k, & \\Phi_i > 0.5\\\\\n0, & \\Phi_i < 0.5\\\\\n\\end{cases}\nHere $l_t$ is the foot swing height command, and the polynomial coefficient ak is determined based on the homogenized phase variable $\\Phi_{t,i}$, as well as the boundary conditions of swing position, velocity, and acceleration. A detailed explanation of the calculation process is provided in the Appendix B-C. Note that Eq. (10) only defines the target trajectory in the z- axis. On natural terrains, the robot's precise foothold planning is not required. As for swing trajectories in the x-axis and the y-axis, which determines the stride, they can be computed based on the gait frequency f and the velocities $\\upsilon, \\omega$ [12, 8]."}, {"title": "D. Mirror Function and Symmetry Loss", "content": "Natural and symmetrical motion behavior is gradually mas- tered by humans through acquired learning, due to its inherent elegance and efficiency in minimizing energy expenditure. Humanoid robots, with highly biomimetic mechanisms, also have symmetrical structural features. However, without prior knowledge, the symmetrical morphology information is difficult to be explored by the policy, especially for policies that generate diverse behaviors. This makes the initial exploration much more difficult, making the policy easily fall into local optima and leading to unnatural movements. To leverage the advantage of this morphological symmetry and inspired by [49], we proposed the mirror function $F(\\cdot)$ for a humanoid robot to encourage policy to generate symmetric and natural motion. Under such a symmetrical structure, ideally, the policy output should satisfy:\n$\\pi(o^t) = F_a(\\pi(F_o(o^t)))$.\nIntuitively, the mirror function produces a mirror output symmetric to the X-Z plane. Here $F_a$ and $F_o$ are called action mirror function and observation mirror function, respectively, which map actions and observations to their mirrored version. Derived from these symmetric functions, we define a symmetry loss function $\\mathcal{L}_{sym}$. The policy learning objective for controlling robots with symmetrical structures can be written as:\n$\\mathcal{L}_{sym} = \\sum_t ||\\pi(o^t) \u2013 F_a(\\pi(F_o(o^t)))||^2$,\nThe $\\mathcal{L}_{sym}$ is independent of the RL objective, making it easy to be extended to different RL algorithms. It is worth noting that the symmetric loss function is in fact encouraging symmetric actions on symmetric states (and commands), and it can be utilized for behaviors from symmetric ones (like walking and jumping) to asymmetric ones, such as hopping gaits, where hopping with the left foot is symmetric to the hopping with the right one.\nOverall training objective. HUGWBC adopt an asymmetric actor-critic framework [33], taking PPO [40] as the RL algorithm to train the whole-body policy. Therefore, the total training objective can be written as:\n$\\mathcal{L} = \\mathcal{L}_{AAC} + \\beta \\mathcal{L}_{sym}$,\nwhere \u03b2 is a weight coefficient to balance between minimizing the RL objective and symmetry gait (we simply set \u03b2 = 0.5 in our practice). We implemented a critic network, an actor network, along with the privileged encoder, all as Multi-Layer Perceptrons (MLPs). The actor network, combined with the encoder, can be directly deployed onto the real robot at a control frequency of 50 Hz."}, {"title": "E. External Upper-Body Intervention Training", "content": "So far we learned a whole-body controller, which controls the upper and lower body jointly. Nevertheless, the goal of this work is not a controller specifically designed for locomotion tasks, but to build a unified and general humanoid controller that can serve as a fundamental support for loco-manipulation tasks. In other words, our controller should also support flexible and precise control of the upper body (arm and hands). Different from some previous works [16, 18] that augment the command space with upper body commands (e.g., arm joint positions), we consider decoupling the upper body control as external control intervention by teleoperation signals [4, 26] or retargeted motion joints [3, 20], while not affecting the lower-body gaits, due to their high control precision. Our solution is sampling alternative actions to replace the upper-body actions produced by the whole-body policy during training, making the policy robust to any intervention.\nSwitching between whole-body control and intervention. Denote I(t) a binary indicator function for whether the external control signal intervenes at each time step t, we assign a small probability of p (p = 0.005 in this work) to reverse I(t). This leads the expected length of a continuous sequence without changing the upper-body control mode to be $\\sum_{n=1}^{\\infty} n p(1 \u2013 p)^n = \\frac{1}{p}$, ensuring infrequently switching between two control modes and most of the trajectories are either long sequence of whole-body controlling or intervention, preventing rapidly switches.\nIntervention sampling. The intervened actions of the hu- manoid upper body are sampled from uniform noises, which introduce the potential for collisions with the body, simulating erroneous operations during external intervention.\nNoise intervention interpolation. To prevent meaningless jitters caused by noise intervention sampling, the intervention action $a^{interv}_{noise}$ is randomly sampled in the action space every $t_{interval}$ = 90 time steps. During the first two third time steps in the interval, linear interpolation is applied to smoothly transition the intervention joint positions from the initial pose $a^{interv}_{init}$ to the target pose $a^{noise}$, while the target intervention action is maintained for the remaining time steps.\n$a^{targ}_{interv,t} = (1 - r) a^{interv}_{init} + r a^{noise}$,\n$r = min \\Big(1, \\frac{3t-t_0}{2 t_{interval}}\\Big)$.\nIn this equation, r is the ratio for the linear interpolation and $t_0$ is the sampled time.\nReward mask. When the intervention is involved, we mask the regularization rewards of the upper body during training, in order to eliminate the potential conflict of the policy output that tries to take over the upper body.\nNoise curriculum. The replaced intervention action $a^{interv}$ is gradually transited from the policy action $a_t$ to the sampled noise $a^{noise}$:\n$a_{interv} = \\alpha a^{noise,t} + (1-\\alpha) a_t$,\nwhere the smoothing factor \u03b1 increases per the progression of the intervention curriculum. In detail, a increases by 0.01 when"}, {"title": "F. Curriculum Learning", "content": "Directly learning a diverse policy from manual rewards presents significant challenges due to the simultaneous opti- mization and exploration of multiple objectives. We thereby propose a curriculum learning approach to improve training efficiency. In particular, we split two distinct parallel robot training groups: an \"agile group\", tasked with learning high- speed, agile locomotion, and an \u201cintervention group\u201d, focused on developing a control policy for managing external upper- body interventions. At the beginning of training, each group of robots randomly samples one specific gait from four humanoid gaits, i.e., standing, walking, jumping, hopping. The remaining behavioral commands ($f_t, l_t, h_t, p_t, w_t$) and the task commands $v_t$ are uniformly sampled from the specified ranges, which can be further referred to in Tab. II. Following Wu et al. [48], we employ a terrain curriculum for both groups, which only consists of rough terrain. Once the robot finishes the most challenging terrain level, we keep the hardest terrain and initiate an intervention noise curriculum and a speed curriculum at the same time. On the one hand, the speed curriculum only works for the agile group, meant to learn high agility, which gradually increases the speed commands $v_t$ following a grid adaptive curriculum strategy [29]. On the other hand, the intervention noise curriculum as described in Section III-E works for the intervention group, focused on working with arbitrary upper- body intervention signals."}, {"title": "IV. EXPERIMENT", "content": "In this section, we conduct comprehensive experiments in both simulation and the real-world robot to address the following questions:\n\u2022 Q1(Sim): How does the HUGWBC policy perform in tracking across different commands?\n\u2022 Q2(Sim): How to reasonably combine various commands in the general command space?\n\u2022 Q3(Sim): How does large-scale noise intervention training help in policy robustness?\n\u2022 Q4(Real): How does HUGWBC behave in the real world?\nRobot and Simulator. Our main experiments in this paper are conducted on the Unitree H1 robot, which has 19 Degrees of Freedom (DOF) in total, including two 3-DOF shoulder joints, two elbow joints, one waist joint, two 3-DOF hip joints, two knee joints, and two ankle joints. The simulation training is based on the NVIDIA IsaacGym simulator [28].\nCommand analysis principle and metric. One of the main contributions of this paper is an extended and general command space for humanoid robots. Therefore, we pay much attention to command analysis (regarding Q1 and Q2). This includes analysis of single command tracking errors, along with the combination of different commands under different gaits. For analysis, we evaluate the averaged episodic command tracking error (denoted as $E_{cmd}$), which measures the discrepancy between the actual robot states and the command space using the first-order norm. All commands are uniformly sampled within a pre-defined command range, as shown in Tab. II\u00b9."}, {"title": "A. Single Command Tracking", "content": "We first analyze each command separately while keeping all other commands held at their default values. It is easily observed that the tracking errors in the walking and standing gaits are significantly lower than those in the jumping and hopping, with hopping exhibiting the largest tracking errors. For hopping gaits, the robot may fall during the tracking of specific commands, like high-speed tracking, body pitch, and waist-yaw control. This can be attributed to the fact that hopping requires rather high stability. Moreover, the complex postures and motions further exacerbate the risk of instability. Consequently, the policy prioritizes learning to maintain the balance, which, to some extent, compromises the accuracy of command tracking.\nWe conclude that the tracking accuracy of each gait aligns with the training difficulty of that gait in simulation. For example, the walking and standing patterns can be learned first during training, while the jumping and hopping gaits appear later and require an extended training period for the robot to acquire proficiency. Similarly, the tracking accuracy of robots under low velocity is significantly better than those under high velocity, since 1) the locomotion skills under low velocity are much easier to master, and 2) the dynamic stability of the robot decreases at high speeds, leading to a trade-off with tracking accuracy. We also found that the tracking accuracy for longitudinal velocity commands $v_x$ surpasses that of horizontal velocity commands $v_y$, which is due to the limitation of the hardware configuration of the selected Unitree H1 robots. In addition, the foot swing height l is the least accurately tracked. Furthermore, the tracking reward related to foot placement outperforms the tracking performance associated with posture control, since adjusting posture introduces greater challenges to stability. In response, the policy adopts more conservative actions to mitigate balance-threatening postural changes."}, {"title": "B. Command Combination Analysis", "content": "To provide an in-depth analysis of the command space and to reveal the underlying interaction of various commands under different gaits. Here, we aim to analyze the orthogonality of commands based on the interference or conflict between the tracking errors of these commands across their reasonable ranges. For instance, when we say that a set of commands are orthogonal, each command does not significantly affect the tracking performance of each other in its range. To this end, we plot the tracking error $E_{cmd}$ as heat maps, generated by systematically scanning the command values for each pair of parameters, revealing the correlation of each command. We leave the full heat maps at Appendix C-A, and conclude our main observation for all gaits.\nWalking. Walking is the most basic gait, which preserve the best performance of the robot hardware.\n\u2022 The linear velocity $v_x$, the angular velocity yaw $w$, the body height h, and the waist yaw w are orthogonal during walking.\n\u2022 When the linear velocity $v$ exceeds 1.5m/s, the orthog- onality between $v_x$ and other commands decreases due to reduced dynamic stability and the robot's need to maintain body stability over tracking accuracy.\n\u2022 The gait frequency f shows discrete orthogonality, with optimal tracking performance at frequencies of 1.5 or 2. High-frequency gait conditions reduce tracking accuracy.\n\u2022 The linear velocity $v_y$, the foot swing height l, and the body pitch p are orthogonal to other commands only within a narrow range.\nJumping. The command orthogonality in jumping is similar to walking, but the overall orthogonal range is smaller, due to the increased challenge of the jumping gait, especially in high- speed movement modes. During each gait cycle, the robot must leap forward significantly to maintain its speed. To execute this complex jumping action continuously, the robot must adopt an optimal posture at the beginning of each cycle. Both legs exert substantial torque to propel the body forward. Upon landing, the robot must quickly readjust its posture to maintain stability and repeat the actions. Consequently, during movement, the robot can only execute other commands within a relatively narrow range.\nHopping. The hopping gait introduces more instability, and the robot's control system must focus more on maintaining balance, making it difficult to simultaneously handle complex, multi-dimensional commands.\n\u2022 Hopping gait commands lack clear orthogonal relationships.\n\u2022 Effective tracking is limited to the x-axis linear velocity $v_x$, the y-axis linear velocity $v_y$, the angular velocity yaw w, and the body height h.\n\u2022 Adjustments to h can be understood that a lower body height improves dynamic stability, therefore, it plays a positive role in maintaining the target body posture.\nStanding. As for the standing gait, we tested the tracking errors of commands related to posture. The results showed that the tracking errors were similar to those observed during walking with zero velocity.\n\u2022 The waist yaw w command is almost orthogonal to the other two commands.\n\u2022 As the range of commands increases, orthogonality between the body height h and the body pitch p decreases. This is because the H1 robot has only one degree of freedom at the waist, limiting posture adjustments to the hip pitch joint.\n\u2022 A 0.3 m decrease of the body height relative to the default height reduces the range of motion of the hip pitch joint to almost zero, hindering precise tracking of body pitch.\nFurthermore, we conclude that gait frequency f highly affects the tracking accuracy of movement commands when it is excessively high and low; the posture commands can significantly impact the tracking errors of other commands, especially when they are near the range limits. For different gaits, the orthogonality range between commands is greatest in the walking gait and smallest in the hopping gait."}, {"title": "C. Ablation on Intervention Training Strategy", "content": "To validate the effectiveness of the intervention training strategy on the policy robustness when external upper-body intervention is involved, we compare the policies"}]}