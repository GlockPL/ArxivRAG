{"title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting", "authors": ["Hyungjun Yoon", "Biniyam Aschalew Tolera", "Taesik Gong", "Kimin Lee", "Sung-Ju Lee"], "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across various domains. However, utilizing LLMs for ubiquitous sensing applications remains challenging as existing text-prompt methods show significant performance degradation when handling long sensor data sequences. We propose a visual prompting approach for sensor data using multimodal LLMs (MLLMs). We design a visual prompt that directs MLLMs to utilize visualized sensor data alongside the target sensory task descriptions. Additionally, we introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. We evaluated our approach on nine sensory tasks involving four sensing modalities, achieving an average of 10% higher accuracy than text-based prompts and reducing token costs by 15.8\u00d7. Our findings highlight the effectiveness and cost-efficiency of visual prompts with MLLMs for various sensory tasks.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown remarkable performance in tasks across diverse domains, including science, mathematics, medicine, and psychology (Bubeck et al., 2023). The recent advent of multimodal LLMs (MLLMs), e.g., GPT-40 (OpenAI, 2024), has further expanded their capabilities to images and audio inputs, broadening their use in fields such as industry and medical imaging (Yang et al., 2023). Meanwhile, sensor data, widely used in healthcare (Ferguson et al., 2022) and environmental monitoring (Hayat et al., 2019), hold potential for ubiquitous applications when effectively integrated with MLLMs. However, the diversity of sensors (Wang et al., 2019) and the heterogeneity among them (Stisen et al., 2015) hinder the implementation of a foundational model that generalizes across various sensing tasks.\nThe expensive data collection (Vijayan et al., 2021) often results in insufficient training data, further complicating the development of such capability.\nRecent studies explored leveraging pre-trained LLMs to solve general sensory tasks (Yu et al., 2023; Liu et al., 2023; Kim et al., 2024). One approach extracts task-specific features from sensor data and composes them as prompts (Yu et al., 2023). However, designing such prompts requires specific domain knowledge. Alternatively, incorporating raw sensor data as text prompts (Kim et al., 2024; Liu et al., 2023) has been a widely used method to handle sensory data with LLMs as a more generalizable solution. Yet, we empirically found that providing raw sensor data with text prompts shows poor performance in real-world sensory tasks with long-sequence inputs and incurs"}, {"title": "2 Related Work", "content": "LLMs with sensor data. Sensory tasks involve sequences of numbers indicating values over time. Initial research for handling sequential data focused on time-series forecasting (Zhang et al., 2024b). Converting time-series data into text prompts for forecasting has been proposed in PromptCast (Xue and Salim, 2023) and LLMTime (Gruver et al., 2024). Other studies (Zhou et al., 2023; Jin et al., 2023a) used specialized encoders to create embeddings compatible with pre-trained LLMs.\nBeyond forecasting, LLMs have been explored in healthcare for their ability to answer questions using physiological sensor data (Liu et al., 2023). For example, LLMs have been used for ECG diagnosis (Yu et al., 2023) by integrating ECG-specific features and retrieval-augmented knowledge from ECG databases. Penetrative AI (Xu et al., 2024) and Health-LLM (Kim et al., 2024) have used raw sensor data in text prompts to solve health problems without task-specific processing. Our study examines whether existing methods can generalize to broader sensing tasks with high-frequency, long-duration data. Building upon these works, we propose visualizing sensor data for MLLMs to improve their performance and cost efficiency.\nMultimodal large language models (MLLMs). Advancements in MLLMs (Zhang et al., 2024a) have equipped popular models such as ChatGPT (OpenAI, 2022) with vision capabilities (OpenAI, 2024). Recent studies explored the in-context learning (Brown et al., 2020) abilities of MLLMs, showing that they can understand images with the interleaved text and few-shot examples (Tsimpoukelli et al., 2021; Alayrac et al., 2022). This capability has been applied in medical diagnostics, including analyzing radiology and brain images with accompanying text instructions (Wu et al., 2023). Our work explores using MLLMs to analyze visualized sensor data for broader applications.\nUsing tools with LLMs. Recent research has shown that augmenting LLMs with external tools can extend their capabilities. Toolformer (Schick et al., 2024) enables LLMs to access public APIs and search engines, while Visual Programming (Gupta and Kembhavi, 2023) uses LLMs to generate and execute codes. HuggingGPT (Shen et al., 2024) and Chameleon (Lu et al., 2024) integrated multiple expert models to enhance functionalities. Our work builds upon these works by enabling MLLMs to utilize sensor data visualization tools. Moreover, we propose a design that not only uses tools but also allows MLLMs to assess their effectiveness, ensuring optimal visualization for specific tasks."}, {"title": "3 Limitations of Representing Sensor Data as Text-based Prompts", "content": "Existing approaches for grounding language models with sensor data primarily rely on text-based prompts (Liu et al., 2023; Jin et al., 2023b; Zhang et al., 2024b; Yu et al., 2023). One approach uses prompts with specialized features extracted from sensor data for specific tasks, such as R-R inter-"}, {"title": "4 Method", "content": "We introduce our method for handling sensory tasks by providing sensor data as image inputs to MLLMs. Section 4.1 overviews our prompt design strategy. Section 4.2 introduces our visualization"}, {"title": "4.1 Visual Prompt Design", "content": "To leverage MLLMs for sensory tasks, we propose a visual prompt, as illustrated in Figure 3. The key idea is to transform numeric sequences of sensor data into visual plots using various methods, such as raw waveforms and spectrograms. Detailed information about these visualization methods is in Section 4.2. For few-shot examples, each plot includes a label as a title above it (i.e., {{Label of example X}}). For unlabeled target data used in queries, the title is simply stated as \"target data\". We provide textual instructions to clarify the data collection process and the task's objectives. These instructions ensure that MLLMs can effectively interpret and utilize the visualized sensor data."}, {"title": "4.2 Visualization Generator", "content": "In our proposed visual prompt, the choice of visualization method is crucial, as it significantly influences the MLLM's ability to comprehend the sensor data. For example, raw waveform plots are ideal for tasks involving amplitude pattern recognition over time, while spectrograms (Ito et al., 2018) are suitable for tasks relying on frequency features.\nWe introduce a visualization generator that automatically chooses the most suitable visualization tool from available public libraries, enabling non-expert users to effectively utilize visual prompts. This generator operates in two main phases: (i) visualization tool filtering and (ii) visualization selection (see Figure 4)."}, {"title": "5 Experiments", "content": "We evaluate the applicability of our approach with MLLMs by conducting experiments on a range of sensory tasks."}, {"title": "5.1 Setups", "content": "We assume a practical scenario where non-expert users attempt to solve sensory tasks using MLLMs (1) without prior knowledge of relevant features and (2) without external resources to fine-tune the MLLM. Given the constraints, we leveraged the few-shot prompting (Brown et al., 2020) approach. For the main evaluation, we used 1-shot examples where users provide the MLLM with minimal examples to guide task-solving.\nSensory tasks We established nine different sensory tasks across four sensor modalities: accelerometer, electrocardiography (ECG) sensor, electromyography (EMG) sensor, and respiration sensor. We used three datasets for tasks using accelerometers: HHAR (Stisen et al., 2015) for basic human activity recognition (running and walking), UTD-MHAD (Chen et al., 2015) for complex activity recognition with fine-grained arm motions, and a swimming style recognition dataset (Brunner et al., 2019). We use the PTB-XL (Wagner et al., 2020) dataset for the arrhythmia diagnosis tasks that use ECG. The dataset includes detection tasks for four different types of arrhythmia symptoms. For EMG data, we used a dataset (Ozdemir et al., 2022) for hand gesture recognition. Finally, we used a stress detection task using respiration sensors provided by the WESAD (Schmidt et al., 2018) dataset. Details on each task, including the classes, sampling rates, windowing durations, and specific configurations, are in Appendix D.\nData processing. We normalized data using the mean and standard deviation values calculated for each user. Test splits were created by randomly sampling 30 samples per class. For the UTD-MHAD dataset, we sampled 10 samples per class"}, {"title": "5.2 Results", "content": "Performance. Table 1 shows the overall performance of utilizing visual prompts for solving sensory tasks. For the same 1-shot prompting, visual prompts consistently showed enhanced accuracies than text-only prompts, achieving an average increase of 10%. Notably, the UTD-MHAD task exhibited a significant accuracy gain of up to 33%. See Appendix E for prompt examples with resulting visualizations.\nIn addition to achieving higher accuracy, visual prompts are more cost-effective. The number of tokens used for visual prompts in Table 1 shows a substantial reduction, averaging 15.8\u00d7 fewer than text-only prompts. MLLMs calculate token costs for images within the same token space as text but with distinct counting criteria. In our experiments, GPT-40 counts tokens for images based on the number of 512 \u00d7 512 pixel blocks (N) covering the image input, calculated at 85 + 170 \u00d7 N. Our visualized sensor data was represented within a single 512 \u00d7 512 pixel image, regardless of the sensor data length, significantly reducing costs. Note that the number of tokens from visual prompts is only affected by the number of examples, as all images are the same size. In contrast, text prompts are heavily influenced by high sampling rates and long durations.\nTo further understand the effectiveness of visual prompts with small tokens, we analyzed the information capacity at the same token cost. Considering a budget of 500 tokens, text-based prompts can"}, {"title": "6 Conclusion", "content": "We addressed sensory tasks by providing visualized sensor data as images to MLLMs. We designed a visual prompt to instruct MLLMs in using visualized sensor data, provided with textual descriptions of the task and data collection methods. Additionally, we introduced a visualization generator that automatically selects the best visualization method for each task using visualization tools available in public libraries. We conducted experiments across nine different sensory tasks and four sensor modalities, each with a distinct task. Our results suggest that the visual prompts generated by our visualization generator not only improve accuracy by an average of 10% over text-based prompts but also significantly reduce costs, requiring 15.8\u00d7 fewer tokens. This indicates that our approach with visual prompts and a visualization generator is a practical solution for general sensory tasks."}, {"title": "Limitations", "content": "Our study demonstrates the effectiveness of visual prompts on nine different sensory tasks, primarily focusing on classification. While visual prompts effectively highlight patterns over images, for tasks requiring numerical retrieval or precise computations\u2014where exact values are critical\u2014text prompts can be more effective due to their inclusion of specific numeric data, which are omitted in visual representations. Notably, our approach integrates both images and texts in prompts, allowing the inclusion of numerical values in the text. Determining the optimal distribution of information between images and text to compose a prompt that effectively addresses sensory tasks presents a future direction for this work.\nVisualizing sensor data as plots often presents challenges. For instance, brain wave analysis using high-density EEG involves up to 256 channels (Fiedler et al., 2022), complicating their representation in a single visual plot. We denote different channels as distinct notations within a plot, making densely populated plots visually indecipherable. An alternative method of plotting distinct channels across separate subplots was explored but resulted in a significant drop in performance (see Appendix 4). We hypothesize that this limitation arises from the dispersion of information across various areas, highlighting that effective visualization of large-channel datasets remains challenging. This underscores the need for improved visualization techniques in such scenarios.\nOur visual prompt design does not incorporate Chain-of-Thought (CoT) prompting (Kojima et al., 2022). Experiments using zero-shot CoT on our datasets revealed inconsistent benefits (see Appendix A), unlike the widely known effect of CoT for enhancing performance. We suspect this may be due to the complexities of reasoning over sensory data. Given the observation, further research is needed to develop methods that effectively integrate reasoning and interpretation into the decision-"}, {"title": "A Effect of Zero-shot Chain-of-Thoughts", "content": "We experimented with zero-shot Chain-of-Thought (CoT) prompting (Kojima et al., 2022) by adding \"let's think step-by-step\" to our prompts, testing this on two accelerometers and two ECG datasets. Table 3 shows the findings. While CoT prompting is generally known to enhance LLM response quality, our results showed inconsistent performance by datasets. Notably, CoT consistently dropped performance for text-only prompts. We analyzed the results by observing the CoT responses, illustrated as examples in Figures 8 and Figure 9, showing wrong predictions with CoT from the HHAR dataset. We found that CoT reasoning in text-only prompts primarily focused on simple statistical comparisons, such as whether values were higher or lower. This simplistic approach proved inadequate for analyzing the complexities of sensor data, leading to suboptimal responses. Likewise, visual prompts indicated reasoning centered around terms like \"variations,\" \"periodic,\" and \"stable,\" but they lacked the necessary depth to effectively assess more intricate features like frequency trends or signal shapes. This superficial reasoning suggests a significant gap in the CoT approach, underscoring the need for more task-specific reasoning prompts for sensory data analysis."}, {"title": "B Use of Subplots for Multi-channel Data", "content": "Sensor data often include multiple channels. Our visual prompts differentiated channels using varying colors within a single plot to maintain a shared axis system. To assess the impact of different plotting approaches, we conducted experiments using accelerometer datasets, which have three channels. Specifically, we compared visualizing three distinct plots for each channel against our current approach. Table 4 shows the results. The results indicated that separated plots for each channel reduced performance by 12%. We hypothesize that multiple subplots distribute visual features over different regions, resulting in problems in understanding the relationship between different channels. To this end, we recommend using an aggregated plot when all channels can be represented within a plot. However, for dense datasets, such as 256-channel EEG (Fiedler et al., 2022), a single plot may not suffice, highlighting a limitation in our current visualization approach. Addressing this challenge will be a focus of future research."}, {"title": "C Visualization Tools", "content": "Our visualization generator employs tools available in public libraries to create visualizations. We have equipped the visualization generator with 16 distinct visualization functions sourced from widely used libraries such as Matplotlib (Hunter, 2007), Scipy (Virtanen et al., 2020), and Neurokit2 (Makowski et al., 2021). The specific visualization tools implemented in our generator and their descriptions are outlined in Table 5. The descriptions presented in the table were directly written inside the prompt for the visualization tool filtering (see Appendix E)."}, {"title": "D Details of Sensory Tasks", "content": "We conducted experiments across nine sensory tasks across four sensor modalities, each with unique objectives. This section provides the details of these tasks, including task descriptions, classifications, sampling rates, window durations, and data collection protocols. We directly followed the given sampling rate with the original dataset to represent data in text prompts. The descriptions of each dataset are used to formulate the instructions for our visual prompts. The complete prompt examples are in Appendix E.\nHuman activity recognition: We used the HHAR (Stisen et al., 2015) dataset to classify six basic human activities: sit, stand, walk, bike, upstairs, and downstairs. Data were collected from"}]}