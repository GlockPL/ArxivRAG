{"title": "CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis", "authors": ["Junying Chen", "Chi Gui", "Anningzhe Gao", "Ke Ji", "Xidong Wang", "Xiang Wan", "Benyou Wang"], "abstract": "The field of medical diagnosis has undergone a significant transformation with the advent of large language models (LLMs), yet the challenges of interpretability within these models remain largely unaddressed. This study introduces Chain-of-Diagnosis (CoD) to enhance the interpretability of LLM-based medical diagnostics. CoD transforms the diagnostic process into a diagnostic chain that mirrors a physician's thought process, providing a transparent reasoning pathway. Additionally, CoD outputs the disease confidence distribution to ensure transparency in decision-making. This interpretability makes model diagnostics controllable and aids in identifying critical symptoms for inquiry through the entropy reduction of confidences. With CoD, we developed DiagnosisGPT, capable of diagnosing 9,604 diseases. Experimental results demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring controllability in diagnostic rigor.", "sections": [{"title": "1 Introduction", "content": "In healthcare, disease diagnosis is pivotal yet complex, serving as a bridge between medical expertise and decision-making [1, 2]. The diagnosis task involves predicting a disease using explicit (self-reported) and implicit (inquired) symptoms [3-5]. Large Language Models (LLMs) offer a promising path for automated diagnosis, due to their superior reasoning and dialogue abilities, coupled with extensive knowledge. These capabilities enable them to address a wide range of diseases and interact effectively with patients [6].\nHowever, the application of LLMs in medical diagnosis encounters significant challenges of interpretability [7, 8]. Considering the issue of hallucinations, LLMs could arbitrarily make a diagnosis. Without interpretability, it is unclear if decisions meet sound analysis and ethical standards [9]. Although LLMs can offer rudimentary explanations for their decision, they lack a comprehensive process to explain why other potential diseases are excluded and to which extent of confidence it made such a decision. [10] This highlights the need for an interpretable LLM solution for diagnosis.\nIn response to these limitations, we propose the Chain of Diagnosis (CoD) to enhance the interpretability of LLMs. CoD provides transparency in both reasoning and decision-making. It transforms the black-box decision-making process into a diagnostic chain that mirrors a physician's thinking process with five distinct steps. It reveals the series of thoughts behind each decision. For decision transparency, CoD outputs a confidence distribution, with higher confidence indicating a stronger belief in diagnosing a specific disease. This allows for control over the LLM's decisions using a confidence threshold. Additionally, diagnostic uncertainty can be quantified by the entropy of the confidences. The goal of entropy reduction can aid in eliciting more effective symptoms for inquiry."}, {"title": "2 Problem Definition of Diagnosis", "content": "The objective of diagnosis is to enhance disease identification accuracy through symptom inquiry. However, there is a trade-off between efficiency and accuracy. Efficient diagnosis is faster with fewer inquiries, while accurate diagnosis requires more detailed inquiries. LLMs may struggle with symptom inquiry, particularly in determining when to ask questions. Additionally, symptom inquiry by LLMs does not significantly improve accuracy, indicating that LLMs might fail to ask for crucial symptom information."}, {"title": "3 Methodology: Chain of Diagnosis", "content": "As depicted on the left side of Figure 2, the CoD outputs a diagnostic chain that represents the thought process of LLM diagnosis. It mirrors a physician's diagnostic thinking, which involves summarizing symptoms, identifying diseases, analyzing and deciding. This interpretability aids in diagnostic decision-making. To implement the CoD, we constructed CoD training data based on patient cases to fine-tune LLMs to perform CoD, as shown on the right side of Figure 2."}, {"title": "3.1 The Philosophy of CoD for Interpretability", "content": "Lipton [12] classified interpretability into two aspects: 1) transparency, i.e., how does the model work? and 2) post-hoc explanations, i.e., what can the learned model tell us? The two aspects inspire us design an interpretable diagnostic LLM, consisting of a CoD framework that includes Property 1, 2 for transparency and Property 3 for post-hoc explanations.\nTransparency [12] encompasses two types: decomposability and algorithmic transparency. Decomposability refers to a system where each component can be individually interpreted. For CoD, this is demonstrated in Property 1.\nProperty 1 Decomposabilty with a pipeline chain: CoD transform the black-box decision of diagnosis into a explainable chain, which can be viewed as a sequence of intermediate steps. Therefore, the designed CoD is a chain pipeline (see Sec. 3.2), each step of which has a clear functionality. The overall chain mimics a real diagnosis of a physician.\nAlgorithmic transparency refers to the understanding of the learning algorithm itself, such as whether it converges and why. Regrading the challenge in Sec. 2.2, the algorithmic transparency of CoD can be understood from a entropy-reduction perspective: with more inquiries made, the uncertainty of the diagnosis estimate (quantitatively measured by entropy) will be reduced, see Property 2.\nProperty 2 Transparency with confidence-driven flow: In the chain, CoD introduces a disease confidence distribution $C = \\{c_d | d \\in D\\}$, where decision-making is based on whether the highest"}, {"title": "3.2 The Diagnosis Chain", "content": "Here, we introduce the response methods and the construction approach of CoD, as illustrated in Figure 2. All prompts for building CoD training data are detailed in Appendix F.\nStep 1: Symptom Abstraction The first step summarizes the symptoms $S$ of the patient's question:\n$S = f_1(q_{patient})$\nIt allow the model to focus on the refined symptoms and provide an understanding of patient's query. For training data, the initial patient question is generated from $(S_{exp})$ with the LLM.\nStep 2: Disease Recall & Knowledge Integration\nNext, CoD identifies the top-K potential diseases based on a disease retriever:\n$D' = f_2(D, S, k)$\nwhere $D' \\subset D$ and $|D'| = k$. A smaller space $D'$ is necessary for subsequent analysis and reasoning, since analyzing all diseases is impractical (considering $|D| = 9604$) and most irrelevant diseases can realistically be excluded. We use Dense Retrieval training methods [13, 14] to train this retriever, with the following training objective:\n$L (S_{exp}, S_{imp}, d_t) = - log \\frac{e^{sim(E_S (S_{exp} \\cup S_{imp}), E_D(d_t))}}{\\sum_{d \\in D} e^{sim(E_S (S_{exp} \\cup S_{imp}), E_D (d))}}$\nwhere $sim$ denotes the cosine similarity, and $E_S$ and $E_D$ are the symptom and disease encoders, respectively. The performance of the disease retriever is detailed in Appendix I.\nThen, for each candidate disease $d \\in D'$, CoD retrieves corresponding disease knowledge from the disease database and integrates it into the output to enhance understanding of the disease. Similarly, other tools like RAG can also be utilized in this step to enhance reasoning.\nStep 3: Diagnostic Reasoning In step 3, CoD generates the diagnostic reasoning process $T$:\n$T = f_3(S,D')$\nSimilar to CoT, $T$ is a thought process that carefully analyzes whether each disease in $D'$ corresponds to the patient's symptoms. To build training data, we prompt a LLM to generate $T$.\nStep 4: Confidence Assessment After generating $T$, CoD generates a confidence distribution:\n$C = f_4(S,D',T)$\n$C$ satisfies $\\sum_{d \\in D'}, c_d = 1$. This distribution indicates the model's tendency towards diagnosing a disease, mainly according to the analysis of $T$. According to $f_3$, $C$ can be considered a posterior probability distribution:\n$C = \\{p_{\\theta}(d|S, D')| d \\in D'\\}$\nHere, $p_{\\theta}$ represents the confidence distribution generated by the LLM $\\theta$. For constructing training data, we validate $C$ against the target disease $d_t$ to ensure $T$ and $C$ are reasonable. If $max_{d \\in D' \\setminus \\{d_t\\}} c_d \\geq \\tau$, the generated data is considered erroneous, i.e., the model assigns high confidence to an incorrect disease. If erroneous, we prompt the model to rethink and correct its reasoning until the distribution is verified. With $C$, CoD can make decisions based on the confidence in its diagnosis.\nStep 5: Decision Making In the last step, a confidence threshold $\\tau$ is set to control the decision-making. The diagnostic task involves two decision types: 1) making a diagnosis $A_{diag}(d)$, where $d$ is"}, {"title": "3.3 CoD as an Entropy-reduction Process", "content": "Symptom inquiry is a key step in diagnosis, serving to gather additional patient information to clarify the diagnosis. This inquiry process can be viewed as a transition from diagnostic uncertainty to certainty. The uncertainty level can be captured by the entropy of confidence:\n$H(C) = - \\sum_{d \\in D'} c_d log c_d$\nSymptom inquiry is a process of entropy reduction. Given a symptom $s$, its post-inquiry entropy is:\n$H(C|s) = \\sum_{d \\in D'} p_{\\theta}(d|S \\cup \\{s\\}, D') log p_{\\theta}(d|S \\cup \\{s\\}, D')$\nFor the diagnostic task, it is preferable to minimize $n$. Hence, the objective of symptom inquiry can be formalized as maximizing the increase in diagnostic certainty to expedite the diagnosis. Accordingly, CoD selects the symptom to inquire about by maximizing the entropy reduction:\n$s_t = argmax_{s \\in S'} (H(C) - H(C|s))$\nwhere $S'$ represents the candidate symptoms for inquiry and $s_t$ is the chosen symptom. $S' = S_{imp} \\cup \\{s_{gen}\\}$, where $s_{gen}$ is the symptom generated by the LLM and $S_{imp}$ comes from the training case data. Through entropy reduction, the CoD training data tuned the model to inquire about more crucial symptoms for diagnosis, thereby enhancing its querying capability."}, {"title": "3.4 Synthesizing Patient Cases", "content": "CoD requires patient cases to build training data. However, due to privacy concerns, the collection of such data is significantly restricted. To address this, we propose generating synthetic case data in reverse from online disease encyclopedias, which provide comprehensive and reliable disease information. As illustrated in Figure 3, the synthesis process is a pipeline consists of two stages:\nStage 1: Constructing Disease Database The first step involves the extraction of essential information from the disease encyclopedia data. This process results in a knowledge base encompassing 9,604 diseases, each detailed with sections on \"Overview,\" \"Symptoms,\" and \"Treatment\". We use regular expression matching to identify and extract these key sections.\nStage 2: Synthesizing Patient Cases In disease diagnosis [3, 15], a patient can be abstracted into a triplet $(S_{exp}, S_{imp}, d_t)$. Using the LLM, we generate structured case data based on the disease"}, {"title": "4 Experiments", "content": "Utilizing the created CoD data, we fully fine-tuned the Yi-34B-Base [16] to develop DiagnosisGPT. To equip it with chat capabilities, ShareGPT data is incorporated into the training data. Training parameters included a batch size of 64 and a learning rate of 2e-5. For the retrieval model, we trained on the all-mpnet-base-v2 [17] model using DRhard [18], with a batch size of 256 and a learning rate of 2e-5. The training was conducted on a GPU server with 8 NVIDIA A100 GPU cards."}, {"title": "4.2 Benchmarking Settings", "content": "Traditional supervised Automatic Diagnosis methods approach the diagnostic task as a decision-making task, where all symptoms and diseases are predefined. In traditional methods, we adhere to the original settings, which involve training on a training set of benchmarks. We compared four models: Basic DQN [15], HRL [19], Diaformer [20] and MTDiag [2].\nLLM baselines Our comparison mainly focused on advanced LLMs including proprietary models like Gemini-Pro [21], ERNIE Bot(\u6587\u5fc3\u4e00\u8a00) [22], Claude-3-Opus [23], GPT-3.5 [24] (GPT-3.5-turbo-1106), and GPT-4 [25] (GPT-4-0125-preview), as well as open-source models including gemma-7b-it [26], Mixtral-8x7B-Instruct-v0.1 [27], and Yi-34B-Chat [16].\nLLM Evaluation We simulate a patient using GPT-4, which presents both $S_{exp}$ and $S_{imp}$. The simulation begins with $S_{exp}$ (chief complaints). When the evaluated LLM inquires about symptoms, the simulator can only respond with \"yes\" or \"no\" to prevent information leakage. Details of the LLM evaluation can be found in Appendix D. For the evaluated LLMs, we prompt them to perform an automated diagnosis task, which is detailed in Appendix C."}, {"title": "4.3 Benchmark", "content": "Public benchmarks To evaluate diagnostic performance, we used two publicly available benchmarks: Muzhi [15] and Dxy [4]. Both are based on real doctor-patient consultations. However, their data scale and disease variety are limited, as shown in Table 2.\nDxBench To better assess diagnostic capabilities, we develop a larger dataset, DxBench. Using the MedDialog [28] dataset, which contains real doctor-patient dialogues, we filtered out 3,121 cases with clear dialogues and definitive diagnoses. Then GPT-4 is employed to extract $S_{exp}$ and $S_{imp}$, and we manually refine this to 1,148 high-quality cases. Details are in Appendix G. DxBench includes over 1,000 real cases, covering 461 disease types from 15 departments and 5,038 symptoms. Considering the large number of diseases in DxBench, each case is provided with three candidate diseases."}, {"title": "4.4 Diagnosis Performance", "content": "As shown in Table 3, we evaluated diagnostic performance using two public benchmarks: Muzhi [15] and Dxy [4]. The results indicate that LLMs' performance approach traditional IID methods while requiring fewer inquiries. Among the LLMs, DiagnosisGPT achieves the highest accuracy improvement with symptom inquiries, demonstrating superior inquiry capabilities. In the Muzhi"}, {"title": "5 More Analysis", "content": "To assess the interpretability of the diagnostic confidence, we examined diagnostic accuracy at various confidence thresholds. The results, depicted in Figure 5, show that increasing the threshold indeed enhances accuracy. With $\\tau$ = 0.55, the model achieves over 90% accuracy across three datasets. This demonstrates that the model's confidence in disease prediction is dependable and aligns with the expected accuracy rates. However, higher thresholds reduce success rates, indicating that the model becomes more stringent to make a diagnosis."}, {"title": "6 Conclusion", "content": "In this paper, we propose the Chain of Diagnosis (CoD) to enhance the interpretability of large language models (LLMs) for disease diagnosis. Using CoD, we developed DiagnosisGPT, an LLM that supports the diagnosis of 9,604 diseases. Distinct from other LLMs, DiagnosisGPT can provide diagnostic confidence and relies on its own disease database for diagnostic reasoning. Experiments show that the diagnostic capabilities of DiagnosisGPT surpass those of other LLMs. Furthermore, higher accuracy can be achieved by adjusting the diagnostic threshold values. This means that CoD can control the trade-off between effectiveness and efficiency in diagnosis. CoD offers a novel solution for medical diagnosis. We believe that the data, models, and methods from this work can advance the field of medical LLMs."}, {"title": "A Related Work", "content": "LLMs for Medical Scenarios The success of models like ChatGPT [24] has inspired research into their application in healthcare, resulting in medical-specific LLMs such as DoctorGLM [29], MedicalGPT [30], DotaGPT [31], HuatuoGPT [32, 6, 33], and Apollo [34]. Despite their focus on medical knowledge, these models have limited capabilities in automating medical diagnoses.\nAutomated Diagnosis Task Medical diagnosis, a key AI application in healthcare [20, 35, 36], has predominantly utilized reinforcement learning (RL). Pioneering works include [37], who introduced neural symptom checking using RL. Subsequent advancements include hierarchical RL for diagnostic and contextual decisions [19], Deep Q-networks for symptom collection from patient interactions [15], and incorporation of medical knowledge into RL policy learning [38]. Two-level hierarchical RL [39], policy gradient frameworks with Generative Adversarial Networks [1], and customization of RL models using multi-level rewards and dialogue data [40, 41] have further enhanced diagnostic accuracy. [20] and [2] conceptualizes automatic diagnosis as a sequence generation task. However, these models are limited by predefined symptoms and diseases, and cannot support open-ended consultations.\nReasoning of LLMs LLMs show promise in complex tasks such as mathematical reasoning [42, 43]. To harness their reasoning abilities, CoT[44] is proposed with intermediate steps, and Tree-of-Thought (ToT)[45] using DFS/BFS for enhanced reasoning paths. Graph of Thoughts (GoT) [46] is introduced for intricate problems. ReAct [47] combines reasoning with actions. Uncertainty of Thoughts (UoT) [11] improves decision-making by simulating multiple requests for information gain."}, {"title": "B DxBench Distribution", "content": "The data distribution in DxBench dataset is illustrated in Figure 7. We categorize the data distribution according to the medical departments responsible for diagnosing the diseases. The data shows a relatively balanced distribution across different departments. Notably, the Dermatovenereology department has the highest number of entries with 121 cases, while the Infectious Diseases and Immunology department has the fewest, with 27 cases."}, {"title": "C The prompt for LLM Diagnosis", "content": "The prompt for LLM diagnosis is shown in Table 8. We instruct the LLMs to determine whether a diagnosis can be made. If a diagnosis is possible, the LLMs output the diagnosed disease. Otherwise, the LLMs query the user with questions regarding a specific symptom."}, {"title": "H Case Study", "content": "Below, two detailed diagnostic cases will be provided to better understand the response of our model."}, {"title": "I Performance of Disease Retriever", "content": "We allocated 10% of the data as a validation set to evaluate retrieval performance. Table 7 shows the retrieval performance of diseases under Diagnosis on the validation set. It can be seen that the top 3 diseases achieve a recall rate of 73%, indicating that most diseases can be effectively excluded."}, {"title": "J Standard Errors of Result", "content": "We report the standard errors of the results from our model in Table 8. The standard errors were obtained by conducting five random experiments."}, {"title": "K Review of Synthetic Cases by Medical Experts", "content": "To verify the quality of the synthetic cases, we had two licensed physicians review the data. Each physician was given 50 randomly sampled synthetic cases and asked to assess whether any cases posed a risk of errors. Based on their feedback, they identified that out of the 100 cases, only 6 might be incorrect, as the symptom information was less likely to be associated with the respective diseases. This suggests that synthesizing cases from a medical encyclopedia is a fairly reliable method."}, {"title": "L Limitations", "content": "Despite its promising performance in diagnostic tasks, DiagnosisGPT has several limitations that must be considered:\n\u2022 Limited Disease Coverage: DiagnosisGPT is trained to identify only a specific set of diseases. This constraint means that the model's diagnostic capabilities are confined to this predefined list, and it may not recognize or provide accurate diagnoses for conditions that fall outside its training parameters. Consequently, this limitation could hinder the model's applicability in a real-world medical setting where a wide range of diseases, including rare and emerging conditions, need to be diagnosed.\n\u2022 Synthetic Data Annotation: The dataset used to train DiagnosisGPT relies on annotations created by Large Language Models (LLMs). While utilizing LLMs for annotation is a cost-effective approach, it raises concerns about the quality and reliability of the data. LLMs can sometimes generate plausible but incorrect information\u2014often referred to as \"hallucinations\"\u2014which can introduce biases or errors into the training data. This could potentially lead to the model making incorrect or misleading diagnoses.\n\u2022 Reliance on Synthetic Cases: DiagnosisGPT's training is based on synthetic medical cases, which are constructed to avoid the privacy concerns associated with using real patient"}, {"title": "M Impact", "content": "The development of DiagnosisGPT raises several potential risks.\n\u2022 Risk of Misdiagnosis: Despite the promising results shown by DiagnosisGPT in diagnosis, it is crucial to underscore that at this stage, it should not be used to provide any medical advice. There is a possibility that it could provide incorrect interpretations or inaccurate diagnoses. Considering the nature of this field, our model and data will only be available for download by researchers. Our model will not be available for public use.\n\u2022 Data Privacy and Ethics: The diagnostic field may involve ethical issues related to patient privacy. To address this, we use synthetic data. The training data for CoD is entirely generated by GPT-4, ensuring that there are no privacy or ethical concerns. As for DxBench, we constructed it using open-source licensed datasets, ensuring compliance with ethical standards."}]}