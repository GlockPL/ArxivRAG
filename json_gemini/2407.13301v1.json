{"title": "CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis", "authors": ["Junying Chen", "Chi Gui", "Anningzhe Gao", "Ke Ji", "Xidong Wang", "Xiang Wan", "Benyou Wang"], "abstract": "The field of medical diagnosis has undergone a significant transformation with\nthe advent of large language models (LLMs), yet the challenges of interpretability\nwithin these models remain largely unaddressed. This study introduces Chain-of-\nDiagnosis (CoD) to enhance the interpretability of LLM-based medical diagnostics.\nCoD transforms the diagnostic process into a diagnostic chain that mirrors a physi-\ncian's thought process, providing a transparent reasoning pathway. Additionally,\nCoD outputs the disease confidence distribution to ensure transparency in decision-\nmaking. This interpretability makes model diagnostics controllable and aids in\nidentifying critical symptoms for inquiry through the entropy reduction of confi-\ndences. With CoD, we developed DiagnosisGPT, capable of diagnosing 9,604\ndiseases. Experimental results demonstrate that DiagnosisGPT outperforms other\nLLMs on diagnostic benchmarks. Moreover, DiagnosisGPT provides interpretabil-\nity while ensuring controllability in diagnostic rigor.", "sections": [{"title": "Introduction", "content": "In healthcare, disease diagnosis is pivotal yet complex, serving as a bridge between medical expertise\nand decision-making [1, 2]. The diagnosis task involves predicting a disease using explicit (self-\nreported) and implicit (inquired) symptoms [3-5]. As illustrated as Figure 1, the doctor either inquires\nfurther or makes a diagnosis, aiming for high accuracy with minimal inquiries. Large Language\nModels (LLMs) offer a promising path for automated diagnosis, due to their superior reasoning and\ndialogue abilities, coupled with extensive knowledge. These capabilities enable them to address a\nwide range of diseases and interact effectively with patients [6].\nHowever, the application of LLMs in medical diagnosis encounters significant challenges of inter-\npretability [7, 8]. Considering the issue of hallucinations, LLMs could arbitrarily make a diagnosis.\nWithout interpretability, it is unclear if decisions meet sound analysis and ethical standards [9].\nAlthough LLMs can offer rudimentary explanations for their decision, they lack a comprehensive\nprocess to explain why other potential diseases are excluded and to which extent of confidence it\nmade such a decision. [10] This highlights the need for an interpretable LLM solution for diagnosis.\nIn response to these limitations, we propose the Chain of Diagnosis (CoD) to enhance the inter-\npretability of LLMs. CoD provides transparency in both reasoning and decision-making. It transforms\nthe black-box decision-making process into a diagnostic chain that mirrors a physician's thinking\nprocess with five distinct steps. It reveals the series of thoughts behind each decision. For decision\ntransparency, CoD outputs a confidence distribution, with higher confidence indicating a stronger\nbelief in diagnosing a specific disease. This allows for control over the LLM's decisions using a\nconfidence threshold. Additionally, diagnostic uncertainty can be quantified by the entropy of the\nconfidences. The goal of entropy reduction can aid in eliciting more effective symptoms for inquiry."}, {"title": "Problem Definition of Diagnosis", "content": "In the diagnostic process, physicians navigate through explicit symptoms ($S_{exp}$), self-reported by\npatients, and implicit symptoms ($S_{imp}$), elicited through further inquiry, to predict a target disease\n($d_t$) from a predefined list (D). This process involves a sequence of decisions: physicians either probe\nfor additional symptoms (Symptom Inquiry) or select the most probable disease from D (Disease\nDiagnosis). The objective is to maximize diagnostic accuracy (a\u2191) while minimizing the number of\ninquiries (n\u2193), ensuring the inquiry count n does not exceed a patient's patience threshold L.\nThe challenge lies in determining when and how to inquire about symptoms to enhance diagnostic\naccuracy. As stated in [11], LLMs don't excel at questioning users. To explore this, we conducted a\npreliminary experiment on diagnostic benchmarks with two LLMs:"}, {"title": "Methodology: Chain of Diagnosis", "content": "As depicted on the left side of Figure 2, the CoD outputs a diagnostic chain that represents the thought\nprocess of LLM diagnosis. It mirrors a physician's diagnostic thinking, which involves summarizing\nsymptoms, identifying diseases, analyzing and deciding. This interpretability aids in diagnostic\ndecision-making. To implement the CoD, we constructed CoD training data based on patient cases to\nfine-tune LLMs to perform CoD, as shown on the right side of Figure 2."}, {"title": "The Philosophy of CoD for Interpretability", "content": "Lipton [12] classified interpretability into two aspects: 1) transparency, i.e., how does the model\nwork? and 2) post-hoc explanations, i.e., what can the learned model tell us? The two aspects inspire\nus design an interpretable diagnostic LLM, consisting of a CoD framework that includes Property 1, 2\nfor transparency and Property 3 for post-hoc explanations.\nTransparency [12] encompasses two types: decomposability and algorithmic transparency. Decom-\nposability refers to a system where each component can be individually interpreted. For CoD, this is\ndemonstrated in Property 1.\nProperty 1 Decomposabilty with a pipeline chain: CoD transform the black-box decision of diag-\nnosis into a explainable chain, which can be viewed as a sequence of intermediate steps. Therefore,\nthe designed CoD is a chain pipeline (see Sec. 3.2), each step of which has a clear functionality. The\noverall chain mimics a real diagnosis of a physician.\nAlgorithmic transparency refers to the understanding of the learning algorithm itself, such as whether\nit converges and why. Regrading the challenge in Sec. 2.2, the algorithmic transparency of CoD can\nbe understood from a entropy-reduction perspective: with more inquiries made, the uncertainty of the\ndiagnosis estimate (quantitatively measured by entropy) will be reduced, see Property 2.\nProperty 2 Transparency with confidence-driven flow: In the chain, CoD introduces a disease\nconfidence distribution $C = \\{c_d \\mid d \\in D\\}$, where decision-making is based on whether the highest"}, {"title": "The Diagnosis Chain", "content": "Here, we introduce the response methods and the construction approach of CoD, as illustrated in\nFigure 2. All prompts for building CoD training data are detailed in Appendix F.\nStep 1: Symptom Abstraction The first step summarizes the symptoms S of the patient's question:\n$S = f_1(q_{patient})$\nIt allow the model to focus on the refined symptoms and provide an understanding of patient's query.\nFor training data, the initial patient question is generated from ($S_{exp}$) with the LLM.\nStep 2: Disease Recall & Knowledge Integration\nNext, CoD identifies the top-K potential diseases based on a disease retriever:\n$D' = f_2(D, S, k)$\nwhere $D' \\subset D$ and $|D'| = k$. A smaller space D' is necessary for subsequent analysis and reasoning,\nsince analyzing all diseases is impractical (considering |D| = 9604) and most irrelevant diseases can\nrealistically be excluded. We use Dense Retrieval training methods [13, 14] to train this retriever,\nwith the following training objective:\n$\\mathcal{L}\\left(S_{\\text {exp }}, S_{\\text {imp }}, d_{t}\\right)=-\\log \\frac{e^{\\operatorname{sim}\\left(E_{S}\\left(S_{\\text {exp }} \\cup S_{\\text {imp }}\\right), E_{D}\\left(d_{t}\\right)\\right)}}{\\sum_{d \\in D} e^{\\operatorname{sim}\\left(E_{S}\\left(S_{\\text {exp }} \\cup S_{\\text {imp }}\\right), E_{D}(d)\\right)}}$\nwhere sim denotes the cosine similarity, and $E_S$ and $E_D$ are the symptom and disease encoders,\nrespectively. The performance of the disease retriever is detailed in Appendix I.\nThen, for each candidate disease $d \\in D'$, CoD retrieves corresponding disease knowledge from the\ndisease database and integrates it into the output to enhance understanding of the disease. Similarly,\nother tools like RAG can also be utilized in this step to enhance reasoning.\nStep 3: Diagnostic Reasoning In step 3, CoD generates the diagnostic reasoning process T:\n$T = f_3(S,D')$\nSimilar to CoT, T is a thought process that carefully analyzes whether each disease in D' corresponds\nto the patient's symptoms. To build training data, we prompt a LLM to generate T.\nStep 4: Confidence Assessment After generating T, CoD generates a confidence distribution:\n$C = f_4(S,D',T)$\nC satisfies $\\sum_{d \\in D'} C_d = 1$. This distribution indicates the model's tendency towards diagnosing a\ndisease, mainly according to the analysis of T. According to f3, C can be considered a posterior\nprobability distribution:\n$C = \\{p_{\\theta}(d \\mid S, D') | d \\in D'\\}$\nHere, $p_{\\theta}$ represents the confidence distribution generated by the LLM $\\theta$. For constructing training data,\nwe validate C against the target disease $d_t$ to ensure T and C are reasonable. If $max_{d \\in D' \\{d_t\\}} C_d \\ge \\tau$,\nthe generated data is considered erroneous, i.e., the model assigns high confidence to an incorrect\ndisease. If erroneous, we prompt the model to rethink and correct its reasoning until the distribution\nis verified. With C, CoD can make decisions based on the confidence in its diagnosis.\nStep 5: Decision Making In the last step, a confidence threshold $\\tau$ is set to control the decision-\nmaking. The diagnostic task involves two decision types: 1) making a diagnosis $A_{diag}(d)$, where d is"}, {"title": "CoD as an Entropy-reduction Process", "content": "Symptom inquiry is a key step in diagnosis, serving to gather additional patient information to clarify\nthe diagnosis. This inquiry process can be viewed as a transition from diagnostic uncertainty to\ncertainty. The uncertainty level can be captured by the entropy of confidence:\n$H(C) = - \\sum_{d \\in D'} C_d \\log C_d$\nSymptom inquiry is a process of entropy reduction. Given a symptom s, its post-inquiry entropy is:\n$H(C|s) = \\sum_{d \\in D'} p_{\\theta}(d | S \\cup \\{s\\}, D') \\log p_{\\theta}(d | S \\cup \\{s\\}, D')$", "formulas": ["S = f_1(q_{patient})", "D' = f_2(D, S, k)", "\\mathcal{L}\\left(S_{\\text {exp }}, S_{\\text {imp }}, d_{t}\\right)=-\\log \\frac{e^{\\operatorname{sim}\\left(E_{S}\\left(S_{\\text {exp }} \\cup S_{\\text {imp }}\\right), E_{D}\\left(d_{t}\\right)\\right)}}{\\sum_{d \\in D} e^{\\operatorname{sim}\\left(E_{S}\\left(S_{\\text {exp }} \\cup S_{\\text {imp }}\\right), E_{D}(d)\\right)}}", "T = f_3(S,D')", "C = f_4(S,D',T)", "C = \\{p_{\\theta}(d \\mid S, D') | d \\in D'\\}", "H(C) = - \\sum_{d \\in D'} C_d \\log C_d", "H(C|s) = \\sum_{d \\in D'} p_{\\theta}(d | S \\cup \\{s\\}, D') \\log p_{\\theta}(d | S \\cup \\{s\\}, D')"]}, {"title": "Experiments", "content": "Utilizing the created CoD data, we fully fine-tuned the Yi-34B-Base [16] to develop DiagnosisGPT.\nTo equip it with chat capabilities, ShareGPT data 2 is incorporated into the training data. Training\nparameters included a batch size of 64 and a learning rate of 2e-5. For the retrieval model, we trained\non the all-mpnet-base-v2 [17] model using DRhard [18], with a batch size of 256 and a learning rate\nof 2e-5. The training was conducted on a GPU server with 8 NVIDIA A100 GPU cards."}, {"title": "Benchmarking Settings", "content": "Traditional supervised Automatic Diagnosis methods approach the diagnostic task as a decision-\nmaking task, where all symptoms and diseases are predefined. In traditional methods, we adhere\nto the original settings, which involve training on a training set of benchmarks. We compared four\nmodels: Basic DQN [15], HRL [19], Diaformer [20] and MTDiag [2].\nLLM baselines Our comparison mainly focused on advanced LLMs including proprietary models\nlike Gemini-Pro [21], ERNIE Bot(\u6587\u5fc3\u4e00\u8a00) [22], Claude-3-Opus [23], GPT-3.5 [24] (GPT-3.5-\nturbo-1106), and GPT-4 [25] (GPT-4-0125-preview), as well as open-source models including\ngemma-7b-it [26], Mixtral-8x7B-Instruct-v0.1 [27], and Yi-34B-Chat [16].\nLLM Evaluation We simulate a patient using GPT-4, which presents both $S_{exp}$ and $S_{imp}$. The\nsimulation begins with $S_{exp}$ (chief complaints). When the evaluated LLM inquires about symptoms,\nthe simulator can only respond with \"yes\" or \"no\" to prevent information leakage. Details of the\nLLM evaluation can be found in Appendix D. For the evaluated LLMs, we prompt them to perform\nan automated diagnosis task, which is detailed in Appendix C."}, {"title": "Benchmark", "content": "Public benchmarks To evaluate diagnostic performance, we used two publicly available benchmarks:\nMuzhi [15] and Dxy [4]. Both are based on real doctor-patient consultations. However, their data\nscale and disease variety are limited, as shown in Table 2.\nDxBench To better assess diagnostic capabilities, we develop a larger dataset, DxBench. Using the\nMedDialog [28] dataset, which contains real doctor-patient dialogues, we filtered out 3,121 cases with\nclear dialogues and definitive diagnoses. Then GPT-4 is employed to extract $S_{exp}$ and $S_{imp}$, and we\nmanually refine this to 1,148 high-quality cases. Details are in Appendix G. DxBench includes over\n1,000 real cases, covering 461 disease types from 15 departments and 5,038 symptoms. Considering\nthe large number of diseases in DxBench, each case is provided with three candidate diseases."}, {"title": "Diagnosis Performance", "content": "As shown in Table 3, we evaluated diagnostic performance using two public benchmarks: Muzhi\n[15] and Dxy [4]. The results indicate that LLMs' performance approach traditional IID methods\nwhile requiring fewer inquiries. Among the LLMs, DiagnosisGPT achieves the highest accuracy\nimprovement with symptom inquiries, demonstrating superior inquiry capabilities. In the Muzhi"}, {"title": "More Analysis", "content": "To assess the interpretability of the diagnostic confidence, we examined diagnostic accuracy at various\nconfidence thresholds. The results, depicted in Figure 5, show that increasing the threshold indeed\nenhances accuracy. With $\\tau$ = 0.55, the model achieves over 90% accuracy across three datasets. This\ndemonstrates that the model's confidence in disease prediction is dependable and aligns with the\nexpected accuracy rates. However, higher thresholds reduce success rates, indicating that the model\nbecomes more stringent to make a diagnosis."}, {"title": "A Related Work", "content": "LLMs for Medical Scenarios The success of models like ChatGPT [24] has inspired research\ninto their application in healthcare, resulting in medical-specific LLMs such as DoctorGLM [29],\nMedicalGPT [30], DotaGPT [31], HuatuoGPT [32, 6, 33], and Apollo [34]. Despite their focus on\nmedical knowledge, these models have limited capabilities in automating medical diagnoses.\nAutomated Diagnosis Task Medical diagnosis, a key AI application in healthcare [20, 35, 36], has\npredominantly utilized reinforcement learning (RL). Pioneering works include [37], who introduced\nneural symptom checking using RL. Subsequent advancements include hierarchical RL for diagnostic\nand contextual decisions [19], Deep Q-networks for symptom collection from patient interactions\n[15], and incorporation of medical knowledge into RL policy learning [38]. Two-level hierarchical\nRL [39], policy gradient frameworks with Generative Adversarial Networks [1], and customization\nof RL models using multi-level rewards and dialogue data [40, 41] have further enhanced diagnostic\naccuracy. [20] and [2] conceptualizes automatic diagnosis as a sequence generation task. However,\nthese models are limited by predefined symptoms and diseases, and cannot support open-ended\nconsultations.\nReasoning of LLMs LLMs show promise in complex tasks such as mathematical reasoning [42, 43].\nTo harness their reasoning abilities, CoT[44] is proposed with intermediate steps, and Tree-of-Thought\n(ToT)[45] using DFS/BFS for enhanced reasoning paths. Graph of Thoughts (GoT) [46] is introduced\nfor intricate problems. ReAct [47] combines reasoning with actions. Uncertainty of Thoughts\n(UoT) [11] improves decision-making by simulating multiple requests for information gain."}, {"title": "Impact", "content": "M.1 Positive Impact\n\u2022 Promotes medical AI development: DiagnosisGPT promotes the development of medical\nAI, as diagnostics are crucial in healthcare AI. Accurate diagnostic capabilities enhance\npatient outcomes and streamline clinical processes.\n\u2022 Improves interpretability in healthcare: DiagnosisGPT improves the interpretability of\nmedical AI by utilizing a disease retriever function and knowledge base integration. This\nincreased interpretability builds trust in AI systems among healthcare providers and patients.\nBy making the diagnostic process more transparent, DiagnosisGPT helps users understand\nthe reasoning behind AI-generated suggestions, fostering greater confidence in AI-assisted\nmedical practices.\n\u2022 Addresses privacy concerns in medical cases: DiagnosisGPT offers a solution to privacy\nissues prevalent in medical case handling by constructing cases using a knowledge base,\nthereby eliminating patient privacy concerns. This approach also alleviates the problem of\ndata scarcity.\n\u2022 Assists healthcare professionals: DiagnosisGPT assists healthcare professionals by rapidly\ncollecting patient symptom information and providing preliminary diagnoses. This capability\nenables medical practitioners to save time and focus on more complex aspects of patient\ncare.\nM.2 Potential Negative Impact\nThe development of DiagnosisGPT raises several potential risks.\n\u2022 Risk of Misdiagnosis: Despite the promising results shown by DiagnosisGPT in diagnosis,\nit is crucial to underscore that at this stage, it should not be used to provide any medical\nadvice. There is a possibility that it could provide incorrect interpretations or inaccurate\ndiagnoses. Considering the nature of this field, our model and data will only be available for\ndownload by researchers. Our model will not be available for public use.\n\u2022 Data Privacy and Ethics: The diagnostic field may involve ethical issues related to patient\nprivacy. To address this, we use synthetic data. The training data for CoD is entirely\ngenerated by GPT-4, ensuring that there are no privacy or ethical concerns. As for DxBench,\nwe constructed it using open-source licensed datasets, ensuring compliance with ethical\nstandards."}]}