{"title": "NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task", "authors": ["Pramit Sahoo", "Maharaj Brahma", "Maunendra Sankar Desarkar"], "abstract": "In this paper, we describe our system for the WMT 24 shared task of Low-Resource Indic Language Translation. We consider eng \u2192 {as, kha, lus, mni} as participating language pairs. In this shared task, we explore the fine-tuning of a pre-trained model motivated by the pre-trained objective of aligning embeddings closer by alignment augmentation (Lin et al., 2020) for 22 scheduled Indian languages. Our primary system\u00b9 is based on language-specific finetuning on a pre-trained model. We achieve chrF2 scores of 50.6, 42.3, 54.9, and 66.3 on the official public test set for eng\u2192as, eng\u2192kha, eng\u2192lus, eng\u2192mni respectively. We also explore multilingual training with/without language grouping and layer-freezing.", "sections": [{"title": "1 Introduction", "content": "The \"Shared Task: Low-Resource Indic Language Translation\" for WMT 2024 (Pakray et al., 2024) extends the efforts initiated in WMT 2023 (Pal et al., 2023), which garnered significant participation from the global community. Recent advancements in machine translation (MT), particularly through techniques like multilingual training and transfer learning, have expanded the scope of MT systems beyond high-resource languages (Johnson et al., 2017). However, low-resource languages continue to present substantial challenges due to the scarcity of parallel data required for effective training (Siddhant et al., 2020; Wang et al., 2022). The shared task focuses on low-resource Indic languages with limited data from diverse language families: Assamese (as), Mizo (lus), Khasi (kha), and Manipuri (mni). The task aims to improve translation quality for the English Assamese, English\u21d4Mizo, English Khasi, and English Manipuri given the data provided in the constrained setting.\nTo address the challenges inherent in translating low-resource languages, participants are encouraged to explore several strategies. First, leveraging monolingual data is essential for enhancing translation quality, especially in the absence of sufficient parallel data. Second, multilingual approaches offer the potential for cross-lingual transfer, where knowledge from high-resource languages can be applied to low-resource pairs (Sen et al., 2019). Third, transfer learning provides a mechanism for adapting pre-trained models from high-resource languages to low-resource settings (Wang et al., 2020). Lastly, innovative techniques tailored to low-resource scenarios, such as data augmentation and language-specific fine-tuning, are crucial for improving performance.\nIn this paper, we describe our system for the WMT 2024 shared task, focusing on fine-tuning two pre-trained models: IndicRASP and Indi-CRASP Seed\u00b2. IndicRASP model is pre-trained with the objective of aligning embeddings inspired by alignment augmentation (Lin et al., 2020) on 22 Indic languages. Our primary approach involves language-specific fine-tuning, leveraging multilingual training setups, language grouping, and layer freezing. We set up experiments in both bilingual and multilingual settings. We achieve BLEU scores of 20.1 for English\u2192Assamese, 19.1 for English Khasi, 30.0 for English\u2192Mizo, and 35.6 for English Manipuri on the public test set, demonstrating the effectiveness of our approach. Specifically, language-specific fine-tuning yielded significant improvements in translation quality, while multilingual setups provided balanced performance across all language pairs. Language grouping and layer freezing are effective techniques for preserving pre-trained knowledge and mitigating the challenges of multilinguality. Our results highlight the importance of tailored fine-tuning"}, {"title": "2 Data", "content": "In this section, we present the details of the IndicNECorp1.0 dataset provided by the IndicMT shared task\u00b3 organizers."}, {"title": "2.1 Monolingual Data", "content": "The official data also includes monolingual data for four languages. The dataset comprises approximately 2.6M sentences for Assamese, 0.1M for Khasi, 2M for Mizo, and 1M for Manipuri."}, {"title": "2.2 Parallel Data", "content": "The dataset includes four bilingual pairs between English and Indic languages: English (en) - Assamese (as), English (en) - Khasi (kha), English (en) - Mizo (lus), and English (en) - Manipuri (mni). These languages are mainly spoken in the North-eastern part of India. The English-Assamese and English-Mizo training sets contain 50k parallel sentences each, while the English-Khasi and English-Manipuri training sets contain 24k and 21.6k parallel sentences, respectively. Dataset statistics are presented in Table 1."}, {"title": "3 Approach", "content": "In this section, we briefly describe our approaches. We explore transfer learning, language grouping, and layer-freezing techniques."}, {"title": "3.1 Transfer Learning", "content": "We explore transfer learning based on two pre-trained models \"IndicRASP\" and \"IndicRASP Seed,\" which is a fine-tuned model of IndicRASP on small and high-quality data. Particularly, the pre-trained model is trained on agreement-based objective (Lin et al., 2020; Yang et al., 2020) for Indic languages. The model is pre-trained in 22 scheduled Indic languages using a subset of the Bharat Parallel Corpus Collection (BPCC) dataset (Gala et al., 2023). Out of these 22 languages, two of the shared task languages, Assamese and Manipuri, are part of the pre-training. Alignment augmentation is performed using bi-lingual dictionaries from MUSE (Conneau et al., 2017) and GATITOS6."}, {"title": "3.2 Language Grouping", "content": "We explore the effect of grouping languages based on script similarity in a multilingual setup. Although our primary focus is on bilingual models, for language grouping experiments, we utilize a multilingual approach where languages sharing similar scripts are trained together. This approach is motivated by the idea that joint training with similar languages can improve translation quality due to shared vocabulary and linguistic properties (Jiao et al., 2022; Gala et al., 2023).\n\u2022 Group 1 (Bengali script): Assamese and Manipuri\n\u2022 Group 2 (Latin script): Khasi and Mizo"}, {"title": "3.3 Layer Freezing", "content": "We explored layer-freezing approaches for Indic-Trans2 Distilled and IndicRASP Seed models.\nFrozen Encoder. In this approach, we freeze the encoder components during the fine-tuning process to preserve their pre-trained weights from the parent model while the embedding and decoder components are updated.\nFrozen Embedding + Encoder. In this setup, we keep the embedding and encoder frozen during fine-tuning to preserve their pre-trained weights while updating only the parameters of the rest of the layers."}, {"title": "4 Experimental Setup", "content": "Settings. We fine-tune pre-trained checkpoints: \"IndicRASP\" and \"IndicRASP Seed\" models on official parallel data using the Adam optimizer (Kingma and Ba, 2014) with \\( \\beta_1 \\) set to 0.9 and \\( \\beta_2 \\) set to 0.98. We set the initial warmup learning rate to 1e-07 and the learning rate to 3e-5, with a warmup step of 4000. We train the models with a dropout rate of 0.3 and a label smoothing rate of 0.1. All experiments are conducted on a single NVIDIA A100 GPU. We use a maximum token count of 512 per batch, accumulating gradients over two steps to simulate a larger batch size. The model is trained for up to 1,000,000 updates. We save checkpoints every 2500 updates. We employed a patience of 10 for early stopping.\nEvaluation Metrics. We use the official dev and test sets of IndicNECorp1.0 for validation and evaluation. We evaluate using BLEU (Papineni et al., 2002), chrF (Popovi\u0107, 2015), and chrF++ (Popovi\u0107, 2017) metrics. We use the SacreBLEU toolkit (Post, 2018) to perform our evaluation7 with a chrF word order of 2. Additionally, as per the evaluation metrics used by the organizers, we report results on TER (Snover et al., 2006), RIBES (Isozaki et al., 2010), and COMET (Rei et al., 2022) for our primary and contrastive submissions.\nModels. We conducted our experiments in both bilingual and multilingual settings. In the bilingual setup, we fine-tuned the IndicTrans2 Distilled model (Gala et al., 2023), IndicRASP, and Indi-CRASP Seed models for both English to Indic and Indic to English directions. In the multilingual setup, we fine-tuned pre-trained checkpoints of IndicRASP and IndicRASP Seed for both directions. Inspired by Chiang et al. (2022), we initialized the bilingual model with a fine-tuned multilingual model for both English to Indic and Indic to English.\nFor experiments with layer freezing, we fine-tune pre-trained checkpoints of IndicTrans2 Distilled and IndicRASP Seed models. Particularly, we perform experiments by freezing the embeddings and encoder and only the encoder component for both English to Indic and Indic to English directions. We conduct all layer-freezing experiments in a bilingual setup. For language grouping experiments, we fine-tune the IndicRASP and IndicRASP Seed models based on script similarity in a multilingual setup."}, {"title": "5 Results and Discussions", "content": "In this section, we report our experimental results and describe our primary and contrastive submissions. The results for our primary and contrastive systems are shown in Table 4. Tables 2, 3, and 5 reports the chrF2, BLEU, and chrF++ scores respectively.\n1 English \u2192 Indic: Our primary English to Indic systems are language pair-specific (bilingual models) fine-tuned on pre-trained Indi-CRASP Seed, achieving chrF2 scores of 50.6, 42.3, 54.9, and 66.3 for Assamese, Khasi, Mizo, and Manipuri respectively. For the contrastive systems, we consider a bilingual model fine-tuned on a pre-trained IndicRASP checkpoint. The contrastive system achieves chrF2 scores of 49.9, 42.2, 36.5, and 65.8 for Assamese, Khasi, Mizo, and Manipuri, respectively. The detailed primary and contrastive system results are reported in Table 4.\n\u2461 Indic \u2192 English: Our primary Indic-to-English systems for Assamese and Manipuri are bilingual models fine-tuned on the pre-trained IndicRASP Seed model, each achieving chrF2 scores of 52.8 and 67.9, respectively. Similarly, for Khasi and Mizo, our primary systems are bilingual models fine-tuned on a pre-trained IndicRASP checkpoint, achieving a chrF2 score of 36.1 and 49.4, respectively.\nFor the contrastive Indic-to-English system, we submit a multilingual system fine-tuned on the pre-trained checkpoint of the IndicRASP model, achieving chrF2 scores of 51.2, 36.0, 46.5, and 65.3 for Assamese, Khasi, Mizo, and Manipuri respectively. Table 4 shows the detailed scores in various metrics.\nBilingual vs. Multilingual. We observe that fine-tuning the pre-trained IndicRASP Seed outperforms the IndicRASP model, likely due to the continued pre-training on a small, high-quality dataset in the IndicRASP Seed model. Bilingual models perform better than multilingual models, showing a +4.1 and +7.7 chrF2 score improvement for English to Manipuri and English to Khasi, respectively. Bilingual models initialized with the weights from multilingual models show improvement over the standalone multilingual models, achieving a +7.8 chrF2 score for English to Khasi. This suggests that initializing bilingual models can be helpful in low-resource settings.\nLanguage Grouping. We observe that script-based language grouping shows improvements over a standalone multilingual model with +1.6, +0.3, +3.3, and +1.4 for English to Assamese, Khasi, Mizo, and Manipuri, respectively. It suggests that grouping languages based on script similarity can be effective in addressing the curse of multilinguality.\nLayer Freezing. We observe that freezing only"}, {"title": "6 Conclusion", "content": "In this paper, we describe NLIP Lab's Indic low-resource machine translation systems for the WMT24 shared task. We explore the translation capabilities of the alignment-augmented pre-trained model, IndicRASP and IndicRASP Seed, to enhance translation quality for low-resource Indic languages. Experimentally, we found that the Indi-CRASP model performs better than the IndicTrans2 Distilled model. Additionally, we experiment with layer-freezing and language grouping techniques. In the future, we will focus on refining these techniques and utilizing monolingual data to enhance MT performance for low-resource Indic languages."}, {"title": "Limitations", "content": "The pre-trained models use bilingual dictionaries whose domain might differ from the shared task training corpus. Additionally, the considered pre-trained models cover only a limited number of shared task languages. Our submission does not utilize the provided monolingual data, which could further improve model performance through back-translation."}]}