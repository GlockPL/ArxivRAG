{"title": "Unveiling Visual Biases in Audio-Visual Localization Benchmarks", "authors": ["Liangyu Chen", "Zihao Yue", "Boshen Xu", "Qin Jin"], "abstract": "Audio-Visual Source Localization (AVSL) aims to localize the source of sound within a video. In this paper, we identify a significant issue in existing benchmarks: the sounding objects are often easily recognized based solely on visual cues, which we refer to as visual bias. Such biases hinder these benchmarks from effectively evaluating AVSL models. To further validate our hypothesis regarding visual biases, we examine two representative AVSL benchmarks, VGG-SS and Epic-Sounding-Object, where the vision-only models outperform all audio-visual baselines. Our findings suggest that existing AVSL benchmarks need further refinement to facilitate audio-visual learning.", "sections": [{"title": "1 Introduction", "content": "Audio-Visual Source Localization (AVSL) [1,35,36] aims to ground the sounding objects in the visual scene. This task assesses an AI system's ability to correlate sound with corresponding visuals, potentially benefiting various downstream applications, such as VR/AR. As one of the key tasks of audio-visual learning, AVSL has been widely investigated over time. Research spans across semi-supervised [10,27], weakly-supervised [19,26], and self-supervised settings [2, 17, 19, 22-25, 32, 34], covering scenarios including both first-person [15] and third-person visual views [27].\nHowever, despite the growing number of model algorithms developed for the AVSL task [2, 14, 17, 23\u201326, 31, 32, 34], the benchmarks commonly used for performance validation have not been thoroughly examined. Beyond issues of the limited data scale and domain diversity, this paper addresses a fundamental question: do we really need audio information for sounding object localization in these benchmarks? For example, when a video shows someone playing an instrument, the source of the sounding object can often be easily deduced from the visual context alone. Similarly, in scenes where someone is cooking in the kitchen, the sounds are likely to originate from human activities such as washing"}, {"title": "2 Related Works", "content": "Learning to localize sound sources in videos, a task known as Audio-Visual Source Localization (AVSL), has attracted considerable research interest. Early"}, {"title": "3VGG-SS: Sounding Bias from Visual Common Sense", "content": "VGG-SS [2,34] is an AVSL benchmark that covers a wide range of daily scenes from the third-person view. It is derived from the large-scale video dataset VGG-Sound [3] and includes 5,158 ten-second video clips with audio. Each clip is annotated with bounding boxes on the middle frame for the AVSL task. In this section, we investigate the visual biases observed in VGG-SS."}, {"title": "3.1 Observation and Analysis", "content": "Ideally, test data should evaluate how well a model can identify the objects associated with a given sound among multiple possible candidates. However, based on our observations of some examples in VGG-SS, the videos typically contain only simple scenes and objects, such as a person playing an instrument or a train passing by, offering a limited number of potential candidates for sound attribution. Furthermore, with basic commonsense knowledge such as instruments being more likely to make sounds than furniture, and moving objects being more likely to produce sounds than static ones the sounding source in videos can often be easily distinguished based solely on visual information. We wonder whether such visual bias exists significantly in the benchmark. Therefore, we conduct a simple user study first. We randomly sample 300 videos from VGG-SS dataset and ask annotators whether the ground truth sounding source can be identified given only visual information, with each video annotated once. As shown in Fig. 1, nearly 90% of the videos demonstrate clearly visual cues that make sounding source easily identified even without the audio. Such results intensify our concerns about the significant visual bias present in the VGG-SS dataset, casting doubt on the benchmark's effectiveness in evaluating AVSL models. To further investigate the impact of visual bias on benchmark evaluation, we conduct experiments using models with only visual input, comparing their performance to existing models specifically designed for the ASVL task."}, {"title": "3.2 Experiments", "content": "Model. We choose MiniGPT-v2 [4] as the vision-only model. Building upon large vision and language models [8,33] and pretrained with diverse vision-language tasks, it is not only proficient in general vision-language understanding and reasoning, but also capable of visual elements grounding. For grounding tasks, the model autoregressively generates a sequence of four values representing the coordinates of the top-left and bottom-right corners of the bounding box. To adapt the model to the AVSL task, we prompt the model to locate the most likely sounding source in the given video frame. Since the model occasionally fails to follow instructions accurately, we construct a pseudo training corpus by rephrasing the MSCOCO [21] detection data to instruction data, which helps to better align the model with the desired task output format. We finetune the model on the pseudo training data for 10 epochs with LoRA [13].\nBaselines and Evaluation. We use recent models benchmarked on VGG-SS as our baselines, including Attention10k [27,28], Hardway [2,34], EZVSL [25], SLAVC [24], SSPL [31], SSL-TIE [23], and FNAC [32], all of which adopt unsupervised contrastive learning frameworks for audio-visual learning. Among them, FNAC serves as the strongest baseline with false negatives suppression and true negatives enhancement. Moreover, we also include a recently proposed semi-supervised framework DMT [10], which utilizes about 80% of the VGG-SS test data for training and reports the performance evaluated on the remaining data. All results are obtained from the original papers. Following previous works [27, 28], we adopt the Consensus IoU (CIoU) and AUC as metrics for model evaluation, with a cIoU threshold of 0.5."}, {"title": "4 Epic-Sounding-Object: Sounding Bias from Hand-Object Interaction", "content": "Epic-Sounding-Object [15] is a first-person AVSL benchmark mainly capturing scenes in kitchens. It is derived from the large-scale egocentric video dataset Epic-Kitchens [5,6]. It contains 3,172 video clips, most of which have an average duration of less than 3 seconds. Each clip is annotated with bounding boxes for the AVSL task on the middle frames. This section investigates the visual biases in Epic-Sounding-Object benchmark."}, {"title": "4.1 Observation and Analysis", "content": "In the egocentric videos in kitchens, sounding objects can often be identified using solely visual information as well. Sounds in a kitchen typically originate from human activities, such as chopping vegetables or washing dishes, while static objects are more likely to remain silent. Moreover, people familiar with cooking can easily discern which kitchen utensils are more likely to produce sounds. As a result, locating the sound source in kitchen videos is not a difficult task, even without audio information. Our manual review of over 300 random samples further confirms this point in more than 90% of the cases, the sound source could be accurately inferred (consistent with the ground truth) using only visual information.\nHowever, kitchen scenes pose greater complexity compared to those in VGG-SS, featuring more and diverse objects that demand fine-grained visual understanding from models and introduce more candidates for distinction. Moreover, unlike the naturally sounding objects in VGG-SS, such as musical instruments and animals, the sound sources in Epic-Sounding-Objects, such as object collisions or boiling water, are less intuitive and straightforward, presenting more challenges for simple visual commonsense reasoning to identify. Consequently,"}, {"title": "4.2 Experiments", "content": "Model. We employ Hand Object Interaction Detector (HOID) [29], an HOI detection model with a ResNet-101 [11] backbone pretrained on ImageNet [7], for AVSL benchmark validation. We test two versions of HOID, a basic version pretrained on a general domain dataset DOH100k [29], and an enhanced version additionally pretrained with 42K egocentric data [5,6,20,30] to better generalize to egocentric HOI detection.\nBaselines and Evaluation. We compare HOID with several recent baselines on Epic-Sounding-Object, including Attention10k [27,28], STM [19], Hardway [2, 34], Mix [14] and EgoLoc [15]. Following previous works [15], we use CIoU@0.2 and AUC as metrics on Epic-Sounding-Object.\nResults. As shown in Tab. 2, both HOID models perform well on the Epic-Sounding-Object benchmark, and the enhanced version trained with egocentric data outperforms all baselines by a large margin. This demonstrates that simply identifying hand-object interactions can often correctly localize the sound source in Epic-Sounding-Object benchmark, revealing an obvious visual bias that diminishes the benchmark's suitability for validating AVSL models."}, {"title": "5 Discussion", "content": "This section presents our observations on VGG-SS and Epic-Sounding-Object, along with qualitative results. Based on these observations, we propose suggestions for future improvements to the benchmarks.\nVGG-SS. We provide some qualitative results of MiniGPT-v2 on VGG-SS. As shown in Fig. 2 (Top), given only video frames, the model accurately identifies the sound sources in these videos, such as animals and musical instruments. It is reasonable to expect that MiniGPT-v2 can easily pinpoint these targets, as they are typically recognized as naturally sounding objects and serve as the main subjects in the video. Another interesting finding is that MiniGPT-v2 exhibits potential in multi-source localization as well, as shown in Fig. 3. However, Fig. 2 (Bottom) illustrates some failure cases where the model struggles with only video frames as input: (1) the reasoning of sound source relies on the video motion, which cannot be captured from a single frame (Row 1); (2) the target objects are difficult to distinguish or easily confused (Row 2); and (3) multiple potential sounding objects present simultaneously (Row 3). While the first type of failure case can be addressed by utilizing a video model to handle motion information, the remaining cases require audio information for the model to accurately determine the true sounding source in a video.\nEpic-Sounding-Object. As most of the sound sources in Epic-Sounding-Object are directly linked to interactions between hands and objects, a simple HOI detection method achieves precise localization in many cases, as illustrated in Fig. 4"}, {"title": "6 Conclusion", "content": "In this paper, we revealed some significant visual biases in Audio-Visual Source Localization (AVSL) benchmarks. We first demonstrated that visual information alone can often suffice for accurately localizing the sounding objects within visual"}]}