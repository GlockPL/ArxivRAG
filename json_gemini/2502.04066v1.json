{"title": "Predicting Large Language Model Capabilities on Closed-Book QA Tasks\nUsing Only Information Available Prior to Training", "authors": ["Changhao Jiang", "Ming Zhang", "Junjie Ye", "Xiaoran Fan", "Yifei Cao", "Jiajun Sun", "Zhiheng Xi", "Shihan Dou", "Yi Dong", "Yujiong Shen", "Jingqi Tong", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "abstract": "The GPT-4 technical report from OpenAI suggests that model performance on specific tasks can\nbe predicted prior to training, though methodologies remain unspecified. This approach is crucial\nfor optimizing resource allocation and ensuring data alignment with target tasks. To achieve this\nvision, we focus on predicting performance on\nClosed-book Question Answering (CBQA) tasks,\nwhich are closely tied to pre-training data and\nknowledge retention. We address three major\nchallenges: 1) mastering the entire pre-training\nprocess, especially data construction; 2) evaluating a model's knowledge retention; and 3) pre-\ndicting task-specific knowledge retention using\nonly information available prior to training. To\ntackle these challenges, we pre-train three large\nlanguage models (i.e., 1.6B, 7B, and 13B) us-\ning 560k dollars and 520k GPU hours. We ana-\nlyze the pre-training data with knowledge triples\nand assess knowledge retention using established\nmethods. Additionally, we introduce the SMI met-\nric, an information-theoretic measure that quan-\ntifies the relationship between pre-training data,\nmodel size, and task-specific knowledge reten-\ntion. Our experiments reveal a strong linear cor-\nrelation (R2 > 0.84) between the SMI metric\nand the model's accuracy on CBQA tasks across\nmodels of varying sizes (i.e., 1.1B, 1.6B, 7B, and\n13B). The dataset, model, and code are available\nat https://github.com/yuhui1038/SMI.", "sections": [{"title": "1. Introduction", "content": "The GPT-4 technical report of OpenAI states, \u201cWe registered\npredictions for GPT-4's performance on HumanEval before\ntraining completed, using only information available prior\nto training\" (Chen et al., 2021; OpenAI, 2023). However,\nthe report lacks detailed explanations of the methodologies\nunderlying this prediction technique.\nThis technique is critical for the pre-training of Large Lan-\nguage Models (LLMs). On the one hand, the cost of a single\npre-training run can reach hundreds of millions of dollars,\nmaking re-pre-training a highly challenging endeavor (Jiang\net al., 2024; Yang et al., 2024; Dubey et al., 2024). The pre-\ndiction technique, which allows for accurate performance\nforecasting on certain tasks based only on the pre-training\ndata, helps mitigate the resource waste associated with un-\nnecessary re-pre-training aimed at improving a model's\nknowledge retention. On the other hand, it can guide the\nconstruction of pre-training data that are more aligned with\ntarget tasks, optimizing resource usage. This is particularly\nvaluable when pre-training LLMs for specialized domains,\nas it reduces the creation of redundant data and minimizes\nunnecessary expenditures (Yu et al., 2023; Xiong et al.,\n2023; Li et al., 2024b).\nTo achieve this vision, we focus on make predictions for the"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Pre-training data and LLM capabilities", "content": "The distribution characteristics of pre-training data play a\ncrucial role in shaping the knowledge retention capacity of\nLLMs. Numerous studies highlight the significance of repe-\ntition frequency in this process. For instance, Carlini et al.\n(2023) identified a logarithmic relationship between repeti-\ntion frequency and memory effects, while Chowdhery et al.\n(2023) demonstrated that sequences repeated over 500 times\nare completed by models with an accuracy exceeding 40%.\nSimilarly, Ju et al. (2024) investigated the influence of data\nfrequency on multi-hop reasoning, uncovering the presence\nof \"factual shortcuts\". Allen-Zhu & Li (2024b) proposed\nthat exposing knowledge 1000 times can achieve a storage\ncapacity of two bits per parameter in LLMs. Additionally,\nRazeghi et al. (2022) and Yadlowsky et al. (2023) demon-\nstrated that lower-order co-occurrences in pre-training data\nenhance numerical reasoning, while McCoy et al. (2023)\nassociated the prevalence of tasks in pre-training data with\nbetter performance on tasks like ROT13.\nAlthough existing research has primarily focused on the\nimpact of occurrence frequency in pre-training data on the\nmemory and reasoning capabilities of LLMs, occurrence-\nbased approaches often lack precision. Moreover, studies\naimed at predicting model performance on specific tasks by\nanalyzing pre-training data remain relatively scarce."}, {"title": "2.2. Evaluating LLMs using knowledge triples", "content": "Knowledge triples (subject, relation, object) play a pivotal\nrole in assessing the storage and retrieval capabilities of\nLLMs. LLMs derive their knowledge from patterns in their\npre-training data. Memory, as defined by (Li et al., 2024a),\nrefers to a model's ability to encode and retrieve knowledge\nby adjusting its weights, with its capacity closely tied to\nmodel size. Petroni et al. (2019) employed the LAMA\nmethod to evaluate the latent knowledge of models like\nBERT, demonstrating the value of knowledge triples in large-"}]}