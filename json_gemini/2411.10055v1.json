{"title": "Towards unearthing neglected climate innovations from scientific literature using Large Language Models", "authors": ["C\u00e9sar Quilodr\u00e1n-Casas", "Christopher Waite", "Nicole Alhadeff", "Diyona Dsouza", "Cathal Hughes", "Larissa Kunstel-Tabet", "Alyssa Gilbert"], "abstract": "Climate change poses an urgent global threat, needing the rapid identification and deployment of innovative solutions. We hypothesise that many of these solutions already exist within scientific literature but remain underutilised. To address this gap, this study employs a curated dataset sourced from OpenAlex, a comprehensive repository of scientific papers. Utilising Large Language Models (LLMs), such as GPT4-0 from OpenAI, we evaluate title-abstract pairs from scientific papers on seven dimensions, covering climate change mitigation potential, stage of technological development, and readiness for deployment. The outputs of the language models are then compared with human evaluations to assess their effectiveness in identifying promising yet overlooked climate innovations. Our findings suggest that these LLM-based models can effectively augment human expertise, uncovering climate solutions that are potentially impactful but with far greater speed, throughput and consistency. Here, we focused on UK-based solutions, but the workflow is region-agnostic. This work contributes to the discovery of neglected innovations in scientific literature and demonstrates the potential of AI in enhancing climate action strategies.", "sections": [{"title": "1 Introduction", "content": "The International Energy Agency (IEA) notes that about half the projected CO2 reductions that will be required to achieve Net Zero by 2050 will depend on technologies that are currently not commercially viable- highlighting the critical need for breakthrough innovations to mitigate the impacts of climate change [1]. Unlike the clear relationship between life sciences and biotech innovation, for example, there is no one academic field that dominates climate innovation. Potential solutions can emerge from disparate fields. Therefore, one likely reason for the neglect of certain climate solutions is the sheer volume and diversity of scientific literature. Traditional methods of knowledge discovery and synthesis may fail to capture innovative approaches buried in vast datasets, leading to missed opportunities for policy and technological advancement [2]. This is especially relevant for countries like the UK, which has a world-leading academic culture and made substantial investments to foster climate innovation but may still have untapped potential in its existing scientific outputs [12]. \u03a4\u03bf address this challenge, we propose the use of machine learning (ML) and Large Language Models (LLMs) to systematically identify climate innovations in scientific literature. We leverage OpenAlex [8], a comprehensive open dataset of scholarly papers and comprehensive meta-data, to provide test data for analysis by state-of-the-art language models, such as GPT4-0 from OpenAI. These models are prompted to evaluate paper abstracts across seven dimensions: climate emissions reduction/removal potential, technology level, deployability, market need, potential to enable subsequent innovation,"}, {"title": "2 Background and Literature Review", "content": "Machine learning (ML) plays a vital role in addressing climate change by identifying trends, assessing risks, and informing policy [9]. Within ML, Natural Language Processing (NLP) has been used to identify climate risks in corporate disclosures, uncovering insights into sustainability efforts [11]. More recently, large language models (LLMs) have been employed for climate-related tasks, such as supporting public sector decision-making through simulations and modelling [3]. LLMs have also been utilised to explore the broader research landscape, especially in mapping innovation trends in countries like the UK [7]. By analysing the links between research papers and patents, LLMs help identify areas of scientific work with high commercialisation potential [6]. Furthermore, studies have leveraged LLM-based evaluators for assessing scientific claims and conducting evaluations traditionally requiring human judgement [10, 5, 4], as well as combining human and LLM assessments has shown promise in enhancing decision-making processes [14]. However, there is a gap in using LLMs to systematically identify climate innovations within a specific region. This study addresses this gap by using LLM evaluators to identify potential climate innovations in the UK, creating a workflow that can be applied to other regions and serve as the foundation for future work."}, {"title": "3 Methods", "content": null}, {"title": "3.1 Human and LLM evaluation", "content": "In this study, we aimed to identify and evaluate climate-related research papers from six UK-based institutions using both human evaluators and a LLM. The process involved multiple steps, including data collection, abstract filtering, human survey design, and LLM evaluation in different scenarios. Below, we outline each part of the methodology in detail."}, {"title": "3.2 Data Collection from OpenAlex", "content": "To collect relevant research papers, we utilised the OpenAlex database. OpenAlex [8] is an open, community-curated database designed to provide comprehensive metadata on scholarly publications across a wide range of disciplines. Developed by OurResearch as the successor to the Microsoft Academic Graph (MAG) [13], OpenAlex was launched in 2022 with the mission of improving access to scientific knowledge. OpenAlex provides open access to all data through an Application Programming Interface (API), supporting complex queries and offering bulk data downloads, making it suitable for large-scale data analysis projects. In addition to standard interoperable metadata for cross-referencing with other research databases, such as Digital Object Identifiers (DOIs), OpenAlex has applied a comprehensive automated topic classification to all research works, based the Centre for Science and Technology Studies (CWTS) taxonomy. This classifies works according to 4 over-arching domains, 252 subfields and 4516 granular topics and enables a richer analysis of the research corpus.\nOur goal was to focus on climate-related innovations from 6 UK institutions: Imperial College London, University of Leeds, Swansea University, UK Centre for Ecology and Hydrology, University of Cambridge and University of Oxford. We queried OpenAlex for papers where a corresponding author was affiliated with one of these six institutions. We excluded papers with primary topic domain specified as Health Sciences or Social Sciences to maintain a focus on technical and scientific research that is most directly relevant to climate innovation. We also limited works to types associated with primary research (article, pre-print, book chapter or dissertation) and published from year 2000 onwards. The combined query resulted in a dataset of approximately 133,619 works (up to September 2024). The data was downloaded using the pyalex Python Library and"}, {"title": "3.3 Human Evaluation via Survey", "content": "We implemented a survey using Qualtrics to collect human evaluations for the 100 selected abstracts. The survey was designed to collect binary responses to seven key questions that assessed different aspects of each paper's potential contribution to climate innovation. The seven questions used in the survey are listed below:\n\u2022 MITIGATION: Could this research feasibly lead to a reduction of greenhouse gas emissions or removal of carbon dioxide from the atmosphere?\n\u2022 TECHNOLOGY: Does this research describe a technology with practical application?\n\u2022 READINESS: Does this research demonstrate that proof-of-concept has been achieved prior to commercialisation or deployment?\n\u2022 MARKET: Does a clear commercial market or industry need exist for this research?\n\u2022 TECH ENABLING: Rather than a stand-alone technology, does this research represent the fundamental science that might enable future technology development?\n\u2022 ECO FOCUS: Was this research conducted with an explicit climate change or sustainability application in mind?\n\u2022 NEGLECTEDNESS: Is this research more likely than not to be neglected by existing innovation support mechanisms in the UK?\nA total of six participants completed the survey, providing binary \"Yes\" (1) or \"No\" (0) responses to all seven questions across each of the 100 test abstracts. The binary response format was chosen to simplify analysis and allow for clear comparisons between human and LLM evaluations. Before answering the questions, participants were provided with identical text-based context providing an explanation of each question's purpose and relevance to climate innovation, guiding the decision-making process. This approach was intended to reduce variability in human interpretation and ensure that participants had a similar understanding of the evaluation criteria."}, {"title": "3.4 Large Language Model (LLM) Evaluation", "content": "In parallel with the human evaluation, we used a large language model (LLM) to analyse the same 100 abstracts and answer the same seven questions mentioned in section 3.3. To investigate how different levels of context affect the LLM's performance, we set up three distinct scenarios for the LLM prompts (all with temperature = 0 to get almost deterministic answers):\n\u2022 No-shot: In this scenario, the LLM was prompted with only the abstract of each paper and a basic instruction: \"Do not hallucinate. Only provide truthful answers.\" This setup aimed to assess the LLM's ability to interpret and evaluate the abstract independently without any additional context or guidance.\n\u2022 Context: For this scenario, we included the same context that was provided to human evaluators. The LLM received the abstract along with a description of the purpose of each question, similar to what was presented to the survey participants. The goal of this scenario was to determine whether providing context improved the LLM's accuracy and alignment with human judgements.\n\u2022 Few-Shot Learning: In the third scenario, we employed a few-shot learning approach to guide the LLM. We supplied the LLM with 10 further example abstracts that described scientific papers leading to spin-out climate-tech companies with high potential to mitigate climate change. The prompt included these examples to demonstrate what types of research"}, {"title": "3.5 Data Processing and Analysis", "content": "For both human and LLM evaluations, the responses were recorded in a structured dataset. The binary results for each of the seven questions across all 100 abstracts were compiled for subsequent analysis. We aimed to compare the LLM's responses in each scenario against the human evaluations to identify areas of agreement and discrepancy. Each abstract received seven binary scores from each human evaluator and from the LLM in each of the three scenarios. To quantify general alignment between the human and LLM responses across all respondants and abstracts, we performed pairwise correlation (using the Pearson standard correlation coefficient) between the seven questions and between the datasets. Additionally, we analysed the influence of context and few-shot learning on the LLM's performance. By comparing the LLM's responses across the three scenarios (no context, context, and few-shot), we assessed whether providing additional information or examples improved the accuracy and consistency of the LLM's evaluations relative to the human benchmarks. Finally, we devised a preliminary algorithm to attempt to rank the outputs from the evaluations. First, the MITIGATION question was used as a binary filter to select abstracts identified as broadly relevant to the overall search objective. Secondly, logistic regression was used to determine weightings for the responses to the other six questions when trained on the 5 positive control abstracts. The sum of the weighted scores was used to rank filtered abstracts. Once an optimal LLM scenario was determined, we validated this with a larger test dataset consisting of 990 random abstracts and 10 independent positive control abstracts, as described above. This methodology allows us to systematically evaluate the capability of LLMs in identifying overlooked climate innovations and to explore how different types of context affect their performance. The results from these evaluations are intended to provide insights into how LLMs can be used to supplement human expertise in the search for neglected climate solutions."}, {"title": "4 Results", "content": null}, {"title": "4.1 Comparison of LLM scenarios against human reasoning", "content": "The 5 positive control abstracts with known linkages to climate tech spin-outs were successfully identified by all three LLM scenarios (no shot, context, few-shot), arguably more convincingly than the 6 human survey participants. shows the mean respondent scores across the seven questions for the controls (numbered by random id) compared to the mean of all abstracts. In all four datasets, the positive controls generally scored more highly than average. Human scoring was highly variable, compared to predominantly deterministic scoring in the LLM scenarios. Interestingly, while the human respondents generally gave the controls above average scores across all questions, the context and few-shot LLM scenarios scored the controls very low on questions relating to whether the research is fundamentally enabling of technology (Q5) or neglected (Q7), which is predictive of the reality that these technologies have been commercially exploited. Considering the average scores across the 100 abstracts, the LLM provided with the same textual context as the human respondents scored most similarly, particularly clear for technology readiness (Q2) and neglectedness.\nCohen's Kappa (\u043a), a statistical measure that evaluates the level of agreement between two raters beyond what would be expected by chance, was computed for each of the seven questions to compare human responses with those generated by the three LLM scenarios (see Table 1). The formula for Cohen's Kappa is given by $\\kappa = \\frac{P_o-P_e}{1-P_e}$, where $P_o$ is the observed agreement between the raters, and $P_e$ is the expected agreement by chance. \u0410 \u043a value of 1 indicates perfect agreement, 0 indicates agreement equivalent to chance, and negative values suggest disagreement. The results show moderate agreement for questions related to mitigation (Q1) and eco-focus (Q6), particularly under the context scenario (\u043a = 0.7043 for Q6). However, there is notably low or even negative"}, {"title": "4.2 Evaluation of abstracts identified by LLM", "content": null}, {"title": "4.2.1 Climate change mitigation potential", "content": "Further analysis was performed solely on the LLM with context scenario. Regardless of consistency, the effectiveness of LLM reasoning ultimately depends on a granular assessment of its performance across the unknown test dataset as a whole. Firstly, an objective logic flow was devised for translating scores across the survey questions into a ranking of abstracts. Given the objective focus on climate"}, {"title": "4.2.2 Weighted scoring of Q2-7 determined by Logistic Regression", "content": "To determine the relative predictive quality of the remaining six evaluation questions as indicators for identifying potentially commercialisable climate research papers, we applied logistic regression exclusively to the set of 5 known positive control abstracts related to successful spin-out ventures."}, {"title": "4.2.3 Extending LLM reasoning with a scalar scoring system", "content": "In an attempt to improving the LLM scoring resolution, we reapplied the context approach but with a scalar 1-10 scoring scale rather than a binary system. This type of scoring system was deemed too difficult to apply consistently enough to human reasoning across a large sample of test abstracts. However, the LLM was able to replicate a qualitatively similar scoring outcome but with significantly increased granularity. At the point of Q1 filtering, an almost identical selection of abstracts (24) passed the threshold despite an increased range of possible values (see Figure 3b: LLM context (scalar)). Pearson's correlation coefficient highlights the similarity between the context LLM output in both binary and scalar scoring scenarios (see Figure 2). Subsequently, with updated weightings calculated (see Table 2: Context (scalar)) and reapplied ranking resolution was improved significantly, with only 2 identical weighted scores out of 24. As depicted in Figure 4a the scaling scoring system (red) allows differentiation of highly scoring abstracts. Interestingly, updated abstract scores vary both positively and negatively with previous binary scoring system (blue). There is no particular directionality evident."}, {"title": "4.2.4 Qualitative assessment of research works identified", "content": "The LLM ranked the 5 positive control abstracts at positions 5, 6, 9, 10 and 14, revealing several high scoring candidates among the remaining random abstracts. The top 10 ranked random abstracts were summarised qualitatively by frequency of associated OpenAlex keywords"}, {"title": "4.3 Validation of LLM search tool on larger sample dataset", "content": "The optimised LLM scoring approach (contextualised, weighted and scalar) was validated with a larger dataset of 1000 abstracts, containing a further 10 positive control abstracts associated with spin-outs. 176 papers passed the initial filtering according to potential for climate emissions mitigation (Q1). Only 8 of the 10 positive control abstracts at passed the filter, ranked at positions 8, 26, 43, 52, 59, 61, 63, 95 (see Figure 4b). Given the low probability that more than half of the filtered papers have as high a potential for commercialisation as the pre-curated controls, this suggests that training the score weightings on more than 5 positive test cases could be worthwhile. Keyword assessment of the top 50 ranked abstracts reveal thermal hybrid and photovoltaic technologies as frequently as carbon dioxide capture, observed previously."}, {"title": "5 Summary and future work", "content": "We show that with appropriate context, prompt engineering and scoring algorithm for interpreting outputs, LLMs represent promising vehicles for the high-throughput identification of high-potential research from the large corpus available. We will further optimise this approach and apply it to the whole UK-wide corpus with the desired output function of predicting climate solutions, a scale of task beyond manual execution. There is significant room for improvement and expansion. Future work will involve experimenting with different LLMs beyond GPT-40, such as LLaMA or other domain-specific models, to enhance the accuracy and diversity of responses. We will also incorporate Retrieval-Augmented Generation (RAG) to provide the LLMs with a more extensive context, potentially increasing the quality of answers. To accelerate RAG processes, binary quantisation techniques will be applied to reduce computational complexity and improve model efficiency. Additionally, a multi-agent approach will be employed to critically evaluate the LLM's responses. By using multiple agents, we aim to establish a more robust consensus in the assessment of each abstract. Furthermore, we are developing a relational layer to link papers with their authors, grants, and social media presence. This relational analysis will enable the identification of not only commercially viable papers but also the entrepreneurial potential of the researchers themselves. By understanding authors' networks and outreach, we hope to target those with a higher likelihood of successfully translating research into impactful climate-related start-up ventures. This study describes the first steps towards the discovery and unearthing of climate solutions in scientific literature."}]}