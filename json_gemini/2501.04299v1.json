{"title": "Circuit Complexity Bounds for Visual Autoregressive Model", "authors": ["Yekun Ke", "Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "Understanding the expressive ability of a specific model is essential for grasping its capacity\nlimitations. Recently, several studies have established circuit complexity bounds for Transformer\narchitecture. Besides, the Visual AutoRegressive (VAR) model has risen to be a prominent\nmethod in the field of image generation, outperforming previous techniques, such as Diffusion\nTransformers, in generating high-quality images. We investigate the circuit complexity of the\nVAR model and establish a bound in this study. Our primary result demonstrates that the VAR\nmodel is equivalent to a simulation by a uniform $TC^0$ threshold circuit with hidden dimension\n$d\\leq O(n)$ and poly(n) precision. This is the first study to rigorously highlight the limitations\nin the expressive power of VAR models despite their impressive performance. We believe our\nfindings will offer valuable insights into the inherent constraints of these models and guide the\ndevelopment of more efficient and expressive architectures in the future.", "sections": [{"title": "1 Introduction", "content": "Visual generation has seen widespread applications across various domains, including image restora-\ntion [LHC+25, GLD+25], augmented reality [AWT+24], medical imaging [AKH+24, MHL+24,\nLLL+24a], and creative industries such as game development [RHR+20, CGX+25]. By generating\nrealistic and diverse images from textual descriptions or other forms of input, visual generation mod-\nels are transforming how machines perceive and produce visual content. Among the most popular\nmodels for visual generation are Variational AutoEncoders (VAE) [Doe16], Generative Adversar-\nial Networks (GAN) [GPAM+20], Diffusion models [SDWMG15, HJA20], and Flow-based models\n[KD18]. These models have made notable progress in producing high-quality, high-resolution, and\ndiverse images, expanding the potential of visual generation through improvements in realism,\ndiversity, and fidelity.\nHowever, the introduction of the Visual AutoRegressive model (VAR) [TJY+24] represents a\nsignificant shift in the paradigm in this field. Instead of the traditional \u201cnext-token prediction\",\nthe VAR model adopts a coarse-to-fine \u201cnext-scale prediction\" approach. Through this innovative\napproach, the VAR model is able to capture visual distributions more effectively, exceeding the\nperformance of diffusion transformers in image generation tasks. Additionally, VAR's zero-shot\ngeneralization capability spans multiple tasks, including image inpainting and manipulation. These\nresults suggest that VAR offers a promising direction for autoregressive models in visual generation.\nAs the VAR model demonstrates its impressive performance, it is crucial to explore the lim-\nitations of the expressiveness of the VAR model. Up to now, the expressiveness from a circuit\ncomplexity perspective of the VAR model remains underexplored. This gap raises an important\nquestion:\nWhat are the limitations of the expressive power of the VAR model in terms of circuit complexity?\nTo explore this issue, we apply circuit complexity theory, which offers valuable tools for analyzing\nthe computational resources needed for specific tasks. By representing the VAR model as complexity\ncircuits, we can systematically evaluate their capabilities and determine the lower bounds of the\nproblems they can address.\nIn this work, we present a comprehensive theoretical investigation into the circuit complexity\nbounds of the VAR models. Our approach involves analyzing and formulating the architecture\nof the VAR model and analyzing the computational complexity of its components, such as up-\ninterpolation layers, convolution layers, transformer blocks, etc. Finally, we show that uniform $TC^0$\ncircuits can efficiently simulate these models.\nThe primary contributions of our work are summarized below:\n\u2022 As far as we know, this is the first paper to present a mathematical formulation of the Visual\nAutoRegressive model (Section 4).\n\u2022 We prove that DLOGTIME-uniform $TC^0$ circuit family can simulate any Visual AutoRegressive\nmodel with O(1) depth, poly(n) size, and poly(n) precision (Theorem 5.11).\nRoadmap. Section 2 offers a summary of the related works. Section 3 introduces the necessary\nnotations and definitions for the subsequent analysis. In Section 4, we present the mathematical\nformulation of the VAR model. Section 5 details the circuit complexity results for the VAR model.\nSection 6 presents the conclusions of our work."}, {"title": "2 Related Work", "content": "Transformer Architecture has shown remarkable success in various fields, particularly in natural\nlanguage processing, reinforcement learning, and computer vision. By leveraging self-attention\nmechanisms to capture long-range dependencies, the Transformer has become the architecture of\nchoice for applications such as machine translation [RT18, WLX+19, YW20] and image generation\n[PVU+18, DYH+21, TJY+24]. Recently, a series of studies have shed insight into the reason-\ning limitations of Transformer Architecture [MSS22, MS23, FZG+24, MS24, WMS+24, LSS+24,\nKLS+24, HSK+24, Chi24, HCL+24, HWL24a, HWG+24]. Specifically, [MSS22] showed that a gen-\neralized form of hard attention can recognize languages that go beyond what the $AC^0$ class can\ncompute, with the $TC^0$ class serving as an upper bound for the formal languages it can identify.\nThe study by [LAG+22] established that softmax-transformers (SMATs) are included in the non-\nuniform $TC^0$ class. As a next step, [MS23] demonstrated that SMATs belong to L-uniform $TC^0$\nclass. Recently, [Chi24] demonstrated that average-hard attention transformers (AHATs), without\napproximation, and SMATs with floating-point precision of O(poly(n)) bits, as well as SMATs with\nat most $2^{-O(poly(n))}$ absolute error, can all be classified in the DLOGTIME-uniform $TC^0$ class."}, {"title": "3 Preliminary", "content": "The notations used in this paper are introduced in Section 3.1. Section 3.2 explains the basics\nof circuit complexity classes. Section 3.3 introduces key simulations of floating-point operations,\nwhich will be used in later sections for the proofs."}, {"title": "3.1 Basic Notations", "content": "We apply [n] to represent the set {1,2,\u2026, n} for any positive integer n. The set of natural numbers\nis denoted by $\\mathbb{N} := \\{0, 1, 2, . . . \\}$. Let $X \\in \\mathbb{R}^{m\\times n}$ be a matrix, where $X_{i,j}$ refers to the element at the\ni-th row and j-th column. When $x_i$ belongs to {0,1}*, it signifies a binary number with arbitrary\nlength. In a general setting, $x_i$ represents a length p binary string, with each bit taking a value of\neither 0 or 1."}, {"title": "3.2 Key Concepts in Circuit Complexity", "content": "We discuss several circuit complexity classes, starting with the concept of a boolean circuit.\nDefinition 3.1 (Boolean Circuit, Definition 6.1 in [AB09]). A Boolean circuit with input size n,\nwhere $n \\in \\mathbb{N}$, corresponding to a function that $C_n : \\{0,1\\}^n \\rightarrow \\{0,1\\}$. This circuit can be typically\nrepresented as a directed acyclic graph (DAG). There are n input nodes in the graph, all with an\nin-degree of 0. Other nodes are classified as logic gates and are assigned one of the labels AND, OR,\nor NOT. We use $|C_n|$ to represent the size of $C_n$, referring to the count of nodes in the Boolean\ncircuit.\nTherefore, we can proceed to define the languages recognizable by certain families of Boolean\ncircuits, considering their structural constraints, gate types, and depth. These factors determine\nthe computational power of the circuits in each family.\nDefinition 3.2 (Language, Definition 6.2 in [AB09]). Let $L \\subseteq \\{0,1\\}^*$ denote a language. L can be\nrecognized by a Boolean circuits family C if, for every string $x \\in \\{0,1\\}^*$, a Boolean circuit $C_{|x|} \\in C$\nexists exists, which takes x as input. This circuit has an input length of |x|, and $x \\in L$ if and only if\n$C_{|x|}(x) = 1$ holds.\nNext, the concept of complexity classes will be given, which categorizes computational problems\nbased on their inherent difficulty, determined by the resources\u2014such as time or space\u2014required\nto solve them. In this context, different complexity classes impose constraints on the resources of\nBoolean circuits, which can be further characterized by factors such as circuit size, depth, number\nof fan-in, and gate types. We introduce the complexity classes as the following\n\u2022 A language belongs to $NC^i$ class if it can be decided by a poly(n) size, $O(log(n))$ depth\nboolean circuits equipped with restricted fan-in basic gates AND, OR and NOT gates.\n\u2022 A language belongs to $AC^i$ class if it can be decided by a poly(n) size, $O(log(n))$ depth\nboolean circuits equipped with no-limit fan-in basic gates AND, OR and NOT gates.\n\u2022 A language belongs to $TC^i$ class if it can be decided by a poly(n) size, $O(log(n))$ depth\nboolean circuits equipped with no-limit fan-in basic gates AND, OR, NOT and MAJORITY\ngates.\n\u2022 A language belongs to P class if it can be decided by a deterministic Turing machine in\npolynomial time with respect to its input size\nThere is a folklore regarding the hierarchical relationships between the complexity classes mentioned\nabove, for every $i \\in \\mathbb{N}$:\n$NC^i \\subseteq AC^i \\subseteq TC^i \\subseteq NC^{i+1} \\subseteq P$.\nNote that the question of whether $TC^0 \\subseteq NC^1$ remains an open problem in circuit complexity.\nIn theoretical computer science, the uniformity of a complexity class refers to whether the circuit\nfamily in question can be constructed by a uniform algorithm, i.e., an algorithm that outputs a\ndescription of the circuit for any input size. Specifically, L-uniformty requires a Turing machine\nthat uses $O(log(n))$ space to output a circuit C which can recognize a given language $L\\subseteq \\{0,1\\}^*$.\nMoreover, DLOGTIME-uniformity stipulates that a random access Turing machine must produce a\ncircuit C that recognizes a given language LC {0,1}*. Except in the case of small circuit complexity\nclasses, where circuits are incapable of simulating the machines that create them, DLOGTIME-\nuniformity is the same as L-uniformity. For further discussion on various notions of uniformity, see\n[BI94, HAB02].\nThroughout this work, any reference to a uniform $TC^0$ should be understood as referring to a\nDLOGTIME-uniform $TC^0$."}, {"title": "3.3 Basic Tools", "content": "In this section, we first define floating-point numbers and then illustrate a series of operations\ninvolving them. Finally, we analyze the circuit complexity associated with these operations, which\nis essential in the later proof.\nDefinition 3.3 (Floating point number, Definition 9 in [Chi24]). Let p be an integer representing\nprecision. Let $m\\in (-2^p, -2^{p-1}] \\cup \\{0\\} \\cup [2^{p-1}, 2^P)$ denote an integer called the significance. Let\n$e\\in [-2^p, 2^p)$ denote an integer called the exponent. A floating point number with p-bits is composed\nof the parts m and e, and its value is given by $m\\cdot2^e$. Throughout this paper, the set of all p-bit\nfloating-point numbers is denoted by $\\mathbb{F}_p$."}, {"title": "Definition 3.4", "content": "(Rounding Operation, Definition 9 in [Chi24]). Given a floating point number x,\nwe use $round_p(x)$ to denote the nearest number to x which is p-bit floating-point.\nFor the definitions of addition, multiplication, division, comparison, and floor operations on\nfloating-point numbers as outlined in Definition 3.3, refer to [Chi24]. In this paper, we introduce\nthe corresponding circuit complexity classes to which these operations belong.\nLemma 3.5 (Operations on floating point numbers in $TC^0$, Lemma 10 and Lemma 11 of [Chi24]).\nAssume the precision $p < poly(n)$. Then we have:\n\u2022 Part 1. Given two p-bits float point numbers $x_1$ and $x_2$. Let the addition, division, and\nmultiplication operations of $x_1$ and $x_2$ be outlined in [Chi24]. Then, these operations can\nbe simulated by a size bounded by poly(n) and constant depth bounded by $d_{std}$ DLOGTIME-\nuniform threshold circuit.\n\u2022 Part 2. Given n p-bits float point number $x_1,...,x_n$. The iterated multiplication of $x_1, x_2..., x_n$\ncan be simulated by a size bounded by poly(n) and constant depth bounded by $d_{\\otimes}$ DLOGTIME-\nuniform threshold circuit.\n\u2022 Part 3. Given n p-bits float point number $x_1,...,x_n$. The iterated addition of $x_1, x_2..., x_n$\ncan be simulated by a size bounded by poly(n) and constant depth bounded by $d_{\\oplus}$ DLOGTIME-\nuniform threshold circuit. To be noticed, there is a rounding operation after the the summation\nis completed.\nThen, we show a lemma stating that we can use a $TC^0$ circuit to simulate the approximated\nexponential function.\nLemma 3.6 (Approximating the Exponential Operation in $TC^0$, Lemma 12 of [Chi24]). Assume\nthe precision $p \\leq poly(n)$. Given any number x with p-bit float point, the $exp(x)$ function can\nbe approximated by a uniform threshold circuit. This circuit has a size bounded by poly(n) and a\nconstant depth $d_{exp}$, and it guarantees a relative error of at most $2^{-p}$.\nFinally, we present a lemma stating that we can use a $TC^0$ circuit to simulate the approximated\nsquare root operation.\nLemma 3.7 (Approximating the Square Root Operation in $TC^0$, Lemma 12 of [Chi24]). Assume\nthe precision $p \\leq poly(n)$. Given any number x with p-bit float point, the $\\sqrt{x}$ function can be\napproximated by a uniform threshold circuit. This circuit has a size bounded by poly(n) and a\nconstant depth $d_{sqrt}$, and it guarantees a relative error of at most $2^{-P}$."}, {"title": "4 Model Formulation", "content": "Section 4.1 provides the definitions related to the VAR model. In Section 4.2, we present the\nmathematical formulation of the components involved in the token map generation phase of the VAR\nmodel. In Section 4.3, we present the mathematical formulation of the components involved in the\nfeature map reconstruction phase of the VAR model. In Section 4.4, we present the mathematical\nformulation of the components involved in the VQ-VAE Decoder phase of the VAR model."}, {"title": "4.1 Definitions", "content": "We give the following notations in our setting.\nDefinition 4.1 (Codebook of VQ-VAE). In VQ-VAE, the Codebook is typically represented as a\nmatrix $C\\in \\mathbb{R}^{CVAE\\times dvae}$, where:\n\u2022 $C_{vae}$ is the number of vectors in the Codebook (i.e., the size of the Codebook),\n\u2022 $d_{vae}$ is the dimensionality of each vector."}, {"title": "4.2 Phase 1: VAR Transformer", "content": "VAR uses the VAR Transformer to convert the initial tokens of the generated image into several\npyramid-shaped token maps. And in the token maps generation phase, the token maps for the next\nscale, $M_{k+1}$, are generated based on the previous k token maps $M_1,..., M_k$. This phase has the\nmain modules as the following:\nUp Sample Blocks. VAR performs an upsampling operation on the (i)-th token map, adjusting\nits size to that of the (i + 1)-th token map, before feeding the k token maps into the VAR Trans-\nformer. Specifically, VAR employs an upsampling method using interpolation for the image. Here,\nwe define the up-interpolation blocks:\nDefinition 4.2 (Bicubic Spline Kernel). A bicubic spline kernel is a piecewise cubic function\n$W : \\mathbb{F}_p \\rightarrow \\mathbb{F}_p$ that satisfies $W(x) \\in [0,1]$ for all $x \\in \\mathbb{F}_p$.\nDefinition 4.3 (Up-interpolation Layer). The Up-Interpolation layer is defined as follows:\n\u2022 Let h and h' represent the heights of the input and output feature maps, respectively, where\n$h, h' \\in \\mathbb{N}$.\n\u2022 Let w and w' denote the widths of the input and output feature maps, respectively, where\n$\\omega, \\omega' \\in \\mathbb{N}$.\n\u2022 Let $c \\in \\mathbb{N}$ denote the number of channels.\n\u2022 Let $X \\in \\mathbb{R}^{h\\times w\\times c}$ denote the input feature map.\n\u2022 Let $Y \\in \\mathbb{R}^{h'\\times w'\\times c}$ denote the output feature map.\n\u2022 Let $s,t \\in \\{-1,0,1,2\\}$.\n\u2022 Let $W: \\mathbb{F}^{hxwxc}\\rightarrow \\mathbb{F}^{hxwxc}$ be a bicubic spline kernel as defined in 4.2.\nWe use $\\phi_{up} : \\mathbb{F}^{hxw xc} \\rightarrow \\mathbb{F}^{hxw'xc}$ to denote the up-interpolation operation then we have\n$Y = \\phi_{up}(X)$. Specifically, for $i \\in [h'], j\\in [w'], l\\in [c]$, we have\n$Y_{i,j,l} := \\sum_{s=-1}^{2}\\sum_{t=-1}^{2}W(s)\\cdot X_{i+\\frac{i}{h'}+s,j+\\frac{j}{w'}+t,l}\\cdot W(t)$"}, {"title": "Definition 4.4", "content": "(Attention Matrix). We use $W_Q, W_K \\in \\mathbb{F}^{d\\times d}$ to denote the weight matrix of the\nquery and key. Let $X \\in \\mathbb{F}^{n\\times d}$ represent the input of the attention layer. Then, we use $A \\in \\mathbb{F}^{n\\times n}$\nto denote the attention matrix. Specifically, we denote the element of the attention matrix as the\nfollowing:\n$A_{i,j} := exp(X_{i,*}\\cdot W_QW_KX_{j,*})^\\top$.\nIn the next step, we proceed to define the single attention layer.\nDefinition 4.5 (Single attention layer). We use $W_V \\in \\mathbb{F}^{d\\times d}$ to denote the weight matrix of value.\nLet $X \\in \\mathbb{F}^{n\\times d}$ represent the input of the attention layer. Let A denote the attention matrix defined\nin Definition 4.4. Let $D := diag(A_{1:n})$ denote a size n \u00d7 n matrix. Then, we use Attn to denote\nthe attention layer. Specifically, we have\n$Attn(X) := D^{-1}AXW_V$.\nThen, we move forward to define the multilayer perceptron layer.\nDefinition 4.6 (Multilayer Perceptron layer). Given an input matrix $X \\in \\mathbb{F}^{n\\times d}$. Let $i \\in [n]$. We\nuse $g^{MLP}$ to denote the MLP layer. Specifically, we have\n$g^{MLP} (X)_{i,*} := W \\cdot X_{i,*} + b$.\nWe then proceed to define the layer-wise normalization layer.\nDefinition 4.7 (Layer-wise normalization layer). Given an input matrix $X \\in \\mathbb{F}^{n\\times d}$. Let $i \\in [n]$.\nWe use $g^{LN}$ to denote the LN layer. Specifically, we have\n$g^{LN}(X)_{i,*} := \\frac{X_{i,*}-\\mu_i}{\\sqrt{\\sigma_i}}$\nwhere $\\mu_i := \\sum_{j=1}^d X_{i,j}/d$, and $\\sigma_i^2 := \\sum_{j=1}^d(X_{i,j} - \\mu_i)^2/d$.\nRecall we have defined $\\phi_{up} : \\mathbb{R}^{h\\times w\\times c} \\rightarrow \\mathbb{R}^{h'\\times w'\\times c}$ in Definition 4.3. Since there is no non-linear\noperation in $\\phi_{up}$, $\\phi_{up}$ is equivalent to a matrix multiplication operation, where the dimension of the\nmatrix is $\\mathbb{R}^{h'w'\\times hw}$. For simplicity, we view $\\phi_{up}$ as a $\\mathbb{R}^{h'w'\\times hw}$ dimension matrix in the following\nproofs.\nRemark 4.8 (Applying $\\phi_{up}$ on $X \\in \\mathbb{R}^{n\\times d}$). The actual input of VAR Transformer Layer are r\ninput token maps, $X_1 \\in [][\\mathbb{R}^{h_1\\times w_1\\times d},..., X_r \\in \\mathbb{R}^{h_r\\times w_r\\times d}$. We denote them as $X \\in \\mathbb{R}^{n\\times d}$, where\n$n := \\sum_{i=1}^r h_iw_i$. We denote $\\phi_{up}(X) \\in \\mathbb{R}^{n'\\times d}$ as applying $\\phi_{up}$ to each $X_i \\in \\mathbb{R}^{h_i\\times w_i\\times d}$ for $i \\in [r]$,\nwhere $n' = \\sum_{i=1}^r h'_iw'_i$\nThen, we can combine multiple attention layers with other components (up-interpolation layers,\nmultilayer perceptron layers, layer-wise normalization layers) to create a complete VAR Transformer\narchitecture."}, {"title": "Definition 4.9", "content": "(VAR transformer). Assume the VAR transformer has m Transformer layers. At\nthe i-th transformer layer, let $g_i$ denote components excluding the attention layer, such as the LN\nlayer or MLP layer. Let $Attn_i$ stand for the self-attention layer, which is defined in Definition 4.5.\nGiven an input token map $X \\in \\mathbb{F}^{n1\\times d}$. We define a VAR transformer as the following\n$TF(X) := g_m \\circ Attn_m \\circ \\Phi_{up}...\\circ \\Phi_{up} \\circ g_1 \\circ Attn_1 \\circ \\Phi_{up}(X) \\in \\mathbb{F}^{n\\times d}$,\nIn this expression, o stands for functional composition."}, {"title": "4.3 Phase 2: Feature Map Reconstruction", "content": "In phase 2, VAR will transform the generated token maps into feature maps. This phase has the\nfollowing main modules:\nUp Sample Blocks. The VAR performs upsampling on token maps of different sizes, scaling\nthem to the size of the final output feature map. In this process, VAR will use the up-interpolation\nblocks defined in Definition 4.3. To mitigate information loss during token map up-scaling, VAR\nemploys convolution blocks to post-process the up-scaled token maps. We define the convolution\nblocks as the following:\nDefinition 4.10 (Convolution Block). Let $X \\in \\mathbb{F}^{h\\times w\\times c}$ represent a feature map, where h and w\ndenote the height and width of the feature map, and c is the number of input channels. Consider\na convolution kernel $K \\in \\mathbb{F}^{h_k\\times w_k\\times c}$, where $h_k$ and $w_k$ are the height and width of the kernel,\nrespectively. Assume the input feature map has zero padding, and the stride of the kernel is 1.\nThe convolution operation performed by this kernel on the input feature map is defined for $i \\in$\n$[1, h \u2013 h_k + 1]$ and $j \\in [1, w \u2013 w_k + 1]$ as the following:\n$Y_{i,j} := \\sum_{m=1}^{h_k}\\sum_{n=1}^{w_k}\\sum_{q=1}^{C} X_{i+m-1, j+n-1,q}\\cdot K_{m,n,q} + b$\nwhere Y is the output feature map of the convolution layer, and b is the bias term of the kernel."}, {"title": "4.4 Phase 3: VQ-VAE Decoder process", "content": "VAR will use the VQ-VAE Decoder Module to reconstruct the feature map generated in Section 4.3\ninto a new image. The Decoder of VQ-VAE has the following main modules:\nResNet Blocks. In the VQVAE decoder, the ResNet block, which includes two (or more) con-\nvolution blocks, plays a crucial role in improving the model's ability to reconstruct high-quality\noutputs. The convolution blocks help capture spatial hierarchies and patterns in the data, while\nthe residual connections facilitate better gradient flow and allow the model to focus on learning the\nresiduals (differences) between the input and output. The definition of convolution block is given\nin Definition 4.10.\nAttention Blocks. The Attention block helps the Decoder fuse information from different lo-\ncations during the generation process, which can significantly improve the clarity and detail of\nthe generated images. When applied to a feature map, the attention mechanism computes atten-\ntion scores for all pairs of pixels, capturing their pairwise relationships and dependencies. The\ndefinitions of blocks in attention are given in Section 4.2."}, {"title": "5 Complexity of VAR Models", "content": "We present the critical findings on the circuit complexity of crucial operations in the computation\nof VAR models. In Section 5.1, we analyze the up-interpolation blocks. In Section 5.2, we examine\nthe matrix operations. In Section 5.3, we proceed to study the single attention layer. In Section 5.4,\nwe move forward to compute the MLP layer and LN layer. In Section 5.5, we study the convolution\nlayer computation. In Section 5.6, We show that we can use a uniform $TC^0$ circuit to model the\nVAR Transformer. In Section 5.7, we show that we can use a uniform $TC^0$ circuit to model the\nfeature map reconstruction layer. In Section 5.8, we show that we can use a uniform $TC^0$ circuit\nto model the VQ-VAE Decoder. Finally, we show our main result in Section 5.9."}, {"title": "5.1 Computing Up Interpolation Blocks", "content": "In this section, we can show that the up-interpolation layers can be computed in $TC^0$.\nLemma 5.1 (Up-Interpolation in $TC^0$). Let $X \\in \\mathbb{F}^{hxwxc}$ denote the origin feature map. Let $Y \\in\n$\\mathbb{F}^{h'xw'xc}$ denote the target up-scaled feature map. Assume the precision $p < poly(n), h, h', w, w'c \\leq$\npoly(n), then we can simulate the up-interpolation layer in Definition 4.3 by a size bounded by\npoly(n) and O(1) depth uniform threshold circuit.\nProof. Firstly, we begin to compute every entry in the targeted feature map $Y_{*,j,l}$. For $i \\in [h'], j\\in$\n$[w'], l \\in [c]$, we have\n$Y_{ijl} = \\sum_{s=-1}^{2}\\sum_{t=-1}^{2}W(s)\\cdot X_{i+\\frac{i}{h'}+s,j+\\frac{j}{w'}+t,q}\\cdot W(t)$\nBy using the result of Part 1 of Lemma 3.5, we can apply a constant depth $2d_{std}$ uniform threshold\ncircuit to compute each product $W(s)\\cdot X_{i+\\frac{i}{h'}+s,j+\\frac{j}{w'}+t,q}\\cdot W(t)$. Since the products for different s and t\ncan be parallel computed, the uniform threshold circuit's depth for all products $W(u) \\cdot X_{i+\\frac{i}{h'}+s,j+\\frac{j}{w'}+t,q}$ Xin\nstays $2d_{std}$.\nThen, by using the result of Part 3 in Lemma 3.5, we can use a $d_{\\oplus}$ depth uniform threshold\ncircuit to model the sum operation:\n$\\sum_{s=-1}^{2}\\sum_{t=-1}^{2}W(s)X_{i+\\frac{i}{h'}+s,j+\\frac{j}{w'}+t,q}W(t)$\nHence, the total depth of the circuit required to compute $Y_{i,j,l}$ is $2d_{std} + d_{\\oplus}$. As we can parallel\ncompute $Y_{i,j,q}$, for all $i \\in [h'], j \\in [w'], and l \\in [c]$. So the total depth is $2d_{std} +d_{\\oplus}$. The circuit size\nis poly(n), which is due to $h', w', c \\leq poly(n)$. Therefore, the entire Up-Interpolation process can\nbe performed by a uniform threshold circuit, where its size is bounded by poly(n) and its depth\nremains constant. This concludes the proof."}, {"title": "5.2 Computing Attention Matrix", "content": "Let us begin by recalling that the matrix multiplication of two matrices belongs to $TC^0$.\nLemma 5.2 (Matrix Multiplication belongs to $TC^0$ class, Lemma 4.2 in [CLL+24b]). Assume the\nprecision $p \\leq poly(n), n_1, n_2 \\leq poly(n)$, and $d \\leq n$. Let $A \\in \\mathbb{F}^{n_1\\times d}$ and $B \\in \\mathbb{F}^{d\\times n_2}$. Then we can\napply a DLOGTIME-uniform threshold circuit with constant depth $(d_{std} + d_{\\oplus})$ and size bounded by\npoly(n) to get the matrix product AB."}, {"title": "5.3 Computing Single Attention Layer", "content": "Subsequently, matrix operations can be applied to compute the attention matrix.\nLemma 5.3 (Attention matrix computation belongs to $TC^0$ class). Assume the precision $p <\npoly(n), then we can use a size bounded by poly(n) and constant depth $3(d_{std} + d_{\\oplus}) + d_{exp}$ uniform\nthreshold circuit to compute the attention matrix A defined in Definition 4.4.\nProof. Based on Lemma 5.2, we can compute the matrix product $W_QW_K$ by using a size bounded\nby poly(n) and constant depth $d_{std} + d_{\\oplus}$ uniform threshold circuit.\nThen, we move forward to compute the scalar product, which is\n$S_{i,j} = X_{i,*}\\cdot W_QW_KX_{j,*}^\\top$\nAnd by using the result of Lemma 5.2, we can compute $t_{i,j}$ by applying a uniform threshold circuit,\nwhere the circuit has a polynomial-size bounded by poly(n) and constant depth $2(d_{std} + d_{\\oplus})$.\nIn the next step, from Lemma 3.6, we can compute the exponential function $A_{i,j} = exp(t_{i,j})$ by\napplying a size bounded by poly(n) and constant depth $d_{exp}$ uniform threshold circuit.\nAfter combining depths from all steps, the total depth of the circuit for computing $A_{i,j}$ is\n$d_{total} = 3(d_{std} + d_{\\oplus}) + d_{exp}$.\nSince we can parallel compute all entries in $A_{i,j}$ for $i, j \\in [n]$, the circuit depth remains $3(d_{std} +\nd_{\\oplus}) + d_{exp}$ and size bounded by poly(n).\nThus, we have proven the result."}, {"title": "5.4 Computing Common Components Layers", "content": "This section outlines the MLP layer circuit complexity.\nLemma 5.4 (MLP computation falls within $TC^0$ class, Lemma 4.5 of [CLL+24b]). Assume the\nprecision $p < poly(n)$. Then, we can use a size bounded by poly(n) and constant depth $2d_{std} + d_{\\oplus}$\nuniform threshold circuit to simulate the MLP layer in Definition 4.6.\nNext, we examine the layer-normalization (LN) layer circuit complexity.\nLemma 5.5 (LN computation falls within $TC^0$ class, Lemma 4.6 of [CLL+24b]). Assume the\nprecision $p < poly(n)$, then we can use a size bounded by poly(n) and constant depth $5d_{std} +$\n$2d + d_{sqrt}$ uniform threshold circuit to simulate the Layer-wise Normalization layer defined in\nDefinition 4.7."}, {"title": "5.5 Computing Convolution Blocks", "content": "We prove in this section that the convolution layers can be computed within $TC^0$.\\"}]}