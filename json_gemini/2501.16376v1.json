{"title": "HWPQ: Hessian-free Weight Pruning-Quantization For LLM Compression And\nAcceleration", "authors": ["Yuhan Kang", "Zhongdi Luo", "Mei Wen", "Yang Shi", "Jun He", "Jianchao Yang", "Zeyu Xue", "Jing Feng", "Xinwang Liu"], "abstract": "Large Language Models (LLMs) have achieved re-\nmarkable success across numerous domains. How-\never, the high time complexity of existing prun-\ning and quantization methods significantly hinders\ntheir effective deployment on resource-constrained\nconsumer or edge devices. In this study, we\npropose a novel Hessian-free Weight Pruning-\nQuantization (HWPQ) method. HWPQ eliminates\nthe need for computationally intensive Hessian\nmatrix calculations by introducing a contribution-\nbased weight metric, which evaluates the impor-\ntance of weights without relying on second-order\nderivatives. Additionally, we employ the Exponen-\ntially Weighted Moving Average (EWMA) tech-\nnique to bypass weight sorting, enabling the se-\nlection of weights that contribute most to LLM\naccuracy and further reducing time complexity.\nOur approach is extended to support 2:4 struc-\ntured sparsity pruning, facilitating efficient execu-\ntion on modern hardware accelerators. Experimen-\ntal results demonstrate that HWPQ significantly en-\nhances the compression performance of LLaMA2.\nCompared to state-of-the-art quantization and prun-\ning frameworks, HWPQ achieves average speedups\nof 5.97x (up to 20.75\u00d7) in quantization time and\n12.29x (up to 56.02\u00d7) in pruning time, while\nlargely preserving model accuracy. Furthermore,\nwe observe a 1.50\u00d7 inference speedup compared\nto the baseline.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed an explosive growth in the\ncapabilities of Large Language Models (LLMs). However,\nthis advancement comes at the cost of exponentially grow-\ning model sizes, leading to substantial monetary and energy\ncosts [Zhao et al., 2023]. Consequently, there have been\ngrowing efforts to reduce these costs through model compres-\nsion, with pruning and quantization emerging as the two most\npopular approaches. Pruning removes network weights by\nsetting them to zero, while quantization reduces the precision\nof neural network weights during storage and computation.\nA key finding is that these two widely used methods are not\northogonal and exhibit a convergent trend [van Baalen et al.,"}, {"title": "2 Motivation & Relate Work", "content": "Post-training compression has become a widely used tech-\nnique, initially developed and extensively applied in quan-\ntization research [Banner et al., 2019; Zhao et al., 2019;\nNagel et al., 2020]. Recently, it has been successfully adapted\nto network pruning, demonstrating promising results [Kwon\net al., 2022; Fu et al., 2022; Sun et al., 2023]. Common\nmethods include magnitude-based [Gale et al., 2019], first-\norder [Kurtic et al., 2022], and second-order [Sanh et al.,\n2020] pruning approaches.\nAmong these approaches, Hessian-based methods, which\ncompute the second-order derivatives of weights, have\ndemonstrated exceptional performance in preserving model\naccuracy without requiring retraining. However, current\nmainstream approaches still require hundreds of seconds to\nsparsify a 2.7B model, even on high-performance GPUs. For\na 175B Transformer model, processing time increases lin-\nearly, reaching hundreds of hours. This substantial time\nrequirement significantly delays the deployment of person-\nalized models, highlighting the challenges of scaling post-\ntraining compression to extremely large models. The pri-\nmary bottleneck lies in constructing the Hessian matrix $H =\\frac{\\partial^2 E}{\\partial w_i \\partial w_j}$, which has a time complexity of O(n\u00b3). There-\nfore, this work focuses on developing a novel compression\napproach that bypasses the high computational cost of the\nHessian matrix and its inverse while closely approximating\nits accuracy-preserving performance, achieving a balance be-\ntween runtime efficiency and precision, and enabling scala-\nbility to very large models."}, {"title": "2.2 Related Work", "content": "The most fundamental sparsification approach is magnitude-\nbased pruning, which achieves sparsity by setting the small-\nest weights to zero [Han et al., 2015; Zhu and Gupta, 2017].\nAlthough these methods scale well, they often cause sig-\nnificant performance degradation in LLMs [Frantar and Al-\nistarh, 2023; Harma et al., 2024]. To improve sparsifica-\ntion, researchers proposed the Optimal Brain Surgeon (OBS)\nmethod [Hassibi et al., 1993], which innovatively uses the\ninverse of the Hessian matrix to update unpruned weights,\nthereby compensating for errors caused by weight removal.\nHowever, OBS faces computational bottlenecks in practical\napplications - calculating and storing the inverse Hessian ma-\ntrix is computationally infeasible for models with millions of\nparameters. To address this challenge, recent research has\nproposed two improvement approaches: one approximates\nthe inverse Hessian matrix calculation, such as the Wood-\nFisher method [Singh and Alistarh, 2020]; the other per-\nforms layerwise pruning, known as Optimal Brain Compres-\nsion (OBC) [Frantar and Alistarh, 2022]. While these meth-\nods perform well on medium-scale networks, they struggle\nwith larger language models [Frantar et al., 2022].\nGPTQ [Frantar et al., 2022] addresses this issue by quan-\ntizing weight matrices using a grouping scheme and com-\npensating updates to all yet-unquantized weights in the\nnext column of that group through the Hessian matrix.\nSparseGPT [Frantar and Alistarh, 2023] applies the same\npruning idea and uses unstructured and semi-structured prun-\ning to simplify large language models, while Sparse Expan-\nsion [Sawmya et al., 2024] improves inference efficiency by\ncomputing a separate Hessian matrix for each input cluster\nto allow specialists to specialize, then using the SparseGPT\npruning algorithm with that matrix to prune the expert weight\nmatrices. Wanda [Sun et al., 2024] simplified this idea by\nusing only the diagonal of the Hessian.\nSimultaneously, to achieve tangible speed improvements\nin practical applications, there has been growing recogni-\ntion of the necessity to implement pruning in a structured\nand hardware-compatible manner [Santacroce et al., 2023;\nMa et al., 2023a; Li et al., 2023; Xia et al., 2024]. This ap-\nproach is typically followed by additional training (or fine-\ntuning) to restore any diminished performance. For example,\nthe LLM-pruner [Ma et al., 2023b] eliminates specific con-\nnection structures within LLMs prior to further training. Sim-\nlarly, the Large Language Model Surgeon [van der Ouderaa\net al., 2024] interleaves recovery fine-tuning with pruning."}, {"title": "3 The HWPQ Method", "content": "Our objective is to identify weights that make minimal con-\ntributions to the loss function, such that their removal would\nnot substantially affect the model's output. In this regard,\nour main focus lies in analyzing the relative importance of\ndifferent weights rather than their absolute values, an aspect\nthat has been largely neglected in previous research. Previous\nstudies have quantified the influence of individual weights on\nthe variation of E by precisely computing their contributions\nthrough the Hessian matrix. The supplementary term in the\nloss function is expressed as follows:\n$L_q = \\frac{1}{2} \\frac{w_q^2}{H_{qq}^{-1}}$\nWhen applied to quantization, the expression becomes:\n$Lq = \\frac{1}{2} \\frac{(w_q \u2013 quant(w_q))^2}{H_{qq}^{-1}}$\nA crucial issue arises from the fact that the matrix (2XXT)\nis not positive definite, as its determinant is zero, meaning it\ndoes not possess an inverse. To address this, we introduce a\nsmall perturbation term, denoted as:\n$H = 2XX + \\sum diag(2XXT)I$\nWhere I represents the identity matrix. This ensures that\nmatrix operations can be performed safely. When using Py-\nTorch, numerical methods are used for matrix computation,\nand due to errors in floating-point calculations, 2XXT can\nresult in matrices with extremely large values, leading to\ninstability. By incorporating these small perturbations, we\nachieve stability in numerical computations with almost zero\noverhead.\nHowever, computing Aw and L for every weight can be\ncomputationally expensive. The time complexity of quanti-\nzation primarily lies in computing the inverse matrix H-1,\nwhich typically has a complexity of O(n\u00b3). Even with the\ncapability to compute Hessian matrices for each row in paral-\nlel, the total time complexity remains at $O(n^3)+O((\\frac{n}{m})^3) =$\nO(n\u00b3).\nTo reduce the overall time complexity, the key is to avoid\nthe computation of H and H\u22121. Our goal is not to obtain\nthe exact value of L for each weight at this stage, but rather\nto construct a numerically stable sequence as contribution-\noriented weight metrics and to derive numerical characteris-\ntics among a series of L values (such as magnitudes, variance,\nand averages).\nDenoting $\\frac{1}{n}\\sum x_i^2$ as S, in Formula 3 that we constructed,\n$H_{qq} = 2(x_q^2 + \\frac{2S}{n})$. Noticed that $H_{qq}$ is independent of\n$x_q$, $H_{qq}$ can actually be written as $det (2X_oX_o +\\frac{2S}{n}I)$\nwhere Xo is the original X without the qth element. Since\n$H_{qq} = \\frac{det(H)}{let(H)_{qq}}$, and\n$H^{-1} = \\frac{\\frac{2S}{n}}{2(2)^n-1(\\frac{2S}{n} + S)}$"}, {"title": "3.1 Contribution-Oriented Weight Metrics", "content": "$H_{qq}^{-1} = \\frac{\\frac{2S}{n} + S - x_q^2}{2(\\frac{2S}{n})^n(\\frac{2S}{n} + S)}$\nThus, we can express $H^{-1}_{qq}$ as:\n$H^{-1}_{qq} = \\frac{S}{2S(\\frac{2S}{n} + S)} + \\frac{S - x_q^2}{2(\\frac{2S}{n})^n(\\frac{2S}{n} + S)}$\nThen, we can simplify further:\n$H^{-1}_{qq} = \\frac{nS + n^2(S - x_q^2)}{2S(S + nS)} = \\frac{S - x_q^2}{2(1+n) S - x^2} + \\frac{S}{2S(1 + n)}$\nIn LLMs, n is sufficiently large(e.g., 4096 in LLaMA2-\n7B), ensuring S >> x\u00b2. Therefore, we can approximate S \u2013\nx\u00b2~ S, leading to:\n$H^{-1}_{qq} = \\frac{1 - x^2/S}{\\frac{n^2 + n}{2S(1 + n)}} = C$\nNow we observe that $\\frac{H^{-1}_{qq}}{1 - x^2/S}$ approaches a constant.\nSince we are concerned with the comparative magnitudes of\nvalues rather than the exact value of each L, we replace H\u00b9\nwith $\\frac{1 - x^2/S}{2S(1 + n)}$ to avoid computations involving the Hessian\nmatrix. Thus, we compute L as follows:\n$L = \\frac{\\frac{1}{2} w_q^2}{1 - x^2/S}$\nWhere S represents the sum of all $x_i^2$ for every $x_i$ in X.\nIn this formulation, the Hessian matrix is no longer needed.\nTo determine which weights should be removed, we can sim-\nply sort the L values of all weights and eliminate those with\nthe smallest L values. As we demonstrated earlier, smaller\nL values indicate that the removal of those weights will have\na minor effect on the loss function. The time complexity of\ncomputing all L values is O(n), while the cost of the most\ncommon sorting algorithms is O(nlogn), thus reducing the\noverall time complexity to O(nlogn). It is worth noting\nthat we perform this operation simultaneously across differ-\nent rows, where n represents the number of weights in a row."}, {"title": "3.2 EWMA Adaption", "content": "To further reduce the time complexity, our next objective is to\nfind an alternative method to replace sorting, allowing us to\nassess where a particular L value stands among all L values.\nThe Exponentially Weighted Moving Average (EWMA) is\na technique used for estimating the mean and variance of a\nsequence of data points. In the context of Transmission Con-\ntrol Protocol(TCP), it is employed to estimate the round-trip\ntime (RTT) of a connection. [Paxson et al., 2011].\nIn the practical implementation of TCP, the EWMA\nmethod exhibits strong adaptability by dynamically estimat-\ning the mean and L1-mean norm error of the recent RTT over\ntime. We apply this method to evaluate L. For each row, we\ntreat the weights as a sequential list.\nFirst, after calculating S as outlined in Step 1 of Figure 2\n(Algorithm 1, line 1), we initialize a tensor state for each\nweight in a row. This tensor state consists of the follow-\ning components: the dynamically updated S, the estimated\nmean (denoted as est), and the L1 mean norm error (denoted\nas dev). Subsequently, following Step 2 of Figure 2 (Algo-\nrithm 1, line 4), we sequentially compute a series of Li val-\nues. If $L_i < est - la \\times dev$ (Al-\ngorithm 1, line 8), we consider its contribution to the loss\nfunction to be minimal and prune it; otherwise, we symmetri-\ncally quantize it to FP8 format, as shown in Step 3 of Figure 2\n(Algorithm 1, lines 9 and 12).\nNext, we update the tensor state according to the proce-\ndure outlined in Table 1, as illustrated in Step 4 of Figure 2\n(Algorithm 1, lines 10, 13, 16, and 17), until all weights in\nthe row are compressed. Throughout this process, the over-\nall time complexity is reduced to O(n), demonstrating that\nwe can evaluate the contribution of each weight to the loss\nfunction and quantize the model to sparse FP8 in linear time.\nThe Full Algorithm. Finally, we present the full pseu-\ndocode for HWPQ in Algorithm 1, including the optimiza-\ntions discussed above."}, {"title": "3.3 2:4 Sparsification", "content": "To achieve efficient computation of structured sparse matri-\nces on dedicated accelerators [Tang et al., 2022; Liu et al.,\n2023], our pruning method supports hardware-friendly struc-\ntured sparsity. In implementation, we adopt fine-grained se-\nlection to support the 2:4 structured sparsity pattern. By lever-\naging Tensor Cores' native support for this pattern, we parti-\ntion each row of weights into groups of four and identify the"}, {"title": "4 Intergration of HWPQ into Taichi\nframework", "content": "HWPQ can significantly reduce the size of LLMs.\nHowever, during inference, many frameworks acquire de-\nquantized weights by multiplying a scaling factor (floating-\npoint) with quantized integers [Lin et al., 2023; Lee et al.,\n2023]. This process inevitably leads to extra time and mem-\nory overhead.\nTo solve this problem, we improved the operator on the\nTaichi framework. During inference procedure, the input acti-\nvation is quantized into FP8. In the Attention mechanism, the\nlinear layers generating the query, key, and value, as well as\nthe MLP, are replaced with FP8 precision, while other compo-\nnents retain FP16 precision. Within Tensor Cores, FP8 matrix\noperations and accumulations are performed with FP16 bit-\nwidth. The computations following the linear layers, often\nRoot Mean Square Normalization (RMSNorm), are not linear\nlayers, and FP16 can conveniently utilize existing operators.\nFinally, when the resultant output is fed into the subsequent\nlinear layer, we re-quantize it to FP8.\nSince the numerical format we use for quantization is fully\nconsistent with the one used in hardware computations, we\ncan directly utilize the quantized weights. In other words, we\ncan deliver the weights directly to GPUs without transform-\ning them again. This avoids redundant dequantization and"}, {"title": "5 Experiments", "content": "Models. We evaluate two model families: Pythia [Bi-\nderman et al., 2023] and LLaMA(including LLaMA2 and\nLLaMA3 [Dubey et al., 2024]). Pythia is a collection of mod-\nels focused on LLM interpretability, developed as a variant of\nGPT-NeoX. LLaMA represents a series of open-source pre-\ntrained models, with LLaMA3 being the latest iteration.\nDatasets. For all pruning and quantization experiments,\nwe utilize evaluation with a zero-shot perplexity (PPL) anal-\nsis on WikiText2 [Merity et al., 2016]. To assess the per-\nformance of the model in the task-agnostic setting, we fol-\nlow LLaMA's evaluation to perform zero-shot task classifi-\ncation on the OpenCompass [Contributors, 2023] and Lm-\nevaluation-harness [Gao et al., 2024] benchmarks. These\nbenchmarks offer a comprehensive assessment for LLMs.\nThe datasets encompassed in this assessment are as follows:\nARC(Easy and Challenge) [Boratko et al., 2018], Wino-\nGrande [Sakaguchi et al., 2021], PIQA [Bisk et al., 2020],\nHellaSwag [Zellers et al., 2019] and OpenbookQA [Mi-\nhaylov et al., 2018].\nPlatforms. We carried out our experiments on RTX 4090\nGPUs. Given that the Tensor Cores of the RTX 4090 sup-\nport FP8 computations, our goal is to demonstrate the com-\nputational benefits of leveraging the FP8 format. The spe-\ncific experimental environment includes 2\u00d7 Intel(R) Xeon(R)\nPlatinum 8358 CPUs @ 2.60GHz, 8\u00d7 RTX 4090 GPUs with\n24GB each, GCC 7.5.0, NVIDIA CUDA release 12.1, and\nPython 3.11.5 with Anaconda 23.9.0. We utilized PyTorch\nversion 2.3.0.dev20240220+cu121, incorporating float8 sup-\nport to take advantage of FP8 (E5M2) computing via Tensor\nCores on the RTX 4090.\nTarget Scenarios. Our focus is on AI-driven personal\ncomputers (PCs), as LLMs demand substantial computational\nresources even during inference. The RTX series, being a\nconsumer-level GPU, is a common accelerator for PCs, and\nwe hope our work will enhance the performance of com-\npressed LLM inference on these platforms. All of our exper-\niments were conducted on RTXs with the Ada Architecture,\nutilizing Tensor Cores that support FP8 GEMM. Given the\ngeneric nature of our method, which compresses the weights\nof the model without altering its computational pattern dur-\ning inference, it can be applied to other scenarios (such as"}, {"title": "5.2 Evaluation of HWPQ Algorithm", "content": "Efficiency: The HWPQ algorithm provides a significant\nspeedup. Our performance improvements stem from two pri-\nmary factors. First, we have made an algorithmic advance-\nment. The O(n) algorithm offers remarkable scalability for\npruning and quantization LLMs, allowing us to efficiently\nevaluate the importance of each weight without substantially\nincreasing time consumption as the model size grows. Sec-\nond, we have developed customized GPU operators using\nTaichi. Because the parameter matrix's rows are independent,\nwe fully exploit row-wise vector parallelism to enhance prun-\ning and quantization efficiency on GPUs.\nOur experiments systematically demonstrate that com-\npared with the state-of-the-art quantization methods such as\nAutoGPTQ, AutoAWQ, and SpQR (detailed in Figure 3), the\naverage speedup reaches 4.88\u00d7 (up to 10.49\u00d7), 2.82\u00d7 (up\nto 4.23\u00d7), and 10.21\u00d7 (up to 20.75\u00d7), respectively. When\ncompared to pruning methods like SparseGPT and Wanda\n(detailed in Table 2), the average speedups reach 43.75\u00d7 and\n12.29x, respectively. The primary overhead in our approach\nstems from the just-in-time (JIT) compilation of kernel func-\ntions, which introduces a cold-start delay occurring only dur-\ning the initial pruning-quantization phase. This characteris-\ntic makes our algorithm particularly advantageous for LLMs,\nwhere the continuous growth in model size necessitates more\nefficient methodologies for assessing weight contributions to\nfinal outputs. The HWPQ method, with its O(n) pruning-\nquantization time complexity, effectively addresses this chal-\nlenge, offering a scalable solution for modern LLMs.\nAccuracy: Zero-shot performance in LLaMA2-7B. We\nconducted comprehensive fine-grained pruning experiments\non the LLaMA2-7B model, systematically evaluating prun-\ning ratios ranging from 10% to 50%, including structured 2:4\nstructured pruning configurations. Model performance was\nrigorously assessed using the Lm-evaluation-harness frame-\nwork. As detailed in Table 2, our experimental results demon-\nstrate that a model with a 20% pruning ratio successfully\nmaintains 99.4% of the baseline model's performance with-\nout requiring any post-training. Remarkably, even at a 50%\npruning ratio, the model retains 91.57% of its original perfor-\nmance. A comprehensive comparative analysis demonstrates\nthat although our method may exhibit slightly inferior per-\nformance compared to well-established approaches such as\nWanda and SparseGPT (which rely on computationally inten-"}, {"title": "5.3 Evaluation of 2:4 sparsification", "content": "Tuning la for Sparsity Control: Adjusting the parame-\nter la yields varying levels of sparsity. As demonstrated\nin Table 1, weight sparsity-measured by the percentage\nof zero weights increases as la decreases. Notably, set-\nting la = 0.5 achieves a global sparsity of approximately\n50%. This enables an additional iteration over the remaining\nweights, producing structured sparsity in a 2:4 pattern. For\nlibraries supporting FP8 2:4 structured sparsity, this pattern\ninherently doubles the throughput of matrix multiplication on\nTensor Cores, delivering genuine performance acceleration\nwhile preserving computational efficiency [Tang et al., 2022;\nXia et al., 2024].\nThe 2:4 sparsification algorithm excels due to its fine-\ngrained sparse format, where every group of four weights\nhas two consistently eliminated. Implementing this operator\nrequires adding basic branching functionality to the original\nalgorithm, involving up to six pairwise comparisons to de-\ntermine which weights to remove. Although this introduces\na constant factor to the computational complexity, the overall\ncomplexity remains O(n), with only minimal additional over-\nhead. From the experimental results presented in Table 2, it is\nevident that the 2:4 sparsity pattern achieves not only a faster\ncompression speedup ratio but also higher model evaluation\naccuracy compared to other methods."}, {"title": "5.4 Evaluation of FP8 transformers", "content": "Accuracy: Minimal impact when substituting linear lay-\ners with FP8 precision. In Figure 5, we present a compar-\native analysis of the evaluation scores for the LLaMA2-7B\nmodel, utilizing FP8 linear layers at varying sparsity levels,\nagainst the scores achieved by the original FP16 model. Our\nfindings indicate that the evaluation scores between the FP8\nand FP16 precision models are generally comparable. This\nsimilarity in performance underscores the potential for infer-\nence acceleration through the adoption of FP8 precision in\nlinear layers. Our approach demonstrates not only consistent"}, {"title": "6 Conclusion", "content": "In this paper, we propose Hessian-free Weight Pruning-\nQuantization method, a hardware-friendly approach for low-\nbit weight-only quantization of LLMs. The core innovation\nof our study is the development of a novel Hessian-free LLM\npruning and quantization method, which significantly reduces\ntime complexity from O(n\u00b3) to O(n) compared to main-\nstream algorithms. This theoretical breakthrough ensures that\nour method consistently outperforms existing approaches in"}]}