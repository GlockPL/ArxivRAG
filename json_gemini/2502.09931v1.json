{"title": "TransGUNet: Transformer Meets Graph-based Skip Connection for Medical Image Segmentation", "authors": ["Ju-Hyeon Nam", "Nur Suriza Syazwany", "Sang-Chul Lee"], "abstract": "Skip connection engineering is primarily employed to address the semantic gap between the encoder and decoder, while also integrating global dependencies to understand the relationships among complex anatomical structures in medical image segmentation. Although several models have proposed transformer-based approaches to incorporate global dependencies within skip connections, they often face limitations in capturing detailed local features with high computational complexity. In contrast, graph neural networks (GNNs) exploit graph structures to effectively capture local and global features. Leveraging these properties, we introduce an attentional cross-scale graph neural network (ACS-GNN), which enhances the skip connection framework by converting cross-scale feature maps into a graph structure and capturing complex anatomical structures through node attention. Additionally, we observed that deep learning models often produce uninformative feature maps, which degrades the quality of spatial attention maps. To address this problem, we integrated entropy-driven feature selection (EFS) with spatial attention, calculating an entropy score for each channel and filtering out high-entropy feature maps. Our innovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial attention to effectively enhance domain generalizability across various modalities by leveraging GNNs alongside a reliable spatial attention map, ensuring more robust features within the skip connection. Through comprehensive experiments and analysis, TransGUNet achieved superior segmentation performance on six seen and eight unseen datasets, demonstrating significantly higher efficiency compared to previous methods.", "sections": [{"title": "1. Introduction", "content": "Medical image segmentation is crucial for the early detection of abnormal tissues and the development of treatment plans [9]. Traditional segmentation algorithms have received considerable attention from medical experts [23, 32, 47, 57]. However, these methods still lack generalizability owing to the severe noise, inhomogeneous intensity distribution, and various clinical settings in medical images [51]. Consequently, this issue has raised concerns about the reliability of computer-based diagnostic procedures [18].\nRecently, convolutional neural networks (CNNs) have been widely employed for medical image segmentation owing to their robustness in capturing local and spatial hierarchical features [44, 52, 77]. However, CNN-based models struggle to capture the global dependencies necessary to understand the complex anatomical structures in medical images [24]. This limitation has expanded the use of transformers to extract global dependencies for medical image segmentation [5, 7]. However, despite their strengths, transformer-based models often fail to bridge the semantic gap between the encoder and decoder, hindering their ability to fully leverage global dependencies and resulting in sub-optimal segmentation performance [64].\nSeveral models have been actively employed to improve skip connections to reduce this semantic gap for medical image segmentation. The most representative of these attempts is UNet++ [79], which uses cross-scale feature fusion through dense connectivity in skip connection. Similarly, UCTransNet [64] and CFATransUNet [63] adopted a transformer-based approach to capture local cross-channel interactions of feature maps from a channel-wise perspective. However, these models suffer from increased computational complexity and ambiguous attention owing to their dense connectivity, extensive use of transformer blocks, and complex background in medical images with severe noise. Consequently, addressing the question, \"How can we efficiently leverage global dependency without ambiguity while reducing the semantic gap between the encoder and decoder?\" is critical to overcoming these challenges and improving performance for medical image segmentation.\nTo answer this question, we focused on graph neural networks (GNNs), which are particularly suitable for flexibly and effectively capturing local and global dependencies, making them ideal for complex visual perception tasks [21]. By leveraging this capability, we propose an attentional cross-scale GNN (ACS-GNN) that can efficiently reduce the semantic gap between the encoder and decoder. It transforms cross-scale feature maps into graphs and applies attention to each node to facilitate robust feature integration. Additionally, we observed that deep learning models often produce uninformative feature maps that degrade the quality of spatial attention maps [8, 53]. To address this issue, we introduce an entropy-driven feature selection (EFS), which calculates entropy per channel and filters out high-entropy channels. By integrating ACS-GNN and EFS-based spatial attention, we designed a new medical image segmentation model called TransGUNet which effectively captures the relationships between patches, regardless of the lesion size and distance between patches (Fig. 1). Extensive experimental results demonstrate that our graph-based approach consistently outperforms transformer- and convolution-based methods. Thus, TransGUNet represents a significant advancement in skip connection frameworks for medical image segmentation and offers a robust and efficient solution to the existing challenges. The main contributions of this study can be summarized as follows:\n\u2022 We propose TransGUNet, a novel medical image segmentation model that leverages the cross-scale GNN-based skip connection framework without ambiguous spatial attention and is applicable to various modalities and clinical settings. To the best of our knowledge, our novel skip connection framework is the first study to successfully and effectively exploit attentional cross-scale GNN with non-ambiguous spatial attention for medical image segmentation.\n\u2022 The proposed attentional cross-scale graph neural network (ACS-GNN) allows the model to comprehend the complex anatomical structures within medical images. Additionally, we incorporated entropy-driven feature selection (EFS) with spatial attention to produce more reliable spatial attention maps.\n\u2022 Our experimental results demonstrate that TransGUNet significantly outperforms transformer- and convolution-based approaches employed for medical image segmentation with various clinical settings."}, {"title": "2. Related Works", "content": "Skip Connection Engineering for Medical Image Segmentation. The introduction of skip connections in UNet marked a significant milestone and made it the most widely used baseline model in medical image segmentation. However, there is still a semantic gap between the encoder and decoder, which results in suboptimal performance [38]. This problem has driven recent efforts to refine skip connections to minimize this semantic gap. UNet++ is a representative model incorporating dense connectivity and neighbor scale features in skip connections. Additionally, MSNet [77] and M2SNet [78] reduce redundant features using subtraction modules to design more efficient models. Recently, transformers have been employed as skip connection modules to capture the global dependencies in medical images [17, 26]. Notably, UCTransNet [64], FCT [58], and CFATransUNet [63] maintain global dependency by leveraging transformer-based skip connection frameworks. However, these models have complex architectures with more than 60M parameters, which can be computationally expensive and inefficient. Our innovative TransGUNet addresses these challenges by utilizing cross-scale GNN with node attention and reducing the semantic gap with a significantly more efficient architecture comprising 25M parameters. We compare the schemes and properties of various skip connection"}, {"title": "3. Method", "content": "We used a Pyramid Pooling Transformer (P2T) [70] comprising multiple pooling-based multi-head self-attention. P2T incurs a significantly lower computational cost and higher representation power than Vision Transformer (ViT) and Pyramid Vision Transformer (PVT), which are adopted in various medical image segmentation models [10, 37, 76]. Inspired by previous studies, we utilized the same encoder architecture as the decoder to fully leverage global dependency. Although we primarily present the experimental results using P2T, we also provide various CNN and transformer backbones to demonstrate the versatility and robustness."}, {"title": "3.1. Encoder and Decoder in TransGUNet", "content": "Motivation: The human visual system (HVS) recognizes objects by dividing them into large parts and understanding them based on the connectivity strengths of each part [39]. This process helps interpret complex scenes by identifying relationships between different parts of an image, leading to a holistic understanding of objects and their interactions [41, 48]. Inspired by these principles, our approach employs a similar strategy of the HVS by transforming the cross-scale feature map into graphs to understand the complex anatomical structures in a high-dimensional feature space. However, significant noise and complex backgrounds create highly ambiguous visual signals that disturb the neural systems. The HVS mitigates this issue through signal filtering and attention processing [49, 59]. Therefore, we propose an entropy-based feature selection strategy that mimics these feature filtering and attention processes, called EFS-based spatial attention. The integration of these components enhances the preservation of global dependencies and the local details without ambiguity in attention mechanism. The overall architecture of the TransGUNet is illustrated in Fig. 2. The ACS-GNN with EFS-based spatial attention can be divided into four steps: 1) Feature Preprocessing, 2) ACS-GNN (Fig. 2 (c)), 3) EFS-based spatial attention (Fig. 3), and 4) Feature Postprocessing.\nFeature Preprocessing. Let \\(f_i \\in \\mathbb{R}^{C_i \\times H \\times W}\\) be the feature maps from the i-th encoder stage for \\(i = 1,2,3,4\\) where (H, W) is the resolution of the input image. Because the number of channels in each stage primarily affects the complexity of the decoder, we employed a 2D convolution with a kernel size of 1 \u00d7 1 to reduce the number of channels to \\(C_r\\). To obtain the cross-scale feature map \\(f\\), we resized them to the same resolution for \\(i = 1, 2, 3, 4\\) as follows:\n\\[f^i = \\text{Resize}(H_t, W_t)(\\text{C2D}_{1\\times1}(f_i)) \\in \\mathbb{R}^{C_r \\times H_t \\times W_t}\\) (1)\nwhere \\(\\text{C2D}_{k\\timesk}(\\cdot)\\), and \\(\\text{Resize}_{H_t,W_t}(\\cdot)\\) denote the 2D convolution with a kernel size of k \u00d7 k, and an operation to resize into the target spatial resolution \\((H_t, W_t)\\), respectively. If the target and original resolutions of the input feature map differ, bilinear interpolation is used to upsample or downsample feature map to match the target resolution. Alternatively, if they have the same resolution, we do not apply resize, denoted as \u201cResolution Fix\u201d in Fig. 2 (b). And then, we concatenate each resized feature map \\([f^1, f^2, f^3, f^4] = f \\in \\mathbb{R}^{4C_r \\times H_t \\times W_t}\\) where [...] denotes concatenation between feature maps along the channel dimension. For convenience, we assume that \\(C = 4C_r\\).\nAttentional Cross-Scale Graph Neural Network. After obtaining the cross-scale feature \\(f \\in \\mathbb{R}^{C \\times H_t \\times W_t}\\), we apply"}, {"title": "3.2. ACS-GNN with EFS-based spatial attention for Skip Connection", "content": "2D convolution with a kernel size of 1\u00d71 and batch normalization (BN). Subsequently, the feature map is converted into a flattened vector \\(x \\in \\mathbb{R}^{C \\times N}\\), where \\(N = H_t\\cdot W_t\\). Each pixel in \\(x\\) acts as a node in the graph. Additionally, a relative positional vector is added to each flattened vector element to preserve the position information. Next, we constructed the feature graph using the dilated K-nearest neighbors (KNN) algorithm. To implement the exchange of information between nodes, we adopted the Max-Relative graph convolution (MRConv) [34] owing to its simplicity and efficiency as it does not require learnable parameters for node aggregation. The MRConv and Update processes are implemented for graph convolution as \\(x_G = G(x)\\). To improve feature aggregation by adaptively weighting the node importance, we applied node attention to prioritize critical features while learning their relevance. Based on ECANet [65], we designed a node attention mechanism using a single 1D convolution operation with kernel size of k and sigmoid function. Firstly, \\(x_G\\) is compressed into \\(z_{\\text{avg}}\\) and \\(z_{\\text{max}}\\) using Global Average Pooling and Global Max Pooling, respectively, and then each statistic is aggregated to produce a node attention map. Finally, such an attention mechanism is readily implemented as follows:\n\\[\\overline{x} = x \\times \\sigma(\\sum_{d \\in \\{\\text{avg,max}\\} }\\text{C1D}_k(z_d))\\label{key} (2)\nwhere \\(\\text{C1D}_k(\\cdot)\\) and \\(\\sigma(\\cdot)\\) denote 1D convolution with a kernel size of k and the sigmoid function, respectively. And, we reshape flattened vector \\(\\overline{x}\\) into original feature map shape and apply 2D convolution with a kernel size of 1 \u00d7 1 and BN for more nonlinearity to obtain the refined feature map \\(X \\in \\mathbb{R}^{C \\times H_t \\times W_t}\\). To address the oversmoothing problem [35, 46] in GNN, we also utilize Feed-Forward Networks with two consecutive convolutional layers and residual connections as follows:\n\\[\\label{eq:2}f = \\text{BN}(\\text{C2D}_{1\\times1}(\\delta(\\text{BN}(\\text{C2D}_{1\\times1}(X)))) + X (3)\nwhere \\(\\delta(\\cdot)\\) and BN(\u00b7) denote the ReLU activation function for non-linearity and batch normalization, respectively.\nSpatial Attention with Entropy-driven Feature Selection. Although a GNN can maintain global dependencies,"}, {"title": "Spatial Attention with Entropy-driven Feature Selection", "content": "various noise and complex backgrounds in medical images still result in an uninformative feature map containing a high-entropy score, leading to a poor spatial attention map [8, 53]. To address this issue, we propose entropy-driven feature selection (EFS)-based spatial attention, which illustrated in Fig. 3, that filters uninformative feature maps and generates a more reliable spatial attention map. We observed that the channel with a high entropy is uniformly or noisily activated as shown in Fig. 4. Therefore, inspired by previous studies [61, 66], we calculated the entropy score \\(E \\in \\mathbb{R}^C\\) at the pixel level for each channel of the input feature map using Shannon Entropy as follows:\n\\[E = -\\frac{1}{HW} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (\\sigma(f^c)_{h,w} \\cdot \\log(\\sigma(f^c)_{h,w})) (4)\nSubsequently, we only retained the feature map channels with the M lowest entropy scores and used them for spatial attention as follows:\n\\[\\label{eq:2}f_G = \\mathbb{X} \\times (\\text{C2D}_{1\\times1}((\\mathbb{f})_{\\text{sorting}(E)[:M]})) (5)\nwhere sorting(\u00b7) denotes the sorting algorithm into ascending. Hence, sorting \\((E)[:M]\\) means the Bottom-M index of feature map \\(f\\) with low entropy score. We used the Introselect algorithm, the default sorting algorithm in PyTorch. This dual process of ACS-GNN and EFS-based spatial attention allows our model to comprehend the intricate anatomical structures in medical images and generate more reliable spatial attention maps.\nFeature Postprocessing. We first divided \\(f_G\\) along the channel dimension with an equal number of channels \\(C_r\\). Subsequently, each feature map was resized and applied residual connection between the original feature map to enhance the training stability for \\(i = 1, 2, 3, 4\\) as follows:\n\\[ f_G^{i} = f_i + \\text{Resize}(\\frac{H_i}{H_t},\\frac{W_i}{W_t}) ((f_G)^{C_r(i-1): C_ri}) (6)\nFor convenience, let \\(\\mathbb{D}_1 = f_G\\). Then, we produce the decoder block output \\(\\mathbb{D}_{i+1}\\) for \\(i = 1,2,3\\) as\n\\[ \\mathbb{D}_{i+1} = \\text{Decoder}_i([\\mathbb{f}^i , \\text{Up}^2(\\mathbb{D}_i)]) \\label{eq:7}(7)\nwhere \\(\\text{Up}^2()\\) and \\(\\text{Decoder}_i()\\) denote a bilinear upsampling with scale factor 2n-1 and i-th transformer decoder block, respectively. We summarized the technical novelty and impact of TransGUNet in the Appendix."}, {"title": "3.3. Training Procedure", "content": "We employed a multi-task learning with deep supervision to enhance the representation power of the model and mitigate the gradient vanishing problem. To implement this, we obtained four predictions \\(O_i\\) from \\(D_i\\) for \\(i = 1,2,3,4\\) by applying 2D convolution with kernel size of 1 \u00d7 1 following sigmoid function and upsampling at each stage. We denote \\(O_i = {R_i, B_i}\\) as the multiple outputs representing region \\(R_i\\) and boundary \\(B_i\\) predictions, respectively. The total loss function is defined as:\n\\[L_{total} = \\sum_{i=1}^{4} (L_R(R_t, R_i) + L_B(B_t, B_i)) (7)\nwhere \\(R_t\\) and \\(B_t\\) denote the ground truths of region and the boundary, respectively. We obtained \\(B_t\\) by applying an anisotropic Sobel edge detection filter [31] to \\(R_t\\). The loss function for region prediction was defined as \\(L_R = L_{IoU} + L_{bce}\\), where \\(L_{IoU}\\) and \\(L_{bce}\\) are the weighted IoU and binary cross entropy (BCE) loss functions, respectively. This loss function is identically defined in previous studies [15, 45, 78]. Additionally, we defined boundary loss function \\(L_B\\) as the BCE loss function."}, {"title": "4. Experiment Results", "content": "Each model was trained and evaluated on five medical segmentation tasks, including multi-organ, skin cancer, COVID-19 infection, breast tumors, and polyp. For convenience, we denote the seen clinical settings (Tab. 2) as the test dataset, which has the same distribution with the training dataset. Moreover, we additionally evaluate the domain generalizability of each model on eight external segmentation datasets using different distributions for the training and test datasets, which are referred to as unseen clinical settings (Tab. 3). Due to the page limit, we present the detailed dataset description and split information in the Appendix (Tab. 6 and 7). To evaluate the performance of each model, we selected two metrics, the Dice Score Coefficient (DSC) and mean Intersection over Union (mIoU), which are widely used in medical image segmentation. Additionally, the quantitative results with more various metrics are also available in the Appendix."}, {"title": "4.1. Experiment Settings", "content": "We implemented TransGUNet on a single NVIDIA RTX 3090 Ti in Pytorch 1.8.\nMulti-Organ Segmentation. Following the previous literature [63], we employ an Adam optimizer with a learning rate of 0.001 for multi-organ segmentation. We optimize each model using a batch size of 24 and train them for 150 epochs. During training, we used flipping with a probability of 50% and rotation between -20\u00b0 and 20\u00b0. Because volumes in seen and unseen clinical settings have different resolutions, all images were resized to 224 \u00d7 224. We would like to clarify that we used identical settings in CFATUNet, which is the most recent multi-organ segmentation model. Note that we utilize the same settings on the multi-organ segmentation task to train all models for fair comparison.\nBinary Segmentation. We started with an initial learning rate of 10-4 using the Adam optimizer and reduced the parameters of each model to 10-6 using a cosine annealing learning rate scheduler. We optimized each model using a batch size of 16 and trained them for 50, 100, 100, and 200 epochs on polyp, skin cancer, breast tumors, and COVID-19 infection segmentation tasks. In the training process, horizontal and vertical flips were applied with a 50% probability, along with rotations ranging from -5\u00b0 to 5\u00b0, as part of a multi-scale training strategy. This approach is commonly utilized in medical image segmentation models [15, 45, 77, 78]. Because images in each dataset have different resolutions, all images were resized to 352 \u00d7 352. Additionally, we would like to clarify that we used identical settings in M2SNet and MADGNet, which are the most representative medical image segmentation methods.\nHyperparameters of TransGUNet. Key hyperparameters for TransGUNet on all datasets were set to \\(C_r = 64\\) for efficiency and \\((H_t, W_t) = (\\frac{H}{8}, \\frac{W}{8})\\), \\(K = 11\\), \\(k = 3\\) in ACS-GNN and M = 64 in EFS-based spatial attention. In the Appendix, we provide the experiment results on various hyperparameter settings (Tab. 9, 5, 10, 11)."}, {"title": "4.3. Comparison with State-of-the-art models", "content": "We used the same model as that in Tab. 2 to evaluate the domain generalizability for unseen clinical settings Tab."}, {"title": "5. Conclusion", "content": "In this study, we proposed TransGUNet, an innovative medical image segmentation model that integrates ACS-GNN and EFS-based spatial attention. The proposed model effectively addresses the limitations of the existing models by reducing the semantic gap between the encoder and decoder, preserving crucial information without ambiguous spatial attention, and leveraging global dependencies. Through extensive experiments on six seen and eight unseen datasets, TransGUNet demonstrated superior performance and higher efficiency than the state-of-the-art models. The results of ablation study demonstrate that incorporating GNNs into skip connection engineering significantly enhances the model's ability to capture and utilize complex anatomical features. Additionally, EFS ensures that only the most informative features are considered, thereby improving the quality of spatial attention maps. Consequently, TransGUNet represents a significant advancement in medical image segmentation, offering a robust, efficient, and accurate model that can be employed in various medical applications. In future work, we will focus on optimizing the memory efficiency and explore its deployment in real-world healthcare settings."}]}