{"title": "Offline RLHF Methods Need More Accurate Supervision Signals", "authors": ["Shiqi Wang", "Zhengze Zhang", "Rui Zhao", "Fei Tan", "Cam Tu Nguyen"], "abstract": "With the rapid advances in Large Language Models (LLMs), aligning LLMs with human preferences become increasingly important. Although Reinforcement Learning with Human Feedback (RLHF) proves effective, it is complicated and highly resource-intensive. As such, offline RLHF has been introduced as an alternative solution, which directly optimizes LLMs with ranking losses on a fixed preference dataset. Current offline RLHF only captures the \"ordinal relationship\" between responses, overlooking the crucial aspect of \"how much\" one is preferred over the others. To address this issue, we propose a simple yet effective solution called Reward Difference Optimization, shorted as RDO. Specifically, we introduce reward difference coefficients to reweigh sample pairs in offline RLHF. We then develop a difference model involving rich interactions between a pair of responses for predicting these difference coefficients. Experiments with 7B LLMs on the HH and TL;DR datasets substantiate the effectiveness of our method in both automatic metrics and human evaluation, thereby highlighting its potential for aligning LLMs with human intent and values.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently emerged as a major milestone in modern natural language processing (NLP), offering unprecedented capabilities in understanding, generating, and translating human language (Ouyang et al., 2022; Touvron et al., 2023; Bai et al., 2023; Achiam et al., 2023a; Taori et al., 2023; Chiang et al., 2023; Lu et al., 2023). Powered by an extremely large number of parameters, LLMs encode the wealth of human knowledge via a pretraining process, which is often conducted on a web-scale text corpus with the next token prediction objective. As LLMs become more capable, it is essential and demanding for them to follow human preferences such as truthfulness, harmlessness, and helpfulness. Unfortunately, the maximum likelihood objective for the next token prediction falls short in capturing such crucial human values (Stiennon et al., 2020).\nReinforcement Learning with Human Feedback (RLHF) has been introduced as an effective method for LLMs alignment (Ouyang et al., 2022; Stiennon et al., 2020). RLHF first leverages a supervised learning objective to equip LLMs with basic instruction-following capabilities. Subsequently, a reward model is trained on a human preference dataset that contains pairwise comparisons of responses from LLMs (for the same query). The reward model is exploited for further finetuning LLMs via reinforcement learning with Proximal Policy Optimization (PPO) (Schulman et al., 2017). Specifically, training with PPO requires four models: a policy model (the targeted LLM), a critic model, a reference model (i.e., a supervised-finetuning version of the targeted LLM), and a reward model. All these models are based on LLMS with billions of parameters, whereas the policy and the critic models need to be updated online. In light of this, RLHF is highly resource-demanding and rather complicated to be applied in practice.\nIn this context, offline RL has been proposed as an alternative approach to LLMs alignment (Rafailov et al., 2023; Yuan et al., 2023). The basic idea of offline RLHF is to directly train LLMs on the preference dataset using some ranking loss such as max-margin ranking loss (Yuan et al., 2023), or negative log-likelihood loss (Rafailov et al., 2023). Optionally, a reward model can be trained to collect comparison data for sampling new responses from LLMs (Yuan et al., 2023). It should be noted that these offline RLHF methods only exploit the ordinal relationship between responses rather than making use of scalar values for rewarding (or penalizing) responses as in RLHF. The ordinal relationship, however, does not reflect the degree to which the preferred response is better than the dispreferred one. For instance, Figure 1 shows an example where the quality difference of the pair (response 1, response 2) is not as significant as that of the pair (response 1, response 3). As such, treating the two pairs equivalently may result in suboptimal performance for offline RLHF.\nTo mitigate the aforementioned issue, we propose a simple yet effective method based on Reward Difference Optimization, shorted as RDO. First of all, the reward difference coefficient is introduced as the degree to which one response is preferred against the other one given the same query. Such coefficients can be exploited as sample weights, which are incorporated into diverse offline RLHF methods as additional supervision signals for calibrating the loss function. Basically, one can train a reward model similar to that in RLHF and then exploit the reward values for reward difference measurement. This method, however, is not effective enough as the reward model independently assigns scores to responses. Inspired by how much easier it is for humans to conduct pairwise judgments, we propose a difference model to directly predict the reward difference between two responses. Unlike the reward model, our difference model leverages attention-based interactions between two responses for prediction. Our contributions are summarized as follows:\n\u2022 We introduce reward difference coefficients and show how they can be incorporated into offline RLHF methods including RRHF (Yuan et al., 2023), DPO (Rafailov et al., 2023) and KTO (Ethayarajh et al., 2024).\n\u2022 We develop the difference model that leverages the rich interactions between response pairs to directly predict the difference in human preference. This innovative method is accompanied by a specifically designed training strategy to ensure its effectiveness.\n\u2022 We conduct extensive experiments with Alpaca-7B (Taori et al., 2023), one of the most well-known open-source LLMs, on the HH dataset (Bai et al., 2023) and TL;DR dataset (Stiennon et al., 2020), two commonly used datasets for alignments with human preference. The experimental results show the effectiveness of our method in automatic metrics based on reward models, GPT-4, and human evaluation."}, {"title": "2 Reward Difference Coefficients", "content": "Given a dataset \\(D = \\{(x^{(i)}, y_w^{(i)}, y_l^{(i)}\\}_{i=1}^N\\) of samples from the targeted LLM, existing offline RLHF methods can be treated as learning to optimize the following general loss function (Zhao et al., 2023):\n\\[\\mathcal{L} = \\sum_{(x, y_w, y_l) \\sim D} \\mathcal{L}_{aln}(x, y_w, y_l; \\theta) + \\mathcal{L}_{reg} (\\theta) \\tag{1}\\]\nwhere \\(\\theta\\) indicates the parameters of the targeted LLM (\\(\\pi_{\\theta}\\)); \\(y_w\\) and \\(y_l\\) respectively denote the preferred and the dispreferred responses for the given query \\(x\\). The preference label can be provided either by human annotators or a well-trained reward model. Here, \\(\\mathcal{L}_{aln}\\) is the loss function for preference optimization (i.e., the alignment loss) and \\(\\mathcal{L}_{reg}\\) is the regularization term, for example, to prevent LLM from drifting too far from the supervised-finetuning (SFT) baseline."}, {"title": "Reward Difference Coefficients", "content": "As aforementioned, offline RLHF methods exploit the ordinal relationship \\(y_w > y_l\\) for measuring the alignment loss (\\(\\mathcal{L}_{aln}\\) in Eq. 1), without differentiating pivotal pairs and the trivial counterparts. We propose a simple yet effective method to address such issues based on reward differences. Specifically, given a well-trained reward model \\(r_{\\phi} : (x, y) \\rightarrow \\mathbb{R}\\) that assigns a reward scalar to each response \\(y\\) for the query \\(x\\), we can quantify the reward difference for each pair of responses \\((y_w, y_l)\\) for preference optimization as follows:\n\\[\\mathcal{L}^{rc} = \\sum_{(x, y_w, y_l) \\sim D} \\alpha \\cdot \\mathcal{L}_{aln}(x, y_w, y_l; \\theta) \\times \\mathcal{R}^{\\alpha} + \\mathcal{L}_{reg} (\\theta) \\\\\\]\n\\[\\mathcal{R}^{\\alpha} = r_{\\phi}(x, y_w) - r_{\\phi}(x, y_l) \\tag{2}\\]\nwhere \\(\\mathcal{R}^{\\alpha}\\) denotes the reward difference coefficient, indicating the degree of how \\(y_w\\) is better than \\(y_l\\). This difference is then leveraged as the coefficient for each response pair in the alignment loss. Here, \\(\\alpha \\in [0, 1]\\) is used to control the effect of the reward difference coefficient. In particular, when \\(\\alpha = 0\\), the coefficient has no effect.\nAssuming that the reward model can pinpoint essential differences from minor mistakes, the coefficient term helps steer more gradients towards sample pairs with larger differences. Note that, a typical way to learn a reward model is based on the Bradley-Terry (BT) model (Bradley and Terry). Specifically, the loss function for learning the reward model is as follows:\n\\[\\mathcal{L} = \\frac{1}{N} \\sum [\\log \\sigma(r_{\\phi}(x, y_w) - r_{\\phi}(x, y_l))] \\tag{3}\\]\nwhere \\(\\sigma\\) indicates the sigmoid function, and \\(r_{\\phi}\\) is the reward model with learnable parameters \\(\\phi\\).\nThe proposed coefficient can be readily incorporated into diverse offline RLHF methods. The following details how our method can be applied to RRHF and DPO.\nRRHF+rc RRHF with reward difference coefficients (RRHF+rc) optimizes the following loss:\n\\[\\mathcal{L}^{RRHF+rc} = \\sum -\\mathcal{R}^{\\alpha} \\cdot \\max(\\pi_{\\theta}(y_w|x) - \\pi_{\\theta}(y_l|x), 0) + \\mathcal{L}_{sft}\\]\nwhere \\(\\mathcal{L}_{sft}\\) indicates the cross-entropy loss similar to supervised fine-tuning on preferred responses. This loss is used as a regularization term to keep the LLM near the supervised-finetuning version.\nDPO+rc DPO with reward difference coefficients (DPO+rc) optimizes the following loss:\n\\[\\mathcal{L}^{DPO+rc} =  \\sum_{(x, y_w, y_l) \\sim D} \\mathcal{R}^{\\alpha} \\times g(y_w, y_l, x, \\theta) \\\\\\]\n\\[g =  \\log \\sigma \\Bigg(\\beta \\Big( \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{sft}(y_w|x)} - \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{sft}(y_l|x)}\\Big)\\Bigg) \\]\nwhere \\(g\\) is the original DPO objective defined for the tuple \\((y_w, y_l, x)\\). To analyze the effect of reward difference coefficients, let us consider the gradient of DPO+rc. Specifically, as \\(\\mathcal{R}^{\\alpha}\\) does not depend on \\(\\theta\\), the gradient with respect to the parameters \\(\\theta\\) of DPO+rc can be written as:\n\\[\\nabla_{\\theta} \\mathcal{L}^{DPO+rc} = - \\sum \\mathcal{R}^{\\alpha} [\\nabla_{\\theta} \\log \\pi_{\\theta}(y_w|x) - \\nabla_{\\theta} \\log \\pi_{\\theta}(y_l|x)] \\\\\\]\n\\[ \\mathcal{R} = \\mathcal{R}^{\\alpha} \\times \\beta \\times \\sigma(\\mathbb{E}_{\\theta}(x, y_l) - \\mathbb{E}_{\\theta}(x, y_w)) \\]\nwhere \\(\\mathcal{R}\\) is the weight associated with each response pair, and \\(\\mathbb{E}_{\\theta}(x, y) = \\beta \\log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{sft}(y|x)}\\) is the reward implicitly defined by the language model \\(\\pi_{\\theta}\\). In DPO (\\(\\mathcal{R}^{\\alpha} = 1\\)), each response pair is weighed by how much higher the implicit reward model \\(\\mathbb{E}_{\\theta}\\) rates the disperferred responses, scaled by the constant \\(\\beta\\) (Rafailov et al., 2023). In contrast, DPO+rc weighs the response pair by taking into account the independent reward model \\(r_{\\phi}(x, y)\\) (for measuring \\(\\mathcal{R}^{\\alpha}\\)) besides the implicit reward \\(\\mathbb{E}_{\\theta}(x, y)\\)."}, {"title": "3 Reward Difference Prediction", "content": "The above reward difference coefficients are measured based on a typical (pointwise) reward model \\(r_{\\phi}(x, y)\\). However, doing so is not effective as the model considers responses independently. In this section, we introduce a reward difference model that simultaneously takes the given query \\(x\\) and two responses \\(y_w, y_l\\) as inputs for predicting the reward difference. We then propose an effective method for training the difference model."}, {"title": "3.1 Reward Difference Model", "content": "The reward difference model \\(R_\\theta : (x, y_1, y_2) \\mapsto \\mathbb{R}\\) is an LLM of which the last layer is replaced with a new linear layer. The input for \\(R_\\theta\\) is of the form \"Query: {x}; Response 1:{y1}; Response 2:{y2}\". We obtain the last token embedding as the representation to be passed to the last linear layer for reward difference prediction. Note that, if the predicted output is negative, the difference model predicts that y\u2081 is worse than y2, and the magnitude (absolute value) indicates how much y\u2081 is worse than y2.\nCompared to the vanilla (pointwise) reward model, the last token embedding in the reward difference model can attend to both responses, thus we obtain a more informative representation for predicting the reward difference. The difference between the typical reward model and the reward difference model is demonstrated in Figure 2."}, {"title": "3.2 Difference Model Training", "content": "To train such a difference model, we design the main loss as follows:\n\\[\\mathcal{L}^{rd} = - \\sum CE (\\sigma(f(x, y_1, y_2)), I(y_1, y_2)) \\tag{4}\\]\n\\[I(y_1, y_2) = \\begin{cases} 1, & \\text{if } y_1 \\succ y_2 \\\\ -1, & \\text{if } y_1 \\prec y_2 \\end{cases}\\]\nwhere CE indicates the cross-entropy loss and I is an indicator of whether y\u2081 is considered better than Y2 in the training dataset.\nRegularization Losses Training the difference model based solely on the previous loss may not be entirely satisfactory. This is because it may require a substantial amount of data to ensure effectiveness. On the other hand, the model might still score f(x, Yi, Yi) as positive or negative even though the two responses are the same. In order to fix such issues and make the training more efficient, we propose two additional regularization loss terms for the difference model training based on the following comparison rules:\n\u2022 Duplication: The comparison score between Yi and itself (i.e. f(x, Yi, Yi)) should be zero.\n\u2022 Reverse: Because the order matters in the model input, the difference score between Yi and yj (i.e. f(x, yi, yj)), and the difference score between yj and yi (i.e. f(x, yj, Yi)) are supposed to be the opposite of each other.\nThe two regulation loss terms for the difference model training are calculated as follows:\n\\[\\mathcal{L}^{dup} = \\frac{1}{N} \\sum_{i=1}^{N-1} f(x, Y_i, Y_i)^2 \\\\\\]\n\\[\\mathcal{L}^{rev} = \\sum [f(x, y_i, y_j) + f(x, y_j, Y_i)]^2  \\tag{5}\\]\nHere N stands for the number of all comparisons in the dataset D. For the \\(\\mathcal{L}^{dup}\\) we only calculate for one randomly selected response between the two compared responses. For the \\(\\mathcal{L}^{rev}\\), we also calculate for one possible order only given two compared responses. The final loss function is:\n\\[\\mathcal{L} = \\mathcal{L}^{rd} + \\beta_0 \\times \\mathcal{L}^{dup} + \\beta_1 \\times \\mathcal{L}^{rev}  \\tag{6}\\]"}, {"title": "4 Experiments", "content": "We conduct experiments to answer multiple research questions: 1) Q1: How is the effect of reward difference coefficients measured by a pointwise reward model? 2) Q2: How is the performance of the difference model on response preference prediction compared to that of the pointwise reward model? and 3) Q3: How is the effect of the difference model on LLM alignment?"}, {"title": "4.1 Common Settings", "content": "Compared Methods We exploit Alpaca with 7B parameter size as the initial model for alignment. We run experiments on three representative offline RLHF methods: 1) RRHF (Yuan et al., 2023), one of the earliest offline RLHF methods; 2) DPO(Rafailov et al., 2023), the most popular offline RLHF method; and 3) KTO (Ethayarajh et al., 2024), the most recent offline RLHF method based on economic theory. We examine the performance of RRHF and DPO in three cases: 1) Vanilla offline RLHF (the original RRHF, DPO and KTO); 2) RRHF+rc, DPO+rc and KTO+rc in which reward difference coefficients are calculated from a pointwise reward model; 3) RRHF+rc and DPO+rc in which reward difference coefficients are calculated from our difference model. Experiments of KTO can be found in Appendix section A.1.\nPreference Datasets Our experiments are conducted on the Anthropic Helpful and Harmless (HH) dataset (Bai et al., 2023)* and OpenAI TL;DR dataset (Stiennon et al., 2020).\nHH dataset. Each dialogue in the HH dataset has preferred and dispreferred responses labeled by humans. The dataset contains 76.3k dialogues (queries) for training and 5.1k for testing. For RRHF training, we use the augmented dataset (HH+) provided by RRHF authors. In the HH+ dataset, for a given query, there are 6 responses of which two are from the original HH dataset and the other four are generated by the Alpaca-7B using dynamic beam search. The preference ordering of the 6 responses (for the corresponding query) is decided by a trained reward model. For DPO training, we follow the DPO paper and directly use the original HH dataset.\nTL;DR dataset. OpenAI TL;DR dataset (Stiennon et al., 2020) is a dataset targeted at summarization tasks with human preference labels. The dataset contains 11.7k training preference pairs and 6.55k test pairs. We use the original dataset for both RRHF and DPO."}, {"title": "Evaluation of language models", "content": "We use three-folds of evaluation. (1). Reward model evaluation. We calculate the average reward given by the reward model on the test set; (2). LLM auto evaluation. We randomly selected 300 samples from the test set and requested the LLMs to score each response without knowing the underlying methods. Subsequently, we calculated the win/tie/loss ratio for each pair. To mitigate positional bias in LLMs (Wang et al., 2023), we shuffled the responses and requested LLMs to provide a detailed explanation before judgment. We include three powerful LLMs, GPT4 (Achiam et al., 2023b), GPT3.5 (i.e. ChatGPT) and moonshot-v1\u2020(i.e. KIMI), and the majority vote determined the outcome for each comparison. Detailed results of each LLM evaluation can be seen in the appendix; (3). Human evaluation. We also conducted a human evaluation in section 4.4 on 300 randomly chosen samples from the HH test set. Three computer science graduate students with strong English skills served as evaluators. For each sample, the response order was shuffled, and the source method identities were hidden. Evaluators were instructed to judge the helpfulness and general quality of each response and choose the better one, or mark \u201cTie\u201d. The majority vote determined the outcome for each comparison."}, {"title": "4.2 Q1: Effect of Reward Difference Coefficients on Offline RLHF", "content": "4.2.1 Experimental Design\nReward Models This section calculates reward coefficients using (pointwise) reward models.\nHH dataset. For RRHF training, we follow the original paper and use the \u201cgpt-j-static\u201d reward model. We refer to this reward model as RRHF-rm. For DPO training, since the original DPO does not require a reward model, we train a new reward model based on Alpaca-7B on the training split of the HH dataset. The resultant reward model is referred to as DPO-rm hereafter. As RRHF and RRHF+rc use RRHF-rm as the reward model during training, DPO-rm is considered the held-out reward model for RRHF/RRHF+rc. Likewise, RRHF-rm is the held-out reward model for DPO.\nTL;DR dataset. For TL;DR dataset, the reward model used to calculate the reward difference (i.e., training rm) is \u201cOpenAssistant/reward-"}, {"title": "4.2.2 Experimental Results", "content": "The experimental results are reported in Tables 1, 2 and 3, showing that the inclusion of the reward difference coefficient consistently enhances the performance of offline RLHF methods."}, {"title": "4.3 Q2: Comparison of Reward Difference Model and Reward Model", "content": "4.3.1 Experimental Settings\nDifference and Reward Models Training We train the difference model and the reward model"}, {"title": "4.3.2 Experimental Results", "content": "We report the accuracy of the difference model and the pointwise reward model in Table 4 where the main observations are as follows:\n\u2022 The difference model achieves higher accuracy than the two baseline models: gptj-rm-static and Alpaca-7B reward models. Here, the gptj-rm-static is the reward model used in (Yuan et al., 2023) and the Alpaca 7B reward model is trained by ourselves.\n\u2022 Alphaca 7b Reward model (3 epochs) is better than Alphaca 7b Reward model, which is trained with one epoch. The model, however, is still inferior compared to the difference model. Note that it is an unfair comparison as the difference model is trained with only one epoch.\n\u2022 The regularization losses help enhance the overall accuracy from 0.708 to 0.715. Further improvement can be obtained by tuning the hyper-parameters \\(\\beta_0\\) and \\(\\beta_1\\) more carefully."}, {"title": "4.4 Q3: Effect of Reward Difference Model On Offline RLHF Methods", "content": "In this section, we run experiments on HH dataset to prove the effectiveness of our proposed difference model compared with vanilla reward model."}, {"title": "4.4.1 Experimental Settings", "content": "Policy Model Training This sections considers two enhanced offline RLHF methods: RRHF+rc (diff) and DPO+rc (diff). These methods leverage the difference model to calculate the reward coefficients. This contrasts with RRHF+rc (reward) and DPO+rc (reward) methods (Section 4.2), which directly subtract scores from the reward model. In both RRHF+rc (diff) and DPO+rc (diff), the parameter \\(\\alpha\\) is set to 0.5, while other settings remain consistent with those described in section 4.2.1. Additionally, we explore RRHF (diff), an RRHF variant where the difference model replaces the reward model for generating preference data in the original RRHF method."}, {"title": "4.4.2 Experimental Results", "content": "Table 5 reveals the advantage of the difference model over the reward model in predicting reward difference coefficients. This observation holds for both base methods (DPO/RRHF) and across evaluators (ChatGPT/GPT-4/moonshot-v1).\nTable 6 presents human evaluation for comparing RRHF+rc (diff) to RRHF+rc (reward). The results confirm the findings of the LLM evaluation, which consistently favors the difference model approach for the RRHF+rc method."}, {"title": "5 Related Work", "content": "5.1 LLMs Alignment with Offline RLHF\nOffline RLHF has recently received significant attention thanks to its simplicity. A number of innovations have been proposed from different perspectives."}, {"title": "5.2 Reward Modeling for Alignment", "content": "In LLMs alignment, a reward model assesses a response to a given query, generating a score representing its quality (Stiennon et al., 2020; Ouyang et al., 2022). This score serves as a guide for the alignment process and can be used for two main purposes: 1) During alignment, the score acts as a training signal for online RLHF (Ouyang et al., 2022); 2) It helps gather human preferences for response selection (Yuan et al., 2023). Typically, training reward models employed the Bradley-Terry (BT) model (Bradley and Terry), which estimates the likelihood of one response being better than another based on their individual scores. However, this method often falls short of capturing true human preferences.\nSeveral recent studies have tackled the limitations of conventional reward models by exploring diverse solutions. Touvron et al. (2023) and Bai et al. (2023) propose to train fine-grained reward models that evaluate the response from different aspects, e.g. helpfulness, and harmlessness. Other studies consider taking uncertainty in the learned functions into account (Liang et al., 2023; Yue et al., 2023). These methods, however, focus on \u201cpointwise\" reward models which independently predict a reward score for each response. Recently, SLiC-HF (Zhao et al., 2023) is introduced to utilize a rank model that takes a query and two response candidates as input, ultimately indicating the chosen response via a token identifier (e.g., A/B). While bearing similarities with our difference model, SLiC-HF's ranking model outputs a categorical token, not a quantitative score reflecting the difference between responses.\nOffline RLHF methods usually omit the process of reward modeling. However, it has been shown in recent studies (Xu et al., 2024; Azar et al., 2023) that the reward model plays an important role in LLM alignment. We verify this finding and provide a simple yet effective way to enhance multiple offline RLHF methods."}, {"title": "6 Conclusion", "content": "This paper proposes a novel approach to address the limitations of existing offline RLHF methods. Our method leverages the reward difference coefficient, which quantifies \u2018how much\" one response is preferred over another. These coefficients are integrated as sample weights during training, putting the focus towards more \u201csure\u201d comparisons. Furthermore, we introduce a reward difference model for directly predicting these coefficients, accompanied by an effective training methodology. Experimental results consistently show the effectiveness of the reward difference coefficients on two representative offline RLHF methods including DPO and RRHF. In addition, LLMs-based evaluation and human evaluation validate the advantages of the difference model over the reward model.\nIn the future, we aim to explore several promising directions: (1). Scaling Law: While this work demonstrates effectiveness with 7B models, scaling to larger architectures is crucial. Evaluating performance on progressively larger LLMs will provide valuable insights into the effectiveness of our approach; (2). Generalization: As LLMs may lose some of their generalization ability after alignment (Gao et al., 2023), exploring techniques to mitigate this phenomenon would ensure LLM retention of their core strengths besides improved alignment."}, {"title": "7 Limitation", "content": "The effectiveness of our proposed methods can be shaped by the quality of reward model or difference model.\nTraining the difference model requires processing only half the number of queries compared to the original reward model training, but it involves a longer sequence length. As a result, training the difference model may take more time and resources when the queries (or chat histories) in the dataset are short but the responses are much longer.\nThe difference model may not be good at directly giving the reward score of the response which is required by RL algorithms like PPO. However, it should be noted that it can still give the reward score for a single response by introducing a baseline response for each query (Zhao et al., 2023).\nAll experiments and analyses are limited to LLM with 7B parameters. It is unknown whether the conclusions still hold as the model gets larger."}, {"title": "8 Ethical Concerns", "content": "Sensitive and offensive content exists within HH and HH+, datasets intended solely for research purposes. The viewpoints expressed in the data do not reflect our beliefs. We aspire for our efforts to contribute to the development of AI technologies aligned with ethical standards."}, {"title": "A Additional Experiments", "content": "A.1 Reward difference coefficient on KTO\nKTO (Ethayarajh et al., 2024) is one of the latest techniques for LLM alignment, which is based on the famous Kahneman & Tversky's prospect theory and only needs a binary signal of whether an output is desirable or undesirable for a given input. We also test the idea of reward coefficient using this method. Pay attention that KTO receives point-wise samples instead of pair-wise comparisons like other methods, so we use the reward difference score as the reward coefficient for both the desirable and undesirable responses paired in the dataset."}, {"title": "A.1.1 Experiment settings", "content": "We basically follow similar settings as in section 4.2.1 and align Alpaca-7b on HH and TL;DR datasets using KTO with or without the reward coefficient to test its effectiveness. For the HH dataset, we use the reward trained from alpaca-7b on the HH dataset as the reward model, and for the TL;DR dataset, we use the open-source deberta reward model to calculate the reward coefficient. To save time and resources, we use OpenRLHF repo (Hu et al., 2023) in Github and leverage Lora (Hu et al., 2021) with rank=8 for KTO training. All experimental settings are exactly the same except for using the reward coefficient(i.e. rc) or not for methods KTO and KTO+rc. We train both of them with one epoch and a learning rate of 5e-7."}, {"title": "A.1.2 Experimental results", "content": "Results of KTO with or without the reward coefficient can be found in table 7 and 8. Both reward evaluation and LLM-as-judge evaluation"}, {"title": "A.2 Additional experiments for difference model", "content": "In this section, we report some other experiments that can better prove the robustness of our experimental conclusion about the superiority of the proposed difference model."}, {"title": "A.2.1 RRHF with Difference model V.S. Alpaca reward model", "content": "In section 4.4, the baseline reward model we used is the one with higher accuracy (i.e. Dahoas/gptj-rm-static). Here we also report the result of RRHF with the Alpaca 7b Reward model in Table 4. We follow the same experimental setting and evaluation setting as in section 4.4.\nThe results are in table 9. The results indicate that the difference model is better than the Alpaca-7B reward model too."}, {"title": "A.2.2 DPO with bigger B", "content": "\\(\\beta\\) of DPO loss controls the implicit KL-divergence penalty and the greater it is, the more the trained policy model resembles the reference model. In our main experiments, we set \\(\\beta\\) of DPO loss to 0.2, which is relatively small. Here, we report the results of DPO with \\(\\beta\\) = 0.5 to better prevent the policy model from going too far from the initial SFT model. The results of the difference model and reward model under this setting can be found in table 10.\nThe results show that the difference model still outperforms the baseline reward model in this case. The percent of \"Tie\u201d is much larger than table 5 due to both two aligned LLMs becoming more similar to the initial SFT model."}, {"title": "B Why Reward Coefficients?", "content": "Doing the theoretical explanation for our method is not trivial", "hard-to-tell\u201d samples and clean samples are easy ones. According to some recent studies (Huang et al., 2019; Arpit et al., 2017), easy samples are learned at the early stage as they contribute more to the gradient computation early on, leading to a sharp decrease in their losses. On the other hand, the \"hard\" samples are usually learned at the late stage of training. As we train the reward or the difference model with only one epoch, the model is likely to be underfitted for hard samples. In other words, the reward and difference model will avoid giving too high scores for those hard-to-tell comparisons and tend to give higher scores for those easy samples.\nSecondly, we briefly prove that adding the reward coefficient does not change the analytical solution of DPO as eq.7 in the IPO paper (Azar et al., 2023) or eq.4 in the DPO paper (Rafailov et al., 2023). We skip the intermediate steps and directly show that the final equation of proof (section A.1) in the IPO paper (Azar et al., 2023) becomes": "n\\[-KL(\\delta||\\delta) = \\frac{\\mathcal{L}_\\tau(\\delta)}{\\tau \\mathcal{R}_0} - C\\"}]}