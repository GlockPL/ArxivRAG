{"title": "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "authors": ["Tom\u00e1s Feith", "Akhil Arora", "Martin Gerlach", "Debjit Paul", "Robert West"], "abstract": "Links are a fundamental part of information networks, turning isolated pieces of knowledge into a network of information richer than the sum of its parts. However, adding a new link to the network is not trivial: it requires not only the identification of a suitable pair of source and target entities but also the understanding of the content of the source to locate a suitable position for the link in the text. The latter problem has not been addressed effectively, particularly in the absence of text spans in the source that could serve as anchors to insert a link to the target entity. To bridge this gap, we introduce and operationalize the task of entity insertion in information networks. Focusing on the case of Wikipedia, we empirically show that this problem is, both, relevant and challenging for editors. We compile a benchmark dataset in 105 languages and develop a framework for entity insertion called LOCEI (Localized Entity Insertion) and its multilingual variant XLOCEI. We show that XLOCEI outperforms all baseline models (including state-of-the-art prompt-based ranking with LLMs such as GPT-4) and that it can be applied in a zero-shot manner on languages not seen during training with minimal performance drop. These findings are important for applying entity insertion models in practice, e.g., to support editors in adding links across the more than 300 language versions of Wikipedia.", "sections": [{"title": "1 Introduction", "content": "From digital encyclopedias and blogs to knowledge graphs, knowledge on the Web is organized as a network of interlinked entities and their descriptions. However, online knowledge is not static: new webpages are created, and existing pages are updated almost every day. While there exists substantial support for content creation (e.g. via translation Wulczyn et al. (2016) or generative AI tools Shao et al. (2024)), adding new knowledge not only requires creating content but also integrating it into the existing knowledge structure. The latter usually leaves editors with the time-consuming task of reading lengthy webpages to identify a relevant text span for inserting an entity that is not yet mentioned on the page. Thus, to support editors in effectively integrating entities in multilingual linked corpora on the Web, we introduce the task of entity insertion.\nEntity insertion. We consider Wikipedia as the primary use case and focus on the task of adding links. Specifically, given a source and target entity, the goal of entity insertion is to identify the most suitable text span in the article describing the source entity for inserting a link to the target entity. Fig. 1 portrays a real example of the entity insertion task with the eventual goal of adding a link from the source entity June Spencer, a former English actress, to the target entity Private school. Most importantly, entity insertion is a different and much more challenging task when compared to entity linking, as no existent text span in the version of the source article (June Spencer) at edit time could be used to link to the target entity (Private school). Rather, a new text span\u2013\u201cShe also worked at a private school.\u201d\u2013was added along with the to-be-inserted target entity."}, {"title": "2 Related work", "content": "In this section, we review works that overlap closely with our study (cf. Appx. A for details).\nEntity linking. Previous work has framed entity insertion as an entity linking problem (Gerlach et al., 2021; Milne and Witten, 2008; West et al., 2009; Arora et al., 2021; \u010culjak et al., 2022; West et al., 2010), where the goal is to assign a unique identity to entities mentioned in the text. The task of entity linking is composed of two sub-tasks: Named Entity Recognition (NER) and Named Entity Disambiguation (NED). Most research (Hoffart et al., 2011; Fu et al., 2020; van Hulst et al., 2020) into entity linking solves first the NER problem, in which the task is to find candidate mentions for named entities in the source article. However, there is recent work (Zhang et al., 2022) exploring the problem in reverse order, first solving NED by finding target entities related to the source article and then NER searching only for mentions for the found targets.\nWhen the mention is present, the task of entity insertion is similar to NER (Zhang et al., 2022), as both tasks can be solved by searching for mentions in the text. However, entity insertion is a more general task as it allows for the mention of the target entity to not yet be present in the text. In this case, the goal is to exploit the context information to find the text span most related to the target entity. NER modules (Finkel et al., 2005; Nothman et al., 2013) are designed to search for the most related mentions, and thus, they are not applicable in scenarios where the mentions are not yet available.\nEntity tagging. Du et al. (2022) introduced this task as a relaxed form of entity linking. An entity tagging model is only tasked with determining the entities present in the text and does not need to find the exact mentions of the entities. However, even though the model is not tasked with extracting an entity's mention, the task of entity tagging still assumes that the text contains some mention of the entity, which distinguishes it from entity insertion.\nLink the Wiki. Huang et al. (2008) ran a track at INEX 2008 with two tasks: file-to-file link discovery and mention-to-BEP (best entry point) link discovery. File-to-file link discovery is a document-level task that can be framed as a link prediction task in networks, where the Wikipedia articles act as nodes and the links act as edges. The mention-to-BEP task is an entity linking task with anchor prediction, where the two-part goal is to find mentions in the source article pointing to other articles, and finding the best point of entry (the anchor) in the target file. This task has more recently resurfaced as an anchor prediction task (Liu et al., 2023).\nPassage ranking. Transformer-based models have revolutionized passage ranking by enhancing semantic understanding beyond traditional lexical methods like BM25 (Robertson and Zaragoza, 2009). BERT demonstrated early success by leveraging contextualized embeddings for re-ranking (Nogueira and Cho, 2019), leading to innovations"}, {"title": "3 Task formulation", "content": "Let Esrc be a source entity and Etgt be a target entity. Let Xsrc be the textual content of the article corresponding to Esrc. The text can be partitioned into a set of (potentially overlapping) text spans, Xsrc = {x1,...,xm}, where M is the number of text spans in the article. Entity insertion is the task of selecting the most relevant span x* to insert the target entity Etgt. Formally,\nx* = arg max R(x, Etgt) \\qquad x \\in X_{src} (1)\nwhere R is an arbitrary relevance function quantifying the relevance of Etgt to each text span x \u2208 Xsrc. We frame entity insertion as a ranking task, where the goal is to rank all the candidate text spans Xsrc based on their relevance to the target entity."}, {"title": "4 Data", "content": "We constructed a new multilingual dataset for studying entity insertion in Wikipedia. The dataset consists of links extracted from all Wikipedia articles, each link's surrounding context, and additional article-level meta-data (such as titles, Wikidata QIDs, and lead paragraphs). Overall, the dataset contains 958M links from 49M articles in 105 languages. The largest language is English (en), with 166.7M links from 6.7M articles, and the smallest language is Xhosa (xh), with 2.8K links from 1.6K articles (cf. Appendix B for details).\nFig. 3 provides an overview of our data processing pipeline. The data processing was done in two steps. We first extracted all the links from the 2023-10-01 snapshot. Next, we found all the links added in the time between 2023-10-01 and 2023-11-01."}, {"title": "5 Entity insertion with LOCEI", "content": "Fig. 4 presents an overview of LOCEI. Our model (\u00a7 5.1) is composed of a transformer-based encoder that jointly encodes the target entity as well as the candidate spans in the source entity, and a multilayer perceptrion (MLP) trained via a list-wise objective capable of ranking candidates based on their relevance to the target. We introduce a novel data augmentation strategy that closely mimics real-world entity insertion scenarios (\u00a7 5.2), a knowledge injection module to incorporate external knowledge about the entities (\u00a7 5.3), and the multilingual variant XLOCEI (\u00a7 5.4).\n5.1 Model\nArchitecture. We use a transformer-based encoder I to jointly encode each candidate text span x \u2208 Xsrc and the target entity Etgt into a sequence of vectors. To reduce this sequence into a single vector, we use the embedding of the [CLS] token (Devlin et al., 2019), which measures how related the candidate x is to the entity Etgt. An MLP A produces a scalar relevance score between the candidate x and the entity Etgt using the relevance embedding produced by the encoder I defined as\nR = \\Gamma (\\varphi; \\theta_{\\Gamma}) (2)\nr = A (R[CLS]; \\theta_a) (3)"}, {"title": "5.2 Two-stage training pipeline", "content": "We extract two types of links for studying entity insertion: existing and added links (\u00a7 4). While added links reflect the entity insertion scenarios observed in the real world, we found that the number of added links is low for most languages (cf. Table 8 in the Appendix), thereby not being sufficient for training our model. To circumvent this challenge, we develop a two-stage training pipeline that uses both existing and added links.\nDynamic context removal. A key challenge with existing links is that they only reflect the text_-"}, {"title": "5.3 Knowledge injection", "content": "While the representation presented in Eq. 4 (\u00a7 5.1) already allows LOCEI to measure the target entity's relevance to the candidate text span, we inject external knowledge about the target entity and knowledge about the structural organization of the source article to produce better relevance embeddings.\nSince section titles provide additional 'local' knowledge in the form of a summarized conceptu-"}, {"title": "5.4 Incorporating multilinguality (XLOCEI)", "content": "To enable the encoder to better model the relationship between an entity target and candidate text spans, we leverage the patterns existent in multiple languages. For this, we train a single model by jointly considering entity insertion examples in multiple languages. This enables cross-lingual transfer, empowering, especially, low-resource languages with lesser and lower quality training data."}, {"title": "6 Experiments", "content": "All the resources required to reproduce the experiments in this paper are available at https://github.com/epfl-dlab/multilingual-entity-insertion.\n6.1 Data\nWe study entity insertion in 105 language versions of Wikipedia. We use a judicious mix (based on size, script, geographic coverage, etc.) of 20 languages for training the benchmarked methods, however, for evaluation, we consider all 105 languages. For dataset statistics, cf. Tables 7 and 8 of Appx. \u0412.\nTraining set. We train LOCEI and XLOCEI using a two-stage training pipeline (\u00a7 5.2). While the data for the first stage is based on the existing links extracted from the 2023-10-01 snapshot, the second stage data is built using the links added between the 2023-09-01 and 2023-10-01 snapshots.\nNegative candidates. During training, we extract N negative candidates for each positive candidate. Negative candidates are text spans in the source Xsrc where the target entity Etgt was not inserted. Whenever possible, we select N negative candidates (\"hard negatives\") from the same source article as the positive candidate. However, when articles are too small to be able to select N negatives, we sample the remaining negative candidates randomly from other articles (\u201ceasy negatives\u201d). Details pertaining to the implementation of negative candidate extraction are provided in Appendix B.5."}, {"title": "Test set", "content": "For evaluation, we use the links added between the 2023-10-01 and 2023-11-01 snapshots. This ensures no overlap between the training and test sets and is therefore advantageous in mitigating data leakages. Unlike training, we use all the D available candidates in an article for evaluation."}, {"title": "6.2 Baselines", "content": "\u2022 Random: ranks candidates uniformly at random.\n\u2022 String Match: searches for previously used mentions in the candidate text spans.\n\u2022 BM25 (Robertson and Zaragoza, 2009): applies the Okapi-BM25 implementation (Trotman et al., 2014) on keywords extracted from the target lead paragraph and the candidate text spans.\n\u2022 EntQA (Zhang et al., 2022) (English only): for independently encoding the candidate text spans and target entity. We then use the retriever model of EntQA to rank text spans based on the cosine similarity between the embeddings.\n\u2022 GET (Du et al., 2022) (English only): use the generative ability of GET to generate the target entity name for each candidate text span. We then rank the text spans based on their likelihood of generating the target entity.\n\u2022 PRP-Allpair (Qin et al., 2024) (Zero-shot only): to assess the relevance of candidate text spans to the target entity in a pairwise manner using GPT-3.5 and GPT-4, and then uncover the ranking from all pairwise comparisons."}, {"title": "6.3 Setup", "content": "Model. We present results for LOCEI and XLOCEI by fine-tuning the pre-trained xlm-roberta-base model (Conneau et al., 2020) as the encoder. The MLP is a 2-layer network with ReLU activations. We also explored different model sizes (e.g. Large and XL) and other pre-trained models (BERT and T5): results in Appendix C.\nEvaluation metrics. We use (1) Hits@1, and (2) mean reciprocal rank (MRR) to evaluate the quality of the benchmarked methods. For each language, we compute the micro aggregates of the metrics over all added links in the test set. Moreover, we present results grouped into three categories: (1) Overall: considering the entire test set, (2) Present: considering links corresponding to the text_present entity insertion scenario, and (3) Missing: considering links corresponding to all the other scenarios, namely, missing_mention, missing_sentence, and missing_span."}, {"title": "6.4 Main results", "content": "We evaluate three variants of our entity insertion model: i) simple fine-tuning: a family of monolingual models fine-tuned in each language without the extensions (data augmentation, knowledge injection, two-stage training) introduced in LoCEI; ii) LOCEI: a family of monolingual models fine-tuned using the full LocEI framework; and iii) XLOCEI, a single multilingual model fine-tuned jointly on all the languages using the full LocEI framework. Table 2 shows the models' performance metrics (Hits@1 and MRR) aggregated (macro-average) over the 20 considered languages.\nOverall performance. We see that XLOCEI achieves the best overall quality and statistically significantly outperforms all other models for all cases considered. The key highlights are as follows: (1) BM25, a hard-to-beat baseline for ranking tasks, is around 20 percentage points inferior to XLOCEI, (2) simple fine-tuning, a baseline that we introduce in this work, substantially outperforms all the other considered methods, but is inferior to LOCEI and XLOCEI by being about 10 and"}, {"title": "Performance on \u2018Missing' and \u2018Present' categories", "content": "The key finding is that the baselines lack robustness to the variation in entity insertion types, which is substantiated by the huge disparity of entity insertion performance (around 50 percentage points) of all the baselines in the 'Present' and 'Missing' categories. This result further highlights the key limitation of the baselines: they cannot address the challenging scenarios of entity insertion. The key reason behind this disparity is that all the existing baselines rely on the existence of a suitable text span to insert a link to the target entity. On the contrary, both LocEI and XLOCEI effectively utilize the signals manifested in the context due to the introduced extensions (e.g. data augmentation) and are therefore robust to different entity insertion scenarios. Consequently, we observe that both LocEI"}, {"title": "Zero-shot vs. Fine-tuned", "content": "We further study the performance of XLOCEI in the zero-shot scenario, i.e., evaluating the model in languages that were not explicitly contained in the data for fine-tuning. This is relevant to assess the potential to support languages for which there is little or no training data available. We consider XLOCEI11, a variant of the multilingual XLOCEI which is trained on only 11 out of the 20 languages (cf. Table 11 in Appx. C.3 for details). We then evaluate the zero-shot performance of XLOCEI11 in the remaining 9 languages not considered for training. For comparison, we also show the nonzero shot performance of the models considered in the previous subsection: i) LOCEI, the family of monolingual models fine-tuned in each language; and ii) XLOCEI20, the single multilingual model trained on all 20 languages. The main result, shown in (Table 4), is that XLOCEI11 retains over 95% performance in the zero-shot scenario in comparison to the results of the best model, XLOCEI20, which was fine-tuned on these languages. Nevertheless, XLOCEI11 still outperforms the language-specific"}, {"title": "6.5 Ablation analysis", "content": "Finally, we investigate in more detail the effect of the extensions, namely, data augmentation, knowledge injection, and two-stage training that we introduce in the training pipeline of our model in comparison to a standard fine-tuning approach. Table 6 portrays the improvement in performance on account for each extension introduced in this work. Overall, we see that each extension has an overall positive impact on performance. First, introducing the dynamic context removal for data augmentation is only effective when including negative examples. In that case, it improves the performance on the missing cases, but at the cost of performance in the present case. This is expected because context removal leads to the model seeing fewer training samples in the present case. Sec-"}, {"title": "7 Discussions", "content": "7.1 Summary of findings\nWe introduced the novel task of entity insertion in information networks. Considering the case of Wikipedia, we justified the relevance and need for solving this task by demonstrating empirically that existing methods such as entity linking are often not suitable in practice. In fact, we showed that in 65% of edits in which links were inserted by editors, none of the existing text is suitable to insert the entity, i.e. new text has to be inserted somewhere in the article along with the inserted entity.\nWe developed a multilingual model (XLOCEI) to effectively solve the entity insertion task across 20 Wikipedia languages outperforming all other models. First, our model substantially outperforms strong baseline approaches based on string matching or BM25, especially in the case when the linked mention was missing. We demonstrate how each of the introduced novelties (data augmentation, knowledge injection, two-stage training pipeline) contribute to improve the downstream performance. Second, the multilingual model yields consistently better results than language-specific models. This shows that our model is capable of collating the knowledge acquired from each language to improve performance over all languages. Third, our model works well in a zero-shot scenario, i.e. not only retaining over 95% of the hypothetical best performance if the language was included but even outperforming the much larger GPT-3.5 and GPT4. This demonstrates that the model is capable of transferring knowledge to languages unseen during fine-tuning which is crucial for the practical"}, {"title": "7.2 Implications and broader impact", "content": "A new benchmark for NLP tasks. The problems of link recommendations and entity linking have been well-studied and many excellent solutions have been brought forward, some of which are denoted even near-optimal (Ghasemian et al., 2020). The problem of entity insertion constitutes a new relevant and challenging task in the domain of NLP. Our multilingual dataset provides a resource for researchers for development and evaluation of new models to solve this task. This will help improve the overall capabilities of large language models when applied in the context of networks that are crucial for organizing textual information.\nSupporting editors to bridge knowledge gaps. Many articles in Wikipedia lack visibility in the hyperlink network capturing a specific aspect of the general problem of knowledge gaps (Redi et al., 2020). For example, there are more than 8.8M so-called orphan articles (Arora et al., 2024), i.e., articles without any incoming links, which are de-facto invisible to readers navigating Wikipedia. Even if suitable link targets are identified, a remaining challenge for editors is to identify a relevant position in the text where the link can be inserted. At the current rate of \"de-orphanization\u201d, it would take editors more than 20 years to work through the backlog of orphan articles, suggesting that existing tools do not support editors in addressing this issue effectively. Our model can support editors in this task, complementing existing approached based on entity linking such as the add-a-link tool for newcomer editors (Gerlach et al., 2021)."}, {"title": "Limitations", "content": "We tried different pre-trained language models for our experiments with RoBERTa outperforming BERT and T5 by a large margin. The use of larger models with more parameters could further improve performance. While differences between ROBERTa-base and -large in English were marginal, we noticed a substantial drop when using the multilingual XLM-ROBERTa instead RoBERTa. This suggests that larger model architectures could be especially beneficial in the multilingual setting in order to improve support for low-resource languages, where performance is typically lower in comparison (Wu and Dredze, 2020). While multilingual models based on transformer architectures support many languages (e.g., XLM-ROBERTa was pre-trained on 100 languages), many of the more than 300 languages in Wikipedia are still not explicitly represented in the training data of these models. Thus, if unaddressed, the use of such models could lead to a language gap constituting a substantial barrier towards knowledge equity (Redi et al., 2020).\nOne practical limitation of the model is that the ranking of all text spans can become expensive if the article is very long and, thus, contains many candidates. This constitutes challenge for deploying the model in the future as a ready-to-use-tools for editors in practice. This requires the integration of potential solutions for improving inference such as via hierarchical searching.\nFurther improvements to the model could come from integrating of additional information from the local Wikipedia graph structure or the candidate context. For example, a very strong signal are the links already existing in the candidate context, as these indicate entities related to the context. Providing these as additional features to the model might help generate better representations of the candidate (Arora et al., 2022) and, as a result, better relevance embeddings. Furthermore, one could take advantage of the multilingual nature of Wikipedia with more than 300 language versions, each having a surprising amount of information not contained in any other languages (Bao et al., 2012). Thus, existing content about a target entity from other languages could provide relevant context (Garc\u00eda-Dur\u00e1n et al., 2022), which could be made available through automatic translation, such as the already available section translation tool in Wikipedia (WMF, 2019).\nIn our operationalization of entity insertion, we assume that the link to be inserted consisting of the pair of the source- and target entity is known. This assumption holds in the specific use-case of article \u201cde-orphanization\u201d (Arora et al., 2024) serving as the motivation for formulating the task of entity insertion. However, when this is not the case, our model requires an additional step to generate a specific link, e.g., via existing link recommendation models.\nOur modeling framework is not suitable for the scenario where links are added in a section that did not exist in the previous version of the article (missing_section). The text from the surrounding sections are not a good indicator for the insertion of a new entity, because they typically cover different subjects. The missing_section scenario could be addressed through complementary approaches based on generative models that produce a draft for new section when none of the existing candidates leads to a high relevance score."}, {"title": "Ethics statement", "content": "We have assessed the ethics of conducting this research and the output from the research and we have not identified ethical risks associated with the research. All the datasets and resources used in this work are publicly available and do not contain private or sensitive information about Wikipedia readers. While the dataset comes from individual edits (e.g., links added to Wikipedia articles), it does not contain any details about the author(s) of those edits. All the findings are based on analyses conducted at an aggregate level, and thus, no individual-level inferences can be drawn. And lastly, the experiments are done on data already collected and no human subject has been involved as part of them. We confirm that we have read and abide by the ACL code of conduct."}, {"title": "A Additional related work", "content": "A.1 Pre-trained language models\nThe transformer architecture, introduced by (Vaswani et al., 2017), has become the de facto architecture for most Natural Language Processing (NLP) applications. A transformer-based pre-trained language model takes as input a text sequence and computes a vector embedding that captures the semantic and structural information contained in the text sequence, which can then be used in downstream applications.\nPre-training is an expensive process. For example, the base variant of BERT (Devlin et al., 2019) took four days to train with 16 TPUs and ROBERTa (Liu et al., 2019) took one day to train with 1024 GPUs. However, pre-trained models can be leveraged to novel downstream tasks by fine-tuning them on task-specific datasets. As a comparison, the authors of BERT (Devlin et al., 2019) introduced several fine-tuned variants of BERT, all of which were fine-tuned in one hour using one TPU, which is much cheaper than pre-training the model for each task. This paradigm of pre-training language models on large amounts of data and then fine-tuning on much smaller amounts can reduce the cost of model training while retaining the knowledge from the pre-trained model and transferring it to the downstream task. Popular pre-trained models for multilingual tasks are mBERT (Devlin et al., 2019), XLM-ROBERTa (Conneau et al., 2020), and mT5 (Xue et al., 2021).\nA.2 Ranking tasks\nSince entity insertion is a ranking task, in this section, we provide a short review of literature focusing on document retrieval and ranking.\nClassical approaches for ranking tasks, such as BM25 (Robertson and Zaragoza, 2009), mainly rely on probabilistic methods that attempt to match keywords between a query and a candidate document. However, these methods cannot capture complex semantic and structural patterns. For example, the sentences \"The hero defeated the dragon and saved the damsel\u201d and \u201cThe knight slayed the beast and rescued the princess\u201d are semantically equivalent, but classical methods would to match them due to the small vocabulary overlap.\nThat said, pre-trained language models have become state-of-the-art for text ranking (Lin et al., 2021). A popular design for transformer-based ranking tasks is the cross-attention model, in which"}, {"title": "A.3 Domain adaption", "content": "(Gururangan et al., 2020) have shown that a second phase of pre-training using domain-specific knowledge can improve the performance of language models. Their experiments started with a pre-trained RoBERTa model and continued pre-training it using unlabelled data from a large corpus of domain-specific text.\nIn our work, we propose a similar approach for fine-tuning, where we apply a first stage of domain-shifted data and then a second stage of domain-specific data to improve the performance further."}, {"title": "B Additional dataset processing details", "content": "B.1 Data preparation steps\nExisting links. For the existing links, we store the following data: source and target titles, Wikidata QIDs, lead paragraphs, the name of the section containing the link, and a context surrounding the link. The context is defined as the sentence containing the link and the five sentences before and after (or until we reach the end of the section). We additionally keep positional information about the mention and the sentence containing the mention relative to the context (i.e., the start and end indices of the mention and the sentence in the context). The positional information is relevant to the data augmentation strategy we introduced (see \u00a7 B.4)."}, {"title": "Added links", "content": "For the added links, we store the same information as in the existing links, except for the positional information. This is because positional information is required primarily for performing data augmentations, which are required only for processing existing links."}, {"title": "B.2 Dataset statistics", "content": "Table 7 shows the summary statistics of the entity insertion dataset for each of the 105 considered language versions of Wikipedia, in particular the number of articles, the number of existing links, and the number of added links.\nTable 8 shows the number of samples contained in the training and test splits, respectively, for each of the 20 Wikipedia language versions considered in the experiments."}, {"title": "B.3 Entity insertion categories", "content": "Table 9 shows an example for each of the entity insertion categories, except for the category missing_section, demonstrating that the problem of entity insertion grows in complexity as more text is missing."}, {"title": "B.4 Dynamic context removal", "content": "Table 10 shows examples of the different types of dynamic context removal. Specifically, we randomly remove a word (rm_mention simulation), a sentence (rm_sent simulation), or a span of sentences (rm_span simulation) during training. Before sending the input to the model, we randomly select one of the masking strategies mentioned above (as well as no masking) to modify the input accordingly. However, before applying the strategy, we verify if the selected strategy does not produce an empty input. This may happen when, for example, the context is a single sentence, in which case simulating the rm_sent strategy would lead to an empty input. If the sampled strategy would produce an empty input, we re-sample a less aggressive strategy.\nWhile performing the rm_span simulation, the number of sentences to remove is chosen randomly between 2 and 5. Note that we used a space-based splitting for ease of implementation, and we acknowledge that this could be an issue for certain languages, such as Japanese or Chinese, which we intend to fix in the future."}, {"title": "B.5 Rules for sampling negative candidates", "content": "We employ the following rules when constructing the negative candidates, both for training and validation.\n1. A candidate's context should not span over two different sections.\n2. A candidate's context should not contain any of the mentions previously used to link to the target entity.\nThe first rule keeps the content of each context consistent, as two distinct sections can cover very different topics. The second rule ensures that all the candidates used to evaluate the module are correctly classified as either positive candidates or negative candidates. For example, if the goal is to insert the entity \u201c1984\u201d (the book - Q208460) and there is a sentence in the article with the word \"1984\u201d not linked to the target article, there could be three reasons for the link to be missing. First, the mention \"1984\" could be related to a different entity (e.g., the year - Q2432), in which case the sentence should belong to a negative candidate. Second, the mention is supposed to be for the target entity but it is not yet linked, in which case the sentence should belong to an additional positive candidate. Finally, the mention is supposed to be for the target entity but it should not be linked because of Wikipedia's editing guidelines, in which case it is not clear whether the sentence should belong to a negative or a positive candidate. Due to this unclear categorization, we choose to remove any sentences containing mentions previously associated with the target entity to be inserted."}, {"title": "C Additional experiments", "content": "C.1 Hyperparameters\nWe train the encoder and MLP with learning rates of le-5 and 1e - 4, respectively, using N = 9 negative candidates. Moreover, we use 5 sentences on either side as context for each candidate text span and set |Mtgt| = 10. The first stage of training uses 20K data points and is trained for 4 epochs, whereas the second stage uses all the available data for 2 epochs. Mimicking the real-world entity insertion scenarios, we set rm_nth=40%, rm_mention=20%, rm_sentence=30%, and rm_span=10%."}, {"title": "C.5 Impact of the model size", "content": "There is a widely known trend in the deep learning community that bigger models tend to perform better than smaller models (Soltanolkotabi et al., 2019; Brutzkus and Globerson, 2019; Simon et al., 2024). To this end, we studied how the model size impacts the entity insertion performance by comparing ROBERTALARGE with ROBERTABASE on the Simple English dataset.\nTable 13 shows that there is no statistically significant difference between the performance of"}]}