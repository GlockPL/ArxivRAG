{"title": "MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models", "authors": ["Huanqia Cai", "Yijun Yang", "Winston Hu"], "abstract": "IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.\nThrough systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large multimodal models (LMMs) has intensified debates about their capacity for human-like abstraction and reasoning. While existing benchmarks evaluate specialized capabilities such as OCR, object localization, and medical image analysis [11, 26, 10], these task-specific metrics fail to quantify the critical cognitive dimensions in multimodal systems. This limitation mirrors a long-standing challenge in human cognitive assessment: early methods conflated domain knowledge with innate reasoning ability until IQ testing emerged to isolate core cognitive competencies through language- and knowledge-agnostic evaluations [18]. Inspired by this paradigm, we argue that multimodal intelligence evaluation should also similarly decouple linguistic proficiency and task-specific knowledge from the measurement of abstract reasoning capacities.\nAbstract Visual Reasoning (AVR) offers a plausible solution to the above challenge. As shown in Figure 4, AVR problems usually contain visual puzzles with simple 2D/3D shapes. Solving these problems requires identifying and understanding the underlying abstract rules and generalizing them to novel configurations. Although there exists a wide range of AVR benchmarks, e.g., RAVEN [27], Bongard-LOGO [16], and SVRT [5], most of them have limited input modalities, reasoning paradigms, and restricted problem configurations, which can lead to biased evaluation results [22].\nTo this end, we propose MM-IQ, a comprehensive AVR benchmark comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms. Like human IQ tests, MM-IQ fully eliminates domain-specific and linguistic biases while systematically diversifying problem configurations to prevent pattern memorization, presenting striking challenges for LMMs: even state-of-the-art models achieve only 27.49% accuracy, marginally exceeding random chance (25%) but far below human-level performance (51.27%). This substantial performance chasm highlights the inadequacy of current LMMs in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide. By applying IQ-testing principles to multimodal models, MM-IQ fills a critical gap in existing multimodal benchmarks, e.g., MMBench [10] and MMMU [26] that focus on broad task coverage rather than core reasoning abilities. Our results demonstrate that current architectures lack the intrinsic abstraction abilities necessary for human-like intelligence, shedding light on potential directions toward developing systems capable of genuine cognitive adaptation."}, {"title": "2 Related Work", "content": "Following [14, 7, 13], all existing AVR benchmarks, including our MM-IQ, can be cataloged along three dimensions: input shape, problem configuration, and reasoning paradigm, as shown in Table 1. Input shape refers to the input forms of the objects in the given image, which contributes to evaluating models' cognition abilities of different shapes. Diverse problem configurations assess models' abstract reasoning capabilities across multi-dimensional aspects, including pattern recognition (Raven's Progressive Matrices [17]), analogical transfer ability (Visual Analogy [6]), discrimination ability (Odd-one-out [15]), extrapolation and generalization ability (Visual Extrapolation [24]), and numerical reasoning ability (Arithmetic Reasoning [29]), etc. MM-IQ's inclusion of diverse problem configurations ensures a thorough evaluation of multimodal models' abstract reasoning capabilities across various AVR problems. Reasoning paradigm is a more fine-grained category that evaluates LMMs' abstract reasoning capabilities, like logical deduction, temporal and spatial cognition, geometric, etc. It includes various reasoning paradigms such as temporal movement, spatial relationships, logical operations, and both 2D and 3D geometry, which are based on the internal forms, relationships, and numbers of objects in the given image. Existing benchmarks have only three paradigms on average except for MARVEL, which has five ones, but its quantity is relatively small. Although RAVEN [27], G-set [15], VAP [6], and DOPT [24] have more than 1,000 instances, all of their data are generated by computer programs, which lack diversity and complexity [4]. MM-IQ comprises a total of 2,710 meticulously selected problems, 3x larger than MARVEL, and covers a diverse spectrum of 8 fine-grained reasoning paradigms."}, {"title": "3 Construction of MM-IQ", "content": "Two features distinguish MM-IQ from other existing benchmarks for LMMs: (1) MM-IQ adopts data from professional and authoritative examinations and performs rigorous quality control, which ensures its correctness and validity; (2) MM-IQ is a comprehensive AVR benchmark for evaluating the intelligence of LMMs, comprising a total of 2,710 problems and covering a diverse spectrum of 8 fine-grained reasoning paradigms."}, {"title": "3.1 Data Collection", "content": "The collection of MM-IQ involves three stages. Initially, we examined existing AVR datasets [27, 15, 4, 16] and discovered that most of them are generated by hand-coded procedures. Although programmatic synthesis can produce substantial amounts of data, it often lacks the necessary diversity. Hence, we chose to collect AVR problems from existing resources. Following [9, 7, 28], we collected problems from publicly available questions of the National Civil Servants Examination of China. These problems are specifically designed to evaluate civil servant candidates' critical thinking and problem-solving skills, and they meet our criteria for both quantity and diversity. The collected data underwent a rigorous filtering process conducted by two human annotators to eliminate any low-quality entries. The filtering principle is that the problems can be solved only by the extraction and utilization of high-level abstract reasoning information based on visual inputs.\nTo create a systematic and comprehensive benchmark, we proceeded to the second stage, which involved classifying the data into different paradigms and further adding more problems to those with fewer instances. Based on the descriptions of collected problems, we classified them into the corresponding reasoning paradigms. Additionally, we identified the common attributes of each paradigm's problems, such as attributes and entity types, and supplemented those with fewer instances to ensure that each fine-grained attribute or entity type had sufficient problems.\nThe final stage involved a more thorough cleaning of the collected data through deduplication and extraction of the final answers. We performed deduplication in two ways. The first way was to employ the MD5 hashing algorithm to find the same images and removed them if their input text was the same. Secondly, we utilized the problems' corresponding information, where similar ones were considered suspected duplicates, and then reviewed by human annotators based on the input image and corresponding information to identify and eliminate duplications.\nAdditionally, the final answers were extracted by human annotators to facilitate efficient evaluation later. To further support the development of the open-source community, we also translated all content of questions and answers from Chinese to English based on GPT-4, resulting in a bilingual version of the dataset. All translations were verified by humans to ensure their correctness. Specifically, the data distribution of the reasoning paradigms is shown in the Fig. 13, where concrete object and visual instruction are less than 2% since they are rare in the existing data."}, {"title": "3.2 Reasoning Paradigms of MM-IQ", "content": "For simplicity and consistency, we follow MARVEL, a dataset evaluating LMMs' AVR ability but 3x smaller than ours, and extend its taxonomy to 8 categories, including logical operation, mathematics, 2D-geometry, 3D-geometry, visual instruction, temporal movement, spatial relationship, and concrete object. Notably, we merge mathematical and quantity categories from MARVEL's taxonomy into mathematics to align more closely with our taxonomy.\nLogical Operation refers to the application of logical operators, such as AND (conjunction), OR (disjunction), XOR (exclusive disjunction), etc. This reasoning process involves observing and"}, {"title": "4 Experiments", "content": "We evaluate open-source and closed-source LMMs on the MM-IQ dataset with zero-shot prompting and employ the same question prompt for all models. The few-shot prompting results will be included in the future version of MM-IQ since how to design appropriate multimodal prompts is still an open problem [25, 19]. For open-source LMMs, we select widely used and state-of-the-art models, including QVQ-72B-Preview [21], Qwen2-VL-72B-Instruct [23], Deepseek-VL-7B-Chat [3], and LLaVA-1.6-7B [8]. For closed-source LMMs, we adopt GPT-40-2024-08-06 [1], Gemini-1.5-Pro-002 [20], and Claude-3.5-Sonnet-2024-06-20 [2]. For a fair comparison, we employ the same settings and default hyper-parameters for all LMMs (please refer to Table 3 for more details). Each model generates a single response to each problem in the dataset. The evaluation process of LMMs consists of three steps: (1) response generation, (2) answer extraction, and (3) accuracy calculation. We extract the final answer using regular expression (regex) matching. For example, the final answer will be extracted from the response \"The correct answer is A.\" as \"A\". If there is no valid answer in the model's response, it will be considered incorrect."}, {"title": "4.1 Experimental Setup", "content": "We evaluate open-source and closed-source LMMs on the MM-IQ dataset with zero-shot prompting and employ the same question prompt for all models. The few-shot prompting results will be included in the future version of MM-IQ since how to design appropriate multimodal prompts is still an open problem [25, 19]. For open-source LMMs, we select widely used and state-of-the-art models, including QVQ-72B-Preview [21], Qwen2-VL-72B-Instruct [23], Deepseek-VL-7B-Chat [3], and LLaVA-1.6-7B [8]. For closed-source LMMs, we adopt GPT-40-2024-08-06 [1], Gemini-1.5-Pro-002 [20], and Claude-3.5-Sonnet-2024-06-20 [2]. For a fair comparison, we employ the same settings and default hyper-parameters for all LMMs (please refer to Table 3 for more details). Each model generates a single response to each problem in the dataset. The evaluation process of LMMs consists of three steps: (1) response generation, (2) answer extraction, and (3) accuracy calculation. We extract the final answer using regular expression (regex) matching. For example, the final answer will be extracted from the response \"The correct answer is A.\" as \"A\". If there is no valid answer in the model's response, it will be considered incorrect."}, {"title": "4.2 Overall Performance", "content": "According to the results from Table 2, we have the following conclusions. Firstly, human performance significantly outperforms all LMMs, achieving an average accuracy of 51.27%, while the best LMM Claude-3.5-Sonnet only achieves 27.49%. This substantial gap highlights LMMs' limitations in AVR tasks and underscores the necessity of our MM-IQ dataset. By comparing small LMMs (7B) with larger ones (72B), we find that increased model size improves performance, from an average accuracy of 20.81% to 26.66%. We further compare the performance between open-source and proprietary models and find that the 72B ones (averaging 26.66%) can achieve comparable performance with proprietary models (averaging 27.07%), highlighting the potential of the open-source community.\nSecondly, several noteworthy phenomena are revealed by the more comprehensive analysis of the results across different reasoning paradigms. Among these paradigms, humans and closed-source LMMs perform better in object-concrete reasoning. Humans achieve an accuracy of 65.79%, while GPT-40 achieves 50%. Their scores are significantly higher than other models, especially the open-source ones. The object-concrete reasoning may require additional knowledge since the objects of the images are concrete. This observation may align with MMbench, which argues that proprietary models significantly outperform the open-source ones on tasks requiring additional knowledge, like celebrity recognition, physical property reasoning, natural relation reasoning, etc. The hardest paradigm for LMMs is the logical operation, which only scores at 23.69% average, because the solving of logical operation needs to identify more fine-grained relationships between multiple objects"}, {"title": "4.3 Failure Analysis of LMMs on MM-IQ", "content": "Table 2 demonstrates that the highest accuracy of LMMs (27.49%) is almost equivalent to randomly guessing a correct answer among four options, which motivates us to ask: Does the strongest LMM, e.g., Claude, actually possess the reasoning abilities required by AVR tasks? To investigate this, we selected three representative models: Claude-3.5-Sonnet, Qwen2-VL-72B-Instruct, and LLaVA-1.6-7B, and examined their generated wrong responses through human-in-the-loop evaluation. We sampled a total of 90 predictions from each model for analysis. The 90 problems include 10 instances drawn from each reasoning paradigm and 20 instances from the mathematics paradigm, as the mathematics paradigm is significantly larger than the other paradigms, constituting 34.5% of the entire MM-IQ dataset.\nFirst of all, we take an in-depth look at the average length of predictions and their response styles. Compared to LLaVA-1.6-7B and Qwen2-VL-72B-Instruct, the best-performing LMM, Claude-3.5-Sonnet, tends to generate longer responses. Moreover, Claude-3.5-Sonnet's responses share a consistent structure: they first offer a detailed caption of the given image and the possible abstract reasoning paradigms, and then discuss each option to identify the correct answer. A visual example of Claude-3.5-Sonnet's response is illustrated in Fig. 11. In contrast, LLaVA-1.6-7B and Qwen2-VL-72B-Instruct fail to generate responses in a structured manner. These observations suggest that structured outputs may enhance reasoning performance.\nFurthermore, we examined each wrong response and categorized them into three types: incorrect reasoning, incorrect visual understanding, and incorrect final answers, examples of which can be found in Fig. 15, Fig. 14 and Fig. 17. As shown in Fig. 12, incorrect paradigm reasoning constitutes a major part of failures (32.3% on average). In these responses, we observe that LMMs tend to solve problems by considering simpler wrong rules or focusing on more superficial changes rather than extracting higher-level abstract rules. Examples of simpler rules include objects in the image becoming progressively more compact or dense, and increasingly complex or detailed. A corresponding visualized example is provided in Fig. 15, where the red parts indicate the incorrect reasoning due to wrongly recognizing simpler rules. Enhancing LMMs' ability to perceive more fine-grained image details and accurately identify abstract paradigms is critical to address these shortcomings.\nAdditionally, as shown in Fig. 12, Qwen2-VL-72B-Instruct and LLaVA-1.6-7B exhibit an additional error type compared to Claude-3.5-Sonnet: incorrect final answers, which accounts for nearly one-third of the errors. To further investigate whether the absence of explanations is a critical factor, we calculated the accuracy of all responses without explanations. Notably, for the top-performing model, Qwen2-VL-72B-Instruct, directly providing the final answer results in a performance drop of 4.7% (from 26.9% to 22.5%) on average. Conversely, for LLaVA-1.6-7B, it leads to an improvement of 2.8% (from 19.4% to 22.2%) on average. This underscores that generating detailed reasoning chains helps improve the performance of larger models."}, {"title": "5 Conclusion", "content": "We propose MM-IQ, a comprehensive benchmark for evaluating the abstract visual reasoning of LMMs. It covers a diverse range of 2,710 AVR problems across 8 distinct reasoning paradigms, enabling a rigorous assessment of LMMs' abstraction and reasoning capabilities. Experimental results reveal striking limitations in current state-of-the-art LMMs, with the leading models achieving only slightly above the accuracy of random guessing, far behind human performance. We conduct a thorough failure analysis that identifies several key points for improvement, including structured reasoning, abstract pattern recognition, visual understanding, and inference-time scaling. MM-IQ is expected to complement existing multimodal benchmarks and provide a valuable resource for steering progress in multimodal research and promoting the advancements of AGI."}]}