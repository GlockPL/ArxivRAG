{"title": "Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU", "authors": ["Daliang Xu", "Hao Zhang", "Liming Yang", "Ruiqi Liu", "Gang Huang", "Mengwei Xu", "Xuanzhe Liu"], "abstract": "On-device large language models (LLMs) are catalyzing novel mobile applications such as UI task automation and person-alized email auto-reply, without giving away users' private data. However, on-device LLMs still suffer from unaccept-ably long inference latency, especially the time to first token (prefill stage) due to the need of long context for accurate, personalized content generation, as well as the lack of paral-lel computing capacity of mobile CPU/GPU.\nTo enable practical on-device LLM, we present mllm-NPU, the first-of-its-kind LLM inference system that efficiently leverages on-device Neural Processing Unit (NPU) offloading. Essentially, mllm-NPU is an algorithm-system co-design that tackles a few semantic gaps between the LLM architecture and contemporary NPU design. Specifically, it re-constructs the prompt and model in three levels: (1) At prompt level, it divides variable-length prompts into multiple fixed-sized chunks while maintaining data dependencies; (2) At tensor level, it identifies and extracts significant outliers to run on the CPU/GPU in parallel with minimal overhead; (3) At block level, it schedules Transformer blocks in an out-of-order manner to the CPU/GPU and NPU based on their hardware affinity and sensitivity to accuracy. Compared to competitive baselines, ml1m-NPU achieves 22.4\u00d7 faster prefill speed and 30.7\u00d7 energy savings on average, and up to 32.8\u00d7 speedup in an end-to-end real-world application. For the first time, mllm-NPU achieves more than 1,000 tokens/sec prefilling for a billion-sized model (Qwen1.5-1.8B), paving the way towards practical on-device LLM.", "sections": [{"title": "1 Introduction", "content": "With rising privacy concerns [5], there is growing interest in running Large Language Models (LLMs) locally on mobile devices, known as on-device LLMs, e.g, Apple Intelligence [2] and Android AI Core [1]. Meanwhile, the advancement of mobile-sized language models (1B-10B parameters), such as Qwen2-1.5B and Phi3-3.7B, has demonstrated their ability to perform comparably to significantly larger models like GPT-3, despite having a reduced parameter count [22, 24, 25]. This progress makes the deployment of on-device language models feasible. Without giving away private data, on-device LLM inference catalyzes novel mobile applications, such as UI task automation [51] (e.g., translating users' language commands into UI operations such as \"forward the unread emails to Alice\u201d) and automated message reply [18].\nHowever, the high inference latency remains as a signif-icant obstacle to practical on-device LLMs. To accomplish a UI task, LLM needs to ingest the screen view hierarchy (typically 600-800 tokens [20, 75]) to generate corresponding UI operations step by step [74]. As will be shown in \u00a72.1, each such step takes 8.1 seconds for Qwen1.5-1.8B [25], and thereby more than 40 seconds to finish a 5-step UI task. Similarly, the Gemma-2B model requires 26.7 seconds to automat-ically reply to an email by mimicking the user's tone based on historical email data (with 1500 tokens). Diving into those tasks, we find out that the prompt processing (prefill stage) often dominates the end-to-end inference latency, e.g., 94.4%-98.8% for UI automation tasks. This is because on-device LLM tasks often involve long-context understanding for handle personalized tasks. Unfortunately, existing research efforts primarily focus on accelerating the text generation speed (decoding stage), such as activation sparsity [67, 82] and spec-ulative decoding [46, 55, 83]. Therefore, this work mainly targets improving the prefill speed of on-device LLMs.\nLLM prefilling is compute-bounded [63, 76, 94]; yet, mo-bile CPU and GPU have limited parallel computing capac-ity [38, 85]. Instead, we are motivated by a key opportunity that Neural Processing Units (NPUs) are ubiquitously avail-able in modern mobile devices, e.g., Qualcomm Hexagon NPU and Google Edge TPU. These mobile NPUs are efficient at integer vector operations, delivering computing capability up to 73 TOPS [15]. On CNNs, their improvements over mo-bile CPU/GPUs are demonstrated to be up to 18\u00d7/4\u00d7, respec-tively [78]. Mobile NPUs are also more energy-efficient, and have less workloads contention as compared to CPU/GPU.\nSurprisingly, with such promised advantages, there exists no systems supporting LLM inference on COTS mobile NPUs. Indeed, our preliminary efforts show that directly employing mobile NPUs for LLM inference does not offer performance benefits due to the following challenges.\n\u2022 Costly preparation for variable-length prompts. Mobile NPUs typically support only inference on static shapes, while LLM prompt length is dynamic (with a max context length). Re-preparing and optimizing the LLM execution graph on NPUs for each different-sized prompt is costly on mobile devices (e.g., 11 seconds for the Gemma-2B model). On the"}, {"title": "2 Background", "content": "2.1 On-device LLM Inference Analysis\nOn-device LLMs are increasingly used in cutting-edge sce-narios such as Apple Intelligence [2], UI automation [74], and automated email reply [18], due to the enhanced privacy pro-tections. To empower these applications, many lightweight LLMs have been developed, as summarized in Table 1. How-ever, their inference latency remains a significant challenge. For instance, the Qwen1.5-1.8B model on llama.cpp exhibits delays of 8.1 seconds for one-step UI automation and 21.7 seconds for automated email replies on average, as evalu-ated on the DroidTask [20, 75] and LongBench datasets [26], which is impractical for real-world deployment.\nTo substantiate this observation, we evaluated the Droid-Task (UI automation tasks) and LongBench (context-aware generation tasks) datasets using the Qwen1.5-1.8B model on state-of-the-art device-side LLM engines (llama.cpp), as illustrated in Figure 1. The results confirm that the prefill stage significantly impacts inference time, accounting for 94.4% to 98.8% of the total latency. As the prompt length increases, the prefill stage's proportion of the total infer-ence time also rises. Several factors contribute to this situa-tion: (1) Mobile CPUs/GPUs lack the parallelism capabilities of cloud GPUs [38, 85], being primarily designed for han-dling application logic or rendering tasks. (2) Mobile LLM tasks often require long prompts for personalized, context-aware generation. For instance, automated email replies may need extensive user data, such as historical emails, schedules, and location information (exceeding 1000 tokens), while on-device LLMs handling UI automation must process extensive UI annotation tokens (XML or HTML) and user commands. (3) Mobile LLMs now support long context windows. For instance, recent models like Qwen2-1.5B can accommodate context windows of up to 32K tokens, as illustrated in Table 1.\n2.2 Opportunity: Mobile NPUs\nTo optimize prefill latency, mllm-NPU leverages a key op-portunity: modern mobile SoCs ubiquitously include mobile neural processing units (NPUs) that are well-suited for in-teger operations, such as INT8-based matrix multiplication. Table 2 summarizes the specifications of well-known mo-bile NPUs provided by mainstream vendors. For example, Qualcomm's mobile SoCs feature Hexagon NPUs, achieving up to 73 trillion INT8 operations per second. According to AI-Benchmark [16], the Hexagon NPU in the Xiaomi 14 can infer the MobileNet-V2 model in just 0.6 ms, 23\u00d7 faster than a mobile CPU and 3.2\u00d7 faster than a mobile GPU.\nMobile NPU architecture and micro experiments. Mo-bile NPUs deliver significant performance benefits by single instruction multiple data (SIMD) architecture. For instance, Hexagon NPUs support 1024-bit INT8 vector arithmetic, al-lowing multiple SIMD instructions to execute in parallel. However, their floating-point computation capabilities are relatively weak compared to mobile GPUs. With clock fre-quencies between 500 and 750 MHz, mobile NPUs are more energy-efficient than mobile CPUs and GPUs. Additionally, unlike cloud GPUs that have separate physical memory, mo-bile NPUs are integrated within mobile SoCs, sharing the same physical memory with mobile CPUs, eliminating the need for memory copying during NPU execution.\nTo evaluate the performance of INT8 MatMul on mobile NPUs, we conducted preliminary experiments on the Xiaomi 14 using MatMul sizes commonly used in mobile LLMs. INT8 MatMul on mobile NPUs achieved a 4.5-5.8\u00d7 speedup com-pared to CPU INT8 and a significant improvement over GPU FP16. The performance gains increase with larger compu-tational workloads. However, performing FP16 MatMul on"}, {"title": "2.3 Gaps between LLMs and Mobile NPUs", "content": "Given its inherent advantages, we are surprised to find that none of existing DNN engines support LLM acceleration on mobile NPUs. We then dig into the underlying reasons and find a huge gap between existing mobile NPUs design and LLM inference pipeline.\n\u2022 LLM prefill phase relies on variable-length prompts, leading to excessive time spent on building and com-piling the NPU graph. As illustrated in Figure 2, before the compute graph can be executed on the mobile NPU, it must be built and optimized, a process taking tens of sec-onds. For instance, building the graph for the Gemma 2B model using QNN framework takes 360 ms, and graph opti-mization requires 11.54 seconds. Unlike CNN models, which are built and optimized once and can be executed multi-ple times with the same input shape, the LLM prefill phase must handle variable-length prompts, necessitating rebuild-ing and re-optimization of compute graphs for each inference. Consequently, using mobile NPUs in this scenario offers no performance benefit and is often slower than using a CPU.\n\u2022 The existence of activation outliers makes LLM diffi-cult to quantize at whole-tensor level, yet a more fine-grained group-level quantization hampers NPU effi-ciency. Our preliminary experiments, shown in Figure 4, in-dicate that two popular quantization algorithms (K-Quant [54]"}, {"title": "3 mllm-NPU Design", "content": "3.1 Overview of ml1m-NPU\nDesign goal. m11m-NPU aims to reduce prefill latency and en-ergy consumption for mobile-sized LLMs through on-device NPU offloading. It supports various mobile-sized LLMs on devices and can be integrated as part of LLM-as-a-System-Service in mobile OS or mobile application services [87, 89].\nWorkflow. Figure 6 illustrates the workflow of ml1m-NPU. The key idea of mllm-NPU is to maximize its execution on mobile NPU for integer operation acceleration; while keep necessary floating point operations on CPU/GPU to not com-promise accuracy. To enable more efficient NPU offloading,"}, {"title": "3.2 Chunk-sharing graph execution", "content": "To tackle the dynamic prompt length challenge, an intuitive solution is to set a fixed length compute graph ahead and use padding [3, 61, 70]. However, this method lacks flexibility and excessive padding wastes compute resources.\nChunk-wise prefill. To enhance flexibility and minimize padding for variable-length prompts, we recognize that pro-cessing a long prompt in a LLM is equivalent to processing several split sub-prompts, or \"chunks\", causally. This is fea-sible because popular LLMs use a decoder-only architecture, where the result of the i-th token depends only on the pre-ceding tokens. To that end, ml1m-NPU first pre-builds and pre-optimizes fixed-length chunk-based NPU compute graph at the preparation stage. During inference, ml1m-NPU splits"}, {"title": "3.3 Shadow outlier execution", "content": "To enable NPU-friendly, per-tensor activation quantization without compromising LLM accuracy, mllm-NPU adopts a novel approach termed shadow outlier execution. As shown in Figure 9, ml1m-NPU extracts the activation channels with outliers at runtime into a more compact tensor, executes it on CPU, and merges it back to the outcome of original operator on NPU. This procedure can be formulated as follows:\n$S_w = \\begin{cases}\n  \\min \\big[\\max (-127), \\frac{x}{s}*128 \\big], 128\\big] + O_w & \\text{on NPU}\\\\\n  \\min \\big[\\max (-, \\frac{x}{s}*128 \\big], 128\\big] & \\text{on CPU}\n\\end{cases}$ (1)\nwhere x, w, s, O, and extract represent the original float activation, INT8 weights, the quantization scale factor, the MatMul operation, and the function of extracting activation outliers into a more compact tensor, respectively. Specifically, the MatMulow can be equivalently divided into the sum of two parts according to the associative law: (1) Mobile NPU for MatMul within the scale. m11m-NPU first quantizes and rounds x to the range of -127 to 128 based on the scale factor s. It then obtains intermediate results by performing a standard W8A8 per-tensor MatMul with weights w. (2) Mobile CPUs/GPUs for MatMul beyond the scale. mllm-NPU calculates the partial values exceeding s. Since these outliers are rare, mllm-NPU extracts these values from the tensor, compresses them into a dense tensor, and performs a MatMul with weights w.\nSince outliers are very sparse (around 5-15 channels, ac-counting for only 0.1%-0.3% of total channels, as shown in Figure 10), the shadow execution on CPU is much faster than the execution of original tensor on NPU, and its execu-tion time can be totally hidden by overlapping. To further minimize the overhead of this extra process, mllm-NPU deter-mines an outlier threshold (i.e., s in Equation 1) by profiling a large corpora at offline, thereby can identify the outliers by simply comparing the activation numbers to this thresh-old. The design of shadow outlier execution is compatible"}, {"title": "3.4 Out-of-order subgraph execution", "content": "As elaborated in (\u00a72.3), LLM quantization algorithms cannot fully eliminate floating point operations, ml1m-NPU thereby divides its execution flow into NPU and CPU/GPU collab-oratively. Typically, LayerNorm, Attention, as well as the shadow outlier computation are placed on the CPU/GPU; while the other linear layers are processed on the NPU. How-ever, we found simply overlapping their execution is ineffi-cient, resulting in large execution bubbles (37% bubble rate in critical path), as illustrated in Figure 13(a).\nOut-of-order execution. To reduce these execution bub-bles, ml1m-NPU is guided by a key insight that, after being partitioned at both chunk and subgraphs levels, the LLM sub-graphs can be scheduled in an out-of-order manner. More specifically, any input-ready subgraph can be executed with-out strictly following the chunk sequence. For instance, the first subgraph of the third chunk (C3-Graph1) can be exe-cuted during the bubble period when C2-Graph1 finishes.\nTo preserve correctness, mllm-NPU considers two types of dependency: (1) Cross-chunk dependency. Operators like Attention rely on data from previous chunks. This means the i-th chunk j-th subgraph Gi,j depends on the j \u2013 1-th subgraph of the 0, 1, . . ., i \u2212 1 chunks:\n$G_{i,j} \\leftarrow G_{o,j-1}, G_{1, j\u22121}, ..., G_{i, j\u22121}$ (2)\n(2) Intra-chunk dependency. Operators like LayerNorm, Lin-ear, and Quantize rely only on previous subgraphs within the same chunk. Therefore, the i-th chunk's j-th subgraph Gi,j depends on the j \u2013 1-th subgraph of the same chunk:\n$G_{i,j} \\leftarrow G_{i,j-1}$ (3)\nAs mobile processors are weak at parallelism and preemp-tion [38, 78, 85], to ensure efficiency, a processor is capable of executing only one subgraph at any given time.:\n$\\sum_{i=0}^{N} \\sum_{j=0}^{M} P_{i,j,t} = 1, \\forall t$ (4)\nwhere Pi,j,t = 1 indicates that subgraph Gi,j is running on processor P at time t, and N and M represent the maximum number of chunks and subgraphs, respectively. mllm-NPU aims to find an execution order minimizing the total execu-tion time of all subgraphs under these constraints. Unfortu-nately, this scheduling problem can be reduced to a classical NP-Hard Traveling Salesman Problem [42]. Moreover, be-cause the number of chunks varies with user prompts, an optimal scheduling strategy cannot be generated offline.\nInstead, mllm-NPU utilizes an online heuristic algorithm. The key idea is to focus not on the execution time of the subgraph g, but on how executing g contributes to reducing NPU stalls, motivated by the observation that during the prefill phase, NPU execution time often dominates inference latency, being the critical path. For instance, with a prompt length of 256 using the Qwen1.5-1.8B model, NPU execution takes 315ms, about twice that of the CPU.\nSpecifically, we define a subgraph g's contribution to re-duce NPU stalls as follows: If subgraph g is to be executed on the CPU/GPU, let S be the set of new subgraphs that can be executed after g is completed. S will be executed on the NPU. A longer execution time of S is beneficial for reducing NPU stalls. Thus, g's contribution is defined as the total execution time of S. Conversely, if g is executed on the NPU, a shorter execution time of S is beneficial, with the negative value of S's execution time as g's contribution, formulated as:\n$C = \\begin{cases} \n  \\sum T_i, \\forall i \\in S & \\text{if g is on the CPU/GPU}\\\\\n  -\\sum T_i, \\forall i \\in S & \\text{if g is on the NPU}\n\\end{cases}$ (5)\nwhere T is the subgraph execution time. mllm-NPU always chooses the subgraph with the largest C, meaning the sub-graph g with S having the longest execution time on the NPU or the shortest execution time on the CPU/GPU.\nIn a nut shell, ml1m-NPU profiles all the subgraph execu-tion time and their dependency offline at the preparation stage. During the prefill stage, it calculates all the pending subgraphs C value and selects one with maximum C to run, with microsecond-level performance overhead."}, {"title": "4 Implementation and Evaluation", "content": "We have fully implemented mllm-NPU for Qualcomm Hexagon NPUs, comprising 10K lines of code in C/C++ and assembly language. We choose Qualcomm SoCs as the target platform for its popularity on mobile devices and powerful NPU ca-pacity. Qualcomm Hexagon is also the only mobile NPU with an open instruction set architecture. mllm-NPU is built on the MLLM [21], one state-of-the-art mobile LLM engines, and QNN framework [23], the Qualcomm Neural Processing SDK. It supports standard LLM formats exported from Hug-ging Face [19]. To facilitate LLM execution, we implemented specific operators like KVCache, SiLU, RMSNorm, ROPE, and etc, in addition to what have been supported by QNN. To reduce context switching overhead between CPUs/GPUs and NPUs, ml1m-NPU leverages shared buffers to synchronize in-termediate results from different processors. For end-to-end inference, mllm-NPU is compatible with any decoding engine and utilizes the MLLM CPU backend for decoding stage as easy implementation, with a default chunk length of 256. The default pruning rate for outlier layers is 85%.\nThe prototype further incorporates two optimizations. (1) Our extensive experiments show that mobile NPUs ofte fa-vor tensor sizes (e.g., equal \u201cheight\u201d and \u201cwidth\u201d) in CNN architectures. For example, a linear layer with weights of 2048x2048 produces the same results for inputs of 1024\u00d71\u00d72048 and 32\u00d732\u00d72048, but using 32\u00d732\u00d72048 reduces execution latency by 1.62\u00d7. Therefore, mllm-NPU profiles all possible equivalent shapes for linear layers during the preparation stage and selects the most efficient one. (2) Mobile NPUs typ-ically access limited memory regions (e.g., 4GB for Hexagon NPU), which can be smaller than the size of LLM weights. To maximize prefill acceleration within this limited memory, mllm-NPU prioritizes executing computationally intensive tasks, such as FFN, on the NPU to enhance efficiency.\n4.1 Experiment setups\nHardware setup. We test mllm-NPU on two smartphones with different Qualcomm SoCs: Xiaomi 14 (Snapdragon 8gen3,"}, {"title": "4.2 Prefill performance.", "content": "We evaluate the ml1m-NPU's prefill performance (speed and energy consumption) at prompt lengths of 64, 256 and 1024 tokens on two devices, as shwon in Figure 14 and 15. Despite outlier variations across datasets, the overall impact on prefill performance is minimal, so we report results from the Long-Bench dataset. The results show that ml1m-NPU consistently outperforms all baselines across both metrics, with benefits becoming more pronounced as prompt length increases.\nPrefill speed. For prompt length of 1024 tokens, mllm-NPU can reduce prefill latency by 18.17\u201338.4\u00d7, 7.3x, 32.5-43.6\u00d7, and 1.27-2.34\u00d7 on Xiaomi 14 compared with llama.cpp-CPU, MNN-CPU, MLC-GPU, TFLite-GPU, respectively. On the Redmi K60 Pro, these improvements are 21.3-41.3\u00d7, 7.43x, 37.2-69.3x, and 1.3-2.6\u00d7, respectively. These speedups are due to mllm-NPU's use of three innovative techniques that fully leverage mobile NPUs, including shadow outlier exe-cution, high-efficiency per-tensor MatMul, and out-of-order subgraph execution. Compared with PowerInfer-V2-NPU, a baseline also using NPU for prefilling, ml1m-NPU can achieve 3.28-5.32x and 3.4-5.6\u00d7 speedup on two devices, respec-tively, by employing NPU-friendly INT8 linear computation and fine-grained subgraph scheduling (\u00a73.4).\nFor prompt length of 64 tokens, the prefill speed of m11m-NPU is 14.86-7.10x, 1.69x, 10.91-17.32x, 1.48x, and 1.81-2.51\u00d7 faster than llama.cpp-CPU, MNN-CPU, MLC-GPU, TFLite-GPU, and PowerInfer-V2-NPU respectively, with speedups averaging 10.5\u00d7, 4.31\u00d7, 2.68\u00d7, 1.02\u00d7, and 1.96\u00d7 lower than those for 1024-token prompts. This is because a shorter prompt can lead to a padding problem and limit ml1m-NPU's out-of-order execution scheduling efficiency.\nPrefill energy consumption. Energy consumption was evaluated on the Redmi K60 Pro, the only rootable device. PowerInfer-V2 was excluded due to the lack of energy con-sumption data and open-source code. For 1024-token prompts, mllm-NPU reduces energy consumption by 35.63\u201359.52\u00d7, 35.21-59.25x, and 1.85-4.32\u00d7 compared to llama.cpp-CPU, MLC-GPU, and TFLite-GPU, respectively. For 64-token prompts, the savings are 10.38-14.12x, 10.38-17.79\u00d7, and 3.22-3.67\u00d7, respectively. These savings are due to the high energy ef-ficiency of mobile NPUs and mllm-NPU's three novel tech-niques for maximizing NPU performance."}, {"title": "4.3 End-to-end performance", "content": "We evaluate the real-world performance of mllm-NPU against baseline systems using two workloads: UI automation on DroidTask datasets and context-aware automated email replies on LongBench datasets. The end-to-end inference latency results are shown in Table 4. Our key observation is that mllm-NPU always achieves the lowest inference latency across all four datasets.\nFor LongBench datasets, mllm-NPU shows significant speed improvements: 23.0-46.2\u00d7 over llama.cpp-CPU, 16.5-36.4\u00d7 over MLC-LLM-GPU, 4.08-4.19\u00d7 over MNN-CPU, 3.51-3.73\u00d7 over PowerInfer-V2-NPU, and 1.27-2.03\u00d7 over TFLite-GPU. This impressive performance is primarily due to mllm-NPU's superior efficiency during the prefill stage. The speedup against TFLite-GPU is lower since mllm-NPU currently re-lies on a CPU backend for decoding with no optimization, while TFLite utilizes GPU. Notably, ml1m-NPU is compatible with any decoding engine, which means once TFLite is open-sourced, mllm-NPU can integrate it as the decoding backend, potentially enhancing performance further.\nFor the DroidTask datasets, ml1m-NPU reduces end-to-end inference latency by 7.9-12.5\u00d7 compared to llama.cpp-CPU, 15.0-32.8x compared to MLC-LLM-GPU, 2.38-2.45\u00d7 com-pared to MNN-CPU, 2.27-2.44\u00d7 compared to PowerInfer-V2-NPU, and 1.35-2.38\u00d7 compared to TFLite-GPU. The perfor-mance gains are slightly smaller for DroidTask datasets due to shorter prompts in UI automation versus email writing."}, {"title": "4.4 Inference accuracy", "content": "We investigate the inference accuracy of mllm-NPU on five LLM benchmarks: LAMBADA [62], HellaSwag [90], Wino-Grande [4], OpenBookQA [56] and MMLU [41]. For compar-ison, we evaluated 4 alternatives: FP16 (non-quantization),"}, {"title": "4.5 Memory consumption", "content": "We compare ml1m-NPU with INT8 weight baselines, as mo-bile NPUs only support INT8 weight computations. Memory consumption results on the Redmi K60 Pro, using a 512-token prompt, are presented in Figure 16. m11m-NPU consumes up to 1.32 \u00d7 more memory than llama.cpp and TFLite. The over-head is due to the MLLM and QNN frameworks, which al-locate independent activation buffers for each operator to enhance speed. The tiny additional memory overhead in-troduced by mllm-NPU is its \u00a73.3 shadow outlier execution technique (in black), which loads tiny float weights into memory, accounting for only 0.6%-1% of the total memory."}, {"title": "4.6 Ablation study.", "content": "We conduct a comprehensive breakdown analysis of the ben-efits brought by each of mllm-NPU's techniques using the Qwen1.5-1.8B, Gemma-2B, and LlaMA2-7B models, as shown"}, {"title": "5 Related work", "content": "On-device LLM optimization. LLMs are resource-hungry, especially when long context is needed [81, 91]. To reduce the substantial memory consumption of on-device LLM infer-ence, various compression techniques have been proposed, including quantization and knowledge distillation [33, 36, 43, 59, 73, 79, 84, 86, 88]. \u03a4\u03bf minimize on-device LLM com-putation, researchers have introduced token pruning [27, 29, 47, 65, 72], which prunes unnecessary tokens during the inference process. Speculative decoding, a method that accel-erates token generation by offloading tasks to a smaller LLM, has been widely adopted in open-source frameworks [3, 54] and extensively researched [30, 34, 40, 46, 55, 83]. Beyond inference, on-device LLM training (especially fine-tuning) is also gaining attentions in mobile research [28, 80] As a sys-tem optimization, mllm-NPU is orthogonal and compatible with these algorithm-level optimizations.\nOn-chip offloading for ML. This has been thoroughly stud-ied to enable faster DNN inference by leveraging heteroge-neous mobile processors like GPUs and NPUs [35, 37, 39, 48-50, 57, 60, 78, 82, 92, 93]. MobiSR [50] utilizes mobile NPUs to speed up super-resolution computation. However, these methods do not address LLM-specific features and are unsuit-able for on-device LLM scenarios. The most relevant work is PowerInfer-V2 [82], which also utilizes mobile NPUs for the prefilling, but mostly focuses on LLM inference with"}, {"title": "6 Conclusions", "content": "This paper has proposed mllm-NPU, the first LLM inference system utilizing on-device NPU offloading to reduce prefill la-tency and energy consumption. mllm-NPU has incorporated novel techniques: chunk-sharing graph, shadow outlier exe-cution and out-of-order subgraph execution to enhance NPU offloading efficiency. Extensive experiments have demon-strated m11m-NPU to show its superior performance benefits, e.g, up to 43.6x speedup and 59.5\u00d7 energy savings."}]}