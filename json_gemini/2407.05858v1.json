{"title": "Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU", "authors": ["Daliang Xu", "Hao Zhang", "Liming Yang", "Ruiqi Liu", "Gang Huang", "Mengwei Xu", "Xuanzhe Liu"], "abstract": "On-device large language models (LLMs) are catalyzing novel\nmobile applications such as UI task automation and person-\nalized email auto-reply, without giving away users' private\ndata. However, on-device LLMs still suffer from unaccept-\nably long inference latency, especially the time to first token\n(prefill stage) due to the need of long context for accurate,\npersonalized content generation, as well as the lack of paral-\nlel computing capacity of mobile CPU/GPU.\nTo enable practical on-device LLM, we present mllm-NPU,\nthe first-of-its-kind LLM inference system that efficiently\nleverages on-device Neural Processing Unit (NPU) offloading.\nEssentially, mllm-NPU is an algorithm-system co-design that\ntackles a few semantic gaps between the LLM architecture\nand contemporary NPU design. Specifically, it re-constructs\nthe prompt and model in three levels: (1) At prompt level,\nit divides variable-length prompts into multiple fixed-sized\nchunks while maintaining data dependencies; (2) At tensor\nlevel, it identifies and extracts significant outliers to run\non the CPU/GPU in parallel with minimal overhead; (3) At\nblock level, it schedules Transformer blocks in an out-of-order\nmanner to the CPU/GPU and NPU based on their hardware\naffinity and sensitivity to accuracy. Compared to competitive\nbaselines, ml1m-NPU achieves 22.4\u00d7 faster prefill speed and\n30.7\u00d7 energy savings on average, and up to 32.8\u00d7 speedup\nin an end-to-end real-world application. For the first time,\nmllm-NPU achieves more than 1,000 tokens/sec prefilling\nfor a billion-sized model (Qwen1.5-1.8B), paving the way\ntowards practical on-device LLM.", "sections": [{"title": "1 Introduction", "content": "With rising privacy concerns [5], there is growing interest\nin running Large Language Models (LLMs) locally on mobile\ndevices, known as on-device LLMs, e.g, Apple Intelligence [2]\nand Android AI Core [1]. Meanwhile, the advancement of\nmobile-sized language models (1B-10B parameters), such as\nQwen2-1.5B and Phi3-3.7B, has demonstrated their ability\nto perform comparably to significantly larger models like\nGPT-3, despite having a reduced parameter count [22, 24, 25].\nThis progress makes the deployment of on-device language\nmodels feasible. Without giving away private data, on-device\nLLM inference catalyzes novel mobile applications, such as\nUI task automation [51] (e.g., translating users' language\ncommands into UI operations such as \"forward the unread\nemails to Alice\u201d) and automated message reply [18].\nHowever, the high inference latency remains as a signif-\nicant obstacle to practical on-device LLMs. To accomplish\na UI task, LLM needs to ingest the screen view hierarchy\n(typically 600-800 tokens [20, 75]) to generate corresponding\nUI operations step by step [74]. As will be shown in \u00a72.1,\neach such step takes 8.1 seconds for Qwen1.5-1.8B [25], and\nthereby more than 40 seconds to finish a 5-step UI task. Simi-\nlarly, the Gemma-2B model requires 26.7 seconds to automat-\nically reply to an email by mimicking the user's tone based\non historical email data (with 1500 tokens). Diving into those\ntasks, we find out that the prompt processing (prefill stage)\noften dominates the end-to-end inference latency, e.g., 94.4%-\n98.8% for UI automation tasks. This is because on-device LLM\ntasks often involve long-context understanding for handle\npersonalized tasks. Unfortunately, existing research efforts\nprimarily focus on accelerating the text generation speed\n(decoding stage), such as activation sparsity [67, 82] and spec-\nlative decoding [46, 55, 83]. Therefore, this work mainly\ntargets improving the prefill speed of on-device LLMs.\nLLM prefilling is compute-bounded [63, 76, 94]; yet, mo-\nbile CPU and GPU have limited parallel computing capac-\nity [38, 85]. Instead, we are motivated by a key opportunity\nthat Neural Processing Units (NPUs) are ubiquitously avail-\nable in modern mobile devices, e.g., Qualcomm Hexagon\nNPU and Google Edge TPU. These mobile NPUs are efficient\nat integer vector operations, delivering computing capability\nup to 73 TOPS [15]. On CNNs, their improvements over mo-\nbile CPU/GPUs are demonstrated to be up to 18\u00d7/4\u00d7, respec-\ntively [78]. Mobile NPUs are also more energy-efficient, and\nhave less workloads contention as compared to CPU/GPU.\nSurprisingly, with such promised advantages, there exists\nno systems supporting LLM inference on COTS mobile NPUs.\nIndeed, our preliminary efforts show that directly employing\nmobile NPUs for LLM inference does not offer performance\nbenefits due to the following challenges.\n\u2022 Costly preparation for variable-length prompts. Mobile\nNPUs typically support only inference on static shapes, while\nLLM prompt length is dynamic (with a max context length).\nRe-preparing and optimizing the LLM execution graph on\nNPUs for each different-sized prompt is costly on mobile\ndevices (e.g., 11 seconds for the Gemma-2B model). On the"}, {"title": "2 Background", "content": "2.1 On-device LLM Inference Analysis\nOn-device LLMs are increasingly used in cutting-edge sce-\nnarios such as Apple Intelligence [2], UI automation [74], and\nautomated email reply [18], due to the enhanced privacy pro-\ntections. To empower these applications, many lightweight\nLLMs have been developed, as summarized in Table 1. How-\never, their inference latency remains a significant challenge.\nFor instance, the Qwen1.5-1.8B model on llama.cpp exhibits\ndelays of 8.1 seconds for one-step UI automation and 21.7\nseconds for automated email replies on average, as evalu-\nated on the DroidTask [20, 75] and LongBench datasets [26],\nwhich is impractical for real-world deployment.\nTo substantiate this observation, we evaluated the Droid-\nTask (UI automation tasks) and LongBench (context-aware\ngeneration tasks) datasets using the Qwen1.5-1.8B model\non state-of-the-art device-side LLM engines (llama.cpp), as\nillustrated in Figure 1. The results confirm that the prefill\nstage significantly impacts inference time, accounting for\n94.4% to 98.8% of the total latency. As the prompt length\nincreases, the prefill stage's proportion of the total infer-\nence time also rises. Several factors contribute to this situa-\ntion: (1) Mobile CPUs/GPUs lack the parallelism capabilities\nof cloud GPUs [38, 85], being primarily designed for han-\ndling application logic or rendering tasks. (2) Mobile LLM\ntasks often require long prompts for personalized, context-\naware generation. For instance, automated email replies may\nneed extensive user data, such as historical emails, schedules,\nand location information (exceeding 1000 tokens), while on-\ndevice LLMs handling UI automation must process extensive\nUI annotation tokens (XML or HTML) and user commands.\n(3) Mobile LLMs now support long context windows. For\ninstance, recent models like Qwen2-1.5B can accommodate\ncontext windows of up to 32K tokens, as illustrated in Table 1.\n2.2 Opportunity: Mobile NPUs\nTo optimize prefill latency, mllm-NPU leverages a key op-\nportunity: modern mobile SoCs ubiquitously include mobile\nneural processing units (NPUs) that are well-suited for in-\nteg er operations, such as INT8-based matrix multiplication.\nTable 2 summarizes the specifications of well-known mo-\nbile NPUs provided by mainstream vendors. For example,\nQualcomm's mobile SoCs feature Hexagon NPUs, achieving\nup to 73 trillion INT8 operations per second. According to\nAI-Benchmark [16], the Hexagon NPU in the Xiaomi 14 can\ninfer the MobileNet-V2 model in just 0.6 ms, 23\u00d7 faster than\na mobile CPU and 3.2\u00d7 faster than a mobile GPU.\nMobile NPU architecture and micro experiments. Mo-\nbile NPUs deliver significant performance benefits by single\ninstruction multiple data (SIMD) architecture. For instance,\nHexagon NPUs support 1024-bit INT8 vector arithmetic, al-\nlowing multiple SIMD instructions to execute in parallel.\nHowever, their floating-point computation capabilities are\nrelatively weak compared to mobile GPUs. With clock fre-\nquencies between 500 and 750 MHz, mobile NPUs are more\nenergy-efficient than mobile CPUs and GPUs. Additionally,\nunlike cloud GPUs that have separate physical memory, mo-\nbile NPUs are integrated within mobile SoCs, sharing the\nsame physical memory with mobile CPUs, eliminating the\nneed for memory copying during NPU execution.\nTo evaluate the performance of INT8 MatMul on mobile\nNPUs, we conducted preliminary experiments on the Xiaomi\n14 using MatMul sizes commonly used in mobile LLMs. INT8\nMatMul on mobile NPUs achieved a 4.5-5.8\u00d7 speedup com-\npared to CPU INT8 and a significant improvement over GPU\nFP16. The performance gains increase with larger compu-\ntational workloads. However, performing FP16 MatMul on"}, {"title": "2.3 Gaps between LLMs and Mobile NPUs", "content": "Given its inherent advantages, we are surprised to find that\nnone of existing DNN engines support LLM acceleration on\nmobile NPUs. We then dig into the underlying reasons and\nfind a huge gap between existing mobile NPUs design and\nLLM inference pipeline.\n\u2022 LLM prefill phase relies on variable-length prompts,\nleading to excessive time spent on building and com-\npiling the NPU graph. As illustrated in Figure 2, before\nthe compute graph can be executed on the mobile NPU, it\nmust be built and optimized, a process taking tens of sec-\nonds. For instance, building the graph for the Gemma 2B\nmodel using QNN framework takes 360 ms, and graph opti-\nmization requires 11.54 seconds. Unlike CNN models, which\nare built and optimized once and can be executed multi-\nple times with the same input shape, the LLM prefill phase\nmust handle variable-length prompts, necessitating rebuild-\ning and re-optimization of compute graphs for each inference.\nConsequently, using mobile NPUs in this scenario offers no\nperformance benefit and is often slower than using a CPU.\n\u2022 The existence of activation outliers makes LLM diffi-\ncult to quantize at whole-tensor level, yet a more fine-\ngrained group-level quantization hampers NPU effi-\nciency. Our preliminary experiments, shown in Figure 4, in-\ndicate that two popular quantization algorithms (K-Quant [54]"}, {"title": "3 mllm-NPU Design", "content": "3.1 Overview of ml1m-NPU\nDesign goal. m11m-NPU aims to reduce prefill latency and en-\nergy consumption for mobile-sized LLMs through on-device\nNPU offloading. It supports various mobile-sized LLMs on\ndevices and can be integrated as part of LLM-as-a-System-\nService in mobile OS or mobile application services [87, 89].\nWorkflow. Figure 6 illustrates the workflow of ml1m-NPU.\nThe key idea of mllm-NPU is to maximize its execution on\nmobile NPU for integer operation acceleration; while keep\nnecessary floating point operations on CPU/GPU to not com-\npromise accuracy. To enable more efficient NPU offloading,\nmllm-NPU re-constructs the prompt and model in following\nways: (1) At prompt level: variable-length prompt is reduced\ninto multiple fixed-sized chunks with data dependency pre-\nserved; (2) At block level: Transformer block is scheduled into\nCPU/GPU and NPU based on their hardware infinity and\naccuracy sensitivity; (3) At tensor level: important outliers\nare identified and extracted to run on CPU/GPU.\n\u2022 Preparation stage. mllm-NPU first uses an enhanced per-\ntensor quantization algorithm to quantize LLMs into W8A8\nformat. The quantization algorithm differs from existing ones\nas it filters out most unimportant activation outliers and\nextracts the rest of them into independent, lightweight oper-\nators that are complementary to the original one. mllm-NPU\nalso generates fixed-length chunk-sharing graphs (\u00a73.2) to\nefficiently handle variable-length prompts.\n\u2022 Execution stage. When receiving a prompt, ml1m-NPU di-\nvides it into fixed-sized chunks and processes them causally.\nThese chunk graphs will be split into subgraphs scheduled\nonto the CPU/GPU and NPU according to their data for-\nmats for efficient execution. To preserve accuracy, certain\nINT8-based linear layers undergo sparse float outlier shadow\nexecution on the CPU/GPU in parallel to compensate for\nquantization errors from outliers (\u00a73.3). To enhance execu-\ntion efficiency, m11m-NPU judiciously schedules the chunks\nin out-of-order manner (\u00a73.4)."}, {"title": "3.2 Chunk-sharing graph execution", "content": "To tackle the dynamic prompt length challenge, an intuitive\nsolution is to set a fixed length compute graph ahead and use\npadding [3, 61, 70]. However, this method lacks flexibility\nand excessive padding wastes compute resources.\nChunk-wise prefill. To enhance flexibility and minimize\npadding for variable-length prompts, we recognize that pro-\ncessing a long prompt in a LLM is equivalent to processing\nseveral split sub-prompts, or \"chunks\", causally. This is fea-\nsible because popular LLMs use a decoder-only architecture,\nwhere the result of the i-th token depends only on the pre-\nceding tokens. To that end, ml1m-NPU first pre-builds and\npre-optimizes fixed-length chunk-based NPU compute graph\nat the preparation stage. During inference, ml1m-NPU splits\nlong prompts into several chunks and processes them using\nthese pre-built chunk graphs, as illustrated in Figure 7(b).\nHowever, solely using chunk graphs is not scalable, as\nm11m-NPU would need to store numerous distinct chunk\ngraphs in memory, significantly increasing memory over-\nhead. This is because different chunk graphs have attention\noperators of varying sizes. For instance, considering a prompt\nlength of 1024 and a chunk length of 32, the QKV dimension\nsizes of the attention operators for the first chunk are all\n32 * hds, while for the last chunk they are 32 * hds, 1024 * hds,\nand 1024 * hds respectively, as shown in Figure 7(b).\nChunk-sharing graph. mllm-NPU introduces a chunk-\nsharing graph, shown in Figure 7(c), based on the insight\nthat LLM operators fall into two distinct categories: (1) static\noperators (in green), such as Linear and LayerNorm, which\ndepend only on the chunk length and can be shared across\ndifferent chunks; and (2) dynamic operators (in red), such as\nAttention, which depend on both chunk length and chunk\nsequence and cannot be shared among different chunks. Con-\nsequently, ml1m-NPU divides the LLM into several subgraphs\nbased on the shareability of operators. The shared subgraphs\nare built and optimized once, whereas non-shared subgraphs\nare constructed individually for different chunks. During the\nprefill phase, activations from different chunks pass through\nthe same static operator subgraphs while dynamically select-\ning the appropriate dimension-specific dynamic operators.\nThis method significantly reduces memory overhead and en-\nhances scalability, as most dynamic operators, like Attention,\ndo not contain weights, requiring only activation buffers.\nOur experiments show that 120 out of 144 subgraphs can\nbe shared in Qwen1.5-1.8B models, reducing memory con-\nsumption by up to 75% (7.2GB) for a prompt length as 1024\nand a chunk length as 256.\nWe also conducted extensive experiments on selecting\na proper chunk length. The results of two popular LLMs\n(Qwen1.5-1.8B and Gemma-2B) on Xiaomi 14 device is il-\nlustrated in Figure 8. Based on the observations, ml1m-NPU\nempirically chooses a chunk length of 256 for Xiaomi 14\ndevice, which effectively utilizes the capabilities of mobile\nNPUs while reducing intra-chunk padding. In practice, such\nprofiling needs to be performed across different NPUs."}, {"title": "3.3 Shadow outlier execution", "content": "To enable NPU-friendly, per-tensor activation quantization\nwithout compromising LLM accuracy, mllm-NPU adopts a\nnovel approach termed shadow outlier execution. As shown\nin Figure 9, ml1m-NPU extracts the activation channels with\noutliers at runtime into a more compact tensor, executes it on\nCPU, and merges it back to the outcome of original operator\non NPU. This procedure can be formulated as follows:\n$$S_w = \\begin{cases}\nmin [max (-127), x \\cdot s \\cdot w] \\times [\\frac{1}{128}] \\times 128] & \\text{on NPU} \\\\\nmin [max (\\frac{x}{s}, -127), 128] \\times [\\frac{1}{128}] \\times 128 \\cdot s \\cdot w  + extract(x, s, w) & \\text{on CPU}\n\\end{cases}$$\n(1)\nwhere x, w, s, O, and extract represent the original float\nactivation, INT8 weights, the quantization scale factor, the\nMatMul operation, and the function of extracting activation\noutliers into a more compact tensor, respectively. Specifically,\nthe MatMul Ow can be equivalently divided into the sum of\ntwo parts according to the associative law: (1) Mobile NPU for\nMatMul within the scale. m11m-NPU first quantizes and rounds\nx to the range of -127 to 128 based on the scale factor s. It then\nobtains intermediate results by performing a standard W8A8\nper-tensor MatMul with weights w. (2) Mobile CPUs/GPUs\nfor MatMul beyond the scale. mllm-NPU calculates the partial\nvalues exceeding s. Since these outliers are rare, mllm-NPU\nextracts these values from the tensor, compresses them into\na dense tensor, and performs a MatMul with weights w.\nSince outliers are very sparse (around 5-15 channels, ac-\ncounting for only 0.1%-0.3% of total channels, as shown in\nFigure 10), the shadow execution on CPU is much faster\nthan the execution of original tensor on NPU, and its execu-\ntion time can be totally hidden by overlapping. To further\nminimize the overhead of this extra process, mllm-NPU deter-\nmines an outlier threshold (i.e., s in Equation 1) by profiling\na large corpora at offline, thereby can identify the outliers\nby simply comparing the activation numbers to this thresh-\nold. The design of shadow outlier execution is compatible"}, {"title": "3.4 Out-of-order subgraph execution", "content": "As elaborated in (\u00a72.3), LLM quantization algorithms cannot\nfully eliminate floating point operations, ml1m-NPU thereby\ndivides its execution flow into NPU and CPU/GPU collab-\noratively. Typically, LayerNorm, Attention, as well as the\nshadow outlier computation are placed on the CPU/GPU;\nwhile the other linear layers are processed on the NPU. How-\never, we found simply overlapping their execution is ineffi-\ncient, resulting in large execution bubbles (37% bubble rate\nin critical path), as illustrated in Figure 13(a).\nOut-of-order execution. To reduce these execution bub-\nbles, ml1m-NPU is guided by a key insight that, after being\npartitioned at both chunk and subgraphs levels, the LLM sub-\ngraphs can be scheduled in an out-of-order manner. More\nspecifically, any input-ready subgraph can be executed with-\nout strictly following the chunk sequence. For instance, the\nfirst subgraph of the third chunk (C3-Graph1) can be exe-\ncuted during the bubble period when C2-Graph1 finishes.\nTo preserve correctness, mllm-NPU considers two types\nof dependency: (1) Cross-chunk dependency. Operators like\nAttention rely on data from previous chunks. This means\nthe i-th chunk j-th subgraph Gi,j depends on the j \u2013 1-th\nsubgraph of the 0, 1, . . ., i \u2212 1 chunks:\n$$G_{i,j} \\leftarrow \\{G_{0,j-1}, G_{1, j-1}, ..., G_{i, j-1}\\}$$\n(2)\n(2) Intra-chunk dependency. Operators like LayerNorm, Lin-\near, and Quantize rely only on previous subgraphs within\nthe same chunk. Therefore, the i-th chunk's j-th subgraph\nGi,j depends on the j \u2013 1-th subgraph of the same chunk:\n$$G_{i,j} \\leftarrow G_{i,j-1}$$\n(3)\nAs mobile processors are weak at parallelism and preemp-\ntion [38, 78, 85], to ensure efficiency, a processor is capable\nof executing only one subgraph at any given time.:\n$$\\sum_{i=0}^{N} \\sum_{j=0}^{M} P_{i,j,t}=1, \\forall t$$\n(4)\nwhere Pi,j,t = 1 indicates that subgraph Gi,j is running on\nprocessor P at time t, and N and M represent the maximum\nnumber of chunks and subgraphs, respectively. mllm-NPU\naims to find an execution order minimizing the total execu-\ntion time of all subgraphs under these constraints. Unfortu-\nnately, this scheduling problem can be reduced to a classical\nNP-Hard Traveling Salesman Problem [42]. Moreover, be-\ncause the number of chunks varies with user prompts, an\noptimal scheduling strategy cannot be generated offline.\nInstead, mllm-NPU utilizes an online heuristic algorithm.\nThe key idea is to focus not on the execution time of the\nsubgraph g, but on how executing g contributes to reducing\nNPU stalls, motivated by the observation that during the\nprefill phase, NPU execution time often dominates inference\nlatency, being the critical path. For instance, with a prompt\nlength of 256 using the Qwen1.5-1.8B model, NPU execution\ntakes 315ms, about twice that of the CPU.\nSpecifically, we define a subgraph g's contribution to re-\nduce NPU stalls as follows: If subgraph g is to be executed on\nthe CPU/GPU, let S be the set of new subgraphs that can be\nexecuted after g is completed. S will be executed on the NPU.\nA longer execution time of S is beneficial for reducing NPU\nstalls. Thus, g's contribution is defined as the total execution"}, {"title": "4 Implementation and Evaluation", "content": "We have fully implemented mllm-NPU for Qualcomm Hexagon\nNPUs, comprising 10K lines of code in C/C++ and assembly\nlanguage. We choose Qualcomm SoCs as the target platform\nfor its popularity on mobile devices and powerful NPU ca-\npacity. Qualcomm Hexagon is also the only mobile NPU with\nan open instruction set architecture. mllm-NPU is built on\nthe MLLM [21], one state-of-the-art mobile LLM engines,\nand QNN framework [23], the Qualcomm Neural Processing\nSDK. It supports standard LLM formats exported from Hug-\nging Face [19]. To facilitate LLM execution, we implemented\nspecific operators like KVCache, SiLU, RMSNorm, ROPE, and\netc, in addition to what have been supported by QNN. To\nreduce context switching overhead between CPUs/GPUs and\nNPUs, ml1m-NPU leverages shared buffers to synchronize in-\ntermediate results from different processors. For end-to-end\ninference, mllm-NPU is compatible with any decoding engine\nand utilizes the MLLM CPU backend for decoding stage as\neasy implementation, with a default chunk length of 256.\nThe default pruning rate for outlier layers is 85%.\nThe prototype further incorporates two optimizations. (1)\nOur extensive experiments show that mobile NPUs ofte fa-\nvor tensor sizes (e.g., equal \u201cheight\u201d and \u201cwidth\u201d) in CNN\narchitectures. For example, a linear layer with weights of\n2048x2048 produces the same results for inputs of 1024\u00d71\u00d72048\nand 32\u00d732\u00d72048, but using 32\u00d732\u00d72048 reduces execution\nlatency by 1.62\u00d7. Therefore, mllm-NPU profiles all possible\nequivalent shapes for linear layers during the preparation\nstage and selects the most efficient one. (2) Mobile NPUs typ-\nically access limited memory regions (e.g., 4GB for Hexagon\nNPU), which can be smaller than the size of LLM weights.\nTo maximize prefill acceleration within this limited memory,\nmllm-NPU prioritizes executing computationally intensive\ntasks, such as FFN, on the NPU to enhance efficiency."}, {"title": "4.2 Prefill performance", "content": "We evaluate the ml1m-NPU's prefill performance (speed and\nenergy consumption) at prompt lengths of 64, 256 and 1024\ntokens on two devices, as shwon in Figure 14 and 15. Despite\noutlier variations across datasets, the overall impact on prefill\nperformance is minimal, so we report results from the Long-\nBench dataset. The results show that ml1m-NPU consistently\noutperforms all baselines across both metrics, with benefits\nbecoming more pronounced as prompt length increases.\nPrefill speed. For prompt length of 1024 tokens, mllm-NPU\ncan reduce prefill latency by 18.17\u201338.4\u00d7, 7.3x, 32.5-43.6\u00d7,\nand 1.27-2.34\u00d7 on Xiaomi 14 compared with llama.cpp-CPU,\nMNN-CPU, MLC-GPU, TFLite-GPU, respectively. On the\nRedmi K60 Pro, these improvements are 21.3-41.3\u00d7, 7.43x,\n37.2-69.3x, and 1.3-2.6\u00d7, respectively. These speedups are\ndue to mllm-NPU's use of three innovative techniques that\nfully leverage mobile NPUs, including shadow outlier exe-\ncution, high-efficiency per-tensor MatMul, and out-of-order\nsubgraph execution. Compared with PowerInfer-V2-NPU, a\nbaseline also using NPU for prefilling, ml1m-NPU can achieve\n3.28-5.32x and 3.4-5.6\u00d7 speedup on two devices, respec-\ntively, by employing NPU-friendly INT8 linear computation\nand fine-grained subgraph scheduling (\u00a73.4).\nFor prompt length of 64 tokens, the prefill speed of m11m-NPU\nis 14.86-7.10x, 1.69x, 10.91-17.32x, 1.48x, and 1.81-2.51\u00d7\nfaster than llama.cpp-CPU, MNN-CPU, MLC-GPU, TFLite-\nGPU, and PowerInfer-V2-NPU respectively, with speedups\naveraging 10.5\u00d7, 4.31\u00d7, 2.68\u00d7, 1.02\u00d7, and 1.96\u00d7 lower than\nthose for 1024-token prompts. This is because a shorter\nprompt can lead to a padding problem and limit ml1m-NPU's\nout-of-order execution scheduling efficiency.\nPrefill energy consumption. Energy consumption was\nevaluated on the Redmi K60 Pro, the only rootable device.\nPowerInfer-V2 was excluded due to the lack of energy con-\nsumption data and open-source code. For 1024-token prompts,\nmllm-NPU reduces energy consumption by 35.63\u201359.52\u00d7,\n35.21-59.25x, and 1.85-4.32\u00d7 compared to llama.cpp-CPU,\nMLC-GPU, and TFLite-GPU, respectively. For 64-token prompts,\nthe savings are 10.38-14.12x, 10.38-17.79\u00d7, and 3.22-3.67\u00d7,\nrespectively. These savings are due to the high energy ef-\nficiency of mobile NPUs and mllm-NPU's three novel tech-\nniques for maximizing NPU performance."}, {"title": "4.3 End-to-end performance", "content": "We evaluate the real-world performance of mllm-NPU against\nbaseline systems using two workloads: UI automation on\nDroidTask datasets and context-aware automated email replies\non LongBench datasets. The end-to-end inference latency\nresults are shown in Table 4. Our key observation is that\nmllm-NPU always achieves the lowest inference latency\nacross all four datasets.\nFor LongBench datasets, mllm-NPU shows significant speed\nimprovements: 23.0-46.2\u00d7 over llama.cpp-CPU, 16.5-36.4\u00d7\nover MLC-LLM-GPU, 4.08-4.19\u00d7 over MNN-CPU, 3.51-3.73\u00d7\nover PowerInfer-V2-NPU, and 1.27-2.03\u00d7 over TFLite-GPU.\nThis impressive performance is primarily due to mllm-NPU's\nsuperior efficiency during the prefill stage. The speedup\nagainst TFLite-GPU is lower since mllm-NPU currently re-\nlies on a CPU backend for decoding with no optimization,\nwhile TFLite utilizes GPU. Notably, ml1m-NPU is compatible\nwith any decoding engine, which means once TFLite is open-\nsourced, mllm-NPU can integrate it as the decoding backend,\npotentially enhancing performance further.\nFor the DroidTask datasets, ml1m-NPU reduces end-to-end\ninference latency by 7.9-12.5\u00d7 compared to llama.cpp-CPU,\n15.0-32.8x compared to MLC-LLM-GPU, 2.38-2.45\u00d7 com-\npared to MNN-CPU, 2.27-2.44\u00d7 compared to PowerInfer-V2-\nNPU, and 1.35-2.38\u00d7 compared to TFLite-GPU. The perfor-\nmance gains are slightly smaller for DroidTask datasets due\nto shorter prompts in UI automation versus email writing."}, {"title": "4.4 Inference accuracy", "content": "We investigate the inference accuracy of mllm-NPU on five\nLLM benchmarks: LAMBADA [62], HellaSwag [90], Wino-\nGrande [4], OpenBookQA [56] and MMLU [41]. For compar-\nison, we evaluated 4 alternatives: FP16 (non-quantization),"}, {"title": "4.5 Memory consumption", "content": "We compare ml1m-NPU with INT8 weight baselines, as mo-\nbile NPUs only support INT8 weight computations. Memory\nconsumption results on the Redmi K60 Pro, using a 512-token\nprompt, are presented in Figure 16. m11m-NPU consumes up\nto 1.32 \u00d7 more memory than llama.cpp and TFLite. The over-\nhead is due to the MLLM and QNN frameworks, which al-\nlocate independent activation buffers for each operator to\nenhance speed. The tiny additional memory overhead in-\ntroduced by mllm-NPU is its \u00a73.3 shadow outlier execution\ntechnique (in black), which loads tiny float weights into\nmemory, accounting for only 0.6%-1% of the total memory."}, {"title": "4.6 Ablation study.", "content": "We conduct a comprehensive breakdown analysis of the ben-\nefits brought by each of mllm-NPU's techniques using the\nQwen1.5-1.8B, Gemma-2B, and LlaMA2-7B models, as shown"}, {"title": "5 Related work", "content": "On-device LLM optimization. LLMs are resource-hungry,\nespecially when long context is needed [81, 91"}]}