{"title": "FEDCVD: THE FIRST REAL-WORLD FEDERATED LEARNING BENCHMARK ON CARDIOVASCULAR DISEASE DATA", "authors": ["Yukun Zhang", "Guanzhong Chen", "Zenglin Xu", "Jianyong Wang", "Dun Zeng", "Junfan Li", "Jinghua Wang", "Yuan Qi", "Irwin King"], "abstract": "Cardiovascular diseases (CVDs) are currently the leading cause of death worldwide, highlighting the critical need for early diagnosis and treatment. Machine learning (ML) methods can help diagnose CVDs early, but their performance relies on access to substantial data with high quality. However, the sensitive nature of healthcare data often restricts individual clinical institutions from sharing data to train sufficiently generalized and unbiased ML models. Federated Learning (FL) is an emerging approach, which offers a promising solution by enabling collaborative model training across multiple participants without compromising the privacy of the individual data owners. However, to the best of our knowledge, there has been limited prior research applying FL to the cardiovascular disease domain. Moreover, existing FL benchmarks and datasets are typically simulated and may fall short of replicating the complexity of natural heterogeneity found in realistic datasets that challenges current FL algorithms. To address these gaps, this paper presents the first real-world FL benchmark for cardiovascular disease detection, named FedCVD. This benchmark comprises two major tasks: electrocardiogram (ECG) classification and echocardiogram (ECHO) segmentation, based on naturally scattered datasets constructed from the CVD data of seven institutions. Our extensive experiments on these datasets reveal that FL faces new challenges with real-world non-IID and long-tail data. The code and datasets of FedCVD are available https://github.com/SMILELab-FL/FedCVD.", "sections": [{"title": "1 Introduction", "content": "Cardiovascular Diseases (CVDs) cause over 18 million deaths globally each year, positioning them as one of the most significant global health challenges [1]. Early detection and accurate diagnosis of CVDs are crucial, as they allow for timely medical interventions and more effective treatment plans, which in turn significantly lower patient mortality rates [2]. In recent years, with the growing availability of electronic health records and other high-quality clinical data, researchers have increasingly utilized machine learning techniques to automate clinical diagnostics [3; 4], a strategy that has proven highly effective in the context of CVDs [5; 6]. This data-driven approach not only facilitates efficient early screening but also optimizes the allocation of healthcare resources, improving overall patient outcomes.\nCompared to models trained in isolation at a single center, collaborations across multiple medical institutions enable the utilization of richer regional and demographic characteristics, fostering more precise and comprehensive research outcomes. However, medical data is considered highly sensitive, and recent privacy regulations (e.g., EU General Data Protection Regulation (GDPR) [7]) restrict its transfer, hindering the expansion of datasets through data sharing among institutions to train more efficient models, i.e., data isolation."}, {"title": "3 The Proposed FedCVD", "content": "In this section, we present the details of the proposed general FL framework for healthcare tasks as shown in Figure 1. Our framework is built upon the lightweight open-source framework FedLab [37] for FL simulation. We present the details of datasets, metrics, and baseline models in Section 3.1. Then, we discuss the main FL challenges that FedCVD supported in Section 3.2."}, {"title": "3.1 Datasets, Metrics and Baseline Models", "content": "Figure 1 provides an overview of the datasets included in FedCVD. In this section, we provide a brief description of each dataset, corresponding metrics, and baseline models.\nFed-ECG. The 12-lead ECG signals in Fed-ECG are sourced from four distinct datasets. The first and third datasets were collected from Shandong Provincial Hospital [38] and Shaoxing People's Hospital [39] in China, respectively. The second dataset is from the PTB-XL database, released by Physikalisch Technische Bundesanstalt (PTB) [40], and the fourth originates from the PhysioNet/Computing in Cardiology Challenge 2020 [41], which represents a large population from the Southeastern United States. These four datasets, originating from geographically diverse regions, are naturally suited for the Federated Learning (FL) setting due to their separation by location.\nThe corresponding task on these four datasets involves multi-label classification for each institution, a challenging problem due to the large number of labels and the long-tail distribution inherent to the data. To provide a more fine-grained evaluation, we focus on detailed label distinctions, which are of particular interest to clinicians, rather than broader label categories. To thoroughly assess performance, we adopt two metrics: micro-F1, which evaluates the overall performance across all labels, and mean average precision (mAP), which specifically measures the impact of the long-tail distribution on model performance.\nThe original four datasets consist of ECG data with varying lengths and labels, each based on different standards, such as AHA [42], SCP-ECG, and SNOMED-CT, making them incompatible for use in a Federated Learning (FL) setting"}, {"title": "3.2 Challenging Traits of FedCVD", "content": "Non-IID. Non-independently and identically distributed (non-IID) is a typical characteristic in FL scenarios, encompassing non-IID features and non-IID labels, where clients' data shows heterogeneity in both feature and label spaces. Quantity imbalance, where institutions hold uneven sample sizes, can further exacerbate these non-IID issues. Among these, non-IID labels have the most pronounced impact on FL model performance. This is because the quantity and types of labels held by each institution can vary greatly, misleading the local supervised training process and causing \"Client Drift\" [48], which hinders global model convergence.\nFed-ECG naturally exhibits these three characteristics. In terms of feature distribution, Figure 2(a) shows significant age distribution differences among institutions' patients, with Institution 1 notably younger, reflected in the ECG features. Regarding sample size, Figure 2(b) depicts significant differences among the four institutions, with Institution 4 having the fewest samples. For label distribution in the Fed-ECG multi-label classification task, each sample may belong to multiple categories, but the quantity and proportion of different labels vary significantly among institutions. For example, the most common label for Institution 1 and Institution 2 is NORM (Normal), while for Institution 3 and Institution 4 it is STACH (Sinus tachycardia). Some institutions may even lack samples with certain labels, such as both Institution 3 and Institution 4 lacking samples labeled as PAC (Atrial premature complex(es)). These non-IID characteristics challenge the four institutions in collaboratively training a multi-label prediction model, as institutions struggle to capture information about the labels they lack during local training, potentially leading to client drift.\nLong-tail Distribution. In addition to the inter-institution heterogeneity caused by non-IID labels, Fed-ECG also exhibits intra-institution and inter-institution heterogeneity in the form of a long-tail distribution of labels. As shown in Figure 2(b), each institution's internal label distribution has a clear long-tail characteristic, with a few dominant labels having many samples and numerous labels having fewer samples (long tail). These tail categories are already troublesome during independent local training, as the model may focus more on the head categories and neglect the tail ones. In FL scenarios with quantity imbalance and non-IID labels, the long-tail problem is further exacerbated. For instance, categories mainly found in the disadvantaged institutions' tails may be in an even worse position within the overall data of all institutions. The long-tail characteristic challenges FL algorithms in ensuring the effectiveness and fairness of handling samples from various categories across institutions.\nLabel Incompleteness. Fed-ECHO presents the most challenging scenario: label-incomplete FL. In Fed-ECHO, three naturally formed institutions hold ECHO video data with annotations (image region segmentation). However, due to varying annotation capabilities, the completeness of labels among the three institutions differs, as shown in Figure 1. Institution 1 has the most complete labels (four labels) due to its ability to identify and annotate all four key regions (including the background). In contrast, Institution 2 and Institution 3 each have labels for only one key region (LV Endo and LVEpi, respectively). This incompleteness introduces (1) label heterogeneity, similar to the label-non-IID in Fed-ECG, where Institution 2 and Institution 3 lack some labels, and (2) mislabeling, where Institution 2 and Institution 3 label unrecognized parts as background, conflicting with Institution 1's labels and causing misleading information. This scenario significantly challenges FL algorithms to effectively utilize the different levels of label completeness from each Institution and leverage highly heterogeneous data to benefit the global model."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experiment Details", "content": "Baseline Algorithms. Our experiments utilize seven typical FL algorithms across both datasets. The first four are classical global FL algorithms: FedAvg [9], the oft-cited FL algorithm, collaboratively trains a global model across participants. FedProx [49] addresses statistical heterogeneity in FL by introducing an L2 proximal term during local training, while Scaffold [48] mitigates client drift through control variates and server-side learning rate adjustments. FedInit [50] also tackles client drift by employing a personalized, relaxed initialization at the start of each local training stage. The last three are personalized FL methods: Ditto [51], which excels in balancing accuracy, fairness, and robustness in FL; FedSM [52], which combines model selection with personalized methods to avoid client drift; and FedALA [53], which reduces the impact of statistical heterogeneity by adaptively aggregating both the global and local models. For the Fed-ECHO dataset, we further evaluate two Federated Semi-Supervised Learning (FSSL) methods: Fed-Consist [10], which uses a consistency-based method for segmentation, and FedPSL [54], which applies separate model aggregation and meta-learning techniques for classification. In addition to the FL family, we include two other baseline algorithms: Client, which refers to training models using only local data without collaboration among participants, and Central., which represents the ideal centralized training scenario where the server has access to all participants' data.\nSetup. The number of institutions involved in federated training for each task is listed in Appendix C. Our experiments mainly focus on the multi-center FL scenario (i.e., cross-silo), where all institutions participate in training at each communication round. Considering the trade-off between computation and communication, we set the local training epoch to 1 and the communication rounds to 50 throughout experiments except Fed-Consist. Since Fed-Consist requires extra rounds for training on clients with full labels before starting federated learning, we set the communication rounds to 100, where 50 rounds are for labeled clients training and another 50 rounds are normal FL training.\nEvaluation Strategies. For a comprehensive evaluation, we build a local and global evaluation set for both datasets in FedCVD. For the local one, we divide each local data into train/test sets by 8:2. For the global one, we collect each local test set together. Our experiments test all algorithms using two evaluation strategies: 1) Global test performance (GLOBAL) is evaluated on the global test set and used to determine whether the model has learned knowledge from other clients in the FL setting. The better results of GLOBAL indicate that the model is closer to the centralized training. 2) Local test performance (LOCAL) is evaluated on each local test set. The LOCAL is more practical in real-world applications than GLOBAL because it indicates performance improvement for its task without centralizing all local data."}, {"title": "4.2 Benchmark on Fed-ECG", "content": "We present the Fed-ECG dataset, which poses significant challenges for FL scenarios, namely non-IID data and long-tailed distribution. We first compared the overall performance of mainstream FL algorithms on Fed-ECG, with the evaluated local and global performance shown in Table 2. The results indicate that FL has advantages over local training. Additionally, FL algorithms designed for heterogeneous scenarios (FedProx, Scaffold, FedInit) outperform the FedAvg algorithm. The personalized algorithms Ditto, FedSM, and FedALA exhibit excellent performance in local tests for certain clients. However, these FL algorithms still lag behind centralized training.\nTo better illustrate the impact of these two challenges on FL performance, we introduced additional experimental settings and evaluation metrics. For the non-IID challenge, we compared the performance differences between natural partitioning and two simulated partitioning methods (random and non-IID), with the simulated non-IID partitioning method described in Appendix C. Figure 3 compares the performance (percentage relative to centralized training) between FL algorithms trained under the three partitioning settings. The results reveal that Fed-ECG's natural partitioning poses significantly greater challenges compared to the two simulated partitioning methods.\nFor the long-tail challenge, we used the mAP metric in Table 2 to evaluate the overall performance of algorithms across different classes. In general, FL algorithms designed for heterogeneous scenarios demonstrate an advantage in addressing long-tail issues, with personalized algorithms like Ditto and FedALA showing better results in local tests. However, in global tests, the FedProx algorithm more effectively handles long-tail problems. Comparisons with centralized training reveal that FL scenarios tend to amplify the impact of long-tail distributions. To further illustrate this challenge, we introduced the F1-STD metric, which measures the standard deviation of F1 scores across classes. This metric reflects the algorithm's ability to manage long-tail problems; the larger the STD, the poorer the algorithm's performance in this regard. The GLOBAL F1-STD results of different FL algorithms are visually presented in Figure 4, showing a pattern consistent with Table 2 and underscoring the challenges posed by long-tail distributions.\nTo more granularly evaluate the performance of algorithms in long-tail scenarios, we introduced a new metric, Top-K. Top-K refers to selecting the K classes with the most samples and the K classes with the fewest samples, calculating the average F1 score for each group, and then computing the relative performance drop between them. A larger performance"}, {"title": "4.3 Benchmark on Fed-ECHO", "content": "The proposed Fed-ECHO dataset presents one of the most challenging FL settings: label incompleteness, which can be viewed as an enhanced version of label-non-IID. Specifically, all annotated video frames from Institution 1 are completely segmented into four regions: BG, LVEndo, LVEpi, and LA. In contrast, Institution 2 and Institution 3 can only recognize the LVEpi and LVEndo regions in their annotated video frames, respectively, with the remaining regions simply labeled as \"BG.\" For convenience, we refer to the BG labels from Institution 2 and Institution 3 as \"Maybe-BG,\" indicating these segmentations may be unreliable. This discrepancy introduces potential conflicts between the \"Maybe-BG\" labels of Institution 2 and Institution 3 and the corresponding \"reliable\" labels of Institution 1, resulting in misleading labels that affect model convergence.\nTo mitigate the impact of misleading labels, we propose a straightforward baseline strategy, supervised-only. During supervised model training, we input the data from all three Institutions into the model without additional processing, allowing the model to benefit from the rich data features. However, when calculating the loss, we mask out the \"Maybe-BG\" regions in the video frames from Institution 2 and Institution 3. This means that for samples from Institution 2 and Institution 3, we only compute the training loss on the \"reliable foreground\". This strategy ensures the model learns segmentation capabilities from completely reliable labels. Additionally, during model segmentation performance evaluation, we also exclude the \"Maybe-BG\" regions from the test samples, preventing them from influencing the model's performance metrics."}, {"title": "5 Conclusion", "content": "This paper has introduced FedCVD, the first real-world multi-center FL benchmark for CVD data, which consists of two datasets and their respective tasks: Fed-ECG and Fed-ECHO. It presents three major challenges due to the heterogeneous distribution of real-world data: non-IID, long-tailed labels, and label incompleteness. We conducted extensive comparative and validation experiments, testing mainstream FL algorithms and centralized training on these tasks. Experimental results show that the natural non-IID characteristics in FedCVD are more challenging than the manually partitioned setups in most previous federated benchmarks, and mainstream algorithms perform poorly in the long-tail tests of FedCVD. For the most difficult task, i.e., the label-incomplete Fed-ECHO, mainstream FL algorithms barely maintain utility, but are still better than non-cooperative algorithms that only utilize unlabeled data on each client. Federated semi-supervised learning algorithms that leverage unlabeled data achieve some performance improvement. Beyond, as a flexible and extensible framework, FedCVD is meant to be a step towards the development of FL on CVD domain."}, {"title": "Limitations and Future Work", "content": "FedCVD presents a realistic and challenging scenario that tests FL algorithms' ability to mitigate data heterogeneity, handle long-tailed classes, and utilize unlabeled data. However, FedCVD currently offers a limited variety of data types and only two tasks. Additionally, the FL algorithms compared in experiments, particularly semi-supervised ones, are limited. In future work, we will expand the data range of FedCVD, aiming for it to inspire future FL research in real-world medical contexts, especially with CVD data."}, {"title": "A Broader Impact", "content": "Considering that this research exclusively involves the repurposing of existing open-source databases, the associated risks are limited. However, it is important to acknowledge that all datasets utilized in this study may be influenced by biases inherent in the original data collection processes, such as those related to gender, age, or race. Unfortunately, identifying the sources of potential biases is challenging because the data have been appropriately pseudonymized. Moreover, records such as electrocardiograms and echocardiograms cannot be easily linked to specific demographic attributes such as age, ethnicity, or gender by non-medical experts. Nonetheless, our work discloses certain metadata of the datasets, including geographical origin, gender distribution, and age distribution. This exposure may aid in identifying underlying geographical biases, which are anticipated in real-world federated learning scenarios.\nWhile prioritizing simplicity and utility, the current benchmark does not include privacy metrics. Nevertheless, privacy remains critically important in the cardiovascular disease domain, and we strongly encourage the research community to address these considerations. Thanks to the modularity of FedCVD, we can add privacy components easily. Therefore, we anticipate that FedCVD will address privacy concerns related to federated learning within the cardiovascular disease domain in the future."}, {"title": "B Datasets repository and Maintenance plane", "content": null}, {"title": "B.1 Dataset repository.", "content": "The code is now available at https://github.com/SMILELab-FL/FedCVD.Considering licenses, users need to download the data manually through the original dataset link."}, {"title": "B.2 Maintenance plan", "content": "We shall adhere to a maintenance plan to uphold the integrity of the codebase and ensure the conformity of supplied datasets to requisite standards. In particular, this maintenance plan encompasses:\n\u2022 Fixing bugs affecting the correctness of our code, whether identified by the community or ourselves;\n\u2022 Introducing additional variants of federated learning techniques, including alternative methods within the scope of cross-silo federated learning and federated semi-supervised learning methodologies;\n\u2022 Adding new functional modules, such as privacy protection components.\n\u2022 Regarding datasets, reviewing potential updates of the datasets referenced in the FedCVD, including but not limited to introducing new tasks or modalities;"}, {"title": "C Fed-ECG", "content": null}, {"title": "C.1 Description", "content": "Fed-ECG consists of four datasets: SPH, PTB-XL, SXPH, and G12EC. The order of leads of each dataset is I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6. The overview of Fed-ECG is shown in Table 5. Table 6 shows demographics information for four datasets in Fed-ECG."}, {"title": "C.2 License and Ethics", "content": "All four databases are open-access. The SPH database is open access at Figshare, while the rest databases are open access at PhysioNet under a Creative Commons Attribution 4.0 International Public License.\nThe PTB-XL database was supported by the Bundesministerium f\u00fcr Bildung und Forschung (BMBF) through the Berlin Big Data Center under Grant 01IS14013A and the Berlin Center for Machine Learning under Grant 01IS180371 and by the EMPIR project 18HLT07 MedalCare. The EMPIR initiative is cofunded by the European Union's Horizon 2020 research and innovation program and the EMPIR Participating States.\nThe institutional review board of Shaoxing People's Hospital and Ningbo First Hospital of Zhejiang University approved the study of the SXPH database, granted the waiver application to obtain informed consent, and allowed the data to be shared publicly after de-identification. The requirement for patient consent was waived."}, {"title": "C.3 Download and preprocessing", "content": null}, {"title": "C.3.1 Download", "content": "The four datasets can be downloaded using the URLs below:"}, {"title": "C.3.2 Preprocessing", "content": "Raw 12-lead ECG signals have varying sequence lengths and raw 12-lead ECG signals have varying sequence lengths and annotated standards which must be standardized before FL training. Therefore, we first set a signal length to 10 seconds. We pad the signal with edge value at the edge for those whose length is shorter than 10 seconds and cut off the signal at 10 seconds for those whose length is longer than 10 seconds. Next, we only save the records whose label occurs in at least two databases. Finally, we align the labels of records in different databases. The relationship between the original label and our label is shown in Table7."}, {"title": "C.4 Baseline, loss function and evaluation", "content": "Baseline Model. We implement a ResNet1d model with 34 layers. The final layer output is passed through a sigmoid function to encode the probability that each label corresponds to one 12-lead ECG signal.\nLoss function. The model was directly trained for the Binary CrossEntropy Loss (BCELoss), defined as:\n$BCE(y, \\hat{y}) = - \\frac{1}{N} [\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) + \\sum_{i=1}^{n} (1 \u2013 y_i) \\log(1 \u2013 \\hat{y}_i)]$\t\t(1)\nEvaluation Metrics. In multi-label classification for Fed-ECG, the micro F1 score is used as the main metric to evaluate the performance of the model. Given N labels, the micro-precision ($P_{micro}$) and micro-recall ($R_{micro}$) are calculated as $P_{micro} = \\frac{\\sum_{i=1}^{N} TP_i}{\\sum_{i=1}^{N} (TP_i + FP_i)}$ and $R_{micro} = \\frac{\\sum_{i=1}^{N} TP_i}{\\sum_{i=1}^{N} (TP_i + FN_i)}$, where TP is the number of true positives for label i,FP is the number of false positives for label i,FN; is the number of false negatives for label i. The micro F1 score (F1micro) is then calculated as:\n$F1_{micro} = \\frac{2 \\cdot P_{micro} \\cdot R_{micro}}{P_{micro} + R_{micro}}$\t\t(2)\nFor Fed-ECG's Multi-Label Classification task, the Mean Average Precision (mAP) is adopted to measure the classification performance across all labels (including long-tailed labels), calculated by averaging the average precision (AP) for each label, defined as:\nmAP = $\\frac{1}{L} \\sum_{i=1}^{L} \\sum_{k=1}^{n} P_i(k) \\Delta r_i(k)$                                                                                       (3)"}, {"title": "C.5 Training Detail", "content": "Optimization parameters. We optimize the ResNet1d using SGD optimizer, with a batch size of 32. We train our model for 50 epochs on one NVIDIA A100-PCIE-40GB.\nHyperparameter Search For centralized and local model training, we first conduct a search for optimal learning rates from the set {1e-5, 1e-4, 1e-3, le-2, 1e-1} during centralized model training. The learning rate that yields the best micro-F1 score is then used for local model training. For the federated learning strategies, we employ the following hyperparameter grid:\n\u2022 For clients' learning rates (all strategies): {1e-5, 1e-4, 1e-3, 1e-2, 1e-1}.\n\u2022 For server size learning rate (Scaffold strategy only): {1e-2, 1e-1, 1.0}.\n\u2022 For FedProx and Ditto strategies, the parameter u is selected from {1e-2, 1e-1, 1.0}.\n\u2022 For FedInit, the parameter \u1e9e is chosen from {1e-1, 1e-2, 1e-3}.\n\u2022 For FedSM, the parameters y and \u03bb are set to values from {0, 0.1, 0.7, 0.9} and {0.1, 0.3, 0.5, 0.7, 0.9}, respectively.\n\u2022 For FedALA, the parameters layer index, \u03b7, threshold, and num_per_loss are fixed at 1, 1.0, 0.1, and 10, respectively, while rand_percent is selected from {5, 50, 80}."}, {"title": "C.6 Supplementary Experiment Results", "content": "We provide additional evaluation metrics here. Table 9 presents an extensive array of evaluation metrics for various federated learning approaches applied to Fed-ECG. The Micro F1-Score (Mi-F1) and Hamming Loss (HL) serve as indicators of the overall performance, given their insensitivity to long-tail distributions. In contrast, the mean Average Precision score (mAP) provides insight into the average performance across individual labels. In addition, Figure 6 presents the evaluation metrics for each label, encompassing F1 score, precision, and recall, which more clearly demonstrates the impact of the long-tail distribution on each label."}, {"title": "D Fed-ECHO", "content": null}, {"title": "D.1 Description", "content": "Fed-ECHO consists of three datasets: CAMUS, ECHONET-DYNAMIC, and HMC-QU. The overview of Fed-ECHO is shown in Table 5."}, {"title": "D.2 License and Ethics", "content": "Both CAMUS and HMC-QU datasets are open-access. HMC-QU database requires the user to have a Kaggle account, while the ECHONET-DYNAMIC database requires the user to have a Stanford AIMI account and to accept its agreement. It is licensed under the Stanford University Dataset Research Use Agreement."}, {"title": "D.3 Download and preprocessing", "content": null}, {"title": "D.3.1 Download", "content": "The three datasets can be downloaded using the URLs below:"}, {"title": "D.3.2 Preprocessing", "content": "Raw echocardiograms have varying frame sizes, modalities, and mask labels, which must be standardized before training. Therefore, as a first step, we extract frames that are annotated and store them as images. We then resize them to a common (112 \u00d7 112) shape. Finally, we align the labels of records in different databases. We use 1, 2, 3 representing LV Endo, LV Epi and LA respectively. The samples of Fed-ECHO are shown in Figure7."}, {"title": "D.4 Baseline, loss function and evaluation", "content": "Baseline Model. A U-net architecture is employed in this study, utilizing echocardiographic images as input to forecast masks delineating four distinct cardiac regions. The U-net model represents a conventional convolutional neural network design frequently deployed in the realm of biomedical image segmentation endeavors. Its application is tailored towards semantic segmentation, a process wherein individual pixels within an image are categorized based on semantic content.\nLoss function. We use a CrossEntropy Loss (CELoss) for training. Note that, for centralized supervised learning and client training in FedAvg, FedProx, Scaffold, and Ditto strategies, we ignore label with value 0 when calculating CELoss for data from client 2 or 3, since region with label 0 may not be true ground truth in these clients.\nEvaluation Metrics. We use the Dice similarity index and 2D Hausdorff distance (dH) to measure the accuracy of the segmentation output. Dice index is calculated as:"}, {"title": "D.5 Training Detail", "content": "Optimization parameters. We optimize our model using the SGD optimizer, with a batch size of 32. We train our model for 50 epochs on one NVIDIA A100-PCIE-40GB.\nHyperparameter Search For centralized and local model training, we first explore learning rates from the set {1e-4, 1e-3, le-2, 1e-1.5, 1e-1} during centralized model training. The learning rate that achieves the best Dice index is then utilized for local model training. For the federated learning strategies, we employ the following hyperparameter grid:\n\u2022 For clients' learning rates (all strategies except Fed-Consist): {1e-4, 1e-3, le-2, 1e-1.5, 1e-1}.\n\u2022 For server size learning rate (Scaffold strategy only): {1e-2, 1e-1, 1.0}.\n\u2022 For FedProx and Ditto strategies, the parameter \u00b5 is selected from {1e-2, 1e-1, 1.0}.\n\u2022 For FedInit, the parameter \u03b2 is chosen from {1e-1, 1e-2, 1e-3}.\n\u2022 For FedSM, the parameters \u03b3 and \u03bb are set to {0, 0.1, 0.7, 0.9} and {0.1, 0.3, 0.5, 0.7, 0.9}, respectively.\n\u2022 For FedALA, the parameters layer index, \u03b7, threshold, and num_per_loss are fixed at 1, 1.0, 0.1, and 10, respectively, while rand_percent is chosen from {5, 50, 80}.\nFor Fed-Consist, we introduce Gaussian noise with a variance of 0.1 as augmentation. The learning rates for labeled clients are searched from {1e-2, 1e-3, 1e-4}, while those for unlabeled clients are explored within {1e-3, 1e-4, 1e-5, 5e-6, 1e-6}. The parameter \u03b3 is varied from {0.5, 0.7, 0.9}."}]}