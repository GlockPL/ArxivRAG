{"title": "GenesisTex2: Stable, Consistent and High-Quality Text-to-Texture Generation", "authors": ["Jiawei Lu", "Yingpeng Zhang", "Zengjun Zhao", "He Wang", "Kun Zhou", "Tianjia Shao"], "abstract": "Large-scale text-guided image diffusion models have shown astonishing results in text-to-image (T2I) generation. However, applying these models to synthesize textures for 3D geometries remains challenging due to the domain gap between 2D images and textures on a 3D surface. Early works that used a projecting-and-inpainting approach managed to preserve generation diversity but often resulted in noticeable artifacts and style inconsistencies. While recent methods have attempted to address these inconsistencies, they often introduce other issues, such as blurring, over-saturation, or over-smoothing. To overcome these challenges, we propose a novel text-to-texture synthesis framework that leverages pretrained diffusion models. We first introduce a local attention reweighing mechanism in the self-attention layers to guide the model in concentrating on spatial-correlated patches across different views, thereby enhancing local details while preserving cross-view consistency. Additionally, we propose a novel latent space merge pipeline, which further ensures consistency across different viewpoints without sacrificing too much diversity. Our method significantly outperforms existing state-of-the-art techniques regarding texture consistency and visual quality, while delivering results much faster than distillation-based methods. Importantly, our framework does not require additional training or fine-tuning, making it highly adaptable to a wide range of models available on public platforms.", "sections": [{"title": "1 Introduction", "content": "Digital assets are essential for the gaming, film, and animation industries. The role of textures is pivotal, as they influence the visual effects and aesthetics. However, creating appealing textures takes considerable effort, even for professionals. Recently, diffusion models trained on billions of image-text pairs have enabled users to generate stunning images from text prompts. However, applying this approach to texture synthesis faces significant challenges, primarily due to: 1) a lack of high-quality text-labeled training data for textures and 2) a domain gap between 2D images and 3D surface textures. Therefore, most methods of text-guided texture generation circumvent the limitations by employing pretrained 2D text-to-image diffusion models. However, creating 3D consistent textures that maintain high quality remains a significant challenge, even with geometric guidance like Depth maps in ControlNet.\nExisting approaches typically navigate a trade-off between single-image quality and multi-view consistency, falling into two main categories. The first group optimizes an underlying 3D structure based on Score Distillation Sampling (Poole et al. 2022; Lin et al. 2023; Wang et al. 2024). However, these optimization-based methods are often time-consuming and struggle to match the diversity and quality of text-to-image generation. The second group generates images from various viewpoints to create the final texture in an optimization-free fashion. This can be achieved through sequential inpainting (Chen et al. 2023b; Richardson et al. 2023) or a multi-view diffusion approach (Liu et al. 2023c; Gao et al. 2024). Our method falls in this category.\nWe tackle the challenges of achieving both consistency and quality by introducing a cross-view local attention technique and a latent space merge pipeline specifically designed for the text-to-texture task, using only pretrained T2I models. For the local attention, we input the 3D mesh and construct dense patch-level weight matrices based on the 3D locations of patches across different views. Patches that are closer in 3D receive higher weights, while farther ones get lower weights. The weight matrices are then incorporated into the self-attention layers during diffusion to amplify or attenuate the effect of specific patches, thereby enhancing local details and improving the consistency of multi-view images. Additionally, we design a latent space merge framework to ensure consistent and high-quality texture synthesis. Finally, we propose an efficient texture completion algorithm to fill uncolored UV pixels caused by self-occlusion. The algorithm approximates color dilation in surface space by discretizing the UV into sub-UV islands.\nOur contributions can be summarized as follows:\n\u2022 We propose a novel local attention mechanism for pretrained T2I models, which leverages 3D priors and establishes patch correspondences across different views.\n\u2022 We design a framework that incorporates a latent merge pipeline and an efficient texture dilation algorithm in surface space, enabling a stable generation of consistent and high-quality textures.\n\u2022 We have conducted extensive evaluations on a variety of 3D objects. The evidence demonstrates that our approach significantly surpasses the performance of the baseline methods by better preserving the generative potential of the original T2I models in aspects of details and color richness while maintaining multi-view consistency."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Text-to-Image Diffusion Models", "content": "Diffusion models are a class of generative models that use Markov chains to transform random noise into high-quality visuals sequentially. A pioneering work, GLIDE (Nichol et al. 2021), is the first to employ diffusion models for generating images in pixel space while supporting text conditioning by adopting classifier-free guidance. Following GLIDE (Nichol et al. 2021), Imagen (Saharia et al. 2022) integrates diffusion models for high-resolution text-guided image generation. DALLE-2 (Ramesh et al. 2022) leverages CLIP (Radford et al. 2021), a popular model that aligns texts and images to generate images from CLIP latent space. Stable diffusion is a landmark work built upon Latent Diffusion Model (LDM) (Rombach et al. 2022) trained on a large-scale text-image dataset (Schuhmann et al. 2022), which proposes to adapt the diffusion process in latent space to further reduce computational cost. Besides text conditioning, various flexible conditions have been introduced for image generation such as ControlNet (Zhang, Rao, and Agrawala 2023) and T2I-Adapter (Mou et al. 2024). These control methods aim to generate results that align with a given spatial condition, such as depth or normal images, which can be either predicted from input images or rendered from 3D meshes, supporting mesh-guided image generation."}, {"title": "2.2 Text-driven 3D Generation", "content": "Many recent studies (Jun and Nichol 2023; Hong et al. 2023; Huang et al. 2023; Xu et al. 2023; Nichol et al. 2022) attempt to replicate the success of 2D diffusion models in text-guided 3D content generation, after the supervision of text-paired 3D data. A common constraint of these methods lies in the scarcity of publicly available labeled 3D data. As such, rather than direct leaning a 3D diffusion model, many works resort to using pretrained 2D image diffusion models for 3D tasks (Gao et al. 2024; Cao et al. 2023; Chen et al. 2023b; Liu et al. 2024, 2023a; Long et al. 2023; Shi et al. 2023). Pioneering works (Poole et al. 2022; Wang et al. 2023) suggest optimizing a 3D representation(E.g., NeRF) by distilling from 2D diffusion models. Subsequent research (Lin et al. 2023; Metzer et al. 2023) further improved such text-to-3D distillation methods in various aspects. A recent remarkable work (Wang et al. 2024) proposed a technique called Variational Score Distillation (VSD) that further enriches the details and diversity. Another line of work (Shi et al. 2023; Liu et al. 2023b; Tsalicoglou et al. 2023) typically fine-tune a multi-view diffusion model by incorporating camera directions to image diffusion models and simultaneously generate multi-view images. Zero-1-to-3 (Liu et al. 2023a) first attempts to leverage 3D data and camera parameters to fine-tune pretrained 2D diffusion models for 3D-consistent novel view synthesis. MVDream (Shi et al. 2023) and SyncDreamer (Liu et al. 2023b) share a similar idea to improve consistency by fine-tuning attention layers in 2D diffusion models using 2D and 3D data."}, {"title": "2.3 Mesh-guided Texture Synthesis", "content": "Beyond generating 3D objects using text prompts, creating textures for given meshes is also a critical and challenging task with various applications. Initial studies (Oechsle et al. 2019; Siddiqui et al. 2022; Yu et al. 2021; Chen, Yin, and Fidler 2022) have shown promising results using GANs. However, their application is limited to specific categories. In contrast, many recent works on mesh-guided text-to-texture synthesis have achieved broader applicability by leveraging large-scale pretrained diffusion models. These methods typically employ strategies such as sequentially generation and inpainting (Chen et al. 2023b; Richardson et al. 2023; Cao et al. 2023), multi-view diffusion (Gao et al. 2024; Liu et al. 2023c) or score distillation (Chen et al. 2023a; Metzer et al. 2023; Youwang, Oh, and Pons-Moll 2023)."}, {"title": "3 Method", "content": "Given a mesh M and a textual prompt P, our goal is to produce a texture T that well depicts the prompt and suits the shape with high quality. An overview of our pipeline is shown in Fig. 1. In this section, We first introduce preliminaries on image space diffusion models and define notations for rendering. Next, we provide details on how to adapt the local attention to the diffusion process to improve the local details in the generated images while preserving consistency. Then, we illustrate our latent merge pipeline, which is combined with the local attention mechanism and ensures the consistency. The final texture can be obtained by inverse rendering and merging the generated multi-view images."}, {"title": "3.1 Preliminary", "content": ""}, {"title": "2D Image Diffusion models", "content": "In this paper, we employ Stable Diffusion (Rombach et al. 2022). Stable Diffusion is a latent diffusion model that operates in the latent space of an autoencoder D(E(\u00b7)), where E and D represent the encoder and decoder, respectively. For a given image I with its corresponding latent feature zo = E(I), the DDPM forward process(Ho, Jain, and Abbeel 2020) iteratively adds gaussian noise to zo.\nq(ZtZt-1) = N(zt; \u221aatzt\u22121, (1 \u2013 at)I), (1)\nwhere t = 1,..., T is the time step, q(ZtZt-1) is the conditional density of zt given zt-1, and at is hyperparameter. In the DDPM backward process, a U-Net is trained to predict the noise and zt-1 can be sampled based on zt and prompt P:\nZt-1 =\u221aat-1\u00dft/1-\u0101t  *(Zt-\u221a1-at/2t0+\n(1-at-1)(\u221aatzt + \u00dft\u03b5t)/1-\u0101t) (2)\nwhere at and \u00dft = 1 \u2212 at are pre-defined hyperparamters, 2t0 is the denoised estimation at time step t, eo (zt, t, P) is the predicted noise for zt, and \u025bt ~ N(0, I). We can sample zo by iteratively performing denoising using Eq. 2 from the standard Guassian noise z\u012b, z\u0442 ~ N(0, I) with DDPM sampling, and decode to the final generated image by D(zo)."}, {"title": "Rendering Representation", "content": "In this paper, textures are defined in 2D image space in an injective UV parameterization of M, represented as UV : p \u2208 \u041c \u2194 (u,v) \u2208 [0,1]2. This parameterization can be automatically constructed using tools like xatlas (Young 2016). We focus on synthesizing base color maps and disregard any shading effects. Given a mesh M, a texture map T and a viewpoint C, we use the rendering function R to get the rendered image x = R(T; M, C). Conversely, the inverse rendering function R-\u00b9 is utilized to reconstruct the texture map from the rendered image: T' = R\u22121(x; M, C). For simplicity, we omit M and C for R and R\u00af\u00b9 throughout this paper."}, {"title": "3.2 Local Attention", "content": "The attention layer is crucial in Stable Diffusion, featuring two types of attention mechanisms: 1) cross-attention, which measures the similarity between the latent features and text embeddings, and 2) self-attention, which can be viewed as patch matching and voting within a single image. In Stable Diffusion, each self-attention layer receives the deep spatial feature (zt) of the noisy latent zt, and linearly projects $(zt) to the query, key, and value matrices Q = lo($(zt)), K = lk($(zt)), V = lv($(zt)), where lo, lk,lv are pretrained linear networks for feature projection. The output of self-attention layers is given by Softmax((QK)/ \u221ad). V, where d is a constant representing the dimension of deep features, we omit \u221ad for simplicity in this paper.\nPrevious works in zero-shot video editing(Yang et al. 2023, 2024; Khachatryan et al. 2023) have demonstrated that modifying the self-attention layers to incorporate cross-frame attention can help regularize style across multiple frames. In texture synthesis, a similar strategy for improving style consistency involves using features from other views as keys and values to perform cross-view attention, as in (Gao et al. 2024; Liu et al. 2023c). The cross-view attention for view n can be written as:\ncross_view_attn(n) = Softmax(QnKm)Vun' Vun, (3)\nwhere un is a set of views that attend to the query view n. The cross_view_attn behaves as the original self-attention when un contains only n.\nHowever, directly adopting this strategy in the diffusion process often leads to a decrease in color diversity and local details in the generated images, as demonstrated in Fig. 3. The root cause of the degradation lies in a reduction of variance in the cross-view attention mechanism, as the predicted feature embedding with the same underlying 3D structure can vary when viewed from different perspectives. This can result in a large attention weight for irrelevant patches, as illustrated in the visualization of attention maps in Fig. 2. In this situation, it becomes necessary to guide the attention module to give greater weight to the same surface area across different viewpoints. This requires considering the correlation of patches among multiple views. Fortunately, we have the input 3D proxy in the texture synthesis task, which naturally builds a strong semantic correspondence between patches of different views.\nInspired by (Hertz et al. 2022), which enables prompt-based image editing by modifying the cross-attention layers in diffusion models, we introduce an attention bias matrix W to reweigh the original attention produced by the pretrained self-attention layers in Stable Diffusion. Similar to the attention mask mechanism that masks certain words in the cross-attention layers, W, in our case, is used to emphasize or diminish the correlation between specific pairs of query-key patches within the self-attention layers. Unlike the previously mentioned cross-view global attention, we refer to our approach as cross-view local attention.\nWe now define the process for calculating the attention bias W. Without loss of generality, let us consider the local attention of the n-th query view with attended views denoted as un. For simplicity, we will omit the subscript n until the end of this section. We render a set of position maps {0} by applying the rendering function R(V) to each view in v, where V denotes the vertex position of M. Then, we calculate a distance matrix d based on the rendered position maps {0}. Each entry of d can be calculated using Euclidean distance: di,j = ||O; \u2013 0;|| for any location i \u2208 {1, ..., NQ} and j\u2208 {1, ..., NK}, where No and NK stands for the number of patches in query and key features, respectively. We do not use geodesic distance due to its significant computational cost, particularly for meshes with a large number of vertices. Furthermore, the precision of the distance calculations is inherently limited by the low resolution of the attention maps, making the choice of distance calculation method less critical.\nThen, we compute W by:\n0,\nif Qi \u2208 BGN\u2229 K; \u2208 BG\nWi,j=-oln(1+rdi,j), if Qi \u2208 FG \u2229 K; \u2208 FG\n-\u221e,\nelse\n(4)\nwhere o and r are hyper-parameters that determine the distribution of the attention bias, BG and FG refer to background and foreground patches, respectively. Intuitively, the attention bias approaches 0 for patch pair located at the same position in 3D and attenuates towards -\u221e as the distance increases. We do not reweigh attention between background patches, and to avoid extreme cases, we set a lower bound d by applying a clamping operation: W = max(W, ln(\u03b4)). In our experiments, we empirically set o, r, d as 2, 20 and 0.1 to get the best performance.\nGiven the original similarity S = QK7 and attention bias W, we can compute the reweighed attention matrix as follows:\nM' = Softmax(S + W), (5)\nwhere each element M\u00b8\u00a1 is calculated by:\nM'i,j = eWij Si,j/\u03a3jewijsij (6)\nIn this way, we manage to manipulate the attention maps by emphasizing on the correspondence of feature patches that are closer in 3D. We empirically find it helpful to replace the original similarity with the weight matrix, to enforce the local appearance consistency, i.e. M' = Softmax(W). However, the replacement operation can lead to blurring and shape distortion in the late steps. Therefore, we limit the replacement strategy to the early stages of the diffusion process for rough consistency guidance."}, {"title": "3.3 Consistent Texture Synthesis", "content": "Latent merge pipeline Applying cross-view local attention in the diffusion process can improve the style consistency across different views, but it's still insufficient for synthesizing 3D consistent views, i.e., two pixels projected to the same point in 3D have the same value. Directly merging these views will inevitably cause inconsistencies in the final texture, as shown in the first two rows of Fig. 5. We consider a latent space alignment strategy similar to (Liu et al. 2023c; Gao et al. 2024; Kim et al. 2024) for better cross-view consistency. However, the alignment operation can lead to an over-smoothed appearance and degradation in diversity due to a loss of variation in the alignment process, see Fig. 4. To overcome these issues while maintaining view consistency, we introduce a novel latent merge pipeline.\nSpecifically, we first initialize a set of noisy latent for each view by {zT,n ~ N(0,I)}=1 and an initial latent texture UT ~ N(0, I) at the beginning of denoising process. At each denoising step t, our goal is to predict 3D consistent Zt-1,n from zt,n. We first obtain the denoised prediction 2t\u21920,n in image space by:\n2t\u21920,n = (Zt,n \u2013 \u221a1 \u2013 Ateo(Zt,n,t,P,dn))/\u221aat, (7)\nwhere dn is the depth condition for ControlNet at view n. We then apply inverse rendering to obtain the per-view partial latent textures by:\n\u00dbt\u21920,n = R-1(2t\u21920,n). (8)\nNote that the partial textures do not exhibit 3D consistency at this moment. One way is to aggregate them into a canonical one by averaging. However, trivially averaging the partial textures of different views can lead to a loss of high-frequency details and color diversity. Hence, we propose to merge them in a view-dependent way:\n\u00dbto =\u03a3\u03b7=1 Wt,nR-1(Nn) \u00dbt\u21920,n/\u03a3\u03b7=1 wt,nR-1(Nn) (9)\nwhere Nn is the cosine similarity map rendered at view point Cn with each pixel representing the cosine similarity between the normal vector of the 3D point and the reversed view direction. The term wt,n denotes the weight for view n at time step t. wt,n is set to 1 at time step T and is then linearly interpolated to max(| cos 0|7, Wmin) at time step t', where @ is the angle between Cn and Co, and y is a hyperparameter that balances the influence of different views. Intuitively, this approach ensures that at the beginning of the diffusion process, different views are merged with similar weights, promoting style consistency. As the diffusion progresses, each texel becomes predominantly influenced by a single view, effectively preserving diversity and preventing the loss of high-frequency details.\nAfter merging the denoised partial textures into a single one, we can update the latent texture Ut-1 by adding back the variance with Eq. 2:\nUt-1 =\u221aat-1\u00dft/1-at * \u00db0+ (1-at-1)/1-at (varUt + \u03b2\u03b5\u03b5). (10)\nThe image space latent Zt-1,n for next step of t - 1 can be then obtained by blending the rendered foreground latent R(Ut-1, Ck) with the image space latent 2t-1,n:\nZt-1,n = MnR(Ut\u22121; Cn) + (1 - Mn) 2t-1,n, (11)\nwhere 2t-1,n can be derived by Eq. 2, and Mn represents the binary foreground mask for viewpoint Cn.\nThe final denoised zo,n of each view can be obtained by iterating the denoising steps. We do not perform latent merge in the last 5 steps to prevent artifacts caused by the reprojection of low-resolution latents.\nFinal Texture Synthesis To reconstruct the texture map, we first decode the latent of each viewpoint to generate multi-view images In by D(zo,n). Subsequently, we finalize the texture by:\nTmerge =\u03a3\u03b7=1 WnR-1(Nn) R-1(In)/\u03a3\u03b7=1 WnR-1(Nn) (12)\nwhere Nn is the similarity mask at viewpoint Cn and wn = max(| cos 07, Wmin).\nAfter merging, the texture map still contains invalid pixels that fail to receive color from any perspective due to self-occlusion. A straightforward approach to address this issue is to expand the valid pixels on the texture map using a flood-fill technique within the image space. However, this naive flood-fill method may propagate colors from pixels that are not adjacent in the 3D space, leading to inaccuracies in the final texture map. An optimal solution involves using geodesic distance, but the computational cost"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Implementation details", "content": "We test our method on an NVIDIA A800 GPU, and the entire process was able to finish within 1 minute. The diffusion process takes around 50s with 25 denoising steps at a resolution of 1280, and the final texture synthesis stage takes around 2s. The CFG scale is set to 12. We linearly interpolate the view-dependent weight w for the first 8 steps. The paramters y and Wmin are set as 8 and le - 3. We adopt SDXL (Podell et al. 2023) as our base model and ControlNet-Depth (Zhang, Rao, and Agrawala 2023) trained for SDXL for spatial control. We replace the self-attention layers in the output layers of SDXL by our proposed 3D-aware local attention mechanism in all experiments.\nThe dataset used in evaluation contains 35 meshes with 63 mesh-prompt pairs. The meshes are collected from the publicly open dataset including objarverse (Deitke et al. 2023), shapenet (Chang et al. 2015), and stanford 3D Scanning Repository (Turk and Levoy 1994). We use Xatlas (Young 2016) to automatically unwrap the UV for all meshes. We normalize all meshes to the range of [-0.5, 0.5]"}, {"title": "4.2 Comparisons", "content": "We conduct comparison with four available methods on text-to-texture synthesis, including Text2Tex (Chen et al. 2023b), TEXTure (Richardson et al. 2023), SyncMVD (Liu et al. 2023c), GenesisTex (Gao et al. 2024). We have also compared our method with Meshy-3 (Meshy 2024), a state-of-the-art commercial software that supports generating textures for 3D models using text prompts. The comparison results with Meshy-3 are placed in the Appendix. We strongly recommend readers check the appendix for more details.\nQualitative comparisons. We compare qualitatively with different baselines in Fig. 4. GenesisTex (Gao et al. 2024) produces visually reasonable renderings, but they tend to generate less diverse images. TEXTure (Richardson et al. 2023) and Text2Tex (Chen et al. 2023b) lacks multi-view consistency since it operates on each view independently. SyncMVD (Liu et al. 2023c) yields visually consistent renderings. However, they tend to get blurry results, see the dragon and lucky cat in Fig. 4, since the latent averaging operation in their approach leads to a loss of high-frequency details and color diversity."}, {"title": "Quantitative comparisons", "content": "Following GenesisTex (Gao et al. 2024) and TexFusion (Cao et al. 2023), we report FID (Heusel et al. 2017) and KID (Bi\u0144kowski et al. 2018) scores. We generate depth maps as conditional images for all meshes by rendering them from 12 different viewpoints, each separated by 30-degree intervals. Using these depth maps and our textual prompts, we sample from pretrained image diffusion model to create a set of ground truth images. Additionally, we render meshes with textures generated by different methods using the same views to get the candidate set. We primary focus on the foreground, and we set the background pixels of all images to white.\nIn addition, we also employ Pick Score (Kirstain et al. 2024) to evaluate the visual quality of our texture synthesis results. Pick Score is an CLIP-based scoring function trained on large-scale user preference regarding generated images paired with text prompts. For each mesh, we compute the average Pick Score using the same 12-view rendered images employed for calculating the FID, identifying the method with the highest score as the winning approach for that mesh and calculating the winning rate for each method.\nWe also conducted a user study to analyze the results across three aspects: 1) consistency, 2) diversity, and 3) overall quality. We render the results of different methods into videos that showcase the textured object from a 360\u00b0 rotating view. We randomly pick 15 meshes for each questionnaire. and ask the participants to judge which method matches best for each aspect. Finally, We collected 30 valid answers from professional artists and non-professionals. The whole quantitative results can be found in Tab 1. Our method achieves the highest pick score compared to other methods and is preferred by most human evaluators in terms of consistency, diversity, and overall quality."}, {"title": "4.3 Ablation Studies", "content": ""}, {"title": "Effectiveness of local attention", "content": "To investigate the impact of the cross-view local attention, we visualize the decoded multi-view images of different attention strategy in Fig. 3 and Fig. 5. Fig. 3 illustrates an example with the prompt A cute shiba inu dog. We can discover that the color and pattern of the dog varies a lot across different viewpoints without any cross-view constrain. With global attention, the query view attends to all views in the attention layer and brings higher consistency, but at a cost of losing image details and variance. Our proposed geometry-aware local attention amplifies the local attentions on pixels that are closer in 3D, which not only leads to vivid color and fine-grained details, but also preserves cross-view consistency. Similar in Fig. 5, the cross-view images are more consistent with local attention than the baseline without cross-view attention."}, {"title": "Effectiveness of latent merge pipeline", "content": "We ablate the latent merge pipeline to evaluate the effectiveness of our latent merge strategy in generating consistent textures. As shown in the last column of Fig. 5, the full pipeline with latent merge exhibits the best consistency compared with baselines in the final renderings. Note how the full method achieves the best multi-view consistency and generates rich details, while the baselines without latent merge exhibit severe inconsistencies."}, {"title": "4.4 More Applications", "content": "Our method is designed to be fully compatible with existing Stable Diffusion models without the need for additional training. This makes it readily applicable to a wide range of models available on platforms such as Civitai (civitai 2024) and HuggingFace (Huggingface 2024). Furthermore, our pipeline can be seamlessly integrated with auxiliary models tailored for Stable Diffusion, thereby enriching its versatility in practical scenarios. For instance, we can incorporate the IP-Adapter into our framework to facilitate image-guided texture generation, and leverage various Lo-RAs to achieve distinct artistic styles. The texturing results with LoRAs and IP-Adapters can be found in the supplementary materials."}, {"title": "5 Discussions", "content": ""}, {"title": "Failure Cases", "content": "Our algorithm employs texture dilation to fill the fully-occluded regions, which may wrongly produce overly smoothed results on these fully-occluded areas which should have complex textures. Additionally, the Janus effect is a challenge inherent to methods that utilize pretrained 2D image diffusion models. While this issue is alleviated through the proposed local attention and perspective prompts (as seen in DreamFusion), the inherent bias presented in 2D image diffusion models can still result in unwanted anatomical features."}, {"title": "Limitation", "content": "As a common limitation in the field of texture synthesis using pretrained 2D diffusion models, the alignment between the mesh and texture is not perfect, which is largely due to the limited control capabilities of the currently available ControlNets. It could be improved along with the development of more powerful control models. The baked-in lighting effect is another common limitation in this field, and we will leave it as our future work."}, {"title": "6 Conclusions", "content": "In this article, we propose a pipeline aiming at generating consistent and high-quality textures for 3D meshes using textual prompts. Our method leverages pretrained Stable Diffusion models without any further training or fine-tuning. This makes it highly versatile, capable of handling a wide range of geometry and texture types, and easily adaptable to various models on model-sharing platforms. We believe this work will advance AI-based texturing and opening up new possibilities for 3D content generation."}, {"title": "A Surface space color dilation", "content": "We first divide the original UV map into sub-UV islands using equal-sized grids, as illustrated in Fig. 6 and Fig. 8. Next, we calculate the connectivity of sub-UV islands and generate an adjacency matrix. Then, we iteratively traverse the invalid pixels on the UV map which are invisible from all perspectives. For each invalid pixel, we first pick candidates from textured pixels based on their relative distance in 3D, the cosine similarity of their vertex normal, and the connectivity recorded by the adjacency matrix. We then calculate the color for the invalid pixel by performing a weighted average of these candidates. We iterate this algorithm until all invalid pixels are filled or reach the max step. The detailed algorithm on surface space color dilation is shown in Algorithm. 1. An illustration of this process is shown in Fig. 6. As demonstrated in column 3 of Fig. 8, the UV space dilation method may propagate colors from pixels that are not adjacent in the 3D space, resulting in inaccuracies in the final texture map. In contrast, our surface space color dilation algorithm propagates valid texture color in surface space instead of UV space, thereby effectively addresses inaccurate color propagation when using naive flood-fill method in UV space."}, {"title": "B Implementation details", "content": "We implement our algorithm using an open-source framework: ComfyUI(ComfyUI 2024), and we adopt nvd-iffrast (Laine et al. 2020) for rendering and inverse rendering. We set the strength of ControlNet as 1.0 in all our experiments. As for parameters of surface space dilation algorithm, the grid size s = 64, the distance threshold dth = 0.02, the angle threshold ath = 90\u00b0, the nearest neighbors number n = 30, and iterations iter = 10."}, {"title": "C More Results", "content": "We present additional ablation experiments on local attention in Fig. 7. This figure illustrates the ablation results for various attention mechanisms in multi-view generation without latent merging. Our local attention method demonstrates superior multi-view consistency while effectively preserving intricate details that close to the images generated by the original unconstrained diffusion (row 1). Furthermore, we include results compared with different methods in Fig. 10, 11, and 12. The qualitative comparison with Meshy-3 (Meshy 2024) can be found in Fig. 9. Meshy-3 produces highly contrasting colors with considerable details but tends to generate ghosting artifacts and sometimes over-saturated results. In contrast, our method can produce textures with better visual quality and considerable diversity, while keeping surface consistency. Additional results showcasing our methods across various meshes and styles can be found in Fig. 13, 14, 15, 16, 17, and 18."}]}