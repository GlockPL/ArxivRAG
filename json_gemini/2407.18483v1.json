{"title": "A Role-specific Guided Large Language Model\nfor Ophthalmic Consultation Based on Stylistic\nDifferentiation", "authors": ["Laiyi Fu", "Binbin Fan", "Hongkai Du", "Yanxiang Feng", "Chunhua Li", "Huping Song"], "abstract": "Ophthalmology consultations are crucial for\ndiagnosing, treating, and preventing eye diseases. However,\nthe growing demand for consultations exceeds the\navailability of ophthalmologists. By leveraging large pre-\ntrained language models, we can design effective dialogues\nfor specific scenarios, aiding in consultations. Traditional\nf\u00edne-tuning strategies for question-answering tasks are\nimpractical due to increasing model size and often ignoring\npatient-doctor role function during consultations. In this\npaper, we propose EyeDoctor, an ophthalmic medical\nquestioning large language model that enhances accuracy\nthrough doctor-patient role perception guided and an\naugmented knowledge base with external disease\ninformation. Experimental results show EyeDoctor achieves\nhigher question-answering precision in ophthalmology\nconsultations. Notably, EyeDoctor demonstrated a 7.25%\nimprovement in Rouge-1 scores and a 10.16% improvement\nin F1 scores on multi-round datasets compared to second\nbest model ChatGPT, highlighting the importance of doctor-\npatient role differentiation and dynamic knowledge base\nexpansion for intelligent medical consultations. EyeDoc also\nserves as a free available web based service and souce code\nis available at https://github.com/sperfu/EyeDoc.", "sections": [{"title": "I. INTRODUCTION", "content": "PHTHALMIC consultations are a critical component\nof the medical field [21], involving the diagnosis,\ntreatment and prevention of eye diseases. With an\naging population and the impact of modern lifestyles, eye health\nissues are becoming increasingly prevalent, driving up the\ndemand for ophthalmology consultations. Doctors typically\ndiagnose eye diseases by examining symptoms, physical signs,\nand imaging test results. Recently, the demand for online\nconsultations has surged due to the growth of internet and\ntelemedicine technologies. People are increasingly opting for\nonline consultations to communicate with doctors, which saves\nthe time and effort of visiting hospitals and waiting in long\nqueues. This trend is particularly beneficial for those living in\nremote areas with limited access to specialized healthcare\nservices [30,31]. Online consultations save time and effort,\nespecially for those in remote areas with limited access to\nspecialized healthcare. They offer convenience, flexibility, and\nreduced travel costs. The COVID-19 pandemic has further\naccelerated the adoption of telemedicine [22,28], making it\ncrucial for maintaining healthcare services while minimizing\ninfection risk.\nHowever, there are some pain points in ophthalmology\nconsultation. Firstly, the number of ophthalmologists in some\nareas is insufficient, leading to long waiting times and\nnegatively impacting patient experience. Secondly, some\nprimary care doctors lack the expertise to accurately diagnose\nand treat ophthalmic conditions, resulting in a high rate of\nmisdiagnosis [31]. Lastly, the traditional ophthalmology\nconsultation process often requires patients to visit the hospital\nin person, increasing their time and cost burden while also\nlimiting the efficiency of doctors' diagnoses and treatments.\nThese issues highlight the need for innovative solutions to\nimprove the accessibility and quality of ophthalmic care, both\nin-person and online.\nIn recent years, with the continuous progress of Natural\nLanguage Processing (NLP) technology and Large Language\nModels (LLMs), LLMs have been gradually applied to various\nvertical domains within the healthcare industry due to their\npowerful semantic understanding and knowledge\ngeneralization capabilities. In the biomedical crossover,\nJinhyuk Lee proposed BioBERT [5], a biomedical domain\nsynthesis model pre-trained on a large biomedical corpus,\nwhich achieved commendable performance in several\nbiomedical natural language processing tasks such as bio-\nknowledge quizzing. What's more, In the field of intelligent"}, {"title": "II. DATA PROCESSING", "content": "Current research in the field of medical consultation mainly\nfocuses on data-driven domain adaptation of generalized\nmacromodels through simulated data generated from structured\nmedical documents or medical knowledge graphs. Given the\nscarcity of real conversation data for ophthalmic disease\ndiagnosis, this paper systematically collect different types of\ndata from three Chinese online resources: QuickAskDoctor's\nwebsite which is an online medical and health service platform\ncreated by Zhuhai Health Cloud Technology Co., Ltd; Ding\nXiang Doctor's website which is a medical and healthcare\nservice platform with legal Internet hospital license developed\nby Lilac Garden team, and 99 Health Net which is an online\nmedical health platform, dedicated to providing users with\ncomprehensive and detailed disease information and medical\nservice resources. Details on the dataset and prompt design are\nin Supplementary Materials.\nWe design prompt statements [10] for the ophthalmic\nconsultation dialogues data by using the powerful semantic\nunderstanding and text generation capabilities of ChatGPT 3.5\nturbo API. This preprocessing step aims to remove irrelevant"}, {"title": "III. METHOD", "content": ""}, {"title": "A. Algorithmic Framework for Intelligent Medical\nQuestioning", "content": "In EyeDoctor, a patient u initiates a dialogue by providing\nthe model with information such as disease symptoms. The\nentire dialogue can be represented as $D = {Su, Su}_{t=1}$, where T\nrepresents the number of dialogue turns. A dialogue turn t\nconsists of the user's dialogue request $Su = {wn}_{n=1}$ and the\nalgorithm's response $S = {wn}_{n=1}$, where $w_$ represents a\nword from the vocabulary W, N represents the length of the\ndialog, n represents the nth word in the dialog statement.\nDuring the dialogue, the disease document knowledge base $k \\in$\nK serves as an external knowledge source to augment the\npreceding dialogue, enhancing the model's diagnostic\ncapabilities [35]. Here, $K = {wn}_{n=1}$ represents a single piece\nof disease information."}, {"title": "B. Knowledge Representation Model for Medical\nConsultations", "content": "In the highly specialized domain of medicine, there are a\nlarge number of medical terminologies and unique text\norganization and information expression patterns, and it is\ndifficult for a generic pre-trained language model that has not\nbeen customized and optimized to fully demonstrate excellent\nexpressiveness and adaptability [23]. For this reason, based on\na large amount of medical text single-round data collected in\nthis paper, the original ROBERTa model is fine-tuned to the\nmedical domain using word masking (MLM) [6], and a\nspecialized model (DiagBERT) [15] for the medical\nconsultation domain is constructed to achieve more accurate\nand efficient processing and understanding of complex text\ninformation in this domain. The constructed formulas are as\nfollows:\nD' = Masked(D).\nCLSD', Ex1:xn = ROBERTa(Embedding(D')).\nWhere D denotes the content of a single round of"}, {"title": "C. Doctor-Patient Role Dual Coding Learner", "content": "Most of the existing medical consultation algorithms rely on\nmassive medical consultation data to optimize diagnostic\naccuracy and treatment plans, and although these models\nperform well in general disease diagnosis and treatment, they\nlargely ignore the significant group differences between doctors\nand patients [29]. In the real medical consultation scenario, the\npatient, as the person who experienced the disease, provides\ninformation about the specific manifestations of the disease, the\nduration of the cycle, and other symptom characteristics [32].\nDoctors, as providers of medical services, have professional"}, {"title": "D. Knowledge Enhancement Algorithm Based on\nDisease Knowledge Repository", "content": "In today's fast-changing medical field, the emergence of\nemerging and rare diseases and the continuous advancement of\ndisease diagnosis and treatment methods pose a serious\nchallenge to the existing knowledge-consolidated medical\ndiagnosis algorithms. Emerging and rare diseases often have\nunique clinical manifestations, diagnostic criteria, and\ntreatment strategies, which require highly specialized\ntheoretical knowledge and the ability to acquire the latest\nmedical information. Therefore, in the design of medical\nconsultation algorithms, a knowledge enhancement algorithm\nbased on the disease knowledge base is proposed to ensure the\nscalability of the diagnostic logic and the implementation of the\ntreatment recommendations of the consultation model, in order\nto enhance the adaptability of the medical consultation model\nin the face of emerging and rare diseases [8].\nAmong the collected knowledge information of a single\ndisease, the disease overview concisely summarizes the main\nfeatures of the current disease; the disease symptoms list in\ndetail the various signs and discomforts that the patient may\nexhibit, providing strong support for the diagnostic algorithm\nto analyze the details of the symptoms mentioned by the user;\nand the disease treatment protocols cover the mainstream\ntreatment protocols in the current medical practice for the\ndiagnostic model to refer to these treatment protocols when\ngenerating responses, ensuring the professionalism and\naccuracy of the recommendations. These three pieces of\ninformation are crucial for the construction of the disease\nknowledge enhancement algorithm. Therefore, the knowledge"}, {"title": "E. Parameter tuning strategy", "content": "In order to solve the problems encountered in the process of\nfull parameter fine-tuning of large language models in terms of\ndata, arithmetic power, compatibility, deployment, and many\nother aspects, we decided to adopt the optimization strategy of\nlarge language models with efficient parameter fine-tuning as\nthe core. In view of the diversity of efficient parameter fine-\ntuning and the difficulty of selection, this paper adopts an\nefficient parameter fine-tuning method that integrates Prefix\nTuning [11] and LoRA [12] to fine-tune the parameters of e.g.,\nLLaMA [17,18] big language model, and constructs a medical\nconsultation algorithm for doctor-patient role perception and\ndisease knowledge enhancement.\nWe combine the low-rank matrices $Wdowm \\in R^{dxr}$ and\n$Wup \\in R^{dxr}$ to be trained with the Query and Value projection\nmatrices in multi-head attention to influence the parameter\nupdate process. Assuming that the input to the multi-head\nattention in layer l is $h\u2081 \u2208 R^{d\u00d7r}$, the fusion formula for Query is\nshown below:\n$Q = (W) + a \u00b7 WdowmWup)h\u2081$.\nWhere W\u2208 Rdxd is the original weight matrix with\nparameters frozen during training and a is the hyperparameter.\nOn the other hand, this paper employs the Prefix Tuning [11]"}, {"title": "IV. EXPERIMENT", "content": ""}, {"title": "A. Evaluation Metrics", "content": "In this experiment, we validate the performance of\nEyeDoctor using several metrics. For the dialog subtask, we use\nDistinct [13] and Bleu [14] to automate the verification of the\nmodel's dialog response diversity and consistency, respectively.\nDistinct@n calculates the ratio of the number of distinct n-\ngrams appearing in all the text generated content to the total\nnumber of words, and a higher score on Distinct@n means that\nthe generated text contains more unique n-grams, i.e., it exhibits"}, {"title": "V. CONCLUSION", "content": "Ophthalmic consultation is an extremely important aspect of\nthe medical field, requiring interaction with doctors to obtain\ndiagnostic information for the diagnosis, treatment, and\nprevention of eye diseases. However, the current number of\nophthalmologists is insufficient to meet the growing demand\nfor ophthalmic consultations. By drawing an analogy between\nconsultation dialogues and dialogue systems in natural\nlanguage processing, the use of large pre-trained language\nmodels for dialogue design in specific scenarios can effectively\nassist consultations, inevitably enhancing efficiency and\naccuracy. However, using traditional fine-tuning to address\nQ&A tasks becomes impractical as model size and number of\ntasks increase. Recent studies have proposed various efficient\nparameter transfer learning methods, where fine-tuning a small\nnumber of additional parameters can yield significant\nperformance improvements. In this work, we propose a multi-\nmodule fine-tuning strategy based on the differentiated styles\nof ophthalmic patients and doctors. By decomposing\nparameters, we achieved an efficient fine-tuning learning\nmethod and established a unified framework for their\ninterconnections. Experimental results show that in ophthalmic\nconsultation Q&A scenarios, our model EyeDoctor achieves\ngreater accuracy in answers. Ablation experiments further\ndemonstrate the importance of fully considering the\ncharacteristics of doctor-patient roles and exogenous\nknowledge augmentation for improving the algorithmic model\nmedical interrogation.\nNonetheless, there are areas for improvement in the current\nmodel. In reality, doctors have different consultation, treatment,\nand prescribing styles, which the model has not yet decomposed"}]}