{"title": "A Role-specific Guided Large Language Model\nfor Ophthalmic Consultation Based on Stylistic\nDifferentiation", "authors": ["Laiyi Fu", "Binbin Fan", "Hongkai Du", "Yanxiang Feng", "Chunhua Li", "Huping Song"], "abstract": "Ophthalmology consultations are crucial for\ndiagnosing, treating, and preventing eye diseases. However,\nthe growing demand for consultations exceeds the\navailability of ophthalmologists. By leveraging large pre-\ntrained language models, we can design effective dialogues\nfor specific scenarios, aiding in consultations. Traditional\nfine-tuning strategies for question-answering tasks are\nimpractical due to increasing model size and often ignoring\npatient-doctor role function during consultations. In this\npaper, we propose EyeDoctor, an ophthalmic medical\nquestioning large language model that enhances accuracy\nthrough doctor-patient role perception guided and an\naugmented knowledge base with external disease\ninformation. Experimental results show EyeDoctor achieves\nhigher question-answering precision in ophthalmology\nconsultations. Notably, EyeDoctor demonstrated a 7.25%\nimprovement in Rouge-1 scores and a 10.16% improvement\nin F1 scores on multi-round datasets compared to second\nbest model ChatGPT, highlighting the importance of doctor-\npatient role differentiation and dynamic knowledge base\nexpansion for intelligent medical consultations. EyeDoc also\nserves as a free available web based service and souce code\nis available at https://github.com/sperfu/EyeDoc.", "sections": [{"title": "I. INTRODUCTION", "content": "PHTHALMIC consultations are a critical component\nof the medical field [21], involving the diagnosis,\ntreatment and prevention of eye diseases. With an\naging population and the impact of modern lifestyles, eye health\nissues are becoming increasingly prevalent, driving up the\ndemand for ophthalmology consultations. Doctors typically\ndiagnose eye diseases by examining symptoms, physical signs,\nand imaging test results. Recently, the demand for online\nconsultations has surged due to the growth of internet and\ntelemedicine technologies. People are increasingly opting for\nonline consultations to communicate with doctors, which saves\nthe time and effort of visiting hospitals and waiting in long\nqueues. This trend is particularly beneficial for those living in\nremote areas with limited access to specialized healthcare\nservices [30,31]. Online consultations save time and effort,\nespecially for those in remote areas with limited access to\nspecialized healthcare. They offer convenience, flexibility, and\nreduced travel costs. The COVID-19 pandemic has further\naccelerated the adoption of telemedicine [22,28], making it\ncrucial for maintaining healthcare services while minimizing\ninfection risk.\nHowever, there are some pain points in ophthalmology\nconsultation. Firstly, the number of ophthalmologists in some\nareas is insufficient, leading to long waiting times and\nnegatively impacting patient experience. Secondly, some\nprimary care doctors lack the expertise to accurately diagnose\nand treat ophthalmic conditions, resulting in a high rate of\nmisdiagnosis [31]. Lastly, the traditional ophthalmology\nconsultation process often requires patients to visit the hospital\nin person, increasing their time and cost burden while also\nlimiting the efficiency of doctors' diagnoses and treatments.\nThese issues highlight the need for innovative solutions to\nimprove the accessibility and quality of ophthalmic care, both\nin-person and online.\nIn recent years, with the continuous progress of Natural\nLanguage Processing (NLP) technology and Large Language\nModels (LLMs), LLMs have been gradually applied to various\nvertical domains within the healthcare industry due to their\npowerful semantic understanding and knowledge\ngeneralization capabilities. In the biomedical crossover,\nJinhyuk Lee proposed BioBERT [5], a biomedical domain\nsynthesis model pre-trained on a large biomedical corpus,\nwhich achieved commendable performance in several\nbiomedical natural language processing tasks such as bio-\nknowledge quizzing. What's more, In the field of intelligent"}, {"title": "II. DATA PROCESSING", "content": "Current research in the field of medical consultation mainly\nfocuses on data-driven domain adaptation of generalized\nmacromodels through simulated data generated from structured\nmedical documents or medical knowledge graphs. Given the\nscarcity of real conversation data for ophthalmic disease\ndiagnosis, this paper systematically collect different types of\ndata from three Chinese online resources: QuickAskDoctor's\nwebsite which is an online medical and health service platform\ncreated by Zhuhai Health Cloud Technology Co., Ltd; Ding\nXiang Doctor's website which is a medical and healthcare\nservice platform with legal Internet hospital license developed\nby Lilac Garden team, and 99 Health Net which is an online\nmedical health platform, dedicated to providing users with\ncomprehensive and detailed disease information and medical\nservice resources. Details on the dataset and prompt design are\nin Supplementary Materials.\nWe design prompt statements [10] for the ophthalmic\nconsultation dialogues data by using the powerful semantic\nunderstanding and text generation capabilities of ChatGPT 3.5\nturbo API. This preprocessing step aims to remove irrelevant"}, {"title": "III. METHOD", "content": null}, {"title": "A. Algorithmic Framework for Intelligent Medical\nQuestioning", "content": "In EyeDoctor, a patient u initiates a dialogue by providing\nthe model with information such as disease symptoms. The\nentire dialogue can be represented as D = {Su, Su}t=1, where T\nrepresents the number of dialogue turns. A dialogue turn t\nconsists of the user's dialogue request Su = {wn}n=1 and the\nalgorithm's response S = {wn}n=1, where w_represents a\nword from the vocabulary W, N represents the length of the\ndialog, n represents the nth word in the dialog statement.\nDuring the dialogue, the disease document knowledge base k \u2208\nK serves as an external knowledge source to augment the\npreceding dialogue, enhancing the model's diagnostic\ncapabilities [35]. Here, K = {wn}n=1 represents a single piece\nof disease information."}, {"title": "B. Knowledge Representation Model for Medical\nConsultations", "content": "In the highly specialized domain of medicine, there are a\nlarge number of medical terminologies and unique text\norganization and information expression patterns, and it is\ndifficult for a generic pre-trained language model that has not\nbeen customized and optimized to fully demonstrate excellent\nexpressiveness and adaptability [23]. For this reason, based on\na large amount of medical text single-round data collected in\nthis paper, the original ROBERTa model is fine-tuned to the\nmedical domain using word masking (MLM) [6], and a\nspecialized model (DiagBERT) [15] for the medical\nconsultation domain is constructed to achieve more accurate\nand efficient processing and understanding of complex text\ninformation in this domain. The constructed formulas are as\nfollows:\nD' = Masked(D).\n(1)\nCLSD', Ex1:xn = ROBERTa(Embedding(D')). (2)\nWhere D denotes the content of a single round of"}, {"title": "C. Doctor-Patient Role Dual Coding Learner", "content": "Most of the existing medical consultation algorithms rely on\nmassive medical consultation data to optimize diagnostic\naccuracy and treatment plans, and although these models\nperform well in general disease diagnosis and treatment, they\nlargely ignore the significant group differences between doctors\nand patients [29]. In the real medical consultation scenario, the\npatient, as the person who experienced the disease, provides\ninformation about the specific manifestations of the disease, the\nduration of the cycle, and other symptom characteristics [32].\nDoctors, as providers of medical services, have professional\nknowledge background and clinical practice experience, and\nthey pay more attention to customizing personalized diagnosis\nand treatment plans and preventive measures according to the\npatient's condition during the consultation and communication.\nTherefore, this paper proposes a dual coding learner for doctor-\npatient roles to more accurately simulate the real doctor-patient\nconsultation scenarios and improve the algorithm performance\nand user experience."}, {"title": "D. Knowledge Enhancement Algorithm Based on\nDisease Knowledge Repository", "content": "In today's fast-changing medical field, the emergence of\nemerging and rare diseases and the continuous advancement of\ndisease diagnosis and treatment methods pose a serious\nchallenge to the existing knowledge-consolidated medical\ndiagnosis algorithms. Emerging and rare diseases often have\nunique clinical manifestations, diagnostic criteria, and\ntreatment strategies, which require highly specialized\ntheoretical knowledge and the ability to acquire the latest\nmedical information. Therefore, in the design of medical\nconsultation algorithms, a knowledge enhancement algorithm\nbased on the disease knowledge base is proposed to ensure the\nscalability of the diagnostic logic and the implementation of the\ntreatment recommendations of the consultation model, in order\nto enhance the adaptability of the medical consultation model\nin the face of emerging and rare diseases [8].\nAmong the collected knowledge information of a single\ndisease, the disease overview concisely summarizes the main\nfeatures of the current disease; the disease symptoms list in\ndetail the various signs and discomforts that the patient may\nexhibit, providing strong support for the diagnostic algorithm\nto analyze the details of the symptoms mentioned by the user;\nand the disease treatment protocols cover the mainstream\ntreatment protocols in the current medical practice for the\ndiagnostic model to refer to these treatment protocols when\ngenerating responses, ensuring the professionalism and\naccuracy of the recommendations. These three pieces of\ninformation are crucial for the construction of the disease\nknowledge enhancement algorithm. Therefore, the knowledge\ninformation of a single disease document is constructed in the\norder of disease name, disease overview, disease symptoms,\ndisease treatment, and other information.\nAfter constructing the disease knowledge base, the\nDiagBERT disease knowledge characterization model is used\nto accurately characterize the knowledge of each disease\nknowledge document in the conversation above D and the\ndisease knowledge base K using the DiagBERT disease\nknowledge characterization model. The knowledge\ncharacterization algorithm for the conversation above D and a\nsingle disease document knowledge K is shown below:\nCLSD, HD = DiagBERT(Embedding(D)). (11)\nCLSK, HK = DiagBERT(Embedding(K)). (12)\n,\nWhere CLSD, CLSK denote the textual semantic\nrepresentation of the dialog above content D and disease\nknowledge K.\nNext, the semantic similarity between the current dialog\nabove and all diseases in the disease knowledge base K is\ncalculated using the cosine similarity matching algorithm. For\nthe dialog above D and a single disease document knowledge K,\nthe similarity calculation formula is as follows:\nS\u0130MD,K = \\frac{CLSD \\cdot CLSK}{||CLSD || ||CLSK||}. (13)"}, {"title": "E. Parameter tuning strategy", "content": "In order to solve the problems encountered in the process of\nfull parameter fine-tuning of large language models in terms of\ndata, arithmetic power, compatibility, deployment, and many\nother aspects, we decided to adopt the optimization strategy of\nlarge language models with efficient parameter fine-tuning as\nthe core. In view of the diversity of efficient parameter fine-\ntuning and the difficulty of selection, this paper adopts an\nefficient parameter fine-tuning method that integrates Prefix\nTuning [11] and LoRA [12] to fine-tune the parameters of e.g.,\nLLaMA [17,18] big language model, and constructs a medical\nconsultation algorithm for doctor-patient role perception and\ndisease knowledge enhancement.\nWe combine the low-rank matrices Wdowm \u2208 Rdxr and\nWup \u2208 Rdxr to be trained with the Query and Value projection\nmatrices in multi-head attention to influence the parameter\nupdate process. Assuming that the input to the multi-head\nattention in layer l is h\u2081 \u2208 Rd\u00d7r, the fusion formula for Query is\nshown below:\nQ = (W)T + \u03b1 \u00b7 (WdowmWup)T hi. (14)\nWhere W\u2208 Rdxd is the original weight matrix with\nparameters frozen during training and \u03b1 is the hyperparameter.\nOn the other hand, this paper employs the Prefix Tuning [11]"}, {"title": "IV. EXPERIMENT", "content": null}, {"title": "A. Evaluation Metrics", "content": "In this experiment, we validate the performance of\nEyeDoctor using several metrics. For the dialog subtask, we use\nDistinct [13] and Bleu [14] to automate the verification of the\nmodel's dialog response diversity and consistency, respectively.\nDistinct@n calculates the ratio of the number of distinct n-\ngrams appearing in all the text generated content to the total\nnumber of words, and a higher score on Distinct@n means that\nthe generated text contains more unique n-grams, i.e., it exhibits\nhigher diversity and the generated text content is richer.\nBleu@n calculates the degree of matching between the\ngenerated text content and the original text content at the n-\ngram level, and a higher Bleu@n score means that the\ngenerated text is more compliant with the original text. In\naddition, we also introduce the ROUGE \u2013 1 evaluation metric to\nverify the consistency characteristics of the model-generated\ntext.\nIn order to demonstrate EyeDoctor's medical consultation\nexpertise and semantic accuracy in medical consultation\nscenarios, we calculates the BERTScore semantic similarity\nscores for each benchmark model based on the BERT-Base-\nChinese knowledge representation model. Compared to\ntraditional evaluation metrics based on rule matching,\nBERTScore is not constrained by fixed-length n-gram matching\nrules. It considers global semantic consistency and long-\ndistance dependencies when evaluating texts, achieving fine-\ngrained semantic understanding and providing evaluation\nresults that are closer to human perception. BERTScore\nevaluates the quality of text generation by calculating the recall\nRBERT, precision PBERT, and F1 value FBERT [16]."}, {"title": "B. Baseline Models", "content": "In order to practically evaluate the actual effectiveness of\nEyeDoctor in medical consultation scenarios, two types of\nmethods, namely, the medical professional consultation grand\nmodel and the generalized grand language model, were selected\nas baseline controls in this study."}, {"title": "C. Hyperparameterization", "content": "EyeDoctor used a NVIDIA A40 graphics card for hardware\nselection for model parameter optimization. Two parametric\nversions of the model, EyeDoctor-1B and EyeDoctor-7B, were\nobtained using TinyLLaMA(1B) and LLaMA-2-7B as the base\nmodel, respectively. During the training process, the base\nmodel parameters are frozen and are not involved in the model\nparameter optimization process. RoBERTa-base-Chinese is\nused as the knowledge fusion learning model for characterizing\nthe disease knowledge base and doctor-patient roles. The\nmaximum length of exogenous knowledge of disease\ndocuments was 512 characters, while the maximum length for\nmulti-round conversation history was set to 1024 characters.\nThe rank of the low-rank matrix in LoRA was set to 8, the\nlength of Prefix Tuning was set to 100, the batch size was set to\n32, the learning rate was set to 0.0005, and the first 10% of the\ntraining set was pre-warmed with a linear learning rate, and the\nparameter optimization was performed using AdamW. We will\ntrain 5 rounds and save the model weights that perform\noptimally on the validation set. The data set is divided\naccording to the 8:1:1 division of training set, validation set,\nand test set. For the baseline model, in order to ensure the\nfairness of the experiment and to play the optimal performance\nof the baseline model, the same test set is used, and the response\ntext is generated through the form of API calls. Finally, when\nconducting the experimental investigation, all the measurement\nindexes were tested several times to ensure that the\nexperimental results meet the demand of statistical significance."}, {"title": "D. Automated Quantitative Assessment", "content": "Tables II and III present the automatic quantization results of\ndifferent dialog algorithms on the single-round and multi-round\nquestioning datasets collected in this paper. The compared\nmethods are categorized into three groups: healthcare-specific\nlarge models, generalized large language models, and the\nmodels proposed in this chapter. Both the healthcare-specific\nlarge models and the generalized large language model are\nbased on the automated evaluation scheme of previous work,\nutilizing a zero-shot approach to test the experimental effects\non the test set [1].\nFrom the automated evaluation results, among the medical-\nspecific baseline models, HuatuoGPT-7B outperforms other\nmedical models in overall performance due to its extensive\nknowledge base of Chinese medical literature and numerous\nmedical single-round Q&A datasets. DoctorGLM-6B contains\na large number of repetitive statements in the generated medical\nresponses, resulting in overall performance inferior to BenTsao\n-7B medical questioning model. In terms of general-purpose\nlarge language models, ChatGPT [2], as a popular high-\nperformance large language model today, has achieved better\nresults in general-purpose natural language processing domain\ntests.\nThe LLaMA-2-7B benchmark model chosen in this paper is\nfully parameter fine-tuned based on massive Chinese dataset,\nwhich has obvious advantages in processing Chinese data.\nTherefore, ChatGPT [2] and Llama-2-7B models have close\nperformance, both surpassing TinyLLaMA and ChatGLM-6B.\nIn the automated evaluation validation, the proposed model\nEyeDoctor outperforms all of the above benchmark models,\nwith a particularly notable advantage in the multi-round\nquestioning dataset. For instance, Rouge-l scores are ahead of\nChatGPT [2] by 7.25% on the single-round dataset and 10.16%\non the multi-round dataset. This demonstrates both the\nprofessional performance of the models mentioned in this paper\nin specialized medical consultations and the importance of"}, {"title": "E. Evaluating the Semantic Accuracy of Medical\nQuestioning", "content": "To show the semantic accuracy of EyeDoctor model for\nmedical consultation, this chapter calculates BERTScore\nsemantic similarity score based on BERT-Base-Chinese\nknowledge representation model. The experimental results are\nshown in Tables IV and V."}, {"title": "F. Ablation Experiment in Doctor-Patient Role\nPerception and Knowledge Enhancement", "content": "In order to show the influence of exogenous knowledge base\nand doctor-patient role representation on medical consultation\nresults in single- and multi-round medical consultation\nscenarios, we designed an ablation experiment in the same\nexperimental environment for experimental validation. In the\nablation experiment, \u201cno knowledge base\u201d means removing the\nexogenous knowledge augmentation module, and \u201cno doctor-"}, {"title": "G. Fusion of High-efficiency Parametric Fine-tuning\nAblation Experiments", "content": "In the medical consultation scenario, to demonstrate the\neffectiveness of the hybrid efficient parameter tuning method\nthat incorporates Prefix Tuning [11] and LoRA [12], we design\nan ablation experiment for the EyeDoctor-1B model in the same\nexperimental environment for experimental validation. In the\nablation experiment, \u201cOnly Prefix Tuning\u201d indicates that only\nPrefix Tuning is used for parameter fine-tuning, and \"Only\nLoRA\" indicates that only LoRA is used for parameter fine-\ntuning.\nThe experimental results, presented in Fig. 9, show that both\nLoRA [12] and Prefix Tuning [11] contribute positively to\nimproving dialog quality. The efficient parameter fine-tuning\nstrategy that integrates both approaches achieves the best\nresults across all metrics. In terms of the degree of impact,\nremoving Prefix Tuning has a greater impact on dialog quality.\nThis is because the doctor-patient role feature data is integrated\ninto the large language model through Prefix Tuning parameter\nfine-tuning. The absence of the Prefix Tuning module results\nin the lack of the model's doctor-patient role perception module,\ndirectly leading to a decline in dialog quality."}, {"title": "V. CONCLUSION", "content": "Ophthalmic consultation is an extremely important aspect of\nthe medical field, requiring interaction with doctors to obtain\ndiagnostic information for the diagnosis, treatment, and\nprevention of eye diseases. However, the current number of\nophthalmologists is insufficient to meet the growing demand\nfor ophthalmic consultations. By drawing an analogy between\nconsultation dialogues and dialogue systems in natural\nlanguage processing, the use of large pre-trained language\nmodels for dialogue design in specific scenarios can effectively\nassist consultations, inevitably enhancing efficiency and\naccuracy. However, using traditional fine-tuning to address\nQ&A tasks becomes impractical as model size and number of\ntasks increase. Recent studies have proposed various efficient\nparameter transfer learning methods, where fine-tuning a small\nnumber of additional parameters can yield significant\nperformance improvements. In this work, we propose a multi-\nmodule fine-tuning strategy based on the differentiated styles\nof ophthalmic patients and doctors. By decomposing\nparameters, we achieved an efficient fine-tuning learning\nmethod and established a unified framework for their\ninterconnections. Experimental results show that in ophthalmic\nconsultation Q&A scenarios, our model EyeDoctor achieves\ngreater accuracy in answers. Ablation experiments further\ndemonstrate the importance of fully considering the\ncharacteristics of doctor-patient roles and exogenous\nknowledge augmentation for improving the algorithmic model\nmedical interrogation.\nNonetheless, there are areas for improvement in the current\nmodel. In reality, doctors have different consultation, treatment,\nand prescribing styles, which the model has not yet decomposed"}, {"title": "SUPPLEMENTARY NOTE", "content": "In this Note, we plan to give a detailed description on Dataset.\nCurrent research in medical consultation primarily focuses on\ndata-driven domain adaptation of generalized models using\nsimulated data from structured medical documents or\nknowledge graphs. Due to the lack of real conversation data for\nophthalmic disease diagnosis, this paper collects and organizes\nthree types of data resources utilizing web crawler technology:\na single-round conversation dataset from QuickAskDoctor's\nwebsite, a multi-round professional conversation dataset from\nDing Xiang Doctor's website, and an ophthalmology disease\ntext knowledge base. To address non-standardized terminology\nand sensitive information (e.g., doctor's names, addresses) in\nthe raw data, we pre-processed each data entry using the\nsemantic parsing capabilities of ChatGPT. This preprocessing\nstep removes irrelevant and sensitive information, standardizes\nmedical terminology, and ensures the data aligns with clinical\nrealities in ophthalmology. After that, these consultation\ndialogues were manually curated by clinical ophthalmologists.\nThis approach provides a robust data foundation for developing\nefficient and accurate ophthalmology intelligent consultation\nalgorithms."}, {"title": "A. Single-Turn Dialogue Dataset", "content": "Quickly Ask Doctor is a Chinese online medical and health\nservice platform created by Zhuhai Health Cloud Technology\nCo., Ltd. integrating the functions of disease inquiry, expert\nconsultation and health care guidance, aiming to provide users\nwith convenient and efficient medical and health services\nthrough the online and offline integration mode. Currently, the\nplatform has accumulated more than 100 million single rounds\nof medical Q&A data, with detailed content, including Q&A\ncontent, drug recommendations, department classification, etc.,\nto fully meet the experimental needs. In this paper, we focus on\nthe medical Q&A data during the period of June 1 to November\n10, 2022, and collect 1,292,012 single rounds of data from all\ndepartments, covering 19 first-level departments such as\npediatrics, internal medicine, surgery, and 127 second-level\ndepartments. In addition to the single-round conversation data,\nadditional information such as the type of department visited,\ntime of visit, and recommended medications are also collected\nduring data collection. In this paper, we will use these single-\nround data to perform unsupervised fine-tuning of the ROBERT\na large language model to facilitate further subsequent\ncharacterization of the doctor-patient relationship and external\nknowledge sources.\nOphthalmology, as an important department under\nPentacameral Medicine, accounted for 3.23% of the total data\nvolume. S_Fig.2 shows a sample of a single ophthalmology\nconsultation data collected. In response to a patient's"}, {"title": "B. Multi-Turn Dialogue Dataset", "content": "Dingxiang Doctor is a famous medical and healthcare service\nplatform with legal Internet hospital license developed by Lilac\nGarden team. It combines online consultation, hospital inquiry,\ndisease self-examination, report interpretation and other\nservices, and has more than 50,000 specialized doctors of T3A\nand above, which is dedicated to solving the problem of\nunbalanced distribution of medical resources in China. In this\npaper, we have collected a dataset of ophthalmology multi-\nround medical consultations from the website of Ding Xiang\nDoctor, which contains 8,647 multi-round conversations from\n108 doctors. The average number of conversation rounds for a\nsingle consultation is 6.27, and the average number of\nconversations for a single doctor is 80.06.\nS_Fig.3 shows a sample of collected multi-round\nconversation data from Ding Xiang Doctor ophthalmology\ndepartment, in which a patient consults Dr. Huang about his\nmother's red and swollen eyes. Different from the generalized\nlarge language models such as ChatGPT [1] or the single-round\nconversations in which the doctor directly gives the user\ntreatment suggestions, the doctor in the sample tries to get more\ninformation about the patient's symptoms in the form of\nquestioning and finally gives the user treatment suggestions.\nThis approach is more in line with real medical consultation\nscenarios, but the raw data needs to be further processed with\nsensitive information such as the doctor's name and the\nstandardization of medical terminology. So we designed\nprompt engineering for processing and filtering these dialogues,\ndetailed in Section D. Eventually, the processed multi-round\nconversation data will be applied to the following experiments\nto construct intelligent consultation algorithms for real\nophthalmology environments. Furthermore, these consultation\ndialogues were manually curated by clinical ophthalmologists\nto ensure the highest data quality and relevance."}, {"title": "C. Ophthalmic Disease Knowledge Repository", "content": "99 Health Net is an another online medical health platform,\ndedicated to providing users with comprehensive and detailed\ndisease information and medical service resources. The\nplatform covers various aspects of diseases, including\noverviews, symptoms, causes, diagnoses, treatments, and"}, {"title": "D. Prompt Engineering Designed for multi-turn\nophthalmic consultation", "content": "Prompt Engineering [3] is a specialized design technique for\nlarge language models, focusing on how to carefully design,\noptimize, and apply input prompts to effectively stimulate the\nlearning ability and knowledge structure within large language\nmodels. This technique guides the model to generate output\ncontent that is more accurate and tailored to the desired\nobjectives. Prompt Engineering is crucial for maximizing the\nvalue of large language models, as it greatly assists researchers\nin utilizing the models more effectively for customized natural\nlanguage processing tasks.\nIn this study, prompt statements are designed for the original\nsingle-turn and multi-turn ophthalmic consultation dialogues.\nThen, leveraging the powerful semantic understanding and text\ngeneration capabilities of ChatGPT 3.5 turbo, the model is\nencouraged to generate a series of professionally oriented\ndialogues with natural fluency, appropriate length, and correct\nuse of ophthalmic medical terminology. At the same time,\nprivacy protection principles are also taken into consideration\nto reduce the risk of sensitive information leakage for doctors\nor patients.\nThe data preprocessing process based on ChatGPT [1] is\nillustrated in Algorithm 1. Firstly, prompt templates are set up\nfor the original single-turn and multi-turn ophthalmic\nconsultation dialogue data, respectively. Subsequently, the raw\ninput content is embedded into the predetermined prompt\ntemplates to form contextually guided input data. Then, the\nintegrated information is submitted to ChatGPT [1] for\nprocessing, resulting in output responses. Finally, upon\nobtaining the response output from the ChatGPT model [1], a\nseries of rule checks are executed. Only responses that fully\nadhere to the preset specifications are accepted as the final\nstandardized output results. In cases where responses do not\nmeet the criteria, ChatGPT [1] is utilized again to generate\nresponses, and the rule verification process is repeated."}]}