{"title": "HASNAS: A Hardware-Aware Spiking Neural Architecture Search Framework for Neuromorphic Compute-in-Memory Systems", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad Shafique"], "abstract": "Spiking Neural Networks (SNNs) have shown capabilities for solv-ing diverse machine learning tasks with ultra-low-power/energycomputation. To further improve the performance and efficiencyof SNN inference, the Compute-in-Memory (CIM) paradigm withemerging device technologies such as resistive random access me-mory is employed. However, most of SNN architectures are devel-oped without considering constraints from the application and theunderlying CIM hardware (e.g., memory, area, latency, and energyconsumption). Moreover, most of SNN designs are derived fromthe Artificial Neural Networks, whose network operations are dif-ferent from SNNs. These limitations hinder SNNs from reachingtheir full potential in accuracy and efficiency. Toward this, we pro-pose HASNAS, a novel hardware-aware spiking neural architecturesearch (NAS) framework for neuromorphic CIM systems that findsan SNN that offers high accuracy under the given memory, area,latency, and energy constraints. To achieve this, HASNAS employsthe following key steps: (1) optimizing SNN operations to achievehigh accuracy, (2) developing an SNN architecture that facilitates aneffective learning process, and (3) devising a systematic hardware-aware search algorithm to meet the constraints. The experimentalresults show that our HASNAS quickly finds an SNN that maintainshigh accuracy compared to the state-of-the-art by up to 11x speed-up, and meets the given constraints: 4x106 parameters of memory,100mm\u00b2 of area, 400ms of latency, and 120\u00b5J energy consumptionfor CIFAR10 and CIFAR100; while the state-of-the-art fails to meetthe constraints. In this manner, our HASNAS can enable efficientdesign automation for providing high-performance and energy-efficient neuromorphic CIM systems for diverse applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in neuromorphic computing have demon-strated the SNN capabilities for solving diverse machine learning(ML)-based applications with ultra-low-power/energy computation.Therefore, many studies have been performed to maximize SNN po-tentials in terms of algorithmic performance and efficiency, rangingfrom SNN developments [16, 24, 38], hardware accelerators [1, 5, 9],and deployments for diverse application use-cases, such as imageclassification [34, 39], object recognition for automotive [8], bio-signal processing for healthcare [20], continual learning for mobilesystems [2, 35, 36], and embodied intelligence for robotics [4, 33].In reality, different ML-based applications have different require-ments of algorithmic performance (e.g., accuracy) and efficiency(e.g., latency and energy consumption). To ensure their practicalityfor real-world implementation, they also need to meet the con-straints from the underlying hardware platforms (e.g., memoryfootprint and area). Therefore, to maximize the SNN benefits fordiverse application use-cases, SNN models have to be designed tofulfill the targeted accuracy and efficiency, while meeting the givenconstraints, which is a non-trivial problem. For instance, a largerSNN model is usually employed to achieve higher accuracy, but itleads to higher energy consumption as the memory accesses domi-nate the overall SNN processing energy in the von Neumann-basedneuromorphic hardware platforms; see Fig. 1(a)-(b).To substantially improve the efficiency of SNN processing, thenon-von Neumann Compute-in-Memory (CIM) paradigm is em-ployed [3, 23]. The CIM paradigm coupled with non-volatile devicetechnologies, such as resistive random access memory (RRAM),overcomes the \"memory wall bottleneck\" by minimizing the datamovements between memory and compute engine. However, sim-ply running the existing SNN on a CIM platform may not sat-isfy the design requirements from the targeted application (e.g., interms of accuracy, memory, area, latency, and energy consumption),since SNN models are typically developed without considering thecharacteristics of the CIM hardware. Moreover, most of SNN archi-tectures are derived from conventional Artificial Neural Networks(ANNs), which have different operations from SNNs [24], hence,they may lose unique features of SNN computation models (e.g.,temporal information) that leads to sub-optimal accuracy [16]. Stud-ies in [16, 38] show that designing an SNN architecture directly inthe spiking domain can achieve better accuracy than deriving fromthe ANN domain; see Fig. 1(a). Therefore, SNNs should be designeddirectly in a spiking domain to meet design requirements and con-straints from the targeted application and underlying CIM hardware.However, manually developing the suitable SNN architecture istime consuming and laborious, thereby requiring an alternativetechnique that can find a desired solution efficiently.Research Problem: How to automatically and quickly find anSNN architecture for neuromorphic CIM systems that achieves highaccuracy and high efficiency (for latency and energy consumption)under memory and area constraints. An efficient solution to this prob-lem will ease the developments of high-performance and energy-efficient neuromorphic CIM systems, and enable their applicabilityfor diverse ML-based applications."}, {"title": "1.1 State-of-the-Art and Their Limitations", "content": "Currently, state-of-the-art still focus on employing neural archi-tecture search (NAS) for finding SNN architectures that offer highaccuracy. They can be classified into the following categories.\u2022 NAS with training: This approach employs \"train-and-search\"process, as it first trains a super-network that consists of allpossible architecture candidates, and then searches a subsetarchitecture that achieves target accuracy. It is employedby [24, 47]. Limitations: This approach typically incurs hugesearching time, memory, and energy consumption.\u2022 NAS without training: This approach addresses limitationsof \"NAS with training\". Its idea is to explore the architecturecandidates using a building block/cell that represents network operations and topology, evaluates the fitness scoreof the investigated architecture, then trains the one withthe highest score. Limitations: SNASNet employs a randomsearch whose effectiveness relies on the number of itera-tion, and does not consider the processing hardware [16].MSE-NAS employs a set of complex excitation-inhibitionconnectivity, and also does not consider the processing hard-ware [27]. AutoST and AutoSpikeformer only target spikingtransformer architectures [7, 44]. LitESNN employs a com-plex topology and compression, and only considers memorycost in its NAS [19]. Meanwhile, SpikeNAS employs opti-mized network operations and considers memory cost in itsNAS [38].In summary, the state-of-the-art NAS works for SNNs do not con-sider multiple design requirements and constraints posed by the tar-geted application and the CIM hardware platform (i.e., memory, area,latency, and energy consumption), thereby hindering the applicabil-ity of neuromorphic CIM systems for diverse ML-based applications.To demonstrate the limitations of state-of-the-art and highlight theoptimization potentials, an experimental case study is performedand discussed in Section 1.2."}, {"title": "1.2 Case Study and Research Challenges", "content": "We study the benefits of applying constraints in the NAS through experiments. To do this, we reproduce the state-of-the-art Cell-Based NAS without training (i.e., SNASNet [16]), then we apply amemory constraint on it to build a memory-aware SNASNet. Here,we consider a constraint of 2x106 (2M) parameters for the CIFAR100workload. We also consider a RRAM-based CIM platform [23] asthe underlying processing hardware. Note, the details of Cell-BasedNAS are discussed in Section 2.3, and the details of experimentalsetup are discussed in Section 4. Fig. 2 presents our experimentalresults, from which we have the following key observations.\u2022 A smaller network may achieve comparable or better accu-racy to the bigger network, meaning that there is a possibilityto maintain high accuracy with a smaller network size.\u2022 Memory saving leads to reduced area, latency, and energyconsumption of SNN processing on an RRAM-based CIMplatform.These observations expose several research challenges that needto be addressed for solving the targeted research problem, as dis-cussed in the following.\u2022 The solution should quickly yet effectively perform its search-ing process to minimize the searching time.\u2022 The solution should incorporate multiple design constraints(i.e., memory, area, latency and energy consumption) into itssearch algorithm to ensure the practicality of SNN deploy-ment on the CIM platform."}, {"title": "1.3 Our Novel Contributions", "content": "To address the research challenges, we propose HASNAS, a novelHardware-Aware Spiking NAS framework which quickly finds theSNN architecture that can achieve high accuracy under the givendesign requirements and constraints (i.e., memory, area, latency, andenergy consumption) for realizing high-performance and energy-efficient neuromorphic CIM systems. This work is also the firstwork that develops an integrated hardware-aware spiking NASfor CIM systems. Our HASNAS employs several key steps as thefollowing (as shown in Fig. 3).\u2022 Optimization of SNN Operations: It aims to investigateSNN operations that can lead to high accuracy, while con-sidering their memory costs. Then, the selected operationswill be utilized for building the network architecture.\u2022 Development of Network Architectures: It aims to builda network architecture that can facilitate effective learningprocess, by leveraging the selected SNN operations and care-fully arranging the neural cell architectures.\u2022 A Hardware-aware Search Algorithm: It aims to finda network architecture that can achieve high accuracy, bysearching for suitable cell architectures, while monitoringthe memory, area, latency, and energy consumption costs.Key Results: We evaluate our HASNAS framework usinga PyTorch-based implementation, and run it on an Nvidia RTX 4090GPU machine. For constraints of 4M parameters of memory, 100mm\u00b2of area, 400ms of latency, and 120\u00b5J of energy consumption, ourHASNAS can quickly find an appropriate SNN with high accuracycomparable to the state-of-the-art by up to 11x speed-up on CI-FAR10 and CIFAR100; while the state-of-the-art cannot meet thelatency and energy constraints."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Spiking Neural Networks (SNNs)", "content": "SNNs are the neural networks' computational model that takes inspiration from how brain works, especially on how spikes are uti-lized for conveying information and data processing [21, 28, 41, 43].SNN operations mainly rely on the neuronal dynamics, which vary depending on the neuron model. Typically, SNNs employ a leaky integrate-and-fire (LIF) neuron model, as it can provide diverse spike train patterns with a relatively low-complexity computa-tion [14, 15]. The neuronal dynamics of LIF neuron model can be characterized with the following equations.\n$Umem(t) = \\lambda Umem (t - 1) + Y(t)$\n(1)\n$Y(t) = \\sum_{i} Wi \\delta i(t)$\n(2)\n$Z(t) = {\\begin{array}{ll}1, & \\text{if } Umem \\geq Uthr\\\\0, & \\text{if otherwise}\\end{array}}$\n(3)\nHere, Umem represents the membrane potential, t represents the timestep, and \u03bb represents the leak factor for membrane potential. Meanwhile, Y represents the weighted input of a neuron, w represents the weight at synapse-i, and \u03b4 represents the input spike from synapse-i at timestep-t. If the Umem reaches the threshold potential (Uthr), then a spike is generated as an output (Z). Otherwise, no spike is generated."}, {"title": "2.2 NAS without Training Approach", "content": "In this work, we consider the \"NAS without training\" approach, as it can quickly find a network that can achieve high accuracy. This approach was originally proposed for ANNs [22], then adopted and modified for SNNs. Its main idea is to perform network evaluation early using a fitness function, hence the performance of network can be evaluated before completing the training phase [22]. For instance, SNASNet [16] leverages the studies in [22] for evaluating the given network through its representation capabilities on diverse samples. Specifically, SNASNet employs the following key steps.\u2022 First, it feeds mini-batch S samples to the network, and mon-itor its LIF neurons' activity. If the Umem reaches Uthr, then the neuron is mapped to 1, otherwise 0. In each layer, this mapping is recorded as binary vector b.\u2022 It computes the Hamming distance H(bi, bj) between sam-ples i and j to construct matrix MH as shown in Eq. 4. Here, B is the number of neurons in the investigated layer, while \u03b3 is the normalization factor to address high sparsity issue from LIF neurons.\n$MH = \\left(\\begin{array}{ccc}B - \\gamma H(b1, b1) & \\cdots & B - \\gamma H(b1, bs)\\\\\\vdots & \\ddots & \\vdots\\\\B - \\gamma H(bs, b1) & \\cdots & B- \\gamma H(bs, bs)\\end{array}\\right)$\n(4)\n\u2022 Afterward, it calculates the fitness score R of the given archi-tecture candidate using Eq. 5. The highest-scored candidate is then selected for training.\n$R = log \\text{ det } | \\frac{MH}{B} |$\n(5)\nIn this work, we also employ the same evaluation steps for our HASNAS framework, as these steps provide an effective and generic measure for evaluating diverse network architectures."}, {"title": "2.3 NAS with Neural Cell Strategy", "content": "The neural cell-based NAS strategy aims at providing a unified benchmark for NAS algorithms [10]. A cell is a directed acyclic graph (DAG) whose edge denotes a specific pre-defined operation. Originally, a cell is designed with 4 nodes with 5 pre-defined ope-rations: No Connection (Zeroize), Skip Connection (SkipCon), 1x1 Convolution (1x1Conv), 3x3 Convolution (3x3Conv), and 3x3 Average Pooling (3x3AvgPool); the connection between 2 nodes refers to a specific pre-defined operation [10, 16]. The main idea of this strat-egy is to find the appropriate connection topology and operations inside the cell, as shown in Fig. 4. In this work, we also employ the similar neural Cell-Based strategy for our HASNAS framework, and focus on the feed-forward connection topology as it has been extensively used in the SNN community."}, {"title": "2.4 CIM Hardware Architecture", "content": "In this work, we consider the CIM hardware platform whose hier-archical architecture is shown in Fig. 5. This CIM design is based on the SpikeFlow architecture from studies in [23].At the top level, the CIM architecture consists of tiles for com-putation, a pooling module (PoolM) for pooling operation, a globalbuffer (GBuff) for storage, a global accumulator (GAccu) for ac-cumulation operation, and a neuron module (NeurM) for LIF neu-ron activity. These modules are connected to each other throughnetwork-on-chip (NoC) interconnection. Each tile consists of a tileinput buffer (TIB), a number of processing elements (PE), a tilebuffer (TB), and a tile accumulator (TAccu). Each PE consists of aPE input buffer (PIB), a number of analog crossbars (C), PE buffer(PBuff), and PE accumulator (PAccu). These modules are connected to each other through H-Tree interconnection. Furthermore, each crossbar consists of input peripherals (IP), RxR non-volatile me-mory (NVM) device array, multiplexers, analog-to-digital converter (ADC), shift-and-add circuit (SAC), and DIFF module. Here, DIFF module receives the dot-product outputs from SAC, and performs signed multiply-and-accumulate (MAC) operations."}, {"title": "3 THE HASNAS FRAMEWORK", "content": "To address the research problem and challenges, we propose a novel HASNAS framework whose design flow is presented in Fig. 6 and its key steps are discussed in the Section 3.1 - Section 3.3, consecutively."}, {"title": "3.1 Optimization of SNN Operations", "content": "To construct an efficient SNN architecture that can achieve high accuracy, we have to understand the significance of SNN operations and then select the ones that lead to high accuracy with small memory cost. To do this, we perform an experimental case study with the following scenarios.\u2022 Scenario-1: We remove a specific operation from the pre-defined ones (i.e., Zeroize, SkipCon, 1x1Conv, 3x3Conv, and 3x3AvgPool), and then perform NAS to evaluate the impact of the investigated operation on accuracy.\u2022 Scenario-2: We investigate the impact of other operations, i.e.,5x5 Convolution (5x5Conv) and 7x7 Convolution (7x7Conv),and then perform NAS to evaluate their impact on accuracy and memory. To do this, the operation that has the role ofextracting features (i.e., 3x3Conv) is replaced by the investi-gated operation. For instance, when 5x5Conv is investigated,the pre-defined operations will consist of Zeroize, SkipCon,1x1Conv, 5x5Conv, and 3x3AvgPool. The same applies when7x7Conv is investigated.Note, all scenarios in this case study consider a Cell-Based networkarchitecture in SNASNet [16] with the CIFAR100 workload. Exper-imental results are presented in Fig. 7, from which we make thefollowing key observations.\u2022 Eliminating Zeroize, 1x1Conv, 3x3AvgPool from the searchspace do not significantly change the accuracy ad comparedto the baseline. Hence, these operations can be removed fromthe search space. If model compression is needed for reducingthe memory size, we can keep 3x3AvgPool in the searchspace. Moreover, it also ensures the connectivity betweennodes and between layers.\u2022 Eliminating SkipCon from the search space slightly reducesthe accuracy, as SkipCon is beneficial for providing featuremaps from previous layer to preserve important information.Similar observation is also discussed in [6]. Hence, SkipConshould be kept in the search space to maintain the learningquality.\u2022 Eliminating 3x3Conv from the search space significantlydegrades accuracy, as the role of 3x3Conv is for extract-ing important features from input samples. Hence, 3x3Convshould be kept in the search space to maintain the learningquality.\u2022 In general, 5x5Conv and 7x7Conv operations have positiveimpact on accuracy since larger kernel sizes are capable ofextracting more unique input features, which is beneficialfor tasks with a large number of classes to distinguish (e.g.,CIFAR100).\u2022 Despite the benefits on accuracy, larger kernel sizes incursignificant memory overheads as compared to the baseline,which usually lead to significant overheads on latency andenergy consumption. Hence, these large kernel sizes arenot suitable for application use-cases that have stringentmemory budgets."}, {"title": "3.2 Development of SNN Architecture", "content": ""}, {"title": "3.2.1 Neural Cell Architecture.", "content": "Our discussion in Section 3.1 sug-gest that we can optimize the search space of Cell-Based NASstrategy for improving/maintaining accuracy while keeping thememory footprint low. To do this, we select a few operations thathave positive impact on accuracy with low memory cost to be con-sidered in the search space. Specifically, in this work, we considerSkipCon, 3x3Conv, and 3x3AvgPool operations in the search space,while keeping 4 nodes of DAG in the neural cell."}, {"title": "3.2.2 SNN Macro-Architecture.", "content": "To improve the applicability of SNNdeployments for diverse ML-based applications, we propose an effi-cient construction/skeleton of SNN macro-architecture; see Fig. 8.The proposed SNN macro-architecture consists of a spike encod-ing layer, N layer(s) of the Cell-A, a reduction cell, 1 layer of theCell-B, a vectorize layer, and a classifier. We also employ a non-fixed number of layer for the Cell-A, as it will be determined inthe NAS process. This strategy aims to facilitate more variations inthe search space to further enhance the learning quality, because adeeper network typically can achieve higher accuracy at the costof larger memory footprint, as shown by our experimental resultsin Fig. 9. Furthermore, the Cell-A and the Cell-B are similar as theyconsider the same kernel size of convolutional operation (3x3Conv),and they differ only in the number of weight filters. The reason isthat, later layers (e.g., the Cell-B) typically have more filters thanthe earlier layers (e.g., the Cell-A), as we need to capture as manycombinations of features as possible in the later layers through theincreasing number of filters. It is also the reason why we applymultiple layers using the Cell-A, as this strategy incurs a smallernumber of weights than applying multiple layers using the Cell-B.In this manner, our HASNAS framework provides a dynamic searchto find a network architecture that has a good trade-off among accu-racy, memory, area, latency, and energy consumption."}, {"title": "3.3 A Hardware-aware Search Algorithm", "content": "To systematically perform NAS while fulfilling multiple design re-quirements and constraints, we propose a novel hardware-awaressearch algorithm. Its pseudo-code is presented in Alg. 1.\u2022 Our search policy is that, each cell has an individual search tofind its architecture, hence exploring all possible candidatesfrom different combinations of architectures across multiplecells: Cell-A (Alg. 1: line 2-8) and Cell-B (Alg. 1: line 21-26).\u2022 To do this, we first explore possible number of layers for Cell-A (Alg. 1: line 2), as well as possible architectures inside theCell-A (Alg. 1: line 3-9). Here, the Cell-B architecture is set thesame as Cell-A in the first layer iteration (Alg. 1: line 10-11). Then, we construct the investigated network architecture based on the states of Cell-A(s) and Cell-B (Alg. 1: line 12).\u2022 Afterward, we evaluate the candidate for its memory cost(Alg. 1: line 13-14). If the memory constraint is met, then wecontinue with the fitness function evaluation (Alg. 1: line 15).Ifthe fitness score is higher than the saved one, then we recordthe network architecture as a solution candidate (Alg. 1: line 16-20).\u2022 After all layer iterations for Cell-A are finished, we explorepossible architectures inside the Cell-B (Alg. 1: line 21-27).Then, we construct the investigated network architecturebased on the states of Cell-A(s) and Cell-B (Alg. 1: line 28).\u2022 Afterward, we evaluate the candidate for its memory cost(Alg. 1: line 29-30). If the memory constraint is met, then wecontinue with evaluation of area, latency, and energy con-sumption, consecutively (Alg. 1: line 31-36). If all constraintsare met, we continue with the fitness function evaluation(Alg. 1: line 37). If the fitness score is higher than the savedone, then we record the network architecture as a solutioncandidate (Alg. 1: line 38-41).\u2022 After all exploration steps are finished, then we consider thesaved network architecture as the solution of the NAS process(Alg. 1: line 42), and this SNN architecture is ready fortraining.Note, in this search algorithm, we perform memory evaluation byleveraging the number weight parameters. Meanwhile, we performarea, latency, and energy evaluations using an energy-latency-area(ELA) simulator from studies in [23], while considering the Spike-Flow CIM hardware architecture with real measurements-basedvalues for RRAM device technology."}, {"title": "4 EVALUATION METHODOLOGY", "content": "To evaluate our HASNAS framework, we build an experimental setup and tools flow as shown in Fig. 10, while considering the same evaluation settings as widely used in the SNN community [26,29-32, 37]. Meanwhile, the design requirements and constraints considered in the evaluation are presented in Table 1. For brevity, we refer the HASNAS with 2x106 (2M), 4x106 (4M), 6x106 (6M), and 8x106 (8M) memory constraints to as HASNAS_2M, HASNAS_4M, HASNAS_6M, and HASNAS_8M respectively.up to 300 training epochs, while gradually evaluating the test ac-curacy during the training to observe the learning curve of the investigated SNN architecture. For the workloads, we consider the CIFAR10 and CIFAR100 datasets. For the comparison partner, we reproduce the state-of-the-art Cell-Based NAS without training SNASNet [16] using its default settings, i.e., 2 neural cells, 5 pre-defined operations, and 5000x iterations of random search. Outputs from the software-level evaluation include the information of ac-curacy, memory, and searching time of the investigated network architecture.Hardware-level Evaluation: To evaluate area, latency, and energy consumption when running the SNN architecture on the RRAM-based CIM platform, we employ the state-of-the-art energy-latency-area (ELA) simulator from the work of [23]. The detailed circuit and device parameters of the CIM platform considered in the ELA simulator for evaluating area, latency, and energy consumption are presented in Table 2."}, {"title": "5 EXPERIMENTAL RESULTS AND DISCUSSION", "content": ""}, {"title": "5.1 Maintaining High Accuracy", "content": "Fig. 11(a) and Fig. 11(b) present the experimental results for the accuracy for CIFAR10 and CIFAR100, respectively. The state-of-the-art SNASNet achieves 92.58% of accuracy for CIFAR10 and 66.78% of accuracy for CIFAR100. The network generation in SNASNet sim-ply relies on the number of search iteration with random search and the fitness score. Meanwhile, HASNAS_2M achieves 91.28% of accu-racy for CIFAR10 and 63.25% for CIFAR100, HASNAS_4M achieves 91.87% of accuracy for CIFAR10 and 64.67% for CIFAR100, HAS-NAS_6M achieves 91.67% of accuracy for CIFAR10 and 65.52% for CIFAR100, HASNAS_8M achieves 91.87% of accuracy for CIFAR10 and 65.52% for CIFAR100. These results indicate that, our HASNAS can maintain the high accuracy as compared to the state-of-the-art across all training epochs. The reasons are the following: (1) HASNAS only employs SNN operations that have high significance and positive impact on the accuracy, and (2) HASNAS constructs the network by systematically stacking neural cell layer one-by-one, while exploring the appropriate architecture for each neural cell, thereby ensuring effective search toward the desired network architecture.These results indicate that, our HASNAS framework successfully maintains the accuracy high and comparable to the state-of-the-art, despite the fact that it has to consider multiple constraints at once during its NAS process, which is not trivial."}, {"title": "5.2 Ensuring Hardware-aware SNN Generation", "content": "Fig. 12 presents the experimental results for hardware-related eval-uation considering CIFAR10 and CIFAR100, including memory foot-print, hardware area, latency, and energy consumption.Number of Weight Parameters (Memory Footprint): Results for memory footprint are shown in Fig. 12(a.1) for CIFAR10 and Fig. 12(b.1) for CIFAR100. For the CIFAR10 case, SNASNet generates a network that has 1.28M weight parameters. Since SNASNet does not include the memory constraint in its NAS process, the number of weight parameters of the generated network is not under con-trol during its NAS process. Meanwhile, our HASNAS techniques ensure that the generated network always fulfills the memory con-straint, as it includes the information of memory budget in its NAS process. For instance, HASNAS_2M generates a network that has 1.77M weight parameters, which satisfies the memory constraint of 2M parameters. Furthermore, HASNAS_4M, HASNAS_6M, and HASNAS_8M generate networks that have 3.25M weight parame-ters, which satisfy the corresponding memory constraints, i.e., 4M, 6M, and 8M weight parameters, respectively. We also observe that similar experimental results are obtained for the CIFAR100 case. Here, SNASNet generates a network that has 1.28M weight parame-ters. Then, our HASNAS_2M generates a network that has 1.77M weight parameters, while HASNAS_4M, HASNAS_6M, and HAS-NAS_8M generate networks that have 3.25M weight parameters. The reason is that, NAS processes in SNASNet and HASNAS find cell architectures that incur similar numbers of weight parameters to the CIFAR10 case, despite having different cell architectures. These results indicate that, our HASNAS framework successfully generates a network architecture that satisfies the given memory constraint, thereby making it possible to store all weight parameters of a network in the memory of underlying hardware platforms.Hardware Area: Results for hardware are shown in Fig. 12(a.2) for CIFAR10 and Fig. 12(b.2) for CIFAR100. For the CIFAR10 case, SNASNet generates a network that occupies 23.9mm\u00b2 area (55% compute and 45% memory parts). Since SNASNet does not include the area constraint in its NAS process, the hardware area of the generated network is not under control during its NAS process. Meanwhile, our HASNAS techniques ensure that the generated network fulfills the area constraint, as it includes the information of area budget in its NAS process. For instance, HASNAS_2M generates a network that occupies 18.9mm\u00b2 area (44% compute and 56% memory parts), while HASNAS_4M, HASNAS_6M, and HASNAS_-8M generate networks that occupy 23.7mm\u00b2 area (55% compute and 45% memory parts), thereby meeting the 100mm\u00b2 area budget. We also observe that similar experimental results are obtained for the CIFAR100 case. Here, SNASNet generates a network that occupies 23.9mm\u00b2 area. Then, our HASNAS_2M generates a network that occupies 18.9mm\u00b2 area, while HASNAS_4M, HASNAS_6M, and HASNAS_8M generate networks that occupy 23.7mm\u00b2 area. The reason is that, the similar numbers of weight parameters from the generated networks by SNASNet and HASNAS for the CIFAR10 and CIFAR100 cases lead to the similar area for hardware imple-mentation.These results indicate that, our HASNAS framework successfully generates a network architecture that satisfies the given area con-straint, thereby making it possible to develop the corresponding hard-ware implementation within a custom area budget.Processing Latency: Results for processing latency are shown in Fig. 12(a.3) for CIFAR10 and Fig. 12(b.3) for CIFAR100. For both the CIFAR10 and CIFAR100 cases, SNASNet generates networks that require 469ms2 processing latency when run on the CIM hardware, which is 17.15% slower than the latency constraint; see label-1 and label-in Fig. 12. The reason is that, SNASNet does not consider the latency constraint in its NAS process, hence the processing latency the generated network cannot be controlled during its NAS process. Meanwhile, our HASNAS techniques ensure that the gen-erated network meets the latency constraint, by incorporating the information of latency constraint and hardware characteristics in its NAS process. For instance, our HASNAS_2M generates a network that can be run on the CIM hardware in 290ms for SNN inference, which is 27.58% faster than the constraint. Meanwhile, our HAS-NAS_4M, HASNAS_6M, and HASNAS_8M generate networks that can be run on the CIM hardware in 392ms, which is still within the constraints' envelope.These results show that, our HASNAS framework successfully generates a network architecture that satisfies the given latency con-straint, which is crucial for SNN deployments on latency-sensitive application use-cases.Energy Consumption: Results for energy consumption are shown in Fig. 12(a.4) for CIFAR10 and Fig. 12(b.4) for CIFAR100. For both the CIFAR10 and CIFAR100 cases, SNASNet generates net-works that incur 134\u00b5J energy consumption, which is 11.51% worse than the energy constraint; see label- and label-in Fig. 12. The reason is that, SNASNet does not consider the energy constraint in its NAS process, hence the processing energy the generated net-work cannot be estimated and managed during its NAS process. Meanwhile, our HASNAS techniques ensure that the generated network meets the energy constraint, by incorporating the informa-tion of energy constraint and hardware characteristics in its NAS process. For instance, our HASNAS_2M generates a network that incurs 76.4\u00b5J, which is 36.37% lower than the constraint. Mean-while, our HASNAS_4M, HASNAS_6M, and HASNAS_8M generate networks that incur 110\u00b5J, which is still within the constraints' envelope.These results indicate that, our HASNAS framework successfully generates a network architecture that satisfies the energy constraint, thereby enabling a better power/energy management for diverse ap-plication use-cases, especially the ones powered by portable batteries."}, {"title": "5.3 Searching Time Speed-Up", "content": "Fig. 13(a) and Fig. 13(b) present the experimental results for the searching time for CIFAR10 and CIFAR100, respectively. In both cases of workloads (CIFAR10 and CIFAR100), our HASNAS offers a significant speed-up for the searching time as compared to the state-of-the-art. For the CIFAR10 case, HASNAS improves the searching time by 11.1x, 5.39x, 5.27x, and 4.92x for HASNAS_2M, HASNAS_-4M, HASNAS_6M, and HASNAS_8M, respectively. Meanwhile, for the CIFAR100 case, HASNAS improves the searching time by 7.4x, 3.72x, 3.66x, and 3.66x for HASNAS_2M, HASNAS_4M, HASNAS_-6M, and HASNAS_8M, respectively. The reason is that"}]}