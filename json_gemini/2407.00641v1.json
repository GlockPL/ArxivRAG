{"title": "HASNAS: A Hardware-Aware Spiking Neural Architecture Search Framework for Neuromorphic Compute-in-Memory Systems", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad Shafique"], "abstract": "Spiking Neural Networks (SNNs) have shown capabilities for solv-ing diverse machine learning tasks with ultra-low-power/energy computation. To further improve the performance and efficiency of SNN inference, the Compute-in-Memory (CIM) paradigm with emerging device technologies such as resistive random access me-mory is employed. However, most of SNN architectures are devel-oped without considering constraints from the application and the underlying CIM hardware (e.g., memory, area, latency, and energy consumption). Moreover, most of SNN designs are derived from the Artificial Neural Networks, whose network operations are dif-ferent from SNNs. These limitations hinder SNNs from reaching their full potential in accuracy and efficiency. Toward this, we pro-pose HASNAS, a novel hardware-aware spiking neural architecture search (NAS) framework for neuromorphic CIM systems that finds an SNN that offers high accuracy under the given memory, area, latency, and energy constraints. To achieve this, HASNAS employs the following key steps: (1) optimizing SNN operations to achieve high accuracy, (2) developing an SNN architecture that facilitates an effective learning process, and (3) devising a systematic hardware-aware search algorithm to meet the constraints. The experimental results show that our HASNAS quickly finds an SNN that maintains high accuracy compared to the state-of-the-art by up to 11x speed-up, and meets the given constraints: 4x106 parameters of memory, 100mm\u00b2 of area, 400ms of latency, and 120\u00b5J energy consumption for CIFAR10 and CIFAR100; while the state-of-the-art fails to meet the constraints. In this manner, our HASNAS can enable efficient design automation for providing high-performance and energy-efficient neuromorphic CIM systems for diverse applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in neuromorphic computing have demon-strated the SNN capabilities for solving diverse machine learning (ML)-based applications with ultra-low-power/energy computation. Therefore, many studies have been performed to maximize SNN po-tentials in terms of algorithmic performance and efficiency, ranging from SNN developments [16, 24, 38], hardware accelerators [1, 5, 9], and deployments for diverse application use-cases, such as image classification [34, 39], object recognition for automotive [8], bio-signal processing for healthcare [20], continual learning for mobile systems [2, 35, 36], and embodied intelligence for robotics [4, 33].\nIn reality, different ML-based applications have different require-ments of algorithmic performance (e.g., accuracy) and efficiency (e.g., latency and energy consumption). To ensure their practicality for real-world implementation, they also need to meet the con-straints from the underlying hardware platforms (e.g., memory footprint and area). Therefore, to maximize the SNN benefits for diverse application use-cases, SNN models have to be designed to fulfill the targeted accuracy and efficiency, while meeting the given constraints, which is a non-trivial problem. For instance, a larger SNN model is usually employed to achieve higher accuracy, but it leads to higher energy consumption as the memory accesses domi-nate the overall SNN processing energy in the von Neumann-based neuromorphic hardware platforms; see Fig. 1(a)-(b).\nTo substantially improve the efficiency of SNN processing, the non-von Neumann Compute-in-Memory (CIM) paradigm is em-ployed [3, 23]. The CIM paradigm coupled with non-volatile device technologies, such as resistive random access memory (RRAM), overcomes the \"memory wall bottleneck\" by minimizing the data movements between memory and compute engine. However, sim-ply running the existing SNN on a CIM platform may not sat-isfy the design requirements from the targeted application (e.g., in terms of accuracy, memory, area, latency, and energy consumption), since SNN models are typically developed without considering the characteristics of the CIM hardware. Moreover, most of SNN archi-tectures are derived from conventional Artificial Neural Networks (ANNs), which have different operations from SNNs [24], hence they may lose unique features of SNN computation models (e.g., temporal information) that leads to sub-optimal accuracy [16]. Stud-ies in [16, 38] show that designing an SNN architecture directly in the spiking domain can achieve better accuracy than deriving from the ANN domain; see Fig. 1(a). Therefore, SNNs should be designed directly in a spiking domain to meet design requirements and con-straints from the targeted application and underlying CIM hardware. However, manually developing the suitable SNN architecture is time consuming and laborious, thereby requiring an alternative technique that can find a desired solution efficiently.\nResearch Problem: How to automatically and quickly find an SNN architecture for neuromorphic CIM systems that achieves high accuracy and high efficiency (for latency and energy consumption) under memory and area constraints. An efficient solution to this prob-lem will ease the developments of high-performance and energy-efficient neuromorphic CIM systems, and enable their applicability for diverse ML-based applications."}, {"title": "1.1 State-of-the-Art and Their Limitations", "content": "Currently, state-of-the-art still focus on employing neural archi-tecture search (NAS) for finding SNN architectures that offer high accuracy. They can be classified into the following categories.\n\u2022 NAS with training: This approach employs \"train-and-search\" process, as it first trains a super-network that consists of all possible architecture candidates, and then searches a subset architecture that achieves target accuracy. It is employed by [24, 47]. Limitations: This approach typically incurs huge searching time, memory, and energy consumption.\n\u2022 NAS without training: This approach addresses limitations of \"NAS with training\". Its idea is to explore the architecture candidates using a building block/cell that represents network operations and topology, evaluates the fitness score of the investigated architecture, then trains the one with the highest score. Limitations: SNASNet employs a random search whose effectiveness relies on the number of itera-tion, and does not consider the processing hardware [16]. MSE-NAS employs a set of complex excitation-inhibition connectivity, and also does not consider the processing hard-ware [27]. AutoST and AutoSpikeformer only target spiking transformer architectures [7, 44]. LitESNN employs a com-plex topology and compression, and only considers memory cost in its NAS [19]. Meanwhile, SpikeNAS employs opti-mized network operations and considers memory cost in its NAS [38].\nIn summary, the state-of-the-art NAS works for SNNs do not con-sider multiple design requirements and constraints posed by the tar-geted application and the CIM hardware platform (i.e., memory, area, latency, and energy consumption), thereby hindering the applicabil-ity of neuromorphic CIM systems for diverse ML-based applications. To demonstrate the limitations of state-of-the-art and highlight the optimization potentials, an experimental case study is performed and discussed in Section 1.2."}, {"title": "1.2 Case Study and Research Challenges", "content": "We study the benefits of applying constraints in the NAS through experiments. To do this, we reproduce the state-of-the-art Cell-Based NAS without training (i.e., SNASNet [16]), then we apply a memory constraint on it to build a memory-aware SNASNet. Here, we consider a constraint of 2x106 (2M) parameters for the CIFAR100 workload. We also consider a RRAM-based CIM platform [23] as the underlying processing hardware. Note, the details of Cell-Based NAS are discussed in Section 2.3, and the details of experimental setup are discussed in Section 4. Fig. 2 presents our experimental results, from which we have the following key observations.\n\u2022 A smaller network may achieve comparable or better accu-racy to the bigger network, meaning that there is a possibility to maintain high accuracy with a smaller network size.\n\u2022 Memory saving leads to reduced area, latency, and energy consumption of SNN processing on an RRAM-based CIM platform.\nThese observations expose several research challenges that need to be addressed for solving the targeted research problem, as dis-cussed in the following.\n\u2022 The solution should quickly yet effectively perform its search-ing process to minimize the searching time.\n\u2022 The solution should incorporate multiple design constraints (i.e., memory, area, latency and energy consumption) into its search algorithm to ensure the practicality of SNN deploy-ment on the CIM platform."}, {"title": "1.3 Our Novel Contributions", "content": "To address the research challenges, we propose HASNAS, a novel Hardware-Aware Spiking NAS framework which quickly finds the SNN architecture that can achieve high accuracy under the given design requirements and constraints (i.e., memory, area, latency, and energy consumption) for realizing high-performance and energy-efficient neuromorphic CIM systems. This work is also the first work that develops an integrated hardware-aware spiking NAS for CIM systems. Our HASNAS employs several key steps as the following (as shown in Fig. 3).\n\u2022 Optimization of SNN Operations: It aims to investigate SNN operations that can lead to high accuracy, while con-sidering their memory costs. Then, the selected operations will be utilized for building the network architecture.\n\u2022 Development of Network Architectures: It aims to build a network architecture that can facilitate effective learning process, by leveraging the selected SNN operations and care-fully arranging the neural cell architectures.\n\u2022 A Hardware-aware Search Algorithm: It aims to find a network architecture that can achieve high accuracy, by searching for suitable cell architectures, while monitoring the memory, area, latency, and energy consumption costs.\nKey Results: We evaluate our HASNAS framework using a PyTorch-based implementation, and run it on an Nvidia RTX 4090 GPU machine. For constraints of 4M parameters of memory, 100mm\u00b2 of area, 400ms of latency, and 120\u00b5J of energy consumption, our HASNAS can quickly find an appropriate SNN with high accuracy comparable to the state-of-the-art by up to 11x speed-up on CI-FAR10 and CIFAR100; while the state-of-the-art cannot meet the latency and energy constraints."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Spiking Neural Networks (SNNs)", "content": "SNNs are the neural networks' computational model that takes inspiration from how brain works, especially on how spikes are uti-lized for conveying information and data processing [21, 28, 41, 43]. SNN operations mainly rely on the neuronal dynamics, which vary depending on the neuron model. Typically, SNNs employ a leaky integrate-and-fire (LIF) neuron model, as it can provide diverse spike train patterns with a relatively low-complexity computa-tion [14, 15]. The neuronal dynamics of LIF neuron model can be characterized with the following equations.\n$Umem(t) = \\lambda Umem (t \u2212 1) + Y(t)$ (1)\n$Y(t) = \\sum Wi di(t)$ (2)\n$Z(t) = \\begin{cases} 1, &\\text{if } Umem \\geq Uthr \\\\ 0, &\\text{if otherwise} \\end{cases}$ (3)\nHere, Umem represents the membrane potential, t represents the timestep, and \u03bb represents the leak factor for membrane potential. Meanwhile, Y represents the weighted input of a neuron, w represents the weight at synapse-i, and \u03b4 represents the input spike from synapse-i at timestep-t. If the Umem reaches the threshold potential (Uthr), then a spike is generated as an output (Z). Otherwise, no spike is generated."}, {"title": "2.2 NAS without Training Approach", "content": "In this work, we consider the \"NAS without training\" approach, as it can quickly find a network that can achieve high accuracy. This approach was originally proposed for ANNs [22], then adopted and modified for SNNs. Its main idea is to perform network evaluation early using a fitness function, hence the performance of network can be evaluated before completing the training phase [22]. For instance, SNASNet [16] leverages the studies in [22] for evaluating the given network through its representation capabilities on diverse samples. Specifically, SNASNet employs the following key steps.\n\u2022 First, it feeds mini-batch S samples to the network, and mon-itor its LIF neurons' activity. If the Umem reaches Uthr, then the neuron is mapped to 1, otherwise 0. In each layer, this mapping is recorded as binary vector b.\n\u2022 It computes the Hamming distance H(bi, bj) between sam-ples i and j to construct matrix MH as shown in Eq. 4. Here, B is the number of neurons in the investigated layer, while \u03b3 is the normalization factor to address high sparsity issue from LIF neurons.\n$MH = \\begin{pmatrix} (\u0392 \u2013 \u03b3\u0397(b1, b1) & ... & B - \u03b3H(b1, bs) \\\\ : & : & : \\\\ B - \u03b3H(bs, b\u2081) & ... & B-H(bs, bs)) \\end{pmatrix}$ (4)\n\u2022 Afterward, it calculates the fitness score R of the given archi-tecture candidate using Eq. 5. The highest-scored candidate is then selected for training.\n$R = log det | \\frac{1}{S}MH |$ (5)\nIn this work, we also employ the same evaluation steps for our HASNAS framework, as these steps provide an effective and generic measure for evaluating diverse network architectures."}, {"title": "2.3 NAS with Neural Cell Strategy", "content": "The neural cell-based NAS strategy aims at providing a unified benchmark for NAS algorithms [10]. A cell is a directed acyclic graph (DAG) whose edge denotes a specific pre-defined operation. Originally, a cell is designed with 4 nodes with 5 pre-defined ope-rations: No Connection (Zeroize), Skip Connection (SkipCon), 1x1 Convolution (1x1Conv), 3x3 Convolution (3x3Conv), and 3x3 Average Pooling (3x3AvgPool); the connection between 2 nodes refers to a specific pre-defined operation [10, 16]. The main idea of this strat-egy is to find the appropriate connection topology and operations inside the cell, as shown in Fig. 4. In this work, we also employ the similar neural Cell-Based strategy for our HASNAS framework, and focus on the feed-forward connection topology as it has been extensively used in the SNN community."}, {"title": "2.4 CIM Hardware Architecture", "content": "In this work, we consider the CIM hardware platform whose hier-archical architecture is shown in Fig. 5. This CIM design is based on the SpikeFlow architecture from studies in [23].\nAt the top level, the CIM architecture consists of tiles for com-putation, a pooling module (PoolM) for pooling operation, a global buffer (GBuff) for storage, a global accumulator (GAccu) for ac-cumulation operation, and a neuron module (NeurM) for LIF neu-ron activity. These modules are connected to each other through network-on-chip (NoC) interconnection. Each tile consists of a tile input buffer (TIB), a number of processing elements (PE), a tile buffer (TB), and a tile accumulator (TAccu). Each PE consists of a PE input buffer (PIB), a number of analog crossbars (C), PE buffer"}, {"title": "3 THE HASNAS FRAMEWORK", "content": "To address the research problem and challenges, we propose a novel HASNAS framework whose design flow is presented in Fig. 6 and its key steps are discussed in the Section 3.1 - Section 3.3, consecutively."}, {"title": "3.1 Optimization of SNN Operations", "content": "To construct an efficient SNN architecture that can achieve high accuracy, we have to understand the significance of SNN operations and then select the ones that lead to high accuracy with small memory cost. To do this, we perform an experimental case study with the following scenarios.\n\u2022 Scenario-1: We remove a specific operation from the pre-defined ones (i.e., Zeroize, SkipCon, 1x1Conv, 3x3Conv, and 3x3AvgPool), and then perform NAS to evaluate the impact of the investigated operation on accuracy.\n\u2022 Scenario-2: We investigate the impact of other operations, i.e., 5x5 Convolution (5x5Conv) and 7x7 Convolution (7x7Conv), and then perform NAS to evaluate their impact on accuracy and memory. To do this, the operation that has the role of extracting features (i.e., 3x3Conv) is replaced by the investi-gated operation. For instance, when 5x5Conv is investigated, the pre-defined operations will consist of Zeroize, SkipCon, 1x1Conv, 5x5Conv, and 3x3AvgPool. The same applies when 7x7Conv is investigated.\nNote, all scenarios in this case study consider a Cell-Based network architecture in SNASNet [16] with the CIFAR100 workload. Exper-imental results are presented in Fig. 7, from which we make the following key observations.\n\u2460 Eliminating Zeroize, 1x1Conv, 3x3AvgPool from the search space do not significantly change the accuracy ad compared to the baseline. Hence, these operations can be removed from the search space. If model compression is needed for reducing the memory size, we can keep 3x3AvgPool in the search space. Moreover, it also ensures the connectivity between nodes and between layers.\n\u2461 Eliminating SkipCon from the search space slightly reduces the accuracy, as SkipCon is beneficial for providing feature maps from previous layer to preserve important information. Similar observation is also discussed in [6]. Hence, SkipCon should be kept in the search space to maintain the learning quality.\n\u2462 Eliminating 3x3Conv from the search space significantly degrades accuracy, as the role of 3x3Conv is for extract-ing important features from input samples. Hence, 3x3Conv should be kept in the search space to maintain the learning quality.\n\u2463 In general, 5x5Conv and 7x7Conv operations have positive impact on accuracy since larger kernel sizes are capable of extracting more unique input features, which is beneficial for tasks with a large number of classes to distinguish (e.g., CIFAR100).\n\u2464 Despite the benefits on accuracy, larger kernel sizes incur significant memory overheads as compared to the baseline, which usually lead to significant overheads on latency and energy consumption. Hence, these large kernel sizes are not suitable for application use-cases that have stringent memory budgets."}, {"title": "3.2 Development of SNN Architecture", "content": ""}, {"title": "3.2.1 Neural Cell Architecture.", "content": "Our discussion in Section 3.1 sug-gest that we can optimize the search space of Cell-Based NAS strategy for improving/maintaining accuracy while keeping the memory footprint low. To do this, we select a few operations that have positive impact on accuracy with low memory cost to be con-sidered in the search space. Specifically, in this work, we consider SkipCon, 3x3Conv, and 3x3AvgPool operations in the search space, while keeping 4 nodes of DAG in the neural cell."}, {"title": "3.2.2 SNN Macro-Architecture.", "content": "To improve the applicability of SNN deployments for diverse ML-based applications, we propose an effi-cient construction/skeleton of SNN macro-architecture; see Fig. 8. The proposed SNN macro-architecture consists of a spike encod-ing layer, N layer(s) of the Cell-A, a reduction cell, 1 layer of the Cell-B, a vectorize layer, and a classifier. We also employ a non-fixed number of layer for the Cell-A, as it will be determined in the NAS process. This strategy aims to facilitate more variations in the search space to further enhance the learning quality, because a deeper network typically can achieve higher accuracy at the cost of larger memory footprint, as shown by our experimental results in Fig. 9. Furthermore, the Cell-A and the Cell-B are similar as they consider the same kernel size of convolutional operation (3x3Conv), and they differ only in the number of weight filters. The reason is that, later layers (e.g., the Cell-B) typically have more filters than the earlier layers (e.g., the Cell-A), as we need to capture as many combinations of features as possible in the later layers through the increasing number of filters. It is also the reason why we apply multiple layers using the Cell-A, as this strategy incurs a smaller number of weights than applying multiple layers using the Cell-B. In this manner, our HASNAS framework provides a dynamic search to find a network architecture that has a good trade-off among accu-racy, memory, area, latency, and energy consumption."}, {"title": "3.3 A Hardware-aware Search Algorithm", "content": "To systematically perform NAS while fulfilling multiple design re-quirements and constraints, we propose a novel hardware-aware search algorithm. Its pseudo-code is presented in Alg. 1.\n\u2022 Our search policy is that, each cell has an individual search to find its architecture, hence exploring all possible candidates from different combinations of architectures across multiple cells: Cell-A (Alg. 1: line 2-8) and Cell-B (Alg. 1: line 21-26).\n\u2022 To do this, we first explore possible number of layers for Cell-A (Alg. 1: line 2), as well as possible architectures inside the Cell-A (Alg. 1: line 3-9). Here, the Cell-B architecture is set the same as Cell-A in the first layer iteration (Alg. 1: line 10-11). Then, we construct the investigated network architecture based on the states of Cell-A(s) and Cell-B (Alg. 1: line 12).\n\u2022 Afterward, we evaluate the candidate for its memory cost (Alg. 1: line 13-14). If the memory constraint is met, then we continue with the fitness function evaluation (Alg. 1: line 15). If the fitness score is higher than the saved one, then we record the network architecture as a solution candidate (Alg. 1: line 16-20).\n\u2022 After all layer iterations for Cell-A are finished, we explore possible architectures inside the Cell-B (Alg. 1: line 21-27). Then, we construct the investigated network architecture based on the states of Cell-A(s) and Cell-B (Alg. 1: line 28).\n\u2022 Afterward, we evaluate the candidate for its memory cost (Alg. 1: line 29-30). If the memory constraint is met, then we continue with evaluation of area, latency, and energy con-sumption, consecutively (Alg. 1: line 31-36). If all constraints are met, we continue with the fitness function evaluation (Alg. 1: line 37). If the fitness score is higher than the saved one, then we record the network architecture as a solution candidate (Alg. 1: line 38-41).\n\u2022 After all exploration steps are finished, then we consider the saved network architecture as the solution of the NAS process (Alg. 1: line 42), and this SNN architecture is ready for training.\nNote, in this search algorithm, we perform memory evaluation by leveraging the number weight parameters. Meanwhile, we perform area, latency, and energy evaluations using an energy-latency-area (ELA) simulator from studies in [23], while considering the Spike-Flow CIM hardware architecture with real measurements-based values for RRAM device technology."}, {"title": "4 EVALUATION METHODOLOGY", "content": "To evaluate our HASNAS framework, we build an experimental setup and tools flow as shown in Fig. 10, while considering the same evaluation settings as widely used in the SNN community [26, 29-32, 37]. Meanwhile, the design requirements and constraints considered in the evaluation are presented in Table 1. For brevity, we refer the HASNAS with 2x106 (2M), 4x106 (4M), 6x106 (6M), and 8x106 (8M) memory constraints to as HASNAS_2M, HASNAS_4M, HASNAS_6M, and HASNAS_8M respectively.\nSoftware-level Evaluation: To evaluate and validate our pro-posed techniques, we develop a PyTorch-based implementation us-ing the SpikingJelly library [11], then run it on the Nvidia GeForce RTX 4090 GPU machine which has 24GB GDDR6X memory. We per-form network training using the surrogate gradient learning [25, 45] up to 300 training epochs, while gradually evaluating the test ac-curacy during the training to observe the learning curve of the investigated SNN architecture. For the workloads, we consider the CIFAR10 and CIFAR100 datasets. For the comparison partner, we reproduce the state-of-the-art Cell-Based NAS without training SNASNet [16] using its default settings, i.e., 2 neural cells, 5 pre-defined operations, and 5000x iterations of random search. Outputs from the software-level evaluation include the information of ac-curacy, memory, and searching time of the investigated network architecture.\nHardware-level Evaluation: To evaluate area, latency, and energy consumption when running the SNN architecture on the RRAM-based CIM platform, we employ the state-of-the-art energy-latency-area (ELA) simulator from the work of [23]. The detailed circuit and device parameters of the CIM platform considered in the ELA simulator for evaluating area, latency, and energy consumption are presented in Table 2."}, {"title": "5 EXPERIMENTAL RESULTS AND DISCUSSION", "content": ""}, {"title": "5.1 Maintaining High Accuracy", "content": "Fig. 11(a) and Fig. 11(b) present the experimental results for the accuracy for CIFAR10 and CIFAR100, respectively. The state-of-the-art SNASNet achieves 92.58% of accuracy for CIFAR10 and 66.78% of accuracy for CIFAR100. The network generation in SNASNet sim-ply relies on the number of search iteration with random search and the fitness score. Meanwhile, HASNAS_2M achieves 91.28% of accu-racy for CIFAR10 and 63.25% for CIFAR100, HASNAS_4M achieves 91.87% of accuracy for CIFAR10 and 64.67% for CIFAR100, HAS-NAS_6M achieves 91.67% of accuracy for CIFAR10 and 65.52% for CIFAR100, HASNAS_8M achieves 91.87% of accuracy for CIFAR10 and 65.52% for CIFAR100. These results indicate that, our HASNAS can maintain the high accuracy as compared to the state-of-the-art across all training epochs. The reasons are the following: (1) HASNAS only employs SNN operations that have high significance"}, {"title": "5.2 Ensuring Hardware-aware SNN Generation", "content": "Fig. 12 presents the experimental results for hardware-related eval-uation considering CIFAR10 and CIFAR100, including memory foot-print, hardware area, latency, and energy consumption.\nNumber of Weight Parameters (Memory Footprint): Results for memory footprint are shown in Fig. 12(a.1) for CIFAR10 and Fig. 12(b.1) for CIFAR100. For the CIFAR10 case, SNASNet generates a network that has 1.28M weight parameters. Since SNASNet does not include the memory constraint in its NAS process, the number of weight parameters of the generated network is not under con-trol during its NAS process. Meanwhile, our HASNAS techniques ensure that the generated network always fulfills the memory con-straint, as it includes the information of memory budget in its NAS process. For instance, HASNAS_2M generates a network that has 1.77M weight parameters, which satisfies the memory constraint of 2M parameters. Furthermore, HASNAS_4M, HASNAS_6M, and HASNAS_8M generate networks that have 3.25M weight parame-ters, which satisfy the corresponding memory constraints, i.e., 4M, 6M, and 8M weight parameters, respectively. We also observe that similar experimental results are obtained for the CIFAR100 case. Here, SNASNet generates a network that has 1.28M weight parame-ters. Then, our HASNAS_2M generates a network that has 1.77M weight parameters, while HASNAS_4M, HASNAS_6M, and HAS-NAS_8M generate networks that have 3.25M weight parameters. The reason is that, NAS processes in SNASNet and HASNAS find cell architectures that incur similar numbers of weight parameters to the CIFAR10 case, despite having different cell architectures. These results indicate that, our HASNAS framework successfully generates a network architecture that satisfies the given memory constraint, thereby making it possible to store all weight parameters of a network in the memory of underlying hardware platforms.\nHardware Area: Results for hardware are shown in Fig. 12(a.2) for CIFAR10 and Fig. 12(b.2) for CIFAR100. For the CIFAR10 case, SNASNet generates a network that occupies 23.9mm\u00b2 area (55% compute and 45% memory parts). Since SNASNet does not include the area constraint in its NAS process, the hardware area of the generated network is not under control during its NAS process. Meanwhile, our HASNAS techniques ensure that the generated"}, {"title": "5.3 Searching Time Speed-Up", "content": "Fig. 13(a) and Fig. 13(b) present the experimental results for the searching time for CIFAR10 and CIFAR100, respectively. In both cases of workloads (CIFAR10 and CIFAR100), our HASNAS offers significant speed-up for the searching time as compared to the state-of-the-art. For the CIFAR10 case, HASNAS improves the searching time by 11.1x, 5.39x, 5.27x, and 4.92x for HASNAS_2M, HASNAS_-4M, HASNAS_6M, and HASNAS_8M, respectively. Meanwhile, for the CIFAR100 case, HASNAS improves the searching time by 7.4x, 3.72x, 3.66x, and 3.66x for HASNAS_2M, HASNAS_4M, HASNAS_-6M, and HASNAS_8M, respectively. The reason is that, our HAS-NAS employs a fewer SNN operations, minimizes redundant archi-tecture candidates to evaluate, incorporates multiple constraints into the search process, hence leading to a smaller search space and a faster searching time as compared to the state-of-the-art. These results demonstrate that, our HASNAS can quickly generate net-work architectures that can maintain high accuracy, while satisfying multiple constraints."}, {"title": "6 CONCLUSION", "content": "We propose the HASNAS framework to find an appropriate SNN architecture that can obtain high accuracy under multiple design re-quirements and constraints (i.e., memory, area, latency, and energy consumption). It is achieved by optimizing SNN operations, develop-ing an efficient SNN architecture, and designing a hardware-aware search algorithm that incorporates the given constraints in the NAS process. Our HASNAS maintains the high accuracy comparable to state-of-the-art, while fulfilling all constraints. For CIFAR10 and CIFAR100 workloads with constraints of 4M parameters of memory, 100mm\u00b2 of area, 400ms of latency, and 120\u00b5J of energy consump-tion, our HASNAS can quickly find an appropriate SNN by up to 11x speed-up than the state-of-the-art; where the state-of-the-art cannot meet the latency and energy constraints. In this manner, our HASNAS framework enables the efficient design automation for developing neuromorphic CIM systems for diverse applications."}]}