{"title": "DYNAPROMPT: DYNAMIC TEST-TIME PROMPT TUNING", "authors": ["Zehao Xiao", "Shilin Yan", "Jack Hong", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu", "Jiayi Shen", "Qi Wang", "Cees G. M. Snoek"], "abstract": "Test-time prompt tuning enhances zero-shot generalization of vision-language models but tends to ignore the relatedness among test samples during inference. Online test-time prompt tuning provides a simple way to leverage the information in previous test samples, albeit with the risk of prompt collapse due to error accumulation. To enhance test-time prompt tuning, we propose DynaPrompt, short for dynamic test-time prompt tuning, exploiting relevant data distribution information while reducing error accumulation. Built on an online prompt buffer, DynaPrompt adaptively selects and optimizes the relevant prompts for each test sample during tuning. Specifically, we introduce a dynamic prompt selection strategy based on two metrics: prediction entropy and probability difference. For unseen test data information, we develop dynamic prompt appending, which allows the buffer to append new prompts and delete the inactive ones. By doing so, the prompts are optimized to exploit beneficial information on specific test data, while alleviating error accumulation. Experiments on fourteen datasets demonstrate the effectiveness of dynamic test-time prompt tuning.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite achieving remarkable successes, foundation models such as Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) still suffer from distribution shifts when adapting to downstream tasks (Zhou et al., 2022a;b; Xiao et al., 2024). To improve test-time adaptation of the model in the presence of distribution shifts, recent works introduce learnable prompts at test time. The methods freeze the CLIP model parameters while only tuning the learnable prompts for test data. As shown in Figure 1a, test-time prompt tuning (TPT) (Shu et al., 2022) adapts the prompt to each test sample individually, which is widely followed by recent works (Ma et al., 2023; Samadh et al., 2023; Yoon et al., 2024). However, tuning in such a way ignores the relatedness among test samples, which offers rich information on the test data distribution. To incorporate the information from relevant test samples, one straightforward method is to follow previous test-time adaptation methods (Wang et al., 2021; Goyal et al., 2022) and tune the test prompts online (Figure 1b). This encourages the prompts to exploit previous test information for better model adaptation. However, as detailed later on in this paper, we establish that online test-time tuning leads to severe prompt collapse due to error accumulation.\nTo have test-time prompt tuning benefit from relevant online information while reducing error accu-\nmulation, we constitute the concept of dynamic test-time prompt tuning, abbreviated as DynaPrompt. Specifically, DynaPrompt adaptively selects and optimizes the relevant online prompts for each sample while freezing the rest, yielding effective adaptation for the entire test set. As illustrated in Figure 1c, a prompt buffer is involved, which enables a set of online prompts to be flexibly selected, updated, and appended for each test sample. To ensure the appropriate prompt selection without collapse in DynaPrompt, we devise a comprehensive selection strategy with two metrics: prediction entropy and probability difference, which measure the model uncertainty of the predictions and the model sensitivity to the input changes. Then we construct the prompt screening threshold from these two metrics to achieve adaptive selection for different test samples. Such a screening rule selects the prompts with lower prediction entropy and larger probability differences and prefers those with"}, {"title": "2 PRELIMINARY", "content": "We first provide a brief preliminary on the CLIP model, prompt learning, and test-time prompt tuning, containing background and commonly used techniques for test-time prompt tuning.\nCLIP model (Radford et al., 2021). This pretrained model consists of an image encoder $F_{\\theta_I}(\\cdot)$ and a text encoder $F_{\\theta_T}(\\cdot)$, where $\\theta_I$ and $\\theta_T$ denote pre-trained parameters of the corresponding encoders. The image and text encoders take an input image x and text prompts as inputs, respectively. Given a downstream classification task with a set of class names, CLIP performs zero-shot classification on each input image x. Specifically, CLIP gets the image feature as $f_x = F_{\\theta_I}(x)$ and text features (i.e., zero-shot classifier) as $\\{f_{t_c}|f_{t_c}=F_{\\theta_T}(t_c)\\}_{c=1}^C$. C is the number of class names and $t_c$ is a manual-crafted text prompt corresponding to class c, e.g., \u201ca photo of a [class c].\u201d The probability of x\nbelonging to class c is $p(\\hat{y} = c|x) = \\frac{exp(cos(f_x,f_{t_c})/\\tau)}{\\sum_{c'=1}^{C} exp(cos(f_x,f_{t_{c'}})/\\tau)}$, where $cos(\\cdot,\\cdot)$ denotes cosine similarity and $\\tau$ is a learned temperature. Thus, CLIP directly obtains the prediction as $\\underset{C}{\\arg \\max} p(\\hat{y} = c|x)$.\nPrompt learning. To further enhance the adaptation of CLIP on downstream tasks, recent methods, e.g. (Zhou et al., 2022a;b; Khattak et al., 2023), introduce learnable prompts $v = [v_1][v_2] \\cdots [v_n]$, while freezing the parameters of CLIP\u2019s encoders. Zhou et al. (2022b) introduce learnable prompts"}, {"title": "3 PROMPT COLLAPSE IN ONLINE PROMPT TUNING", "content": "In several real-world applications, there are a large number of related test samples that arrive sequentially. In such cases, earlier observed samples can provide beneficial information about the test distribution and reserve the potential to improve the prediction for subsequent samples. Inspired by this insight, we propose to extend test-time prompt tuning (Shu et al., 2022) to online scenarios, formulating online test-time prompt tuning (Online TPT).\nOnline TPT retains most of the setups in TPT (Shu et al., 2022), where prompts are obtained through one-step optimization using entropy minimization for each test sample. As illustrated in Figure 1, the primary distinction lies in the initialization of the prompt for each test sample. While TPT (Shu et al., 2022) resets the prompt to the initial state $v_0$ for each sample, Online TPT uses the optimized prompt from the previous sample as the starting point for the current sample.\nAs shown in Figure 2, Online TPT performs competitively with TPT initially, but the performance declines rapidly, nearly approaching 0% at the end. This work refers to this phenomenon as prompt collapse, where the prompt tends to accumulate excessive noise and prevent the model from making"}, {"title": "4 DYNAMIC PROMPT TUNING", "content": "As previously mentioned, this work develops DynaPrompt to enrich the family of test-time prompt tuning. Our motivation is to exploit beneficial information from prompt histories and alleviate the error accumulation in online prompt tuning. DynaPrompt adaptively selects relevant online prompts for each test sample to optimize and includes an online update prompt buffer $V_n$ for each specific test sample $x_n$. The buffer contains a set of online learnable prompts $V_n = \\{v_i\\}_{i=1}^{M_n}$ to store the distribution information in past samples, where $M_n$ denotes the number of prompts in the buffer at test step n. Each online prompt $v_i$ is initialized with a hand-crafted prompt (Shu et al., 2022) or a pretrained prompt (Zhou et al., 2022b) $v_0$ and adaptively optimized with the online samples. During prompt tuning, each test sample first selects a subset of the online prompts in the buffer and updates the buffer by optimizing the selected prompts. As shown in Figure 3, DynaPrompt consists of dynamic prompt selection (Section 4.1) and prompt appending (Section 4.2) strategies, as well as optimization and prediction with the dynamic prompts."}, {"title": "4.1 DYNAMIC PROMPT SELECTION", "content": "DynaPrompt introduces the dynamic prompt selection strategy to select appropriate prompts for each test sample from the online prompt buffer. Such a strategy returns a subset of the selected prompts $S_n = \\{v_i \\in V_n | f(v_i)\\}$ for sample $x_n$, where $f(v_i)$ denotes selection conditions with specific metrics. We introduce the prediction entropy and probability difference as the selection metrics."}, {"title": "Entropy-based selection.", "content": "We employ the prediction entropy as one of our prompt selection metrics. Widely used in classification tasks, entropy quantifies the uncertainty of predictions, assessing how confident the prompt is on the test data. Prompts with lower entropies reflect more confident predictions on the test sample (Niu et al., 2022; 2023), indicating the prompt has more prior knowledge and relevant information on the sample. Given a test sample $x_n$ and the online prompt in the corresponding prompt buffer $v_i \\in V_n$, we can calculate the entropy as:\n$D_{ent}(x_n, v_i) = - \\sum_{c=1}^{C} p(\\hat{y} = c/X_n, v_i) \\log p(\\hat{y} = c/X_n, v_i),$ (2)\nwhere $p(\\hat{y} = c|X_n, v_i)$ denotes the averaged prediction across the selected augmentations similar to Eq. (1). In operation, we use the entropy of the initial prompt $v_0$ as the threshold and select the online prompts with lower entropy, where the selected prompts are more confident on the test sample (Niu et al., 2022). Formally, we have the entropy-selected online prompts subset $E_n$:\n$E_n = \\{v_i \\in V_n | D_{ent}(x_n, v_i) \\leq D_{ent}(X_n, v_0)\\}$. (3)\nTherefore, $E_n$ contains the online prompts that produce more confident predictions than the initial prompt, indicating they incorporate more relevant information and are better suited for $x_n$."}, {"title": "Probability difference selection.", "content": "However, the entropy is not always reliable in test-time tuning, especially when encountering distribution shifts (Lee et al., 2024). When continually selected and optimized for low entropy, the online prompts can be tuned overconfidently and produce incorrect predictions with very low entropy, causing prompt collapse discussed in Section 3. To avoid selecting overconfident prompts, we further introduce a probability difference metric for dynamic prompt selection. The probability difference $D_{pro}(x_n, v_i)$ quantifies the prediction probability differences between the original test sample $x_n$ and its augmentations $X_n$, assessing the sensitivity of the prompts to the changes in the structure information of the input sample.\nGiven a test sample $x_n$ and online prompt $v_i \\in V_n$, we calculate the probability difference as:\n$D_{pro}(x_n, v_i) = p(\\hat{y} = c^*|x_n, V_i) - p(\\hat{y} = c^*|X_n, V_i),$ (4)\nwhere $c^* = \\underset{c}{\\arg \\max} p(\\hat{y} = c|x_n, v_i)$ denotes the pseudo-label of the test sample $x_n$ predicted by prompt $v_i$. Prompts with higher $D_{pro}$ are more sensitive to the changes of the sample $x_n$, which are less likely to be overconfident. By contrast, lower $D_{pro}$ indicates similar predictions regardless of input modifications, increasing the risk of overconfidence and prompt collapse, especially with low prediction entropy.\nTo circumvent overconfident prompts during dynamic selection, we propose to select the online prompts with higher $D_{pro}$ values. Similar to the entropy metric, we use the probability difference of the initial prompt as the threshold for prompt selection. Formally, the subset of online prompts with high probability difference on the test sample $x_n$ are formulated as:\n$R_n = \\{v_i \\in V_n | D_{pro}(x_n, v_i) \\geq D_{pro}(X_n, v_0)\\}$. (5)"}, {"title": "Dynamic selected prompts.", "content": "Combining Eq. (3) and Eq. (5) together, we obtain the subset of selected prompt $S_n$, where the selected prompt meets the requirements in both above selection processes:\n$S_n = E_n \\cap R_n$. (6)\nBy taking the intersection of the two subsets in Eq. (6), the selected prompts simultaneously satisfy both lower entropy in Eq. (3) and larger probability differences in Eq. (5), which produce more confident predictions and are more sensitive to the changes of the test sample. Therefore, the selected prompts are more relevant to the test samples and have low risks of collapse. Moreover, since we utilize the entropy and probability differences of the initial prompt as thresholds, our method enables adaptively prompt selection for each test sample. By autonomously selecting relevant prompts for each test sample, the online prompts are enriched with more specific data distribution information, enhancing both the predictive performance of the current test sample and the optimization of the selected prompts. Since the irrelevant and collapse prompts are frozen, potential conflicting optimization directions for these prompts are avoided, thereby reducing error accumulation.\nNote that the predictions and entropy in Eq. (2) and (4) are inherently calculated for test-time prompt tuning in Eq. (1). Thus, our method introduces very few extra operations for prompt selection."}, {"title": "Optimization and prediction.", "content": "For each test sample $x_n$, after dynamically selecting the online prompts $V_n$, we first optimize the selected prompts by minimizing the entropy as:\n$\\mathcal{L}_{ent}(S_n; x_n) = - \\sum_{c=1}^{C} ( p(\\hat{y} = c|X_n, S_n) \\log p(\\hat{y} = c|X_n, S_n), S_n \\leftarrow S_n - \\alpha \\nabla \\mathcal{L}_{ent}(x_n, S_n),$ (7)\nwhere $p(\\hat{y} = c|X_n, S_n)$ denotes the average prediction probabilities across the selected prompts and sample augmentations. With the updated prompts $S_n$, we perform prediction for the test sample $x_n$ as $\\underset{C}{\\arg \\max} p(\\hat{y} = c|x_n, S_n)$."}, {"title": "4.2 DYNAMIC PROMPT APPENDING", "content": "During dynamic prompt selection, there can be no appropriate prompt in the prompt set $V_n$ for specific test samples, leading to an empty $S_n$. That means the online prompts in $V_n$ are either irrelevant for the current sample or collapsed. In this case, utilizing existing online prompts for the sample can lead to conflict optimization or severe error accumulation. To fix this issue, we introduce dynamic prompt appending as a complementary strategy. Specifically, our method appends an initial prompt $v_0$ into the prompt set $S_n$ when it is empty. As a result, the selected prompt set $S_n$ is reformulated as:\n$S_n = \\begin{cases} \\{v_0\\}, & \\text{if } E_n \\cap R_n = \\O; \\\\ E_n \\cap R_n, & \\text{otherwise}. \\end{cases}$ (8)\nThe prompt in $S_n$ is optimized the same as Eq. (7) and then utilized in the inference of the sample.\nHowever, the size of the prompt buffer $V_n$ cannot be infinite when appending new learnable prompts due to memory constraints and computational costs. To avoid infinitely increasing numbers of prompts in the prompt buffer, we set the maximum number M of prompts in $V_n$ as a hyperparameter, i.e., $M_n \\leq M$ and introduce a prompt deletion mechanism: when a new prompt is appended into the prompt buffer and the current number of prompts is M, the method will remove the most inactive prompt $v_{inactive}$ from the buffer. By optimizing $S_n$ in Eq. (8) to $S_m$ through entropy minimization in Eq. (7), we formulate the update of the prompt buffer with dynamic prompt appending as:\n$V_{n+1} = \\begin{cases} V_n + S_n - \\{v_{inactive}\\}, & \\text{if } E_n \\cap R_n = \\O \\text{ and } M_n = M; \\\\ V_n + S_n - S_n, & \\text{otherwise}. \\end{cases}$ (9)\nIt is worth noting that the \u201c+\u201d and \u201c-\u201d operations are the append and delete operations for the prompt buffer. As shown in Figure 3, we always put the optimized prompts in $S_n$ at the start of the prompt buffer. Therefore, we achieve the deletion mechanism by entirely removing the online prompt at the end of the buffer, which has not been activated for the maximum allowed time. By appending new online prompts and ejecting the inactive ones, our dynamic prompt tuning effectively incorporates information from new data distributions and reduces error accumulation. We provide an algorithm of our method in Appendix A."}, {"title": "5 RELATED WORK", "content": "Prompt learning. To adapt vision-language models such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) to downstream tasks, prompt learning methods are introduced (Lester et al., 2021; Li & Liang, 2021; Zhou et al., 2022b). Zhou et al. (2022b) propose learnable prompts in the input embedding space of the language model in CLIP. ProGrad (Zhu et al., 2023) aligns the gradients of the learnable prompts with the original prompt. In addition to the language input space, Bahng et al. (2022) introduces prompt learning into the vision branch of the CLIP model. Khattak et al. (2023) further proposes joint prompts for both vision and language encoders. To improve the generalization ability of the learned prompts, Zhou et al. (2022a) introduce imaging conditions into the language prompts. KgCoOp (Yao et al., 2023) reduces the forgetting of the general knowledge in the CLIP model by reducing the discrepancy between the learnable and handcrafted prompts. Derakhshani et al. (2023) propose Bayesian prompt learning to incorporate uncertainty in the learnable prompts. CoPrompt (Roy & Etemad, 2024) enforces the prediction consistency of the trainable and pre-trained models to prevent overfitting on the downstream task. Any-shift prompting (Xiao et al., 2024)"}, {"title": "6 EXPERIMENTS", "content": "Fifteen datasets. Following previous methods (Shu et al., 2022; Samadh et al., 2023), we conduct experiments across two settings that suffer from distribution shifts to demonstrate the effectiveness of our method: domain generalization and cross-dataset shifts. For the domain generalization setting, we evaluate the method on ImageNet (Deng et al., 2009) and its four variant datasets: ImageNet-V2 (Recht et al., 2019), ImageNet-(S)ketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet-R (Hendrycks et al., 2021a). For the cross-dataset setting, we evaluate our method on 10 image classification datasets covering various tasks: Caltech101 (Fei-Fei et al., 2004), OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014), FGVCAircraft (Maji et al., 2013), SUN397 (Xiao et al., 2010), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), and UCF101 (Soomro et al., 2012).\nImplementation details. Based on the CLIP model with ViT-Base-16 (Dosovitskiy et al., 2020), we initialize our dynamic prompts with the manually crafted \u201ca photo of a\u201d and optimize the prompts online in the text input embedding space. The prompt set optimized by one test sample is utilized for the next sample. Following TPT (Shu et al., 2022), we generate 63 augmentations by random resize crops for each individual test image to construct a batch of 64 images including the original image. During the dynamic tuning, we calculate the entropy and augmentation probability differences over these 63 augmented images as the dynamic prompt selection metrics. The thresholds are obtained in the same way based on the initial prompt. We set the maximum number of the prompt set M as 10. We append new prompts to the dynamic prompt set when no appropriate prompt is selected for the test sample. Once the number of prompts in the prompt set V exceeds M, we remove the prompt that has been inactive for the longest time. For optimization, we select the top 10% confident samples"}, {"title": "6.1 COMPARISIONS", "content": "Comparisons on domain generalization setting. We compare our method on the domain general-ization setting with both prompt learning (Zhou et al., 2022a;b; Khattak et al., 2023; Roy & Etemad, 2024) and test-time prompt tuning methods (Shu et al., 2022; Samadh et al., 2023). The prompt learning methods train their prompts by supervised cross-entropy loss on ImageNet. As shown in Table 1, our method achieves better overall performance compared with the prompt learning methods. Moreover, since our DynaPrompt is orthogonal to most of these prompt learning methods, applying our method together with prompt learning methods like CoOp (Zhou et al., 2022b) and MaPLe (Khattak et al., 2023)) further improves the performance.\nDynaPrompt also surpasses recent test-time prompt tuning methods. Our method outperforms TPT (Shu et al., 2022) on all datasets with both hand-crafted and learned prompts (Zhou et al., 2022b; Khattak et al., 2023), achieving at least 1% improvements. The method also achieves better overall performance compared with other recent methods DiffTPT (Feng et al., 2023), AdaPrompt (Zhang et al., 2024), and C-TPT (Yoon et al., 2024). Compared with PromptAlign (Samadh et al., 2023), which is constructed on MaPLe (Khattak et al., 2023) and utilizes extra source data during test-time tuning, our method is also superior.\nComparisons on cross-dataset setting. On the cross-dataset setting, we also compare our method with both prompt learning and test-time prompt tuning methods. As shown in Table 2, our method outperforms the common prompt learning methods for 8 of the 10 datasets and achieves the best overall performance. Compared with the test-time prompt tuning methods, the proposed method again outperforms TPT with both hand-crafted and learned prompts (Khattak et al., 2023). Our method also achieves higher accuracy compared with the recent test-time prompt tuning methods Samadh et al. (2023). We observe that the improvements on these datasets for our method, and even most test-time prompt tuning methods, are not as obvious as in the domain generalization setting. The"}, {"title": "6.2 ABLATION STUDIES", "content": "Effectiveness of dynamic prompt selection and appending. To investigate the roles of our dynamic selection and appending strategies of dynamic prompt learning, we conduct experiments on domain generalization datasets. As shown in Table 3, removing either the entropy or the probability difference metric during dynamic prompt selection results in obvious performance degradation on all datasets. Without the entropy, the selected prompts can be either confident or uncertain to the test sample, which is not suitable for the sample, leading to performance degradation. Without the probability difference, the proposed method may select collapsed prompts during test-time tuning, resulting in the wrong direction of the prediction and optimization.\nAs shown in Table 4, the performance degrades considerably when we remove the dynamic prompt appending strategy from our method. Without the appending and removing strategy, the method can only select the online prompts in the buffer, even if there is no appropriate one. In this case, the risks of optimization in conflict directions are highly amplified. The error during the optimization is then accumulated in the sequential online samples, leading to prompt collapse."}, {"title": "Analyses on error accumulation.", "content": "Here we provide more analysis on our DynaPrompt. As shown in Table 5, TPT achieves good overall performance as it avoids error accumulation by in-dependent prompt tuning. Online TPT performs competitively at the start but declines rapidly as shown in Figure 2. While Oracle also has a per-formance degradation initially, the performance stabilizes since it avoids error accumulation by tuning prompts only with correct predictions. By incorporating the beneficial information from online samples, Oracle surpasses TPT and Online TPT. Our DynaPrompt reduces error accumulation and achieves stable performance by dynamically selecting, appending, and deleting the online prompts. Therefore, the method achieves good overall performance and beats TPT by incorporating relevant information in online data. Moreover, our method performs better during online learning, proving its capability to capture beneficial information."}, {"title": "Influence of the prompt buffer size.", "content": "We also ablte the influence of the prompt buffer size M on our method in Figure 4a. The experiments are conducted on ImageNet-A. As the prompt buffer size increases, the proposed method shows an upward slope. The improvement is faster when the buffer size is smaller than 10. Figure 4b shows the time costs of the proposed method, which continually increases with larger buffer sizes. Compared with TPT, our method requires more time costs, which can be a limitation of the approach. Note that most of the additional time cost stems from optimizing multiple prompts rather than the selection and appending strategy. For instance, with the buffer size 10, the total processing time per test sample is approximately 0.39 seconds, of which the selection and appending steps account for only 0.004 seconds. For a good trade-off between performance and time costs, we set the maximum size of the prompt buffer to 10."}, {"title": "Sensitivity to test time sample order.", "content": "As our DynaPrompt optimizes the prompt online, the performance of the method can be influenced by the order of the test samples. To investi-gate this influence, we conduct experiments on ImageNet-A and ImageNet-R for six rounds, which have different sample orders. We shuffle the sample order with different random seeds at test time, which leads to different sample orders. The results are provided in Figure 5a and 5b, respectively. Order 0 denotes the default order the same as the experiments of TPT (Shu et al., 2022). We observe that there are fluctuations in the performance of both datasets. The performance on ImageNet-R is more stable with a larger number of test samples (30,000) than ImageNet-A (7,500). Nevertheless, independent of the test order, the proposed method surpasses TPT consistently."}, {"title": "7 CONCLUSION", "content": "In this paper, we propose DynaPrompt, a new test-time prompt tuning approach, which exploits beneficial information from the online test samples while alleviating error accumulation. Our method introduces a dynamic prompt buffer, which adaptively selects and optimizes prompts for each test sample. The selected prompts incorporate relevant information from previous test samples, thereby benefiting the prediction for the current sample. By optimizing the selected prompts while freezing the rest, the method further enhances the learned prompts to incorporate relevant information from test data. DynaPrompt also enables the buffer to autonomously append new learnable prompts and delete the inactive ones, improving adaptability to new test data and reducing the risk of error accumulation. Experiments on fourteen benchmarks validate the effectiveness of our proposal."}, {"title": "REPRODUCIBILITY", "content": "We include all necessary details to facilitate the reproducibility of our work. The experimental setup, including benchmarks, model configurations, hyperparameters, and evaluation protocols, is thoroughly explained in the experiments section. We also give an algorithm in the Appendix to provide the detailed process of our method. We will make our code publicly available."}, {"title": "A ALGORITHM", "content": "Algorithm 1 Dynamic Test-Time Prompt Tuning (DynaPrompt)\n1: Input: Test samples $\\{X_n\\}_{n=0}^{N}$; a prompt buffer $V_n$ with length $M_n$, initialized as $V_0 = \\O$; maximum buffer size M; hand-crafted or pretrained initial prompt $v_0$\n2: for n = 0: N do\n3: Randomly augment $x_n$ to $X_n$.\n4: Obtain predictions $\\{p(y|X_n, v_i)\\}$, and $p(y|X_n, v_0)$.\n// Dynamic prompt selection.\n5: Calculate $D_{ent}(x_n, v_i)$ and $D_{ent}(x_n, v_0)$ by Eq. (2), then select prompts subset $E_n$ by Eq. (3).\n6: Calculate $D_{pro}(x_n, v_i)$ and $D_{pro}(x_n, v_0)$ by Eq. (4), then select prompts subset $R_n$ by Eq. (5).\n7: Select relevant prompts $S_n$ by $E_n \\cap R_n$.\n// Dynamic prompt appending.\n8: if $S_n = \\O$ then\n9: $S_n = \\{V_0\\}$.\n10: end if\n// Optimizing selected prompts.\n11: Tune the prompts in $S_n$ by entropy minimization in Eq. (7): $S_n \\leftarrow S_n - \\alpha \\nabla \\mathcal{L}_{ent}(X_n, S_n)$.\n// Update the prompt buffer $V_n$ to $V_{n+1}$.\n12: if $M_n = M$ and $E_n \\cap R_n = \\O$ then\n13: Append the updated prompt $S_n$ to the top of $V_n$.\n14: Remove the prompt $v_{inactive}$ at the bottom of $V_n$.\n15: else\n16: Append the optimized prompts in $S_n$ to the top of $V_n$.\n17: Remove the selected prompts in $S_n$ from $V_n$.\n18: end if\n19: end for"}, {"title": "B DETAILED IMPLEMENTATIONS", "content": "Details of data augmentation. To generate the augmented data $X_n$ for each sample, we follow the same data augmentation strategy used in TPT (Shu et al., 2022). That is, we use AugMix (Hendrycks et al., 2020) to augment the original test image into 63 different augmentation samples, leading to 64 samples in total for each test image. Each test image is first augmented by resize and random crop, then fed into the AugMix strategy with several augmentation methods including auto contrast, equalization, posterization, rotation, solarization, shearing, and translating."}, {"title": "C EXTRA EXPERIMENTS", "content": "Effect of initial prompts. The initial prompts can affect the final performance in prompt learning (Zhou et al., 2022a). To investigate the effect of the initial prompts of the proposed method, we conducted experiments on ImageNet-A using various initial text prompts. As shown in Table 6, the initial prompts affect the performance of CLIP (Radford et al., 2021), TPT (Shu et al., 2022), as well as our method. The reason can be related to the initial predictions of the original CLIP model. Nonetheless, our method consistently outperforms TPT, showing robustness despite variations in initialization.\nAblations on prompt length for online test-time prompt tuning. To investigate the prompt length effect, we experiment with longer prompts for both TPT and Online TPT. We set the prompt length to 40, which is 10 times longer than the 4-item original \"a photo of a\". We consider two types of long prompts: (A) 10 times copy of \"a photo of a\u201d, (B) \u201cLet us solve an image classification task: a photo of a distinct object, animal, plant, or scene, captured in diverse environments and representing meaningful categories. Carefully analyze its features; the exact category of the photo is a\", generated by GPT-40."}]}