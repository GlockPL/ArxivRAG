{"title": "SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval", "authors": ["Bhavin Jawade", "Jo\u00e3o V. B. Soares", "Kapil Thadani", "Deen Dayal Mohan", "Amir Erfan Eshratifar", "Benjamin Culpepper", "Paloma de Juan", "Srirangaraj Setlur", "Venu Govindaraju"], "abstract": "Compositional image retrieval (CIR) is a multimodal learning task where a model combines a query image with a user-provided text modification to retrieve a target image. CIR finds applications in a variety of domains including product retrieval (e-commerce) and web search. Existing methods primarily focus on fully-supervised learning, wherein models are trained on datasets of labeled triplets such as FashionIQ and CIRR. This poses two significant challenges: (i) curating such triplet datasets is labor intensive; and (ii) models lack generalization to unseen objects and domains. In this work, we propose SCOT (Self-supervised COmpositional Training), a novel zero-shot compositional pretraining strategy that combines existing large image-text pair datasets with the generative capabilities of large language models to contrastively train an embedding composition network. Specifically, we show that the text embedding from a large-scale contrastively-pretrained vision-language model can be utilized as proxy target supervision during compositional pretraining, replacing the target image embedding. In zero-shot settings, this strategy surpasses SOTA zero-shot compositional retrieval methods as well as many fully-supervised methods on standard benchmarks such as FashionIQ and CIRR. Our code and models are available at https://github.com/yahoo/SCOT.", "sections": [{"title": "1. Introduction", "content": "The field of image retrieval is advancing rapidly, with growing interest in multimodal queries that incorporate both images and text. Compositional Image Retrieval (CIR) is a recently proposed task that aims at retrieving images using a query composed of both an image and text [14, 37]. The query or reference image defines some initial desired elements, while the text describes the relative modification that a user would like to see in the retrieved images. CIR provides users with a versatile way to communicate their intent through iterative query refinement, which is potentially valuable in a broad range of real-world tasks such as product retrieval in e-commerce and fine-grained web search.\nCIR can be framed as a multimodal fused representation learning task in which the goal is to train an effective feature fusion network. This sits in contrast to other well-studied vision-language tasks such as image-text matching, image captioning, and visual question answering, as CIR uniquely learns a representation to jointly capture visual cues and text descriptors that match the target image of interest. Most CIR methods [9, 12, 19,37] are trained in a fully-supervised manner using curated human-annotated datasets of triplets, with each triplet consisting of a reference image, a user-provided modification text, and a target image.\nCurrent supervised CIR approaches do not generalize well to unseen domains or zero-shot scenarios, as illustrated in Fig 1. They are dependent on the availability of large datasets of image-text-image triplets, which are typically domain-specific and have limited applicability to open-world settings. Manual labeling for new triplet datasets is also labor-intensive. To overcome these challenges, a recent line of work explores zero-shot CIR using textual inversion [2,6,31,35], e.g., using image-text pairs to learn to map images into text token embeddings. An image-derived token embedding-which can be thought of as corresponding to a pseudo-token)\u2014can then be combined with text token embeddings from the modification text and encoded as text to produce a composite embedding for retrieval. These approaches do not require annotated image-text-image triplets and can adapt to new domains thanks to the generalizability of contrastively-pretrained image-text encoders.\nIn this work, we propose a novel pretraining strategy for zero-shot CIR (ZS-CIR) which we name SCOT (Self-supervised COmpositional Training). This approach does not require human-annotated triplets and demonstrates open-world generalizability by using captioned images from large and varied datasets. We specifically exploit the proximity of visual and textual representations of the same concept in the embedding space of large-scale contrastively-pretrained vision-language models, which enables the use of target text embeddings instead of target image embeddings for supervision. Given an image and its caption, we first generate a training example by feeding the caption into a large language model (LLM) and prompting it to output a creative modification text and a corresponding modified caption. A CIR model is then trained by using the reference image and the generated modification text as input, with the generated modified caption as the target.\nSCOT models are trained to compose reference images with modification texts by optimizing a contrastive image retrieval loss. This differs from inversion-based techniques, which do not directly train a composition model but rely on the composition capabilities of existing frozen pretrained image-text encoders. SCOT pretraining is agnostic to the choice of composition model, which can include unfrozen encoders [25] and early image-text fusion [21]. Comprehensive experiments show that SCOT surpasses current ZS-CIR techniques and nears fully-supervised performance on FashionIQ [40] and CIRR [26] without domain-specific training. The key contributions of this work are:\n1.  We introduce a novel compositional pretraining strategy that requires only image-text pairs, using LLMs to create image-text-text triplets and pretrained vision-language models to encode both images and text.\n2.  We demonstrate zero-shot generalizability on domain-specific (FashionIQ [40]) and open-world (CIRR [26]) compositional retrieval datasets, showing that SCOT outperforms existing zero-shot approaches.\n3.  Through quantitative and qualitative experiments, we evaluate the impact of various parameters such as training dataset size, sample distribution, backbone and supervision type on zero-shot generalizability."}, {"title": "2. Related Work", "content": "Compositional Image Retrieval (CIR): Numerous methods have been proposed to learn composite representations of visual and text features for retrieval. Most research lies within the supervised setting [3, 4, 8, 9, 12, 17, 19, 21, 26, 27, 36, 37], with earlier work relying on fashion datasets containing human-annotated triplets [15, 40]. The DCNet approach [19] jointly trains feature extractors with a composition and correction network on FashionIQ [40]. COSMO [20] uses content and style modulator networks to combine the image and text representations. FashionVLP [12] is a recently-proposed multimodal Transformer trained with a variety of fashion image inputs including crops, landmarks and ROIs. The need to go beyond fashion products and motivate research in open-world interactive retrieval led to the creation of open-domain annotated datasets: CIRR [26] (using images from NLVR2 [34]), CIRCO [2], and LaSCo [21] (the latter two using images from MS-COCO [24]). Despite this progress, the zero-shot generalizability of traditional fully-supervised models has been limited.\nZero-Shot Compositional Retrieval (ZS-CIR): To overcome these limitations, recent work [2,5,6,13,18,25,31,35] has developed zero-shot annotation-free strategies for CIR. One line of work [2, 6, 31, 35] adopts textual inversion, which had previously found success in the text-to-image generation literature [11]. Recently, Saito et al. [31] proposed Pic2Word wherein an MLP is trained to map a picture to a pseudo-token, which the text encoder can then combine with the modification text to produce a composite embedding. Baldrati et al. [2] present SEARLE, which involves a two-stage process for training a textual inversion network. The first stage runs Optimization-based Textual Inversion (OTI) with CLIP [30] image and text encoders to find a text token embedding that corresponds to a given image encoding. In the second stage, those token embeddings are used as targets to learn a textual inversion network. Note that textual inversion approaches focus on learning how to invert the image into token embeddings, while taking advantage of the existing composition capabilities of pretrained text encoders. In contrast, our approach directly optimizes a contrastive loss by training with triplets that closely mimic those of the CIR task. It can thus use any choice of composition model (including unfrozen encoders), and can be easily fine-tuned further with domain-specific data. A variety of other ZS-CIR approaches have been recently proposed. Gu et al. [13] train a denoising Transformer for image-text composition on 18M synthetic images along with 2B captioned images from LAION [32]. Karthik et al. [18] introduce CIReVL, a training-free approach that involves captioning the reference image, modifying the caption using an LLM and retrieving the target image using the modified caption. Chen and Lai [5] propose masking-augmented contrastive pretraining for visual and textual encoders to recover masked visual information through text prompts. Jang et al. [17] train a model to generate the modification text given a pair of images. The model is sued for generating synthetic training data, resulting in a semi-supervised approach. In concurrent work, Liu et al. [25] propose an approach for automatic construction of image-text-image training triplets. They source captioned images from the LAION-COCO dataset [33] and use either text templates or LLMs to generate modification texts and corresponding modified captions. Modified captions are then used to retrieve images to serve as supervision targets. The authors note that this approach of retrieving supervision target images from a corpus can be problematic due to the eventual absence of suitable images and/or retrieval errors [25]. In Section 4.4, we show example triplets illustrating these issues and present a controlled experiment demonstrating that SCOT's use of semantically-relevant text targets significantly outperforms the use of retrieved image targets."}, {"title": "3. Method", "content": "This section describes SCOT, a ZS-CIR technique requiring only captioned image datasets. The approach is outlined in Fig. 2. We review contrastively pretrained image-text encoders in Section 3.1. Sections 3.2, 3.3 and 3.4 detail our pretraining strategy, loss function, and inference.\n3.1. Large-Scale Contrastive Pretraining\nFollowing previous work, we use image and text representations from large-scale contrastively-pretrained models: CLIP [30], BLIP [23] and BLIP-2 [22]. CLIP (Contrastive Language-Image Pretraining) [30] aims to jointly learn visual and textual representations that are semantically aligned. For a given image-caption pair $(v_i, t_i)$, let $V_i = f_v(v_i)$ denote the normalized image embedding from image encoder $f_v$ and $T_i = f_t(t_i)$ denote the normalized text embedding from text encoder $f_t$. CLIP contrastively enforces high similarity between positive pairs $(V_i, T_i)$ and low similarity between negative pairs $(V_i, T_j), V_i \\neq j$. This is implemented via a symmetric cross-entropy loss over the similarity scores of image and text embeddings $V_i$ and $T_j$. The image-to-text part of the loss is defined as:\n$L_{i2t} = \\frac{1}{N} \\sum_{i=1}^{N} -log \\frac{e^{(V_i, T_i)/\\kappa}}{\\sum_{j=1}^{N} e^{(V_i, T_j)/\\kappa}} $\n3.2. Self-Supervised Compositional Pretraining\nOur approach is primarily motivated by the fact that contrastively-pretrained models are able to align related visual and textual representations in the embedding space. This enables us to use the aligned textual representation as a proxy for an image representation, thereby eliminating the need for a target image during training. For inference, we can search across gallery images by encoding them using the the contrastively-paired visual encoder.\nThe goal of this method is to train a composition operation $f_c$ to combine the representations from a user-provided image and modification text. We rely on contrastively-paired image and text encoders, denoted respectively as $f_v$ and $f_t$. Given a captioned image dataset $D = \\{(v_i, t_i)\\}_{i=1}^{N}$, we compute the image embeddings $V_i = f_v(v_i)$ and corresponding caption embeddings $T_i = f_t(t_i)$.\nWe prompt a large language model (LLM) to generate a modification text $m_i$ given an original caption $t_i$. The modification text will be used as one of the inputs to the composition function $f_c$ during training. To provide the supervision signal for the predicted composed representation, we use the same LLM to generate a modified caption $u_i$, which should be similar to $t_i$ but with modification $m_i$ applied. Fig. 3 contains samples of LLM-generated triplets.\nNext, we compute the embeddings for $m_i$ and $u_i$ using $f_t$.\n$m_i, u_i \\leftarrow LLM(t_i)$\n$T_m^i, T_u^i = f_t(m_i), f_t(u_i)$\nWe pass the modification text representation $T_m^i$ and the image representation $V_i$ through the learnable composition function $f_c$ to obtain the composed image representation $V_c^i = f_c (V_i, T_m^i)$. We use the recently proposed Combiner network [3] to implement $f_c$. Briefly, it performs a learn-able weighted fusion of the image and text embeddings. We encourage readers to refer to [3] or the supplementary material of this paper for details on the Combiner network.\n3.3. Training Objective\nWe minimize a modified contrastive loss in order to pull the predicted composed embedding $V_c^i$ towards the generated target text embedding $T_u^i$ for an input sample $(V_i, t_i)$ while pushing it away from target text embeddings $T_u^j, \\forall j \\neq i$ from other examples within its batch. Let $S(x, y)$ denote the cosine similarity between vectors x and y, i.e., $S(x, y) = \\frac{x \\cdot y}{||x||_2 ||y||_2}$. We define:\n$L_{pos} = -log S(V_c^i, T_u^i)$\n$L_{neg} = -\\sum_{i,j} log S_\\lambda (V_c^i, T_u^j) \\cdot (1 - \\delta_{ij})$\nwhere $\\delta_{ij}$ is the Kronecker delta function, which is 1 when $i = j$ and 0 otherwise.\nPrevious work in traditional cross-modal retrieval [10, 39] has demonstrated the effectiveness of hard-negative mining. To improve the robustness of our embeddings, we follow [10,38] and adopt a margin-based hard-negative mining strategy. Let $\\lambda$ be a fixed scalar margin, then we define:\n$S_\\lambda (x, y) = S(x, y) \\cdot \\Theta (S(x, y) > \\lambda)$\nwhere $\\Theta$ is the Heaviside step function, which is 1 if the condition inside is true and 0 otherwise. This is used in Eq. 5 resulting in an updated negative loss.\n$L'_{neg} = -\\sum_{i,j} log S_\\lambda (V_c^i, T_u^j) \\cdot (1 - \\delta_{ij})$\nFor stronger supervision, we also include the original unmodified caption embeddings $T_u^j \\forall j < N$ as hard negatives for $V_c^i$. This moves the composed representation away from the original caption and closer to the desired modified caption, ensuring it does not retain features from the original sample that are absent in the target caption. We use all the original captions in a batch as negatives for that batch, resulting in the following combined loss for negatives.\n$L_{neg} = L'_{neg} + \\sum_j log S_\\lambda (V_c^i, T_i^j)$\nUsing Eqs. (4, 8) we minimize the following final loss with respect to the parameters of the composition function $f_c$:\n$L = \\alpha_{pos} \\cdot L_{pos} + \\alpha_{neg} \\cdot L_{neg}$\nwhere $\\alpha_{pos}$ and $\\alpha_{neg}$ are positive and negative scaling factors respectively. In summary, the composition function is trained to apply the LLM-generated modification text to the reference image such that the resulting composed representation lies close to the embedding of the modified caption.\n3.4. Inference\nAs shown in Fig. 2, all gallery images for retrieval are encoded with the image encoder $f_v$. During inference, we combine the embeddings of the reference image and user-provided modification text using the learned composition function $f_c$, as in training. This composite representation is used to retrieve the most similar gallery images by computing cosine similarity with their image embeddings."}, {"title": "4. Experiments", "content": "We now turn to quantitative and qualitative evaluations of SCOT for ZS-CIR. Additional results are in the appendix.\n4.1. Datasets\nWe train on three datasets of captioned images: MS-COCO [24] (189K pairs), Flickr30K [41] (45K pairs), and ABO [7] (58K pairs), totaling 290K image-text pairs. Following previous works [2, 25, 31], we assess zero-shot capabilities on FashionIQ [40] and CIRR [26], two compositional retrieval datasets with annotated triplets. Here, FashionIQ assess zero-shot generalizability in the fashion domain and CIRR on open-world retrieval setting.\n4.2. Implementation Details\nEncoders. Unless otherwise stated, we use BLIP-21 as a frozen\u00b2 image and text encoder.\nTextual triplet generation. To generate modification texts m and modified captions u, we use the instruction-tuned Falcon-7B LLM [1]. As directly prompting this model produces noisy and inconsistent generations on our task, we generate 4K text triplets from the better-performing GPT-4 model [29] and use them for LoRA fine-tuning [16] of 4-bit quantized Falcon-7B [1]. Finally, we generate a dataset of over 290K text triplets using the finetuned Falcon-7B, which can be reused in subsequent training runs. SCOT is not reliant on any specific LLM, so newer or stronger models can also be used to refine and expand the triplet dataset.\nOther training details. We train with AdamW [28], batch size 1024 and learning rate 1x10-4. In the loss (Sec. 3.3), we set positive scaling factor $\\alpha_{pos}$ = 10 and negative scaling factor $\\alpha_{neg}$ = 0.1, and margin $\\lambda$ = 0.2. For the Combiner, we use the same hyperparameters as the original work [3]. Training and inference uses 2 NVIDIA A100 GPUs.\n4.3. Comparison with state-of-the-art methods\nEvaluation metrics. We present a quantitative comparison against the state-of-the-art on the FashionIQ [40] and CIRR [26] datasets. The evaluation metric for FashionIQ is the average recall at rank K (R@K). Following prior work [2, 25, 31] we present R@10 and R@50 on the validation set. For CIRR, we follow the authors' proposed protocol to report Recall@K at four different ranks, i.e., K \u2208 {1, 5, 10, 50}, along with Recallsubset@K, which uses small subsets with fully labelled negatives for each query image [26]. We show results for existing zero-shot approaches and fully-supervised approaches.\nBaselines. As reference, we present results of retrieving using just the image embedding (Image-Only), just the modification text embedding (Text-Only), or the sum of the two (Image+Text). For a fair comparison against prior zero-shot methods such as Pic2Word [31] and SEARLE [2], which rely on frozen backbones, we include results from TransAgg [25] with frozen backbones. Baldrati et al. [2] present two variants of their approach: SEARLE-OTI, which requires inference-time optimization, and SEARLE, which trains a textual inversion network to reproduce the OTI outputs in a single forward pass. Here, we use the reported results for SEARLE and its larger version SEARLE-XL. Finally, we note that existing methods use different backbones, amounts and types of data, fusion architectures, and pretraining strategies. For instance, Pic2Word [31] uses 3M images with a frozen CLIP L/14 backbone within a textual inversion-based approach, whereas TransAgg [25] uses 32K synthetic triplets with BLIP and a Transformer-based fusion method. We provide results segregated by backbone in Tables 1 and 2, and further analyze the importance of different contrastively-trained backbones in Section 4.4.\nResults on FashionIQ. From Table 1, the best-performing SCOT model improves by 11.78% on R@10 and by 13.8% on R@20 over SEARLE-XL [2]. SCOT also demonstrates notable data efficiency: utilizing only 290K image-text pairs for training, in contrast to the 3M images used in Pic2Word's training with Conceptual Captions, we achieve 13.75% improvement over Pic2Word on R@10. The zero-shot performance of SCOT exceeds many fully-supervised methods, such as DCNet [19], CLIP4Cir [3], and Fashion-"}, {"title": "4.4. Discussion", "content": "1. Qualitative analysis. In Fig. 4 (Top) we present zero-shot qualitative retrieval results on FashionIQ, illustrating domain-specific behavior. The figure shows that SCOT effectively composes images and text to retrieve the most accurate product image. The second row is particularly interesting: all methods retrieve a gray tank top, but only SCOT specifically retrieves one with the Adidas logo, which was also present in the reference image. We evaluate the qualitative performance on open-world images using CIRR in Fig. 4 (Bottom). As discussed earlier, often in CIRR the modification text can be informative enough to retrieve the correct target image. The learned dynamic scalar scores of the Combiner network are shown in the last column. In cases where the modification text completely describes the target image-such as in the third row-SCOT assigns a high weight to the text representation. In last row, it can be observed that the dog breed can only be inferred through the reference image; consequently SCOT assigns nearly equal weight to both image and text representations.\n2. Impact of image-text alignment backbones. Using text embeddings as a proxy for image embeddings requires the image and text embedding spaces to be well-aligned. Here, we study the behavior of SCOT and other methods as we vary the encoder backbones. To recall, based on previous results [22, 23, 30], the relative ranking of the backbones we experimented with is CLIP-B/32 < CLIP-L/14 < BLIP < BLIP-2. From Table 1, with CLIP B/32, SCOT gets an average R@10 of 24.14% on FashionIQ [40]. With CLIP L/14, we observe 28.27%, nearly 4% higher. With BLIP, we observe another 2% improvement at R@10, while TransAgg produces a drop of 1.6%. Finally, for SCOT with BLIP-2, we see the largest improvement, of 8% over BLIP. On CIRR, as seen in Table 2, when using the CLIP B/32 backbone, SCOT is behind both TransAgg and SEARLE. SCOT then surpasses SEARLE when using the CLIP L/14 backbone, and surpasses TransAgg when switching to the BLIP backbone. Thus, as with FashionIQ, the relative performance of methods changes with different backbones, with SCOT's advantage increasing as the backbones improve. This can be more clearly seen in Fig. 5 where we present the relative gains of different methods with respect to the 'Text-Only' baseline. We define relative gain \u0394R@K as the difference between Recall@K of a given method and that of the 'Text-Only' baseline with the corresponding backbone. On both FashionIQ and CIRR, Fig. 5 shows that as we improve the backbones, the relative gain of SCOT increases. Thus, not only does SCOT benefit from better backbones as represented by the performance of the 'Text-Only' baseline on those backbones, but its gain over that baseline also increases. Of note, with the BLIP backbone, SCOT has relative gains that are 2-3 larger than that of TransAgg with BLIP, showing that SCOT is unique in obtaining higher relative gains with better backbones.\n3. Impact of dataset distribution. In Fig. 6, we illustrate how performance changes as we expand the training set. On both FashionIQ and CIRR, recall increases when utilizing larger subsets of the 189K MSCOCO image-caption pairs. This trend continues with the addition of Flickr30K. While both MSCOCO and Flickr30K contain generic real-world images, we wanted to also evaluate improvements brought by including domain-specific images. The Amazon Berkeley Objects (ABO) [7] dataset contains a variety of retail products, such as phone cases and furniture, accompanied by detailed captions. By including 58K image-caption pairs from ABO, we see around a 1% improvement on FashionIQ's average R@10, going from 37.21% to 38.45%. Specifically for Shirt and Top/Tee, performance improves by around 2% when adding ABO. On CIRR, as shown in Fig. 6(b), incorporating ABO yields only a marginal gain in R@1 and no improvement in Rsubset @1, likely due to the differing image distributions between ABO and CIRR.\n4. Text supervision vs retrieved image supervision. An alternative way of using LLM-generated text triplets for ZS-CIR involves using each of the generated modified captions as a query to retrieve an image from a large corpus. Each retrieved image is then used as target supervision for its corresponding reference image and generated modification text. Concurrently to our work, Liu et al. [25] experimented with this type of approach. Fig. 7 displays examples from the LAION-CIR-LLM dataset they proposed, which is based on image-caption pairs taken from LAION-COCO [33].3 As shown in the figure, the retrieved target images often do not match the expected modified caption due to the absence of a relevant image in the corpus and/or retrieval errors. Table 3 presents an experiment comparing the use of the retrieved image target supervision from LAION-CIR-LLM [25] against text supervision using the dataset's modified captions. The experiment uses BLIP-2 [22] as image and text encoder, and the Combiner [3] as composition function. We see that using retrieved images as targets gives an average R@10 on FashionIQ of 29.02%, whereas using text targets as proposed in our approach achieves 35.17%."}, {"title": "5. Conclusion", "content": "We propose a novel approach towards annotation-free ZS-CIR which leverages existing large captioned image datasets, along with contrastively-pretrained vision-language models. We demonstrate the zero-shot generalizability of this technique through extensive experimentation on domain-specific and open-world datasets. Our proposed approach, SCOT, achieves state-of-the-art performance in zero-shot settings while being on par with various fully-supervised approaches. We further substantiate this work with qualitative and quantitative experiments to analyze the impact of various components of our pretraining strategy."}]}