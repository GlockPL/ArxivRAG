{"title": "Advanced Reasoning and Transformation Engine for Multi-Step\nInsight Synthesis in Data Analytics with Large Language Models", "authors": ["Atin Sakkeer Hussain"], "abstract": "This paper presents the Advanced Reasoning and Trans-\nformation Engine for Multi-Step Insight Synthesis in\nData Analytics (ARTEMIS-DA), a novel framework\ndesigned to augment Large Language Models (LLMs)\nfor solving complex, multi-step data analytics tasks.\nARTEMIS-DA integrates three core components: the\nPlanner, which dissects complex user queries into struc-\ntured, sequential instructions encompassing data prepro-\ncessing, transformation, predictive modeling, and visual-\nization; the Coder, which dynamically generates and ex-\necutes Python code to implement these instructions; and\nthe Grapher, which interprets generated visualizations to\nderive actionable insights. By orchestrating the collabo-\nration between these components, ARTEMIS-DA effec-\ntively manages sophisticated analytical workflows involv-\ning advanced reasoning, multi-step transformations, and\nsynthesis across diverse data modalities. The framework\nachieves state-of-the-art (SOTA) performance on bench-\nmarks such as WikiTableQuestions and TabFact, demon-\nstrating its ability to tackle intricate analytical tasks with\nprecision and adaptability. By combining the reasoning\ncapabilities of LLMs with automated code generation and\nexecution and visual analysis, ARTEMIS-DA offers a ro-\nbust, scalable solution for multi-step insight synthesis, ad-\ndressing a wide range of challenges in data analytics.", "sections": [{"title": "1 Introduction", "content": "The advent of Large Language Models (LLMs), such as\nGPT-3 [1], GPT-4 [15], and Llama 3 [5], has revolution-\nized the fields of artificial intelligence (AI) and natural\nlanguage processing (NLP). These models have demon-\nstrated remarkable capabilities in complex interpretation,\nreasoning, and generating human-like language, achiev-\ning success in diverse applications such as language trans-\nlation, summarization, content generation, and question-\nanswering. Their ability to process and generate coher-\nent text has also made them powerful tools for tasks re-\nquiring nuanced understanding and reasoning. However,\nwhile LLMs have been extensively explored in these do-\nmains, their potential in transforming the field of data an-\nalytics remains underutilized. The inherent reasoning and\ncode-generation capabilities of LLMs suggest immense\npromise for enabling non-technical users to interact with\ncomplex datasets using natural language, bridging the gap\nbetween advanced analytics and accessibility.\nRecent efforts, including TableLLM [12], CABINET\n[18], and Chain-of-Table [22], have begun to explore this\nintersection. These studies focus on table-based ques-\ntion answering and reasoning tasks, leveraging LLMs to\ninterpret and respond to queries grounded in structured\ndatasets. While these works demonstrate the potential of"}, {"title": "2 Related Works", "content": "In recent years, Large Language Models (LLMs) have\ndemonstrated promise in addressing complex tasks in nat-\nural language processing and reasoning. However, when\napplied to structured data, such as large tables, unique\nchallenges arise, requiring specialized frameworks and\nadaptations. Recent studies have made significant strides\nin enhancing LLMs' reasoning capabilities with tabu-\nlar data, providing the foundation for the ARTEMIS-DA\nframework proposed in this paper.\nTabular Reasoning with Pre-trained Language Mod-\nels. Traditional approaches with pre-trained language\nmodels such as TaBERT [27], TAPAS[7], TAPEX[11],\nReasTAP[30], and PASTA[6] were developed to combine\nfree-form questions with structured tabular data. These\nmodels achieved moderate success by integrating table-\nbased and textual training, enhancing tabular reasoning\ncapabilities. However, their ability to generalize under ta-\nble perturbations remains limited[2, 31]. Solutions such\nas LETA[31] and LATTICE[21] addressed these limita-\ntions using data augmentation and order-invariant atten-\ntion. However, they require white-box access to models,\nmaking them incompatible with SOTA black-box LLMs.\nTable-Specific Architectures for LLMs. Table-specific\nmodels further refined the use of structured tabular\ndata, emphasizing row and column positioning such\nas TableFormer[25] which introduced positional embed-\ndings to capture table structure, mitigating the impact of\nstructural perturbations. Despite these advances, frame-\nworks like StructGPT[9] highlighted that generic LLMs\nstill struggle with structured data unless enhanced with\nexplicit symbolic reasoning. Recently proposed frame-\nworks such as AutoGPT[20] and DataCopilot[29] began\naddressing table-specific challenges by incorporating ad-\nvanced reasoning techniques. However, their performance"}, {"title": "3 ARTEMIS-DA Framework", "content": "The Advanced Reasoning and Transformation Engine\nfor Multi-Step Insight Synthesis in Data Analytics\n(ARTEMIS-DA) is designed to address the challenges of\ncomplex data analytics by combining advanced reason-\ning capabilities with dynamic, real-time code generation,\nexecution and visual analysis. The framework, shown in\nFigure 2, consists of three core components: the Plan-\nner, the Coder and the Grapher. Working in unison,\nthese components decompose complex queries into se-\nquential tasks, automatically generate and execute the re-\nquired code for each task, and synthesize insights based\non generated graphs with minimal user intervention.\nFor the experiments in this paper, all three components\nutilize the LLaMA 3 70B model[5], which demonstrated\nsuperior performance across benchmarks. The Grapher\ncomponent also employs the LLaMA 3.2 Vision 90B\nmodel for understanding generated graphs. However, the\nframework is model-agnostic and can be adapted to work\nwith other state-of-the-art large language models (LLMs),\nsuch as GPT-4[15] or Mixtral-8x7B[8].\nThe following sections provide an in-depth exploration\nof the Planner, Coder and Grapher components, detail-\ning their roles and interactions. Together, they exemplify\nARTEMIS-DA's ability to streamline end-to-end data an-\nalytics workflows effectively and efficiently."}, {"title": "3.1 Planner Component", "content": "The Planner serves as the central reasoning and task-\ndecomposition unit within the ARTEMIS-DA framework,\nconverting user queries into structured sequences of tasks.\nIts primary responsibilities include parsing user inputs,\norganizing them into a logical workflow, and interpret-\ning the outputs of executed code and generated visual\ninsights to guide subsequent steps in the analytical pro-\ncess. The Planner is adept at managing complex, multi-\nfaceted queries that require diverse operations, such as\ndata transformation, predictive modeling, and visualiza-\ntion. By leveraging the advanced reasoning capabilities\nof large language models (LLMs), it decomposes intri-\ncate requirements into optimized task sequences aligned\nwith the input data, prior outputs, and specific details of"}, {"title": "3.2 Coder Component", "content": "Following the Planner's generation of a structured task se-\nquence, the Coder translates these tasks into executable\nPython code. It processes instructions for each step-such\nas loading data, creating visualizations, or training predic-\ntive models-producing contextually relevant and func-\ntionally precise code tailored to the task's requirements.\nOperating in real time within a Python environment, the\nCoder executes each code snippet, generating intermedi-\nate outputs that are fed back to the Planner for further\nanalysis and informed decision-making.\nThe Coder's capabilities encompass a broad range of an-\nalytical operations, from basic data cleaning and sum-"}, {"title": "3.3 Grapher Component", "content": "The Grapher serves as a critical component for deriv-\ning actionable insights from visual data, responding to in-\nstructions generated by the Planner. Upon receiving a\ndirective to analyze a generated graph from the Planner,\nthe Grapher processes the visual output and provides in-\nsights in a structured question-and-answer format. Its an-\nalytical capabilities encompass a broad spectrum, ranging\nfrom basic data extraction from plots to advanced inter-\npretation and trend analysis of complex plots.\nFor instance, if the Planner requests an analysis of time-\nseries data, and the Coder generates a corresponding line\nplot, the Grapher can identify trends, detect anomalies,\nand highlight significant observations. This feedback en-\nables the Planner to refine its understanding and produce\nmore nuanced insights. Additionally, the Grapher sup-\nports a wide variety of graph types, including bar charts,\npie charts, scatter plots, and heatmaps, among others, en-\nsuring versatility across diverse data visualization needs.\nBy seamlessly integrating graph interpretation into the\nworkflow, the Grapher enhances the framework's capac-\nity to provide meaningful, data-driven conclusions."}, {"title": "3.4 Workflow and Interaction between Components", "content": "The ARTEMIS-DA framework employs a systematic\nworkflow to process user queries with precision and ef-\nficiency. This process, illustrated in Figure 3, highlights\nthe seamless interaction between the Planner, Coder, and\nGrapher components as outlined below:\n1. Input: The workflow begins when the user submits a\ndataset and a natural language query describing their\nanalytical objectives. The Coder starts by generating\nthe Python code to load the dataset and display the col-\numn types and the first five rows of the dataset.\n2. Decomposition: The Planner analyzes the query and\ndecomposes it into a structured sequence of tasks,\nleveraging outputs from df.info() and df.head()\nto extract actionable context. For instance, a query to\ncompare media categories might involve counting the\nnumber of TV shows and movies, creating a pie chart\nfor visualization, and analyzing the generated chart for\ninsights. Each task is methodically assigned to the\nCoder or Grapher as appropriate."}, {"title": "3.5 Advantages of the ARTEMIS-DA Framework", "content": "The ARTEMIS-DA framework represents a significant\nadvancement in data analytics by integrating sophisti-\ncated natural language understanding with precise com-\nputational execution and visual insight synthesis. Its tri-\ncomponent architecture\u2014comprising the Planner, Coder,"}, {"title": "4 Experiments and Evaluation", "content": "This section provides an overview of the benchmark\ndatasets utilized for evaluation, the metrics employed\nto compare the performance of the models, the results\nachieved by the ARTEMIS-DA framework, and a com-\nprehensive analysis of these results."}, {"title": "4.1 Datasets", "content": "The ARTEMIS-DA framework is evaluated on three\ntable-based reasoning datasets: WikiTableQuestions [17],\nTabFact [3], and FeTaQA [13]. We evaluate ARTEMIS-\nDA exclusively on the test sets of these datasets without\nany training or fine-tuning on the training sets. The details\nof each dataset are as follows:"}, {"title": "4.2 Evaluation Metrics", "content": "To assess the performance of ARTEMIS-DA across the\ndatasets, we employ a range of metrics tailored to the spe-\ncific tasks. For the TabFact [3] dataset, which focuses on\ntable-based fact verification, we utilize binary classifica-\ntion accuracy to evaluate the correctness of statements in\nrelation to the provided tables. In the case of WikiTable-\nQuestions [17], we measure denotation accuracy to deter-\nmine whether the predicted answers align with the ground\ntruth, leveraging the LLaMA 3 70B[5] model for veri-\nfication. Since FeTaQA [13] aims to generate compre-\nhensive, long-form answers rather than short phrases, we\nadopt both BLEU [16] and SacreBLEU [19] as our eval-\nuation metrics. BLEU, a widely used metric in machine\ntranslation, assesses the quality of generated text by com-\nparing it to high-quality reference translations, producing\nscores that range from 0 to 1, with higher values indicating\ngreater similarity. SacreBLEU addresses inconsistencies\ncommonly found in BLEU score reporting by providing\na standardized framework for computation, ensuring that\nresults are shareable, comparable, and reproducible across\nstudies. The use of two distinct metrics for the FeTaQA"}, {"title": "4.3 Results", "content": "Table 1 highlights ARTEMIS-DA's state-of-the-art per-\nformance across the WikiTableQuestions[17], TabFact[3],\nand FeTaQA[13] datasets. These results highlight the\nframework's adaptability and effectiveness in understand-\ning, processing, and reasoning over structured tabular data\nacross diverse domains and use cases.\nIn the WikiTableQuestions[17] dataset, ARTEMIS-DA\nachieves an accuracy of 80.8%, outperforming previous\nmodels such as SynTQA (GPT)[28], CABINET[18], and\nLEVER[14]. This significant improvement of +6.4%\nover the prior best underscores ARTEMIS-DA's superior\ncompositional reasoning abilities, enabling it to effec-\ntively address complex table-based questions that require\nmulti-step logical reasoning.\nSimilarly, for the TabFact[3] dataset, ARTEMIS-DA\nachieves an accuracy of 89.9%, surpassing Mix-SC[12]\nby +1.4%. This result demonstrates ARTEMIS-DA's ef-\nficiency in verifying factual statements against tabular\ndata with high precision, showcasing its strength in fact-\nchecking tasks across diverse data representations.\nFor the FeTaQA [13] dataset, ARTEMIS-DA delivers ex-\nceptional results in generating high-quality long-form an-\nswers. It achieves S-BLEU[19] and BLEU[16] scores\nof 62.7 and 46.4, respectively, representing a sub-\nstantial improvement over previous models, including\nCABINET[18] and Chain-of-Table[22], with gains of\n+22.2 and +13.8, respectively. These results highlight\nARTEMIS-DA's ability to extract, synthesize, and inte-\ngrate discontinuous information from tables, providing\ncoherent and contextually rich answers.\nOverall, ARTEMIS-DA establishes itself as a highly ver-\nsatile and effective solution for table-based reasoning and\ncomputational tasks. Leveraging the LLaMA 3 70B[5]\nand LLaMA 3.2 Vision 90B models, ARTEMIS-DA con-\nsistently delivers state-of-the-art results across multiple\ndatasets, further cementing its role as a transformative tool\nfor compositional reasoning, fact verification, and long-\nform answer generation in diverse domains."}, {"title": "4.4 Ablation Study", "content": "To thoroughly assess the necessity of the multi-step de-\nsign in the ARTEMIS-DA framework for addressing com-\nplex data analytics tasks, we conduct a detailed ablation\nstudy. The Single-Step ARTEMIS-DA variant eliminates\nthe multi-step reasoning capability and instead processes\neach query in a single step. By including this simplified\nvariant, we aim to isolate and highlight the specific con-\ntributions of the multi-step reasoning approach employed"}, {"title": "5 ARTEMIS Capability Demonstration", "content": "This section presents ARTEMIS-DA's capabilities in plot\nvisualization and predictive modeling across a range of\ndatasets, highlighting its versatility and robustness."}, {"title": "5.1 Plot Visualization", "content": "Figure 4 presents a diverse collection of visualizations\ngenerated by ARTEMIS-DA, demonstrating its analyt-\nical capabilities using the Spotify Most Streamed Songs\ndataset from Kaggle. These visualizations address a range\nof analytical questions, showcasing ARTEMIS-DA's abil-\nity to effectively generate, interpret, and extract insights\nfrom various plot types. For instance, the box plot com-\npares streams for tracks released in the last 10 years, iden-\ntifying streaming trends over time. The pie chart analyzes\nthe proportion of tracks released by year over the past\nfive years, highlighting production patterns and identify-\ning peak and low-release years. A heatmap reveals cor-\nrelations among attributes like danceability%, energy%,\nand valence, uncovering relationships between musical\nfeatures. Additionally, the bar plot highlights the top 10\ntracks by total streams, offering a view of popular listener\npreferences, while the scatter plot explores potential cor-\nrelations between energy% and danceability%. The his-\ntogram details the distribution of danceability% across all\ntracks, providing insights into its variability. A line plot\nillustrates the trend of average energy% of tracks over the\nyears, shedding light on changes in the energy levels over\ntime. The violin plot compares the distributions of dance-\nability% and energy%, providing a nuanced understand-\ning of their variability. Lastly, the radar chart contrasts at-\ntributes such as danceability%, valence%, energy%, and\nacousticness% for the top 5 tracks by total streams, of-\nfering a comparison of these popular tracks. Together,\nthese plots underscore ARTEMIS-DA's versatility and ef-\nfectiveness in generating insightful, data-driven visualiza-\ntions from complex datasets."}, {"title": "5.2 Predictive Modeling", "content": "Figure 5 illustrates ARTEMIS-DA's predictive modeling\ncapabilities, emphasizing its effectiveness across diverse\ndatasets and analytical tasks. These examples demon-\nstrate the framework's ability to adapt seamlessly to vari-\nous problem domains, showcasing ARTEMIS-DA as a ro-\nbust solution for a wide range of predictive applications.\n\u2022 Classification of Gender Data: The top-left sub-\nfigure highlights ARTEMIS-DA's ability to handle bi-\nnary classification tasks using the Gender Classification\ndataset. By building a model to predict gender based on\ninput features, ARTEMIS-DA achieves an accuracy of\n96.7%, demonstrating its proficiency in solving classi-\nfication problems and its applicability to tasks such as\ncustomer profiling and demographics analysis.\n\u2022 Clustering of Superhero Powers: The top-right sub-\nfigures demonstrate ARTEMIS-DA's clustering capa-\nbilities using the Superhero Power Analytics Dataset.\nARTEMIS-DA groups superheroes based on their\npower attributes, leveraging dimensionality reduction\nand visualization techniques. This task highlights\nARTEMIS-DA's versatility in unsupervised learning,\nparticularly in applications such as customer segmenta-\ntion, anomaly detection, and clustering-based insights.\n\u2022 Time Series Forecasting on Stock Exchange Data:\nThe middle-left sub-figure demonstrates ARTEMIS-\nDA's strength in time-series forecasting. Utiliz-\ning an advanced Long Short-Term Memory (LSTM)\nmodel, ARTEMIS-DA predicts trends and fluctua-\ntions in stock prices using historical data from the\nStock Exchange Data dataset. This capability high-\nlights ARTEMIS-DA's applicability to predictive finan-\ncial analysis, such as stock market forecasting, eco-\nnomic modeling, and investment strategy development.\n\u2022 Text Classification of BBC Articles: The middle-right\nsub-figures showcase ARTEMIS-DA's application to\nnatural language processing (NLP), specifically in text\nclassification. Using the BBC Document Classification\ndataset, ARTEMIS-DA successfully categorizes news\narticles by topic. This task underscores ARTEMIS-\nDA's ability to process unstructured text data, making\nit an effective tool for applications like content catego-\nrization, sentiment analysis, and automated tagging.\n\u2022 Sentiment Analysis on Disneyland Reviews: The\nbottom-left sub-figure illustrates ARTEMIS-DA's abil-\nity to perform sentiment analysis on text reviews using\nthe Disneyland Reviews dataset. ARTEMIS-DA effec-\ntively identifies the Disneyland location with the highest"}, {"title": "6 Conclusion", "content": "This paper presents the Advanced Reasoning and Trans-\nformation Engine for Multi-Step Insight Synthesis\nin Data Analytics (ARTEMIS-DA), a groundbreak-\ning framework that extends the analytical capabilities"}]}