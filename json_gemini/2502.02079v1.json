{"title": "Online Clustering of Dueling Bandits", "authors": ["Zhiyong Wang", "Jiahang Sun", "Mingze Kong", "Jize Xie", "Qinghua Hu", "John C.S. Lui", "Zhongxiang Dai"], "abstract": "The contextual multi-armed bandit (MAB) is a\nwidely used framework for problems requiring se-\nquential decision-making under uncertainty, such\nas recommendation systems. In applications in-\nvolving a large number of users, the performance\nof contextual MAB can be significantly improved\nby facilitating collaboration among multiple users.\nThis has been achieved by the clustering of ban-\ndits (CB) methods, which adaptively group the\nusers into different clusters and achieve collabo-\nration by allowing the users in the same cluster to\nshare data. However, classical CB algorithms typ-\nically rely on numerical reward feedback, which\nmay not be practical in certain real-world applica-\ntions. For instance, in recommendation systems, it\nis more realistic and reliable to solicit preference\nfeedback between pairs of recommended items\nrather than absolute rewards. To address this lim-\nitation, we introduce the first \"clustering of du-\neling bandit algorithms\" to enable collaborative\ndecision-making based on preference feedback.\nWe propose two novel algorithms: (1) Cluster-\ning of Linear Dueling Bandits (COLDB) which\nmodels the user reward functions as linear func-\ntions of the context vectors, and (2) Clustering of\nNeural Dueling Bandits (CONDB) which uses a\nneural network to model complex, non-linear user\nreward functions. Both algorithms are supported\nby rigorous theoretical analyses, demonstrating\nthat user collaboration leads to improved regret\nbounds. Extensive empirical evaluations on syn-\nthetic and real-world datasets further validate the\neffectiveness of our methods, establishing their\npotential in real-world applications involving mul-\ntiple users with preference-based feedback.", "sections": [{"title": "1. Introduction", "content": "The contextual multi-armed bandit (MAB) is a widely\nused method in real-world applications requiring sequential\ndecision-making under uncertainty, such as recommenda-\ntion systems, computer networks, among others (Li et al.,\n2010). In a contextual MAB problem, a user faces a set\nof K arms (i.e., context vectors) in every round, selects\none of these K arms, and then observes a corresponding\nnumerical reward (Lattimore & Szepesv\u00e1ri, 2020). In or-\nder to select the arms to maximize the cumulative reward\n(or equivalently minimize the cumulative regret), we often\nneed to consider the trade-off between the exploration of\nthe arms whose unknown rewards are associated with large\nuncertainty and exploitation of the available observations\ncollected so far. To carefully handle this trade-off, we often\nmodel the reward function using a surrogate model, such as\na linear model (Chu et al., 2011) or a neural network (Zhou\net al., 2020).\nSome important applications of contextual MAB, such as\nrecommendation systems, often involve a large number (e.g.,\nin the scale of millions) of users, which opens up the pos-\nsibility of further improving the performance of contextual\nMAB via user collaboration. To this end, the method of\nonline Clustering of Bandits (CB) has been proposed, which\nadaptively partitions the users into a number of clusters and\nleverages the collaborative effect of the users in the same\ncluster to achieve improved performance (Gentile et al.,\n2014; Wang et al., 2024a; Li et al., 2019).\nClassical CB algorithms usually require an absolute real-\nvalued numerical reward as feedback for each arm (Wang\net al., 2024a). However, in some crucial applications of con-\ntextual MAB, it is often more realistic and reliable to request\nthe users for preference feedback. For example, in recom-\nmendation systems, it is often preferable to recommend a\npair of items to a user and then ask the user for relative\nfeedback (i.e., which item is preferred) (Yue et al., 2012).\nAs another example, contextual MAB has been successfully\nadopted to optimize the input prompt for large language\nmodels (LLMs), which is often referred to as prompt opti-\nmization (Lin et al., 2024a;b). In this application, instead of\nrequesting an LLM user for a numerical score as feedback,\nit is more practical to show the user a pair of LLM responses\ngenerated by two candidate prompts and ask the user which"}, {"title": "2. Problem Setting", "content": "This section formulates the problem of clustering of duel-\ning bandits. In the following, we use boldface lowercase letters for vectors and boldface uppercase letters for ma-\ntrices. The number of elements in a set A is denoted as\n|A|, while $[m]$ refers to the index set $\\{1, 2, ..., m\\}$, and\n$\\|x\\|_M = \\sqrt{x^T M x}$ represents the matrix norm of vector $x$\nwith respect to the positive semi-definite (PSD) matrix $M$.\nClustering Structure. Consider a scenario with $u$ users,\nindexed by $\\mathcal{U} = \\{1, 2, ..., u\\}$, where each user $i \\in \\mathcal{U}$ is\nassociated with a unknown reward function $f_i: \\mathbb{R}^{d'} \\rightarrow \\mathbb{R}$\nwhich maps an arm $x \\in \\mathcal{X} \\subset \\mathbb{R}^{d'}$ to its corresponding re-\nward value $f_i(x)$. We assume that there exists an underlying,\nyet unknown, clustering structure over the users reflecting\ntheir behavior similarities. Specifically, the set of users $\\mathcal{U}$ is\npartitioned into $m$ clusters $\\mathcal{C}_1, \\mathcal{C}_2,..., \\mathcal{C}_m$, where $m < u$,\nand the clusters are mutually disjoint: $\\bigcup_{j \\in [m]} \\mathcal{C}_j = \\mathcal{U}$ and\n$\\mathcal{C}_j \\cap \\mathcal{C}_{j'} = \\emptyset$ for $j \\neq j'$. These clusters are referred to as\nground-truth clusters, and the set of clusters is denoted by\n$\\mathcal{C} = \\{\\mathcal{C}_1, \\mathcal{C}_2, ..., \\mathcal{C}'_m\\}$. Let $f_j$ denote the common reward\nfunction of all users in cluster $j$ and let $j(i) \\in [m]$ be the\nindex of the cluster to which user $i$ belongs. If two users\n$i$ and $l$ belong to the same cluster, they have the same re-\nward function. That is, for any $l \\in \\mathcal{U}$, if $l \\in \\mathcal{C}_{j(i)}$, then\n$f_l = f_i = f_{j(i)}$. Meanwhile, users from different clusters\nhave distinct reward functions.\nModeling Preference Feedback. At each time step $t \\in [T]$,\na user $i_t \\in \\mathcal{U}$ is served. The learning agent observes a\nset of context vectors (i.e., arms) $\\mathcal{X}_t \\subseteq \\mathcal{X} \\subset \\mathbb{R}^{d'}$, where\n$|\\mathcal{X}_t| = K < \\infty$ for all $t$. Each arm $x \\in \\mathcal{X}_t$ is a feature vector\nin $\\mathbb{R}^{d'}$ with $\\|x\\|_2 \\leq 1$. The agent assigns the cluster $\\mathcal{C}_t$\nto user $i_t$ and recommends two arms $x_{t,1}, x_{t,2} \\in \\mathcal{X}_t$ based\non the aggregated historical data from cluster $\\mathcal{C}_t$. After\nreceiving the recommended pair of arms, the user provides\na binary preference feedback $y_t \\in \\{0,1\\}$, in which $y_t = 1$\nif $x_{t,1}$ is preferred over $x_{t,2}$ and $y_t = 0$ otherwise. We\nmodel the binary preference feedback following the widely\nused Bradley-Terry-Luce (BTL) model (Hunter, 2004; Luce,\n2005). Specifically, the BTL model assumes that for user $i_t$,\nthe probability that the first arm $x_{t,1}$ is preferred over the\nsecond arm $x_{t,2}$ is given by\n$\\begin{equation}\nP_t(x_{t,1} > x_{t,2}) = \\mu(f_{i_t}(x_{t,1}) - f_{i_t}(x_{t,2})),\n\\end{equation}$\nwhere $\\mu : \\mathbb{R} \\rightarrow [0, 1]$ is the logistic function: $\\mu(z) = \\frac{1}{1+e^{-z}}$.\nIn other words, the binary feedback $y_t$ is sampled from the\nBernoulli distribution with the probability $P_t(x_{t,1} > x_{t,2})$.\nWe make the following assumption about the preference\nmodel:\nAssumption 2.1 (Standard Dueling Bandits Assumptions).\n1. $|\\mu(f(x)) - \\mu(g(x))| \\leq L_\\mu |f(x) - g(x)|, \\forall x \\in \\mathcal{X}$, for\nany functions $f, g: \\mathbb{R}^{d'} \\rightarrow \\mathbb{R}$.\n2. $\\min_{x \\in \\mathcal{X}} \\nabla \\mu(f(x)) \\geq \\kappa_\\mu > 0$.\nAssumption 2.1 is the standard assumption in the analysis\nof linear bandits and dueling bandits (Li et al., 2017; Bengs"}, {"title": "2.1. Clustering of Linear Dueling Bandits", "content": "For the linear setting, we assume that each reward function\n$f_i$ is linear in a fixed feature space $\\phi(\\cdot)$, such that $f_i(x) =$\n$\\theta_i^T \\phi(x), \\forall x \\in \\mathcal{X}$. The feature mapping $\\phi : \\mathbb{R}^{d'} \\rightarrow \\mathbb{R}^{d}$\nis a fixed mapping with $\\|\\phi(x)\\|_2 \\leq 1$ for all $x \\in \\mathcal{X}$. In\nthe special case of classical linear dueling bandits, we have\nthat $\\phi(x) = x$, i.e., $\\phi(\\cdot)$ is the identity mapping. The use\nof $\\phi(x)$ enables us to potentially model non-linear reward\nfunctions given an appropriate feature mapping.\nIn this case, the reward function of every user $i$ is repre-\nsented by its corresponding preference vector $\\theta_i$, and all\nusers in the same cluster share the same preference vector\nwhile users from different clusters have distinct preference\nvectors. Denote $\\theta_j$ as the common preference vector of\nusers in cluster $\\mathcal{C}_j$, and let $j(i) \\in [m]$ be the index of the\ncluster to which user $i$ belongs. Therefore, for any $l \\in \\mathcal{U}$, if\n$l \\in \\mathcal{C}_{j(i)}$, then $\\theta_l = \\theta_i = \\theta_{j(i)}$.\nThe following assumptions are made regarding the cluster-\ning structure, users, and items:\nAssumption 2.2 (Cluster Separation). The preference vec-\ntors of users from different clusters are at least separated by\na constant gap $\\gamma > 0$, i.e.,\n$\\begin{equation}\n\\|\\theta_j - \\theta_{j'}\\|_2 \\geq \\gamma, \\text{ for all } j \\neq j' \\in [m].\n\\end{equation}$\nAssumption 2.3 (Uniform User Arrival). At each time step\n$t$, the user $i_t$ is selected uniformly at random from $\\mathcal{U}$, with\nprobability $1/u$, independent of previous rounds.\nAssumption 2.4 (Item regularity). At each time step $t$, the\nfeature vector $\\phi(x)$ of each arm $x \\in \\mathcal{X}_t$ is drawn inde-\npendently from a fixed but unknown distribution $p$ over\n$\\{\\phi(x) \\in \\mathbb{R}^d : \\|\\phi(x)\\|_2 \\leq 1\\}$, where $\\mathbb{E}_{x \\sim p}[\\phi(x)\\phi(x)^T]$ is\nfull rank with minimal eigenvalue $\\lambda > 0$. Additionally, at\nany time $t$, for any fixed unit vector $\\theta \\in \\mathbb{R}^d$, $(\\theta^T\\phi(x))^2$\nhas sub-Gaussian tail with variance upper bounded by $\\sigma^2$.\nRemark 1. All these assumptions above follow the previous\nworks on clustering of bandits (Gentile et al., 2014; 2017;\nLi & Zhang, 2018; Ban & He, 2021b; Liu et al., 2022; Wang\net al., 2024a;b). For Assumption 2.3, our results can easily\ngeneralize to the case where the user arrival follows any\ndistribution with minimum arrival probability $\\geq P_{min}$."}, {"title": "2.2. Clustering of Neural Dueling Bandits", "content": "Here we allow the reward functions $f_i$'s to be non-linear\nfunctions. To estimate the unknown reward functions $f_i$'s,\nwe use fully connected neural networks (NNs) with ReLU\nactivations, and denote the depth and width (of every layer)\nof the NN by $L > 2$ and $m_{NN}$, respectively (Zhou et al.,\n2020; Zhang et al., 2021). Let $h(x; \\theta)$ represent the output\nof an NN with parameters $\\theta$ and input vector $x$, which is\ndefined as follows:\n$\\begin{equation}\nh(x; \\theta) = W_L \\text{ReLU} (W_{L-1} \\text{ReLU} (\\cdots \\text{ReLU} (W_1 x))),\n\\end{equation}$\nin which $\\text{ReLU}(x) = \\max\\{x,0\\}$, $W_1 \\in \\mathbb{R}^{m_{NN} \\times d}$, $W_l \\in\n$\\mathbb{R}^{m_{NN} \\times m_{NN}}$ for $2 \\leq l < L$, $W_L \\in \\mathbb{R}^{1 \\times m_{NN}}$. We denote\nthe parameters of NN by $\\theta = (\\text{vec} (W_1);...;\\text{vec} (W_L))$,\nwhere $\\text{vec}(A)$ converts an $M \\times N$ matrix $A$ into a $MN$-\ndimensional vector. We use $p$ to denote the total number of\nNN parameters: $p = dm_{NN} + m_{NN}^2 (L - 1) + m_{NN}$, and use\n$g(x; \\theta)$ to denote the gradient of $h(x; \\theta)$ with respect to $\\theta$.\nThe algorithmic design and analysis of neural bandit algo-\nrithms make use of the theory of the neural tangent kernel\n(NTK) (Jacot et al., 2018). We let all $u$ users use the same\ninitial NN parameters $\\theta_0$, and assume that the value of\nthe empircal NTK is bounded: $\\langle g(x; \\theta_0), g(x; \\theta_0) \\rangle \\leq\n1, \\forall x \\in \\mathcal{X}$. This is a commonly adopted assumption in\nthe analysis of neural bandits (Dai et al., 2023; Kassraie &\nKrause, 2022). Let $T^j$ denote total number of rounds in\nwhich the users in cluster $j$ is served. We use $H_j$ to denote\nthe NTK matrix (Zhou et al., 2020) for cluster $j$, which is\na $(T_j K) \\times (T_j K)$-dimensional matrix. Similarly, we de-\nfine $h_j$ as the $(T_j K) \\times 1$-dimensional vector containing the\nreward function values of all $T_j K$ arm feature vectors for\ncluster $j$. We provide the concrete definitions of $H_j$ and\n$h_j$ in App. C.1. We make the following assumptions which\nare commonly adopted by previous works on neural bandits\n(Zhou et al., 2020; Zhang et al., 2021), for which we provide\njustifications in App. C.1.\nAssumption 2.5. The reward functions for all users are\nbounded: $|f_i(x)| \\leq 1, \\forall x \\in \\mathcal{X}, \\forall i \\in \\mathcal{U}$. There exists\n$\\lambda_0 > 0$ s.t. $H_j \\geq \\lambda_0 I, \\forall j \\in \\mathcal{C}$. All arm feature vectors\nsatisfy $\\|x\\|_2 = 1$ and $x_j = x_{j+d/2}, \\forall x \\in \\mathcal{X}_t, \\forall t \\in [T]$.\nDenote by $f_j$ the common reward function of the users\nin cluster $\\mathcal{C}_j$, and let $j(i) \\in [m]$ be the index of the\ncluster to which user $i$ belongs. Same as Sec. 2.1, here\nall users in the same cluster share the same reawrd func-\ntion. Therefore, for any $l \\in \\mathcal{U}$, if $l \\in \\mathcal{C}_{j(i)}$, then\n$f_l(x) = f_i(x) = f_{j(i)}(x), \\forall x \\in \\mathcal{X}$. The following lemma\nshows that when the NN is wide enough (i.e., $m_{NN}$ is large),\nthe reward function of every cluster can be modeled by a\nlinear function.\nLemma 2.6 (Lemma B.3 of (Zhang et al., 2021)). As long\nas the width $m_{NN}$ of the NN is large enough: $m_{NN} >$"}, {"title": "3. Algorithms", "content": "Our Clustering Of Linear Dueling Bandits (COLDB) algo-\nrithm is described in Algorithm 1. Here we elucidate the\nunderlying principles and operational workflow of COLDB.\nCOLDB maintains a dynamic graph $G_t = (\\mathcal{U}, E_t)$ encom-\ncompassing all users, whose connected components represent\nthe inferred user clusters in round $t$. Throughout the learning\nprocess, COLDB adaptively removes edges to accurately\ncluster the users based on their estimated reward function\nparameters, thereby leveraging these clusters to enhance on-\nline learning efficiency. The operation of COLDB proceeds\nas follows:\nCluster Inference $\\mathcal{C}_t$ for User $i_t$ (Line 2-Line 5). Ini-\ntially, COLDB constructs a complete undirected graph\n$G_0 = (\\mathcal{U}, E_0)$ over the user set (Line 2). As learning pro-\ngresses, edges are selectively removed to ensure that only\nusers with similar preference profiles remain connected. At\neach round $t$, when a user $i_t$ comes to the system with a fea-\nsible arm set $\\mathcal{X}_t$ (Line 4), COLDB identifies the connected\ncomponent $\\mathcal{C}_t$ containing $i_t$ in the maintained graph $G_{t-1}$,\nwhich serves as the current estimated cluster for this user\n(Line 5).\nEstimating Shared Statistics for Cluster $\\mathcal{C}_t$ (Line 6-Line\n7). Once the cluster $\\mathcal{C}_t$ is identified, COLDB estimates a\ncommon preference vector $\\theta_t$ for all users within this cluster\nby aggregating the historical feedback from all members of\n$\\mathcal{C}_t$. Specifically, in Line 6, the common preference vector\nis determined by minimizing the following loss function:\n$\\begin{equation}\n\\theta_t = \\text{arg} \\min_\\theta -\\frac{1}{m} \\sum_{\\substack{s \\in [t-1]: \\\\\ni_s \\in \\mathcal{C}_t}} (y_s \\log \\mu (\\theta^T [\\phi(x_{s,1}) - \\phi(x_{s,2})]) \\\n+ (1 - y_s) \\log \\mu (\\theta^T [\\phi(x_{s,2}) - \\phi(x_{s,1})])) + \\frac{\\lambda}{2} \\|\\theta\\|_2^2,\n\\end{equation}$\nwhich corresponds to the Maximum Likelihood Estima-\ntion (MLE) using the data from all users in the cluster $\\mathcal{C}_t$.\nAdditionally, in Line 7, COLDB computes the aggregated\ninformation matrix for $\\mathcal{C}_t$, which is subsequently utilized\nin selecting the second arm $x_{t,2}$:\n$\\begin{equation}\nV_{t-1} = V_0 + \\sum_{\\substack{s \\in [t-1]: \\\\\ni_s \\in \\mathcal{C}_t}} (\\phi(x_{s,1}) - \\phi(x_{s,2}))(\\phi(x_{s,1}) - \\phi(x_{s,2}))^T,\n\\end{equation}$\nArm Recommendation Based on Cluster Statistics (Line\n8-Line 9). Leveraging the estimated common preference\nvector $\\theta_t$ and the aggregated information matrix $V_{t-1}$,\nCOLDB proceeds to recommend two arms as follows:\n$\\bullet$ First Arm Selection ($x_{t,1}$). In Line 8, COLDB selects\nthe first arm by greedily choosing the arm that maximizes\nthe estimated reward according to $\\theta_t$:\n$\\begin{equation}\nx_{t,1} = \\text{arg} \\max_{x \\in \\mathcal{X}_t} \\theta_t^T \\phi(x).\n\\end{equation}$\n$\\bullet$ Second Arm Selection ($x_{t,2}$). Following the selection\nof $x_{t.1}$, in Line 9, COLDB selects the second arm by\nmaximizing an upper confidence bound (UCB):\n$\\begin{equation}\nx_{t,2} = \\text{arg} \\max_{x \\in \\mathcal{X}_t} \\theta_t^T \\phi(x) + \\frac{\\beta_t}{\\kappa_\\mu} [\\|\\phi(x) - \\phi(x_{t,1})\\|\\bigtriangleup_{V_{t-1}}].\n\\end{equation}$\nIntuitively, Eq.(4) encourages the selection of the arm\nwhich both (a) has a large predicted reward value and\n(b) is different from $x_{t,1}$ and the arms selected in the\nprevious $t-1$ rounds when the served user belongs to\nthe currently estimated cluster $\\mathcal{C}_t$. In other words, the\nsecond arm $x_{t,2}$ is chosen by balancing exploration and\nexploitation.\nUpdating User Estimates and Interaction History (Line\n10-Line 11). Upon recommending $x_{t,1}$ and $x_{t,2}$, the\nuser receives binary feedback $y_t = \\mathbb{I}(x_{t,1} > x_{t,2})$ from\nuser $i_t$, and then updates the interaction history $\\mathcal{D}_t =$\n$\\{\\langle i_s, x_{s,1}, x_{s,2}, y_s \\rangle\\}_{s=1}^t$ (Line 10). Moreover, COLDB up-\ndates the preference vector estimate for user $i_t$ while keep-\ning the estimates for the other users unchanged (Line 11)."}, {"title": "3.2. Clustering Of Neural Dueling Bandits (CONDB)", "content": "Our Clustering Of Neural Dueling Bandits (CONDB) algo-\nrithm is illustrated in Algorithm 2 (App. A), which adopts\nneural networks to model non-linear reward functions. Sim-\nilar to COLDB, our CONDB algorithm also maintains a\ndynamic graph $G_t = (\\mathcal{U}, E_t)$ in which every connected\ncomponent denotes an inferred cluster, and adaptively re-\nmoves the edges between users who are estimated to belong\nto different clusters.\nCluster Inference $\\mathcal{C}_t$ for User $i_t$ (Line 5). Similar to\nCOLDB (Algo. 1), when a new user $i_t$ arrives, our CONDB\nfirstly identifies the connected component $\\mathcal{C}_t$ in the main-\ntained graph $G_{t-1}$ which contains the user $i_t$ and then uses\nit as the estimated cluster for $i_t$ (Line 5).\nEstimating Shared Statistics for Cluster $\\mathcal{C}_t$ (Line 6). Af-\nter the cluster $\\mathcal{C}_t$ is identified, our CONDB algorithm uses\nthe history of preference feedback observations from all\nusers in the cluster $\\mathcal{C}_t$ to train a neural network (NN) to\nminimize the following loss function (Line 6):\n$\\begin{equation}\n\\mathcal{L}_t(\\theta) = -\\frac{1}{m} \\sum_{\\substack{s \\in [t-1]: \\\\\ni_s \\in \\mathcal{C}_t}} (y_s \\log \\mu (h(x_{s,1}; \\theta) - h(x_{s,2}; \\theta)) + \\\n(1 - y_s) \\log \\mu (h(x_{s,2}; \\theta) - h(x_{s,1}; \\theta))) + \\frac{\\lambda}{2} \\|\\theta - \\theta_0\\|_2^2\n\\end{equation}$\nto yield parameters $\\theta_t$. In addition, similar to COLDB\n(Algorithm 1), our CONDB computes the aggregated infor-\nmation matrix for the cluster $\\mathcal{C}_t$ following Eq.(2). Note that\nhere we replace $\\phi(x)$ from Eq.(2) by the NTK feature repre-\nsentation $\\phi(x) = (1/\\sqrt{m_{NN}}) g(x; \\theta_0)$, in which $\\theta_0$ represents\nthe initial parameters of the NN (Sec. 2.2).\nArm Recommendation Based on Cluster Statistics (Line\n8-Line 9). Next, our CONDB algorithm leverages the"}, {"title": "4. Theoretical Analysis", "content": "In this section, we present the theoretical results regarding\nthe regret guarantees of our proposed algorithms and provide\na detailed discussion of these findings.\n4.1. Clustering Of Linear Dueling Bandits (COLDB)\nThe following theorem provides an upper bound on the\nexpected regret achieved by the COLDB algorithm (Algo. 1)\nunder the linear setting.\nTheorem 4.1. Suppose that Assumptions 2.1, 2.2, 2.3 and\n2.4 are satisfied. Then the expected regret of the COLDB"}, {"title": "5. Experimental Results", "content": "We use both synthetic and real-world experiments to evalu-\nate the performance of our COLDB and CONDB algorithms.\nFor both algorithms, we compare them with their corre-\nsponding single-user variant as the baseline. Specifically,\nfor COLDB, we compare it with the baseline of LDB_IND,\nwhich refers to Linear Dueling Bandit (Independent) (Bengs\net al., 2022), meaning running independent classic linear\ndueling bandit algorithms for each user separately; simi-\nlarly, for CONDB, we compare it with NDB_IND, which\nstands for Neural Dueling Bandit (Independent) (Verma\net al., 2024).\nCOLDB. Our experimental settings mostly follow the\ndesigns from the works on clustering of bandits (Wang et al.,\n2024a; Li et al., 2019). In our synthetic experiment for\nCOLDB, we design a setting with linear reward functions:\n$f_i(x) = \\theta_i^T x$. We choose $u = 200$ users, $K = 20$ arms and\na feature dimension of $d = 20$, and construct two settings\nwith $m = 2$ and $m = 5$ groundtruth clusters, respectively.\nIn the experiment with the MovieLens dataset (Harper &\nKonstan, 2015), we follow the experimental setting from\nWang et al. (2024a), a setting with 200 users. Same as\nthe synthetic experiment, we choose the number of arms\nin every round to be $K = 20$ and let the input feature\ndimension be $d = 20$. We construct a setting with $m = 5\nclusters. We repeat each experiment for three independent\ntrials and report the mean $\\pm$ standard error.\nFig. 1 plots the cumulative regret of our COLDB and the\nbaseline of LDB_IND. The results show that our COLDB al-\ngorithm significantly outperforms the baseline of LDB_IND\nin both the synthetic and real-world experiments. Moreover,\nFig. 1 (a) demonstrates that when $m = 2$ (i.e., when a larger\nnumber of users belong to the same cluster on average), the\nperformance of our COLDB is improved, which is consisent\nwith our theoretical results (Sec. 4.1).\nCONDB. We also construct both a synthetic and real-\nworld experiment to evaluate our CONDB algorithm. Most\nof the experimental settings are the same as those of the"}, {"title": "6. Related Work", "content": "Our work is closely related to: online clustering of bandits\n(CB), dueling bandits, and neural bandits.\n6.1. Clustering of Bandits\nThe concept of clustering bandits (CB) was first introduced\nin (Gentile et al., 2014), where a graph-based approach\nwas proposed for solving the problem. In subsequent work,\n(Li et al., 2016) explored the incorporation of collaborative\neffects among items to aid in the clustering of users. Fur-\nther extending this idea, (Li & Zhang, 2018) tackled the CB\nproblem in the context of cascading bandits, where feedback\nis provided through random prefixes. Another direction of\nthis research, presented in (Li et al., 2019), investigates the\nscenario where users have varying arrival frequencies. In\n(Liu et al., 2022), a federated setting for CB is proposed,\nwhich addresses both privacy concerns and the communica-\ntion overhead in distributed environments. More recently,\ntwo papers by (Wang et al., 2024a) and (Wang et al., 2024b)\nexamine the design of robust CB algorithms in the presence\nof model mis-specifications and adversarial data corruptions,\nrespectively.\nAll these works in CB assume the agent recommends a sin-\ngle arm per round, with a real-valued reward reflecting user\nsatisfaction. However, this does not apply to scenarios such\nas large language models seeking user preference feedback"}, {"title": "6.2. Dueling Bandits and Neural Bandits", "content": "Dueling bandits has been receiving growing attention over\nthe years since its introduction (Yue & Joachims, 2009;\n2011; Yue et al., 2012) due to the prevelance of preference\nor relative feedback in real-world applications. Many earlier\nworks on dueling bandits have focused on MAB problems\nwith a finte number of arms (Zoghi et al., 2014b; Ailon\net al., 2014; Zoghi et al., 2014a; Komiyama et al., 2015;\nGajane et al., 2015; Saha & Gopalan, 2018; 2019a;b; Saha\n& Ghoshal, 2022; Zhu et al., 2023). More recently, con-\ntextual dueing bandits, which model the reward function\nusing a parametric function of the features of the arms, have\nattracted considerable attention (Saha, 2021; Saha & Krish-\nnamurthy, 2022; Bengs et al., 2022; Di et al., 2023; Li et al.,\n2024; Verma et al., 2024).\nTo apply MABs to complicated real-world applications with\nnon-linear reward functions, neural bandits have been pro-\nposed which use a neural network to model the reward\nfunction (Zhou et al., 2020; Zhang et al., 2021). Recently,\nwe have witnessed a significant growing interest in further\nimproving the theoretical and empirical performance of neu-\nral bandits and applying it to solve real-world problems\n(Xu et al., 2020; Kassraie & Krause, 2022; Gu et al., 2021;\nNabati et al., 2021; Lisicki et al., 2021; Ban et al., 2022; Ban\n& He, 2021a; Jia et al., 2021; Nguyen-Tang et al., 2022; Zhu\net al., 2021; Kassraie et al., 2022; Salgia et al., 2022; Dai\net al., 2022; Hwang et al., 2023; Qi et al., 2023; 2024). In\nparticular, the work of Ban et al. (2024) has adopted a neural\nnetwork as a meta-learner for adapting to users in different\nclusters within the framework of clustering of bandits, and\nthe work of Verma et al. (2024) has combined neural bandits\nwith dueling bandits."}, {"title": "7. Conclusion", "content": "In this work, we introduce the first clustering of dueling\nbandit"}]}