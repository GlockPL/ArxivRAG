{"title": "Towards Aligned Data Removal via Twin Machine Unlearning", "authors": ["Yuyao Sun", "Zhenxing Niu", "Gang hua", "Rong jin"], "abstract": "Modern privacy regulations have spurred the evolution of machine unlearning, a technique that enables the removal of data from an already trained ML model without requiring retraining from scratch. Previous unlearning methods tend to induce the model to achieve lowest classification accuracy on the removal data. Nonetheless, the authentic objective of machine unlearning is to align the unlearned model with the gold model, i.e., achieving the same classification accuracy as the gold model. For this purpose, we present a Twin Machine Unlearning (TMU) approach, where a twin unlearning problem is defined corresponding to the original unlearning problem. As a results, the generalization-label predictor trained on the twin problem can be transferred to the original problem, facilitating aligned data removal. Comprehensive empirical experiments illustrate that our approach significantly enhances the alignment between the unlearned model and the gold model. Meanwhile, our method allows data removal without compromising the model's accuracy.", "sections": [{"title": "Introduction", "content": "ML model providers tend to collect extensive data from the Internet and utilize it to train their machine learning models. The recent introduction of data privacy and protection regulations (European Union's GDPR (Regulation 2018), and California Consumer Privacy Act (CCPA) (Goldman 2020)) obligate these model providers to comply with the request-to-delete (Dang 2021) from the data owner. For example, a corporation offering facial recognition services might acquire facial images from the Internet to train their facial recognition models. Subsequently, a user might discover that the company has utilized his/her facial images in model training. In such a scenario, the user reserves the right to petition the company to revise the facial recognition model to forget his/her facial image data.\nThe straightforward solution is to entirely discard the trained model, delete his/her facial images from the training data, and re-train a new model (i.e., called gold model). Unfortunately, training from scratch is expensive and time-consuming. Therefore, machine unlearning (Garg, Goldwasser, and Vasudevan 2020; Ginart et al. 2019; Cao and Yang 2015; Gupta et al. 2021; Sekhari et al. 2021; Brophy and Lowd 2021; Ullah et al. 2021) has garnered considerable attention, aiming to efficiently revise a trained model to forget a cohort of training data, without affecting the performance on the remaining data.\nIn most previous unlearning methods (Graves, Nagisetty, and Ganesh 2021; Tarun et al. 2023), the 'removal' of a cohort data $D_f$ is commonly implemented by decreasing the classification accuracy on $D_f$, i.e., pursuing $ACC_{D_f} = 0$. This implementation works well when deleting an entire class $k$ (namely the forgetting class), where we have $D_f = D_k$. However, it is not suitable when the task is to forget only a subset of class $k$, i.e., $D_f \\subset D_k$. In this case, even though $D_f$ are not involved in the training of the gold model, some samples may still be correctly classified by the gold model, resulting in $ACC_{D_f} \\neq 0$. It is because the remaining samples from class $k$ (i.e., samples $\\in (D_k \u2013 D_f)$) are still involved in the model training, hence the gold model has the generalization ability to recognize class $k$.\nOn the other hand, the actual objective of machine unlearning is to obtain an unlearned model that is well-aligned with the gold model, i.e., requiring the unlearned model to possess the same classification accuracy $ACC_{D_f}$ as the gold model (Golatkar, Achille, and Soatto 2020b). Obviously, it is not appropriate to pursue $ACC_{D_f} = 0$.\nIn this paper, we denote the samples within $D_f$ that can be correctly classified by the gold model as the 'easy' samples. Conversely, the samples that cannot be correctly classified are referred to as the 'hard' samples. As these easy/hard labels for $D_f$ are defined based on the model's generalization ability, we refer to them as generalization-labels. Therefore, a well-aligned unlearning algorithm should specifically decrease the classification accuracy on hard (rather than all) samples in $D_f$. However, as the gold model is unknown, obtaining these generalization-labels becomes challenging. This is the reason why previous unlearning methods have to adopt reducing classification accuracy as the objective of data removal.\nIn this paper, our focus is on enhancing the alignment between the unlearned model and the gold model. To achieve this, we propose to train a binary classifier to predict the generalization-labels for samples in $D_f$. With these predicted labels, $D_f$ is partitioned into an easy subset $D_e$ and a hard subset $D_r$. Subsequently, we specifically decrease the classification accuracy on $D_r$, while retaining the classification accuracy on $D_e$. It is worth noting that our approach is compatible with previous unlearning mechanisms, such as Negative Gradient (Golatkar, Achille, and Soatto 2020a) or Random Labeling (Graves, Nagisetty, and Ganesh 2021), as we can employ them to decrease the classification accuracy on $D_r$. The most challenging aspect of our approach lies in training such a binary classifier on a specific dataset, which involves two sub-challenges:\n\u2022 Build a specific labeled dataset for training the binary classifier\n\u2022 Construct a discriminative representation for the binary classifier\nTo address the first challenge, we formulate a twin unlearning problem corresponding to the original unlearning problem. Specifically, the original model $M_o$ is fine-tuned with $D_{test}$ to generate the twin model $M_t$. As shown in Fig. 1, In the context of $M_t$, the original model $M_o$ can be seen as analogous to $M_t$'s gold model (i.e., $M_o = M_g$), and $D_{test}$ can be seen as analogous to $M_t$'s forgetting data. Hence, we can define a new unlearning problem, namely the twin unlearning problem: given $M_t$, the task is to forget $D_{test}$ and obtain an unlearned model aligned with its gold model $M_g = M_o$.\nDue to the Independent and Identically Distribution (I.I.D) assumption among $D_r$, $D_f$, and $D_{test}$, the two unlearning problems should exhibit similar properties (e.g., similar model generalization ability). Therefore, the binary classifier trained on the twin problem can be transferred to the original unlearning problem. The reason for formulating such a twin unlearning problem is that we can easily obtain a labeled dataset - the dataset $D_{test}$, which generalization-labels are easy to known, i.e., the classification results from the model $M_o$ (the gold model of the twin unlearning problem).\nRegarding the construction of discriminative representation, we propose to integrate three complementary features, each possessing strong discriminability to distinguish between easy and hard samples. (1) The first feature is the Nearest-distance Feature (NF). For each sample $x$ in $D_f$, we identify its nearest neighbor in $D_r$, and the distance between them is considered the nearest-distance feature. Intuitively, if $x$ is an easy sample, it typically has similar samples in $D_r$, resulting in a small distance to its nearest neighbor. In contrast, the nearest-distance feature for a hard sample tends to be large. (2) The second feature is the Adversarial-attack Feature (AF). We construct this feature by employing adversarial attacks(Madry et al. 2017; Goodfellow, Shlens, and Szegedy 2014; Moosavi-Dezfooli et al. 2017), which aim to fool a trained model into making incorrect predictions with small adversarial perturbations. Clearly, hard samples are more vulnerable to adversarial attacks compared to easy samples due to their proximity to the decision boundary. Hence, the results of adversarial attack can be considered a discriminative feature.(3) The third feature is the Curriculum-learning-loss Feature (CF). Curriculum learning theory (Kumar, Packer, and Koller 2010; Tullis and Benjamin 2011; Bengio et al. 2009) suggests that easy samples are learned earlier than hard samples. Therefore, the loss value of the first fine-tuning epoch is considered a feature to distinguish between easy and hard samples."}, {"title": "2 Related Work", "content": "Machine unlearning methods can be broadly classified into retraining-based and parameter-update-based methods. For the first category, training a DNN from scratch using the remaining data can yield the optimal solution to the forgetting problem. However, this approach requires a significant amount of time and computational resources.\nFor the second category, it aims to achieve data removal by directly modifying the parameters of the model. (Graves, Nagisetty, and Ganesh 2021) introduced a method where gradients of every batch are saved during the training phase. To achieve forgetting, the gradients corresponding to the forgotten data are subtracted from the accumulated gradients. (Golatkar, Achille, and Soatto 2020a) propose Fisher Forgetting which use Fisher information (Martens 2020) of the remaining dataset to remove the information about the forgetting data from the network by applying noise to the parameters of the DNN model. Afterward, (Golatkar, Achille, and Soatto 2020b) propose NTK (Jacot, Gabriel, and Hongler 2018) based forgetting, which transforms the trained DNN into a linearized version and adds the noise to the model. However, these methods require a significant amount of computational and storage resources. Additionally, they may lead to catastrophic forgetting of the model. Recently, (Tarun et al. 2023; Rissanen 1996) proposed a novel machine unlearning framework with error-maximizing noise generation and weight manipulation. (Chundawat et al. 2023) proposed a novel student-teacher framework for machine unlearning. For the forget set, the student model learns knowledge from a randomly initialized model, while for the remain set, the student model learns from the origin model."}, {"title": "3 Problem of Data Removal", "content": "Let $D = \\{(x_i, y_i)\\}_{i=1}^n$ be a dataset of images $x_i$, each with a label $y_i \\in \\{1,... K\\}$ representing a class. The original model $M_o(\\theta)$ is trained with the $D$, which could be a DNN with parameters $\\theta$.\nLet $D_f$ be a subset of the $D$, whose information we want to remove/forget from the trained model $M_o(\\theta)$. The $D_f$ is called the forgetting data, which could be all or some of the data with a given label $k$, i.e., corresponding to forgetting an entire class or a subset of a class, respectively. In this paper, we are particularly interested in the case of forgetting a subset of a class, as the alignment problem becomes more critical in this scenario. The remaining data is denoted by $D_r$, whose information is desired to be kept unchanged in the model. $D_f$ and $D_r$ together represent the entire training set $D$ and are mutually exclusive, i.e., $D_r \\cup D_f = D$ and $D_r \\cap D_f = \\emptyset$.\nSince $D_f$ has been used to train the $M_o(\\theta)$, the model parameters will contain information about $D_f$. The task of data removal aims to remove/forget the information of $D_f$ from the trained model $M_o(\\theta)$. The ideal solution is to train a new model from scratch with $D_r$, which is denoted as the gold model $M_g (\\theta)$. However, obtaining the gold model is time-consuming due to the expensive re-training procedure. Thus, the practical data removal aims to efficiently revise the original model $\\theta$ with a 'scrubbing/forgetting' function $s()$, so that the revised model $s(\\theta)$ is as close to the gold model $\\theta_r$ as possible. The $s()$ is often implemented by a machine unlearning algorithm. Obtaining the revised/unlearned model $s(\\theta)$ should be lightweight and much cheaper than obtaining the gold model $\\theta_r$, i.e., machine unlearning aims to achieve data removal efficiently.\nLet us formulate the machine unlearning mathematically. Since DNNs are typically trained with a stochastic algorithm, such as stochastic gradient descent (SGD), thus the outputs of algorithm (i.e., model weights) should be described by a distribution $P()$. Let $P(\\theta|D)$ denote the distribution of weights given the training data $D$. Thus, the objective of data removal can be formulated as follows,\n$\\min KL(P(s(\\theta)|D)||P(\\theta_r|D_r))$ (1)\nwhere $P(\\theta_r|D_r)$ represents the distribution of weights for the gold model, and $P(s(\\theta)|D)$ represents the distribution of weights for the unlearned model. The Kullback-Leibler (KL) divergence implies the alignment between the unlearned model and the gold model."}, {"title": "3.1 Alignment in Data Removal", "content": "The model alignment in terms of KL divergence indicates that the unlearned model has the same behaviors as the gold model. In practice, the KL divergence is often replaced by some metrics (called readout functions (Golatkar et al. 2021)), such as: (i) accuracy on the test set $D_{test}$, i.e., $ACC_{D_{test}}$; (ii) accuracy on the forgetting data $D_f$, i.e., $ACC_{D_f}$; (iii) accuracy on the remaining data $D_r$, i.e., $ACC_{D_r}$.\nAchieving alignment on $ACC_{D_{test}}$ and $ACC_{D_r}$ is straightforward, as the target is to increase them as much as possible. In contrast, the challenge lies in aligning the $ACC_{D_f}$, i.e., the unlearned model is desired to have the same classification accuracy $ACC_{D_f}$ as the gold model. Since the gold model is unknown, obtaining its $ACC_{D_f}$ is impossible, let alone achieving accuracy alignment.\nAs a result, previous unlearning methods tend to replace this objective with a surrogate objective, i.e., decreasing the accuracy $ACC_{D_f}$. For the case of forgetting an entire class, this approximating implementation is acceptable since, after removing an entire forgetting class, the gold model tends to have $ACC_{D_f} = 0$, aligning with the surrogate optimization objective. However, in the case of forgetting a subset of the forgetting class, the remaining data will still contain samples of the forgetting class. Due to the generalization ability of ML models, the gold model may correctly classify some samples in $D_f$, i.e., $ACC_{D_f} \\neq 0$. Thus, we have to carefully consider the alignment challenges.\nFurthermore, as highlighted by (Golatkar, Achille, and Soatto 2020a), approximating data removal by merely decreasing $ACC_{D_f}$ may give rise to other issues such as information exposure. For instance, it could lead to the Streisand effect. This effect describes unexpected model behavior on forget samples, potentially leaking information about that data. If the samples in $D_f$ are consistently misclassified, it might raise suspicious. Therefore, it is crucial to emphasize the alignment in data removal."}, {"title": "4 Our Approach", "content": "In this paper, we introduce a Twin Machine Unlearning (TMU) approach, where a twin unlearning problem is defined corresponding to the original unlearning problem. As a results, the generalization-label predictor trained on the twin problem can be transferred back to the original problem to facilitate aligned data removal."}, {"title": "4.1 Twin Unlearning Problem", "content": "Before addressing the alignment challenges, we first construct a new unlearning problem corresponding to the original one. As shown in the left part of Fig.2, for the original unlearning problem, we have an original model $M_o$ that is trained with $D_r + D_f$. Our task is to get an unlearned model $M_u$ from $M_o$, so that $M_u$ is well aligned with the gold model $M_g$. The $M_g$ is defined as a model trained from scratch with only $D_r$.\nAs shown in the right part of Fig.2, if we fine-tune the $M_o$ with another dataset $D_{test}$, we obtain a new model, namely the twin model $M_t$ in this paper. Thus, we can define the twin unlearning problem: we regard the $M_t$ as the 'original model' for the twin problem, which is trained with $D_r + D_f + D_{test}$. We regard the $D_{test}$ as the 'forgetting data' for the twin problem. Obviously, the twin problem's 'gold model' $M_g^t$ is known, which is exactly the original problem's original model $M_o$,\n$M_g^t = M_o$ (2)\nRegarding the constructed twin unlearning problem, since its gold model $M_g^t$ is known, we can easily obtain the ground-truth generalization-labels for its forgetting data $D_{test}$, i.e., run the inference of the model $M_g^t$ over the $D_{test}$ to obtaining the classification results. So far, we have build a specific labeled dataset. We can train a binary classifier $f()$ to distinguish between hard and easy samples on $D_{test}$ for the twin problem.\nIt is usually to assume that $D_{test}$ and $D_f$ have independent and identical distribution (I.I.D). Thus, the binary classifier $f()$ can be transferred from the twin problem back to the original unlearning problem for predicting the generalization-labels on $D_f$. In this way, we can effectively partition $D_f$ into a hard subset $D_h$ and an easy subset $D_e$. Finally, when solving the original unlearning problem, we can optimize to reduce classification accuracy on $D_h$ while retaining the accuracy on $D_e$, as shown in Fig.2."}, {"title": "4.2 Constructing Twin Model", "content": "We construct the twin model $M_t$ based on the original model $M_o$. Particularly, our $M_t$ is created by fine-tuning $M_o$ with $D_{test}$. For a real-world application of data removal, the amount of data to be forgotten is determined by users, i.e., $D_f$ could vary from one image to all images in the forgetting class. This implies that our approach should be able to handle varying sizes of $D_f$. Obviously, the size of $D_f$ will influence the generalization ability of the gold model. For instance, if more data is forgotten, the generalization will degrade noticeably. This crucial factor can be described by the ratio between the size of forgetting data $D_f$ and the size of remaining data $D_r$, i.e., $R_o = \\frac{|D_f|}{|D_r|}$.\nTo ensure the analogy between the twin problem and the original problem, we must maintain this forgetting ratio. This can be achieved by controlling the number of samples in $D_{test}$ involved in fine-tuning. Specifically, we need to ensure that the forgetting ratio in the twin problem $R_t$ is the same as that in the original problem $R_o$,\n$\\frac{|D_f|}{|D_r|} = \\frac{|D_{test}|}{|D_r| + |D_f|}$ (3)\nIn other words, when constructing the twin model, we need to control the size $|D_{test}|$ to satisfy the Eq.3. In practice, if the actual size of the provided $D_{test}$ is larger than the required quantity, we randomly sample a portion of them, otherwise, we employ data augmentation techniques to produce the necessary amount."}, {"title": "4.3 Discriminative Features", "content": "Besides building a specific labeled dataset for training the binary classifier, another core of our approach is to construct discriminative features for the classifier. Briefly, we propose to integrate three complementary features by employing adversarial attack and curriculum learning strategies.\nNearest-distance Feature. As mentioned earlier, in the case of forgetting a subset of a class, the remaining data $D_r$ will contain samples (which belong to the forgetting class) similar to those in $D_f$. Therefore, for each sample $x \\in D_f$, we can identify its nearest neighbor $x'$ in $D_r$ and calculate their distance $l(x, x')$ with respect to a feature extractor $g()$,\n$l(x, x') = |g(x) - g(x')|$ (4)\nIntuitively, the nearest-distance $l(x, x')$ for an easy sample would be smaller than that for a hard sample. Thus, the nearest-distance can be regarded as a discriminative feature to distinguish between easy and hard samples. In practice, we adopt the activation value of the penultimate layer (i.e., the layer just before the last fully-connected layer) as the $g()$.\nAdversarial-attacking Feature. Whether a sample can be correctly generalized or not (i.e., belong to a easy or hard sample) is closely related to the positional relation between it and the classification boundary. If it is near the boundary, it is hard to correctly classify it with high confidence. Thus, the easy samples often stay far from the boundary, while the hard samples tend to stay near the boundary.\nIn order to leverage such positional relation knowledge to distinguish between easy and hard samples, we employ the untargeted adversarial attack technique. As we know, given any sample $x$, we can generate a corresponding adversarial sample $\\tilde{x}$ whose classification result is different from its ground-truth label. The adversarial sample $\\tilde{x}$ is often generated by perturbing $x$ with a certain perturbation $r$. The perturbation $r$ is typically constrained by a certain attack budget $\\epsilon$, as follows,\n$\\mathop{\\max}_r L(x, y; \\theta)$ s.t. $||r||_p < \\epsilon, \\tilde{x} = x + r$ (5)\nThe essence of adversarial attack is to move $x$ across the classification boundary, i.e., changing the classification result. Thus, given a sufficiently large budget $\\epsilon$, we can successfully conduct the adversarial attack on any sample, i.e., successfully generating $\\tilde{x}$.\nIntuitively, the attack budget for a sample is related to the positional relation between it and the classification boundary. If the sample is near the boundary, a small budget is enough to conduct a successful attack. Thus, we can distinguish between hard and easy samples like that: we use a relatively small attack budget to conduct adversarial attack, and we identify easy and hard samples based on whether the attack is successful or fails, i.e., hard samples can be successfully attacked with a small attack budget, since it is near the classification boundary.\nFurthermore, the bipolar results about successful or failure attack cannot be regarded as a good continuous discriminative feature. Instead, we compute the classification score/logits $s(\\tilde{x})$ for the adversarial sample $\\tilde{x}$. On the other hand, we calculate the classification score $s(x)$ for the clean sample $x$, and measure the cross-entropy between $s(x)$ and $s(\\tilde{x})$,\n$dist(x, \\tilde{x}) = H(s(x), s(\\tilde{x}))$ (6)\nObviously, the $dist(x, \\tilde{x})$ will be large if the attack is successful and vice versa. The larger the $dist(x, \\tilde{x})$ is, the higher the likelihood of a successful attack. Thus, the $dist(x, \\tilde{x})$ is regarded as the second discriminative feature to distinguish between easy and hard samples.\nCurriculum-learning-loss Feature. Curriculum learning theory suggests that easy samples are learned earlier than hard samples. Thus, we propose to employ a curriculum learning strategy to build our third feature. Curriculum learning has illustrated that neural networks tend to learn easy samples very quickly at the beginning of the training iterations. Conversely, the hard samples are learned at the later iterations. Specifically, we train a randomly-initialized network $M_r$ from scratch, where the architecture of $M_r$ is the same as the architecture of original model $M_o$. To distinguish between easy and hard samples, we just train the model $M_r$ for one or two epochs, instead of completing the full training procedure.\nRegarding whether a sample $x$ has been well-learned by $M_r$, we use the loss $loss(x)$ as a metric, i.e., a small loss value implies that $x$ has been well learned (indicating it is a easy sample) while a large loss value implies that $x$ has not been well learned (indicating it is a hard sample). Therefore, the curriculum-learning-loss $loss(x)$ is considered as the third feature to distinguish between easy and hard samples.\nNote that the model $M_r$ is trained with a part of $D_r + D_f$ and $D_{test}$. In practice, we find that it is enough to train $M_r$ by just utilizing 30% of all $D_r + D_f$. Thus, the training of $M_r$ is much cheaper than the training of $M_o$."}, {"title": "4.4 Binary Classifier", "content": "We have build a specific labeled dataset $D_{test}$. On the other hand, we have devised three discriminative features to distinguish between easy and hard samples. Next, we will train a binary classifier to distinguish between easy and hard samples with these feature over the labeled dataset. Specifically, we employ an MLP network with two hidden layers as the binary classifier. We concatenate the three discriminative features as the input of the binary classifier. Note that we have opted for a straightforward binary classifier due to the discriminative nature of our features and our desire to keep the unlearning process cost-effective."}, {"title": "5 Experiment", "content": "5.1 Experimental setting\nDatasets & Models. We evaluate our approach using two public image classification datasets, i.e., CIFAR-10 and CIFAR-100 (Krizhevsky, Hinton et al. 2009) and VGGFaces2 (Cao et al. 2018). CIFAR-10 comprises 10 classes, and we perform data removal evaluations for each class independently. For CIFAR-100, we randomly select 10 out of 100 classes for evaluation due to space constraints (the class names are listed in Table.2). Since we emphasize on forgetting only a subset of one class, we randomly select 100 images as $D_f$. For VGGFaces2, we randomly select 10 out of 100 celebrities and then randomly choose 50 facial images to comprise $D_f$.\nThree common deep neural networks for image classification are employed for evaluation: ResNet-18 (He et al. 2016), AllCNN (Springenberg et al. 2014), and Vision Transformer (Lee, Lee, and Song 2021). The original models are trained for 200 epochs using Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.9, weight decay of $5e - 4$, and an initial learning rate of 0.01. The learning rate is divided by 10 after 100 and 150 epochs.\nBaseline Methods. We compare our approach against four machine unlearning methods. 1) Negative Gradient (Golatkar, Achille, and Soatto 2020a): fine-tune the original model on $D$ by increasing the loss for samples in $D_f$, which is the common surrogate objective adopted by most unlearning methods. 2) Fine-tuning: fine-tune the model on $D_r$ using a slightly large learning rate. This is analogous to catastrophic forgetting, as fine-tuning without $D_f$ may cause the model to forget D f. 3) Random Labeling (Graves, Nagisetty, and Ganesh 2021): fine-tune the model on D by assigning random labels to samples in Df, causing those samples to receive a random gradient. 4) Bad Teacher (Chundawat et al. 2023): it explores the utility of competent and incompetent teachers in a student-teacher framework to induce forgetfulness. Note that this work is closely related to our approach since it also emphasizes alignments in data removal.\nEvaluation Metrics. We assess the alignment quality of unlearning methods using two metrics. 1) Accuracy on Df and Dtest: Ideally, the unlearned model is expected to achieve the same accuracy as the gold model. Let $ACC_{D_f}$ and $ACC_{D_f}^{gold}$ denote the accuracy of the unlearned model and the gold model on $D_f$. Thus, the differences between them, $\\delta = |ACC_{D_f} - ACC_{D_f}^{gold}|$, can be considered as the measure of alignment quality. Additionally, accuracy on $D_{test}$ is employed to assess whether the model accuracy is compromised after data removal. 2) Activation Distance: This metric calculates the average L2-distance between the activation values of the unlearned model and the gold model on Df. A smaller activation distance indicates a better alignment between two models.\nImplementation Details. All our experiments were conducted on a single NVIDIA RTX 3090 GPU. For the binary classifier, we employ an MLP network with two hidden layers, where the sizes of the hidden layers are 64 and 32, respectively. We use the ReLU activation function in the classifier. The classifier is trained for 100 epochs using SGD with fixed learning rate of 0.01, momentum 0.9 and weight decay 0.0005. Regarding the Nearest-distance Feature (NF), the nearest distance $l_{min}$ is computed by the average of the top-5 nearest distances within the forget class in Dr. For the Adversarial-attacking Feature (AF), we set the attack budget as $\\epsilon = 4/255$."}, {"title": "5.2 Main Results", "content": "The comparison between our approach and other methods on the CIFAR-10 is shown in Table.1. We provide the accuracy of the gold model on both Dtest and Df, i.e., $ACC_{D_{test}}^{gold}$ and $ACC_{D_f}^{gold}$. For each method, we provide the $ACC_{D_{test}}$, $ACC_{D_f}$, and $\\delta = |ACC_{D_f} - ACC_{D_f}^{gold}|$. The closer the accuracy between the gold model and the unlearned model, i.e., the smaller the $\\delta$ is, the better the alignment is achieved. In addition, the accuracy on Dtest is employed to assess whether the model's normal performance is compromised after data removal. The higher the accuracy on Dtest is, the better the normal performance is maintained.\nTable.1 shows the results for the original model using the ResNet18. For the original model, its average accuracy over 10 classes is 85.37. For each class, 100 samples are randomly selected be forgotten. We conduct the data removal experiment for each class independently, as depicted in each row of Table.1.\nFrom Table.1, it's evident that the Fine-tuning method cannot effectively accomplish data removal, as the $ACC_{D_f}$ remains almost at 100%. It implies that Fine-tuning can only forget an entire class but struggles to forget a subset of a class. In contrast, the Random Labeling method tends to misclassify all samples in Df, resulting in $ACC_{D_f} = 0\\%$. The Negative Gradient method performs better than Fine-tuning and Random Labeling, with an average difference of $\\delta = 76.3\\%$. Due to explicitly addressing the alignment challenge, the Bad Teacher method makes significant progress, achieving a remarkable average difference of $\\delta = 8\\%$.\nNevertheless, our approach outperforms the Bad Teacher method in terms of both the accuracy on $D_{test}$ and $\\delta$. Particularly, we improve the average difference $\\delta$ from 8% to 4.2%.\nThe comparison on the CIFAR-100 is shown in Table.2. We draw similar conclusions as with CIFAR-10. The Random Labeling method no longer misclassifies all samples in Df, with an average $ACC_{D_f} = 14.60\\%$. Our approach demonstrates a greater advantage than the Bad Teacher method, with $\\delta = 8.4\\%$.\nActivation Distance. Activation distance (Golatkar et al. 2021) is another effective metric for evaluating the alignment in unlearning. A smaller activation distance indicates a higher similarity between the unlearned model and the gold model.\nFrom Table.3, our approach continues to exhibit excellent performance, significantly outperforming other methods. One interesting fact is that, despite Negative Gradient performing much worse than Bad Teacher in terms of $ACC_{D_f}$, it actually outperforms the Bad Teacher in terms of Activation Distance.\nMore Results. The results on the VGGFaces2 dataset are presented in the supplementary material. Additionally, the results for AllCNN and ViT are also presented in the supplementary material. Similarly, our approach consistently outperforms all other methods for the two network architectures."}, {"title": "5.3 Discussion", "content": "Ablation Study. Three discriminative features are integrated to train the binary classifier. We evaluate their individual contributions through an ablation study, using the accuracy of predicted generalization-labels as the metric. This metric reflects the quality of alignment. The higher the accuracy, the better the alignment. From Table.4, the Curriculum-learning-loss Feature (CF) is slightly more discriminative than Nearest-distance Feature (NF) and Adversarial-attacking Feature (AF). Nonetheless, the three features are complementary to each other.\nForgetting More Data. To assess the performance when forgetting a larger amount of data, we increase the size of $D_f$ from 100 to 4000. The performance is measured by the accuracy of predicted generalization-labels. From Fig.3, we can see that the alignment becomes more challenging with the increase of Df size. However, our approach consistently outperforms the Bad Teacher method, and its advantages become more pronounced with the increase in the size of Df. More results are presented in the supplementary material."}, {"title": "6 Conclusion", "content": "This paper focuses on the task of data removal, emphasizing the alignment in data removal. It introduces a Twin Machine Unlearning approach, where a twin unlearning problem is constructed and leveraged to solve the original problem. Furthermore, three discriminative features are devised by employing adversarial attack and curriculum learning strategies. Extensive empirical experiments show that our approach significantly improve the alignment in data removal."}]}