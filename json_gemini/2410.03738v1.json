{"title": "ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation", "authors": ["Fillipe dos Santos Silva", "Gabriel Kenzo Kakimoto", "Julio Cesar dos Reis", "Marcelo S. Reis"], "abstract": "Cluster analysis plays a crucial role in various domains and applications, such as customer segmentation in marketing. These contexts often involve multimodal data, including both tabular and textual datasets, making it challenging to represent hidden patterns for obtaining meaningful clusters. This study introduces ERASMO, a framework designed to fine-tune a pretrained language model on textually encoded tabular data and generate embeddings from the fine-tuned model. ERASMO employs a textual converter to transform tabular data into a textual format, enabling the language model to process and understand the data more effectively. Additionally, ERASMO produces contextually rich and structurally representative embeddings through techniques such as random feature sequence shuffling and number verbalization. Extensive experimental evaluations were conducted using multiple datasets and baseline approaches. Our results demonstrate that ERASMO fully leverages the specific context of each tabular dataset, leading to more precise and nuanced embeddings for accurate clustering. This approach enhances clustering performance by capturing complex relationship patterns within diverse tabular data.", "sections": [{"title": "1 Introduction", "content": "Tabular data is ubiquitous in various fields such as finance, healthcare, and marketing, where it serves as a primary source of information for Machine Learning (ML) tasks [15]. Despite its widespread use, extracting meaningful insights from tabular data remains a complex challenge, particularly in clustering tasks [6]. These challenges include handling heterogeneous feature types, dealing with high-dimensional spaces, and ensuring meaningful distance metrics. These"}, {"title": "2 Related Work", "content": "Several studies have explored the application of LLMs to transform tabular data for clustering tasks, demonstrating the potential to enhance user segmentation and data analysis [6,14,19,21,23,24]. Zhu et al. [24] proposed a novel method named Word Embedding of Dimensionality Reduction (WERD) for document clustering. Their approach integrates pre-trained word embeddings with dimensionality reduction techniques. In their work, Sentence-BERT embeds them into high-dimensional vectors after preprocessing documents, which PaCMAP then reduces. Spectral clustering is applied, followed by Non-Negative Matrix Factorization to extract keywords.\nCLUSTERLLM [23], a novel text clustering framework, leverages feedback from LLMs such as ChatGPT. This method enhances clustering by utilizing LLMs to refine clustering perspectives and granularity through two stages: a triplet task for fine-tuning embedders based on user preferences and a pairwise task for determining cluster granularity. Extensive experiments on fourteen datasets demonstrated that CLUSTERLLM consistently improves clustering quality and is cost-effective, outperforming traditional clustering methods. Both WERD [24] and CLUSTERLLM [23] presented limitations compared to the ERASMO framework (our proposal). WERD might not fully capture the contextual nuances of each dataset due to its focus on dimensionality reduction techniques. At the same time, CLUSTERLLM's reliance on general-purpose LLMs for guidance may overlook specific dataset characteristics.\nA method demonstrating that LLMs enables few-short learning applied to clustering tasks was proposed in [21]. Their study showed how LLMs can perform clustering tasks with minimal labeled data by leveraging their extensive pretraining, significantly reducing the need for large annotated datasets and achieving reasonable clustering performance with few-shot learning. Similarly, Tipirneni et al. [18] explored context-aware clustering using LLMs, highlighting how these models can utilize contextual information to enhance clustering accuracy. Both methods, however, may not fully leverage the dataset-specific nuances as effectively as ERASMO because we employ a fine-tuning step, allowing the model to capture better and utilize dataset-specific details, leading to more accurate and reliable clustering results.\nTissera, Asanka, & Rajapakse developed [19] an approach to enhancing customer segmentation using LLMs and DICE. Their method combined LLMs with DICE to generate consistent and deterministic embeddings across different"}, {"title": "3 ERASMO", "content": "This section introduces ERASMO, our framework that leverages transformer-based language models to generate high-quality embeddings from tabular data. These embeddings are particularly effective for clustering analysis, allowing for identifying patterns and groupings within the data that might not be immediately apparent.\nThe process involves two main stages: (1) fine-tuning a pretrained LLM on a textually encoded tabular dataset; and (2) utilizing the fine-tuned model to generate embeddings, which are used by a clustering algorithm. These designed stages were inspired by [2]. Subsection 3.1 details the fine-tuning phase, whereas Subsection 3.2 reports on the embedding generation processes."}, {"title": "3.1 Phase 1: Fine-Tuning", "content": "Standard pretrained generative LLMs expect sequences of words as inputs. Hence, we convert each row of our dataset into a textual representation to apply an LLM to tabular data, which can contain categorical, numerical, and textual information.\nDefinition 1 (Textual Converter). Given a tabular dataset with m columns with feature names f1, f2,..., fm and n rows of samples 81,..., Sn, let the entry Vi,j, i \u2208 {1, ..., n}, j \u2208 {1, ..., m} represent the value of the j-th feature of the i-th data point. Taking the feature name and value into account, each sample si of the table is transformed into a textual representation ti using the following subject-predicate-object transformation:\n$t_{i,j} = [f_j, \\text{``is''}, v_{i,j}, \\text{``,''}] \\quad \\forall i \\in \\{1, ..., n\\}, j \\in \\{1,...,m\\}$ (1a)\n$t_i = [t_{i,1}, t_{i,2},..., t_{i,m}], \\quad \\forall i \\in \\{1, ..., n\\}$ (1b)"}, {"title": "Definition 2 (Random Feature Sequence Shuffle).", "content": "Let ti,i \u2208 {1,..., n}, be a textual representation. Consider a sequence k = (k\u2081,...,km) that is a permutation of the sequence of indices (1,...,m). A random feature sequence shuffle is defined as ti(k) = [ti,k1, ti,k2,..., ti,km].\nWe fine-tune our generative language model on samples without order dependencies when using shuffled orders of the textually encoded features. Moreover, such permutations are highly beneficial as they allow for arbitrary conditioning in tabular data generation. In our experiments, we refer to ERASMObase as the baseline model, utilizing only the Textual Converter and Random Feature Sequence Shuffle. In addition, there is evidence that verbalizing numerical tokens can enhance effectiveness in specific scenarios [8]. In this sense, we explore this approach, naming it ERASMONV, as follows."}, {"title": "Definition 3 (Number Verbalizer).", "content": "Let ti, i \u2208 {1, ..., n}, be a textual representation, and tij be the set of words of the j-th feature of ti. A number verbalizer is a function v that receives as input a word w of tij and is defined as:\n$\\upsilon(\\omega) = \\begin{cases} \\omega, \\text{ if } \\omega \\text{ is not numerical,} \\\\ \\text{verbalized } \\omega \\text{ otherwise.} \\end{cases}$ (2)\nBy applying this transformation on every token of every textual representation, we ensure that any numerical information in the text is verbalized. In some NLP tasks, such as clustering with embeddings, sentiment analysis, and text classification, verbalizing numbers can improve the model's understanding of the context and meaning of numerical values, leading to more accurate and meaningful results. This transformation might not be beneficial in some cases, depending on the specific nature of the data and the task at hand [8,9].\nFine-Tuning a Pretrained Auto-Regressive Language Model: We describe the fine-tuning procedure of a pretrained LLM on the encoded tabular data for generation tasks. We suppose a textually encoded tabular dataset T = {ti(k)}i=1,...,n that was transformed into text by the proposed encoding scheme. Let k be a randomly drawn permutation, and n denote the number of rows. Based on user choice, the pipeline can proceed directly to fine-tuning the LLM to generate ERASMObase, or it can first apply a number verbalizer to"}, {"title": "3.2 Phase 2: Embedding Generation and Clustering Analysis", "content": "We generate embeddings from the model after fine-tuning the LLM on the textually encoded tabular dataset. These embeddings capture the contextual relationships and features encoded during the training phase.\nWe start by feeding the test dataset, transformed into its textual representation, into the fine-tuned LLM. The model generates embeddings for each input sequence, providing a high-dimensional representation for each sample. This process ensures that the embeddings preserve the contextual and feature relationships learned during fine-tuning. Depending on the user's choice in the pipeline, the embeddings are generated from either ERASMObase or ERASMONV models, reflecting whether the number verbalizer step was applied.\nTo generate these embeddings, the input sentences t \u2208 Ttest are encoded into sequences of tokens and processed by the fine-tuned LLM. The embeddings are obtained from the final hidden states of the model, resulting in rich and informative representations of the data. These embeddings can then be utilized for various downstream tasks, including clustering analysis, to gain deeper insights into the data structure (cf. Figure 2)."}, {"title": "4 Experimental Methodology", "content": "Our experiments evaluated the quality assessment for the best-performing clustering algorithms for each dataset and approach (model) combination (cf. Table 1 for the obtained results). Subsection 4.1 describes the datasets used for training and testing. Subsection 4.2 presents an overview of the clustering algorithms."}, {"title": "4.1 Datasets", "content": "We selected a diversified set of datasets to encompass a variety of challenges related to text categorization and clustering, and we used them to evaluate text clustering algorithms.\nBanking Marketing Targets: Composed of data from direct marketing campaigns of a banking institution, which includes client attributes like age and job, along with the response to the campaign [10].\nE-Commerce Public Dataset by Olist: A Brazilian e-commerce dataset with over 100,000 orders from 2016 to 2018 across multiple marketplaces [11]. It includes 72,794 training and 18,199 testing samples. The Recency, Frequency, Monetary (RFM) model was used for customer segmentation, as described in [19].\nYelp: Comprises reviews from Yelp businesses, including text reviews, star ratings, and business attributes, offering a rich resource for sentiment analysis and review classification tasks [4].\nPetFinder.my: Features adoption records from the PetFinder.my website, encompassing various pet attributes, descriptions, and adoption status, valuable for text classification and clustering related to animal welfare [5].\nWomen Clothing Reviews: Contains reviews of women's clothing, with each review detailing text feedback, ratings, and customer information, suitable for sentiment analysis and recommendation system research [3].\nEach unlabeled dataset was processed through the proposed pipeline, which involves training a pretrained LLM. This approach ensures that the clustering algorithms can perform optimally across diverse textual inputs, enhancing their ability to effectively identify and group related data points."}, {"title": "4.2 Clustering Algorithms", "content": "The clustering algorithms chosen are well-suited for handling complex patterns in structured and textual data, ensuring efficient categorization.\nWe used the k-means algorithm for its simplicity and effectiveness with large datasets and k-means++ for its strategic centroid initialization to enhance clustering efficiency and quality [12]. Unlike k-means, which assigns each data point to a single cluster, Fuzzy C-Means (FuzzyCM) employs a probabilistic membership approach, effectively capturing the nuances and polysemy typical of textual data. We used Agglomerative Hierarchical Clustering (AHC) to uncover hierarchical structures and spectral clustering for its proficiency in recognizing"}, {"title": "4.3 Approaches (Baselines)", "content": "We utilized various embedding techniques from state-of-the-art LLMs, including OpenAI, Falcon, Llama 2, GPT-2 Medium, and an MPNet-based model, each enhancing text representation by capturing contextual nuances.\nFor the MPNet-based model, we used sentence-transformers/all-mpnet-base-v2 (MPNet-v2) [17]. For the OpenAI model, we utilized text-embedding-3-large, and for the Falcon model, we used tiiuae/falcon-7b [1]. Additionally, we employed Llama-2-7b-chat-hf [20] for chat applications and gpt2-medium [16] for faster textual representations. We used the embeddings from the last layer of all models for the most contextually rich text representations.\nWe integrated an additional baseline from a recent study that explores customer segmentation using LLMs combined with DICE [19]. They used the paraphrase-multilingual-mpnet-base-v2 model from Sentence Transformers to generate 768-dimensional sentence embeddings. This model, based on the MPNet architecture, has about 278 million parameters and is designed for clustering and semantic search."}, {"title": "4.4 Evaluation Metrics", "content": "We use a set of metrics to evaluate our proposed framework and baselines thoroughly. Specifically, we employed the SS, CHI, and DBI metrics to assess the cohesion, compactness, and separation of clusters, ensuring a robust analysis of their structural integrity.\nThe SS metric, which assesses the separation and cohesion of clusters, is calculated for each data point i as:\n$s(i) = \\frac{b(i)-a(i)}{\\text{max}\\{a(i),b(i)\\}}$\nwhere a(i) measures the average intra-cluster distance, and b(i) is the minimum inter-cluster distance for the point i.\nCHI measures the ratio of between-cluster dispersion to within-cluster dispersion, providing insights into the overall clustering structure. The index is formulated as:\n$CHI = \\frac{\\frac{\\text{Tr}(B_k)}{(k-1)}}{\\frac{\\text{Tr}(W_k)}{(N-k)}}$,\nwhere Tr(Bk) is the trace of the between-group dispersion matrix, and Tr(Wk) the trace of the within-group dispersion matrix, thus evaluating both the separation and compactness of the clusters.\nThe DBI evaluates the average similarity ratio of each cluster with its most similar one, offering a measure of cluster separation. The index is calculated as:\n$DBI = \\frac{1}{k}\\sum_{i=1}^{k}\\text{max}_{j \\ne i} (\\frac{\\sigma_i + \\sigma_j}{d_{ij}})$,\nwhere \u03c3i and \u03c3j represent the average distance of all elements in clusters i and j to their respective centroids, and dij is the distance between centroids of clusters i and j. Lower values of DBI indicate better cluster separation."}, {"title": "4.5 Our implemented Setup", "content": "We compare the described baselines (cf. Subsection 4.3) using a pretrained transformer-decoder LLM model, GPT \u2013 2 medium [16], which has 355 million trainable parameters, 24 layers, 16 attention heads, an embedding size of 1024, and a context size of 1024. To facilitate this comparison, we convert the tabular dataset into text for all baselines, applying the random feature sequence shuffling function and comparing the results with ERASMObase and ERASMONV. Both models were trained with a batch size of 8 over 60 epochs. We applied a dropout rate of 0.1 and utilized 500 warmup steps. The models incorporated a weight decay of 0.01 and used the Adam optimizer with e set to 1e-8 and \u1e9e values of [0.7, 0.9]. The initial learning rate was set to 5e-5, with a schedule starting at 1e-8, ranging from a minimum of 1e-5 to a maximum of 4e-5. The model was developed using PyTorch \u00b9 and is made available at a GitHub repository \u00b2. It ran on a system equipped with five NVIDIA RTX A6000, each having 48 GB of Random Access Memory (RAM)."}, {"title": "5 Experimental Results", "content": "We present key results obtained organized by datasets. Table 1 presents the SS, CHI, and DBI metrics results for the test dataset for all evaluated approaches; values in bold indicate the best outcomes. The best algorithm was determined by choosing the algorithm with the highest SS.\nBanking. In the Banking dataset, the ERASMObase strategy outperformed all other strategies, achieving the highest SS of 0.75, the highest CHI of 12, 038.44, and the lowest DBI of 0.37. This indicates well-defined and compact clusters. ERASMONY also performed strongly with an SS of 0.71 and CHI of 7,570.38, though it had a slightly higher DBI of 0.43 compared to ERASMObase. Other strategies like MPNet-v2 and Falcon showed moderate performance with SS values of 0.27 and 0.23, respectively, while OpenAI had the lowest SS at 0.11 and the highest DBI at 2.73, indicating poor clustering.\nOlist. For the Olist dataset, ERASMONY achieved the highest SS of 0.77, indicating the best clustering quality. It reached the highest CHI of 62, 036.87 and a low DBI of 0.32. ERASMObase followed closely with an SS of 0.75 and a CHI of 54236.31, along with the lowest DBI of 0.30. Other strategies like LLaMA-2 and Falcon performed reasonably well, with SS values of 0.71 and 0.66, respectively. However, OpenAI and MPNet-v2 showed lower SS values, with OpenAI achieving an SS of 0.19 and MPNet-v2 an SS of 0.24.\nYelp. In the Yelp dataset, ERASMONV and ERASMObase both demonstrated superior performance, with SS values of 0.79 and 0.78, respectively. ERASMONV also achieved the highest CHI of 8, 410.94 and tied with ERASMObase for the lowest DBI of 0.28. Other strategies, such as GPT2 Medium and LLaMA-2, showed moderate clustering performance with SS values of 0.39 and 0.29, respectively. OpenAI, with an SS of 0.07, and MPNet-v2, with an SS of 0.23, indicated less effective clustering.\nPetFinder.my. In the PetFinder.my dataset, ERASMONV slightly outperformed ERASMObase with an SS of 0.73 compared to 0.72. ERASMObase had the highest CHI of 3, 351.95 and a low DBI of 0.40, while ERASMONV had a CHI of 3,063.55 and the lowest DBI of 0.34. Other strategies, such as GPT2 Medium and Falcon, showed moderate results with SS values of 0.55 and 0.20, respectively. MPNet-v2 had the lowest SS of 0.14, indicating poor clustering performance.\nClothings. For the Clothings dataset, ERASMObase achieved the highest SS of 0.72 and the highest CHI of 6,208.52, along with the lowest DBI of 0.39. ERASMONY also performed well with an SS of 0.71 and a CHI of 5, 916.57, matching the lowest DBI of 0.39. Other strategies like GPT2 Medium and LLaMA-2 showed moderate performance, with SS values of 0.52 and 0.37, respectively. OpenAI and MPNet-v2 had the lowest SS values of 0.07 and 0.12, respectively, indicating poor clustering quality."}, {"title": "6 Discussion", "content": "The SS results consistently highlight the superior clustering effectiveness of the ERASMObase and ERASMONV strategies across all datasets. ERASMObase achieved the highest SS in the Banking and Clothing datasets, while ERASMONV led in the Olist, Yelp, and PetFinder.my datasets. This indicates that both strategies help to form well-defined clusters, with ERASMObase having a slight edge in some datasets. The higher SS values for these models suggest they are better at creating distinct clusters than other strategies like MPNet-v2, OpenAI, and Falcon, which showed significantly lower SS values, indicating poorer representation relevance for clustering quality.\nThe DBI results further support the effectiveness of ERASMO base and ERASMONV. Both strategies consistently achieved the lowest DBI values, particularly excelling in the Banking, Olist, and Yelp datasets. A lower DBI indicates more compact and well-separated clusters, affirming that our proposed models form tight and distinct clusters. ERASMObase generally showed slightly better DBI scores, suggesting it produces more compact clusters than ERASMONV. In contrast, approaches like OpenAI and MPNet-v2 had significantly higher DBI values, reflecting less adequate cluster compactness and separation.\nThe CHI results corroborate the trends observed in SS and DBI. ERASMObase and ERASMONY achieved the highest CHI scores across most datasets, with"}, {"title": "7 Conclusion", "content": "Exploring structured and textual data simultaneously for clustering analysis is a challenging problem. This study presented and evaluated the ERASMObase and ERASMONY clustering approaches, demonstrating their superior empirical effectiveness across multiple datasets. The results, based on SS, DBI, and CHI, consistently revealed that our proposed models help to create well-defined, compact, and dense clusters, outperforming all strategies. Despite their high computational demands and dependency on data quality, we found the ERASMO models suited for practical clustering tools. Future work might focus on optimizing computational efficiency and enhancing robustness for diverse and noisy datasets."}]}