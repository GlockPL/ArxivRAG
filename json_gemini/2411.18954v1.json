{"title": "NEUROLIFTING: NEURAL INFERENCE ON MARKOV RANDOM FIELDS AT SCALE", "authors": ["Yaomin Wang", "Chaolong Ying", "Xiaodong Luo", "Tianshu Yu"], "abstract": "Inference in large-scale Markov Random Fields (MRFs) is a critical yet challeng-\ning task, traditionally approached through approximate methods like belief prop-\nagation and mean field, or exact methods such as the Toulbar2 solver. These\nstrategies often fail to strike an optimal balance between efficiency and solution\nquality, particularly as the problem scale increases. This paper introduces NEU-\nROLIFTING, a novel technique that leverages Graph Neural Networks (GNNs) to\nreparameterize decision variables in MRFs, facilitating the use of standard gradi-\nent descent optimization. By extending traditional lifting techniques into a non-\nparametric neural network framework, NEUROLIFTING benefits from the smooth\nloss landscape of neural networks, enabling efficient and parallelizable optimiza-\ntion. Empirical results demonstrate that, on moderate scales, NEUROLIFTING per-\nforms very close to the exact solver Toulbar2 in terms of solution quality, signif-\nicantly surpassing existing approximate methods. Notably, on large-scale MRFs,\nNEUROLIFTING delivers superior solution quality against all baselines, as well as\nexhibiting linear computational complexity growth. This work presents a signifi-\ncant advancement in MRF inference, offering a scalable and effective solution for\nlarge-scale problems.", "sections": [{"title": "1 INTRODUCTION", "content": "Markov Random Fields (MRFs) stand as a fundamental computational paradigm for modeling com-\nplex dependencies among a large collection of variables, permeating a variety of domains such\nas computer vision (Wang et al., 2013; Su et al., 2021), natural language processing (Almutiri &\nNadeem, 2022; Ammar et al., 2014; Lin et al., 2020), and network analysis (Wu et al., 2020; Yun-\nfei Ma & Razavi, 2022). MRF's capacity to encode intricate probabilistic interactions underscores\nits widespread utility. However, unraveling the optimal configurations in high-dimensional settings\nremains a formidable task owing to the inherent computational complexity involved.\nTraditional inference methodologies for MRFs bifurcate into approximate and exact strategies, each\nwith its own set of advantages and limitations. Approximate inference techniques, such as belief\npropagation (Pearl, 2022; Wainwright et al., 2005) and mean field (Saito et al., 2012; Zhang, 1993)\napproximations, strive for computational efficiency but often at the expense of solution quality,\nparticularly as the scale of the problem escalates. Conversely, exact inference methods, epitomized\nby the Toulbar2 solver (De Givry, 2023; Hurley et al., 2016), aspire to optimality but are frequently\nhampered by exponential time complexities that render them infeasible for large-scale MRFs."}, {"title": "2 PRELIMINARY", "content": "Markov Random Field. An MRF is defined over a undirected graph G = (V,C), where V repre-\nsents the index set of random variables and C \u2286 2V is the clique set representing the (high-order)\ndependencies among random variables. Throughout this paper, we associate a node index i with a\nrandom variable xi \u2208 X, where X is a finite alphabet. Thus, given graph G, the joint probability of\na configuration of X = {xi}i\u2208v can be expressed as\n$$P(X)=\\frac{1}{Z}exp(-E(X)) = \\frac{1}{Z}exp(-\\sum_{i \\in V} \\theta_i(x_i) - \\sum_{C_k \\in C} \\theta_{C_k}({x_i|i \\in C_k})))$$\nwhere Z is the partition function, 0\u2081(\u00b7) denotes the unary energy functions, \u03b8c() represent the\nclique energy functions. In this sense, MRF provides a compact representation of probability by\nintroducing conditional dependencies:\n$$P(x_i|X\\{x_i}) = P(x_i|\\{x_j \\text{ for } i,j \\in C_k \\text{ for } C_k \\in C\\}).$$\nIn this paper, we consider the Maximize a Posterior (MAP) estimate of Equation 1, which requests\noptimizing Equation 1 via X* = minx E(X). One can consult Koller & Friedman (2009) for more\ndetails.\nGraph Neural Networks. GNNs represent a distinct class of neural network architectures specif-\nically engineered to process graph-structured data (Kipf & Welling, 2017; Hamilton et al., 2017;\nXu et al., 2019; Veli\u010dkovi\u0107 et al., 2018). In general, when addressing a problem involving a graph\nG = (V, E), where & is the edge set, GNNs utilize both the graph G and the initial node representa-\ntions {h) \u2208 RdVi\u2208 V} as inputs, where d is the dimension of initial features. Assuming the total\nnumber of GNN layers to be K, at the k-th layer the graph convolutions typically read:"}, {"title": "3 METHODOLOGY", "content": "An overview of NEUROLIFTING is in Figure 1, with an exemplary scenario involving an energy\nfunction devoid of unary terms, yet comprising three clique terms. Initially, the clique-based repre-\nsentation of this function (depicted in the leftmost shaded diagram) undergoes a transformation to\na graph-based perspective, which subsequently integrates into the network architecture. To address\nthe absence of inherent node feature information in the original problem, we elevate the dimension-\nality of decision variables within this framework. This transformation facilitates a paradigm shift\nfrom the identification of optimal state values to the learning of optimal parameters for encoding and\nclassification of these variables. Furthermore, we devised a novel approach to circumvent the ab-\nsence of a traditional loss function, thereby extending the applicability of our framework to Markov\nRandom Fields (MRFs) of arbitrary order."}, {"title": "3.2 PREPOSSESSING", "content": "We discuss several necessary preprocessing steps to adapt standard MRF to a GNN style.\nTopology construction for GNNs. In an MRF instance, the high-order graph structure consists\nof nodes and cliques, diverging from typical GNNs allowing only pairwise edges (2nd-order). To\nfacilitate the power of GNNs, we need to convert high-order graph into a pairwise one.\nBy the very definition of a clique, any two nodes that appear within the same clique are directly\nrelated. Thus, for any two nodes i, j\u2208 Ck in a clique Ck, we add a pairwise edge (i, j) to its\nGNN-oriented graph. An example can be observed in Figure 1. It is worth noting that an edge may\nappear in multiple cliques; however, we add each edge only once to the graph.\nInitial feature for GNNs. As there is no initial features associated to MRF instances, we initialize\nfeature vectors to GNNs randomly with a predefined dimension d. Detailed information on how we"}, {"title": "3.3 GNNS AS NON-PARAMETRIC LIFTING", "content": "In this section, we detail how NEUROLIFTING generates features that capture the hidden informa-\ntion of the given MRF and solves the original MAP problem by optimizing in a high-dimensional\nparameter space. As mentioned in Section 3.2, we initially generate learnable feature vectors ran-\ndomly using an encoder that embeds all nodes, transforming the integer decision variables into d\u2081-\ndimension vectors h(0) 2(0) \u2208 Rdi for node i, where di is a hyperparameter representing the dimension\nafter lifting.\nThe intuition for utilizing GNNs in the implementation of lifting techniques is inspired by Loopy\nBelief Propagation (LBP) (Weiss & Freeman, 2001). When applying LBP for inference on MRFs,\nthe incoming message Mji to node i from node j is propagated along the edges connecting them.\nNode i can then update its marginal distributions according to the formula in Eq. 4.\n$$p_{posterior} (x_i|X\\{X}) = p_{prior} (x_i|X\\{x})\\prod_{(i,j)\\in E} \\sum_{x_j}M_{ji}$$\nImportantly, the incoming messages are not limited to information solely about the directly con-\nnected nodes; they also encompass information from sub-graphs that node i cannot access directly\nwithout assistance from its neighbors. This allows a more comprehensive aggregation of informa-\ntion, enabling node i to merge these incoming messages with its existing information. This process\nof message aggregation bears resemblance to the message-passing procedure used in GNNs, where\nnodes iteratively update their states based on the information received from their neighbors.\nGraph convolutions should intuitively treat adjacent nodes equally, consistent with the principle in\nMRFs, where the information collected from neighbors is processed equally. Typical GNNs are\nsummarized in the followings:"}, {"title": "3.4 ENERGY MINIMIZATION WITH GNN", "content": "As indicated by Equation 1, the energy function can serve as the loss function to guide network train-\ning since minimizing this energy function aligns with our primary objective. Typically, the energy\nfunction for a new problem instance takes the form of a look-up table, rendering the computation\nprocess non-differentiable. To facilitate effective training in a fully unsupervised setting, it is crucial\nto transform this computation into a differentiable loss aligning with the original energy function.\nThe initial step involves transforming the decision variable from xi \u2208 {1, ..., si}, where si is the\nnumber of states of variable xi, to vi \u2208 {0, 1}si. At any given time, exactly one element of the vec-\ntor vi can be one, while all other elements must be zero; the position of the 1 indicates the current\nstate of the variable xi. Define Vk = i\u2208CkVi, where is the tensor product. The corresponding\nenergy function would be Equation 5. Subsequently, we relax the vector vi to pi(\u03b8) \u2208 [0, 1]si, where\npi(0) represents the output of our network and @ denotes the network parameters. This output can\nbe interpreted as the probabilities of each state that the variable xi might assume.\n$$E(\\{v_i|i \\in V\\}) = \\sum_{i \\in V}(V_i(\\theta), \\phi(x_i)) + \\sum_{C_k \\in C} (\\psi(C_K), V_k)$$\n$$L(\\theta) = \\sum_{i \\in V}(P_i(\\theta), \\phi(x_i)) + \\sum_{C_k \\in C} (\\psi(C_K), P_k)$$\nwhere (,) refers to the tensor inner product. The applied loss function is defined in Equation 6,\nhere Pk =iECkPi. The rationale behind our loss function closely resembles that of the cross-\nentropy loss function commonly used in supervised learning. Let P represent the true distribution\nand Q denote the predicted distribution. A lower value of cross-entropy H(P, Q) indicates greater\nsimilarity between these two distributions. However, our approach differs in that we are not seeking\na predicted distribution that closely approximates the true distribution. Instead, for each variable,\nwe aim to obtain a probability distribution that is highly concentrated, with the concentrated points\ncorresponding to the states that minimize the overall energy.\nOnce the network outputs are available, we can easily determine the assignments by rounding the\nprobabilities p(\u03b8) to obtain binary vectors v. Using these rounded results, the actual energy can\nbe calculated using Equation 5. It is observed that after the network converges, the discrepancy\nbetween L(0) and E({vi|i \u2208 V}) is minor and we won't see any multi-assignment issue in decision\nvariables. We choose Adam (Kingma & Ba, 2015) as the optimizer, and employ simulated annealing\nduring the training process, allowing for better exploring the loss landscape to prevent sub-optima."}, {"title": "3.5 ANALYSIS AND DISCUSSION", "content": "Relation to lifting. In this innovative framework of using GNNs for inference on MRFs, a natural\nand sophisticated parallel emerges with the classical concept of lifting in optimization (Balas et al.,\n1993). By mapping each unary term of an MRF to a node within a GNN and translating clique\nterms into densely connected subgraphs, the traditional MRF energy minimization transforms into\noptimizing a multi-layer GNN with extra dimensionality. This procedure aligns with the lifting tech-\nnique where the problem space is expanded to facilitate more efficient computation. Akin to the prin-\nciple of standard lifting to ease optimization, the GNN-based reparameterization can leverage the\ngradient descent optimization paradigm inherent in the smooth neural network landscape (Dauphin\net al., 2014; Choromanska et al., 2015), ensuring efficient computation and convergence. Therefore,\nwhile offering an enhanced approach to inference, the GNN reparameterization mirrors the core\nprinciples of lifting by transforming and extending the solution space into a computation-friendly\none to achieve computational efficacy and scalability. More empirical evidence is in Sec. 4.4.\nComplexity analysis. The primary computations in this model arise from both the loss calculation\nand the operations within the GNN. For the loss function, let Cmax denote the maximum clique\nsize. The time complexity for the loss calculation is given by O(|V||X| + \u0421\u0442\u0430\u0445|C||X|). For the\nGNN component, let N denote the average number of neighbors per node in the graph. The time\ncomplexity for neighbor aggregation in each layer is O(N\u2082|V|), and merging the results for all nodes\nrequires O(|V|d) where d is the feature dimension. Thus, for a K-layer GraphSAGE model with"}, {"title": "4 EXPERIMENT", "content": "Evaluation metric. For all instances used in the experiments, we utilize the final value of the\noverall energy function E(X) as defined in Equation 1. Without loss of generality, all problems are\nformulated as minimization problems.\nBaselines. We compare our approach against several well-established baselines: Loopy Belief Prop-\nagation (LBP), Tree-reweighted Belief Propagation (TRBP) (Wainwright et al., 2005), and Toul-\nbar2 (De Givry, 2023). LBP is a widely used approximate inference algorithm that iteratively\npasses messages between nodes. TRBP improves upon LBP by introducing tree-based reweight-\ning to achieve better approximations, particularly in complex graph structures. Toulbar2 is an exact\noptimization tool based on constraint programming and branch-and-bound methods Notably, Toul-\nbar2 is the winner on all MPE and MMAP task categories of UAI 2022 Inference Competition 1.\nThese baselines allow us to evaluate the performance of our proposed solution under fair settings.\nMRF format and transformation. The MRF data files are in UAI format and we interpret the data\nfiles in the same way as Toulbar2. Detailed information about unary and clique terms will be treated\nas unnormalized (joint) distributions, and the energies are calculated as 0i(xi = a) = \u2212log(P(xi =\na)), where P(xi = a) represents the probability provided by the data file. Note that we use the\nunnormalized values during the transformation process. The transformation for the clique energy\nterms will follow the same procedure. More details are in Appendix E."}, {"title": "4.1 SYNTHETIC PROBLEMS", "content": "We first conduct experiments on synthetic problems generated randomly based on Erd\u0151s-R\u00e9nyi\ngraphs (Erd\u00f6s & R\u00e9nyi, 1959). The experiments are divided into pairwise cases and higher-order\ncases. We will compare the performance of NEUROLIFTING with LBP, TRBP, and Toulbar2 on\npairwise MRFs. For the higher-order MRF cases, we will compare NEUROLIFTING exclusively\nwith Toulbar2, as LBP and TRBP are not well-suited for handling the complexities inherent in high-\norder MRFs. The raw probabilities (energies) on the edges/cliques are randomly generated using\nthe Potts function (Equation 7), representing two typical types found in the UAI 2022 dataset. The\nparameters a and \u1e9e serve as constant penalty terms and I is the indicator function.\n$$\\theta_{ij} = a\\mathbb{I}(X_i = x_j) + \\beta$$\nFor all the random cases, all the probabilities values of the unary terms and pairwise (clique) terms\nare generated randomly range from 0.2 to 3.0. For the Potts models, \u03b1, \u03b2\u2208 [0.00001, 1000]. Each\nrandom node can select from 2 to 6 possible discrete labels, and the values of the unary terms are\nalso generated randomly, ranging from 0.2 to 3.0. LBP and TRBP are allowed up to 60 iterations,\nwith a damping factor 0.1 to mitigate potential oscillations. Toulbar2 operates in the default mode\nwith time limit 18000s. We employ a 5-layer GNN to model all instances. The learning rate is set\nto le-4, and the model is trained for up to 150 iterations for each instance, utilizing a simple early\nstopping rule with an absolute tolerance of 1e-4 and a patience of 10. The data generation method\nand the parameter settings are the same for both pairwise cases and high order cases.\nPairwise instances. The inference results on pairwise cases are summarized in Table 1. Prefix\n\"P_potts_\" and \"P_random_\" indicate instances generated with Potts energy and random energy, re-\nspectively. It is evident that as the problem size scales up, NEUROLIFTING outperforms the baseline\napproaches; meanwhile, it also achieves comparable solution quality even when the problem sizes\nare small. This trend is consistent across both energy models.\nHigher-order instances. The inference results on high oreder cases are summarized in Table 2. The\n\"H\" in the prefix stands for High-order and all the instances are generated using Potts model. The"}, {"title": "4.2 UAI 2022 INFERENCE COMPETITION DATASETS", "content": "We then evaluate our algorithm using instances from the UAI 2022 Inference Competition datasets,\nincluding both pairwise cases and high-order cases. The time settings will align with those estab-\nlished in the UAI 2022 Inference Competition, specifically 1200 seconds and 3600 seconds. LBP\nand TRBP algorithms are set to run for 30 iterations with a damping factor of 0.1, and the time\nlimit for Toulbar2 is configured to 1200 seconds, which is generally sufficient for convergence. For\nNEUROLIFTING, we utilize an 8-layer GNN to model all instances, with the model trained for up\nto 100 iterations for each instance; other settings remain consistent with those used in the synthetic\nproblems. We also experimented with lifting dimensions of 64, 512, 1024, 4096, and 8192.\nPairwise cases. We evaluate pairwise cases from the UAI MPE dataset. The full results of NEU-\nROLIFTING are detailed in Appendix B. From Table 3, we see that on trivial pairwise cases, where\nToulbar2 successfully identifies the optimal solutions, NEUROLIFTING achieves comparably high-\nquality solutions that are on par with those obtained by LBP and TRBP. In cases where the problems\nbecome more challenging, although NEUROLIFTING does not surpass Toulbar2, it outperforms both\nLBP and TRBP. This suggests that NEUROLIFTING demonstrates improved performance on real-\nworld datasets compared to simpler artificial instances."}, {"title": "4.3 PHYSICAL CELL IDENTITY", "content": "Physical Cell Identity (PCI) is an important parameter used in both LTE (Long-Term Evolution) and\n5G (Fifth Generation) cellular networks. It is a unique identifier assigned to each cell within the\nnetwork to differentiate and distinguish between neighboring cells. We transform PCI instances into\npairwise MRFs, thus all the baselines could be evaluated. Appendix F details how to perform the\ntransformation.\nWe employ an internal real-world PCI data collection along with a synthetic PCI dataset for evalua-\ntion. The configurations for LBP, TRBP, and our proposed NEUROLIFTING approach are consistent\nwith those outlined in Section 4.1. For the Toulbar2 method, a time limit of 3600 seconds is set,\nwhile other parameters remain at their default values. The results are summarized in Table 5. The\nfirst five instances are real-world PCI cases sourced from a city in China, while the latter five in-\nstances are generated. We see for smaller problem instances, Toulbar2 is able to solve them exactly.\nHowever, as the problem scale increases, it becomes increasingly challenging for Toulbar2 to ef-\nfectively explore the solution space, and both LBP and TRBP struggle to converge. In contrast,\nNEUROLIFTING demonstrates strong generalization ability across all scales. Notably, it achieves\ncommendable performance on large scales."}, {"title": "4.4 ANALYSIS AND ABLATION STUDY", "content": "Choice of GNN backbones. We evaluate the model's performance when implemented with dif-\nferent GNN backbones, as classified in Section 3.3. We compare their average performance across\nseveral datasets: pairwise cases from the UAI Inference Competition 2022, real-world PCI instances"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced NEUROLIFTING and its application to solving MAP problems for\nMRFs. Our experiments showed that NEUROLIFTING effectively handles MRFs of varying or-\nders and energy functions, achieving performance on par with established benchmarks, as verified"}, {"title": "A RELATED WORK", "content": "MRF and Inference. In Markov Random Fields (MRFs), the energy function is associated with a\ngraph-structured probability distribution. A key inference challenge is determining the maximum\na posteriori (MAP) configuration. Although minimizing the energy function is NP-hard, advances\nin inference techniques have enhanced model capabilities. For cycle-free graphs, the MAP problem\ncan be effectively addressed using a variant of the min-sum algorithm (Clifford & Hammersley,\n1971; Besag, 1974; Kumar et al., 2005), which extends the Viterbi algorithm (Yedidia et al., 2003)\nto arbitrary cycle-free structures. In graphs with cycles, graph cut methods (Komodakis et al., 2007;\nRoy & Cox, 1998; Boykov et al., 1998; Ishikawa & Geiger, 1998; Szummer et al., 2008) utilize\nmin-cut/max-flow strategies to efficiently minimize energy, although they require MRFs to be graph-\nrepresentable and are unsuitable for multi-labeled MRFs. Two graph-cut-based strategies (Ishikawa,\n2003; Schlesinger & FLACH, 2006) have been developed: the label-reduction method, for specific\nMRFs requiring binary conversion, and the move-making method, influenced by the size of node\nstate combinations.\nThe belief propagation (BP) algorithm (Pearl, 1982; 1988), introduced by Pearl in 1982, is a widely\nused iterative inference method for Bayesian networks, functioning through message passing. How-\never, BP struggles with loops, leading to loopy belief propagation (LBP) (Weiss & Freeman, 2001;\nFelzenszwalb & Huttenlocher, 2004; Frey & Mackay, 2002), which iterates message passing even in\ncyclic graphs. While LBP has shown effectiveness in vision tasks, it lacks guaranteed convergence.\nRecent advancements aim to enhance BP's performance, such as adjusting message significance with\ndiscount factors (Grim & Felzenszwalb, 2023) and constructing hierarchical frameworks for large-\nscale MRFs (Yan et al., 2023). The Junction Tree Algorithm (JTA) (Aji & McEliece, 2000) provides\nexact inference for arbitrary graphs but is NP-hard, limiting its practicality. In pairwise MRFs, inte-\nger linear programming (ILP) formulations yield solutions through tree-reweighted message passing\n(TRBP) (Wainwright et al., 2005), which includes edge-based and tree-based schemes, though they\nlack guaranteed convergence. The sequential TRW-S (Kolmogorov, 2006) scheme achieves weak\ntree agreement, ensuring lower bounds stabilize, but requires substantial time for convergence.\nLifting in Optimization. Lifting techniques have garnered significant attention in the optimiza-\ntion field, particularly in tackling combinatorial problems and enhancing the performance of var-\nious algorithms (Marchand et al., 2002). These techniques involve transforming a problem into a\nhigher-dimensional space, which facilitates more effective representation and solution strategies.\nThey are applied to both mixed 0-1 integer programming problems (Balas et al., 1993) and more\ngeneral mixed-integer programming (MIP) problems in conjunction with primal cutting-plane algo-\nrithms (Dey & Richard, 2008). Additionally, lifting techniques have been integrated with variable\nupper bound constraints in applications such as the Knapsack problem (Shebalov et al., 2015). The\nuse of lifting methods has also extended into robust optimization scenarios (Georghiou et al., 2020;\nBertsimas et al., 2019). Furthermore, combining lifting techniques with Newton's method has shown\npromise in addressing nonlinear optimization problems (NLPs) (Albersmeyer & Diehl, 2010).\nUnsupervised GNNs for Combinatorial Optimization. Graph Neural Networks (GNNs) have\nbeen proved to be powerful in optimization (Yu et al., 2019; Ying et al., 2024) and recent advance-\nments in unsupervised GNNs have demonstrated their effectiveness in tackling combinatorial op-\ntimization problems. By leveraging the structural properties of graph data, unsupervised GNNS\ncan learn meaningful representations of nodes and edges without requiring labeled datasets. It was\nshown that unsupervised GNNs can effectively capture the combinatorial structure inherent in these\nproblems, leading to improved heuristics and solution strategies (Peng et al., 2021). This capabil-\nity is particularly advantageous for problems such as the Traveling Salesman Problem (TSP) (Gaile\net al., 2022; Min et al., 2023), the Vehicle Routing Problem (VRP) (Wu et al., 2024) and Boolean\nsatisfiability problem(SAT) (Cappart et al., 2023), where traditional optimization methods often\nface challenges related to scalability and solution quality. The Max Independent Set (MIS) and\nMax Cut problems can also be solved efficiently in this way (Schuetz et al., 2022). However, the\nloss functions may lack the flexibility to effectively handle higher-order relationships beyond mere\nedges."}, {"title": "F PCI PROBLEM FORMULATION", "content": "The Mixed Integer Programming format of PCI problems is as follows:\n$$\\min_{z,L} \\sum_{(i,j) \\in E} a_{ij} L_{ij}$$\n$$\\text{s.t.} \\quad z_{np} \\in \\{0,1\\}, \\forall n \\in N, p \\in P$$\n$$\\sum_{p \\in P} z_{np} = 1, \\forall n \\in N.$$\n$$\\sum_{P \\in M_{ih}} z_{nip} + \\sum_{P \\in M_{jh}} z_{njp} - 1 \\leq L_{ij}, \\forall (i, j) \\in E, \\forall h \\in \\{0,1,2\\}.$$\nwhere n is the index for devices, and N is the set of these indices. P stands for the possible states\nof each device. Mih stands for the possible states set for node ni. Lij is the cost when given a\ncertain choices of the states of device i and device j, aij is the coefficient of the cost in the objective\nfunction. There is an (i, j) \u2208 E means there exists interference between these two devices.\nWhen using MRF to model PCI problems, each random variable represent the identity state of the\ngiven node and the interference between devices would be captured by the pairwise energy functions.\nNext we will introduce how to transform the PCI problem from MIP form to MRF form.\nIn the original MIP formulation of the PCI problems, three types of constraints are defined. By\ncombining Equation 8a and Equation 8b, we establish that each device must select exactly one state\nat any given time. Furthermore, the constraint in Equation 8c indicates that interference occurs\nbetween two devices only when they select specific states. The overall impact on the system is\ngoverned by the value of Lij and its corresponding coefficient. Given that interference is always\npresent, the objective is to minimize its extent.\nTo transform these problems into an MRF framework, we utilize Equation 8b to represent the nodes,\nwhere each instance of Equation 8a corresponds to the discrete states of a specific node. The con-\nstraints set forth in Equation 8a and Equation 8b ensure that only one state can be selected at any\ngiven time, thus satisfying those conditions automatically. By processing Equation 8c, we can iden-\ntify the edges and their associated energies. Ifznip and Znjp appear in the same constraint from\nEquation 8c, we can formulate an edge (i, j). By selecting different values for Znip and Znjp, we\ncan determine the minimum value of Lij that maintains the validity of the constraint.\nThe product of Lij and aij represents the energy associated with the edge (i, j) under the combina-\ntion of the respective states. Once the states of all nodes are fixed, the values of the edge costs also\nbecome fixed. This leads to the conclusion that the objective function is the summation of the ener-\ngies across all edges. Since the PCI problems do not include unary terms, we can omit them during\nthe transformation process. This establishes a clear pathway for converting the MIP formulation\ninto an MRF representation, allowing us to leverage MRF methods for solving the PCI problems\neffectively."}]}