{"title": "A Multivocal Literature Review on Privacy and Fairness\nin Federated Learning", "authors": ["Beatrice Balbierer", "Lukas Heinlein", "Domenique Zipperling", "Niklas K\u00fchl"], "abstract": "Federated Learning presents a way to revolutionize Al applications by\neliminating the necessity for data sharing. Yet, research has shown that infor-\nmation can still be extracted during training, making additional privacy-preserv-\ning measures such as differential privacy imperative. To implement real-world\nfederated learning applications, fairness, ranging from a fair distribution of per-\nformance to non-discriminative behavior, must be considered. Particularly in\nhigh-risk applications (e.g. healthcare), avoiding the repetition of past discrimi-\nnatory errors is paramount. As recent research has demonstrated an inherent ten-\nsion between privacy and fairness, we conduct a multivocal literature review to\nexamine the current methods to integrate privacy and fairness in federated learn-\ning. Our analyses illustrate that the relationship between privacy and fairness has\nbeen neglected, posing a critical risk for real-world applications. We highlight\nthe need to explore the relationship between privacy, fairness, and performance,\nadvocating for the creation of integrated federated learning frameworks.", "sections": [{"title": "1 Introduction", "content": "In today's digital age, the increasing availability of data offers unprecedented opportu-\nnities for innovation, particularly for machine learning (ML) (Courville et al. 2018).\nHowever, utilization remains a challenge as data is usually not stored centrally, but\ndistributed across data owners, i.e., clients, like edge devices or organizations (Zheng\net al. 2023; Wang et al. 2022; Hosseini et al. 2023). Yet, storing data centrally is infea-\nsible due to privacy and intellectual property (IP) exposure risks (Wieringa et al. 2021).\nFederated Learning (FL) can inherently mitigate these privacy risks by avoiding the\ncentralization of sensitive data (Mothukuri et al. 2021; Wen et al. 2023). To harness all\ndata, clients utilize their data to train an ML model locally and only transfer model\nupdates to a central server, aggregating these updates to a final model (McMahan et al.,\n2016). By not sharing raw data, FL has opened up new possibilities for ML applications\nfor edge devices (Saylam und \u0130ncel 2023) or more sensitive domains such as healthcare\n(Joshi et al. 2022). Although the centralization of data is avoided, the risk of infor-\nmation leakage remains as shared updates can be analyzed to extract information about\nthe training data, thereby posing a privacy and/or IP exposure risk (Wei et al. 2020;\nZhang et al. 2020). Additionally, aggregating multiple local model updates into a single\nfinal model that is then utilized by all clients can lead to unequal performance across\nclients or disparate outcomes for underrepresented social groups (Su et al. 2024) This\nis a common issue when dealing with machine learning models based on historical data,\noften referred to as algorithmic fairness (Ledford 2019; Shi et al. 2021). Applications\nof FL in healthcare illustrate the importance of privacy and fairness quite vividly as\npatient data, due to its sensitive nature, must be kept private. Additionally, the applica-\ntion's performance, e.g., detection of skin cancer (Kumar et al. 2024), should neither\nvary across hospitals nor ethnic groups as it is not the case among physicians (Fahmy\net al. 2023). Unfortunately, achieving both objectives simultaneously is not trivial and\nmeasures to ensure privacy may negatively affect efforts to ensure fairness (Gu et al.\n2022). While privacy has been a focus within FL research for quite some time, less\nfocus has been on fairness demonstrated by the lack of fairness aspects in recent surveys\nregarding current challenges in FL (Guo et al. 2024; Guendouzi et al. 2023; Wen et al.\n2023).\nWith our research, we want to explore current methods that enhance privacy or foster\nfairness in FL while especially focusing on efforts to fuse the two as they contribute to\nthe development of FL systems that are both technologically sophisticated and ethically\nsound. To evaluate the current methods, we will explore the following research ques-\ntion (RQ):\nWhat are current methods that ensure privacy and fairness in federated\nlearning independently and together?\nUsing a multivocal literature review (MLR), based on Kitchenham und Charters\n(2007) guidelines for conducting a structured literature review, extended by the inclu-\nsion of gray literature as suggested by Garousi et al. (2019), we assess current methods\nfor privacy and fairness in FL. Choosing an MLR instead of a strategic literature ap-\nproach allows us to capture literature that is not formally published and thereby might\ncapture emerging research topics, which is especially relevant when analyzing the cur-\nrent body of knowledge in fast-evolving fields like ML and FL.\nOur findings highlight that research is centered around avoiding data leakage, and\nensuring data integrity while fostering fairness regarding performance across clients\n(client fairness) and social groups (group fairness). The identified literature demon-\nstrates that the relationship between privacy and fairness has been neglected. Further-\nmore, we find that over half of the identified literature does not focus on certain do-\nmains or explicit applications but rather on developing new methods. Therefore, the\nresults of our work encourage and call for research at the intersection of privacy and\nfairness with a focus on real-world applications."}, {"title": "2 Conceptual Background", "content": "The conceptual background provides a basic understanding of FL and its relation to\nprivacy and fairness. Therefore, this chapter explores the mechanisms of FL, while de-\nfining the terms of privacy and fairness more closely.\nFL is an innovative paradigm for enabling decentralized training of ML models\n(McMahan et al. 2016). Training takes place on distributed devices or clients, eliminat-\ning the need for centralized storage of raw data. Thereby, FL protects privacy as the\ndata remains on individual clients contributing to the improvement of the model by\nsharing model updates (Mothukuri et al. 2021). As a result, FL facilitates the cross-\ncompany use of AI and improves data access without disclosing personal data, which\nis crucial for exploiting the full potential of AI in the industrial sector (Feuerriegel et\nal. 2024). This not only improves data protection but also enables flexible adaptation\nto the different operating conditions of different companies. The typical process in-\ncludes initializing a global model, distribution to devices or clients, independent train-\ning with local data, aggregation of updates, and iterative refinement of the model. FL\nis used in areas such as healthcare, finance, and edge computing to address privacy\nconcerns while enabling collaborative model building. This approach represents a tech-\nnical advance and marks a shift toward decentralized, privacy-centric ML methods\n(Rieke et al. 2020). Although FL avoids sharing raw data, privacy concerns are not\nmuted. Privacy within information systems is usually connected to information and\ndefinitions \"typically include some form of control over the potential secondary uses\nof one's personal information\" (B\u00e9langer und Crossler 2011). Various attacks such as\nmembership inference attacks can affect the control over potential use by extracting\ninformation about the training data based on the shared model updates, thereby com-\npromising privacy (Zhang et al. 2020). This is often referred to as data leakage (Wang\net al. 2019; Ren et al. 2022a).\nWhile privacy has been a focus within FL research for quite some time, less focus\nhas been on fairness which is demonstrated by the lack of fairness aspect in a recent\nsurvey regarding the current state and challenges in FL (Guo et al. 2024; Guendouzi et\nal. 2023; Wen et al. 2023). Yet, fairness is crucial when designing inclusive and effi-\ncient FL applications (Shi et al. 2021). To address this challenge, Su et al. (2023) em-\nphasize the importance of two key dimensions of fairness: group and client fairness.\nClient fairness focuses on equal distribution of performance and fair compensation for\nclients' contribution of data or computational resources. Group fairness refers to debates\nabout algorithmic fairness and the mitigation of biases towards underrepresented social\ngroups."}, {"title": "3 Method", "content": "Our study aims to provide a comprehensive overview of the current methods to ensure\nfairness and privacy in FL applications and to identify future research directions. There-\nfore, we conducted a multivocal literature review (MLR) following Kitchenham und\nCharters (2007) guidelines for academic literature (AL), supplemented by the inclusion\nof gray literature (GL) to ensure a more holistic perspective by including not formally\npublished literature. As recommended by (Garousi et al. 2019) including gray literature\nis especially useful to identify emerging research topics. As the field of FL and ML in\ngeneral evolves rapidly, including gray literature is important to capture all relevant\ntopics. Choosing an MLR facilitates the identification of a significant number of rele-\nvant articles and the development of a concise dataset, supporting the identification of\nfurther research needs.\nFor AL, we screened eight databases covering various interdisciplinary fields using\nthe search string, \"Federated Learning\" AND (Priva* OR Fair*), yielding 2,746 hits.\nWe defined inclusion and exclusion criteria (Table 1) and performed a two-step title\nscreening, assessing the relevance and contextual contribution of the articles individu-\nally and then collectively. In addition, a stop criterion was defined to include only the\nfirst eight pages of a database if it does not offer an export function. To enhance the\nrelevance of the publications in our database, we analyzed the citation rates of the arti-\ncles using the importance of citations as described by Kladroba et al. (2021). Specifi-\ncally, for articles published until 2022, a minimum of three citations is required, while\nworks from 2023 must have at least one citation.\nThe papers underwent relevance evaluation by the author team using a 1-to-5 Likert\nscale, where 5 indicates high relevance, 3 denotes neutrality, and 1 reflects irrelevance.\nA rating of 5 was assigned to papers that made a direct and significant contribution to\nthe field of research. A rating of 4 implied that the paper contained relevant information\nbut was less central. A score of 3 indicated general relevance but without direct contri-\nbution to the core of the research. Only papers scoring 4 or above were included in the\ndataset. Any disagreements were resolved through discussion, leading to 62 relevant\nAL items. Regarding the GL, the identical search terms and stop criterion were used in\nGoogle, Google Scholar, and arXiv to ensure consistency in our methodology. We\nchoose the sources in alignment with the methodology and other papers performing an\nMLR (Paez 2017). The preliminary search yielded 488 GL items of GL. However, these\nwere subject to a rigorous evaluation process, employing the same inclusion and exclu-\nsion criteria used for AL. Following this evaluation, only eleven GL items were deemed\nrelevant and of sufficient quality to be included in the final dataset. Furthermore, snow-\nballing was performed and evaluated using the previously defined criteria, with the ob-\nject of exhausting all literature sources. This yielded 38 additional relevant papers,\nbringing the total number of items in our final dataset to 133 (see Figure 1).\nOur research aims to examine current methods in the area of privacy and fairness in\nthe context of FL. To this end, a two-stage analysis process was undertaken to provide\na comprehensive overview of these issues. The first phase focused on privacy in the\ncontext of FL, followed by a detailed consideration of fairness approaches in this area.\nEach section of the analysis presents the technologies identified and their interrelation-\nships with other technologies. The goal is to provide a comprehensive overview of the\nmost common approaches and techniques, thereby deepening the understanding of\nthese key areas."}, {"title": "4 Results", "content": "To capture the current state of research on fairness and privacy in FL, we examine the\ncharacteristics of the identified literature. We analyze the distribution concerning liter-\nature classification and investigate specific sub-dimensions of fairness and privacy. Our\ngoal is to pinpoint where recent research has concentrated its efforts and thereby iden-\ntify which dimensions, sub-dimensions, and cross-dimensional inquiries have been ne-\nglected.\n4.1 Characteristics of the included publications.\nA review of eleven databases was conducted to identify relevant literature on privacy\nand fairness. A total of 45 significant papers were found in Web of Science, 15 in IEEE\nXplore, 8 in ScienceDirect, 7 in ACM Digital Library, 5 in SpringerLink, and 4 in\nWiley Online Library. ArXiv had the highest number of publications in the field of GL\nwith 24 entries, followed by Google Scholar (23) and Google (2). Analyzing the the-\nmatic distribution of articles, there is a strong overrepresentation in the privacy dimen-\nsion, with 98 articles addressing privacy concerns and only 24 articles discussing fair-\nness issues. Privacy research is predominantly associated with AL (70%), while fair-\nness research is almost equally split between AL and GL. Only 11 papers examine both\nprivacy and fairness simultaneously (four from AL and seven from GL). We identified\ntwo sub-dimensions for each dimension: data leakage and data integrity for privacy,\nand client and group fairness for fairness. Data leakage has attracted the most attention,\nwith 72 papers addressing it alone and another 35 in combination with data integrity.\nData integrity specifically is addressed in two papers. Fairness research focuses on em-\nphasizing client fairness (23 papers) while group fairness is addressed in eight papers.\nFour papers address both dimensions of fairness. Cross-dimensional research is scarce,\nwith eleven articles addressing at least one privacy and one fairness dimension simul-\ntaneously. No literature was identified that covers both dimensions of privacy and both\ndimensions of fairness simultaneously.\n4.2 Overview of methods to address privacy and fairness in federated learning\nAfter describing the characteristics of the included literature, we will overview different\nmethods to ensure privacy or fairness, including brief descriptions. We will also illus-\ntrate how these methods are combined to achieve various objectives within or across\ndimensions. Figure 3 shows the number of papers focusing on one or both dimensions\nand the specific sub-dimensions they address. Additionally, the figure provides insight\ninto the applications: the first number shows the papers that are domain-unspecific, and\nthe second shows those focusing on domain-specific applications. A detailed descrip-\ntion of the domains is provided in chapter 4.3.\nMethods used to ensure privacy. Preventing data leaks and maintaining system integ-\nrity are crucial challenges in the field of FL. A variety of different methods are used to\nprotect user data from disclosure by anonymization or encrypting. The most common\ntechniques include Differential Privacy (DP) with 56 papers, Homomorphic Encryption\n(HE) with 34 papers, and Secure Multi-Party Computation (SMPC) with 17 papers. DP\nadds noise to the actual dataset thereby enabling the extraction of aggregated or static\ninformation from a data set while protecting the privacy of individuals (Li et al. 2023)\naddressing privacy concerns in industries like healthcare. Exemplarily, DP is employed\nin Label DP (Tang et al. 2023) or combination with stochastic gradient descent for se-\ncure aggregation. Label DP is used in classification tasks obscuring the direct link be-\ntween features and their labels while DP for secure aggregation preserves the privacy\nof model updates when shared with a server for model aggregation (Ghazi et al. 2021).\nHE prevents data leakage by enabling computations to be performed directly on en-\ncrypted data without prior decryption or reduction in the liability of the result (Lloret-\nTalavera et al. 2021). 17 papers are identified combining DP with HE providing two\nlevels of privacy: a user and a model-based level. Another approach is to combine HE\nwith secret sharing (Shi et al. 2023). Secret sharing divides information into several\nparts, called shares. The shares themselves do not reveal any insights and the infor-\nmation can only be reconstructed by combining a certain number of shares. Thereby,\ngreater flexibility and scalability are offered as new shares can be added and removed\nwithout changing the entire system while increasing data security. SMPC enables mul-\ntiple users to pool their data and perform computations without disclosing their private\ninformation to each other while cryptographic techniques ensure the correctness of the\nfinal result (Zhu 2020). All identified papers consistently use SMPC in conjunction\nwith DP enhancing both privacy preservation and data usability. SMPC enables secure\ncomputation, while DP adds noise to the data to ensure user anonymity (Abaoud et al.\n2023). Another layer of security can be provided by applying HE to a combination of\nSMPC and DP, as introduced by seven identified papers. The combination of these\nthree methods results in a robust data protection system that ensures the secure ex-\nchange and analysis of information while maintaining privacy (Kadhe et al. 2023).\nYang et al. (2023) and Maurya und Prakash (2023) introduce specialized approaches\nthat use a particular form of SMPC called secure aggregation to complement the appli-\ncation of DP and HE.\nSecure aggregation is specifically designed for the aggregation of FL systems, while\nSMPC has a much broader application area (Bonawitz et al. 2016). Smahi et al. (2023)\npropose an approach that emphasizes privacy protection to ensure data integrity. The\napproach primarily focuses on using FL to protect privacy while utilizing BC and zero-\nknowledge proofs (ZKPs) as sub-elements. Thereby, model updates are stored securely\nin an immutable and transparent manner while keeping the actual data confidential. BC-\nbased systems therefore generally represent a technological advance, as these decen-\ntralized ledger technologies contribute significantly to the transparency and reliability\nof digital transactions and foster trust in decentralized processes (Hayati et al. 2022).\nOur analysis of the literature shows that BC, complemented by other techniques, is\noftentimes utilized to address data leakage as well as data integrity (Kalapaaking et al.\n2023). The Federated Visual Learning System, known as FedVisionBC, mitigates is-\nsues such as single point of failure, model poisoning, and membership inference attacks\n(Zhang et al. 2023b). Consensus protocols such as Delegated Proof of Stake and Proof\nof Authority strengthen the security mechanisms and trustworthiness of decentralized\nsystems (Fang et al. 2022, Ullah et al. 2023). Recent advancements in secure FL for\nIoT and edge computing include the integration of Learning with Errors encryption and\nConsistent Hashing Algorithm for secure data computations (Ren et al. 2022b) and a\nmodel parameter aggregation protocol that ensures data privacy on user devices intro-\nduced by Eltaras et al. (2023) Verifiable Privacy-Preserving Federated Learning for\nedge computing systems uses HE and combines Distributed Selective Stochastic Gra-\ndient Descent with the Paillier cryptosystem to achieve efficient distributed encryption\n(Zhang et al. 2023a). Dynamic Successive Verification Mechanisms use HE thereby\nenhancing security and privacy through dynamic verification and encryption (Gao et\nal. 2023a). This protocol, based on Boneh-Lynn-Shacham signatures and multi-party\nsecurity, verifies the integrity of client-uploaded parameters and the correctness of\nserver-aggregated results. Additionally, approximate homomorphic Cheon-Kim-Kim-\nSong encryption protects client data privacy (Ma et al. 2022).\nMethods used to achieve fairness. In the area of FL, there are a variety of research\nefforts to improve both fairness and accuracy. A wide range of innovative approaches\nand technologies are being explored, including developing personalized models, inno-\nvative aggregation methods, and asynchronous mechanisms. The papers mainly present\nuse cases related to inter-hospital collaboration and learning from histopathology im-\nages (Hosseini et al. 2023, Li et al. 2019, Zhao und Joshi 2022). Client fairness in FL\naims to evenly distribute benefits among clients, focusing on equitable performance\ndistribution, resource-driven participation, collaboration incentives, trust assessment,\nand reducing bias through algorithmic fairness. A key approach involves crafting per-\nsonalized models tailored to the unique needs and data profiles of each client, essential\nfor fostering more equitable and impactful outcomes. Innovative aggregation methods,\nsuch as the use of double momentum gradients, can enable more efficient and equitable\nintegration of contributions from various clients (Huang et al. 2022). Asynchronous\nmechanisms address the variability in client availability and computing power by al-\nlowing clients to submit updates on their schedules without the need for synchronized\ncommunication. This approach leads to more efficient resource utilization and ensures\nfairer participation among clients (Gu und Zhang 2023). Another approach uses incen-\ntivization mechanisms, such as reverse auctions and trust assessments, to promote effi-\nciency and ensure a fair distribution of resources (Pan et al. 2023). Proportional Fair\nFL adjusts weights dynamically addressing the challenge of unequal starting conditions\nof clients (Hosseini et al. 2023). Multi-gradient descent and adaptive accuracy control\nmechanisms enhance the efficiency and fairness of the methods used by allowing mod-\nels to learn in different directions and dynamically adjust learning rates and other pa-\nrameters to achieve optimal results under varying conditions (Fu et al. 2020).\nGroup fairness is addressed by five articles from GL and one from AL introducing\nsix different approaches: FedGFT (Wang et al. 2023), cCFLVvis (Huang et al. 2023),\nFair-Fate (Salazar et al.), multiparty calculation (Pentyala et al. 2022), FedMinMax\n(Papadaki et al. 2022), and FedFB (Zeng et al. 2021). These approaches aim to reduce\nsystematic discrimination towards certain population groups. Fair-Fate adjusts the\nweighting of clients based on their contribution to the overall model, providing a fairer\ndistribution of performance results across subgroups defined by sensitive characteris-\ntics (Salazar et al.). In contrast, FedGFT is designed to achieve global fairness by reg-\nulating an objective function that balances individual performance and collective ben-\nefits (Wang et al. 2023). This approach promotes demographic fairness and mitigates\nthe Matthew effect (Gao et al. 2023b). Techniques such as dynamic reweighting and\nadaptive accuracy checks can promote a more even distribution of performance, creat-\ning a more inclusive and equitable learning environment (Zhao und Joshi 2022). Two\nidentified papers cover both, client and group fairness. Fair Hypernetworks (Carey et\nal. 2022) adjust the parameters of a target network based on various fairness criteria.\nThey are trained to minimize discrimination and promote fair decisions between differ-\nent groups, aiming to balance model performance and fairness (Qu et al. 2022). Dy-\nnamic Q Fairness Federated Learning Algorithm with Reinforcement Learning (Chen\net al. 2023) combines FL with reinforcement learning methods to improve both fairness\nand efficiency. The algorithm uses a dynamic Q-function to make optimal decisions for\nmodel updates and resource allocations, while continuously adapting to changing con-\nditions (Cao et al. 2024). This maximizes the overall performance of the federated\nlearning system while ensuring that all participating devices are treated fairly (Woo et\nal. 2023).\nMethods to address privacy and fairness. Eleven papers were identified that con-\nsider both privacy and fairness in their analysis. The papers present a variety of methods\nto improve fairness, privacy, and efficiency in FL, covering the use of cryptographic\nmethods, as well as the development of specific algorithms and frameworks. You et al.\n(2023) and Ratnayake et al. (2023) cover data leakage and client fairness. While\nRatnayake et al. (2023) present a review that examines and extends the existing taxon-\nomy of fairness approaches in FL and analyzes the implementations of different meth-\nodological groups, the other three papers discuss distinct methodological approaches.\nYou et al. (2023) introduce the FedACC framework, a server-led approach for control-\nling the global model accuracy. It ensures the validity of client gradients, and measures\nthe cumulative contributions of new clients, while only granting access to a model if\nthe accuracy matches their contributions. Utilizing differential privacy adds a layer of\nprivacy. Annapareddy et al. (2023) address both fairness dimensions (group and client\nfairness) while additionally considering data leakage by introducing the FedFa using a\ndouble momentum mechanism to process non-IID data. The server aggregates client\ninformation by considering both historical gradient information and quantity infor-\nmation about client accuracy and participation frequency. An appropriate weight is de-\ntermined for each client based on this information. Furthermore, R\u00fcckel et al. (2022)\ncover both dimensions of privacy (data leakage and integrity) while addressing client\nfairness by introducing a BC-based architecture for FL systems that combines ZKPs,\nlocal DP, and BC to create a fair, and manipulation-proof FL environment. This system\nallows clients to verify the integrity of each other's model training, promoting fair com-\npensation. The novel use of ZKPs ensures both accurate model inferences and fairness\nthroughout the training process.\n4.3 Overview of application domains and technology-focused research\nWhile identifying methods to ensure group and client fairness, and data integrity, and\nmitigate data leakage is important, it's crucial to analyze their application to specific\nuse cases. About 60% of the literature focuses on theoretical and initial empirical evi-\ndence without specifying an explicit domain. The applied research is mainly centered\naround healthcare, IoT, and edge devices. The aggregated data shows that Healthcare\n(8 privacy, 1 fairness, 1 both), IoT & Edge Computing (15 privacy, 0 fairness, 2 both),\nand Big Data & Cloud Computing (6 privacy, 2 fairness) are frequently discussed in\nterms of privacy concerns. Privacy research is almost equally divided between domain-\nspecific and domain-unspecific studies, while fairness research is mostly domain-un-\nspecific. Categories such as Cyber-Physical & Autonomous Systems (3 privacy, 1\nboth), Finance (4 privacy, 1 both), Industry & Communication (7 privacy), and Energy\nSector (1 fairness) show broader applications. Mobility & Traffic (2 privacy) and Mul-\ntimedia & Visual Detection (4 privacy) also focus more on privacy. This indicates a"}, {"title": "5 Discussion", "content": "Using a multivocal literature review, we identified 133 papers surrounding privacy\nand/or fairness in federated learning. The results show a clear focus of research on pri-\nvacy (98) and demonstrate the lack of focus on fusing privacy and fairness concerns as\nonly 11 papers were identified that look at (minimum) one privacy and one fairness\ndimension simultaneously. When it comes to privacy, there were three core methods\nidentified: differential privacy, homomorphic encryption, and secure multi-party com-\nputation. Interestingly, the identified research regarding privacy also focuses on secu-\nrity with an emphasis on the integrity of data (35 papers). Here, one or multiple men-\ntioned technologies are combined with blockchain technology. Research on fairness\n(24 papers) is mostly centered around client fairness (16 papers) while six papers focus\non group fairness and only four papers consider both dimensions. Additionally, almost\nhalf of the research on privacy focuses more on the implementation of methods in real-\nworld applications. In contrast, only 17% of research regarding fairness focuses on spe-\ncific domains. Especially research on group fairness has no connection to specific do-\nmains, indicating that research on (group) fairness in FL is mostly abstract and still in\nits infancy as literature focuses on the development of methods rather than their imple-\nmentation in the real world.\nThe lack of fairness-centered research, especially group fairness, is concerning in\nlight of newly passed legalization like the AI Act (European Comission 2024) calling\nfor the detection and mitigation of biases during the development and deployment of\nmachine learning-based applications. Furthermore, as current legislation on privacy\nlaws like the General Data Protection Regulation (GDPR) (General Data Protection\nRegulation. European Council, 2024) put high demands on the privacy of data com-\nbined with known trade-offs between dimensions, e.g., anonymization for privacy vs.\nensuring group fairness, research must focus on the interplay between different meth-\nods (Kusner et al. 2017, Dwork et al. 2011). Therefore, we call for the development of\na holistic FL framework considering the dependencies across dimensions.\nTo achieve this goal, we propose the following research directions. Firstly, we call\nfor interdisciplinary research incorporating technical, ethical, legal, and social consid-\nerations into the design and deployment of FL applications. Initial research must extend\nthe knowledge of trade-offs between the dimensions on an empirical level. Inspired by\n(Gu et al, 2022) we propose to analyze the interplay between two sub-dimensions, and\nhow they impact each other. Furthermore, we are interested in how they could influence\nmodel accuracy in a controlled environment. To keep complexity managed, future re-\nsearch should focus first on understanding relationships between two sub-dimensions\nmore deeply. In the next step, research should push toward understanding the depend-\nencies between data leakage, data integrity, client fairness, and group fairness. Addi-\ntionally, research should focus on implementations in real-world scenarios thereby\nleaving the laboratory-like evaluation. Here, a design science research approach could\nhelp identify suitable design requirements, principles, and features which is especially\ntrue for fairness concerns.\nFurthermore, we want to highlight a neglected aspect of the privacy-preservation\ndebate in FL: the right to be forgotten, a key component of data privacy laws. Despite\nsome integration efforts into FL, our findings indicate it has been overlooked in current\ndiscussions. Ignoring such a crucial right could hinder the real-world application of FL.\nAdditionally, we advocate for a life-cycle-centric approach to ensure group fairness.\nFairness must be considered holistically, not just during model training (Deck et al.\n2024). Furthermore, focusing on additional fairness dimensions will help to gain a ho-\nlistic perspective. A prominent candidate could be informational fairness (Schoeffer et\nal. 2022). Lastly, we highlight a managerial challenge in ensuring group fairness in FL.\nGroup fairness relies on a normative definition that influences the choice of fairness\nmetrics and is already challenging in centralized ML applications (Saleiro et al. 2018;\nFranklin et al. 2022). How independent clients, such as organizations or companies, can\nagree on a single definition of fairness and its metric remains a major challenge for FL\nto become practical."}, {"title": "6 Conclusion", "content": "This paper provides a comprehensive overview of the methods currently presented and\ndiscussed for ensuring privacy and fairness in federated learning systems. For this pur-\npose, an extensive multivocal literature review was conducted to highlight recent re-\nsearch efforts, summarize methods to ensure privacy and fairness and identify research\ndirections. Subsequently, we identified sub-dimensions on which privacy and fairness\nresearch has been focused: data leakage, data integrity, client fairness, and group fair-\nness. This was followed by a detailed discussion of methods in each (sub-)dimension\nand whether there have been efforts to combine them. Furthermore, application areas\nare discussed illustrating that the focus of research has been on applying federated\nlearning in healthcare. Here, ensuring privacy and fairness are paramount for real-world\napplications. We discussed the challenges of integrating privacy and fairness in feder-\nated learning to identify further research avenues for future research. Among them is\nmore research in the trade-off between multiple dimensions, implementing them in\nmore real-world scenarios while utilizing the design science research paradigm. Fur-\nthermore, missing components such as unlearning or informational fairness were pro-\nposed to illustrate further research directions. Yet, our research is still limited as it only\nprovided a superficial analysis of methods. In future work, the collected data can be\nused to create a more detailed picture of methods, their precise intentions as well as\ntheir technological readiness. Nevertheless, we hope our findings will serve as a valu-\nable roadmap towards fairer and more private applications for federated learning."}]}