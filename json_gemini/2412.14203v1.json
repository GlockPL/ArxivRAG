{"title": "BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement", "authors": ["Yuhao Dua", "Shunian Chen", "Wenbo Zan", "Peizhao Li", "Mingxuan Wang", "Dingjie Song", "Bo Li", "Yan Hu", "Benyou Wang"], "abstract": "The application of Large Language Models (LLMs) in Computer-Aided Design (CAD) remains an underexplored area, despite their remarkable advancements in other domains. In this paper, we present BlenderLLM, a novel framework for training LLMs specifically for CAD tasks leveraging a self-improvement methodology. To support this, we developed a bespoke training dataset, BlendNet, and introduced a comprehensive evaluation suite, CAD-Bench. Our results reveal that existing models demonstrate significant limitations in generating accurate CAD scripts. However, through minimal instruction-based fine-tuning and iterative self-improvement, BlenderLLM significantly surpasses these models in both functionality and accuracy of CAD script generation. This research establishes a strong foundation for the application of LLMs in CAD while demonstrating the transformative potential of self-improving models in advancing CAD automation. We encourage further exploration and adoption of these methodologies to drive innovation in the field. The dataset, model, benchmark, and source code are publicly available at https://github.com/FreedomIntelligence/BlenderLLM", "sections": [{"title": "1 Introduction", "content": "CAD is extensively used in industries such as automotive, aerospace, manufacturing, and architecture for 3D design (Heesom and Mahdjoubi, 2004; Pottmann et al., 2005; Susic et al., 2017). Despite its widespread application, the effective use of CAD often demands specialized skills and substantial training, making the design process both labor-intensive and time-consuming. Tasks like parameter adjustments and model validation require considerable human effort, leading to increased project costs and slowing down rapid iteration and innovation (Kreis et al., 2020).\nLarge language models (LLMs) have experienced rapid advancements in recent years, par-\nProblem Definition This paper addresses the challenge of reducing the manual workload associated with CAD design by leveraging the capabilities of LLMs. As illustrated in Figure 1, we utilize LLMs to automate the generation of CAD scripts from natural language inputs. These scripts can be executed in Blender to create precise 3D models. By converting user instructions into executable CAD scripts, our approach streamlines the CAD process, thereby alleviating the manual workload for engineers and designers.\nChallenges Although recent work (Kreis et al., 2020; Aarya, 2023; Wu et al., 2023; Zhang et al., 2024) has explored the application of LLM in the CAD field, several significant challenges still hinder its widespread adoption. Firstly, some work is limited by the complexity of input forms, resulting in a high threshold for use. Secondly, there is a notable shortage of high-quality, domain-specific"}, {"title": "2 Related Work", "content": "CAD is a widely used technology in various industries, enabling engineers and designers to create precise digital representations of objects, offering significant advantages in precision, flexibility, and speed. Early efforts leveraged rule-based systems and simple machine learning algorithms to assist in CAD tasks (Chavali et al., 2008). Later, convolutional neural networks were used to convert 2D sketches into 3D models (Li et al., 2020). However, these methods have limitations. Rule-based systems lack flexibility, while machine learning require extensive labeled data and are constrained by their training data's scope (Rapp et al., 2021).\nRecent work has begun to explore how LLMs can be adapted for CAD tasks. For instance, CADGPT (Kapsalis, 2024) directly parses natural language inputs into executable commands for CAD software. BlenderGPT (Aarya, 2023) and 3D-PREMISE (Zeqing et al., 2024) have utilized LLMs like GPT-4 to generate CAD scripts based on natural language prompts. Additionally, CAD-LLM (Wu et al.,"}, {"title": "3 Methodology", "content": "We design and implement a multi-module pipeline for generating high-quality training data for SFT. The pipeline for data construction is illustrated in Figure 2. The multi-module pipeline is composed of three primary components: the Text Module, the Image Module, and the Verification Module. The Text Module generates instructions and their corresponding bpy scripts. The Image Module executes these bpy scripts within Blender to produce images. The Verification Module ensures that the images align with the instructions, thereby validating the data quality.\nTo encompass a broad range of item types, emulate various communication styles (Sims, 2017), and craft instructions with differing levels of complexity, the diversity of the instructions is categorized along three dimensions:\nObject Categories: Objects are classified into 16 categories following the Locarno classification system (Organization, 2013), as detailed in Appendix B.1.1.\nInstruction Types: We employ the Myers-Briggs Type Indicator (MBTI) (Myers, 1985) to create eight distinct tones for instructions, as detailed in Appendix B.1.2.\nComplexity: To manage the complexity of instructions, we vary their length, classifying them into five categories, as detailed in Appendix B.1.3.\nBased on these dimensions, we manually create a set of 135 diverse seed instructions, denoted as $L_{seed} = \\{l_1, l_2, ..., l_{135} \\}$, where $l_i$ denotes the ith natural language instruction. Next, we employ Self-Instruct data distillation techniques (Wang et al., 2022) to expand these seed instructions into a larger dataset. In each iteration of instruction generation, we randomly sample instances from the $L_{seed}$. These sampled instances are used to generate new instructions. Through multiple iterations, this process results in a comprehensive dataset of approximately 50k instructions, denoted as $L_{gen}$.\nThe distribution of both seed instructions $L_{seed}$ and generated instructions $L_{gen}$ by category, type, and length is illustrated in Figure 3. The detailed process is outlined in Appendix B.2.\nWe then utilize GPT-40\u00b9 to generate pairs $(l_j, s_j)$ based on given instructions $l_j$. For each instruction $l_j \\in L_{gen}$, GPT-40 produces a corresponding script $s_j$. The generation process ensures that each script is derived from its instruction, as detailed in Appendix B.4.\nWe render the scripts using Blender to generate corresponding images. For each generated 3D object, four images are captured from different angles to better capture the full view of 3D objects, resulting in $(l_j, I_j)$ pairs, where $I_j = \\{i_{j,1}, i_{j,2}, i_{j,3}, i_{j,4}\\}$ is the set of images.\nWe use GPT-40 as the validator. The model is required to determine whether the images match the instruction based on the given $(l_j, I_j)$ pairs, detailed instruction can be found in Appendix B.5.\nTo verify the reliability of GPT-40 as the validator, we perform manual cross-validation on a portion of the data. We manually validate 10k data points, of which 89.7% produce consistent re-sults with the GPT-40 verification, demonstrating"}, {"title": "3.2 Model Optimization", "content": "The development of BlenderLLM involves a two-phase optimization process: Supervised Fine-tuning (SFT) and Self-improvement.\nWe utilize the aforementioned data to fine-tune the Qwen2.5-Coder-7B-Instruct model, thereby obtaining the BlenderLLM-base, which serves as the base model for the next step's optimization, denoted as $M_0$.\nDue to the limited data, we employed a self-improvement approach, allowing the model to further optimize itself using data it generates. Specifically, we trained a filter with previous data to select high-quality data generated by the model, and then iteratively optimized the model through a cycle of data generation and model training.\nWe utilize BlendNet-Human and BlendNet-GPT as positive examples. 8k samples are selected as negative examples from the remaining $(l_j, s_j)$ pairs. These data are employed to fine-tune the Qwen2-VL-7B-Instruct model, resulting in the Coarse Filter. Combined with GPT-40, which functions as the Fine Filter, they form a Cascade Filter through a cascaded mechanism. Appendix C.2 summarizes the precision of each filter.\nIn the i-th iteration, we generate training data using the model from the previous iteration $M_{i-1}$. Specifically, for each instruction $l_j$, we obtain a script $s_j$ through inference with $M_{i\u22121}$. We denote the generated dataset for iteration i as $D_i = \\{\\{l_j, S_j\\}_i\\}$. These pairs are rigorously filtered using the Cascade Filter $F(l_j, s_j) \u2192 \\{0, 1\\}$ to ensure high-quality data selection, retaining only those pairs for which $F(l_j, s_j) = 1$.\nThe selected high-quality data from the data generation phase is used to fine-tune the model $M_{i-1}$. This process uses the filtered data to update $M_{i-1}$, thereby resulting $M_i$.\nThe process alternates between data generation and model training, creating an iterative approach to model refinement through Self-improvement, until the loss doesn't drop on the validation set. More details can be found in Appendix C."}, {"title": "4 Benchmarking CAD", "content": "In response to the lack of a benchmark for assessing CAD script generation, we develop CADBench, a system designed to quantitatively evaluate this capability utilizing the method of MLLM-as-a-judge (Ge et al., 2024). CADBench comprises 700 meticulously designed instructions, offering a com-"}, {"title": "4.1 Design Principles", "content": "CADBench is developed by the principles of user-centric, comprehensiveness, granularity and reliability.\nTo simultaneously meet the diversity of test cases and align with practical applications, we constructed CADBench-Sim and CADBench-Wild through synthesized data and the collection of real data, respectively. CADBench-Sim provides controlled synthetic data for baseline testing, covering multiple scenarios, while CADBench-Wild offers real-world internet-sourced data to assess the model's practical performance and adaptability.\nThe comprehensive nature of CADBench is driven by the necessity to rigorously evaluate 3D generative models across a wide array of object categories, instruction types, and complexities. By systematically covering all categories defined in Appendices B.1, the benchmark provides a robust and inclusive assessment of model performance and generalizability.\nThe fine-grained evaluation approach of CADBench significantly enhances the benchmark's ability to provide detailed insights into model performance. By incorporating evaluation criteria across three dimensions, as show in Figure 4CADBench ensures that models are thoroughly evaluated on diverse aspects, leading to a deeper understanding of their strengths and weak-"}, {"title": "4.2 CADBench Construction", "content": "CADBench-Sim comprises 500 synthetic samples. To ensure the comprehensiveness of CADBench-Sim, we employed the Text Module from Section 3.1 to generate the instruction data for CADBench-Sim. The resulting distribution is shown in Figure 3.\nCADBench-Wild incorporates 200 real-world 3D modeling questions, sourced from various CAD-"}, {"title": "4.3 Criteria", "content": "Given the open-ended evaluation characteristics of CAD model assessment, we assist GPT-40 in evaluation by providing customized criteria, instead of ground truth, for each test sample. To achieve a comprehensive and detailed assessment, we designed the criteria from top to bottom into 3 major dimensions and 8 minor dimensions, as shown in the Figure 4. After determining the criteria dimensions, we employ GPT-40 to generate a draft criteria for each sample, and thenmanually verify the criteria following the instruction in Appendix G.2, with criteria examples available in the Appendix D.2. The introduction of criteria not only enhances the comprehensiveness of the evaluation but also improves the consistency between model assessment and human evaluation, as mentioned in the next section."}, {"title": "4.4 Evaluation Protocol", "content": "CADBench operates through three distinct stages.\nThe first stage is script generation. Let e represent the one-shot example used to guide the LLM. The LLM generates a bpy script $s = f(l, e)$ based on these instructions and the context. This ensures improved responses and maintains comparability with BlenderLLM's results.\nSecond, the generated script s is executed in Blender to produce a set of rendered images $I = \\{I_1, I_2, I_3, I_4\\}$, where each $I_k$ is a screenshot captured from different angles.\nFinally, these images I along with the script are evaluated by GPT-40 using predefined scoring criteria. For each criterion $c_i$, we define the evaluation function $E(l, I, s, c_i) \u2192 \\{0, 1\\}$, where $E(l, I, s, c_i) = 1$ if the criterion is satisfied and 0 otherwise."}, {"title": "4.5 Evaluation Metrics", "content": "For each model, the final score is calculated by averaging the outputs across all criteria:\n$Score = \\frac{1}{C} \\sum_{C_i \\in C} E(l, I, s, c_i)$\nNote that for some of the criteria, the image input I is empty, while for others, script input s is empty. See Appendix D.4 for more details."}, {"title": "5 Experiments", "content": "We use Qwen2.5-Coder-7B-Instruct as the base model and fine-tune it on BlendNet-Human to obtain the BlenderLLM-base. For subsequent rounds, the input data size is fixed at 2k samples to prevent training data saturation and overfitting. During the SFT, full parameter fine-tuning is applied. Each"}, {"title": "5.3 Main Results", "content": "As shown in Table 2, BlenderLLM achieves SOTA performance across all dimensions in both CADBench-Sim and"}, {"title": "5.4 Analysis and Discussion", "content": "The experimental results demonstrate that BlenderLLM exhibits significant advantages in $attr.$, $spat.$, $inst.$, and $E_{syntax}$. Combining the performance of different models on sub-dimensions, as shown in Appendix I, with the comparison of visualization results presented in Appendix J and Table C, these achievements can be attributed to two key factors. First, the BlendNet enables BlenderLLM to learn a variety of instructions. Also, This comprehensive training helped BlenderLLM develop a deeper understanding of the rationality of object attributes, such as the relative size and position of components, as well as the matching of colors and materials. Second, the Self-improvement training strategy allowed BlenderLLM to continuously learn and adapt, progressively enhancing its spatial reasoning capabilities over iteration."}, {"title": "6 Ablation", "content": "To demonstrate that Self-improvement Training strategy is more effective than conventional iterative training strategy with similar computational resources, we conducte two comparative experiments:"}, {"title": "7 Conclusion", "content": "In this paper, we propose a comprehensive framework that spans from data construction to self-improvement-based SFT model training and benchmark testing. Through this framework, BlenderLLM, has demonstrated superior performance across various metrics compared to mainstream models. Our results highlight the effectiveness of combining Self-improvement with high-quality dataset, leading to significant advancements in model capabilities."}, {"title": "Limitation", "content": "This study has several limitations. First, the data construction and model training primarily focused on basic CAD modeling aspects and did not address more intricate elements, such as material properties, surface treatments, or internal complexity. These factors could influence the model's performance in handling more advanced CAD tasks. Second, our work focused solely on generating CAD scripts from user instructions, without exploring the potential for direct CAD model generation or the integration of multimodal inputs, such as combining user instructions with images. Future research could investigate these avenues to enhance model versatility. Lastly, the model has not been trained for multi-turn dialogues, limiting its ability to engage in more complex, interactive conversations. These limitations highlight key areas for future improvement and expansion of the model's capabilities."}, {"title": "Ethics Statement", "content": "This research involves the development and evaluation of a novel dataset and methodology for applying Large Language Models (LLMs) to Computer-Aided Design (CAD). The study does not involve human subjects, nor does it utilize any personally identifiable information. The research adhere to ethical guidelines regarding data privacy and intellectual property. The authors declare no conflicts of interest related to this work. The datasets and models we provide follow the CC-BY 4.0 License."}, {"title": "B.1.3 Instruction Length", "content": "We set the length of the instruction to enhance the variety. We place instruction into 5 classes regarding to their words count, as L = {VS, S, . . ., E}."}, {"title": "B.7 The Complexity of BlendNet", "content": "we define three key metrics to quantify the complexity of BlendNet:\nThis metric calculates the average complexity per shape, defined as:\n$Parameter \\ Density = \\frac{Parameter \\ #}{Unit \\ #} (1)$\nA higher parameter density indicates that each shape is more parameterized, implying greater irregularity and higher computational complexity. This value reflects how intricately the shapes are defined and how complex the relationships between the parameters are within the 3D model.\nEntropy: Entropy measures the spatial diver-"}, {"title": "as", "content": "$H = - \\sum p_i \\log(p_i) (2)$"}, {"title": "Evaluation Metrics", "content": "The average score for sub-dimension j within dimension k, denoted as $SubDimScore_{k,j}$, is calculated as follows. Here, $N_{kj}$ represents the total number of criteria in sub-dimension j, and $S_{kji}$ is the score for the i-th criterion:\n$SubDimScore_{k,j} = \\frac{1}{N_{kj}} \\sum_{i=1}^{N_{kj}} S_{kji} (3)$\nThe average score for a specific dimension k, denoted as $DimScore_k$, is calculated using Equation 4. In this equation, $N_k$ represents the number of sub-dimensions within dimension k:"}, {"title": "3.1.1 Text Module", "content": "The objective of the text module is to develop diverse instructions and corresponding bpy scripts."}]}