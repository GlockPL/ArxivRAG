{"title": "Adaptive Querying for Reward Learning from Human Feedback", "authors": ["Yashwanthi Anand", "Sandhya Saisubramanian"], "abstract": "Learning from human feedback is a popular approach to train robots to adapt to user preferences and improve safety. Existing approaches typically consider a single querying (interaction) format when seeking human feedback and do not leverage multiple modes of user interaction with a robot. We examine how to learn a penalty function associated with unsafe behaviors, such as side effects, using multiple forms of human feedback, by optimizing the query state and feedback format. Our framework for adaptive feedback selection enables querying for feedback in critical states in the most informative format, while accounting for the cost and probability of receiving feedback in a certain format. We employ an iterative, two-phase approach which first selects critical states for querying, and then uses information gain to select a feedback format for querying across the sampled critical states. Our evaluation in simulation demonstrates the sample efficiency of our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "A key factor affecting an autonomous agent's behavior is its reward function. Due to the complexity of real-world environments and the practical challenges in reward design, agents often operate with incomplete reward functions corresponding to underspecified objectives, which can lead to unintended and undesirable behaviors such as negative side effects (NSEs) [1]-[3]. For example, an indoor robot that optimizes distance to goal may break a vase as a side effect if its reward function does not model the undesirability of breaking a vase while navigating to the goal [4] (Figure 1).\nSeveral prior works have examined learning from various forms of human feedback to improve robot performance, including avoiding side effects [5]-[11]. In many real-world settings, the human can provide feedback in many forms, ranging from binary signals indicating action approval to correcting robot actions, each varying in the granularity of information revealed to the robot and the human effort required to provide it. To efficiently balance the trade-off between seeking feedback in a format that accelerates robot learning and reducing human effort involved, it is beneficial to seek detailed feedback sparingly in certain states and complement it with feedback types that require less human effort in other states. Such an approach could also reduce the sampling biases associated with learning from any one format, thereby improving learning performance [12]. In fact, a recent study indicates that users are generally willing to engage with the robot in more than one feedback format [13]. Existing approaches utilize a single feedback format throughout the learning process and do not support gathering feedback in different formats in different regions of the state space [14], [15]."}, {"title": "II. BACKGROUND", "content": "Markov Decision Processes (MDPs) are a popular framework to model sequential decision making problems. An MDP is defined by the tuple $M = (S, A, T, R, \\gamma)$, where $S$ is the set of states, $A$ is the set of actions, $T(s, a, s')$ is the probability of reaching state $s' \\in S$ after taking an action $a \\in A$ from a state $s \\in S$ and $R(s,a)$ is the reward for taking action $a$ in state $s$. An optimal deterministic policy $\\pi^* : S \\rightarrow A$ is one that maximizes the expected reward. When the objective or reward function is incomplete, even an optimal policy can produce unsafe behaviors such as side effects.\nNegative Side Effects (NSEs) are immediate, undesired, unmodeled effects of an agent's actions on the environment [3], [16], [17]. We focus on NSEs arising due to incomplete reward function [2], which we mitigate by learning a penalty function using human feedback.\nLearning from Human Feedback is a widely used tech- nique to train agents when reward functions are unavailable or incomplete [9], [18], [19], including to improve safety [2], [11], [20]-[23]. Feedback can take various forms such as demonstrations [24], [25], corrections [26]\u2013[28], critiques [2], [5], ranking trajectories [29], or may be implicit in the form of facial expressions and gestures [6], [30]. Existing approaches focus on learning from a single feedback type, limiting learning efficiency. Recent studies consider combinations such as demonstrations and preferences [31], [32], but assume a fixed order and do not scale to multiple formats. Another recent work examines feedback format selection by estimating the human's ability to provide feedback in a certain format [33]. Unlike these approaches, we dynamically select the most informative feedback without any pre-processing.\nThe information gain associated with a feedback quantifies the effect of a feedback in improving the agent's understanding of the underlying reward function, often measured using Kullback-Leibler (KL) Divergence [33], [34], $D_{KL}(P||Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}$ where $P$ is the prior distribution and $Q$ is the posterior distribution after observing evidence."}, {"title": "III. PROBLEM FORMULATION", "content": "Setting: Consider a robot operating in an environment mod- eled as a Markov Decision Process (MDP), using its acquired model $M = (S, A, T, R_T)$. The robot optimizes the com- pletion of its assigned task, which is its primary objective described by reward $R_T$. A primary policy, $\\pi^M$, is an optimal policy for the robot's primary objective.\nAssumption 1. Similar to [2], we assume that the agent's model $M$ has all the necessary information for the robot to successfully complete its assigned task but lacks other superfluous details that are unrelated to the task.\nSince the model is incomplete in ways unrelated to the primary objective, executing the primary policy produces neg- ative side effects (NSEs) that are difficult to identify at design time. Following [2], we define NSEs as immediate, undesired, unmodeled effects of a robot's actions on the environment. We focus on settings where the robot has no prior knowledge about the NSEs of its actions or the underlying true NSE penalty function $R_N$. It learns to avoid NSEs by learning a penalty function $R_N$ from human feedback that is consistent with $R^*_N$.\nWe target settings where the human can provide feedback in multiple ways and the robot can seek feedback in a specific format such as approval or corrections. This represents a significant shift from traditional active learning methods, which typically gather feedback only in a single format [2], [10], [23]. Using the learned $R_N$, the robot computes an NSE-minimizing policy to complete its task by optimizing:\n$R(s,a) = \\theta_1 R_T(s,a) + \\theta_2 R_n(s,a)$, where $\\theta_1$ and $\\theta_2$ are fixed, tunable weights denoting priority over objectives.\nHuman's Feedback Preference Model: The feedback format selection must account for the cost and human preferences in providing feedback in a certain format. The user's feedback preference model is denoted by $D = (F, \\psi, C)$ where,\n$\\cdot$ $F$ is a predefined set of feedback formats the human can provide, such as demonstrations and corrections;\n$\\cdot$ $\\psi : F \\rightarrow [0, 1]$ is the probability of receiving feedback in a format $f$, denoted as $\\psi(f)$; and"}, {"title": "A. Learning RN from multiple forms of feedback", "content": "Since the agent has no prior knowledge about NSEs, it assumes none of its actions produce NSEs. We examine learning an NSE penalty function $R_N$ using the following popular feedback formats and their annotated (richer) versions. In our settings, an action in a state may cause either mild, severe, or no NSEs. In practice, any number of NSE categories can be considered, provided the feedback formats align with them.\nApproval (App): The robot randomly selects $N$ state-action pairs from all possible actions in critical states and queries the human for approval or disapproval. Approved actions are labeled as acceptable, while disapproved actions are labeled as unacceptable.\nAnnotated Approval (Ann. App): An extension of Approval, where the human specifies the NSE severity (or category) for each disapproved action in the critical states.\nCorrections (Corr): The robot performs a trajectory of its primary policy in the critical states, under human supervision. If the robot's action is unacceptable, then the human intervenes with an acceptable action in these states. If all actions in a state lead to NSE, the human specifies an action with the least NSE. When interrupted, the robot assumes all actions except the correction are unacceptable in that state.\nAnnotated Corrections (Ann. Corr): An extension of Corrections, where the human specifies the severity of NSEs caused by the robot's unacceptable action in critical states.\nRank: The robot randomly selects $N$ ranking queries of the form (state, action 1,action 2), by sampling two actions for each critical state. The human selects the safer action among the two options. If both are safe or unsafe, one of them is selected at random. The selected action is marked as acceptable and the other is treated as unacceptable.\nDemo-Action Mismatch (DAM): The human demonstrates a safe action in each critical state, which the robot compares with its policy. All mismatched robot's actions are labeled as unacceptable. Matched actions are labeled as acceptable.\nGaze: In this implicit feedback format, the robot requests to collect gaze data of the user and compares its action outcomes with the gaze positions of the user [10]. Actions with outcomes aligning with the average gaze direction are labeled as acceptable, and unacceptable otherwise.\nNSE Model Learning: We use $l_m$, $l_h$, and $l_a$ to denote labels corresponding to mild, severe and no NSEs respectively. An acceptable action in a state is mapped to label $l_a$, $(s, a) \\rightarrow l_a$, while unacceptable action is mapped to label $l_h$. If the NSE severity of unacceptable actions are known, then actions with mild NSE are mapped to $l_m$ and those with severe NSEs mapped to $l_h$. Mapping feedback to these labels provides a consistent representation of NSE severity for learning under various feedback types."}, {"title": "IV. ADAPTIVE FEEDBACK SELECTION", "content": "Given an agent's decision making model $M$ and the hu- man's feedback preference model $D$, adaptive feedback selec- tion (AFS) enables the agent to query for feedback in critical states in a format that maximizes its information gain.\nLet $p^*$ be the true underlying NSE distribution, unknown to the agent but known to the human. $p^*$ deterministically maps state-action pairs to an NSE severity level (i.e., no NSE, mild NSE or severe NSE). Human feedback, when available, is sampled from this distribution. Let $p \\sim p^*$ denote the distribution of state-action pairs causing NSE of varying severity, aggregated from all feedback received thus far. In other words, $p$ represents the accumulated NSE information known to the agent, based on human feedback. Let $q$ denote the agent's learned NSE distribution, based on all feedback received up to that point i.e., from $p$.\nBelow we describe an approach to select critical states, followed by an approach for feedback format selection, based on the KL divergence between $p$ and $q$."}, {"title": "A. Critical States Selection", "content": "Intuitively, when the budget for querying a human is limited, it is useful to query in states with a high learning gap-the divergence between the agent's knowledge of NSE distribution and the underlying NSE distribution, given feedback data collected so far. States with a high learning gap are called critical states ($\\Omega$) and querying in these states can reduce the learning gap. The learning gap at iteration $t$ is measured as the KL divergence between the information gathered so far ($p_t$) and the agent's learned NSE distribution ($q_{t-1}$):\n$D_{KL}(p_t||q_{t-1})$.\nWe compare $p_t$ with $q_{t-1}$ since we want to identify states where the agent's knowledge of NSE was incorrect, thereby guiding the selection of next batch of critical states. While $D_{KL}(p_t||q_t)$ may seem to be a better choice to guide critical states selection, this measure only shows how well the agent learned using the feedback at t but does not reveal states where the agent was incorrect about NSEs."}, {"title": "B. Feedback Format Selection", "content": "A feedback format $f^*$ that maximizes the information gain is selected to query across $\\Omega$. The information gain of a feedback format $f$ at iteration $t$, for $N = |\\Omega|$ critical states, is calculated using the KL divergence between $p_t$ and $q_t$:\n$V_f = \\frac{1}{\\sum_{s \\in \\Omega} D_{KL}(p_t||q_t^f)} \\cdot I[f = f_h] + V_s \\cdot (1 - I[f = f_h])$.\nwhere, $I[f=f_h]$ is an indicator function that checks whether the format in which the human provided feedback, $f_h$, matches the requested format $f$. If no feedback is provided, the information gain of that format remains unchanged.\nThe following equation is used to select the feedback format $f^*$, accounting for feedback cost and user preferences:\n$f^* = argmax_{f \\in F} \\frac{\\psi(f) \\cdot \\frac{V_f}{\\log{t}} + \\eta}{\\frac{C(f)}{\\sqrt{N}} + \\epsilon}$.\nwhere $\\psi(f)$ is the probability of receiving a feedback in format $f$ and $C(f)$ is the feedback cost, determined using the human preference model $D$. $t$ denotes the current learning iteration, $n_f$ is the number of times $f$ was received, and $\\epsilon$ is a small value added for numeric stability. Note that $f^*$ is the current most informative feedback, based on the formats previously used. This may change as the agent explores and incorporates feedback of other formats. Thus, our feedback selection approach effectively manages the trade-off between selecting feedback formats that were previously used to gather information and examining unexplored formats."}, {"title": "V. EXPERIMENTS", "content": "Baselines (i) Naive Agent: The agent naively executes its pri- mary policy without learning about NSEs, providing an upper bound on the NSE penalty incurred. (ii) Oracle: The agent has complete knowledge about $R_T$ and $R_N$, providing a lower bound on the NSE penalty incurred. (iii) Reward Inference with $\u03b2$ Modeling (RI) [33]: The agent selects a feedback format that maximizes information gain according to the human's inferred rationality $\\beta$.\nDomains, Metrics and Feedback Formats We evaluate the performance of various techniques on four domains in simulation: outdoor navigation, vase, safety-gym's push, and Atari freeway."}, {"title": "VI. CONCLUSION", "content": "The proposed Adaptive Feedback Selection (AFS) facilitates querying a human in different formats in different regions of the state space, to effectively learn a reward function. Our approach uses information gain to identify critical states for querying, and the most informative feedback format to query in these states, while accounting for the cost and uncertainty of receiving feedback in each format. Our empirical evaluations using four domains in simulation demonstrate the effectiveness and sample efficiency of our approach in mitigating avoidable and unavoidable negative side effects (NSEs), based on explicit and implicit feedback formats. In the future, we aim to validate our assumptions and results using user studies and extend our approach to continuous settings."}]}