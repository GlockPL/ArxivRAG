{"title": "PARAMETER-EFFICIENT FINE-TUNING MEDICAL MULTIMODAL LARGE LANGUAGE MODELS FOR MEDICAL VISUAL GROUNDING", "authors": ["Jinlong He", "Pengfei Li", "Gang Liu", "Shenjun Zhong"], "abstract": "Multimodal Large Language Models (MLLMs) inherit the superior text understanding capabilities of LLMs and extend these capabilities to multimodal scenarios. These models achieve excellent results in the general domain of multimodal tasks. However, in the medical domain, the substantial training costs and the requirement for extensive medical data pose challenges to the development of medical MLLMs. Furthermore, due to the free-text form of answers, tasks such as visual grounding that need to produce output in a prescribed form become difficult for MLLMs. So far, there have been no medical MLLMs works in medical visual grounding area. For the medical vision grounding task, which involves identifying locations in medical images based on short text descriptions, we propose Parameter-efficient Fine-tuning medical multimodal large language models for Medcial Visual Grounding (PFMVG). \u03a4\u03bf validate the performance of the model, we evaluate it on a public benchmark dataset for medical visual grounding, where it achieves competitive results, and significantly outperforming GPT-4v. Our code will be open sourced after peer review.", "sections": [{"title": "1. INTRODUCTION", "content": "Medical visual grounding associates regions of interest (ROIs) in medical images with corresponding textual descriptions through the analysis of medical images. This is a challenging task requiring comprehensive understanding and precise alignment of visual and textual modalities. Integrating this technology into clinical workflows enhances the interpretation of medical findings within images, aiding physicians in swiftly identifying key information, accurately localizing diseases, and facilitating precise clinical decisions. Despite its importance, medical visual grounding remains underexplored. Existing methods primarily utilize vision-language pre-training (VLP) models [1, 2], learning general representations by globally aligning image and text features via contrastive learning, followed by fine-tuning on medical visual grounding tasks [3, 4]. However, these approaches often lack semantic granularity, neglecting crucial local image features. To enhance semantic understanding, some studies align textual words with local image regions [5, 6]. However, word-level alignment is constrained by contextual variability, hindering accurate capture of pathological descriptions. To overcome this limitation, Yang et al. [7] proposed a framework combining global and local contrastive learning, it leverages global features while precisely aligning sentence-level features with local image regions.\nRecent advancements in Multimodal LLMs (MLLMs) [8\u201310], have significantly enhanced foundational model capabilities. These models facilitate interactive image-based communication, demonstrating exceptional performance in visual tasks and complex content understanding and generation [11, 12]. Inspired by MLLMs, we posit that an optimal solution for medical visual grounding lies within their framework. MLLMs deeply integrate cross-modal (image and text) information, improving medical knowledge representation and reasoning. They can identify subtle image details, such as lesion characteristics, and comprehend complex textual descriptions like pathology reports. In contrast, although VLP models process visual and linguistic data, their cross-modal understanding and reasoning are less profound than MLLMs, particularly with specialized medical images and terminology.\nTraining a medical MLLM from scratch is resource-intensive due to the need for comprehensive medical knowledge coverage, large specialized datasets, and significant computational resources. To circumvent these challenges, we propose PFMVG, a framework that leverages a foundational MLLM with Parameter-Efficient Fine-Tuning (PEFT) techniques for medical visual grounding. Specifically, we fine-tune the pre-trained weights of MiniGPT-v2 [12], adapting it to medical-specific content to align medical textual and visual knowledge, followed by task-specific fine-tuning for medical visual grounding. Our resource-efficient strategy involves freezing certain model layers, such as the visual encoder, and updating critical layers like low-rank adaptation (LoRA) layers [13] for the LLM and linear projection lay-"}, {"title": "2. METHODOLOGY", "content": "PFMVG features a ViT for image encoding and a LLM for image-text embeddings processing and response generation. The ViT is connected to a trainable linear projection layer. Specifically, we use the pre-trained ViT-G model from EVA [14], which remains frozen during both fine-tuning stages. To reduce computational resources, we shorten the length of visual embeddings. By concatenating every four consecutive tokens into a single embedding, we reduce the number of visual input tokens by a factor of four. These concatenated embeddings are then fed into a trainable linear projection layer that maps them to the LLM. In this phase, the projected visual embeddings serve as the <ImageFeature> component within our multimodal text input template. Combined with the question, they are input into the LLM to generate responses. We utilize the open-source, pre-trained LLaMA2-Chat (7B) as our LLM, employing LoRAfor efficient fine-tuning by updating only the LoRA parameters while keeping the rest of the model frozen."}, {"title": "2.2. Two Stages Fine-tuning and Multimodel Instruction Template", "content": "To enhance the MLLM's adherence to medical visual grounding instructions, we implement a two-stage fine-tuning process using pre-trained MiniGPT-v2 weights. Stage 1 focuses on image captioning to facilitate the MLLM's acquisition of multimodal medical knowledge. Stage 2 then advances the model's proficiency in medical visual grounding. In both stages, we employ the Llama-2 conversation template as the multimodal instruction template: [INST]<Img><Image Feature></Img>[Task Identifier][Instruction][/INST]. [Task Identifier] denotes the task type, [INST] and [/INST] represent the user and assistant roles, respectively, and <Image-Feature> contains the image embeddings encoded by the ViT.\nStage 1 fine-tuning: medical image-text knowledge learning. In stage 1 fine-tuning, we freeze all model parameters except the linear projection layer and the LoRA parameters. We fine-tune the model using three medical image-text datasets, ROCO [15], CLEF2022 [16], and MIMIC-CXR [17], to enhance its understanding of medical multimodal contexts. We employ a multimodal instruction template for the image captioning task: [INST]<Img><ImageFeature> </Img>[caption] [Instruction] [/INST]. In this template, [Task Identifier] is replaced with [caption], and [Instruction] is a randomly selected question from an instruction pool.\nStage 2 fine-tuning: medical visual grounding. In stage 2, similar to stage 1, we keep the ViT frozen while updating the linear projection layer and LoRA parameters. To enhance the model's proficiency in medical visual grounding, we train it on the MS-CXR dataset [6]. We employ a specialized instruction template for fine-tuning the visual grounding task: [INST]<Img><ImageFeature></Img>[refer] [Instruction] [label text] [/INST]. In this template, [label text] complements [Instruction], the [Instruction] is derived from an instruction pool of visual grounding questions. To improve performance and robustness, we design multiple instructions that express the same meaning. [label text] is derived from the pathological descriptions corresponding to the medical images in the MS-CXR dataset. [Instruction] combined with [label text] formulates a complete question regarding the visual grounding of the relevant medical image.\nDuring stage 2 fine-tuning, bounding boxes labeled by medical experts are converted into coordinates expressed in a specific textual format: {< $X_{left}$ >< $Y_{top}$ >< $X_{right}$ >< $Y_{bottom}$ >}. < $X_{left}$ > and < $Y_{top}$ > denote the coordinates of the bounding box's top-left corner, < $X_{right}$ > and < $Y_{bottom}$ > indicate the bottom-right corner's coordinates. These coordinates are normalized to the range [0, 100]. During validation, the model-generated coordinates are inversely transformed back to the original image resolution to ensure precise medical visual grounding."}, {"title": "3. EXPERIMENTS", "content": "Four publicly available datasets were used throughout the experiments: ROCO [15], MIMIC-CXR [17], CLEF2022 image caption dataset [16], and MIMIC-CXR dataset [17]. ROCO contains over 80,000 images with corresponding captions. MIMIC-CXR includes 473,057 chest X-ray images with 206,563 accompanying reports. CLEF2022 contains over 90,000 image-caption pairs. MS-CXR specifically designed for medical visual grounding. MS-CXR includes 1,153 samples with bounding boxes, covering eight diseases along with succinct radiological reports. To ensure fair evaluation, we randomly split the dataset into training, validation, and testing sets with a 7:1:2 patient ratio and assessed the model's average performance across all eight diseases.\nWe implemented our approach on four NVIDIA Tesla A40 GPUs. In both fine-tuning stages, we used randomly cropped 448 \u00d7 448 images. The MiniGPT-v2 pre-trained weights were utilized to initialize the model for the first fine-tuning stage. During the first fine-tuning stage, the model underwent pretraining for 3 epochs with a batch size of 2. This stage utilized the AdamW optimizer with a weight decay coefficient of 0.05, an initial learning rate of 1e-4, which was gradually decreased to 8e-5 following a cosine schedule. In the second fine-tuning stage, the model was trained on the MS-CXR dataset for 50 epochs with a batch size of 4. The AdamW optimizer was employed, with the learning rate decreasing from an initial 3e-5 to a final le-5."}, {"title": "3.2. Experimental Results", "content": "TABLE 1 and 2 compare the performance of our model against existing models on the MS-CXR dataset, with the performance metrics for GPT-4v sourced from [18], and those for other models from [7]. As demonstrated in TABLE 1, our model surpasses the performance of existing models in six of the eight disease categories. Notably, the IoU for Pneumothorax, the disease category experiencing the most significant improvement, escalated from 0.137 to 0.303, marking an increment of 0.166. The mean IoU across the eight disease categories is 0.049 higher than that of MSLL [7], representing an increase of 14.94%. Considering the sample count in each category, the wIoU improved from 0.308 for MSLL to 0.407, an enhancement of 32.14%. It is worth noting that the mean IoU of GPT-4v [10], which is also a multimodal large model, is only 0.0833, which is much lower than the performance of our model. TABLE 2 presents the Dice scores performance across various models on the MS-CXR dataset. Similar to the IoU performance, the Pneumothorax category exhibits the most substantial improvement, with the Dice score rising from 0.217 to 0.43, marking an increase of 0.213.The mean Dice score across the eight categories surpasses that of MSLL by 0.047, attaining a score of 0.513. Furthermore, the wDice score sees a 23.64% increase over MSLL's 0.44, reaching 0.544.\nWe present the visual results of our model in Fig. 2 It is evident that our model can locate the disease locations across various categories. As illustrated in Fig. 2(b), for a sample classified under \"Pneumothorax\", the label text is \"small right apical pneumothorax\". While the model-generated box does not entirely encompass the ground-truth box, it aligns more accurately with the textual description of the label."}, {"title": "3.3. Ablation Study", "content": "We conducted an ablation study to explore the impacts of the two stage fine-tuning strategy on the medical visual grounding performance. In TABLE 3 and 4, they show the IoU and Dice measurements of models that (i) perform zero-shot without fine-tuning; (ii) are only fine-tuned on image caption"}, {"title": "4. CONCLUSION", "content": "In this paper, we introduce a parameter-efficient fine-tuning medical MLLM for medical visual grounding. We adopted a two-stage fine-tuning strategy, focusing training on linear projection layers by freezing the LLM and the visual encoder. This approach not only significantly enhances the efficiency of the model training process but also minimizes resource consumption to the greatest extent, ensuring a more precise alignment with medical visual-textual knowledge. Notably, our model demonstrated superior performance on the MS-CXR dataset, significantly outperforming the GPT-4v model. Furthermore, our framework's utility extends beyond medical visual grounding tasks, showcasing its potential for a wide range of other medical multimodal tasks."}, {"title": "6. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using human subject data made available in open access. Ethical approval was not required as confirmed by the license attached with the open-access data."}]}