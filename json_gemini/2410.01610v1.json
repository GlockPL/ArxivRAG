{"title": "UPCYCLING INSTRUCTION TUNING FROM DENSE TO\nMIXTURE-OF-EXPERTS VIA PARAMETER MERGING", "authors": ["Tingfeng Hui", "Zhenyu Zhang", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Sen Su"], "abstract": "Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and\ndemonstrates outstanding performance in plentiful natural language processing\ntasks. However, existing methods transforming LLMs from dense to MoE face\nsignificant data requirements and typically rely on large-scale post-training. In this\npaper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient approach\nfor tuning a dense pre-trained model into a MoE instruction model. Specifically,\nwe first point out that intermediate checkpoints during instruction tuning of the\ndense model are naturally suitable for specialized experts, and then propose an\nexpert expansion stage to flexibly achieve models with flexible numbers of experts,\nwhere genetic algorithm and parameter merging are introduced to ensure sufficient\ndiversity of new extended experts. To ensure that each specialized expert in the\nMoE model works as expected, we select a small amount of seed data that each\nexpert excels to pre-optimize the router. Extensive experiments with various data\nscales and upcycling settings demonstrate the outstanding performance and data\nefficiency of UpIT, as well as stable improvement in expert or data scaling. Further\nanalysis reveals the importance of ensuring expert diversity in upcycling.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated remarkable performance on various NLP tasks\nand are gradually becoming part of our daily lives through chatbot applications such as ChatGPT,\nCopilot, etc (Ouyang et al., 2022; Touvron et al., 2023; OpenAI, 2024). As LLMs become increasingly\nprevalent, the high computational of traditional dense architecture with high computational costs\nin the inference phase poses significant obstacles to downstream deployment. How to improve the\nmodel performance without proportionally increasing computing resources become a hot topic in the\nfield (Muennighoff et al., 2024; Xue et al., 2024). In response to this challenge, Mixture-of-Experts\n(MoE) receives extensive attention due to its excellent scalability, which expands model capacity with\nalmost no extra inference overhead (Fedus et al., 2022; Zoph et al., 2022). Recently, many MoE-based\nLLMs have emerged in various scenarios with outstanding effectiveness and efficiency (Dai et al.,\n2024; Jiang et al., 2024; Zhu et al., 2024a).\nUpcycling is garnering increasing attention for converting dense LLMs through a series of processes,\nincluding expanding experts, integrating routers, and subsequent post-training, ultimately yielding\nMoE-style models. As depicted in Figure 1, current solutions are broadly classified into two categories:\n(a) Vanilla Upcycling, which directly upcycle a dense model to a MoE model by replicating FFN\nlayers, followed by a large-scale post-training to optimize the additional experts and corresponding\nrouters (Komatsuzaki et al., 2023). Due to the homogeneity of experts in the initial stage, a large\namount of post-training data is usually necessary, such as ~1T tokens for full parameter training\nor ~5M instruction data for parameter efficient fine-tuning (Dou et al., 2024; Zhu et al., 2024a).\n(b) Specialized Upcycling, which first trains specialized experts based on meticulously designed\ndomain-specific datasets and then proceeds with upcycling and post-training (Sukhbaatar et al., 2024).\nDespite having superior performance, it still requires hundreds of billions of elaborately constructed"}, {"title": "2 METHODOLOGY", "content": "In this section, we provide a detailed exposition of UpIT. Generally speaking, UpIT perpetuates the\nconcept of specialized upcycling, further using intermediate checkpoints to reduce data requirements,\nexpanding experts to fit the flexible number of experts, and pre-optimizing routing vectors to ensure\nthat each expert in the instruct MoE model leverages their strengths."}, {"title": "2.1 PRELIMINARIES", "content": "Before introducing UpIT, here we first briefly review some basic concepts of MoE.\nMixture-of-Experts (MoE). MoE significantly scale up the total parameter number and increases\nthe knowledge capacity of language models, by selectively activating some of the parameters during\ninference, it does not proportionally increase the computational workload. In transformers-based\nmodels, the feed-forward neural network (FFN) layer in each transformer block is typically replaced\nwith a MoE layer, which comprises N identical and independent experts {E_i}^N_{i=1}, along with a\nrouter R(\u00b7) for assigning experts, where each expert generally corresponds to an FFN layer, and\nin the scenario of parameter-efficient fine-tuning (PEFT), the expert may also be LoRA matrices.\nFormally, for the hidden states h of the attention layer, the output o of the MoE layer is represented\nas o = \\Sigma_{i=1}^N R(h)E_i(h). Here, E_i(h) is the output of the i-th expert, R(h) denotes the score for\nall experts, where experts with the highest scores are usually selected to calculate the final output.\nUpcycling. Upcycling seeks to avoid training MoE models from scratch by transforming an existing\ndense model into a MoE model, followed by a post-training stage to integrate all the parameters\ninto an organic whole. It starts with dense models, forming experts by expanding the FFN layer or\ncreating new LoRA branches, and then adding routers to control the dispatch of input tokens. In this\nprocess, the remaining embedding layers, attention blocks, normalization modules, and output layers\nare directly transferred from the initial dense model to the ultimate MoE model, and the router is\nrandomly initialized and optimized in the post-training stage."}, {"title": "2.2 WORKFLOW OF UPIT", "content": "Starting from the dense pre-trained model, UpIT achieves a MoE instruction model. Algorithm 1\nprovides the working sketch. In this section, we provide a detailed explanation of each process.\nExpert Preparation. In the instruction tuning of LLMs, as shown in Figure 2, the performance\nof intermediate checkpoints varies across different benchmarks, and different checkpoints exhibit\nunique strengths in different domains, highlighting the potential to serve as specialized experts.\nCompared to the labour-intensive method of training diverse expert models with massive domain-\nspecific data (Sukhbaatar et al., 2024), we believe that the natural variations among checkpoints\nprovide a more efficient pathway to developing specialized expert models. By training dense models\nto generate multiple checkpoints and saving them at regular intervals during training, we can easily\nobtain a series of expert models proficient in different domains, resulting in a more cost-effective\nmethod for preparing specialized expert models.\nExpert Expansion. Given that the fixed number of checkpoints only sometimes corresponds with the\nflexible requirements of expert number, acquiring additional checkpoints entails redundant training if\nthe number of experts exceeds the saved checkpoints. Here, we propose to address these challenges\nby generating distinct experts from existing ones without extensive retraining (see also Algorithm 2).\nSpecifically, we draw inspiration from genetic algorithms, where two experts with the greatest\ndifferences are selected as parents in each iteration. We simulate the mutation process by randomly\nassigning weights to the parents and apply DARE (Yu et al., 2024) to introduce mutations into the\nnewly created expert further, enhancing its discrepancy and adaptability. Such an expansion process\nnot only eliminates the need for additional retraining but also facilitates the flexible expansion of the\nnumber of experts, ultimately improving the scalability of UpIT."}, {"title": "Router Initialization.", "content": "Since routers remain randomly initialized after upcycling, which leads to the\nmisallocation of tokens in the early post-training stages, in UpIT, such misallocation will weaken the\nexpert differences in the previous stage and impact the learning efficiency of MoE models. To solve\nthis problem, we propose a data selection approach to curate expert-specific data tailored to each\nexpert model and pre-optimize additional routing vectors to ensure the discrepancy among experts\n(see also Algorithm 3). Specifically, we initially embed one-dimensional routing vectors R before\nthe MoE layer in each transformer block and participate in the training process as expert-specific\nrouters. Next, we introduce an auxiliary loss L_aux intending to maximize the output probability of\ncorresponding routing vectors. This ensures that the likelihood of tokens being assigned to appropriate\nexperts increases when they pass through the router. The pre-optimizing objective of i-th expert\nmodel is formulated as follows,\nO_i = min_{E_i}(\u03b1L_{lm}(E_i) + (1 \u2212 \u03b1)L_{aux}(E_i))\nwhere \u03b1 is the balance coefficient, which we set to 0.5 in our experiments, and L_lm(\u00b7) is the causal\nlanguage model loss. The auxiliary loss L_{aux} (\u00b7) is defined as follows,\nL_{aux} (E_i) = CrossEntropy(Sigmoid(h_r), I)\nwhere h_r, is the output of routing vector and I is the unit matrix. We use Sigmoid function to scale\nthe output to (0, 1) and minimize its difference from I, which is equivalent to maximizing the output\nprobability of the routing vector on the data that current expert model excels at.\nModel Upcycling. Finally, we upcycle the dense model O_d to MoE model O_m by merging all the\nexpert models E and routing vectors R. Specifically, for the initialization of experts, we utilize\npre-optimized expert models from E. In terms of router initialization, we concatenate all routing\nvectors from R to form a complete router R \u2208 R^{d_h\u00d7n}, where d_h is the dimension of hidden states,\nThis way, the obtained MoE block could allocate different tokens to experts skilled in processing\nthem. Finally, we continue to utilize D for post-training to achieve the final MoE model."}, {"title": "3 EXPERIMENTS", "content": "3.1 EXPERIMENTAL SETUP\nBaselines. To assess the effectiveness of UpIT, we compare its performance against several baselines.\nFor LoRA-based settings, we consider the following baselines. (1) LoRA (Hu et al., 2021), (2)\nSelf-MoE (Kang et al., 2024), (3) PESC (Wu et al., 2024a), (4) LoRAMOEPT (Dou et al., 2024), and\n(5) LORAMOESFT. For FFN-based settings, we compare UpIT with (1) SFT, (2) Self-MoE (Kang\net al., 2024), (3) Upcyclep\u0442 (Komatsuzaki et al., 2023), and (4) UpcyclesFT. For a more detailed\ndescription of the baselines, please refer to Appendix A.1."}, {"title": "2.3 TRAINING DETAILS", "content": "To comprehensively evaluate the effectiveness of UpIT, we utilize two types of upcycling settings:\n(1) FFN-based Upcycling: Initially, we fully fine-tune all parameters of the dense pre-trained model\nto accumulate several expert models. In the expert expansion stage, we apply the genetic algorithm\nto construct expert modules (i.e. FFN layers), average the parameters of backbone modules (i.e. all\nlayers except FFN) in candidate expert models, and result in new diverse expert models. We select\nexpert-specific data to pre-optimize the FFN layers and routing vectors during the router initialisation\nstage. Finally, in the model upcycling stage, we average the backbone parameters of all expert models\nand concatenate the routing vectors, integrating FFN layers to produce the final MoE models.\n(2) LoRA-based Upcycling: The key difference from FFN-based upcycling is that in parameter-\nefficient fine-tuning, parameters of backbone modules remain unchanged. Instead, we augment FFN\nlayers with LoRA matrices, and operate on the values of LoRA matrices during expert expansion.\nFollowing previous work (Fedus et al., 2022), during post-training, we also use load balancing loss,\nL_{load} = n\u00b7 \\Sigma_{i=1}^n f_i \u00b7 P_i, where n is the expert number, f_i is the fraction of tokens dispatched to\nexpert E_i, P_i is the average fraction of the router probability allocated for expert E_i."}, {"title": "3.2 MAIN RESULTS", "content": "Table 1 showcases a comparative analysis of benchmark results for LoRA-based and FFN-based\nmodels across diverse domains, revealing the following key insights.\n(1) The proposed UpIT framework demonstrates remarkable performance across various bench-\nmarks, highlighting its effectiveness compared to existing upcycle solutions. Specifically, when\ncompared with LoRAMOESFT (8E,A2), the LoRA-based UpIT (8E,A2) achieves an average per-\nformance improvement of 2.22%. When the number of experts is expanded to 16, UpIT (16E,A2)\nsustains a competitive edge over LoRAMOESFT(16E,A2), exhibiting a lead of 2.76%. Similar\ntrends are observed in FFN-based scenarios, where UpIT (4E, A2) and UpIT (8E, A2) outperform\nUpcyclesFT (4E, A2) and UpcyclesFT (8E, A2) by 1.28% and 2.21%, respectively. This compre-\nhensive analysis further corroborates the applicability of UpIT across diverse MoE architectures,\nconsistently yielding optimal performance.\n(2) In comparisons with PESC (8E, A2) and PESC (16E, A2), which take adapter structure as ex-\nperts, LoRAMOEPT (8E, A2) and LoRAMOEPT (16E,A2) display respective advantages of 0.25%\nand 0.13%, thereby underscoring a slightly superiority of LoRA-based MoE models over adapter-\nbased counterparts. More than that, in FFN-based upcycling, two Self-MoE models experience a\ncollapse in performance, a phenomenon not observed in LoRA-based settings. We posit that this is\ndue to the excessive number of expert parameters introduced in FFN-based upcycling, and the small\ndata in instruction tuning is insufficient to differentiate the experts sufficiently, which hinders the\nability of only training routers to fit the diverse data effectively."}, {"title": "3.3 SCALING THE TRAINING DATASET", "content": "To assess the data-efficient nature of UpIT, we validate UpIT and vanilla upcycling approaches by\nrandomly sampling 50K, 100K, and 200K samples from the full 500K dataset, enabling experiments\nacross four data sizes. As illustrated in Figure 3, we have several intriguing findings.\n(1) In the context of LoRA-based scenarios, UpIT (8E,A2) demonstrates comparable performance\nto LoRAMOE (8E, A2) trained on 500K samples, with only 50K training samples. When scaling up\nto 16 experts, UpIT (16E,A2) outperforms LoRAMOE (16E, A2) trained on the full 500K dataset\nagain with 100K training samples. These findings extend to FFN-based settings, underscoring the\ndata-efficient essence of UpIT and the ability to diminish the data demands of upcycling notably.\n(2) Both existing LoRA-based and FFN-based models face performance growth saturation issues with\ntraditional SFT (LoRA) and vanilla upcycling strategies. Specifically, while a noticeable performance\nincrease occurs as the dataset scales from 50K to 200K, the performance growth stabilizes as it\ncontinues to expand from 200K to 500K, with the average performance exhibiting a log-like curve.\nIn contrast, UpIT demonstrates nearly linear growth trends in FFN-based models and even exhibits\naccelerated performance gains as the dataset size increases in LoRA-based models. This strongly\nindicates that MoE models trained using UpIT could efficiently capture the principles of token\ndispatch more and possess a higher performance upper bound."}, {"title": "3.4 SCALING THE NUMBER OF EXPERTS", "content": "To examine the impact of scaling the number of total experts and activated experts on MoE models,\nwe investigate the effects of UpIT and vanilla upcycling methods with different expert numbers.\nThe first conclusion drawn from Figure 4 is that UpIT demonstrates superior performance across\nall configurations. Furthermore, as the number of activated experts increases, the growth trend\nof average performance gradually slows down, which is attributed to the fact that the evaluation\nbenchmark is domain-specific, and simply increasing the number of activated parameters does not\nconsistently yield substantial improvements. We also find that under the same activated parameters,\nas the number of experts increases, vanilla upcycling even experiences several performance drops,\nwhereas UpIT consistently shows improvements in performance as the number of experts grows. Due\nto the inefficiency of data utilization in vanilla upcycling, increasing the number of experts during\ntraining leads to a reduction in data allocation for each expert, and the router fails to dispatch tokens\nto experts appropriately, results in unpredictable model performance."}, {"title": "3.5 ROUTER ANALYSIS", "content": "To assess the efficiency and interoperability of UpIT, understanding its token dispatch mechanism is\nessential. We comprehensively analyze the distribution patterns of designated experts across four\nrepresentative benchmarks: HumanEval, GSM8K, NQ, and MMLU. The results of this examination\nare illustrated in Figure 5, focusing specifically on the 15th layer of LoRAMoE and UpIT with 8 total\nexperts and 2 activated experts. Significantly, Expert 4 exhibits significantly higher activation within\nthe HumanEval benchmark compared to the other datasets, while Expert 3 demonstrates a substantial\nactivation rate in MMLU compared to other experts. The analysis reveals that, aside from exhibiting"}, {"title": "3.6 EXPLORE THE UPPER BOUND OF UPIT", "content": "In the main experiment, we aligned total training amounts fairly to compare UpIT with baseline\nmethods. Here, we extend the training epochs to better understand the performance upper bound\nof UpIT. For the baselines, we continue training for an additional 4 epochs, but they do not show\nsignificant performance gains. Instead, we expand the training for UpIT in two ways: UpIT (2,6)\nincludes 2 epochs for expert preparation followed by 6 epochs for post-training, while UpIT (4,4)\ncomprises 4 epochs for expert preparation followed by 4 epochs for post-training. Figure 6 shows\nthat UpIT demonstrates continuous performance improvement with more training epochs, indicating\ngreater potential than the baselines. Besides, it is interesting that UpIT (4, 4) experiences longer\niteration epochs during expert preparation, with greater divergences between expert models, leading\nto a more rapid upward trend. In the post-training stage, after only 4 epochs, it achieves comparable\nresults to UpIT (2,6) and is expected to reach higher performance in the upper bound."}, {"title": "3.7 FURTHER ANALYSIS", "content": "Different Checkpoint Selection Strategies during Expert Preparation. In this section, we would\nlike to answer the question of how to select checkpoints if the number of saved checkpoints exceeds\nthe required number of experts. As shown in Table 2 (left), the latter-half selection performs better.\nDetailed results in Table 8 indicate that the primary performance differences stem from improvements\nin mathematical reasoning and coding abilities as training progresses, and selecting later checkpoints\nmight enhance these capabilities.\nDifferent Parameter Merging Strategies during Expert Expansion. Next, we investigate various\nexpert merging strategies. Table 2 (middle) reveals that the genetic algorithm-based expert expansion\nachieves performance comparable to the method of constructing experts with more checkpoints,\nhighlighting the effectiveness of our merging strategy in generating diverse experts. Additionally,"}, {"title": "4 RELATED WORK", "content": "Mixture of Experts. Mixture of Experts (MoE) (Jacobs et al., 1991) modifies the FFN layers or\ninserts additional branches to construct experts and activates them sparsely, thereby significantly\nenlarging the model capacity without noticeably increasing computational costs. The exploration\nof MoE has increasingly captured attention in recent years. Vanilla upcycling (Komatsuzaki et al.,\n2023) copies FFN layers, followed by post-training, have achieved a more convenient MoE training\nstrategy. LoRAMOE (Dou et al., 2024), MoELORA (Luo et al., 2024), MixLoRA (Li et al., 2024b)\nand MOLE (Wu et al., 2024b) develop an MoE model by incorporating several LoRA branches as\nexperts, utilizing sparse activation or linear weighting for model construction. PESC (Wu et al.,\n2024a) introduces adapter-based structures after the FFN layers, exploring a parameter-efficient MoE\nmodel that diverges from the LoRA paradigm. MoExtend (Zhong et al., 2024) adapt to new tasks\nby expanding the MoE layer during the training process, mitigating catastrophic forgetting. MoE\nJetpack (Zhu et al., 2024b) introduces checkpoint recycling, which leverages checkpoints to enhance\nthe flexibility and diversity of expert initialization. In contrast, Branch-Train-MiX (Sukhbaatar et al.,\n2024) and Self-MoE (Kang et al., 2024) explore a Specialized Upcycling method by introducing\nspecialized experts. Despite superior performance, they still require considerable domain data to\nacquire specialized experts. In this paper, we integrate the advantages of the work above and utilize\nintermediate checkpoints for expert preparation, innovatively propose an expert expansion strategy\nand an stage of pre-optimizing routing vectors to enhance flexibility, scalability and data efficiency.\nModel Merging. Model merging has emerged as a prominent research direction in recent years,\nfocusing on consolidating multiple task-specific models into a unified model with diverse capabili-\nties (Wortsman et al., 2022; Ilharco et al., 2023). Model merging usually considers the combination\nof model parameters without accessing the original training data. Average Merging (Wortsman\net al., 2022) is one typical model merging approach, which utilizes averaged parameters to construct\nthe merged model. Task Arithmetic (Zhang et al., 2023) employs a pre-defined scaling term to\ndistinguish the importance of various models. Fisher Merging (Matena & Raffel, 2022) performs au-\ntomatic weighted merging of parameters, where the Fisher information matrix calculates the weights.\nTIES-Merging (Yadav et al., 2023) tackles the task conflicts in (Zhang et al., 2023) by trimming\nlow-magnitude parameters, resolving sign disagreements, and disjointly merging parameters with\nconsistent signs. DARE (Yu et al., 2024) first sparsifies delta parameters of several SFT homologous\nmodels and then merges them into a single model. We innovatively integrate the model merging\nconcept into the MoE model, leveraging genetic algorithms and DARE to expand and evolve new\nexperts. This approach enhances the scalability of our framework."}, {"title": "5 CONCLUSION", "content": "In this paper, we present a novel, flexible, scalable, and data-efficient approach, Upcycling Instruction\nTuning (UpIT), for transforming dense pre-trained models into MoE instruction models. By leveraging\nintermediate checkpoints as specialized experts and implementing an expert expansion stage with\ngenetic algorithms and parameter merging, UpIT successfully enhances expert diversity while\nallowing for a flexible number of experts. The strategic selection of seed data ensures that each expert\nand router performs optimally within the MoE framework. Our extensive experiments demonstrate\nthat UpIT not only achieves superior performance across various benchmarks but also maintains\nstability in expert and data scaling. Further analysis emphasizes the critical importance of expert\ndiversity in the upcycling process. Overall, UpIT offers a promising pathway for enhancing the\nefficiency and effectiveness of MoE models, paving the way for future advancements in the field."}, {"title": "A APPENDIX", "content": "A.1 DETAILED DESCRIPTION OF BASELINES\nFor LoRA-based settings, we compare several baselines with our proposed UpIT. (1) LoRA (Hu et al.,\n2021): It adds low-rank matrix branches for parameter-efficient fine-tuning. (2) Self-MoE (Kang\net al., 2024): It employs specialized experts to build MoE model, and only train the routers during\nthe post-training stage. We solely reuse the training strategy of Self-MoE, and only train the routers\nafter upcycling with intermediate checkpoints. (3) PESC (Wu et al., 2024a): It adds several adapter\nstructures after the FFN block as experts. (4) LORAMOEPT (Dou et al., 2024): It employs the same\nstructure as UpIT which insert several LORA branches as experts, and (5) LORAMOESFT: It copies\nthe final-step checkpoint to form MoE blocks.\nSimilarly, for FFN-based settings, we compare UpIT with (1) SFT, It is the standard fine-tuning solu-\ntion. (2) Self-MoE (Kang et al., 2024), It is similar with the LoRA-based method. (3) Upcycle\u0440\u0442 (\u041a\u043e-\nmatsuzaki et al., 2023): It is the vanilla upcycling approach for transforming a dense pre-trained\nmodel to the MoE model. (4) UpcyclesFT. It copies the final-step checkpoint to form MoE blocks."}, {"title": "A.2 EVALUATION METRICS.", "content": "To validate the effectiveness of our method, we employ comprehensive evaluation benchmarks, which\ncontain various abilities. (1) Factual Knowledge: To assess the LLMs' factual knowledge, we\nemploy the Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021), ARC-e\nand ARC-c (Clark et al., 2018) datasets. MMLU comprises questions across 57 subjects from\nelementary to professional difficulty levels. ARC-e and ARC-c contain questions for science exams\nfrom grade 3 to grade 9. We report the 0-shot accuracy based on answer perplexity for MMLU and\nARC. (2) Reasoning: We utilize the test split of the Grade School Math (GSM8K) (Cobbe et al.,\n2021), HellaSwag (HellaS.) (Zellers et al., 2019) and Big-Bench-Hard (BBH) (Suzgun et al., 2022)\nbenchmarks to evaluate reasoning abilities. We report the 8-shot accuracy for GSM8K and the 3-shot\naccuracy for HellaSwag. (3) Coding: To probe the LLMs' ability to generate functionally correct\nprograms from docstrings, we utilize HumanEval (HumanE.) (Chen et al., 2021) and report the\npass@1 performance. (4) World Knowledge: We adopt Natural Question (NQ) (Kwiatkowski et al.,\n2019) and TriviaQA (Joshi et al., 2017) to evaluate the commonsense question-answering ability. All\nof the above evaluations are performed using opencompass (Contributors, 2023) framework, and to\nexpedite evaluation, we enable batch-padding with a batch size of 32."}]}