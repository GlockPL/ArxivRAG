{"title": "Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism", "authors": ["Jiafeng Zhong", "Bin Li", "Jiangyan Yi"], "abstract": "The task of partially spoofed audio localization aims to accurately determine audio authenticity at a frame level. Although some works have achieved encouraging results, utilizing boundary information within a single model remains an unexplored research topic. In this work, we propose a novel method called Boundary-aware Attention Mechanism (BAM). Specifically, it consists of two core modules: Boundary Enhancement and Boundary Frame-wise Attention. The former assembles the intra-frame and inter-frame information to extract discriminative boundary features that are subsequently used for boundary position detection and authenticity decision, while the latter leverages boundary prediction results to explicitly control the feature interaction between frames, which achieves effective discrimination between real and fake frames. Experimental results on PartialSpoof database demonstrate our proposed method achieves the best performance. The code is available at https://github.com/media-sec-lab/BAM.", "sections": [{"title": "1. Introduction", "content": "With the rapid advancement of Artificial Intelligence Generated Content (AIGC), technologies like text-to-speech (TTS) [1] and voice conversion (VC) [2] can generate realistic human voices. Partially spoofed audio, where synthesized speech segments are inserted or spliced into genuine utterances, pose significant threats. An attacker can alter the meaning of sentences by manipulating small and specific units (e.g., words, characters, or even phonemes), deceiving both machines and humans.\nIn recent years, numerous methods and datasets have been developed for Partially Spoofed Audio Detection (PSAD). Yi et al. [3] constructed the first partially spoofed audio dataset named HAD, which replaces some nature segments with synthesized speech segments having different semantic content. The Audio Deep Synthesis Detection (ADD) challenge 2022 [4] involves a detection track containing partially spoofed audio. The organizers provided only real and entirely fake data for training, and the task was target to detect fake at utterance level. In this challenge, notable detection performance was achieved by fine-tuning pretrained self-supervised learning (SSL) models with different back-end classifiers [5, 6]. Boundary detection task was only introduced as a proxy task to achieve utterance-level detection [7, 8].\nBeyond merely detecting the presence of spoofing in audio, the Partially Spoofed Audio Localization (PSAL) task has become an emerging topic for better analyzing deepfake audio. Zhang et al. [9] developed a dataset named Partial-Spoof, which includes segment-level labels. They then investigated an approach to detect spoofing at both segment and utterance levels within a multi-task learning framework [10]. Subsequently, they extended the PartialSpoof dataset and used Wav2vec2 (W2V2) [11] as a front-end to detect segment-level and utterance-level fakes simultaneously [12]. Moreover, the ADD 2023 [13] introduced a PSAL track, which has resulted in numerous promising works [14, 15, 16]. Among them, an effective approach was proposed to achieve the best localization score by integrating the decisions from three countermeasure (CM) systems including boundary detection, frame-level fake detection, and utterance-level fake detection [16]. Recently, an approach incorporating a contrastive learning module and temporal convolution was proposed to effectively capture better features for localization [17].\nSome aforementioned methods [7, 8, 16] utilize boundary information solely for PSAD at utterance-level detection. However, leveraging boundary features to aid in pinpointing the exact locations of spoofing within a single CM system is still an unexplored topic. As we know, all frames within a segment delineated by boundaries are assigned the same label. Once the correct boundary position is identified, the authenticity of a frame can be determined by referencing other frames within the same segment. Thus, boundary information serves as vital auxiliary knowledge that can steer authenticity decisions at the frame level. To exploit this idea, in this paper, we have designed a new attention mechanism that leverages boundary information, as illustrated in Figure 1. Message passing between those frames that belong to different authenticity classes is explicitly weakened by the attention mechanism. Our proposed method is named Boundary-aware Attention Mechanism (BAM). Specifically, pre-trained W2V2 [11] or WavLM [18] is used as front-end feature extractor. Subsequently, a Boundary"}, {"title": "2. Proposed method", "content": "The flowchart of our BAM is depicted in Figure 2. Firstly, we use a pre-trained SSL model to extract the features of speech and apply an attentive pooling layer [19] to make each frame represents a specific temporal resolution (e.g., 160 ms). The output of the pooling layer is fed into the Boundary Enhance (BE) module to enhance boundary feature representation and identify the boundary frames with a simple fully connected layer. Then, the Boundary-aware Frame-wise Attention (BFA) module takes the boundary prediction results and the outputs of the pooling layer as inputs to capture the correlation information among frames. Finally, the outputs of the BFA module and the BE module, are concatenated and fed into a fully-connected layer to make frame-level authenticity decisions.\n2.1. Pretrained self-supervised front-end\nWe start by processing raw speech using a pretrained SSL speech model, W2V2 [11, 20] or WavLM [18], to extract effective front-end features. Compared to traditional handcrafted features (e.g., LFCC [21], MFCC [22]), the SSL-based front-end features exploit extensive volumes of unlabeled data for pre-training, significantly improving data representation capabilities and facilitating the identification of intricate patterns within audio data. In our implementation, we utilize WavLM-large [18] or XLS-R-300M [20] and fine-tune their weights in conjunction with other modules, and use their last hidden states of the transformer as front-end features.\n2.2. Boundary enhancement module\nIn the context of PSAL, frames at the boundaries that contain both spoofed and genuine samples are labeled as spoofed. During training with binary authenticity labels, boundary frames (especially those with a smaller proportion of spoofed data) are close to fully genuine frames in the feature space, even though they have opposite labels. This situation can lead to training instability and performance degradation. To mitigate this issue, we have developed a boundary enhancement module specifically to model and distinguish boundary frames from non-boundary frames.\nAs shown in Figure 2, the boundary enhancement module transforms the front-end feature $F_g \\in \\mathbb{R}^{T \\times D}$ into the boundary feature $F_b \\in \\mathbb{R}^{T \\times 2D}$, where $F_g$ is the output feature of a pooling layer, T is the number of frames, and D is the feature dimension. The boundary feature serves dual purposes.. Firstly, it is fed into a fully-connected layer with a sigmoid function to obtain the boundary prediction probability $b \\in \\mathbb{R}^{T \\times 1}$ as following:\n$b = \\text{sigmoid}(\\phi(F_b))$,\nwhere the linear mapping function $\\phi$ with a learnable weight matrix W and a bias term b is given by:\n$\\phi(x) = Wx + b,$\nThen we conduct binarization with a fixed threshold to make binary boundary prediction B. Secondly, the boundary feature is processed by a fully-connected layer with an activation function and concatenated for the final frame-level authenticity decision. The detail of BE module is illustrated in the middle of Figure 2. Specifically, two different branches are designed to respectively extract the intra-frame and inter-frame features. For the inter-frame branch, a Frame-wise Attention Block (FAB) is employed to capture inter-frame correlations using an attention mechanism. The right part of Figure 2 illustrates the details of the FAB. We start by computing the attention score $s \\in \\mathbb{R}^{T \\times T \\times D}$ through the element-wise multiplication of each frame feature with others. Then, a learnable attention weight $W_a \\in \\mathbb{R}^{D \\times H}$ is defined to weigh the attention score. This process can be written as:\n$A_t = \\tanh(\\phi(s))W_a,$\nwhere $A_t \\in \\mathbb{R}^{T \\times T \\times H}$ represents the attention map. Subsequently, each frame within $F_g$ aggregates information from other frames, utilizing the attention map as following:\n$F_a = \\text{softmax}(A_t)F_g,$\nwhere $F_a \\in \\mathbb{R}^{H \\times T \\times D}$ represents updated frame features. The residual structure is employed to stabilize training. The output of FAB is obtained from $F_a$ and $F_g$, according to:\n$F_{\\text{inter}} = S(\\text{BN}(\\phi(F_a) \\oplus \\phi(F_g))),$\nwhere $\\oplus$ represents element-wise addition and BN represents 1-D batch normalization, and S is SELU [23] activation function. For the intra-frame branch, each frame is individually fed into a 1D-ResNet [24], followed by a fully-connected layer to learn intra-frame features. Then, the intra-frame feature $F_{\\text{intra}}$ and inter-frame feature $F_{\\text{inter}}$ are concatenated to obtain the boundary feature $F_b$.\n2.3. Boundary frame-wise attention module\nAs shown in Figure 2, the BFAM module consists of two stacked Boundary Frame-wise Attention Blocks (BFAB). Specifically, the structure of BFAB is similar to FAB but is equipped with a boundary masking component, which takes binary boundary prediction B and $F_g$ to weaken the message passing between those frames that belong to different genres. A boundary adjacency matrix $A_b \\in \\{0,1\\}^{T \\times T}$ is constructed based on the boundary prediction B. Its element is defined as follows:\n$A_{b_{i,j}} = \\begin{cases}\n1 & \\text{if } i = j, \\\\\n\\prod_{n=i}^{j-1} (1 - B[n]) & \\text{if } i < j, \\\\\n\\prod_{n=j}^{i-1} (1 - B[n]) & \\text{if } i > j,\n\\end{cases}$"}, {"title": null, "content": "where B[n] represents the nth element of sequence B, and i, j\u2208 [0,1,...,T \u2013 1] represent the indices of frames. In other words, equation 6 determines whether there is a boundary between the ith and the jth frame; if there is, the value in the ith row and the jth column of $A_b$ is set to 0, otherwise, it is set to 1. Once the adjacency matrix is obtained, the attention map (refer to equation 3) can be updated to a boundary attention map $A_t$, as illustrated in Figure 3, and given as:\n$A_t = A_t \\odot A_b,$\nwhere $\\odot$ denotes element-wise multiplication.\n2.4. Loss function\nWe employ two loss functions to constrain the model for supervised training: boundary loss $L_b$ and frame-level authenticity loss $L_s$ as shown in Figure 2. For boundary loss, binary cross-entropy loss is used and the ground-truth boundary labels B can be derived from the segment-level authenticity labels Y. It is worth noting that we only set the boundary frames as label 1 and all other frames as label 0, which differs from previous methods [7, 8, 16] where they also set the label of frames near the boundary frames to 1. This labeling strategy [7, 8, 16] may introduce additional noise in our case and lead to diminished localization performance. For authenticity loss, we follow prior work and utilize the standard cross-entropy loss. In summary, the overall loss can expressed as:\n$L = L_s(\\hat{y}, Y) + \\lambda L_b(\\hat{B}, B),$\nwhere $\\hat{y} \\in \\mathbb{R}^{T \\times 1}$ represents the authenticity prediction result of model, and $\\lambda$ is set to 0.5 in practice."}, {"title": "3. Experiments and results", "content": "3.1. Dataset and implementation details\nTo validate the effectiveness of our method, we conducted experiments on the PartialSpoof [12]. The dataset is constructed by segmenting the fully spoofed and genuine utterances from the ASVspoof2019 [25] LA database using Voice Activity Detection (VAD) and then concatenating those segments with basic digital signal processing. Four evaluation metrics are used to compare model performance: Equal error rate (EER), precision, recall and $F_1$ score.\nIn the training phase, for experiments conducted at a 160 ms temporal resolution, we fix the length of training samples at 4s. Samples exceeding this length are randomly trimmed, while those shorter than are padded with zero. Then raw data is padded to correspond with the 20 ms shift of W2V2 and WavLM. Therefore, the attentive pooling stride is set to 8, the number of frames T is 25, the feature dimension D is 1024, block number N is 2, and the learnable weight head number H is 1. For experiments conducted at a 20 ms temporal resolution, the number of frames T is 200, and we use a linear transformation to reduce the feature dimension D to 256. Other hyperparameters remain consistent with those used in 160 ms experiments. In the test phase, variable-length test samples are directly fed into the model to obtain the prediction result. we use Adam optimizer with a learning rate of $10^{-5}$ and halved every 10 epochs, and all of model are trained for 50 epochs.\n3.2. Comparison with existing methods\nTo ensure a fair comparison with existing methods [9, 10, 12, 16, 17], we conducted experiments with 160 ms temporal resolution. The results are shown in Table 1. Based on our observation, WavLM achieves better results compared with W2V2-XLSR. Moreover, Our method gets EER as 3.58% and $F_1$ score as 96.09%, reaching the best single CM performance on Partial-Spoof dataset.\n3.3. Ablation study\nWe conducted ablation experiments to assess the effectiveness of each modules within our framework under two settings: 1) localization task and 2) boundary detection task. Table 2 presents the results for localization task. The baseline model, denoted as , is equipped with a WavLM front-end, succeeded by a max pooling layer and a fully-connected layer. The models and represent simplified versions of the BAM. Model incorporates only the right branch as illustrated in Figure 2, while model includes both the left and right branches but omits the middle pathway from the BE module to the BFA module. Model 4 is actually the proposed BAM. It can be seen that each module contributes to the enhancement of localization performance. Specifically, compared to the baseline model, our method reduced the EER by 2.21% and increased the $F_1$ score by 1.73%.\nTable 3 presents the results for boundary detection task. It is clear that the inter-frame feature is more effective than intra-frame. The BE module combined with both inter-frame and intra-frame achieves the best boundary detection performance with EER as 3.33% and $F_1$ score as 92.25%.\n3.4. Finer-grained resolution experiment\nExtra experiment was conducted at a 20 ms temporal resolution. Specifically, we trained the BAM without pooling directly on the front-end feature with a 20 ms shift for 50 epochs as the base model. Subsequently, we removed the last fully-connected layer from pretrained base model and replaced it with an attentive pooling layer succeeded by a new fully-connected layer. We finetuned this model for 10 epochs to predict the authenticity at specific temporal resolutions. The results are shown in Table 4. The base model achieves remarkable localization performance at 20 ms resolution with EER as 5.20% and $F_1$ score as 95.82%. As the temporal resolution decreases, the $F_1$ score remains almost constant while the EER continually increase. We explain the reason behind this trend is that with the decreases in resolution, the total number of frames and the proportion of genuine frames both rise, and EER is particularly sensitive to changes in class proportions while the $F_1$ score is relatively robust. Moreover, there is a marginal decrease in localization performance compared to BAM executed on frames of 160 ms (3.66% v.s 3.58%). We attribute this decline in performance to the increased challenge of boundary detection at finer resolutions. The base model gets EER as 5.12% and $F_1$ score as 82.97% at 20 ms resolution for boundary detection task, which is worse than the boundary detection performance at 160 ms (refer to Table 3)."}, {"title": "4. Conclusion", "content": "In this paper, we propose a novel partially spoofed audio localization method. Our method simultaneously conduct boundary detection and frame-level authenticity determination tasks within a single CM model. The boundary information is utilized to enhance the accuracy of localization. Experimental result show that the proposed method achieves the best performance on PartialSpoof dataset. The finer-grained resolution experiment demonstrates that accurately detecting boundary position at a finer resolution is a more challenging task and will be a direction of our future work."}]}