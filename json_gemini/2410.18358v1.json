{"title": "Data Publishing in Mechanics and Dynamics: Challenges, Guidelines, and Examples from Engineering Design", "authors": ["Henrik Ebel", "Jan van Delden", "Timo L\u00fcddecke", "Aditya Borse", "Rutwik Gulakala", "Marcus Stoffel", "Manish Yadav", "Merten Stender", "Leon Schindler", "Kristin Miriam de Payrebrune", "Maximilian Raff", "C. David Remy", "Benedict R\u00f6der", "Peter Eberhard"], "abstract": "Data-based methods have gained increasing importance in engineering, especially but not only driven by successes with deep artificial neural networks. Success stories are prevalent, e.g., in areas such as data-driven modeling, control and automation, as well as surrogate modeling for accelerated simulation. Beyond engineering, generative and large-language models are increasingly performing and helping with tasks that, previously, were solely associated with creative human processes. Thus, it seems timely to seek artificial-intelligence-support for engineering design tasks to automate, help with, or accelerate purpose-built designs of engineering systems, e.g., in mechanics and dynamics, where design so far requires a lot of specialized knowledge. However, research-wise, compared to established, predominantly first-principles-based methods, the datasets used for training, validation, and test become an almost inherent part of the overall methodology. Thus, data publishing becomes just as important in (data-driven) engineering science as appropriate descriptions of conventional methodology in publications in the past. This article analyzes the value and challenges of data publishing in mechanics and dynamics, in particular regarding engineering design tasks, showing that the latter raise also challenges and considerations not typical in fields where data-driven methods have been booming originally. Possible ways to deal with these challenges are discussed and a set of examples from across different design problems shows how data publishing can be put into practice. The analysis, discussions, and examples are based on the research experience made in a priority program of the German research foundation focusing on research on artificially intelligent design assistants in mechanics and dynamics.", "sections": [{"title": "Introduction", "content": "In engineering research and practice, in recent years, data-oriented thinking and the usage and research of data-based methods have become much more prevalent, especially in modeling, simulation, and control. However, engineering research in these fields relies on decades of established processes regarding research communication, publishing, and methodology that have been geared toward classical, first-principles approaches. One manifestation of this is that, in areas that were pioneering the latest surge in data-based approaches, i.e., mostly subfields of computer science, quite different customs regarding publications are now established. There, datasets and executable code are often openly available with articles. Value is put on open review processes and speedy dissemination of research results, where relevance is preferably ensured through convincing performance in benchmark problems and executable code that can be tested by readers. Yet, this need not mean that copying these customs to data-based research in engineering is the way to go. Engineering problems have their own character, and it may be that some of the traditional customs of disseminating engineering research have value in their own right even in new contexts. After all, engineering research nowadays often uses data-based components, but still also relies on classical methodologies, like first-principles modeling and analytical argumentations or even proofs where it is feasible and successful.\nThe described way of thinking has also emerged with researchers partaking in the German Research Foundation's priority program \"Daring More Intelligence - Design Assistants in Mechanics and Dynamics\u201d (SPP 2353), which unites at least thirteen senior researchers and a similar number of research associates in the common endeavour to leverage artificial intelligence to assist in designing the mechanics and dynamics of the systems of the future. During their collaborative and diverse work on problems from engineering design, it became evident that some common thought needed to be put on data publishing and sharing. Most importantly, it appeared that dealing with mechanics and dynamics and, in particular, with engineering design leads to specific challenges regarding data. This article is the result of discussions in the priority program and of the experience from nine collaborative research projects therein. It serves three key purposes.\nFirstly, the article shall raise awareness to the challenges of data publishing in mechanics and dynamics and system design, in particular for engineering scientists also embarking on more data-driven research, and it shall present some possibilities and guidelines to deal with these challenges.\nSecondly, the article presents a set of six examples from the aforementioned research projects that outline how the described challenges of data publication were overcome. The authors hope that these concrete examples, all based on published, state-of-the-art research, will further nurture ongoing discussions and change within the community and will be of help to other scientists in mechanics and dynamics who want to publish data and data-driven research.\nThirdly, while not the immediate goal, the article and its examples may show to scientists from outside engineering that very challenging and worthwhile problems exist in mechanics, dynamics, and engineering design, where good data-driven contributions could lead to sustained technological progress. The authors invite data scientists to try their methods on the data published in the publications associated with the described examples.\nIn general, data publishing and also scientific treatments of how and why to publish data are emerging topics. However, the authors are not aware of comparable treatises specifically for mechanics and dynamics and, therein, the topic of engineering design. Still, many existing works and guidelines are also relevant to the field looked at here, and will also partly appear in the following, such as, e.g., the well-known FAIR principles on data publishing [1]. Interestingly, some proposed and recognized guidelines on how to document datasets in the machine-learning community take inspiration from practices established in engineering industries [2], used there"}, {"title": "The Value of Publishing Data", "content": "Traditionally, with classical methods of mechanics and dynamics, calls for the publication of data and code were rooted in the desire to enable a straight-forward reproducibility of results in the spirit of good science. However, if described well enough, proposed methods could also be tried and checked, sometimes quite readily, by implementing and testing them on problems with known reference solutions. It was simple to separate the proposed methodology from input data, such as initial conditions of a simulation. In contrast to that, for data-based methods, the lines between methodology and data can blur significantly. Put bluntly, training data can become an integral part of the methodology itself [8,9]. After all, there can be a certain interdependency between the makeup of the dataset, such as the selection of data and its mathematical representation, and the quality of the results obtained with a certain data-based method. In many applications, coming up with a suitable amount of data with the required properties can be much harder than the setup of the learning method and the execution of the training process. Therefore, in such scenarios, the principles of good science require publishing datasets together with the proposed methodology or algorithm. For this, it is adequate to publish the data exactly in the way the (also to be published) code demands. To people more familiar with 'traditional' first-principles methods, this may seem like a nuisance brought up by data-driven methods.\nHowever, when the publication of data is thought in a broader fashion, it can present significant opportunities for research. If properly published, data can serve as an abstraction layer. This enables researchers not familiar with the concrete problem behind the data to propose data-driven solutions, based on abstract properties of the data and the problem, accelerating scientific progress. The underlying problem in question, as represented by the data, can then become a benchmark or model problem used by data scientists to test and develop new methods. The term benchmark problem should be understood slightly differently than it has sometimes been used in mechanics and dynamics. In classical dynamics, it can mean a certain kind of physical problem that every simulator should be able to solve accurately to be deemed 'correct' in the sense that it can reproduce an accepted ground-truth solution. Instead, subsequently, benchmark problems"}, {"title": "Challenges in Data Publishing", "content": "Some challenges to be faced for the publication of data are quite common beyond the specific characteristics of the priority program. These include:\n\u2022 Interpretability and Domain Knowledge: Raw data can be tough to understand for anybody unfamiliar with the generation process of the data. Furthermore, the representation of the data may heavily depend not only on the underlying system but also on the chosen analysis method and observation approach. In simulations, the deformation of a simple mechanical beam, for example, can be computed using numerical discretization schemes that involve the partitioning of the physical domain into small elements. The shape and/or numbering of those elements may however greatly vary across different simulation approaches, while displaying the same or very similar elastic deflection of the beam. Comparability of different simulation techniques and the resulting data can be difficult for the very same physical system. This can happen analogously in experiments, if measurements are taken at different points of the same mechanical object or with different measurement hardware, which may come with different built-in filtering, e.g., for noise reduction. In the light of this challenge, it is maybe not surprising that many of the first, renewed successes of machine learning in recent years happened for data interpretable without special education, like text, speech, and image data.\n\u2022 Problem Complexity: There can be conflicting interests regarding a desirable level of the studied problem's complexity, and depending on it, it may be hard to place publications that focus on the proposition of a new model problem and accompanying datasets. In mechanics and dynamics, this type of publication is less common than in data science, and the usual conventions in the field may prefer engineering problems at the upper end of the complexity scale, deeming them publishable even outside any machine-learning context. However, overly complex scenarios significantly complicate the explanation of scenarios and data, limiting the appeal to machine learning experts and thus the potential data-scientific impact.\n\u2022 Generalizability: Compared with language models or computer vision, problems from mechanics and engineering design can appear (or sometimes are) overly specific in the eyes of a researcher from, e.g., the machine learning community, even in cases where the"}, {"title": "Dealing with the Challenges", "content": "The following ideas and approaches may help to overcome the identified challenges.\nEvaluation: If the evaluation mechanism happens to be lightweight and easy to install and use, it can be sufficient to distribute it with the dataset or model problem. In other cases, a workable, yet more involved, solution is to publish a (web-)interface to an evaluator, to which results can be uploaded. The evaluation is performed on the server, and the evaluation results are made available to the user. The advantage is that the server and evaluation toolchain are set up and configured only once by the domain experts. Luckily, the required technology is already available from other domains, albeit for slightly different reasons. In data science, it is sometimes even undesirable to publish test datasets used for evaluation since then researchers may tune their method not only with the training dataset but also with the test dataset. For these reasons, there exists open-source software that greatly helps to implement such online-evaluation or benchmarking tools. A popular example is Codabench [10], which is even used to do public competitions for defined tasks. Some challenges, like the funding and maintenance of the server, remain. However, a growing number of success stories shows that this can work and can significantly help to popularize a benchmark problem, particularly in the crucial phase when the problem is new.\nTo find new, interesting papers, data scientists nowadays often browse through websites such as Papers with Code [11], where papers appear jointly with their open-source code and their performance in public benchmark problems. There, papers are not ranked by traditional measures, such as the prestige of the journal they appear in, but based on benchmark results as well as on ratings of users, which may base their rating on their experience when executing the code. State of the art approaches are grouped by tasks (closely related to the type of data such as time series, images, etc.). The visibility of papers is dictated by the performance on known benchmark problems rather than on their number of citations. Thus, papers without code or benchmark results will most likely remain invisible there, highlighting the culture change from the 'traditional' ways of mechanics and dynamics. Another popular platform to share datasets"}, {"title": "Examples: Published Code and Data", "content": "Subsequently, published data and code from within the priority program are listed and discussed, to serve as inspiration, and to learn which types of data and problems appear in mechanics, dynamics, and engineering design."}, {"title": "Vibrating Plates", "content": "Noise and vibrations of mechanical structures such as machinery and vehicles impact the human well-being and health [19], which could be mitigated with the aid of design assistants aimed at reducing noise and vibrations. Motivated by this, the vibrating plates dataset addresses the design of plates with indentations to minimize vibrations. In mechanical structures, vibrations emit noise and can be transmitted from sound sources like engines to, for example, the passenger cabin in a car. Indentations in plate-like structures can help reduce vibrations, if they are well-placed.\nFigure 1 visualizes typical pieces of input and output data. On the input side, the dataset consists of image-type data describing the indentation patterns and scalar parameters describing the material as well as geometric properties of the plate. On the output side, the dataset describes the dynamic response of the plate at discrete excitation frequency steps. Here, different fidelity resolutions are provided: Image-type data of the velocity field represent high-fidelity data, whereas the domain averaged mean squared velocity represents an integral quantity. Regarding problem complexity, this dataset exhibits many representative characteristics of a vibroacoustic problem, but the choice of indented plates instead of more complex geometries enables straightforward application of many standard machine learning methods. The dataset is"}, {"title": "Database of Crashworthiness Analysis of Crash Boxes", "content": "Every year, approximately 2.54 million people in Germany alone are involved in automotive crashes, which makes vehicle crashworthiness a crucial part of the development of automobiles [25]. To help reduce the seriousness of the impact and prevent serious injuries and fatalities, both active and passive crash management systems are installed in automobiles. Passive safety systems are usually structural components which are built to absorb impact energies and reduce potential harm to the occupants. Since these systems are static, they must be meticulously designed and optimized to perform in various scenarios. One such passive safety system is a crash\nbox behind the front bumper, as shown in Figure 2. Crash boxes are thin-walled hollow structures that are installed between the subframe and the front bumper. These absorb the kinetic energy from the impact by undergoing axial deformation and local buckling, thereby reducing the forces experienced by the occupants. The optimal design of the crash box is crucial since frontal impacts are the most common impact type in road traffic.\nIn this context, a publicly accessible database has been established that contains valuable information on various crash box configurations and their performance. This database is not merely a result of an associated research project; it has been developed to provide other researchers with valuable insights and tools. The database is publicly available in a GitHub repository [26], see also the corresponding results published in the article [27]. The detailed information on the database is presented subsequently. The provided data enables scientists and engineers to:\n\u2022 Compare different methods: Researchers can utilize the data to compare their optimization methods against the results from the analysis from [27], fostering constructive dialogue about different approaches to improve crash safety. Researchers can utilize the data to investigate classical optimization methods such as evolutionary algorithms, response surface methods (RSM) or modern random search algorithms, which is one example of utilizing the data for a comparative study.\n\u2022 Verify their own implementations: The detailed simulation results offer a solid foundation for testing and validating individual models or simulation techniques in crash analysis.\n\u2022 Develop new approaches: Access to the data allows others to develop innovative methods or adapt existing techniques to better meet specific requirements of their research projects.\n\u2022 Encourage data sharing and collaboration: Even if the collection of examples in the dataset is small, it may well inspire others to share their own datasets and consider how these could be beneficial for the community.\nThe database maps the relation between the structural and crash test parameters to that of various crashworthiness metrics used to evaluate the crash box design. In the provided database, crash box parameters (width, thickness, and length), crash test parameters (impactor mass, impact velocity) and time required for simulations are recorded. Multiple crash box configurations are evaluated by varying the thickness of the crash box and impact velocity. The objective crashworthiness parameters are peak impact force, energy absorbed, maximum deformed length, mean contact force and mass of the crash box. They are calculated for every configuration based on the FE simulation results. The neighboring components define maximum peak impact force as they should not buckle before the crash box, and generally, the energy absorbed should be maximized. Other parameters, such as maximum deformed length, mean contact force, and mass of the crash box, are informative parameters of the crash box's overall performance. Assuming the energy absorbed has to be maximized while reducing the maximum deformed length following non-dominated solutions, as shown in Figure 3, is obtained. This is called the Pareto front, and it denotes the solutions where both objectives are important.\nThis database is utilized in a multi-objective optimization of the crash box design to determine the optimal thickness for an ideal peak impact force and allowed maximum deformed length of the crash box [27,28]. A list of all the dependencies required for executing the scripts is mentioned in the Git repository."}, {"title": "Backbone Reconstruction of a Non-slender Soft Robot", "content": "The research field of soft robotics is based on the investigation of robots made up of elastic soft materials like silicone. The use of such elastic soft materials results in a continuous deformation of the robot under load or even under its dead weight. However, compared to conventional robots made up of rigid connections, the computation of this continuous deformed state can be more involved, but is a crucial part of the control of the soft robot and for the verification of models. The so-called backbone can be seen as a one-dimensional description of the continuous deformation of the soft robot, as visualized in Figure 4 for a deformed soft robot. Especially when manufacturing soft robots by hand, variances in the geometry and the manufacturing accuracy change the behavior significantly. Hence, being able to identify the parameters of the soft robot via image processing is a fast and easy way to enhance the reliability of subsequent modeling and to validate simulation results of the soft robot with geometrically accurate beam models. To that end, first, a stereo camera system is set up and calibrated. The soft robot is then actuated with different pressures while both cameras capture images to obtain the raw data. The dataset contains the calibration data, raw images, and images annotated with the reconstructed\nbackbones. The corresponding parameters of the reconstruction are available in [29]. Figure 4 displays an annotated entry from this dataset. The blue, dashed line represents the reconstructed backbone, while the white markers indicate the estimated points used in the optimization process.\nApplying the image-based method published by [30], with modifications described in [31], we adapt the technique for reconstructing the backbone of a non-slender soft robot. The original method is provided as the Python software package icpReconstructor, and the modifications and extensions for a modular bending actuator, along with a short example, can be found in the associated GitHub repository [32], as detailed in [30]. The image dataset is available in [29].\nIn the dataset, the raw images are annotated to face the issue of interpretability of the results. Since the optimization for the reconstruction of the backbone operates in pixel space and can be depicted in both figures at each iteration step, the results can be interpreted and even evaluated visually, allowing the practitioner to verify the inner workings of the reconstruction method. Furthermore, with a rendering of a deformed geometry using the reconstructed backbone from the dataset, the evaluation can be further supported. While the reconstruction method can be implemented in software, its usefulness is only leveraged with data captured in experiments. Therefore, the dataset can be seen as part of the modified model. Even if the adaptation contained in the dataset is specific to the employed soft robot, it can be viewed as an example of how well the method introduced in [30] can be generalized to different tasks. The use cases are not limited to soft robots, but can be modified to fit different applications.\nA single soft robot bending actuator, as shown in Figure 4, has limited applicability on its own, as it is usually part of a larger soft robot system. However, focusing on a single module reduces the problem complexity that arises when considering an entire soft robot system with multiple actuation parameters and potential continuous deformations, and makes the reconstruction method more feasible. Nevertheless, the deformation resulting from the pressurization of one or more chambers of the soft robot with or without external loads can be evaluated and compared with the beam deflection. The described datasets of stereo images offer numerous further possibilities for the analysis of soft robots. Besides the original detection of the backbone, the dataset can be used to analyze and identify parameters of different mechanical properties of the soft robot, e.g., axial stiffness, bending stiffness and torsional stiffness, where the pressure actuation and resulting deformations have to be put in relation. In addition, the behavior of the robot can be evaluated for different material models, providing valuable insights for optimizing design and performance. These applications can significantly improve the understanding and development of efficient and adaptable soft robotic systems."}, {"title": "Duffing Oscillator Response Analysis (DORA)", "content": "In mechanical engineering, the state-of-the-art system design paradigms are mostly based on a number of crucially simplifying and idealizing assumptions on dynamic loads and system complexity. In particular, the non-stationarity of the system dynamics is often regarded as perturbation to some steady-state operation. However, static load cases actually occur only rarely in real-life systems: aircraft engines, wind turbines and vehicle components consistently operate under non-stationary, non-periodic, multi-scale, multi-physical, or in total complex external loads and control inputs. Hence, the transient dynamics of engineering structures must be taken into account early in the design process. At the same time, data acquisition can be expensive, rendering the available sequential data sparse and small: typically, only few operating conditions and corresponding system responses are available. This situation requires models that generalize very well to unseen system parameters or external loads, and which are able to predict complex dynamic behavior. The problem complexity thus arises from four core aspects: (a) the problem involves time-dependent multivariate quantities on input and response sides, (b)\nmany variable system parameters and external loads that affect the system response, (c) critical transitions occur in the system response due to minor changes in the system parameters, and (d) limited observability and small data in the form of time series. Interpretability is not at the core of this modeling task, however highly welcome to elaborate how a model picks up transient behavior.\nThe dataset presented in this section deals with the aforementioned challenges in the following ways: it provides data in the form of time series of a minimal mechanical oscillator, namely the Duffing oscillator. This model is complicated enough to show all qualitatively possible deterministic dynamics (regular to irregular) with transitions governed by parametric changes. Moreover, using a synthetic data approach, it is possible to generate training datasets of varying size, thereby allowing to scale the complexity stemming from data availability. The Duffing oscillator represents an externally forced nonlinear oscillator with dynamics\n$\\begin{aligned}\n\\dot{q_1} &= q_2,\\\\\n\\dot{q_2} &= -cq_2 - k q_1 \u2013 \\beta q_1^3 + f \\cos(\\omega t)\n\\end{aligned}$$\nwhere the parameters are chosen as c = 0.32, k = \u22121, \u03b2 = 1, \u03c9 = 1.5 to provoke a wide range of differing dynamics under changes to the external forcing. Duffing-type oscillators have been studied and implemented in a wide range of physics and engineering applications such as MEMS resonators [33], in energy harvesting devices [34], and to study periodic forcing dependent chaotic behavior of nonlinear systems [35]. Various qualitative changes (bifurcations) of the system dynamics are obtained by varying the forcing amplitude (f) while keeping the forcing frequency constant. This variation allows to generate period-1 limit cycle solutions for small forcing amplitude, but also chaotic dynamics for larger forcing amplitudes. Rapid changes in the dynamics occur under minimal changes to the forcing amplitude (period-doubling bifurcation route to chaos) as displayed in Figure 5.\nThe modeling challenge takes the general form of multivariate sequence prediction for parameterized dynamics, and requires auto-regressive models that can advance from a given temporal context into the future. The dataset is comprised of time-series data obtained for specific values of the forcing amplitude (f = [0.46,0.49], corresponding to period-2 cycle motions). From this dataset, the model is requested to predict vibration time series for different forcing values far from the training set, spanning values that are multiples smaller or much larger than the training regime. The modeling task hence asks for time series prediction of qualitatively strongly varying dynamics, and extreme out-of-sample generalization from a minimal amount of training data. Successful models will allow to predict the system's transient dynamics across a wide range of forcings, and accurately predict critical transition points (bifurcations).\nFor evaluation purposes, the success of the prediction model will depend on the accurate capture of the overall dynamics (the climate) as well as the short-term fully-resolved temporal dynamics. The system response characteristics are quantified in terms of (mean) amplitude of the system state qi(t). The mean squared error (MSE) can be used as an accuracy quantifier to obtain the deviation of predicted system response characteristics relative to the original ones for each of the external forcing amplitudes, given by\n$\\begin{aligned}\n\\varepsilon_A &= MSE \\left( \\text{Amp} \\left( q_{1, \\text{prediction}}(t) \\right), \\text{Amp} \\left( q_{1, \\text{original}}(t) \\right) \\right), \\\n\\varepsilon_M &= MSE \\left( \\text{Mean} \\left( q_{1, \\text{prediction}}(t) \\right), \\text{Mean} \\left( q_{1, \\text{original}}(t) \\right) \\right)\n\\end{aligned}$$\nwhere $\\varepsilon_A$ and $\\varepsilon_M$ are the response amplitude and response mean errors, respectively. A more involved description of the DORA task, files, and data are provided in an open access GitHub repository [36]."}, {"title": "Passive One-Legged Hopper", "content": "Similar to the Duffing Oscillator presented in the previous section, this dataset is associated with the challenge to learn the dynamics, approximate the first order return map, and predict the bifurcation behavior of a complex nonlinear system. It is based on the dynamic model of a one-legged hopper, see Figure 6. In comparison to the Duffing oscillator, it stands for a far more complex class of dynamical systems, which are not only nonlinear, but also hybrid and non-smooth and which cannot be modeled using a single set of ordinary differential equations. Such dynamics greatly increase the complexity associated with simulation and optimization problems. They are found, for example, in robotic systems that interact with the environment such as in object manipulation or legged locomotion. This interaction introduces non-smooth dynamical effects (rapid changes in a system's velocities), necessitating more sophisticated simulation tools that need to detect when such non-smooth effects happen and treat their effects appropriately.\nIn periodic tasks, such as legged locomotion, the resulting motion is often regarded on the basis of a recurrence map, which computes how the state is updated during one full period. As a consequence, the resulting simulations are often computationally expensive. This holds in particular, when gradient information must be computed within the simulation to obtain sensitivities. This derivative information is of great importance not only for the study of the bifurcation behavior, but also for understanding stability and developing efficient and robust controllers for robotic systems. Despite the high complexity involved in generating time-series data for non-smooth dynamical systems, the resulting dataset can still be low dimensional, which makes it an interesting candidate for learning surrogate functions.\nThe one-legged hopper is a good example of such a hybrid dynamical system. It has only two parameters (swing-leg frequency wswing, leg stiffness k\u2081) and six states (x, y, p, x, y, y). Yet,\nthe system exhibits non-smooth behavior when the foot comes in contact with the ground and generating time-series data is thus quite challenging. As a consequence of the nonlinear dynamics, the hopper also exhibits a rich variety of periodic motions, similar to the Duffing oscillator in the previous section. A projection of these periodic motions is depicted in the bifurcation diagram shown in Figure 6. The hopper also serves as a prominent template model within the fields of biomechanics and robotics [37-39].\nData-driven predictions on the hopper's dynamics can take one of three forms, depending on how its periodic time-series data is interpreted:\n1. The evolution of the time-series data can be used to do a sequence prediction or to train a model for the right-hand side of a discrete dynamical system. This is similar to how the data in the DORA dataset is handled.\n2. Each time-series in the dataset corresponds to a periodic motion and thus represents a fixed point of a first-return map. Learning such a map significantly simplifies the system dynamics by reducing a recurrent motion to a single point. Again, this can be interpreted as a sequence prediction for parameterized dynamics.\n3. The bifurcation diagram can also be learned directly, requiring a model that takes the bifurcation parameter (E) as its sole input.\nThese three approaches to training a machine-learning-based model are listed in order of increasing complexity, as the availability of training and testing data decreases significantly for the latter approaches. The evaluation of the dataset depends on the chosen prediction method. For the first approach, the metrics presented in Equations (3) and (4) are suitable for data evaluation, while the latter two approaches can be assessed by their ability to generate the bifurcation diagram shown in Figure 6. Regarding generalizability, the hopper example belongs to the complex class of non-smooth dynamical systems making suitable machine-learning techniques directly applicable to other datasets from this class. The dataset is publicly available in DaRUS, the curated data repository of the University of Stuttgart [41]. The model is described in detail in [40]. This paper is accompanied by code on GitHub [42]. The time-series data of periodic motions is provided in comma-separated text files encoding matrices of varying sizes. The provided"}, {"title": "Motion Data of a Four-Bar Mechanism", "content": "Creating virtual models of mechanical systems is a common task in engineering to gain better insights and to develop suitable control algorithms. However, this requires careful and extensive parameter identifications of the systems and the resulting models usually do not perfectly describe the behavior due to unknown dynamics, complex state-dependent forces, or other, intentionally unmodeled effects. Utilizing recorded data from hardware experiments can help to improve the virtual model and better align it with the real-world system. This motivates the development of pure data-driven methods or hybrid models that combine data-driven and physics-based approaches such as in [43].\nThe presented dataset contains motion and motor-current data from hardware experiments of a four-bar mechanism. Despite its simplicity, the four-bar linkage is a widely used mechanism, because when carefully designed, it can convert rotational motion into translational motion in a very flexible manner. The mechanism is driven by a Dynamixel XH430-W350-R motor that allows for current control, i.e., desired current values are supplied, which the on-board electronics strive to reach. The prescribed current trajectories serve as the input to the system, whereas the measured, actual current, and the resulting angular positions and the angular velocities of the motor shaft are recorded. Figure 7 shows the hardware prototype and Figure 8 shows one of the recorded trajectories. By mounting a stirrer onto the mechanism and moving it through a viscous liquid, different damping effects can be introduced into the system. The complete dataset, a notebook for intuitive handling of the data, and a detailed description of the mechanism are published in a curated data repository of the University of Stuttgart [44]. They were used by a partner research group to compare two different hybrid-modeling approaches in [45].\nThe measurements can be utilized by others to train an end-to-end data-driven model of the system capable of predicting the behavior without any expert knowledge. Another approach is to build a hybrid model where either an error correction model can be learned or the model parameters are fitted from the data. This can lead to a better understanding of the model and potentially allow insights into the previously unknown dynamics. Additionally, as with any data\nfrom a physical system, the presented data exhibits measurement noise and other disturbances such as coarse measurement resolutions, which are often not dealt with in purely theoretical works on data-driven methods in engineering, but which are a regular occurrence and key limiting factor on real hardware. Hence, the dataset can also be used to test different preprocessing strategies for handling noisy and quantized data.\nFor the evaluation of proposed models, a part of the data can be held back as test datasets to assess the prediction accuracy and generalization capabilities. Various metrics can be defined such as the mean error for a one-step prediction or comparisons of the complete time series data to check for slow drifts over time. Even though the system is conceptually simple, learning models from hardware data exhibits a high problem complexity due to unknown external influences such as manufacturing inaccuracies, the presence of measurement noise and quantization, imperfections of the motor unit, and introduced damping stemming from the fluid-structure interaction of the stirrer with a viscous liquid. Concerning the interpretability, the dataset itself is easy to understand and is provided in text files which can be quickly loaded into any programming environment. Additionally, a python notebook is provided to guide the user through the data and to show how to load and visualize it. This allows also non-domain experts to develop own data-driven models without any system knowledge on a simple mechanical system commonly used in engineering. Furthermore, the dataset can be utilized as a benchmark to compare against hybrid models developed by domain experts. Regarding generalizability and the scarcity of data from hardware experiments, the presented dataset also serves as a starting point for the development and verification of algorithms applicable to more complex systems."}, {"title": "Conclusions and Outlook", "content": "Data science, machine learning, and artificial intelligence have in recent years expanded from first success stories in computer vision and natural language processing to progress in natural sciences and engineering. Regarding the latter, artificial intelligence also starts to have a lasting impact on engineering design, and will transform how humanity conceives new machines, vehicles, robots, etc. As this article has outlined, engineering design brings about characteristic perspectives and challenges of which not every aspect and challenge are present for the currently"}]}