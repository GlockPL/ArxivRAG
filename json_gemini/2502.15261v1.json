{"title": "Corrections Meet Explanations: A Unified Framework for Explainable Grammatical Error Correction", "authors": ["Jingheng Ye", "Shang Qin", "Yinghui Li", "Hai-Tao Zheng", "Shen Wang", "Qingsong Wen"], "abstract": "Grammatical Error Correction (GEC) faces a critical challenge concerning explainability, notably when GEC systems are designed for language learners. Existing research predominantly focuses on explaining grammatical errors extracted in advance, thus neglecting the relationship between explanations and corrections. To address this gap, we introduce EXGEC, a unified explainable GEC framework that integrates explanation and correction tasks in a generative manner, advocating that these tasks mutually reinforce each other. Experiments have been conducted on EXPECT, a recent human-labeled dataset for explainable GEC, comprising around 20k samples. Moreover, we detect significant noise within EXPECT, potentially compromising model training and evaluation. Therefore, we introduce an alternative dataset named EXPECT-denoised, ensuring a more objective framework for training and evaluation. Results on various NLP models (BART, T5, and Llama3) show that EXGEC models surpass single-task baselines in both tasks, demonstrating the effectiveness of our approach.", "sections": [{"title": "1 Introduction", "content": "Writing proficiently poses significant challenges for language learners who often struggle to produce grammatically correct and coherent texts (Li et al., 2024c, 2022d; Ye et al., 2023c). Therefore, GEC systems (Bryant et al., 2023) are developed to detect and rectify all grammatical errors in texts (Ye et al., 2023a; Huang et al., 2023). Research advancements in GEC encompass multi-language (Rothe et al., 2021; Li et al., 2023c; Ma et al., 2022), multi-modality (Fang et al., 2023; Li et al., 2022c, 2024d), and document-level (Yuan and Bryant, 2021; Du et al., 2024). However, the explainability of GEC (Hanawa et al., 2021; Ye et al., 2024a,b) remains under-explored due to its intrinsic difficulties. Given that neural GEC systems generally function as intricate black-box models, their internal processes are not transparent (Zhao et al., 2023). The absence of explainability can result in inadequacies in educational scenarios, where L2 learners might find it difficult to completely understand outputs from GEC systems without knowing the rationale behind corrections. Providing corrections with explanations fosters appropriate trust by clarifying the linguistic principles and logical mechanisms underpinning model decisions in a comprehensible way, thereby aiding educationally K-12 students and L2 speakers (Bitchener et al., 2005; Sheen, 2007; Li et al., 2025; Ye et al., 2025). Moreover, explainability offers insights that help identify unintentional biases and risks for researchers, functioning as a debugging tool to enhance model performance (Ludan et al., 2023; Zhang et al., 2023a). The paper focuses on EXPECT (Fei et al., 2023), an explainable GEC dataset characterized by human-labeled evidence words and grammatical error types annotations, designed to assist language learners in understanding the corrections from GEC systems. These evidence words, referred to as extractive rationales, provide precise cues for corrections, thereby enabling learners to comprehend the rationale underlying the corrections. The error types within EXPECT encompass 15 categories grounded in pragmatism (Skehan, 1998; Gui, 2004), facilitating learners in inferring abstract grammatical rules from particular errors through inductive reasoning. However, existing studies (Song et al., 2024) primarily concentrate on post hoc explanation, neglecting the interaction between the explanation and correction tasks as represented in Figure 1. To explore the interaction between explanation and correction tasks, we introduce EXGEC (Explainable Grammatical Error Correction), a unified multi-task explainable GEC framework that formulates the multi-task problem as a generative task. The framework can jointly correct ungrammatical sentences, extract evidence words, and classify grammatical errors (Zou et al., 2025) in different prediction orders within an architecture. Our research indicates that learning correction and explanation tasks together can be mutually beneficial and the prediction orders affect the task performance. More specifically, pre-explaining models achieve better correction performance but lower explanation performance compared to post-explaining models. Nevertheless, both models show improved or comparable correction and explanation performance compared to their respective baselines. Moreover, we find that EXPECT is not an ideal dataset for explainable GEC. This is due to the presence of numerous unidentified grammatical errors in EXPECT, which would disturb the extraction of evidence words and the prediction of grammatical errors. Therefore, it will lead to a bias in the training and evaluation process. Consequently, we reconstruct EXPECT to correct the unidentified errors while ensuring each sentence contains only one distinct error (Fei et al., 2023). The resulting dataset is called EXPECT-denoised. By training and evaluating EXGEC models on our proposed EXPECT-denoised, we can obtain unbiased results reflecting their real abilities in both the correction and the explanation tasks. In summary, our contributions are three folds:\n(1) We present EXGEC, a comprehensive framework that integrates correction and explanation components. This adaptable design facilitates the investigation of the interplay between correction and explanation tasks when utilizing various prediction sequences.\n(2) We recognize a potential critical limitation in EXPECT and reconstruct it into EXPECT-denoised, thereby enhancing the training and evaluation framework for EXGEC models.\n(3) We perform extensive experiments employing three language models (BART, T5, and Llama3) to demonstrate the beneficial interaction between the two tasks and substantiate the efficacy of our approach."}, {"title": "2 Related Work", "content": "Many GEC systems focus solely on correction without offering explanations (Davis et al., 2024; Ye et al., 2022, 2023b). To address this limitation, recent research has investigated multiple techniques to enhance the explainability of GEC. One technique is Example-based GEC (Kaneko et al., 2022; Vasselli and Watanabe, 2023), which boosts explainability by retrieving examples that are similar to the input instance based on defined grammar rules. GEE (Song et al., 2024) develops a two-step pipeline for GEC explanation generation. Kaneko and Okazaki (2024) explore the generation of natural language explanations by prompting large language models (LLMs). Another relevant task is feedback comment generation (FCG) (Nagata, 2019; Nagata et al., 2021; Hanawa et al., 2021), which aims to automatically create feedback comments, like hints or explanatory notes, to aid in writing learning. However, it suffers from expensive costs associated with data annotation (Nagata et al., 2020). Furthermore, it is often explored with limited access to only a subset of English grammatical error types due to the complexity of the task (Nagata, 2019).\nDespite these efforts, no research has comprehensively examined the positive interaction between correction and explanation tasks during training. In contrast, our work focuses on studying whether learning a multi-task model can outperform the respective single-task models."}, {"title": "2.2 Learning with Explanations", "content": "Explainability of NLP tasks is a critical research direction and has been given serious attention recently, especially due to the \"black box\" nature of LLMs (Dalal et al., 2024; Hu and Yu, 2024; Yu et al., 2024a; Saeed et al., 2024; Yu et al., 2024b; Xu et al., 2025). Prior studies have shown that training models to produce task predictions and explanations concurrently can boost performance in vision-language tasks (Majumder et al., 2022; Li et al., 2023b, 2024b) and several downstream NLP tasks, including text classification (Li et al., 2022a, 2024e), commonsense reasoning (Veerubhotla et al., 2023; Yan et al., 2025; Huang et al., 2024b; Li et al., 2022b, 2023d), and complaint detection (Singh et al., 2023). An essential aspect of this research is the development of self-Rationalization models that generate task predictions along with corresponding explanations to enhance the explainability or task performance of neural networks. There are two main methods for building self-Rationalization models: 1) extracting key input tokens responsible for task predictions, referred to as extractive rationales (DeYoung et al., 2020), and 2) creating natural language explanations (Narang et al., 2020), which serve as a natural interface between models and human users. To refine the performance and trustworthiness of Seq2Seq models, Lakhotia et al. (2021) developed an extractive fusion-in-decoder architecture within the ERASER benchmark (DeYoung et al., 2020), a well-known benchmark for rationale extraction across various datasets and tasks. Li et al. (2022a) introduced a combined text classification and rationale extraction model to improve explainability and robustness. Recognizing the synergy between extractive rationales and natural language explanations, Majumder et al. (2022) integrated both components into a self-Rationalization framework.\nExplanation-augmented knowledge distillation. Leveraging in-context learning (Brown et al., 2020) and the chain-of-thought (CoT) reasoning (Chu et al., 2023) of LLMs, many recent studies employ the natural language explanations produced by LLMs with chain-of-thought prompting (Lampinen et al., 2022; Li et al., 2023a) to enhance the development of smaller reasoning models using knowledge distillation (Zhang et al., 2024), thereby boosting task performance (Li et al., 2024a; Ho et al., 2023; Hsieh et al., 2023) or improving faithfulness (Wang et al., 2023). However, convincing and wrong explanations generated by LLMs can foster unwarranted confidence in tackling NLP tasks (Madsen et al., 2024; Pruthi et al., 2022), particularly in educational contexts emphasizing faithfulness (Lyu et al., 2024) and correctness (Huang et al., 2024a). Consequently, this paper emphasizes model training with human-annotated datasets."}, {"title": "3 Motivation and Methodology", "content": "The objective of this study is to simultaneously address the correction and explanation tasks using a Seq2Seq-based generation approach. Specifically, given an ungrammatical input sentence X = {X0, X1, . . . , Xn}, where n represents the length of the input sentence, the joint models are designed to learn both tasks. The correction task involves converting the ungrammatical input into a grammatical output Y = {yo, Y1, . . . , Ym}, where m denotes the length of the output. The explanation task is divided into two sub-tasks: 1) classifying grammatical errors, and 2) extracting evidence words. For the classification sub-task, the joint models are required to output a grammatical error type label c (c\u2208 C), where C consists of 15 possible grammatical error type classes as defined in EXPECT. For the extraction sub-task, the models must extract evidence words E(X) = {eo, e1, . . . , ek} C X that provide clear and comprehensive clues for corrections."}, {"title": "3.2 Explainable GEC as Generation Task", "content": "We introduce four distinct training settings to examine the interaction between explanation and correction tasks during the training phase, as illustrated in Figure 2. These configurations are as follows: 1) the absence of explanations (Baseline), which represents the conventional GEC setup, 2) the integration of explanations as supplementary input (Infusion), 3) the generation of explanations as output (Explanation), and 4) the inclusion of explanations as an auxiliary output (Self-Rationalization). This paper presents EXGEC, a unified generative framework designed for explainable grammatical error correction, which seamlessly integrates multiple settings within a singular architecture. In the Infusion setting, a special token, \"<sep>\", acts as a delimiter between the source sentence and the ensuing explanation, comprising evidence words and the identified error type. Conversely, in the Explanation setting, the model derives an explanation based solely on the information provided by the source sentence. Within the Self-Rationalization paradigm, the models concurrently generate both a correction and an explanation. The Self-Rationalization setting is depicted in detail in Figure 2. Other settings can be adapted with minimal modifications. Moreover, the sequence in which corrections and explanations are predicted can be altered, facilitating a deeper understanding of task interactions.\nWe first describe how our EXGEC addresses tasks in a unified generative framework within the Self-Rationalization setting. Utilizing a pointing mechanism (Vinyals et al., 2015), EXGEC can identify evidence words by directly generating the source indices of an ungrammatical sentence, thus eliminating the possibility of generating invalid evidence words. Given an ungrammatical source sentence X, the encoder translates X into a hidden representation H as follows:\n$H^e = Encoder(X),  (1)$\nwhere $H^e \u2208 R^{n\u00d7d}$, and d is the hidden size. At each time step t, the decoder produces the hidden state $h^d \u2208 R^d$ based on the previous output sequence $\u0176_{<t}$, which is computed as follows:\n$h^d = Decoder(H^e, \u0176_{<t}).(2)$\nNext, the hidden state $h^d \u2208 R^d$ is utilized to compute three forms of logits: 1) correction token logits, responsible for the correction phase (Vaswani et al., 2017), 2) evidence pointer logits, which determine the probabilities of source indices for evidence extraction, and 3) error type logits, used for classifying error types. Drawing inspiration from Yan et al. (2021), we add an extra MLP layer (Liu et al., 2022) and calculate the probability distribution Pt as follows:\n$E = TokenEmbed(X) \u2208 R^{n\u00d7d}, (3)$\n$H^o = \u03b1E + (1 \u2212 \u03b1) MLP(H^e) \u2208 R^{n\u00d7d}, (4)$\n$V^d = TokenEmbed(V) \u2208 R^{|V|\u00d7d}, (5)$\n$C^d = TypeEmbed(C) \u2208 R^{|C|\u00d7d}, (6)$\n$P_t = softmax([V^d \u2297 h^d_t; H^e \u2297 h^d_t; C^d \u2297 h^d_t]), (7)$\nwhere TokenEmbed refers to the embeddings that are shared between the encoder and decoder, \u03b1 \u2208 R is a hyper-parameter responsible for balancing the trade-off between embeddings and encoder hidden representation, V represents the token vocabulary, [;] denotes the concatenation operation in the first dimension, the symbol \u2297 means the dot product operation, and $P_t \u2208 R^{|V|+n+|C|}$ represents the probability distribution at the current time step t.\nThe pointer index cannot be directly inputted into the decoder. So we utilize the Index2Token conversion to transform the indexes into tokens (Yan et al., 2021). Furthermore, the sequence of generating corrections and explanations can be restructured, potentially offering valuable insights into the deeper understanding of the interaction between both tasks. The probability distribution of Baseline and Infusion models is restricted to the token vocabulary. Conversely, the probability distribution of Explanation models is confined to the combination of pointer indexes and error types."}, {"title": "3.3 Loss Weighting", "content": "Taking into account the heterogeneity of correction and explanation tasks, we construct the overall loss function in the form of a weighted sum, which is defined as follows:\n$L = L_{cor} + \u03bb L_{exp} = \\sum_{i=0}^m [I(y_i \u2208 V ) log p_i + \u03bbI(y_i \\notin V ) log p_i],(8)$\nwhere \u03bb is responsible for balancing both tasks and I is the indicator function. During the inference stage, we generate the entire target sequence in an autoregressive manner and then separate different parts from the target."}, {"title": "4 EXPECT-denoised Dataset", "content": "Our experiments employ the EXPECT dataset (Fei et al., 2023) with human-labeled explanations instead of other LLM-generated explanations (Song et al., 2024), ensuring the reliability of explanations. The source and target sentences of EXPECT draw from W&I+LOCNESS (Bryant et al., 2019), an explanation-free GEC dataset encompassing a broader spectrum of English proficiency levels. EXPECT follows a unique methodology for task simplification. Specifically, for a W&I+LOCNESS sentence with n grammatical errors, the sentence is replicated n times, each time keeping only one individual error. Considering the complexities of explainable GEC, this approach is sensible and preferred, as it simplifies the task by isolating and extracting evidence for one grammatical error at a time, thereby avoiding the confusion caused by multiple interacting errors within a single sentence.\nHowever, we contend that the original EXPECT dataset includes unidentified grammatical errors, thus biasing the training and the evaluation process. For original sentences with n (n > 1) grammatical errors from W&I+LOCNESS, the authors of EXPECT correct a single error and leave the remaining n \u2212 1 errors uncorrected, as illustrated in Table 1. These uncorrected errors can confuse models, creating uncertainty about which error to correct and explain, thus complicating model training and evaluation.\nTo address this issue, we re-correct the previously overlooked grammatical errors while preserving the original single error in EXPECT. This rebuilding process is fully automated. Specifically, we re-correct all uncorrected errors by comparing sentences from EXPECT with those in W&I+LOCNESS. First, we retrieve the original parallel samples from W&I+LOCNESS using the open-source toolkit TheFuzz2. Then, we identify and correct the underlying grammatical errors using GEC evaluation tools ERRANT (Bryant et al., 2017) and CLEME (Ye et al., 2023d). Notably, during the reconstruction, the grammatical errors, evidence words, and error types in both the EXPECT and EXPECT-denoised datasets are preserved, except in a few extreme cases (one sample from the development set and one from the test set lack evidence words due to overlapping with the uncorrected errors). In total, 277 (1.82%), 1,311 (54.33%), and 1,323 (54.76%) sentences in our reconstructed training, development, and test sets, respectively, differ from their originals in EXPECT. All these altered samples, initially detected with unidentified grammatical errors, are subsequently re-corrected in EXPECT-denoised. By ensuring that the evidence words and error types align with the single remaining grammatical error, EXPECT-denoised facilitates unbiased training and evaluation. Detailed dataset statistics are provided in Table 2."}, {"title": "5 Experiments", "content": "We employ the pre-trained language models BART-Large (Lewis et al., 2020) and T5-Base (Raffel et al., 2020), in conjunction with the decoder-only Llama3-8B (Dubey et al., 2024), as our foundational models. These models have been demonstrated to serve as effective backbones for state-of-the-art GEC models in prior research (Ye et al., 2023a; Zhang et al., 2023b; Wang et al., 2024). It is worth noting that tokenization may divide a word into multiple BPE units, thereby complicating the extraction of evidence words. Considering that evidence words are typically concise and contiguous, we mandate that the pointer indices encompass all BPE units of the evidence words. In cases where an instance does not contain an evidence word, the target does not predict any pointer index. The specifics of fine-tuning and the hyperparameter configurations are detailed in Appendix A."}, {"title": "3.1 Problem Definition", "content": "As outlined in Section 3.2, we aim to conduct experiments within four distinct training settings using a single unified framework with minimal adjustments. In line with the standard approach, the Baseline and Explanation settings train models to generate corrections or explanations from input source sentences. In contrast, the Infusion models are trained to generate corrections from input source sentences and human-labeled explanations. Significantly, the Self-Rationalization setting is further categorized into two sub-settings based on the sequence of generating corrections and explanations: 1) pre-explaining models produce the explanation first, followed by the correction, whereas 2) post-explaining models generate the correction first and then the explanation. Generally, we first extract evidence words and subsequently predict error types, as our preliminary experiments indicate that the prediction order of evidence words and error types does not substantially impact performance."}, {"title": "5.2 Comparison of Original and\nEXPECT-denoised Datasets", "content": "We first demonstrate the effectiveness of our reconstruction method. To this end, we independently train models respectively on the original EXPECT dataset and its reconstructed version. As shown in Table 3, our EXPECT-denoised dataset significantly enhances performance across both tasks, with the average correction $F_{0.5}$ and explanation $F_{0.5}$ scores increasing by 12.9% and 4.5%, respectively. These improvements underscore the high quality of the denoised samples, which result from detecting and rectifying previously overlooked grammatical errors. Consequently, all subsequent experiments will be conducted using the EXPECT-denoised dataset."}, {"title": "5.3 Main Results", "content": "Here, we inspect and evaluate the relationship between the correction and the explanation tasks through various experimental training setups. The main results presented in Table 4 reveal the following insights.\nEvidence words offer more valuable information than grammatical error types for corrections. First, we investigate the Infusion setting, where we append different supplementary explanation data to the input source. Infusion models serve as an oracle since human-annotated explanations are typically lacking in real-world applications, allowing us to understand the impact of explanations on the correction task. Recent research has demonstrated that adding human-annotated explanations as additional input can improve task performance (Hase et al., 2020; Yao et al., 2023), and we have observed similar results in the Infusion setting. Specifically, we find that the additional information provided by grammatical error types enhances the correction performance of Llama3, but does not positively affect the correction performance of BART and T5. We suspect the significant enhancement of Llama3's correction performance is due to that Llama3 can understand the semantic meaning of error types. In contrast, offering evidence words can consistently boost the correction $F_{0.5}$ scores by 5~20 points for three different language models, even though only about 60% of the samples in the dev and test sets are annotated with evidence words, showing that accurate evidence words are very beneficial for the correction task.\nJointly learning the two tasks is advantageous. In practical applications, explanations are generally unavailable during inference. Thus, Self-Rationalization models investigate whether incorporating explanations as additional output can enhance performance. The Self-Rationalization models' correction $F_{0.5}$ scores improve by an average of 1.3 points for BART and 0.88 points for T5 compared to the Baseline setting. Similarly, explanation $F_{0.5}$ scores increase by an average of 1.3 points for BART, 1.8 points for T5, and 17.7 points for Llama3, compared to the Explanation setting. The error classification accuracy scores see an average improvement of 9.8 points for BART, 0.9 points for T5, and 6.8 points for Llama3.\nPost-explaining models perform better in the explanation performance. Experiments reveal variations in performance between models that explain beforehand and those that explain afterward across three language models. We observe that post-explaining models predict evidence words and error types more accurately. This could be attributed to the challenge of predicting evidence words and error types without a specific correction. Furthermore, pre-explaining models exhibit marginally better correction performance for BART, contrasting with the trends observed in other models.\nFurthermore, a BERT model based on sequence labeling is trained under consistent training and evaluation conditions from Fei et al. (2023). The generative language models exhibit significantly lower performance in grammatical error type classification compared to BERT-based models, which we hypothesize is due to intrinsic biases introduced by the differences in auto-regressive text generation and BERT's masked language model (MLM) pre-training objectives. This hypothesis is supported by the experiments in Section 6.1, indicating that sequence labeling is not essential for grammatical error type classification. We report detailed results of EXGEC models in Appendix B and provide an investigation of varying loss weights \u03bb in Appendix C."}, {"title": "6 Analyses", "content": "In this section, we adopt BART-Large for further analyses, aiming to provide insights into our framework design and the effects of explanations."}, {"title": "6.1 Does Sequence Labeling Help?", "content": "Motivated by recent studies in multi-task GEC frameworks (Zhao et al., 2019; Yuan et al., 2021), which combine a sequence labeling task with a sentence-level correction task, we also develop a multi-task baseline for explainable GEC, keeping the experimental setup the same as our other experiments. Specifically, we append a randomly initialized tagging head after the encoder to perform the explanation task as a sequence labeling task, like BERT-based models. To predict each token's tag, we pass the encoder's hidden representation $H^e$ through a softmax after an affine transformation, which is computed as follows:\n$P(T | X) = softmax(WH^e), (9)$\nwhere T denotes the tagging sequence in the BIO scheme. To replace the pointer mechanism, we employ a token-level sequence labeling task, focusing solely on the correction task in the decoder. We also implement loss weighting to balance the correction generation and sequence labeling losses, defined as follows:\n$L = L_{cor} + \u03b3 \u00b7 L_{tag}, (10)$\nwhere \u03b3 represents the trade-off factor, and we minimize the cross-entropy between predicted tokens/labels and ground truth tokens/labels.\nThe outcomes of varying \u03b3, chosen from the alternative set {0.5, 0.8, 1.0, 1.5, 2.0}, are presented in Table 5. Compared to Self-Rationalization models, sequence labeling-based multi-task models demonstrate inferior correction performance; however, they provide an intermediary level of explanation performance between pre-explaining and post-explaining models. So it can be inferred that our proposed EXGEC exhibits greater efficacy than sequence labeling-based baselines."}, {"title": "6.2 Position Leakage", "content": "One might suspect that the improved performance of Infusion models is driven, at least in part, by a positional leakage effect\u2014where evidence words often appear within the first- or second-order nodes of correction words in the dependency parsing tree (Fei et al., 2023). To address this concern, we synthesize datasets with artifact explanations using two methods: (1) random explanations, which are randomly sampled from the entire set of source tokens, and (2) adjacent explanations, which are randomly chosen from candidate source tokens located within a distance of 1 to 5 tokens from the correction position (noting that over 90% of evidence words in EXPECT fall within this range).\nGiven that many samples lack annotated evidence words, we generate a number of synthesized evidence words matching the count of the ground-truth annotations to ensure experimental fairness. While our models are trained using these synthesized evidence words, evaluation is conducted with the ground-truth evidence words. This setup allows us to investigate whether the models can effectively learn to extract evidence words through an unsupervised approach. The results of these experiments are presented in Table 6.\nUnder the Infusion setting, it is unsurprising that incorporating random evidence words does not enhance correction performance as we anticipated. However, we observe that using adjacent synthesized evidence words produces a noticeable impact\u2014yielding a moderate improvement over random evidence words, albeit still falling short of the gains achieved with ground-truth evidence words. This indicates that a positional leakage effect does exist, although it alone cannot fully realize the benefits of ground-truth evidence.\nIn the pre-explaining and post-explaining settings, generating adjacent evidence words yields a modest improvement in correction performance. However, this gain remains inferior to the enhancement achieved when incorporating ground-truth evidence words, underscoring the critical importance of jointly training the correction and explanation tasks. In contrast, employing random evidence words offers no discernible benefit for the correction task, and further evaluations reveal that their contributions are largely disregarded in the model's explanations. Notably, the absence of ground-truth evidence words results in a marked decline in explanation performance\u2014reflecting the inherent challenge of aligning model-generated explanations with human preferences in an unsupervised context.\nThese findings emphasize that high-quality and contextually relevant evidence is essential for improving correction accuracy and producing explanations that resonate with human evaluators."}, {"title": "7 Limitations", "content": "The limited nature of EXPECT. The explanations provided in the EXPECT and EXPECT-denoised datasets are confined to simple evidence words and types of grammatical errors. These may not be intuitive or comprehensive for L2 speakers, who are usually the primary users of educational GEC systems. However, our experiments demonstrate that the explanations can significantly aid the correction task by effectively utilizing the EXPECT-denoised dataset. In our future work, we aim to investigate more general free-text explanations in real-world GEC applications, presenting an encouraging direction for enhancing user-focused GEC systems.\nEXPECT and EXPECT-denoised suppose only one grammatical error exists in a sentence, which is a less realistic setting. Due to the undeveloped research on the explainability of GEC, there are few high-quality datasets to study. Focusing on the EXPECT-denoised dataset, our experiments follow the setting even though the proposed EXGEC can flexibly adapt to other real-life settings.\nInherent nature of generative language models. It has been observed that the backbone models we employ, specifically BART, T5, and Llama3, exhibit deficiencies in classifying grammatical errors when compared to BERT-based models. This shortcoming can be ascribed to their fundamental characteristics as generative language models. Such limitations may adversely affect correction performance, especially in post-explanatory models that rectify sentences by relying on previously forecasted explanations. Our future research intends to explore more effective methodologies for managing and integrating both these tasks."}, {"title": "8 Conclusion", "content": "This paper aims to improve the explainability of GEC systems. To this end, we introduce EXGEC, a unified generative framework that simultaneously performs correction and explanation tasks. The framework empirically explores and establishes the interaction between the two tasks. We then identify the potential drawback of EX\u0420\u0415\u0421\u0422 and propose EXPECT-denoised. Results on three language models reveal performance enhancement when multi-task learning correction and explanation tasks. In the future, we plan to extend our framework to free-text explanations."}, {"title": "A Fine-tuning Details", "content": "We update all parameters of BART-Large and T5-Base during fine-tuning. For Llama3-8B, we leverage LORA (Hu et al., 2022) to implement parameter-efficient fine-tuning. Detailed hyperparameter configurations are provided in Table 8."}, {"title": "C Impact of Loss Weighting", "content": "This section examines the balance between learning for correction and explanation tasks by adjusting the loss weight \u03bb in Equation (8). An elevated value of \u03bb indicates an increased focus on learning the explanation task. We experiment with various models employing different loss weights \u03bb selected from {0.5, 1.0, 1.5, 2.0}, and present our results in EXPECT-denoised-dev in Table 9. The findings indicate that focusing predominantly on a single task may cause a deterioration in the performance of both tasks. We hypothesize that with a reduced \u03bb, the weak supervisory signals in the explanations hinder the model's capacity to generate accurate explanations, thereby compromising the correction learning process. Conversely, a larger \u03bb might neglect correction learning, leading to subpar explanation performance since the quality of explanations depends on the predicted corrections in post-explaining models. An optimal \u03bb must be chosen to attain a balance between the learning of both tasks and achieve mutually beneficial results."}]}