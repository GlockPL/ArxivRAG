{"title": "Feature Fusion Transferability Aware Transformer for Unsupervised Domain Adaptation", "authors": ["Xiaowei Yu", "Zhe Huang", "Zao Zhang"], "abstract": "Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from labeled source domains to improve performance on the unlabeled target domains. While Convolutional Neural Networks (CNNs) have been dominant in previous UDA methods, recent research has shown promise in applying Vision Transformers (ViTs) to this task. In this study, we propose a novel Feature Fusion Transferability Aware Transformer (FFTAT) to enhance ViT performance in UDA tasks. Our method introduces two key innovations: First, we introduce a patch discriminator to evaluate the transferability of patches, generating a transferability matrix. We integrate this matrix into self-attention, directing the model to focus on transferable patches. Second, we propose a feature fusion technique to fuse embeddings in the latent space, enabling each embedding to incorporate information from all others, thereby improving generalization. These two components work in synergy to enhance feature representation learning. Extensive experiments on widely used benchmarks demonstrate that our method significantly improves UDA performance, achieving state-of-the-art (SOTA) results.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks (DNNs) have achieved remarkable breakthroughs across various application fields owing to their impressive automatic feature extraction capabilities. However, such success often relies on the availability of large labeled datasets, which can be challenging to acquire in many real-world scenarios due to the significant time and labor required. Fortunately, unsupervised domain adaptation (UDA) techniques [40] offer a promising solution by harnessing rich labeled data from a source domain and transferring knowledge to target domains with limited or no labeled examples. The essence of UDA lies in identifying discriminant and domain-invariant features shared between the source domain and target domain within a common latent space [44]. Over the past decade, as interests in domain adaptation research have grown, numerous UDA methods have emerged and evolved [20, 24, 50], such as adversarial adaptation, which focuses on discriminating domain-invariant and domain-variant features and acquiring domain-invariant feature representations through adversarial learning [24, 52]. Besides, deep unsupervised domain adaptation techniques usually employ a pre-trained Convolutional Neural Network (CNN) backbone [19].\nRecently, the self-attention mechanism and vision transformer (ViT) [7,41,48] have received growing interest in the vision community. Unlike convolutional neural networks that gather information from local receptive fields of the given image, ViTs leverage the self-attention mechanism to capture long-range dependencies among patch features through a global view. In ViT and many of its variants, each image is partitioned into a series of non-overlapping fixed-size patches, which are then projected into a latent space as patch tokens and combined with position embeddings. A class token, representing the entire image, is prepended to the patch tokens. All tokens are then fed into a specific number of transformer layers to learn visual representations of the input image. Leveraging the superior global content capture capability of the self-attention mechanism, ViTs have demonstrated impressive performance across various vision tasks, including image classification [7], video understanding [11], and object detection [1].\nDespite increasing interest, only a few studies have explored the application of ViTs for unsupervised domain adaptation tasks [34, 43, 44, 50]. In this work, we introduce a novel Feature Fusion Transferability Aware Transformer, designed for unsupervised domain adaptation. FFTAT builds upon TVT [44], the first ViT-based UDA model, by introducing two key components: (1) a transferability graph-guided self-attention (TG-SA) mechanism that enhances information from highly transferable features while suppressing information from less transferable features, and (2) a carefully designed features fusion (FF) operation that makes each embedding incorporate information from other embeddings in the same batch. Fig. 1 illustrates the transferability graph guided self-attention and feature fusion.\nFrom a graph view, vanilla self-attention among patches can be seen as an unweighted graph, where the patches are considered as nodes, and the attention between nodes is regarded as the edge connecting them. Unlike vanilla self-attention, our proposed transferability graph guided self-attention is controlled by a weighted graph, where the information communication between highly transferable patches is emphasized via a large-weight edge, and the information communication between less transferable patches is attenuated by a small-weight edge [47,48]. The transferability graph is automatically learned and updated through learning iterations in the transferability-aware layer, where we design a patch discriminator to evaluate the transferability of each patch. The TG-SA allows for integrative information processing, facilitating the model to focus on domain-invariant features shared between domains and gather important information for domain adaptation. The Feature Fusion (FF) operation enables each embedding to integrate information from other embeddings. Different from recent work PMTrans [53] for unsupervised domain adaptation, our feature fusion occurs in the latent space rather than on the image level.\nThese two new components synergistically enhance robust feature representation learning and generalization in UDA tasks. Extensive experiments on widely used UDA benchmarks demonstrate that FFTAT significantly improves UDA performance, achieving new state-of-the-art results. In summary, our contributions are as follows:\n\u2022 We introduce a novel transferability graph-guided attention mechanism in ViT architecture for UDA, enhancing performance by promoting attention between highly transferable features while suppressing attention between less transferable ones.\n\u2022 We propose a feature fusion technique that enhances feature learning and generalization capabilities for UDA.\n\u2022 Our proposed model, FFTAT, integrates transferability graph-guided attention and feature fusion mechanisms, resulting in notable advancements and state-of-the-art performance on widely used UDA benchmarks."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Unsupervised Domain Adaptation", "content": "UDA aims to learn transferable knowledge across the source and target domains with different distributions [25,"}, {"title": "2.2. Data Augmentation", "content": "Data Augmentation has been an integral part of modern deep learning vision models. The general idea is to transform the given data without severely altering the semantics. Common data augmentation techniques include random flip and crop [17], MixUp [51], CutOut [6], AutoAugment [14] RandAugment [3] and so on. While these techniques are generally employed in the image space, recent works have explored data augmentation in the embedding space and found promising outcomes in different applications, such as supervised image classification [37,46] and semi-supervised learning [15]. Our feature fusion can be viewed as data augmentation in embedding space for unsupervised domain adaptation."}, {"title": "3. Method", "content": null}, {"title": "3.1. Preliminaries", "content": "Let $D_s = \\{(x_i^s, y_i^s)\\}_{i=1}^{n_s}$ represents data in the labeled source domain, where $x$ denotes the images, $y$ denotes the corresponding labels, and $n_s$ denotes the number of samples in labeled source domain. Similarly, let $D_t = \\{(x_i^t)\\}_{i=1}^{n_t}$ represents data in the target domain, consisting $n_t$ images but no labels. UDA algorithms aim to learn transferable knowledge to minimize domain discrepancy, thereby achieving the desired prediction performance on unlabeled target data. A common approach is to devise an objective function that jointly learns feature embeddings and a classifier. This objective function is formulated as follows:\n$L = L_{CE} + \\alpha L_{dis}$                                                                                                                                                                            (1)\nwhere $L_{CE}$ is the standard cross-entropy loss supervised in the source domain, while $L_{dis}$ denotes the divergence loss, which varies in implementation across different algorithms. Hyperparameter $\\alpha$ is used to balance the weight of $L_{dis}$."}, {"title": "3.2. Overview of FFTAT", "content": "We aim to advance ViT-based solutions for Unsupervised Domain Adaptation. Fig. 2 illustrates the overall framework of our proposed FFTAT. The framework employs a vision transformer backbone, consisting of a domain discriminator for the images (using the class tokens), a patch discriminator for assessing the transferability of the patches, a self-clustering module, and a classifier head. The vision transformer backbone is equipped with our proposed transferability graph-guided attention mechanism (comprising the Transferability Graph Guided Transformer Layer and the Transferability Aware Transformer Layer in Fig. 2), and a feature fusion mechanism (implemented as the Feature Fusion Layer, as shown in Fig. 2).\nDuring training, both the source and target images are used. Each image is divided into non-overlapping fixed-size patches which are linearly projected into the latent space and concatenated with positional information. A class token is prepended to the patch tokens. All tokens are subsequently processed by the vision transformer backbone. From a graph view, we consider the patches as nodes and the attention between the patches as edges. This allows us to manipulate the strength of self-attention among patches by adaptively learning a transferability graph during the training process. In the first iteration, we initialize the transferability graph as an unweighted graph. At the end of each iteration, the patch discriminator evaluates the transferability score of the patches and updates the transferability graph. The learned transferability graph is used to rescale the self-attention in the Transferability Aware Transformer Layer and the Transferability Graph Guided Transformer Layers, amplifying information from highly transferable features while damping the less transferable ones.\nThe Feature Fusion Layer is placed before the Transferability Aware Layer to mix patch token embeddings for better generalization. The classifier head predicts class labels based on the output class token of the labeled source domain images, while the domain discriminator predicts whether images belong to the source or target domain for output class token embeddings of both domains. The self-clustering module utilizes the class token of the target domain to encourage the clustering of learned target domain image representation. Details are introduced below."}, {"title": "3.3. Transferability Aware Transformer Layer", "content": "The patch tokens correspond to partial regions of the image and capture visual features as fine-grained local representations. Existing work [44, 45, 49] shows that the patch tokens are of different semantic importance. In this work, we define the transferability score on a patch to assess its transferability (detailed definition below). A patch with a higher transferability score is more likely to correspond to the highly transferable features.\nTo obtain the transferability score of patch tokens, we adopt a patch-level domain discriminator $D_i$ to evaluate the local features with a loss function:\n$L_{pat}(x, y) = - \\frac{1}{nP} \\sum_{xi \\in D} \\sum_{p=1}^{P} L_{CE} (D_i (G_f (x_{ip})), y_{ip})$                                                                   (2)\nwhere $P$ is the number of patches, $D = D_s \\cup D_t$, $G_f$ is the feature encoder, implemented as ViT, $n = n_s + n_t$, is the total number of images in $D$, $x_{ip}$ denotes the $p$ th patch of the $i$ th image, $y_{ip}^d$ denotes the domain label of the pth token of the ith image, i.e., $y_{ip}^d = 1$ means source domain, else the target domain. $D_i (f_{ip})$ gives the probability of the patch belonging to the source domain, where $f_{ip}$ denotes the features of the pth token of the ith image, i.e., $f_{ip} = G_f (X_{ip})$. During the training process, $D_i$ tries to discriminate the patches correctly, assigning 1 to patches from the source domain and 0 to those from the target domain.\nEmpirically, patches that cannot be easily distinguished by the patch discriminator (e.g., $D_i$ is around 0.5) are more likely to correspond to highly transferable features. These highly transferable features capture underlying patterns that remain consistent despite variations in the data distribution between the source and target domains. In the paper, we use the phrase \u201ctransferability\u201d to capture this property. We define the transferability score of a patch as:\n$c(f_{ip}) = H (D_i (f_{ip})) \\in [0, 1]$                                                                                                              (3)\nwhere $H(\\cdot)$ is the standard entropy function. If the output of the patch discriminator $D_i$ is around 0.5, then the transferability score is close to 1, indicating that the features in the patch are highly transferable. A high transferability score means that the features in a patch are highly transferable, and vice versa. Assessing the transferability of patches allows a finer-grained view of the image, separating an image into highly transferable and less transferable patches. Features from highly transferable patches will be amplified while features from less transferable patches will be suppressed.\nLet $C_i = \\{C_{i1}, ..., C_{iP}\\}$ be the transferability scores of patches of image $i$. The adjacency matrix of the transferability graph can be formulated as:\n$M_{ts} = \\frac{1}{BH} \\sum_{h=1}^{H} \\sum_{i=1}^{B} [C_i C_i^T]_h$                                                                                                        (4)\n$B$ is the batch size, $H$ is the number of heads, $[\\u2022]_x$ means no gradients back-propagation for the adjacency matrix of the generated transferability graph. $M_{ts}$ controls the connection strength of attention between patches.\nThe vanilla Self-Attention (SA) can then be reformulated as Transferability Aware Self-Attention (TSA) in the Transferability Aware Transformer Layer by integrating with transferability scores:\n$TSA(q_{cls}, K, V) = softmax(\\frac{q_{cls} K^T}{\\sqrt{d_k}}) [1; C K_{patch}]V$                                                                                                       (5)\nwhere $q_{cls}$ is the query of the class token, $K$ represents the key of all tokens, including the class token and patch tokens, $K_{patch}$ is the key of the patch tokens, $C K_{patch}$ denotes the transferability scores of the patch tokens, is the dot product, and $[;]$ is the concatenation operation. TSA encourages the class token to take more information from highly transferable patches with higher transferability scores while suppressing information from patches with low transferability scores. The Transferability Aware Multi-head Self-Attention is therefore defined as:\n$T-MSA(q_{cls}, K, V) = Concat(head_1, ..., head_H)W^O$                                                                                                                        (6)\nwhere $head_i = TSA (q_{cls}W_{cls}^i, KW_K^i, VW_V^i)$. Taking them together, the operations in the transferability aware transformer layer can be formulated as:\n$z^{l^\\prime} = T-MSA (LN (z^{l-1})) + z^{l-1}$\n$z^l = MLP (LN (z^{l^\\prime})) + z^{l^\\prime}$                                                                                                                                 (7)\nwhere $z^{l-1}$ are output from previous layer. In this way, the Transferability Aware Transformer Layer focuses on fine-grained features that are highly transferable and are discriminative for classification. Here $l = L$, $L$ is the total number of transformer layers in ViT architecture."}, {"title": "3.4. Feature Fusion", "content": "Emerging evidence shows that adding perturbations enhances model robustness [15,34,46]. To enhance the robustness of the generated transferability graphs and to make the model resistant to noisy perturbations, we propose a novel feature fusion technique into our FFTAT framework, implemented as a Feature Fusion Layer, placed before the Transferability Aware Transformer Layer. Given an image $x_i$, let $b_i = \\{b_{i1},\u2026, b_{iP}\\}$ denote the embeddings of its patches. As illustrated in Fig. 1, each embedding is perturbed by incorporating information from all the other embeddings. We perform the embedding fusion for the source and target domain separately, as instructed in [46]:\n$\\bar{b}_{ip}^s =  \\frac{b_{ip}^s}{\\sqrt{\\frac{1}{B+1} \\sum_{j=1}^{B} \\sum_{j_p=1}^{P} b_{jj_p}^2}} +  \\frac{1}{B} \\sum_{j \\neq i} \\frac{b_{jj_p}^s}{\\sqrt{\\frac{1}{B+1} \\sum_{j=1}^{B} \\sum_{j_p=1}^{P} b_{jj_p}^2}}$\n$\\bar{b}_{ip}^t =  \\frac{b_{ip}^t}{\\sqrt{\\frac{1}{B+1} \\sum_{j=1}^{B} \\sum_{j_p=1}^{P} b_{jj_p}^2}} +  \\frac{1}{B} \\sum_{j \\neq i} \\frac{b_{jj_p}^t}{\\sqrt{\\frac{1}{B+1} \\sum_{j=1}^{B} \\sum_{j_p=1}^{P} b_{jj_p}^2}}$                             (8)\nwhere $B$ is the batch size, $s$ and $t$ indicate the source and target domain. The FF aids in generating more robust transferability graphs and improves model generalizability."}, {"title": "3.5. Transferability Graph Guided Transformer Layer", "content": "As introduced in Section 3.2, we consider the patches as nodes and the attention between patches as edges. The learned transferability information can be effectively and conveniently integrated into the self-attention mechanism by updating the graph, as illustrated in the right part of Fig. 2. With the guidance of the transferability graph, the self-attention in Transferability Graph Guided Transformer Layers will focus on the patches with more transferable features, thus steering the model to learn transferable knowledge across the source and target domain.\nThe transferability graph can be represented by $G = (V,E)$, with nodes $V = \\{V_1,..., V_n\\}$, edges $E = \\{(V_p, V_q)|V_p, V_q \\in V\\}$, and adjacency matrix $M_{ts}$. The transferability graph-guided self-attention for a specific patch $p$ at $l$-th layer in the Transferability Graph Guided Transformer Layer is:\n$b_p^{l+1} = \\sigma (\\frac{q_p^{(l)}(K_{V_p}^{(l)})^T}{\\sqrt{d_k}}) C_{KV, p} U_p$\n                                                                                                                                                                    (9)\nwhere $\\sigma(\\cdot)$ is the activation function, which is usually the softmax function in ViTs, $q_p^{(l)}$ is the query of the $p$ th patch ($p$ th node in $G$), $N(p)$ are the neighborhood nodes of node $p$, $d_k$ is the scale factor with the same dimension of queries and keys, $K_{V_p}^{(l)}$ and $V_{V_p}^{(l)}$ are the key and value of nodes $p$, and $C_{KV, p}^{(l)}$ denotes the transferability scores. Therefore, the transferability graph guided self-attention that is conducted at the patch level can be formulated as:\n$TG-SA(Q, K, V, M_{ts}) = softmax(\\frac{QK^T M_{ts}}{\\sqrt{d_k}})V$                                                                                                                                                                                                                                                          (10)\nwhere queries, keys, and values of all patches are packed into matrices $Q$, $K$, and $V$, respectively, $M_{ts}$ is the adjacency matrix defined in Eq 4. The transferability graph guided multi-head attention is then formulated as:\n$MSA(Q, K, V, M_{ts}) = Concat(head_1, ..., head_H)W^O$                                                                                                                        (11)\nwhere $head_i = TG - SA(QW_Q^i, KW_K^i, VW_V^i, M_{ts})$. The learnable parameter matrices $W_Q^i, W_K^i, W_V^i$ and $W^O$ are the projections. Multi-head attention helps the model to jointly aggregate information from different representation subspaces at various positions. In this work, we apply the transferability guidance to each representation subspace."}, {"title": "3.6. Overall Objective Function", "content": "Since our proposed FFTAT has a classifier head, a self-clustering module, a patch discriminator, and a domain discriminator, there are four terms in the overall objective function. The classification loss is formulated as:\n$L_{clc} (x, y) = \\frac{1}{n_s} \\sum_{x_i \\in D_s} L_{CE} (G_c (G_f (x_i)),y)$                                                                                                             (12)\nwhere $G_c$ is the classifier head, and $G_f$ is the feature extractor, i.e., the ViT with transferability graph-guided self-attention and feature fusion in our work.\nThe domain discriminator takes the class token of images from the source and target domain and tries to discriminate the class token, i.e., the representation of the entire image, to the source or target domain:\n$L_{dis} (x,y) = - \\frac{1}{n} \\sum_{x_i \\in D} (L_{ce}(D_g (G_f (x_i),y)))$                                                                                                                         (13)\nwhere $D_g$ is the domain discriminator, and $y^d$ is the the domain label (i.e., $y^d = 1$ means source domain, $y^d = 0$ is target).\nThe self-clustering module is inspired by the cluster assumption [2] for images from the target domain without labels and the probability $p_t = softmax (G_c (G_f (x_t)))$ of target image $x_t^t$ is optimized to maximize the mutual information with $x^t$ [44]. The self-clustering loss term is formulated as:\n$\\mathbb{I} (p_t; x^t) = \\mathbb{H} (\\bar{p_t}) - \\frac{1}{N_t} \\sum_{i=1}^{N_t} \\mathbb{H} (p_t^i)$                                                                                                                                                                               (14)\nwhere $\\bar{p_t} = \\mathbb{E} [p_t]$. The self-clustering loss encourages the model to learn clustered target features.\nTake classification loss (Eq. 12), domain discrimination loss (Eq. 13), patch discrimination loss (Eq. 2), and self-clustering loss (Eq. 14) together, the overall objective function is therefore formulated as:\n$L = L_{clc} (x, y) + \\alpha L_{dis} (x, y^d) + \\beta L_{pat} (x, y^d) - \\gamma \\mathbb{I} (p_t; x^t)$                                                                                                                                                                                               (15)\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are the hyperparameters that control the influence of each term on the overall function."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Dataset and Setting", "content": "We evaluate our proposed FFTAT on widely used UDA benchmarks, including Office-31 [30], Office-Home [36], Visda-2017 [27], and DomainNet [26]. Office-31 contains 4,652 images of 31 categories collected from three domains, i.e., Amazon (A), DSLR (D), and Webcam (W). Office-Home has 15,500 images of 65 classes from four domains: Artistic (Ar), Clip Art (Cl), Product (Pr), and Real-world (Re) images. Visda2017 is a simulation-to-real dataset, with more than 0.2 million images in 12 classes. DomainNet dataset has the largest scale containing around 0.6 million images of 345 classes in 6 domains: Quickdraw (qdr), Real (rel), Sketch (skt), Clipart (clp), Infograph (inf), Painting (pnt).\nWe use the ViT-base with a 16x16 patch size (ViT-B/16) [7] [32], pre-trained on ImageNet [29], as the backbone. We use minibatch Stochastic Gradient Descent (SGD) optimizer [28] with a momentum of 0.9 as the optimizer. The batch size is set to 16 for Office-31, Office-Home, Visda-2017, and DomainNet by default. We initialized the learning rate as 0 and linearly warm up to 0.06 after 500 training steps then a cosine decay strategy is applied. For small to middle-scale datasets Office-31 and Office-Home, the training step is set to 5000. For large-scale datasets Visda-2017 and DomainNet, the training step is set to 20000. The hyperparameters $\\alpha$, $\\beta$, and $\\gamma$ are set to [1.0,0.01, 0.1] for Office-31 and Office-Home, and to [0.1, 0.1, 0.1] for Visda-2017 and DomainNet by default, but can be adjusted for optimal performance on specific tasks."}, {"title": "4.2. Results", "content": "Table 1 presents the evaluation results on the Office-Home dataset. The methods listed above the horizontal line are based on CNN architectures, while those below the horizontal line utilize Transformer architectures. Compared to its predecessor, TVT [44], which was the first to adopt the Transformer architecture for UDA, our proposed method, FFTAT, achieves a significant performance improvement of 7.8%. FFTAT represents the first work to surpass an average accuracy exceeding 90% on the Office-Home dataset. Additionally, it is noteworthy that FFTAT outperforms existing methods in each domain adaptation task as well as in the average results by a considerable margin.\nTable 2 shows results on the Visda-2017 dataset. We can observe that the FFTAT achieves impressive performance on average results and in most adaptation tasks. Compared to TVT, FFTAT increased performance by 9.9%. Even when compared to the latest work, PMTrans [53], our method outperforms it in every task, and by 5.8% in average accuracy. It is worth noting that PMTrans utilizes the more powerful architecture of SwinTransformer [23] with pre-trained weights from ImageNet-21K. FFTAT is the first work to achieve an average accuracy exceeding 93% on the Visda-2017 dataset, where the performances of other methods are below 89%. FFTAT also achieves SOTA results on Office-31, with an average accuracy of 96% as shown in Table 4.\nExperimental results on DomainNet, the largest dataset, are presented in Table 3. The performance of the vanilla ViT baseline, standing at 38.1%, is far from satisfactory. In contrast, FFTAT achieves significant improvements across all domain adaptation tasks, with an average accuracy of 51.9%. FFTAT surpasses recent Transformer-based methods like CDTrans and SSRT, and remains competitive with the latest work, PMTrans 1.\nOverall, the results highlight the superior performance of Transformer-based over CNN-based models, owing to their robust transferable feature representations. Our FFTAT achieves state-of-the-art results across these benchmarks, demonstrating its effectiveness for UDA tasks."}, {"title": "4.3. Learned Transferability Graphs", "content": "The Transferability Aware Transformer Layer dynamically learns transferability graphs in a data driven manner. To gain insights into the identified patterns, we visu"}, {"title": "4.4. Ablation Studies", "content": "We conducted comprehensive ablation studies to assess the impact of the two key components, TG-SA and FF, on model performance across these four datasets. Each component was removed individually to isolate its effect. It is noteworthy that removing the transferability graph-guiding mechanism restores vanilla self-attention in the regular ViT. Table 5 reports the average accuracy across all tasks in each dataset. On average, the removal of Feature fusion or Transferability Graph Guidance leads to performance degradation. These findings demonstrate that each component contributes to the final performance of FFTAT."}, {"title": "5. Conclusion", "content": "In this study, we introduce FFTAT, a novel ViT-based solution tailored for unsupervised domain adaptation. FFTAT leverages transferability graphs to guide self-attention (TG-SA) within the Vision Transformer framework, enhancing the emphasis on highly transferable features. and the Feature Fusion (FF) operation to intentionally perturb embeddings, thereby promoting robust feature learning. Extensive experiments demonstrate the efficacy of the Transferability Graph Guided Self-Attention and the Feature Fusion, paving the way for further advancements in this field."}]}