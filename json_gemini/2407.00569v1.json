{"title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models", "authors": ["Weihong Zhong", "Xiaocheng Feng", "Yuxuan Gu", "Weitao Ma", "Liang Zhao", "Yuan Xu", "Qiming Li", "Lei Huang", "Bing Qin"], "abstract": "Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least 31%, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not have supported without distractions. We term this phenomenon Multimodal Hallucination Snowballing. To mitigate this, we further propose a training-free method called Residual Visual Decoding, where we revise the output distribution of LVLMs with the one derived from the residual visual input, providing models with direct access to the visual information. Experiments show that our method can mitigate more than 24% of the snowballed multimodal hallucination while maintaining capabilities.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (LVLMs) have shown remarkable abilities in observing and understanding the real world in human languages (Achiam et al., 2023; Zhu et al., 2023; Liu et al., 2023d; Ye et al., 2023b; Dai et al.). However, multimodal hallucinations, in which LVLMs provide responses misaligned with the corresponding visual information, remain to be the Achilles' heel (Cui et al., 2023; Kamath et al., 2023; Li et al., 2023b; Liu et al., 2023a; Lu et al., 2023; Rawte et al., 2023; West et al., 2023; Huang et al., 2023).\nPrevious research has revealed that hallucinations generated by large language models may accumulate due to models' over-commitment to early mistakes, leading to more mistakes that they otherwise would not make (Zhang et al., 2023a; Azaria and Mitchell, 2023; Kang et al., 2023), especially for the user-model interaction scenarios such as conversation (Huang et al., 2022; Tian et al., 2024; Gong et al., 2023). However, the extent to which accumulated multimodal hallucinations mislead LVLMs into generating false claims requires further exploration. In this work, we conducted an investigation into this issue for the first time. As shown in Figure 1, we seek the answer to the question: When presented with a query relevant to the previously generated hallucination that contradicts the visual information, can models make the correct judgment when they could have given a correct answer independently? We conduct a preliminary studyon GPT-4V (Achiam et al., 2023), LLaVA 1.5 (Liu et al., 2023c), and mPLUG-Owl2 (Ye et al., 2023b). Similar to the setting of Figure 1, given an image, we start a conversation by asking the model to describe the image in detail. When observing hallucinations in the LVLM's responses, we continue to ask a relevant question according to the model-generated hallucination. In addition, we ask the same question separately to see if the model can answer it correctly without distractions. As demonstrated in Figure 2(a), we find that when the text context contains relevant hallucination, the model performance declines significantly, compared to the model response when asking the same question separately. We further select those question samples that the LVLM can correctly answer separately, and manually identify the response change when asking the same question with the related model-generated hallucinatory context. As Figure 2(b) depicts, we find that more than 59% of the answers are semantically the same as the generated hallucination, indicating that they were misled by the previously generated hallucinations.\nTo systematically investigate this phenomenon, we propose to identify whether the LVLM is misled by hallucinations via checking if a specific claim is flipped due to previous hallucinations. We design a framework called MMHalSnowball to construct hallucinatory visual conversations, where models are required to answer the question based on the image and the hallucinatory conversation. The result shows that LVLMs' multimodal hallucinations are easy to mislead the later generation because their strong language capabilities make them prone to be over-confident in the hallucinated context, thereby generating false claims that they normally would not support, which we term as Multimodal Hallucination Snowballing.\nIn addition to mitigating this issue, we further proposed a training-free decoding method called Residual Visual Decoding (RVD). By residual connecting the visual information and the current user instruction, distributions that emphasizing the visual information are derived to revise the original output distribution. Our RVD achieves more than 24% of improvements in reducing the multimodal hallucination snowballing while maintaining the contextual modeling ability."}, {"title": "2 Evaluating the Multimodal Hallucination Snowball Phenomenon", "content": "In this section, we design a question-answer task in the conversation scenario, where a model is first asked to describe a picture in detail and then answers a visual question. As shown in Figure 3, we propose the MMHalSnowball framework to carefully simulate hallucinatory conversations and evaluate whether the model generates a wrong answer due to the hallucinatory context. Next, we will describe our evaluation framework in detail, including conversation creation, experimental settings, and evaluation metrics. We experimentally analyze the multimodal hallucinations snowball in \u00a72.7. The prompts used are listed in Appendix A.2."}, {"title": "2.1 Dataset Source", "content": "We use the validation set of the GQA dataset (Hudson and Manning, 2019) as our data source, which contains a balanced aspect of visual questions that focuses on objective perceptional questions. We adopt images, question-answer pairs, and regional description annotations from the Visual Genome (Krishna et al., 2017). Note that we use its balanced validation set to minimize the impact of dataset contamination and language prior."}, {"title": "2.2 Hallucination Allocation", "content": "To be more practical, we construct hallucinations based on the common types generated by LVLMs. Inspired by Wang et al. (2023a); Zhai et al. (2023), we categorize the hallucinations as follows:\n\u2022 Existence Hallucination, which refers to the incorrect recognition of visible objects in the image or the belief that specific visible objects are absent in the image.\n\u2022 Attribute Hallucination, which refers to the inaccurate characterization of objects and mis-representations of attributes such as color, shape, size, and actions.\n\u2022 Relation Hallucination, which refers to the inaccurate depiction of the relationships or interactions among objects, including erroneous interaction states, relative positions, and spatial positions of objects relative to the image.\n\u2022 Imagination Hallucination, which refers to the erroneous imagination of objects in the picture that do not appear.\nTo incorporate hallucinations, we first utilize ChatGPT (OpenAI, 2022) to rewrite a fact sentence that best describes the question-answer pair. In addition, the annotated regional descriptions and the fact sentence are used to generate an image description. The ChatGPT is prompted to ensure the image description semantically entails the fact sentence. Hallucination can be created by properly modifying the fact sentence. Our goal is to make the answer to the original question no longer correct according to the modified fact sentence. However, not all types of hallucination will make the original answer invalid (e.g. modify the fact sentence \"The color of the trousers is blue\" to \"the color of the bike is blue\" introduces an imagination hallucination, but won't invalidate the answer to the question: \"What color are the trousers that this boy is wearing in the image?\"). To match the hallucination errors in the curated contexts with the corresponding question-answer pairs, We then allocate a proper hallucination type from the above definition to each fact sentence. Appendix A.1 shows details about the rules of allocating proper hallucination types."}, {"title": "2.3 Hallucination Creation", "content": "In this part, we describe how we utilize the question-answer pair, the fact sentence, and the regional descriptions to generate hallucinatory image descriptions. Rather than directly modifying the fact sentence according to the hallucination type to create hallucinations, we find it more stable to ask the ChatGPT to rewrite a hallucinatory answer that contradicts the original answer. Then, the fact sentence, as well as all the regional descriptions are heuristically modified to hallucinatory ones according to the hallucinatory answer. With the hallucinatory fact sentence and hallucinatory regional descriptions as inputs, the ChatGPT is asked to generate a detailed image description that entails the hallucinatory fact. the original answers $Y^{+} = {y^{+}_{1}, y^{+}_{2}, ..., y^{+}_{n}}$ and the rewritten hallucinatory answers $Y^{-} = {Y^{-}_{1}, Y^{-}_{2}, \u2026\u2026\u2026, Y^{-}_{n} }$ are kept for evaluation, where n represents the dataset size."}, {"title": "2.4 Conversation Construction", "content": "Before constructing a hallucinatory conversation, we should ensure that the generated hallucinatory answer and descriptions contradict the image content, while the hallucinatory description supports the hallucinatory answer. To do this, we provide ChatGPT with descriptions, answers, and their corresponding hallucinatory ones to check if the modification and generation meet our requirements. We also check if the image description generated in Section 2.2 entails the fact sentence. See Figure 12 for the prompt used. Note that only those descriptions that conflict with the original answer but can deduce the hallucinatory answer will be kept. After checking, we utilize the generated hallucinatory descriptions and the question-answer pairs to construct a question-answering conversation, as Figure 3 step 3 shows. Conversation examples for each hallucination type are in Appendix A.5."}, {"title": "2.5 Statistics", "content": "With our meticulous data curation and checking process, Our curated dataset D contains 4,973 samples in total. The detailed sample number for each hallucination type is as Figure 4 (a) shows. What's more, from Figure 4 (b), we can observe that the diverse nature of GQA is maintained.\nTo check the effectiveness of the modifications made by our framework, we sample 400 data and manually review them by several professionals. As Table 5 shows, our generated hallucinatory answers and conversations mostly meet our expectations. Please refer to Appendix A.3 for more details about manual checking."}, {"title": "2.6 Evaluation", "content": "To gain a deep understanding of the LVLMs' multimodal hallucination snowballing, given visual question-answering pairs from our dataset, We generate model responses under two different settings as Figure 1 shows and compare the results under these two conversation settings. The first setting is that the model generates the response to the question in our curated corresponding hallucinatory conversation, which we refer to as HalluConv. setting. The second is that the model answers the same visual question alone, without the distraction of hallucinatory context, termed CleanConv. setting. Since LVLMs' response format can be diverse due to the ambiguous query prompt, it might make the automatic evaluation result slightly imprecise. To address this, we follow (Liu et al., 2023c) to add a formatting prompt right after the question: \"Please answer the question using a single word or phrase.\", namely Formatting Prompt setting. The user input with the question only is named as Question Prompt. Note that we conduct experiments with Formatting Prompt if not specified."}, {"title": "2.6.1 Evaluation Metrics", "content": "In this part, we introduce our evaluation metrics. First, to evaluate the correctness of each generated answer, we adopt the following criteria:\nEntailment Matching Score: Considering both the original answer and the hallucinatory answer were short, while models tends to generate longer answers with explanations. We evaluate the correctness for the ith sample by checking if the answer is entailed in the generated response:\n$Score = 1 \\ if \\ y_{i} \\in \\hat{y}_{i} \\ else \\ 0$, (1)\nwhere $y_{i}$ and $\\hat{y}_{i}$ stand for the expected answer and the generated response, respectively. With a proper scoring method for one sample, we can calculate the overall accuracy with the following method:\nAccuracy (Acc):\n$Acc(Y, \\hat{Y}) = \\frac{\\sum^{n}_{n=1} Scorer(y_{i}, \\hat{y}_{i})}{n}$, (2)\nwhere $Acc(Y, \\hat{Y})$ represents the model's accuracy score over the entire dataset.\nFlip Rate (FR): In order to systematically measure whether one model is affected by the hallucination snowballing phenomenon, we propose the FR to evaluate how many model responses are misled by hallucinatory context and are matched with our curated hallucinatory answers:\n$FR = \\frac{\\sum_{i \\in D^{+}} Score (y^{-}_{i}, \\hat{y}^{-}_{i})}{Acc(Y^{+}, \\hat{Y}^{+})}$, (3)\n$D^{+} = {i | Score(y^{+}_{i}, \\hat{y}^{+}_{i}) = 1, \\hat{y}^{-}_{i} \\in \\hat{Y}, y^{+}_{i} \\in Y^{+}}$, (4)\nwhere $Y^{+}$ = {$y^{+}_{1},y^{+}_{2},....,y^{+}_{n}$} and $\\hat{Y}$ = {$\\hat{9}_{1}, \\hat{9}_{2}, \u2026\u2026\u2026, \\hat{9}_{N}$} represent generated answers under CleanConv. and HalluConv. settings, $D^{+}$ represents the sample indexes that the LVLM correctly answers in the CleanConv. setting.\nFurthermore, we designed a more generalized flip-rate metric named weak flip-rate(WFR) which only evaluates how many model responses are distracted by hallucinatory context and conflict with the original answers:\n$WFR = \\frac{\\sum_{i \\in D^{+}} (1 \u2013 Score (y^{+}_{i}, \\hat{y}^{-}_{i}))}{Acc(Y^{+}, \\hat{Y}^{+})}$, (5)"}, {"title": "2.6.2 Models", "content": "We investigate the multimodal snowballing phenomenon in the following mainstream LVLMs:\nLLaVA-1.5 (Liu et al., 2023c), MiniGPT-4 (Zhu et al., 2023), MiniGPT-v2 (Chen et al., 2023a), InternLM-XComposer (Zhang et al., 2023b), ShareGPT4V (Chen et al., 2023b), CogVLM (Wang et al., 2023b), mPlug-Owl (Ye et al., 2023a), mPlug-Owl2 (Ye et al., 2023c), Qwen-VL-Chat (Bai et al., 2023), Otter (Li et al., 2023a), IDEFICS (Lauren\u00e7on and Strien, 2023), InstructBLIP (Dai et al.) and GPT-4V (gpt-4-vision-preview)(Achiam et al., 2023). All experiments are completed under a zero-shot setting. Please refer to Appendix A.4 for more generation details."}, {"title": "2.7 Do LVLMs Suffer from Multimodal Hallucination Snowballing?", "content": "To answer this question, we compare the model responses under the conversation settings of HalluConv. and CleanConv., as Section 2.6 describes. The results are depicted in the Table 1. Though advanced in answering visual questions even in a zero-shot manner (See accuracy in CleanConv.), most models struggle to stick to their judgment when there are specious hallucinations in the context (See accuracy in HalluConv.), resulting in extremely low accuracy. For LLaVA-1.5, ShareGPT4V, mPlug-Owl2, and InstructBLIP, despite their advanced model ability, they still suffer an over 50% performance drop. However, we also recognize that GPT-4V is significantly less affected by hallucinations. We observed a correction process in the responses of GPT-4 (See Appendix B.2 for examples), indicating that it is capable of paying attention to visual information to a certain extent and realizing that some hallucinations have been generated in the conversation. In addition, we find that GPT-4 often refuses to answer the user question due to its strict safety protocol, especially in the Clean Conv. setting (around 12%), indicating a potential cause of such a comparably low accuracy. But in general, all the LVLMs suffer from multimodal hallucination snowballing at different levels. What's more, a high flip rate indicates that the model responses are easily misled by the hallucinatory conversation, even when the model can make a correct claim in CleanConv. setting. An even higher weak flip rate is observed, which shows that LVLMs' responses are corrupted due to the hallucinatory context. Hence, comparing the same LVLMs with different scale LLM backbones, we find no significant performance improvement in mitigating the multimodal hallucination snowballing, except for the InstructBLIP.\nComparing the experiments between two different query prompts, we find that the Formatting Prompt shows clearer instructions, which not only improves question-answering ability but also eases the multimodal hallucination snowballing phenomenon for most of the LVLMs.\nWe further present the accuracy of two different conversation settings and the flip rate for each hallucination type in figure 5. The result shows that existence, attribute, and imagination hallucinations are easier to snowball. We even observe a nearly 100% flip rate on the imagination hallucination where LVLMs readily accept objects that are mistakenly imagined to exist, which could attributed to the LVLMs' nature to generate positive response (Liu et al., 2023b). while the relation hallucinations have a higher probability of being correct while answering the question. For detailed results, please refer to Appendix B.1."}, {"title": "2.8 Will LVLMs Be Affected by the Hallucination-Free Context?", "content": "Compared to CleanConv. setting, where the conversation context only contains one image and one user question, LVLMs under HalluConv. setting are required to answer the same user question with an additional round of conversation. How does a longer context length affect the model performance? To answer this question, we further create two conversation settings that have similar context length to HalluConv. setting, in which there is also an additional conversation round but without hallucinatory content related to the user question. Specifically, we first replace the hallucinatory descriptions in Halluconv. setting with the image descriptions generated in Section 2.2, which are semantically consistent with the fact sentence. We name the resulting new conversation setting as Fact-Conv. setting. In addition, we replace the 1st round conversation in HalluConv. with a single question-answer pair that is irrelevant to any specific visual information in the image, namely IrrConv. setting (See Appendix B.3 for more details). The results are as Table 2 shows. From the results, we can observe that all the models benefit a lot from a correct image description, which further proves that LVLMs tend to rely on text context when there is text format visual information that can help to generate the response. Such nature could potentially lead to the hallucination snowballing with a hallucinatory conversation. What's more, when the context provides no useful information, the models' abilities are not severely influenced by the context, which further indicates the performance drop in HalluConv. setting is caused by hallucination snowballing, not the context length."}, {"title": "3 Residual Visual Decoding", "content": "From the phenomenon of multimodal hallucination snowballing, we find that LVLMs tend to condition on text context when there are plausible clues to help make responses, thereby ignoring the visual information and could be easily misled by erroneous context. To remedy this, we manage to emphasize the visual information during the inference process without additional training or external tools under the multi-turn conversation scenario."}, {"title": "3.1 Residual Visual Predictions", "content": "Given a visual input v, a dialog history h, and the current text query x, one LVLM parametrized by $\\theta$ generates a response y token-wisely. With generated tokens $y<t$ up to time step t \u2212 1, the output distribution in time step t is formulated as $P_{\u03b8}(y_{t}|v, h, x, y<t)$, where the output token $y_{t}$ is sampled from the output distributions:\n$y_{t} ~ p_{\u03b8}(y_{t}|v, h, x, y<t) = softmax(logit_{\u03b8}(y_{t}|v, h, x, y<t))$, (6)\nSince the hallucinatory context could interfere with the process of reasoning over the visual input, we first construct an input that residual connects the visual input v with the current text query x, and derive an output distribution from it:\n$p_{\u03b8}(y_{t}|v, x, y<t) = softmax(logit_{\u03b8}(y_{t}|v, x, y<t))$, (7)\nin which the output distribution will naturally shift from dependence on text context to reliance on visual information. We term it the Residual Visual Predictions, which are based entirely on visual information and the query while sacrificing attention to the text context."}, {"title": "3.2 Residual Visual Decoding", "content": "In order to put an emphasis on the visual information under a multi-turn visual text conversation scenario, inspired by (Leng et al., 2023; Liu et al., 2021), we introduce Residual Visual Decoding (RVD), where residual visual predictions are utilized to enhance the perception of the visual information. The revised distribution $P_{RVD}$ is formulated as:\n$P_{RVD}(y|v, h, x) = softmax(\u03b1logit_{\u03b8}(y|v, x) + (1 - \u03b1)logit_{\u03b8}(y|v, h, x))$, (8)\nwhere a larger \u03b1 indicates a higher model focus on the visual information. Note that when the length of dialog history h is 0, the RVD degenerates to the regular decoding."}, {"title": "3.3 Adaptive Distribution Blending", "content": "However, as we tune up the \u03b1, the text context gets to be ignored when generating responses, which possibly does harm to the model's inherited contextual ability. To preserve the contextual ability while tackling the hallucination snowballing, we propose to adaptively adjust the scaling parameter. Specifically, we derive an output distribution $p_{\u03b8}(y|x)$ given the current user query x only, and calculate the Jensen-Shannon divergence (JSD) between it and residual visual predictions, which evaluates the similarity between two output distributions:\n$\u03c4 = JSD(p_{\u03b8}(y|v,x)||p_{\u03b8}(y|x)), \\ \u03c4 \\in [0,1]$, (9)\nwhere \u03c4 is the JSD score between $p_{\u03b8}(y|v, x)$ and $p_{\u03b8}(y|x)$. We suspect that when responding to the query depends on the visual information v, t gets larger, since the latter is barely making guesses. Meanwhile, when responding to the query depends on the dialog history h, the corresponding two distributions tend to make guesses. However, they still have access to the nearest user query from the current round of conversation. Thus, We assume that conditioned on these two output distributions tend to make similar guesses so that the \u03c4 will get smaller. Therefore, we dynamically adjust the \u03b1 with and a scaling factor \u03b2:\n$\u03b1 = Min(\u03b2 * \u03c4, 1)$, (10)\nWith the dynamic adjusted \u03b1, we can adaptively blend the residual visual distribution into the original output distribution with equation (8)."}, {"title": "3.4 Experiments", "content": "By blending the residual visual distribution into the original output distribution, the models' contextual ability could be harmed. Inspired by Chen et al. (2023c), to quantitatively evaluate the LVLMs' contextual ability with our pipeline, we construct a multiple choice task called Who Provides This Image (WPI). Specifically, we randomly insert a template sentence \"The image is provided by #key\" into the first-round model response, where #key is a random 6-digit number. We then change the corresponding question and answer to \"Who provides this image?\". An LVLM that can correctly access the context will have over 90% accuracy in answering this question. For more details, please refer to Appendix A.6.\nAs a result, We test our proposed RVD in our proposed multimodal hallucination snowballing evaluation and the aforementioned WPI task to evaluate its ability to alleviate the multimodal hallucination snowballing while maintaining contextual ability."}, {"title": "3.4.1 Baselines", "content": "To show the effectiveness of our proposed RVD, we compare our method with the following strategies:\n\u2022 Prompt is utilized to require the model to focus on the given image instead of concentrating on the text context that could cause the hallucination to snowball. Specifically, we explicitly ask the model with the following query: {#Question, Please answer the question based on the given image.}.\n\u2022 Visual Contrastive Decoding(VCD) (Leng et al., 2023) is proposed to contrast the output distribution with that of the distorted visual input, which aims to alleviate the language prior in the context while focusing on the visual information.\nWe evaluate the effectiveness of the aforementioned strategies and our RVD on three trending open-source LVLMs: LLaVA-1.5, mPlug-owl2, and ShareGPT4V. We set the \u03b2 = 2 if not specified."}, {"title": "3.4.2 Experiment Results", "content": "The results are shown in Table 3. We find that incorporating the prompt methods will do harm to the model performance, which might be because of the inability of LVLMs to follow complex instructions. Though shown to be effective in correcting the snowballed hallucination, the VCD contrasts the output distribution with the distorted visual input, which could do harm to the model performance when the context is utilized to respond to the query. However, by dynamically emphasizing the visual information whenever needed, our proposed RVD makes a large accuracy improvement in overcoming the multimodal hallucination snowballing while maintaining contextual ability."}, {"title": "3.4.3 Effect of Parameters", "content": "We evaluate the effect of our proposed hyperparameters a and \u03b2. The results are shown in Figure 6 and 7. First, we remove the adaptive distribution blending and adjust the a manually, the result shows that a larger a clearly revises the output distribution more towards the golden visual information. However, the context is ignored in return. With adaptive distribution blending, the model performance is more balanced when we enlarge the \u03b2, which won't cause a large performance drop on contextual abilities. See Appendix B.4 for more experiment results."}, {"title": "4 Related work", "content": "Inspired by the recent success of large language models (LLMs) (Zhao et al., 2023), researchers have devoted significant effort to integrating LLMs into vison-language models to utilize their powerful language understanding and generation capabilities (Wu et al., 2023). In addition to the advanced capabilities demonstrated by closed-source models such as GPT-4V(Achiam et al., 2023), open-source large vision-language models(LVLMs), building upon powerful open-source LLMs such as LLaMa (Touvron et al., 2023) and Vicuna (Chiang et al., 2023), have adopted a powerful instruction following abilities to tackle visual-language tasks in a zero-shot manner (Zhu et al., 2023; Liu et al., 2023d; Dai et al.; Ye et al., 2023b). Possessing both visual perception abilities and language capabilities, LVLMs are further utilized to perform real-world tasks, such as tool-using (Liu et al., 2023e), web browsing (Zheng et al., 2024), and autonomous driving (Xu et al., 2023). However, current LVLMs still suffer from severe multi-modal hallucination problems (Liu et al., 2024), which brings challenges to evaluating and maintaining the reliability of LVLMs."}, {"title": "4.1 Large Vision-Language Models", "content": "Multimodal hallucinations (Liu et al., 2024) refer to the responses generated by LVLMs that are misaligned with the corresponding visual information. Multimodal hallucination can arise due to overfitting to specific patterns in the training data, inferior abilities to recognize the visual elements, or an inability to model the multimodal input. Li et al. (2023b), Lovenia et al. (2023), take the first step towards evaluating the hallucinations in the LVLMs. Furthermore, Liu et al. (2023b), Zong et al. (2023) and Liu et al. (2023a) show that LVLMs can be easily fooled and experience a severe performance drop due to their over-reliance on the strong language prior. In addition, efforts have been made towards mitigating multi-modal hallucinations by further finetuning or post-hoc rectify(Gunjal et al., 2023; Lu et al., 2023; Liu et al., 2023b; Zhou et al., 2023; Yin et al., 2023). However, current methods are unable to completely eliminate the hallucinations generated by models, yet no one has explored the subsequent impacts of the generated hallucinations. In this paper, we take the first step towards it by systematically evaluating the multimodal hallucination snowballing phenomenon and propose a training-free method to ease LVLMs from it."}, {"title": "4.2 Multimodal Hallucination", "content": "In this paper, we raise the question of Whether LVLMs suffer from multimodal hallucination snowballing. We meticulously designed the MMHalSnowball framework to simulate hallucinatory conversations and study models' behaviors when encountering hallucinations. Our investigation proved that LVLMs are being severely affected by hallucinations in the context, thus generating snowballed hallucinations. Further, we proposed the Residual Visual Decoding to alleviate the multimodal hallucination snowballing while maintaining its contextual abilities. However, our methods still have limitations when deployed to a general-purpose assistant, which we left as future works."}, {"title": "5 Conclusion", "content": "In this work, with a carefully designed evaluation framework, we have revealed that current LVLMs severely suffer from multimodal hallucination snowballing. We further proposed the RVD to mitigate the phenomenon. However, our work still has limitations. Firstly, despite the greater variety of hallucination snowballing phenomena in the real-world setting, the scenarios we focus on are still relatively simplistic. This is because constructing rich and diverse scenarios would be more difficult and would require a significant amount of effort. Secondly, instead of meticulously finding real hallucinations generated by each LVLM and constructing relevant question-answer pairs, we choose to conduct experiments on our simulated hallucinatory conversations. This is because the evaluation processes based on responses from a single LVLM will make it difficult to scale up the evaluation data and adapt to more models. Thirdly, our experiments are conducted on models of 7B and 13B sizes, and we evaluate our proposed RVD only on a few selected models. This is due to computational limitations. Fourthly, our proposed RVD is currently still limited in several conversation scenarios. We will further explore expanding this method to more diverse conversation scenarios."}, {"title": "6 Limitations", "content": "Xiaocheng Feng is the corresponding author of this work. We thank the anonymous reviewers for their insightful comments. This work was supported by the National Key R&D Program of China via grant No. 2021ZD0112905, National Natural Science Foundation of China (NSFC) via grant (62276078, U22B2059), the Key R&D Program of Heilongjiang via grant 2022ZX01A32, the International Cooperation Project of PCL, PCL2022D01 and the Fundamental Research Funds for the Central Universities via grant No. HIT.OCEF.2023018."}, {"title": "7 Acknowledgments", "content": "After carefully analyzing the question-answer pairs in the dataset, we manage to create an answer vocabulary for answers suitable for introducing relation errors. What's more, we utilize Part-of-Speech of the answer in the fact sentence to choose proper hallucination types. Specifically, we allocate attribute hallucination for those answers tagged as adjectives and verbs and allocate existence hallucination for those answers tagged as nouns. For imagination hallucination, instead of using the annotated question-answer pair, we provide ChatGPT with all annotated objects and ask ChatGPT to generate an object that is not present in the image but is reasonable to be in the corresponding scene. Then, we directly construct a question-answer pair with the template: \"question: Is there a in the image? answer: No\"."}, {"title": "A Additional Experimental Details", "content": "In this section, we list all prompts used during the process of constructing hallucinatory conversations, which include fact generation (Figure 9), conflict creation (Figure 10), description generation (Figure 11) and conflict verification (Figure 12). Note that we reuse the description generation prompt (Figure 11) to generate the ground image description by giving the annotated regional description and fact sentence."}, {"title": "A.1 Hallucination Allocation", "content": "We randomly select 100 data for each hallucination type in our curated dataset, 400 in total. We ask three annotators to check each of them from three aspects, as Table 5 depicts. The annotation results show that the generated hallucinatory description mostly meets our requirements."}, {"title": "A.2 Prompts", "content": "Through all our experiments, we follow a consistent generation configuration to ensure fairness. Specifically, we set the inference hyperparameter as follows: do_sample=True, temperature=1.0, top_p=0.95, top_k=None and num_beams=1."}, {"title": "A.3 Manual Checking", "content": "We list four examples to demonstrate curated hallucinatory conversation of each hallucination type, namely existence(Figure 13), attribute(Figure 14),relation(Figure 15), and imagination(Figure 16), where the ground answers are highlighted green and the hallucinated answers are highlighted red."}, {"title": "A.4 Generation Details", "content": "We construct the Who Provides This Image (WPI) task to evaluate the contextual capabilities of LVLMs. To achieve this goal, we utilize the Fact-Conv. setting described in Section 2.8 and insert a random digit into the first-round model response. We adopt a multi-choice approach and judge the answer by checking if it contains only the correct option, instead of matching the option content. Both the correct option and interference option are randomly generated six-digit numbers, and the third option is \"None of the options are correct\". To further ensure fairness and effectiveness, the order of choices is also random. An example is shown in Figure 8. By resampling from GQA and constructing dialogues, Our WPI task contains 1,005 samples in total."}, {"title": "A.5 Hallucinatory Conversation Example", "content": "We show our detailed evaluation results for each hallucination type in Table 6."}, {"title": "A.6 Details of Who Provides This Image Task", "content": "We present GPT-4 answer examples with thePrompt. The first example is represented in Figure 17, which illustrates that GPT-4V is able to adaptively focus on golden visual information, and further identify and clarify the hallucinations in the previous hallucinatory description in some cases. The second example is represented in Figure 18, which demonstrates that GPT-4V tends to refuse to answer some categories of questions, leading to difficulty in the evaluation and the degradation of the evaluation results."}, {"title": "B Additional Experimental Results", "content": "In order to exclude the interference of irrelevant factors and to check whether LVLMs are affected by the hallucination-free context, we further set up two conversation settings, namely FactConv. and IrrConv. settings. Corresponding examples are shown in Figure 19. We follow equation 3 and equation 4 to calculate FR and WFR, respectively, but we modify the definition of Y to represent generated answers under FactConv. or IrrConv. setting. We further present full experiment results for these two conversation settings in Table 7."}, {"title": "B.1 More Evaluation Results", "content": "In our RVD, we choose JSD to evaluate the output distribution similarity, because it's a symmetric metric that measures the difference between two distributions, with a range [0, 1], which fits our goal of adjusting the a (range is also [0, 1]) dynamically using the difference between two distributions. We also try to use the Kullback-Leibler divergence (KLD) as the similarity measurement method, which is not a symmetric metric with a range [0, +\u221e] and can't be directly applied to our Residual Visual Decoding (RVD). Specifically, to transform the range into [0, 1], We modify the Equation 9 to the following form:\n$T = 1 - EXP(-KLD(p_{\u03b8}(y|v, x)||p_{\u03b8}(y|x)))$. (11)\nWe compare the experiment results of our RVD using KLD and JSD as the similarity measurement method on our proposed MMHalSnowball framework with LLaVA-1.5 7B. The results are as the Table 4 shows. We can observe from the table that RVD with KLD aggressively puts more emphasis on visual information, resulting in a better result in hallucination snowballing with smaller \u03b2, but a worse contextual ability. The result indicates that the JSD has a generally smaller value and is more balanced compared to the KLD in alleviating snowballed hallucinations while maintaining contextual ability."}, {"title": "B.2 GPT-4 Answer Examples", "content": "In this part, we present some cases of the LLaVA-1.5-7B model equipped with our proposed RVD. All cases are using the Question Prompt. We provide some case studies in Figure 20, one for each hallucination type, to demonstrate the effectiveness of our methods, where our RVD with LLaVA-1.5-7B successfully mitigated the snowballed hallucinations. In these examples, we can observe that with our proposed RVD, the model can focus more on the visual information to avoid generating snowballed hallucinations, rather than solely rely on the previously generated hallucinatory text and thus generate snowballed hallucinations. What's more, in the example of Imagination Hallucination, the model with our RVD can even correct its previous mistakes, further illustrating the model's contextual capability is preserved while avoiding hallucination snowballing."}, {"title": "B.3 Hallucination-free Context Experiment Details", "content": "In our RVD, we chose JSD to evaluate the output distribution similarity, because it's a symmetric metric that measures the difference between two distributions, with a range [0, 1], which fits our goal of adjusting the \u03b1 (range is also [0, 1]) dynamically using the difference between two dis-"}, {"title": "B.4 Effect of Different Similarity Measurement Methods", "content": "In this part, we present some cases of the LLaVA-1.5-7B model equipped with our proposed RVD. All cases are using the Question Prompt. We provide some case studies in Figure 20, one for each hallucination type, to demonstrate the effectiveness of our methods, where our RVD with LLaVA-1.5-7B successfully mitigated the snowballed hallucinations. In these examples, we can observe that with our proposed RVD, the model can focus more on the visual information to avoid generating snowballed hallucinations, rather than solely rely on the previously generated hallucinatory text and thus generate snowballed hallucinations. What's more, in the example of Imagination Hallucination, the model with our RVD can even correct its previous mistakes, further illustrating the model's contextual capability is preserved while avoiding hallucination snowballing."}, {"title": "B.5 Case Study", "content": "In this part, we present some cases of the LLaVA-1.5-7B model equipped with our proposed RVD. All cases are using the Question Prompt. We provide some case studies in Figure 20, one for each hallucination type, to demonstrate the effectiveness of our methods, where our RVD with LLaVA-1.5-7B successfully mitigated the snowballed hallucinations. In these examples, we can observe that with our proposed RVD, the model can focus more on the visual information to avoid generating snowballed hallucinations, rather than solely rely on the previously generated hallucinatory text and thus generate snowballed hallucinations. What's more, in the example of Imagination Hallucination, the model with our RVD can even correct its previous mistakes, further illustrating the model's contextual capability is preserved while avoiding hallucination snowballing."}]}