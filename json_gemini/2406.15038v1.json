{"title": "Online detection and infographic explanation of spam reviews with data drift adaptation", "authors": ["Francisco DE ARRIBA-P\u00c9REZ", "Silvia GARC\u00cdA-M\u00c9NDEZ", "F\u00e1tima LEAL", "Benedita MALHEIRO", "Juan C. BURGUILLO"], "abstract": "Spam reviews are a pervasive problem on online platforms due to its significant impact on reputation. However, research into spam detection in data streams is scarce. Another concern lies in their need for transparency. Consequently, this paper addresses those problems by proposing an online solution for identifying and explaining spam reviews, incorporating data drift adaptation. It integrates (i) incremental profiling, (ii) data drift detection & adaptation, and (iii) identification of spam reviews employing Machine Learning. The explainable mechanism displays a visual and textual prediction explanation in a dashboard. The best results obtained reached up to 87% spam F-measure.", "sections": [{"title": "1. Introduction", "content": "Online reviews are a valuable source of information that influences public opinion and directly impacts customers' decision to acquire a product or service (Zhang et al., 2018). However, some reviews are fabricated to promote or undervalue goods and services artificially, i.e., creating spam data (Reyes-Menendez et al., 2019; Hutama and Suhartono, 2022). Spammers can be humans or bots dedicated to creating deceptive reviews (Garc\u00eda-M\u00e9ndez et al., 2022b; Hamida et al., 2022). In this context, spam detection is a critical task in online systems.Spam negatively impacts the user experience and the performance and security of the system (Wang et al., 2021).\nConsequently, a broad set of Machine Learning (ML) methods has been explored for spam detection, mainly supervised learning (Crawford et al., 2015). In recent years, Natural Language Processing (NLP) techniques (Garc\u00eda-M\u00e9ndez et al., 2022a) have been"}, {"title": "2. Related work", "content": "As previously mentioned, online reviews have become an essential source of information for consumers to make purchasing decisions (Zhang et al., 2018; Al-Otaibi and Al-Rasheed, 2022). However, spam reviews, which are fake or biased reviews, have become a significant problem, leading to distrust and confusion among consumers (Bian et al., 2021). Accordingly, detecting spam reviews is challenging due to the variety of spamming techniques used by spammers; hence, researchers have proposed various approaches for spam review detection (Wu et al., 2018). These techniques are based on ML methods (Albayati and Altamimi, 2019; Liu et al., 2019; Sun et al., 2022) and social network analysis (Liu et al., 2016; Sun et al., 2022). A representative example of the latter is the work by Rathore et al. (2021) on fake reviewer group detection. Their offline graph-based solution, where nodes and edges represent reviewers and products reviewed, respectively,"}, {"title": "2.1. Profiling and classification", "content": "Profiling is the process of modeling stakeholders according to their contributions and interactions (Kakar et al., 2021; Garc\u00eda-M\u00e9ndez et al., 2022b). In the case of spam detection, individual profiles are built from the content generated by each stakeholder, humans or bots alike. To overcome information sparsity, the profiles are expected to include side and content information (Faris et al., 2019; Mohawesh et al., 2021), since a richer profile impacts the quality of ML results (Rustam et al., 2021). Mainly, with stream-based modeling, profiles are incrementally updated and refined over time (Veloso et al., 2019, 2020). Concerning online spam detection, the literature considers primary profiling methodologies:\nContent-based profiling explores textual features extracted from the text to identify the meaning of the content (Song et al., 2016; Henke et al., 2021; Mohawesh et al., 2021). It can be obtained using linguistic and semantic knowledge or style analysis via NLP approaches.\nUser-based profiling focuses on both the demographic and the behavioral activity of the user (Miller et al., 2014; Eshraqi et al., 2015; Liu et al., 2016, 2019; Sun et al., 2022). It contemplates demography information, frequency, timing, and content of posts to distinguish legitimate from spammer users. In addition, exploiting the social graph can be relevant since spammers have many followers or friends who are also suspected of being spammers.\nSpam detection is a classification task (Vaitkevicius and Marcinkevicius, 2020; Mohawesh et al., 2021). The main classification techniques encompass supervised, semi-supervised, unsupervised, and deep learning approaches (Crawford et al., 2015) and can be applied offline or online. While offline or batch processing builds static models from pre-existing data sets, online or stream-based processing computes incremental models from live data streams (Leal et al., 2021). This paper focuses on stream-based environments. Regarding transparency, classification models can be divided into interpretable and opaque. Opaque mechanisms behave as black boxes (e.g., deep learning), and interpretable models are self-explainable (e.g., trees- or neighbor-based algorithms) (Carvalho et al., 2019). Interpretable classifiers explain classification outcomes, clarifying why a given content is false or misleading (\u0160krlj et al., 2021)."}, {"title": "2.2. Stream-based spam detection approaches", "content": "Social networking has increased spam activity (Kaur et al., 2018). In this context, spam detection approaches have been explored by social networks (e.g., Twitter\u00b2, or Facebook\u00b3)\n(Miller et al., 2014; Eshraqi et al., 2015; Liu et al., 2016; Sun et al., 2022), email boxes (Henke et al., 2021), or crowdsourcing platforms (e.g., Wikipedia, Yelp5, and TripAdvi-sor) (Mohawesh et al., 2021). Stream mining became the most effective spam detection approach due to the speed and volume of data. It has been explored in the literature using:\n\u2022 Data stream clustering approaches. Miller et al. (2014) treated spam detection as an anomaly prediction problem. The proposed solution identifies spammers on Twitter using account information and streaming tweets employing stream-based clustering algorithms. Eshraqi et al. (2015) followed the same methodology, creating clusters of tweets and considering outliers as spam. Song et al. (2016) proposed a new ensemble approach named Dynamic Clustering Forest (DCF) for the classification of textual streams, which combines decision trees and clustering algorithms.\n\u2022 Data stream classification for spam detection. Sun et al. (2022) proposed a near real-time Twitter spam detection system employing multiple classification algorithms and parallel computing.\n\u2022 Outlier detection for stream data. Liu et al. (2019) proposed solution identifies outlier reviews, analyzes the differences between the patterns of product reviews, and employs an isolation forest algorithm."}, {"title": "2.2.1. Drifts in spam detection", "content": "Model drift occurs when the performance of an ML model loses accuracy over time (Ma et al., 2023). The literature identifies two types of drifts: (i) data drifts and (ii) concept drifts. While data drift occurs when the characteristics of the incoming data change, in concept drifts, both input and output distributions present modifications over time (Desale et al., 2023). According to Gama et al. (2014), concept drift detection methods can be divided into three categories: (i) sequential analysis, (ii) statistical analysis, and (iii) sliding windows. In addition, for Lu et al. (2018), drift detection involves four stages: (i) data retrieval, (ii) data modeling, (iii) test statistics calculation, and (iv) hypothesis test.\nLiu et al. (2016) proposed and applied two online drift detection techniques to improve the classification of Twitter spam reviews: (i) fuzzy-based redistribution and (ii) asymmetric sampling. While the fuzzy-based redistribution technique explores information decomposition, asymmetric sampling balances the size of classes in the training data. Song et al. (2016) analyzed the distribution of textual information to identify concept drifts in a textual data classification approach. Moreover, Mohawesh et al. (2021) employed a comprehensive analysis to address concept drift in detecting fake Yelp reviews. Finally,"}, {"title": "2.2.2. Explainability", "content": "Explainable spam detection refers to explaining why an input was classified as spam. It promotes transparency and clarity, detailing why a particular review was flagged as spam (Stites et al., 2021). Accordingly, interpretable models, such as rule-based systems or decision trees, can explain their reasoning, enhancing trust, reducing bias, and helping to discover additional insights (Rudin, 2019). In addition, NLP enriches the explanations by adding a textual description (Upadhyay et al., 2021). Explainable spam detection has been explored in the literature using Local Interpretable Model Agnostic Explanation (LIME)\n(Ribeiro et al., 2016) and Shapley Additive Explanations (SHAP) (Reis et al., 2019; Han et al., 2022; Zhang et al., 2022).\nThe literature shows that existing explainable detectors of fake content in online platforms adopt essentially supervised classification and implement offline processing (Crawford et al., 2015; Henke et al., 2021). Therefore, this paper intends to address this problem by proposing an online solution for identifying and explaining spam reviews, incorporating data drift detection and adaptation."}, {"title": "2.3. Research contribution", "content": "The literature review shows a research gap in detecting data drifts and explaining the classification of textual reviews as spam in real time. In this respect, Rao et al. (2021) identifies spam drift detection as a challenge requiring more research. Table 1 provides an overview of the above works considering the data domain, profiling (user- and content-based), spam detection, drift detection, and explainability.\nTherefore, this work contributes with an online explainable classification method to recognize spam reviews and, thus, promote trust in digital media. The solution employs data stream processing, updating profiles, and classifying each incoming event. First, user profiles are built using user- and content-based features engineered through NLP. Then, the proposed system monitors the incoming streams to detect data drifts using static and sliding windows. Tree-based classifiers are exploited to obtain an interpretable stream-based classification for classification. Finally, the proposed method provides the user with a dashboard combining visual data and natural language knowledge to explain why an incoming review was classified as spam.\nAs previously explained, concept drift refers to changes in the predicted target over time (i.e., changes in the statistical properties of the spam and non-spam entries), while"}, {"title": "3. Method", "content": "The proposed method explores online reviews for stream-based spam classification with drift detection. In addition, it explores self-explainable ML models for transparency. Hence, the data stream classification pipeline, represented in Figure 1, comprises: (i) feature engineering & incremental profiling (Section 3.1), (ii) feature selection (Section 3.2), (iii) data drift detection & adaptation (Section 3.3), (iv) ML classification (Section 3.4), and (v) explainability (Section 3.5)."}, {"title": "3.1. Feature engineering & incremental profiling", "content": "The proposed solution processes the content of the reviews with the help of NLP techniques. The content-based features extracted represent relevant linguistic (morphological, syntactical, and semantic) attributes of the reviews. The engineered features are the ratio of adjectives, adverbs, interjections, nouns, pronouns, punctuation marks, verbs, char, word, difficult word, and URL counters. Moreover, the system also considers the emotional charge of the content (i.e., anger, fear, happiness, sadness, and surprise). The same applies to the polarity charge among negative, neutral, and positive sentiments. More sophisticated linguistic features include readability, using the Flesch readability score, the McAlpine EFLAW score, and the reading time. In the end, the content itself, i.e., the words, are an-"}, {"title": "3.2. Feature selection", "content": "Feature selection reduces the feature space dimension by choosing the most relevant features for the classification and contributes to improving the quality of the input data. The adopted selection technique relies on feature variance to discard those with variance lower than a configurable threshold, as suggested by the literature (Engelbrecht et al., 2019; Treistman et al., 2022). In the case of online classification, where the arriving data may evolve with time, the selection of representative features must be performed continuously or periodically."}, {"title": "3.3. Data drift detection and adaptation", "content": "The variability of real data over time may affect the performance of ML models, namely the values of evaluation metrics (e.g., accuracy, precision, recovery, etc.). However, the source of the problem may be due to data drifts, concept drifts, ineffective hyper-parameter optimization, and/or class imbalance.\nThus, the proposed system continuously monitors the incoming stream for data drifts and, periodically, under-samples and optimizes the hyperparameters, using two windows: the past (P) static window and the current adaptive (ca) sliding window, holding n and w samples, respectively.\nThe data drift detector starts operating when the cold start ends, and the P window is initialized with the expected n samples. The detector identifies a data drift whenever: (i) the inter-window word-gram p-value is lower than 0.05, and (ii) the inter-window absolute accuracy difference (AAD) is higher than 0.05. Algorithm 1 details the data drift detection and adaptation process. The threshold values of 0.05, 0.1, and 0.5 were inspired by the works by Solari et al.; Leo and Sardanelli; Ritu Aggrawal, respectively. Figure 2 illustrates this process. The data drift detector works as follows:\n\u2022 Calculates the word-gram frequency matrices (i.e., the columns represent the word-grams and the rows, the entries) for the p and ca windows.\n\u2022 Sum_wordgrams method transforms the latter matrices into vector format (a vector for P and a vector for ca) by summing the word-gram frequency for all entries.\n\u2022 Discards the columns with a frequency lower than 6 in both sum_wordgrams vectors.\n\u2022 Computes the p-value between the word-grams frequency vectors of P and ca windows.\n\u2022 Computes the inter-window AAD.\n\u2022 Updates the size of the CA:\nIf the p-value < 0.1, the ca windows size decrements by one.\nIf the p-value > 0.1 and p-value < 0.5, the ca windows size remains unchanged.\nIf the p-value \u2265 0.5, the ca windows size increments by one.\n\u2022 Identifies a data drift when the inter-window word-gram p-value is lower (or equal) and the inter-window AAD is higher (or equal) than 0.05. Then, it replaces the P with the ca window and recalculates the optimal hyperparameters. The hyperparameter_computation method applies an exhaustive search technique over the configuration parameters listed in Figure 3. Ultimately, the ML model is trained using the ml_update function with the hyperparameters selected and the ca samples."}, {"title": "3.4. ML classification", "content": "The following online ML algorithms were used as they exhibited good performance in similar classification problems (Liu et al., 2016; Song et al., 2016; Sun et al., 2022)."}, {"title": "3.5. Explainability", "content": "This module provides information about the most relevant features for the classification, i.e., those with a frequency of appearance greater than a configurable threshold. This information is extracted from the estimators of the tree models used (see Figure 3): \u043d\u0442C"}, {"title": "4. Experimental results", "content": "This section describes the experimental data set (Section 4.1) and the implementation of the different modules10: (i) feature engineering & incremental profiling (Section 4.2), (ii) feature selection (Section 4.3), and (iii) data drift detection & adaptation (Section 4.4). The classification and explainability results are detailed in Section 4.5 and Section 4.6, respectively.\nThe experiments contemplate four stream classification scenarios, incorporating feature selection, hyperparameter optimization\u00b9\u00b9 and incremental accuracy updating.\nScenario 1. The data stream classification runs on a single processing thread.\nScenario 2. The data stream classification runs on a range of 10-20 parallel threads based on the workload to reduce the experimental run-time. To preserve the original data distribution, the chronologically ordered data stream was divided into consecutive sub-streams, and then, each sub-stream was processed in a dedicated thread.\nScenario 3. The data stream classification includes data drift detection & adaptation and runs according to scenario 2.\nScenario 4. The data stream classification runs on a single processing thread with data drift detection & adaptation 12."}, {"title": "4.1. Experimental data set", "content": "The Yelp data set13 is composed of 359 052 leisure activity entries between October 2004 and January 2015, distributed between 36 885 and 322 167 samples of spam and non-spam content, respectively (see Table 2). Moreover, the MediaWiki data set14 contains contributions to travel wikis between August 2003 and June 2020. It is composed of 319 856 entries, distributed between 24877 and 249 979 samples of spam and non-spam content, respectively (see Table 2)."}, {"title": "4.2. Feature engineering & incremental profiling", "content": "This section details the implementations and NLP techniques used to create the classification features. Table 3, Table 4, and Table 5 detail the content features, the incremental user features, and the incremental item features for Yelp and MediaWiki data sets, respectively.\nMost ratio and counter features in Table 3 (features 1, 2, 7, 9, 11, 12, 15 in Table 3) are computed using the spaCy15 tool to gather their grammatical category (token.pos_feature). The char and word count (features 3 and 16, respectively) have been directly calculated with the Python len function16. The URL count (feature 14) was computed using"}, {"title": "4.3. Feature selection", "content": "To reduce the feature space dimension, the variance of the features in Table 3 and Table 4 is analyzed with the help of the VarianceThreshold26 from River 0.11.127. The threshold is set to 0, the default value. In the case of Yelp, only feature 14 in Table 3 and its incremental versions in Table 4 and Table 5 were discarded. The discarded MediaWiki features include features 21 and 22 in Table 3 and their incremental versions in Table 4"}, {"title": "4.4. Data drift detection and adaptation", "content": "While standard online ML models can adapt to data changes over time, they are still affected by data drift, also known as covariate shift. To address this issue, scenario 3 incorporates data drift detection & adaptation. Moreover, it defines that: (i) the cold start spans over the first 500 samples, corresponding to the initial width of the P window; (ii) the maximum width of ca sliding windows is 2000 samples. The proposed data drift detector determines the inter-window word-gram p-value and the inter-window AAD, using the Chi2ContingencyResult function 28 and the accuracy_score function29, respectively.\nFigure 4 shows the evolution of the inter-window AAD and word-gram p-value. The lens marks the detected data drift when p-value drops to 0.05, and AAD is above 0.05.\nOnce a drift is identified, the hyperparameter optimization starts. This process, which is the most time demanding, employs GridSearch30 with reduced configuration parameters (see Figure 3)."}, {"title": "4.5. ML classification", "content": "The selected classification techniques include HTC31, HATC32, and ARFC33 from River 0.11.134."}, {"title": "4.6. Explainability", "content": "Figure 5 displays the graphical and textual explanation of the classification of an incoming review. The buttons on the left vertical bar enable: (i) administrator profile access, (ii) search reviews by textual content, (iii) search reviews by timestamp, (iv) access to alerts, (v) visualization of the decision tree and associated natural language description (see Figure 6), (vi) saving the results in the cloud, and (vii) configuring the color layout (i.e., dark or clear mode). The most representative features for the classification are shown in the"}, {"title": "5. Conclusion", "content": "The use of crowdsourcing platforms to get information about products and services is growing. Customers search for reviews to make the best decision. Individuals submit dishonest and misleading feedback to manipulate a product or service's reputation or perception. These spam reviews can be created for various reasons, including financial gain, personal grudges, or competitive advantage. To address this problem, the proposed online method identifies and explains spam reviews. In addition, this research contributes with an online explainable classification engine to recognize spam reviews and, thus, to promote trust in digital media.\nSpecifically, the proposed method comprises (i) stream-based data processing (through feature engineering, incremental profiling, and selection), (ii) data drift detection & adaptation, (iii) stream-based classification, and (iv) explainability. The solution relies on stream-based processing, incrementally updating the profiling and classification models on each incoming event. Specifically, user profiles are computed using user- and content-based features engineered through NLP. Monitoring the incoming streams, the method detects data drifts using static and sliding windows. The classification relies on tree-based classifiers to obtain an interpretable stream-based classification. As a result, the user dashboard includes visual data and natural language knowledge to explain the classification of each incoming event. The experimental classification results of the proposed explainable and stream-based spam detection method show promising performance: 78.75 % accuracy and 78.44% macro F-measure obtained with the Yelp data set, and 86.13% accuracy and 85.89% macro F-measure with the MediaWiki data set. Moreover, the proposed data drift detection & adaptation approach performs better than well-known drift detectors (23.24 percent points higher in the F-measure for spam detection). According to the related work analysis, this proposal is the first to jointly provide stream-based data processing, profiling, classification with data drift detection & adaptation, and explainability."}]}