{"title": "GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for Improved Visual Localization", "authors": ["Gennady Sidorov", "Malik Mohrat", "Ksenia Lebedeva", "Ruslan Rakhimov", "Sergey Kolyubin"], "abstract": "Although various visual localization approaches exist, such as scene coordinate and pose regression, these methods often struggle with high memory consumption or extensive optimization requirements. To address these challenges, we utilize recent advancements in novel view synthesis, particularly 3D Gaussian Splatting (3DGS), to enhance localization. 3DGS allows for the compact encoding of both 3D geometry and scene appearance with its spatial features. Our method leverages the dense description maps produced by XFeat's lightweight keypoint detection and description model. We propose distilling these dense keypoint descriptors into 3DGS to improve the model's spatial understanding, leading to more accurate camera pose predictions through 2D-3D correspondences. After estimating an initial pose, we refine it using a photometric warping loss. Benchmarking on popular indoor and outdoor datasets shows that our approach surpasses state-of-the-art Neural Render Pose (NRP) methods, including NeRFMatch and PNeRFLoc.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual localization is a crucial task in computer vision that involves determining the pose (position and orientation) of a moving camera relative to a predefined environment map. This capability is essential for machines to understand their position within a 3D space and forms a foundational component in Simultaneous Localization and Mapping (SLAM) and Structure from Motion (SfM) systems. It also supports a range of practical applications, including mobile manipulation, autonomous driving, and augmented/virtual reality (AR/VR) experiences [1], [2].\nEarly methods for visual re-localization used image retrieval techniques. These approaches involved comparing a query image with a database of photos with known camera poses to approximate the environment's representation. The pose of the query image was inferred by identifying the closest match in the database. While straightforward, these methods often struggled with scalability and accuracy.\nSubsequent methods employed structured approaches, such as sparse feature matching. In these methods, a global map of 3D points was used to find correspondences between 2D feature points extracted from the query image and 3D points in the map. These techniques are known for their robustness, scene agnosticism, and high accuracy, even with short query times. However, they face challenges due to their significant memory requirements, especially in large-scale environments [3].\nPose regression methods introduced the use of neural networks to predict the pose from a query image. By integrating the environmental map into the network's architecture, these methods enabled end-to-end training without requiring a detailed 3D structure of the environment. However, they often lag behind structured methods in pose accuracy [4]. Absolute Pose Regression (APR) techniques offer high-quality pose estimations while addressing memory and time constraints. Scene Coordinate Regression (SCR) further refines this by optimizing small-sized maps to implicitly learn the scene and match input pixel locations with their 3D counterparts. This approach achieves accuracy comparable to structured methods but suffers from lengthy optimization processes.\nRecent advancements have explored neural-based 3D representations, such as Neural Radiance Fields (NeRF) [5] and 3D Gaussian Splatting (3DGS) [6]. These methods enhance localization by synthesizing new images from various views as a data augmentation step during training. They improve performance by encoding both 3D geometry and the appearance of the environment. Some methods establish correspondences between the query image and its rendered counterpart, but they often face issues with artifacts in"}, {"title": "II. RELATED WORK", "content": "A. Structure-Based Localization Methods\nStructure-based approaches use 3D scene information from Structure-from-Motion (SfM) to register a query image from the same scene via explicit 2D-3D correspondences, typically employing the PnP algorithm with RANSAC. While these methods can achieve high accuracy in pose estimation, they are sensitive to noisy correspondences [12], [13].\nScene coordinate regression (SCR) methods, a subset of structure-based approaches, also offer high accuracy. These methods regress 3D scene coordinates onto 2D images, allowing the use of reprojection error for pose estimation [14], [15]. In these models, the environment is represented by a trainable function that predicts dense correspondence fields between images and scenes. Neural networks can regress 2D-3D correspondences to serve as inputs for pose optimization using RANSAC. These methods may use random forest models [16], [17], convolutional neural networks with RGB-D input [18], [19], [20], or solely RGB images, as in ACE [15].\nIn our paper, we show that it is feasible to extend further the accuracy of structure-based models with a simple, yet effective method to firstly find a coarse camera pose and then refine the estimated pose by utilizing feature fields stored in the map representation of 3D Gaussians.\nB. Keypoints Description and Matching Models\nRecent advancements utilize Convolutional Neural Networks (CNNs) to enhance sparse detectors and local descriptors [21], [22], [23]. Some works draw on transformer architectures to improve the spatial relationship between key points and their visual appearance. Although transformer-based models offer robustness, they are computationally expensive. To address this, more efficient architectures have been proposed [24], [25]. For instance, XFeat [26] is a lightweight CNN-based feature extractor that produces compact dense descriptors and is trained on large-scale datasets with pixel-level ground truth correspondences. Feature matching-based localization methods have employed these networks for deep feature matching and pose refinement [27], [10], [28]. In our work, we chose the XFeat model to balance computational efficiency with accuracy.\nC. Localization Based on Novel View Synthesis\nNeRF [5] can generate high-quality images from new perspectives, which can be used for localization tasks. Algorithms such as DFNet [28] and NeFeS [10] use feature extractors to optimize differences between synthesized and reference images. Subsequent methods like iNeRF [29] refine pose estimation through an inverted NeRF task, and Direct-PN [30] uses NeRF as a direct matching module to compute photometric loss between synthesized and query images.\nRecently, NeRF models have been extended to synthesize and render feature fields along with radiance fields. These feature fields are typically learned through supervision from 2D feature extractors using volumetric rendering. Works such as [8], [7] have shown that 3D feature fields outperform 2D baselines in tasks like 3D segmentation and visual localization. For example, PNeRFLoc [31] improves pose estimation accuracy by minimizing warping loss for pixel alignment. NeRFMatch [32] enhances positioning by finding 2D-3D matches with specialized feature extractors. However, these methods require extensive training and rendering time.\nConversely, the 3DGS [6] approach represents scenes using differentiable 3D Gaussians. Methods like iComMa [33] integrate matching and comparison losses between rendered and query images to improve pose estimation robustness. GSLoc [11] refines pre-estimated poses from external pose extractors during test time.\nIn our paper, we explore distilled neural feature fields from scene-agnostic keypoint detector and descriptor models for the camera re-localization task. We demonstrate their role in providing pose prior and refining pose during test time, utilizing the advantages of 3D-GS such as explicit map representation and fast rendering speed."}, {"title": "III. PRELIMINARIES", "content": "Our method integrates keypoint descriptor models with 3D Gaussian Splatting, combining keypoint features into the 3D representation to improve re-localization accuracy. We outline this process briefly."}, {"title": "A. Feature Distillation into 3D Gaussian Splatting", "content": "Building on the Feature-3DGS framework [9], we employ a modified 3DGS to distill high-dimensional features into a feature field, alongside a radiance field. This technique uses parallel N-dimensional Gaussian rasterization and a speed-up module, making it compatible with various 2D foundation models.\nWe initialize the 3D Gaussians using a point cloud from an SfM model. Their projection into 2D space involves transforming covariance matrices and incorporating rotation, scaling, opacity, spherical harmonics, color, and other visual features.\nPixel color and feature values are computed through \u03b1-blending, supervised by a \"teacher\" model, using differentiable rendering for distillation. The joint optimization method rasterizes both RGB images and feature maps simultaneously, ensuring high fidelity and per-pixel accuracy.\nThe optimizable attributes of the i-th 3D Gaussian \u03a8\u1d62 are:\n\u03a8\u1d62 = {y\u1d62, q\u1d62, s\u1d62, \u03b1\u1d62, c\u1d62, f\u1d62}, (1)\nwhere y\u1d62 \u2208 \u211d\u00b3 represents the 3D position, q\u1d62 \u2208 \u211d\u2074 denotes the rotation quaternion, s\u1d62 \u2208 \u211d is the scaling factor, \u03b1\u1d62 \u2208 \u211d is the opacity value, c\u1d62 \u2208 \u211d\u00b3 represents the diffuse color from Spherical Harmonics (SH), and f\u1d62 \u2208 \u211d\u2c7d is the feature embedding from the supervised model Ft, where V denotes the dimension of the feature vector. Each Gaussian \u03a8\u1d62 is thus positioned at y\u1d62 with a feature vector f\u1d62 that encodes local spatial and visual content.\nThe following equations define the computation of pixel color C and pixel feature F\u1d63 during rendering:\nC = \u2211\u1d62\u2208N c\u1d62\u03b1\u1d62T\u1d62, F\u1d63 = \u2211\u1d62\u2208N f\u1d62\u03b1\u1d62T\u1d62, (2)\nwhere N represents the set of overlapping 3D Gaussians for a given pixel, and T\u1d62 denotes the transmittance, which is the product of the opacity values of previous Gaussians overlapping the same pixel.\nTo train the 3DGS model for a specific scene with grounded feature maps, we use the loss function L_GS, which combines photometric loss L_color and feature loss L_features. The photometric loss L_color consists of L\u2081 and L_SSIM losses between the ground truth image I and the rendered image \u00ce, while L_features measures the difference between the supervised feature map F\u209c(I) and the rendered feature map F\u1d63:\nL_GS = L_color + L_features, (3)\nwhere:\nL_color = (1 \u2212 \u03bb)L\u2081(I, \u00ce) + \u03bbL_SSIM(I, \u00ce),\nL_features = ||F\u209c(I) \u2013 F\u1d63||\u2081."}, {"title": "IV. METHODOLOGY", "content": "Our method comprises a two-stage pipeline, as depicted in Figure 2. The first stage involves modeling the scene, and the second stage focuses on estimating an initial coarse pose and refining it for improved accuracy.\nInitially, we model the scene using a feature-based 3D Gaussian Splatting (3DGS) approach [9], guided by a keypoint descriptor network. We utilize the deep feature extractor XFeat [26] due to its robustness in extracting reliable and distinctive features across various environments, both indoor and outdoor, even in the presence of dynamic elements.\nFor each reference image I\u209c \u2208 \u211d^(W\u00d7H\u00d73), the XFeat network computes a feature map F\u209c(I) \u2208 \u211d^((W/8)\u00d7(H/8)\u00d764) which is bilinearly upsampled to the full resolution. We train the 3D Gaussian Splatting model by minimizing the loss function described in Equation 3.\nOnce the scene is learned by 3DGS, we estimate the pose for the query image through a two-step process: initially estimating a coarse pose and then refining it."}, {"title": "a) Obtaining the Initial Coarse Pose:", "content": "This stage aims to establish correspondences between 2D keypoints in the query image and 3D points in the 3DGS model of the scene. We use a Perspective-n-Point (PnP) solver within a RANSAC loop to provide an initial pose estimate.\nFor a given query image q with extracted keypoints Pq and features fq from a descriptor model, we perform 2D-3D correspondence matching with the 3D Gaussian point cloud P \u2208 \u211d^(N\u00d73) and the associated distilled XFeat features fp \u2208 \u211d^(N\u00d764), where N denotes the number of points. Following the PNeRFLoc method [31], we use cosine similarity to match the 2D query image features with the 3D scene features distilled in the 3D Gaussians. The 2D-3D correspondences V(i) for each i-th pixel are determined by maximizing this measure:\nV(i) = arg max_p\u2208P (fp \u22c5 fq) / (||fp|| ||fq||) (4)\nWhile PNeRFLoc accelerates the search by learning a reliability score for each point and filtering based on this score, we use only the sparse, reliable keypoints from the query image extracted by XFeat and all points in the point cloud. This approach supports accurate and efficient pose estimation through semi-dense matching between the query image and the Gaussian cloud of distilled features."}, {"title": "b) Test-time Camera Pose Refinement:", "content": "We refine the coarse pose estimate by aligning the rendered image with the input query image.\nPrevious works [34], [35] used gradient descent to minimize photometric residuals, requiring neural rendering at each step. PNeRFLoc [31] improved this by introducing a warping loss function that requires rendering only once. We adopt this warping loss approach. Additionally, the 3DGS framework enhances speed with its faster rendering process compared to PNeRFLoc.\nFor a given query image q and an initial coarse pose (R, t), GSplatLoc first renders an image q\u1d63 and a depth map d\u1d63 using the initial pose. We aim to optimize the pose estimate (R', t') by minimizing a warping loss, defined as the sum of pixel-wise RGB differences between the reference and query images:\nL_rgb-warp = \u2211_p\u1d62 ||C(q, W(p\u1d62, R, t, R', t')) - C(q\u1d63, p\u1d62)||\u2082 (5)\nW(p\u1d62, R, t, R', t') = \u03a0(R'(R\u207b\u00b9\u03a0\u207b\u00b9(p\u1d62, d\u1d63(p\u1d62)) - R\u207b\u00b9t) + t'), (6)\nHere, C(q\u1d63, p\u1d62) \u2208 \u211d\u00b3 is the RGB color at pixel p\u1d62 \u2208 \u211d\u00b2 on the rendered image q\u1d63, and the warp function W(\u22c5) finds the corresponding pixel on the query image q by warping p\u1d62 from the reference image q\u1d63. Specifically, W back-projects p\u1d62 into the 3D space of the camera coordinate system of q\u1d63 using the rendered depth d\u1d63, transforms it to the camera coordinate system of q using the optimized pose (R', t'), and then projects it onto the image q."}, {"title": "V. EXPERIMENTS", "content": "To evaluate our work, we use the 7Scenes dataset [42] for indoor validation and the Cambridge Landmarks dataset [36] for outdoor validation.\nDuring the modeling phase, we use COLMAP [43] to create point clouds and initialize poses for each scene in these datasets. We then train the 3D Gaussian Splatting (3DGS) model from [9] on each scene for 15,000 iterations. XFeat [26] extracts dense features to be used in 3DGS.\nIn the testing phase, we start by obtaining an initial coarse pose. We sample 1,000 of the most reliable descriptors and match them with the 3D feature cloud. We set the number of RANSAC iterations to 20,000.\nFor the refinement step, we render a visual reference for the coarse pose once and use the Adam optimizer with a learning rate of 0.001. We optimize both translation and rotation in quaternion form.  shows that our rendering-based optimization gradually improves localization accuracy from the initial coarse pose, with about 250 iterations needed for indoor scenes and 350 for outdoor scenes while we render the visual reference process only once."}, {"title": "Discussion on results", "content": "Our method achieves state-of-the-art performance among neural rendering pose estimation (NRP) methods in both indoor and outdoor scenarios, although it performs slightly worse than ACE in indoor environments.\nWe argue that there are two reasons for this. First, feature distillation combined with 3DGS enables accurate structure-based coarse pose estimation. Second, representations that provide realistic images facilitate effective photometric optimization.\nIn contrast, ACE, a scene coordinate regression (SCR)-based pose estimation method, struggles with outdoor scenarios due to its limited modeling capabilities, constrained by its shallow MLP-based scene encoding design.\nOverall, NRP is a more versatile framework than SCR, as it allows solving multiple tasks in parallel using the same scene representation. For example, 3DGS can be used to encode semantic or language-aligned instances or even model dynamic scenes-tasks that are crucial in robotics. This flexibility enables a broader range of interactions with the environment and enhances localization.\nOne key advantage of our proposed pipeline is its speed. It requires only 0.3 seconds to estimate an initial coarse pose, compared to around 3 seconds in the PNeRFLoc [31].\nWhile not explored in this paper, we anticipate that our method could further improve by removing floaters from the trained 3DGS model."}, {"title": "VI. CONCLUSION", "content": "In this paper, we demonstrate how to leverage the powerful 3D Gaussian Splatting (3DGS) scene representation and introduce the GSplatLoc framework for accurate and efficient pose estimation and refinement. By incorporating structure-based feature matching with rendering-based optimization, our approach estimates an initial pose without relying on a separate pose estimator. The proposed framework's ability to extract scene-agnostic descriptors from 3DGS enhances pose estimation quality. This unified method estimates the initial pose through 2D-3D correspondences and refines it by minimizing warping loss between the query and rendered images.\nFuture work may focus on more challenging, large-scale outdoor scenarios where 3DGS has already shown promising results. Extensions such as CityGaussian [44] or VastGaussian [45] could be valuable avenues for exploration."}]}