{"title": "Implementation and Application of an Intelligibility Protocol for Interaction with an LLM", "authors": ["Ashwin Srinivasan", "Karan Bania", "Shreyas V", "Harshvardhan Mestha", "Sidong Liu"], "abstract": "Our interest is in constructing interactive systems involving a human-expert interacting with a machine learning engine on data analysis tasks. This is of relevance when addressing complex problems arising in areas of science, the environment, medicine and so on, which are not immediately amenable to the usual methods of statistical or mathematical modelling. In such situations, it is possible that harnessing human expertise and creativity to modern machine-learning capabilities of identifying patterns by constructing new internal representations of the data may provide some insight to possible solutions. In this paper, we examine the implementation of an abstract protocol developed for interaction between agents, each capable of constructing predictions and explanations. The PXP protocol, described in [12] is motivated by the notion of \u201ctwo-way intelligibility\u201d and is specified using a pair of communicating finite-state machines. While the formalisation allows the authors to prove several properties about the protocol, no implementation was presented. Here, we address this shortcoming for the case in which one of the agents acts as a \"generator\" using a large language model (LLM) and the other is an agent that acts as a \"tester\" using either a human-expert, or a proxy for a human-expert (for example, a database compiled using human-expertise). We believe these use-cases will be a widely applicable form of interaction for problems of the kind mentioned above. We present an algorithmic description of general-purpose implementation, and conduct preliminary experiments on its use in two different areas (radiology and drug-discovery). The experimental results provide early evidence in support of the protocol's capability of capturing one- and two-way intelligibility in human-LLM in the manner proposed in [12].", "sections": [{"title": "Introduction", "content": "In the design of human-machine systems, the need for predictions made by machine-constructed models to be intelligible to a human has been evident for at least four decades. To the best of our knowledge, the earliest identification of a possible mismatch in the representations used by humans and machines was by Michie in [8]. He describes the notion of a \u2018Human Window' of comprehension based on constraints on computation and storage constraints imposed by the biology of the brain. Consequences of machine-constructed assistance falling outside this human window are examined on synthetic problems (chess endgames) in [5], who also describe some real-life disasters arising from the use of machine-constructed assistance for humans operating in safety-critical areas (the Three Mile Island reactor meltdown being one such). Assuming the existence of the human window, Michie went on to propose a classification of machine-learning (ML) systems into three categories [9]. Weak ML systems are concerned only with improving performance, given sample data. Strong ML systems improve performance, but are also required to communicate what it has learned in some human-comprehensible form (Michie assumes this will be symbolic). Ultra-strong ML systems are Strong ML systems that can also teach the human to improve his or her performance. This categorisation has recently informed a similar 3-way categorisation for the use of AI tools in scientific discovery [6], and to evaluate an Inductive Logic Programming (ILP) as a form of Ultra-Strong Machine Learning [1].\nThe following aspects of Michie's characterisation are worth emphasising. Firstly, it is clearly intended for use in a human-in-the-loop setting, though the human can be a teacher, student or collaborator. Secondly, the characterisation is about intelligibility, not intelligence. Intelligibility as stated is a relation between the ML system (the sender), what the ML system communicates (the message), and the human (the receiver). Thus, the ML system can employ any representation for its internal model; all that is needed is that it can communicate the \"what\" and \"why\" in a form that lies within the human window of comprehension.\u00b9 Thirdly, it appears to be a classification of an ML system based on one-way communication from the machine to the human. It is not apparent what happens in situations where the communication is from the human to the machine (this may well occur in collaborative scientific discovery, for example). Symmetry would suggest the existence of a 'Machine Window' and associated requirements of the machine receiving comprehensible messages, but this is not considered in [9]\u00b2. Finally, nothing is proposed by way of a quantitative or qualitative assessment for one-way intelligibility of the machine's communication (this is addressed by [1], who propose a quantitative measure of how beneficial the machine's explanation was to the human).\nRecently, a proposal for inferring \"two-way intelligibility\" has been proposed, as a property of execution of a communication protocol between agents that can make predictions, and provide explanations for their predictions [12]. The proposal rests on the development of a formal model for communication in which agents are modelled as finite-state machines. The authors specify labelled transitions allowed, initiated by sending (respectively, receiving) messages that are essentially tagged by one of \u201c4R's\u201d: ratification, refutation, revision, and rejection. Properties like one- and two-way intelligibility are then proposed based on the sequences of message-tags exchanged. The proposal contains a complete specification of the protocol, and also includes proofs of correctness and termination.\nRemarkable recent successes with the use of large language models, both as a tool for (conditional) sampling from very flexible spaces and for generating answers to questions in natural language, suggest that they may be an appropriate choice of machine in human-machine collaborative systems. Of particular interest is the possibility of using an LLM to generate predictions, and explanations in a human-readable form when provided with an appropriate choice of conditioning information. But it is in this last caveat that much of the difficulty lies. How should the interaction with an LLM proceed so that we arrive at this appropriate form of conditional information? This paper is concerned with restricting the interaction to the exchanges allowed in the protocol in [12]. As will be seen below, the predictions and explanations generated by an LLM are examined by a tester agent (in the experiments, a human-expert or a proxy for such an expert). The tester sends back a message. The interaction updates a growing repository of messages and context information which is used to automatically update the conditioning prompt for the LLM.\nAn immediate practical difficulty arises from the fact that no implementation of the protocol"}, {"title": "The Main Aspects of the PXP Protocol", "content": "The PXP protocol is an interaction model described in [12]. It is motivated by the question of when an interaction between agents could be said to be intelligible to either. Restricting attention to specific kinds of agents-called PEX agents-the paper is concerned with when predictions and explanations for a data instance produced by one PEX agent is intelligible to another PEX agent. The agents are modelled as finite-state machines, and \"intelligibility\" is defined as a property inferrable from the messages exchanged between the finite-state machines. The transitions of either automaton is reproduced here in Fig. 1. It also shows, as edge-labels, the messages sent and received by the PEX automaton. For us, the main feature of the protocol is that messages only have 1 of 4 \"tags\", namely: RATIFY, REFUTE, REVISE, or REJ\u0415\u0421\u0422."}, {"title": "An Implementation for Modelling Human-LLM Interaction", "content": "The implementation described here is restricted to interaction between a single human-agent and a single machine-agent. The protocol in [12] is akin to a plain-old-telephone-system (POTS), and it sufficient for our purposes to implement it as a rudimentary blackboard system with a simple scheduler that alternates between the two agents. The blackboard consists of 3 tables accessible to the agents. The tables are: (a) Data. This is a table consisting of (s,x) pairs where s is a session-id, and x is a data instance; and (b) Message. A table of 5-tuples (s, j, \u03b1, \u03bc, \u03b2) where: s is a session identifier, jis a message-number, a is a sender-id, \u03bc is a message and \u03b2 is a receiver-id; and (c) Context. This a table consisting of 3-tuples (s, j, c) where s is a session-id, j is the message number, and c is some domain-specific context information.\nFor simplicity, message-numbers will be assumed to be from the set {0, 1, 2, . . .}; \u03b1, \u03b2 are from {h, m} where h denoting \u201chuman\" and m denotes \u201cmachine\u201d; and messages \u03bc are (l, y, e) tuples, where l is from {RATIFY, REFUTE, REVISE, REJECT}, and y and e are the prediction and explanation respectively. We will treat the blackboard as a relational database A consisting of the set of tables {D, M, C'}. For ease of presentation, \u2206 is denoted as a \"shared\" input in the agent-functions below.\nThe procedure in Fig. 1 implements the interaction. It is evident that the interaction is initiated by the machine, with its prediction and explanation for a data instance (one-per-session), and proceeds until all data instances have been examined. The procedure returns a summary of the interaction in the form of the contents of the common storage. This contains a record of the sessions in terms of the data provided (table D), and messages exchanged (table M). This information is mainly generated through the use of a function called ASK_AGENT, implemented as in Fig. 3. These functions ask the corresponding machine and human-agents to provide: (a) an assessment of the prediction and explanation provided for the data instance; and (b) the agent's own prediction and explanation from the data instance. The assessment is a message-tag, obtained in the manner described in [12] and summarised in Sec. 2 above, with the following small modification:\n\u2022 We assume that both agents only send a REJECT tag after the interaction has proceeded for some minimum number of messages. Until then, the machine's message will be tagged either as REVISE (if it is able to revise its output to match one or both of the human-agent's prediction or explanation) or REFUTE otherwise. After the bound, the machine can send a machine with a REJECT tag. Similarly, the human-agent will send a REFUTE message to the machine until this bound is reached, after which the message tag can be REJECT. Since the message-length is bounded, it will not affect the termination properties of the bounded version of PXP\nThe procedure for calling the human- or machine-agent is in Fig. 2. It is not surprising that the same procedure suffices for both kinds of agents, since PXP is a symmetric protocol that does not distinguish between agents (other than a special agent called the oracle).\nAgent-specific details arise in the MATCH and AGREE relations, and in the question-answering step (the ASK_AGENT function), which is shown in Fig. 3. The ASSEMBLE_PROMPT is domain-dependent, and not described algorithmically here. Instead, we present it by example below. The MATCH and AGREE functions used are described in the experimental section, Sec. 4."}, {"title": "Experimental Evaluation", "content": "In this section, we examine Human-LLM interaction through the use of the INTERACT procedure. We will focus on one aspect of using an LLM, namely prompt-tuning. Specifically, we consider whether an LLM improves its predictive and explanatory performance by adjusting its prompt based on feedback received from a human-source. Experiments reported here look at two real-world problems:\nX-Ray Diagnosis (RAD). We want to be able to use LLMs for diagnosing X-rays and producing reports as explanations. Use cases motivating this need are the diagnosis of disease; and assistance in emergency medicine in hospitals where specialised clinical expertise is limited or unavailable.\nMolecule Synthesis (DRUG). We want to be able to use LLMs for proposing synthesis pathways for small molecules that can become potential drugs (\"leads\"). The primary use-case for this is in extending computational drug-design beyond the stage of generating novel molecules, to include plans for their cost-effective synthesis prior to biological testing.\nThe reasons of the choice of LLMs is not important here. Instead, in each case we provide experimental evidence for the following:\nThe INTERACT Conjecture. The INTERACT procedure allows us to estimate the proportion of interactions exhibiting: (a) one- and two-way intelligibility; and (b) Strong- and Ultra-Strong Intelligibility.\nWe note that this is a conjecture about intelligibility and not about performance (accuracy of prediction, or quality of explanations). We intend to examine that aspect separately."}, {"title": "Materials", "content": "The RAD problem is concerned with obtaining predictions and explanations for up to 5 diseases from X-ray images. In this paper, an LLM is used to generate diagnoses and reports, given image data. We use data from the Radiopedia database [11]. This is a multi-modal dataset compiled and peer-reviewed by radiologists. Each tuple in our database consists of: (a) An X-ray image; (b) A prediction consisting of a set of diagnosed diseases; and (c) An explanation in the form of a radiological report. Both (b) and (c) are from expert human annotators. An example of a database entry is shown in Fig. 2. We use a subset of the full Radiopedia database, focussing on 5 ailments (Atelectasis, Pneumonia, Pneumothorax, PleuralEf fusion, and Cardiomegaly) as our database with 4 instances per disease (20 instances overall)."}, {"title": "Agents", "content": "For RAD, the \"human-agent\" is an agent that has access to the database of the human-derived predictions and explanations. In addition, to obtain the message tags we require the agent includes a MATCH function for comparing predictions; and a AGREE function for comparing explanations. For RAD, we implement these as follows:\nMATCH. Check whether the prediction of the machine-agent matches with the ground truth, that is check whether the machine-agent correctly predicts the presence of the disease. This is a simple equality check.\nAGREE. Check whether the explanation produced by the machine-agent is consistent with the explanation provided by the human-compiled database. Usually, this involves whether the two explanations concur. This is done here by querying a second (\"tester\") LLM with a simple prompt similar to \"Are these two reports consistent with each other?\".\nWe note that since the predictions and explanations are from an immutable database, there is no possibility of the human-agent in RAD sending a REVISE tag.\nFor DRUG, the human-agent has access to a chemist (one of the authors of this paper) who assesses the predictions and explanations from the LLM. We assume the chemist nominally employs MATCH and AGREE functions, although these are not implemented as computational procedures. Instead, the chemist directly provides the message-tag along with predictions and explanations. In principle, the chemist's message can be tagged with any of the tags allowed in PXP\nFor RAD and DRUG, the machine-agent uses an LLM for generating predictions and explanations. The MATCH function for RAD is a simple matching of the predictions (for RAD, this is simply checking the Yes/No labels; for DRUG this involves checking if the synthesis pathways are identical). For both problems, the machine-agent uses a separate LLM is used to implement AGREE. The task of this second (\u201cchecker\u201d) LLM is to determine if the explanation obtained from the human-agent is consistent with the explanation generated by (the generator LLM used by) the machine-agent. This is similar to the approach used by the human-agent in RAD."}, {"title": "Algorithms and Machines", "content": "In the experiments, the following LLMs were used: (a) For RAD, the generation of predictions and explanations is done by GPT-40-mini [10], and the check for agreement in explanations is done by GPT-40 [10]; and (b) For DRUG, the generation of predictions, explanations and the check for agreement in explanations was done by Claude-3 Opus [2]. Python code (version: 3.9.18) for the INTERACT procedure is available at https://github.com/karannb/interact. Data used for the RAD experiments can be obtained by writing to SL; and for the DRUG experiments from SV. All RAD experiments are conducted using a Macbook Air machine with 8GB of main memory and dual-core Intel i3 1.1GHz processors. All DRUG experiments were conducted using a Macbook Pro machine with 16GB of main memory and Apple Silicon M2 Pro processor with 12 core CPU and 19 core GPU."}, {"title": "Method", "content": "Our method is straightforward:\nFor each problem:\n1. For r = 1 ... R:\n (a) Initiate the INTERACT procedure with the data available obtain a record A on termination of the INTERACT procedure\n (b) Store A as Ar\n2. Using the records in \u03941, ..., \u0394R, obtain the frequency of sessions that are: (a) one-way and two-way Intelligible; and (b) Strong and Ultra-Strong Intelligible\n3. Estimate the proportions of interest from the median values obtained above\nThe following details are relevant:\n\u2022 Repetitions are needed since we are using LLMs. These can yield different results, depending on the value of the \"temperature\" parameter. However, while repetitions can be performed without difficulty in RAD, the involvement of a chemist in DRUG makes this difficult. Therefore, for the results reported, R = 5 for RAD, and R = 1 for DRUG;\n\u2022 The INTERACT procedure requires a bound on the total number of messages exchanged. For the experiments here, this bound is set to 10 (that is, 5 interaction cycles between the agents);\n\u2022 For RAD, the \"temperature\" parameter for the LLM used to check explanations (used by both agents) is set to 0.0. The LLM used to to generate predictions and explanations by the machine-agent uses the the default value of temperature in the OpenAI API (1.0);\n\u2022 For DRUG, the \"temperature\" parameter for the LLM used to check explanations by the machine is set to 0.0. The LLM used to generate predictions and explanations by the machine-agent has temperature value of 0.3;\n\u2022 As specified in the PXP protocol, a session between human and machine exhibits one-way intelligibility for the human-agent (reply. machine-agent) if there is at least one entry in the message-table in a A with the human (machine) as sender that has a REVISE or RATIFY tag. A session is two-way Intelligible if it is one-way Intelligible for the human and one-way Intelligible for the machine;\n\u2022 As proposed in [12], a session between human and machine is Strongly Intelligibility for the human-agent (resply. machine-agent) if every entry in the message-table in a A with the human (machine) as sender has a REVISE or RATIFY tag. A session is Ultra-Strong for the human (machine) if it is Strongly Intelligible and contains at least one entry in the message-table in A with the human (machine) as sender that has a REVISE tag.\n\u2022 We will use the usual (maximum-likelihood) estimate of proportion as the ratio of the number of sessions with a property to the total number of sessions."}, {"title": "Results", "content": "The statistics of interaction are tabulated in Fig. 4. It is evident that the results support the INTERACT conjecture. However, the following additional observations have possibly more interesting long-term consequences of using the INTERACT implementation of PXP :\nOne- and Two-Way Intelligibility. The frequencies of sessions that are 1-way intelligible are high for both agents in RAD, and very high in DRUG. Some of this is due simply to the vast store of prior data and information within an LLM that allows it to provide correct predictions and explanations almost immediately. For example, in 11 of 20 sessions in DRUG, the chemist and machine-agent agree on the prediction and explanations within 3 exchanges. However, interestingly, in an additional 7 sessions, they agree after some exchange of refutations and revisions. For human-in-the-loop systems, this is indicative of the advantage to the human of being provided with explanations in natural language. It is also evidence that the LLM is able to act to text-based feedback from the human (this is consistent with the findings in [3]). The number of sessions that are 2-way intelligible clearly cannot be more than than the lower number of 1-way intelligible sessions.\nStrong- and Ultra-Strong Intelligibility. While it is not surprising to find the number of strongly intelligible sessions is lower than the number of 1- or 2-way intelligible sessions, the difference in numbers between strong- and ultra-strong sessions is surprisingly high. Resolving this requires examining first the sessions exhibiting strong intelligibility. Closer examination shows that these are exactly those sessions which are immediately ratified by both human- and machine-agents. This is a degenerate form of strong-intelligibility (see Defn. 2). More interesting strong-intelligibility would arise from longer sessions (for example, having a hypothetical sequence of message-tags: \u3008INITm, REVISEh, REVISEm, RATIFY\u0127, RATIFYm)). In fact, no such sequences occur. Thus, if we ignore these very short interactions, there are in fact, very few strongly intelligible sessions. It is interesting though that in DRUG, we are able to observe 18 such sessions for the machine-agent."}, {"title": "Some Additional Observations", "content": "The relatively more controlled setting of RAD allows us to make the following additional observations:\n\u2022 Both the human- and machine-agents use an LLM to implement their respective AGREE functions. The use of LLMs in this manner brings out an important aspect of LLM behaviour and of the INTERACT implementation Consider for example the following (observed) sequence of messages in the message-table (with some extra annotations for clarity): ( (s1,1, m, (INITm, y, em), h), (s1,2, h, (REFUTE\u04bb, y, eh), m), (s1,3, m, (REFUTEm,y,em), h), (s1, 4, h, (RATIFY\u0127,y,eh),m) ). That is, the human-agent appears to change its mind about en and em agreeing. Recall For RAD there is no opportunity for the human-agent to change the database of predictions and explanations. That is, human-agent cannot revise its \"model\". Therefore this must arise from the LLM-based check for AGREEh, apparently returning different responses even with the same inputs. However: (a) the reader can verify that the INTERACT implementation actually sends the explanation obtained at j = 3 and not j = 1. Nonetheless, the LLM-based check of consensus between the machine-explanations at j = 1 and j = 3 does not require them to be identical. Thus, em at j = 3 is really \u2248 em. This difference may be sufficient for the subsequent check at j = 4 by the human-agent to result in a change, and (b) it is in fact not correct to say that the LLM at j = 2 and j = 4 have the same inputs. At j = 4, the human-agent has more information and the corresponding context for the LLM (and therefore its state) changes. Correspondingly the sampling distribution changes, as does the result. Both these observations hold even if the \"temperature\" parameter of the LLM is set to 0.\n\u2022 The INTERACT procedure processes instances one-at-a-time, as required by PXP. We have chosen not to reset the state of the generator LLM after each session. This has the advantage of the LLM acquiring more information as the sessions proceed. However, it does introduce an order-dependence into the results. Thus, we would expect differences in the numbers tabulated from a different session-ordering."}, {"title": "Conclusion", "content": "In the second half of his seminal 1950 paper [13], Alan Turing describes an autonomous agent that has the capacity to alter its programming based on experiments and mistakes, rather than relying on human programmers. Since then, developments in mathematics and computing have been making steady progress in providing the groundwork for Machine Learning, or ML. But it is only recently that we have witnessed a sea-change in the use of ML methods. This has meant that ML can now be part of almost any kind of activity for which data can be collected and analysed. A difficulty has arisen, however, when attempting to exploit the predictive performance of modern ML techniques when the models they construct have to be examined by humans who are not ML specialists. For example, a modern-day deep neural network may be able to predict, with very high accuracy, the occurrence of malignancies from X-ray images. If what is required by a junior clinician assisting in A&E is not just what the prediction is, but also an explanation of how that prediction was arrived at, then we hit an intelligibility bottleneck\". More generally, one could view this as a problem arising from the information shared over sequences of communicative interactions [4].\nThe notion of \"intelligibility as communication\", particularly with a view to applicability to the design of human-ML systems collaborating to predict and explain data is the main motivation of the abstract interaction model in [12]. However, the paper's focus is conceptual, and no practical implementation is provided to test the ideas proposed. The paper does however contain some case-studies from the ML literature that have involved human-ML interaction, that indicate how the \"intelligibility protocol\u201d proposed could be used to characterise qualitatively whether the communication between human and machine was intelligible to one or the other or both. This paper aims to extend this research further by developing a simple interactive procedure that exchanges messages in the manner proposed in [12], and to test the implementation on problems that involve human interaction with a large language model (LLM). The reason for the specific focus on LLMs is twofold. First, we believe that the use of natural language afforded by LLMs is well-suited for developing interactive systems with humans who are not ML experts (but nevertheless have substantial domain-specific expertise). Secondly, the availability of very large \"foundational LLM\" models provide us with the opportunity of using ML models with vast stores of generic knowledge, of the kind that is likely to be needed for performing complex data-driven analysis.\nWhile the obvious kind of use of a computational tool for human-ML interaction is to collaboratively address difficult problems in areas such as scientific discovery, climate change, health and so on, the goal in this paper is less ambitious. We have chosen to focus on calibrating the implementation's ability to quantify the intelligibility of interaction. The encouraging results here therefore are only to be seen as a first step in providing empirical support for the use of the PXP protocol as a basis for the design of human-ML collaborative systems."}]}