{"title": "NCT-CRC-HE: Not All Histopathological Datasets Are Equally Useful", "authors": ["Andrey Ignatov", "Grigory Malivenko"], "abstract": "Numerous deep learning-based solutions have been proposed for histopathological image analysis over the past years. While they usually demonstrate exceptionally high accuracy, one key question is whether their precision might be affected by low-level image properties not related to histopathology but caused by microscopy image handling and pre-processing. In this paper, we analyze a popular NCT-CRC-HE-100K colorectal cancer dataset used in numerous prior works and show that both this dataset and the obtained results may be affected by data-specific biases. The most prominent revealed dataset issues are inappropriate color normalization, severe JPEG artifacts inconsistent between different classes, and completely corrupted tissue samples resulting from incorrect image dynamic range handling. We show that even the simplest model using only 3 features per image (red, green and blue color intensities) can demonstrate over 50% accuracy on this 9-class dataset, while using color histogram not explicitly capturing cell morphology features yields over 82% accuracy. Moreover, we show that a basic EfficientNet-BO ImageNet pretrained model can achieve over 97.7% accuracy on this dataset, outperforming all previously proposed solutions developed for this task, including dedicated foundation histopathological models and large cell morphology-aware neural networks. The NCT-CRC-HE dataset is publicly available and can be freely used to replicate the presented results. The codes and pre-trained models used in this paper are available at https://github.com/gmalivenko/NCT-CRC-HE-experiments.", "sections": [{"title": "1 Introduction", "content": "Digital histopathology is a rapidly evolving field that focuses on automatic computer-assisted analysis of high-resolution microscopy photos of stained tissue regions, also called whole slide images (WSIs). These tissue photos provide lots of valuable morphological information on the cellular level that is relevant for clinical diagnostics, including cell type composition and cell-cell interactions, activity of the immune system, cell cycle progression, various abnormalities in cell structure and shape that are often good indicators of cellular stress, etc. Previous research works demonstrated that this histopathological data can be used for designing diagnostic tools for many different biomedical tasks including tissue lesion detection and cancer classification, tumor grading, predicting gene mutants, biomarkers and overall gene expression levels, detecting mitosis, quantifying the activity of the immune system, predicting patient survival, etc.\nA large amount of rich visual data provided by WSIs led to a rapid development of various deep learning-based solutions for the analysis of histopathological images. As deep neural networks can automatically learn complex patterns directly from the data, taking into account all morphological features and revealing hidden data structures, they were able to achieve top results on the majority of whole slide image analysis tasks, often outperforming the results demonstrated by professional pathologists. However, the real predictive power of such solutions strongly depends on the quality of the datasets used for their training, and might be biased towards some specific data properties not related to the task itself. When it comes to histopathological datasets, the biggest source of bias here is related to the overall data formation procedure: as one usually cannot collect data for multiple diseases or patients in the same institution, large-scale datasets represent a compilation of microscopy images obtained in different laboratories or even countries. This often leads to a pronounced batch effect: since images are collected with different equipment, by different technicians using slightly varying tissue staining / handling techniques, and additionally post-processed with different libraries and tools, they might contain site-specific signatures that can be used to uniquely identify image origin. While this variation might not be an issue when all images are sampled randomly from different places, in practice each laboratory usually specializes in a specific disease or tissue type, and thus the entire data for some classes is often obtained in one specific place, encompassing the corresponding low-level image signatures. A number of image normalization methods have been proposed to deal with this issue, however, several research works indicate low efficiency of such tools in eliminating all inherent site-specific image properties. Therefore, one key question remains: do the advanced deep learning methods form their decision rules based on disease-specific tissue morphology, or they largely rely on variation in staining, resolution and image processing artifacts specific for each tissue class.\nIn this work, we focus on the exploration of the NCT-CRC-HE [21] colorectal cancer dataset consisting of 100,000 training / 7,180 test image patches belonging to nine tissue classes: adipose, background, debris, lymphocyte, mucus, smooth muscle, normal colon mucosa, cancer-associated stroma and colorectal adenocarcinoma epithelium. This dataset gained high popularity among the research community with numerous approaches proposed for tissue classification and patient survival prediction, starting from basic CNNs to advanced foundation transformer models and dedicated cell morphology-"}, {"title": "2 Exploring and Analyzing the NCT-CRC-HE Dataset", "content": "NCT-CRC-HE dataset [21] consists of two independent partitions: NCT-CRC-HE-100K with 100,000 training patches extracted from 86 whole slide images, and CRC-VAL-HE-7K containing 7180 test patches from 50 separate patients with colorectal adenocarcinoma. The corresponding tissue samples combine data obtained from the tissue bank of the National Center for Tumor diseases (NCT) and the pathology archive at the University Medical Center Mannheim (UMM). All images were normalized with the Macenko method [30], the resolution of the extracted patches is 224\u00d7224 pixels. The dataset is publicly available and can be downloaded from https://zenodo.org/records/1214456.\nThe initial visual inspection of patches belonging to different tissue classes (Fig. 1) indicated the presence of various artifacts on the considered images and a potential difference in color intensities for different tissue classes. Therefore, a more detailed analysis of the found issues was performed to analyze their severity and potential effect on the trained deep learning models."}, {"title": "2.1 RGB Channel Intensities and Color Distribution", "content": "When observing visualized image crops (Fig. 1), one can notice the difference in the color intensity / brightness for different tissue classes. In principle, this difference should be partly eliminated by using various stain normalization techniques developed to reduce any potential batch effect. The authors of the NCT-CRC-HE dataset used the Macenko normalization method [30], nevertheless, the normalized images still have a pronounced color signature.\nTo quantify our observations, we first decided to visualize average red, green and blue color intensities for images from different classes. For this, we averaged the corresponding RGB color channels, thus each image became encoded by three features. The resulting 3D scatter plots as well as 2D projections to the corresponding color spaces are provided in Fig. 2 and Fig. 3 for the training and test sets, respectively. One can observe that samples from different classes are not well mixed, there exists clear overlapping clusters corresponding to different tissue types. Additionally, there is a slight mismatch in RGB intensities distribution between the training and test sets that might potentially contribute to reduced test accuracy for previously proposed transformer and CNN models.\nNext, we performed a more detailed color distribution analysis by assessing the average color histogram of each class. The results for the training and test NCT-CRC-HE sets are depicted in Fig. 4 and Fig. 5, respectively. Here, we can see an even better separation of different tissue types: all tissue classes except for debris (DEB), smooth muscle (MUS) and cancer-associated stroma (STR) have a unique overall histogram profile when combining R, G and B color distributions. This suggests that we can possibly build an accurate classifier for the NCT-CRC-HE dataset by using only color profiles of each image, and not taking into account any complex histopathological features such as cell type composition, vasculature, immune infiltration, etc. In the experimental section of this paper, we will validate this assumption by building and evaluating a model which predictions are based only on image histogram data.\nWe should again highlight a small mismatch in color distributions between the training and test sets. For the latter, there are also noticeable long tails on the right of the histogram for debris (DEB), lymphocyte (LYM) and cancer-associated stroma (TUM) tissue classes that are caused by \u201coverexposed\u201d image regions obtained after color normalization."}, {"title": "2.2 JPEG Compression Artifacts", "content": "While all provided images are saved in TIFF format, these are not real raw tissue photos: instead, the compressed JPEG images (obtained presumably after color normalization procedure) were re-saved in this format. The logic behind this action is rather questionable as such procedure only increases the size of the dataset by approximately a factor of 10 without any quality gains. However, a more surprising finding is that the JPEG compression quality level varies across different tissue classes and sometimes even within images of the same class. Figure 6 illustrates the observed behavior: e.g., on many images from classes adipose and background we can see extreme JPEG compression artifacts (checkerboard pattern) corresponding to compression quality level presumably lying between 30-60%, while for other classes like debris and normal mucosa this quality level was higher than 70%. Additionally, for almost all tissues we see intra-class compression quality variation suggesting that different pipelines were used for processing and saving images even of the same class.\nThis creates a major issue when training deep learning models on such data: as these compression artifacts can be easily detected with just a few convolutional filters, they might become one of the primary features used by the model when learning the decision rule. The contribution of compression artifacts become more significant for larger models that are capable to detect even very small image"}, {"title": "2.3 Corrupted Images", "content": "Visual observation of training and validation patches revealed that the majority of images from class background are totally corrupted (Fig. 7, top row): a combination of inappropriately processed image dynamic range obtained after color normalization and extreme JPEG compression rate resulted in pixelated images that no longer represent any biological meaning. While even the simplest machine learning model can correctly classify all images of this class, the resulting accuracy has little relation to the overall task of colorectal cancer tissue analysis.\nA similar issue related to incorrect image dynamic range handling can be observed for a fraction of images from class debris (Fig. 7, bottom row). Almost"}, {"title": "2.4 Other Potential Issues", "content": "Besides the above mentioned pronounced problems, one can also notice smaller image quality variations related, e.g., to over-sharpening, blur or upsampling that are specific to patches of different tissue types. It was demonstrated in [11] that deep learning models can uniquely identify the origin of the photo based on such image quality aspects, which potentially allows the network to detect tissue classes without learning tissue morphology. While this should not be generally the case here as there exists more straightforward features allowing to distinguish between different image classes for this dataset, these low-level quality aspects can still introduce some contribution to the final decision rule and accuracy, especially when training large models that tend to learn more complex features."}, {"title": "3 Proposed Method", "content": "The dataset analysis performed in the previous section led to two important outcomes. First, we identified that the complexity of this specific task itself is relatively low since even basic color information should be sufficient to distinguish between the majority of tissue classes. Secondly, various artifacts and unique low-level image properties specific for different tissue classes might significantly affect model predictions and accuracy, especially since there is a noticeable mismatch in their strength between NCT-CRC-HE training and validation sets."}, {"title": "4 Experimental Results", "content": "This section provides numerical results obtained with different baseline solutions and the proposed approach based on the EfficientNet-B0 model. We used the conventional NCT-CRC train / validation splits in all experiments, where NCT-CRC-HE-100K data is used for training and CRC-VAL-HE-7K for validation."}, {"title": "4.1 Baseline Solution 1: Using R, G and B Color Intensities", "content": "In Section 2 and Fig. 2, we observed that one might be able to partially separate different tissue classes using only mean red, green and blue color intensities. To validate this assumption, we used these three intensity features generated for all NCT-CRC-HE images and trained a Random Forest classifier model on the obtained data. The results of this experiment are provided in Table 1. While one might expect all tissue classes to be indistinguishable from each other by their mean brightness and intensity values, the considered approach achieved an accuracy of 53.8%. This means that by using only these three intensity features it is possible to correctly classify more than half of the validation images. This confirms our initial assumption that the majority of the NCT-CRC-HE tissue classes have a unique color signature."}, {"title": "4.2 Baseline Solution 2: Using Color Histograms", "content": "Even higher results can be obtained when using more detailed color information extracted from the images. In this experiment, we computed a simple color histogram for each image and for each color channel. The entire 0-255 color intensity range was divided into 16 intervals, which resulted in 48 features generated per image patch. These features were then used by the Random Forest classifier with 200 trees. The results in Table 1 demonstrate that this model was able to achieve an overall accuracy of 82.2% on the entire dataset, and over 89% of accuracy for five out of nine tissue classes. It should be noted that this model was not using any histopathological features related to cell types and shapes, tissue morphology or immune system activity: only image color distributions largely affected by staining intensities. Despite its high accuracy, this solution has little practical application since its predictions are entirely dependent on the color distribution of the NCT-CRC-HE dataset."}, {"title": "4.3 Baseline Solution 3: Using ImageNet Features", "content": "One can further improve the results on this dataset without using any specific histopathological information by using ImageNet features. In this experiment, such features were obtained using a pretrained EfficientNet-B0 ImageNet model that generated a feature representation of dimension 1280 for each NCT-CRC-HE image. An SVM classifier was trained on top of these features to learn the decision rule. Table 1 presents the results of this solution: the model achieved an accuracy of 92.2%, for five out of nine tissue classes the accuracy exceeded 96%. When observing the results of CNN models previously tuned on this dataset"}, {"title": "4.4 Efficient Net-B0 Based Solution", "content": "Next, we performed evaluation of the proposed EfficientNet-B0 based model. We tested two versions of this solution: a single tuned EfficientNet-B0 network and an ensemble of two EfficientNet-B0 models obtained by simple averaging of their predictions. The results of both approaches are shown in Tables 1 and 2: the proposed solutions achieved an overall accuracy of 97.7% and 98.3% for a single model and an ensemble, respectively. With only 4M/8M parameters, they outperformed all previously proposed deep learning models, including foundation transformer-based solutions (CONCH, iBOT, DINO, CTransPath) and a large DeepCMorph model with 87M parameters that was pre-trained to learn cell morphology and tuned on the TCGA dataset with 32 different cancer types. Such results confirm our expectations: due to a low complexity of the task, huge color bias and numerous image artifacts that are not always consistent between the training and validation sets, using large models does not bring any benefits for this dataset. Instead, this might lead to numerous overfitting issues: big models tend to learn complex decision rules, additionally taking into account low-level image quality properties that should not be in general considered in this task. To demonstrate the impact of such low-level image quality aspects on the final model prediction, we performed an extra experiment described below."}, {"title": "4.5 Estimating the Effect of JPEG Compression Artifacts and Color Bias on Model Predictions", "content": "To analyze how the mentioned compression artifacts and color bias influence the decision rules and model accuracy, we performed an experiment where JPEG artifacts and color alterations were introduced to the images from the validation set and the change in the resulting model classification accuracy was assessed. We used three models: the recently presented DeepCMorph model as its source codes and pre-trained weights for this dataset are publicly available 3, the proposed single EfficientNet-B0 model and the ensemble of two Efficient-Nets. Four different compression quality levels (80%, 60%, 40% and 20%) and"}, {"title": "5 Conclusion", "content": "In this paper, we deviated from the standard pathway followed by all previous works designing solutions for the NCT-CRC-HE colorectal cancer dataset. As our initial experiments revealed abnormalities in the results and learning curves obtained on this dataset, we started with a detailed exploration of the images it is composed of. The performed dataset analysis revealed a number of critical issues significantly limiting its applicability for designing biomedical tools for histopathological image analysis. The first prominent problem is a strong color signature present for the majority of tissue classes. We demonstrate that by using only three features - mean red, green and blue color intensities - one can achieve over 50% of classification accuracy on this dataset. By using a simple color histogram not explicitly capturing histopathological features, it is possible to correctly classify 8 out of 10 test images. In addition to color-related issues, severe JPEG compression artifacts can be found in images belonging to several tissue classes that might contribute to the final decision rules learned by deep learning models. Another problem is related to incorrect dynamic range processing of images obtained after stain normalization, which resulted in a large number of corrupted image patches that, though are easily identifiable even with the simplest machine learning models, no longer have any biological meaning. Taking into account the above issues, we proposed a shallow EfficientNet-B0 based solution that demonstrated an accuracy of over 97.7% on the CRC-VAL-HE-7K validation set, outperforming all foundation transformer models and cell morphology-aware networks previously proposed for this dataset. Finally, the experiment analyzing the effect of compression artifacts and color bias on deep learning model predictions confirmed that large networks trained on this dataset tend to use low-level image quality aspects for deriving the classification decisions, suggesting that the results obtained on this dataset should be interpreted with caution."}]}