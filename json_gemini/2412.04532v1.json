{"title": "WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting Time Series Deep Learning Models", "authors": ["Md. Khairul Islam", "Judy Fox"], "abstract": "Interpreting complex time series forecasting models is challenging due to the temporal dependencies between time steps and the dynamic relevance of input features over time. Existing interpretation methods are limited by focusing mostly on classification tasks, evaluating using custom baseline models instead of the latest time series models, using simple synthetic datasets, and requiring training another model. We introduce a novel interpretation method called Windowed Temporal Saliency Rescaling (WinTSR) addressing these limitations. WinTSR explicitly captures temporal dependencies among the past time steps and efficiently scales the feature importance with this time importance. We benchmark WinTSR against 10 recent interpretation techniques with 5 state-of-the-art deep-learning models of different architectures, including a time series foundation model. We use 3 real-world datasets for both time-series classification and regression. Our comprehensive analysis shows that WinTSR significantly outranks the other local interpretation methods in overall performance. Finally, we provide a novel and open-source framework to interpret the latest time series transformers and foundation models.", "sections": [{"title": "1 Introduction", "content": "Time-series deep learning models have achieved unprecedented performance in recent years. However, the lack of explainability remains one of the key challenges for their widespread use. Explanations provide the necessary transparency to make reliable decisions, especially in sensitive data such as healthcare, finance, energy, traffic, weather, stocks, and many other science domains (Benidis et al. 2022). These explanations can be either Global, the logic and reasoning of the entire model, or Local, the model's specific decision on an instance. Post-hoc interpretation methods are generally applied after the model has already been trained, while In-hoc methods work during the model training time. Model-agnostic methods work on black-box models and do not require specific model architecture to work. Our proposed interpretation method is local, post-hoc, and model-agnostic.\nUnlike image and text data, the interpretability of multivariate time series models has been relatively under-explored and difficult to visualize. By explaining time series models, one can highlight the importance of input features to the prediction of the model (Rojat et al. 2021), find intuitive patterns (Lim et al. 2021), and visualize saliency maps (Leung et al. 2023) without relying on the model architecture. This work focuses on the local interpretation techniques for deeper insights into the importance of temporal features. Interpretation methods commonly estimate how relevant each input feature is to the model's output. However, existing works are limited by: (1) benchmarking using simple baseline models (e.g. LSTM, GRU), not recent SOTA time series models that are used in practice (2) focusing mostly on classification tasks, which makes generalization difficult (3) not efficiently capturing temporal dependency (4) train another model to interpret one model.\nWe propose the Windowed Temporal Saliency Rescaling (WinTSR) method to address these challenges. It is benchmarked using the latest time series models of different architectures (including an LLM-based foundation model), tested on both classification and regression tasks, efficiently considers the temporal importance when interpreting, and does not require training a separate model. Our overall framework is summarized in Figure 1. In short, our contributions are,\n1) A novel local interpretation method named WinTSR that calculates the time importance along the look-back window and rescales the feature importance based on it. This significantly improves the performance by capturing the delayed impact between input and output, while still being relatively fast.\n2) Extensive analysis with 3 real-world datasets for classification and regression tasks, and interpretation evaluation using robust metrics.\n3) Benchmark WinTSR with 5 state-of-the-art time series models (DLinear, MICN, SegRNN, iTransformer) including a foundation model (CALF) to demonstrate that WinTSR is generalizable and consistently outperforms in different model architectures.\n4) A unified open-source framework that includes 20+ recent time series models (including 3 foundation models, Appendix E) with 10+ popular interpretation methods. Also, visualize and compare the multivariate"}, {"title": "2 Related Works", "content": "Time series interpretation methods cover a wide range of tasks and datasets (Rojat et al. 2021; Turb\u00e9 et al. 2023). Table 1 summarizes the comparisons of our work with the related methods. Gradient based methods, such as Integrated Gradients (Sundararajan, Taly, and Yan 2017), and GradientSHAP (Erion et al. 2019) use the gradient of the model predictions to input features to generate importance scores. Perturbation based methods, such as Feature Ablation (Suresh et al. 2017), and Augmented Feature Occlusion (Tonekaboni et al. 2020) replace a feature or a set of features from the input using some baselines or generated masks and measure the importance based on the model output change. These methods are mainly proposed for image or language models and do not consider the temporal dependencies in the time series data.\nRecent works like Dyna Mask (Crabbe and van der Schaar 2021), and Extremal Mask (Enguehard 2023a) focus on learning the masks to better perturb the input features. Model-based saliency methods, such as (Kaji et al. 2019; Lim et al. 2021; Islam et al. 2023), use the model architecture e.g. attention layers, to generate importance scores. ContraLSP (Liu et al. 2024d), proposed a contrastive learning method to learn locally sparsed perturbation. TIMEX (Queen et al. 2024) trained interpretable surrogates to learn stable explanations from latent space. However, TIMEX is limited to classification and assumes access to latent pretrained space. These methods overall have performed very well on synthetic data or real-world classification tasks. However, they require training another model to interpret the target model, which adds additional complexity. These are not benchmarked for regression tasks and often include algorithm design exclusively for classification (Queen et al. 2024). Also, heavily uses simple RNN or LSTM baselines which are not state-of-the-art time series models.\nTSR (Ismail et al. 2020) improved interpretation by considering the temporal dimension separately. However, this comes with a heavy computational cost and was not benchmarked on real-world time series data. Feature Importance in Time, FIT (Tonekaboni et al. 2020) conditioned on the last input time step to calculate the distribution shift using KL-divergence of the predicted probability, but only supports classification tasks. WinIT (Leung et al. 2023) proposed a sliding window-based"}, {"title": "3 Windowed Temporal Saliency Rescaling Problem Statement", "content": "We consider a multivariate multi-horizon time series setting with length T, the number of input features J, target outputs O, and total N instances. $X_{j,t} \\in R^{J \\times T}$ is the input feature j at time $t \\in {0,......, T - 1}$. Past information within a fixed look-back window L is used to forecast for the next Tmax time steps or the target class. The target output at time t is $Y_t$. The black-box model f is defined as,\n$\\hat{y}_t = f(X_t)$\nwhere, $X_t = X_{t-(L-1):t} = [X_{t-(L-1)}, X_{t-(L-2)},\u00b7\u00b7\u00b7, X_t]$\n$ = {x_{j,l,t}}, j \\in {1,...... , J}, l\\in {1,......,L}$\n(1)\nwhere $\\hat{y}_t$ is the predicted class or the forecast at $\\tau\\in {1,..., T_{max}}$ time steps in the future. $X_t$ is the input slice at time t of length L. An input feature at position (j, l) in the full input matrix at time step t is denoted as $x_{j,1,t}$.\nThe interpretation goal is to calculate the importance matrix, $\\phi_t = {\\phi_{j,1,t}}$ for each output o \u2208 O or prediction horizon $\\tau\\in {1,\u2026\u2026\u2026, T_{max} }$. This is a matrix of size O \u00d7 J \u00d7 L for classification and O \u00d7 Tmax \u00d7 J \u00d7 L for regression. We find the relevance of the feature $x_{j,l,t}$ by masking it in the input matrix $X_t$ and measuring the model output change,\n$\\phi_{j,1,t} = distance\\_score(f(X_t), f(X_t \\setminus X_{j,1,t}))$\n(2)\nwhere $X_t \\setminus X_{j,1,t}$ is the input after masking/perturbing feature $x_{j,l,t}$. The $distance\\_score$ can vary based on interpretation methods and prediction tasks. For example, 11-norm for regression and 11-norm or KL-divergence for classification."}, {"title": "Our Approach", "content": "We propose Windowed Temporal Saliency Rescaling (WinTSR) across the input time window to calculate the importance score matrix ot at a time t. Our method differs from previous approaches by accounting for the importance of a feature observation in a multi-horizon setting over multiple windows containing the target time step. The details are in the following Algorithm 1. The method returns an importance matrix & which is later used to evaluate the interpretation performance.\nFor a distance score, we calculate the simple 'L1 distance between the original and perturbed prediction for classification and regression. Unlike TSR, which uses the 'L1 distance between the original and perturbed importance matrix returned from another interpretation method. This significantly improves our run time compared to TSR and removes the dependency on a second interpretation method. WinIT perturbs all values in a sliding window with and without the target feature, then finds the feature importance by subtracting them. Since we perturb only the individual feature, this reduces the computation overhead of perturbing a range of features and removes the dependency of choosing a best-fit sliding window.\nThe time relevance score enables us to skip less important time steps to speed up the computation similar to (Ismail et al. 2020). TSR uses the first feature value for masking, while WinIT uses feature augmentation (generated from past features). We generated random values from a normal distribution for masking and the input features are already normalized during the data pre-processing period."}, {"title": "4 Experimental Setup", "content": "We compare WinTSR to ten recent local and post-hoc interpretation methods. We evaluate them with five state-of-the-art deep learning time series models across three datasets for classification and regression tasks."}, {"title": "Datasets", "content": "We use the datasets shown in Table 2. Electricity and Traffic datasets contain the electricity consumption rate and traffic occupancy over the past hours respectively. The task is to forecast those values for the next 24 hours based on past observations and time-encoded features from the last 96 hours. The MIMIC-III dataset contains patient info and lab reports from a hospital. The goal is to predict whether the patient will die during their hospital stay, based on the"}, {"title": "Models", "content": "We use five neural network architecture groups (Linear, CNN, RNN, Transformer, and LLM) for our experiment. Multiple models are chosen to generalize the proposed method across different network architectures. We show how the same interpretation method impacts different models. A complete list of available models in our framework is given in Appendix E. We selected these five models based on their state-of-the-art performance in their respective architecture. These models are: (1) DLinear (Zeng et al. 2023) - Linear, (2) SegRNN (Lin et al. 2023) - Recurrent Neural Network (RNN), (3) MICN (Wang et al. 2023) Convolutional Neural Network (CNN), and (4) iTransformer (Liu et al. 2024b) - Transformer, and (5) CALF (Liu et al. 2024a) - A recent pretrained LLM model for generalized time series forecasting using cross-modal fine-tuning."}, {"title": "Interpretation Methods", "content": "We use the following post-hoc interpretation analysis methods for comparison in this work: (1) Feature Ablation (FA, Suresh et al. (2017)) (2) Augmented Feature Occlusion (AFO, Tonekaboni et al. (2020)) (3) Feature"}, {"title": "Evaluating Interpretation", "content": "We follow (Ozyegen, Ilic, and Cevik 2022; Turb\u00e9 et al. 2023) to evaluate interpretation when no interpretation ground truth is present. Figure 2 (b) briefly illustrates the evaluation framework. The steps are:\n1) Sort relevance scores $\\phi(X)$ so that $R_{\\ell}(X_{i,t})$ is the $\\ell^{th}$ element in the ordered rank set ${R_{\\ell}(x_{i,t})}_{L \\times N}$. Here Lis the look-back window and N is the number of features.\n2) Find top k% (we used, k \u2208 {5, 7.5, 10, 15}) features in this set, where $R(x_{i,t}) \\in {R_{\\ell}(x_{i,t})}_{\\ell=1}^L$. Mask these top features or every other feature in the input.\n3) Calculate the change in the model's output to the original output using different metrics. We use the AUC drop for classification (Leung et al. 2023) and Mean Absolute Error (MAE) for regression.\nDeYoung et al. (2019) proposed to measure the comprehensiveness and sufficiency to ensure the faithfulness of the explained rationales. Which are similar to the precision and recall from Ismail et al. (2020). (1) Comprehensiveness: Were all features needed to make a prediction selected? Once important features are masked, the model should be less confident in its prediction. (2) Sufficiency: Are the top feature sufficient to make the prediction? This is achieved by masking all features except the top k%. In summary, the higher the comprehensiveness loss and the lower the sufficiency loss the better. We define the set of top k% relevant features selected by the interpretation method for the i-th input $X_i$ as $X_{i,1:k}$, the input after removing those features as $X_i \\setminus_{1:k}$. Then these two terms are calculated as:\n$Comp. = evaluation\\_metric(f(X_i), f(X_{i, \\setminus 1:k}))$\n$Suff. = evaluation\\_metric(f(X_i), f(X_{i,1:k}))$\n(3)\nFor K bins of top k% features (we use top 5%, 7.5%, 10%, and 15% features, hence K = 4.), the aggregated comprehensiveness score (DeYoung et al. 2019) for the classification task is called the \"Area Over the Perturbation Curve\" (AOPC, Equation 4). For AUC drop, this will calculate the drop for each output class o after masking top k% features for each k \u2208 K, then calculate the average drop. Similarly for regression, (Ozyegen, Ilic, and Cevik 2022) defined the \"Area Over the Perturbation Curve for Regression\" (AOPCR, Equation 5). For MAE, it calculates the change in prediction for each output o and prediction horizon \u03c4 by masking top k% features for each k \u2208 K then takes the average. AOPC and AOPCR for sufficiency are calculated similarly after replacing $X_i \\setminus 1:k$ with $X_{i,1:k}$.\n$AOPC = \\frac{\\sum\\limits_{o,k} metric(f(X_i), f(X_{i, \\setminus 1:k})_o)}{K \\times O}$\n(4)\n$AOPCR = \\frac{\\sum\\limits_{o,\\tau,k} metric(f(X_i)_{o,\\tau}, f(X_{i, \\setminus 1:k})_{o,\\tau})}{K \\times O \\times T_{max}}$\n(5)"}, {"title": "5 Results", "content": "This section shows the interpretation results and visualizations. Then discuss our time complexity and the effect of changing the lookback window."}, {"title": "Benchmark Evaluation", "content": "Table 4 shows the overall results. Our method performs the best or second best in most cases. This is consistent across different datasets and models. We ranked the methods for each dataset and model in terms of overall comprehensiveness and Sufficiency. Then we averaged the ranks in the rightmost columns and used for the final rank. WinTSR achieves the best average rank in each dataset, 1(1.4 \u00b1 0.5), 1(1.4 \u00b1 0.05), and 1(2.4 \u00b1 1.5) in the Electricity, Traffic, and MIMIC-III respectively.\nIntegrated Gradient achieves the best results in a few cases for comprehensiveness in regression but fails in others. TSR performs significantly better for comprehensiveness in the MIMIC-III dataset, but its high sufficiency in the same dataset shows the top features it selects are not sufficient. Feature Ablation method also consistently performed well and achieved 2nd rank overall. We also see the mask learner methods, in practice do not interpret the SOTA models well."}, {"title": "Visualizing Interpretation", "content": "Visualizing the interpretations helps to understand their meaning. However, unlike images and texts, time series interpretations are harder to visualize and to verify intuitively. Here we 1) visualize persistent temporal patterns (trends present across the dataset) and 2) highlight the top features across time. Figure 3 shows the raw input feature (left) and the interpretation of these features (using normalized relevance/importance score). The MIMIC-III dataset is visualized with a heatmap due to many features (31), the other two datasets are shown with line plots. The interpretation results of the four selected methods are presented for comparison using the best-performing iTransformer model of 1st iteration.\nThe relevance scores shown here are for forecasting the target for the next hour (r = 1) or predicting mortality class (o = 1) for simplicity. Electricity and traffic features show a daily pattern, where the importance is highest at the most recent time step (t = 96) and the same time the previous day (t = 72). Sometimes at the last peak. This means, to predict the electricity consumption or traffic occupancy, past observations from recent times or the same daytime or last peak hour are important. For MIMIC-III the goal is to interpret which factors at which time were important in predicting the patient's death. Figure 3 (c) shows the top three points interpreted by the methods, where WinIT and TSR display the features important in the last 12 hours, whereas WinTSR and FA identify these features much earlier, within the first 12 hours, and then again around the last 12 hours. Temporal change of the important features is visible in WinTSR, WinIT, and TSR as they all consider temporal dependency."}, {"title": "Time Complexity", "content": "We summarize the run time information in Table 5 for some selected methods, Appendix 11 includes the complete results. Our WinTSR method's time complexity is O(L + L \u00d7 J), where L is the lookback window and J is the number of features. The perturbation-based methods (FA,"}, {"title": "Varying Lookback Window", "content": "Since the lookback window size is an integral part of capturing temporal dependency, it is important to analyze the effect of changing the window size. By design, the WinIT method supports variable window length, where TSR and WinTSR compute over the whole training window size. We retrained the best-performing iTransformer model for"}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we present a novel local interpretation method \"Windowed Temporal Saliency Rescaling\" that explicitly accounts for the dynamic temporal nature of input data and explains their features' importance. Through extensive experiments and metric comparisons, our analysis 1) shows WinTSR provides a more accurate interpretation of temporal dependencies among features; 2) benchmarks different neural network models: DLinear (Linear), SegRNN (RNN), MICN (CNN), iTransformer (Transformer), and CALF (LLM). 3) compares with ten widely used interpretation methods; 4) presents an easy-to-use framework by combining a popular time series library with interpretation libraries. This framework enables the quantitative measurement of time series interpretation across many recent models and methods.\nFor future work, we will identify higher-level patterns and trends in time series models by explicitly incorporating both spatial and temporal domains, to enhance the effectiveness and efficiency of AI interpretability in time series datasets. We will explore using the pre-trained foundation models to explain features in the time series domain."}, {"title": "A Dataset and Features", "content": "The UCI Electricity dataset (Trindade 2015) contains the consumption of 321 customers from 2012 to 2014. We aggregated it on an hourly level. This data has been used as a benchmark in many time series forecasting models (Wu et al. 2023; Zeng et al. 2023; Ozyegen, Ilic, and Cevik 2022). Following (Wu et al. 2021) we use the past 96 hours to forecast over the next 24 hours. And we added four time-encoded features: month, day, hour, and day of the week."}, {"title": "Traffic", "content": "The UCI PEM-SF Traffic Dataset describes the occupancy rate (with yt \u2208 [0,1]) of 440 SF Bay Area freeways from 2015 to 2016. It is also aggregated on an hourly level. Following (Wu et al. 2021) we used a look-back window of 96 hours, a forecast horizon of 24 hours, and the 821st user as the target variable. We added four time-encoded features: month, day, hour, and day of the week."}, {"title": "MIMIC-III Mortality", "content": "A multivariate real-world clinical time series dataset with a range of vital and lab measurements taken over time for over 40,000 patients (Johnson et al. 2016). It is widely used in healthcare and medical AI-related research, and also in time series interpretation (Tonekaboni et al. 2020; Leung et al. 2023). We follow the pre-processing procedure by (Leung et al. 2023) to drop patients with missing information and then aggregate. Among the 22988 patient data left in the dataset, 2290 died during their hospital stay. We use the measurements hourly over 48 hours to predict patient mortality (whether the patient died)."}, {"title": "B Parameters and Notations", "content": "The model and training parameters are chosen following (Wu et al. 2023) for a consistent comparison with the state-of-the-art. Table 8 and 9 list the training parameters and the model hyperparameters used during our experiments. Table 10 summarizes the notations mainly defined during the problem statement and interpretation evaluation framework."}, {"title": "C Interpretation methods", "content": "The following describes the interpretation methods we have compared within this paper.\n1. Feature Ablation (FA): The difference in output after replacing each feature with a baseline. (Suresh et al. 2017).\n2. Augmented Feature Occlusion (AFO): Tonekaboni et al. (2020) ablated the input features by sampling counterfactuals from the bootstrapped distribution.\n3. Feature Permutation (FP): Permutes the input feature values within a batch and computes the difference between original and shuffled outputs (Molnar 2020).\n4. Integrated Gradients (IG): Sundararajan, Taly, and Yan (2017) assigned an importance score to each input feature by approximating the integral of gradients of the model's output to the inputs.\n5. Gradient Shap (GS): Lundberg and Lee (2017) approximated SHAP values by computing the expectations of gradients by randomly sampling from the distribution of baselines/references."}, {"title": "Reproducibility Statement", "content": "Our source code and documentation are already publicly available on GitHub. The public link will be shared upon acceptance. The documentation includes detailed settings and instructions for reproducing the experiments. The Electricity and Traffic datasets are publicly available. The private MIIMIC-III dataset can be accessed by following the steps at https://mimic.mit.edu/docs/gettingstarted/. In addition, we follow the procedures outlined in previous studies to preprocess the datasets and also include the code in our project. Our GitHub repository has singularity and docker definitions to ensure a reproducible container environment. The experiments are run on a single-node Linux server with 16 GB RAM and NVIDIA RTX GPU. We use a Python 3.12 environment with Pytorch 2.3.1 and Cuda 11.8. Version details of other libraries are given in our repository. The random processes are seeded to ensure reproducibility."}]}