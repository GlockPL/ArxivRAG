{"title": "Enhancing Worldwide Image Geolocation by Ensembling Satellite-Based Ground-Level Attribute Predictors", "authors": ["Michael J. Bianco", "David Eigen", "Michael Gormish"], "abstract": "Geolocating an image of a ground-level scene entails estimating the location on Earth where the picture was taken, in absence of GPS or other location metadata. Typically, methods are evaluated by measuring the Great Circle Distance (GCD) between a predicted location and the ground truth. However, this measurement is limited because it only evaluates a single point, not estimates of regions or score heatmaps. This is especially important in applications to rural, wilderness and under-sampled areas, where finding the exact location may not be possible, and when used in aggregate systems that progressively narrow down locations.\nIn this paper, we introduce a novel metric, Recall vs Area (RvA), which measures the accuracy of estimated distributions of locations. RvA treats image geolocation results similarly to document retrieval, measuring recall as a function of area: For a ranked list of (possibly non-contiguous) predicted regions, we measure the accumulated area required for the region to contain the ground truth coordinate. This produces a curve similar to a precision-recall curve, where \u201cprecision\u201d is replaced by square kilometers area, allowing evaluation of performance for different downstream search area budgets.\nFollowing directly from this view of the problem, we then examine a simple ensembling approach to global-scale image geolocation, which incorporates information from multiple sources to help address domain shift, and can readily incorporate multiple models, attribute predictors, and data sources. We study its effectiveness by combining the geolocation models GeoEstimation [12] and the current SOTA GeoCLIP [2], with attribute predictors based on LandScan [17] and ESA-CCI Land Cover [1]. We find significant improvements in image geolocation for areas that are under-represented in the training set, particularly non-urban areas, on both Im2GPS3k and Street View images.", "sections": [{"title": "1. Introduction", "content": "Accurate localization of ground-level imagery is of great significance to a variety of tasks, including navigation, tourism, and security, and the subject of recent research activity [2,4,6,12,21]. However, even large publicly available geotagged image datasets, such as MP-16 [9] and Street View [11], sample only a small fraction of the Earth's surface and mostly concentrate on cities and other population centers. Moreover, much imagery that is available with labeled geographic coordinates is clustered around highly trafficked areas, such as famous landmarks, parks, restaurants and other sites of interest. These include most common benchmark datasets, e.g. Im2GPS3k [19] and YFCC26k [12,18]. Thus domain shift is a major problem as the imagery in many deployment applications do not reflect the training distribution, particularly when applied to rural areas or wilderness, or areas that are under-represented by the datasets' collection methods.\nWe introduce an novel image geolocation metric, Recall vs Area (RvA). This metric helps account for ground-level image similarity from different geographic regions, and naturally characterizes model performance when predicted regions are discontiguous. It is particularly apt for measuring systems that suggest areas for downstream tasks, for example, to be further refined using landmark-matching or other more expensive methods, or to flag images that may have come from certain regions of the globe for further study by human analysts. Further, the metric naturally enables ensembling geolocation models with additonal ground-level attribute models.\nTo help address the challenge of generalization to under-sampled areas, we propose a general ensembling approach to global-scale image geolocation, which incorporates ground-level attribute predictors based on satellite data products. While ground level training data are generally limited, satellite datasets have global coverage. These predictors bring additional information to bear on the geolocation problem, e.g. relating global land cover types to available ground-level imagery. The attributes are assumed proxy for geographically specific image scene concepts and objects. Estimates from these predictors are combined with those from other global geolocation models, e.g. Geo-CLIP [2]. In addition, while most geolocation models require geotagged ground level imagery for training, many attribute predictors (e.g. vegetation, buildings, roads, etc.) can also leverage additional non-geotagged data sources in their training process.\nWe evaluate our method, combining the foundational GeoEstimation [12] and the recent SOTA GeoCLIP [2] approach with ground-level attributes corresponding to population density [17] and land use/land cover [1] maps, to demonstrate the utility of the additional features and validate our approach. In future work we plan on incorporating additional additional data sources and maps, such as OpenStreetMap tags.\nWe evaluate our models using both the standard Im2GPS3k [19], as well as a geo-diverse Street View dataset (see Sec. 4.2) that is different in geographic distribution and scene concepts from MP-16 and related benchmarks, as shown in Fig. 2. The Street View dataset was constructed by gathering at least 426 panormanas from 90 countries [11]. None of the Street View data was used in training our models, so it provides a reasonable measure of domain shift robustness."}, {"title": "1.1. Contributions", "content": "\u2022 We develop a novel metric, Recall vs Area, for evaluating geolocation systems, which can account for visual similarity of ground-level imagery across the Earth.\nWe use this method to assess image geolocation approaches.\n\u2022 We show that, in isolation, geolocation methods trained only on geotagged imagery can be vulnerable to image domain shift with respect to training data.\n\u2022 We develop an approach to combine ground-level attribute predictors, based on satellite data products, with geolocation models. The incorporation of ground-level information related to satellite data promises to improve generalization to areas of the Earth for which there are few ground-level picture samples.\n\u2022 We show ensembling ground-level attribute predictors improves image geolocation performance on both in- and out-of-domain datasets on SOTA image geolocation methods."}, {"title": "2. Related Work", "content": "M\u00fcller-Budack et al. [12] develop a hierarchical pixel-based approach to geolocation. They fine-tune their model on the large MP-16 dataset, and quantize the Earth using S2 cells (using the Google S2 geometry Library\u00b9), with 3 resolutions {'coarse', 'medium','fine'} depending on data density. The output of the model is image geolocations for each level. Subsequent papers have built on this work, proposing various ground-level gridding strategies, including CPlaNet [16] and Translocator [13].\nOther recent works have brought additional informa-tion to the geolocation problem, including relationships between scene and geographic scales and variations in environment [4] and segmentation models [13]. In Luo et"}, {"title": "3. Approach", "content": ""}, {"title": "3.1. Recall vs Area Metric", "content": "The most common metric currently used to measure geolocation performance calculates the distance between a single predicted location and the ground truth coordi-nates. Localization accuracy is calculated for different distance radius buckets, measured using great circle dis-tance (GCD). For example [12] used GCD thresholds of [1, 25, 200, 750, 2500] km to evaluate model performance, even giving these distances names to represent how close the solution was, namely street, city, region, country, continent.\nWhile this measure works well to characterize perfor-mance of a single predicted location, it does not extend to measuring performance of a larger (possibly discontiguous) predicted area. This is important for many practical appli-cations, where the image geolocation algorithm is used to propose a set of possible locations that are used in down-stream tasks. Thus localization performance is measured in terms of an area budget.\nIn addition, many distant regions on Earth look simi-lar from ground-level, as illustrated by the often scattered global distribution of high confidence S2 cell predictions by GeoEstimation. In the context of a system that progressively narrows down the possible locations an image was taken from, for example, it is important to maintain recall at each stage while still reducing the set of predicted regions.\nWe propose a new metric, Recall vs Area, to characterize image geolocation model performance. We rely on the conventions of recall and precision, and formulate a geographic analogy to document retrieval. For each image a set of region proposal areas can be obtained, sorted by confidence score. We then accumulate areas by score, up to a given search budget. Recall is determined according to whether the ground truth lies in this area. We then compute recall for different area budgets (thresholds), resulting in a ROC-like curve comparing recall on one axis and area budgets on the other.\nHere, recall is determined by whether the geographic area returned by the model for a query image contains the image's location. More formally, for a query image $x_i \\in X$ with ground truth location $y_i$, a geolocation model will return a list of location areas (S2 cells, points, etc.) $a_{ik}, k \\in [1..K]$, along with confidence scores. We accumulate area sorted by model confidence, to create a (possibly discontiguous) region, up to an area budget (threshold). For an area threshold a, recall is then\n$recall(a) = \\frac{\\sum_{x_i} 1[y_i \\in \\{a_{ik}: \\sum_{k'=1}^{k} a_{ik'} \\leq \\alpha\\}]}{\\vert X \\vert},$ (1)\ni.e. the fraction of images whose ground-truth location lies within the model's top-scoring predicted areas, up to the area threshold a.\nWe then plot the curve recall(a) for different values of the area budget a, resulting in a description of model perfor-mance, akin to a precision-recall curve (see Fig ??).2 The curve is used to evaluate model performance, or to help se-lect an appropriate trade-off between maintaining recall and size of a proposed area."}, {"title": "3.2. Algorithm", "content": "Given an image x, with corresponding geotag coordinates y, we ensemble geolocation and ground-level attribute predictors to find the a set of areas on Earth most likely to contain the image.\nBecause we combine the geolocation area predictions of multiple models, we map each of their outputs to a common image over the surface of the Earth, with \"pixel\" co-ordinates given by latitude and longitude (WGS-84 projection). Note that we index by (lat, lon), so each \"pixel\" has a different area on the Earth depending on its latitude coordinate. To account for this, we also create an array $A_{pix}$ that"}, {"title": "Algorithm 1 Rasterized GeoEstimation/GeoCLIP", "content": "Given x, f, $M_f, E_p$\np = f(x) # S2 cell probabilities and masks\n# sort probs and masks descending by probability\norder = argsort (p)\np = p[order]\nm = $M_f$[order]\n# combine into rasterized grid\nP = $\\sum_{i=1} \\sum_{j=1}P_{jm_{j}}$\nreturn P"}, {"title": "Algorithm 2 Ensembling ground-level attribute predictors with Rasterized GeoEst., Algo. 1", "content": "Given x, {$f_1 ... f_k$}, {$M_1 ... M_K$}\nreturn P = $\\prod_{i=1}^{K} P_k$, with $P_k = \\sum_j P_{kj}m_{kj}$ as in Alg.1"}, {"title": "Algorithm 3 Model evaluation", "content": "Given dataset D, model, pixel spherical areas map $A_{pix}$\nReturns accumulated areas necessary to the include ground truth location for each image.\nA \u2190 {} # eval. areas\nfor (image $x_i$, g.t.location $y_i$) \u2208 D do\nP\u2190 model(x)\n$p^* = P[y_i]$ # pred probability at ground truth location\narea = $\\sum A_{pix} \\cdot 1[P > p^*]$ # sum area of preds > $p^*$\nA $\\leftarrow$ A$\\,\\cup$ area\nend for\nreturn A"}, {"title": "4. Datasets", "content": ""}, {"title": "4.1. Ground-level attribute datasets and processing", "content": "Here, we describe the satellite data products used to develop our Earth surface attribute models, and the image datasets we used to train and evaluate our ensembling approach and related methods.\nIn our processing, we consider two satellite data prod-ucts: LandScan Global [17] from Oak Ridge National Laboratory (ORNL) and Land Cover [1], from the Euro-pean Space Agency (ESA) Climate Change Initiative (CCI). Each data product provides high resolution estimates of surface attributes: LandScan Global predicts 24-hour average population density and ESA-CCI Land Cover predicts 38 natural and anthropogenic land cover classes.\nWe used three image datasets. MP-16 [18] and Im2GPS3k [19] are common training evaluation bench-marks in image geolocation literature. MP-16 was used to train our attribute classifiers, and Im2GPS3k was used for testing. Additionally, we collected a set of images from Google Street View, using training-set panorama IDs from [11] to evaluate the generalization of the models to image location distributions and image capture pipelines that do not match those of MP-16 and Im2GPS3k."}, {"title": "4.1.1 LandScan Global", "content": "The ORNL LandScan Global dataset is a global map of population density at ~ 1 km\u00b2 resolution [17], see Fig. 1. The data is a large TIFF image, 21600 \u00d7 43200 pixels (lat x lon), with 24-hour average population densities.\nEach pixel indicates whether the corresponding region is land and if so, the population density. The coordinate system of the image is WGS-84, which is a standard lat-long. representation. The resolution is ~ 1km\u00b2."}, {"title": "4.1.2 ESA Landcover", "content": "ESA-CCI Land Cover is a global land cover and land use data product [1], see Fig. 1. It is generated using deep learn-ing classifiers on Sentinel-2 satellite images. The product is a 64800 \u00d7 129600 pixel (lat x lon, WGS-84) GeoTIFF with 300 m resolution. Each pixel is labeled as the most likely land cover from 38 classes (22 super-classes), see Fig. 1. The classes include cropland, grassland, tree cover, and de-veloped cover/use categories. Maps are available for the years 1992-2020. We used the ESA-CCA Land Cover 2015 dataset for development of region reduction classifiers."}, {"title": "4.2. Image datasets", "content": "Currently, the most comprehensive geotagged ground-level image dataset is MP-16 [9]. This dataset is a subset of the Yahoo Flickr Creative Commons 100 Million datasets (YFCC100M) [18], which have geotags. This dataset was used to train ground-level attribute predictors.\nThe size of the dataset is 4.7M images. We were able to obtain ~4.2 M of the total, as approximately 500 k were no longer available at the URLs provided by [12]. The MP-16 image concepts and objects are quite diverse, with indoor and outdoor imagery of natural and human-made scenes and objects.\nMP-16 was the training set from the GeoEstimation pa-per [12], and is still used today, e.g. [2, 13]. Im2GPS3k is an additional benchmark introduced in [19]. We use this dataset to test our models.\nFor evaluation, we use Im2GPS3k [19] and a collection of images from Google Street View [5]. To evaluate model generalization and specifically, the 'domain-gap' robustness of geolocation methods, we do not train on any of the Street View images, using them only for evaluation.\nThe Street View [5] data was obtained using the Google Street View panorama IDs from Luo et al [11]. The geo-diverse Street View dataset was constructed by gathering at least 426 panoramas from 90 countries. From this overall distribution, train, valid and test sets were randomly constructed. We obtained the imagery using a subset of their training panorama IDs, chosen sequentially from the total of 322536 panoramas. Of the 50000 IDs chosen, 49829 IDs were still valid. We thus obtained 49829 images - each a single 90\u00b0 FOV slice from a full panorama, with 15\u00b0 downward pitch (toward ground for imaging road markings) from the Street View API and the corresponding image geotags."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Ground-level attribute predictors", "content": "We trained two ground-level attribute predictors, one for each satellite data product (LandScan and Land Cover), using MP-16 imagery relabeled with attributes from each source. In our experiments, we use the acronyms LS for LandScan and LC for Land Cover. For each image, the ground-level attribute labels were obtained by indexing the nearest pixel value in the attribute maps at the native TIFF resolution. As described in Section 3.2 the outputs of these models are projected on global masks and combined to find a global distribution of probable locations.\nSince the LS population densities are continuous, we mapped these to four density buckets for use in a softmax classifier, according to $log_{10}$ population density using equal bin width over the range of population densities in LS. In Fig. 6 we show the LS population density range as sam-pled by MP-16 and the locations of the buckets. This re-sults in four buckets of ['lowest', \u2018low', \u2018medium', 'high'] density, with corresponding percentage of land area being [93.9, 4.93, 1.10, 0.029]%, respectively. Thus the LS classi-fier predicts these 4 classes. In Fig. 3 the medium popula-tion density bucket is overlain with the S2 cells (fine) used for base(M, f*). The population density mask is more re-strictive than the S2 cells alone.\nFor LC, we use the top 22 top-level land cover classes to relabel the MP-16 images. We then merge these into 7"}, {"title": "5.2. Ensembling", "content": "In our experiments, the common image size was H =\n5400 (latitude) by W = 10800 (longitude). This size was\nchosen as the dimensions are common factors to the the\nLS and LC TIFF image sizes, and was small enough to\nyield reasonable time complexity and geolocalization per-\nformance. Thus average resolution of the common image is\n~ 4 km, which corresponds to zero recall at ~ 16 km\u00b2 in\nFigs. 4 and 5, and Table 1.\nFor GeoEstimation, we used the hierarchical model\n(\"base(M, f*)\" from [12]). For GeoCLIP, we used the"}, {"title": "5.3. Results", "content": "We evaluate the geolocation methods with Algo. 3 for both Im2GPS3k and the Street View dataset, based on pano-ids from [11]. Im2GPS3k and Street View both predomi-nantly consist of urban areas with higher population den-sity. To reduce this bias and better measure the effects on the rural and undersampled areas we are interested in, we rebalance the data by randomly sampling equal number of images from each LandScan mask bucket. The results are summarized in terms of Recall vs Area in Figs. 4 and 5 both with and without balancing, and Table 1 gives results for balanced imagery. We present results for baseline rasterized algorithm for GeoEst. or GeoCLIP predictions Algo. 1; en-sembling per Algo. 2, with GeoEst. or GeoCLIP and LS (+LS); GeoEst.or GeoCLIP and LC (+LC); and GeoEst. or GeoCLIP with LS and LC (+LS+LC). We note that the res-olution of our common image prevents us from locating im-ages within the 1 km GCD threshold (see Sec. 5.2). Black dashed lines in the figures show the areas of spherical caps with radii corresponding to the GCD thresholds typically used in literature, e.g. [2, 12]. The recall of the remaining ensembles are summarized as a function of global spherical cap areas. Since for our proposed approach, we are accumulating all proposed areas in our final probability map, per Eq. 1, we consider recall as a function of total area of these often discontinuous regions.\nFor Im2GPS3k, the image counts for each Land-Scan mask (from lowest to high population density) are [540, 548, 1159,715]. To rebalance, we randomly sample 540 from each bucket to obtain 2160 images. Similarly, for Street View the images in each mask bucket were [11846, 17623, 18668, 1692], with the smallest number of images in the highest pop. density mask. Rebalancing re-sults in 6768 images.\nIn Figs. 4, we see larger gains for Street View, and with the largest gains for all datasets in the larger area thresholds that generally correspond to rural areas. (Note there is also a drop in performance for both base models on the out-of-domain Street View dataset relative to the typical Im2GPS3k evaluation set; we are able to make up some portion of this difference). Figs. 4-5(c) show the absolute improvement in recall over the base models.\nWe also include an 'urban prior' model as an additional baseline, '+LS, urban' which always applies a mask restrict-ing to the two highest-density LS buckets. Since the original imagery is drawn primarily from urban regions, such a con-stant prior could also be effective. We find that the +LS attribute predictors outperform it, as expected, on larger area regions: the urban prior model discards these wholesale, while predicted population density improves performance for these areas. Thus the gains are not due simply to applying a mask for urban regions, and predicting different density attribute values is necessary to obtain improvements.\nFor the rebalanced datasets, there are reductions in re-call overall for the Im2GPS3k set, and improvements for Street View. This is because the original Im2GPS3k dataset is biased toward the highest population density regions, and Street View samples the highest density mask considerably less. The recall on Im2GPS3k is reduced at smaller ar-eas using our ensembling approach for the original and re-balanced datasets, with improvements at larger scales (see Fig.4-5, Table 1), and peak improvements of 0.21 and 2.96 in absolute recall, for GeoEst and the SOTA GeoCLIP base models, respectively.\nIn Table 1, at smaller scales, e.g. spherical cap areas corresponding to 25 and 200 km, the +LS ensemble gave the largest improvements, with +LC and the full ensemble, +LS+LC, giving some benefits at larger scales.\nThe benefits of ensembling the ground-level attribute predictors at a global scale with a baseline 'strong' image geolocation predictor are apparent. The attribute predictors bring additional ground-truth information to the solution, and improve performance relative to baseline."}, {"title": "6. Conclusions and Future Work", "content": "We have introduced a new image geolocation metric, Recall vs Area, and an ensembling approach to global-scale image geolocation, which incorporates ground-level attribute predictors based on satellite data products. Our metric helps account for ground-level image similarity from different geographic regions, able to measure discontiguous predicted areas. Practical image geolocation systems are often concerned with search area, and Recall vs Area accounts for these needs naturally. By ensembling our ground-level attribute predictors, we have improved upon the SOTA method in image geolocation, GeoCLIP [2], in terms of RvA.\nOur ensembling approach combines ground-level at-tribute predictors with geolocation models, to obtain im-provements in image geolocation recall as a function of search area. The attribute predictors bring additional in-formation to bear on the solution using satellite-based at-tribute masks, and improve performance relative to strong baselines, particularly in areas that are under-sampled in the original dataset.\nInterestingly, the relative gain from ensembling pre-dicted attribute masks was larger for GeoCLIP than GeoEst, even though it started at a higher performance level. The information in the embeddings could be complementary to our ground-level attributes. We will explore this in future work.\nSatellite data has global coverage, and is not as con-strained by geotagged information. By associating satellite-recognizable attributes to ground level images, we can ex-tend information about location to areas of the world not in-cluded in any geotagged image data, since they are included in the satellite coverage. We validate this approach using population density and land use, but we hope to extend it to more general attributes, including those found implicitly through large datasets of image captions. More generally, automatically associating ground-level image features with mask areas recognizable by satellite promises to extend the geographic reach of these image features to undersampled regions."}]}