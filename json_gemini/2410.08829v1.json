{"title": "UNVEILING MOLECULAR SECRETS: AN LLM-\nAUGMENTED LINEAR MODEL FOR EXPLAINABLE AND\nCALIBRATABLE MOLECULAR PROPERTY PREDICTION", "authors": ["Zhuoran Li", "Xu Sun", "Wanyu Lin", "Jiannong Cao"], "abstract": "Explainable molecular property prediction is essential for various scientific fields,\nsuch as drug discovery and material science. Despite delivering intrinsic explain-\nability, linear models struggle with capturing complex, non-linear patterns. Large\nlanguage models (LLMs), on the other hand, yield accurate predictions through\npowerful inference capabilities yet fail to provide chemically meaningful explana-\ntions for their predictions. This work proposes a novel framework, called MoleX,\nwhich leverages LLM knowledge to build a simple yet powerful linear model for\naccurate molecular property prediction with faithful explanations. The core of\nMoleX is to model complicated molecular structure-property relationships using\na simple linear model, augmented by LLM knowledge and a crafted calibration\nstrategy. Specifically, to extract the maximum amount of task-relevant knowledge\nfrom LLM embeddings, we employ information bottleneck-inspired fine-tuning\nand sparsity-inducing dimensionality reduction. These informative embeddings\nare then used to fit a linear model for explainable inference. Moreover, we intro-\nduce residual calibration to address prediction errors stemming from linear mod-\nels' insufficient expressiveness of complex LLM embeddings, thus recovering the\nLLM's predictive power and boosting overall accuracy. Theoretically, we provide\na mathematical foundation to justify MoleX's explainability. Extensive experi-\nments demonstrate that MoleX outperforms existing methods in molecular prop-\nerty prediction, establishing a new milestone in predictive performance, explain-\nability, and efficiency. In particular, MoleX enables CPU inference and accelerates\nlarge-scale dataset processing, achieving comparable performance 300\u00d7 faster\nwith 100,000 fewer parameters than LLMs. Additionally, the calibration improves\nmodel performance by up to 12.7% without compromising explainability. The\nsource code is available at https://github.com/MoleX2024/Molex.", "sections": [{"title": "1 INTRODUCTION", "content": "Molecular property prediction, aiming to analyze the relationship between molecular structures\nand properties, is crucial in various scientific domains, such as computational chemistry and bi-\nology (Xia et al., 2024; Yang et al., 2019). Deep learning advancements have significantly improved\nthis field, showcasing the success of AI-driven problem-solving in science. Representative deep\nmodels for predicting molecular properties include graph neural networks (GNNs) (Lin et al., 2022;\nWu et al., 2023b) and LLMs (Chithrananda et al., 2020; Ahmad et al., 2022). In particular, re-\ncently developed LLMs have exhibited remarkable performance by learning chemical semantics\nfrom text-based molecular representations, e.g., Simplified Molecular Input Line Entry Systems\n(SMILES) (Weininger, 1988). By capturing the chemical semantics and long-range dependencies\nin text-based molecules, LLMs show promising capabilities in providing accurate molecular prop-\nerty predictions (Ahmad et al., 2022). Nevertheless, the black-box nature of LLMs hinders the\nunderstanding of their decision-making mechanisms. Inevitably, this opacity prevents people from\nderiving reliable predictions and insights from these models (Wu et al., 2023a)."}, {"title": "2 RELATED WORK", "content": "Explainable Molecular Property Prediction. Given that molecules can be naturally represented\nas graphs, a collection of explainable GNNs have been proposed to explain the relationship between\nmolecular structures and properties (Lin et al., 2021; Pope et al., 2019). However, these atom or\nbond-level explanations are not chemically meaningful to interpret their sophisticated relationships.\nBesides, through learning chemical semantics, the transformer-based LLMs can effectively capture\ninteractions among substructures (Wang et al., 2024) and thus demonstrated their potential in under-\nstanding text-based molecules (Ross et al., 2022; Chithrananda et al., 2020). However, the opaque\ndecision-making process of LLMs obscures their operating principles, risking unfaithful predictions\nwith severe consequences, especially in high-stakes domains like drug discovery (Chen et al., 2024).\nExplainability Methods for LLMs. To obtain trustworthy output, various techniques were in-\ntroduced to unveil the LLM's explainability. The gradient-based explanations analyze the feature\nimportance by computing output partial derivatives with respect to input (Sundararajan et al., 2017).\nThese methods, nevertheless, lack robustness in their explanations due to sensitivity to data per-\nturbations (Kindermans et al., 2019; Adebayo et al., 2018). The attention-based explanations use\nattention weights to interpret outputs (Hoover et al., 2020). Yet, recent studies challenge their re-\nliability as attention weights may not consistently reflect true feature importance (Jain & Wallace,\n2019; Serrano & Smith, 2019). The perturbation-based explanations elucidate model behaviors by\nobserving output changes in response to input alterations (Ribeiro et al., 2016). However, these ex-\nplanations are unstable due to the randomness of the perturbations (Agarwal et al., 2021). To resolve\nthese issues, we extract informative embeddings from the LLM to fit a linear model for inference.\nThis approach leverages both the LLM's knowledge and the linear model's explainability, offering\nreliable substructure-level explanations."}, {"title": "3 PRELIMINARIES", "content": "Let $g^{(i)} \\in G = \\{(g^{(i)},y^{(i)})\\}$ be a molecular graph, our goal is to train a model $f$ to map the\nmolecular representation $g$ to its property $y$, denoted as $f: g \\rightarrow y$. We first convert the molecular\ngraph $g^{(i)}$ into a text-based molecular representation (i.e., Group SELFIES), denoted as $x^{(i)} = \\{x^{(i)}_1,...,x^{(i)}_n\\}$, where $x^{(i)}_j$ is the j-th functional group and $S_D$ is the dataset we used. The LLM-\naugmented linear model consists of two modules: an explainable model and a residual calibrator.\nAfter the explainable model $h$ predicts, its residuals are fed into the residual calibrator $r$, which\nboosts the performance without incurring any explainability impairment. We denote $f_h$ and $f_R$ as\nfeatures of explainable model and residual calibrator, $L(\\hat{y}, y)$ as the training loss. To learn $h$ and $r$,\nwe freeze parameters of the former and sequentially refit its residuals with the objective:"}, {"title": "4 OUR FRAMEWORK: MoleX", "content": "MoleX does two things, i.e., (1) maximizing and preserving the task-relevant information in LLM\nembeddings via fine-tuning and dimensionality reduction and (2) extracting these embeddings to\nbuild an LLM-augmented linear model with residual calibration. We thus divide it into two stages:\nLLM knowledge extraction and LLM-augmented linear model fitting. This section details our\nframework and provides theoretical foundations for its explainability."}, {"title": "4.1 LLM KNOWLEDGE EXTRACTION WITH IMPROVED INFORMATIVENESS", "content": "Fine-tuning. We fine-tune a pre-trained chemical LLM using Group SELFIES data to enhance its\nunderstanding of functional group-based molecules. To maximize the linear model's effectiveness,\nwe aim to extract embeddings that are as informative as possible, fully exploiting the LLM's interior\nknowledge. However, empirically fine-tuning the LLM to produce embeddings with the desired\ninformativeness poses a significant challenge. To address this, we apply the Variational Information\nBottleneck (VIB) (Alemi et al., 2022) into the fine-tuning process by crafting a training loss to\nimprove embedding informativeness. By optimizing the VIB-based loss function, we encourage the\nLLM to produce embeddings with the maximum amount of downstream task-relevant information,\nthus significantly enhancing their informativeness. Particularly, given Group SELFIES inputs x,\nproperties y, and LLM embeddings t, we define $p_0(t)$ as the prior distribution over t (e.g., standard\nnormal distribution), and $q_e(y|t)$ as the variational approximation to the conditional distribution of\nthe properties given the embeddings t. The mutual information between t and y is defined as:\n$I(t; y) = E_{p(t,y)} \\Big[log \\frac{p(t,y)}{p(t)p(y)}\\Big] = E_{p(t,y)} \\Big[log \\frac{p(y|t)}{p(y)}\\Big]$,\nand the mutual information between t and x is defined as:\n$I(t; x) = E_{p(t,x)} \\Big[log \\frac{p(t,x)}{p(t) p(x)}\\Big] = E_{p(t,x)} \\Big[log \\frac{p(t|x)}{p(t)}\\Big] = D_{KL} (p_e(t|x) ||p_0(t)) \\Big]$.\nSince the marginal distribution $p(t)$ is intractable, we approximate it with the prior $p_0(t)$. Under\nthis approximation, we use $D_{KL} (p_e(t|x) || p_0(t))$ as a tractable surrogate for $I(t; x)$. Inspired by\nKingma et al. (2015), we approximate the encoder $p_e(t|x)$ by a Gaussian distribution. Let $f_\\mu(x)$\nand $f_\\sigma(x)$ be neural networks that output the mean and covariance matrix of the latent variable t.\nThen, the encoder is given as:\n$p_e(t|x) = N (t | f_\\mu(x), f_\\sigma(x)) $.\nApplying the reparameterization trick, we sample t as:\n$t = f_\\mu(x) + f_\\sigma(x)^{1/2} \\cdot \\epsilon$, where $\\epsilon \\sim N(0, I)$.\nPutting all these together, we design our training loss as:\n$L(\\theta) = \\sum_{(X_i,Y_i) \\in S_F} \\Big(E_{p_e(t|x)} [-log q_e(y_i|t)] + \\beta \\cdot D_{KL} (P_0(t|x_i) || P_0(t))\\Big)$,                               (4.1)"}, {"title": "4.2 DIMENSIONALITY-REDUCED EMBEDDINGS FOR LINEAR MODEL FITTING", "content": "Dimensionality Reduction. As the aggregated n-gram embeddings are still high-dimensional and\nnoisy, eliminating the redundancy in these informative embeddings becomes our new problem.\nDrawing inspiration from Lin et al. (2016), we design an explainable functional principal component\nanalysis (EFPCA) that leads to effective dimensionality reduction. Accordingly, we can preserve a\ncompact yet statistically significant set of features for the linear model. In this part, we demon-\nstrate how to convert this dimensionality reduction into an optimization problem by introducing a\nsparsity-inducing penalty into the optimization objective function. We define our method as\nDefinition 4.1 (EFPCA). Let X(t) be a stochastic process defined on a compact interval [a, b] with\nmean function $\\mu(t) = E[X(t)]$. Assume that X(t) has a covariance operator $\\hat{C}$ derived from the\ncentered data X(t) \u2013 $\\mu(t)$. The EFPCA seeks to find functions $\\xi_k(t)$ that maximize the variance\nexplained by the projections of X (t) while promoting sparsity for explainability. Then, the EFPCA\nsolves the following optimization objective:\n$\\max_{\\xi_k} \\Big{\\langle \\xi_k, \\hat{C} \\xi_k\\rangle - \\rho_k \\cdot S(\\xi_k) \\Big\\}$\nsubject to $||\\xi_k ||^2 = ||f_k||^2 + \\gamma||D^2\\xi_k ||^2 = 1$ and $\\langle \\xi_k, \\xi_j \\rangle = 0$ for all $j < k$,\nwhere $\\gamma > 0$ is a tuning parameter that balances the fit and smoothness, $S(\\xi_k)$ measures the length\nof the support of $\\xi_k(t)$, promoting sparsity and explainability, and $\\rho_k > 0$ is a tuning parameter\ncontrolling the sparsity of $\\xi_k(t)$.\nThe $l_0$ penalty term $\\rho_k||a_k||_0$ promotes sparsity by encouraging many coefficients $a_{kj}$ to be ex-\nactly zero when $\\rho_k$ is sufficiently large. This forces principal components $\\xi_k(t)$ to be exactly zero\nover extensive portions of the domain [a, b]. Since $\\xi_k(t)$ is a linear combination of basis functions,\nzero coefficients directly cause zero contributions from those basis functions across their supports.\nSpecifically, the optimization balances maximizing the variance captured by $\\xi_k(t)$ while minimizing\nthe number of nonzero coefficients, thereby preserving only the most significant components. Fur-\nthermore, since the basis functions $\\phi_j(t)$ have local support on subintervals $S_j \\subset [a, b]$, the nonzero\ncoefficients correspond to basis functions that are active only over those specific intervals. There-\nfore, the sparsity in the coefficients $a_k$ and the local support of the basis functions $\\phi_j(t)$ result in\nprincipal components $\\xi_k(t)$ being nonzero only over certain intervals. Mathematically, the support\nof $\\xi_k (t)$ is the union of the supports $S_j$ corresponding to the nonzero coefficients $a_{kj}$. The EFPCA\nthus produces principal components that are sparse and explainable due to their localized structure,\nas they highlight the regions where the data exhibits significant variation.\nIn summary, EFPCA provides a framework for obtaining highly explainable principal compo-\nnents, enabling effective dimensionality reduction. We demonstrate that by combining the sparsity-\ninducing penalty with the local support of the basis functions, the resulting principal components are\nboth sparse and localized, successfully capturing significant features of the data. In our implemen-\ntation, we exclude irrelevant functional groups and detect principal ones from the high-dimensional\nembeddings. Based on this, we claim the following theorem:\nTheorem 4.2. The EFPCA produces sparse functional principal components $\\xi_k(t)$ that are exactly\nzero in intervals where the sample curves exhibit minimal variation. Consequently, the FPCs $\\xi_k(t)$\nare statistically significant and explanatory, facilitating effective dimensionality reduction."}, {"title": "Linear Model Fitting.", "content": "Applying dimensionality-reduced n-gram embeddings as features, we train\na logistic regression model for our classification tasks, which takes the form:\n$h(f(x)) = \\sigma \\Big(w^T f_h(x) + b\\Big) = \\frac{1}{1+e^{-(w^Tf_h(x)+b)}}$,                               (4.2)\nwhere $\\sigma$ is the sigmoid function, $w \\in R^n$ is the weight vector, $b \\in R$ is the bias term, and $f_h(x)$\nare explainable features as defined in eq. (3.1). In our settings, the logistic regression is explainable\nsince the log-odds transformation establishes a linear relationship between features and the target\nh(f(x))\nvariable such that $log \\frac{h(f(x))}{1-h(f(x))} = wf_h(x) + b$. Differentiating with respect to a feature\ncomponent $[f(x)]_i$ shows that each coefficient $w_j$ quantifies the impact of that feature on the log-\nh(f(x))\nodds such that $\\frac{\\partial}{\\partial [f(x)]_i} log \\frac{h(f(x))}{1-h(f(x))} = w_j$. Moreover, if $f_h$ is a linear transformation, i.e.,\n$f_h(x) = Cx$, the chain rule relates changes in the original features to the log-odds which can be ex-\nh(f(x))\npressed as $\\frac{\\partial}{\\partial [x]_i} log \\frac{h(f(x))}{1-h(f(x))} = \\sum_{k=1} W_kC_{kj}$. Therefore, this linearity allows straightforward\ninterpretation of each feature's influence on the predicted probabilities, making logistic regression\nhighly explainable (Hastie et al., 2009). By directly linking coefficients to features, it offers an\nintuitive understanding of each feature's impact on the output."}, {"title": "Residual Calibration.", "content": "The final step of MoleX involves learning a residual calibrator r to improve\npredictive accuracy while preserving explainability. We freeze parameters of the explainable model\nh and calibrate samples that h fails to predict. By optimizing the objective function in eq. (3.1),\nwe iteratively fix prediction errors, sequentially driving the overall predictions closer to the target\nvalues. The residual calibrator is designed as a linear model, ensuring that the overall model remains\nexplainable. Specifically, we define the residual calibrator r with weights $w_r \\in R^{d_r}$ corresponding\nto each residual feature and bias $b_r$:\n$r(f_R(x)) = w_r^T f_R(x) + b_r$.\nHere, $f_R(x)$ represents the residual features obtained from the decomposition of the feature space\n$R^d$ into orthogonal subspaces such that $f(x) = f_h(x) + f_R(x)$, where $f_h(x) \\in R^{d_c}$ contains the\nexplainable features used by h, and $f_R(x) \\in R^{d_r}$ contains the residual features used by r, with the\northogonality condition given by $\\langle f_h(x), f_R(x) \\rangle = 0$. Then, the overall prediction combines the\ncontributions from h and r:\n$\\hat{y}(x) = \\underbrace{w_h^T f_h(x) + b_h}_{Explainable Model Contribution} + \\underbrace{w_r^T f_R(x) + b_r}_{Residual Calibrator Contribution}$.\nThe orthogonality and linearity between $f_h(x)$ and $f_R(x)$ guarantee that the contributions from\nh and r are additive and independent, making the residual calibrator r theoretically explainable.\nMoreover, each feature's impact on the prediction can be directly understood through the cor-\nresponding weights in $w_h$ and $w_r$. Since $f_h(x)$ and $f_R(x)$ are orthogonal, the inner products\n$\\langle w_h, f_R(x) \\rangle = 0$ and $\\langle w_r, f_h(x) \\rangle = 0$ vanish. This ensures that h and r do not influence each other's\nfeature contributions, thus preserving the explainability of both models in the combined prediction.\nWe thus formalize the following theorem:"}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETTINGS\nDatasets. We empirically evaluate MoleX's performance on six mutagenicity datasets and one hepa-\ntotoxicity dataset. The mutagenicity datasets include Mutag (Debnath et al., 1991), Mutagen (Morris\net al., 2020), PTC family (i.e., PTC-FM, PTC-FR, PTC-MM, and PTC-MR) (Toivonen et al., 2003)\nand the hepatotoxicity dataset includes Liver (Liu et al., 2015). To demonstrate that MoleX can\nexplain molecular properties using chemically meaningful substructures, we introduce the concept\nof ground truth: substructures verified by domain experts to have significant impacts on molecular\nproperties. The ground truth substructures for six mutagenicity datasets are provided by Lin et al.\n(2022); Debnath et al. (1991), while those for the hepatotoxicity dataset are provided by Cheng et al.\n(2023). Further details are available in appendix A.5.\nEvaluation Metrics. We show that MoleX achieves notable predictive performance, explainability\nperformance, and computational efficiency. We apply a specific metric to evaluate each aspect of\nthe model performance. For predictive performance, we define $\\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}(y^{(i)} = \\hat{y}^{(i)})$ to compute\nthe classification accuracy. For explainability performance, we follow the settings in GNNExplainer\n(Ying et al., 2019) where explanations are treated as a binary classification of edges (the prediction\nthat matches ground truth substructures as positive, and vice versa) and use the Area Under the\nCurve (AUC) to quantitatively measure the explanation accuracy. For computational efficiency, we\nmeasure the execution time in microseconds for each method.\nBaselines. To extensively compare MoleX with different methods, we utilize (1) GNN baselines,\nincluding GCN (Kipf & Welling, 2016), DGCNN (Zhang et al., 2018), edGNN (Jaume et al., 2019),\nGIN (Xu et al., 2018), RW-GNN (Nikolentzos & Vazirgiannis, 2020), DropGNN (Papp et al., 2021),\nand IEGN (Maron et al., 2018); (2) LLM baselines, including Llama 3.1-8b (Dubey et al., 2024),\nGPT-40 (Achiam et al., 2023), and ChemBERTa-2 (Ahmad et al., 2022); (3) explainable model\nbaselines, including logistic regression, decision tree (Quinlan, 1986), XGBoost (Chen & Guestrin,\n2016), and random forest (Breiman, 2001).\nImplementations. Our model is pre-trained on the full ZINC dataset (Irwin et al., 2012) using\nChemBERTa-2, with 15% of tokens in each input randomly masked. We then fine-tune this model\non the Mutag, Mutagen, PTC-FM, PTC-FR, PTC-MM, PTC-MR, and Liver datasets (in Group\nSELFIES). To evaluate model performance, we compute the average and standard deviation of each\nmetric for each method after 20 rounds of execution. Further details are provided in appendix A.6."}, {"title": "5.2 RESULTS", "content": "Predictive Performance. Table 1 presents a comparison of predictive performance across differ-\nent methods. MoleX consistently outperforms all baselines, indicating its robustness and general-\nizability. Specifically, as a combination of LLMs and explainable models, MoleX achieves better\nperformance compared to either of them (i.e., 16.9% and 23.1% higher average classification accu-\nracy than LLM and explainable model baselines, respectively) and demonstrates the effectiveness\nof augmenting explainable models with LLM knowledge. Moreover, by integrating residual cali-\nbration, MoleX raises the average classification accuracy by 7.0% across seven datasets. Notably,\nthe classification accuracy of our base model, logistic regression, improves by 27.8% after LLM\nknowledge augmentation and then by an additional 5.5% after residual calibration on the Mutag\ndataset. Therefore, by maximizing task-relevant semantic information in the LLM knowledge and\nemploying a residual calibration strategy, we enable a simple linear model to achieve predictive\nperformance even superior to that of GNNs and LLMs in molecular property predictions.\nExplainability Performance. Table 2 reports the explanation accuracy across different methods.\nBy encoding functional group-level molecular representation, MoleX offers significantly better ex-\nplainability than baselines on six datasets. Additionally, the residual calibration improves average\nexplanation accuracy by 8.8%, reflecting a remarkable enhancement of explainability. Similarly, on\nthe Mutag dataset, the explanation accuracy of logistic regression is boosted by a total of 33.4%\nvia LLM knowledge augmentation and residual calibration. Interestingly, while most methods excel\non simpler datasets like Mutag but falter on complex ones like Liver, our method maintains high\nexplanation accuracy across both, showing adaptability and high explanation quality."}, {"title": "5.3 ABLATION STUDIES", "content": "In this section, we introduce ablation studies on the number of n in n-gram, principal components\nin EFPCA, training iterations of the residual calibrator, and the selection of the base model.\nNumber of n in N-grams. We empirically compare the choice of n in n-grams. As shown in\nfig. 6, the overall model performance improves as n increases from 1 to 3, then declines for n\nfrom 4 to 9. Three of four datasets in our studies indicate the optimal performance at n = 3.\nIncreasing n captures more contextual semantics, including functional group interactions and raises\nthe model performance. However, overlarge n values incorporate excessive or irrelevant contextual\ninformation and reduce model utility correspondingly. Further details are in appendix A.10.\nDimensionality Reduction via EFPCA. We use EFPCA to reduce the dimensionality of LLM\nembeddings, obtaining explainable and compact embeddings. As shown in fig. 5, cross-validation\nacross four datasets determines the optimal number of principal components. Empirically, compo-\nnents beyond 20 contribute minimally to the molecular property prediction. Additional components\nyield diminishing returns while increasing model complexity and reducing explainability. Further\ndetails are in appendix A.8. Moreover, we also investigate the effect of our dimensionality reduction.\nAs presented in table 5, we compare the model performance without dimensionality reduction. We"}, {"title": "6 CONCLUSION", "content": "This work develops MoleX, a novel framework utilizing LLM knowledge to build a powerful lin-\near model for accurate molecular property predictions with chemically meaningful explanations.\nSpecifically, MoleX extracts task-relevant knowledge from LLM embeddings using information\nbottleneck-inspired fine-tuning and sparsity-inducing dimensionality reduction to train a linear\nmodel for explainable inference. Additionally, a residual calibration module is designed to recover\nthe original LLM's performance and further enhance the linear model via recapturing prediction\nerrors. During its inference, MoleX precisely reveals crucial substructures with their interactions as\nexplanations. Notably, MoleX enjoys the advantage of LLM's predictive power while preserving\nthe linear model's intrinsic explainability. Extensive theoretical and empirical analysis demonstrate\nMoleX's exceptional predictive performance, explainability, and efficiency."}, {"title": "A APPENDIX", "content": "A.1 PROOF OF N-GRAM COEFFICIENTS AS VALID CONTRIBUTION SCORES FOR\nDECOUPLED N-GRAM FEATURES\nIn this section, we demonstrate that n-gram coefficients in the linear model can be interpreted as\nfeature contribution scores based on the statistical properties of the linear model.\nProof. Suppose $E \\in R^{n \\times d}$ is the matrix of n-gram embeddings, where each row $e_i^T$ is the embed-\nding of the i-th n-gram. Let $v_{ij} \\in R^d$ be the embedding of the j-th feature in the i-th n-gram, and\nsuppose that each n-gram consists of m features (m is a constant across all n-grams). Let $c_{ij}$ denote\nthe contribution score of the j-th feature in the i-th n-gram. We formulate the following assumptions\nbased on OLS properties to ensure the validity of using n-gram coefficients as contribution scores:\n1. Linearity. The relationship between the input embeddings and the output is linear. Namely,\nfor all i,\n$y_i = e_i^T w^* + \\epsilon_i,$\nwhere $w^* \\in R^d$ is the true coefficient vector, and $\\epsilon_i$ is the error term.\n2. N-gram Embedding Decomposition. Each n-gram embedding $e_i$ is the average of its\nconstituent feature embeddings:\n$e_i = \\frac{1}{m} \\sum_{j=1}^m v_{ij}$.\n3. Ordinary Least Squares (OLS). The linear model is estimated using OLS by minimizing\nthe residual sum of squares:\n$w = arg \\min_w \\sum_{i=1}^n (y_i - e_i^T w)^2$.\n4. Error Properties.\n(a) Zero Mean Errors. The errors $\\epsilon_i$ have zero mean given the embeddings:\n$E[\\epsilon_i | E] = 0$.\n(b) Homoscedasticity. The errors have constant variance given the embeddings:\n$Var[\\epsilon_i | E] = \\sigma^2,$\nwhere $\\sigma^2$ is a positive constant.\n(c) No Autocorrelation. The errors are uncorrelated with each other:\n$Cov[\\epsilon_i, \\epsilon_j | E] = 0$ for $i \\ne j$.\n5. Full Rank. The matrix $E^T E$ is invertible (i.e., E has full column rank).\nWe define the contribution score of each decoupled n-gram feature as follows:\nDefinition A.1. The feature contribution score $c_{ij}$ for the j-th feature in the i-th n-gram is defined\nas\n$c_{ij} = v_{ij}^T w,$\nwhere $\\hat{w}$ is the estimated coefficient vector from the linear model.\nLemma A.1 (Prediction as Sum of Feature Contributions). Under Assumption 2, the predicted out-\nput for the i-th n-gram is\n$y_i = e_i^T w = \\frac{1}{m} \\sum_{j=1}^m c_{ij}$"}, {"title": "A.2 PROOF OF THEOREM 4.1 (EXPLAINABILITY OF VIB-BASED TRAINING OBJECTIVES)", "content": "Proof. We demonstrate the Variational Information Bottleneck (VIB) framework, which aims to\nlearn a compressed representation Z of input variable X that preserves maximal information about\nthe target variable Y while being minimally informative about X itself. This is achieved by opti-\nmizing the objective function as follows:\n$L_{IB}(\\theta) = I(Z; X) \u2013 \\beta I(Z;Y )$\nwhere $I(\\cdot;\\cdot)$ is mutual information, $\\beta \\ge 0$ is a tuning parameter, and $\\theta$ is the parameters of the\nencoder. Our goal is to derive a tractable variational lower bound of this objective function that can\nbe optimized using stochastic gradient descent.\nDefinition A.2 (Mutual Information). For random variables X and Z with joint distribution\n$p(X, Z)$, the mutual information $I(X; Z)$ is defined as\n$I(X; Z) = E_{(X,Z)} \\Big[log \\frac{p(X, Z)}{p(X)p(Z)}\\Big]$\nAlternatively, it can be expressed as\n$I(X; Z) = E_{p(X)} \\Big[D_{KL}(p(Z|X)||p(Z))\\Big]$"}, {"title": "A.3 PROOF OF THEOREM 4.2 (EXPLAINABILITY OF EFPCA)", "content": "Proof. To demonstrate the explainability of the EFPCA", "b": "such as B-spline basis\nfunctions. Each $\\phi_j(t)$ is nonzero only over a subinterval $S_j \\subset [a"}, {"b": ".", "functions": "n$\\xi_k(t) = \\sum_{j=1}^p a_{kj}\\phi_j(t)$                              (A.4)\nwhere $a_k = (a_{k1}, a_{k2},...,a_{kp})^T$ is the coefficient vector for the k-th principal component. There-\nfore, the"}]}