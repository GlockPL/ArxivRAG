{"title": "Hybrid classical-quantum architecture for vectorised image classification of hand-written sketches", "authors": ["Y. Cordero", "S. Biswas", "F. Vilari\u00f1o", "M. Bilkis"], "abstract": "Quantum machine learning (QML) investigates how quantum phenomena can be exploited in order to learn data in an alternative way, e.g. by means of a quantum computer. While recent results evidence that QML models can potentially surpass their classical counterparts' performance in specific tasks, quantum technology hardware is still unready to reach quantum advantage in tasks of significant relevance to the broad scope of the computer science community. Recent advances indicate that hybrid classical-quantum models can readily attain competitive performances at low architecture complexities. Such investigations are often carried out for image-processing tasks, and are notably constrained to modelling raster images, represented as a grid of two-dimensional pixels. Here, we introduce vector-based representation of sketch drawings as a test-bed for QML models. Such a lower-dimensional data structure results handful to benchmark model's performance, particularly in current transition times, where classical simulations of quantum circuits are naturally limited in the number of qubits, and quantum hardware is not readily available to perform large-scale experiments. We report some encouraging results for primitive hybrid classical-quantum architectures, in a canonical sketch recognition problem.", "sections": [{"title": "1 Introduction", "content": "Quantum information processing [1, 2] holds the promise to be a game-changer in a wide variety of scenarios. Examples are \u2014but not restricted to\u2014 providing energy-savings in large-scale compu- tations [3, 4], speeding-up linear-algebra subroutines [5], boosting data-base search-routines [6, 7] with applications in optimization problems [8], factorizing prime-numbers in polynomial time [9], breaking commonly-used cryptography protocols [10], and enhancing precision for sensing appli- cations [11] among a constantly growing number of quantum algorithm applications [12]. In turn, experimental quantum-advantage has already been attained in specific scenarios related to learning quantum data [13], sampling from very complex distributions [14, 15, 16], or proof-of-concepts quantum-speeding-ups in reinforcement-learning [17], and this collection of results constitute a milestone in quantum hardware development. Nonetheless, there is still a long way to go until the aforementioned game-changer quantum algorithms can be implemented in real quantum hardware, since this requires a large amount of logical (e.g. error-corrected) qubits 1.\nDriven by the advent of Artificial Intelligence, Quantum Machine Learning (QML) [21, 22, 23, 24, 25, 26, 27, 28, 29, 30] has emerged as an appealing bridge between two rapidly-growing and highly impactful fields, holding a considerable attention from both scientific and industry communities. In this context, one of the most prominent fields that can potentially benefit from QML is Computer"}, {"title": "2 Framework", "content": "In the following we provide a short introduction to quantum machine learning (QML), and consequently to the field of sketch processing. Readers unfamiliar with quantum physics are encouraged to first go through Appendix A, where basic notions such as quantum states, observables and measurements are discussed.\n2.1 Quantum Machine Learning\nQuantum models (QMs) exploit effects such as quantum superposition or entanglement in order to make computations in a probabilistic way. While universal and provably more powerful than classical computing [1, 97], we will here conceive them as special-purpose processing units, and restrict the following discussion to noiseless variational quantum computing; more interested readers are encouraged to follow Refs. [27, 28] for further details.\nVariational Quantum Algorithms. VQAs make use of Parametrized Quantum Circuits (PQCs) to prepare target quantum states. Here, a fiducial n-qubit quantum state, often |0)\u00aen, gets transformed by a PQC, consisting on a series of quantum gates. Such quantum gates are chosen from a pre-defined dictionary (e.g. single-qubit rotations and CNOTs), and placed at different positions in the circuit. We remark that devising mechanisms to build quantum-circuit layouts that can readily be adapted to current NISQ hardware and tackle trainability challenges is a topic of current research [75, 76, 77, 78, 79, 98].\nIn analogy to classical neural networks scenarios, the goal of VQAs is to minimize a given cost function C(0), depending on trainable parameter values appearing in the quantum circuit, which we denote as 6, and are usually associated to the single-qubit rotations. Somewhat misleading, this setting is often known as a quantum neural network.\nHere, a quantum device is used to estimate cost-function and cost-gradient values, whereas a classical optimization algorithm, e.g. Adam optimizer, is in charge of navigating the parameter landscape. In its most basic format, the VQA training mechanism implies solving an optimization problem encoded into the cost-function structure as per the expectation value of a quantum observable 0:\n\\(C(x; 0) = (4\uae09(x)|0|4\uae09(x)),\\) \nwhere 4(x)) is the quantum state prepared by the PQC. Here, we remark the potential dependence of the quantum state in classical data x, which can be injected by an appropriate encoding strategy. Note that such data x can in turn be the output of a classical machine-learning algorithm, as discussed in Sec. 3.\nThe estimation of the cost via the PQC, out of running it many times (so to accurately estimate its expected value) and its modification by its gradient, defines a VQA-cycle, which is iterated until meeting a stopping criteria 3. We remark that cost-function gradients can be estimated via the so-called parameter-shift rule [99, 100, 101]. In short, this implies that for a broad class of cost-functions (as those considered in Eq. 1), cost-function gradients can be estimated using the same circuit, but shifting the parameters according to a pre-defined recipe. This is utmost importance for VQAs implementation on real quantum hardware, since it provides a route to compute gradients in an experimental way. Alternatively, for classical simulation of quantum circuits (as done in this manuscript), quantum automatic-differentiation libraries have been developed (such as PennyLane [102], TensorFlowQuantum [103], Qiskit [104] among others). As such, they are restricted in the number of qubits that can classically simulate, though provide a convenient framework"}, {"title": "2.2 Sketch Processing", "content": "Unlike pre-trained language models like GPT-k [110] or BERT [111], humans can effortlessly interpret diverse and abstract conceptual sketches by leveraging our robust mental models, which integrate perceptual processing and high-level cognitive functions [112]. This ability allows us to understand even the most ambiguous sketches, a skill that current Al systems struggle to match. Existing works [93, 113] have tried to bridge the gap between human-like reasoning and AI models (which directly learn from raw data) by using neuro-symbolic models that capture compositionality and causal knowledge for flexible generalization. Other works [114, 115] have developed autoregressive approaches, utilizing similar stroke primitives to model the causal representation of sketches. By combining a classical autoregressive model of sketches with an integrated quantum processing module, we seek to explore a new spectrum of vector sketch modelling.\nLearning Sketch Representations. Just as language comprises a set of syntactic rules and semantic structures, sketches can be interpreted as visual sentences [116, 117], constructed through a series of pen strokes or words that follow a certain grammar. In this proposed approach, an image is treated as a sequence of lines and represented as a vector image. This analogy drives us towards modelling sketches in a sequential nature, where the drawings parallel the linear construction of sentences in natural language [118]. The most commonly adopted format for a vector image in the classical sketch literature [114] is a sequence D, each containing a 2D canvas coordinate X; sampled from a continuous drawing flow and a pen-state flag bit fi denoting whether the pen touches the canvas or not. The vectorized sketch drawing D can be represented as:\n\\(D = {(Xi, fi)}=1,\\)\nwhere X\u2081 = (xi, yi)T \u2208 R2, fi \u2208 {0, 1} depending on whether the pen is up or down, and L denotes the length of the whole sketch.\nIn particular, SketchRNN [114] learns a parametric Recurrent Neural Network (RNN) to model the joint probability distribution Q of 2D coordinates and pen state as a product of conditionals as\n\\(Q(D;0) = \\prod_{i=1}^{L} p(Xi, fi | X<i, f<i; 0),\\)\nwhere @ is the set of learnt parameters of the model, and X<i and f<i denote the list of canvas locations and pen-state bits respectively before X; and qi.\nVectorized Sketch Data Processor: SketchRNN treats sketches as a digitized sequence of 2D points on a drawing canvas sampled along the ink-flow trajectory, e.g. a sequence of dense line segments. This representation strategy has several drawbacks. In this sense, SketchRNN provides limited scalability, since it still models sketches as a sequence of (line-connected) pixels. As such, it is limited in processing long stroke sequences before the memory of the underlying RNN collapses."}, {"title": "3 Methodology", "content": "In this work we approach the task of understanding abstract sketches from a QML angle. The search of quantum advantage, one of the most visited topics for quantum computing, is beyond the scope of this manuscript; our intention is to help elucidating novel research directions, related to learning how to conceptualize out of quantum machines by analyzing symbolic data [93].\nHere, we take first steps towards that direction by tackling the sketch recognition problem. This relies more on causal and compositional conceptual reasoning rather than classification tasks where models directly learn from pixel-based raw image data. In the latter case, the MNIST data-set [128] is used in a strikingly frequent way. This standard dataset is composed of handwritten digits and currently used a standard benchmark for QML models. However, it consists on 60K pixel-based images (28 x 28), which exceeds the dimension of data that can be handled by either currently available quantum hardware (or classical simulators), if we do not rely on dimensionality reductions and constrain the number of labels to be classified, posing serious limitations to benchmark QML models. This issue points out the necessity for the QML community to explore different tasks and associated datasets"}, {"title": "3.1 QuantumDraw Framework", "content": "The overview of our hybrid classical-quantum model is illustrated in Fig. 2, and consists on the following three building blocks: the Sequential Stroke Data Handler, the Quantum Processing Module and the Final Classification Layer. Sketches are vectorised in a set containing the Bezier point coordinates for every stroke, together with a flag for end-of-stroke. In order to keep the length of every sample uniform, samples (sketches) are normalised to the maximum number of points in the database, which we denote by N, padding with zeros to keep constant N-length. No time-stamps are used for this work.\nSequential-Stroke Data Handler: The Sequential-Stroke Data Handler consists of two Long Short-Term Memory (LSTM) units [129], chosen to build a sequential-encoder block that mimics Sketch-RNN [114]. Such recurrent layers encode the input vector image-sequence D, and lead to a sequence of hidden states hi. The final hidden state is further post-processed by means of a max-pool operation (that halves the size of the data).\nQuantum Processing Module: In order to make feasible the hybrid classical-quantum approach, some sort of dimensionality reduction is required [85, 86, 87, 88, 89, 90, 91]. Here, we consider three fully-connected neural networks whose output is fed to a quantum circuit. The fully connected networks are two-layer networks, the initial pair keeping the original dimensionality of the hidden state output by the Sequential Stroke Data Handler (the recurrent module), and the final one ending in five output networks, which are fed to a five-qubit quantum circuit as rotation angles of Rx gates acting on each qubit.\nWe chose a standard quantum circuit layout, known as Hardware Efficient Anstaz (HEA) [58], and use a single layer (classical network outputs are fed by an angle-embedding in the first single-qubit rotation acting on each qubit). We remark that there is room to improve on such circuit layout (e.g. by a variable-ansatz approach [75, 76, 77, 78, 79]), and our choice is aligned with keeping the architectures as simple as possible. The HEA consists of three consecutive single-qubit rotations (Ry, Rz, Ry) followed by a CNOT gate; rotations are parametrized by real numbers defining rotation angles, which here serve as the trainable parameters of the parametrized quantum circuit. CNOTs, on the other hand, act on two qubits, and generate quantum correlations between them; this differentiates classical to quantum computing paradigm, being thus of utmost relevance. We note that a dictionary comprised of single-qubit rotations and CNOTs connecting all qubits in the circuit is approximately universal [1]: any target n-qubit quantum transformation can be implemented provided enough gates of the dictionary are used. However, issues arise since when stacking many HEA-layers together, the circuit becomes too expressible and BPs rise [71].\nFinal classifier: A final fully-connected neural network layer provides the classification. This is done by feeding the five expected-values that the quantum circuit computes (((i))) for i = 1, ..., 5 to the classical network, resulting in three numbers describing the probability that the original sketch belong to each class-type (in our case: Calculator, Cellular and Camera)."}, {"title": "3.2 Classical Baseline", "content": "As a baseline, we adopt a simple RNN encoder consisting of a couple of LSTM layer, followed by a feed-forward layer that outputs the probability for each category. The primary aim of using a"}, {"title": "3.3 Implementation Details", "content": "The QuickDraw dataset is initially pre-processed to normalize the images, ensuring a consistent input shape suitable for our model. The data is divided into training and validation sets to facilitate the evaluation of the model's performance during training. We use the Adam method as our optimizer, which adapts the learning rate for each parameter, and cross entropy as our objective function, which is well-suited for recognition tasks. The model is trained over a series of 100 epochs, during which the optimizer updates the model weights to minimize the loss function. Throughout the training, both training and validation losses are monitored. Moreover, we perform experiments on multiple seeds to assess model's learning progress and generalization ability."}, {"title": "4 Results", "content": "We present results for the sketch-processing problem by means of hybrid classical-quantum models. All the experiments and results can be found in open access through to the code provided as supporting material to this piece of research 4.\nWe train our classical baseline on the three classes considered of the vectorized-image QuickDraw dataset. Next, we train our QuantumDraw model. As detailed in Sec. 3.1, our model is an hybrid architecture that first process the full sketch by two consecutive LSTM layers. The final hidden state is reduced in dimensionality and provides the input to a feed-forward network, whose output is in turn processed by a five-qubit quantum circuit. Finally, the output of the five expected values provided by the quantum circuit are post-processed by a single-layered feed-forward network that retrieves the probability for the sketch belonging to each category.\nAdditionally, and for comparative purposes, we analyze the same dataset, but now by keeping each sample encoded as a raster-image (instead of a vectorised one). To this end, we implemented the Hybrid Quantum Neural Network-Parallel (HQNN-Parallel) circuit, proposed in Ref. [105]. Such a model consists on a series of (classical) convolutional layers that extract the relevant features of the (pixel-based) image to reduce the original dimensionality. These features are then processed by parallel parametrized quantum circuits, whose outputs (expected values) are further post-processed by a feed-forward layer retrieving the probability of the sketch belonging to a given category."}, {"title": "4.1 Discussion", "content": "Our results indicate that our QuantumDraw model is able to capture complex patterns and generalize effectively to unseen data, with a performance comparable to that of its classical baseline. However, care must be taken when claiming such a competitive results, and in the following we further analyze the quantumness associated to the proposed model and its resulted trained version.\nTo shed light into this result, we studied two different versions of QuantumDraw, whose learning curves are reported in Appendix B. First, we considered QD-Frozen, a version of QuantumDraw in which the trainable (quantum) parameters are frozen to their (initially randomly chosen) value; here the classical weights of the LSTM need to adapt in order to counteract the quantum layer, which is not being trained. The aim of such a scheme is to test whether the classical part of the model can compensate the presence of the quantum circuit. Our results point in the opposite direction, indicating that training both the classical and the quantum networks leads to an improved performance, as compared to trianing only the classical part.\nAs pointed out in the introduction, little is known on whether genuine quantum effects are actually employed in hybrid approaches as the one proposed in the this piece of research. For instance, even though there is strong theoretical basis to include quantum correlations as core ingredient on the design of hybrid classical-quantum systems, and a growing number of bibliographic contributions are supporting its implementation in a diverse number of applications [92], this is still a highly exploratory field, and it is worth to analyse to what extent the presence or absence of the entanglement are actually impacting the final result. For this reason, we consider a second (separable) version of the QuantumDraw, deemed QD-Sep, in which the CNOTs present in the quantum circuit layout are removed. By doing this, we remove quantum correlations present in the circuit, and thus the model can be simulated classically in an efficient way. We trained many several instances of QD-Sep and obtained competitive performances than its original version with quantum correlations. This result, which is in strong alignment with current warnings of the overall trainability and classically simulatibility of quantum models [72, 92] is to be taken with care.\nOur hybrid model is to be understood as a platform to allow further explorations, rather than providing a quantum advantage. In this sense, we did not tackle in this paper the optimization of the quantum circuit layout to take into account potential symmetries and advantages of the sketch structure and recognition problem, nor to fully benefit from the intrinsic temporality of the data. Rather than ruling out any room for quantum advantage in frameworks similar to our QuantumDraw, the QD-Sep results should be understood - we believe as a message to both QML community and computer science community. When searching for concrete applications of quantum technology, room for basic science and negative results should be allowed. The path that scientific knowledge has historically taken is erratic, and we shall not expect that straightfowardly implementing \u201cquantum\" analogs of neural networks can lead to an improvement over state-of-the art mechanisms."}, {"title": "5 Conclusion and Future Scopes", "content": "In this paper, we have shown how to tackle the sketch-processing problem from the alternative paradigm of quantum computing, initiating this field of research.\nRather than inspecting quantum advantages, our goal is to bring two distant communities closer, since we believe that interesting synergies can emerge from such approach. In particular, we analyzed the possibility of processing vector images by means of a hybrid classical-quantum model for a three-class sketch-recognition problem. While further investigations are required, and in particular increasing the number of classes (at a considerably high computational cost), our results are encouraging, and further investigations need to be carried out in order to reduce the variance of our results and analyze whether the baseline considered can be simplified further.\nFrom a research direction path, this work opens up the possibility to import many canonical sketch- processing tasks to the QML realm. This is not constrained to recognition, but also generation, completion and abstraction among others [93]. However, we remark that to do this, a different QML architecture is to be considered, since partial sketch information needs to be accessed in order to predict the next stroke. This is an ongoing investigation, and it is an open question whether quantum memory effects can help in such auto-regression or not.\nWhile the quantum processing of classical information is still at early stages, and little is known on how to cleverly approach it, we believe that computer vision problems such as sketch classification can serve as good test-beds, precisely for the reasons mentioned above. In contrast, most of the QML literature tests their models on raster images, more often the MNIST dataset [128], a dataset made requiring relevant computational power to train even on hybrid classical-quantum models [105]. Moreover, images are represented as a two-dimensional pixel grid, which needs to be down-scaled in order to fit into either simulable quantum devices or near-term quantum hardware. On the contrary, vector-based images are not only of lower dimension, but also present an interesting and largely unexplored temporal nature. In turn, quantum-enhanced memory agents can potentially be beneficial at the time of processing sketches.\nAs we mentioned along the paper, the database used for the sketches include the timestamp for every stroke. Time stamps were not strictly included in the analysis carried out on this paper, and only the end-of-stroke flag was integrated into the image representation. However, this can be considered as a promising future line of work, by conditioning the quantum model with temporal dependencies provided by the available time stamp information.\nFinally, we believe that much of the discussions hold by the computer vision community can be beneficial in order to re-consider different aspects of quantum machine learning, not necessarily constrained to classical-data processing. In this sense, it would be interesting to study how conceptualizations of (quantum) data can be done by a quantum computer, a matter that will ultimately lead us to better understand human reasoning."}, {"title": "A Selected basic concepts of quantum physics and quantum information", "content": "The smallest quantum system is deemed a qubit, and for composite n-qubit systems we have d = 2n 5. This poses a serious limitation to classical simulations of quantum systems, due to exponentially-growing memory requirements, being brute-force approaches constrained to \u2264 32 qubits [132].\nA quantum system is fully characterized by its quantum state, represented by a d-dimensional complex vector\n|\\(\u03c8>\\) =  \u03a3kCk|k>,\nwhose squared-absolute entry values sum up to one, i.e. \u03a3k|ck|\u00b2 = 1. Here, {|k>} denotes an orthonormal basis in a d-dimensional Hilbert space, and ck are complex coefficients denoting amplitudes of quantum state |). Note we are here using the so-called Dirac notation, denoting vector k = |k>.\nThe way to extract information out of a quantum system is by measuring it; here we are interested in projective measurements (which are only a sub-class of the whole set of quantum measurements), and are obtained by projecting the quantum state into proper sub-spaces of an observable K of interest. The latter denotes a physical quantity (e.g. position, angular-momentum, hamiltonian) and is mathematically represented by an hermitian operator, implying its equipped with an associated eigenbasis {|k>}. In particular, a projective measurement on | over {|k>} retrieves a measurement outcome k with probability p(k||) = |ck|2. This provides an operational interpretation of the coefficients ck in Eq. (6). Importantly, when the quantum system is (projectively) measured, it is also destroyed, and the state | needs to be prepared again; for a detailed discussion about quantum measurements we refer the reader to Refs. [1, 133].\nAs an example, a (pure) state of a qubit can be parametrized as per |) = cos(\u03b8/2)|0) + e-i\u03c6 sin(\u03b8/2)|1), with {|0), |1>} being the eigenstates of oz, i.e. the Pauli matrix representing the observable intrinsic angular momentum of a spin-1/2 particle in the z direction. Quantum states are connected by unitary transformations, which in the single-qubit case are given by rotations; (\u03b8, \u03c6) are parameters associated to rotation-angles. By appropriately choosing the rotation-angles, we can reach any target state. While this idea generalizes to bigger systems, the exponential growth of Hilbert space make finding the adequate transformation a highly non-trivial task.\nIn turn, the Hilbert space associated to composite n-qubit systems is given by the direct product of individual single-qubit Hilbert spaces. As a consequence, the direct product of local eigenbasis spans a global eigenbasis as {|k>} = {|0>,|1>}\u00aen. The latter is in correspondence to the observable \u2295n i=1 \u03c3(i) z ), where \u03c3(i) z ) is the spin-z operator of the ith qubit (up to a constant factor). This example is relevant in gate-based quantum computing, since such is the quantum measurement usually performed after preparing the quantum state 6"}, {"title": "B Additional experiment details", "content": "We report further numerical analysis the QuantumDraw model behaviour. To begin with, we report its average learning curve, when (trainable) quantum paramters are randomly initialized over different seeds (see Fig. 4).\nNext, we study the case in which the quantum trainable parameters are freezed, while the classical weights of the network are allowed to be modified; learning curves are shown in Fig. 5 (left). Here, we want to understand whether the quantum circuit, plays a relevant role or not, and if its presence can be overcomed by adapting only modifying classical weights. Our results indicate that such is not the case (see Table 1), since the performance of the freezed quantum model (QM-Frozen) is worst.\nFinally, we consider the case in which the full hybrid QuantumDraw model is trained, but the entangling gates are removed, e.g. making it separable; learning curves are shown in Fig. 5 (right). This makes"}]}