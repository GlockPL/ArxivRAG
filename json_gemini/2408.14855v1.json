{"title": "Enhancing Analogical Reasoning in the Abstraction and Reasoning Corpus via Model-Based RL", "authors": ["Jihwan Lee", "Woochang Sim", "Sejin Kim", "Sundong Kim"], "abstract": "This paper demonstrates that model-based reinforcement learning (model-based RL) is a suitable approach\nfor the task of analogical reasoning. We hypothesize that model-based RL can solve analogical reasoning\ntasks more efficiently through the creation of internal models. To test this, we compared DreamerV3, a\nmodel-based RL method, with Proximal Policy Optimization, a model-free RL method, on the Abstraction\nand Reasoning Corpus (ARC) tasks. Our results indicate that model-based RL not only outperforms\nmodel-free RL in learning and generalizing from single tasks but also shows significant advantages in\nreasoning across similar tasks.", "sections": [{"title": "1. Introduction", "content": "The Abstraction and Reasoning Corpus (ARC) is known as a benchmark for evaluating abstrac-\ntion and reasoning abilities [1]. Examples of ARC tasks can be seen in Fig. 1. To solve an ARC\ntask, one needs the ability to abstract a common rule among given demos and apply this rule\nto a new test input grid to infer the corresponding grid [2]. Additionally, to solve untrained\ntasks, one must have the ability to discover various knowledge during the learning process and\nselectively apply this knowledge in the task-solving process [2].\nThis study interprets the two abilities required by ARC not as separate skills but as one\nconcept of analogical reasoning. Analogical reasoning refers to the process of understanding\nnew situations or solving problems based on existing knowledge that can be applied to different\ncontexts or problems [3]. First, the process of solving an ARC task can be seen as solving a\nnew problem (test input grid) based on the transformations from input grids to output grids in\neach demo (existing knowledge), which exactly fits the definition described above. Second, for\na model trained on various ARC tasks to solve a new ARC task, it must understand the demos\nof the new ARC task (new situations) based on the solution processes (existing knowledge)\napplicable to the training tasks (different problems) and solve the task, which also aligns with"}, {"title": "2. Model-Based RL and Model-Free RL", "content": "RL has two main categories: model-based RL and model-free RL. Before conducting experiments\nto compare these categories from the perspective of abstraction reasoning, this paper briefly\nreviews the characteristics of each category and compares the algorithms that will be used in\nthis experiment."}, {"title": "2.1. Model-Based RL", "content": "Model-based RL builds an internal model of the environment to predict various possibilities\nand establish more efficient policies based on these predictions. In problems requiring abstract\nthinking and pattern recognition like ARC, the model-based RL approach provides the ability\nto generalize to new situations that the agent has not experienced through its internal model.\nRepresentative algorithms utilizing the model-based RL approach include Dyna-Q [7], I2A [8],\nMBMF [9], and World Model [10]. Also, AlphaZero [11], which successfully surpassed human\nperformance in board games such as Go, Chess, and Shogi based on Monte Carlo Tree Search\n(MCTS), is the most well-known. Recently, DreamerV3 [12], based on the World Model, has\nbeen the subject of extensive research.\nA key feature of DreamerV3 is its ability to extract important features from input data and\nconvert these into a latent representation, which is then used to learn a predictive model of\nthe environment's dynamics. This model functions by taking the current state and actions as\ninputs and predicting the next state's representation and rewards. This approach allows for the\nsimulation of future scenarios and the evaluation of various scenarios. In summary, DreamerV3\noperates by repeating the following three processes: 1) collect data based on a random policy or\nthe current policy, 2) update the agent's model based on the collected data, and 3) generate and\nevaluate virtual future scenarios using the updated model to improve the policy. This process\nexcels in multi-step tasks like mining diamonds in Minecraft. We chose DreamerV3 for its\nsuperior handling of ARC, compared to other model-based approaches."}, {"title": "2.2. Model-Free RL", "content": "Model-free RL operates directly through interactions with the external environment, basing its\naction policies on reward information. This methodology utilizes only the agent's experiences\nwithout prior modeling of the environment, making it relatively simple to implement and fast\nto execute. These characteristics could constrain the flexible reasoning and rapid adaptation\nrequired in ARC. Representative algorithms of model-free RL include DQN [13], PPO [14], and\nSAC [15].\nAmong these, the Proximal Policy Optimization (PPO) algorithm is one of the most commonly\nused algorithms in RL, based on the policy gradient method. PPO is designed to achieve stable\nand efficient learning even in complex environments. Its process can be summarized similarly\nto DreamerV3 in the following two steps: 1) perform actions in the environment based on the\ncurrent policy to collect data, and 2) incrementally improve the policy using the policy gradient\ntechnique based on the collected data."}, {"title": "3. Experiments", "content": "We conducted an experiment comparing the ARC task learning of DreamerV3 and PPO, which\nare representative algorithms of model-based RL and model-free RL, respectively, to demonstrate\nthe differences in analogical reasoning capabilities between these methodologies. Through this\nexperiment, we aimed to answer the following research questions (RQs):\nRQ1. Can Model-Based RL Learn a Single Task? (A \u2192 A)\nRQ2. Can Model-Based RL Reason about Tasks Similar to Pre-Trained Task? (A \u2192 A')\nRQ3. Can Model-Based RL Reason about Sub-Tasks of Pre-Trained Task? (AB \u2192 A)\nRQ4. Can Model-Based RL Learn Multiple Tasks Simultaneously? (A, B \u2192 A)\nRQ5. Can Model-Based RL Reason about Merged-Tasks of Pre-Trained Tasks? (A, B \u2192 AB)\nWe scope our paper to provide answers on RQ1 and RQ2. By addressing RQ1 and RQ2 through\nexperiments, we aim to demonstrate the abilities of model-based RL in efficiently learning and\napplying knowledge across similar tasks. The following descriptions are the common setting of\nevery experiment.\nAction The vast action space of ARC is one of the biggest obstacles to the RL [2]. According\nto the ARCLE framework [16], action is defined as a combination of operation and selection;\nwhere operation means the type of action to be performed, and selection determines grids to\nwhich these actions are applied. We force strict restrictions on these operations and selections\nto weaken the challenge posed by the vast action space. In experiments, RL agents can select\nonly five operations: Rotate90, Rotate270, FlipH, FlipV, and Submit [16], and selections are\nalways fixed as the entire grid. These limitations influenced the selection of tasks.\nTask We found tasks from among the 400 training tasks that could be solved with the restricted\noperations and selections of actions, and 7 tasks satisfied this condition. Among these tasks,\nwe selected 4 tasks for our experiment as shown in Fig. 1. First, we selected tasks that need\ndiagonal flipping on a $3 \\times 3$ grid or $N \\times N$ grids. These tasks need the right combination\nof actions (rotate and flip) before submission. Thus, we thought these tasks were good for\nmeasuring analogical reasoning ability. Next, we chose tasks that need to rotate a $3 \\times 3$ grid and\nflip horizontally on $N \\times N$ grids. These tasks need only one action except submit, so analogical\nreasoning ability are not necessary for agents.\nReward We designed the reward based on a sparse reward such that an agent gets a reward of\n1000 if it does the submit action and the state matches the correct grid. Additionally, even if the\nsubmit action is not executed, reaching the correct grid grants an extra reward of 1. Although\nsparse rewards can present challenges in learning tasks with a wide search space, we overcame\nthese difficulties through simple tasks and restricted action spaces. Furthermore, by giving a\nsmall reward for just finding the correct grid, the RL agent can learn the importance of achieving\nthe goal state, and by offering a large reward for the submit action, we prevented the agent\nfrom repeatedly reaching the correct grid without submitting in a single episode."}, {"title": "Metric", "content": "In all experiments, we measured the data efficiency of learning through the accuracy\nrelative to the number of environment steps used during training. We decided to use environment\nsteps to ensure consistency with the experimental settings of DreamerV3 [12], as we wanted\nto monitor data efficiency throughout the learning process. Additionally, we employed the\npass@3 method used in ARC to measure accuracy [2]. The term pass@3 grants three submission\nopportunities per episode, and an episode is considered successful if the correct answer is found\nwithin three attempts. If three incorrect submissions are made or the predetermined episode\nlength (50) is exceeded, the attempt is judged unsuccessful."}, {"title": "Training", "content": "All tasks used in the experiments were trained using the RL environment for ARC,\nARCLE [16]. Each task was originally composed of a few demos and one test input; however,\nthere was a concern that this could lead to overfitting during the learning process. Therefore, we\naugmented the training with 1,000 demos and used an additional 100 test inputs for evaluating\nthe model. Furthermore, when training a single task, the agent was trained over 100,000\nenvironment steps, whereas for fine-tuning a new task on a pre-trained model, the agent was\ntrained for 50,000 environment steps."}, {"title": "3.1. Can Model-Based RL Learn a Single Task? (A \u2192 A)", "content": "The first experiment focused on a single ARC task requiring analogical reasoning, where the\nobjective was to learn the common rules from given demo pairs and apply these rules effectively\nto a test input. This experiment involved training on four ARC tasks, each illustrated in Fig. 1,\nusing the DreamerV3 and PPO algorithms. Tasks requiring multiple actions to solve, such\nas Diagonal Flipping, demand analogical reasoning, whereas tasks that can be solved with a\nsingle action, such as CCW Rotation and Horizontal Flipping, are less complex. DreamerV3,\na model-based RL algorithm, exhibited superior performance in Diagonal Flip tasks, which\ninvolve more complex reasoning. In contrast, PPO, recognized for its stable learning capabilities,\nperformed comparably or even better in simpler tasks.\nIt is noteworthy that DreamerV3 achieved 100% performance on uniformly sized 3 \u00d7 3 tasks\nbut only managed about 40% on tasks with varying sizes. Conversely, PPO's performance\nremained consistent regardless of grid size and was influenced solely by task difficulty. This\nsuggests a performance drop in DreamerV3 during the encoding processes.\nA common interval of zero performance was observed in DreamerV3's training across various\ntasks, followed by rapid performance improvement. This pattern suggests that DreamerV3\nis not merely learning simple problem patterns but is developing a deeper understanding of\nconcepts essential for analogical reasoning. Initially, DreamerV3 demonstrated about 40%\neffectiveness due to learning straightforward patterns. Later, the agent recognized that this\napproach was insufficient for fully solving tasks, prompting it to explore and learn from various\ntrials. Consequently, DreamerV3 displayed significant performance improvements in later\nlearning stages, outperforming earlier results. These observations imply that DreamerV3's latent\nrepresentations capture conceptual content that enhances learning efficiency and illustrates the\npotential for analogical reasoning within model-based RL frameworks."}, {"title": "3.2. Can Model-Based RL Reason Tasks Similar to Pre-Trained Task? (A \u2192 A')", "content": "The second experiment evaluated the learning efficiency of an agent when facing a task\nsimilar to one it was previously trained on. For instance, in Fig. 3a, we observed the performance\nof diagonal flip tasks across various grid sizes for agents pre-trained on a 3 \u00d7 3 diagonal flip\ntask. Conversely, in Fig. 3b, we reversed the roles of the pre-trained and adaptation tasks to\nevaluate both agents' performances.\nIn both scenarios, PPO consistently displayed a performance level of around 20%. This\noutcome was anticipated, considering that the agent's performance on pre-trained tasks had\nsimilarly been around 20%, as evidenced in previous experiments (Fig. 2a and Fig. 2b). The\ninsufficient performance of the pre-trained model likely inhibited any positive influence on the\nadaptation process, resulting in no notable performance gains compared to the non-pre-trained\nscenario.\nAn interesting finding from the DreamerV3 results was the direct impact of the pre-trained\nmodel's performance on adaptation. For example, while the model without pre-training achieved\nabout 40% performance on the same task, the pre-trained model approached 100%, mirroring the\n3 \u00d7 3 Diagonal Flip task's success previously encountered. This pattern was further supported\nby the performance contrasts observed between Fig. 2a and Fig. 3b. Here, a task that once\nachieved 100% performance dropped to around 40% after pre-training, reflecting the outcomes\nseen with the N \u00d7 N Diagonal Flip tasks. These outcomes illustrate that DreamerV3 tailored\nits problem-solving strategies based on insights drawn from pre-trained tasks, suggesting that\nbetter results could have been obtained if adaptation in Fig. 3b had also been initiated from a\nhigher-performing pre-trained model.\nTowards the latter part of training in Fig. 3a, a sudden decline in performance was noted.\nWhile the exact cause of this drop cannot be definitively established, we hypothesize that it\nmight be due to the same type of interval observed in previous results (Fig. 2). Although the\nexperiment's step limit prevented precise verification, it seems probable that the DreamerV3\nagent was undergoing an interval similar to earlier tests, attempting to learn new concepts.\nFurther experimentation beyond 50,000 steps and additional theoretical analysis are required to\nexplore this phenomenon more deeply, which could further substantiate DreamerV3's capability\nfor analogical reasoning."}, {"title": "4. Discussion", "content": "Restriction of Action Space Our study examined the analogical reasoning abilities of\nreinforcement learning (RL) algorithms and their ability to apply learned concepts. In the\nARCLE, an agent's actions consist of operation and selection, requiring the policy to adeptly\nhandle both components. However, the challenge of learning the selection process significantly\nincreases the complexity of the tasks. To mitigate this, we simplified the action space by\nproviding selections through an oracle in this experiment. This adjustment allowed the agents\nto focus more on mastering operations without the added difficulty of selection, streamlining the\nlearning process. Future research should explore the possibility of incorporating the selection\nprocess more autonomously within the reasoning tasks. This would test the agent's ability\nto handle the complete process of decision-making, potentially leading to more analogical\nreasoning ability for agents."}, {"title": "Restriction of Tasks", "content": "This paper focused on learning simple tasks involving just one or two\noperations, such as diagonal flipping tasks, a Rotating task, and a horizontal flipping task. Future\nresearch should explore additional challenges such as the Rotate and the Flip task, which could\nprovide valuable insights into the model's ability to generalize and reason about sub-components\nof learned tasks. It is also crucial to assess the efficiency of learning multiple related actions\n(multi-tasking) simultaneously. To overcome this challenge, previously learned knowledge must\nbe retained while new knowledge is acquired. In other words, it is essential to be robust against\ncatastrophic forgetting. Should such an agent be developed, two types of experiments will be\nessential: 1) The agent should be trained on a large and diverse set of tasks to verify the absence\nof catastrophic forgetting. 2) The agent should be trained on a wide range of tasks with varying\ndifficulties, followed by an evaluation of its capability to adapt to untrained tasks."}, {"title": "Applying Meta-Learning", "content": "In addressing the ARC tasks, the ability to apply learned concepts\nto entirely new contexts is essential. Meta-learning, or 'learning to learn,' can be considered to\nadapt quickly to new tasks. Future studies should focus on developing meta-learning frameworks\nthat can efficiently abstract underlying task structures, allowing for rapid generalization across\nvarying problem settings. A deeper exploration into meta-learning could utilize frameworks such\nas Model-Agnostic Meta-Learning (MAML) [17] which has shown promise in various domains.\nImplementing and refining meta-learning techniques like MAML could lead to breakthroughs\nin developing AI models that can learn not only untrained tasks more efficiently but also their\nknowledge to solve untrained tasks."}, {"title": "Applying Transfer Learning", "content": "Transfer learning can significantly enhance the model's ability\nto utilize knowledge acquired in one context and apply it to different yet related tasks. This\nprocess often involves the use of deep neural networks, specifically their ability to approximate\nand adapt policies through layers that capture generalizable features, which are crucial for the\nsuccessful transfer of knowledge across tasks within the same domain or between different\ndomains [18]. Conducting such studies would provide deeper insights into the flexible applica-\ntion of acquired knowledge, a critical aspect of analogical reasoning. These investigations will\npave the way for creating adaptable and efficient learning systems that thrive in dynamically\nchanging environments."}, {"title": "Theoretical Analysis of Model-Based RL", "content": "Finally, our findings indicate that model-based\nRL can facilitate more efficient learning in analogical reasoning tasks compared to model-free\napproaches. The inherent capabilities of model-based methods to infer and generalize from\nlimited data were particularly beneficial. However, our study also highlights the need for further\ninvestigation into the specific mechanisms through which these models store and retrieve\ntask-specific patterns. A deeper understanding of these processes could inform the development\nof more robust model-based systems, enhancing their capability to handle a wider array of\ncomplex reasoning tasks."}, {"title": "5. Related Works", "content": "The winner of the ARC Challenge [19] utilized a total of 142 domain-specific languages (DSLs)\nto combine various transformed images [20]. The ARC Challenge later expanded into AR-\nCathon [21], and the ARCathon 2022 winner's approach involved exploring using 166 DSLs\nthat included more complex and diverse features [22]. This attempted to challenge the broad\ngeneralization of DSLs; however, it also introduced limitations due to the inclusion of very\ninfrequently used features, making the DSLs too complex. Compared to humans can solve about\n80% of ARC evaluation tasks [23], DSL-based search algorithms have shown performances\naround 30-40% [4, 21]. However, these approaches, being fundamentally simple search-based,\nhave inherent weaknesses in complex tasks and are prone to overfitting to specific tasks defined\nby artificially designed DSLs, making them difficult to apply or generalize to other tasks. This\nfundamental limitation may render them unsuitable for analogical reasoning.\nSubsequent studies have attempted to solve tasks using neural networks. Some research ef-\nforts were based on program synthesis, progressively recognizing increasingly complex patterns,\nwhile other studies have tried to learn the complexities of ARC through the computational abili-\nties of Large Language Models (LLMs). However, studies without DSLs have shown relatively\nlow performance. Even the most recent studies based on program synthesis [24] or LLMs [25]\nhave only achieved performances of 6.5-6.75% on untrained tasks [26]. Recently, there was\ninteresting research utilizing human-solving processes [27], but this study did not contain the\nperformance on untrained tasks. The low performance of these studies is attributed to the\nfollowing reasons: In the case of program synthesis, the method of combining all possessed\nknowledge to acquire new knowledge leads to poor learning efficiency [28], and LLMs are\nknown to be weak in incremental reasoning [29]. These limitations can be particularly harmful\nin evaluation tasks, which are known to be comparatively more complex and difficult than\ntraining tasks [4]. Additionally, the research utilizing human-solving processes has a risk to\nuntrained tasks due to their reliance on offline learning from given solutions [30].\nEnsemble-based research that combines the previous approaches has also been announced [4,\n26]. These methods were expected to compensate for the weaknesses of existing approaches\nthrough the ensemble. An ensemble-based study combining program synthesis and LLMs\nshowed a performance of 14.75% [26], and the approach of the ARCathon 2023 winner exhibited\nabout 33% performance, significantly surpassing previous studies. However, the performance of\nensemble-based research that combined DSL, program synthesis, and LLMs was only 40.25% [4],\nwhich is just 0.25% higher than the performance of a baseline that used only DSL. This result\nmeans that just one more task out of 400 evaluation tasks was solved. Ultimately, the experimen-\ntal results of these ensemble-based studies further highlighted the limitation that the presence\nof program synthesis and LLMs does not significantly impact performance compared to DSL.\nMeanwhile, there are some attempts to apply reinforcement learning (RL) algorithms in a\nlimited way based on program synthesis and LLMs methods [26, 31]. The limited application of\nRL algorithms can be attributed to factors such as the absence of appropriate rewards, high-\ndimensional complex states, and extensive search spaces, but fundamentally, the absence of\nan environment for ARC was the main reason. However, an environment for training ARC\ntasks [16] has been developed recently. Furthermore, this research demonstrated successful\ntraining of an ARC task using PPO, suggesting the potential applicability of various RL models."}, {"title": "6. Conclusion", "content": "In this paper, we compared analogical reasoning abilities between model-based and model-free\nRL, using a subset of ARC to demonstrate the effectiveness of model-based RL. This approach has\nshown superior learning efficiency and adaptability in tasks compared to model-free methods\nlike PPO. Specifically, DreamerV3 outperformed in tasks similar to those it had previously\nsolved, displaying a remarkable ability to generalize across various task dimensions.\nThe observed performance dips followed by rapid recoveries during DreamerV3's training are\nthought to represent periods of conceptual consolidation. These intervals align with theoretical\nexpectations of analogical reasoning in AI systems and highlight DreamerV3's potential to\nmanage complex cognitive tasks. Future research should further explore these learning dynamics\nto better understand the mechanisms that enable such advanced reasoning. This will help\nenhance RL agents capable of analogical reasoning and adaptation in dynamic environments,\nreducing the gap between the reasoning ability of humans and RL agents."}]}