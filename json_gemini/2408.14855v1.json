{"title": "Enhancing Analogical Reasoning in the Abstraction and Reasoning Corpus via Model-Based RL", "authors": ["Jihwan Lee", "Woochang Sim", "Sejin Kim", "Sundong Kim"], "abstract": "This paper demonstrates that model-based reinforcement learning (model-based RL) is a suitable approach for the task of analogical reasoning. We hypothesize that model-based RL can solve analogical reasoning tasks more efficiently through the creation of internal models. To test this, we compared DreamerV3, a model-based RL method, with Proximal Policy Optimization, a model-free RL method, on the Abstraction and Reasoning Corpus (ARC) tasks. Our results indicate that model-based RL not only outperforms model-free RL in learning and generalizing from single tasks but also shows significant advantages in reasoning across similar tasks.", "sections": [{"title": "1. Introduction", "content": "The Abstraction and Reasoning Corpus (ARC) is known as a benchmark for evaluating abstraction and reasoning abilities [1]. Examples of ARC tasks can be seen in Fig. 1. To solve an ARC task, one needs the ability to abstract a common rule among given demos and apply this rule to a new test input grid to infer the corresponding grid [2]. Additionally, to solve untrained tasks, one must have the ability to discover various knowledge during the learning process and selectively apply this knowledge in the task-solving process [2].\nThis study interprets the two abilities required by ARC not as separate skills but as one concept of analogical reasoning. Analogical reasoning refers to the process of understanding new situations or solving problems based on existing knowledge that can be applied to different contexts or problems [3]. First, the process of solving an ARC task can be seen as solving a new problem (test input grid) based on the transformations from input grids to output grids in each demo (existing knowledge), which exactly fits the definition described above. Second, for a model trained on various ARC tasks to solve a new ARC task, it must understand the demos of the new ARC task (new situations) based on the solution processes (existing knowledge) applicable to the training tasks (different problems) and solve the task, which also aligns with the concept of analogical reasoning. Third, since the evaluation tasks in ARC are typically more complex and challenging than the training tasks [4], the ability to infer new problem-solving processes based on learned knowledge through analogical reasoning can enhance the performance on ARC.\nIn this study, noting the critical importance of analogical reasoning capabilities in solving ARC, we conducted reinforcement learning (RL) for training ARC tasks. Learning ARC with RL can offer the following potential benefits from an analogical reasoning perspective. First, RL can enhance its learning strategies through reward-based learning. In the process of learning ARC tasks, RL agents learn to maximize rewards across various tasks, thereby developing analogical reasoning capabilities. Second, RL can learn policies that effectively operate in new, yet similar environments based on experience. During the learning of various ARC tasks, RL agents can learn to apply rules learned in one scenario to another, a key element of analogical reasoning. Third, RL allows agents to produce their own data, enabling effective learning even without high-quality given data. In tasks like ARC, where data may be limited, RL agents can quickly infer patterns from a few demos and learn to apply these to the new input.\nThis study compared model-based RL and model-free RL to evaluate the efficiency of acquiring analogical reasoning abilities. The key difference between these two methodologies lies in the presence or absence of an internal environmental model. Model-based RL builds an internal model of the environment based on the agent's experiences and learns policies through predictable scenarios, which has the potential to significantly improve decision-making in complex situations. On the other hand, model-free RL learns to optimize rewards through direct interaction with the environment, allowing for the development of effective action policies without constructing an environmental model.\nThis study hypothesized that the internal model of model-based RL could aid in analogical reasoning. To test this hypothesis, we observed the learning processes of ARC tasks using DreamerV3 [5], a representative model-based RL algorithm, and Proximal Policy Optimization (PPO) [6], a general model-free RL algorithm, demonstrating the potential for RL agents to acquire analogical reasoning. Additionally, we evaluated the performance and learning efficiency of both algorithms by fine-tuning them on tasks similar to the pre-trained tasks and comparing the analogical reasoning capabilities they acquired."}, {"title": "2. Model-Based RL and Model-Free RL", "content": "RL has two main categories: model-based RL and model-free RL. Before conducting experiments to compare these categories from the perspective of abstraction reasoning, this paper briefly reviews the characteristics of each category and compares the algorithms that will be used in this experiment."}, {"title": "2.1. Model-Based RL", "content": "Model-based RL builds an internal model of the environment to predict various possibilities and establish more efficient policies based on these predictions. In problems requiring abstract thinking and pattern recognition like ARC, the model-based RL approach provides the ability to generalize to new situations that the agent has not experienced through its internal model. Representative algorithms utilizing the model-based RL approach include Dyna-Q [7], I2A [8], MBMF [9], and World Model [10]. Also, AlphaZero [11], which successfully surpassed human performance in board games such as Go, Chess, and Shogi based on Monte Carlo Tree Search (MCTS), is the most well-known. Recently, DreamerV3 [12], based on the World Model, has been the subject of extensive research.\nA key feature of DreamerV3 is its ability to extract important features from input data and convert these into a latent representation, which is then used to learn a predictive model of the environment's dynamics. This model functions by taking the current state and actions as inputs and predicting the next state's representation and rewards. This approach allows for the simulation of future scenarios and the evaluation of various scenarios. In summary, DreamerV3 operates by repeating the following three processes: 1) collect data based on a random policy or the current policy, 2) update the agent's model based on the collected data, and 3) generate and evaluate virtual future scenarios using the updated model to improve the policy. This process excels in multi-step tasks like mining diamonds in Minecraft. We chose DreamerV3 for its superior handling of ARC, compared to other model-based approaches."}, {"title": "2.2. Model-Free RL", "content": "Model-free RL operates directly through interactions with the external environment, basing its action policies on reward information. This methodology utilizes only the agent's experiences without prior modeling of the environment, making it relatively simple to implement and fast to execute. These characteristics could constrain the flexible reasoning and rapid adaptation required in ARC. Representative algorithms of model-free RL include DQN [13], PPO [14], and SAC [15].\nAmong these, the Proximal Policy Optimization (PPO) algorithm is one of the most commonly used algorithms in RL, based on the policy gradient method. PPO is designed to achieve stable and efficient learning even in complex environments. Its process can be summarized similarly to DreamerV3 in the following two steps: 1) perform actions in the environment based on the current policy to collect data, and 2) incrementally improve the policy using the policy gradient technique based on the collected data."}, {"title": "3. Experiments", "content": "We conducted an experiment comparing the ARC task learning of DreamerV3 and PPO, which are representative algorithms of model-based RL and model-free RL, respectively, to demonstrate the differences in analogical reasoning capabilities between these methodologies. Through this experiment, we aimed to answer the following research questions (RQs):\nRQ1. Can Model-Based RL Learn a Single Task? (A \u2192 A)\nRQ2. Can Model-Based RL Reason about Tasks Similar to Pre-Trained Task? (A \u2192 A')\nRQ3. Can Model-Based RL Reason about Sub-Tasks of Pre-Trained Task? (AB \u2192 A)\nRQ4. Can Model-Based RL Learn Multiple Tasks Simultaneously? (A, B \u2192 A)\nRQ5. Can Model-Based RL Reason about Merged-Tasks of Pre-Trained Tasks? (A, B \u2192 AB)\nWe scope our paper to provide answers on RQ1 and RQ2. By addressing RQ1 and RQ2 through experiments, we aim to demonstrate the abilities of model-based RL in efficiently learning and applying knowledge across similar tasks. The following descriptions are the common setting of every experiment.\nAction The vast action space of ARC is one of the biggest obstacles to the RL [2]. According to the ARCLE framework [16], action is defined as a combination of operation and selection; where operation means the type of action to be performed, and selection determines grids to which these actions are applied. We force strict restrictions on these operations and selections to weaken the challenge posed by the vast action space. In experiments, RL agents can select only five operations: Rotate90, Rotate270, FlipH, FlipV, and Submit [16], and selections are always fixed as the entire grid. These limitations influenced the selection of tasks.\nTask We found tasks from among the 400 training tasks that could be solved with the restricted operations and selections of actions, and 7 tasks satisfied this condition. Among these tasks, we selected 4 tasks for our experiment as shown in Fig. 1. First, we selected tasks that need diagonal flipping on a 3 \u00d7 3 grid or N \u00d7 N grids. These tasks need the right combination of actions (rotate and flip) before submission. Thus, we thought these tasks were good for measuring analogical reasoning ability. Next, we chose tasks that need to rotate a 3 \u00d7 3 grid and flip horizontally on N \u00d7 N grids. These tasks need only one action except submit, so analogical reasoning ability are not necessary for agents.\nReward We designed the reward based on a sparse reward such that an agent gets a reward of 1000 if it does the submit action and the state matches the correct grid. Additionally, even if the submit action is not executed, reaching the correct grid grants an extra reward of 1. Although sparse rewards can present challenges in learning tasks with a wide search space, we overcame these difficulties through simple tasks and restricted action spaces. Furthermore, by giving a small reward for just finding the correct grid, the RL agent can learn the importance of achieving the goal state, and by offering a large reward for the submit action, we prevented the agent from repeatedly reaching the correct grid without submitting in a single episode.\nMetric In all experiments, we measured the data efficiency of learning through the accuracy relative to the number of environment steps used during training. We decided to use environment steps to ensure consistency with the experimental settings of DreamerV3 [12], as we wanted to monitor data efficiency throughout the learning process. Additionally, we employed the pass@3 method used in ARC to measure accuracy [2]. The term pass@3 grants three submission opportunities per episode, and an episode is considered successful if the correct answer is found within three attempts. If three incorrect submissions are made or the predetermined episode length (50) is exceeded, the attempt is judged unsuccessful.\nTraining All tasks used in the experiments were trained using the RL environment for ARC, ARCLE [16]. Each task was originally composed of a few demos and one test input; however, there was a concern that this could lead to overfitting during the learning process. Therefore, we augmented the training with 1,000 demos and used an additional 100 test inputs for evaluating the model. Furthermore, when training a single task, the agent was trained over 100,000 environment steps, whereas for fine-tuning a new task on a pre-trained model, the agent was trained for 50,000 environment steps."}, {"title": "3.1. Can Model-Based RL Learn a Single Task? (A \u2192 A)", "content": ""}, {"title": "3.2. Can Model-Based RL Reason Tasks Similar to Pre-Trained Task? (A \u2192 A')", "content": "The first experiment focused on a single ARC task requiring analogical reasoning, where the objective was to learn the common rules from given demo pairs and apply these rules effectively to a test input. This experiment involved training on four ARC tasks, each illustrated in Fig. 1, using the DreamerV3 and PPO algorithms. Tasks requiring multiple actions to solve, such as Diagonal Flipping, demand analogical reasoning, whereas tasks that can be solved with a single action, such as CCW Rotation and Horizontal Flipping, are less complex. DreamerV3, a model-based RL algorithm, exhibited superior performance in Diagonal Flip tasks, which involve more complex reasoning. In contrast, PPO, recognized for its stable learning capabilities, performed comparably or even better in simpler tasks.\nIt is noteworthy that DreamerV3 achieved 100% performance on uniformly sized 3 \u00d7 3 tasks but only managed about 40% on tasks with varying sizes. Conversely, PPO's performance remained consistent regardless of grid size and was influenced solely by task difficulty. This suggests a performance drop in DreamerV3 during the encoding processes.\nA common interval of zero performance was observed in DreamerV3's training across various tasks, followed by rapid performance improvement. This pattern suggests that DreamerV3 is not merely learning simple problem patterns but is developing a deeper understanding of concepts essential for analogical reasoning. Initially, DreamerV3 demonstrated about 40% effectiveness due to learning straightforward patterns. Later, the agent recognized that this approach was insufficient for fully solving tasks, prompting it to explore and learn from various trials. Consequently, DreamerV3 displayed significant performance improvements in later learning stages, outperforming earlier results. These observations imply that DreamerV3's latent representations capture conceptual content that enhances learning efficiency and illustrates the potential for analogical reasoning within model-based RL frameworks."}, {"title": "4. Discussion", "content": "Restriction of Action Space Our study examined the analogical reasoning abilities of reinforcement learning (RL) algorithms and their ability to apply learned concepts. In the ARCLE, an agent's actions consist of operation and selection, requiring the policy to adeptly handle both components. However, the challenge of learning the selection process significantly increases the complexity of the tasks. To mitigate this, we simplified the action space by providing selections through an oracle in this experiment. This adjustment allowed the agents to focus more on mastering operations without the added difficulty of selection, streamlining the learning process. Future research should explore the possibility of incorporating the selection process more autonomously within the reasoning tasks. This would test the agent's ability to handle the complete process of decision-making, potentially leading to more analogical reasoning ability for agents.\nRestriction of Tasks This paper focused on learning simple tasks involving just one or two operations, such as diagonal flipping tasks, a Rotating task, and a horizontal flipping task. Future research should explore additional challenges such as the Rotate and the Flip task, which could provide valuable insights into the model's ability to generalize and reason about sub-components of learned tasks. It is also crucial to assess the efficiency of learning multiple related actions (multi-tasking) simultaneously. To overcome this challenge, previously learned knowledge must be retained while new knowledge is acquired. In other words, it is essential to be robust against catastrophic forgetting. Should such an agent be developed, two types of experiments will be essential: 1) The agent should be trained on a large and diverse set of tasks to verify the absence of catastrophic forgetting. 2) The agent should be trained on a wide range of tasks with varying difficulties, followed by an evaluation of its capability to adapt to untrained tasks.\nApplying Meta-Learning In addressing the ARC tasks, the ability to apply learned concepts to entirely new contexts is essential. Meta-learning, or 'learning to learn,' can be considered to adapt quickly to new tasks. Future studies should focus on developing meta-learning frameworks that can efficiently abstract underlying task structures, allowing for rapid generalization across varying problem settings. A deeper exploration into meta-learning could utilize frameworks such as Model-Agnostic Meta-Learning (MAML) [17] which has shown promise in various domains. Implementing and refining meta-learning techniques like MAML could lead to breakthroughs in developing AI models that can learn not only untrained tasks more efficiently but also their knowledge to solve untrained tasks.\nApplying Transfer Learning Transfer learning can significantly enhance the model's ability to utilize knowledge acquired in one context and apply it to different yet related tasks. This process often involves the use of deep neural networks, specifically their ability to approximate and adapt policies through layers that capture generalizable features, which are crucial for the successful transfer of knowledge across tasks within the same domain or between different domains [18]. Conducting such studies would provide deeper insights into the flexible application of acquired knowledge, a critical aspect of analogical reasoning. These investigations will pave the way for creating adaptable and efficient learning systems that thrive in dynamically changing environments.\nTheoretical Analysis of Model-Based RL Finally, our findings indicate that model-based RL can facilitate more efficient learning in analogical reasoning tasks compared to model-free approaches. The inherent capabilities of model-based methods to infer and generalize from limited data were particularly beneficial. However, our study also highlights the need for further investigation into the specific mechanisms through which these models store and retrieve task-specific patterns. A deeper understanding of these processes could inform the development of more robust model-based systems, enhancing their capability to handle a wider array of complex reasoning tasks."}, {"title": "5. Related Works", "content": "The winner of the ARC Challenge [19] utilized a total of 142 domain-specific languages (DSLs) to combine various transformed images [20]. The ARC Challenge later expanded into ARCathon [21], and the ARCathon 2022 winner's approach involved exploring using 166 DSLs that included more complex and diverse features [22]. This attempted to challenge the broad generalization of DSLs; however, it also introduced limitations due to the inclusion of very infrequently used features, making the DSLs too complex. Compared to humans can solve about 80% of ARC evaluation tasks [23], DSL-based search algorithms have shown performances around 30-40% [4, 21]. However, these approaches, being fundamentally simple search-based, have inherent weaknesses in complex tasks and are prone to overfitting to specific tasks defined by artificially designed DSLs, making them difficult to apply or generalize to other tasks. This fundamental limitation may render them unsuitable for analogical reasoning.\nSubsequent studies have attempted to solve tasks using neural networks. Some research efforts were based on program synthesis, progressively recognizing increasingly complex patterns, while other studies have tried to learn the complexities of ARC through the computational abilities of Large Language Models (LLMs). However, studies without DSLs have shown relatively low performance. Even the most recent studies based on program synthesis [24] or LLMs [25] have only achieved performances of 6.5-6.75% on untrained tasks [26]. Recently, there was interesting research utilizing human-solving processes [27], but this study did not contain the performance on untrained tasks. The low performance of these studies is attributed to the following reasons: In the case of program synthesis, the method of combining all possessed knowledge to acquire new knowledge leads to poor learning efficiency [28], and LLMs are known to be weak in incremental reasoning [29]. These limitations can be particularly harmful in evaluation tasks, which are known to be comparatively more complex and difficult than training tasks [4]. Additionally, the research utilizing human-solving processes has a risk to untrained tasks due to their reliance on offline learning from given solutions [30].\nEnsemble-based research that combines the previous approaches has also been announced [4, 26]. These methods were expected to compensate for the weaknesses of existing approaches through the ensemble. An ensemble-based study combining program synthesis and LLMs showed a performance of 14.75% [26], and the approach of the ARCathon 2023 winner exhibited about 33% performance, significantly surpassing previous studies. However, the performance of ensemble-based research that combined DSL, program synthesis, and LLMs was only 40.25% [4], which is just 0.25% higher than the performance of a baseline that used only DSL. This result means that just one more task out of 400 evaluation tasks was solved. Ultimately, the experimental results of these ensemble-based studies further highlighted the limitation that the presence of program synthesis and LLMs does not significantly impact performance compared to DSL. Meanwhile, there are some attempts to apply reinforcement learning (RL) algorithms in a limited way based on program synthesis and LLMs methods [26, 31]. The limited application of RL algorithms can be attributed to factors such as the absence of appropriate rewards, high-dimensional complex states, and extensive search spaces, but fundamentally, the absence of an environment for ARC was the main reason. However, an environment for training ARC tasks [16] has been developed recently. Furthermore, this research demonstrated successful training of an ARC task using PPO, suggesting the potential applicability of various RL models."}, {"title": "6. Conclusion", "content": "In this paper, we compared analogical reasoning abilities between model-based and model-free RL, using a subset of ARC to demonstrate the effectiveness of model-based RL. This approach has shown superior learning efficiency and adaptability in tasks compared to model-free methods like PPO. Specifically, DreamerV3 outperformed in tasks similar to those it had previously solved, displaying a remarkable ability to generalize across various task dimensions.\nThe observed performance dips followed by rapid recoveries during DreamerV3's training are thought to represent periods of conceptual consolidation. These intervals align with theoretical expectations of analogical reasoning in AI systems and highlight DreamerV3's potential to manage complex cognitive tasks. Future research should further explore these learning dynamics to better understand the mechanisms that enable such advanced reasoning. This will help enhance RL agents capable of analogical reasoning and adaptation in dynamic environments, reducing the gap between the reasoning ability of humans and RL agents."}]}