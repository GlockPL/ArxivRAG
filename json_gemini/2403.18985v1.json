{"title": "Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal\nClassification with Reinforcement Learning", "authors": ["Soumyendu Sarkar", "Ashwin Ramesh Babu", "Sajad Mousavi", "Vineet Gundecha", "Avisek Naug", "Sahand Ghorbanpour"], "abstract": "We present a generic Reinforcement Learning (RL) frame-\nwork optimized for crafting adversarial attacks on different\nmodel types spanning from ECG signal analysis (1D), im-\nage classification (2D), and video classification (3D). The\nframework focuses on identifying sensitive regions and in-\nducing misclassifications with minimal distortions and vari-\nous distortion types. The novel RL method outperforms state-\nof-the-art methods for all three applications, proving its ef-\nficiency. Our RL approach produces superior localization\nmasks, enhancing interpretability for image classification and\nECG analysis models. For applications such as ECG analysis,\nour platform highlights critical ECG segments for clinicians\nwhile ensuring resilience against prevalent distortions. This\ncomprehensive tool aims to bolster both resilience with ad-\nversarial training and transparency across varied applications\nand data types.", "sections": [{"title": "Introduction", "content": "Deep learning models, despite their prowess, are vulnera-\nble to input data corruption, posing challenges in safety-\ncritical applications like self-driving cars and facial recogni-\ntion. Black-box attacks generally work with limited model\ninformation but tend to be inefficient, relying heavily on\nhand-crafted heuristics (2; 1; 23). There has been sub-\nstantial progress in Reinforcement Learning across vari-\nous fields, from complex control systems to training LLMs\n(19; 3; 6; 14; 12; 15; 16; 13; 18; 17; 9; 21; 4). These smart\nagents can navigate through various environments and take a\nsequence of actions to converge on the goal with a long-term\nview, which makes them very powerful. Addressing these\nissues, we introduce a Reinforcement Learning agent for\na Platform (RLAB) capable of efficient adversarial attacks.\nThis agent employs a \"Bring Your Own Filter\" (BYOF) ap-\nproach (figure 1) and utilizes a dual-action mechanism to\nmanipulate image distortions, with the aim of high success\nrates with fewer queries. For each application, we evalu-\nated the performance of the proposed frameworks with var-\nious models and data sets to show the reliability of our\nmethod. We consider three types of metrics to evaluate per-\nformance, and our results show that the proposed reinforce-\nment learning-based attack strategy generates superior re-\nsults in all three applications compared to the state-of-the-art\napproaches. The main contributions of this work are:\n\u2022 A common attack framework that spans multiple di-\nmensions, from 1D ECG signals to 2D images, and\n2+D videos\n\u2022 Reinforcement Learning-based Adversarial Attack with\nmultiple custom distortion types to measure the lowest\ndistortion needed for misclassification as a metric for ro-\nbustness and resiliency.\n\u2022 Visual explanation in the form of localization and heat\nmap derived from RL attack agent.\n\u2022 Adversarial training to enhance robustness."}, {"title": "Proposed Method", "content": "Problem Formulation\nA trained Deep Neural Network (DNN) model under eval-\nuation can be represented as y = argmaxf (x; \u03b8). Our ap-\nproach generates perturbation 6 such that, y/= f(x + \u03b4; \u03b8).\nThe distance between the original and the adversarial sam-\nple, D(x, x + 6) will be any function of the lp norms. The\nobjective is to fool the classifier while keeping D to a mini-\nmum.\nRobustness Evaluation\nThe input data are divided into fixed size patches of size n\nfor 1D, n x n for 2d data, t\u00d7n\u00d7n for 3D data where t\nrepresents the temporal dimension. For every step, the RL\nagent decides to take two actions,\n1. Patches to which distortions are added"}, {"title": "", "content": "2. patches from which distortions are removed\nThis process is done iteratively until the model misclassi-\nfies the data or until the budget for the number of maximum\nallowed steps is reached. This loop is represented as the \"ro-\nbustness evaluation\" block in the figure 2. The intuition\nbehind having two actions (addition and removal) is inspired\nby the application of reinforcement learning for board games\nwhere the most effective moves or actions can be determined\nusing methods such as Deep Tree Search (DTS) (22). Unlike\nboard games, there is a possibility to reset earlier actions that\nwere taken in the past that proved to be less effective, reduc-\ning the computational complexity from O(Nd) to O(N).\nHere, N represents the computational complexity of one\nlevel of evaluation and corresponds to the size of the data,\nand d represents the depth of the tree search which translates\nto the number of actions taken ahead of time. The generated\nadversarial samples are further used to fine-tune the victim\nmodel to improve robustness which is represented in the fig-\nure 2 as \"model refinement to enhance robustness\".\nBring Your Own Filter\nThe RLAB platform is extremely versatile with any type of\ndistortion of choice. The RL algorithm learns a policy to\nadapt to the filter used such that the adversarial samples are\ngenerated with minimum distortion D. Furthermore, the al-\ngorithm can be used with a mixture of filters such that the\nagent first decides which filter to use for every step and fur-\nther determines the patches to which the distortion should\nbe added. We experimented with four naturally occurring\ndistortions (Gaussian Noise, Gaussian blur, dead pixel, and\nilluminate).\nExplainability\nThe RL agent has been trained to add distortion to the most\nsensitive region of the data such that the misclassification\ncan be introduced with a minimum number of steps. This ap-\nproach has encouraged the agent to add distortion to the re-\ngion of the data that corresponds to the predicted class. This\ncreates an accurate localization of the objects/peaks in the\nscene/signal, which is represented in figure 2 as \"explain-\nability\"."}, {"title": "Results and Discussion", "content": "For all three applications (ECG analysis, Image Classifica-\ntion, Video Classification), we use three evaluation metrics,\naverage success rate, number of queries, and the 12, linf,\nto measure the effectiveness of the attack framework. For\nall applications, we have evaluated more than 1 dataset and\nmore than three different victim models to assess the effec-\ntiveness of the proposed framework. Results prove that the\nRL agent could generate an \"average success rate\" of 100\npercent most of the time with a much smaller query bud-\nget when compared to the competitors (8; 10; 11; 16). Fur-\nthermore, the proposed framework could maintain the \"av-\nerage number of queries\u201d lower than the competitors for all\nthree applications. Also, the effectiveness of localization is\nevaluated with metrics such as dice coefficient and IOU and\ncompared with the popular gradient and non-gradient-based\napproaches (20; 5; 7), with the proposed method showing\nsuperiority over the other approaches. Also, retraining the\nmodel with adversarial samples significantly improved ro-\nbustness when evaluated on benchmark datasets (10)."}, {"title": "Conclusion", "content": "The proposed reinforcement learning-based attack frame-\nwork is effective causing misclassification for many appli-\ncations with different data dimensions, showing its ability\nto generalize for different data dimensions. The approach is\ncapable of using any distortion types that suit the use case to\ngenerate meaningful adversarial samples. Furthermore, the\nvisual explanations generated by the RL agents provide in-\nsights into the decisions of the AI models. The framework is\ncurrently being evaluated for LLMs."}]}