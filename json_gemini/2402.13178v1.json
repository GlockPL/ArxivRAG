{"title": "Benchmarking Retrieval-Augmented Generation for Medicine", "authors": ["Guangzhi Xiong", "Qiao Jin", "Zhiyong Lu", "Aidong Zhang"], "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MEDRAG toolkit introduced in this work. Overall, MEDRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the \"lost-in-the-middle\" effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized the way people seek information online, from searching to directly asking chatbots for answers. Although recent studies have shown their state-of-the-art capabilities of question answering (QA) in both general and medical domains (OpenAI et al., 2023; Anil et al., 2023; Touvron et al., 2023b; Singhal et al., 2023a; Nori et al., 2023a), LLMs often generate plausible-sounding but factually incorrect responses, commonly known as hallucination (Ji et al., 2023). Additionally, the training corpora of LLMs might not include the latest knowledge, such as recent updates of clinical guidelines. These issues can be especially dangerous in high-stakes domains such as healthcare (Tian et al., 2024).\nBy providing LLMs with relevant documents retrieved from up-to-date and trustworthy collections, Retrieval-Augmented Generation (RAG) has the potential to address the above challenges (Lewis et al., 2020; Gao et al., 2023). RAG also improves the transparency of LLMs by grounding their reasoning on the retrieved documents. As such, RAG has already been quickly implemented in various scientific and clinical QA systems (L\u00e1la et al., 2023; Zakka et al., 2024). However, a complete RAG system contains several flexible modules, such as document collections (corpora), retrieval algorithms (retrievers), and backbone LLMs, but the best practices for tuning these components are still unclear, hindering their optimal adoption in medicine.\nTo systematically evaluate how different components in RAG affect its performance, we first compile an evaluation benchmark termed MIRAGE, representing Medical Information Retrieval-Augmented Generation Evaluation. MIRAGE includes 7,663 questions from five commonly used QA datasets in biomedicine. To evaluate RAG in realistic medical settings, MIRAGE focuses on the zero-shot ability in RAG systems where no demonstrations are provided. We also employ a question-only setting for the retrieval phase of RAG, as in real-world cases where no options are given. For a comprehensive comparison on MIRAGE, we provide MEDRAG, an easy-to-use toolkit that covers five corpora, four retrievers, and six LLMs including both general and domain-specific models.\nBased on the MIRAGE benchmark, we systemat-"}, {"title": "2 Related Work", "content": "2.1 Retrieval-augmented Generation\nRetrieval-Augmented Generation (RAG) was proposed by Lewis et al. (2020) to enhance the gen-\n2.2 Biomedical Question Answering\nBiomedical or medical question answering (QA) is a widely studied task since various information needs are expressed by natural language questions in biomedicine (Zweigenbaum, 2003; Athenikos and Han, 2010; Jin et al., 2022). While BERT-based (Devlin et al., 2019) models used to be the state-of-the-art methods of medical QA (Abacha et al., 2019; Lee et al., 2020; Soni and Roberts, 2020; Gu et al., 2021; Yasunaga et al., 2022), they are outperformed by LLMs with large margins (Singhal et al., 2023b; Chen et al., 2023b; Nori et al., 2023b). Due to their knowledge-intensive nature, QA datasets are commonly used to evaluate the biomedical capabilities of both general LLMs (Nori et al., 2023a,b) and domain-specific LLMs (Luo et al., 2022; Chen et al., 2023b; Wu et al., 2023; Singhal et al., 2023a,b). Following these studies, we also use medical QA datasets to test if a RAG system can retrieve and leverage relevant contexts. Unlike prior efforts, our evaluation employs both RAG and question-only retrieval settings, a more realistic evaluation for medical QA."}, {"title": "3 The MIRAGE Benchmark", "content": "3.1 Evaluation Settings\nThe main objective of this work is to evaluate RAG systems in a setting that reflects real-world medical information needs as much as possible while being practically scalable. As such, our MIRAGE\n3.2 Component Datasets\nAs shown in Figure 1, MIRAGE contains five commonly used datasets for medical QA for the eval-"}, {"title": "4 The MEDRAG Toolkit", "content": "To comprehensively evaluate how different RAG systems perform on our MIRAGE benchmark, we propose MEDRAG, a toolkit with systematic implementations of RAG for medical QA. As shown in Figure 2, MEDRAG consists of three major components: Corpora, Retrievers, and LLMs, which are briefly introduced in this section. More details of each component can be found in the appendix.\nFor corpora used in MEDRAG, we collect raw data from four different sources, including the commonly used PubMed for all biomedical abstracts, StatPearls for clinical decision support, medical Textbooks (Jin et al., 2021) for domain-specific knowledge, and Wikipedia for general knowledge. To the best of our knowledge, this is the first work that evaluates new corpora like StatPearls. We also provide a MedCorp corpus by combining all four corpora, facilitating cross-source retrieval. Each corpus is chunked into short snippets. Statistics of used corpora are shown in Table 3.\nFor the retrieval algorithms, while many general and domain-specific retrievers have been proposed (Remy et al., 2022; Ostendorff et al., 2022; Karpukhin et al., 2020; Xiong et al., 2020), we only select some representative ones in MEDRAG due to limited resources, including a lexical retriever (BM25, Robertson et al., 2009), a general-domain semantic retriever (Contriever, Izacard et al., 2022), a scientific-domain retriever (SPECTER, Cohan et al., 2020), and a biomedical-domain retriever (MedCPT, Jin et al., 2023a). Their statistics are presented in Table 4. In our experiments, 32 snippets are retrieved by default. Additionally, we utilize Reciprocal Rank Fusion (RRF, Cormack et al., 2009) to combine results from different retrievers, including RRF-2 (fusion of BM25 and MedCPT), and RRF-4 (fusion of all four retrievers).\nSimilarly, although various LLMs have emerged in recent years (Singhal et al., 2023a,b; Taylor et al., 2022; Luo et al., 2022; Yang et al., 2022), we select several frequently used ones in MEDRAG, including the commercial GPT-3.5 and GPT-4 (OpenAI et al., 2023), the open-source Mixtral (Jiang et al., 2024) and Llama2 (Touvron et al., 2023b), and the biomedical domain-specific MEDITRON (Chen et al., 2023b) and PMC-LLaMA (Wu et al., 2023). Statistics of the used LLMs can be found in Table 5. For all LLMs, we concatenate and prepend retrieved snippets to the question input, and perform chain-of-thought (CoT) prompting (Wei et al., 2022) in MEDRAG to fully leverage the reasoning capability of the models. Temperatures are set to 0 for deterministic outputs. CoT without RAG is used as the baseline for comparison."}, {"title": "5 Results", "content": "We systematically evaluate MEDRAG on our MIRAGE benchmark, which provides us with a multi-dimensional analysis of different components in RAG for medicine. Section 5.1 presents the results for different LLMs, and Section 5.2 includes the results of different corpora and retrievers.\n5.1 Comparison of Backbone LLMs\nWe first benchmark various LLMS on MIRAGE under both the CoT and the MEDRAG settings. For different LLMs, we use the same MedCorp corpus and the RRF-4 retriever and prepend 32 retrieved snippets for RAG. Results are shown in Table 6.\nUnder the CoT setting, GPT-4 significantly outperforms other competitors, with an average score of 73.44% on MIRAGE. While the best average score of other backbone LLMs can only achieve about 61% (GPT-3.5 and Mixtral) in the CoT setting, their performance can be significantly improved to around 70% with MEDRAG, which is comparable to GPT-4 (CoT). These results suggest the great potential of RAG as a way to enhance the zero-shot capability of LLMs to answer medical questions, which can be a more efficient choice than performing larger-scale pre-training. On all five tasks in MIRAGE, Mixtral shows an accuracy of 61.42% on average in the CoT setting, which slightly surpasses the performance of GPT-3.5. However, Mixtral is still outperformed by GPT-3.5 with MEDRAG by 3.0%, indicating the advantage of GPT-3.5 in following MEDRAG instructions.\nOur results also demonstrate that domain-\n5.2 Comparison of Corpora and Retrievers\nWe also compare how different corpora and retrievers affect the MIRAGE performance with MEDRAG. Based on the results in Table 6, we conduct the fol-lowing experiments with GPT-3.5 as it benefits the most from MEDRAG (+17.9%)."}, {"title": "6 Discussions", "content": "6.1 Performance Scaling\nWe explore how the performance of MEDRAG scales with the increase in the number of snippets used for medical QA. To study the scaling properties, we use GPT-3.5 as the backbone LLM, RRF-4 as the retriever, and MedCorp as the corpus.\n6.2 Position of Ground-truth Snippet\nLiu et al. (2023) found the RAG performance is lowest when the relevant information is placed in the middle, a phenomenon known as \u201clost-in-the-middle\". In our MIRAGE benchmark, PubMedQA* and BioASQ-Y/N are the tasks that have ground-truth labels of the supporting snippets for each question. Here we use PubMed as the corpus, and take GPT-3.5 and RRF-4 as the LLM and retriever, respectively. For each dataset, we group the positions of ground-truth snippets into several bins, on which we evaluate how accurate MEDRAG is in answering questions whose ground-truth snippets are in corresponding bins. For PubMedQA*, we only show the results of the first 18 positions, since no ground-truth snippets have been placed after it.\n6.3 Proportion in the MedCorp Corpus\nWe also examine the proportion of different sources in the retrieved snippets from MedCorp, and explore how this proportion changes across different tasks. Figure 5 displays the proportions of four different sources in MedCorp and the actually retrieved sources in the top 64 retrieved snippets for each task in MIRAGE. It can be observed from the figure that, in general, the proportion of Wikipedia drops in the retrieved snippets for medical questions, which is expected as many snippets in Wikipedia are not related to biomedicine.\n6.4 Practical Recommendations\nIn this section, we discuss the practical indications and recommendations based on our evaluation results of different MEDRAG settings on MIRAGE.\nCorpus selection. Results in Table 7 indicate that PubMed and the MedCorp corpus are the only corpora with which MEDRAG can outperform CoT on all tasks in MIRAGE. As a large-scale corpus, PubMed serves as a suitable document collection for various kinds of medical questions. If"}, {"title": "7 Conclusion", "content": "To evaluate RAG systems in medicine, we introduced the MIRAGE benchmark and the MEDRAG toolkit. Based on our comprehensive evaluations, we presented many novel observations and practical recommendations to guide the research and real-world deployments of medical RAG systems.\nLimitations\nWhile our study provides systematic evaluations and practical recommendations for medical RAG systems, there are several limitations that need to be acknowledged. First, there have been novel developments in the architecture of RAG (e.g., active RAG, Jiang et al., 2023). However, we mainly evaluate the vanilla RAG architecture where the retrieved documents are directly prepended in the LLM context because this is the most widely implemented architecture. Evaluating new RAG system designs remains an important direction to explore. Second, while the coverage of corpora, retrievers, and LLMs in MEDRAG is reasonably comprehensive, there are other potentially useful resources that can also be incorporated into MEDRAG in future work, such as the full-text articles from PubMed Central (PMC) and Frequently Asked Questions (FAQs) from trustworthy sources (Ben Abacha and Demner-Fushman, 2019). Third, we only evaluate the retrieval component for PubMedQA* and BioASQ-Y/N since the other three examination datasets lack labels of ground-truth supporting documents. Further research should also evaluate whether the retrieved snippets are actually helpful for the examination datasets, and explore the use of cross-encoder re-rankers to improve the retrieval performance for relevant information. Fourth, while QA is the most commonly used task for evaluating biomedical LLMs, there are also other knowledge-intensive tasks that might benefit from MEDRAG, such as claim verification (Wadden et al., 2020). Following most other studies, we use the format of multi-choice questions for large-scale and automatic evaluation of medical QA. Although we restrict the retrieval phase to having no access to the choices, LLMs still need to use them as input for the final prediction. The rationales generated by MEDRAG remain to be evaluated as well. As the goal of this study is to systematically benchmark the most commonly used medical RAG settings, we leave the potential solutions of the above-mentioned limitations to future work."}, {"title": "Appendix", "content": "A Details of MIRAGE Datasets\nMMLU-Med. Massive Multitask Language Understanding (MMLU) is a benchmark for the evaluation of the multitask learning capability of language models. The benchmark contains a variety of 57 different tasks (Hendrycks et al., 2020). To measure the performance of medical RAG systems, we select a subset of six tasks that are related to biomedicine following (Singhal et al., 2023a), including anatomy, clinical knowledge, professional medicine, human genetics, college medicine, and college biology. The subset is collectively denoted as MMLU-Med. Only the test set of each task is used in our benchmark, which contains 1089 questions in total.\nMedQA-US. MedQA (Jin et al., 2021) is a multi-choice QA dataset collected from professional medical board exams. Specifically, we focus on the English part, which includes real-world questions from the US Medical Licensing Examination (MedQA-US). The 1273 four-option test questions are included in our MIRAGE benchmark.\nMedMCQA. MedMCQA (Pal et al., 2022) contains 194k multi-choice questions collected from Indian medical entrance exams. The questions cover a wide range of 2.4k healthcare topics and 21 medical subjects. Since the ground truth of its test set is not provided, the dev set of the original MedMCQA is chosen for MIRAGE, including 4183 medical questions.\nPubMedQA*. PubMedQA (Jin et al., 2019) is a biomedical research QA dataset. It has 1k manually annotated questions constructed from PubMed abstracts. Different from the datasets above, PubMedQA also provides a relevant context for each question to evaluate the reasoning ability of language models. To test the capability of RAG systems to find related documents and answer the question accordingly, we build PubMedQA* by removing given contexts in the 500 expert-annotated test samples of PubMedQA following (L\u00e1la et al., 2023). The possible answer to a PubMedQA* question can be yes/no/maybe, reflecting the authenticity of the question statement based on scientific literature.\nBioASQ-Y/N. BioASQ (Tsatsaronis et al., 2015; Krithara et al., 2023) is an annual competition for biomedical QA, which includes both the information retrieval track (Task A) and machine reading comprehension track (Task B). To leverage the resources of BioASQ for our medical RAG benchmark, we select the Yes/No questions in the ground truth test set of Task B from the most recent five years (2019-2023), including 618 questions in total. In the original task, questions are constructed based on biomedical literature, and the ground truth snippets are provided as a basis for machine reading comprehension. Similar to PubMedQA*, BioASQ-Y/N is also a modified version on which RAG systems are supposed to answer the questions without the ground-truth snippet provided.\nB Detailed Descriptions of MEDRAG\nB.1 Document Collections\nPubMed. PubMed is the most widely used literature resource (Lu, 2011; Jin et al., 2024), containing over 36 million biomedical articles. Many relevant studies solely use PubMed as the retrieval corpus (Frisoni et al., 2022; Naik et al., 2022). For MEDRAG, we use a PubMed subset of 23.9 million articles with valid titles and abstracts.\nStatPearls. StatPearls is a point-of-the-care clinical decision support tool similar to UpTo-Date. We use the 9,330 publicly available Stat-Pearl articles through NCBI Bookshelf to construct the StatPearls corpus. We chunked StatPearls according to the hierarchical structure, treating each paragraph in an article as a snippet and splicing all the relevant hierarchical headings as the corresponding title. To the best of our knowledge, our work presents the first evaluation of StatPearls in the biomedical NLP community.\nTextbooks. Textbooks (Jin et al., 2021) is a collection of 18 widely used medical textbooks, which are important references for students taking the United States Medical Licensing Examination (USLME). In MEDRAG, the textbooks are processed as chunks with no more than 1000 characters. We used the RecursiveCharacterTextSplitter from LangChain to perform the chunking.\nWikipedia. As a large-scale open-source encyclopedia, Wikipedia is frequently used as a corpus in information retrieval tasks (Thakur et al., 2021). We select Wikipedia as one of the corpora to see if the general domain database can be used to improve the ability of medical QA. We download the processed Wikipedia data from HuggingFace and also chunked the text with LangChain.\nB.2 Retrieval Systems\nBM25. BM25 (Robertson et al., 2009) is a commonly used baseline retriever which use bag-of-words and TF-IDF to perform lexical retrieval. In MEDRAG, BM25 is implemented with Pyserini (Lin et al., 2021) using the default hyperparameters to index snippets from all corpora.\nContriever. Contriever (Izacard et al., 2022) is a dense retriever pre-trained on Wikipedia and CCNet (Wenzek et al., 2020) with contrastive learning. It is shown to be competitive with BM25 on retrieval tasks in the general domain (Thakur et al., 2021).\nSPECTER. SPECTER (Cohan et al., 2020) is a document-level scientific dense retriever which was pre-trained on the Semantic Scholar corpus (Ammar et al., 2018) to encode similar documents with close embeddings.\nMedCPT. MedCPT (Jin et al., 2023a) is a biomedical embedding model that is contrastively pre-trained by 255 million user clicks from PubMed search logs (Fiorini et al., 2018). It achieved state-of-the-art performance on several biomedical IR tasks. We use the MedCPT Query Encoder and Article Encoder to encode the questions and corpus snippets, respectively.\nRRF. Cormack et al. (2009) proposed to merge results from different retrievers with Reciprocal Rank Fusion (RRF), which effectively fuses the information from different sources by selecting shared predictions. In MEDRAG, we provide two versions of RRF systems, RRF-2 and RRF-4. RRF-2 is the fusion of results from BM25 and MedCPT, which appear to be the optimal lexical and dense retrievers in our experiments. RRF-4 is a more comprehensive system which fuses the information from all individual retrievers used.\nB.3 Backbone LLMs\nGPT-3.5 & GPT-4. GPT-3.5 and GPT-4 (OpenAI et al., 2023) are two popular commercial LLMs developed by OpenAI, which have already shown great capabilities in answering medical questions (Nori et al., 2023b; Li\u00e9vin et al., 2022). In MEDRAG, we use the specific version of GPT-3.5-turbo-16k-0613 and GPT-4-32k-0613 accessed through Microsoft Azure OpenAI Services.\nMixtral. In MEDRAG, we use Mixtral-7\u00d78B, which is an open-source sparse mixture of expert models. Compared with existing open-source models, Mixtral-7\u00d78B can achieve both good task performance and fast inference speed (Jiang et al., 2024).\nLlama2. Llama2 (Touvron et al., 2023b) is a series of open-source models that are pre-trained on large-scale data and fine-tuned with human instructions. In MEDRAG, we use Llama2-70B, which is the largest model in the Llama2 series.\nMEDITRON. MEDITRON (Chen et al., 2023b) is a series of biomedical LLMs that are built based on Llama2 and fine-tuned on open-source biomedical literature. Its 70B version model is contained in MEDRAG.\nPMC-LLaMA. PMC-LLaMA (Wu et al., 2023) is fine-tuned based on LLaMA (Touvron et al., 2023a) using PubMed Central (PMC) papers. Its largest version, PMC-LLaMA-13B, is included in MEDRAG."}, {"title": "C Prompt Templates", "content": "Here are the prompt templates used in our experiments. Figures 6 and 7 show the template for all LLMs except MEDITRON. Since the officially released checkpoint of MEDITRON is only the pre-trained version without any instruction tuning, it cannot follow the given system prompt well. Therefore, we provide a pseudo one-shot demonstration in the prompt for MEDITRON, where the demonstration does not contain any information of real examples. The templates for MEDITRON are provided in Figures 8 and 9."}, {"title": "Prompt template for medical QA with CoT", "content": "You are a helpful medical expert, and your task is to answer a multi-choice medical question. Please\nfirst think step-by-step and then choose the answer from the provided options. Organize your output in\na json formatted as Dict{\u201cstep_by_step_thinking\u201d: Str(explanation), \u201canswer_choice\u201d: Str{A/B/C/...}}.\nYour responses will be used for research purposes only, so please have a definite answer.\nHere is the question:\n{{question}}\nHere are the potential choices:\n{{options}}\nPlease think step-by-step and generate your output in json:"}, {"title": "Prompt template for medical QA with MEDRAG", "content": "You are a helpful medical expert, and your task is to answer a multi-choice medical question using\nthe relevant documents. Please first think step-by-step and then choose the answer from the provided\noptions. Organize your output in a json formatted as Dict{\"step_by_step_thinking\": Str(explanation),\n\"answer_choice\": Str{A/B/C/...}}. Your responses will be used for research purposes only, so please\nhave a definite answer.\nHere are the relevant documents:\n{{context}}\nHere is the question:\n{{question}}\nHere are the potential choices:\n{{options}}\nPlease think step-by-step and generate your output in json:"}, {"title": "Prompt template for medical QA with CoT on MEDITRON", "content": "You are a helpful medical expert, and your task is to answer a multi-choice medical question. Please\nfirst think step-by-step and then choose the answer from the provided options. Organize your output in\na json formatted as Dict{\"step_by_step_thinking\": Str(explanation), \"answer_choice\": Str{A/B/C/...}}.\nYour responses will be used for research purposes only, so please have a definite answer.\n### User: Here is the question:\n...\nHere are the potential choices:\n\u0391. ...\n\u0392. ...\nC. ...\nD. ...\n\u03a7. ...\nPlease think step-by-step and generate your output in json.\n### Assistant:\n{\"step_by_step_thinking\u201d: ..., \u201canswer_choice\u201d: \u201cX\u201d}\n### User:\nHere is the question:\n{{question}}\nHere are the potential choices:\n{{options}}\nPlease think step-by-step and generate your output in json.\n### Assistant:"}, {"title": "Prompt template for medical QA with MEDRAG on MEDITRON", "content": "You are a helpful medical expert, and your task is to answer a multi-choice medical question using\nthe relevant documents. Please first think step-by-step and then choose the answer from the provided\noptions. Organize your output in a json formatted as Dict{\"step_by_step_thinking\": Str(explanation),\n\"answer_choice\": Str{A/B/C/...}}. Your responses will be used for research purposes only, so please\nhave a definite answer.\nHere are the relevant documents:\n{{context}}\n### User:\nHere is the question:\n...\nHere are the potential choices:\n\u0391. ...\n\u0392. ...\nC. ...\nD. ...\n\u03a7. ...\nPlease think step-by-step and generate your output in json.\n### Assistant:\n{\"step_by_step_thinking\u201d: ..., \u201canswer_choice\u201d: \u201cX\u201d}\n### User:\nHere is the question:\n{{question}}\nHere are the potential choices:\n{{options}}\nPlease think step-by-step and generate your output in json.\n### Assistant:"}]}