{"title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta's Ads System", "authors": ["Fang Zhou", "Yaning Huang", "Dong Liang", "Dai Li", "Zhongke Zhang", "Kai Wang", "Xiao Xin", "Abdallah Aboelela", "Zheliang Jiang", "Yang Wang", "Jeff Song", "Wei Zhang", "Chen Liang", "Huayu Li", "ChongLin Sun", "Hang Yang", "Lei Qu", "Zhan Shu", "Mindi Yuan", "Emanuele Maccherani", "Taha Hayat", "John Guo", "Varna Puvvada", "Uladzimir Pashkevich"], "abstract": "The increasing complexity of deep learning models used for calculating user representations presents significant challenges, particularly with limited computational resources and strict service-level agreements (SLAs). Previous research efforts have focused on optimizing model inference but have overlooked a critical question: is it necessary to perform user model inference for every ad request in large-scale social networks?\nTo address this question and these challenges, we first analyze user access patterns at Meta and find that most user model inferences occur within a short timeframe. T his observation reveals a triangular relationship among model complexity, embedding freshness, and service SLAs.\nBuilding on this insight, we designed, implemented, and evaluated ERCache, an efficient and robust caching framework for large-scale user representations in ads recommendation systems on social networks. ERCache categorizes cache into direct and failover types and applies customized settings and eviction policies for each model, effectively balancing model complexity, embedding freshness, and service SLAs, even considering the staleness introduced by caching.\nERCache has been deployed at Meta for over six months, supporting more than 30 ranking models while efficiently conserving computational resources and complying with service SLA requirements.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning techniques have been shown to significantly improve user representation in recommendation systems [1, 4, 5, 15, 16].\nBy leveraging neural networks and other deep learning architectures, these models can learn complex patterns and relationships between users and items, resulting in more accurate recommendations. Therefore, there has been a growing trend towards developing increasingly complex deep learning models to enhance the performance of user representation [1-4, 6, 10-13, 15-17, 20-24].\nSince user representation is inferred through online serving, the increasing complexity of models in ads recommendation systems has significant challenges: constrained computational resources and service SLA limitations.\nThese challenges necessitate the development of more efficient computational strategies and robust system architectures to ensure that the deployment of complex models does not compromise user experience, recommendation performance, and system reliability.\nPrior to our work, researchers have focuses more on how to speed up the model inference requests, using scalable embedding structures [7, 9], heterogeneous caching embeddings [14, 18], etc. However, no prior work has attempted to investigate and understand whether it is necessary to perform model inference for every ads request in large-scale social network. Our investigation into user access patterns reveals that 76% of consecutive user tower inferences occur within ten minutes, and 52% occur within one minute. This observation highlights the potential benefits of using cached user embeddings to reduce the number of requests for model inference. In addition, it reveals a crucial triangular relationship between user embedding freshness, model complexity, and service SLAs in ads recommendation systems. This interplay highlights the need for a balanced approach that takes into account the trade-offs between these factors to achieve optimal system performance and efficiency.\nTo address these challenges, we propose ERCache, an efficient and reliable caching framework specifically designed for large-scale user representation within ads recommendation systems. Our approach is based on the observation that consecutive user tower inferences often occur within a short time frame. The primary"}, {"title": "2 MOTIVATION", "content": "This section presents some challenges and opportunities that motivates our work."}, {"title": "2.1 Challenges", "content": "The complexity of models used in ad recommendation systems is increasing at a faster rate than the available computational resources, creating challenges in terms of constrained computational resources and service SLA limitations.\nConstrained computational resources: Increased model complexity raises demand for computational resources (e.g., CPUs, GPUs). However, the availability of these resources is limited in reality, thus not all models' computational needs can be satisfied.\nService SLA limitations: Incorporating complex models may increase e2e latency and are vulnerable to failures due to computational demands. This could potentially violate service SLAs."}, {"title": "2.2 Model Serving Triangle", "content": "As shown in Figure 1, we have observed a triangular relationship in model serving practice that it is impossible for a model serving system to simultaneously provide all three of the following guarantees:\n\u2022 Model complexity: the computation resource required by ML models used in the model serving system.\n\u2022 Embedding freshness: how up-to-date the embeddings (i.e., user tower embeddings) are in the model serving system.\n\u2022 Service SLAs: the requirements of important system metrics, like e2e latency, model fallback rate, etc.\nIn other words, if a model serving system is designed to handle complex models and provide fresh embeddings, it may compromise on service SLAs. Similarly, if the system prioritizes embedding freshness and meets the requirements of service SLAs, it may sacrifice model complexity. Alternatively, if the system aims to achieve both model complexity and service SLAs, it may not be able to maintain embedding freshness.\nSince model complexity tends to increase over time, and service SLAs remain unchanged in production, it is essential to explore opportunities for improvement in embedding freshness. This can help maintain a balance between the three factors and ensure that the model serving system continues to perform optimally."}, {"title": "2.3 Opportunities", "content": "To find the opportunities, we review the access pattern for online users interacting with ads recommendation systems at Meta. As shown in Figure 2, there is a significant likelihood of multiple user tower inferences occurring at a short time. These findings"}, {"title": "3 DESIGN AND IMPLEMENTATION", "content": "In this section, we introduce the design details of ERCache."}, {"title": "3.1 Architecture of ERCache.", "content": "ERCache is a caching system independent from ads ranking systems, shown in Figure 4. ERCache consists of two components: direct cache and failover cache.\nThe direct cache stores generated user embeddings to bypass user tower inference requests when cached embeddings are valid. The failover cache applies the cached user embeddings to recover from failed requests."}, {"title": "3.2 ERCache functionalities", "content": "ERCache offers three functionalities to enhance the efficiency and robustness of ads ranking systems:\n1. Direct Cache Check: System checks if model's direct cache is valid before sending requests to inference; uses cached embedding if valid, otherwise continues normally.\n2. Failover Cache Assistance: For failed inference requests, system checks failover cache for valid embeddings; replaces failed requests with valid cached embeddings, otherwise reports failure.\n3. Cache update: Upon receiving the latest embeddings from the model inference requests, we will update the cache in ERCache by issuing a write request.\nThe sequence diagram of ERCache is shown in Figure 3."}, {"title": "3.3 Customized cache configurations", "content": "We chose the TTL-based eviction policy due to its alignment with user access patterns and time-based prioritization, which is more"}, {"title": "3.4 Update combination", "content": "ERCache employs a two-layer update combination mechanism to minimize the number of cache write requests per user across multiple ranking stages, shown in Figure 5. By consolidating user embeddings from various ranking models across multiple ranking stages into a single request, rather than having one request per model embeddings per stage, we significantly reduce the write QPS on the ERCache."}, {"title": "3.5 Asynchronous write", "content": "After grouping all cache write requests into one single request, we send the write request to ERCache asynchronously. The asynchronous operation moves write out ot the critical path and does not impact the e2e latency."}, {"title": "3.6 Regional consistency", "content": "ERCache guarantees the regional consistency through its internal memcache system. Since most requests are routed to the same region as their previous serving for good locality, both the request and cache remain in the same region most of the time, ensuring efficient data access and minimizing latency."}, {"title": "3.7 Reliability", "content": "ERCache may face cascading effects due to traffic oscillations, regional outages, and site events, leading to increased load and reduced performance. To enhance system reliability, a rate limiter has been implemented. This rate limiter filters requests based on regional thresholds if there is a sudden spike in QPS."}, {"title": "4 EVALUATION", "content": "In this section, we evaluate ERCacheto answer the following questions:\n1. How much computational resources can ERCache save?\n2. What's the impact of ERCache on service SLAs?\n3. How can ERCache affect model performance with different cache TTL?\n4. What is the effect of varying cache TTL settings on cache performance?\n5. What are the serving costs of ERCache?\n6. Is ERCache reliable when faced with cascading effects, such as sudden changes in traffic or system failures?"}, {"title": "4.1 Experimental setup", "content": "In this study, all experiments were conducted on industrial datasets using A/B testing in our production system.\nTo measure the effcetiveness of ERCache, we compare the computational resources and key service SLAs for enabling and disabling ads ranking models at Meta.\nSpecifically, we measure computational resources by the power consumed during model inference using both CPU and GPU. In terms of service SLAs, we focus on the key metrics that impact our system's performance, including end-to-end (e2e) p99 latency and model fallback rate. Additionally, we evaluate model performance based on Normalized Cross Entropy (NE).\nTo further understand the performance of ERCache, we measure the cache hit rate with different cache TTL settings. We also evaluate the serving cost of ERCache and its reliability during cascading effects."}, {"title": "4.2 Computational resource evaluation", "content": "As mentioned earlier, we measure the power usage for a model and compare the change with and without direct cache to evaluate the effectiveness of ERCache in reducing computational resources. From Table 2, we find ERCache can significantly reduce computational resource usage by 42% to 64%, depending on the cache TTL settings. Furthermore, we note that the power savings achieved by ERCache vary across different models, due to their distinct access patterns and model profiles."}, {"title": "4.3 Service SLAs evaluation", "content": "To understand the imapct of ERCache on service SLAs, we evaluate e2e p99 latency and model fallback rate with ERCache enabled.\nE2E p99 latency. According to Table 2, we achieved an average reduction of 0.2% in end-to-end p99 latency. This improvement is attributed to the decrease in the number of model inference requests and reduced workload in the ads recommendation systems. Notably, we did not observe any NE loss for the models with direct cache enabled, using the cache TTL shown in Table 2. Notably, we did not observe any NE loss for the models with direct cache enabled, using the cache TTL shown in Table 2.\nModel fallback rate. Table 3 shows that the failover cache effectively reduces the fallback rate for ads ranking models, with an average reduction of 79.6%. The most notable improvement is observed in the CVR model at the first ranking stage, where the fallback rate decreased from 6.5% to 0.1%."}, {"title": "4.4 Impact of cache TTL", "content": "To further understand the impact of cache TTL, we evaluate model performance and cache performance using different cache TTL settings.\nModel performance evaluation. We investigate the relation-"}, {"title": "4.5 Serving cost of ERCache", "content": "We evaluate the serving cost of ERCache from QPS, latency, and bandwidth.\nQPS. Figure 7 shows the write QPS, which ranges from 0.93 M/s to 1.63 M/s, and the read QPS, which varies between 2.43 M/s and 3.778 M/s. By applying the caching grouping technique, we have successfully reduced the number of cache reads and writes. If we were to support 30 models without this grouping technique, the QPS would increase by at least 30x.\nLatency. Figure 8 displays the CDF of read latency in ERCache. The results show that 50% of read requests have a latency of less than 1 ms, while 80% have a latency of less than 2 ms. The p50 read latency is 0.77 ms and the p99 read latency is 8.47 ms. Most read requests can be completed within 10 ms. The low latency of ERCache does not hurt the performance of ads recommendation systems.\nSince we use asynchronous write to ERCache, we do not prioritize the write latency.\nWrite bandwidth. The write bandwidth of ERCache varies between 7.26 GB/s and 12.43 GB/s, with an average of 9.16 GB/s, shown in Figure 9. We do not discuss the read throughput as it is relatively inexpensive in memory."}, {"title": "4.6 Reliability of ERCache", "content": "To assess the reliability of ERCache, we conducted a drain test on one region and monitored its performance during this special situation. The drain test involved intentionally taking down a data center/region to simulate a disaster scenario, such as a fire or power outage.\nWe ran a 6-hour drain test on one region out of 13 main regions. Figure 10 presents the results of the reliability test. The drain test began at hour 21 and ended at hour 26. During the test period, we did not observe any unusual changes in ERCache's primary metrics. The cache hit rate of ERCache remained stable throughout the period. The results demonstrate that ERCache can withstand severe situations, such as cascading effects, and exhibits good reliability."}, {"title": "4.7 Key takeaways", "content": "ERCache has been deployed at Meta for over two years, providing support to more than 30 ranking models and ensuring improved model performance in accordance with service SLAs.\nThe success of ERCache demonstrates that\n1. Model inference is not necessary for every ads request, despite the importance of embedding freshness to model performance.\n2. The triangular relationship between model complexity, embedding freshness, and service SLAs is useful and reasonable. It can serve as a reference for other researchers and engineers developing models and systems.\n3. The serving cost of ERCache is not expensive due to its low QPS, latency, and bandwidth.\n4. ERCache can be easily applied to other ads recommendation systems in large-scale social networks or other areas with similar access patterns to Meta."}, {"title": "5 RELATED WORK", "content": "Training Optimization Recent publications focus on optimizing DRAM cache and GPU resident cache utilization for training purposes. HierPS [25] is a distributed GPU hierarchical parameter server for massive scale deep learning ads systems with 3-layer hierarchical storage including GPU HBM, CPU memory and SSD. AIBox [26] is a centralized system to train CTR models with tens-of-terabytes-gb by SSDs and GPUs. While they prioritize training optimization, our focus is on design of caching systems for efficient and reliable model inference.\nEmbedding optimization AdaEmbed [8] is a complementary system, to reduce the size of embeddings needed for the same accuracy via in-training embedding pruning. It prioritizes embeddings with high runtime access frequencies and large training gradients, dynamically pruning less important ones to optimize per-feature embeddings. AdaEmbed targets embedding optimization during the training phase, which is a distinct area of caching research within our work.\nInference optimization Fleche [18] presents a comprehensive cache scheme with detailed designs for efficient GPU-resident embedding caching. UGACHE [14] introduces a novel factored extraction mechanism that mitigates bandwidth congestion to fully utilize high-speed cross-GPU interconnects. RECom [9] proposes the first ML compiler designed to optimize the massive embedding columns in recommendation models on the GPU. EVStore [7] is a 3-layer table lookup system using both structural regularity in inference operations and domain-specific approximations to provide optimized caching. However, these works focus on optimizing the performance of model inference, whereas ERCache targets the caching system before sending requests to model inference.\nCache case study Twitter has published an analysis of its internal caching system [19]. The paper aims to characterize cache workloads based on traffic patterns, TTL, popularity distribution, and size distribution. However, this analysis is too broad and not specifically tailored to ads recommendation systems. As a result, ads recommendation systems may not find much value in such a general analysis."}, {"title": "6 CONCLUSION", "content": "We introduce ERCache, a caching framework specifically designed to efficiently and reliably manage large-scale user representations. ERCache helps alleviate computational resource limitations for increasingly complex models while ensuring that onboarding complex models meets SLAs.\nBy utilizing a direct and failover cache system alongside customized eviction policies, ERCache effectively balances model complexity, embedding freshness, and SLAs, despite the inherent staleness introduced by caching.\nERCache has been successfully deployed in Meta's production systems for over half a year, supporting more than 30 ad ranking models. This deployment has significantly reduced computational resource requirements while maintaining service SLAs.\nApart from the practical contributions, the triangular relationship identified in this study, along with the success of ERCache, provides a valuable reference for the design and research of ad recommendation systems."}]}