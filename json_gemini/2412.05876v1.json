{"title": "MG-3D: Multi-Grained Knowledge-Enhanced 3D Medical Vision-Language Pre-training", "authors": ["Xuefeng Ni", "Linshan Wu", "Jiaxin Zhuang", "Qiong Wang", "Mingxiang Wu", "Varut Vardhanabhuti", "Lihai Zhang", "Hanyu Gao", "Hao Chen"], "abstract": "3D medical image analysis is pivotal in numerous clinical applications. However, the scarcity of labeled data and limited generalization capabilities hinder the advancement of AI-empowered models. Radiology reports are easily accessible and can serve as weakly-supervised signals. However, large-scale vision-language pre-training (VLP) remains underexplored in 3D medical image analysis. Specifically, the insufficient investigation into multi-grained radiology semantics and their correlations across patients leads to underutilization of large-scale volume-report data. Considering intra-patient cross-modal semantic consistency and inter-patient semantic correlations, we propose a multi-task VLP method, MG-3D, pre-trained on large-scale data (47.1K), addressing the challenges by the following two aspects: 1) Establishing the correspondence between volume semantics and multi-grained medical knowledge of each patient with cross-modal global alignment and complementary modality-guided local reconstruction, ensuring intra-patient features of different modalities cohesively represent the same semantic content; 2) Correlating inter-patient visual semantics based on fine-grained report correlations across patients, and keeping sensitivity to global individual differences via contrastive learning, enhancing the discriminative feature representation. Furthermore, we delve into the scaling law to explore potential performance improvements. Comprehensive evaluations across nine uni- and cross-modal clinical tasks are carried out to assess model efficacy. Extensive experiments on both internal and external datasets demonstrate the superior transferability, scalability, and generalization of MG-3D, showcasing its potential in advancing feature representation for 3D medical image analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "Three-dimensional (3D) radiologic image analysis plays a crucial role in healthcare, offering detailed insights into anatomical structures and diseases of patients. AI-driven vision techniques promise to effectively assist various clinical tasks with 3D radiologic images, including disease diagnosis [1], surgical planning [2], prognosis prediction [3], and beyond. Foundation models (FMs) for 3D medical images built via large-scale pre-training are expected to advance extensive clinical tasks [4]. In the realm of pre-training paradigms for building vision FMs, self-supervised learning has emerged as a label-efficient way to learn robust and generalizable visual feature representations, advancing diverse clinical tasks and has recently gained significant attention [5], [6], while it overlooks the valuable knowledge from radiologists' reports. Radiology reports, paired with 3D medical images, are easily available and provide highly detailed medical semantics analyses, which can serve as weakly supervised signals in VLP, strengthening the radiologic semantics capturing, as shown in Fig. 1. VLP in the medical domain can be divided into 2D and 3D strategies. 2D VLP strategies, using large-scale paired X-ray-report data [7], [8], have demonstrated promising prospects in enhancing visual understanding and can be adapted to extensive downstream tasks, however 3D VLP has not been sufficiently explored since complex anatomical structures of 3D volumes and the verbose nature of radiology reports bring greater challenges to the alignment between visual semantics and textural descriptions. 3D VLP in medical domains also has its own typical challenges compared with that in general domains, since short-text captions suffice for describing sparse 3D point clouds [9] and video sequences [10] in general domains, while the rich semantics contained within lengthy reports introduce complexities. In particular, every report sentence corresponds to different semantics in a high-dimensional global 3D volume. Despite recent advancements in 3D VLP strategies [11], [12], how to effectively strengthen the 3D visual representation with multi-grained radiologic knowledge from reports via cross-modal interaction remains challenging. Three primary challenges limit the pre-training performance: a) Pre-training Data Scale and Model Capacity: An inadequate understanding of scaling laws for 3D medical VLP can lead to data underutilization and model bias, limiting the model efficacy and generalization abilities. b) Pre-training Strategy: Current 3D medical VLP methods [11], [13] overlook multi-grained correspondence between modalities within individual patients (intra-patient) and multi-grained semantic relationships across patients (inter-patient), resulting in a deficiency in mining representative multi-grained visual representations among large-scale patient groups from rich report semantics. c) Extensive Validation: There is a scarcity of extensive validation, across a wide range of clinical downstream tasks, for comprehensively evaluating 3D medical vision-and-language models (VLMs). To address the challenges, we aim to propose a generalizable 3D medical VLM with a novel pre-training strategy based on large-scale volume-report data. We observe that each report provides multi-grained descriptions for various anatomies and lesions within their paired 3D volume. Moreover, different reports offer consistent fine-grained descriptions for similar visual characteristics and contrasting expressions for distinct pathological conditions within the same anatomical structures. Inspired by this, we propose MG-3D to mine intra-patient multi-grained semantics and inter-patient semantics correlations via the delicate design of multiple pretext tasks with effective cross-modal interaction. Furthermore, we scale up the pre-training data size and increase the model capacity to verify the scalability of MG-3D, leading to further performance improvement of our proposed VLM. To thoroughly evaluate the model's effectiveness and generalization ability, extensive validations across nine clinical tasks are carried out, including disease diagnosis, prognosis, organ and lesion segmentation, report generation, report-to-volume retrieval, etc. The above-mentioned consideration leads to our main contributions: \u2022 The proposed 3D VLMs pre-trained on the current largest-scale volume-report data consistent to CT-CLIP [11] achieve superior performance through extensive validation across nine clinical tasks, which verify the effectiveness, scalability, and generalization ability of MG-3D. \u2022 To enrich the vision comprehension by report guidance, we propose to unify volume semantics with multi-grained medical knowledge for each patient via global cross-modal alignment and complementary modality-guided local information reconstruction. \u2022 To strengthen discriminative representation, we correlate visual semantics across patients based on fine-grained report correlations and keep sensitivity to global semantic differences of various patients via contrastive learning. \u2022 To enhance the understanding of MG-3D's model behavior on different tasks, we delve into the scaling law [14] of data and model capacity, providing insights into how our proposed VLMs learn and adapt as resources increase. In the reminder of this paper, Section II offers a review of related work, Section III describes the proposed contributions at length, Section IV evaluates the performance of the proposed VLM via comprehensive comparisons with related state-of-the-arts (SOTA), Section V concludes the main contents."}, {"title": "II. RELATED WORK", "content": "A. 3D Medical Image Pre-training In recent years, self-supervised learning (SSL) has emerged as the predominant label-efficient pre-training strategy for advancing 3D vision tasks in medical imaging [5], [6], [15], [16]. In particular, various pretext tasks have been developed to learn visual representations from vast amounts of unlabeled data. These tasks primarily fall into four categories: 1) predictive; 2) generative; 3) contrastive; and 4) mixed strategies. By applying various spatial transformations, predictive tasks enable the learning of spatial information in volumetric structures, including jigsaw puzzles [17], Rubik's Cube recovery [18], and relative position [19] or rotation angle [6] prediction. Generative tasks can enhance a network's ability to perceive spatial context by reconstructing masked [20], [21], noised [22], or distorted regions [23] in 3D volumes. Contrastive learning was employed to learn feature correlations in different 3D volumes, with the selection of positive and negative pairs being a central challenge. Typically, different augmented views of a single volume served as positive pairs, while volumes from other patients were considered negative pairs [24]. However, capturing local differences across patients is difficult due to similarities in inter-patient anatomical structures. In order to capture local contexts, overlapping sub-volumes were selected as positive pairs, and nonintersecting sub-volumes were treated as negative pairs [25]. Furthermore, geometric similarities between volumes [26], [27] were introduced to learn anatomical information. Mixed learning strategies [28], [29] combined multiple types of pretext tasks to elevate task complexity and enrich feature representation. SSL strategies have significantly advanced data-efficient 3D medical image analysis, while they overlook the valuable knowledge from radiologists.\nB. Medical Vision-Language Pre-training VLP has made significant progress in the 2D medical image domain, however 3D VLP has not been sufficiently researched. In 2D medical VLP, two-lag networks [30] can model visual and textual tokens as sequences for multimodal feature fusion, using additional multimodal encoders with multi-task pre-training. However, their performance in uni- and cross-modal downstream tasks often falls short compared to two-tower structures [31] with Contrastive Language-Image Pre-Training (CLIP)-style strategies [32], aligning visual and textual features within the same feature space to enhance cross-modal semantics understanding. Beyond CLIP-style global image-report alignment [32], [33], local image region-report alignment [34] and cross-modal local reconstruction [7] were introduced to improve fine-grained semantic perception. In order to further analyze radiologic semantics in 2D images, additional entity descriptions [35] and knowledge graphs [36] were incorporated to aid VLP. Given the challenges of aligning intra-patient information from paired images and long-text reports, sentence-wise encoding [37] has been employed to align"}, {"title": "III. METHODOLOGY", "content": "A. Overall Framework Learning strategies play crucial roles in large-scale 3D medical VLP to capture radiology semantics from reports. To effectively extract the unique fine-grained characteristics of each patient, and robustly model broad patterns across diverse patients, we propose multi-task VLP strategies, enabling the network to learn generalized representations that enhance each task and foster synergistic learning across all tasks. As shown in Fig. 2, our learning strategies encompass two main aspects: a) Intra-Patient Multi-Grained Semantics Extraction: To strengthen the understanding of multi-grained radiological contexts within each volume-report pair, Section III.B proposes global cross-modal alignment (CML) with a novel cross-modal interaction mechanism, modeling the global semantic correspondence between different modalities. To unify fine-grained semantic consistency and capture the complementarity of different modalities, we integrate complementary modality-guided local information reconstruction, including masked image modeling (MIM), word-level masked language modeling (MLM), and sentence-level feature reconstruction (SFR), to enhance fine-grained cross-modal knowledge transfer. b) Inter-Patient Fine-Grained Semantics Alignment: To ensure robust representation consistency for similar medical conditions while maintaining feature discrimination for different anatomical details across patients, Section III.C proposes an inter-patient fine-grained semantics similarity matching (SSM) strategy to align disentangled fine-grained visual semantics among different patients. For simultaneously keeping sensitivity to global individual differences of various patients, the disentangled feature aggregation (DFA) learning is developed, differentiating globally aggregated semantics across patients and facilitating the transfer of aggregated fine-grained semantics to the 3D vision encoder.\nB. Intra-Patient Multi-Grained Semantics Extraction Given that paired volume-report data contain rich multi-grained semantic correlations, designing holistic learning strategies with efficient cross-modal interaction is crucial for data mining. Single-task VLP strategies only tend to memorize the knowledge specific to their designated tasks, leading to limited data utilization and inadequate generalization capabilities. Furthermore, cross-modal interaction in VLP plays a key role in dominant-modality information preservation and learning difficulty management. Our goal is to develop an effective multi-task learning strategy for intra-patient semantics extraction, incorporating cross-modal interaction that emphasizes the context of the dominant modality while effectively leveraging information from the complementary modality.\n1) Complementary Modality-Guided Local Reconstruction: To ensure local context capturing with the guidance of complementary modalities, we propose to reconstruct masked sub-volumes via fine-grained report clues and recover masked reports at word- and sentence-levels with visual assistance. a) Sentence-Informed Volume Reconstruction: As shown in Fig. 3(a), the 3D volume is partitioned into sub-volumes and randomly masked. Compared to entire reports or brief report words, sentences provide appropriate information of medical findings, maintaining semantic coherence that supports fine-grained visual semantic analysis in 3D volumes. Thus, we aggregate word embeddings $F_T$, extracted from the text encoder, into sentence features $F_S$ by sentence-wise average pooling. Specifically, when separating a report using punctuation, we prioritize creating sentences that are as concise as possible while maintaining a complete and coherent structure. The masked volume features $I_m$ are fused with sentence features via cross-modal attention to predict unseen sub-volumes. As shown in Fig. 4(a), MIM with classical cross attention emphasizes cross-modal generation by treating textual features as the key $K_{TM}$ and value $V_{TM}$ to dominate the text-to-volume generation, complicating the network modeling, since textual features might not fully capture the details in visual content, which can lead to potential information loss. To overcome this challenge, we propose a novel cross-modal attention to leverage masked volume features as the key $K_{IM}$ and value $V_{IM}$ to dominate MIM with the guidance of sentence semantics as the query $Q_S$, focusing on visual semantic context understanding. Specifically, as shown in Fig. 4(b), the multi-modal feature fusion between features of each sentence and unmasked sub-volume is adopted via Transformer layers, in which three types of sub-layers are integrated, including a self-attention layer, the proposed cross-modal attention layer, and a feed-forward layer. The cross-modal attention layer generates an attention map representing correlations between report sentences and local volume regions. In particular, the entire attention map is decomposed into sentence-specific attention maps in the text-modal dimension. In order to capture fine-grained volume semantics corresponding to each sentence, masked volume features $I_m$ are weighted by each sentence-specific attention map, leading to $D_S$ sentence-informed volume features $H_1$:\n$H_1 = Softmax(\\frac{Q_S K_{IM}^T}{\\sqrt{d}}) V_{IM}$. (1)\nwhere $W_Q, W_K$, and $W_V$ are projection matrices of queries, keys, and values, respectively; $D_S$ is the sentence-level token number. d is the channel dimension of the leveraged features. A decoder with convolutional and up-sampling layers is integrated with the multi-modal fusion layers for masked sub-volume reconstruction to get the reconstructed volume $I_{REC}$. The Mean-Square-Error (MSE) loss is used as the objective:\n$L_{MIM} = \\frac{1}{B} \\sum_{b=1}^B ||I - I_{REC}||^2$. (2)\nb) Volume-Informed Report Reconstruction: Given report word tokens $T_M$ after masked modeling, we also propose the volume-guided MLM to encourage the understanding of volume-to-report correlations, as shown in Fig. 3(b). Similar to the volume reconstruction process, the text-dominant multi-modal feature fusion between features of each sub-volume and remaining unmasked words $T_M$ is adopted via Transformer layers, leading to $D_I$ sub-volume-informed word features $H_T$:\n$H_T = \\sum_{i=0}^{D_I} Softmax(\\frac{Q_I W_K^T}{\\sqrt{d}}) W_V V_{TM}$. (3)\nwhere $D_I$ is the number of sub-volume tokens. The decoder for report reconstruction is designed as a simple multi-layer perceptron (MLP) with a Softmax function, resulting in the predicted masked textual entities $P_{MLM}$. The training objective is to maximize the following conditional log-likelihood:\n$L_{MLM} = - \\frac{1}{B} \\sum_{b=1}^{B} log P_{MLM}(T_{REC}|I, T_M)$. (4)\nBeyond word-level reconstruction, we also develop sentence-level feature reconstruction (SFR) to enhance the hierarchical semantic context understanding, as shown in Fig. 3(c). Specifically, the reconstructed word features are aggregated as $S_{REC}$ at the sentence level, which is further aligned with the original sentence feature representation $F_S$ using cosine similarity:\n$Sim (F_S, S_{REC}) = \\frac{F_S \\cdot S_{REC}}{||F_S|| \\cdot || S_{REC}||}$. (5)\nSFR loss encourages consistent sentence-level representation:"}, {"title": null, "content": "$L_{SFR} = \\frac{1}{B} \\sum_{b=1}^{B} ||Sim (F_S, S_{REC}) - 1||_1$, (6)\nwhere 1 is an identity matrix with the same shape as $S_{REC}$.\n2) Cross-Modal Global Feature Alignment: The global semantics of radiology reports summarize the overall evaluation of medical conditions, providing effective clinical decision support for Al models. As shown in Fig. 5, global visual features $F_I$ are extracted from the 3D vision encoder with average pooling, and global textual features $F_T$ are aggregated from sentence features by a self-attention pooling layer. Leveraging the proposed cross-modal attention in Sec III.B, the sentence-informed volume features and volume-informed text features are generated and further globally aggregated as $H_I$ and $H_T$, by the same way in uni-modal feature aggregation. To advance global visual semantics comprehension, we aim to inject global semantics into the 3D vision encoder from reports by cross-modal learning (CML), in which we maximize the normalized report-to-volume similarity between $F_I$ and $H_I$:\n$L_{T \\to I} = - \\frac{1}{B} \\sum_{i=1}^B log \\frac{exp(F_I^T H_I/ \\tau)}{\\sum_{j=1}^B exp(F_I^T H_I/ \\tau)}$ (7)\nwhere $\\tau$ denotes the temperature parameter; B is the mini-batch size. Similarly, the normalized volume-to-report similarity between $F_T$ and $H_T$ is optimized by\n$L_{I \\to T} = - \\frac{1}{B} \\sum_{i=1}^B log \\frac{exp(F_T^T H_T/ \\tau)}{\\sum_{j=1}^B exp(F_T^T H_T/ \\tau)}$. (8)\nThus, the CML loss $L_{CML}$ is defined as\n$L_{CML} = L_{T \\to I} + L_{I \\to T}$. (9)\nOverall, the total loss for intra-patient semantics learning is\n$L_{Intra} = \\lambda_{\\alpha} (L_{MIM} + L_{MLM}) + \\lambda_{\\beta}(L_{SFR} + L_{CML})$. (10)\nwhere hyperparameters $\\lambda_{\\alpha}$ and $\\lambda_{\\beta}$ are utilized to balance the intra-patient multi-task learning.\nC. Inter-Patient Multi-Grained Semantics Alignment Although global individual variations can be mined by patient-wise contrastive learning to distinguish overall differences of patients' visual semantics, the relevance of fine-grained radiology semantics across patients is also nonnegligible. Considering that different reports contain consistent fine-grained descriptions for similar visual characteristics or contrasting expressions for varying pathological conditions in the same anatomical structure, capturing inter-patient fine-grained correlations can strengthen the robustness of visual representations across large-scale patient groups. Meanwhile, we also propose to maintain the model's sensitivity to global individual variations via disentangled feature aggregation learning.\n1) Inter-Patient Fine-Grained Semantics Similarity Matching: We observe that different sentences in each report involve distinct properties and positions of fine-grained findings within anatomical structures, corresponding to different large-scale visual semantics of the paired volume. Inspired by this, our goal is to decouple fine-grained visual semantics with the report sentence guidance from the global volume features $F_I$. As shown in Fig. 6, in order to decouple fine-grained visual semantics, different sentence-wise features $F_S$ are regarded as the queries, and the global visual features $F_I$ serve as the keys for implementing the cross-modal attention in Section III.B, leading to sentence-specific global visual features:\n$H_S = Softmax(\\frac{F_S Q_S W_K^T}{\\sqrt{d}}) W_{F_I} V_{F_I}$ (11)\nIt is important to reasonably measure the relevancy between different fine-grained visual semantics. Thus, we propose to construct an inter-patient fine-grained semantics similarity matrix as the standard. Given that global report contexts of different patients are not always the same, inter-patient sentence-wise features with similar descriptions still exhibit differences if we directly send whole reports to the text encoder, leading to difficulties in precisely measuring the semantics similarity. In order to tackle this, split report sentences are separately sent into the frozen text encoder to get their sentence-wise features, ensuring consistent sentence-wise feature representation for similar fine-grained descriptions across different patients. By calculating pair-wise feature similarities of report sentences across patients, the similarity matrix for inter-patient fine-grained semantics can be derived. We propose the semantics similarity matching loss $L_{ssm}$ to align the similarity matrix of inter-patient fine-grained visual semantics with that of inter-patient fine-grained report semantics:"}, {"title": null, "content": "$L_{SSM} = \\frac{2}{\\beta (\\beta - 1)} \\sum_{i=1}^{\\beta-1} \\sum_{j=i+1}^{\\beta} ||Sim(F_i, F_j) - Sim (H_i, H_j)||_1$ (12)\nIn mini-batch training, SSM losses between all pairs of patients are calculated once, and the average values are regressed. 2) Disentangled Fine-Grained Semantics Aggregation: To capture differences in inter-patient global semantics contexts, we also introduce patient-wise contrastive learning. Specifically, sentence-specific global visual features are aggregated as $H_S$ by a self-attention pooling layer. We align the aggregated features with global visual features to model the disentangled fine-grained semantics into the vision encoder. Furthermore, the aggregated features across patients are separated for mining global individual variations, leading to the disentangled fine-grained semantics aggregation learning:\n$L_{DFA} = - \\sum_{i=1}^{\\beta} log \\frac{exp(H_T F_I / \\tau)}{\\sum_{j=1}^{\\beta} exp(H_T F_I / \\tau)}$ (13)\nOverall, the total loss for inter-patient semantics capturing is\n$L_{Inter} = \\lambda_{\\gamma} (L_{SSM} + L_{DFA})$, (14)\nwhere hyperparameter $\\lambda_{\\gamma}$ is utilized to balance the intra- and inter-patient multi-task learning. The overall training objective $L_{Total}$ of the proposed end-to-end VLP strategy is:\n$L_{Total} = L_{Intra} + L_{Inter}$. (15)"}, {"title": "IV. EXPERIMENTS", "content": "In this section", "Datasets": "The CTRG-Chest dataset [46", "11": "dataset with 50", "Indexes": "We carry out extensive validation on nine uni- and cross-modal tasks to comprehensively assess the performance of MG-3D. The downstream tasks and evaluation indexes are listed as follows. Disease Classification: The CC-CCII [52", "1": "dataset for nodule classification with 888 volumes are leveraged for external testing. In particular", "52": "and Area Under Curve (AUC) [1", "Segmentation": "The MSD Task 06 [49", "50": "dataset for pneumonia segmentation with 189 volumes are adopted for external testing", "49": "is used to access the performance. Organ Segmentation: The ACDC [51", "Prediction": "The STOIC 2021 [3", "Generation": "The CTRG-Chest dataset is used in this task", "Classification": "We randomly sampled 1"}, {"11": "and the rest 3", "Retrieval": "The dataset splitting is the same as that in the Vocabulary Prompt-Driven Anomaly Classification. Recall [11", "Setup": "For the implementation", "2": "with a hierarchical structure serves as the default 3D vision encoder", "53": "is adopted as the text encoder. The text encoder's parameters are frozen to force the 3D vision encoder to learn radiology semantics. The input volume undergoes center-cropping with cropping ratios of [0.88", "0.88": "to increase the foreground proportion and then is resized to the size of [128", "128": ".", "lambda_{\\gamma}$": "for balancing the losses are set to [1.0", "0.1": ".", "Settings": "The configuration settings for each downstream task are summarized as follows. For disease classification and prognosis prediction"}, {"2": "is introduced for organ and lesion segmentation. In report generation", "54": "with a transformer layer is introduced. In the CT-VocabFine", "20": "and SwinMM [5", "17": "and Rubik++ [18", "28": "PCRLv2[25", "6": "and 6 recent medical VLP methods", "7": "and MRM [48", "36": "CLIP-like CT-CLIP [11", "43": "and multi-task learning-based PTunifier [8", "Classification": "Table I presents the average performance of three bootstrapping iterations on two external datasets. The results show that 3D UNet", "Segmentation": "The 3rd and 4th columns of Table II show the average performance of three bootstrapping iterations on two external datasets. The decoders from Swin-UNETR [6", "47": "are separately incorporated for 3D Swin-B and 3D ViT-B. The results illustrate that MG-3D achieves the highest performance in tumor segmentation and pneumonia segmentation"}, {"Segmentation": "Table II's 5th column displays the average performance of three bootstrapping iterations on the external dataset", "Prediction": "Table II's 6th column presents the average performance on STOIC-2021 of three bootstrapping iterations", "Generation": "The 7th-10th columns of Table II present the BLEU scores [46", "Retrieval": "The last two columns of Table II show the results on the default CT-RATE test set [11", "Learning": "Table III illustrates the performance with different intra-patient learning strategies"}, {"Learning": "To evaluate the impact of inter-patient learning", "Size": "We investigate the data scaling law [14", "Capacity": "The last two rows of Table VI present the results concerning the scaling law of model capacity [14", "25": "Vox2Vec [55", "23": "and SwinUNETR [6", "11": "M3D [43", "39": ".", "Classification": "As illustrated in the 2nd and 3rd columns of Table VII", "Segmentation": "The visualization results in MSD Task 06 are shown in Fig. 7(a)-(c), in which nearly all competitive models have difficulty in precisely segmenting tumors; in contrast, our 3D Swin-L achieves ideal results. Furthermore"}]}