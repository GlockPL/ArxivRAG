{"title": "The impact of internal variability on benchmarking deep learning climate emulators", "authors": ["Bj\u00f6rn L\u00fctjens", "Raffaele Ferrari", "Duncan Watson-Parris", "Noelle Selin"], "abstract": "Full-complexity Earth system models (ESMs) are computationally very expensive, limiting their use in exploring the climate outcomes of multiple emission pathways. More efficient emulators that approximate ESMs can directly map emissions onto climate outcomes, and benchmarks are being used to evaluate their accuracy on standardized tasks and datasets. We investigate a popular benchmark in data-driven climate emulation, ClimateBench, on which deep learning-based emulators are currently achieving the best performance. We implement a linear regression-based emulator, akin to pattern scaling, and find that it outperforms the incumbent 100M-parameter deep learning foundation model, ClimaX, on 3 out of 4 regionally-resolved surface-level climate variables. While emulating surface temperature is expected to be predominantly linear, this result is surprising for emulating precipitation. We identify that this outcome is a result of high levels of internal variability in the benchmark targets. To address internal variability, we update the benchmark targets with ensemble averages from the MPI-ESM1.2-LR model that contain 50 instead of 3 climate simulations per emission pathway. Using the new targets, we show that linear pattern scaling continues to be more accurate on temperature, but can be outperformed by a deep learning-based model for emulating precipitation. We publish our code, data, and an interactive tutorial at github.com/blutjens/climate-emulator.", "sections": [{"title": "1 Introduction", "content": "Earth system models (ESMs) are usually run many times to fully account for humanity's imperfect knowledge of future emissions and the chaotic nature of Earth's dynamics. But many runs come at a computational cost: Simulating the climate impacts of a single emission scenario can take multiple weeks and cost hundreds of thousands of USD in compute. As a result, full-complexity ESMs are often only run for a few representative emission scenarios and more efficient emulators are used to explore a larger range of scenarios. Recently, deep learning-based climate emulators have been proposed and evaluated on benchmarks with standardized tasks and datasets . However, considering a 'climate emulator' to be any approximation of a full-complexity ESM, there exist many"}, {"title": "2 Data & Methods", "content": "In Section 2.1, we first review the ClimateBench inputs, targets, evaluation protocol, and metrics. In Section 2.2, we describe the Em-MPI targets dataset. In Section 2.3, we detail the linear pattern scaling emulator and in Section 2.4 we introduce our reimplementation of the CNN-LSTM-based emulator from with details being reviewed in Appendix B3. In Section 2.5, we discuss our approach to measuring the impact of internal variability on model performance (hereafter referred to as 'the internal variability experiment'). Specifically, we illustrate how internal variability affects the relative performance of an LPS- vs. CNN-LSTM-based emulator on the Em-MPI targets dataset."}, {"title": "2.1 Background on ClimateBench dataset, evaluation protocol, and metrics", "content": "The ClimateBench dataset contains emission trajectories and climate projections from three realizations of the climate model, NorESM2-LM , for each of seven emission scenarios, $e \\in S_e = {\\text{hist-ghg, hist-aer, historical, ssp126, ssp245, ssp370, ssp585}}$. The {hist-aer,hist-ghg} scenarios and Shared Socioeconomic Pathway (ssp)-based scenarios are model experiments in ScenarioMIP and DAMIP, respectively, both of which are activities"}, {"title": "2.2 Em-MPI data: Addressing internal variability with the MPI-ESM1.2-LR ensemble", "content": "We select the MPI-ESM1.2-LR single model initial-condition large ensemble for extracting the data subset, Em-MPI. We choose the MPI model instead of other climate models, by first subselecting the CMIP6 models from the multi-model large ensemble archive that ran at least 30 realizations for the main scenarios in ScenarioMIP (ssp126, -245, -370, and -585). This leaves only three models: MPI-ESM1.2-LR, EC-Earth3, and CanESM5. Out of this selection, the MPI model is the only one with a transient climate response within the \"likely\" range and a sufficient number of online-available realizations, as detailed in Appendix B2. Further, MPI-ESM1.2-LR simulates internal variability in surface temperature comparable to observations and only slightly underestimates the variability in precipitation."}, {"title": "2.3 Linear Pattern Scaling emulator", "content": "Linear Pattern Scaling (LPS) is a commonly used emulation technique to analyse regional climate change as a function of global emissions. In our LPS emulator, $f_{\\text{lps}} : x_{t,e} \\rightarrow \\hat{Y}_{t,e}$, a linear regression fit first relates scalar global cumulative CO2 emissions, $x_{t,e} = \\text{CO}_{2um} \\in \\mathbb{R}$, to global surface temperature anomalies, $T_{\\text{surf}} \\in \\mathbb{R}$, for a chosen year, $t \\in \\{1850, 1851, ..., 2100\\}$, and emission scenario, $e \\in S_e$:\n$T^{2;t,e}_{\\text{surf}} = w_{\\text{global}}x_{t,e} + b_{\\text{global}}$\nThis regression for global temperature does not utilize other emission variables consistent with the evidence that $T^{2;t,e}_{\\text{surf}}$ is predominantly linearly proportional to \\text{CO}_{2um} . Many techniques emulate the global mean temperature more accurately than a linear fit by considering non-CO2 forcings or multiple time-scales , but we are choosing to sacrifice accuracy for added simplicity and interpretability.\nThen, we fit local linear regression functions independently for each grid cell, $(i, j)$, to map global temperature anomalies onto the local climate variable of interest, $Y_{i,j,t,e} \\in \\mathbb{R}$:\n$\\hat{Y}_{i,j,t,e} = w_{\\text{local}} w_{i,j} T^{2;t,e}_{\\text{surf}} + b_{\\text{local}}$\nThis regression assumes that any local climate variable can be described as a linear function of the global mean temperature and that this function is consistent across emission scenarios. This assumption is known to be relatively accurate for local surface temperature and less accurate for local precipitation . The existing implementation of LPS on ClimateBench uses $T^{2;t,e}_{\\text{surf}}$ from the target dataset as inputs (instead of estimating the variable from emissions). Our LPS emulator can be represented as matrix operations: $\\hat{Y}_{t,e} = W_{\\text{local}} (w_{\\text{global}}x_{t,e} + b_{\\text{global}}) + B_{\\text{local}}$ with $\\hat{Y}_{t,e} = \\{\\hat{y}_{i,j,t,e}\\}_{i \\in I,j \\in J}$. The LPS emulator including the global and local weights and biases, $w_{\\text{global}}, b_{\\text{global}}, w_{\\text{local}}, b_{\\text{local}} \\in \\mathbb{R}$, are independent of time and scenario. We split the global and local regression into Eqs. (3) and (4) for interpretability, but adding a global weight and bias is redundant; thus, the total number of free parameters is the number of local weights and biases: $2 * |I| * |J|$. By construction, LPS preserves the global average if local and global variable are temperature"}, {"title": "2.4 Review of the CNN-LSTM emulator", "content": "A CNN-LSTM is a commonly used neural network architecture for learning spatiotemporal correlations and has been evaluated by on ClimateBench. In comparison to LPS, the CNN-LSTM is expected to be capable of learning more complex functions by having more free parameters, using emission trajectories over 10 years, and using all emission variables, including spatially-resolved aerosols as inputs. We follow the implementation in to reimplement the CNN-LSTM for our internal variability experiment, but noticed that the CNN-LSTM converges to significantly different minima depending on the weight initialization random seed. To compensate for this variation, we deviate from the original implementation by training the CNN-LSTM multiple times with different weight initialization random seeds and reporting the average scores across those seeds whenever we evaluate the CNN-LSTM."}, {"title": "2.5 Testing the influence of internal variability on model performance", "content": "We construct the following experiment to explore the impact of internal variability on model performance (with results in Fig. 4 and Section 3.2.2). We fit two emulation techniques (LPS and CNN-LSTM) on random ensemble subsets of n realizations and evaluate them for each n on the mean of all 50 realizations. We evaluate on the mean of 50 (instead of n) realizations, because we aim to benchmark emulators of the forced mean climate response. We assume that 50 realizations are sufficient to approximate the true forced climate response of a given climate model, which is a reasonable assumption for most variables and spatiotemporal resolutions. Thus, evaluating against the mean of 50 realizations will determine for each n-member training set which technique we define as the 'better' emulator for the MPI-ESM1.2-LR model.\nIn detail, we randomly draw the ensemble subsets, $M_{n,k} \\subset M = \\{1,2,..., N_{\\text{em-mpi}}\\}$, where $n = |M_{n,k}|$ denotes the number of realizations in the subset, m a realization ID, and k the index of the random draw. Each ensemble subset is drawn from the Em-MPI data that has $N_{\\text{em-mpi}} = 50$ realizations without replacement, i.e., within each subset there are no duplicate realization IDs. To compensate for the variation across subsets, we draw K = 20 ensemble subsets of the same size for every n and report the mean and standard deviation. The subsets, for example, contain the realization IDs, $M_{1,1} = \\{3\\}, M_{1,2} = \\{48\\}, ..., M_{3,1} = \\{2, 18, 36\\}, M_{3,2} = \\{1, 10, 42\\}, etc.$\nWe fit each emulation function, $f_{\\text{LPS}} : X \\rightarrow \\hat{Y}_{\\text{LPS}}$ and $f_{\\text{CNN-LSTM}}: X \\rightarrow \\hat{Y}_{\\text{CNN-LSTM}}$, on the ensemble-mean of each ensemble subset - denoting the fitting or training process as $P : (X,Y) \\rightarrow f$ where the subscript is omitted for brevity. We use the same inputs as in ClimateBench, targets from the Em-MPI data, and a data split equivalent to the one in ClimateBench, i.e., $S_{\\text{em-mpi}} = S_{\\text{em-mpi}, \\text{train}} \\cup S_{\\text{em-mpi}, \\text{val}} = \\{historical, ssp126, ssp370, ssp585\\}$ and $S_{\\text{em-mpi}, \\text{e,test}} = \\{ssp245\\}$ with $T_{\\text{test}} = \\{2080, 2081, ..., 2100\\}$. After training, we evaluate the emulators on the ensemble-mean of all 50 realizations. As evaluation metrics, we use the spatial and global RMSE* (from Eqs. (1) and (2)), averaged over the ensemble subsets, and the difference between the emulator scores, $\\Delta RMSE_{*,n} = RMSE_{*,n,\\text{CNN-LSTM}} - RMSE_{*,n,\\text{LPS}}$. The train and evaluation process is conducted independently for precipitation and surface temperature. In summary:"}, {"title": "3 Results", "content": "In Section 3.1, we quantify the performance of LPS on ClimateBench. For spatial precipitation, LPS achieves better scores than deep learning-based emulators even though the relationships are known to be nonlinear (discussed in Section 4.1). In Section 3.2, we illustrate that the internal variability in the 3-member ClimateBench targets is high in comparison to the 50-member Em-MPI data. Then, we illustrate that this high internal variability can skew benchmarking scores in favor of LPS, i.e., LPS often has comparatively better scores than the CNN-LSTM if there is more internal variability in the training set."}, {"title": "3.1 Evaluation of Linear Pattern Scaling on ClimateBench", "content": "Figure 3 shows that LPS (middle) predicts surface temperature patterns that are similar to the ClimateBench test set targets (left). Important warming patterns are visible in the LPS predictions; for example, a North Atlantic \"cold blob\", Arctic amplification, and proportionately higher warming over land and the Northern Hemisphere. Figure 3-right shows that LPS predicts the surface temperature anomalies in most regions within a $\\pm0.25\u00b0C$ error bar of the target ESM-simulated values. The global patterns in the LPS predictions also resemble the target patterns for other climate variables, as shown in Figs. C1 to C3."}, {"title": "3.1.2 Quantitative comparison of LPS with deep learning emulators", "content": "Table 1 compares LPS against all other emulators that have reported scores on ClimateBench including the transformer-based foundation model, ClimaX, with 108M free parameters that is currently being cited as the best performing emulator . The Gaussian Process and Random Forest entries are described in . The CNN-LSTM by is summarized in Appendix B3, CNN-LSTM (reproduced) is reimplementation of this CNN-LSTM by , and Cli-ViT is a vision transformer detailed in . We highlight the emulator with the lowest NRMSE score (i.e. best performance) in bold. But, we caution the reader not to draw a definitive conclusion which emulation technique is most accurate beyond the ClimateBench benchmark because of the internal variability in the NorESM2-LM targets, as analysed in Section 3.2.\nLPS has the lowest spatial NRMSE scores for 3 out of 4 variables including surface temperature, precipitation, and 90th percentile precipitation (see Table 1). Further, LPS requires 4 orders of magnitude fewer parameters than ClimaX. This result implies that LPS is the new incumbent on spatial variables, i.e., the emulation technique that achieves the lowest score in the most 'Spatial' columns, on ClimateBench.\nClimaX and the CNN-LSTM (reproduced) have better scores than LPS on diurnal temperature range, which is strongly influenced by aerosols, as discussed in . ClimaX and CNN-LSTM (reproduced) also have better global scores in precipitation and 90th percentile precipitation which we discuss in Appendix C6. Giving every column in the results table equal weight, LPS and ClimaX are on par with each other with each having better scores on 6 out of 12 columns.\nLPS outperforming deep learning on surface temperature aligns with the domain intuition that there is a linear relationship to cumulative CO2 emissions. But, it is surprising that LPS also outperforms the nonlinear deep learning-based emulators on spatial precipitation even though the relationships are known to be nonlinear."}, {"title": "3.2 Analysing the relationship between internal variability and performance assessments of climate emulators", "content": "In this section, we analyse if internal variability contributes to the comparatively good scores of LPS on the ClimateBench spatial precipitation. We cannot conduct our internal variability experiment on the ClimateBench targets, because the experiment requires many ensemble members. Thus, we first demonstrate the magnitude of internal variability in the ClimateBench targets in comparison to the Em-MPI targets. Then, we show that the magnitude of internal variability in a target dataset affects the benchmark scores using the Em-MPI data. This suggests that internal variability could potentially impact benchmark scores on any targets dataset that does not contain enough members to average out internal variability, including the ClimateBench dataset, as discussed in Section 4.3."}, {"title": "3.2.1 Magnitude of internal variability in 3-member NorESM2-LM and 50-member MPI-ESM1.2-LR ensemble average", "content": "Figure 2 illustrates the magnitude of internal variability in the 3-member NorESM2-LM targets in comparison to the 50-member MPI-ESM1.2-LR targets. First, Fig. 2a shows the ensemble-mean surface temperature anomalies over time for the NorESM (left) and Em-MPI (middle) data. The anomalies are plotted as a global-average (black) and regional-average over a sample region (red). Interannual fluctuations from internal variability are still visible in the global NorESM (left,black) data and are higher at the regional scale, exemplified by fluctuations up to $\\pm1.5\u00b0C$ over a regional-average in the IPCC AR6 region Greenland/Iceland (left, red). Similar plots in Fig. 2b illustrate that internal variability is even higher for precipitation and dominates the signal on a regional scale, here over S. Central-America.\nThe 50-member mean in the Em-MPI data (middle) shows notably lower fluctuations from internal variability for temperature and precipitation. Figure C4 shows that the fluctuations from internal variability also seem lower across a diverse set of regions. Additionally, we plot averages over a 21-year sliding window in Fig. C4, because the spatial RMSE in Eq. (1) computes a 21-year average before calculating the spatial errors. In most regions, the fluctuations in the 21-year averaged precipitation are also visibly lower, in comparison to the NorESM2-LM ensemble-mean."}, {"title": "3.2.2 Effect of internal variability on benchmark scores", "content": "Figure 4-top shows the results of our internal variability experiment for precipitation anomalies from the Em-MPI data. When high internal variability is present in the training set at n = 3 realizations, the LPS (orange) has a better spatial RMSE than the CNN-LSTM (blue). This mirrors the results on the ClimateBench results table in Table 1. As the internal variability in the training set is increasingly averaged out, the RMSE of the CNN-LSTM decreases. The LPS and CNN-LSTM RMSEs become comparable at n=6 realizations. For more realizations up to 50 the CNN-LSTM has a better score than the LPS. This trend is also visible when plotting the difference, $\\Delta RMSE_{s}(n)$, which is positive at n = 3 and crosses the zero-intercept at n\u22486 in Fig. 4-bottom.\nWe illustrate that high internal variability skews the benchmarking scores towards a lower complexity emulator by fitting a linear trend to $\\Delta RMSE(n)$ and observing a negative slope. The linear fits are shown in Fig. C6 and have a slope of -0.0005767 for spatial precipitation and a slope of -0.001224 for spatial surface temperature. We fit the linear trend only over the x-axis inset n \u2208 [0, 20], because the subsets contain an increasing number of duplicates across n, which increasingly flattens out the slope even if there is still leftover internal variability, a known issue of ensemble analyses . We discuss possible reasons for internal variability affecting benchmarking scores in Section 4.2 and implications in Section 4.3."}, {"title": "4 Discussion", "content": "Our analysis indicates that quantitative comparisons with LPS can help determine if deep learning-based emulators are making meaningful progress on known issues in climate emulation. For example, it is known that the relationships between global annual mean surface temperature vs. cumulative CO2 emissions and local vs. global annual mean surface temperature are predominantly linear and these could be seen as minimum criteria for validating deep learning-based emulators. Deep-learning-based emulators may also hold more promise for emulating variables and statistics that are known limitations of LPS, such as emulating time-lagged responses in overshoot scenarios, abrupt nonlinear transitions, bifurcations, higher spatiotemporal resolutions, or extremes in regions with narrow temperature distributions . Further, nonlocal aerosols or convection over tropical oceans can impact precipitation nonlinearly and are not captured in LPS. Our analysis has shown that a deep learning-based emulator can improve marginally on LPS for emulating the Em-MPI spatial precipitation targets (see Table C1). But, many nonlinear phenomena remain for studying in which ways advanced emulation techniques could improve on LPS baselines.\nGenerally speaking, our study emphasizes the importance of benchmarking machine learning (ML)-based emulators against established techniques. While the allure of novel ML approaches is understandable, it is not a good reason to neglect well-developed methods such as LPS. Continuing to neglect well-developed methods may have been able to lead to an overestimation of ML's effectiveness, such as, transformer-based emulators being cited as the most accurate emulators of surface temperature. Our work serves as a cautionary tale, highlighting the possibility of overlooking simpler techniques that may achieve comparable or superior performance."}, {"title": "4.2 Deep learning over- and LPS underfitting on noisy data.", "content": "Deep learning models are in theory able to learn functional relationships that are linear and/or noisy . Yet, the CNN-LSTM does not match the performance of LPS on the predominantly linear spatial temperature targets or the noisy spatial precipitation targets, in case of the target dataset being a 3-member ensemble mean (see Table 1 for NorESM2-LM and Figs. 4 and 5; n=3 for Em-MPI targets). The small size of the training dataset (here < 1000 temporally correlated data samples) is likely a key reason for this"}, {"title": "4.3 Limitations and applicability of our results", "content": "The difference in spatial precipitation RMSE between the two emulation techniques is relatively low $(\\Delta RMSE < 0.015mm/day \\forall n \\in \\{1,..., N\\})$. In this context, we note that justifications to switch from LPS to a deep learning-based emulator will depend on the application and likely require additional application-oriented evaluation metrics. But, independent of the error magnitude, our work shows that reporting the difference on an average of 50 instead of 3 realizations will give a more robust estimate of an emulator's accuracy.\nWhile we illustrated the impact of internal variability only on emulators of spatial patterns of temperature and precipitation in the MPI-ESM1.2-LR model, we expect similar results to apply to many other variables and models. In particular, we would expect that the benchmark scores on the NorESM2-LM 3-member mean spatial precipitation targets are also affected by internal variability due to the illustrated high magnitude of internal variability. Further, regionally-resolved near-surface wind velocities, for example, are known"}, {"title": "5 Outlook and conclusion", "content": "Our work aims to improve the evaluation process of climate emulators by focusing on the impacts of internal variability. Besides internal variability, however, there are interesting directions towards improving climate emulation benchmarking (e.g., for ClimateBenchv2.0): First, adding more scenarios, especially, ones that only perturb a single forcer or contain overshoot scenarios could cover a larger and less correlated input space. Second, the current evaluation metric, NRMSE, can be dominated by local extremes and only measures the accuracy of a the predicted ensemble-mean. A future benchmark could add evaluation metrics that are more application-oriented (e.g., drought indices) or focus on assessing stochastic techniques, such as weather generators or diffusion models, that may aim to emulate the distribution of future climate states. A new benchmark could also include more diverse climate variables (e.g., highly varying regional winds or changes beyond 2100), a comparison with existing emulation techniques beyond LPS (e.g., or ), and a cross-validation strategy.\nIn conclusion, we showed that linear pattern scaling emulates the local annual mean temperature, precipitation, and extreme precipitation in ClimateBench more accurately than current deep learning emulators. We identified internal variability as a key reason for the comparatively good performance of linear pattern scaling on precipitation. This implies that addressing internal variability is necessary for benchmarking climate emulators. We assembled a data subset, Em-MPI, from the MPI-ESM1.2-LR ensemble that contains 50 instead of 3 realizations and can be used to augment the NorESM2-LM temperature and precipitation datasets in ClimateBench. Using the new dataset we showed that a CNN-LSTM can be more accurate for emulating precipitation, while linear pattern scaling continues to be the more accurate emulator for surface temperature anomalies, in terms of spatial RMSE scores."}, {"title": "Appendix A Definitions", "content": ""}, {"title": "A1 Background on internal variability", "content": "Internal variability refers to natural fluctuations in weather and climate. In climate modeling, for example, fluctuations that occur on timescales of 2 10 years can be described by climate modes, such as the El Ni\u00f1o Southern Oscillation, Madden-Julian Oscillation, or Pacific Decadal Oscillation. The magnitude of the resulting interannual fluctuations can be at the same scale or higher than the emission-induced climate change, depending on the climate variable of interest. It is known that the exact transition times of those climate modes is unpredictable on climate timescales . Thus, climate scientists often treat the fluctuations as 'climate noise' and identify the signal by taking an ensemble average over a sufficiently large set of climate simulations . These ensembles can be called single model initial-condition ensembles (SMILES) and contain multiple realizations, i.e., simulations, runs, or members, of a single model that is forced by the same emission pathway, but slightly different initial conditions on every run."}, {"title": "Appendix B Extended Data & Methods", "content": ""}, {"title": "B1 Appendix to Background on ClimateBench dataset, evaluation protocol, and metrics", "content": ""}, {"title": "B11 Definition of variables in ClimateBench inputs and targets", "content": "The variables are defined as:\n\u2022 $CO_{2;t}$ are the carbon dioxide emissions, from anthropogenic sources, summed over the globe and the year, t. The cumulative carbon dioxide emissions, $CO_{2um}$, are summed from 1850 until the end of the target year, t. We visualize the data in gigatons of carbon dioxide, GtCO2, (one gigaton is one billion tons) in Fig. B1.\n\u2022 $CH_{4;t}$ are the anthropogenic methane emissions summed over the globe and year, t.\n\u2022 $SO_{2;i,j,t}$ are the anthropogenic sulfur-dioxide emissions summed over the area of grid cell (i, j) and year, t. Figure B1 shows that the emission pathways for SO2 contain a discontinuity between historical and ssp scenarios at year 2014-15. This discontinuity exists in the official CMIP6 data and has been updated in the 2022 CEDS data, as illustrated in C. J. Smith et al. (2021)-Fig. S2, but the NorESM2-LM and Em-MPI targets were still generated with the official CMIP6 forcings.\n\u2022 $BC_{i,j,t}$ are the anthropogenic black carbon emissions summed over the area of grid point (i, j) and year, t.\n\u2022 $tas_{i,j,t}$ are the annual mean surface temperature anomalies at 2m height. The surface temperature anomaly is the surface temperature minus the average surface temperature of the preindustrial control simulation at each grid point. The unit is deg C. The globally-averaged values are plotted in Fig. B2.\n\u2022 $dtr_{i,j,t}$ is the annual mean diurnal temperature range anomaly wrt. the preindustrial control run in deg C. The diurnal temperature range was calculated by first subtracting the minimum from the maximum daily temperature and then averaging over the year.\n\u2022 $pr_{i,j,t}$ is the annual mean total precipitation per day in $mm/day$. The value is reported as anomaly wrt. to the preindustrial control run.\n\u2022 $pr90_{i,j,t}$ is the 90th percentile of the daily precipitation in each year.\n\u2022 We refer to Watson-Parris et al. (2022) for more details.\nTo analyze the linearity we have plotted the correlation between temperature and global CO2 in Fig. B2."}, {"title": "B12 Evaluation metrics in ClimateBench", "content": "For the results table in ClimateBench (see Table 1), the RMSE is normalized by the magnitude of the target variable to compute the NRMSEs, where we denote the placeholder * \u2208 {s, g}:\n$NRMSE_{*}(Y_e, \\hat{Y}_e) = \\frac{1}{\\text{labs}} \\frac{RMSE_{*}}{\\frac{1}{|I|} \\sum_{i} \\sum_{j} \\sum_{t} \\frac{1}{|M|} \\sum_{m} \\sum_{e} Y_{i,j,t,m,e}}$\nThe ClimateBench evaluation protocol and resutls table also contain the weighted sum of 5\u00d7 global + spatial NRMSE, called 'total' NRMSE. The total NRMSE was proposed in the ClimateBench paper as a metric on which the 'best' emulator can be declared and the factor of 5 was chosen to equalize the scales between both errors.\nThe ClimateBench evaluation metrics in Eqs. (1), (2) and (B1) are consistent with the ClimateBench paper Sect. 3.1. Eq. (1-3) except for a slightly different notation."}, {"title": "B2 Appendix to Em-MPI data", "content": "We decided against using the CanESM5 model, following the recommendation to avoid using 'hot' models. The CanESM5 has a transient climate response (TCR) of \u2248 2.7\u00b0C  and an equilibrium climate sensitivity (ECS) of \u2248 5.6\u00b0C . This is outside of the likely 1.4 \u2013 2.2\u00b0C TCR and the very likely 2.3 \u2013 4.7\u00b0C ECS range . We decided against using EC-Earth3 because many realizations were unavailable on the Earth System Grid Federation (ESGF) Portal. For MPI-ESM1.2-LR all realizations were either already available or made available on ESGF after inquiry with the data owners."}, {"title": "B3 Details of the CNN-LSTM-based emulator", "content": "The CNN-LSTM in the ClimateBench results table uses the spatial input fields of CO2, CH4, BC, and SO2 over a 10-year time window as input, resulting in the input shape: (4, I, J, ht) with the time history length ht = 10 years. The input grid size, (I = 96, J = 144), is independent of which targets are being predicted. As CO2 and CH4 are global variables, their input fields are filled with a single scalar value that is repeated along the I, J dimensions.\nThe emulator predicts the spatial field for the target year at the end of the time history with the output shape (1, I, J, 1) with (I, J) = (96,144) or $(I_{\\text{em-mpi}}, J_{\\text{em-mpi}})$ = (96, 192) for the NorESM2-LM or Em-MPI targets, respectively. For each target climate variable, a different CNN-LSTM is trained independently. The input variables are normalized to zero-"}, {"title": "B4 Appendix to Testing the influence of internal variability", "content": "The mean RMSE in the internal variability experiment for the CNN-LSTM with random weight initialization seed, l, is calculated as:\n$RMSE^{\\text{Exp}}_{*,n} = \\frac{1}{K} \\sum_{k \\in \\{1,...,K\\}} \\frac{1}{L} \\sum_{l \\in \\{1,...,L\\}} RMSE_*(f^{k,l}_{n,k,l}(X_{\\text{test}}), \\frac{1}{|M|} \\sum_{m \\in M} Y_{m,e})$"}, {"title": "Appendix C Extended Results & Discussion", "content": ""}, {"title": "C1 Appendix to Evaluation of Linear Pattern Scaling on ClimateBench", "content": "For completeness, we repeat the linear pattern scaling error map for diurnal temperature range, precipitation, and 90th percentile precipitation in Figs. C1 to C3, respectively. Equivalent plots for the Gaussian Process, Neural Network, and Random Forest emulators can be found in (Watson-Parris et al., 2022)-Fig. 4."}, {"title": "C2 Appendix to Magnitude of internal variability in 3-member NorESM2-LM and 50-member MPI-ESM1.2-LR ensemble average", "content": "Figure C4 shows the internal variability in the 3-member ensemble mean of the NorESM2-LM (used in ClimateBench) and 50-member Em-MPI data for multiple regions. The regions were selected to representatively illustrate the local variation in the dataset."}, {"title": "C3 Appendix to Effect of internal variability on benchmarking scores", "content": "Figure C5 shows the difference in spatial RMSE in surface temperature over the number of realizations."}]}