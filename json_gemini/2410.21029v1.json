{"title": "FAIRSTREAM: FAIR MULTIMEDIA STREAMING BENCHMARK FOR REINFORCEMENT LEARNING AGENTS", "authors": ["Jannis Weil", "Julian Barthel", "Jonas Ringsdorf", "Yi-Ping Phoebe Chen", "Tobias Meuser"], "abstract": "Multimedia streaming accounts for the majority of traffic in today's internet. Mechanisms like adaptive bitrate streaming control the bitrate of a stream based on the estimated bandwidth, ideally resulting in smooth playback and a good Quality of Experience (QoE). However, selecting the optimal bitrate is challenging under volatile network conditions. This motivated researchers to train Reinforcement Learning (RL) agents for multimedia streaming. The considered training environments are often simplified, leading to promising results with limited applicability. Additionally, the QoE fairness across multiple streams is seldom considered by recent RL approaches. With this work, we propose a novel multi-agent environment that comprises multiple challenges of fair multimedia streaming: partial observability, multiple objectives, agent heterogeneity and asynchronicity. We provide and analyze baseline approaches across five different traffic classes to gain detailed insights into the behavior of the considered agents, and show that the commonly used Proximal Policy Optimization (PPO) algorithm is outperformed by a simple greedy heuristic. Future work includes the adaptation of multi-agent RL algorithms and further expansions of the environment.", "sections": [{"title": "1 Introduction", "content": "Multi-Agent Reinforcement Learning (MARL) has been widely applied in the field of communication systems, as many control problems from this field can be framed as finding an optimal policy in a multi-agent system [16]. These problems cover a variety of challenges from basic Reinforcement Learning (RL) research. We will briefly describe selected challenges in the following.\nPartial observability and multiple objectives are very common in communication systems and frequently considered by related works in that area. Partial observability is given when agents do not"}, {"title": "2 Scenario and Challenges", "content": "The main goal of our streaming environment is to provide a benchmark environment for selected challenges in MARL research and to improve comparability in the area of fair multimedia streaming. Each agent in the environment controls a streaming client and aims to maximize its quality while considering fairness across all clients. An exemplary scenario is shown in Fig. 1. The following sections describe this scenario by highlighting the main emerging challenges."}, {"title": "2.1 Partial Observability", "content": "We consider heterogeneous streaming clients that access services from different content providers over a shared bottleneck link with a time-varying bandwidth (see Fig. 1 A). While most related works"}, {"title": "2.2 Agent Asynchronicity", "content": "Clients in real communication systems usually act asynchronously in an event-based manner. In our streaming scenario, agents select a new bitrate each time the assigned streaming client finishes downloading a segment (see Fig. 1 B). The time between steps varies, as the download duration of a segment depends on the available bandwidth and the chosen bitrate. Additionally, there are no constraints on the ordering of the clients or the frequency of the steps. One client might advance multiple steps during a single step of another client.\nThis is challenging, because the agents may act at different time scales. In contrast, existing MARL approaches [7, 2] typically assume synchronous and equidistant steps."}, {"title": "2.3 Agent Heterogeneity", "content": "Clients in streaming applications usually have different requirements based on the content type, level of user interactivity, device type and display resolution (see clients 0 and 1 in Fig. 1 A). These factors strongly influence the users' quality perception when interacting with the content [17] and should be considered by Adaptive Bitrate (ABR) algorithms [28]. For example, mobile users might not notice artifacts that are obvious when viewing the same content on a television or with a Head-Mounted Display. To achieve the same QoE, mobile users might be satisfied with lower bitrates. For our scenario, we assume that the perceptual quality of a client is a function of the stream's bitrate $q^i(b)$ (see Fig. 1 C), similar to the work by Mao et al. [20] for homogeneous clients. In the heterogeneous case, however, the functions may differ between the clients [6, 31]. Higher bitrates are associated with higher perceptual qualities, but will lead to rebuffering if the total demanded bitrate of all clients exceeds the bandwidth of a shared bottleneck link.\nThis is challenging, as the reward functions differ between clients and the corresponding agents have to learn different behavior."}, {"title": "2.4 Multiple Objectives", "content": "The objective of our fair streaming scenario consists of two factors, a QoE metric $QoE^i$ for each client $i \\in I$ and a fairness metric $F$ over all clients (see Fig. 1 C), similar to previous works that consider QoE fairness [2, 28, 24]. Each client wants to maximize its own QoE, while ensuring fairness over all clients. We assume that a given bandwidth $bw_{total}$ of a shared bottleneck restricts the bitrates that can be chosen by the clients without leading to rebuffering, and therefore to a significant deterioration of the perceived quality of each individual client.\nThis is challenging, because with given bandwidth constraints, clients usually cannot maximize their own QoE and achieve high fairness at the same time. Instead, they have to trade off between the two components."}, {"title": "3 Problem Statement", "content": "Considering these challenges, we now formulate the optimization problem associated with the streaming environment. Its objective is composed of the individual QoE of a client and the fairness across clients, which are detailed in the following subsections. Finally, we propose a time-independent variant of the problem that will later be used to analyze the space of optimal solutions."}, {"title": "3.1 Quality of Experience", "content": "Our platform allows researchers to integrate desired quality metrics for arbitrary content types. For reference, we define the QoE of a streaming client based on the model by Yin et al. [35], which has been widely adopted by related works [20, 24]. To keep the QoE values bounded, we follow the suggestions of ITU-T Rec. P.1203.3 [12] and use an exponential decay to model the impact of the initial stalling delay and rebuffering during playback. Under these considerations, we define the QoE of streaming client i at timestep t > 0 as\n$QoE^i(\\tau) := \\frac{q^i(b_t) + \\delta [1 - |q^i(b_t) - q^i(b_{t-1})|]_{t>0}}{1 + [\\delta]_{t>0}} \\cdot exp(-A_{init} T_{init} (s_0^i) - A_{reb} T_{reb} (s_t^i)),$\nwhere $\\tau^i = (s_{-1}^i, b_0^i, s_0^i, ..., b_t^i, s_t^i)$ is the trajectory of client i after downloading segment t > 0. It contains the initial state $s_{-1}^i$ and all states $s_k^i$ after downloading segment $0 \\leq k \\leq t$ with bitrate $b_t^i \\in B$ from a finite set of available bitrates $B^i$.\nThe first factor consists of the difference between the normalized perceptual quality $q^i(b) \\in [0, 1]$ for bitrate $b_t$ with segment t of agent i, and the switching penalty for the quality changes between the current and the previous segment $|q^i(b_t) - q^i(b_{t-1})| \\in [0, 1]$, weighted by a coefficient $\\delta > 0$. The switching penality for the initial step t = 0 is zero, as indicated by the brackets. We normalize the segment quality with the switching penality to [0, 1] for improved interpretability.\nThe second factor represents the impact of rebuffering on the quality. It is split into an initial buffering time $T_{init}(s_0^i) \\in [0, \\infty)$ when starting the stream and the rebuffering time during playback $T_{reb}(s_t^i) \\in [0, \\infty)$ with coefficients $A_{init}, A_{reb} \\geq 0$. Rebuffering significantly affects the perceived quality and should be avoided at all times by switching to lower bitrates when necessary.\nNote that the perceptual quality $q^i(b)$ can be any function that represents the perceived quality for a given segment. Examples include full-reference metrics like the Structural Similarity Index Measure (SSIM) [32] and fused quality assessment methods like VMAF [17]. The same content can lead to different perceived qualities based on the media type, the device that is used for streaming, and the users' preferences, i.e., the perceptual quality of two clients $j \\neq i$ may differ $q^i \\neq q^j$ ."}, {"title": "3.2 Fairness", "content": "Fairness between clients can be interpreted in various ways. Let $\\vec{v} := (v^1,..., v^l) \\in R^I$ be a quality vector of l clients for some quality metric. Fairness measures map $\\vec{v}$ to a score that represents the fairness of this solution. This work focuses on fairness in terms of QoE between clients.\nWhile notions of fairness such as Jain's fairness index [14] and max-min fairness are widely applied in the context of communication systems, the QoE fairness index F by Ho\u00dffeld et al. is specifically designed with QoE in mind [9]:\n$F(\\vec{v}) := 1 - \\frac{\\sigma(\\vec{v})}{\\frac{H - L}{2}} = 1 - \\frac{2\\sigma(\\vec{v})}{H-L},$\nBy normalizing the standard deviation $\\sigma(\\vec{v})$ of the qualities according to their range $v_k \\in [L, H]$, F is independent of the quality range. This is important when comparing QoE definitions with different ranges. Kim and Chung [15] follow a similar line of thought by normalizing QoE differences in their utility function. We integrate the normalization directly into the QoE definition, see Eq. (1). The fairness index F yields scores in [0, 1], where higher values indicate higher fairness. A value of 0 is reached for the maximum standard deviation, i.e. when half of the clients retrieve scores L and H, respectively. A value of 1 indicates that all clients have the same QoE.\nTo capture the average streaming quality over time, we consider fairness over an exponential moving average [25] of the QoE with smoothing factor $\\kappa \\in [0, 1]$\n$\\tilde{v}_t^i = \\begin{cases}  \\frac{1}{\\kappa}  & \\text{if } t = -1 \\\\  \\kappa \\tilde{v}_{t-1}^i + (1 - \\kappa)QoE^i(\\tau) & \\text{otherwise}  \\end{cases},$\nThis only considers clients $i \\in L_{ri} \\subseteq I$ that are streaming at simulation time $T_i \\in R$, denoting the elapsed time since starting the environment when client i finished performing step t $\\geq$ 0.\nThe vector of exponentially averaged QoE values at the time when client i finishes downloading segment t is given as\n$\\vec{\\upsilon} := (\\tilde{v}_{last^1(T_i)}^1,..., \\tilde{v}_{last^l(T_i)}^l),$\nwhere $last^k(T) \\in N$ indicates the last completed step of client k at simulation time T $\\in$ R. The moving average is bounded $\\upsilon^i \\in [0, 1]^I$ and used as input for fairness index F."}, {"title": "3.3 Utility Function", "content": "The agents' goal is to maximize their QoE and the fairness between clients. We consider the naive linear combination\n$U(\\tau) := \\alpha QoE(\\tau) + (1 - \\alpha)F(\\vec{\\upsilon}) =  \\alpha QoE^i(\\tau_i) + (1 - \\alpha) (1 - \\frac{2\\sigma(\\vec{\\upsilon}))}{H-L}),$\nfor each client i with a quality-fairness coefficient $\\alpha \\in [0, 1]$ and the trajectories $\\vec{\\tau} := (\\tau_{last^1(T_i)}^1,..., \\tau_{last^l(T_i)}^l)$ of all clients from the perspective of client i. In this formulation, the optimal client behavior depends on the quality-fairness coefficient $\\alpha$. A high $\\alpha \\rightarrow 1$ leads to selfish clients that try to maximize their own QoE, while $\\alpha \\rightarrow 0$ leads to clients that neglect their own quality to prioritize fairness. In the following, we refer to this combined objective as utility and will explore the effect of coefficient $\\alpha$ based on a time-independent version of this problem."}, {"title": "3.4 Time-Independent Formulation", "content": "For a computationally tractable investigation of the effect of coefficient $\\alpha$ on the space of optimal solutions, we propose a simplified version of the optimization problem with a time-independent bandwidth. Instead of storing downloaded segments in a buffer, we assume that segments of infinitesimal size are played back instantly. The number and type of clients are fixed for the whole duration and"}, {"title": "4 Environment and Experiment Design", "content": "In the following, we build upon the problem statement and describe the streaming environment."}, {"title": "4.1 Streaming Clients", "content": "In our streaming environment, each agent controls a multimedia client. We consider four exemplary client types. Their individual perceptual quality functions $q^i(b)$ are shown in Fig. 2. The first three client types represent streaming of traditional video on a phone (Phone), a HD television (HDTV), and a 4K television (4KTV). The mapping from bitrates to perceptual qualities is based on results of the Video Multi-Method Assessment Fusion (VMAF) model [17] for the Big Buck Bunny movie [1]. The last client represents streaming a Point Cloud Video (PCV) with normalized quality values taken from a subjective study [33]. We select seven quality settings for each client type. Further details are provided in the appendix, see Sec. A.1.\nAll clients reach higher quality levels for higher bitrates, but the increase in quality per bit depends on the client type. For example, the PCV stream requires more than 7.5 Mbps to reach a quality that is comparable with the lowest quality setting of the Phone stream at around 0.5 Mbps.\nEach agent controls a client and has a discrete action space according to the available bitrates $B^i$. Based on the considered quality settings, each agent has $|B^i| = 7$ actions. In our case, the minimum total bandwidth required for all clients to stream seamlessly with the lowest quality is 2.75 Mbps. For all clients to stream with the highest quality, a total bandwidth of 82.68 Mbps is required."}, {"title": "4.2 Bandwidth of Bottleneck Link", "content": "We assume that the bandwidth of the bottleneck link varies over time. Consequently, the agents have to estimate the available bandwidth in order to select bitrates that yield the highest utility. The simulations should cover a variety of different scenarios. For example, there should be scenarios"}, {"title": "4.3 Bandwidth Allocation", "content": "Motivated by the bandwidth slicing approach of Nathan et al. [24], we consider a weight-based bandwidth allocation mechanism. At simulation time T, the total bandwidth of the bottleneck link $bw_{total}(T)$ is distributed across all clients that are downloading segments $D_T \\subseteq I$ according to\n$bw_{weighted}^i(T) := bw_{total}(T) \\frac{w_i}{\\sum_{k \\in D_T} w_k},i \\in D_T,$\nwhere $w_k$ is the weight of client k $\\in D_T$.\nNote that this covers static and dynamic bandwidth allocation schemes. For example, fixing $w_i = 1$ for all clients would result in equal bandwidth allocation irrespective of the demands of each client, similar to the concept of TCP fairness. In comparison, setting $w_i = b_t^i$ results in a bandwidth allocation that is proportional to the requested bitrates $b_t^i$. In this case, segments of equal duration would require the same time to download, irrespective of the individual bitrates."}, {"title": "4.4 Observations", "content": "At the beginning of an episode and after downloading a segment $s_t^i$ at step t, agent i receives a partial observation $o_t^i$ representing the view of the client. This observation is of form\n$o_t^i := (QoE^i(\\tau_t^i), \\upsilon_t^i, q^i(b_t^i), b_t^i, \\Delta T_t^i, T_{init}(s_0^i), T_{reb}(s_t^i), bf_t^i, n_t^i, \\vec{b_t^i}, \\vec{q_t^i}),$\nwith the following elements\n$QoE^i(\\tau_t^i)$ The QoE associated with this segment. This component is directly connected to the reward of the agents. Agents should learn to increase their QoE by adapting their bitrate depending on the network conditions.\n$\\upsilon_t^i$ The exponential moving average of the QoE, as considered in the fairness metric. Note that the fairness is not included in the agent's observations, because it requires global knowledge.\n$q^i(b_t^i)$ The perceptual quality of the segment. It is the main contributor of the QoE. Agents should learn to increase their perceptual quality in an under-utilized network.\n$b_t^i$ The bitrate of the downloaded segment.\n$\\Delta T_t^i$ The time spent downloading the last segment $\\Delta T_t^i := T_t - T_{t-1}$. Especially in the beginning, it is crucial to keep the download time lower than the segment time to avoid re-buffering. Short download durations suggest that a higher bitrate can be chosen.\n$T_{init}(s_0^i)$ The initial stalling time while downloading the last segment, only happens at the beginning of the stream."}, {"title": "4.5 Optimal Solutions of the Time-Independent Formulation", "content": "We expect the choice of the quality-fairness coefficient $\\alpha$ from Eq. (5) to strongly affect the optimal agent behavior. In order to make a well-founded selection of $\\alpha$ for the reward function, we first analyze the space of optimal solutions of the time-independent formulation in Eq. (6) from Sec. 3.4.\nFig. 4 provides an overview of the feasible and pareto-optimal solutions of the problem, using the total bitrate of a solution as the bandwidth constraint. The feasible solutions represent all bitrate combinations that can be selected by clients. With pareto-optimal solutions, we denote feasible solutions that are not dominated by other solutions with a lower or equal total bitrate, i.e. there are no solutions with less resource requirements that have the same or higher quality and fairness and a higher sum of both metrics. Depending on the parameter $\\alpha$ and a given bandwidth $bw_{total}$, the optimal solutions of Eq. (6) are a subset of these pareto-optimal solutions. The plot suggests that bandwidths below 25 Mbps will be the most challenging for learning-based approaches, as the pareto-optimal solutions differ greatly and small changes in an agent's actions might have a big effect on its return. Above 25 Mbps, the solutions are quickly getting close to the optimum of (1, 1) with smaller steps. This should be easier to learn.\nFig. 5 shows the optimal solutions for the clients from Fig. 2 for different bandwidths $bw_{total}$ and $\\alpha \\in \\{0.25, 0.5, 0.75\\}$. Remember that the quality-fairness coefficient $\\alpha$ balances quality and fair-ness in the objective. A higher $\\alpha$ means that more weight is assigned to the QoE. The figure indicates that this leads to more frequent and earlier bitrate changes, while the difference between the qualities increases. Although this leads to a higher mean quality, these rapid quality changes would be undesirable with respect to the stability of the learning target and result in lower fairness."}, {"title": "4.6 Reward Function", "content": "Based on the results from Sec. 4.5, we set $\\alpha = 0.25$ for the majority of our experiments to reduce the possibility that fairness gets overshadowed by the client's individual QoE. The switching penality coefficient is set to $\\delta = 0.025$ analogous to the paramaters chosen by Nathan et al. [24]. The rebuffering penalty coefficients are $A_{init} = 1$ and $A_{reb} = 10$, with the intention to greatly reduce the QoE during playback upon rebuffering. For example, rebuffering for 0.1 seconds reduces QoE by around 63%. The resulting reward $R_t^i$ of agent i after selecting bitrate $b_t^i$ at at step t is given by\n$R_t^i := U_{.25}^i(\\tau),$"}, {"title": "5 Results", "content": "We implement the environment based on the RLlib multi-agent interface [18] and consider the following agent types:\n\u2022 Min: Always chooses the minimum bitrate.\n\u2022 Max: Always chooses the maximum bitrate.\n\u2022 Random: Chooses a random bitrate at each step.\n\u2022 Greedy-k: Heuristic baseline agent that was implemented for this environment. The agent initially selects the minimum bitrate. Then, it computes the average download rate of up to k previous segments. Assuming this is the available bandwidth of future steps, the agent greedily selects the maximum bitrate that could be streamed without rebuffering.\n\u2022 PPO: Agents trained using the Proximal Policy Optimization (PPO) algorithm [27]. PPO is considered one of the state-of-the-art RL algorithms [36] and has shown to outperform previous approaches when learning ABR control [23].\nFor bandwidth sharing, we consider two modes:\n\u2022 Proportional: The bandwidth is distributed proportionally to the selected bitrate of each client.\n\u2022 Minerva: The Minerva algorithm by Nathan et al. [24] considers a linear interpolation between the discrete bitrate-quality mappings of each client and computes the bandwidth shares that would allow each agent to stream at the same interpolated quality."}, {"title": "5.1 Evaluation on the Test Traces", "content": "The performance of all approaches on the test traces is summarized in Tab. 2 and Fig. 7. The parameter of the Greedy-k agent was set to k = 8 as a trade-off between adaptivity and stability. More details regarding this choice are in the appendix, see Sec. A.2. While the Min agent allows streaming with nearly no interruptions, the corresponding QoE varies across the different clients, resulting in a high QoE standard deviation in Fig. 7 b) and a low fairness in Fig. 7 c). Conversely, the Max agent is very close to maximum fairness. However, this is accompanied by a very low QoE due to excessive rebuffering. In the veryhigh trace class, the QoE of the Max agent shows a high standard deviation. This is because some traces from the veryhigh class allow all clients to stream at the highest quality, while others have insufficient bandwidth. The random agent outperforms the Min and Max agents in terms of reward in four out of five classes. The Greedy-8 agent outperforms"}, {"title": "5.2 Validation Results during Training", "content": "Fig. 8 shows the return and mean rebuffer time of the PPO agent on the validation traces during training. All traffic classes show a noticeable increase in reward over the first 100 training iterations, the results afterwards are comparatively stable. The standard deviation for the low, normal, and fluctuation traffic classes is considerably higher than for the high and veryhigh traffic classes. This is consistent with the test results in Fig. 7 a). The rebuffering time on the high and veryhigh traces is close to zero and the standard deviation on the veryhigh traces decreases during training. While the rebuffering time for the remaining traffic classes low, normal, and fluctuation decreases considerably during training, the PPO agent is unable to reduce it to zero."}, {"title": "5.3 Effect of Heterogeneous Clients", "content": "The previous sections have shown that the performance of the agents varies across the different traffic classes. Additionally, the agents have heterogeneous requirements. Fig. 9 shows the mean QoE and the mean selected bitrate by the Greedy-8-Minerva and PPO agents on the test traces. On the left side, we can see that the median QoE of the PPO agent is slightly lower than the median QoE of the Greedy-8-Minerva agent, while showing a much higher variance. This is an indication for rebuffering of the agents, which is consistent with the findings from the previous subsections. On the right side, we can see that the download rate of both approaches increases from left to right with incrementally higher resource requirements. While the variance of the Greedy-8-Minerva agent is rather consistent across the different clients, the PPO agent shows a high variance for the PVC client and a comparably low variance for the other client types."}, {"title": "5.4 Behavior for an Exemplary Test Trace", "content": "Fig. 10 shows the behavior of Greedy-8-Minerva and PPO for an exemplary test trace from the fluctuation class. The Greedy-8-Minerva agents react swiftly to the declining bandwidth at around 60 seconds and finish downloading below 100 seconds with very little rebuffering. The PPO agents also reduce their bitrate at around 70 seconds, but react too greedily to short bandwidth spikes."}, {"title": "5.5 Effect of Quality-Fairness Coefficient", "content": "The previous results for the PPO agent are for a quality-fairness coefficient $\\alpha = 0.25$. Fig. 11 shows these results in comparison with PPO agents trained using $\\alpha = 0.5$ and $\\alpha = 0.75$. We can see that none of the selected values for $\\alpha$ leads to behavior that clearly dominates the others in terms of QoE and fairness. As expected, a higher $\\alpha$ tends to increase the QoE at the cost of decreased fairness. One side-effect of increasing the QoE is that $\\alpha = 0.5$ and $\\alpha = 0.75$ show lower rebuffering times during playback, as rebuffering drastically reduces the QoE. However, they still show rebuffering during playback except for the high and veryhigh traffic classes. In these classes, even PPO with $\\alpha = 0.25$ was able to achieve very low rebuffering times. At the same time, the initial rebuffering times for $\\alpha = 0.5$ and $\\alpha = 0.75$ increase as the agents presumably select higher initial bitrates to reduce the negative effect of the switching penalty. This shows that agents find yet another undesirable trade off between two factors in the QoE definition in order to maximize their return."}, {"title": "6 Discussion", "content": "The results from Sec. 5 demonstrate that a simple greedy heuristic based on the previously measured bandwidths can achieve comparably good results on average. A combination with the Minerva bandwidth sharing approach from Nathan et al. [24] leads to even higher returns due to increased fairness. Our experiments show that this is a tough baseline for learning approaches. While the PPO agent outperforms the Greedy-8 baseline without Minerva, it has high rebuffering times in three out of five traffic classes and would be unusable in practice. In particular, we find that the agent is too greedy with respect to changes in the bitrate. Combining the PPO agent with Minerva leads to worse results. A potential reason for this is that agents with the proportional bandwidth allocation are synchronous, while Minerva bandwidth allocation introduces asynchronicity.\nWith a quality-fairness coefficient of $\\alpha = 0.25$, the PPO agent prioritizes fairness over QoE and fails to reach acceptable QoE on many traces due to a high amount of rebuffering. The reason for this is that a low QoE caused by rebuffering leads to high fairness, as illustrated by the Max agent in Fig. 7. While the Max agent has poor returns due to clearly suboptimal behavior, the PPO agent learns a trade off between QoE and fairness that is effective in terms of return, but undesirable in practice. Sec. 5.5 suggest that simply changing the value of $\\alpha$ does not suffice, as even $\\alpha = 0.75$ shows rebuffering with simultaneously decreased fairness. The high initial rebuffering times for $\\alpha = 0.75$ in Fig. 11 suggests that increasing the initial rebuffering penality coefficient $A_{init}$ could"}, {"title": "7 Related Work", "content": "The application of RL in communication networks covers various domains and is expected to play an essential role in the future internet [19, 16]. This work focuses on adaptive bitrate control for multimedia streaming, which has been extensively studied with traditional heuristics and deep RL approaches [3]. Mao et al. [20] and Gadaleta et al. [5] consider a single client that interacts with a streaming environment. Leveraging the popular RL algorithms Asynchronous Advantage Actor-Critic [21] and Deep Q-Networks [22], they show that learning-based ABR approaches can outperform previous ABR algorithms under various network conditions.\nWhen considering multiple clients and shared resources in a streaming system, the objective commonly contains a metric that is shared across all clients, e.g. fairness or joint throughput [6]. While carefully designed ABR heuristics address the trade-off between the QoE of individual clients and the fairness across all clients [24, 26, 28], recent related work started to explore whether learning-based approaches can yield further improvements in the multi-agent setting [31]. For example, Altamimi and Shirmohammadi [2] propose a RL-based ABR approach that controls streams to multiple clients via a centralized server in order to improve the clients' individual QoE and the fairness across clients. Instead of learning the ABR policy directly, they let agents synchronously select the set of bitrates that a given ABR algorithm can choose from and show that their method outperforms previous approaches. Han et al. [7] consider streaming from a different perspective, where agents represent multiple QUIC paths of a multimedia streaming application with a single client. The agents in this environment are heterogeneous, as the paths may utilize different transmission mechanisms. Considering a combination of individual and shared objectives across agents, they show that their MARL approach can outperform previous state-of-the-art scheduling algorithms.\nAs learning-based approaches are predominately trained and evaluated in simulations, the question arises if these results are representative. Yan et al. [34] argue that in a real-world setting, it is difficult for learning-based ABR methods to outperform even simple heuristics. A potential reason is that the experiment setup of current learning approaches does not correctly capture the heavy-tailed trace distributions of the real internet. Huang et al. [10] argue that defining the correct weights for individual objectives in learning-based ABR approaches is challenging, as a linear combination with fixed weights can hardly represent the requirements of all types of traffic.\nFor the future of RL methods for ABR, we think that it is essential for agents trained in simulations to analyze their behavior with respect to different trace distributions and different weights for individual objectives. We also find that existing multi-agent approaches for fair streaming come with assumptions that limit their real-world applicability, in particular centralized control, homogeneous agents and synchronous steps. With our work, we propose a novel multi-agent environment that addresses this research gap and takes a step towards lifting these assumptions."}, {"title": "8 Conclusion", "content": "With this paper, we model the problem of fair multimedia streaming and propose a novel multi-agent environment that encompasses the challenges of partial observability, multiple objectives, agent heterogeneity and asynchronicity. We analyze the optimal solutions of a time-independent version of the problem and show that the problem is particularly challenging for lower bandwidths. This is in line with the empirical results from our experiments. We categorize the considered bandwidth traces into five classes and show that the agents' behavior can vary drastically between classes. While the combination of Minerva [24] and a greedy heuristic performs well across all traffic classes, we show that a naive PPO agent fails to learn behavior that would be acceptable in practice. In particular, the agent can only avoid rebuffering for traces with comparatively high and stable bandwidth. We argue that fine-tuning the weights of the objective to achieve subjectively better behavior is counter-productive, as this will likely depend on the given trace distribution. Instead, we suggest that future learning-based approaches for fair streaming should apply and extend methods from multi-objective RL and analyze the influence of each objective."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Perceptual Quality", "content": "The perceptual qualities from Fig. 2 for the 4K, HD and Phone clients are computed as follows:\n1. We downloaded the Big Buck Bunny movie as 4K PNG images from http://bbb3d.renderfarming.net/explore.html.\n2. We encoded the movie with x264 in 16:9 format with vertical resolutions 2160p, 1440p, 1080p, and 720p and target bitrates of 0.5, 1, 2.5, 5, 7.5, 10, and 20 Mbps.\n3. The score of an encoded video is given by the arithmetic average of the VMAF scores for all frames in the clip. For each client, we computed scores for all resolutions below or equal to the respective reference resolution at all bitrates. The HD and Phone clients use the vmaf_v0.6.1.json model with 1080p at 20 Mbps as the reference, and the 4K client uses the vmaf_4k_v0.6.1.json model with 2160p at 20 Mbps as the reference.\n4. For each client type and target bitrate, we selected the resolution that leads to the highest VMAF score.\n5. For each client, this results in a VMAF score vector $\\vec{v} \\in [20, 100]^7$, containing a score for each bitrate. We normalized the scores to range [0, 1]^7 with\n$\\frac{\\vec{v}-20}{max(\\vec{v})-20}.$\nFor the PCV client, we select 7 quality settings from a subjective study on the quality of point cloud sequences [33] in the near distance setting. As the QoE values $\\vec{p}$ are given in an Absolute Category Rating scale from 1 to 5, we normalize them analogously to the VMAFs with\n$\\frac{\\vec{p}-1}{max(\\vec{p})-1}$."}, {"title": "A.2 Effect of the Greedy Parameter", "content": "The results of Greedy-k and Greedy-k-Minerva for values $k \\in \\{1, 2, 4, 8, 16, 32\\}$ are shown in Fig. 12. While the mean return for Greedy-k keeps increasing with a higher value of k, the mean return of Greedy-k-Minerva decreases after a peak at k = 4.\nA possible"}]}