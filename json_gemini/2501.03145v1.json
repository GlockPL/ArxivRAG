{"title": "Geometry Restoration and Dewarping of Camera-Captured Document Images", "authors": ["Valery Istomin", "Oleg Pereziabov", "Ilya Afanasyev"], "abstract": "This research focuses on developing a method for restoring the topology of digital images of paper documents captured by a camera, using algorithms for detection, segmentation, geometry restoration, and dewarping. Our methodology employs deep learning (DL) for document outline detection, followed by computer vision (CV) to create a topological 2D grid using cubic polynomial interpolation and correct nonlinear distortions by remapping the image. Using classical CV methods makes the document topology restoration process more efficient and faster, as it requires significantly fewer computational resources and memory. We developed a new pipeline for automatic document dewarping and reconstruction, along with a framework and annotated dataset to demonstrate its efficiency. Our experiments confirm the promise of our methodology and its superiority over existing benchmarks (including mobile apps and popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both visually and in terms of document readability via Optical Character Recognition (OCR) and geometry restoration metrics. This paves the way for creating high-quality digital copies of paper documents and enhancing the efficiency of OCR systems.", "sections": [{"title": "1 Introduction", "content": "Digital document management is becoming increasingly important in the modern world, permeating all areas of public life, including government institutions, business, trade, healthcare, education, etc. As private companies and government agencies strive for efficient information management, the number of studies related to document digitization, electronic document management, image processing, and restoration increases. Among other things, the drivers of this process include: Implementation of Electronic Document Management Systems (EDMS) [1]; Growing popularity of cloud solutions for document storage and exchange; Digitization of documents and books [2]. Such a transition from paper-based document management to digital methods is becoming increasingly common due to the convenience of transferring, storing, searching, and processing documents [3]. However, creating a high-quality digital copy can require specialized equipment that may not always be available. Therefore, there is a need for effective, cheap and convenient methods to produce digital copies of paper media.\nWith the development of portable devices equipped with high-quality cameras, the process of converting documents into digital format has become much easier. Nevertheless, document images captured by a user camera, unlike the controlled operating environment of a scanner, often suffer from problems with illumination, shadows, unstable shooting conditions, and camera positioning, as well as distortions associated with both camera lens distortions and the physical deformation of the paper at the moment of photographing [4,5,6]. These factors affect the quality of document digitization and increase the complexity of information retrieval when using machine methods for digital document processing. To eliminate the influence of distortions in document image processing, a variety of approaches have been proposed in the literature (see Section 2). However, the automatic dewarping of digital copies captured by cameras remains a challenging research problem in computer vision and pattern recognition. The purpose of this work is to develop an automated method for restoring the image topology of a hard copy of a document obtained using a digital camera. The object of the study is an image of a document from camera shots, and the subject of the study is a pipeline of algorithms for detection, segmentation, geometry restoration, and text recognition from a document."}, {"title": "2 Background", "content": "In this section, we review papers and methods that analyze geometric distortions in document images and the solutions applied for restoring document geometry. Additionally, we examine popular mobile applications for document scanning, highlighting the main features and drawbacks of the digitization they provide."}, {"title": "2.1 Challenges and Methods in Document Geometry Restoration", "content": "Analyzing geometric distortions in document images is a key area of research in document processing, image recognition, restoration, and computer vision. Geometric distortions encompass changes in the shape, orientation, and size of document elements caused by factors such as perspective distortion at various camera angles and paper surface curvature. Understanding and quantifying these distortions are crucial for various applications, including optical character recognition (OCR), document classification, and content extraction. There exist various methods for analyzing document images to dewarp geometric distortions, including:\n\u2022\n\u2022\nFeature-based methods that rely on detecting and matching key features, such as corners, edges, or line intersections, to estimate geometric transformation parameters. For instance, methods like Scale-Invariant Feature Transform (SIFT) [7], Speeded-Up Robust Features (SURF) [8], and Random Sample Consensus (RANSAC) [9] are widely used for these purposes.\nPattern matching, where a pattern of document image is compared to a target image to find the best match, accounting for geometric transformations [10].\nDeep learning approaches that utilize Convolutional Neural Network (CNN) and Recurrent Neural Networks (RNN) to analyze geometric deformations [11,12].\nAmong these methods, DL approaches are increasingly used for document geometry restoration, as they can learn complex data dependencies and efficiently approximate nonlinear transformations. They are capable of automatically extracting objects from images and accurately restoring document geometry. However, the primary challenge with document geometry restoration using popular desktop DL methods is their lack of precision in delineating document boundaries, which often exacerbates image distortions due to blurred boundary boundaries. Additionally, applying OCR technologies to restored documents and counting the correctly recognized characters frequently reveals a high number of recognition errors (see Section 6.2). This poses a potential barrier to integrating such neural network-based document scanning technologies into electronic document management systems."}, {"title": "2.2 Related Work", "content": "Numerous studies focus on enhancing the dewarping of document images captured by mobile devices and improving OCR accuracy. As noted in [4, 13], there are learning-free and deep learning-based approaches.In traditional learning-free methods, after implicit or explicit warp estimation, distortion correction is performed, which can be evaluated using 2D and 3D document models. In our classification, we consider: (1) learning-free methods based on classical computer vision (CV) techniques, (2) deep learning (DL) methods, and (3) hybrid methods.\nAmong learning-free methods using CV approaches, we would like to highlight the following. The multistage curvilinear coordinate transform method [14] employs an iterative approach with curvilinear homography and quality estimation without the need for ground truth data. Unlike regular homography that works with flat surfaces, this method handles curved surfaces using mathematical models to correct distortions due to document curvature. Instead of comparing with ground truth data, it assesses the dewarping quality based on metrics like parallelism, orthogonality, and linearity of text lines and objects, making it more flexible and applicable when such data is unavailable. If the quality is unsatisfactory, the dewarping process is repeated with finer approximations. Research [13] uses a math model to automatically assess deformation factors in book pages, considering comic panel boundaries. The method detects and identifies the boundaries of such panels as key structural elements of the page, then builds a distortion model, evaluates deformation factors (including curvature, bends, and perspective distortions), and aligns the comic panels. The Grid Regularization technique [15] minimizes image distortions by enforcing a regular grid structure and using mathematical optimization to find the best grid point configuration. This ensures that grid lines (both horizontal and vertical) remain as straight as possible, helping maintain the alignment and readability of text and other document elements. The Geometric Control Points method [6] uses geometric elements like document boundaries and text lines to correct distortions. The key idea is to identify control points on the image to align and correct the document's shape. These points create a grid reflecting the document's geometry, enabling more accurate correction. Thin plate spline interpolation is then applied for smooth and precise transformation based on these control points. This technique is akin to bending a thin metal plate at several fixed points. In document image dewarping, thin plate splines create a smooth transition between control points, allowing for precise alignment and distortion correction, ensuring the image's integrity and quality. The Probabilistic Discretization of Vanishing Points method [16] calculates vanishing points to reconstruct the 3D shape of pages, dewarping documents and improving their readability and text recognition accuracy. Vanishing points (where parallel lines appear to converge in the distance) help determine the angles and slopes of document planes. This allows for the reconstruction of the 3D page shape from distorted 2D images. The method is based on geometric properties such as lines and edges, not on text content, so it does not require text recognition or segmentation, making it less sensitive to preprocessing errors.\nMany solutions use a hybrid of DL and classical computer vision. For instance, [17] employs a semi-CNN approach, evaluating pixel position changes using deformation and control parameters, assessed by a CNN on a synthetically created dataset. The Inv3D approach [18] utilizes a new Inv3D dataset and structural templates, extending the existing GeoTr method to enhance invoice image dewarping using an attention mechanism [19]. The Text-Lines and Line Segments strategy [20] applies document image dewarping by searching for regions of interest (text lines and line segments). First, it detects these regions: horizontal lines for text alignment, table and picture borders, underlines, and other elements for document alignment. By integrating this information, the strategy creates a comprehensive understanding of the document layout and existing distortions, then corrects them. The process may be iterative, where initial corrections are re-evaluated for further improvements, ensuring reliable and precise dewarping. Fourier Document Restoration (FDRNet) [21] for robust document dewarping and recognition restores"}, {"title": "2.3 Analysis of Mobile Apps for Document Scanning", "content": "At the outset of our research, we explored existing mobile solutions available on the Google Play Store (in June 2023) and assessed the effectiveness of commercial applications for creating digital copies of documents using an Android smartphone camera. The aim of the analysis was to identify acceptable results for document geometry reconstruction. We used keywords such as \"document scan\" and selected the following four relevant applications from the list suggested: DocScan [35], PDF scanner [36], TapScanner [37], and CamScanner [38].\nAs part of the study, original images of randomly chosen documents (a crumpled medication package insert and a disease prevention leaflet) were uploaded to each of the selected applications. The documents were digitized automatically by the mobile apps without any manual intervention. The resulting images from the mobile applications are presented in Fig. 1 without additional processing.\nLet's highlight the following features and drawbacks of digitization using these mobile applications (Fig. 2):\n\u2022 Document search algorithms generally handle scenarios with foreign objects in the background successfully. However, false positives may occur if fragments of other documents are present in the image.\n\u2022 Pages of an open book are often scanned as a single sheet and are not separated.\n\u2022 For non-standard-sized documents (non-ISO format), detecting the edges of the page may be difficult even if there are no geometric distortions.\n\u2022 Page edge detection may fail when the background color is similar to the document color or if the document has colored borders.\n\u2022 When a document has geometric distortions, none of the evaluated mobile scanners can reliably restore the original document's geometry. This can lead to issues like missing parts of the digitized document or adding unwanted information (such as background) to the image. Additionally, in most cases, the image of the digitized document remains distorted."}, {"title": "3 Analysis of Document Outline Detection Approaches", "content": "In this section, we explore document outline detection approaches using both classical computer vision algorithms and deep learning algorithms. We test document images and demonstrate the advantages of neural network approaches for document boundary detection with mask generation. After a comparative analysis of YOLOv8 and Mask R-CNN for document detection, we choose the YOLOv8 architecture for the dewarping and restoration pipeline of camera-captured document images."}, {"title": "3.1 Classical Computer Vision Approaches for Document Contour Detection", "content": "We have explored edge detection algorithms to address the task of document boundary recognition. Edge detection algorithms in computer vision are represented by edge operators and detectors. These algorithms can detect object contours in an image by using intensity differences (gradients) between neighboring pixels. This allows them to highlight object boundaries and makes them applicable for outlining documents. The most widely used edge detection algorithms include the Roberts and Sobel operators, as well as the Canny detector [39].\nThe Roberts Cross operator is a gradient-based operator that calculates the sum of squared differences between diagonally adjacent pixels in an image through discrete differentiation, followed by gradient approximation using 2x2 diagonal kernels (also known as masks). By convolving the original image with these kernels, the Roberts operator quickly computes the two-dimensional spatial gradients on the image, effectively detecting edges, particularly diagonal ones. Similarly, the Sobel operator uses 3x3 kernels to compute brightness gradients in both horizontal and vertical directions. It divides the image into 3x3 blocks, computes the gradients and their directions, combines the results, and forms an image of contour gradients. The Canny operator, being more complex and precise, includes stages like blurring, gradient computation, Non-Maximum Suppression, and thresholding. The sequential steps involve blurring the image with a Gaussian filter, computing the gradient using the Sobel operator, applying thresholding to determine significant contour gradients, performing non-maximum suppression, and finally, using a two-threshold process to remove noisy contours and highlight the main contours.\nExamples of these algorithms' performance in document edge detection are shown in Fig. 3. Initially, we used the Canny operator for edge detection in images. However, its universality was questioned due to the need for threshold adjustment based on lighting and color conditions. To address this drawback, we implemented algorithms based on the Sobel and Roberts operators, which do not require threshold tuning and are applicable to any image. The Roberts operator showed inferior results, while the Sobel operator demonstrated higher efficiency but faced challenges in contour detection on noisy images. Due to the low efficiency of these algorithms, we decided to discontinue the use of edge operators and detectors for our Document Outline Detection task."}, {"title": "3.1.2 Superpixel Algorithms for Document Outline Detection", "content": "Superpixel algorithms are image segmentation methods that divide an image into compact and interconnected regions known as superpixels. Superpixels are clusters of pixels with similar colors and textures, combined into a single unit for image processing. At this stage of the research, we considered four superpixel algorithms: SLIC [40,41], SEEDS [42], Felzenszwalb [43], and Quickshift [44].\n\u2022 The SLIC algorithm combines cluster analysis and dimensionality reduction methods. It divides the image into rectangular regions, each containing roughly the same number of pixels. The algorithm then performs clustering in color and spatial feature spaces to identify superpixels, resulting in a set of superpixels that correspond to connected areas in the image [40,41].\n\u2022\n\u2022\n\u2022\nThe SEEDS algorithm is based on iterative assignment of pixels to superpixels. Initially, it divides the image into blocks and iteratively refines the boundaries of superpixels [42].\nThe Felzenszwalb algorithm merges regions of similar pixels into superpixels. It uses a connectivity measure based on the difference in pixel intensity and image gradients [43].\nThe Quickshift algorithm identifies superpixels based on local pixel density and their color properties. It employs dimensionality reduction and smoothing techniques to determine the density map of the image. The algorithm then locates local extrema on this map to define superpixel centers and expands these centers to form the final superpixels [44].\nAll four types of superpixel algorithms were tested to compare their performance and suitability for document segmentation tasks. Since the SLIC algorithm demonstrated the best results among the other superpixel algorithms, it was chosen for further testing to detect document boundaries. To implement this algorithm, we selected the open-source project Fast-SLIC [45], developed in C++, which operates 7-20 times faster than other existing implementations. Based on this implementation, a document mask search function was developed. This function takes an image as input, applies morphological closing to remove small details and noise, and then performs SLIC segmentation to create a segmented image. The segmented image and its mean value are passed to the function, which returns an image representing the per-pixel average of each segment. To enhance the contrast between the document and the background, the resulting image is converted to the HSV color model. A Gaussian filter and thresholding are then applied to create a binary mask."}, {"title": "3.1.3 Conclusion on the Use of Edge Detection and Superpixel Algorithms for Document Outline Detection", "content": "During testing, we found that the reviewed edge detection algorithms are ineffective when processing images that contain extraneous objects along with the scanned document. The presence of such objects, e.g., stationery items, can lead to erroneous document image processing, complicating the unambiguous identification of its contours. Moreover, the algorithms are limited in their ability to handle documents with significant geometric deformations, leading to distortion of the document's quadrilateral shape. In contrast to edge detection algorithms, the SLIC superpixel algorithm demonstrates greater stability and predictability in document detection tasks. It effectively handles anomalous document shapes, noise, and varying lighting conditions, ensuring more accurate detection of document contours, even when the document is not fully visible. However, the SLIC algorithm has drawbacks, such as increased processing time compared to other methods and the need for careful tuning of optimal parameters. Despite its efficiency, it does not address the issue of extraneous objects in the image, which can lead to incorrect document mask construction.\nAs a result, we abandoned the use of edge detection and superpixel algorithms, as they are ineffective in the presence of extraneous objects in the frame and require manual parameter tuning. Instead, we decided to focus on exploring deep learning approaches."}, {"title": "3.2 Deep Learning-Based Document Outline Detection Algorithms", "content": ""}, {"title": "3.2.1 Dataset Preparation for Using DL Approaches", "content": "To tackle the task of document border detection using neural networks, it was necessary to prepare a training dataset. For this purpose, we compiled a dataset that includes several subsets with images of documents. The basis for the dataset was the DocUNet dataset [22,46], which includes 130 images of documents with various deformations and their corresponding scanned versions. Additionally, we utilized images from the widely-used COCO dataset [47], which contains over 330,000 images of diverse objects, scenes, and contexts. Although COCO does not have a specific \"document\" class, it includes a \"book\" class that features both book and document images. From this class, we initially obtained 5000 images, from which we manually selected 72 where the book or document is fully visible"}, {"title": "3.2.2 Mask R-CNN for Document Outline Detection and Masking", "content": "For tasks involving image classification or single-object detection, simple CNNs are typically used. However, these may be insufficient for more complex scenarios with multiple objects in an image. For such scenarios, an open-source library for object detection in images and videos based on PyTorch, MMDetection [53], can be employed. It includes various state-of-the-art object detection algorithms, such as Faster R-CNN [54], Mask R-CNN [52], RetinaNet [55], Cascade R-CNN [56], and others.\nFor our research, we selected the Mask R-CNN algorithm, which allows simultaneous segmentation and recognition of multiple objects in images. The Mask R-CNN model is developed based on Faster R-CNN. Unlike Faster R-CNN, which has two outputs for each object candidate a class label and a bounding box offset - Mask R-CNN adds a third branch that generates an object mask. Generating an additional mask differs from generating a class and bounding box, as it requires a more accurate spatial representation of the object. The Mask R-CNN algorithm consists of two primary stages, similar to Faster R-CNN [54]. The first stage is the Region Proposal Network (RPN), which suggests candidates for object bounding boxes. The second stage extracts features using the RoIPool operation for each candidate box and performs classification and bounding box regression. The shared features used in both stages can be separated to improve computational efficiency. In addition to class and bounding box predictions, Mask R-CNN also generates a binary mask for each Region of Interest (Rol)."}, {"title": "3.2.3 YOLOv8 for Document Outline Detection and Masking", "content": "It is known that YOLO is architecturally a convolutional neural network designed for real-time simultaneous object detection and classification in images. YOLOv8 [51] supports a wide range of computer vision tasks, including detection, segmentation, pose estimation, tracking, and classification.\nThe YOLO algorithm operates as follows:\n1.  The input image is divided into a grid of fixed-sized cells, each responsible for detecting objects.\n2.  For each cell, the model generates predictions of objects, determining their position, size, and class.\n3.  Predictions are filtered using a threshold value.\n4.  The class of the object with the highest probability is determined for each prediction.\n5.  Predictions that overlap and belong to the same object are merged.\n6.  The final result of YOLO includes the coordinates of bounding boxes for the detected objects and their respective classes.\nSegmentation in YOLOv8 is implemented in the YOLOX-Seg module, which consists of several deconvolution layers and decoding blocks. To improve segmentation quality, YOLOv8 applies augmentations such as changes in brightness, contrast, and color saturation. It also uses a pretrained object detector to generate preliminary predictions. For each prediction, features are extracted using the deconvolution method, which allows the expansion of the prediction size to the original image size. These features are fed into a network consisting of decoding blocks, sequentially increasing the original image size and generating a segmentation mask as the output."}, {"title": "3.2.4 Comparative analysis of Mask R-CNN and YOLOv8", "content": "We conducted a comparative analysis of Mask R-CNN and YOLOv8 to address the task of document boundary detection followed by mask generation. Initially, we employed an object detection technique using the Mask R-CNN architecture. This approach enables the direct extraction of object masks during the detection phase. We opted to train the neural network on a specially curated dataset from various sources (see Section 3.2.1), using a pre-trained"}, {"title": "4. Methodology for the Document Geometry Restoration and Dewarping Algorithm", "content": "In this section, we thoroughly describe the methodology of our algorithmic pipeline for correcting distortions and restoring the topology of camera-captured document images. We demonstrate the functionality of our algorithmic pipeline with specific examples of document images."}, {"title": "4.1 Algorithm Overview", "content": "To address the task of obtaining a digital copy of a document using a smartphone camera, we developed a dewarping algorithm for document images based on reconstructing the document's topology using information about its shape"}, {"title": "4.2 Document Mask Detection Using the YOLOv8 Model", "content": "In the initial stage of the document restoration and dewarping algorithm, the document mask is identified utilizing the YOLOv8 model. Detection results encompass bounding boxes, segments, and confidence scores. In instances where multiple documents are detected within an image, the algorithm selects the two documents with the highest scores and checks for intersections between their segments. If an intersection is detected, both documents are included in further analysis, assuming they represent two pages of an open book. If no intersection is found, the document with the highest confidence score is considered.\nNext, each document undergoes processing. Initially, based on segments obtained via YOLO, the document contours are constructed, and the detected document mask is visualized. Subsequently, a guided filter [57] is applied to the mask. This filter operates on the principle of a hybrid filter, which merges information from the input image (also referred to as \"guidance\") and the target image (Eq. 1):\n$Q(i) = a(i) \u00d7 I(i) + b(i),$\n(1)"}, {"title": "4.3. Polynomial Estimation for Document Mask", "content": "The list of detected masks is utilized in the polynomial search function. The contour of the largest mask by area is identified using an algorithm based on the edge-following method for topological structural analysis of digital binary images [58]. Subsequently, the value of the \u025b smoothing coefficient for polynomial curve approximation is calculated as the product of the threshold coefficient and the contour length (the perimeter of the closed contour). The contour is then approximated by a polynomial curve with fewer vertices so that the distance between them is less than or equal to the specified precision. The Douglas-Peucker algorithm [59] is employed for this purpose, serving to simplify geometric shapes, such as lines or polygons, by removing some points.\nFinally, the convex hull of the contour is computed using the Sklansky algorithm [60]. This algorithm is used to calculate the convex hull of a point set and has a complexity of $O(Nlog N)$, where N is the number of points in the input set. As a result, the function returns a list containing the outcomes of the polynomial curve approximation, convex hulls, and contours for each document mask."}, {"title": "4.4. Detection and Interpolation of Document Edges", "content": "Once the document contour is obtained, it is segmented into parts corresponding to each side of the document. Initially, the coordinates of the edges and corners are extracted from the contour and convex hull arrays. Then, the corners are sorted in a specific order. This involves calculating the center of the figure by averaging the coordinates along the X and Y axes. The list of corner coordinates is then ordered based on the increasing angle formed between the direction from the center of the figure to each point and the positive Y-axis. This is achieved by computing the arctangent of the ratio of the difference between the X and Y coordinates for each point to the difference between the coordinates of the figure's center. Diagonal lines connecting the corners are then formed.\nNext, the edges lying to the left of the diagonal lines are highlighted. This is achieved by calculating the cross product between each diagonal line and each point on the polygon's edge. For instance, if the cross product result is negative for one diagonal line and positive for another, then the edge point is to the left of the diagonal lines. Similarly, the cross product is calculated for each edge of the polygon. Then, the arrays of points forming the top edges are sorted in ascending order. The order of points for the right edges is reversed.\nThe dimensions of the polygon in the image are then calculated. The length of each polygon side is determined using the Euclidean distance function (3):\n$\\sqrt{\\sum_{k=1}^{n} (p_k - q_k)^2}$ \n(3)\nwhere $p_k$ and $q_k$ are the coordinates of vectors p and q; k is the index of the vector coordinate; n is the dimensionality of the vectors.\nNext, the average length and width of the polygon are computed by finding the arithmetic mean of the corresponding side lengths. Subsequently, new coordinates for the opposite sides of the polygon are calculated using linear interpolation:\n$f(x) = f(x_0) + \\frac{f(x_1) - f(x_0)}{x_1 - x_0} (x - x_0)$ \n(4)\nwhere $x_0$ and $x_1$ are the coordinates of adjacent points; x is the coordinate of the intermediate point for which interpolation is performed; $f(x_0)$ and $f(x_1)$ are the values of the function at points $x_0$ and $x_1$, respectively.\nThe obtained coordinates are then smoothed using the Savitzky-Golay filter [61], which is a linear filter used for smoothing time series or one-dimensional signals. This is achieved, in a process known as convolution, by fitting successive sub-sets of adjacent data points with a low-degree polynomial by the method of linear least squares:"}, {"title": "4.5. Construction of Approximation Grid and Its Interpolation", "content": "The x and y coordinates obtained earlier (see section 4.4) are used to generate approximation lines on the image. These lines form the basis for creating a document grid, defining its topology. The number of lines is a parameter of the algorithm and can be set manually. Intermediate grid lines are constructed by linearly interpolating the coordinates of the lines representing the sides of the document image. Approximations of the coordinates of lines passing through the left and right sides (vertical) as well as through the bottom and top sides (horizontal) are calculated.\nSubsequently, for each line constituting the grid, approximation and interpolation are performed individually. Each line is approximated using the method of nonlinear least squares with variable constraints (Eq. 6) and a cubic polynomial (Eq. 7):\n$F(x) = \\frac{1}{m} \\sum_{i=0}^{m-1} p_i f_i^2 (x),$ \n(6)\nwhere p is the loss function; $f_i(x)$ represents the difference between the observed values and the values predicted by the approximating function; m is the dimensionality of differences.\n$P(x) = ax^3 + bx^2 + cx + d,$\nhere, a, b, c, d are the coefficients of the polynomial.\nThe results of the approximation are stored as arrays of coordinate points. The function then performs extrapolation of the approximated lines to obtain additional points on each line. For horizontal lines, extrapolation is performed along the Y-axis, while for vertical lines, it is performed along the X-axis. This process increases the number of points on each line and ensures their intersection."}, {"title": "4.6. Searching for Intersections of Interpolation Lines", "content": "Based on the arrays of coordinate points that form the grid lines, the process of finding intersection coordinates between the approximated lines in the image is carried out. This involves identifying the nearest points among all pairs of vertical and horizontal grid lines. To implement the intersection search for each pair of grid lines, a method"}, {"title": "5. Evaluation Metrics and Test Results on Document Readability and Topology Recovery", "content": "In this section, we examine the evaluation metrics for document readability and geometry restoration that we use in comparative testing with the results provided by popular mobile applications for correcting distorted images. The tests demonstrate the effectiveness of our proposed document restoration and dewarping algorithm."}, {"title": "5.1. Document Readability and Geometry Restoration Evaluation Metrics", "content": "To assess the accuracy of the developed algorithm, we conducted an analysis of the results using metrics that can be categorized into two main types: (1) Metrics for Evaluating Text Readability by Optical Character Recognition (OCR) models; and (2) Document Geometry Restoration Metrics."}, {"title": "5.1.1 Metrics for Evaluating Text Readability by OCR models", "content": "Let's consider the Text Readability Metrics: (1) Levenshtein Distance; (2) Jaro-Winkler Similarity; (3) Character Error Rate (CER); and (4) Comparison of recognized characters with the true value."}, {"title": "5.1.1.1 Levenshtein Distance", "content": "The Levenshtein Distance [64], also known as the edit distance, quantifies the difference between two strings by determining the minimum number of edit operations, such as insertions, deletions, and substitutions, needed to transform one string into the other."}, {"title": "5.1.1.2 Jaro-Winkler Similarity", "content": "Jaro-Winkler Similarity (Eq. 9-10) is a measure of string similarity used to determine the distance between two sequences of characters. It builds on the Jaro distance [65] and incorporates a modification [66] known as the Winkler coefficient to account for the higher similarity of strings starting with the same characters:\n$dj =\\begin{cases}0 & \\text{if } m = 0\\\\\\frac{1}{3} (\\frac{m}{s_1} + \\frac{m}{s_2} + \\frac{m-t}{m}) & \\text{if } m\\neq 0\\end{cases}$ \n(9)\nwhere $s_1$ denotes the length of string $s_1$; m represents the number of matching characters; t is half the number of transpositions.\n$d_j = d_j + l(1 - d_j)$ \n(10)\nwhere $d_j$ represents the Jaro distance for strings $s_1$ and $s_2$; l denotes the length of the common prefix from the beginning of the string up to a maximum of 4 characters; p is a constant scaling factor."}, {"title": "5.1.1.3 Character Error Rate (CER)", "content": "The Character Error Rate (CER) (Eq. 11), also known as symbol error rate, is used to assess the quality of character or text recognition in optical character recognition tasks, automatic speech recognition, and other natural language processing tasks. CER measures the percentage of errors that occur when comparing the recognized text with the original source text.\n$CER = \\frac{S+D+I}{N}$ \n(11)\nwhere S is the number of substituted characters; D is the number of deleted characters; I is the number of inserted characters; N is the total number of characters."}, {"title": "5.1.1.4 Comparison of recognized characters with the true value", "content": "Comparison of recognized characters with the true value measures the proportion of correctly recognized characters from the total number of characters in the reference text. Unlike CER, which focuses on errors, this metric focuses on correct recognitions and is expressed as a percentage of accuracy."}, {"title": "5.1.2 Document Geometry Restoration Metrics", "content": "Now let's review the Document Geometry Restoration Metrics: (1) Structural Similarity Index (SSIM); (2) Mean Squared Error (MSE); (3) Normalized Root Mean Squared Error (NRMSE)."}, {"title": "5.1.2.1 Structural Similarity Index (SSIM)", "content": "SSIM (Eq. 12-13) is a metric used to evaluate the quality of compressed or processed images. It allows for comparing and measuring the structural similarity between the original and processed images. Introduced in 2004 [67], SSIM measures similarity based on three main aspects of the image: brightness, contrast, and structure.\n$\\begin{cases}C_1 = (k_1L)^2\\\\C_2 = (k_2L)^2\\end{cases}$ \n(12)\nwhere L refers to the dynamic range of pixels; $k_1$ is a constant valued at 0.01; $k_2$ is a constant valued at 0.03.\n$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y+c_1)(2\\sigma_{xy}+c_2)}{(\\mu_x^2 + \\mu_y^2 +c_1)(\\sigma_x^2 + \\sigma_y^2+c_2)}$\n(13)\nwhere \u03bc_x is the mean of x; \u03bc_y is the mean of y; \u03c3_x^2 is the variance of x; \u03c3_y^2 is the variance of y; \u03c3_{xy} denotes the covariance of x and y; c\u2081and C2 are two variables."}, {"title": "5.1.2.2 Mean Squared Error (MSE) and Normalized Root Mean Squared Error (NRMSE)", "content": "MSE and NRMSE [39", "amplitudes": "n$MSE = \\frac{1}{MN} \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} [f(x, y) \u2013 g(x, y)"}]}