{"title": "Interactive Example-based Explanations to Improve Health Professionals' Onboarding with AI for Human-AI Collaborative Decision Making", "authors": ["Min Hun Lee", "Renee Bao Xuan Ng", "Silvana Xinyi Choo", "Shamala Thilarajah"], "abstract": "A growing research explores the usage of AI explanations on user's decision phases for human-AI collaborative decision-making. However, previous studies found the issues of overreliance on 'wrong' Al outputs. In this paper, we propose interactive example-based explanations to improve health professionals' onboarding with AI for their better reliance on AI during AI-assisted decision-making. We implemented an AI-based decision support system that utilizes a neural network to assess the quality of post-stroke survivors' exercises and interactive example-based explanations that systematically surface the nearest neighborhoods of a test/task sample from the training set of the AI model to assist users' onboarding with the AI model. To investigate the effect of interactive example-based explanations, we conducted a study with domain experts, health professionals to evaluate their performance and reliance on AI. Our interactive example-based explanations during onboarding assisted health professionals in having a better reliance on AI and making a higher ratio of making 'right' decisions and a lower ratio of 'wrong' decisions than providing only feature-based explanations during the decision-support phase. Our study discusses new challenges of assisting user's onboarding with AI for human-AI collaborative decision-making.", "sections": [{"title": "Introduction", "content": "Advanced artificial intelligence (AI) has been increasingly being explored and considered to provide data-driven insights for improving various decision-making tasks (e.g. social services [46] and health [5, 10, 25]). Researchers have investigated how to effectively form human-AI teams [10, 40, 25] instead of applying a fully autonomous approach for AI systems in high-stake contexts. However, it remains challenging to explain the rationale of an AI output [30, 37, 10], build an appropriate trust on AI [3, 45, 8, 7], and integrate these AI systems in practice [39, 16, 17]. To address these challenges of forming effective human-AI teams, a growing body of research has explored various explainable AI techniques [28, 36, 44, 20] and the interactive visualization techniques of high-dimensional embedding data [6, 29, 1, 4, 42]. In addition, AI and HCI researchers have conducted studies [27, 11, 10, 5, 25, 44, 15] with the stakeholders to understand what they need and explore how they can effectively interact with AI explanations in specific applications. Some studies have shown the utility of AI explanations"}, {"title": "Related Work", "content": "We describe prior work at the intersection of explainable AI and visualization techniques and empirical studies of human-AI collaborative decision-making."}, {"title": "Explainable AI & Visualization Techniques", "content": "Researchers have explored human-AI collaborative decision-making, in which the AI system provides users data-driven insights on a decision-making task to assist and improve human's final decision-making [19, 11, 25, 46, 5]. However, users have difficulty with understanding why the AI system with a complex algorithm provides a certain outcome [30, 37]. They may resist and abandon the usage of these systems in practice [16]. To this end, researchers have explored diverse explainable AI (XAI) and visualization techniques to improve user's understanding of how AI/ML-based models reach their outcomes [28, 20, 6, 21]. XAI techniques can be categorized into inherently interpretable models, such as linear regression models, rule-based models, and decision trees, and post-hoc methods that generate an approximate of the model's decision logic by producing understandable representation, such as relevant examples or feature importance scores [20]. In this work, we focus on exploring two widely used post-hoc XAI techniques: an important feature explanation and an example-based explanation. Important feature explanations compute the contributions of input features to a model output and present the list of identified features or highlighted pixels [20, 12, 38]. Example-based explanations identify and present samples that are the most relevant and influential to an AI output [20, 12, 9]. In addition, various techniques have been proposed to visualize and interpret high-dimensional input or representations of AI/ML models [29]. Specifically, researchers have projected the high-dimensional input data or embedding representations of AI/ML models into two or three reduced dimensions using dimensional reduction techniques (e.g. principal component analysis) [1, 4, 42] to visualize reduced dimensions on an interactive tool [6]."}, {"title": "Studies of Human-AI Collaborative Decision Making", "content": "For human-AI collaborative decision-making, researchers have suggested engaging with the stakeholders, understanding what users need, and exploring how they can effectively interact with AI explanations in specific applications [11, 10, 5, 25] In addition, there have been increasing studies to explore the effect of AI explanations for various decision-making tasks (e.g. deception detection [18], cancer diagnosis [10], and stroke rehabilitation assessment [25]). Some of the previous studies discussed that providing explanations could lead to a harmful effect of user's over-reliance on the system [8, 3, 22, 3]. For the issue of overreliance on AI, researchers have conducted various empirical studies to understand the effect of strategies or factors (e.g. cognitive forcing intervention [7] or presenting AI explanations at the decision-making phase [22, 43]. For instance, Buccinca et al. [7] discussed the cognitive forcing intervention, such as slowing down the process and asking the person to make a decision before seeing the AI recommendation. Lee and Chew [22] demonstrated the potential of counterfactual explanations to increase users' analytical reviews on AI outputs and reduce their overreliance on AI during human-AI collaborative decision-making. However, a growing research work has mostly studied the issue of AI overreliance by presenting AI explanations at the decision support phase [18, 3, 18, 7, 45]. We have a limited understanding of how we can effectively support a user's onboarding phase with AI [11] when an AI system has been first introduced to a user for the user's trust and reliance on AI. In this work, we focused on the context of the AI-assisted clinical decision-making task (i.e. physical stroke rehabilitation assessment) and contributed to exploring the effect of an interactive AI explanation to support user's onboarding with AI for human-AI collaborative decision-making. To this end, we engaged with the domain experts to seek their opinions on how AI explanations can be used to support user's onboarding. In addition, compared to other works that utilize a mock-up decision support system that operates with the wizard-of-oz approach [8] or a simulated AI model [7, 43], this work utilized the dataset of 15 post-stroke survivors to implement AI model outputs and explanations and conducted an experiment with domain experts."}, {"title": "Study Designs", "content": "In this work, we explore the effect of interactive AI explanations to support users' onboarding with Al for human-AI collaborative decision-making. Building upon increasing research on explainable AI techniques for AI-assisted decision making [8, 7, 45, 10, 25] and previous research work that describes the importance of communicating the strength of AI for onboarding with AI [11], we focused on studying how an explainable AI technique can be used to introduce the strength of AI for user's onboarding with AI and understanding its effect during human-AI collaborative decision making (i.e. physical stroke rehabilitation assessment). To investigate this research question, we first conducted semi-structured interviews with domain experts (i.e. therapists) to probe their opinions on how an explainable AI technique can be used to communicate the strengths and limitations of Al for onboarding. Building upon the findings from the interviews, we created an interactive example-based explanation to support user's onboarding with an Al-based system. We then experimented with therapists to examine the effect of an interactive example-based explanation for their trust and reliance on AI during decision-making tasks of assessing post-stroke survivors' quality of motion. The study materials and procedures were approved by the Institutional Review Board (IRB)."}, {"title": "Study Context", "content": "This work focuses on the context of a clinical decision-making task of assessing the quality of motion of post-stroke survivors. We built upon the previous research on AI-assisted decision-making on stroke rehabilitation assessment [24] to specify an upper limb exercise and performance components of rehabilitation assessment. For an exercise, a post-stroke survivor has to raise the survivor's wrist to the mouth as if drinking water. The performance components of rehabilitation assessment include 'Range of Motion (ROM)' that checks how closely a post-stroke survivor achieves the target position of an exercise and 'Compensation' that checks whether a post-stroke survivor involves any unnecessary, compensatory joint movements to perform an exercise (e.g. leaning to the side) [24]."}, {"title": "Interviews about Onboarding with AI", "content": "To understand how an explainable AI technique can be used as an onboarding material to communicate the strengths and limitations of AI, we conducted semi-structured interviews with ten therapists, who have experience with managing stroke rehabilitation (Appendix. Table 3) [26]. We recruited the participants through advertisements sent to the hospital staff, the mailing lists, and the contacts of the research team. For the interviews, we first introduced an AI-based system and three AI explanations for physical stroke rehabilitation assessment and asked the participants to rank which Al explanations are useful to support onboarding (i.e. when a user first reviews and interacts with AI to understand the strength of AI) and decision support (i.e. when a user review AI outputs for a decision-making task) [26]. All interview sessions were conducted remotely on a video platform for 60 to 80 minutes. To introduce an AI-based system AI explanations, we utilized existing guidelines for human-AI interaction [2, 34, 11], an AI model card [33], and tutorials of AI explanations [20] to create introduction materials of an AI-based decision support system for physical stroke rehabilitation assessment. Also, we had discussions with domain experts in stroke rehabilitation to refine our introduction materials before conducting the interviews. For onboarding, therapists ranked an example-based explanation (36.7%) as the most useful, a counterfactual explanation (35.0%) as the second most useful, and a feature importance explanation (28.3%) as the third most useful [26]. For decision support, therapists ranked an example-based explanation and a feature importance explanation as equally useful (33.9%) and a counterfactual explanation (32.2%) as the third most useful [26]. For onboarding, therapists described that they want to \"briefly validate the correctness of AI outputs to develop a trust with AI\". An example-based explanation is useful for onboarding as \"reviewing a pool of similar samples\" is \"easier to interpret than others\" [26]. In addition, therapists suggested presenting benchmarkable information to understand the strength of an AI model, such as characterizing the conditions of similar post-stroke survivors, how much therapists had agreed on assessment, and how well the AI model can replicate therapist's assessment scores [26]."}, {"title": "System Implementations", "content": "Informed by the findings of the interviews with domain experts, we created an AI-based decision support system (Figure 1). Given a video of a post-stroke survivor's rehabilitation exercise, this system utilizes a neural network model to classify the quality of motion. In addition, the system includes an interactive example-based explanation (Figure 1b) for facilitating the user's onboarding with the AI system and an important feature explanation (Figure 1c) for assisting the user's decision-making on rehabilitation assessment. In the following section, we described the dataset of our study and the implementations of an AI model, AI explanations, and the interface in detail."}, {"title": "Dataset", "content": "This work utilizes the dataset of a \"Bring a cup to the mouth\" upper-limb exercise from 15 post-stroke survivors with diverse status of functional abilities [23]. The dataset contains (1) 300 videos of 15 post-stroke survivors, who performed ten trials of the exercise using their unaffected and affected side by stroke, (2) estimated joint positions of their exercise motions using a Kinect sensor v2, and (3) the annotations by the expert therapist, who utilized clinically validated assessment tool [13] to check the status of post-stroke survivors and another therapist, who had not had any interactions with 15 post-stroke survivors. For the annotations on performance components of rehabilitation assessment, therapists individually watched the videos of post-stroke survivors without reviewing any AI outputs."}, {"title": "Al Model", "content": "Following the previous research on rehabilitation assessment [23], we processed the estimated joint positions of post-stroke survivors' exercises to extract various kinematic features. The kinematic features of the 'Range of Motion' (ROM) include joint angles, such as elbow flexion, shoulder flexion, and elbow extension, and normalized relative trajectory (i.e. the Euclidean distance between two joints - head and wrist; head and elbow), and the normalized trajectory distance (i.e. the absolute distance between two joints - head and wrist, shoulder and wrist) in the x, y, and z coordinates [23]. The features of the 'Compensation' include the normalized trajectories, which indicate the distances between joint positions of the head, spine, and shoulder in the x, y, and z coordinates from the initial to the current frame over the entire exercise motion [23]. Given the extracted kinematic features and labels of post-stroke survivors' exercises, we implemented a feed-forward neural network (NN) model to classify the quality of post-stroke survivors' motion using Pytorch libraries [35], following its outperformance shown in the previous research [23]. For the labels, we utilized the labels by the expert therapist, who conducted the clinically validated assessment test. We grid-searched various architectures (i.e. one to three layers with 32, 64, 128, 256, and 512 hidden units) and different learning rates (i.e. 0.0001, 0.0005, 0.0001, 0.005, 0.001) while training a feed-forward NN model with cross-entropy loss and the mini-batch size of 1 and epoch of 4. For training and evaluating the model, we utilized the leave-one-subject-out cross-validation, where we trained the model with data from all post-stroke survivors except one post-stroke survivor and tested the model with data from the held-out post-stroke survivor. The final model architectures and learning rates are three layers with 256 hidden units and 0.005 of the learning rate for the ROM and three layers with 64 hidden units and 0.005 of the learning rate. The models achieved 82% F1-score and 77% F1-score to replicate therapists' assessment on 'ROM' and 'Compensation' components respectively."}, {"title": "AI Explanations and Interface", "content": "Our interactive system interface presents an AI prediction on performance components of rehabilitation assessment along with its confidence score [2]. In addition, our system provides 1) interactive example-based explanations for improving users' onboarding with AI and 2) feature-based explanations for assisting their decision-making. For the implementation of our system interface, we utilized the python, flask [14], HTML, and javascript libraries."}, {"title": "Interactive Example-based Explanation for Onboarding", "content": "Our interactive example-based explanation (Figure 1b) aims to assist users in onboarding with AI and developing initial trust with AI by reviewing similar cases of a test/query input. Specifically, the interactive example-based explanation shows the global views of the embedding spaces of the entire data [6] and the local views of the embedding spaces of k-nearest neighbors [6] of a case to be reviewed. In addition, our interactive example-based explanation shows common (green color) and unique (purple color) neighbor lists of embedding data from two performance components ('Range of Motion' - ROM and 'Compensation' - COMP). A user can specify the embedding space to review and the number of k-nearest neighbors. A user can click embedding data to review the image of a neighbor post-stroke survivor. A user can hover around embedding data to quickly review the benchmarkable information of a neighbor on a tooltip (Figure 1b). The benchmarkable information includes the status of a neighboring post-stroke survivor, the performance of the AI model, and the therapists' agreement level on selected neighboring post-stroke survivor's data. We hypothesize that by reviewing AI performance and therapists' agreement on nearest neighbor data, a user can have a better understanding of the strength of AI and build a better calibrated initial trust in AI for effective human-AI collaborative decision-making. For an example-based explanation, we explored the representation of input and intermediate layers of the feed-forward NN models and explored principal component analysis (PCA) [1], Uniform Manifold Approximation and Projection (UMAP) [4], and t-distributed Stochastic Neighbor Embedding (t-SNE) [42] to compute embedding data while implementing a K-nearest neighbor classifier over various sizes of k (i.e. 5, 10, 15, 20, 25, 30) with cosine or Euclidean distance [41]. Based on the experimental results, we utilized the default value of k as 5, the Euclidean distance metric, UMAP, and the first input layers of the feed-forward Neural network models as embedding data. For presenting embedding data, we utilized the Embedding Comparator [6] and revised it to present images of similar data points when a user clicks a data point and show benchmarkable information on a tooltip when a user hovers a data point."}, {"title": "Feature-based Explanation for Decision-Support", "content": "Among various types of AI explanations, we built upon the previous research that describes therapists' preferences in reviewing feature-based explanations on rehabilitation assessment tasks [24] and the findings of our interviews and utilized a feature-based explanation to assist users' decision-making tasks. For a feature-based explanation (Figure 1c), we first identify user-specific important features of rehabilitation assessment using the trained feed-forward neural network models and the SHAP library [32]. We then utilized only the top three features to avoid overwhelming users with a list of features [2]. Following the practices of therapists [24], we utilized a radar chart to compare these features on post-stroke survivors' unaffected and affected sides by stroke to assist users' decision-making tasks."}, {"title": "Experiments", "content": "Given the issue of overreliance on AI [45, 19, 22], this work hypothesizes that our interactive example-based explanation will enable users to understand the strengths and limitations of an AI model and develop a better-calibrated trust and reliance on AI for human-AI collaborative decision making. We specified two conditions (\"Features\" without interactive example-based explanation and \"Examples + Features\") and conducted a with-in subject study with sixteen health professionals to explore the effectiveness of our interactive example-based explanations on users' Al-assisted decision-making. \\u2022 \"Features\": the baseline, controlled condition of an AI-based decision support system that presents videos of post-stroke survivors' exercises (Figure 1a), AI predicted assessment scores, and an important feature explanation (Figure 1c) without interactive example-based explanations \\u2022 \"Examples + Features\": the experimental condition of an AI-based decision support system that includes the same functionalities of the baseline condition along with an additional, interactive example-based explanation for onboarding with the AI model (Figure 1b) As previous work describes therapists' preferences to review feature-based explanations and find evidence [24], we included the feature-based explanations by default to confirm therapists' hypothetical assessment with an Al explanation [44]. In addition, an interactive example-based explanation is included in the experimental condition to support user's onboarding with AI. In our study, we referred to two systems as \"Condition A\" and \"Condition B\" to avoid biasing participants. We referred to these conditions as \"Features\" and \"Examples + Features\" for clarity throughout the paper. For the study, we recruited 16 health professionals, therapists, who have experience with stroke rehabilitation through an advertisement sent to the hospital staff, the mailing list, and the contacts of the research team. We described the detailed demographics in the Appendix. Table 4."}, {"title": "Protocol", "content": "We conducted a within-subject experiment to understand the effect of interactive example-based explanations on users' reliance on AI during human-AI collaborative decision-making. After a participant completed the informed consent form, each participant was randomly assigned to (i) either first use the AI with only important feature explanations (Condition A - 'Features') without interactive AI explanations and then AI with interactive example-based explanations for onboarding and important feature explanations (Condition B - 'Examples + Features') or vice-versa. On each condition, the participants conducted two sub-tasks of completing the rehabilitation assessment on their assigned cases (i) without reviewing AI outputs and explanations (Figure 1a) and (ii) after reviewing AI outputs and explanations. In each condition, the participant conducted eight initial assessments and eight final assessments on post-stroke survivors' exercises after reviewing AI outputs and explanations. Among eight assigned cases, we included the cases of 4 'right' AI outputs and 4 'wrong' AI outputs by the trained feed-forward neural network models to investigate the effect of AI explanations on user's overreliance on 'wrong' AI outputs. We counterbalanced the assigned cases of each condition and randomized the order of the two conditions and the presentations of assigned cases of post-stroke survivors. All participants received a fixed compensation for their participation based on the rate recommended by the domain experts."}, {"title": "Evaluation Metrics", "content": "We built upon previous studies on human-AI collaborative decision making [19, 10, 22] and utilized the following evaluation metrics:"}, {"title": "'Right' and 'Wrong' Decisions", "content": "Table 2 summarizes the ratios of 'right' and 'wrong' decisions on rehabilitation assessment tasks with AI outputs ('Human + AI') by participants. In addition, Table 2 describes the detailed ratios of 'right' decisions (i.e. agreeing with 'right' AI outputs and rejecting 'wrong' AI outputs) and 'wrong' decisions (i.e. agreeing with 'wrong' AI outputs and rejecting 'right' AI outputs). Participants using 'Examples + Features' had 0.7% higher ratio of 'right' decisions and 0.7% lower ratio of 'wrong' decisions than using 'Features'. For 'right' decisions, participants using 'Features' and 'Examples + Features' had the same ratio of agreeing with 'right' AI outputs and participants using 'Examples + Features' had a 0.7% higher ratio of rejecting 'wrong' AI outputs than using 'Features'. For 'wrong' decisions, participants using 'Examples + Features' had a 0.7% lower ratio of agreeing with 'wrong' AI outputs than using 'Features' and the same ratio of rejecting 'right' AI outputs using 'Features'"}, {"title": "Duration", "content": "Using the system with 'Features', the participants spent an average of 60 seconds on an assessment. Using the system with 'Examples + Features', the participants spent an average of 45 seconds. Overall, the usage of the system with only 'Features' requires an average of 15 seconds more than that of the system with 'Examples + Features' for a decision-making task."}, {"title": "Discussion", "content": "Our experimental results suggested that interactive example-based explanations for user's onboarding with AI ('Example + Features') are more effective for the domain experts, health professionals (i.e. therapists) to have a better-calibrated reliance on AI than presenting only an important feature explanation ('Features'). When the AI system presented 'right' AI outputs to participants, the performance of human + AI has been improved compared to that of humans alone. When the AI system presented 'wrong' AI outputs to participants, the performance of human + AI decreased compared to that of humans alone. Our study results follow the findings of the previous research on understanding the effect of AI explanations during the decision-support phase [3, 22]. Compared to using only 'Features', participants using 'Examples + Features' had 2.7% higher ratio of making 'right' decisions, lower performance degradation and less overreliance on AI (Table 1). In addition, the participants using 'Examples + Features' spent an average of 15 seconds less on an assessment task than those using only 'Features'. Our study results imply the potential of improving users' onboarding with interactive example-based explanations and their effective AI-assisted decision making than just using feature-based explanations during the decision-support phase. Our findings contrast with the findings of the previous study that during the decision-support phase, an example-based explanation underperformed a feature-based explanation to support calibrated trust and reliance on on AI [45]. Even if our results showed that interactive example-based explanations can be effective for therapists to improve their onboarding with AI and have a better-calibrated reliance on AI during human-AI collaborative decision-making, some participants were confused about how interactive parts of the system work. Thus, it is important to further explore how we can better educate and onboard with AI and AI explanations [31]. Also, this work is limited to exploring the effect of interactive example-based explanations for user's onboarding with AI and does not provide generalization as we had a small size of participants and specified a particular AI/ML model (i.e. a feed-forward neural network model), the format of input data (i.e. videos), and a single clinical decision-making task (i.e. rehabilitation assessment). Thus, further studies are required to explore how to improve users' onboarding with AI for effective human-AI collaborative decision-making."}, {"title": "Conclusion", "content": "In this work, we contributed to an empirical study with domain experts, health professionals (i.e. therapists) to understand the effect of interactive example-based explanations to onboard users with AI for human-AI collaborative decision making. Our results showed that our proposed interactive example-based explanations (\"Examples + Features\") during an onboarding phase assisted therapists to have a better-calibrated reliance on AI and to have a higher ratio of 'right' decisions and a lower ratio of 'wrong' decision than \"Features\" that presents only feature-based explanation during decision-support phase. We discuss the potential of using interactive Al explanations to support users' onboarding with AI for better-calibrated reliance on AI and challenges to improve human-AI collaborative decision-making."}]}