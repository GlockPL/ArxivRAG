{"title": "MAS-ATTENTION: MEMORY-AWARE STREAM PROCESSING FOR ATTENTION\nACCELERATION ON RESOURCE-CONSTRAINED EDGE DEVICES", "authors": ["Mohammadali Shakerdargah", "Shan Lu", "Chao Gao", "Di Niu"], "abstract": "The advent of foundation models have revolutionized various fields, enabling unprecedented task accuracy and\nflexibility in computational linguistics, computer vision and other domains. Attention mechanism has become an\nessential component of foundation models, due to their superb capability of capturing correlations in a sequence.\nHowever, attention results in quadratic complexity in memory and compute as the context length grows. Although\nmany fusion-based exact attention acceleration algorithms have been developed for datacenter-grade GPUs and\naccelerators leveraging multi-core parallelism and data locality, yet it remains a significant challenge to accelerate\nattention on resource-constrained edge neural accelerators with limited compute units and stringent on-chip\ncaches. In this paper, we propose a scheme for exact attention inference acceleration on memory-constrained\nedge accelerators, by parallelizing the utilization of heterogeneous compute units, i.e., vector processing units\nand matrix processing units. Our method involves scheduling workloads onto these different compute units in a\nmulti-tiered tiling scheme to process tiled vector workloads and matrix workloads in attention as two streams,\nrespecting the workload dependencies. We search for tiling factors to maximize the parallelization of both compute\nunits while considering I/O overhead, and propose a proactive cache overwrite strategy to avoid undesirable cache\nspills in reality. Extensive results based on open-sourced simulation frameworks show up to 2.75\u00d7 speedup and\n54% reduction in energy consumption as compared to the state-of-the-art attention fusion method (FLAT) in the\nedge computing scenario. Further experiments on a real-world edge neural processing unit demonstrate speedup\nof up to 1.76x for attention as compared to FLAT, without affecting model output accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models (Vaswani et al., 2017; Kitaev et al., 2020;\nKaplan et al., 2020; Peebles & Xie, 2023; Li et al., 2024a)\nhave driven recent advancements in generative AI on edge\ndevices such as smartphones, especially in AI agents (Zhang\net al., 2023; Wang et al., 2024; Fan et al., 2025), large lan-\nguage models (LLMs) (Radford et al., 2018; Ouyang et al.,\n2022; Glaese et al., 2022; Mehta et al., 2024) and text-to-\nimage diffusion models (Poole et al., 2022; Esser et al.,\n2024). Central to these models is the attention mechanism,\nwhich captures long-range dependencies between tokens,\nbut incurs quadratic memory and computational complexity\ndue to pairwise token interactions. Deploying these mod-\nels is challenging, especially on resource-constrained edge\ndevices with limited on-chip cache and processing power.\nSignificant efforts have been made to accelerate atten-\ntion computation through software fusion techniques on\ndatacenter-grade hardware. For cloud servers, multi-core\nparallelism (Shoeybi et al., 2019; Rasley et al., 2020;\nNarayanan et al., 2021; Kwon et al., 2023; Liu et al., 2023;\nCho et al., 2024) and efficient utilization of on-chip SRAM\nin GPUs (Kirk et al., 2007) are employed to enhance per-\nformance. FlashAttention (Dao et al., 2022; Dao, 2023;\nShah et al., 2024; dao; Hong et al., 2023) related meth-\nods design I/O-aware exact attention speedup algorithms,\nleveraging GPU CUDA cores and on-chip SRAM to min-\nimize access to High Bandwidth Memory (HBM), saving\nmemory and reducing runtime. FuseMax (Nayak et al.,\n2024) uses Einsums and a spatial array accelerator, em-\nploying ping-pong scheduling to overlap MatMul and soft-\nmax operations. While FlashAttention-3 and FuseMax par-\nallelize MatMul and softmax on multi-core architectures,\nthese cloud-based acceleration methods cannot be applied\non resource-constrained edge accelerators directly, which\nhave a limited number of processing units and on-chip mem-\nory.\nTo speed up attention inference on edge devices, current\nmethods mainly leverage graph fusion (Ivanov et al., 2021;\nNiu et al., 2021; Aminabadi et al., 2022; Mei et al., 2023) to"}, {"title": "2 RELATED WORK", "content": "Sequential Attention Execution: The Layer-Wise atten-\ntion computation processes operations sequentially. This\nmethod relies on transferring intermediate results between\noff-chip and on-chip memory, creating a memory-bound\nworkflow that poses significant deployment challenges on\nedge devices with limited memory bandwidth.\nApproximate Attention Acceleration Methods: For ap-\nproximate acceleration methods of transformer-based foun-\ndation models, methods like palletization (Cho et al., 2021;\nTabani et al., 2021; Wang et al., 2020a; app, b), quantization\n(Liu et al., 2021; Lin et al., 2021; Wang et al., 2022; Li et al.,\n2022; Piao et al., 2022; Yao et al., 2022; Li & Gu, 2023; Yu\net al., 2023), pruning (Mao et al., 2021; Peng et al., 2021;\nYu et al., 2022b;a), and knowledge distillation (Sun et al.,\n2019; Wang et al., 2020c;b; Ganesh et al., 2021; Huang"}, {"title": "3 MAS-ATTENTION OVERVIEW", "content": "While prior exact attention acceleration methods for\nresource-constrained edge devices, such as oneDNN (Li\net al., 2024b) and FLAT (Kao et al., 2023), enhance data lo-\ncality and reduce memory access overhead through operator\nfusion, they still execute tiled MatMul and softmax opera-\ntors sequentially, missing the chance for parallel execution\nwithin the attention mechanism. In our work, we leverage\nthe heterogeneous computing capabilities of edge devices to\nachieve parallel execution of tiled MatMul and softmax for\nthe exact attention acceleration. Our method further min-\nimizes the latency with this parallelization scheme while\nreducing I/O and redundant memory access with a novel\nmulti-tiered tiling scheme and a proactive memory-aware\nbuffer management, making it advantageous even for single\ninference requests in AI scenarios on resource-constrained\nedge devices.\nHeterogeneous Workloads of Attention Mechanism: On\nresource-constrained edge devices, heterogeneous comput-\ning is often used to perform computation and memory access\nconcurrently. However, prior edge-based attention accelera-\ntion methods have not explored the heterogeneous nature of\nMatMul and softmax workloads within the attention mech-\nanism. Given their distinct computational characteristics,\nthe compute-intensive MatMul operation runs on the MAC\nunit, while the element-wise softmax operation is processed\non the VEC unit. Leveraging this heterogeneity, enables\nparallel execution of MatMul and softmax computations,\nproviding further acceleration of the attention mechanism.\nHardware-Software Co-design Scheduling on Resource-\nConstrained Edge Devices: Given the limited comput-\ning cores and on-chip memory, efficiently scheduling tiled\nMatMul and softmax operators with parallel execution in\nthe attention workload requires consideration of both hard-\nware parameters (e.g., L1 and L0 memory sizes, MAC and\nVEC core counts) and software parameters (e.g., MatMul\nand softmax workload shapes). To address this challeng-\ning hardware-software co-design scheduling problem, we\npropose a novel multi-tiered tiling scheme that accommo-\ndates both short and long sequence lengths while enhancing\nthe utilization of on-chip processing units. Specifically,\nwe introduce sub-matrix tiling granularity for MatMul and\nrow-wise tiling granularity for softmax workloads. This\napproach creates distinct tiling search spaces for different\nworkloads, allowing for higher search efficiency. We employ\nadvanced search algorithms like MCTS to conduct offline\nsearches for obtaining optimal tiling parameters across vari-\nous attention workloads and hardware configurations. Our\ntiling scheme and search algorithm aim to balance MAC\nand VEC operations in a fused, pipelined, semi-synchronous\nattention computation, maximizing processing unit utiliza-\ntion, minimizing idle time, and reducing I/O and redundant\nmemory access to ultimately optimize inference latency and\nenergy consumption.\nMemory-aware Optimizations for the Limited Shared\nOn-chip Memory: While our multi-tiered tiling scheme\nallocates search budgets for different workloads within the\nattention mechanism and enhances the efficiency of the\nsearch algorithm, limited search budgets can lead to locally\noptimal tiling parameters, particularly for long input se-\nquences with extensive search spaces. Additionally, the\nconstrained shared on-chip memory in edge devices com-"}, {"title": "4 METHODOLOGY", "content": "Given the query, key and value matrices, $Q, K, V \\in\\mathbb{R}^{B\\times H\\times N\\times E}$, where $B$ is the batch size, $H$ is the number\nof heads, $N$ is the sequence length and $E$ is the embed-\nding size, the attention output $O$ is computed through the\nfollowing steps:\n\\begin{align}\n    C &= QK^T \\in \\mathbb{R}^{B \\times H \\times N \\times N}, \\\\ \n    P &= \\text{softmax}(C) \\in \\mathbb{R}^{B \\times H \\times N \\times N}, \\\\ \n    O &= PV \\in \\mathbb{R}^{B \\times H \\times N \\times E},\n\\end{align}\nwhere softmax is applied to every row of $QK^T$.\nTo efficiently perform these computations on resource-\nlimited spatial accelerators, we propose a semi-synchronous\nMAC-VEC parallel execution scheme. Our method is ap-\nplicable to a wide range of spatial accelerators that have at\nleast one MAC unit for matrix multiplications and one VEC\nunit for element-wise operations. Our scheme is achieved\nthrough the strategic scheduling and pipelining of two Mat-\nMul operations alongside a single Softmax operation, as\nillustrated in Figure 1. This approach allows the three oper-\nators to concurrently process different tiles within the same\ncomputation round, thereby accelerating the attention mech-\nanism. Additionally, we leverage advanced heuristic search\nalgorithms to optimize the tiling sizes across all memory\nlevels within our dataflow. These algorithms adaptively tune\nthe tiling parameters based on input dimensions, workload\ncharacteristics, and pipelining criteria to ensure a balanced\ndistribution of workloads across compute units. We also\nimplement an on-chip memory management strategy that\nselectively overwrites non-essential data to free up mem-\nory resources, prioritizing Softmax computation for longer\nsequences while ensuring the subsequent recovery of inter-\nrupted MatMul operations. Detailed descriptions of these\nstrategies are provided in the following."}, {"title": "4.1 Stream Processing Mechanism", "content": "We propose a stream processing scheme to handle continu-\nous streams of tiled MatMul and Softmax workloads. There\nare two streams of tiled tasks: one for tiled MatMul com-\nputation (defined in Algorithms 2 and 4) and another for\ntiled Softmax computation defined in Algorithm 3. These\nstreams are scheduled in a pipelined fashion to overlap tiled\nMatMul-Softmax computations, as illustrated in Figure 1.\nOur approach operates at a row granularity, where the input\nmatrix Q is divided into smaller chunks along the batch,\nhead, and sequence dimensions, resulting in row-wise sub-\nmatrices denoted as $Q_i$. This granularity is driven by the\ninherently row-wise nature of the Softmax operation, align-\ning the processing scheme with Softmax's requirements.\nIterations thus proceed based on the segmented sequence\ndimension of the query, allowing for efficient parallelism.\nThe detailed stream processing scheme is outlined in Al-\ngorithm 1, where there are warm-up, regular, and finalize\ncomputation rounds."}, {"title": "4.2 MAS-Attention Tiling Scheme", "content": "We introduce a multi-tiered tiling strategy for MAS-\nAttention dataflow. For matrices K, P and V, used in\nthe MatMul operations in Equation 1 and Equation 3, a\nfine-grained sub-matrix tiling is applied. This approach is\ncrucial, especially when the sequence length is significantly\nlonger than the embedding dimension ($N \\gg E$), as it helps\naddress the constraints of limited on-chip memory. Without\nsuch tiling, handling the matrix K in $C_i = Q_iK^T$ and the\nmatrices $P_i$ and V in $O_i = P_iV$ becomes problematic due\nto excessive memory demands. For intermediate tensors\n$C_i$ and $P_i$ used in the Softmax operation in Equation 2, a\nrow-granularity tiling is employed, aligning with the inher-"}, {"title": "4.3 Proactive Overwrite Strategy for Optimized\nMemory Utilization", "content": "The tiling parameters obtained from heuristic search algo-\nrithms, such as Genetic Algorithm and Monte Carlo Tree\nSearch, may not always yield optimal results. Due to the\ncomplexity of the search space and the heuristic nature of\nthese algorithms, there is a possibility of suboptimal config-\nurations, which can impact the efficiency and correctness of\nstream processing. To mitigate these potential inefficiencies\nand ensure robust performance across a variety of workloads\nand scenarios, we introduce a selective overwrite strategy.\nThis proactive approach enables the system to adaptively\nmanage on-chip memory by selectively overwriting specific\nnon-essential data when memory constraints arise.\nDuring the computation of $P_i$, if the on-chip memory\nreaches capacity, impeding further calculations, two cases\nmay arise. First, as shown in Figure 2, if the MAC unit is en-\ngaged in processing $P_{i-1}V$, $P_i$ will overwrite the V matrix\non chip and stop the MAC from continuing its operation,\nresulting in no more writes from the MAC unit to on-chip\nbuffer. Second, as shown in Figure 3, if the MAC unit is\noccupied with $Q_{i+1}K^T$, $P_i$ will overwrite the K matrix\non chip, thereby interrupting the MAC unit's process and\npreventing any further writes to the on-chip memory. Once\nthe final result of $P_i$ is fully calculated and stored on chip,\nthe MAC unit can resume its process by reloading either the\nV or K matrix from DRAM to on-chip memory if it was\noverwritten and redoing the MatMul calculation.\nThe rationale is that maintaining the integrity of critical"}, {"title": "5 EXPERIMENTS", "content": "This section provides details on the simulation and modeling\ntools utilized, describes the hardware specifications, and\noutlines the experimental workloads and baseline algorithms\nused for analysis. We conduct a comprehensive evaluation\nof the proposed method, comparing it against state-of-the\nart attention fusion and acceleration techniques tailored for"}, {"title": "5.1 Experimental Setup", "content": "This section provides details on the simulation and modeling\ntools utilized, describes the hardware specifications, and\noutlines the experimental workloads and baseline algorithms\nused for analysis. We conduct a comprehensive evaluation\nof the proposed method, comparing it against state-of-the\nart attention fusion and acceleration techniques tailored for"}, {"title": "5.2 Execution Time Analysis", "content": "Table 2 presents a detailed analysis of execution cycles and\nspeedup ratios for MAS-Attention compared to other meth-\nads across all tested networks. The data highlights that\nMAS-Attention consistently achieves superior performance,\nwith speedup factors ranging from 1.02\u00d7 to 8.50\u00d7 over\nLayer-Wise, and up to 5.25\u00d7 over Soft-Pipe, 2.75\u00d7 over\nFLAT, and 2.00\u00d7 over TileFlow methods. The geometric\nmean of these speedup values-5.09x, 2.78\u00d7, 1.70\u00d7, and\n1.31\u00d7 respectively-demonstrates MAS-Attention's over-\nall efficiency in reducing execution time. This substantial\nperformance improvement underscores MAS-Attention's\neffectiveness as an advanced solution for optimizing com-\nputational efficiency in attention mechanisms."}, {"title": "5.2.1 Analysis on Simulated Hardware", "content": "Table 2 presents a detailed analysis of execution cycles and\nspeedup ratios for MAS-Attention compared to other meth-\nads across all tested networks. The data highlights that\nMAS-Attention consistently achieves superior performance,\nwith speedup factors ranging from 1.02\u00d7 to 8.50\u00d7 over\nLayer-Wise, and up to 5.25\u00d7 over Soft-Pipe, 2.75\u00d7 over\nFLAT, and 2.00\u00d7 over TileFlow methods. The geometric\nmean of these speedup values-5.09x, 2.78\u00d7, 1.70\u00d7, and\n1.31\u00d7 respectively-demonstrates MAS-Attention's over-\nall efficiency in reducing execution time. This substantial\nperformance improvement underscores MAS-Attention's\neffectiveness as an advanced solution for optimizing com-\nputational efficiency in attention mechanisms."}, {"title": "5.2.2 Analysis on Real Hardware", "content": "Figure 5 shows the analysis of normalized execution time for\nLayer-Wise, Soft-Pipe, FLAT, and MAS-Attention methods\non Huawei MatePad Pro 13.2 with DaVinci DNN Accel-\nerator. MAS-Attention achieves substantial performance\nimprovements, with speedups ranging from 1.94\u00d7 to 3.50\u00d7\nover Layer-Wise, 1.35\u00d7 to 2.87\u00d7 over Soft-Pipe, and\n1.30x to 1.76\u00d7 over FLAT. The geometric mean speedups\nare 2.33x, 1.73\u00d7, and 1.42\u00d7, respectively. It is worth not-\ning that TileFlow was not included in this analysis as its\nimplementation details were not fully described in (Zheng\net al., 2023), which limited us from deploying it on this\nedge device. Overall, the data validates MAS-Attention's\neffectiveness in enhancing computational efficiency on real\nhardware."}, {"title": "5.3 Power and Energy Analysis", "content": "Table 3 presents a comprehensive analysis of energy con-\nsumption and savings achieved by MAS-Attention com-\npared to other methods across various networks. The data re-\nveals that MAS-Attention consistently demonstrates signifi-\ncant energy efficiency improvements, with savings ranging\nfrom 1.98% to 75.00% compared to Layer-Wise, 36.83%\nto 66.08% compared to Soft-Pipe, 0.02% to 54.03% com-\npared to FLAT, and 36.83% to 65.05 compared to Tile-\nFlow. The geometric mean of these savings-52.97%,\n63.07%, 18.55%, and 53.16% respectively-highlights\nMAS-Attention's overall effectiveness in reducing energy\nconsumption. This substantial reduction underscores MAS-\nAttention's potential for optimizing energy efficiency in\nresource limited environments.\nIn addition, we provide an energy consumption breakdown\nfor each network on all algorithms as shown in Figure 6,\nfocusing on Off-Chip (DRAM) and On-Chip (L1, L0) mem-\nories, and PEs in MAC and Vector units."}, {"title": "5.3.1 Off-Chip Memory Energy Consumption", "content": "Compared to Layer-Wise and Soft-Pipe methods, MAS-\nAttention significantly reduces off-chip energy consump-\ntion by minimizing DRAM accesses and eliminating the\nneed to store intermediate C and P matrices off-chip. How-\never, MAS-Attention's off-chip energy consumption in some\ncases is slightly higher than FLAT due to the need of reload-\ning K and V matrices in the case of them being overwritten\nby the selective overwriting mechanism during pipelining.\nSoft-Pipe consumes more energy than MAS-Attention as\nit stores the P matrix back to DRAM, but less than Layer-\nWise as it does not store the C matrix to DRAM."}, {"title": "5.3.2 On-Chip Memory Energy Consumption", "content": "Layer-Wise, Soft-Pipe and TileFlow usually consumes\nmuch more on-chip energy compared to MAS-Attention,\nindicating less efficient on-chip memory utilization. FLAT"}, {"title": "5.3.3 PEs Energy Consumption", "content": "Energy consumption in PEs remains constant across differ-\nent algorithms for each network, as the actual computation\nrequired by different algorithms is the same, with differ-\nences only in the scheduling process."}, {"title": "5.4 DRAM Access Analysis", "content": "Since the FLAT method is most comparable to MAS-\nAttention in terms of both cycle and energy performance,\nwe will focus on comparing the DRAM access between"}, {"title": "5.4.1 DRAM Write Operations", "content": "Both MAS-Attention and FLAT algorithms exhibit an iden-\ntical number of write operations to DRAM. This uniformity\narises because both algorithms confine their DRAM write\noperations to the final result of the attention block (O), es-\nchewing the need to write intermediate results to DRAM.\nInstead, these intermediate results are processed entirely\non-chip, thereby minimizing off-chip memory accesses and\nenhancing overall efficiency."}, {"title": "5.4.2 DRAM Read Operations", "content": "Across the tested workloads, MAS-Attention matches FLAT\nin DRAM read operations but surpasses it for specific\nnetworks. Notably, for BERT-Base & T5-Base (1.5\u00d7),\nBERT-Large & T5-Large (1.5\u00d7), and Llama3-8B & T5-3B\n(1.49\u00d7), MAS-Attention shows increased DRAM read op-\nerations. This phenomenon arises because MAS-Attention\nrequires reloading specific data chunks, particularly K and\nV matrices, which may have been overwritten during the\npipelining stages on-chip. These matrices are reloaded from\nDRAM to resume the halted MAC operations, allowing the\nattention mechanism to maintain data dependencies and con-\ntinue processing seamlessly. While this incurs additional\nDRAM reads, the proactive buffer overwriting mechanism\nmaintains efficient on-chip memory usage and pipelined\nexecution integrity, with total cycle counts and energy con-\nsumption still outperforming all other baselines."}, {"title": "5.5 Limitations", "content": "On the simulated edge hardware, MAS-Attention can handle\na maximum sequence length of approximately 1 million\ntokens in half precision (FP16), which is half the maximum\nsequence length that FLAT can handle. The computation\nof $P_i$ happens in parallel with either $O_{i-1}$ or $C_{i+1}$. Since\nSoftmax operates row-wise, at least one row is used in the\ncomputation of $P_i$. In the case of $P_i$ computed in parallel\nwith $O_{i-1} = P_{i-1}V$, $O_{i-1}$ requires at least one entire row\nof $P_{i-1}$ to be calculated. Also, in the case of $P_i$ computed\nin parallel with $C_{i+1} = Q_{i+1}K^T$, one entire row of $C_{i+1}$\nis computed and written on-chip. In both scenarios, on-\nchip memory should have the capacity for either $P_i$ and\n$P_{i-1}$ or $P_i$ and $C_{i+1}$. In the case of half precision with\na sequence length of 1M, one row of $P_i$, $P_{i-1}$, and $C_{i+1}$\nconsumes 2MB each on-chip, which fits within the 5MB\non-chip cache size in either scenario. Since FLAT does not\nemploy such a pipelining scheme and operates sequentially,\nit can handle a sequence length of 2 million tokens. In this\ncondition, one row of $P_i$ consumes 4MB on-chip, which can\nbe managed by the 5MB on-chip cache size in the simulated\nedge device."}, {"title": "6 CONCLUSION & FUTURE WORK", "content": "In this paper, we propose MAS-Attention dataflow to ac-\ncelerate attention mechanism on resource-constrained edge\ndevices. Our approach uses a stream processing scheme to\nexecute tiled MatMul and Softmax workloads in a pipelined\nmanner, with MAC and VEC units operating in parallel. A\nmulti-tiered tiling strategy ensures balanced workloads for\nefficient pipelined attention execution. Additionally, our\nproactive buffer overwrite strategy enhances on-chip mem-\nory utilization by freeing up buffer space when it runs out\nof memory, such as with longer input sequences. While this\nstrategy increases off-chip memory reads, MAS-Attention\nachieves superior speedup and energy savings over previous\nmethods like Layer-wise, Soft-Pipe, FLAT, and TileFlow,\non both simulated and real edge devices.\nFuture work will extend MAS-Attention to support training,\nwhich adds complexity in backpropagation that challenges\nefficient workload management on resource-constrained\nedge devices."}]}