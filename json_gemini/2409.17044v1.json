{"title": "How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not", "authors": ["Francesco Verdini", "Pierfrancesco Melucci", "Stefano Perna", "Francesco Cariaggi", "Marco Gaido", "Sara Papi", "Szymon Mazurek", "Marek Kasztelnik", "Luisa Bentivogli", "S\u00e9bastien Brati\u00e8res", "Paolo Merialdo", "Simone Scardapane"], "abstract": "The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM.", "sections": [{"title": "I. INTRODUCTION", "content": "The success of Large Language Models (LLMs) [1] has attracted significant interest in extending their capabilities to handle various input modalities such as vision [2] and speech [3]. In the speech scenario, several studies [3]\u2013[7] have proposed the integration of a pretrained Speech Foundation Model (SFM) encoder with a pretrained LLM through an adapter module, realizing the SFM+LLM new architectural paradigm [8]. The adapter can be decomposed into two components, as shown in Fig. 1: a length adapter, which compresses the speech sequence along the time dimension, and a modality adapter, which maps the compressed input into an embedding space compatible with the LLM. The SFM+LLM solution exploits, on the one hand, the SFM ability to extract high-quality semantic representations of the speech input and, on the other, the fluency and vast linguistic knowledge of LLMs, achieving competitive scores for widespread tasks such as Automatic Speech Recognition (ASR) [9] and Speech Translation (ST) [7]."}, {"title": "II. METHODOLOGY", "content": "Fig. 1 illustrates the design of SFM+LLM solutions, highlighting the three key components: the SFM encoder, the adapter, and the LLM. The input audio a is first processed by the SFM encoder SE, whose output goes through the adapter A and the resulting embeddings are concatenated to the embeddings P of a textual prompt.\u00b9 The operation J, which joins the audio and prompt embeddings, is the same as that used in LLaVa [2] for prompting. Defining Las the LLM, the transcription or translation y is then obtained as follows:\n\n$y = L (J (P, A(S_E(a))))$                                                                                                                                                                       (1)\n\nAs the main rationale behind the SFM+LLM solution comes from the possibility of training a high-quality ASR or ST system without large training datasets thus with limited computational costs and memory requirements we keep L and SE frozen, training only A. This solution is coherent with previous work [15], which showed that the gains obtained by fine-tuning the whole SFM encoder and LLM do not justify the additional costs. Within this framework, we answer our research question on the relative importance of the three components for the downstream performance by varying each of them as illustrated in the following sections."}, {"title": "A. SFM Encoder", "content": "To investigate the impact of the SFM encoder, we use two widely recognized SFMs for speech representation extraction: Whisper [12] and SeamlessM4T [13]. In particular, we use the large version of both of them, namely Whisper large-v3,2 and SeamlessM4T v2-large.3 While Whisper is the most popular SFM in recent works on SFM+LLM [3], [4], [16]\u2013[18], representing a natural choice, the usage of SeamlessM4T has never been explored to the best of our knowledge. Nonetheless, we opted for it not only for its recognized quality, but also because its design is very different from Whisper. SeamlessM4T is built with a customized version of Conformer layers [19] instead of Transformer ones, and also the compression factor of the input sequence is very different. While both process audio sequences where each vector of the sequence represents 10ms of audio, Whisper emits one vector every 20ms (2x downsampling), while SeamlessM4T encoder returns one vector every 160ms (16\u00d7 downsampling). In light of these peculiarities, experimenting with these two SFM encoders lets us understand whether their behavior impacts on the best adapter design (in particular, the length adapter)."}, {"title": "B. Adapter", "content": "As we keep the SFM and LLM frozen, we design adapters with high representation capacity, allowing for an effective mapping of the embeddings to the LLM input space. To do so, we follow [4], [15] that use a stack of vanilla Transformer [20] encoder layers with bidirectional self-attention as modality adapters, and investigate different methods as length adapters. The adapters are trained using a cross-entropy loss on the output of the LLM having the transcripts for ASR and translations for ST as target, unless stated otherwise. Overall, we investigate the following 5 types of adapters.\nBase. 4 Transformer encoder layers are used and no length adaptation is performed.\nConv-based. 2 convolutional layers with stride 2 are introduced after the second layer of the Base adapter. No auxiliary loss is used. The final compression factor is 4.\nCIF-based. Similarly to Conv-based, the adapter is extended by introducing a Continuous Integrate-and-Fire (CIF) [10] length adapter after the second Transformer layer. CIF is a sequence compression mechanism that accumulates input features over time and emits an output when a given integration threshold is reached, enabling variable-length sequence compression while preserving key information. To train this module, we add two auxiliary losses: a Connectionist Temporal Classification (CTC) loss [21] with the transcripts as target, following [22], and a quantity loss that controls the compression factor. The weight associated to both auxiliary losses is 0.1. On average, this corresponds to a compression factor of 2 with SeamlessM4T and 12 with Whisper.\nCTC-based. In this case, the length adapter is a CTC-based compression [11], which collapses consecutive equal predictions of a CTC module by averaging the corresponding vectors, trained on the transcripts with an auxiliary CTC loss as done in CIF-based. On average, this corresponds to a compression factor of 1.5 for SeamlessM4T and 9 for Whisper.\nWLQ-former. This adapter performs both modality and length adaptation with a window-level Q-Former [3]. This module processes variable-length encoded speech sequences by dividing them into fixed-length windows of encoded frames and feeding each of these non-overlapping windows to a Q-former architecture [23]. The Q-former uses a fixed and configurable number of learnable query vectors to attend to each window through cross-attention. As a result, the compression factor is"}, {"title": "C. LLM Decoder", "content": "As LLMs, we select Mistral-7B-Instruct-v0.3,4 and Llama-3.1-8B-Instruct. The former is an English-centric model while the latter is trained in a multilingual setting, covering English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. Moreover, Llama is the most popular choice in previous works [4], [17], [24]. Also in this case, our goal is to maximize the difference between the investigated LLMs."}, {"title": "III. EXPERIMENTAL SETTINGS", "content": "We trained all our models on CoVoST 2 [25] using English, German, Spanish, French, and Italian as source languages and German and English as target languages, and on MuST-C [26] using German, French, Italian, Spanish as target languages. The datasets for ASR were obtained by taking the audios in the source languages and their corresponding transcriptions for CoVOST, while for MuST-C we took audios and associated transcriptions from the English-German partition. We trained for 2 epochs, corresponding to 28k steps, on 4 NVIDIA GH200 96GB GPUs, using a micro-batch size of 10 samples"}, {"title": "IV. RESULTS", "content": "In Table III, we report the ASR and ST results of the SFM+LLM architectures with the various combinations of SFMs, LLMs, and length adapters introduced in Section II. As previous works mostly rely on BLEU [29], we highlight that our Whisper, Base adapter, and Llama model \u2013 a basic configuration adopted by previous SFM+LLM works \u2013 scores 28.7 BLEU on CoVOST en-de (the most popular language direction), a significantly higher result than the 25.1 BLEU of Qwen-Audio [30], one of the best performing SFM+LLM solutions. This confirms the soundness of our experimental settings and the competitiveness of our results.\nFirst, we observe that the choice of the SFM is the most critical factor in terms of downstream performance. This is shown not only by the fact that, in each configuration, the version equipped with SeamlessM4T outperforms the counterpart with Whisper on both tasks (ASR and ST) on average, but also by the average improvement of more than 2 COMET on ST and more than 1 WER on ASR of the best SeamlessM4T configurations (with Llama and the WLQ-former or Base adapters) over the best Whisper ones (with Llama and the Conv-based adapter for ST and Llama and Base for ASR).\nInstead, the choice of the LLM is less critical, as demonstrated by the small gap (<0.2 on both ASR and ST) between the best configuration with Llama (Seamless as SFM and WLQ-former as adapter) and that with Mistral (SeamlessM4T as SFM and Base as adapter). Moreover, the best results on average in ST are obtained with Llama, while the best ones in ASR with Mistral.\nSecond, results clearly show that there is no one-size-fits-all solution for the length adapter. Interestingly, the LLM plays an important role in the choice of the adapter. With Mistral, the Base adapter generally yields the best results, even though the WLQ-former is competitive, especially in ST. With Llama, instead, the best adapter varies with the SFM used. While WLQ-former is the best option with SeamlessM4T, the Conv-based and Base adapters emerge with Whisper, with the former being the best in ST and close in ASR, where differences among the two are almost always not statistically significant. Across all SFM and LLM configurations and tasks, the Base adapter always ranks first or second except for Llama+SeamlessM4T in ST, where it is third. Moreover, content-based length adapters consistently underperform other strategies. Together with the observation that there is no clear trend of the results with respect to the compression factor, these insights suggest that reducing the length mismatch between textual and speech representations is not critical for the quality of the outputs. However, reducing the speech sequence length lowers computational costs, making length adapters still a useful module to consider.\nAll in all, our results demonstrate the need for experimenting in different settings in terms of SFM and LLM when comparing adapter solutions, as improvements in one specific scenario may not generalize. In addition, LLMs show to be robust to input sequences of very different lengths, as the Base adapter, which does not compress the speech sequence, and the WLQ-former, which has high compression factors (16 with Whisper), achieve competitive scores in most settings."}, {"title": "V. CONCLUSIONS", "content": "This work systematically analyzed the importance and design of the various building blocks that compose speech-to-text models by connecting an SFM encoder and an LLM through an adapter. To this aim, we compared all the combinations of 2 SFMs, 2 LLMs, and 5 adapters, which mostly differ for their length reduction module. With comprehensive experiments covering two tasks \u2013 ASR and ST \u2013 and 5 language directions, our results demonstrate that the choice of the SFM is the most critical factor influencing downstream performance. We also established that there is no one-size-fits-all solution for the length adapter, as the optimal choice varies depending on the specific combination of SFM and LLM. Notably, the Base and WLQ-former adapters, which feature very different compression factors, demonstrate strong performance across tasks, suggesting that reducing sequence length mismatch between speech and text is less crucial than previously assumed."}]}