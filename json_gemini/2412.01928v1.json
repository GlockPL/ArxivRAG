{"title": "MALT: Improving Reasoning with Multi-Agent LLM Training", "authors": ["Sumeet Ramesh Motwani", "Chandler Smith", "Rocktim Jyoti Das", "Markian Rybchuk", "Philip H. S. Torr", "Ivan Laptev", "Fabio Pizzati", "Ronald Clark", "Christian Schroeder de Witt"], "abstract": "Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. Although LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. In this paper, we present a first step towards 'Multi-agent LLM training' (MALT) on reasoning problems. Our approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. We propose a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome-based rewards. This enables our post-training setup to utilize both positive and negative trajectories to autonomously improve each model's specialized capabilities as part of a joint sequential system. We evaluate our approach on MATH, GSM8k, and CSQA, where MALT using Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, our work provides a concrete direction for research around multi-agent LLM training approaches.", "sections": [{"title": "1. Introduction", "content": "In tasks requiring complex reasoning, such as mathematical problem solving, research assistance, coding, or creative writing, decomposing the system into multiple cooperating agents focusing on different aspects of the problem has been shown to improve performance (Li et al., 2023; Wang et al., 2024b). This mirrors human collaboration, where specialized roles lead to better solutions (Minsky, 1988). For instance, in software development, a programmer may propose an initial solution by providing some code, which is then analyzed by a tester for bugs or edge cases. Finally, a supervisor integrates this feedback to improve the original code's quality. LLM agents can interact in a similar way (Qian et al., 2024).\nRecent advancements in Large Language Models (LLMs) have demonstrated their potential to collaborate effectively in multi-agent systems across a wide range of scenarios, ranging from virtual settings such as software development and interactions between personal assistants to market trading and autonomous driving (Li et al., 2023; Chan et al., 2024; Liang et al., 2024; Wu et al., 2024). However, these collaborative systems are composed of individually-trained agents that have not been jointly optimized for cooperative tasks. To the best of our knowledge, we are the first to address this critical gap with Multi-Agent LLM Training (MALT)."}, {"title": "2. Related Work", "content": "Our work intersects a broad spectrum of research areas, including agent design, self-improvement, reasoning, and reinforcement learning. In the following sections, we provide a concise overview of related works that have shaped these domains."}, {"title": "2.1. Fine-tuning Methods and Foundation Models", "content": "Advancements in generative language models have led to the development of powerful foundation models that serve as the basis for various fine-tuning approaches. Supervised Fine-Tuning (SFT) adapts a pre-trained models to perform more effectively on specific downstream tasks by updating model parameters using labeled data. In Full Fine-Tuning, all parameters of the pre-trained model are updated during training on the target task, allowing the model to adjust its entire representation to new data (Devlin et al., 2019). While this method can yield high performance, it is computationally intensive and may risk overfitting.\nTo address these limitations, parameter-efficient fine-tuning (PeFT) methods were developed (Houlsby et al., 2019). These Adapter Modules introduce small, trainable layers within each transformer block while keeping the original model parameters fixed, while Low-Rank Adaptation (LORA) inserts trainable low-rank matrices into the weight matrices of the pre-trained model (Hu et al., 2022). Both approaches significantly reduce the number of trainable parameters, leading to faster training times and lower memory consumption without substantial performance loss. Recently, Direct Preference Optimization (DPO) has emerged as an effective method for fine-tuning language models based on preference data (Rafailov et al., 2023). DPO directly optimizes the model to align with human preferences by fine-tuning on datasets containing comparisons of model outputs. This has significantly improved model alignment and performance across various domains, including reasoning and problem-solving tasks."}, {"title": "2.2. Synthetic Data Generation for Model Improvement", "content": "The generation of synthetic data for fine-tuning has gained traction as strategy for improving the reasoning capabilities of LLMs, reducing the dependence on large human-annotated datasets. In Self-Taught Reasoner (STaR), models iteratively refine their reasoning through self-produced explanations, leading to notable gains (Zelikman et al., 2022). Similarly, it has been shown that LLMs can serve as prompt engineers, generating high-quality synthetic data that, when used for fine-tuning, enhance model capabilities (Singh et al., 2024). However, these methodologies are constrained by their exclusive focus on correct data, limiting the models' ability to learn from their own errors.\nTo address this, Setlur et al. (2024) leverage reinforcement learning on synthetic incorrect data, enabling models to learn from mistakes and iteratively enhance performance. Analogous strategies have been explored in the field of computer vision (Liu et al., 2024), where positive and negative examples generated by LLMs improve robustness against adversarial attacks. Further, Pang et al. (2024) propose Iterative Self-Learning, where models are trained on their own erroneous outputs, refining their predictions over successive iterations.\nInspired by Setlur et al. (2024), we leverage a combination of SFT and DPO to enable better performance on role-specific tasks. Our our novel sampling algorithm uses a combination of fine-tuning techniques to train a multi-agent setup that consists of generators, verifiers and refinement models. These advancements underscore the potential of synthetic data generation techniques for LM improvement. Our work builds on these foundations, exploring synthetic data generation in multi-agent systems to improve individual agent performance and overall system efficacy."}, {"title": "2.3. Self-improving Models and Iterative Refinement", "content": "Self-improvement mechanisms have become a crucial aspect of advancing language model capabilities. Agent Q combines guided search, self-critique, and iterative fine-tuning to improve generalization in complex reasoning tasks (Putta et al., 2024). Saunders et al. (Saunders et al., 2022) further explored self-critiquing models that assist human evaluators by generating critiques of their own responses. By fostering an internal feedback mechanism, models can better align with human values and expectations, enhancing both safety and effectiveness. Recent work has shown that inference-time compute can be scaled sequentially by teaching a model to self-correct mistakes over multiple turns (Qu et al., 2024). These self-improving methodologies underscore the potential of iterative refinement and self-feedback in advancing the capabilities of language models. By enabling models to learn from their successes and failures, they achieve greater autonomy and adaptability in complex reasoning tasks. A concurrent work that focuses on a multi-agent setting is presented in (Chen et al., 2024), which uses an MCTS-based generation strategy and performs preference optimization similar to (Putta et al., 2024; Zhang et al., 2024) in limited multi-agent debate or information-exchange settings with two agents. In contrast, we present a more scalable, intuitive method extending beyond a two-agent setting in a sequential setup of heterogeneous agents, leveraging data generated via exponential tree based sampling."}, {"title": "2.4. Inference-time Search, Scaling, and Consistency", "content": "Recent research has shifted focus towards optimizing inference-time computation to increase model performance without increasing model size or training data. Prior work has shown that using more computation at inference time can significantly improve task performance, allowing smaller models to outperform larger ones (Snell et al., 2024; Brown et al., 2024). The Self-Consistency technique (Wang et al., 2022), which enhances reasoning by sampling multiple reasoning paths and aggregating them to select the most consistent answer.\nApproaches that scale parallel inference-time compute like Best-of-N, majority voting, and revision have all proven valuable (Cobbe et al., 2021). Recent work has shown that inference-time compute can be scaled sequentially by teaching a model to self-correct mistakes over multiple turns (Qu et al., 2024). These strategies highlight the efficacy of leveraging increasing computational resources at inference time, and lay the foundation for the feasible implementation of our novel algorithm.\nThe convergence of fine-tuning methodologies, synthetic data generation, iterative self-improvement, enhanced inference-time computation, and multi-agent systems underscores significant advancements in language model capabilities. By building upon these foundational works, our approach aims to push the boundaries of collaborative reasoning and problem-solving in language models, contributing to the development of more sophisticated AI systems."}, {"title": "3. Preliminaries", "content": "We consider reasoning tasks to be represented by a problem description and an associated solution. For example, a problem description $x \\in X$ may contain one or several natural language statements involving an unknown numerical variable $\\S$ that is to be reasoned over, and a unique correct solution $y \\in Y \\subseteq R$ for $\\S$. Given a 0-parameterized policy $\\pi_\\theta$, the task is then to learn parameters $\\theta^*$ given a training dataset $D_{train} = \\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}$, $N \\in N$, and a test dataset $D_{test}$ under the generalization objective"}, {"title": "4. Method: Multi-Agent LLM Training (MALT)", "content": "Systems of decentralized LLMs arise naturally in any circumstances in which each LLM agent has differing objectives and/or partial observability. For example, autonomous web agents commonly follow the goals of different principals and condition on different instructions and private contexts.\nHowever, even in fully observable and cooperative cases where systems of LLM agents could technically be simulated by a single centralized LLM, a decomposition into"}, {"title": "4.1. Multi-Agent Inference Setting", "content": "We formulate our multi-agent inference setting as a collaborative reasoning framework designed to solve complex tasks. Let Q denote a dataset of natural language questions, where each $q \\in Q$ represents a specific task instance. The objective is to generate a prediction $a \\in A$ for a given input q, where A is the set of all possible answers. We assume the existence of a ground truth function $f: Q \\rightarrow A$, where $f(q) = a_{GT}$ serves as the ideal prediction for evaluating a.\nOur framework consists of three specialized Large Language Models (LLMs) acting as distinct agents, each defined as a function:\n1. Generator (G : Q \u00d7 PG \u2192 OG): Produces an initial response to the question.\n2. Verifier (V : OG \u00d7 Q \u00d7 Pv \u2192 Ov): Critiques the generated response for potential errors.\n3. Refinement Model (R : OG \u00d7 Ov \u00d7 Q \u00d7 PR \u2192 A): Integrates feedback to improve the final prediction.\nHere, PG, PV, PR are the sets of possible prompts for each model, and OG, Ov are the sets of possible outputs for the generator and verifier, respectively.\nFormally, we define the interaction between these agents as:\n$g_o = G(q, p_g)$ where $g_o \\in O_G$, $p_g \\in P_G$  (5)\n$v_o = V(q, p_v, g_o)$ where $v_o \\in O_V$, $p_v \\in P_V$  (6)\n$a = R(q, p_r, g_o, v_o)$ where $a \\in A$, $p_r \\in P_R$  (7)\nLet $\\Theta_G$, $\\Theta_V$, $\\Theta_R$ denote the parameter sets for the generator, verifier, and refinement model, respectively. The multi-agent"}, {"title": "4.2. Capturing Reasoning Trajectories", "content": "Standard zero- or few-shot settings typically involve a Generator producing an answer $a$ for a given question $q$. While a base Generator could just be fine-tuned on the original training data to obtain a generator $G_{sft}$, our multi-agent setting allows further improvement.\nWe define a reasoning trace for a question $q$ as $r_q = [r_g, r_v, r_r]$, where $r_g$, $r_v$, and $r_r$ represent the reasoning steps (outputs) of the generator, verifier, and refinement model, respectively. An outcome reward model $R: A \u00d7 A \u2192 {0,1}$, based on the ground truth in the training set, evaluates $r_r$ to mark the trajectory as either correct (1) or incorrect (0). Specifically, for a predicted answer $a$ and ground truth $a_{GT}$, we define:\n$R(a, a_{GT}) = \\begin{cases} 1, & \\text{if } a = a_{GT}, \\\\ 0, & \\text{otherwise.} \\end{cases}$  (9)\nTo generate diverse reasoning trajectories, we employ the following sampling strategy with a branching factor of $n$ for all models:\n1. For each problem $x_i \\in D_{query}$ in the training data, we sample $n$ completions from the generator:\n$\\left\\{g_{i,j} \\sim G(x_i)\\right\\}_{j=1}^n$  (10)\n2. For each generator output $g_{i,j}$, we produce $n$ verifications:\n$\\left\\{v_{i,j,k} \\sim V(g_{i,j}, x_i)\\right\\}_{k=1}^n$  (11)\n3. For each verifier output $v_{i,j,k}$, we generate $n$ refinements:\n$\\left\\{r_{i,j,k,l} \\sim R(g_{i,j}, v_{i,j,k}, x_i)\\right\\}_{l=1}^n$. (12)\nThis process results in $n^3$ trajectories for each training example, totaling $|D_{query}| \u00d7 n^3$ trajectories. We use the outcome reward model R to label the refinement outputs as correct (\u2714) or incorrect (\u00d7).\nTo effectively utilize the reward signals from the refinement outputs, we adopt a value iteration approach to propagate values backward through the reasoning chain. Specifically, we compute the expected value of each node (generator"}, {"title": "5. Experiments", "content": "In this section, we detail our experiments. We begin by outlining the experimental details, including a description of the benchmarks used for evaluation, and the proposal of several comprehensive baselines for comparison. We then present our main experimental results, where we show the effectiveness of MALT on three standard LLM evaluation benchmarks."}, {"title": "5.1. Experimental Details", "content": "Models Our experiments are based on the Llama-3.1-8B-Instruct model (Grattafiori et al., 2024), chosen for the strong balance it provides between competitive initial baseline performance and size that fits within a limited computational budget. The performance of Llama 3.1 8B across benchmarks provides a good testbed, allowing us to extract positive and negative samples for each model that we can use for post-training and pushing performance further. Moreover, the baseline benchmark scores enable us to see clear results while remaining sufficiently performant and not saturating benchmarks. Open-source smaller-scale models allow for reproducability and accessibility while providing meaningful insights into multi-agent interactions and post-training.\nBenchmarks We use three popular benchmarks for our experiments. First, we consider the GSM8K benchmark (Cobbe et al., 2021), which consists of 7.47k training examples and 1.32k test questions, focused on linguistically diverse grade school math problems. Furthermore, to evaluate MALT on a more difficult mathematical reasoning benchmark, we use MATH (Hendrycks et al., 2021), consisting of 7.5k training and 5k test problems. MATH has proven to be consistently difficult, especially for smaller language models, with Llama 3.1 8B performing around 49.50% test-accuracy based on our baselines. Lastly, to extend the scope beyond mathematical tasks, we evaluate on CommonsenseQA (Talmor et al., 2019) with 9.74k training examples and 1.22k dev-set questions. CommonsenseQA is a multiple-choice question answering dataset around commonsense reasoning problems and has been used similarly by prior work in this line (Zelikman et al., 2022; Grattafiori et al., 2024; Zelikman et al., 2024; Wei et al., 2023).\nBaselines To establish a comprehensive set of baselines, we evaluate against a total of eight baselines in 2 primary settings. First, we employ inference-only baselines that use only pre-trained models with the same prompts we use for our trained setup to provide answers for a given question. Within this setting, we propose (1) a single-agent (SA) naive setting in which a single model is used as a generator to provide a reply, (2) a multi-agent (MA) setting, where the pre-trained baseline model operates in a sequential way as a generator, verifier, and refinement agent solving a problem in a multi-agent inference setting. Moreover, in order to provide an fair comparison to MALT, we implement a simple majority voting based consistency mechanism as part of our baselines (discussed further in Section 5.3). Our second setting of baselines is with supervised fine-tuning on models in the four previous baselines mentioned. We refer to this as our STaR baseline with the generator and all three models trained on the synthetic rationales collected as part of MALT with next-token prediction loss. All eight of our baselines are based on the Llama 3.1 8B model. A future version of our work will discuss detailed ablation studies in addition to our current baselines.\nSynthetic Data Generation For MALT's synthetic data generation pipeline, we generate the sequential trees for each question only in the training set with the process mentioned in Algorithm ?? or more concisely in Section 4.2. Using $n = 3$ allows us to generate 27 trajectories per question and between approximately 2k to 6k trajectory pairs based on different models and benchmarks. We provide more details on these sizes in the Appendix. MALT samples from Llama 3.1 8B models with a temperature of 0.3 to ensure some controlled variability in responses. Each model has its fixed prompt for role conditioning that is also used for baseline performance calculation. The question is"}, {"title": "5.2. Experimental Results", "content": "Our experimental results along with all baselines over all three datasets are presented in Table 1. Our results are averaged over four runs on random subsets (due to computational constraints) across seeds of the large test-sets"}, {"title": "5.3. Discussion", "content": "We have introduced \"Multi-Agent LLM Training\" (MALT), which demonstrates improvements in reasoning performance over all our benchmarks and represents an important first step in the training of multi-agent role specific LLM systems over joint reward propagated to individual models in a discrete manner.\nMALT is a simple yet intuitive approach that addresses a significant gap in LLM post-training and multi-agent inference settings. Multi-agent training remains underexplored due to key challenges, including the lack of role-specific training data, the absence of setups that benefit from multi-agent interactions, and the inherent complexity of training such systems. MALT bridges this gap by leveraging advances in synthetic data generation to improve model performance, guiding a scalable strategy for training across multiple models. MALT tackles the problem of attributing synthetic data to individual models based on outcome rewards and enhances them using stable training techniques such as SFT and DPO. A core strength of our empirical results lies in how we approach multi-agent problem solving, joint-training a critic whose outputs guide subsequent refinements. This setup also enables backtracking, addressing a key limitation of single LLMs (Bachmann & Nagarajan, 2024).\nSynthetic data is crucial not only for improving the generator but also for enhancing the verifier and refinement models, which require targeted training due to limited role-specific data. SFT helps these models develop reasoning structures aligned with their roles, while DPO enables higher-quality verification and refinement strategies, such as recalculating steps to double-check answers. Setlur et al. (Setlur et al., 2024) show that using incorrect synthetic data helps models unlearn spurious correlations, boosting reasoning efficiency and generalization. Building on these insights, we design MALT to further enhance robustness and effectiveness.\nWe now discuss key design choices. Simplicity with theoretical grounding\u2014particularly the assurance of a trained policy with a monotonically increasing value was a primary consideration. We opted for DPO over PPO-based methods due to our focus on a straightforward offline data generation process, treating our approach as an independent learning problem in a centralized setting (Lerer et al., 2019) with a single iteration (the key difference being that our agent policies post-training differ). In this setting, DPO is more stable than PPO and requires less overhead. While PPO could use the value computed at each branch as a reward for post-training nodes (a promising future direction), it introduces significant computational complexity.\nOur value iteration method, when binarized, resembles global majority-based pooling: for a given node and branch, the binary reward of the leaf nodes in the subtree determines"}, {"title": "6. Conclusion and Future work", "content": "Multi-Agent LLM Training (MALT) is an introductory advance towards building a novel algorithm combining supervised fine-tuning and preference optimization to train systems of generators, critiques, and refinement models on joint multi-agent trajectories. MALT enables small LLMs to achieve reasoning performance comparable to much larger models, paving the way for post-training optimization of general and collaborative LLM agent systems. With our experiments on three benchmarks, we provided new insights into multi-agent LLM post-training and demonstrated higher performance than baselines.\nSafety Our approach can be used not just to enhance the reasoning capabilities of LLM systems, but also address crucial open problems in the safety of multi-agent systems. Importantly, MALT-trained systems of trusted small models could attain better task performance while retaining high degrees of trust, producing more powerful overseers within the AI control setting (Greenblatt et al., 2024). Another prominent application of our approach would be to train verifiers as safety critics within a multi-agent setup. This could scale up the settings such as OpenAI CriticGPT (McAleese et al., 2024) to any number of models, resulting in more powerful safety critics and potentially allowing for the legibility of solutions to be improved.\nFuture Directions Our findings, while still preliminary, showcase the potential of multi-agent LLM systems optimized with fine-tuning and collaborative inference techniques. There are several future directions from this line of work: Using PPO (Schulman et al., 2017b) and the exact value propogated backward for each trajectory to update model weights might produce strong results with additional computational overhead (Ivison et al., 2024). Moreover, we provide several levers around the number of models"}, {"title": "A. Appendix", "content": "A.1. Qualitative Examples\nIn this section, we present three key qualitative examples: a synthetic training example for SFT and DPO, a qualitative\nperformance comparison between an untrained and a trained verifier, and a complete MALT setup demonstrating a\nbacktracking and improvement reasoning cycle that generates a correct answer."}, {"title": "A.2. Theoretical Justification for MALT (Work in Progress)", "content": "A.2.1. CREDIT ASSIGNMENT STRATEGY\nHere, we provide a theoretical justification for why our framework, when updating the agent policies based on binarized\npooled rewards with a threshold at 0 = 0.5, leads to policy improvements. We formalize the pooling operation, define\nthe dynamic programming approach of value iteration in our context, and show that our policy updates lead to monotonic\nincreases in expected reward.\nWe consider a tree-structured reasoning process where each node represents a model in our system: Generator (G), Verifier\n(V), or Refinement Model (R). The branches connecting these nodes represent the outputs (actions) generated by each\nmodel. For each model, multiple outputs (branches) are possible, representing the different actions or decisions the model\ncan make. Each leaf node corresponds to the terminal outputs of the Refinement model R and receives a binary reward:\n$R(r) =\n\\begin{cases}\nif r is correct,\n0, otherwise.\n\\end{cases}$\n (20)\nWe define the value function V(n) for a node n (model) as the expected reward of its downstream refinements starting from\nthat node:\n$V(n) = E [R(r) | n]$,\n (21)\nwhere the expectation is over all possible branches under the current policy \u03c0, starting from node n. Starting from the leaves:\n$V(r) = R(r) \u2208 {0,1}$,\n (22)"}, {"title": "A.3. Additional Information", "content": "For SFT, we used LoRA with a learning rate multiplier of 0.1 and a batch size of 8 to avoid overfitting. For preference\noptimization, we used Direct Preference Optimization (DPO) with a preference tuning learning rate multiplier to 0.1, training\nbeta parameter of 0.2, and adapter weight configured to 0.2. We varied the number of epochs between 1 to 10 based on\nthe size of the synthetic dataset for each model and leave a deeper exploration of hyperparameter configurations that could\nrequire a significant amount of compute to future work. SFT training was often until convergence. DPO training did not\nnecessarily converge by the end of all iterations.\nWe keep our prompts the same for every baseline and trained model on a given benchmark. Our prompts use CoT and\nzero-shot prompting. We use a temperature of 0.3 for Llama 3.1 8B since it was qualitatively good enough to prevent\nhallucinations and still led to diverse enough samples. MALT requires the presence of an initial training set containing\nquestion-answer pairs, which led to the use of MATH, CSQA, and GSM8K."}]}