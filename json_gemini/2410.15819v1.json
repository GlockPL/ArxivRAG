{"title": "LiMTR: Time Series Motion Prediction for Diverse Road Users through Multimodal Feature Integration", "authors": ["Camiel Oerlemans", "Bram Grooten", "Michiel Braat", "Alaa Alassi", "Emilia Silvas", "Decebal Constantin Mocanu"], "abstract": "Predicting the behavior of road users accurately is crucial to enable the safe operation of autonomous vehicles in urban or densely populated areas. Therefore, there has been a growing interest in time series motion prediction research, leading to significant advancements in state-of-the-art techniques in recent years. However, the potential of using LiDAR data to capture more detailed local features, such as a person's gaze or posture, remains largely unexplored. To address this, we develop a novel multimodal approach for motion prediction based on the PointNet foundation model architecture, incorporating local LiDAR features. Evaluation on the Waymo Open Dataset shows a performance improvement of 6.20% and 1.58% in minADE and mAP respectively, when integrated and compared with the previous state-of-the-art MTR. We open-source the code of our LiMTR model.", "sections": [{"title": "1 Introduction", "content": "Time series motion prediction involves generating potential future trajectories of a road user and estimating their likelihood of occurrence based on a limited set of temporal points [24]. This is a crucial task in the safe operation of autonomous vehicles, as part of the high-level functional pipeline of autonomous vehicles: environment perception, motion prediction, planning, and control [12]. Current motion prediction solutions mainly receive a relatively coarse-grained modality; the output of the object detection step, which contains the objects' positions, velocities, accelerations, and bounding boxes in combination with accurate road information [24, 22]. This representation of the objects is quite efficient due to their high information density [9]. However, it potentially leaves out more fine-grained information about a target such as a person's gaze or pose direction [17, 4] during the environment perception step. Including these modalities could potentially help the model predictions for vulnerable road users, e.g. pedestrians and cyclists. Li et al. [14] have shown that light detection and ranging (LiDAR) data could effectively be used to predict pedestrian crossing. We aim to investigate how directly incorporating LiDAR data into motion prediction models can enhance the accuracy of predicted future trajectories, particularly for vulnerable road users.\nWe introduce a new LiDAR encoder for motion prediction called LiMTR,2 based on the Motion Transformer by Shi et al. [21, MTR]. We provide the encoder with LiDAR data pertaining only to the target road user, to help the model focus on features such as a person's pose or gaze direction. We refer to this as providing local LiDAR features, compared to global scene-level features. We demonstrate its effectiveness by experimentation on the Waymo Open Dataset [4, WOD, see Appendix A], gaining an overall performance improvement of 6.20% and 1.58% in minimum average displacement error (minADE) and mean average precision (mAP) respectively, compared to the baseline without LiDAR."}, {"title": "2 Time Series Motion Prediction", "content": "In this section, we explain the task of time series motion prediction, specifically how it is defined for the Waymo Motion Prediction challenge. For this task, the model is provided with 1 second of past information on a scene at 10Hz, and needs to generate up to m potential future trajectories for each of the n road users, referred to as agents, where m = 6 and 1 \u2264 n \u2264 8 for a single scene. A trajectory consists of the (x, y) coordinates on a local map for 8 seconds into the future at 2Hz. For further background on motion prediction, refer to Appendix B."}, {"title": "2.1 Performance Metrics", "content": "We compare the models with metrics used for the Waymo Motion Prediction challenge, also com- monly employed in research; for full definitions see [7]. All metrics are computed at time horizons of 3, 5, and 8 seconds and for each class of road user (pedestrian, cyclist, vehicle). Lastly, the mean over these time horizons and classes is taken to compute the final metric.\nminADE\u2193 \u2208 [0, \u221e) Minimum Average Displacement Error, takes the minimum of the L2-norm between the ground truth trajectory and the m predicted trajectories.\nMR\u2193 \u2208 [0, 1] Miss rate is the fraction of trajectory sets where none of the trajectories are correct. A trajectory is determined correct if its final position is within a certain threshold distance of the ground truth. This distance threshold is different for lateral and longitudinal distances and scales with the velocity of the target at time 0.\nmAP\u2191 \u2208 [0, 1] Mean Average Precision uses the same definition of correctness as the miss rate to determine true positives and false positives, and only one true positive is allowed per target. The predicted trajectories per target are bucketed over 8 behaviors, i.e. left turn, right turn, etc. Then the area under the precision-recall curve is calculated across various thresholds to get the average precision per bucket. The mean value of these buckets is the mAP."}, {"title": "3 LIMTR", "content": "We proceed to design our LiDAR encoder for motion prediction. We provide our network with local sections of the raw point cloud directly, training all of our components end-to-end."}, {"title": "3.1 Local LiDAR Features", "content": "The input to our LiDAR encoder is the subset of 3D LiDAR points of each road user we are currently predicting. We select all 3D LiDAR points that fall into the provided bounding box of the object, where we include a 15% margin, to include possible missed points due to errors in bounding box labeling. We use this subset to greatly reduce computational requirements, and we hypothesize that the LiDAR points of the target are most informative for its future trajectory. This may include potential features such as a person's gaze direction or a cyclist signaling to change course by pointing.\nThe 3D LiDAR points are centered on the target and rotated to align with the forward-facing direction of the target, similar to the preprocessing of the trajectories [3, 21]. The number of LiDAR points taken is limited to 512; if the number of available points exceeds this, a random subset is taken, if fewer are available the set is padded with zeros. The PointNet architecture [18] has been shown to be quite robust to random removal of points. Furthermore, we add a one-hot encoding of the target class to each LiDAR point, similar to how Vora et al. [25] add the probabilities of another detector. Additionally, we include the intensity feature and leave out range and elongation, see subsection 4.4."}, {"title": "3.2 LiDAR Encoder", "content": "Figure 1 presents the architecture of our LiDAR encoder, based on the PointNet [18] architecture, which was chosen as it can directly process LiDAR points without the need for voxelization or other extensive preprocessing. The diagram shows the dimensions of how a single-object LiDAR point set is transformed into a feature vector; T = 11 for the number of frames and N = 512 for the number of points. The encoder consists of two parts: the first part compresses the point dimension and the second compresses the time dimension, resulting in a single feature vector of 256, highlighted in green, orange, and blue, respectively. The point compression part uses shared MLPs over the feature dimension and a max-pool layer. Max-pooling is permutation invariant, which is advantageous as the LiDAR points do not have a distinct order [18]. Additionally, as done in [18, 21], we add a global feature by taking the max-pool, repeating the feature, and concatenating it to the feature dimension. The time compression part flattens the time dimension and then applies multiple MLP layers.\nThe three MLP blocks consist of 12 linear layers, each followed by Batch Normalization [13] and ReLU activation [11]. The hidden dimensions within each MLP block are [256, 512, 1024]. From scaling experiments, discussed in subsection 4.3, we determined that 12 layers for each shared MLP gave optimal performance. Figure 2 shows how the LiDAR encoder is integrated into MTR [21]. The LiDAR data is first passed through the encoder and then sent to the local self-attention module. Next, the LiDAR feature is combined with the processed agents' features to be used in cross-attention and is also directly sent to the motion prediction head."}, {"title": "4 Results and Discussion", "content": null}, {"title": "4.1 Experimental setup", "content": "We train LiMTR with the AdamW [15] optimizer with \u03b2\u2081 = 0.9, \u03b22 = 0.999 a learning rate of 3.10-4, and 0.01 weight decay. We use a Linear Decay learning rate scheduler with a linear warmup period of 5%, following [5]. We train the model for 60 epochs and a batch size of 192 scenarios on 12 NVIDIA A100 GPUs, which takes approximately 80 hours. We compare our LiMTR model against multiple baselines, the most important one being MTR [21] without the LiDAR modality. Furthermore, we compare against MGTR [8] which uses global scene-level features, and against Wayformer [4], that uses a pretrained object detection model for LiDAR."}, {"title": "4.2 Main Results", "content": "Table 1 presents our results on the Waymo Motion Prediction [4] validation set. The bottom section shows the performance of Wayformer with [4] and without [17] LiDAR, for which the authors only reported at t = 8s. The top section shows averages over t = 3, 5, 8s, as is standard practice [8, 21]. Our LiDAR approach shows consistent performance improvement on all metrics compared to MTR with an average increase of 6.20% and 1.58% in minADE and mAP, respectively. In comparison to Wayformer with a pretrained LiDAR model, LiMTR achieves an improved mAP, but a slightly higher minADE. Their LiDAR solution predicts all objects in the scene which might explain the extra benefit on that metric. LiMTR performs especially well on vulnerable road users, such as pedestrians and cyclists. These show the largest increase in mAP, suggesting that gazes or poses may indeed be inferred from the LiDAR data."}, {"title": "4.3 Scaling experiments", "content": "We conduct a network scaling experiment to investigate the LiDAR encoder size required to extract relevant features. We increase the depth of the 3 MLP blocks simultaneously from 2 to 14 layers with a step of 2. The resulting model ranges from 7.8M to 24M parameters. In the scaling and ablation experiments, we train for 30 epochs on 10% of the data, using batch size 64 and learning rate 10-4. Figure 3 shows the LiDAR encoder size in the number of parameters to minADE. The plot indicates that model performance correlates with size, where larger models are preferred. For the main model, we use the 12-layer, 22M parameter model as it demonstrates better performance on the mAP metric."}, {"title": "4.4 Ablation study", "content": "We perform ablation experiments to determine two aspects: which LiDAR input features are important and what is the effect of using multiple LiDAR frames. First, as described in Appendix A, the dataset also includes additional LiDAR features; range, intensity, and elongation. We trained our model with each feature alone and with all three included. Second, over the 1 second of past data, we have 11 timeframes of LiDAR. To determine the importance of the time series aspect of the LiDAR encoder, we trained LiMTR with 11, 6, 3, or 1 LiDAR timeframe(s) provided. Each option was trained with 2 different seeds and the mean and standard deviation of the minADE and mAP are shown in Table 2.\nTime series. For the number of timestamps, there is no clear distinction on which is better. The full 11 timestamps displays the best performance on mAP, however, the worst performance on minADE. The variance between the runs is too large to determine a clear choice.\nFeatures. The intensity feature performs better than range and elongation when looking at the minADE metric. Further, the experiments with all features included show similar performance to the intensity feature. This suggests that the intensity feature meaningfully contributes to the task."}, {"title": "5 Conclusion", "content": "We propose LiMTR, a novel time series motion prediction model for autonomous driving, that incorporates local LiDAR features. The model is trained directly on LiDAR data allowing it to capture intricate motion details. When compared to the previous state-of-the-art MTR model, we show that LiMTR gains 6.20% and 1.58% in minADE and mAP, respectively. By only providing our model with the LiDAR data of a target road user, we help it focus on target-specific features. This makes LiMTR complementary to other techniques that incorporate global LiDAR features."}, {"title": "Appendix", "content": null}, {"title": "A Waymo Open Dataset", "content": "The release of the LiDAR data for the Waymo Open Dataset (WOD) [4] opened up new possibilities for using LiDAR data for motion prediction, which has remained low due to the low availability of large motion prediction datasets with LiDAR data. The Waymo Motion Prediction dataset is two orders of magnitude larger compared to the second largest open motion prediction dataset with LiDAR NuScenes [1] with 1k scenarios compared to 104k scenarios for Waymo [4]. The scenarios in the WOD are often interesting traffic situations, such as an intersection, and the data includes tracked and labeled road users in combination with road layout information. Each scene is 9 seconds long and recorded at 10 Hz. The data for a scenario consists of three parts: the road information, the agents' information, and LiDAR data. The road and agent information is made with their offboard perception system from the raw sensor data of the car [7].\nRoad information. The road information consists of static map features and dynamic features. The latter are traffic light states connected to a lane, given at each timestamp. The static map features represent the 2D road layout and contains a polyline or outline of the following types: lane, road edge, road line, stop sign, crosswalk, speedbump, and driveway.\nAgents information. The agents' information consists of information on the dynamic objects of the scenario, which are classified as vehicles, cyclists, or pedestrians. Each object consists of a state for every timestamp, where the state has a position in local x, y, and z coordinates, velocity in x and y, and the object's bounding box.\nLiDAR. The LiDAR data is collected from five LiDAR sensors, one on top of the vehicle with a sensor resolution of 64 pixels in height and 2650 pixels in width, and four on the sides of the vehicle with a sensor resolution of 116x150. Each pixel records its position in x, y, and z and its corresponding features range, intensity, and elongation."}, {"title": "B Motion Prediction Background", "content": "In this Appendix, we describe related machine learning work on the problem of time series motion prediction using LiDAR data and explain the MTR [21] model used as a baseline for this work. The field of time series motion or trajectory prediction is activity searching for the best and safest algorithms to use for autonomous driving [4, 8, 17, 19, 20]. The starting point for this research is the Waymo Motion Prediction challenge. For this task, the model is given the first second of past data and should predict the exact locations of various road users up to 8 seconds into the future. The provided data consists of road information, road users' information, and raw LiDAR data."}, {"title": "B.1 Motion Prediction with LiDAR", "content": "There are a few motion prediction works that make use of LiDAR data. Early work by Uber includes LiDAR by voxelizing the data into a birds-eye view (BEV) 3D grid with a resolution of 0.2 [16, 2] or 0.16 [6] meters. The voxelized LiDAR is then processed with convolutional neural network (CNN) backbones to extract relevant features and then jointly perform object detection and motion prediction. Similarly, MGTR [8] also includes BEV 3D voxelized LiDAR data, with two different resolutions of 0.8 and 1.6 meters. These are encoded with a pre-trained LiDARMultiNext [26] model and used in their Transformer encoder-decoder architecture to do motion prediction. The MGTR model achieved state-of-the-art performance winning the Waymo Motion Prediction challenge in 2023. The voxelization is an important processing step to reduce computation requirements and makes it easier to process with CNN backbones which are good at capturing local structures in the LiDAR data [2]. However, the voxelization might leave out finer details due to lowering the resolution, where details that require a higher resolution will not be distinguishable anymore, such as potentially a person's pose or gaze direction."}, {"title": "B.2 Motion Transformer: MTR", "content": "The MTR model [21] is the baseline for this work and the model we extend with our LiDAR approach. We select MTR as the open-source state-of-the-art during our research, following the first edition of the MTR model [21]. Shi et al. later published the improved MTR++ [22] version that includes symmetric scene context encoding and joint motion decoder, allowing the model to predict multiple objects in the same scene more efficiently. The MTR model uses the Transformer encoder-decoder architecture. Figure 2 shows an outline of the model, where the grey parts are the original architecture.\nData representation. The road information and agent history are encoded using the vectorized representation [9]. The coordinates of the road and agent polylines are centered and rotated on the current target agent coordinate system, adopting an agent-centric strategy [10, 24].\nContext encoder. First, a polyline encoder based on the PointNet [18] architecture is used to encode the agent and road vectors to produce their respective features. These encoded features are then processed with local self-attention by only attending to nearby objects. This is done by applying k-nearest neighbors (KNN) on each object based on its initial position to collect a neighborhood of k neighbors. This reduces the computation requirements and crucially preserves the local structure of the scene [22].\nMotion decoder. The motion decoder uses the transformer-based decoder structure to apply cross- attention on the previously encoded road and agent features. They employ learnable query embeddings from intention points to act as intention queries. The intention points are made by applying k-means clustering on the set of relative final locations of all targets in the training set, separately for the three classes, vehicles, pedestrians, and cyclists. These intention points act as anchor points, similar to [10] goal points, and are crucial to generating a distinct set of trajectories. The query embeddings are then used in cross-attention with the agent features to collect relevant features for each intention point.\nBefore applying attention to map features, a dynamic map collection technique is used. Similar to the context encoder local attention method, this also applies KNN on the map features to collect a set of relevant features. For the first decoder layer, the distance to the intention point is used for KNN, and in consecutive layers, the smallest distance to any point in the previously predicted trajectories. This allows the model to iteratively refine the trajectory on relevant features.\nThe motion prediction head takes the queried features per intention point and uses multilayer perceptron (MLP) layers to generate the trajectories with accompanying probabilities. A trajectory consists of 80 timestamps which cover 8 seconds with 10Hz. Following [3, 24] they represent the trajectories as Gaussian Mixture Model (GMM) components x, y, stdx, stdy, p, for each timestamp. This is an important part of modeling the ambiguous behavior of the agents.\nLoss. The loss function consists of three parts, the negative log-likelihood loss for the GMM components, an L1 loss on the predicted velocity, and a cross-entropy loss on the probabilities of the trajectories."}, {"title": "C Analysis", "content": "In this section we include additional figures and information that complement the scaling and ablation studies in the paper. For the time series ablation: we take equally spaced timestamps over the 11 total. Thus, e.g., for 6 time steps these are the frames (0, 2, 4, 6, 8, 10), and for the 1 timeframe we use the last time step."}]}