{"title": "Towards training music taggers on synthetic data", "authors": ["Nadine Kroher", "Steven Manangu", "Aggelos Pikrakis"], "abstract": "Most contemporary music tagging systems rely on large volumes of annotated data. As an alternative, we investigate the extent to which synthetically generated music excerpts can improve tagging systems when only small annotated collections are available. To this end, we release GTZAN-synth, a synthetic dataset that follows the taxonomy of the well-known GTZAN dataset while being ten times larger in data volume. We first observe that simply adding this synthetic dataset to the training split of GTZAN does not result into performance improvements. We then proceed to investigating domain adaptation, transfer learning and fine-tuning strategies for the task at hand and draw the conclusion that the last two options yield an increase in accuracy. Overall, the proposed approach can be considered as a first guide in a promising field for future research.", "sections": [{"title": "I. INTRODUCTION", "content": "Systems which can automatically classify or tag music tracks are an essential component of large-scale music and multimedia indexing pipelines. Recent methods commonly rely on deep architectures which are either trained end-to-end on large annotated datasets (i.e. [1]) or pre-trained on large volumes of real-world audio data and then fine-tuned for various downstream tasks on smaller collections (i.e. [2], [3]). Although both approaches have shown promising results, they still rely on large volumes of labelled data to effectively pre-train or train end-to-end deep neural networks, a prerequisite which comes at great cost. Manual labelling is a time-consuming and tedious task and crowd-sourced or user-contributed annotations are often noisy or inconsistent.\nTo alleviate the problem, recent approaches commonly employ data augmentation strategies (see i.e. [4]) which create variants of training instances by applying small modifications, i.e. pitch shifting or time stretching. While data augmentation does generally yield improvements, the augmented examples are still strongly correlated with the originals and do not prevent overfitting to the extent that adding new data would.\nIn an attempt to overcome the dependence on annotated data further, we explore an alternative strategy that makes use of synthetically generated music to train a tagging system. Similar approaches have been adopted by computer vision systems to address tasks [5] for which annotated data is hard to assemble, as it is for example the case with defect detection in the steel industry [6], vessel classification from overhead imagery [7] or medical image analysis [8]. In the audio domain, [9] recently demonstrated that environmental sound classification can be improved by augmenting real-world datasets with synthetic examples generated with text-to-audio models.\nIn order to explore a similar approach in the music domain, we recently conducted a preliminary proof-of-concept experiment which involved only a small taxonomy of five perceptually easy to distinguish genres [10], which showed promising results. Based on these findings, we now extend our work to GTZAN [11], a small genre classification dataset, and explore various strategies for training on synthetic music data. Specifically, we first create GTZAN-synth, a collection of artificially generated music excerpts that follows the GTZAN taxonomy while being 10 times larger in data volume. We make all code for reproducing and scaling this collection publicly available to foster further research into this direction.\nIn addition, we explore several strategies towards improving the classification of a deep convolutional neural network on the GTZAN dataset by incorporating synthetic music excerpts into the training procedure. Our experiments cover a wide spectrum of options, including the simple integration of the synthetically generated data with the training split of GTZAN, the investigation of transfer learning and fine-tuning strategies, and a domain adaptation [12] mechanism to mitigate the distributional shift between real and synthetic data."}, {"title": "II. RELATED WORK", "content": "Music tagging systems analyse audio content in order to automatically annotate music tracks with high-level descriptors related to mood (i.e. \u201cmelancholic\" or \"upbeat\u201d), genre (i.e. \u201crock\u201d or \u201cjazz\u201d) or instrumentation (i.e. \u201cstring quartet\u201d or \u201csaxophone\u201d). Most recent methods rely on deep neural network architectures, such as convolutional neural networks [13] and transformers [14], which are usually trained end-to-end on large annotated music collections like the Million Song Dataset [15] or the Magna Tag-a-Tune dataset [16]. Other approaches leverage representations learned in a supervised setting [3] on even larger datasets and then formulate specific tagging problems as downstream tasks which are solved via transfer learning. In both scenarios, large volumes of annotated data are required, which may not be available for every use-case.\nIn order to overcome this need, some recent efforts have focused on self-supervised techniques (partially [17] or fully [18]) to learn audio representations. While this approach, which is also commonly used in natural language processing, has shown promising results, we explore in this paper the radically different idea of leveraging generative music systems to create artificial training data. To the best of our knowledge, with the exception of our previous exploratory study [10], this is the first work aimed at investigating frameworks for training music taggers on synthetically generated music datasets."}, {"title": "B. Generative music systems", "content": "The recent advancements in generative artificial intelligence for natural language processing and image analysis tasks have also driven rapid improvements of state of the art systems that can generate music excerpts conditioned on text and melody prompts. Prominent generative music models include Jukebox [19], MusicLM [20], Jen-1 [21] and MusicGen [22]. In this study, we focus on MusicGen, for which pre-trained models are publicly available via the Audiocraft\u00b9 library.\nWhile the distribution of synthetically created music via streaming services or its use in movies, TV or social media content poses legal issues [23] and has raised controversial discussions from an ethical point of view [24], we believe that there is unexplored potential in leveraging such systems for the creation of synthetic training data for tagging and analysis models."}, {"title": "C. Domain adaptation", "content": "Although generative systems for natural language and image content have become very sophisticated over the recent years and are now capable of producing convincing output, generative music systems, while advancing rapidly, are still limited in their ability to produce realistic music tracks and are highly relevant to the input prompt. Consequently, with a view on training music taggers, it is reasonable to expect a content distribution shift between real and synthetic data.\nThis issue has also been addressed in other domains, mainly in computer vision, via the so-called domain adaptation (DA) methods, which essentially introduce additional loss terms during the training stage to ensure that a machine learning model trained partially or fully on synthetic data will generalise its performance to real world data. For a comprehensive review of different methods on various use-cases, training frameworks and domains, we refer to [12]. In the context of music, existing work on domain adaptation has been limited to specific use-cases that do not involve synthetic data, i.e. cross-cultural emotion recognition [25] or the compensation of differences among microphones for the purposes of piano transcription [26].\nFor the music tagging task at hand, we investigate the method proposed in [27], which addresses supervised DA in neural network training via an additional loss term that operates on a bottleneck layer and essentially acts as a contrastive loss that forces intermediate representations from real and synthetic data of the same class to be in close proximity in a Euclidean sense while maintaining large Euclidean distances to instances of different classes."}, {"title": "III. DATA", "content": "Our experiments make use of two datasets, the well-known GTZAN [11] genre detection dataset and GTZAN-synth, a synthetic dataset which we generated using the MusicGen [22] generative music system. GTZAN-synth follows the GTZAN class taxonomy but it is ten times larger."}, {"title": "A. Real music dataset", "content": "During the early days of music tagging, the GTZAN dataset was widely used as a benchmark collection to evaluate and compare competing approaches. It contains a total of 1000 music excerpts of length 30s belonging to 10 genres (e.g., \"blues\", \"classical\u201d or \u201crock\u201d), with each genre being represented by 100 tracks. The dataset, including audio files and metadata, is publicly available\u00b2 via the Kaggle platform.\nIn our experimental evaluation, we the use artist-filtered validation splits proposed in [28] which are available on GitHub\u00b3."}, {"title": "B. Synthetic music dataset", "content": "In order to study the suitability of synthetically generated music for training tagging systems and foster future work on this topic, we release the synthetic music dataset GTZAN-synth. Following the GTZAN taxonomy, we create genre-specific prompts that are used to condition the medium-sized MusicGen model. Compared to the original GTZAN, we scale by factor of 10, thus generating 1000 tracks per genre.\nMusicGen is controllable via text prompts and while its generative process is to a certain degree stochastic, e.g. the prompt \"a rock song\" will give different results if run repeatedly, we observed during our initial experiments that large volumes of excerpts generated with a single prompt do not exhibit sufficient diversity. Consequently, the main challenge in generating a synthetic music dataset lies in the creation of a large volume of genre-specific text prompts that yield sufficient variety while maintaining genre-specific characteristics.\nDuring initial experimentation, we also observed that MusicGen does not appear to generate vocals, even if prompted to do so. In addition, we noticed that prompts mentioning vocals often lead to output artefacts. As a result, we are restricted to generating instrumental examples.\nTo assemble an adequate number of text prompts for the MusicGen synthesis engine, we used a large language model (LLM) of the GPT-3 [29] family which can be accessed via"}, {"title": "IV. MODEL ARCHITECTURE", "content": "Since the aim of this paper is not to propose a novel genre tagging method but rather to explore the potential of using synthetically generated data, our neural network design roughly follows the MusiCNN architecture described in [1] which was intended for end-to-end training of music taggers on large volumes of data.\nThe network operates on 96-band mel-spectrograms extracted from 10s of audio with a sampling rate of 16kHz, a window size of 512 samples and a hop size of 256 samples. The input is then processed by a first layer of parallel convolutional filters of different kernel shapes which are designed to learn different temporal and timbral features. The resulting feature maps are then pooled across the full frequency axis, concatenated and processed by further convolutional layers with residual connections and pooling operations before being passed through a normalised dense layer with 512 units and a final softmax classification layer with 10 units corresponding to the 10 genre classes. The architecture is shown in Figure 2. For a detailed description of the motivation behind the design choice the reader is referred to [1].\nIn our experimental evaluation, we train the model end-to-end and also in transfer learning and fine-tuning settings. In the case of transfer learning, the entire network is pre-trained on a large dataset and then only the penultimate dense layer and the final classification layer are trained from scratch on the smaller dataset with all other layers keeping their weights frozen. In the fine-tuning scenario, the entire network is first trained on a large collection and then training for all weights is resumed on the smaller collection."}, {"title": "V. SUPERVISED DOMAIN ADAPTATION", "content": "In order to compensate for potential distributional discrepancies between synthetic and real music data, we experiment with the supervised domain adaptation approach proposed in [27]. We can describe the genre classification network as $f = g \\circ h$, where $g : X \\rightarrow Z$ takes the mel-spectrogram $X$ as input and maps it to the penultimate dense layer $Z$, which we will treat as an intermediate feature representation. The classification layer $h : Z \\rightarrow Y$ maps this intermediate representation Z to the final classifier output Y.\nWithout any counter measures, any distributional shift between real $X$ and synthetic $X$, inputs can potentially propagate to the intermediate representation, resulting in discrepancies between $Z_r$ and $Z_s$. In order to drive the network towards learning feature representations which encode commonalities between the two domains, we employ a contrastive loss $L_{SA}$ in addition to the classification loss $L_{CLS}$. $L_{SA}$ encourages semantic alignment rather than domain-specific alignment of the intermediate feature representation.\nGiven a synthetic instance $x^s$, we can compute the contrastive loss by comparing its embedding to that of a real sample $x^r_i$ of the same label and to that of a real sample $x^r_k$ of a different label, where $y^r_i = y^s$ and $y^r_k \\neq y^s$ as follows:\n$d(g(x^s), x^r_i)) = \\frac{1}{2}||g(x^s) - g(x^r_i)||^2$ (1)\nand\n$d(g(x^s), x^r_k)) = max(0, m - ||g(x^s) - g(x^r_k)||^2)$ (2)\nwhere $m$ denotes a margin parameter related to the separability in the embedding space and the semantic alignment loss becomes $L_{SA} = d(g(x^s), x^r_i) + d(g(x^s), x^r_k)$. In practice, we did not observe a difference between comparing to a single randomly selected target instance versus computing the average distance over all target samples as originally suggested by [27]. The second option is computationally very inefficient since the forward pass over all target samples has to be computed at each training step.\nFinally, both loss terms can be combined as $L = \\gamma L_{SA} + (1 - \\gamma) L_{CLA}$, where $\\gamma$ is a balancing factor which controls the relative weight of each term."}, {"title": "VI. EXPERIMENTAL EVALUATION", "content": "In order to assess the potential of using synthetically generated music for training tagging systems, we run a series of experiments. For all settings, we train with a batch size of 4 and use the Adam [31] optimizer with an initial learning rate of 0.001, except for the fine-tuning setup where we decrease the initial learning rate to 0.0001. For the domain adaptation experiment, we set the margin to m = 2 and the balance parameter to $\\gamma = 0.7$. We furthermore use an early stopping criterion based on the classifier's categorical cross-entropy loss with a patience period of 5 epochs and select the best performing model for evaluation. For the GTZAN dataset, where excerpts are of length 30s, we use a random 10s excerpt during training and the central 10s segment during validation.\nFor all experiments evaluated on the GTZAN dataset, we report mean and standard deviation of the categorical accuracy and of the categorical cross-entropy loss over the three validation splits proposed in [28]. When evaluating on synthetic data, we report mean and standard deviation over three random splits instead. All results are shown in Table I\nFirst, we train and evaluate the model solely on the GTZAN dataset (E2E-real). Given the relatively large number of network parameters and the rather small dataset volume, we observe that the model overfits early and yields a mean accuracy of 46.7%. We also train and evaluate the same model on the larger GTZAN-synth dataset (E2E-synth) and observe a much higher mean accuracy of 90.6%.\nWhile the result on the synthetic data appears encouraging, it does not guarantee that this classifier will generalise well to real-world music excerpts. It is for example possible that the synthetic music excerpts only represent a narrow fraction of the distribution of real music examples of the same genre. Another possibility is that the generative model causes artefacts or characteristics that are not found in real-world examples, thus causing distributional discrepancies between real and synthetic data. The fact that such issues do indeed exist is confirmed when we repeat the first experiment and simply add GTZAN-synth to the training splits of GTZAN (E2E-add). Here, we observe that the mean accuracy on the GTZAN validation does not seem to improve (47.3%) and the loss even slightly worsens (1.74 vs. 1.61).\nIn an attempt to mitigate any distributional discrepancies between real and synthetic data, we repeat the previous experiments but employ the DA method (E2E-DA) described in Section V. We first visualise the intermediate representations after the penultimate layer with and without the additional DA loss term by approximating a two-dimensional representation using the t-sne [32] algorithm. The resulting plot in Figure 3 (a)) reveals that there does indeed appear to be a distributional discrepancy between real and synthetic data which seems to largely disappear when DA is used (Figure3 (b)). However, the results do not indicate a significant improvement in classification accuracy (47.6%) and only the loss appears to have somewhat improved (1.54). One reason for this behaviour could be that by driving synthetic data towards the real-world examples, the lack of variety in the training dataset persists and thus generalisation does not improve.\nAs a next step, we investigate if the model trained solely on synthetic data (E2E-synth) can be adapted for to real world data via transfer learning or fine-tuning. In the transfer learning experiment (TL), we freeze the convolutional layers from E2E-synth and only train the last two layers on GTZAN. For this setup, we observe an increase in mean accuracy to 52.6%. When we fine-tune on GTZAN by initialising the network with the weights from E2E-synth and by resuming training on all layers, we obtain an even slightly higher increase in mean accuracy to 54.8%. This indicates that training on synthetic data allows the network to learn useful features that aid in learning downstream tasks.\nWhile these last two experiments demonstrate that synthetic data can help to improve performance of deep architectures on small datasets, it is worth noting that the results still do not reach those reported for systems that rely on transfer learning from large annotated datasets (i.e. 82.1% reported by [33]) or more lightweight end-to-end systems (i.e. 65.8% reported by [34])."}, {"title": "VII. CONCLUSIONS", "content": "We have investigated the use of artificially created music excerpts for training tagging systems on the example of a genre classification task. To this end, we release GTZAN-synth, a collection of synthetic music excerpts that follows the taxonomy of the well-known GTZAN dataset but is 10 times larger. In a series of experiments, we first demonstrated that simply adding synthetic data during training does not yield a significant performance improvement. We furthermore investigated domain adaptation, transfer learning and fine-tuning and showed that the two last options are suitable methods for increasing the performance of deep architecture trained on small datasets. We believe that these findings together with the release of GTZAN-synth can foster future research into this direction, for example on advanced prompt engineering for generative music systems, on the influence of scaling the amount of synthetic data or on extending this work to other custom tagging tasks."}]}