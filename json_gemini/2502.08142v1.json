{"title": "Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences", "authors": ["Shanshan Han", "Salman Avestimehr", "Chaoyang He"], "abstract": "We present Wildflare GuardRail, a guardrail pipeline designed to enhance the safety and reliability of Large Language Model (LLM) inferences by systematically addressing risks across the entire processing workflow. Wildflare GuardRail integrates several core functional modules, including Safety Detector that identifies unsafe inputs and detects hallucinations in model outputs while generating root-cause explanations, Grounding that contextualizes user queries with information retrieved from vector databases, Customizer that adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer that corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector. Results show that our unsafe content detection model in Safety Detector achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets. Meanwhile, the lightweight wrappers can address malicious URLs in model outputs in 1.06s per query with 100% accuracy without costly model calls. Moreover, the hallucination fixing model demonstrates effectiveness in reducing hallucinations with an accuracy of 80.7%.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are increasingly deployed in latency-sensitive, high-stakes systems-from network automation to real-time decision support in critical fields such like healthcare [20, 58, 79] and finance [40, 75]. However, their widespread adoption is hindered by significant safety risks. Malicious inputs can exploit prompt injection vulnerabilities to manipulate outputs [7, 34, 44, 69, 83, 84], while unconstrained responses may propagate hallucinations, biases, nonsensical or factually incorrect knowledge, or security threats like phishing URLs [16, 24, 74, 76, 80, 82]. These issues not only compromise user trust but also pose systemic risks, such as resource misuse in cloud networks or erroneous configurations in software-defined infrastructures [53-56, 62].\nSafeguarding LLMs is crucial and can never be overstated. Un-safe inputs can manipulate LLM outputs, reveal sensitive infor-mation, bypass system instructions, or execute malicious com-mands [5, 7, 34, 44, 64, 69, 77, 83, 84]. Problematic outputs can confuse users, perpetuate biases, and undermine users' trust in LLM-based systems, particularly in domains like healthcare and finance, where inaccuracies or biases can have legal or societal repercussions.\nAddressing safety issues in LLM inference is complex, as risks can arise at any point during processing user queries. While existing work addresses isolated aspects of LLM safety [14, 27, 35, 46], no uni-fied solution holistically mitigates risks across the entire inference pipeline. Standalone detection models [21, 36, 48] operate reactively to flag unsafe content, but they require full retraining or finetun-ing to adapt to new safety requirements and lack mechanisms to correct errors in the LLM outputs. Post-hoc correction methods rewrite problematic content in the LLM outputs but fail to address their root causes, such as hallucinations, that often stem from in-sufficient, inaccurate, or outdated source information [24, 76, 82].\nWhile retrieval-augmented generation (RAG) [6, 18, 37] can mitigate hallucinations by enriching user queries with external contextual knowledge, the probabilistic retrieval of knowledge cannot enforce deterministic safety policies, e.g., blocking mandated sociopolitical terms. Rule-based post-processing \"wrappers\" [52], on the other hand, offer agility for time-sensitive updates and excel at syntactic filtering (e.g., regex-based phishing URL detection via APIs like Google Safe Browsing [19]), but fail to address semantic risks in the contents generated by LLMs.\nWe argue that, enhancing the overall safety of LLM inference de-mands a comprehensive pipeline that orchestrates heterogeneous func-tional components, such as ML models, RAG, and light-weighted wrap-pers. A well-designed guardrail pipeline not only mitigates safety risks from a global perspective but also enable users to customize their workflows to high flexibility and efficiency.\nThis paper introduces Wildflare GuardRail, a guardrail pipeline that systematically integrates detection, contextualization, correc-tion, and customization to ensure robust safety and adaptability during LLM inference. Wildflare GuardRail integrates four compo-nents, including i) Safety Detector that identifies unsafe content (e.g., toxicity, bias, hallucinations) in user inputs and LLM out-puts; ii) Grounding that contextualize user queries with vector databases; iii) Customizer that leverages lightweight wrappers to edit LLM output according to user needs in a real-time manner; and iv) Repairer that corrects hallucinated content detected in the LLM outputs. Our contributions are summarized as follows:"}, {"title": "2 Related Work", "content": "Moderation-based harmfulness mitigation approaches leverage rule-based methods, ML classifiers, and human interfaces to moni-tor, evaluate, and manage the outputs produced by LLMs to ensure the outputs generated by LLMs are safe, appropriate, and free from harmful content [21, 36, 48, 52, 61]. We compare our approaches with the existing approaches in Table 1.\nClose-sourced solutions. OpenAI Moderation API [48] and Per-spective API [36] utilize ML classifiers to detect undesired contents. These approaches provide scores for pre-defined categories of harm-ful content, such as toxicity, identity attacks, insults, threats, etc. These tools are widely used in content moderation to filter out harmful content and has been incorporated into various online platforms to protect user interactions [67]. However, they are less adaptable to emerging safety risks as they are not open-sourced and cannot be finetuned.\nOpensourced solutions. LlamaGuard [25] leverages the zero-shot and few-shot abilities of the Llama2-7B architecture [72] and can adapt to different taxonomies and sets of guidelines for different applications and users. Despite its adaptability, LlamaGuard's relia-bility depends on the LLM's understanding of the categories and the model's predictive accuracy. However, deploying LlamaGuard on edge devices is challenging due to its large number of parameters, which typically exceed the computing resources available on edge devices. Detoxify [21] offers open-source models designed to detect toxic comments. These models, based on BERT [11] and RoBER-Tac [45] architectures, are trained on the Jigsaw datasets [29-31]. Detoxify provides pre-trained models that can be easily integrated into other systems to identify toxic content. Also, the models are able to recognize subtle nuances in language that might indicate harmful content, making them effective for moderation.\nCustomizable solutions. Guardrails [52] and Nvidia NeMo [61] employ customizable workflows to enhance safety in LLM inference. Guardrails [52] define flexible components, called \"rails\", to enable users to add wrappers at any stage of inference, which enables users to add structure, type, and quality guarantees to LLMs outputs. Such rails can be code-based or using ML models. However, it does not have self-developed model and miss a unified solution for general cases. Nvidia NeMo Guardrails [61] functions as an intermediary layer that enhances the control and safety of LLMs. This framework includes pre-implemented moderation dedicated to fact-checking, hallucination prevention, and content moderation, which offers a robust solution for enhancing LLM safety."}, {"title": "3 Wildflare GuardRail Overview", "content": "Wildflare GuardRail enhances safety of LLM inputs and outputs while improving their quality. Specifically, it achieves two goals, 1) all user inputs are safe, contextually grounded, and effectively processed, such that the inputs to the LLMs are of high-quality and informative; and 2) the output generated by the LLMs are evaluated and enhanced, such that the outputs passed to users can be both relevant and of high quality. The pipeline can be partitioned into two parts, including 1) processing before LLM inference that enhances user queries, and 2) processing after LLM inference that detects undesired content and handle them properly. We overview our pipeline in Figure 1.\nPre-inference processing. Before sending user queries to LLMs, Wild-flare GuardRail detects if there are any safety issues in the queries"}, {"title": "4 Wildflare GuardRail Safety Detector", "content": "Safety Detector addresses unsafe inputs and inappropriate LLM responses to ensure that both the user queries provided to the models and the LLM outputs are safe and free from misinformation.\n4.1 Unsafe Input Detection\nWe developed a model to detect unsafe contents in user queries before they are processed by LLMs for inference. While existing approaches categorize unsafe content into various types (e.g., tox-icity, prompt injection, stereotypes, harassment, threats, identity attacks, and violence) [21, 48, 74], our method employs a unified, binary classification model finetuned based on our opensourced LLM [68], classifying content as safe or unsafe.\nThis strategy offers several key advantages, as follows: i) By fine-tuning our base model, which has been trained on vast amounts of data, the classification model can leverage pre-existing knowledge relevant to safety detection. ii) A binary classification of \"safe\" and \"unsafe\" is both efficient and sufficient for LLM services, as any unsafe query should be rejected, regardless of the specific risk. iii) This approach avoids the complexities and potential inaccuracies of categorizing overlapping or ambiguous types of unsafe content in some publicly available datasets. For example, toxicity toward minority groups could also be classified as bias, but current datasets may inadequately capture such nuances. iv) Using straightforward code logic, we can transform public datasets for safety detection into clear safe/unsafe labels, minimizing ambiguity and ensuring high-quality training data.\nThe biggest challenge in training such model is the discrepancy between the training data and real-world user query distributions, where using traditional datasets alone can result in poor perfor-mance due to their divergence from actual user queries [48]. \u03a4\u03bf mitigate these issues, we integrated data of various domains and contexts to better simulate the variety of unsafe queries that users might submit. We crafted a training dataset by combining samples randomly selected from 15 public datasets, as will be introduced in Table 2 in \u00a78. Such a dataset captures diverse contents in user inputs in practice, thus can be more representative on potential real-world inputs.\n4.2 Hallucination Detection and Reasoning\nHallucinations occur when the LLM generates responses that is inaccurate, fabricated, or irrelevant [17, 23, 49, 60]. Despite ap-pearing coherent and plausible, hallucinated LLM responses are unreliable, often containing fabricated, misleading information that is divergent from the user input, thus fail to meet users' expecta-tions and severely undermine the trustworthiness and utility of the LLM applications. While grounding can mitigate hallucinations"}, {"title": "5 Wildflare GuardRail Grounding", "content": "Wildflare GuardRail Grounding enhances the contextual richness and informativeness of user queries by leveraging external knowl-edge in vector database. Thus, LLMs can utilize such contextual knowledge to generate high-quality outputs, particularly by ground-ing user queries before they are passed to the LLMs for inference.\nTo support similarity search over the knowledge data, Wild-flare GuardRail creates vector indexes by vectorizing plaintext knowledge.Wildflare GuardRail employs two primary methods for indexing: i) Whole Knowledge Index that creates indexes based on"}, {"title": "6 Wildflare GuardRail Customizer", "content": "Wildflare GuardRail Customizer utilizes lightweight wrappers to flexibly edit or customize LLM outputs to fix some small errors or enhancing the format of the answer. The wrappers integrate code-based rules, APIs, web searches, and small models to effi-ciently handle editing and customization tasks according to user-defined protocols. Wildflare GuardRail Customizer offers several key advantages. It facilitates rapid development and deployment of user-defined protocols, which crucial in production environ-ments where real-time adjustments are necessary. In scenarios where training or fine-tuning LLMs is unfeasible due to time or resource constraints, this method provides an alternative for imme-diate output customization. Moreover, the wrappers enable flexible incorporation of various tools and data sources, which enhances the applicability of Wildflare GuardRail and reduces resource-intensive LLM calls.\nEXAMPLE 1 (WARNING URLs). The objective was to detect if LLM outputs contain URLs and prepend a warning message of the unsafe URLs at the beginning of the LLM outputs. Customizer should check the safety of the URLs founded, i.e., whether they are malicious or unreachable, and includes such information in the warning if they were unsafe. Customizer utilizes a regular expression pattern to iden-tify URLs within the text. Upon URLs founded, Customizer calls APIs for detecting phishing URLs, such as Google SafeBrowsing [19], and assess the accessibility of the benign URL by issuing web requests. Malicious URLs, as well as unreachable URLs that return status codes of 4XX, are added in the warning at the beginning of the LLM outputs.\nNote that the task in Example 1 cannot be achieved through prompt engineering when querying LLMs, as the warning must appear at the beginning, and LLMs generate content token by token, making later content unpredictable. We use the following example to illustrate this property, and experimentally evaluate the efficiency of Customizer wrappers in Exp 4 in \u00a78.\nEXAMPLE 2. We present a concrete demonstration of tasks that cannot be reliably accomplished through prompt engineering alone, due to the token-by-token generation mechanism inherent in LLMs. This sequential generation process fundamentally precludes antici-patory knowledge of future token occurrences during text production. Consider the following prompt submitted to GPT-4:\nWrite an English poem about a rabbit; please include information at the beginning of the poem about how many times the word \"rabbit\" appears in the poem.\nThe generated response (shown below) claims four occurrences of \"rabbit,\" while actual analysis reveals five occurrences:"}, {"title": "7 Wildflare GuardRail Repairer", "content": "Wildflare GuardRail Repairer addresses errors in the LLM out-puts that are challenging to resolve through editing with wrappers in Customizer, particularly, hallucinated content. Repairer analyzes and corrects the hallucinated output based on the reason for the hallucinations generated by the hallucination detection model.\nWildflare GuardRail Repairer takes several key inputs, including the user's original query, the context retrieved with Grounding, the hallucinations responses generated by the LLM, as well as the reason for hallucination. Given these inputs, Repairer corrects the flawed output according to the hallucination reason. To enable Repairer to handle hallucinations effectively, we leverage the same hallucina-tion detection dataset as Safety Detector, i.e., HaluEval [38], that contains user questions, contexts, hallucinated LLM answers, and correct answers. We also designed a customized data template that incorporates the information. The data templates for training, infer-ence, as well as an example for the training data, are demonstrated in Figure 3."}, {"title": "8 Experiments", "content": "We evaluate the performance of different modules in Wildflare GuardRail. We use our self-developed model, Fox-1 [68], as our base model for finetuning three models, including an unsafe content detection model for Safety Detector, an explainable hallucination detection model for Safety Detector, and a hallucination fixing model for Repairer. Below we first introduce Fox-1 and the fine-tuned the models for different functional modules, then introduce experiment settings, and finally present our evaluation results.\nBase Model. Fox-1 is self-developed, decoder-only transformer-based language model with only 1.6B parameters [68]. It was trained with a 3-stage data curriculum on 3 trillion tokens of text and code data in 8K sequence length. The base model uses grouped query attention (GQA) with 4 KV heads and 16 attention heads and has a deeper architecture than other SLMs. Specifically, it has 32 trans-former decoder blocks, 78% deeper than Gemma-2B [66], 33% deeper than Qwen1.5-1.8B [3] and StableLM-2-1.6B [4], and 15% deeper than OpenELM-1.1B [50, 51].\nModel Finetuning. Safety Detector model is trained with a com-bined dataset that extract from 15 datasets to simulate real world un-safe content. Hallucination detection and explanation model and the hallucination fixing model are trained with HaluEval dataset [38]. The datasets for training and evaluation are summarized in Table 2.\nExperimental Setting. We utilized datasets that contain important knowledge to evaluate Grounding, where inaccurate retrieval can cause financial losses or harmful medical advice. We selected E-Commerce dataset [71] that contains customer service interactions\nDEFINITION 1 (PROBABILITY OF HALLUCINATION). Let a be an LLM answer, let {t1, ..., tk} be the top-k potential first token, and let {p1,..., pk} be their top-k probabilities. Let T be a tokenization function, and let T(\"Yes\") and T(\"No\") be the tokens corresponding to \"Yes\" and \"No\u201d, respectively. The probability of hallucination in a is \nPhalu(a) = \\Sigma_{t_i \\in T(\"Yes\")}p_i / (\\Sigma_{t_i \\in T(\"Yes\")}p_i + \\Sigma_{t_i \\in T(\"No\")}p_i)\nDetection results with Phalu (*) \u2265 0.5 indicate the content is classified as \"hallucinated\"; otherwise, the content is \"safe\". The detailed procedure of inference is described in Algorithm 2.\nDEFINITION 2 (CALLBACK). Let Do be a vector data storage that contains n records, let Q be a plaintext user query set, and let I(Q) be the vector index created based on Q. For each query q \u2208 Q, let Iq be the vector index created based on q, and let D\u028a(Iq) denote the set of Top-k records returned by querying Do with I(q), and let rq denote the most relevant record of q in D. The callback for Top-k queries on the query set Q is defined as: \nCk(Q) = \\frac{1}{|Q|} \\Sigma_{q \\in Q} [r_q \\in D_0(I_q)],\nwhere [] is Iverson Bracket Notation [26], equal to 1 if the condition inside is true, and 0 otherwise.\nTo ensure effective and informative grounding, the distribution of the index should closely align with query patterns, i.e., query distributions. By grounding user queries with knowledge retrieved with a proper index, the LLMs can generate contextually appropri-ate responses, and further, reduce hallucinations and improve the quality of the responses.\nin Figure 5 and Figure 6, respectively. The results indicate that Key Information Indexing outperformed Whole Knowledge Indexing, as key information indexes reflects the user queries better. Also, both original queries and rephrased queries achieved high callback rates, which demonstrates the effectiveness of vector retrieval when handling varied user inputs.\nExp 4. Efficiency of wrappers in Customizer. We evaluated the efficiency of Customizer in with the URL detection and validation task in Example 1 in \u00a76. We randomly selected 15 records from the each of the E-Commerce dataset [71] and the RedditSYACURL Dataset [13], combined each record to construct texts that contained URLs, and set 20% probability of inserting some malicious URLs into the text. In implementation, we leveraged Regex pattern for detect-ing URLs, Google SafeBrowsing [19] for detecting malicious URLs, and sent HTTP requests to the safe URLs to verify their reachability. We compared Customizer with several models, including TinyL-Lama [81], Mistral-7B [28], LLama2-7B [72], and Falcon-40B [2]. The results are shown in Table 3. We record average time to pro-cess one query, the success rate of detecting URLs (Detection Acc.), and the accuracy of identifying unsafe URLs (Validation Acc.). The results show that Wildflare Guard Rail Customizer takes much less time (1.06s per query) and significantly outperforms calling the models for editing LLM outputs. Also, TinyLLama and Falcon-40B failed to detect any URLs in the contents. Though Mistral is able to detect URLs with a high accuracy of 91.67%, the accuracy of identifying unsafe URLs is only 45.83%.\nExp 5. Effectiveness of fixing hallucinations in Repairer. We fine-tuned our fixing model using the HaluEval dataset [38]. We selected the QA and dialogue subsets. For each subset, we utilized 8,000 data samples for training, 1,000 for validation, and 1000 for testing. Also, we augmented the hallucination correction dataset with a hallucination_reason column, derived from the detection results of Safety Detector. Such annotation categorizes root causes of hallucinations identified during the detection phase, enabling mitigation strategies in the fixing stages. We utilize Vectara halluci-nation detection model [73] for evaluating the consistency between the LLM outputs and the information provided in the original data, including the user questions, the contexts, and the correct answers. We utilized the 100 records in the test dataset of the HaluEval-QA dataset for evaluation. Results show that our fixing model improves the quality of the LLM outputs by a lot. Moreover, 80.7% of the hallucinated data were fixed using Repairer."}, {"title": "9 Conclusion", "content": "The increasing development of LLMs demands robust safeguards against safety risks such as toxicity, hallucinations, and adversar-ial attacks during LLM inference. While existing solutions often address safety risks in isolation, they fail to mitigate safety risks from a global perspective. Wildflare GuardRail addresses these challenges through a guardrail pipeline that integrates different functional modules for detection, contextualization, correction, and customization. It not only bridges the safety gap in current LLM deployments but also sets a foundation for future research in trust-worthy AI. Potential directions include extending its modular design to emerging threats, optimizing resource efficiency for low-latency applications, and integrating multimodal safety checks."}]}