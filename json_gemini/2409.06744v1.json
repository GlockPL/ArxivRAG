{"title": "PROTEINBENCH: A HOLISTIC EVALUATION OF PROTEIN FOUNDATION MODELS", "authors": ["Fei Ye", "Zaixiang Zheng", "Dongyu Xue", "Yuning Shen", "Lihao Wang", "Yiming Ma", "Yan Wang", "Xinyou Wang", "Xiangxin Zhou", "Quanquan Gu"], "abstract": "Recent years have witnessed a surge in the development of protein foundation models, significantly improving performance in protein prediction and generative tasks ranging from 3D structure prediction and protein design to conformational dynamics. However, the capabilities and limitations associated with these models remain poorly understood due to the absence of a unified evaluation framework. To fill this gap, we introduce ProteinBench, a holistic evaluation framework designed to enhance the transparency of protein foundation models. Our approach consists of three key components: (i) A taxonomic classification of tasks that broadly encompass the main challenges in the protein domain, based on the relationships between different protein modalities; (ii) A multi-metric evaluation approach that assesses performance across four key dimensions: quality, novelty, diversity, and robustness; and (iii) In-depth analyses from various user objectives, providing a holistic view of model performance. Our comprehensive evaluation of protein foundation models reveals several key findings that shed light on their current capabilities and limitations. To promote transparency and facilitate further research, we release the evaluation dataset, code, and a public leaderboard publicly for further analysis and a general modular toolkit. We intend for ProteinBench to be a living benchmark for establishing a standardized, in-depth evaluation framework for protein foundation models, driving their development and application while fostering collaboration within the field.", "sections": [{"title": "1 INTRODUCTION", "content": "Proteins are fundamental molecules playing pivotal roles in a vast array of biological processes, from enzymatic catalysis and signal transduction to structural support and immune response. Their functions are determined by their amino acid sequences, often mediated through folding into specific three-dimensional structures. Understanding the complex interplay between protein sequence, structure, and function is crucial for advancing science and engineering spanning pharmaceuticals, agriculture, specialty chemicals, and biofuels.\nIn recent years, there has been a surge in the development of protein foundation models\u00b9 aimed at understanding fundamental biological processes by capturing the intricate mechanisms of proteins.\nThese models, leveraging advanced deep-learning and generative AI techniques, have demonstrated remarkable capabilities and marks a significant shift from traditional, task-specific approaches to more generalizable frameworks capable of learning complex patterns and relationships within vast protein datasets. For instance, AlphaFold3, which is based on diffusion models, has achieved unprecedented accuracy in full"}, {"title": "2 BACKGROUND AND TASK DEFINITION", "content": "In this section, we provide a concise overview of the tasks addressed by various protein foundation models as shown in Table 1, with a particular focus on two key generative tasks: protein design and conformational dynamics. These two areas are further divided into eight subtasks. For each task, we focus on the following aspects, with detailed information provided in the appendix:\n[Task Definition] A clear and concise description of the task, including its objectives and relevance to protein science. Specification of the input data format and expected output for each task.\n[Evaluation Metrics] Description of the metrics used to assess model performance, including quality, novelty, diversity, and robustness measures.\n[Datasets] Overview of the datasets used for each task, including their size, diversity, and any pre-processing steps applied."}, {"title": "2.1 PROTEIN DESIGN", "content": null}, {"title": "2.1.1 INVERSE FOLDING", "content": "[Task Definition] The objective is to predict an optimal amino acid sequence for a given target protein structure, considering factors such as stability, refoldability, and potential functionality.\n[Evaluation Metrics] Performance in protein sequence design is assessed using multiple complementary metrics: (1) Sequence Recovery: This metric compares the designed sequences to natural sequences with similar structures. It quantifies how well the design method can recapitulate evolutionarily conserved sequence patterns associated with specific structural motifs. (2) Refoldability: This measure evaluates the structural similarity between the target backbone and the predicted structure of the designed sequence. The prediction is performed using AlphaFold2. Similarity is quantified using self-consistent template modeling score (scTM) and self-consistent root-mean-square deviation (scRMSD), providing insight into how well the designed sequence would fold into the intended structure. (3) Stability: This is assessed using the predicted local distance difference test (pLDDT) calculated by AlphaFold2. The pLDDT score serves as a proxy for the predicted stability of the designed protein, which is used in.\n[Datasets] Evaluations were conducted on different datasets targeting two distinct objectives of structure-based sequence design: (1) capture the native evolutionary distribution: we evaluated two independent datasets containing newly released PDB structures: CASP15 and CAMEO. We collected new structures from the ongoing CAMEO assessment between January and July 2024, resulting in a total of 332 complex structures. Additionally, 32 protein structures were collected from CASP15, which includes only protein entities, excluding nucleic acids or ligands. (2) de novo protein design: RFdiffusion was used to generate backbones of varying lengths: specifically, 100, 200, 300, 400, and 500 residues. For each length, 10 different structures were randomly sampled, using a sampling temperature of 0.1 for all methods. The designability of these sequences was evaluated using AlphaFold2, with the scTM score and pLDDT metrics serving as the primary assessment criteria. Existing benchmarks for inverse folding, such as PDB-Struct and Proteininvbench, provide standardized protein structure sets for evaluating inverse folding methods. While these benchmarks have significantly contributed to the field's advancement, there is a growing need for more comprehensive evaluation frameworks. These expanded evaluations should align more closely with diverse user objectives in protein design, encompassing aspects like accuracy in capturing natural evolutionary distributions and robustness in de novo backbone-based sequence design."}, {"title": "2.1.2 PROTEIN BACKBONE DESIGN", "content": "[Task Definition] Protein backbone design focuses on creating new protein folds to achieve de novo design objectives. This task is essential for expanding the repertoire of protein structures beyond those found in nature, with significant applications in fields such as drug discovery, biomaterials, and therapeutics.\n[Evaluation Metrics] The evaluation of backbone design encompasses multiple criteria to assess both the quality and novelty of generated structures. Structural quality is primarily measured using self-consistent TM-score and RMSD, which provide quantitative measures of the backbone's refoldability measured by ProteinMPNN and ESMFold. Equally important are novelty metrics, which gauge the method's capacity to explore new structural space beyond known protein folds. This aspect is evaluated using two key metrics: The maximum TM-score obtained when comparing designed structures to existing entries in the RCSB Protein Data Bank. This comparison is performed using Foldseek , a rapid structural alignment tool. Diversity metrics, which include: (a) Pairwise maximum TM-scores among the designed structures. (b) The number of distinct structural clusters identified within the set of designed backbones, also determined using Foldseek."}, {"title": "2.1.3 PROTEIN SEQUENCE DESIGN", "content": "[Task Definition] The aim of this task is to generate amino acid sequences of desired properties, such as quality, diversity and novelty. Besides sequence-based evaluation, the structural characteristics of the generated sequences are also important.\n[Evaluation Metrics] For sequence naturalness, we use perplexity from an autoregressive protein language model (ProGen2) to quantify if the patterns of generated sequences lie in natural sequence distribution. For structure-based evaluation, we use single-sequence folding model, i.e., ESM-Fold, to predict the structure of the generated sequences, and then measure the structural quality by pLDDT as the proxy of structural stability of the sequence using their predicted structures from AlphaFold2, as well as structural diversity and novelty using the same protocol as in backbone design.\n[Datasets] UniRef50 is the commonly used dataset for training protein sequence generative models and language models."}, {"title": "2.1.4 STRUCTURE AND SEQUENCE CO-DESIGN", "content": "[Task Definition] Protein structure-sequence co-design involves simultaneously optimizing both the backbone structure and amino acid sequence of a protein to achieve desired properties or functions. This task is more complex than sequence design or structure design alone, as it explores a larger solution space.\n[Evaluation Metrics] Evaluation metrics are derived from those used for both sequence and structure design: structure quality assessments, sequence-structure compatibility, as well as novelty of both sequence and structure compared to known proteins is also crucial.\n[Datasets] High-resolution protein structures from the Protein Data Bank (PDB) is the commonly used datasets for this task, with careful consideration given to remove redundancy."}, {"title": "2.1.5 \u041cOTIF SCAFFOLDING", "content": "[Task Definition] Motif scaffolding involves designing a protein structure that incorporates a specific functional motif or binding site. The goal is to create a stable protein framework (scaffold) that presents the desired motif in the correct geometry for its function.\n[Evaluation Metrics] Following Yim et al. , key metrics include the structural accuracy of the motif within the designed scaffold (typically measured by RMSD), overall protein stability, and retention of the motif's functional properties. Experimental validation through binding assays or enzymatic activity tests is often crucial.\n[Datasets] Datasets typically include libraries of known functional motifs (e.g., catalytic sites, binding interfaces) and diverse scaffold structures that can potentially accommodate these motifs. The Protein Data Bank is a primary source, but curated datasets of functional sites like the Catalytic Site Atlas are also valuable.\n[Related benchmarks] Enzyme Design Challenge provides relevant test cases. However, given the specificity of motif scaffolding tasks, benchmarks often need to be tailored to the particular class of motifs or functions being targeted. Currently, there exists no comprehensive benchmark for this task in the field. A widely used benchmark containing 17 (25) motif-scaffolding problems was used in RFDiffusion ."}, {"title": "2.1.6 ANTIBODY DESIGN", "content": "[Task Definition] The goal of antibody design is to generate antibodies that can specifically bind to a given antigen. Since the Complementarity-Determining Regions (CDRs) of antibodies are highly"}, {"title": "2.2 PROTEIN CONFORMATION PREDICTION", "content": null}, {"title": "2.2.1 PROTEIN FOLDING: SINGLE-STATE PREDICTION", "content": "[Task Definition] Protein folding is the task of predicting the folded structure of a protein from its sequence. Folding models, such as AlphaFold2, have played a pivotal role in the recent development of models for protein conformation prediction. Therefore, we recognize the necessity of including protein folding in this benchmark, viewing it as a specific instance of protein conformation prediction for a single conformational state.\n[Evaluation Metrics] The accuracy of a predicted structure is evaluated by compared with its reference structures deposited in PDB using RMSD, TM-score, global distance test (GDT), and local distance difference test (IDDT). We also evaluate the quality of predicted structures by measuring the rate of clashing alpha carbons (CA-clash), disconnecting neighbor alpha carbons (CA-break), and disconnecting peptide bonds (PepBond-break) in predicted structures. See Appendix B.2.2 for details.\n[Datasets] Most of the folding models compared in this benchmark were established prior to 2022. We use CAMEO2022 from Jing et al. (2023) for evaluation, which consists of 183 short-to-mid-length single protein chains (< 750 amino acids) from the targets of CAMEO between Aug 1 and Oct 31, 2022."}, {"title": "2.2.2 MULTIPLE-STATE PREDICTION", "content": "[Task Definition] As an extension of the single-state prediction task, multiple-state prediction aims to accurately predict (by sampling) two or more distinct conformational states of a protein that have been observed under different conditions (e.g., ligand binding) or through molecular dynamics simulations. The ability to predict these \u201calternative\" conformations in addition to the folded structure could provide insights into conformational changes and protein functions.\n[Evaluation Metrics] We evaluate this task based on accuracy, diversity, and quality. The accuracy of predicting a state is determined by the best structural similarity of the samples to the reference structure, measured by TM-score or RMSD. The overall accuracy of multiple-state prediction is assessed by \"ensemble accuracy\", which is the average accuracy across all reference states (TMens or RMSDens where \u201cens\u201d stands for ensemble), similar to Jing et al. . For sample structural diversity, we measure the pairwise TM-score (or RMSD) among the samples. Finally, we assess the structural quality of generated samples, similar to single-state prediction, using CA-clash, CA-break, and PepBond-break.\n[Datasets] We benchmark the models on two public datasets from previous works: 1) apo-holo, which contains 91 proteins, each with a pair of experimental structures (apo or unbound, and holo or bound) related to ligand-binding-induced conformational changes; (2) \u0392\u03a1\u03a4I (Bovine Pancreatic Trypsin Inhibitor), a 58 amino acids protein, where a previous long-time MD simulation revealed five clusters of distinct conformations ."}, {"title": "2.2.3 DISTRIBUTION PREDICTION", "content": "[Task Definition] In contrast to multiple-state prediction, where the main goal is to recover specific conformational states, distribution prediction focuses on generating a sample distribution that resembles a target distribution\u2014such as the empirical distribution sampled from molecular dynamics (MD). This task further bridges the gap between protein conformation prediction models and current"}, {"title": "3 PROTEINBENCH", "content": "In this section, we provide ProteinBench, a holistic evaluation framework for protein foundation models. By systematically evaluating protein foundation models on the following tasks, we aim to provide a comprehensive understanding of their capabilities and limitations. This approach allows for a nuanced comparison of different model architectures and strategies, highlighting areas of strength and identifying opportunities for improvement. All data used in this benchmark are publicly available, ensuring reproducibility and facilitating wider participation in the research community."}, {"title": "3.1 PROTEIN DESIGN", "content": "In this section, we present a comprehensive evaluation of various protein foundation models across fundamental protein design tasks, including single-modal approaches (structure-based sequence design, structure design, and sequence design), multi-modal structure-sequence co-design, and the application-specific task of antibody design. This holistic assessment allows us to examine the versatility and effectiveness of different modeling approaches across a spectrum of protein engineering challenges. Notably, for backbone design, sequence design, co-design, and motif scaffolding, the quality, novelty, and diversity metrics are calculated using the same method. By utilizing common evaluation metrics across tasks, we enable cross-task comparisons, hoping to provide performance analysis to identify the strengths and limitations of each modeling approaches, and help to uncover potential synergies between different protein modals for future research."}, {"title": "3.1.1 INVERSE FOLDING", "content": "In this section, we evaluate the performance of various inverse-folding models for structure-based sequence design, focusing on two distinct objectives: natural evolutionary fitness (in-distribution proteins) and de novo designed backbone-based sequence design. The latter represents an out-of-distribution problem that tests the robustness of the methods, as these structures typically contain some noise different from high-resolution structure deposited in PDBs.\nOur analysis of native distribution fitness reveals that language model-based methods, for example LM-DESIGN in our investigation, effectively capture the natural evolutionary distribution, achieving high sequence recovery rates for native protein structure-based sequence design. This suggests that these models effectively learn and replicate the intricate patterns of amino acid selection that have emerged through evolutionary processes. However, its performance decreases when applied to de novo backbone-based sequence design. Conversely, ProteinMPNN , a method specifically developed for de novo design and trained using coordinates perturbed with 0.2\u00c5 added noise, consistently demonstrates superior performance in de novo design tasks. However, ProteinMPNN's performance shows a decline when evaluated on objective to fiting to native evolution. This finding has significant implications for the field, suggesting that no single model currently excels across all protein design objectives. The choice of model should be carefully aligned with the intended applications."}, {"title": "3.1.2 STRUCTURE DESIGN", "content": "In this section, we evaluate the performance of protein foundation models for backbone design. The results are presented in Table 3. Our analysis focuses on the quality, novelty, and diversity of the generated structures across various chain lengths. Based on the quality metrics of scTM-score and scRMSD, RFdiffusion demonstrates exceptional performance in backbone design for chain lengths ranging from 50 to 300 amino acids. FrameFlow achieves the second-best performance in this range. However, we observe a significant performance decrease across all models for longer chains (500 amino acids), with scTM scores dropping by more than 20%. This decline suggests that developing methods for long-chain backbone design remains an important challenge for future research. Novelty is an equally important metric, as it gauges a method's capacity to explore new structural space beyond known protein folds. Under moderate quality constraints (scTM score >0.5), FoldFlow and Genie exhibit good performance in generating novel structures. When we increase the quality threshold (scTM score >0.8), Chroma generally shows the best performance across chain lengths from 50 to 500 amino acids. In terms of structural diversity, Chroma shows commendable performance across the tested chain lengths. It is important to note that for this evaluation, we used the released FoldFlow model trained on a smaller training set with shorter sequences. This limitation may lead to an unfair comparison of the model architecture to other methods trained on the entire PDB database, particularly for longer chain lengths. We will soon update our evaluations to include more methods, such as Foldingdiff and Proteous ."}, {"title": "3.1.3 SEQUENCE DESIGN", "content": "In this section, we assess the performance of various protein sequence generative models based on the quality, diversity, and novelty of their generated sequences across different chain lengths. The evaluation metrics include AlphaFold2 predicted PLDDT scores for structural plausibility (quality), maximum TM-score and maximum cluster values for structural diversity, and maximum TM-score to PDB structures for structural novelty. We choose representative methods of distinct modeling foundations for evaluation. Among the methods evaluated, ProGen2 is an autoregressive protein language model (AR-LM), while EvoDiff is designed as an order-agnostic autoregressive diffusion model (OADM). DPLM and ESM3 share a probabilistic foundation as absorbing discrete diffusion models or generative masked language models. Notably, ESM3 is a multimodal model that advances beyond other sequence-only methods by jointly learning protein sequences, structures, and functions through tokenization. For each model and sequence length, we sample 50 sequences to evaluate their performance."}, {"title": "3.1.4 STRUCTURE AND SEQUENCE CO-DESIGN", "content": "In this section, we examine the performance of protein structure-sequence co-generation, a topic that has recently gained significant interest within the research community. We inspect the performance of ProteinGenerator, ProtPardelle, Multiflow and ESM3 for different lengths. The performance is assessed using metrics similar to those applied in backbone generation. Note that, however, the quality here is about structure-sequence compatibility measuring how well the designed sequence can fold into the corresponding designed structure, using scTM and scRMSD. The key difference is that co-design models are tasked with simultaneously generating both the sequence and structure, while backbone design models require an additional inverse folding model, such as ProteinMPNN, to design the sequence. Other metrics used for evaluation include diversity (max cluster) and novelty (max TM-score to PDB).\nAs shown in Table 5, ProteinGenerator and Multiflow consistently show strong performance of structure-sequence compatibility across all sequence lengths, with high scTM scores (up to 0.96\u00b10.06) and relatively low scRMSD values, indicating superior structural quality in generated sequences. ProteinGenerator particularly excels at shorter lengths, showing a balanced performance between quality and diversity metrics. Multiflow maintains high performance even as sequence length increases, demonstrating its robustness with consistently high scTM scores and lower scRMSD values, which indicates its capability to generate high-quality structures. ProtPardelle and ESM3, on the other hand, shows degradation in performance with increasing sequence length, as indicated by its low scTM scores and very high scRMSD values, suggesting that it struggles with maintaining structure quality for longer sequences. Overall, these findings suggest that while ProteinGenerator and Multiflow are effective models for generating high-quality protein structures across different lengths, Multiflow is particularly robust across all tested lengths."}, {"title": "3.1.5 \u041c\u043eTIF-SCAFFOLDING", "content": "In this section, we evaluate the performance of various motif-scaffolding methods across different scaffolds used in  and Yim et al. , focusing on their effectiveness in designing scaffold structures. The primary objective of this evaluation is to compare the efficacy of structure-based and sequence-based approaches in generating designable scaffolds. For purely sequence-based methods, e.g., EvoDiff and DPLM , we use ESMFold to predict the structures of their designed motif-scaffold sequences."}, {"title": "3.1.6 ANTIBODY DESIGN", "content": "In this section, we selected five antigen-specific antibody design methods (HERN , MEAN , dyMEAN , DiffAb , AbDPO ) and two of their variants (dyMEAN-FixFR implemented according to Appendix B.1.2 and AbDPO++), making a total of seven methods, to evaluate their performance in CDR-H3 generation towards the given antigens. All methods were trained on the same dataset with parameters reported in the corresponding papers and tested on a common set of 55 test cases from the RAbD dataset , details refer to Appendix B.1.3. Notably, dyMEAN-FixFR is not an official variant of dyMEAN; we modified dyMEAN to align its task setting with the other methods and allow it to generate different antibodies for the same antigen. The final evaluation results are shown in Table 6. For each evaluation metric, we highlighted the best performance in bold and the second-best with the underline, the detailed implementation of each metric could be seen at Appendix B.1.4.\nIn the Accuracy evaluation, dyMEAN and MEAN achieved the best performance in terms of sequence and structure (highest AAR and lowest RMSD), while DiffAb performed best in TM-score. However, considering multiple evaluation metrics, these methods did not perform as well overall."}, {"title": "3.2 PROTEIN CONFORMATION PREDICTION", "content": "In the second part of ProteinBench, we focus on conformation prediction, another class of cross-modality tasks aimed at predicting protein structures (conformations) from their sequences. While the current models are based on a body of work distinct from the design tasks, the ability to predict protein conformations provides insight into a model's understanding of the physics and dynamics of protein structures. This capability is essential for future protein foundation models to fully understand, predict, and design proteins that embody the key sequence-structure-function relationships\nThe development of conformation prediction models is still in its early stages, with only a handful of exploratory methods proposed. A comprehensive comparison between these methods has yet to be conducted. To the best of our knowledge, this is the first benchmark study on current conformation prediction models that includes the major strategies proposed to date: (1) perturbing the sequence input of folding models ; (2) perturbing protein structures through a structural-only diffusion model ; (3) training generative models on large-scale structural data from experiments or simulations ; (4) improving the conformational sampling using physical models ."}, {"title": "3.2.1 PROTEIN FOLDING: SINGLE-STATE PREDICTION", "content": "While most folding models, such as AlphaFold2 and ESMFold , are not generative in nature, we still consider them \"protein foundation models\" for conformation prediction because (1) they are trained on large amounts of structural and/or sequence data; (2) they have played a fundamental role in understanding sequence-structure relationships; and (3) they are closely related to foundation models like ESM2 and AlphaFold3 , and serve as pivotal building blocks for conformation prediction models"}, {"title": "3.2.2 MULTIPLE-STATE PREDICTION", "content": "In this section, we evaluate the performance of predicting multiples conformational states for five conformation prediction models and their variants: EigenFold , MSA-subsampling , Str2Str , AlphaFlow/ESMFlow and ConfDiff . Here we highlight the key differences of these methods. MSA-subsampling perturbs OpenFold's model input by reducing the number of input MSAs (referred to as \"depth\"), allowing the sampling of different conformations for a protein. Str2Str uses a structure-only diffusion model (i.e., a backbone design model) to generate conformations by perturbing initial folding predictions through a forward-backward diffusion process. The level of perturbation is controlled by the maximum diffusion time, Tmax, and ensemble outputs are generated by sampling structures at various diffusion times t < Tmax. EigenFold, AlphaFlow/ESMFlow, and ConfDiff take a similar approach by training diffusion or flow models through fine-tuning folding models using structural data from PDB. While AlphaFlow/ESMFlow open all layers of the original model for fine-tuning, EigenFold and ConfDiff only use pretrained representations from folding models and train a lightweight module for score or structural prediction. Additionally, both AlphaFlow/ESMFlow and ConfDiff provide versions further fine-tuned on a recent MD conformation dataset , indicated by the \u201c-MD\u201d suffix. ConfDiff introduced two guidance techniques to improve conformational sampling: (1) classifier-free guidance, which combines a sequence-conditioned conformation model with an unconditional (structure-only) model to explore conformational space (ConfDiff-ClsFree), and (2) energy/force guidance, which directs sampling toward regions with lower potential energy (ConfDiff-Energy/Force) through auxiliary prediction modules for intermediate energy/force guidance. However, such physical prediction modules are dataset-specific and are only available for the BPTI dataset."}, {"title": "3.2.3 DISTRIBUTION PREDICTION", "content": "In this final task, we benchmark models on the ATLAS test set, which includes 82 proteins, and focus on each model's ability to recover the conformational distribution observed in classic protein molecular dynamics simulations. The results are summarized in Table 10. For comparison, we include reference performances of (1) i.i.d. samples (MD iid) from MD-generated structures and (2) 250 consecutive samples, corresponding to 2.5 ns of simulation time (MD 2.5 ns).\nOverall, generative models trained to sample protein conformations from sequence (AlphaFlow/ESMFlow, ConfDiff) perform considerably better than perturbation-based methods (MSA subsampling and Str2Str) across nearly all accuracy metrics of flexibility prediction, distributional accuracy, and ensemble observables. In our experiments, adjusting perturbation levels for Str2Str (the maximum forward time Tmax) and MSA-subsampling (the MSA depth) did not improve distribution-related metrics, suggesting that perturbation alone may not be sufficient to accurately capture the sample distribution. We identified two factors consistently improve the model performance of AlphaFlow/ESMFlow and ConfDiff: (1) choosing a strong base folding model (e.g., AlphaFold or OpenFold), which, while potentially reducing sample diversity, improves distribution-related metrics; and (2) fine-tuning process on MD conformation data, which further enhances the models' ability to predict the target distribution. The latter again emphasizes the importance of aligning the model's distribution to the target distribution through supervised methods, rather than relying solely on conformation exploration strategies (e.g., classifier-free guidance), to accurately predict distributions. In addition, the results align with previous tasks regarding the trade-offs between diversity, prediction performance, and sample quality: for example, while fine-tuning on MD conformation data improves sample diversity and prediction performance for AlphaFlow/ESMFlow, it also significantly increases the level of peptide bond breaking in these models.\nWhile current conformation prediction models have shown promising signs in capturing dynamics-related features and approximating conformation distributions, it is important to note that there remains a clear gap between the performance of these models and that of classic MD simulations, even at short simulation times (e.g., 2.5 ns). Achieving performance comparable to i.i.d. sampling from MD conformational samples has yet to be achieved."}, {"title": "4 CONCLUSIONS AND FUTURE WORK", "content": "In summary, we present the first comprehensive study evaluating the capabilities of various protein foundation models across eight distinct tasks, with a particular focus on protein design and conformation dynamics. We have developed a unified, multi-metric evaluation framework, which is essential for unbiased assessment of protein foundation models from multiple facets. Based on the performance results, we provide insights and considerations for the development and effective use of protein foundation models, offering guidance for future research. We highlight the key observations from our holistic evaluation as follows."}, {"title": "4.1 KEY OBSERVATIONS", "content": "Valid evaluation of protein foundation models necessitates the use of correct and comprehensive evaluation metrics. The emergence of advanced folding models, exemplified by AlphaFold2 and ESMFold, has opened up valuable opportunities for accurately assessing the quality, stability, and precision in protein generative tasks. However, it is crucial to acknowledge that, due to their current limitations in complex structure prediction capabilities, certain tasks may still lack sufficiently"}, {"title": "4.2 LIMITATIONS AND FUTURE WORK", "content": "We acknowledge several limitations and opportunities for enhancement in our current benchmark: (1) The selection of foundation models may not be exhaustive. Future iterations should incorporate additional foundation models to provide a more comprehensive comparison. (2) Inconsistencies in training data across models currently hinder direct comparisons of different model architectures. Future work could address this by standardizing datasets, allowing for more accurate comparisons of architectural performance. (3) The benchmark could be expanded to include a wider range of tasks, further broadening its scope and utility. We are committed to continually refining and expanding ProteinBench. Our vision is for it to evolve into a dynamic, growing benchmark that accelerates progress in the field of protein modeling and design."}, {"title": "A OVERVIEW OF PROTEIN FOUNDATION MODEL BENCHMARKS", "content": "In this section, we provide a comprehensive overview of existing benchmarks for protein foundation models. Table 11 illustrates the current landscape of these benchmarks, revealing significant limitations in the scope and applicability. The majority of existing benchmarks are narrowly focused, primarily addressing task-specific evaluations rather than offering a holistic assessment of protein foundation models.\nThe benchmarks we examined can be broadly categorized into two main groups: those focused on protein design tasks and those evaluating protein conformational dynamics. Within the protein design category, we observe that while inverse folding is well-represented across multiple benchmarks, other crucial aspects such as backbone design, sequence design, and structure-sequence co-design"}, {"title": "B.1.1 EVALUATION CONCEPT", "content": "As mentioned in the main text, antibody design can ultimately be simplified to the design of CDR-H3. Therefore, in this study, we evaluate the performance of different antibody design methods by evaluating the CDR-H3 sequences generated by these methods. Given the primary objective of this study is to assess the relative performance of various design models rather than the in vivo/vitro functionality of the antibodies they generate, we opted to directly evaluate the designed antibodies using their predicted structures. This approach is grounded in several considerations: firstly, it ensures a clear focus on evaluating the design methodology itself, independent of experimental constraints. Secondly, the significant time and resources required for extensive experimental validations, as well as the limitations of methods that can accurately simulate the real binding structure of antibodies, render in vivo/vitro assessments impractical. Direct evaluation of the designed structures presents a feasible and efficient strategy that aligns with the study's goals and resource constraints while still providing valuable theoretical benchmarks for subsequent experimental investigations.\nFor methods capable of generating multiple antibodies for the same antigen, we generated 64 CDR-H3 sequences per antigen using each method and calculated the average performance across these different generated samples. Additionally, we also calculated the standard deviation of the performance among different samples generated for a single antigen."}, {"title": "B.1.2 VARIANT OF DYMEAN", "content": "Unlike other methods, which are designed to accept the true structure of the antibody-antigen complex and generate the missing CDR-H3 region, dyMEAN is set up to accept only the structure of the antigen and the sequence of the non-CDR-H3 regions of the antibody. Therefore, the model needs to both generate the CDR-H3 region and predict the overall structure of the antibody as well as the binding pose between the antibody and antigen. Incorrect pose estimation can severely affect the interactions between CDR-H3 and the antigen, making a direct comparison between dyMEAN and other methods unfair. To compare dyMEAN with other methods more fairly, we made some modifications to dyMEAN by providing the true structure of the non-CDR-H3 regions of the antibody and the binding pose, aligning dyMEAN with the other methods. In dyMEAN-FixFR, we also used Rosetta to repack the side chains, consistent with other methods, to avoid the influence of the side chains generated by dyMEAN on the evaluation results. Additionally, we introduced some randomness in the initialization of the structure, which allows dyMEAN-FixFR to generate multiple different antibodies for the same antigen."}, {"title": "B.1.3 DATASET", "content": "To retrain all the methods for a fair comparison, we use antibody-antigen complex structural data from the SAbDab dataset under the IMGT scheme (Lefranc et al., 2009) as the training dataset. We collected antigen-antibody complexes with both heavy and light chains and protein antigens. We then discarded duplicate data with the same CDR-L3 and CDR-H3 sequence. The remaining complexes are used to cluster via MMseqs2 (Steinegger & S\u00f6ding, 2017) with 40% sequence similarity as the threshold based on the CDR-H3 sequence of each complex. Finally, we select the clusters that do not contain complexes in the RAbD dataset and split the complexes into training and validation sets with a ratio of 9:1 (1786 and 193 complexes respectively).\nThe test set contains 55 antibody-antigen complexes extracted from the RAbD dataset. The original RAbD dataset contains 60 antibody-antigen complexes. In this study, we hope that the evaluation of antibody design methods is based on antibodies that contain both light and heavy chains, and simultaneously the antigen contains at least one protein chain. In practice, 2ghw and 3uzq lack light chains, while 3h3b lack heavy chains. 5d96 is excluded because of the incorrect chain ID information in rabd_summary.jsonl, where heavy chain J and light chain I do not bind to antigen chain A. 4etq is excluded as HERN reported an error when running for this complex."}, {"title": "B.1.4 IMPLEMENTATION OF EVALUATION METRICS", "content": "[Accuracy", "AAR": "For the calculation of AAR (Amino Acid Recovery Rate)", "RMSD": "In the calculation of RMSD (Root Mean Square Deviation)", "TM-score": "We calculated the TM-score only for the CDR-H3 region. To this end", "CDR-H3.\n[Functionality": "n\u2022 Binding Energy: The calculation of binding energy requires the all-atom structure of the protein, while most methods only generate the backbone atom structure. Therefore, we first used Rosetta to pack the missing side-chain atoms. Subsequently, we optimized the side-chains in the CDR-H3 region using Rosetta minimization while keeping the backbone structure unchanged to ensure that the CDR-H3 generated by the model reaches the minimum energy state in the binding environment with the antigen. During"}]}