{"title": "WEAK-TO-STRONG GENERALIZATION THROUGH THE DATA-CENTRIC LENS", "authors": ["Changho Shin", "John Cooper", "Frederic Sala"], "abstract": "The weak-to-strong generalization phenomenon is the driver for important machine learning applications including highly data-efficient learning and, most recently, performing superalignment. While decades of research have resulted in numerous algorithms that produce strong empirical performance, understanding what aspects of data enable weak-to-strong generalization has been understudied. We propose a simple data-centric mechanism that characterizes weak-to-strong generalization: the overlap density. Intuitively, generalization tracks the number of points that contain overlaps, i.e., both easy patterns (learnable by a weak model) and challenging patterns (only learnable by a stronger model), as with such points, weak predictions can be used to learn challenging patterns by stronger models. We provide a practical overlap detection algorithm to find such points in datasets and leverage them to learn, among multiple sources of data, which to query when seeking to maximize overlap density and thereby enhance weak-to-strong generalization. We present a theoretical result showing that the generalization benefit is a function of the overlap density and a regret bound for our data selection algorithm. Empirically, we validate the mechanism and the overlap detection algorithm on a wide array of settings.", "sections": [{"title": "1 INTRODUCTION", "content": "A recurring theme in machine learning is the idea of a less-capable entity (e.g., a weak model or an individual with limited expertise) supervising a stronger, more capable one (a more powerful or higher-capacity system). The goal is to enable the stronger entity to generalize beyond the capabilities of its weaker counterpart\u2014despite relying on its supervision. This idea undergirds classical approaches (e.g., self-training, co-training) for data-efficient learning that date back fifty years. Most recently, it drives embryonic attempts to perform superalignment\u2014the process of ensuring systems with capabilities far beyond those of humans align with human values (Burns et al., 2023).\n\nThe typical flavor of works studying weak-to-strong generalization is to introduce techniques that, given a fixed dataset, can obtain the best performance (i.e., provide a strong model that best generalizes to an unseen test set). A vast literature has studied thousands of techniques, including entire areas, such as semi-supervised learning (Zhu & Goldberg, 2022; Ouali et al., 2020), co-training (Blum & Mitchell, 1998; Ling et al., 2009), pseudolabeling (Lee, 2013; Arazo et al., 2020), self-training (Scudder, 1965; Amini et al., 2023) student-teacher methods (Matiisen et al., 2020), and more. Much less attention, however, has been focused on what aspects of the data enable such techniques to succeed and how to acquire additional data that further promotes weak-to-strong generalization.\n\nThis work focuses on this missing element. We begin by proposing a simple mechanism that captures the potential for weak-to-strong generalization. This measure, the overlap density, considers the potential presence of two kinds of patterns (i.e., sets of features or mechanisms for prediction) within each datapoint: an easy pattern\u2014usable by weak and strong models and a hard pattern, only accessible to the strong model. Intuitively, weak models can label points that have both patterns-the overlapping points-by taking advantage of the weak pattern (but cannot accurately label using the hard patterns). However, the strong model, using weak predictions obtained on overlapping points, can learn the harder patterns, and therefore generalize to points only containing these hard patterns."}, {"title": "2 RELATED WORK", "content": "We give a brief description of related areas. Our work is complementary to many of these, as our focus is on understanding what forms of data promote weak-to-strong generalization\u2014and how to obtain more of it rather than new frameworks or training approaches. Extend related work is provided in Appendix B.\n\nSelf-Training and Data-Efficient Learning. Strategies that attempt to train high-quality models with less labeled data date back to the infancy of machine learning (Scudder, 1965). This idea has spawned entire fields, including semi-supervised learning (Zhu & Goldberg, 2022), weak supervision (Ratner et al., 2016; Shin et al., 2022), self-training (Amini et al., 2023; Wei et al., 2022), and more. The key distinction between such works and ours is that we are not concerned with improving performance on benchmark datasets via algorithmic improvements. Instead, we seek to understand what aspects of data result in stronger performance\u2014and how to obtain more of the data that drives it.\n\nWeak-to-Strong Generalization and Superalignment. A particularly interesting application of weak-to-strong generalization is that of superalignment. Superalignment, in the narrow sense, is the notion of aligning a superintelligent system to human values. More broadly, it can be thought of as aligning any large-scale system at a level of complexity beyond any individual person. As such systems may be far off into the future, researchers are currently studying proxies, such as smaller large language models supervising larger and more capable ones (Burns et al., 2023). Recently, several studies (Lang et al., 2024; Charikar et al., 2024; Somerstep et al., 2024) have proposed theoretical frameworks to understand weak-to-strong generalization. However, these works have yet to explore the specific data characteristics that facilitate weak-to-strong generalization. In contrast, our work provides a concrete characterization of the data that induces weak-to-strong generalization: overlap density. Building on the theoretical framework from Lang et al. (2024), we derive new theoretical results that illuminate how overlap density drives weak-to-strong generalization."}, {"title": "3 A SIMPLE DATA MECHANISM FOR WEAK-TO-STRONG GENERALIZATION", "content": "Our goal is two-fold. First, we seek to understand what properties of our data provide the possibility of weak-to-strong generalization. We introduce a simple mechanism (easy-hard overlap), formalize it, and provide a theoretical result showing that it indeed characterizes generalization."}, {"title": "3.1 THE OVERLAP DENSITY MECHANISM", "content": "We start with an extremely simple theoretical model; afterwards, we will comment on extensions that are likely to be encountered in real-life data. Nevertheless, perhaps surprisingly, the basic mechanism often tracks weak-to-strong generalization in real settings.\n\nSetup. We have access to a dataset $D_{train} = \\{(X_1,Y_1),..., (X_n, Y_n)\\}$. Here, $(x,y) \\sim D$, $x \\in X$, $y \\in Y$, and $D$ is some distribution. In addition, we have access to a dataset $D_{w2s} = \\{X_{n+1},...,X_{n+m}\\}$ where we do not have access to any ground-truth labels. We use two models, a weak model $f_{weak}$ and a strong model $f_{w2s}$. $f_{weak}$ is trained (or fine-tuned) on $D_{train}$ and used to output predictions $\\hat{y}_j = f_{weak}(x_j)$ for points $x_j \\in D_{w2s}$. $f_{w2s}$ is then trained or fine-tuned on $D_{w2s}$ with the predictions provided by $f_{weak}$. Our goal is to understand in what settings the strong model $f_{w2s}$ generalizes better than the weak model $f_{weak}$\u2014despite only being trained on supervision obtained from $f_{weak}$.\n\nAssumptions and Notation. For simplicity, we will assume that $x = [x_{easy}, x_{hard}]$, where $x_{easy} \\in \\mathbb{R}^{d_{easy}}$ and $x_{hard} \\in \\mathbb{R}^{d_{hard}}$ (in practice, this is not necessary). Here, $x_{easy}$ are features producing easy patterns, learnable by the weak model, while $x_{hard}$ are hard patterns, which cannot be used by the weak model to obtain accurate predictions. In practice, feature vectors are not a priori decomposed into such patterns. To address this, we introduce an algorithm to estimate this identification later.\n\nWe note that any dataset D can be partitioned into\n\n\u2022 $D_{overlap}$: points containing both patterns, i.e., overlapping points,\n\n\u2022 $D_{hard\\_only}$: points that only contain the hard pattern. For convenience, in our simplified model, we take $x_{easy} = 0$ for such points.\n\n\u2022 $D_{easy\\_only}$: points that only contain the easy pattern. We take $x_{hard} = 0$ for such points.\n\n\u2022 $D_{neither}$: points that contain neither pattern.\n\nThese four possibilities (we ignore $D_{neither}$ for simplicity) are illustrated in Fig. 1 (left). This simple categorization explains the weak-to-strong generalization phenomenon.\n\nAfter training, $f_{weak}$ has learned the easy pattern, and can therefore make reliable predictions on any points in $D_{overlap} \\cup D_{easy\\_only}$, as these points contain the easy patterns. However, it will not be able to"}, {"title": "3.2 DATA SELECTION FOR MAXIMUM OVERLAP DENSITY", "content": "A direct application of our proposed mechanism is data source selection. Specifically, we consider the following common scenario. We have access to multiple sources of data, which we call $D_1, . . . , D_K$. Given a limited budget to obtain unlabeled data points from these sources, which ones should we prioritize? Clearly, to maximize weak-to-strong generalization, we should target the sources $D_i$ with the largest overlap density. However, we face two challenges: (C1) we do not know a priori which sources have this property, and, (C2) even with access to data from these sources, overlaps are latent. That is, it is not clear how to distinguish between points in $D_{overlap}$ and points in $D_{easy\\_only}$\u2014as weak and strong models are both capable of accurate predictions on such points.\n\nWe propose Algorithm 1 to address these two challenges. For C1, we leverage stochastic bandit algorithms (Lattimore & Szepesv\u00e1ri, 2020), which balance exploration and exploitation. Here, data sources act as arms, and their average reward correspond to overlap density. Using the UCB (Upper Confidence Bound) algorithm (Auer, 2002), we explore underutilized data sources while exploiting"}, {"title": "4 THEORETICAL ANALYSIS", "content": "We introduce a theoretical result showing that weak-to-strong generalization is governed by the overlap density. Afterwards, we provide and interpret theoretical guarantees for our overlap detection and data source selection algorithms."}, {"title": "4.1 WEAK-TO-STRONG GENERALIZATION VIA OVERLAP EXPANSION", "content": "We build off of the framework in Lang et al. (2024), where generalization is governed by an expansion property. Specifically, we show that the weak-to-strong model can correct the weak model's pseudolabels on hard data points $D_{hard\\_only}$, since the pseudolabels produced by the weak model are (relatively) accurate on overlapping points and the strong model can learn hard patterns that address hard data points. We first introduce the relevant definitions and outline our setup.\n\nDefinition 1 (Expansion). (Lang et al., 2024) Fix sets $A, B \\subset X$ and a neighborhood function $N$. We say the distribution $P_X$ satisfies $(c, q)$-expansion on $(A, B)$ if for all sets $U \\subset B$ with $P(U|B) > q$, $P(N(U)|A) > cP(U|B)$.\n\nDefinition 2 (\u03b7-robust). (Lang et al., 2024) For a classifier $f$ and a point $x$, define $r(f,x) = P(f(x') \\neq f(x)|x' \\in N(x))$ as the probability of label disagreement between $x$ and its neighbor $x'$. A classifier $f$ is \u03b7-robust at $x$ if $r(f,x) \\leq \\eta$. The set of \u03b7-robust points for $f$ is $R_\\eta(f) = \\{x : r(f,x) \\leq \\eta\\}$."}, {"title": "4.2 THEORETICAL GUARANTEES FOR OVERLAP DETECTION AND DATA SELECTION", "content": "Equipped with the previous result, we provide a theoretical guarantee of our overlap detection algorithm under a Gaussian mixture assumption. We derive a regret bound of our UCB-based data selection algorithm for overlap density maximization.\n\nOverlap Detection. We provide a theoretical guarantee for the overlap score under the assumptions described in Section 3.1, i.e. $x = [x_{easy}, x_{hard}]$, where $x_{easy} \\in \\mathbb{R}^{d_{easy}}$ and $x_{hard} \\in \\mathbb{R}^{d_{hard}}$. Let $\\tilde{x} = g(x) = [x_{easy}, 0]$ represent the input vector for the weak model, where hard features from $x$ are zeroed out. More detailed setup specific to this section is described in Appendix D.3. For $x \\in D_{hard\\_only}$, $x = [0, 0]$, so the weak model prediction probability is $f_{weak}(x) = \\sigma(0^T\\tilde{x}) = \\sigma(0) = 0.5$, which corresponds to the minimum confidence score. This ensures the perfect accuracy of detecting hard-only points using Algorithm 2 with $T_{hard} = 0.5$. Next, we aim to separate overlap points from easy-only points. Under the Gaussian mixture assumption in D.3, we have $x_{easy\\_only} \\sim \\mathcal{N}(\\mu_{easy}, cI)$, and $x_{overlap} \\sim \\mathcal{N}(\\mu_{overlap}, cI)$, where $\\mu_{overlap} = [\\mu_{easy}^T, \\mu_{hard}^T]^\\top$, $\\mu_{easy} = [\\mu_{easy}^T, 0]^\\top$. To show the effectiveness of overlap separation from easy-only points in Algorithm 2, we demonstrate that $x_{overlap}^T x_{hard\\_only}$ and $x_{easy\\_only}^T x_{hard\\_only}$ exhibit a distributionally distinguishable gap."}, {"title": "5 EXPERIMENTS", "content": "We first validate the role of the data overlap mechanism in weak-to-strong generalization, examining two cases: large language models following the setup of Burns et al. (2023) and the weak supervision setting, where the weak model is a label model, often a probabilistic graphical model. Next, we evaluate the effectiveness of our UCB-based data selection strategy from Algorithm 1. Afterwards, we confirm our theoretical claims in a controlled setting, showing that the performance gains from overlap density primarily benefit hard-only data points, and that our data selection algorithm maximizes overlap density, improving weak-to-strong generalization."}, {"title": "5.1 WEAK-TO-STRONG GENERALIZATION VIA OVERLAP DENSITY MECHANISM", "content": "We follow the approach in Burns et al. (2023), where the goal is to use large language models as proxies for weak agents supervising superintelligent agents. Our hypothesis is that the overlap density mechanism elicits weak-to-strong generalization in this setting. We anticipate that a higher overlap density generally enhances the performance of weak-to-strong models. Additionally, we hypothesize the existence of three regimes of weak-to-strong generalization from datasets can be observed depending on the amount of overlap data points in the dataset and the noise level of overlap detection.\n\n\u2022 Low overlap regime: Insufficient overlap points or overly noisy detection hinder weak-to-strong generalization, leading to performance worse than $f_{weak}$.\n\n\u2022 Medium overlap regime: Adequate overlap points and moderate noise levels enable weak-to-strong generalization, resulting in performance comparable to, or slightly better than, $f_{weak}$.\n\n\u2022 High overlap regime: Sufficient overlap points with minimal noise in overlap detection induce strong weak-to-strong generalization, with performance approaching $f_{strong}$.\n\nSetup and Procedure. We split the original training data into two subsets, $D_{train}$ and $D_{w2s}$. The weak models are trained on $D_{train}$ and then generate weak labels for $D_{w2s}$. The weak-to-strong models are subsequently trained on $D_{w2s}$ using these weak labels. Using the overlap detection algorithm (Algorithm 2), we identified subsets $D_{overlap}$ and $D_{nonoverlap}$, and sampled $n_{controlled}$ data points to control overlap density between 0% and 100%, creating the dataset $D_{controlled, \\alpha}$, where \u03b1 denotes the overlap ratio. The weak-to-strong (W2S) models were then trained on $D_{controlled, \\alpha}$. Crucially, if the total quantity of overlap points is small (i.e., because the overlap density is small), building a dataset whose ratio is high translates into fewer overall points for training. Details on the distribution of detected easy-only, hard-only, and overlap points can be found in Appendix E.\n\nFor the language model experiments, we followed the setup described in EleutherAI (2021), which replicates Burns et al. (2023). We used the Qwen1.5 0.5B model as the weak model and the Llama3 8B model as the strong model. We used linear probing based on the observation in Appendix D.2 of Burns et al. (2023) that linear probing results often align with those from full fine-tuning. We used 19 datasets from EleutherAI (2021). For the weak supervision setting, we used 9 datasets from the WRENCH weak supervision benchmark (Zhang et al., 2021). We used Snorkel (Ratner et al., 2018) as the label model (weak model), and a 4-layer MLP was used as the strong model.\n\nResults. Figure 2 presents the results of this experiment. As expected, we observe that the strong model performance improves as the overlap proportion increases, providing evidence for the overlap density mechanism's role in weak-to-strong generalization. Also, we were able to observe three regimes of weak-to-strong generalization by our overlap detection method. We showcased each case in LLM and weak supervision settings, respectively. Full experimental results are provided in Appendix F.1. Additionally, the ablation study on model architecture in the weak supervision setting is presented in Appendix F.5, and the transferability study in Appendix F.6."}, {"title": "5.2 DATA SOURCE SELECTION VIA OVERLAP DETECTION", "content": "Next, we validate our data selection procedure instantiated in Algorithm 1. We hypothesize that the UCB-based overlap maximization strategy leads to better weak-to-strong generalization by identifying the optimal data source given multiple sources with varying overlap densities."}, {"title": "5.3 SYNTHETIC EXPERIMENTS", "content": "We verify our overlap density mechanism and data selection algorithm in fully controllable settings."}, {"title": "5.3.1 OVERLAP DENSITY MECHANISM", "content": "We validate the claim that overlap density enhances the performance of a weak-to-strong model on hard data points, while the weak model exhibits random accuracy on those same points.\n\nSetup and Procedure. We simulate weak-to-strong generalization using a simple mixture of Gaussians setup with logistic regression models, as described in Section 3. In this setup, the hard features are intentionally blocked (set to 0) for the weak models to mimic the common scenario where weak models lack access to these features. Full details are provided in Appendix E. The weak model $f_{weak}$ is trained on a dataset $D_{train}$ and generates pseudolabels for $D_{w2s}$. The weak-to-strong model $f_{w2s}$ is then trained on $D_{w2s}$ using these pseudolabels and evaluated on $D_{test}$. We set $n_{easy} = 100$ and $n_{hard} = 100$, incrementing $n_{overlap}$ by 5 in each iteration. The performance of the weak-to-strong model is assessed on easy-only, hard-only, and overlap data points in the test set, respectively.\n\nResults. Figure 4 illustrates how accuracy varies with the overlap ratio across easy-only, hard-only, and overlap data points. As expected, the most substantial performance improvement over the weak pseudolabeler occurs on the hard-only data points. On the easy-only data, the weak-to-strong model initially underperforms compared to the weak model due to label noise. However, as the overlap density increases, the weak-to-strong model's performance approaches that of the weak model. On"}, {"title": "5.3.2 DATA SOURCE SELECTION", "content": "We validate the claim that the algorithm effectively identifies data sources with higher overlap density and progressively maximizes it with each round, leading to improved weak-to-strong generalization.\n\nSetup. We set $K = 5$ data sources, each characterized by different overlap densities: [0.1, 0.15, 0.2, 0.05, 0.8]. For the nonoverlap distribution, we assumed that the half of the nonoverlap distribution is easy-only, and the rest half is hard-only. We compare our data source selection algorithm against random sampling and an oracle setting, where data are always sampled from the data source with the optmal overlap density. In each round, 100 data points were sampled from the selected data source, with the total number of rounds set to $T = 50$.\n\nResults. Figure 5 presents the experimental results. As the rounds progress, our algorithm increas-ingly identifies the optimal data source, achieving better weak-to-strong generalization compared to random sampling. Additional experimental results on easy-only and hard-only training are provided in Appendix F.3, along with a sensitivity analysis for overlap detection noise in Appendix F.4."}, {"title": "6 CONCLUSION", "content": "We studied a data-centric mechanism that explains weak-to-strong generalization. This mechanism is centered on easy-hard overlaps: points where weak models leverage easy patterns for accurate labeling, while strong models learn hard patterns to generalize to hard-only points. We studied this idea conceptually, theoretically, and empirically; the latter in multiple popular settings. Finally, we introduced algorithms for identifying overlapping points and determining, given a limited data budget, which data sources should be queried to maximize weak-to-strong generalization.\n\nOur study was limited to a simple version of what is likely to be a more complex mechanism in many realistic settings. We are interested in extending this work in several directions. These include allowing more complex patterns (multiple levels of overlapping difficulties), further theoretical results, and studying additional variations for the overlap identification procedure."}, {"title": "APPENDIX", "content": "The appendix contains additional details, proofs, and experimental results. The glossary (Appendix A) contains a convenient reminder of our terminology. Appendix B provides more related works and discussion about the relationship between our work and related papers. In Appendix C, we describe the details of our algorithms and discuss their implementations. Appendix D provides the proofs of theorems that appeared in Section 4. Finally, we provide additional details and analysis of the experiments in Appendix E and present further experimental results in Appendix F."}, {"title": "A GLOSSARY", "content": "The glossary is given in Table A1."}, {"title": "B EXTENDED RELATED WORK", "content": "Theory of Weak-To-Strong Generalization. Several theoretical approaches have been proposed to explain weak-to-strong generalization, reflecting growing interest in this area of research. Somerstep et al. (2024) frame weak-to-strong generalization as a transfer learning problem, where latent knowledge from weak models is transferred to strong models. They propose label refinement to address the limitations of naive fine-tuning on pseudolabels, i.e. naive fine-tuning often leads to the"}, {"title": "C ALGORITHM DETAILS", "content": "We provide the detailed version of Algorithm 1 in Algorithm A1, and discuss details of Algorithm 2.\n\nThreshold selection. For the thresholds in Algorithm 2, we used a change point detection algorithm, specifically the binary segmentation method (Sen & Srivastava, 1975), applied to the sorted confidence"}, {"title": "D THEORY DETAILS", "content": "We provide a theoretical result building off Lang et al. (2024). The main idea is that the strong model can generalize better by learning from overlap data points, which leads to the expansion property and pseudolabel correction phenomenon in Lang et al. (2024). We begin by adopting the definitions from Lang et al. (2024)."}, {"title": "D.2 COVERAGE EXPANSION BY OVERLAP DENSITY", "content": "Theorem D.2. Suppose $\\mathcal{M}_{\\eta}(T_i \\cap D_{hard\\_only}, \\mathcal{F})$ satisfies $(c,q,\\eta)$-robust expansion on $(S_{good}^i, T_i \\cap D_{hard\\_only})$ for some $c > 0$. Fix an arbitrary classifier $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$. The error of $f$ on $T_i \\cap D_{hard\\_only}$ is bounded by:\n$$\\text{err}(f, y | T_i \\cap D_{hard\\_only}) \\leq P(\\mathcal{R}_{\\eta}(f) \\backslash T_i \\cap D_{hard\\_only}) + \\text{max}\\left(q, \\frac{\\text{err}(f, f_{weak} | S_i \\cap D_{overlap})}{c(1 - \\epsilon_1)}\\right)$$\nProof. Again, the proof follows the similar steps to Lang et al. (2024). Define $M_i = \\{x : f(x) \\neq y(x)\\} \\cap T_i \\cap D_{hard\\_only}$ as the set of mistakes of $f$ in $T_i \\cap D_{hard\\_only}$, and let $U_i = M_i \\cap \\mathcal{R}_{\\eta}(f)$. Let $E_i = \\{x \\in S_i \\cap D_{overlap} \\}$ be the set of points in $S_i \\cap D_{overlap}$ where $f$ disagrees with the weak labels. Since we have $\\text{err}(f, f_{weak} | S_i \\cap D_{overlap}) = P(E_i | S_i)$ and $\\text{err}(f, y | T_i \\cap D_{hard\\_only}) = P(M_i | T_i \\cap D_{hard\\_only}) \\leq P(U_i | T_i \\cap D_{hard\\_only}) + P(\\mathcal{R}_{\\eta}(f) | T_i \\cap D_{hard\\_only})$ by union bound, it suffices to bound $P(\\mathcal{R}_{\\eta}(f) | T_i \\cap D_{hard\\_only})$. Since $U_i \\subset \\mathcal{R}_{\\eta}(f)$, we have $P(\\tilde{N}(U_i) | S_{good} \\cap D_{overlap}) > P_{1-\\eta} (U_i, S_{good} \\cap D_{overlap})$ by Lemma D.2. Also, $U_i \\in \\mathcal{M}_{\\eta}(T_i, \\mathcal{F})$ by definition. Then, $P(U_i|T_i \\cap D_{hard\\_only}) > q$ and $(c, q, \\eta)$-robust expansion implies\n$$P(\\tilde{N}(U_i) | S_{good} \\cap D_{overlap}) \\geq P_{1-\\eta} (U_i, S_{good} \\cap D_{overlap}) > cP(U_i | T_i \\cap D_{hard\\_only}).$$\nWe proceed in two cases.\n\nCase 1: $P(U_i | T_i \\cap D_{hard\\_only}) \\leq q$. In this case, we directly obtain $\\text{err}(f, y | T_i \\cap D_{hard\\_only})= P(M_i | T_i \\cap D_{hard\\_only}) < P(\\mathcal{R}_{\\eta}(f) | T_i \\cap D_{hard\\_only})+ P(U_i|T_i \\cap D_{hard\\_only}) < P(\\mathcal{R}_{\\eta}(f) | T_i \\cap D_{hard\\_only})+ q$.\n\nCase 2: $P(U | T_i \\cap D_{hard\\_only}) > q$. In this case, by the assumption that $(S_{good}^i, T_i \\cap D_{hard\\_only})$ satisfy $(c, q, \\eta)$-robust expansion and $P(\\tilde{N}(U_i) | S_{good} \\cap D_{overlap}) > cP(U_i | T_i \\cap D_{hard\\_only})$, we have\n$$P(\\tilde{N}(U_i) \\cap S_{good} \\cap D_{overlap} | S_i \\cap D_{overlap}) = (1 - \\epsilon_1) P(\\tilde{N}(U_i) \\cap S_{good} \\cap D_{overlap})$$\n$$\\geq (1-\\epsilon_1) cP(U_i | T_i \\cap D_{hard\\_only}).$$\nSuppose $x \\in \\tilde{N}(U_i) \\cap S_{good} \\cap D_{overlap}$. By the definition of $\\tilde{N}(U_i)$, there exists a point $x' \\in U_i$ reachable from $x$ by a good edge, such that $f(x) = f(x')$. Then, since $x \\in S_{good}^i, f_{weak}(x) = y(x) = y(x')$. Also, since $x' \\in M_i, y(x') \\neq f(x') = f(x)$. Thus, $f (x) \\neq f_{weak}(x)$, which implies $x \\in E_i$. This leads to\n$\\text{err}(f, f_{weak}|S_i\\cap D_{overlap}) = P(E_i | S_i \\cap D_{overlap}) > P(\\tilde{N}(U_i) \\cap S_{good}|S_i) \\geq c(1-\\epsilon_1)P(U_i |T_i \\cap D_{hard\\_only})$, Rearranging the inequality, we have\n$$\\text{err}(f, y | T_i \\cap D_{hard\\_only}) \\leq P(\\mathcal{R}_{\\eta}(f) | T_i \\cap D_{hard\\_only}) + \\frac{\\text{err}(f, f_{weak} | S_i \\cap D_{overlap})}{c(1 - \\epsilon_1)}.$$"}, {"title": "D.3 PROOF OF THEOREM 4.2", "content": "Setup We extend the setup in Section 3. We consider label-conditioned Gaussian mixtures as follows. We denote mean parameters as $\\mu_{easy} = \\begin{bmatrix} \\mu_{easy}^\\top \\\\ 0 \\end{bmatrix}, \\mu_{hard} = \\begin{bmatrix} 0 \\\\ \\mu_{hard}^\\top \\end{bmatrix}, \\mu_{overlap} = \\begin{bmatrix} \\mu_{easy}^\\top \\\\ \\mu_{hard}^\\top \\end{bmatrix}$ where $\\mu_{easy} \\in \\mathbb{R}^{d_{easy}}$ and $\\mu_{hard} \\in \\mathbb{R}^{d_{hard}}$.\nWe set $ \\Sigma = \\begin{bmatrix} \\mathcal{C}I_{easy} & 0 \\\\ 0 & \\mathcal{C}I_{hard} \\end{bmatrix}$ to instantiate the distribution of overlap, easy-only and hard-only data points. We set up a common covariance $ \\Sigma =  \\begin{bmatrix} \\mathcal{C}I_{easy} & 0 \\\\ 0 & \\mathcal{C}I_{hard} \\end{bmatrix}$ where $I_{easy}$ and $I_{hard}$ are identity matrices. The distribution of input $x$ follows Gaussian mixtures\n$P(x|y = 1) = \\pi_{easy}\\mathcal{N}(\\mu_{easy}, \\Sigma) + \\pi_{hard}\\mathcal{N}(\\mu_{hard}, \\Sigma) + \\pi_{overlap}\\mathcal{N}(\\mu_{overlap}, \\Sigma)$"}, {"title": "F.5 OVERLAP DENSITY MECHANISM WITHOUT NEURAL NETWORKS", "content": "One might assume that the overlap density mechanism is primarily a feature of deep neural network architectures, given that our experiments mainly use deep neural networks as strong models. While deep neural networks offer useful representations, we demonstrate that the overlap density mechanism can also be shown even without the use of neural networks.\n\nSetup. We adopt the same weak supervision experiment setup as in Section 5.1, except that we use raw inputs for the overlap detection algorithm and XGBoost (Chen & Guestrin, 2016) as the strong model.\n\nResults. Figure A14 presents the experimental results. Although the outcomes appear noisier due to the less powerful representations, which lead to a noisier overlap detection algorithm, we can still observe that the overlap density mechanism is effective-improvements in the weak-to-strong model correspond with increases in overlap density."}, {"title": "F.6 TRANSFERABILITY OF DETECTED OVERLAP DENSITY", "content": "Since overlap detection relies on the model's representation, one might assume that overlap points are model-dependent \u2013 different weak models and weak-to-strong models would have different overlap points. However, we hypothesize that the overlap property is a latent property of data and therefore the detected overlap points are transferable across models.\n\nSetup. We use the same setup as in Section 5.1, except that overlap/non-overlap points are detected using a 4-layer DNN trained on pseudolabels in $D_{w2s}$, and then the weak-to-strong model evaluation is performed with XGBoost after training it on $D_{w2s}$.\n\nResults. Figure A15 shows the experimental results. We can observe a similar trend to that in Section 5.1, supporting our hypothesis."}]}