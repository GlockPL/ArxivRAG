{"title": "CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation", "authors": ["Zheng Chong", "Wenqing Zhang", "Shiyue Zhang", "Jun Zheng", "Xiao Dong", "Haoxiang Li", "Yiling Wu", "Dongmei Jiang", "Xiaodan Liang"], "abstract": "Virtual try-on (VTON) technology has gained attention due to its potential to transform online retail by enabling realistic clothing visualization of images and videos. However, most existing methods struggle to achieve high-quality results across image and video try-on tasks, especially in long video scenarios. In this work, we introduce CatV2TON, a simple and effective vision-based virtual try-on (V2TON) method that supports both image and video try-on tasks with a single diffusion transformer model. By temporally concatenating garment and person inputs and training on a mix of image and video datasets, CatV2TON achieves robust try-on performance across static and dynamic settings. For efficient long-video generation, we propose an overlapping clip-based inference strategy that uses sequential frame guidance and Adaptive Clip Normalization (AdaCN) to maintain temporal consistency with reduced resource demands. We also present ViViD-S, a refined video try-on dataset, achieved by filtering back-facing frames and applying 3D mask smoothing for enhanced temporal consistency. Comprehensive experiments demonstrate that CatV2TON outperforms existing methods in both image and video try-on tasks, offering a versatile and reliable solution for realistic virtual try-ons across diverse scenarios.", "sections": [{"title": "1. Introduction", "content": "The rapid evolution of image and video synthesis techniques has driven significant advancements in downstream tasks, including vision-based virtual try-on, which can be categorized into image-based and video-based approaches. Image-based virtual try-on methods [7, 8, 25, 40, 44, 47, 48, 51, 57] have been extensively explored, achieving high levels of garment realism and detail on static images. Meanwhile, video-based virtual try-on [11, 13, 16, 23, 52], bolstered by recent advancements in video generation, has garnered increasing research interest for dynamic applications.\nHowever, current methods for image-based and video-based try-ons often rely on separate model designs and frameworks. For instance, warping-based methods [14, 25, 47, 48] are tailored specifically for image try-on, adjusting garment shapes to match each pose but unable to maintain temporal consistency across frames. Methods that utilize specialized networks like ReferenceNet or GarmentNet [37, 40, 44, 51, 57] achieve highly realistic try-on effects, but the added encoding networks increase computational overhead. While video-based try-on methods [13, 16, 52] have made progress and can perform image try-on tasks, their performance still lags behind models designed exclusively for image try-on. Based on these observations, we introduce CatV2TON, a streamlined vision-based virtual try-on diffusion transformer framework. By temporally concatenating garment and person inputs and training on a combination of image and video datasets, CatV2TON addresses both static and dynamic try-on scenarios within a single, cohesive model. With only 20% of the backbone parameters allocated to trainable components and no additional modules, it offers a flexible and efficient framework for diverse try-on applications.\nBesides, a critical challenge in video generation is producing long, temporally consistent sequences, which is often resource-intensive and susceptible to quality degradation over time. To generate high-quality long videos, many methods [4, 49] typically involve pre-training on short videos followed by fine-tuning on a limited set of long video data. However, the high hardware requirements and extended inference times needed to produce high-definition long videos significantly restrict their practical application.\nTo address this, we propose an overlapping clip-based inference strategy that leverages preceding frames as temporal guidance and integrates Adaptive Clip Normalization (AdaCN) to ensure consistency while reducing resource costs. Specifically, during training, we use only short video segments with a probability of exposing preceding frames to the model, enabling frame-guided generation. At inference, long video sequences are generated in segments, with the final frames of the previous segment serving as guidance for the next. However, this sequential approach may introduce flickering and color mismatches between segments. To counteract these issues, Adaptive Clip Normalization (AdaCN) is introduced to rectify segments based on guiding frames, thereby maintaining coherence in segmented long video generation.\nAdditionally, we identified several issues with current video try-on datasets. On the one hand, video try-on datasets include human videos with turning or rotating actions but usually contain only garment images in front view. Due to the lack of back-view garment information, generating realistic try-on results when the person is in a rear-facing pose becomes unfeasible. This discrepancy is particularly noticeable for garments with logos, text, or intricate designs, and Using these back-facing frames as ground truth in training will slow model convergence and reduce garment consistency. To address this, we trained a specialized recognition model to detect the person's orientation in each frame, filtering out back-facing frames to create a cleaner dataset. On the other hand, while large quantities of continuous person videos are available as video try-on training data, the indispensable clothing-agnostic masks still rely on frame-by-frame processing using image-based parsing models, which introduces temporal discontinuities. To reduce flickering and edge inconsistency caused by this frame-by-frame processing, we propose a 3D Mask Smoothing operation. This involves applying spatial and temporal average pooling to the clothing-agnostic masks followed by re-binarization, effectively reducing flicker and leakage of edge information across frames.\nIn summary, the contributions of this work include:\n\u2022 We introduce CatV2TON, a simple yet efficient vision-based virtual try-on diffusion transformer that seamlessly handles both static and dynamic try-on scenarios. It is trained on a mixed image-video dataset with more than 80% parameters of the backbone frozen by temporally concatenating garment and person inputs.\n\u2022 We propose an overlapping clip-based inference strategy for long try-on video generation, utilizing preceding frames as guidance and applying Adaptive Clip Normalization (AdaCN) to reduce resource demands while maintaining temporal consistency.\n\u2022 We present ViViD-S, a refined video try-on dataset that has undergone noise data reduction and quality enhancement through specially trained recognition models and 3D mask smoothing, providing high-quality samples suitable for video try-on tasks."}, {"title": "2. Related Work", "content": "Significant advancements have been made in video synthesis and generation, especially in generating coherent and high-quality video sequences from text descriptions. Models like StableVideo [4] and Dreamix [29] leverage diffusion models to capture both content and style with temporal consistency, while Imagen Video [20] and Make-A-Video [38] focus on high-resolution outputs with fine details across frames. Methods such as FateZero [34] and PVDM [55] emphasize inter-frame coherence, crucial for natural-looking animations. For portrait-relevant editing, maintaining identity and expression consistency across frames remains a key challenge. Approaches like [12] balance structural and content control to preserve facial identity, while MagicVideo [59] utilizes latent diffusion for smooth and temporally stable animations. Methods like Tune-A-Video [46] and Follow-Your-Pose [28] bring innovations in one-shot tuning and pose-guided generation, respectively, for realistic human animations in videos. Additionally, image animation methods like EasyAnimate [49], MagicAnimate [53], and AnimateAnyone [21] use transformer-based and reference-guided techniques to enhance temporal coherence and identity preservation for character-specific animations. However, despite these remarkable achievements, high-quality video synthesis still faces challenges in maintaining fine-grained detail consistency, particularly in subject-driven video generation."}, {"title": "2.2. Vision-based Virtual Try-On", "content": "Vision-based virtual try-on includes both image-based and video-based approaches. Image-based try-on generates realistic garment fittings on a target person's photo from a garment image. Methods such as OOTDiffusion [51], IDM-VTON [7], StableGarment [44], and OutfitAnyone [40], utilize diffusion models and dual-stream networks to achieve high-fidelity garment rendering, addressing challenges like pose variation and complex backgrounds. Methods like GP-VTON [48], DCI-VTON [14], WarpDiffusion [25], and GarDiff [42] use pre-warped garments as guidance to enhance realism and detail. Approaches like MMTryon [57], IMAGDressing-v1 [37], and Wear-Any-Way [5] allow for user-controlled try-ons with multimodal conditioning. Lightweight solutions, such as CatVTON [8], provide efficient methods by spatially concatenating garment and person images. Video-based try-on techniques aim to address temporal consistency and garment deformation across frames. Flow-based methods such as FW-GAN [10] and ClothFormer [23] employ warping modules to manage garment alignment and occlusions over various poses and backgrounds. Diffusion-based models like ViViD [13], WildVidFit [16], and VITON-DiT [58] integrate garment encoding and pose tracking to ensure consistent fitting across diverse body movements. Meanwhile, approaches like Tunnel Try-On [52] enhance motion stability using Kalman filtering for commercially viable, smooth, and detailed garment displays. Despite these advancements, current methods are typically limited to single-domain applications, focusing solely on either image or video-based try-ons. A unified vision-based try-on model, capable of seamless virtual garment fitting across both images and videos, remains an area for further exploration."}, {"title": "3. Method", "content": "Our approach aims to develop a streamlined, efficient vision-based virtual try-on network that addresses high resource demands and continuity challenges in generating extended try-on videos. To this end, we propose a refined diffusion transformer model based on a pre-trained video generation model [49], achieving task adaptation with less than 20% of the parameters required for full training (see Section 3.1). Furthermore, we introduce a novel Overlapping Clip-Based Inference strategy along with Adaptive Clip Normalization (AdaCN), facilitating the segmented generation of long-form videos (see Section 3.2)."}, {"title": "3.1. Vision-based Try-On Diffusion Transformer", "content": "As shown in Figure 2, CatV2TON takes as input images or videos of persons, clothing-agnostic masks, pose representations, and target garment images. These inputs are encoded by the video VAE encoder and projected into the latent space. The main backbone, DiT [32], generates the try-on result through multiple denoising steps, which is then decoded into a video by the video VAE decoder.\nWe apply the mask to occlude the input person video or image, resulting in a masked person representation. This masked person is concatenated with the mask along the channel dimension as conditioning information.\nWe use an all-zero mask concatenated along the channel dimension to ensure alignment with the person input. The garment conditions are then concatenated with the person conditions along the temporal dimension. Pose guidance is crucial for maintaining motion continuity in dynamic video generation.\nWe use DensePose [15] as the pose representation, which provides more detailed information compared to skeleton-based methods like OpenPose [2] and MMPose [9]."}, {"title": "3.1.2. Network Structure", "content": "State-of-the-art video generation models [2, 49] commonly use Diffusion Transformers (DiTs) [32] as the backbone. To leverage pre-trained video generation models and speed up training, we adopt DiT as our backbone and initialize our weights from Easy Animate V4 [49], removing the cross-attention layers. This modification is made because our task does not require CLIP [35] or text conditions. Our network consists of N stacked DiT blocks, each with temporal and positional embeddings (using RoPE [39]), and includes only self-attention, feedforward, and normalization layers. For the Pose Encoder, inspired by ControlNeXt [33], we encode DensePose [15] sequences by duplicating the first DiT block and unlocking training, then injecting it into the network using element-wise addition after the first backbone block. This approach is lightweight compared to ControlNeXt [33], as our pose encoder is integrated into the backbone, eliminating the need for feature normalization before element-wise addition."}, {"title": "3.1.3. Training Strategy", "content": "Due to the use of pre-trained weights, and inspired by [8], we unlock only the self-attention layers that involve interactions. The Pose Encoder is fully unlocked, with only 89.90 M trainable parameters, so the entire trainable portion accounts for less than 20% of the backbone network's total parameters. During training, we apply a 10% chance of dropping the garment condition, which enhances generation quality during inference by enabling classifier-free guidance [18]. To facilitate the generation of long videos in segments, we do not mask the first k frames of the person video during training, setting their masks to all-zero. This allows the model to learn the ability to generate subsequent frames based on the preceding frames. In our experiments, this probability is set to 20%."}, {"title": "3.2. Overlapping Clip-Based Inference", "content": "To address the high computational resource requirements of generating long video try-ons, we propose the Overlapping Clip-Based Inference strategy. Specifically, during training, we actively expose earlier frames as prompt frames to enable the model to learn the ability to continue generation (as described in Section 3.1.3). This allows the model to use the results from previous inference steps as prompts to continue generating during inference.\nIn detail, as shown in Figure 3a, for a long video that requires try-on generation, we first divide it into n clips, each containing repeated frames. For each clip, all frames are masked during inference, and after generating the results, the last k frames of the clip are used as prompt frames for generating the next clip. This process repeats for the entire video.\nHowever, this inference method presents a challenge: when generating sequentially, the prompt frames, after denoising, may undergo feature shifts, leading to color discrepancies and motion misalignment. This results in relative discontinuities between clips in the final generated video.\nTo address this issue, inspired by AdaIN [22], we extend it to the level of video clips and propose Adaptive Clip Normalization (AdaCN). As shown in Figure 3b, specifically, for a clip x with prompt frames, we first compute the mean $\u03bc_y$ and standard deviation $\u03c3_y$ of the prompt frame features y, and the mean $\u03bc_\u03b1$ and standard deviation $\u03c3_\u03b1$ of the denoised prompt frames $x_0$. We then normalize the entire clip x using these statistics:\n$x = \u03c3_\u03c5 \\frac{\u03a7 - \u03bc_\u03c7}{\u03c3_\u03c7} + \u03bc_\u03c8.$"}, {"title": "4. Experiments", "content": "We utilized two publicly available image-based try-on datasets-VITON-HD [6] and DressCode [30]-comprising 11,647 and 48,392 paired training samples, respectively, to construct our image-based training dataset. Comparative experiments for image-based virtual try-on were conducted on the test sets of VITON-HD [6] and DressCode [30] datasets.\nWe constructed the ViViD-S dataset based on the ViViD [13] dataset. Specifically, we trained a human orientation classifier based on EfficientNet [41] to detect sequences of frontal frames from the videos, filtering for videos that contain more than 24 consecutive frontal frames. Consequently, we selected 6,064 videos with a total of 513,896 frontal frames from the 7,759 videos in the ViViD [13] training dataset to serve as our training set. Due to the impracticality of testing on thousands of videos from a time and cost perspective, we randomly selected 180 videos (60 for dresses, 60 for uppers, and 60 for bottoms) from the ViViD [13] test set, each containing 64 consecutive frontal frames, to form the test set. Additionally, we utilized the VVT [11] dataset, which is a standard video virtual try-on dataset comprising 791 paired person videos and clothing images, with a resolution of 192\u00d7256. The comparative experiments are also conducted on the VVT test set which comprises 130 videos."}, {"title": "4.2. Implementation Details", "content": "We initialized the DiT backbone using the pre-trained weights from EasyAnimateV4 [50], which was fine-tuned based on HunyuanDiT [26], and employed the pre-trained MagViT [54] as the video VAE to handle video data. We employed a progressive training strategy across three stages. In the first stage, we trained the model at a resolution of 256\u00d7192 using four datasets DressCode, VITON-HD, VIVID, and VVT. Each training video sample consisted of 72 frames, with a batch size of 16, and the training ran for 128K steps. In the second stage, we increased the resolution to 512x384 and focused on three datasets-DressCode, VITON-HD, and VIViD. The number of frames per training sample was reduced to 48, with a batch size of 8, and the training continued for 64K steps. Finally, in the third stage, we used a high resolution of 832x624, training on the DressCode, VITON-HD, and VIViD-S datasets. In this stage, each sample contained 32 frames, the batch size was reduced to 1. This progressive strategy allowed the model to first learn at lower resolutions before refining its performance at higher resolutions. For all stages, we applied the AdamW optimizer [27] with a constant learning rate of le-5 and gradient clipping set to 1.0. The entire training process was carried out on 4 NVIDIA A100 GPUs. Additionally, all model variants used in the ablation study were trained under identical hyperparameter settings to ensure fair comparison."}, {"title": "4.3. Metrics", "content": "For image-based try-on settings, we employ four widely used metrics: SSIM [45], LPIPS [56], FID [36], and KID [1]. SSIM and LPIPS are used to measure the similarity between two images, while FID and KID assess the similarity between two sets of images. In paired try-on tests, we use all four metrics, whereas in unpaired scenarios, we only use FID and KID. For video try-on scenarios, we use SSIM [45], LPIPS [56], and VFID with I3D [3] and ResNext as metrics to evaluate video quality."}, {"title": "4.4. Qualitative Comparison", "content": "Figure 4, Figure 5 and Figure 6 present qualitative comparisons of our method with StableVITON [24], OOTDiffusion [51], and ViViD [13] on the ViViD-S test set for unpaired visual try-on with dress, lower and upper clothing. Our approach demonstrates superior performance in generating try-on videos with improved temporal coherence and garment consistency compared to other baseline methods. Figure 7 presents the results of our method applied to the same person in a video with different types of clothing changes. This demonstrates the capability of our approach to maintain texture consistency, clothing shape, and temporal coherence. Additional comparison results are provided in the supplementary materials."}, {"title": "4.5. Quantitative Comparison", "content": "We conducted quantitative comparisons with advanced image-based try-on methods [14, 24, 31, 43, 44, 48, 51] under both paired and unpaired settings on the VITON-HD [6] and DressCode [30] datasets. During inference for image try-on, we used the DDPM [19] noise schedule for 20 steps, with a CFG [17] strength set to 3.0. As shown in Table 1 and Table 2, although our approach is designed for unified visual try-on, it outperforms traditional image-based try-on methods across various metrics for generated image quality, particularly in the unpaired scenario. This demonstrates that our method exhibits strong generalization performance even when trained on a single dataset.\nDue to the limited availability of open-source video try-on"}, {"title": "4.6. Ablation Studies", "content": "To validate the contribution of different components or strategies to the final performance, we conducted ablation experiments on PoseNet, training data (ViViD [13] dataset or ViViD-S dataset), and AdaCN."}, {"title": "5. Limitations", "content": "In comparison to image data, video data, despite having a similar resolution, cannot achieve the same level of clarity due to the dynamic changes it exhibits. Even though the current resolution of 832x624 is sufficient in terms of pixel count, it still falls short of meeting application requirements in terms of clarity. A higher-quality, higher-resolution video try-on dataset is crucial to addressing this issue.\nThe key difference between video and image data lies in the dynamic nature of video, which must adhere strictly to physical laws. Otherwise, unrealistic artifacts may emerge, particularly in try-on tasks where clothing movement during different actions is critical. Currently, there is a lack of foundational video generation models that can accurately simulate these physical behaviors. Achieving this would likely require larger-scale models and a higher-quality, larger-volume video dataset."}, {"title": "6. Conclusion", "content": "In this work, we proposed CatV2TON, a simple and efficient diffusion transformer framework for both image and video virtual try-on tasks. By temporally concatenating garment and person inputs and training with a mixed image-video dataset, our model achieves high-quality results with only 20% of the backbone parameters as trainable components. To support long, temporally consistent try-on video generation, we introduced an overlapping clip-based inference strategy with Adaptive Clip Normalization (AdaCN), reducing resource demands while maintaining temporal continuity. Additionally, we propose a curated video try-on dataset, ViViD-S, created by filtering out back-view frames and applying 3D Mask Smoothing to enhance the temporal consistency of masks. Extensive experiments demonstrate that CatV2TON outperforms baseline methods in both quantitative and qualitative evaluations, marking a significant step forward for unified models in vision-based virtual try-on research."}]}