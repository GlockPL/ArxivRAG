{"title": "Constructing Fair Latent Space for Intersection of Fairness and Explainability", "authors": ["Hyungjun Joo", "Hyeonggeun Han", "Sehwan Kim", "Sangwoo Hong", "Jungwoo Lee"], "abstract": "As the use of machine learning models has increased, numerous studies have aimed to enhance fairness. However, research on the intersection of fairness and explainability remains insufficient, leading to potential issues in gaining the trust of actual users. Here, we propose a novel module that constructs a fair latent space, enabling faithful explanation while ensuring fairness. The fair latent space is constructed by disentangling and redistributing labels and sensitive attributes, allowing the generation of counterfactual explanations for each type of information. Our module is attached to a pretrained generative model, transforming its biased latent space into a fair latent space. Additionally, since only the module needs to be trained, there are advantages in terms of time and cost savings, without the need to train the entire generative model. We validate the fair latent space with various fairness metrics and demonstrate that our approach can effectively provide explanations for biased decisions and assurances of fairness.", "sections": [{"title": "Introduction", "content": "With the rapid advancement of machine learning models, the demand for fairness has grown, especially in protecting sensitive features like age and gender from bias (Caton and Haas 2024). Although recent research has incorporated fairness metrics to ensure fairness, the sufficiency of these metrics alone in addressing stakeholders' concerns remains an open question (Fig. 1A). In the US, predictive policing tools have been criticized for disproportionately targeting individuals based on race and gender biases, sparking protests (Richardson, Schultz, and Crawford 2019). In this context, it is crucial for stakeholders to identify whether the decision is free from gender bias. Thus, presenting compelling evidence to stakeholders and practitioners becomes essential when addressing sensitive issues. Practitioners can adjust the model based on this evidence, while stakeholders can transparently rely on and utilize the model's decisions, grounded in the provided evidence (Fig. 1C-blue). In addition, when the model has been adjusted, providing assurances through explanations that decisions are not based on sensitive attributes (Fig. 1C-red) is essential for building trust in decision-making systems (Jacovi et al. 2021).\nThere has been research at the intersection of fairness and explainability, which attempts to explain the causes of unfairness through decomposing model disparities via features (Begley et al. 2020) or causal paths (Chiappa 2019; Plecko and Bareinboim 2023). However, these studies have several limitations. Firstly, they provide explanations for the overall fairness of the model, which are not sufficiently persuasive about the fairness of individual predictions. Secondly, although there are methods to explain the model's decisions (Qiang et al. 2022; Dash, Balasubramanian, and Sharma 2022), they do not fully reveal the underlying reasons because the explanations are based on variations in the model's outputs rather than the model's internal behavior.\nTo address the aforementioned limitations and provide a faithful explanation of decisions to each individual, we propose a novel framework that functions as a module for a generative model to construct a fair latent space. In the fair latent space, where sensitive attributes are disentangled from decision-making factors, we can elucidate why a predictor returns a specific outcome and ensure that this outcome is not influenced by sensitive attributes. This is achieved by manipulating these distinct attributes within the latent space and generating counterfactuals (Fig. 1B). As these counterfactual explanations (simulating alternative inputs with specific changes to the original) are generated directly by the model, they provide trustworthy explanations due to their inherent interpretability (Rudin 2019; Joo et al. 2023).\nTo construct fair latent space, we conduct the training of this module on the results of our theoretical exploration of mutual information. The fundamental concept involves adjusting latent representations to effectively disentangle and redistribute information associated with labels and sensitive attributes. By disentangling and optimizing information related to attributes, our method not only provides explanations but also consistently demonstrates a high level of fairness across various metrics. In addition, while recent generative models perform well, they entail significant training time and costs. Therefore, to reduce computational costs and training time, we propose constructing a fair latent space by training an invertible neural network (INN) on the latent space of pre-trained generative models, instead of retraining them from scratch (Fig. 1B). This approach leads to the development of a versatile module applicable to the generative model, which offers counterfactual reasoning."}, {"title": "Related work", "content": "Fairness in machine learning Fairness in machine learning has gained focus in recent research. One approach involves in-processing methods that incorporate fairness-aware regularization into the training objective (Donini et al. 2018; Sagawa* et al. 2020; Han et al. 2024) or soften fairness constraints into score-based constraints for optimization (Zafar et al. 2017, 2019; Zhu et al. 2023). However, with research identifying the underlying biases in training data as the primary source of unfairness (Barocas and Selbst 2016; Tommasi et al. 2017; Gustafson et al. 2023), there have been notable efforts on direct interventions through augmentation to tackle this issue (Ramaswamy, Kim, and Russakovsky 2021; Qiang et al. 2022; Zietlow et al. 2022; Zhang et al. 2024). Consequently, studies have investigated fairness evaluation using counterfactual samples generated by generative models (Denton et al. 2019; Joo and K\u00e4rkk\u00e4inen 2020; Dash, Balasubramanian, and Sharma 2022). However, existing counterfactual generation methods differ from ours in that they conduct an analysis by examining how the classifier handles counterfactual samples, rather than providing counterfactual explanations.\nFair representation learning As fairness becomes a crucial issue in the practical application of models, various approaches have been developed to learn fair representations through disentangling. Kim and Mnih (2018) propose a direct method that uses disentanglement learning, and Creager et al. (2019) isolate the latent space into sensitive and non-sensitive components. Other approaches enforce independence by learning target and sensitive codes to follow orthogonal priors (Sarhan et al. 2020) or by minimizing distance covariance, offering a non-adversarial alternative (Liu et al. 2022). However, these methodologies all rely on variational autoencoders, which are inherently limited in terms of generalization. Furthermore, because the entire generative model is trained, image reconstruction quality is compromised (Shao et al. 2022).\nResearch has also been conducted employing contrastive learning methods, which have proven highly successful in learning effective representations in recent years (Chen et al. 2020; Khosla et al. 2020). Similar to our approach, these methods learn representations by reducing the distance between positive samples and increasing it for negative samples. FSCL (Park et al. 2022) focuses on regulating the similarity between groups, ensuring it remains unaffected by sensitive attributes. Meanwhile, other approaches concentrate on enhancing the robustness of representation alignment (Zhang et al. 2022) or devising methods effective even with partially annotated sensitive attributes (Zhang et al. 2023). However, methods developed after FSCL assume unlabeled scenarios; as a result, FSCL has demonstrated the best performance in labeled situations."}, {"title": "Method", "content": "In this paper, our objective is to achieve a fair latent space in generative models, thereby gaining insights into the model's fairness through counterfactual explanations and fair classification. Therefore, we first conduct a theoretical analysis on separating information about labels and sensitive attributes in the latent space. Secondly, we connect this theoretical analysis to practical training methods."}, {"title": "Disentangling sensitive attributes from labels", "content": "Previously, many methods concentrated on being invariant to information associated with sensitive attributes, to depend solely on labels for fairness in classification (Kehrenberg et al. 2020; Ramaswamy, Kim, and Russakovsky 2021; Park et al. 2022). On the other hand, our approach does not aim to exclude but rather to separate information regarding the sensitive attributes. We aim to disentangle data related to sensitive attributes from labels and assign them to distinct dimensions. By redistributing this data in the latent space, we enrich the representation with label information, thus enhancing the fairness of classification. Therefore, our goal is to maximize the information associated with each assigned attribute within its respective dimension.\nLens of the information bottleneck Let S denote a sensitive attribute, such as race or gender, and Y a label. With an invertible network $f_e$ and pre-trained generative model $G = f_{dec} \\circ f_{enc}$, where $f_{enc}$ is the encoder and $f_{dec}$ is the decoder, the latent representation of image data X can be obtained as $E = f_{enc}(X)$. Concurrently, the latent representation of the invertible network is derived as $Z = f_e(E)$. Then, we allocate information pertaining to labels and attributes in separate dimensions, denoted as $Z_Y$ and $Z_S$.\nIn the case of the label dimension, the objective is to maximize its information using the compact invertible model. This aligns with the objective of the Information Bottleneck (IB) (Tishby and Zaslavsky 2015), which aims to maximize the information between the representation and the target in situations where the model's complexity is limited, as our training takes place in a compact invertible model. With the lens of the IB principle, we maximize the mutual information $I(Z_Y, Y)$ subject to a complexity constraint specified as $I(Z_Y, E) < b$ with a constant b. In this scenario, we can express our objective using the loss function $L_{IB} = I(Z_Y,E) - \\beta I(Z_Y, Y)$ with $\\beta > 1$, as we focus more on the relationship between Y and $Z_Y$. Furthermore, substituting mutual information with entropy allows us to reformulate the loss function using the determinant of the covariance matrix C (Ahmed and Gokhale 1989), facilitating the transformation as follows.\nTheorem 1. Let the representation ZY follow a Gaussian distribution, and $\\beta > 1$. The information bottleneck-based loss $L_{IB} = I(Z_Y, E) - \\beta I(Z_Y, Y)$ can be reformulated as:\n$L_{IB} = E_Y [log det(C_{Z_Y|Y})] - \\Lambda log det(C_{Z_Y}), \\Lambda > 0$.  (1)\nAdditional details for the proof are provided in the Appendix. To minimize $L_{IB}$, we focus on maximizing the second term $log det(C_{Z_Y})$, given that the first term remains constant when optimizing the representation $Z_Y$. If we set the dimension of fair representation as d and apply Jensen's inequality, the second term $log det (C_{Z_Y})$ can be rewritten as:\n$\\frac{1}{d} \\sum_{i=1}^{d}log(\\lambda_i(C_{Z_Y})) \\leq dlog(\\frac{1}{d} \\sum_{i=1}^{d} (C_{Z_Y}))$, (2)\nwhere $\\lambda_i(C_{Z_Y})$ is i-th eigenvalue of $C_{Z_Y}$. In Jensen's inequality, equality holds when all values are equal as $\\lambda_i(C_{Z_Y}) = \\lambda_j(C_{Z_Y})$ for $\\forall j \\in [1,......,d]$. Given that the covariance matrix is symmetric and positive semi-definite, it allows for diagonalization of $C_{Z_Y}$ with maximum determinant through an orthogonal matrix Q (i.e. det(Q) = \u00b11), resulting in diag(c, c,........, c) for $c = \\lambda_i(C_{Z_Y})$. Therefore, our objective is achieved when the covariance matrix is a diagonal matrix with identical diagonal entries.\nConnection between opposite sensitive attributes In addition to maximizing mutual information along separate dimensions, our strategy involves directly mitigating the influence of sensitive attributes on decision-making processes. This is accomplished by ensuring that the label dimension corresponding to the label contains solely relevant information. Therefore, our additional goal is to maximize the mutual information between inputs with different sensitive attributes within the label's dimension.\nConsidering a scenario with a binary sensitive attribute, for data with the same label y, we denote the data with a positive sensitive attribute as $X^0$ and the data with a negative sensitive attribute as $X^1$. However, directly computing mutual information between two random variables is infeasible. To address this, we employ a widely-used approach that approximates the mutual information using noise-contrastive estimation (Oord, Li, and Vinyals 2018; Poole et al. 2019). Given the two random variables $X^0$ and $X^1$, the mutual information lower bound is defined as follows.\n$I_{NCE} = E [\\frac{1}{K} \\sum_{i=1}^{K} log \\frac{e^{g(x^0_i,x^1_i)}}{\\sum_{j=1}^{K} e^{g(x^0_i,x^1_j)}} + log(K)]$, (3)\nwhere the expectation is over K independent samples.\nTo maximize the mutual information $I(X^0, X^1)$, our strategy necessitates numerically increasing $g(x^0_i, x^1_i)$ while concurrently reducing $g(x^0_i, x^1_j)$. In our scenario, the representation vectors are derived from a combination of a pre-trained generative model and an invertible model. Consequently, the product between encoded representations $g(x^0_i, x^1_i)$ can be formally denoted as $z^Y_0, z^Y_1 = f_e(f_{enc}(x_0)) f_e(f_{enc}(x_1))$ at the dimension $Z_Y$.\nAs a result, the term to be increased can be denoted as $(z^Y_0, z^Y_1)^T (z^Y_0, z^Y_1)$, and the term to be decreased can be expressed as $(z^Y_0)^T (z^Y_1)$. Consequently, with the objective of transforming the covariance matrix into a scalar matrix, the term to be increased is represented as a negative squared $L_2$ distance, whereas the term to be decreased is upper bounded by the $L_2$ distance, in accordance with the Cauchy-Schwarz inequality. Detailed proofs are included in the Appendix, and these findings lead to the following theorem.\nTheorem 2. Let the mutual information $I(Z_Y, Y)$ be maximized within a network of constrained capacity. Then, maximizing the mutual information $I(X^0, X^1)$ can be achieved by minimizing the $L_2$ distance between samples from the groups $X^0$ and $X^1$.\nWe can confirm that in our scenario, reducing the $L_2$ distance enhances the mutual information between the two groups. This shift leads to an emphasis on accurate information rather than spurious attributes."}, {"title": "Constructing fair latent space", "content": "In this section, we elucidate how the theoretical analysis in Sec. 3.1 seamlessly translates into the proposed methodology, resulting in the losses illustrated in Fig. 2. Our approach involves training an invertible network $f_e$, thereby enabling the acquisition of a fair representation without incurring additional costs for a generative model.\nOne of our objectives is to convert the covariance matrix of the representation into a scalar matrix, akin to dimension-contrastive methods in semi-supervised learning (Zbontar et al. 2021; Bardes, Ponce, and LeCun 2022), which decorrelate each dimension. Let $B = \\{X, Y, S\\} = \\{(x_i, y_i, s_i)\\}_{i=1}^{n}$ represent the training batch, comprising images $x_i$, labels $y_i$, and sensitive attributes $s_i$. Subsequently, the representation corresponding to the label in the training batch, which is derived through the invertible network can be denoted as $Z_Y = f_e(f_{enc}(X))_Y \\in \\mathbb{R}^{n \\times d_Y}$. Then, the covariance matrix for each latent dimension of this representation can be defined as follows.\n$C(Z^Y) = E [(Z^Y - E(Z^Y))^T(Z^Y - E(Z^Y))] \\in \\mathbb{R}^{d_Y \\times d_Y}$ (4)\nTo diagonalize the obtained covariance matrix, we subtract the term that is element-wise multiplied (\\odot) by the identity matrix $I_{d_Y}$, isolating the off-diagonal elements. This process yields a diagonalizing loss term by summing the squared elements of the obtained matrix and incorporating a normalization factor of $1/d_Y$.\n$L_{dg}(Z^Y) = \\frac{1}{d_Y} ||C(Z^Y) - C(Z^Y) \\odot I_{d_Y}||^2_F$ (5)\nNext, we further employ a loss function to equalize the diagonal elements. Instead of directly setting the diagonal elements to c, this approach is inspired by the observed performance improvements achieved by adjusting the scale through variance (Bardes, Ponce, and LeCun 2022). Given the covariance matrix $C'(Z^Y) \\in \\mathbb{R}^{d_Y \\times d_Y}$, we constrain the variance of the batch to c for each dimension. Utilizing the ReLU activation, which computes max(0,x) for the input x, the loss is defined as follows.\n$L_{eq}(Z^Y) = \\frac{1}{d_Y} \\sum_{j=1}^{d_Y} max(0, c - Var(z^Y_j) + \\epsilon)$,  (6)\nwhere $z^Y_j \\in \\mathbb{R}^{n}$ denotes the vector of the j-th dimension in the latent dimension of $Z^Y$, $\\epsilon$ is a small constant to prevent gradient collapse, and $Var(\\cdot)$ represents the variance.\nAnother objective is to maximize the mutual information between groups with different values in unintended factors. From Thm. 2, our method minimizes $L_2(x^0, x^1)$, which is a feasible strategy for maximizing $I(X^0, X^1)$ when focusing on $Z_Y$. Additionally, to enhance the influence of the intended factor, we incorporate a term into our loss function that increases the distance $L_2(x^Y, x^{Y'})$ between groups with the same sensitive attribute but different labels, where y \u2260 y'. This results in the following formulation.\n$L_{di}(Z^Y) = \\frac{1}{M_{max}} \\sum_{i=1}^{n} \\sum_{j \\neq i}^{n} M_{max}D(z^Y_i, z^Y_j) + \\frac{1}{M_{min}} \\sum_{i=1}^{n} \\sum_{j \\neq i}^{n} M_{min} D (z^Y_i,z^Y_j)$, (7)\nwhere $M_{max} = \\delta(y_i, y_j)(1 - \\delta(s_i, s_j))$ is the mask for selecting samples to maximize, and $M_{min} = \\delta(s_i, s_j) (1 - \\delta(y_i, y_j))$ selects samples to minimize. Here, $\\delta(x, y)$ is the Kronecker delta, which equals 1 if x = y and 0 otherwise. The loss function uses $D(x,y) = log((\\|x - y\\|^2_2 + 1)/(\\|x - y\\|^2_2 + \\epsilon))$, a monotonically decreasing function with respect to the distance, instead of the $L_2$ distance. This modification mitigates potential instability caused by unbounded values as distances grow infinitely. By employing a bounded function, we enhance training stability and focus on regions where the $L_2$ distance is minimized.\nThe loss function for constructing a fair latent space in training the invertible network is derived by integrating these losses. Drawing inspiration from the technique of segregating information into distinct dimensions (Esser, Rombach, and Ommer 2020), we decompose the latent dimensions in the representation from the invertible network as $Z = [Z_Y, Z_S] \\in \\mathbb{R}^d$ where $Z_Y \\in \\mathbb{R}^{d_Y}$ and $Z_S \\in \\mathbb{R}^{d_S}$. The loss function, based on theoretical justification and designed to construct a fair latent space, is defined as follows.\n$L_{fair}(Z^Y) = \\lambda_{dg}L_{dg}(Z^Y) + \\lambda_{eq}L_{eq}(Z^Y) + \\lambda_{di}L_{di}(Z^Y)$. (8)"}, {"title": "Gaussianizing embeddings through INN", "content": "We train an invertible neural network (INN) instead of a convolutional network to ensure that transformations in the fair latent space can appropriately extend to the latent space of the generative model. In our method, the INN $f_e$ maps generative representation $e$ to a fair representation $z$ with a forward mapping $f_0 : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ and can also serve as an inverse mapping $f_0^{-1}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$.\nWe use Normalizing Flows (NFs) for exact log-likelihood computation and precise inference. However, they often produce severe artifacts due to exploding inverses when generating images from out-of-distribution data (Behrmann et al. 2021; Hong, Park, and Chun 2023). To bypass this, we train NFs on high-level semantic representation (Kirichenko, Izmailov, and Wilson 2020). Our model computes the base distribution $p_Z(z)$ with an encoder distribution $p_E(e)$ in the latent space. Given the assumption in Thm. 1 that Z follows a Gaussian distribution, a standard Gaussian as the $p_Z(z)$ completes our theoretical framework. We minimize the negative log-likelihood objective using the following equation, where the Jacobian of $f_e$ is denoted as $J_{f_e}$.\n$L_g = - \\frac{1}{n} \\sum_{i=1}^{n}(\\|f_e(e)\\|^2 + log \\vert det(J_{f_e}(e)) \\vert)$  (9)\nFurthermore, we train a one-layer fully connected classifier using $Z_Y$ for labels Y and $Z_S$ for sensitive attributes S, employing a cross-entropy loss denoted by $L_{cls}$. During training, the overall loss is derived by summing $L_{fair}(Z_Y), L_{fair}(Z_S), L_g$, and $L_{cls}$."}, {"title": "Experiment", "content": "We conducted experiments with the Diffusion Autoencoder (DiffAE) (Preechakul et al. 2022), a recent encoder-decoder structure generative model, to evaluate our proposed approach and demonstrate its practical applicability. Additionally, we employed Glow (Kingma and Dhariwal 2018) as the invertible neural network."}, {"title": "Experimental details", "content": "Fairness metrics The most commonly used metrics for measuring group fairness are Demographic Parity (DP) (Chouldechova 2017) and Equalized Odds (EO) (Hardt, Price, and Srebro 2016). DP aims to equalize the rate of positive outcomes irrespective of the sensitive attribute. EO aims to equalize the true positive rate and false positive rate, which is appropriate for problems where negative outcomes are as important as positive outcomes, such as facial attribute classification. Worst-Group-Accuracy (WGA) has also been used as a fairness metric; Zietlow et al. (2022) argues that reaching fairness by performance degradation not only for the highest-scoring group but also for the lowest-scoring group is received substantial criticism in other areas (Brown 2003; Christiano and Braynen 2008; Doran 2001). Following Zhang et al. (2023), we defined EO and DP as follows.\n$EO = \\sum_y \\vert P_{s1} (\\hat{Y} = y \\vert Y = y) - P_{s0} (\\hat{Y} = y \\vert Y = y)\\vert$ (10)\n$DP = \\vert P_{s1} (\\hat{Y} = Y_p) - P_{s0} (\\hat{Y} = Y_p)\\vert$,   $Y_p$ = positive (11)\nDatasets We conduct experiments on DiffAE using three datasets. In CelebA (Liu et al. 2015), which features 40 binary attribute labels, we designate S = {male} as the sensitive attribute and classify gender-dependent attributes (Ramaswamy, Kim, and Russakovsky 2021) such as Y = {attractive, young, bushy brows}. In CelebAHQ (Karras"}, {"title": "Fair latent space evaluation", "content": "We evaluated the fairness of the latent space by using fairness metrics. As one of our main objectives is to establish a fair latent space, we compared our approach with methods proposed for learning visual representations, such as SimCLR (Chen et al. 2020), SupCon (Khosla et al. 2020), and FSCL (Park et al. 2022). While there have been studies aiming to train fair representations even without the notion of fairness, to our knowledge, FSCL demonstrates state-of-the-art performance in terms of the group fairness metric for facial attribute classification.\nGender discrimination is one of the most important topics addressed in fairness. To address this issue within facial attribute classification, which is used in a wide range of practical applications including face verification and image search, we have designated gender as a sensitive attribute. We conducted experiments on the CelebA dataset, focusing on three attributes known to be related to gender (Ramaswamy, Kim, and Russakovsky 2021).\nThe classification results are shown in Tab. 1. Firstly, our proposed method demonstrates significant performance improvements across all three metrics: EO, DP, and WGA. Through our proposed method, we observed a significant reduction of 86.8% in EO, 52.0% in DP, and a 39.9% increase in WGA, indicating a successful transition to a fair latent space from a previously biased one. Additionally, our method displayed a clear distinction from FSCL, which also aimed for fair representation.\nSecondly, Tab. 2 demonstrates that our method extends beyond a single dataset, proving effective on large-scale image datasets like CelebAHQ. Its efficacy is further validated through experiments on the UTKFace dataset, assessing performance across diverse datasets. Additionally, experiments on the CelebA dataset, which includes two sensitive attributes, evaluate the method's performance in scenarios involving multiple sensitive attributes."}, {"title": "Ablation study", "content": "In this section, we assess the alignment between our theoretical analysis and practical outcomes through an ablation study. Our method's theoretical design is developed incrementally, with each stage building upon the previous one. As a result, performance improves with the addition of each component, as shown in Tab. 3, which confirms the effectiveness of the approaches discussed in Sec. 3.1 for ensuring fairness within the latent space. Additional results for other datasets and settings are provided in the Appendix.\nWe further confirm the necessity of assumptions underlying each theoretical analysis in Sec. 3.1. Initially, we assess the necessity of decomposing (De) representation dimensions into labels and sensitive attributes, by comparing scenarios in which the covariance matrix is diagonalized without such decomposition. As shown in Tab. 4, applying $L_{dg}$ without De leads to the maximization of entangled information, which has a minimal impact on fairness. Furthermore, to ensure that information is optimally structured when transforming the covariance matrix into a scalar matrix, we compare the scenarios involving $L_{eq}$ and $L_{dg}+L_{eq}$ with the application of De. The second set of rows in Tab. 4 emphasizes the importance of transforming the diagonal matrix to ensure identical diagonal entries. Finally, we investigate the importance of maximizing mutual information between the label Y and the representation $Z_Y$, as assumed in Thm. 2. The third set of rows in Tab. 4 shows that without this assumption, $L_{di}$ fails to perform accurately."}, {"title": "Explaining the fairness by counterfactual", "content": "In the previous section, we observed an increase in the worst group's accuracy after applying our framework. In this section, we demonstrate how this improvement is reflected in the explanations provided by our framework. With the constructed fair latent space, our method can generate counterfactual explanations by adjusting the dimensions corresponding to the label or sensitive attribute and observing the model's behavior. Specifically, the vector of each classifier weight h in the latent space is the best choice for the basis corresponding to the label or sensitive attribute, as these classifiers most clearly represent how the model differentiates information. Hence, we base our counterfactual explanation on the representations obtained by moving the representation $z = f_e(f_{enc}(x))$ in the direction of the classifier weight vector $h = \\frac{h}{\\|h\\|}$.\nA counterfactual explanation of the label reveals the factors that influenced the model's decision, while a counterfactual"}, {"title": "Conclusions & Limitations", "content": "We propose a module that enhances the understanding of the model's fairness by offering explanations to individuals, accompanied by fair decisions supported through a fair latent space. However, since our model involves integrating a module with a frozen generative model, there is a limitation in that the average accuracy depends on the performance of the pre-trained generative model. Despite this limitation, we have demonstrated the practical application of provided explanations and the fairness of constructed latent space."}, {"title": "A Proofs", "content": "A.1 Proof for Theorem 1\nTheorem. Let the representation ZY follow a Gaussian distribution", "as": "nLIB = EY [log det(CZY|Y)", "Y)": "log det (CZY )\n24\n= EY [log det (CZY |Y)", "right": ".", "d_Y": "."}]}