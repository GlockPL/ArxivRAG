{"title": "UA-PDFL: A Personalized Approach for Decentralized Federated Learning", "authors": ["Hangyu Zhu", "Yuxiang Fan", "Zhenping Xie"], "abstract": "Federated learning (FL) is a privacy preserving machine learning paradigm designed to collaboratively learn a global model without data leakage. Specifically, in a typical FL system, the central server solely functions as an co-ordinator to iteratively aggregate the collected local models trained by each client, potentially introducing single-point transmission bottleneck and security threats. To mitigate this issue, decentralized federated learning (DFL) has been proposed, where all participating clients engage in peer-to-peer communication without a central server. Nonetheless, DFL still suffers from training degradation as FL does due to the non-independent and identically distributed (non-IID) nature of client data. And incorporating personalization layers into DFL may be the most effective solutions to alleviate the side effects caused by non-IID data. Therefore, in this paper, we propose a novel unit representation aided personalized decentralized federated learning framework, named UA-PDFL, to deal with the non-IID challenge in DFL. By adaptively adjusting the level of personalization layers through the guidance of the unit representation, UA-PDFL is able to address the varying degrees of data skew. Based on this scheme, client-wise dropout and layer-wise personalization are proposed to further enhance the learning performance of DFL. Extensive experiments empirically prove the effectiveness of our proposed", "sections": [{"title": "1. Introduction", "content": "Federated learning (FL) [32] enables multiple users to jointly learn a shared global model without uploading their private data, significantly alleviating the concerns about privacy leakage. It has been widely used in many real-world applications, such as recommendation systems [15], health care [42, 58], Internet-of-Things (IoTs) [17, 7], and so on. However, most existing FL frameworks contain a central server for model aggregation, which is not suitable for scenarios requiring more flexible connectivity, such as unmanned aerial vehicle (UAV) network [39], mobile Ad hoc networks and smart manufacturing [27]. In addition, the server-client structure of FL is more susceptible to single-point transmission bottlenecks and security threats [50, 31]. Therefore, decentralized federated learning (DFL) [40, 22] is introduced, where participating clients are peer-to-peer communicated without the requirement of a central server. But eliminating the central server may boost the overall communication costs across the entire learning system. As a simple example depicted in Fig. 1, possible connection paths significantly increases (from 5 paths in Fig. 1(a) to 10 paths in Fig. 1(b)) when clients are peer-to-peer communicated. Hence, minimizing communication overheads while maximizing learning performance become increasingly important, posing a significant challenge for DFL. Some studies have suggested such as employing a proxy model [19] and gossip-based pruning [46] to reduce the communication costs in DFL. Furthermore, to simultaneously avoid system dominance and address security concerns [61, 10], handling data heterogeneity [23, 63] and promoting a more sustainable DFL economy system through incentive mechanisms [49, 28, 36] are frequently discussed. However, the inherent non-independent and identically distributed (non-IID) nature of users' training data inevitably impairs the learning performance of DFL [63]. This phenomenon occurs because each client model is prone to converging to different directions, leading to a significant divergence of the aggregated model from the expected ideal model.\nTo address this issue, maintaining personalized models on each local device during federated training is considered one of the most effective ap-"}, {"title": "2. Background and Motivation", "content": "In this section, a concise overview of decentralized federated learning is given at first, followed by an introduction to personalized federated learning. Lastly, we reiterate the motivation behind the present work."}, {"title": "2.1. Decentralized Federated Learning", "content": "Decentralized federated learning (DFL) [40] has been proposed to mitigate issues inherent in the vanilla FL framework, such as centralized server dependencies, transmission bottlenecks, and trustworthiness concerns. As shown in Fig. 2, M clients conduct local training at first. Subsequently,"}, {"title": "2.2. Personalized Federated Learning", "content": "The fundamental concept of PFL-based methods is constructing a personalized model according to the local task to tackle data heterogeneity. In general, there are three major types of PFL methods:\nThe first approach involves clustering-based methods, which employ a multi-center framework by grouping clients into different clusters. Only clients within the same cluster group participate in model aggregation. CFL [41] represents a classical example, performing bi-partitioning to recursively divide participants based on the uploaded model gradient from each client. Similar approaches such as IFCA [11] and FL-HC [3] utilize computed local loss value as clustering metric for grouping operations [62, 34, 29].\nThe second category encompasses regularization-based methods. For example, Li et al. [25] proposed Ditto framework, which involves training a personalized local model subsequent to acquiring a globally generalized model, accomplished by incorporating a regularization term, as shown in Eq.(1).\n$\\min _{w_{m}} f_{m}\\left(w_{m}, w ; D_{m}\\right)=\\mathcal{L}\\left(w_{m} ; D_{m}\\right)+\\lambda\\left\\|w_{m}-w\\right\\|^{2}$,\nwhere $f_{m}$ represents local training objective on client m, $w$ indicates global model parameters and $\\mathcal{I}$ indicates the local loss function. The hyperparameter $\\lambda$ controls the strength of regularization applied during federated training, shrinking the difference between the global model and local models. RCFL [55] and other methods [6, 4] share similar principles, differing only in the application of various techniques such as utilizing public datasets, among others.\nThe third one represents architecture-based methods, allowing each client to possess personalized model structure. Approaches like FedPer [1] partition the shared neural network model into personalization layers and base layers (in Fig. 3). Only the base layers need to be uploaded to the server for global"}, {"title": "3. Proposed Method", "content": "In this section, the technical details of the proposed UA-PDFL would be discussed. The math formulation of personalized decentralized federated learning (PDFL) is given at first. Followed by the description of introduced unit representation, client-wise dropout and layer-wise personalization. Finally, the overall framework of UA-PDFL is illustrated at last."}, {"title": "3.1. Problem Description", "content": "Assume there are total M clients, each aiming to minimize its local training object $f_{m}(W_{m})$, we can formulate PDFL problem based on literature [32, 13] as an optimization process described by Eq. (2):\n$\\min _{w_{1: M} \\in \\mathbb{R}^{d}} F\\left(w_{1: M}\\right)=\\sum_{m=1}^{M} f_{m}\\left(w_{m} ; D_{m}\\right)=\\sum_{m=1}^{M} \\mathcal{L}_{(x, y) \\sim D_{m}}\\left(w_{m} ;(x, y)\\right)$,\nwhere $w_{m}$ denotes model parameters of client m, $F(W_{1:M})$ represents the global loss, which is the summation of local loss $f_{m}(w; D_{m})$, and $\\mathbb{R}^{d}$ denotes the space of model parameters. In summary, the goal of the PDFL is to obtain a set of model weights $W_{1:M}$ to maximize the learning performance on each local client data $D_{m}$.\nDue to the absence of central server, it is necessary for clients to maintain their own communication queue, denoted as $Q_{m}$, where $|Q_{m}| \\leq M$. And owing to the variations in computational capacities, the transmission of client model occurs asynchronously."}, {"title": "3.2. Unit Representation", "content": "In the realm of FL, researchers commonly rely on publicly available dataset to discern local data distribution for further client clustering [62, 4, 35]. However, in the absence of a central server, maintaining this functionality requires the transmission of public dataset among clients. This practice significantly increases the risk of data leakage due to the involvement of multiple entities in data handling. Furthermore, the reliance on peer-to-peer (P2P) model transmission exacerbates the demands on network bandwidth and availability, thereby posing additional challenges to the scalability and overall efficiency of FL systems. As a result, the straightforward application of such methodologies necessitates careful modification and consideration of these factors.\nDue to the fact that parametric models inherently encapsulate the information of the training data [57], it is unnecessary to rely on public dataset for identifying local data distributions. And we have observed a phenomenon in the sub-experiment wherein neural networks exhibit a similar characteristic of inertial thinking as the human brain. That is, for instance, when a neural network model is trained on a specific dataset, the model inferences for any unknown inputs tend to closely resemble the output labels of"}, {"title": "3.3. Client-wise Dropout", "content": "Dropout is a widely adopted technique for mitigating overfitting in neural networks [44]. Previous research work have indicated that dropout is mathematically equivalent to an approximation of a deep Gaussian process [8, 9]. This concept can be extended to DFL framework that each client user can be viewed as a neuron within the model layers. And according to Eq. (2), the model aggregation operation can be interpreted as a straightforward linear combination of the feedforward pass. Consequently, Eq. (2) can be modified to the following Eq. (8) by incorporating client-wise dropout:\n$\\mathcal{L}_{d r o p o u t}=\\sum_{m=1}^{M} f_{m}\\left(\\Theta_{m} ; D_{m}\\right)+\\lambda\\left\\|\\Theta\\right\\|_{2}^{2}$,\nwhere $\\Theta=\\left{\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{M}\\right}$ represents a set of $M$ connection weights. To approximate this model, the predictive distribution of Gaussian process for a new input $x^{*}$ on $\\Theta$ is given by Eq. (9):\n$p\\left(y^{*} \\mid x^{*}, X, Y\\right)=\\int p\\left(y^{*} \\mid x^{*}, \\Theta\\right) p(\\Theta \\mid X, Y) d \\Theta=\\int q(\\Theta) \\log p(Y \\mid X, \\Theta) d \\Theta-K L(q(\\Theta) \\| p(\\Theta)),$\nwhere $X \\in \\mathbb{R}^{N \\times Q}$ and $Y \\in \\mathbb{R}^{N \\times 1}$ are $N$ training inputs and labels, respectively. Maximizing the above predictive distribution is equivalent to minimizing $K L(q(\\Theta) \\| p(\\Theta))$. According to the theoretical analysis introduced in [8], Eq. (9) can be approximated into $\\mathcal{L}_{G P-M C}$ by adopting variational inference and Monte Carlo integration:\n$\\mathcal{L}_{G P-M C}=\\sum_{n=1}^{N} \\log p\\left(y_{n} \\mid x_{n}, \\Theta\\right)-K L(q(\\Theta) \\| p(\\Theta))=\\frac{1}{N} \\sum_{n=1}^{N} \\log p\\left(y_{n} \\mid x_{n}, \\theta_{n}\\right)+\\frac{P}{2 N_{T}}\\|\\Delta\\|^{2},$\nwhere $\\widehat{\\theta}_{n} \\sim q(\\Theta)$ is a random connection weight sampled from variational dis-"}, {"title": "3.4. Layer-wise Personalization", "content": "In transfer learning, to retain prior knowledge while adapting to a new domain, it is common to further decompose a neural network into a feature extractor $g(\\cdot)$ and a classifier $h(\\cdot)$ [16]. The concept of personalization layers in FL [1] operates on a similar principle as transfer learning but mantains a fixed number of shared layers. Meanwhile, in scenarios involving multiple participants, the distribution may vary distinctively, rendering it unsuited to share the same number of layers.\nMotivated by the methods discussed above, we propose a federated layer-wise personalization method, which emphasizes training an universal feature extractor and migrating similar classifiers, thereby adapting the the local personalization capability. This approach is particularly well-suited for DFL settings, where each client retains full control over its private model. The overall mechanism is illustrated in Fig. 6, where green planes represent the general feature extractor, and yellow and orange planes denote personalized classifiers. It is important to note that dissimilar classifiers (red plane) do not perform client model aggregation.\nTo guide the converge direction of the local feature extractor, obtaining the auxiliary representation is imperative. This can be achieved through Eq. (11):\n$I_{m}^{A u x}=g_{m}\\left(X_{\\text {unit }}\\right)$,\nwhere $g_{m}$ is the feature extractor on client m, and $I_{m}^{A u x}$ is the corresponding auxiliary representation. For each client, we propose to add a proximal term $\\left\\|I_{A u x}^{a v g}-I_{A u x}\\right\\|_{2}^{2}$ to the original loss function $\\mathcal{L}\\left(w_{m} ; D_{m}\\right)$ to suppress the impact from local updates, where $I_{a u x}^{a v g}$ is the average auxiliary representation computed by $C_{i} \\in Q_{m}$. To be more specifically, the local loss is then derived in Eq. (12):\n$f_{m}\\left(w_{m}\\right)=\\mathcal{L}\\left(w_{m} ; D_{m}\\right)+\\mu\\left\\|I_{A u x}^{a v g}-I_{A u x}\\right\\|_{2}^{2}$,\nwhere $\\mu$ is the mixture coefficient of the regularization term. In addition, to further enhance the classification performance, we fuses model parameters $w_{h_{i}}$ of similar classifiers $h(\\cdot)$ when $\\operatorname{Div}(m, i)$ is lower than the threshold $t h_{1}$.\nThe pseudo code for layer-wise personalization on client m is illustrated in Algorithm 1."}, {"title": "3.5. Overall Framework of UA-PDFL", "content": "By integrating the method of unit representation with client-wise dropout and layer-wise personalization, we present the overall procedure using pseudo code in Algorithm 2, where $C_{M}$ represents all the participating clients in UA-PDFL.\nAt beginning of the training procedure, each client $C_{i}$ initializes its model parameters $w_{i}^{0}$ and constructs its own communication queue $Q_{i}$ with $N_{c o m}$"}, {"title": "4. Convergence Analysis of UA-PDFL Framework", "content": "In this section, we present a convergence analysis of our proposed UA-PDFL framework, building upon the methodologies and proofs established in the previous work [30, 2]."}, {"title": "4.1. Assumptions", "content": "Assumption 1. Assume that each local loss function $f_{m}(w)$ is L-smooth and $\\mu$-strongly convex:\n$f_{m}\\left(w^{\\prime}\\right) \\leq f_{m}(w)+\\nabla f_{m}(w)^{T}\\left(w^{\\prime}-w\\right)+\\frac{L}{2}\\left\\|w^{\\prime}-w\\right\\|^{2}$\n$f_{m}\\left(w^{\\prime}\\right) \\geq f_{m}(w)+\\nabla f_{m}(w)^{T}\\left(w^{\\prime}-w\\right)+\\frac{\\mu}{2}\\left\\|w^{\\prime}-w\\right\\|^{2}$"}, {"title": "4.2. Gradient Update Rule", "content": "For M participating clients in DFL, the global objective is to minimize the weighted global loss:\n$F\\left(w_{1: M}\\right)=\\sum_{m=1}^{M} \\alpha_{m} f_{m}\\left(w_{m}\\right)$\nwhere $\\alpha_{m}$ is the aggregation coefficient for client m, satisfying $\\sum_{m=1}^{M} \\alpha_{m}=1$ and the global update rule for $F(W_{1:M})$ is:\n$w^{r+1}=w^{r}-\\eta \\nabla F\\left(w^{r}\\right)+\\eta \\xi^{r}$\nwhere $\\xi^{r}$ is the gradient noise introduced by data heterogeneity:\n$\\xi^{r}=\\sum_{m=1}^{M} \\alpha_{m}\\left(\\nabla f_{m}\\left(w_{w m}\\right)-\\nabla F\\left(w^{r}\\right)\\right)$\nBy applying client dropout to reduce the influence of similar clients, let the variance of unbiased $\\xi^{r}$ bounded as:\n$\\operatorname{Var}\\left(\\xi^{r}\\right)=\\mathbb{E}\\left[\\left\\|\\xi^{r}\\right\\|^{2}\\right] \\leq \\frac{\\sigma^{2}}{M}$\nFurthermore, the auxiliary term reduces divergence across local models, i.e., $\\left\\|I_{A u x}^{a v g}-I_{A u x}\\right\\|_{2}^{2} \\rightarrow 0$ as the training round $r \\rightarrow \\infty$. Therefore, we can omit this part in the following theoretical analysis."}, {"title": "4.3. Decomposing the Error Using Smoothness", "content": "According to the update $w^{r+1}-w^{r}=-\\eta \\nabla F\\left(w^{r}\\right)+\\eta \\xi^{r}$, we can easily get:\n$\\left\\|w^{r+1}-w^{r}\\right\\|^{2}=\\eta^{2}\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}-2 \\eta^{2} \\nabla F\\left(w^{r}\\right)^{\\prime} \\xi^{r}+\\eta^{2}\\left\\|\\xi^{r}\\right\\|^{2}$\nSubstituting $w^{\\prime}=w^{r+1}$ and $w=w^{r}$ into L-smooth assumption of Eq. (13) and taking the expectation on both sides of inequality:\n$\\mathbb{E}\\left[F\\left(w^{r+1}\\right)\\right] \\leq \\mathbb{E}\\left[F\\left(w^{r}\\right)\\right]+\\mathbb{E}\\left[\\nabla F\\left(w^{r}\\right)^{T}\\left(w^{r+1}-w^{r}\\right)\\right]+\\frac{L}{2} \\mathbb{E}\\left[\\left\\|w^{r+1}-w^{r}\\right\\|^{2}\\right]$\nThen, substituting the update rule into $\\mathbb{E}\\left[\\nabla F\\left(w^{r}\\right)^{T}\\left(w^{r+1}-w^{r}\\right)\\right]$, we have:\n$\\mathbb{E}\\left[\\nabla F\\left(w^{r}\\right)^{T}\\left(w^{r+1}-w^{r}\\right)\\right]=-\\eta \\mathbb{E}\\left[\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}\\right]-\\eta \\mathbb{E}\\left[\\nabla F\\left(w^{r}\\right)^{T} \\xi^{r}\\right]$\nIf $\\xi^{r}$ is unbiased and independent of $\\nabla F\\left(w^{r}\\right)$, we have $\\mathbb{E}\\left[\\nabla F\\left(w^{r}\\right)^{T} \\xi^{r}\\right]=0$, thus:\n$\\mathbb{E}\\left[\\nabla F\\left(w^{r}\\right)^{T}\\left(w^{r+1}-w^{r}\\right)\\right]=-\\eta \\mathbb{E}\\left[\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}\\right]$\nSimilarly, the expectation of $\\left\\|w^{r+1}-w^{r}\\right\\|^{2}$ is:\n$\\mathbb{E}\\left[\\left\\|w^{r+1}-w^{r}\\right\\|^{2}\\right]=\\eta^{2} \\mathbb{E}\\left[\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}\\right]+\\eta^{2} \\mathbb{E}\\left[\\left\\|\\xi^{r}\\right\\|^{2}\\right]$\nCombing Eq. (23) and Eq. (24) into the smoothness inequality, the following formula would be derived:\n$\\mathbb{E}\\left[F\\left(w^{r+1}\\right)\\right] \\leq \\mathbb{E}\\left[F\\left(w^{r}\\right)\\right]-\\eta \\mathbb{E}\\left[\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}\\right]+\\frac{\\eta^{2} L}{2} \\mathbb{E}\\left[\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}\\right]+\\frac{\\eta^{2} L}{2} \\mathbb{E}\\left[\\left\\|\\xi^{r}\\right\\|^{2}\\right] \\leq \\mathbb{E}\\left[F\\left(w^{r}\\right)\\right]-\\eta\\left(1-\\frac{\\eta L}{2}\\right) \\mathbb{E}\\left[\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}\\right]+\\frac{\\eta^{2} L \\sigma^{2}}{2 M}$"}, {"title": "4.4. Applying Strong Convexity", "content": "For the case of strong convexity satisfying $\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2} \\geq 2 \\mu\\left(F\\left(w^{r}\\right)-F\\left(w^{*}\\right)\\right)$ ( $w^{*}$ is the minimizer), and letting $\\delta^{r}=F\\left(w^{r}\\right)-F\\left(w^{*}\\right)$, we have:\n$\\mathbb{E}\\left[\\delta^{r+1}\\right] \\mathbb{E}\\left[\\delta^{r}\\right]-\\eta\\left(1-\\frac{\\eta L}{2}\\right) \\mathbb{E}\\left[\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}\\right]+\\frac{\\eta^{2} L \\sigma^{2}}{2 M} \\mathbb{E}\\left[\\delta^{r}\\right]-\\eta\\left(1-\\frac{\\eta L}{2}\\right) 2 \\mu \\mathbb{E}\\left[\\left\\|\\nabla F\\left(w^{r}\\right)\\right\\|^{2}\\right]+\\frac{\\eta^{2} L \\sigma^{2}}{2 M}$"}, {"title": "5. Experiments", "content": "To exemplify the viability and robustness of the proposed UA-PDFL framework, extensive experiments were conducted together with several state-of-the-art FL methods. Due to the scarcity of the PDFL method, we modified the communication protocol of some of them. In this section, we first compare the performance of UA-PDFL on image classification tasks across three different datasets. Subsequently, we present a case study to validate the effectiveness of standard representation. Finally, an ablation study is conducted."}, {"title": "5.1. Experimental Settings", "content": ""}, {"title": "5.1.1. Models", "content": "To simulate and evaluate FL system across various scales, three levels of neural network models are utilized. The first is a simple convolutional neural network (CNN) comprising two convolutional layers and two fully connected classification layers, which mirrors the structure of our previous work [62]. This CNN model is introduced to simulate a light-weight FL system. The second model is ResNet18 [14], known for its residual connections that address the vanishing gradient problem in deep neural networks. ResNet18 is adopted to simulate a FL system with relatively heavier computational costs. Lastly, we incorporate VGG11 [43], a model renowned for its heavy memory requirements due to its larger parameter count compared to the other two models. VGG11 is utilized to simulate a FL system with significant memory costs.\nTo further divide the models into a feature extractor $g(\\cdot)$ and a personalized classifier $h(\\cdot)$, we assume that all convolutional layers constitute $g(\\cdot)$,"}, {"title": "5.1.2. Datasets", "content": "To simulate tasks with varying difficulties, four benchmark image classification datasets, CIFAR10, CIFAR100 [20], and SVHN [37] along with a real-world dataset, PathMNIST [54] are adopted. CIFAR10 offers a moderately challenging training task with its ten distinct object classes. Conversely, CIFAR100 presents an exceptionally challenging task by encompassing 100 diverse object classes, thereby posing significant classification challenges. In contrast, SVHN represents a comparatively easier task, comprising digits ranging from 0 to 9, which are thus relatively straightforward to classify. Finally, PathMNIST comprises over 100,000 real-world images, representing 9 distinct categories of histological images of human colorectal cancer and healthy tissue. These images are sourced from various clinical centers, providing a diverse and comprehensive dataset for medical image analysis.\nTo simulate non-IID data reflecting the real-world scenarios, each client is assigned a proportion of training data for each label class based on the Dirichlet distribution [24]. The hyperparameter $\\beta \\sim \\operatorname{Dir}(\\beta)$ governs governs the degree of heterogeneity in the allocation. In our experiments, we use $\\beta=0.5$ and $\\beta=5$ to simulate highly non-IID and slightly non-IID data, respectively. A smaller $\\beta$ results in a more imbalanced data partition. To"}, {"title": "5.1.3. Algorithms Under Comparisons", "content": "In the experiments, we benchmark our proposed method against the following baseline approaches:\n*   Local: Training is locally performed without communication among clients.\n*   FedAvg [32]: The vanilla FL method conducts weighted averaging directly without any intermediate aggregation mechanisms.\n*   CriticalFL [53]: Adjust the number of client communications according to gradient information. This serves as the baseline for communication efficiency.\n*   FedPer [1]: The first to introduce personalized layers into FL, serving as a classical PFL baseline.\n*   DisPFL [5]: A recent state-of-the-art algorithm in PDFL utilizes decentralized sparse training to achieve a personalized model."}, {"title": "5.1.4. Other Settings", "content": "All other training setups for UA-PDFL are listed as follows:\n*   Total number of clients |C|: 30\n*   Total number of communication rounds R: 150\n*   Number of communicated clients Ncom: 5\n*   Initial learning rate $\\eta$: 0.05\n*   Leaning momentum: 0.5\n*   Learning rate decay: 0.95\n*   Training batch size: 50"}, {"title": "5.2. UA-PDFL VS Baseline Approaches", "content": "In this section, we verify the effectiveness of our proposed UA-PDFL across varying degrees of data heterogeneity. The learning performance of UA-PDFL is compared with the aforementioned five baseline approaches.\nThe final test accuracy averaged across participating clients on highly heterogeneous data ($\\beta=0.5$ ) are presented in Table 1 . It is evident that the proposed UA-PDFL outperforms other methods in most experiments, especially on CIFAR100 with an accuracy of approximate 40.03% for ResNet18 model. However, the performance of UA-PDFL is slightly lower than FedPer and DisPFL in a few cases, but by no more than 1%. In addition, it can be inferred that the incorporation of personalization layers can yield substantial performance enhancements, particularly in highly heterogeneous data environments. Except that, the real-time learning performance across"}, {"title": "5.3. The Number of Communicated Clients", "content": "Considering the critical importance of communication efficiency in DFL, this section focuses on evaluating the performance of the proposed algorithm in this context. By varying the number of communicating clients, $N_{c o m}$, different levels of communication overhead in DFL are simulated, allowing for an analysis of their impact on the model's final performance."}, {"title": "5.4. Simulation Results on the Real-World Dataset", "content": "In this section, we evaluate our proposed algorithm alongside baseline methods using a real-world dataset, PathMNIST [54]. The corresponding"}, {"title": "5.5. Ablation Study on Unit Representation", "content": "To further verify the effectiveness of the proposed unit representation, we conduct additional experiments outlined in Section 3.2. The results of divergence metric between two clients are shown in Fig. 12. It is evident that all the metrics undergo significant fluctuations at the early stage of the training rounds. Specifically, client 1 and 2, sharing identical data distributions, exhibit convergence towards a common direction, resulting in a relatively low divergence metric. Conversely, client 3 and 4, with a distinct data distribution, yields a comparatively higher divergence metric. Moreover, the divergence metric stabilizes around the 30th epoch.\nFurthermore, the impact of client-wise dropout and layer-wise personalization on learning performance, evaluated based on unit representation, is depicted in Fig. 13. To evaluate their individual contributions to UA-PDFL performance, these mechanisms are applied independently. For relatively non-IID data, both layer-wise personalization (Without CD) and client-wise dropout (Without LP) significantly enhance learning performance. Furthermore, the combination of these two mechanisms (Full UA-PDFL) achieves"}, {"title": "6. Conclusion", "content": "In this paper, we explore scenarios within the DFL paradigm, where the coordination of a central server is absent. To address the challenge of non-IID data prevalent in DFL, we introduce a novel Unit representation Aided Personalized DFL framework called UA-PDFL. Unlike approaches relying on centralized public data, UA-PDFL leverages unit representation to discern local data distributions, facilitating the measurement of divergence metrics among clients. Moreover, we employ a client-wise dropout mechanism to alleviate overfitting issues in cases of relatively IID client data. Additionally, a layer-wise personalization technique is proposed to guide the general feature extractor alongside a personalized classifier. This approach enables clients to enhance their feature extraction capabilities while retaining local classification biases.\nExtensive experiments have been conducted to compare our proposed UA-PDFL with five baseline DFL methods. The results showcase that our method yields learning performance that is superior or comparable to others, demonstrating its effectiveness across varying levels of data heterogeneity. This advantage can be attributed to the client-wise dropout mechanism,"}]}