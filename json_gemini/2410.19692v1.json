{"title": "AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions for Conversational Search with LLMs", "authors": ["Clemencia Siro", "Yifei Yuan", "Mohammad Aliannejadi", "Maarten de Rijke"], "abstract": "Generating diverse and effective clarifying questions is crucial for improving query understanding and retrieval performance in open-domain conversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration, and evaluation of Clarifying Questions), an end-to-end LLM-based framework addressing the challenges of scalability and adaptability faced by existing methods that rely on manual curation or template-based approaches. AGENT-CQ consists of two stages: a generation stage employing LLM prompting strategies to generate clarifying questions, and an evaluation stage (CrowdLLM) that simulates human crowdsourcing judgments using multiple LLM instances to assess generated questions and answers based on comprehensive quality metrics. Extensive experiments on the ClariQ dataset demonstrate CrowdLLM's effectiveness in evaluating question and answer quality. Human evaluation and CrowdLLM show that AGENT-CQ \u2013 generation stage, consistently outperforms baselines in various aspects of question and answer quality. In retrieval-based evaluation, LLM-generated questions significantly enhance retrieval effectiveness for both BM25 and cross-encoder models compared to human-generated questions. We will make publicly available the data, labels and prompts used by AGENT-CQ framework.", "sections": [{"title": "1 Introduction", "content": "Conversational search (CS) systems have gained significant attention in recent years, offering users a more natural and interactive way to find information than single-shot search interactions. To resolve the ambiguity inherent in user queries, these systems may ask users clarifying questions. Generating diverse and effective clarifying questions is crucial for improving query understanding and retrieval performance, which remains a challenge. Existing methods for generating clarifying questions in CS systems rely on manual curation by experts and template-based approaches: human experts craft clarifying questions, while using their ability to intuitively understand complex user intents and contextual nuances. While this method ensures high relevance and accuracy, it poses challenges for scalability in large-scale applications. Moreover, human curators do not necessarily have deep knowledge about the topic of a conversation. In contrast, template-based methods employ pre-defined templates to automate the generation of clarifying questions, significantly enhancing scalability and efficiency. However, these methods often lack flexibility, leading to generic or less-diverse questions that could hurt the overall user interaction experience. Recently, large language models (LLMs) have"}, {"title": "2 AGENT-CQ Framework", "content": "We introduce the two key stages of AGENT-CQ: a framework for generating clarifying questions and a framework for evaluation."}, {"title": "2.1 AGENT-CQ: Generation framework", "content": "AGENT-CQ's generation framework generates and scores clarifying questions using state-of-the-art LLMs in an end-to-end manner. The framework has three main phases; see Figure 1 (top)."}, {"title": "2.1.1 Question generation (Phase 1)", "content": "Let $Q = {q_1, q_2, ..., q_n }$ be a set of n initial user queries. For each $q_i \u2208 Q$, we aim to generate a set of clarifying questions $C_i = {C_{i1}, C_{i2},..., C_{im} }$, where m is the number of clarifying questions per set. We define a question generation function:\n$G:qi \\rightarrow {C_i^1,C_i^2,...,C_i^k}$ (1)\nthat generates k sets of clarifying questions for query $q_i$. We explore two prompt-based approaches (i.e., p = 2).\nFacet-based approach. We adopt the approach of diverse query interpretation based on Aliannejadi et al. (2021b), aiming to generate clarifying questions that address multiple interpretations of a given query. We introduce the facet-based method. Here, an LLM takes a query as input, then generates facets as a way of exploring the topic of the query and finally generates a clarifying from a query-facet pair. Algorithm 1 (Appendix A.1) details the implementation of this approach.\nTemperature-variation-based approach. This method generates diverse clarifying questions by systematically adjusting an LLM's temperature parameter. Starting from a low temperature and incrementing it over multiple iterations, it produces"}, {"title": "2.1.2 Question filtering (Phase 2)", "content": "In this phase, we take two major characteristics of questions to reject low-quality questions. Preliminary experiments indicated that LLMs can sometimes generate questions that are not actually clarifying questions, or are not on the same topic as the user query. We define a function S to filter out questions based on relevance and clarification potential:\n$S(q_i, C) = \\alpha \\cdot R(q_i, C) + (1 - \\alpha) \\cdot L(C),$ (2)\nwhere $R(q_i, C)$ is the relevance score; $L(C)$ is the clarification score, evaluating the questions' potential to clarify user intent; and \u03b1 is a weighting parameter. We keep the top 10 ranked questions for each query in the collection for each LLM and experimental setup."}, {"title": "2.1.3 User response simulation (Phase 3)", "content": "In phase 3 of AGENT-CQ's generation stage, we simulate user responses to the ranked clarifying questions from Phase 2. Recent work has demonstrated the efficacy of simulated users as cost-effective proxies for real users in conversational systems. Using this insight, we employ an approach that takes LLM as a simulator for generating answers to system-generated clarifying questions. We introduce a parameterized-user simulation approach, inspired by Sekulic et al. (2024). This method incorporates user characteristics (U) in the simulation, to generate diverse and realistic answers (details see Algorithm 3 Appendix A.2). Our parameterized function is defined as:\n$A_{ij} = A_p(q_i, U_i, C_{ij}, U),$ (3)\nwhere $q_i$ is the original query, $u_i$ is the user information need, $C_{ij}$ is the clarifying question, and U is the set of user characteristics. $A_p$ extends the basic non-parameterized method by incorporating user characteristics U, primarily verbosity, which controls the response length, detail and revealment probability used to determine the likelihood of disclosing the true user information need."}, {"title": "2.2 AGENT-CQ: Evaluation framework", "content": "Next, we detail CrowdLLM, AGENT-CQ's evaluation framework. CrowdLLM is a multi-LLM and multi-dimensional framework evaluating the generated questions and simulated responses using the scalability of LLMs to simulate a crowd of evaluators, with validation from human experts.\nMulti-LLM evaluation. CrowdLLM employs an LLM-as-a-judge approach, in an ensemble of LLM instances with varying temperature settings. We hypothesize that with varying temperatures different LLM instances will bring different angles to the evaluation. This design simulates the setup used with crowdsourced workers, crucial for comprehensive assessment in NLP tasks. Each LLM instance evaluates questions and answers on multiple aspects using a 10-point scale for questions and pairwise comparison between LLM and human answers. To validate CrowdLLM's performance, we incorporate human expert assessment.\nEvaluation metrics. Evaluation in CrowdLLM is based on distinct sets of metrics for clarifying questions and simulated answers, drawn from prior work on conversational information seeking and general conversational systems. For clarifying questions, we assess clarification potential, on-topic relevance, specificity, usefulness, clarity, and question complexity. Simulated answers are evaluated on relevance, usefulness, naturalness, and overall quality.\nDetails about our experimental setup, implementation details and prompts used are included in Appendix A.4 and B."}, {"title": "3 Reliability of CrowdLLM", "content": "In this section, we study the reliability of our evaluation framework (CrowdLLM) from multiple angles."}, {"title": "3.1 Clarifying questions", "content": "In Table 1, we report the inter-annotator agreement (IAA) among the LLM instances (i.e., GPT-40) using intraclass correlation coefficient (ICC) and weighted K. We further assess the agreement of CrowdLLM with human evaluators to inform on the quality of the evaluations. Because"}, {"title": "3.2 Simulated answers", "content": "We assessed CrowdLLM's reliability in evaluating generated answers, comparing it with human evaluators (H-Eval). Evaluators judged which answer in presented pairs was better for naturalness, relevance, usefulness, and overall quality, with an option to rate them equal. Table 2 presents inter-rater reliability results for this comparative evaluation.\nInternal agreement. CrowdLLM shows high internal consistency. Naturalness has highest agreement (\u043a = 0.81, 89% agreement), indicating near-perfect consensus. Relevance (\u043a = 0.68, 86% Ag.) and overall quality (k = 0.71, 79% Ag.) show substantial agreement. Usefulness, while substantial, has lowest scores (k = 0.62, 73% Ag.).\nExternal agreement. CrowdLLM and human evaluators align strongly in overall quality (75%) and relevance (73%). Usefulness agreement was moderate (68%). Siro et al. (2024a) showed humans better evaluate usefulness of system re-"}, {"title": "3.3 Effectiveness of evaluation aspects", "content": "Which aspects most influence perceived quality? Table 3 shows correlations between question aspects and overall quality. Usefulness has the strongest association (\u03c4 = 0.80, \u03c1 = 0.90), followed by clarification (r = 0.76, p = 0.87), clarity (r = 0.75, p = 0.85), and on-topic relevance (\u03c4 = 0.71, \u03c1 = 0.81). Specificity shows moderate to strong correlation (r = 0.63, \u03c1 = 0.73); query-complexity has negligible impact (r = 0.07, p = 0.08). Thus, usefulness, clarification, clarity, and topical relevance are key determinants of perceived question quality in CrowdLLM evaluation.\nFigure 3 shows Spearman's p correlations between answer aspects and overall quality. Usefulness (p = 0.76) and relevance (p = 0.72) correlate strongest with overall quality, indicating their critical role in perceived answer quality. Naturalness shows moderate correlation (p = 0.50), suggesting less impact. Strong correlations between relevance and usefulness (p = 0.70) highlights their interconnectedness in high-quality answers. Naturalness correlates less with relevance (p = 0.49) and usefulness (p = 0.39); it captures a distinct dimension"}, {"title": "4 Evaluation of Generated Clarifying Questions", "content": "We perform a comprehensive analysis to assess the quality of data generated by AGENT-CQ."}, {"title": "4.1 Clarifying question evaluation", "content": "We conduct an analysis to explore the characteristics of the generated clarifying questions. We focus on three areas: identifying recurring question patterns, categorizing questions based on their intent, and classifying the expected response types. We employ a hierarchical matching system to analyze linguistic patterns and key phrases. Categorization uses LLMs to capture subtle distinctions between types such as disambiguation and information seeking as shown in Table 8 (Appendix D.1). We classify response types into Yes/No, Multiple Choice, Open-ended, and Factual using a rule-based approach. Detailed analysis techniques in Appendix D.1.\nQuestion length and readability analysis. Table 5 shows that human questions are concise (9.71 words) and simple (5th-grade level). LLM outputs vary: GPT-Facet generates complex, lengthy questions (college-level, 23.53 words), Llama 3.1 generates variable-length high school-level questions, and GPT-baseline closely matches human question length but with higher complexity. There is a consistent gap in LLMs' ability to replicate the brevity and simplicity of human-written questions.\nQuestion categories. Table 4 shows that all models except Llama 3.1 favor preference identification questions, with GPT-Facet leading at"}, {"title": "4.2 Quality analysis of clarifying questions using CrowdLLM", "content": "We assess clarifying questions from different models and prompting strategies across seven aspects: clarification, on-topic, specificity, usefulness, clarity, query complexity, and overall quality. Figure 4 shows mean scores across all aspects per model. We use one-way ANOVA with post-hoc Tukey's HSD for statistical analysis (p < 0.05). Detailed explanations and test suitability are in Appendix D.3.\nGPT-Temp consistently outperforms other approaches across most aspects (Figure 4), surpassing GPT-Baseline in usefulness (mean difference = 3.781, p < 0.001). Facet-based models (GPT-Facet and Llama 3.1) show improvements over the baseline, with GPT-Facet often ranking second. GPT-Facet excels in specificity, significantly outperforming GPT-Baseline, as it generates specific facets before producing targeted clarifying questions. Thus, facet-based approaches enhance specificity but generate more complex questions (GPT-Facet: 3.5) than GPT-Temp and human-generated questions (both 1.9), aligning with our Fleisch readability and Kincaid analysis.\nHuman-generated questions score lowest across most aspects, except for complexity. GPT-Temp significantly outperforms human questions in usefulness (8.4 vs. 4.2, p < 0.001), challenging assumptions about human expertise in question formulation. LLMs' superior performance can be attributed to their knowledge and consistent optimization for specific criteria. Recent research shows LLMs favor their own content; our human evaluators corroborate CrowdLLM's results by also ranking human questions as least helpful. This alignment between human and LLM evaluations validates our conclusions and suggests that CrowdLLM is not biased in this case.\nIn summary, LLM-generated clarifying questions, particularly from GPT-Temp, outperform human-generated ones across most quality aspects."}, {"title": "4.3 Evaluation of simulated answers", "content": "We conducted pairwise comparisons of 200 answer pairs across four aspects: relevance, usefulness, naturalness, and overall quality. Each pair was evaluated by three human workers and three CrowdLLM instances. We define a win for a model when at least two out of three (human or LLM) evaluators agree that the model's answer is superior; if the majority rates the answers as equal, we have a tie.\nLLM responses are longer (mean 13.21 vs. 8.19 words) and more variable (std dev. 8.06 vs. 4.36) than human-generated ones. Table 6 shows that LLM answers perform comparably to human answers in relevance and usefulness, demonstrating our approach's success in generating contextually appropriate and valuable responses. Naturalness assessments yield intriguing results: human evaluators slightly favor LLM answers (34% vs. 32%), while CrowdLLM shows a stronger preference (55.16% vs. 37.74%), suggesting potential bias in automated evaluation systems. Overall quality marginally favors LLM answers with statistical significance, contrasting with previous non-parametric"}, {"title": "5 Retrieval Performance Comparison", "content": "Following Aliannejadi et al. (2021c), we evaluate the impact of clarifying questions on document retrieval performance. Our methodology simulates a typical conversational search scenario: a user initiates a search, the system poses a clarifying question, and the user provides an answer. The retrieval system then uses this additional information to retrieve an updated set of documents. We hypothesize that higher-quality clarifying questions and their corresponding answers lead to improved post-QA retrieval performance.\nTable 7 presents the retrieval performance using BM25 and BERT models with clarifying questions and their corresponding answers. GPT-Tempgenerated clarifying questions significantly enhance retrieval effectiveness for both the BM25 and BERT models. For BM25, GPT-Temp questions achieve the highest NDCG@1 (0.225), demonstrating superior performance in top-rank retrieval. In BERT-based retrieval, GPT-Temp leads with NDCG@1 of 0.312 and NDCG@5 of 0.296. This performance aligns with our earlier findings, where both evaluators ranked GPT-Temp questions as the most helpful; its ability to generate precise, contextually relevant questions directly translates to improved retrieval outcomes.\nHuman-generated questions with human answers perform better in BM25 retrieval at NDCG@5 (0.221) and NDCG@10 (0.246). This effectiveness likely stems from two factors: humans' tendency to use terms overlapping with the original query, enhancing lexical matching, and the overall quality"}, {"title": "6 Related Work", "content": "LLM-based CS systems. Conversational search is an interactive paradigm where users engage in a dialogue with a search system. In conversational search systems, LLMs enhance the search experience through query understanding, retrieving relevant documents, and generating clear responses. LLMs have been adopted to simulate users and their interactions with the system, reducing the need for human resources. E.g., Abbasiantaeb et al. (2024) simulate teacher-student interactions in a conversational setting. Sekulic et al. (2024) focus on evaluating query clarification via an LLM-based user simulator.\nOur work focuses on query clarification, i.e., the process of refining or elaborating on a user's initial search query or question to better understand their intent. Prior work on the role of clarifying questions in conversational search recognizes their potential to enhance search quality and the user experience. How LLMs can benefit the task remains underexplored. Our work evaluates LLMs on query clarification at the clarifying question and user response levels, assessing their ability to generate effective questions and responses.\nEvaluation of generated content. Traditional automated metrics for evaluating generated content like BLEU and ROUGE often correlate poorly with human judgments for open-ended text generation. Human evaluation is the gold standard but time-consuming and costly to scale. Newer metrics (USR, BLEURT, BERTScore) use pretrained language models to improve correlation with human judgments. Multi-dimensional evaluation frameworks have also emerged. Recent research explores using LLMs as evaluators for natural language tasks. Zheng et al. (2023) explore \"LLM-as-a-judge\u201d for chat assistants. Lin and Chen (2023) introduce LLM-Eval, a multi-dimensional evaluation method for open-domain conversations.\nCrowdLLM shares similarities with recent LLM-based evaluation techniques and multi-dimensional frameworks. However, it uniquely employs multiple LLM instances to simulate diverse evaluators, addressing scalability issues of human evaluation. CrowdLLM also incorporates a second tier where humans assess the reliability of LLM-generated evaluations, combining automated efficiency with human judgment accuracy."}, {"title": "7 Conclusion", "content": "We introduce AGENT-CQ, a framework for generating and evaluating clarifying questions and answers in CS systems. Our study reveals that GPT-Temp consistently outperforms other methods in generating high-quality clarifying questions. Surprisingly, human-generated questions, despite lower quality ratings, excelled in term-based retrieval at NDCG@5 and 10. LLM-simulated answers, while matching human answers in quality assessments, underperformed in retrieval tasks when paired with human questions. CrowdLLM, our evaluation framework, showed general alignment with expert assessments but demonstrated potential biases towards LLM-generated content. Future work should explore on enhancing LLMs' ability to generate retrieval-effective questions and answers, improving the integration of LLMgenerated content with existing retrieval models."}, {"title": "Limitations", "content": "Our study, using the ClariQ dataset, demonstrates the potential of LLMs in generating effective clarifying questions, but has several limitations. While we employed various LLM models and prompting strategies to broaden our question range, the dataset may not fully represent real-world query diversity, potentially limiting result generalizability. The nature of LLMs challenges full understanding of their decision-making process, which we addressed through detailed analyses of question patterns, response types, and quality aspects. Additionally, while we've made significant efforts in prompt design, the effectiveness of our method still inherently relies on the quality of the prompts for both generation and filtering. Moreover, LLMs may not consistently capture all relevant query aspects, especially for complex or niche topics."}, {"title": "Ethical Considerations", "content": "We carefully considered ethical implications throughout our study. To address potential biases in LLMs, we compared their outputs with human-generated questions from the ClariQ dataset and conducted thorough evaluations using both CrowdLLM and human assessors. This approach helped identify and mitigate potential unfairness in question generation across different topics. We prioritized privacy by using only publicly available datasets and ensuring that no personally identifiable information was processed or generated. In our human evaluation process, we ensured fair compensation, adhering to ethical guidelines for crowdsourced work.\nWhile our study shows LLMs' superior performance in many aspects of question generation, we recognize that there are critical domains where they may not be applicable, emphasizing the continued importance of human involvement. This is crucial in fields requiring specialized knowledge, ethical decision-making, or handling sensitive information, where human expertise remains irreplaceable.\nTo address these limitations and ethical concerns, future work should focus on several key areas. Domain-specific adaptation is crucial, exploring how our approach can be tailored for specialized fields requiring expert knowledge. This could involve developing domain-adapted LLMs or targeted prompts for more precise clarifying questions in specific contexts. Further research is needed to identify and mitigate biases in LLM-generated"}, {"title": "A Additional Methodology Details", "content": "In this section we give additional details on the implementation of AGENT-CQ."}, {"title": "A.1 Clarifying Question Generation Algorithms", "content": "The two alternative methods used for clarifying question generation in Phase 1 of AGENT-CQ are detailed in Algorithm 1 and 2."}, {"title": "A.2 User Response Simulation Algorithm", "content": "Algorithm 3 describes the parameterized answer simulation approach by LLM.\nThe ConstructParameterizedPrompt function generates a structured prompt by incorporating the original query $q_i$, user information need $u_i$, and clarifying question $c_{ij}$, along with verbosity level and reveal probability randomly selected from the user characteristics set U. This approach generates a wider range of responses, better reflecting real-world user behavior diversity. It also provides a richer dataset for training and evaluating conversational search systems, enabling systematic study of user characteristics' impact on system performance."}, {"title": "A.3 CrowdLLM Question and Answer Evaluation Metrics", "content": "Question quality metrics. CrowdLLM evaluated the quality of the generated clarifying questions from a multidimensional perspective capturing the following quality aspects:\n1. Clarification: Assesses how well the question seeks to understand the original query without introducing unrelated topics.\n2. On-topic: Measures the question's direct relation to the subject matter of the original query.\n3. Specificity: Evaluates the question's focus on particular aspects of the query rather than being general.\n4. Usefulness: Gauges how much answering the question would improve the response to the original query.\n5. Clarity: This measure evaluates how easily understood and unambiguous the clarifying question is from the user's perspective.\n6. Question complexity: This aspect examines whether the clarifying question introduces technical terms, specialized concepts, or requires domain-specific knowledge not present in the original query.\n7. Overall quality: Assesses the overall quality of the question based on the above metrics\nAnswer quality metrics. Similar to questions answers we also evaluated from a multidimensional perspective on the following three metrics and overall quality.\n1. Relevance: How directly the user's answer addresses the system's clarifying question.\n2. Usefulness: The value of the user's answer in clarifying their original information need.\n3. Naturalness: The human-like quality and conversational tone of the user's response."}, {"title": "A.4 Implementation Details", "content": "A.4.1 Dataset\nWe use an existing question clarification dataset, named ClariQ. ClariQ is one of the most widely-used question clarification dataset and aligns well with our setting. Each data sample in ClariQ includes a topic originated from the TREC Web Track 2009\u20132012 that represents an initial user query. These topics can be further divided into multiple facets that capture the user's true intent. For each facet, a set of manually collected clarifying questions are provided which helps the system better understand the underlying user intention. Subsequently, user responses are collected for each clarifying question, providing insights to the corresponding facet. Notably, the ground-truth retrieved documents are also attached given each topic-facet pair. Specifically, in our setting, we reuse the queries from ClariQ but prompt LLMs to generate diverse clarifying questions and simulate user responses. This diversification allows us to better simulate real-world scenarios where users may have different perspectives or require more specific information. Overall, it consists 198 topics with 891 different facets and over 8k questions, with 9.49 terms on average per question.\nA.4.2 Models\nFor our experiments we use GPT and Llama models in our framework. For question generation, we primarily employ GPT-based models. In the facetbased method, we use a hybrid approach: GPT-3.5 generates query facets, which are then fed to Llama for question generation, as Llama alone was ineffective in facet generation. The same generation model is used in the filtering stage to evaluate and select the most appropriate questions. We used the 8B variant of Llama-3.1. For simulating user responses, we rely on GPT-3.5 due to its versatility in generating diverse and contextually appropriate answers. Our CrowdLLM evaluation framework uses GPT-4o as the base model, leveraging its advanced capabilities for assessing question and answer quality.\nA.4.3 Hyperparameters\nOur framework employs various hyperparameters, carefully chosen to balance performance and diversity:\n1. Question generation:\nTemperature variation: We use temperatures ranging from 0.5 to 0.9, incrementing by 0.1. We set n_sets = 3.\nFacet-based approach: Temperature is set to 0.7, top_p = 0.95, for Llama: top_k = 50 and max_length = 1024.\nBaseline: A fixed temperature of 0.7 is used to generate 10 questions for each query.\n2. Question filtering:\nWe set \u03b1 = 0.4 in the filtering stage to balance relevance and clarification potential of the selected questions.\nTemperature is set to 0.7.\n3. User simulation:\nVerbosity: 10\u201360 tokens\nCooperativeness: reveal probabilities 0.0-0.9\nAnswer generation: temperature = 0.7, top_p = 0.98, frequency_penalty = 0.5, presence_penalty = 0.2\nThese parameters simulate diverse user behaviors while maintaining coherent responses.\n4. CrowdLLM evaluation: We use three GPT-4 instances to simulate diverse human judgments:\nConservative judge (temperature 0.2): Produces predictable, focused judgments, simulating a strict evaluator.\nBalanced judge (temperature 0.5): Provides a mix of creativity and focus, representing a typical evaluator.\nCreative judge (temperature 0.7): Generates more exploratory judgments, simulating a lenient evaluator.\nThe selection of these hyperparameters was based on: Extensive experimentation with various setups to optimize performance. Analysis of output quality and diversity across different parameter combinations. Alignment with observed patterns in human evaluation behaviors from prior crowdsourcing studies."}, {"title": "B Prompts", "content": "In this section, we list the prompts used in different prompting strategies and stages of AGENT-CQ.\nB.0.1 Facet-based Prompt\nFor the user query: '{query}'\nGenerate a list of 40 diverse facets that this query might be addressing.\nThis query represents multiple user information needs. Generate diverse facets to capture these varied needs.\nEnsure each facet is unique and explores different aspects or interpretations of the query. Avoid repetition and strive for a wide range of perspectives in your facets.\nFor the user query: '{query}'\nAnd considering this specific facet: '{facet}'\nGenerate a clarifying question that addresses this facet and helps to better understand the user's specific information need.\nUse diverse language and question structure to formulate the questions.\nB.1 Temperature-variation prompt\nfor i in range(n_sets):\nFor the user query: '{query}'\nGenerate a set of 10 clarifying questions.\nThe goal is to better understand the user's specific information need.\nThis query represents multiple user information needs. Generate diverse clarifying questions to capture these varied needs.\nEnsure each question is unique and explores different aspects or interpretations of the query. Avoid repetition and strive for a wide range of perspectives in your questions.\nIMPORTANT GUIDELINES:\nEach question should aim to clarify a different aspect of the user's intent or information need.\nEnsure all questions are unique. Do not repeat questions.\nFocus on questions that will help narrow down or specify the user's request.\nConsider potential ambiguities or multiple interpretations of the query.\nB.2 Scoring and filtering prompt\nEvaluate the following question for the user query: '{query}'\nQuestion: \"{question}\"\nConsider these aspects:\nClarification: How well does this question help to better understand the user's original query?\nOn Topic: To what degree does this question directly relate to the subject matter of the user's original query?\nProvide a score (0-10) for each aspect and a brief explanation.\nB.2.1 User response simulation prompt\nYou are a user who initially made this request: '{query}'.\nYour actual information need is: '{facet}'.\nRespond to the clarifying question based on this information need.\nYour verbosity level is {verbosity_level}.\nYour reveal probability is {reveal_probability:.2f}.\nKeep your response short, ideally under {verbosity[\"max_tokens\"]} tokens.\nRemember: Your answer should not include any additional information that is not part of your actual information need ('{facet}').\nB.3 CrowdLLM prompt\nBelow is an example of CrowdLLM prompt for question complexity. Other metrics follow the same prompt except for the definition of the metric. Each metric is evaluated independently to avoid bias from previous metric rating. Overall quality followed a slightly different approach, apart from having access to the query and system clarifying question, it also included the ratings from the other six metrics in order to ground the overall quality on these metrics.\nAs a user, you are evaluating the complexity of the system's clarifying question in relation to your original query.\nDefinition:\nQuestion Complexity: The degree to which the clarifying question introduces technical terms, specialized concepts, or requires domain-specific knowledge not present in the original query.\nScale:\n1-10, where 1 is very simple (uses only general terms and concepts) and 10 is highly complex (introduces specialized terminology or concepts).\nYour original query: \"{original_query}\"\nSystem's clarifying question:\n\"{system_question}\"\nEvaluate the complexity of the system's question compared to your original query. Consider:\nDoes it introduce technical terms or jargon not present in the original query?\nDoes it require specialized knowledge that might not be evident from the original query?"}, {"title": "C Human Evaluation", "content": "To quantify the effectiveness of our evaluation framework (CrowdLLM) we conducted human evaluation to assess both the question and answer quality. We employed the so called Master crowdworkers from Amazon Mechanical Turk from the US with an approval rate of more 95% in over 10000 HITS. Each HIT was done by 3 workers and they were paid $8.5 per hour.\nDifferent from CrowdLLM which evaluated each question on six dimensions then gave overall quality, humans assessed the questions based on preference. This is because of the large number of the questions and associated costs. In each HIT a worker was shown the initial user request and five generated clarifying questions for the query from each system: Llama 3.1, GPT-Facet, GPT-Temp, GPT-Baseline and Human question. They were tasked to rank the questions from the most helpful (Rank 1) to the least helpful (Rank 5) using a drag and drop option. To avoid position bias, where a system's question is always placed at the top, we uniformly randomized the order of the questions at each HIT so that each system question was placed at the top in 20% of the HITS. A total of 1000 questions were assessed, 200 from each system.\nSimilar to CrowdLLM, humans assessed a pair of answers on three dimensions: Relevance, use-"}, {"title": "D Supplementary Results and Analyses", "content": "D.1 Analyses\nQuestion categories We developed a classification framework for clarifying questions based on Zamani et al. (2020a) and Braslavski et al. (2017).\nTable 8 presents each question type with descriptions and examples, including user questions (UQ) and corresponding clarifying questions (CQ) to illustrate their application in real conversations. This taxonomy provides a robust framework for comparing clarification strategies across various LLMs in conversational information seeking.\nInitial rule-based classification attempts proved inadequate for capturing nuances. For instance, \"Did you mean the book or the movie?\" could be categorized as disambiguation or information gathering, depending on context. To address this, we employed GPT-3.5 for categorization, leveraging its context-awareness to select the most appropriate category. This approach enabled more accurate classification, especially for questions requiring nuanced interpretation or potentially fitting multiple categories.\nQuestion patterns We developed a systematic approach to identify and classify question patterns using a hierarchical matching system. This process analyzes linguistic structure and key phrases, starting with primary question words (e.g., \u201cWhat\u201d, \u201cHow\u201d, \u201cAre you\u201d) and then examining subsequent"}]}