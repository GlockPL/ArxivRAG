{"title": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios", "authors": ["Junchao Wu", "Runzhe Zhan", "Derek F. Wong", "Shu Yang", "Xinyi Yang", "Yulin Yuan", "Lidia S. Chao"], "abstract": "Detecting text generated by large language models (LLMs) is of great recent inter- est. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world ap- plications remains underexplored. In this study, we present a new benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating advanced prompt usages, human revisions like word substitutions, and writing errors. Our development of DetectRL reveals the strengths and limitations of current SOTA detectors. More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe DetectRL could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors.", "sections": [{"title": "Introduction", "content": "Detecting text generated by LLMs is a challenging task. It is often more difficult for humans than for detection techniques to identify LLM-generated text, as humans typically underperform detection methods designed for this purpose [1]. Recently, the implications of LLM-generated content have come into focus, highlighting their significant societal and academic impacts and associated risks [2, 3]. The main concerns stem from the hallucinations and misuse of LLMs [4], leading to issues such as plagiarism [5], the spread of fake news [6], and challenges to educators and human scholarship in AI-assisted academic writing [7]. Previous and current popular detection benchmarks, such as TuringBench [8], MGTBench [9], MULTITUDE [10], MAGE [11] and M4 [12], have primarily focused on evaluating detectors' performance across various domains, generative models, and languages by constructing idealized test data. However, they have overlooked the assessment of detectors' capabilities in more common scenarios encountered in practical applications [4], such as various prompt usages and human revisions, as shown in Table 1.\nIn this paper, we study the following questions: (1) How do SOTA LLM-generated text detectors perform in real-world application scenarios? (2) What real-world factors influence detector"}, {"title": "DetectRL", "content": "Previous datasets were mainly constructed by directly collecting human-written texts and those generated by LLMs using the same questions or prompt prefixes. This approach assumes an ideal detection environment and overlooks critical design considerations such as application domains, generative models, potential attacks, and text lengths. We improve the current dataset construction approach to better align with real-world detection scenarios. In this section, we introduce DetectRL, a new benchmark designed to facilitate such assessments, with its overall framework shown in Figure 1."}, {"title": "Framework", "content": "Data sources DetectRL is a comprehensive benchmark consisting of academic abstracts from the arXiv Archive, covering the years 2002 to 2017. It also includes news articles from the XSum dataset [17], creative stories from Writing Prompts [18], and social reviews from Yelp Reviews [19]. The texts generated by LLMs within these domains are considered to pose higher risk of misleading content when misused, which underscores the importance of effective detection strategies. We extracted 2,800 samples per dataset as human-written texts. To avoid the potential contamination from text generated by LLMs, all selected data was released prior to the advent of ChatGPT.\nModels Based on the collected human-written texts, we selected several LLMs that widely used in real-world, including GPT-3.5-turbo [13], PaLM-2-bison [14], Claude-instant [15], and Llama-2- 70b [16], to perform text generation tasks. These models are mostly black-box and require substantial computational resources, making white-box detection methods challenging. We obtain text samples generated by these LLMs through interactive sessions with each model. For more details on the LLMs and text generation settings, please refer to Appendix D.\nData generation We employed various attack methods to simulate complex real-world detection scenarios. Following the classifications from the studies by [4] and [20], we categorized our attack methods into prompt attacks, paraphrase attacks, and perturbation attacks. Additionally, we treated data mixing as a separate scenario in our study. Please see Appendix D.4 for implementation details.\nPrompt attacks are intended to use carefully designed prompts to guide LLMs in generating text that closely mimics human writing style. Our employed prompt attacks include Few-Shot Prompting [21] and ICO Prompting, which is part of SICO Prompting [22].\nParaphrase attacks have been extensively studied in recent research on LLM-generated text detection [23], focusing on rewriting text while maintaining its original meaning. Alongside using the DIPPER- paraphraser [23], we also employed Back-translation via Google Translate and Polishing using LLMs, which are two paraphrasing methods commonly utilized in everyday scenarios.\nPerturbation attacks mainly involve introducing adversarial perturbations on text directly generated by LLMs. These attacks can effectively simulate common writing errors, word substitutions or other noises in real-world applications. We utilize TextFooler [24] for Word-level Perturbations, DeepWord-"}, {"title": "Task definition", "content": "Based on the meticulously curated dataset, we manifest the DetectRL framework into four distinct tasks for LLM-generated text detectors assessment, described as follows:\nTask 1: In-domain robustness assessment: multi-domain, multi-LLM, and multi-attack assess- ment. This task aims to evaluate the foundational performance of detectors in different domains, generators, and attack strategies, focusing specifically on their in-domain performance in various real-world scenarios. We use the average performance score as the metric for foundational in-domain capabilities assessment.\nTask 2: Generalization assessment. This task assesses the generalization of detectors from three perspectives: domain, LLM, and attack, to determine their effectiveness in diverse scenarios. Unlike Task 1, this task emphasizes the detector's ability to handle out-of-distribution samples. For example, we evaluate the performance of detectors trained on texts from one domain when applied to texts from different domains to determine their generalization score across domains. The same approach is used to assess generalization across different LLMs and attack strategies."}, {"title": "Benchmark statistics", "content": "The statistics for the collected data are presented in Appendix Table 9. This dataset includes a total of 100,800 human-written samples, with 11,200 being raw samples and 89,600 modified through attack manipulations. Additionally, there are 134,400 samples generated by LLMs, categorized as follows: 11,200 with direct prompt, 22,400 with prompt attacks, 33,600 with paraphrase attacks, 33,600 with perturbation attacks, and 22,400 with data mixing. For the assessment task we designed, we extracted relevant data from this dataset to develop the DetectRL benchmark. Detailed statistics for each task in the benchmark are presented in Figure 2. Samples were carefully selected from the constructed dataset to align with task design, ensuring a balance across domains, LLMs, and attacks. The training data was specifically tailored for both supervised and zero-shot detectors, and performance was evaluated using common test sets. The statistics of the textual features of DetectRL samples are illustrated in Figure 2. For a more detailed analysis, please refer to Appendix subsection D.6."}, {"title": "Evaluation metrics", "content": "We employ AUROC and F\u2081 Score as the main evaluation metrics. AUROC is widely used for assessing zero-shot detection methods [27] because it considers the True Positive Rate (TPR) and False Positive Rate (FPR) across different classification thresholds. This makes AUROC particularly useful for evaluating detector performance at different thresholds. The F\u2081 Score provides a comprehensive evaluation of detector capabilities by balancing the model's Precision and Recall. Additionally, we provide detailed Precision and Recall scores in Appendix F for further reference, with a specific focus on Recall to highlight the detectors' effectiveness in identifying LLM-generated text."}, {"title": "Experiments and discussion", "content": "In this section, we organize our experiments and discussions from five distinct perspectives: (1) Benchmarking the cutting-edge detectors: We evaluate the current SOTA detectors against our benchmark to identify ongoing challenges. (2) Robustness analysis: We examine factors contributing to robustness issues in various domains, LLMs, and attack scenarios. (3) Assessing generalization: We investigate how well detectors perform on data they were not specifically trained on, highlighting their out-of-distribution robustness. (4) Length discrimination: We examine the ability of detectors to differentiate between texts of varying lengths and the impact of training on such texts. (5) Real- world human writing scenarios: We analyze the effects of real-world post-processing and mistake in human-written texts, discussing their implications to provide more nuanced and valuable insights."}, {"title": "Benchmarking detectors", "content": "Detectors We employ a variety of SOTA detectors to assess the difficulty of DetectRL. Given that LLMs in real-world scenarios are often black-box and inaccessible, we exclude watermarking methods from our evaluation. Our evaluation encompasses prominent zero-shot techniques and supervised fine-tuned classifiers, including Log-Likelihood [28], Entropy [29], Rank [30], Log-Rank [30], LRR [31], NPR [31], DetectGPT [27], Fast-DetectGPT [32], Revise-Detect. [33], DNA-GPT [34], Binoculars [35], RoBERTa Classifier (RoB [36]), and XLM-ROBERTa Classifier (X-RoB [37])."}, {"title": "In-domain Robustness", "content": "Effectiveness of zero-shot detectors varies with the stylistic nature of domain data. As shown in Table 4, our results indicate that texts with a more formal style present greater challenges for detection. Detectors generally perform better with informal data, such as that from social media, but their effectiveness decreases markedly in more formal settings like news writing. Interestingly, this decrease in performance is even more pronounced in advanced detectors like Fast-DetectGPT [32]. Despite this variability, supervised classifiers demonstrate consistent reliability in detection across various domains. This finding aligns with insights from [40], emphasizing the robustness of supervised classifiers in diverse textual environments.\nDifferences in statistical patterns of LLMs pose significant challenges to detector performance. As illustrated in Table 4, our experiments reveal a notable phenomenon: nearly all zero-shot LLM- generated text detectors exhibit a significant decline in performance when processing texts generated by Claude. This suggests that the effectiveness of detectors is influenced by the specific generative model used, and their performance can deteriorate with varying statistical patterns. We hypothesize that these differences arise from variations in data, architecture, and training methods of the models, though verifying this is difficult due to the opaque nature of black-box models. Moreover, supervised detectors are more affected by the type of generative model than by the domain, particularly in models with larger sizes. For example, Rob-Large achieved an AUROC of only 86.72% and an F\u2081 Score of 76.17% on texts generated by Llama-2, whereas X-Rob-Large achieved an AUROC of 91.67% and an F\u2081 Score of 82.24% on texts generated by Claude.\nAdversarial perturbation attacks represent a significant threat to zero-shot detectors. As shown in Table 4, our findings indicate that the adversarial perturbation attacks drastically reduce the effectiveness of zero-shot detectors, reducing their performance to an average AUROC of 34.32%, which is less than half compared to their performance under paraphrase attacks. Additionally, data mixing presents a new challenging scenario, resulting in performance levels similar to semantic attacks, with detectors achieving an average AUROC of 52.51%. While prompt attacks, such as few-shot prompts, can generate higher-quality text more aligned with human preferences, their impact on zero-shot detectors is minimal. However, refining LLM-generated texts via human-written prompts still challenges detectors, decreasing their effectiveness by an average of 9.15% AUROC. This finding suggests that prompt-based methods remain a viable means of compromising detector performance. In contrast, supervised detectors consistently maintain robust performance across various attack types, demonstrating their potential for practical applications."}, {"title": "Generalization of detectors", "content": "In real-world applications, there is a significant demand for detectors that can effectively adapt to various types of text. In this paper, we further investigate this requirement, specifically focusing on the relationship between the distribution of training and test data for these detectors. We assessed the generalization of three representative detectors: LRR [31], Fast-DetectGPT [32], and the RoB-Base Classifier [36]. We discussed their generalization from three perspectives: domain, LLM, and attack. Notably, we observed phenomena that align with the findings discussed in Section 3.2.\nAs shown in Table 5, our experimental results indicate that detectors trained on less formal stylistic domain data, such as creative writing and social media, exhibit stronger generalization. Their comprehensive performance is 10% better than detectors trained on more formal stylistic domain data, such as academic writing and news writing. The variations in statistical patterns of generative models significantly impact the generalization of detectors. Detectors trained on texts generated by models with similar statistical patterns, such as GPT-3.5, PaLM-2, and Llama-2, generally perform well with each other. However, they struggle with texts generated by Claude. As discussed in Section 3.2, data with perturbation attacks poses the greatest challenge for generalization. Taking LRR as"}, {"title": "Impact of text length", "content": "Shorter training samples for stronger detectors. We assessed the performance of detectors trained on datasets with varying text lengths, using a test set within a specific pivot length interval of 160-180 words. The results, as shown in Figure 3 (a), revealed a golden length interval of 60-80 words, where texts consistently demonstrated strong detection performance across all detectors. However, as the length of the training texts increased, the performance of all zero-shot detectors gradually declined. This indicates that zero-shot detectors trained on shorter texts might be more effective than those trained on longer texts. In contrast, supervised detectors maintained consistent performance both within the golden length interval and in tests involving longer text lengths.\nLonger test samples for better zero-shot detection. Similarly, we trained a detector using data from the pivotal length interval and assessed its performance on test sets with varying text lengths. The experimental results, shown in Figure 3 (b), reveal that as test text length increased, the performance of the zero-shot detectors improved steadily. This suggests a positive correlation between zero-"}, {"title": "Impact of real-world human writing scenarios", "content": "We explored a critical question in real-world detection: How do human-driven factors impact detector performance? To investigate this, we simulated various modifications to human-written texts. We introduced paraphrase attacks to mimic text revisions and incorporated spelling errors through perturbation attacks. Moreover, we mixed LLM-generated sentences with human-written content to simulate AI-assisted writing scenarios. Experimental results, as shown in Table 6, indicate that attacks on human-written texts yield markedly different outcomes compared to those on LLM-generated texts. Specifically, paraphrasing attacks on human-written texts effectively confused zero-shot detectors, reducing the AUROC by an average of 4.77%. In contrast, data mixing had a minimal impact on zero-shot detectors' performance, with only a slight decline of 4.48% in AUROC. This contrasts sharply with the significant 20.29% decline in AUROC when human-written texts were mixed with LLM-generated texts. The resilience of human-written texts to such mixing may be attributed to their inherent complexity, making it difficult for zero-shot detectors to identify the inclusion of LLM- generated content. Interestingly, perturbation attacks on human-written texts appeared to enhance the discernment capabilities of zero-shot detectors, resulting in an average increase of 11.05% in AUROC. Similar trends were observed with supervised detectors. This suggests that human-written texts may inherently contain more adversarial features [41], which are utilized by detectors for identification. Such perturbations can further emphasize these distinctions, leading to improved performance."}, {"title": "Conclusion", "content": "In this paper, we introduce DetectRL, a novel benchmark designed to evaluate the detection capa- bilities of detectors against LLM-generated text. DetectRL compiles texts from human sources in high-risk and abuse-prone domains, utilizes popular and powerful LLMs, employs well-designed at- tack techniques, and constructs datasets encompassing a diverse range of text lengths. This benchmark aims to assess the usability of detectors in scenarios that closely resemble real-world applications. Our experimental findings reveal the primary reasons why existing detectors for LLM-generated texts struggle in practical applications. Additionally, we engage in an in-depth discussion of the potential factors influencing detector performance, offering valuable insights into current detection research. Furthermore, DetectRL provides a data curation framework to facilitate the future development of LLM-generated text detection technologies. This framework supports the rapid creation of an evolv- ing, comprehensive, and adversarial benchmark, enabling continuous adaptation and improvement of detectors in the ongoing cat-and-mouse game of LLM-generated text detection."}, {"title": "Related work", "content": "LLM-generated text and risk\nWith the expansion of model size [42] and the development of efficient preference alignment meth- ods [43, 44], LLMs have emerged with powerful capabilities for text understanding and generation [45, 46]. The text produced by the current LLMs closely resembles the quality of human-written text, particularly in terms of coherence, fluency, and grammatical accuracy, making it difficult for humans to distinguish between the two [47]. The release of ChatGPT has propelled human society into the era of LLMs, with these models finding widespread application across various aspects of daily life, such as generating advertising copy [48], writing news articles [49], storytelling [50], and coding [51]. They are also significantly influencing various fields and industries, including education [52], law [53], and medicine [54], gaining broad acceptance among people.\nHowever, the use of LLMs has raised several concerns. Recent research by [4] highlights significant challenges and potential risks associated with LLM-generated text from five perspectives: regulatory oversight related to artificial intelligence and copyright [55], erosion of user trust in internet content, homogenization of generated text that could impede LLM progress [56], challenges posed to education and academia by LLM misuse [57], and the formation of information echo chambers in society."}, {"title": "LLM-generated text detection", "content": "Given the potential misuse of LLMs, it is crucial to develop detectors that can effectively identify LLM-generated text. These detectors can help minimize the threats posed by misuse, thereby promoting the trustworthy AI applications in the era of LLMs [58, 59]. Existing LLM-generated text detection technologies [4, 60] mainly includes watermarking technology, statistics-based methods, neural-based detectors and human-assisted methods. Despite the impressive progress in LLM- generated text detection task, [23, 61] point out that these detectors become unreliable when under real-world scenarios and well-designed attacks. Building more effective and robust detectors remains a significant challenge."}, {"title": "Detection benchmark", "content": "Previous work has already dedicated significant effort to the construction of benchmarks for LLM- generated text detection, mainly encompassing early deepfake research such as the TweepFake dataset [62] and the GROVER Dataset [63], as well as prior work on detecting LLM-generated texts like the GPT-2 Output Dataset and TuringBench [8]. HC3 [47] is one of the recent impressive datasets, containing ChatGPT-generated text data in both English and Chinese, covering multi-domain and multi-lingual evaluation. Other benchmarks, such as MGTBench [9], ArguGPT [64], and the DeepfakeTextDetect Dataset [65], also consider texts generated by various LLMs. M4 [66] is a comprehensive dataset recently released, covering multi-domain, multi-lingual, and multi-generator evaluation scenarios.\nHowever, these benchmarks still mainly focused on ideal detection settings, such as using some open-source language models with limited performance and simple text generation settings, while they lack simulations and explorations of real-world application scenarios, which has been explicitly highlighted in [4]. Our work aims to bridge this gap by offering a benchmark for detecting LLM- generated texts in a form more adapted to real-world scenarios, primarily including high-risk and abuse-prone domain, the use of more powerful and commonly employed LLMs, well-designed attack methods, varied text lengths for training and testing, and factors related to real-world human writing."}, {"title": "Limitations", "content": "Considering the rapid innovation within the NLP community, we acknowledge that our benchmark's temporal relevance could be a potential limitation. This is due to the fast-paced development of LLMs and the emergence of new application scenarios and challenges. From the perspective of LLM development, new LLMs are being created at an astonishing rate and will continue to impact existing detectors, while our benchmark only examines the detectors' ability to discriminate against"}, {"title": "Ethics statement", "content": "We developed DetectRL by collecting publicly available human-written texts in high-risk and abuse- pron domains, generating similar texts using advanced and popular LLMs, designing and applying various attack methods for data augmentation. The release of DetectRL aims to advance research on detecting LLM-generated texts, enhancing their robustness and applicability in real-world scenarios. However, while promoting this research, we have also considered the potential for misuse. By making our dataset construction framework publicly available, there's a possibility that our well-designed attack methodologies could be used to develop defenses that might undermine existing detection systems.\nDespite this risk, we believe that our work will significantly contribute to the development of more robust and applicable detectors for LLM-generated text. These detectors can be continuously improved and employed to enhance LLM-generated text applications in the era of LLMs, all while participating in the ongoing cat-and-mouse game with evolving attack methods.\nAdditionally, although we have manually reviewed most of the data, there remains a risk that the data may still contain personally identifiable information or offensive content. Therefore, please ensure that our data is used solely for academic purposes and exercise caution."}, {"title": "Data collection", "content": "Human-written datasets\nThe human-written texts we utilized were sourced from domains where real-world applications of LLMs present higher risks. We selected Arxiv Abstracts to represent academic writing, Xsum for news writing, Writing Prompts for creative writing, and Yelp Reviews for social media interactions. The specific details of these datasets are as follows:\nArXiv Abstracts The ArxivPapers dataset is an unlabelled collection of over 104K papers related to machine learning published on arXiv.org between 2007 and 2020. The dataset includes around 94K papers (with available LaTeX source code) organized into a structured format comprising titles, abstracts, sections, paragraphs, and references.\nXSum The Extreme Summarization dataset serves as a benchmark for evaluating abstractive single- document summarization systems. This collection includes 226,711 news articles sourced from BBC reports between 2010 and 2017, covering a diverse range of topics such as news, politics, sports, weather, business, technology, science, health, family, education, entertainment, and the arts [17].\nWriting Prompts The Writing Prompts dataset is a dataset focused on the art of story generation, comprising 300,000 human-written stories, each paired with a unique writing prompt from an online community. This extensive collection is designed to support hierarchical story generation, a process that starts with creating a story premise and evolves into a complete narrative [18].\nYelp Reviews The Yelp Reviews Polarity dataset originates from the Yelp Dataset Challenge 2015, featuring reviews posted on Yelp. Refined by [19] for text classification research, the dataset"}, {"title": "Generative models and hyper-parameters", "content": "The generative models we selected are powerful LLMs commonly used in daily life. Table 7 lists the model paths or API services of these LLMs. The temperature for all models is set to the default parameter of 1, promoting the generation of creative and unpredictable text. The specific details of these LLMs are as follows:\nGPT-3.5-turbo GPT-3.5-turbo [67], developed by OpenAI, is a variant of the Generative Pre- trained Transformer (GPT) model, specifically tailored for generating human-like text based on the input it receives. This model has been trained on a diverse range of internet text, enabling it to understand and produce responses across a vast array of topics and styles.\nPaLM-2-bison PaLM-2-bison [14] represents the latest advancement in Google's LLMs technology, building upon the foundation of PaLM [68]. This model showcases exceptional capabilities in advanced reasoning tasks such as code interpretation and mathematical problem-solving, classification and question-answering, adept translation, and multilingual communication, as well as in generating natural language with improved proficiency over previous models.\nClaude-instant Claude-instant [15] represents a significant leap forward in the realm of AI assis- tants, developed from Anthropic's rigorous research into crafting AI systems that are helpful, honest, and harmless. Designed to accommodate a wide array of use cases, Claude excels in summarization, search functionalities, creative and collaborative writing, question answering, and coding, among other tasks.\nLlama-2-70b Llama-2-70b is a SOTA generative open-source LLMs developed by Meta, part of the broader Llama 2 collection [42]. This model outperforms numerous open-source chat models in benchmark evaluations and equates to the leading closed-source models like ChatGPT and PaLM in terms of helpfulness and safety."}, {"title": "Data generation settings", "content": "All text generation tasks were conducted through chat with LLMs. Specifically, for academic writing abstracts, we provided the article's title to the LLMs and asked them to generate an abstract based on the title; for news articles, we provided the summary of the article and asked the LLMs to generate the complete news article based on the summary; for creative writing, we provided writing prompts to the LLMs and requested that they engage in creative storytelling based on these prompts. Social media was the simplest task, as the LLMs would continue writing based on the first sentence of the social commentary text. Below, we provide the generation instructions for texts in different domains:\nAcademic writing\n[\n{'role': 'user', 'content': 'Given the academic article title, write an academic article abstract with <sentences num> sentences: \\n academic article title: <prefix> \\n academic article abstract:'},\n]"}, {"title": "Direct prompt for academic writing", "content": "The <sentences num> refers to the sentences length corresponding to the human-written sample, and <prefix> is the specific article title. For example, it could be like \u201cCalculation of prompt diphoton production cross sections at Tevatron and LHC energies\u201d, and the response is supposed to write an academic abstract based on the sentences length and article title of the human-written sample."}, {"title": "News writing", "content": "[\n<\n{'role': 'user', 'content': 'Given the news article summary, write a news article with <sentences num> sentences: \\n news article summary: < prefix > \\n news article:'},\n]\nDirect prompt for news writing\nThe <sentences num> refers to the sentences length corresponding to the human-written sample, and <prefix> is the specific news article summary. For example, it could be like \u201cA former Lincolnshire Police officer carried out a series of sex attacks on boys, a jury at Lincoln Crown Court was told.\u201d, and the response is supposed to write a news article based on the sentences length and the news article summary of the human-written sample."}, {"title": "Creative writing", "content": "[\n{'role': 'user', 'content': 'Given the writing prompt, write a story with <sentences num> sentences: \\n writing prompt: <prefix> \\n story:'},\n]\nDirect prompt for creative writing\nThe <sentences num> refers to the sentences length corresponding to the human-written sample, and <prefix> is the specific writing prompt. For example, it could be like \u201cThrough Iron And Flame\u201d, and the response is supposed to write a story based on the sentences length and the writing prompt of the human-written sample."}, {"title": "Social media", "content": "[\n{'role': 'user', 'content': 'Given the review's first sentence, please help to continue the review with <sentences num> sentences: \\n review's first sentence: <prefix> \\n continued review:'},\n]\nDirect prompt for social media\nThe <sentences num> refers to the sentences length corresponding to the human-written sample, and <prefix> is the specific writing prompt. For example, it could be like \u201cI don't know what Dr. Goldberg was like before moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office.\u201d, and the response is supposed to write the continued review based on the sentences length and the first sentence of the human-written sample."}, {"title": "Data attacks settings", "content": "Prompt attacks\nPrompt attacks are designed to use carefully crafted prompts to guide LLMs to generate text that aligns more closely with human writing styles. The Prompt Attacks we use include Few-Shot Prompt [21]"}, {"title": "Paraphrase attacks", "content": "Paraphrase attacks involve rewriting text to preserve its original meaning. We utilize various tech- niques, including the DIPPER paraphrasing tool [23], Back-translation, and Polishing with LLMs. The Discourse Paraphraser (DIPPER), as described by [23], is an advanced 11-billion parameter model designed for generating paraphrases by considering context and managing lexical diversity and content order. Inspired by real-world applications, we implement machine translation for para- phrasing through back-translation. Specifically, we use the Google Translate API to translate each LLM-generated sample from English to Chinese and then back to English. To ensure that the text maintains good semantic consistency before and after Back-translation, we use BERTScore [69], an automatic evaluation metric that assesses translation quality from a semantic perspective. Polishing with LLMs is a widely used paraphrasing method in the era of LLMs, typically initiated via prompts. Below, we provide an example of polishing with LLMs for academic writing tasks:\n[\n{'role': 'user', 'content': 'Given the academic article abstract,\npolish the writing to meet the review style, improve the spelling,\ngrammar, clarity, concision and overall readability: \\n academic article\nabstract: <prefix> '},\n]\nPolish Prompt\nThe <prefix> is the specific article abstract, and the response is supposed to polish the provided academic article abstract."}, {"title": "Perturbation attacks", "content": "Perturbation attacks primarily focus on adversarial perturbations on the text directly generated by LLMs, effectively simulating post-processing of LLM-generated text by humans and common writing errorslike spelling mistakes in real life. Our approach employs adversarial perturbation methods include TextFooler [24], DeepWordBug [25], and TextBugger [25], which correspond to word-level, character-level, and sentence-level adversarial perturbations, respectively. DeepWordBug [25] is a black-box perturbation method that can efficiently generate character-level text perturbations with the goal of minimizing the edit distance of the perturbation. TextFooler [24] is a text-based"}, {"title": "Data mixing", "content": "Data mixing is a common real-world scenario. Our data mixing methods include a mixing of texts generated by various LLMs (Multi-LLMs Mixing) and texts centered around LLM-generated text with a mixing of human-written text (LLM-Centered Mixing). The Multi-LLMs Mixing refers to a single text composed of sentences from different generative models. LLM-Centered Mixing involve replacing one quarter of the sentences in an LLM-generated text with human-written text at random. To facilitate this, we ensured that both human-written and LLM-generated texts contained at least four sentences during collection, providing a solid foundation for our data mixing process.\nFor Multi-LLMs Mixing, we sample and recombine sentences from texts generated by four different LLMs, aligning with the length of human-written texts to create a new sample. Similarly, for LLM- Centered Mixing, one-quarter of the sentences in the LLM-generated text are randomly replaced with sentences from the corresponding human-written text. This approach presents a more challenging scenario, as the data-mixed samples often lack coherent semantics."}, {"title": "Datasets statistics", "content": "The statistics for the curated datasets are presented in Table 9. The datasets include 100,800 human-written samples, consisting of 11,200 raw samples and 89,600 that have undergone attack manipulations. Additionally, there are 134,400 samples generated by LLMs, categorized as follows: 11,200 with direct prompt, 22,400 with prompt attacks, 33,600 with paraphrase attacks, 33,600 with perturbation attacks, and 22,400 involving data mixing."}, {"title": "Textual features analysis", "content": "In this section, we analyze the textual features of DetectRL samples to provide additional potentially valuable insights.\nText length We performed a statistical analysis of text length distribution in DetectRL, as shown in Figure 4. Compared to academic writing and social media texts, news writing and creative writing exhibit notably longer average lengths. The distributions for texts generated by Claude-instant, PaLM-2-bison, and Llama-2-70b are similar, whereas GPT-3.5-turbo tends to produce longer texts. Additionally, we observed that samples subjected to attack manipulation show almost no significant difference in length, except in the data mixing setup.\nN-grams We performed statistical analysis of the n-gram distribution in DetectRL, focusing on unigrams, bigrams, and trigrams. The results are presented in Figure 5. Among the four domains, creative writing exhibits the greatest variety of unigrams, bigrams, and trigrams, indicating a higher n-gram diversity. In contrast, academic writing shows the lowest diversity. Among the different LLMs, GPT-3.5-turbo demonstrates the most extensive vocabulary usage, followed by Claude-instant, Llama-2-70b, and PaLM-2-bison, in order of decreasing n-gram richness. Additionally, samples"}, {"title": "Readability", "content": "We carried out a statistical analysis of the readability distribution within DetectRL", "70": "assesses reading difficulty by considering word length and sentence length. The formula used to calculate this score is as follows:\n$FRES = 206.835 - 1.015 \\times (\\frac{Total Words}{Total Sentences}) $\n$-84.6 \\times (\\frac{Total Syllables}{Total Words}) $ (1)\nScores range from 0 to 100, with higher scores indicating better readability. The results revealed significant differences in text readability across various domains. Among all categories, creative writing texts exhibit the highest readability, followed by social media and news writing texts, while academic writing texts are the least readable. When comparing texts generated by different LL"}]}