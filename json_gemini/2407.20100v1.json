{"title": "F-KANS: Federated Kolmogorov-Arnold Networks", "authors": ["Engin Zeydan", "Cristian J. Vaca-Rubio", "Luis Blanco", "Roberto Pereira", "Marius Caus", "Abdullah Aydeger"], "abstract": "In this paper, we present an innovative federated learning (FL) approach that utilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. By utilizing the adaptive activation capabilities of KANs in a federated framework, we aim to improve classification capabilities while preserving privacy. The study evaluates the performance of federated KANS (F-KANs) compared to traditional Multi-Layer Perceptrons (MLPs) on classification task. The results show that the F-KANs model significantly outperforms the federated MLP model in terms of accuracy, precision, recall, F1 score and stability, and achieves better performance, paving the way for more efficient and privacy-preserving predictive analytics.", "sections": [{"title": "I. INTRODUCTION", "content": "Classification tasks are of central importance for various applications such as medical diagnosis, spam detection and image recognition [1]. Traditional methods often rely on large centralized datasets, which raises concerns about data privacy and potential data breaches. Machine learning models, particularly deep learning architectures, have shown promise when it comes to capturing complicated patterns for classification [2]. However, these models often require large amounts of data, which poses privacy and data security issues. Federated Learning (FL) provides a solution by enabling collaborative model training across multiple decentralized devices while keeping the data localized [3]. In this study, federated KANs (F-KANs) are presented that combine the strengths of FL with the innovative architecture of KANs [4] inspired by the Kolmogorov-Arnold representation theorem. In contrast to conventional neural networks, KANs use spline-based univariate functions as learnable activation functions, which improves their adaptability and interpretability. Previous works on KANs have explored the extension of KANs into convolutional architectures [5], graph-structured data [6], time series prediction [7], [8], vision tasks [9], GNN model leveraging KANs [10]. Despite these advances, the application of KANs in federated learning (referred to as F-KANs) has not yet been explored. This paper investigates the application of F-KANs to classification tasks and compares their performance with standalone KANs and traditional MLPs on a diverse"}, {"title": "II. PROBLEM STATEMENT AND FEDERATED KOLMOGOROV-ARNOLD NETWORKS BACKGROUND", "content": "We address the problem of classification in a federated learning environment. Specifically, we are concerned with classifying data points based on features that are distributed across multiple clients. Each data point at time t is represented by a feature vector $x_t$ and the goal is to predict the class label $y_t$. The Kolmogorov-Arnold representation theorem underlies the architecture of KANs and allows any multivariate continuous function to be represented as a composition of simple univariate functions. This study extends this concept to a federated environment where each client trains a KAN model locally and a central server aggregates the model updates without accessing the raw data, thus preserving privacy.\nFederated Kolmogorov-Arnold Networks (F-KANs) extend the concept of Kolmogorov-Arnold Networks (KANs) by leveraging the Kolmogorov-Arnold representation theorem within a federated learning framework. The theorem states that any multivariate continuous function can be represented by a finite composition of univariate functions. This allows F-KANs to replace traditional linear weights with spline-based univariate functions along the edges of the network, structured as learnable activation functions, while enabling decentralized training across multiple clients. An F-KAN layer is defined by a matrix comprising univariate functions $\\varphi_{i,j}(\\cdot)$ with $i = 1,..., N_{in}$ and $j = 1,..., N_{out}$, where $N_{in}$ and $N_{out}$"}, {"title": "III. PROPOSED METHOD", "content": "Fig. 1 illustrates the functionality of F-KANs and the interaction between the global model and several clients. At the beginning, the global KAN model is initialized. Each client has its share of the local data. Each client trains a local KAN model using its local data set and sends the model updates back to the central server. The central server performs federated averaging to aggregate the local updates and update the global model. This process is repeated for a certain number of rounds, continuously improving the global model. Finally, the updated global model is evaluated on a test dataset. Fig. 1 shows these steps with nodes representing the main operations and arrows indicating the flow and iteration of the federated learning process. In summary, F-KANs uses decentralized data to jointly train a robust global model without sharing local data directly. Algorithm 1 provides the pseudo-code of the F-KAN algorithm. The main algorithm comprises loading and preprocessing data, partitioning data for clients, defining and initializing the KAN classifier and calling the federated learning KAN process. The individual function definitions in this algorithm are as follows: create_dataset converts a data loader into tensors for training input and labels. fed_avg averages the parameters of client models to update the global model. compute_metrics computes the average loss, accuracy, precision, recall, and F1 score over a dataset. federated_learning_KAN manages the federated learning process by iterating over rounds, training client models, updating the global model, and computing metrics."}, {"title": "IV. NUMERICAL RESULTS", "content": "The experimental setup comprises a federated learning environment with multiple clients (two in our case), each of which has a subset of a different classification dataset (iris dataset in our example\u00b9). The dataset contains features and corresponding labels that come from different sources. The clients train their KAN models locally with the Adam optimizer and a cross-entropy loss function. After local training, the model updates are aggregated by a central server using a federated"}, {"title": "B. Evaluation Results", "content": "Training and evaluation were conducted for both the KAN and MLP models over 20 rounds of federated learning. In each round, the model is trained on two client datasets and then the performance is evaluated on a test dataset. From the training information in Fig. 2 results in the following comparison between the federated KAN and MLP models. Regarding training and test losses, the F-KAN model shows a rapid decrease in training and test losses, reaching values close to zero in the middle of the training rounds. This indicates efficient learning and excellent generalization abilities. The"}, {"title": "C. Key Observations", "content": "The rapid convergence and stable performance metrics of the KAN model indicate a highly efficient learning process. The spline-based univariate features enable KAN to capture complex patterns quickly and accurately, resulting in stable and high performance with fewer training rounds. Although the MLP model improves over time, it shows greater variability in performance. This could be due to its reliance on traditional linear weights and activation functions, which may not capture the complexity of the data as well as KAN's spline-based approach. The ability of the KAN model to achieve and maintain high accuracy precision, recall, and F1 score on both the training and test datasets in relatively early rounds indicates excellent generalization. This is crucial for applications where consistent and reliable performance on unseen data is critical, such as medical diagnosis or autonomous driving. The MLP model shows adequate but less consistent generalization capabilities. Its fluctuating performance metrics suggest that it is more prone to overfitting or underfitting, making it less reliable for applications that require high accuracy and consistency.\nThe KAN model takes approximately 301.19 seconds to train, reflecting the complexity of the model and the computational cost of the spline-based functions. However, these costs are justified by the superior performance and stability achieved. The MLP model trains much faster (1.83 seconds), which could be advantageous in scenarios where fast model updates are required, such as in real-time systems. However, the trade-off is lower and more variable performance. The high accuracy, precision, recall, and F1 scores make the KAN ideal for applications where errors can have significant consequences, e.g. in critical infrastructure monitoring. KAN's ability to capture complex patterns with fewer parameters makes it suitable for tasks involving complicated data relationships, such as image and speech recognition. In addition, the shorter training time makes MLP suitable for applications where models need to be updated quickly and frequently, such as recommendation systems or real-time analysis. MLPS may be preferred in scenarios where computing resources are limited and a slight compromise in performance in favour of"}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "This paper shows the effectiveness of federated KANs in classification tasks. F-KANs combine the strengths of federated learning with the innovative architecture of KANs to achieve higher accuracy with fewer parameters while preserving data privacy. The results highlight the potential of F-KANs as a powerful tool for classification in decentralized environments. Overall, the F-KAN model proved to be more reliable and effective for the federated learning task under consideration in comparison to federated MLP model, as shown by its consistently high performance across metrics and rounds. On the other hand, the computational costs of spline-based functions must be better optimized. Future work will focus on further optimizing the KAN architecture and investigating its applicability to other classification tasks in federated environments, e.g. in satellite networks."}]}