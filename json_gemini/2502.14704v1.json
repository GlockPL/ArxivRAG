{"title": "Not All Data are Good Labels:\nOn the Self-supervised Labeling for Time Series Forecasting", "authors": ["Yuxuan Yang", "Dalin Zhang", "Yuxuan Liang", "Hua Lu", "Huan Li", "Gang Chen"], "abstract": "Time Series Forecasting (TSF) is a crucial task in\nvarious domains, yet existing TSF models rely\nheavily on high-quality data and insufficiently\nexploit all available data. This paper explores a\nnovel self-supervised approach to re-label time\nseries datasets by inherently constructing candi-\ndate datasets. During the optimization of a simple\nreconstruction network, intermediates are used\nas pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We\nintroduce the Self-Correction with Adaptive Mask\n(SCAM), which discards overfitted components\nand selectively replaces them with pseudo labels\ngenerated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization\n(SNR) to further suppress overfitting from a\nloss landscape perspective. Our experiments\non eleven real-world datasets demonstrate that\nSCAM consistently improves the performance of\nvarious backbone models. This work offers a new\nperspective on constructing datasets and enhanc-\ning the generalization of TSF models through\nself-supervised learning.", "sections": [{"title": "1. Introduction", "content": "Time Series Forecasting (TSF) is a crucial task with exten-\nsive applications in energy, finance, engineering, and many\nother domains. Recent advances in deep learning have re-\nsulted in TSF methods that outperform traditional methods\nin precision, robustness, and scalability (Zhou et al., 2021;\nSalinas et al., 2020; Taylor & Letham, 2018).\nNevertheless, deep learning-based TSF methods still face\nsignificant challenges such as overfitting, dependence on"}, {"title": "2. Preliminary", "content": ""}, {"title": "2.1. Problem Formulation", "content": "Many previous TSF studies adopt a paradigm that learns\na direct mapping between two adjacent windows: the his-\ntory series (inputs) and the future series (labels). Let the\nhistory series (resp. future series) be {x\u2081, x\u2082,...,xN} =\nX \u2208 R^{L\u00d7N} (resp. {y\u2081, y\u2082,..., yN} = Y \u2208 R^{H\u00d7N}) with\ntime series length L (resp. H) and dataset size (number\nof segmented windows) N. For simplicity, we formulate\nthe problem in the univariate scenario, as it naturally ex-\ntends to the multivariate case by treating each variable as an\nadditional dimension."}, {"title": "Definition 1.", "content": "A typical TSF process is formulated as\na supervised-learning problem, i.e., to find \u03b8\u2217 =\narg min\u03b8||f(X; \u03b8) \u2212 Y||, where a specified metric || \u00b7 || is\nused to measure errors, typically the l\u2081- or l\u2082-norm.\nWhen splitting the data into training and test sets, the train-\ning set Dtrn = {Xtrn, Ytrn} and the model f (\u00b7; \u03b8) can\ndetermine a minimal target error Ltar = ||f(Xtest; \u03b8\u2217) \u2212\nYtest|| on the test set Dtest = {Xtest, Ytest}."}, {"title": "2.2. Proposed g(\u00b7; \u03c6) for Reconstruction", "content": "We proceed to introduce a\nsimple reconstruction net-\nwork used in subsequent\nexploration. Similar to a\npredictor model, the recon-\nstruction network operates\nin a sequence-to-sequence\nfashion, learning a function\ng(; ) that maps raw series\nY to reconstructed series Y.\nNote that reconstruction is\napplied only to Y, time se-"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Initial Case", "content": "We begin with a simple case where g(\u00b7; \u03c6) evolves from a\nrandomly initialized state toward an approximation of the\nraw target series Y. This process can be viewed as a grid\nsearch along the axis of the following reconstruction loss:\nlrec = ||g(y; \u03c6) \u2212 y||.   (2)"}, {"title": "3.2. Co-objective Training", "content": "The grid search (Section 3.1) is framed as a two-step opti-\nmization process with two distinct objectives involved in\nfinding optimal candidate datasets. The reconstruction opti-\nmization primarily provides a trajectory of parameters \u03c6i,\nwithout emphasizing optimality. In contrast, the prediction\noptimization evaluates the predictor\u2019s actual performance.\nOur analysis of grid search results suggests that simplifying\nthe training process into a co-objective optimization would\nbe beneficial. Since the solution of two-step optimization\n(though not optimal on the test set) essentially lies on the\nPareto Front of the corresponding co-objective optimization,\na natural approach is to search along\nAgain, the trajectory of the optimization, rather\nthan its strict optimality, contributes to improved test set\nperformance, the co-objective training can still facilitate the\nconstruction of effective candidate datasets.\nA single-step optimization using mini-batch SGD would\nbe sufficient, enabling a more smooth trajectory of \u03c6 dur-\ning updates (Figure 1(b) vs 1(a) and Figure 5(a) vs Fig-\nure 4(b)). Moreover, enabling gradient computation of\nlpred = ||\u1ef9 \u2212 \u0177|| w.r.t. \u03c6 introduces a regularization ef-\nfect, making \u1ef9 updated more cautiously towards y. This\nallows us to constrain the update of candidate datasets (now\nspecifically represented by the reconstructed \u1ef9) by jointly\nconstraining gradients w.r.t. both \u03b8 and \u03c6:\nminimize\u03b8,\u03c6L = ||\u1ef9 \u2212 y|| + ||\u1ef9 \u2212 \u0177||\nsubject to\n\u1ef9 = g(y; \u03c6), \u0177 = f(x; \u03b8),\n||\u2207\u03b8,\u03c6\u1ef8i|| \u2264 \u03b4, \u2200\u1ef9i \u2208Y.   (3)\nwhere \u03c6 is trained using both loss terms whereas \u03b8 is trained\nsolely on lpred = ||\u1ef9 \u2212 \u0177||. A gradient constraint \u03b4 is added\nto ensure a smooth trajectory of \u1ef9, enabling the co-objective\nto traverse potential candidate datasets more carefully.\nThe gradient constraint has a surrogate ||\u2207\u03c6 f || and practi-\ncally implemented by Spectral Norm Regularization (SNR),"}, {"title": "3.3. Self-Correction with Adaptive Mask (SCAM)", "content": "Mask Form of Self-supervised Loss. In a traditional\nsupervised-learning paradigm, the target loss ltarget =\n||\u0177 \u2212 y|| is used only to train a model, implying that all\ndata points are equally treated as valid labels. However,\nin our approach, where predictors are trained alongside a\nsearch for candidate datasets guided by the reconstruction\nloss, the labels perceived by the predictors are adaptively\nshifted. Specifically, \u0177 = f(x; \u03b8) is trained to fit \u1ef9, meaning\nonly the second term is optimized for the predictor f(\u00b7; \u03b8)\n(Eq. 3). The reconstruction loss term, on the other hand, is\noptimized to provide self-supervised labels for the predictor.\nWe frame this as a self-supervised-learning paradigm that\nadaptively adjusts labels in TSF problems. By comparing\nthe revised loss with the traditional supervised loss, we can\nexplicitly separate the auxiliary loss from the supervised loss:"}, {"title": "3.4. Spectral Norm Regularization (SNR)", "content": "Note that we have omitted the gradient constraints in Eq. 3.\nThis term is, in fact, positively correlated with \u2207\u03c6 f (x; \u03b8)\n(discussed in Appendix D). This further supports our pre-"}, {"title": "4. Experiment and Analysis", "content": "We address three major questions for experiments:\n\u2022 Q1: Is SCAM effective across different backbone models\nand datasets with varying features?\n\u2022 Q2: How do SCAM and SNR contribute to the potential\nimprovement in model performance?\n\u2022 Q3: How does the self-supervised reconstruction task\nbenefit the predictor models?\nOur main evaluation involves seven datasets: Electricity,\nWeather, Traffic, and four ETT datasets (ETTh1, ETTh2,\nETTm1, ETTm2), all of which are well-established TSF\nbenchmarks and publicly available (Wu et al., 2021). We\nalso test the proposals using four PeMS datasets of a\nlarger scale, as reported in Appendix G. The predictor (i.e.,\nthe backbone model integrated with SCAM) covers rep-\nresentative TSF models, including both MLP-based and\nTransformer-based architectures. MLP (Li et al., 2023) is a\nvanilla 2-layer baseline equipped with RevIN (Kim et al.,\n2021) while CYCLENET (Lin et al., 2024b) is a SOTA MLP-\nbased model explicitly capturing cyclic trend in time series.\nPATCHTST (Nie et al., 2022) and ITRANSFORMER (Liu\net al., 2024b) are Transformer-based models, representing\nchannel-independent and channel-dependent methods, re-\nspectively. Following previous settings (Zhou et al., 2021;\nWu et al., 2021; 2023) for direct and fair comparison, we set\nprediction length H \u2208 {96, 192, 336, 720} and look-back\nlength to 96 for all datasets. We provide dataset descrip-\ntions, implementation details, and reproduction instructions\nin Appendix F."}, {"title": "4.1. Main Experiment (Q1)", "content": "Table 1 demonstrates consistent performance improvements\nin all major backbones when SCAM and SNR are incorpo-\nrated in the self-supervised-learning paradigm. The full re-\nsults with detailed breakdowns by prediction lengths are pro-\nvided in Appendix G. These gains are particularly notable\non ETT datasets, which are known for their noisy nature and\nrelatively small size. Notably, Transformer-based models\nlike PATCHTST and ITRANS, which typically underperform\ncompared to lightweight models MLP and CYCLENET on\nthese datasets, show significant enhancements in general-\nization with SCAM. On the Weather dataset, the boost is\nmore modest, likely due to the intrinsic chaotic nature of\natmospheric data.\nRegarding Q1, our method demonstrates general effective-\nness across various backbones and datasets. A well-known\ndiscrepancy between MLP-based and Transformer-based\nmodels is their dataset preferences: Transformer-based\nmethods excel on large, regular datasets, while MLP-based\nmethods perform better on noisy datasets. SCAM helps\nbridge this gap by enabling Transformer-based models to\nperform competitively on traditionally challenging datasets\nand enhancing the robustness of MLP-based models."}, {"title": "4.2. Ablation Study (Q2)", "content": ""}, {"title": "4.2.1. CONTRIBUTIONS OF SCAM AND SNR", "content": "To answer Q2, we present the performance gains from\nSCAM and SNR through an ablation study (Tables 2 and 3),\nusing ITRANS (Transformer-based) and CYCLENET (MLP-"}, {"title": "4.3. SCAM: A Multiple Instance Learning View (Q3)", "content": "Multiple Instance\nLearning (MIL) is\na classical weakly-\nsupervised binary\nclassification prob-\nlem where typically\na bag of instances\nis labeled positive if\nat least one instance\nis positive. (Earlyet al., 2024; Chen et al., 2024b) extend MIL to Time Series\nClassification by treating an input window as a bag of\ninstances (time points), enabling a model to predict based\non instance-level classifications that are more interpretable.\nTo answer Q3, we hypothesize that the effectiveness of\nSCAM shares similarities with MIL by leveraging instance-\nlevel signals, though it does not strictly follow a MIL frame-\nwork. For a quick illustration, we synthesize a toy dataset\nwhere the ground truth is defined as y = Asin(\u03c9\u2081x) +\nB sin(\u03c9\u2082x), with added noise sampled alternately from\nN(0, \u03c3\u2081) and N(0, \u03c3\u2082) in different windows. As shown\nin Figure 11, when the noise deviation \u03c3 is large (left part),\nSCAM tends to optimize lrec = M \u25cb ||\u1ef9 \u2212 y||, pri-\noritizing robust reconstructions; when \u03c3 is small, SCAM\nshifts focus to optimizing lpred = M \u25cb M< ||\u1ef9 \u2212 y|| and\nltarget = ||\u0177 \u2212 y||, emphasizing accurate predictions.\nThis study only reveals a part of SCAM\u2019s self-supervision\neffectiveness, which is further explored in Appendix D."}, {"title": "5. Related Work", "content": "We discuss related techniques from meta-learning and self-\nsupervised learning perspectives, with an inventory of TSF\nmodels in Appendix E."}, {"title": "Meta-Learning for Time Series.", "content": "Meta-Learning, by defi-\nnition, seeks to perform in a learn-to-learn paradigm. Gen-\nerally speaking, meta-learning includes optimization-based,\nmodel-based, and metric-based methods. Optimization-\nbased methods target optimal initial parameters (Finn et al.,\n2017), often involving a two-loop optimization process (Ore-\nshkin et al., 2021; Woo et al., 2022). Model-based methods\naim to determine suitable models, typically from an ensem-\nble, based on predefined tasks (Lines et al., 2018; Middle-\nhurst et al., 2021) or activation states (Abdallah et al., 2022).\nMetric-based methods (Du et al., 2021; Woo et al., 2022)\nlearn a metric function that provides more expressive mea-\nsurements of distances between data samples, commonly\ncomparing training and validation samples.\nOur method SCAM aligns with the boarder scope of meta-\nlearning. Specifically, the grid search (Algorithm 1) follows\nthe two-loop structure similar to optimization-based meth-\nads. However, it diverges by focusing on dataset space\nrather than parameter space. The goal of the grid search is\nnot to solve the optimization problem directly but to leverage\nthe trajectory of optimization. Additionally, the final mask\nform of SCAM in essence provides a more accurate metric\ntailored for a supervised-learning setting. While DEEPTIME\n(Woo et al., 2022) and ADARNN (Du et al., 2021) share\na related idea, there are key differences. DEEPTIME fo-\ncuses more on a time-index forecast paradigm, whereas\nADARNN applies metric learning to hidden states. Both\nworks learn metrics on the sample level (a window of time\nseries), whereas ours focuses on the instance level (individ-\nual data points of time series)."}, {"title": "Self-supervised Learning for Time Series.", "content": "Self-\nsupervised learning trains models without relying on man-\nually labeled data by using auxiliary tasks like generation,\ncontrast, and reconstruction to learn expressive represen-\ntation or create pseudo labels. In the realm of time se-\nries, this approach is discussed more in Time Series Clas-\nsification (TSC) (Jawed et al., 2020; Xi et al., 2022; Liu\net al., 2024c). Recent works (Early et al., 2024; Chen et al.,\n2024b) present a novel perspective that instances in time\nseries/segments can have multiple labels. They propose\ncorresponding weakly-supervised-learning methods that sig-\nnificantly improve both performance and interpretability.\nAs manual labeling is usually not required, TSF is treated\nas a generation task by self-supervised methods. Recent\nworks focus on the use of TSF as an auxiliary task to learn\nuniversal representations that improve performances of other\ntasks (Nie et al., 2022; Senane et al., 2024; Liu & Chen,\n2024). This paradigm shows the potential to scale time\nseries models to levels comparable to large language models.\nIn this work, we integrate both perspectives by employing\nan auxiliary reconstruction task, commonly used in TSC, to\nenhance the performance of TSF. The pseudo labels, often\ndiscussed in TSC, are derived from existing, manual ones,\nwhile ours are created in a self-supervised paradigm."}, {"title": "6. Conclusion and Future Work", "content": "This paper presents a self-supervised approach SCAM that\nenhances TSF models by selectively replacing overfitted\ncomponents with pseudo labels derived from intermediate\nreconstructions. Combined with Spectral Norm Regular-\nization applied to the linear layers of the backbone, SCAM\nimproves generalization and robustness across TSF models.\nFuture work will explore extending SCAM to tasks such as\ntime series outlier detection and error correction."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here."}, {"title": "A. Grid Search Algorithm in Initial Case", "content": "In Section 3.1, we have introduced the grid search process to illustrate how the reconstruction network g(\u00b7; \u03c6) evolves to\napproximate the raw target series Y by minimizing the reconstruction loss lrec = ||g(y; \u03c6) \u2212 y||. The following Algorithm 1\nprovides a detailed implementation of this process, where the reconstruction network g(\u00b7; \u03c6) and the predictor model f(.; \u03b8)\nare jointly optimized to minimize both reconstruction and prediction losses. Specifically, for each candidate reconstruction\nparameter \u03c6i (line 1), the predictor\u2019s parameters \u03b8 and its optimizer are initialized (line 2). In the inner loop (line 3), the\npredictor is optimized by first reconstructing \u1ef9i using g(\u00b7; \u03c6i) (line 6), then calculating the prediction loss lpred (line 7)\nand reconstruction loss lrec (line 8). The prediction loss lpred is backpropagated to update \u03b8 (line 8), and this process\nrepeats until the gradient \u2207\u03b8 falls below a threshold \u03b1 or the maximum steps J are reached (line 3). After optimizing \u03b8, the\nreconstruction loss lrec is backpropagated to update \u03c6 (line 11), and this process is repeated for all N candidates to find the\nbest \u03c6 (line 1).\nAlgorithm 1 Grid Search along lrec\nParameters: \u03c6 w.r.t reconstruction network g(\u00b7; \u03c6);\n\u03b8 w.r.t predictor model f(\u00b7; \u03b8)\nOptimizers: Opt\u03c6 w.r.t \u03c6 (outer loop);\nOpt\u03b8 w.r.t \u03b8 (inner loop)\nInitialize: \u03c6, Opt\u03c6\n1 for i \u2190 0 to N do \u25b7 N for number of candidates\nInitialize \u03b8, Opt\u03b8; j\u2190 0\n2\n3 while \u2207\u03b8 > \u03b1 and j < J do\n4 \u25b7 J for maximum steps to optimize a predictor\nlrec \u2190 0\n5 foreach (x, y) \u2208 (X, Y) do\n6 yi \u2190 g(y; \u03c6i)\n7 lpred \u2190 || f(x; \u03b8) \u2212 yi ||\n8 lrec \u2190 lrec + || yi \u2212 y||\n9 Backpropagate lpred and update \u03b8 using Opt\u03b8\n10 j \u2190 j + 1\n11 Backpropagate lrec and update \u03c6 using Opt\u03c6"}, {"title": "B. Further Discussion on Gradient Constraint", "content": "Recall that in Section 3.2, we have proposed the co-training objective as:\nminimize\u03b8,\u03c6 L = ||\u1ef9 \u2212 y|| + ||\u1ef9 \u2212 \u0177||\nsubject to\n\u1ef9 = g(y; \u03c6), \u0177 = f(x; \u03b8),\n||\u2207\u03b8,\u03c6\u1ef8i|| \u2264 \u03b4, \u2200\u1ef9i \u2208Y.\nWith constraints on the gradient of \u1ef9, the co-training update allows for a more stable optimization compared to the grid\nsearch using a two-step optimization. When viewing each intermediate \u03c6i as an individual candidate dataset, applying\ncautious updates to \u1ef9i = g(y; \u03c6i) introduces additional candidate datasets along the optimization trajectory, enriching the\nsearch process.\nHowever, since \u2207\u03b8,\u03c6\u1ef8i is not practically computable for each \u1ef9i within a single step of optimization, we turn to look for a\nsurrogate term to replace this constraint."}, {"title": "Note that", "content": "\u2207\u03b8,\u03c6 \u1ef8i = [\u2207\u03c6g \u2207\u03b8g]\n=  \u2207\u03c6g \u2207\u03b8 g  \n=  \u2207\u03c6g \u2207\u03b8g 2||\u2207\u03c6g(\u2212y) + (\u1ef9\u2212\u0177)\u2207\u03c6g \n=  \u2207\u03c6g 2(\u2212y)\u2207\u03c6g + 2(\u1ef9\u2212\u0177)\u2207\u03c6g + (\u1ef9\u2212\u0177)2] \u2207\u03b8 [(\u2212y)2 +(\u1ef9\u2212\u0177)2]  \n=  \u2207\u03c6g 2(\u0177\u2212\u1ef9)\u2207\u03b8f  \u2207\u03b8 [\u2207\u03c6g(\u2212y) + (\u1ef9\u2212\u0177)\u2207\u03c6g \n=  \u2207\u03c6g 2 \u0177 - \u1ef9 \u2207\u03b8f 2 \u2207\u03b8 [(\u2212y) + (\u1ef9\u2212\u0177) ] \n\u2248  \u2207\u03c6g \u2212\u2207\u03b8f , because |\u1ef9 \u2212 y| < |\u1ef9 \u2212 \u0177|.   (8)"}, {"title": "Reconstruction is inherently", "content": "Reconstruction is inherently a simpler task compared to forecasting, which allows the last approximation in Eq. 8 to hold\nafter just a few steps of initial optimization. When \u1ef9 is far distinct from original labels y, the reconstructed series \u1ef9 becomes\nnearly unpredictable, leading to instability in the optimization process. Therefore, adding a constraint on g can interfere with\nthe convergence of the predictor model. To address this, Eq. 8 offers an optional assurance of gradient constraint, which\nuses ||\u2207\u03b8f || \u2264 \u03b4 as a surrogate for maintaining stability during optimization.\nThe constrained form of the optimization is equivalent to the penalized form using Lagrangian Duality. Eq. 3 can be\nrewritten as:\nminimize L = ||g(y; \u03c6) \u2212 y|| + ||g(y; \u03c6) \u2212 f(x; \u03b8) ||\n+ \u03b2||\u2207\u03c6 f (x; \u03b8) ||,   (9)\n\u03b8,\u03c6\nFor readers who are familiar with Reinforcement Learning, this derivation resembles the transfer from a constrained\noptimization to a penalized one (e.g., from TRPO (Schulman, 2015) to PPO (Schulman et al., 2017)). In brief, while the\npenalized form is theoretically equivalent to the constrained form, it is challenging to choose a fixed \u03b2 that works universally\nacross all datasets or even within a single dataset (because intrinsic characteristics can vary over time). Thus, a more general\nform of constraint is required to better serve the penalty, similar to the concept of gradient clipping in PPO."}, {"title": "Note that", "content": "Note that \u2207\u03b8 f < \u03b4 implies the Lipchitz condition for an arbitrary function f. This means\n||f(x1; \u03b8) \u2212 f(x2; \u03b8)|| \u2264 C(\u03b8)||x1 \u2212 x2||,   (10)\nwhere C(\u03b8) is a constant with respect to the parameter \u03b8. When considering a typical Fully Connected Layer defined as\nf(x; W, b) = \u03c3(Wx + b), the condition becomes\n||\u03c3' W(x1 \u2212 x2)|| \u2264 C(W,b)||x\u2081 - x2||,   (11)\nWhen the gradient of activation function \u03c3 has an upper bound (as is often the case for common activation functions like\nReLU, Sigmoid, etc.), the Lipchitz condition holds as long as\n||W(x1 \u2212 x2)|| \u2264 C(W,b)||x1 - x2||.   (12)"}, {"title": "We expect", "content": "We expect the constant C to be relatively small so that the penalty works. In fact, C here corresponds to the spectral norm of\nthe matrix W, which is defined as\n||W||2 = maxx\u22600||Wx||||x||\n(13)\nBy applying the Spectral Norm Regularization (SNR) in Eq. 7, we can ensure the constant C equals to exactly 1.\nHowever, in practice, SNR have limitations when applied to the parameter matrix in self-attention mechanisms. This\nphenomenon is termed entropy catastrophe as discussed by (Ilbert et al., 2024). In this paper, by analyzing the sharpness\nof different components in the predictor model, we propose to use pre-SNR and post-SNR combined, which specifically\nnormalizes the first and last linear layer in the TSF models (see Section 3.4)."}, {"title": "C. Implementation Details of g(\u00b7; \u03c6)", "content": "As introduced in Section 2.2, we propose a simple enough reconstruction network g(\u00b7; \u03c6) that serves our objective. Despite\nits simplicity, the architecture incorporates some special designs that enhance its performance. Specifically, these include\nthe conv-concat layer and the point-wise FFN, which are detailed in Appendix C.1 and Appendix C.2, respectively."}, {"title": "C.1. Conv-concat Layer", "content": "Transpose and Unfold. The implementation details of the conv-concat layer involve two key operations that are designed\nfor the following two benefits:\n1. The convolution outputs can be concatenated into embeddings of the same length, enabling features from different\nfrequencies to be ideally fused into one embedding.\n2. The features are evenly arranged along the temporal dimension, ensuring that each embedding in the sequence has the\nsame large Receptive Field.\nTo achieve these benefits, we introduce a two-step operation: Transpose and Unfold, which work together to ensure both\nuniform embedding structure and large Receptive Fields.\nSpecifically, we set kernel size = 3, stride = 2, padding = 1, and the number of kernels doubled for each subsequent layer.\nUsing this setup, as illustrated in Figure 12, we ensure that the number of features remains invariant across different layers,\nwith only the shape of the features changing. Now we can fuse the outputs from different convolution layers together by\nflattening/unfolding the features to the original shape of (L \u00d7 1). Again, considering the effectiveness of point-wise FFN\npresented in Appendix C.2, we expect the concatenated features to be near-equally arranged along the temporal dimension\nto preserve the sequential relationships in the embedding. To achieve this, we first transpose the features and then unfold\nthem. This practice can ensure a Receptive Field of (2l+1 \u2212 1) wide for each embedding, where l is the total number of\nconvolution layers."}, {"title": "Effective Receptive Field.", "content": "As what was proposed by (Luo & Wang, 2024), the Effective Receptive Field (ERF) is a\nreasonable consideration for designing convolution-based architectures. To evaluate the ERF of our conv-concat layer, we\ninput an impulse function and visualize the resulting ERF, as shown in Figure 13. The visualization demonstrates that,\nwithout requiring an extra-large convolution kernel, our proposed method achieves a near-global ERF. This is made possible\nby combining the outputs from different layers, each capturing distinct frequency patterns."}, {"title": "C.2. Point-wise FFN", "content": "We employ a point-wise FFN as a parameter-sharing module to decode the outputs from the convolution layer. The FFN is\nessentially a two-layer MLP that resembles the common design of a linear projector in predictor models, as mentioned in\nSection 4.2.2.\nTo further illustrate, we present the three different parameterizations of the linear projector commonly used in TSF models\nin Figure 14:"}, {"title": "D. Further Analysis", "content": ""}, {"title": "D.1. Distribution of Reconstructed Datasets in Grid Search", "content": "The distribution of sampled predictions is shown in Figure 15. The predictions are evaluated using three loss metrics: (1)\nPrediction loss: lpred = ||\u0177 \u2212 \u1ef9||, (2) Reconstruction loss: lrec = ||\u1ef9 \u2212 y||, and (3) Target loss: ltarget = ||\u0177 \u2212 y||. In\nFigure 15, scatter points illustrate the relationships among these losses across various candidate datasets generated by g(\u00b7; \u03c6)\non the ETTh1 dataset. Each scatter point is colored according to the mean reconstruction loss (lrec) of its corresponding\ndataset. Lighter colors (e.g., yellow) represent datasets with higher lrec, while darker colors (e.g., purple) correspond to\ndatasets with lower lrec.\nThe distribution is visualized across three projections:\n1. lrec-lpred Plane illustrates the relationship between the reconstruction loss lrec and the prediction loss lpred, both of\nwhich are actively optimized during training. As lrec decreases (darker colors), the points in the distribution become\nmore condensed, indicating reduced flexibility in the candidate datasets. This trend suggests that datasets with very low\nreconstruction loss may lack the diversity needed for optimal predictor performance.\n2. lrec-ltarget Plane highlights the relationship between the reconstruction loss lrec and the target loss ltarget, where\nltarget serves as the primary evaluation metric for predictor performance. Interestingly, datasets closer to the raw data\n(darker colors, with lower lrec) do not consistently lead to better ltarget values. This observation, which indicates that"}, {"title": ""}]}