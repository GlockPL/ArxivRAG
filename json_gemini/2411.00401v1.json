{"title": "Statistical Guarantees for Lifelong Reinforcement Learning using PAC-Bayesian Theory", "authors": ["Zhi Zhang", "Haochen Zhang", "Chris Chow", "Eric Hanchen Jiang", "Yuchen Cui", "Yasi Zhang", "Han Liu", "Yanchao Sun", "Furong Huang", "Oscar Hernan Madrid Padilla"], "abstract": "Lifelong reinforcement learning (RL) has been developed as a paradigm for extend- ing single-task RL to more realistic, dynamic settings. In lifelong RL, the \"life\" of an RL agent is modeled as a stream of tasks drawn from a task distribution. We propose EPIC (Empirical PAC-Bayes that Improves Continuously), a novel algorithm designed for lifelong RL using PAC-Bayes theory. EPIC learns a shared policy distribution, referred to as the world policy, which enables rapid adap- tation to new tasks while retaining valuable knowledge from previous experiences. Our theoretical analysis establishes a relationship between the algorithm's generalization per- formance and the number of prior tasks pre- served in memory. We also derive the sample complexity of EPIC in terms of RL regret. Extensive experiments on a variety of envi- ronments demonstrate that EPIC significantly outperforms existing methods in lifelong RL, offering both theoretical guarantees and prac- tical efficacy through the use of the world policy.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (RL) has excelled in chal- lenging tasks including abstract strategy games (Silver et al., 2017, 2016), visual navigation (Zhu et al., 2017), and control (Mnih et al., 2015; Lillicrap et al., 2015). However, RL is a data intensive learning paradigm, therefore training a policy for every task from scratch is computationally expensive and time-consuming. In reality, many tasks an agent encounters are not entirely novel but instead belong to a broader task distribu- tion, meaning they share commonalities that can be leveraged. This insight highlights the inefficiency of retraining for every individual task. Lifelong RL, also known as continual RL, emerges as a promising frame- work where an agent interacts with a sequence of tasks, continuously adapting and improving its policy by lever- aging knowledge from past task instances (Khetarpal et al., 2022).\nIn lifelong RL, an agent's objectives are primarily to achieve fast adaptation with limited samples and effective knowledge retention (Abel et al., 2024). In other words, lifelong RL agents experience a stability- plasticity dilemma, where the agent must balance re- taining useful long-term knowledge with the ability to rapidly adapt to new situations.\nRecent methods addressing knowledge retention and transfer in lifelong RL include Q-value function transfer (Lecarpentier et al., 2021), optimizing Q-value func- tion initialization (Abel et al., 2018), decomposing the value function into permanent and transient compo- nents (Anand and Precup, 2023), reusing knowledge by sampling from past experiences (Kessler et al., 2023), detecting change points in rewards and environment dynamics(Steinparz et al., 2022), and using a Bayesian approach to learn a common task distribution for better data efficiency and transfer (Fu et al., 2022).\nIn lifelong RL, domain shifts induce non-stationarity, which occurs not only due to changing transition dynamics and reward functions, but also through variations in available actions or decisions over time (Boutilier et al., 2018; Chandak et al., 2020). Such scenarios are common in real-world applications. For example, in robotics, additional control components are integrated throughout the robot's lifetime, and in medical decision support systems, new treatments and medications must be incorporated.\nFurthermore, tasks encountered over an agent's lifetime can be highly diverse, yet certain high-level strategies"}, {"title": "2 Preliminaries", "content": "These results underscore EPIC's effectiveness in lifelong learning scenarios, offering a robust and theoretically grounded solution for continual adap- tation in RL.\nIn RL, an agent interacts with the environment by taking actions, observing states and receiving rewards. The environment is modeled by a Markov Decision Process (MDP), which is denoted by a tuple $M = (S, A, T, R, \\gamma, \\nu)$, where S is the state space, A is the action space, T is the transition kernel, R is the reward function, $\\gamma \\in (0,1)$ is the discount factor, and $\\nu$ is the initial state distribution.\nA trajectory $ \\tau \\sim \\pi$generated by policy $ \\pi$ is a se- quence $s_1, a_1, r_1, s_2, a_2,$\u2026\u2026, where $s_1 \\sim \\nu, a_t \\sim \\pi(a|s_t)$, $s_{t+1} \\sim T(s'|S_t, a_t)$ and $r_t = R(s_t, a_t)$. The goal of an RL agent is to find an optimal policy $ \\pi^*$ that maxi- mizes the expected total rewards $J(\\pi) = E_{\\tau \\sim \\pi}[r(T)] = E_{s_1, a_1,\u2026\u2026\\sim \\nu,\\pi,T,R}[\\sum_{t=1}^{T} \\gamma^{t-1}r_t]$.\nIn lifelong RL, the agent interacts with a (potentially infinite) sequence of tasks, which come from an under- lying task distribution (Khetarpal et al., 2022), denoted as $D_i, i = 1, ...,\\infty$. Suppose that tasks share the same $ \\gamma$, but may have different S, A, transition probabilities T and rewards R. The learning process is:\n1. Initialize a policy $ \\pi_0$;\n2. Sample a task (MDP) $M_i \\sim D_i$;\n3. Starting from $ \\pi_0$, learn a policy $ \\pi_i$ for task $M_i$ to maximize rewards.\nAn effective lifelong RL agent should quickly adapt to new tasks that it encounters throughout its life.\nPAC-Bayes analysis applies to learning algorithms that output a distribution over hypotheses $h \\in H$. This refers to $h$ is sampled independently from a distribu- tion over functions in a function class $H$. For example, for a linear predictor of d dimension, $h(x) = \\langle w,x\\rangle$, we let $w \\sim N(0, I_d)$. Generally, such algorithms will be given a prior distribution $P_0 \\in P$ at the begin- ning and learn a posterior distribution $P \\in P$ af- ter observing training data samples ${z_i}_{i=1}^N$. We de- fine the expected loss (generalization error) $l_P(P) = E_{h \\sim P}[E_{z \\sim D} [l_h(z)]]$, and the empirical loss (training er- ror) $l_S(P) = E_{h \\sim P} [\\frac{1}{N} \\sum_{i=1}^{N}l_h(z_i)]$, which are under the expectation of hypothesis $h \\sim P$.\nThe main application of PAC-Bayes analysis in ma- chine learning is to produce high-confidence bounds"}, {"title": "3 Methods", "content": "that can be shared across tasks. Not only should the world model (Ha and Schmidhuber, 2018; Fu et al., 2022; Anand and Precup, 2023), which captures the general knowledge distribution of tasks, be continuously refined, but it is also crucial for an agent to develop a world policy. The key consideration of this world policy is to adapt the parameters of the policy so they are captured by a global distribution, which represents the uncertainty over policy parameters. This allows for better generalization and adaptability across tasks.\nMotivated by the above need, we raise two questions:\n1. Can we extract the common structure present in policies from previously encountered tasks, allow- ing the agent to quickly learn the policy specific to new task to enable fast adaptation with theoretical guarantees?\n2. How many samples are required to achieve a given level of performance?\nTo answer these two questions, we develop a unified framework based on a Bayesian method that learns a rapidly adaptable policy distribution from past tasks, retaining valuable information while remaining capable of quickly adapting to unseen situations.\nWe also provide a theoretical analysis of the algorithm's generalization performance relative to the number of effective tasks and retained knowledge in the finite Markov Decision Process (MDP) setting, along with its sample complexity to demonstrate efficiency.\nWhen addressing the first question, we must also ac- count for both catastrophic forgetting and generalizabil- ity - the aforementioned stability-plasticity dilemma. Agents that can quickly solve traditional RL problems risk abruptly losing performance on tasks they have seen before due to their flexibility or plasticity. On the other hand, agents that do not forget any of their past experience may give up a measure of their plasticity. These issues are central in lifelong RL, and can be approached from a Bayesian perspective (Khetarpal et al., 2022). Bayesian methods have been applied to meta learning (Amit and Meir, 2018), lifelong learning for bandits (Flynn et al., 2022), and learning controls for robots in multiple environments (Majumdar et al., 2021), aiming to learn a fast adapted policy. Instead of learning a specific policy, we leverage the PAC-Bayes theory to learn a distribution of policy hypotheses shared across multiple tasks. Further details about PAC-Bayes theory can be found in Section 2.3 and the Related Works section in Appendix \u00a7B. When a new task arises, we can initialize a policy hypothesis by sam- pling from this learned distribution. A well-constructed distribution of hypotheses promotes effective long-term memory, mitigating catastrophic forgetting. Unlike"}, {"title": "3.1 PAC-Bayes Framework for Lifelong RL", "content": "for the true or generalization error in terms of the training error plus $R(D_{KL}(P||P))$, which is a function of the KL divergence between the prior and posterior distributions, as shown below (McAllester, 1999),\n$l_P(P) \\leq U(P) := l_S(P) + R(D_{KL}(P||P))$, (1)\nwith\n$R(D_{KL}(P||P))\n:= \\frac{1}{2N} [D_{KL} (P||P) + log (2N^{1/2}/\\delta)]$, (2)\nwhere $U(P)$ in right-hand side of Equation (1) is called the generalization error bound that depends on $P$, and minimization of this bound leads to generalization error guarantees.\nWe propose a PAC-Bayes lifelong RL algorithm, EPIC (Algorithm 1), to minimize the novel bound in (3). The algorithm utilizes a Bayesian posterior to distill the common policy distribution learned from previous tasks, which is then used to sample the policy and serves as a prior for new tasks. We provide a generalization guarantee for EPIC in Theorem 3.3. Furthermore, we employ the Gaussian family for the posterior and prior in EPICG (Algorithm 2). The sample complexity of EPICG is given in Theorem 3.4.\nWe learn a general policy distribution P for lifelong RL by leveraging the core concept of the PAC-Bayes Method. We explicitly formulate $U(P)$ for the lifelong RL setting and employ it to propose an algorithm that learn the P by minimizing $U(P)$ to accomplish the lifelong learning objective.\nDefine P as the whole policy space for P. Rather than considering a general distribution P for hypotheses where II can be infinite, we let P be parameterized by $ \\theta \\in \\mathbb{R}^d$ such that $ \\theta \\sim P$. Note $ \\theta$ could be a neural network.\nNaturally, the distribution P is the posterior distribu- tion of policy $ \\theta$ in the PAC-Bayes framework. Then let $P_0$ be the prior distribution of the parameter. In the life- long setting, as the tasks stream in, assume the agent has encountered K tasks so far, then the PAC-Bayes lifelong RL problem is formulated as follows:\n$\\min_P U(P)$\n$:=\\frac{1}{K} \\sum_{i=1}^{K} E_{\\theta \\sim P} [-J_{M_i} (\\pi_{\\theta})] + R(D_{KL}(P||P))$, (3)\nwhere $R(D_{KL}(P||P))$ is derived later in our theory,"}, {"title": "3.2 An Algorithm based on PAC-Bayes Lifelong Framework", "content": "Unlike prior methods, we sample a random policy function according to this distribution. This function sampling approach is seen in modern popular deep learning meth- ods, for example, in-context learning(Garg et al., 2022).\nFor the second question, an agent has to keep learning as well as forgetting. Too much experienced knowledge kept in memory may decrease the learning efficiency, while too little may be insufficient to learn an effective policy distribution. To address the second question, we derive a relationship between the performance of our algorithm and the number of experienced tasks (denoted as N in a later section) that need to be re- tained in the agent's memory, based on PAC-Bayes theory. We use the negative expected long term re- wards, where the expectation is taken with respect to tasks and policies (also known as the generalization error), as a measure of the algorithm's performance from a statistical perspective.\nFrom our theoretical result, where we provide an ex- pression of this relationship, we discovered a trade-off between this value and the algorithm's performance, which aligns with natural intuition, a double sided ef- fect. In practice, our expression allows us to optimize the performance of our algorithm by optimizing N, although we recommend using hyperparameter tuning. Furthermore, to demonstrate the efficiency of our al- gorithm, we derive its sample complexity from a RL regret perspective, showing that our algorithm learns an optimal policy as more tasks encountered.\nOur learning process involves a loop of times to evolve the policy distribution. So we index the policy dis- tribution at each time by a subscript. First, denote $ \\Theta := {\\theta_l}_{l=0}^{T-1}$ and let $P := P({\\theta_l}_{l=0}^{T-1})$ denote the joint posterior distribution of $ \\theta_0,...,\\theta_{\\lfloor \\frac{K}{N} \\rfloor -1}$ across all"}, {"title": "4 Experiments", "content": "We now develop an algorithm to exploit the PAC-Bayes framework to efficiently perform lifelong RL.\nConsider a time where we have seen K tasks so far, and denote them ${M_k}_{k=1}^{K}$. They are drawn from the lifelong task distribution ${D_i}_{i=1}^{K}$. It's worth noting that these tasks can exhibit distinct distributions and interdependencies. Each distribution $D_i$ should possess non-zero support and boundedness both from above and below. Critically, once the agent interacts with a task, revisiting previously encountered MDPs is not guaranteed.\nOur objective is to learn a shared lifelong learning model - the distribution of $ \\theta$ using the K tasks the algorithm has encountered so far.\nTo achieve this, we propose the following lifelong RL learning algorithm based on the learning objective in Equation (3), and provide its theoretical justification. The main idea is to learn a policy distribution P as a policy initializer using the objective in Equation (3), referred to as the default policy This approach allows the default policy to capture common knowledge among tasks, addressing the challenge of task divergence.\nIn the lifelong setting, the agent receives a new task, stores it, learns from it, then forgets. We allow the agent to keep a number of N tasks in memory. We update the default policy every N tasks and estimate the training cost based on the most recent N tasks. At the K-th task, the agent has performed $ \\lfloor\\frac{K}{N} \\rfloor$ updates to the default policy so far. At each time step 1 = $1,\u2026, \\lfloor\\frac{K}{N} \\rfloor$, the agent has $ \\theta_{l-1}$ as its policy parameters from $P_{l-1}$. It encounters the ith task $M_{i,i}$'s MDP, and receives $J_{M_{i,i'}} (\\pi_{\\theta_{l-1}})$ as the total discounted expected reward. The collects trajectory data of H steps for task $M_{i,i}$, using $ \\pi_{\\theta_{l-1}}$, resulting in a dataset $ \\tau_l = (\\tau_{l,1},..., \\tau_{l,N})$ with a size of $|\\tau_l| = HN$."}, {"title": "3.3 Posterior Distribution and Prior Distribution", "content": "These results underscore EPIC's effectiveness in lifelong learning scenarios, offering a robust and theoretically grounded solution for continual adap- tation in RL.\nIn Equation (5), $P_l$ represents the posterior distribution of $ \\theta_l$. To optimize the posterior $P_i$, we need to choose appropriate hyperparameters for its distribution. For instance, in a Gaussian distribution, we optimize its mean $ \\mu$ and variance $ \\sigma^2$.\nLet $ \\tau_l \\sim \\theta_{l-1} \\times M_l \u2026\u2026\u2026 \\times ... M_N$ be the data induced from previous $ \\theta_{l-1}$, and define the likelihood func- tion $p(g(\\tau_l)| \\theta_{l-1})$. Suppose the prior distribution is a probability density function $p(\\theta_{l-1};q)$, parametrized by q, such as a Gaussian prior $P_l = N(\\mu_l, \\sigma_l)$, where $q := (\\mu_l, \\sigma_l)$.\nBased on Bayes' Rule, the posterior distribution is uniquely given by $p(\\theta_{l-1}|g(\\tau_l); q) = \\frac{p(g(\\tau_l)|\\theta_{l-1})P(\\theta_{l-1};q)}{c(q)}$ where $c(q)$ is the normalization constant depends on q. We can optimize the hyperparameter q using the following equation:\n$ \\min_{q} U_T(P; q) := \\sum_{i=1}^{N} E_{\\theta_{l-1} \\sim p(\\theta_{l-1}/g(\\tau_l);q)} [-J_{M_{l,i}} (\\pi_{\\theta_{l-1}})] +R(D_{KL} (P||P; q))$, (6)\nwhere $R(D_{KL} (P||P;q))$ is defined in (5). In Equa- tion (6), the posterior distribution is unknown, and obtaining an explicit expression requires knowing the data likelihood. In the RL regime, data samples con- sist of states, actions, and value functions, and one approach is to use the exponential of the negative squared temporal difference (TD) error as an unnor- malized likelihood, as suggested in Dann et al. (2021), which is left for future research.\nIn PAC-Bayes, the prior and posterior distributions can belong to different families. However, it is often practical to consider them belonging to a common distribution family, as it simplifies the computation of KL divergence. Hence, we assume the default and prior policy distributions for $ \\theta$ to be d-dimensional Gaussians with unknown parameters $ \\theta \\sim N(\\mu, \\sigma^2)$. These parameters are updated by minimizing the upper bound.\nBased on equation (6), we solve the following problem"}, {"title": "4.1 Experimental Setup", "content": "prior methods, we sample a random policy function according to this distribution. This function sampling approach is seen in modern popular deep learning meth- ods, for example, in-context learning(Garg et al., 2022).\nFor the second question, an agent has to keep learning as well as forgetting. Too much experienced knowledge kept in memory may decrease the learning efficiency, while too little may be insufficient to learn an effective policy distribution. To address the second question, we derive a relationship between the performance of our algorithm and the number of experienced tasks (denoted as N in a later section) that need to be re- tained in the agent's memory, based on PAC-Bayes theory. We use the negative expected long term re- wards, where the expectation is taken with respect to tasks and policies (also known as the generalization error), as a measure of the algorithm's performance from a statistical perspective.\nFrom our theoretical result, where we provide an ex- pression of this relationship, we discovered a trade-off between this value and the algorithm's performance, which aligns with natural intuition, a double sided ef- fect. In practice, our expression allows us to optimize the performance of our algorithm by optimizing N, although we recommend using hyperparameter tuning. Furthermore, to demonstrate the efficiency of our al- gorithm, we derive its sample complexity from a RL regret perspective, showing that our algorithm learns an optimal policy as more tasks encountered."}, {"title": "5 Conclusion and Future Works", "content": "where $\\phi(\\theta; \\mu, \\sigma)$ is the Multivariate Gaussian PDF:\n$\\min_{q} U(P; \\mu, \\sigma) := \\sum_{i=1}^{N} E_{ \\theta_{l-1} \\sim p(\\theta_{l-1}/g(\\tau_l);q)} [-J_{M_{l,i}} (\\pi_{\\theta_{l-1}}) \\phi(\\theta; \\mu, \\sigma) d\\theta + R (D_{KL} (N(\\mu, \\sigma^2) ||N^{\\prime} (\\mu; \\sigma^2))) $. (7)\nEvaluating the integral in equation (7) analytically is intractable in practice. Therefore, we resort to Monte Carlo Methods, where we sample $ \\theta_{l-1,j} j\\in [M]$ to approx- imate the gradient descent updates by:\n$ \\nabla_{\\mu,\\sigma} \\hat{U}(P, {M_i}_{i\\in [N]}, {\\theta_{l-1,j} }_{j\\in [M]}; \\mu, \\sigma)$ $\n=\\frac{1}{MN} {\\sum_{j=1}^{M} \\sum_{i=1}^{N} -\\nabla_{\\mu,\\sigma} {J_{M_{l,i}} (\\pi_{\\theta_{l-1,j}}\n+ R (D_{KL} (N(\\mu, \\sigma^2) ||N^{\\prime} (\\mu; \\sigma^2))) }, (8)\nwhere $ \\theta_{l-1,j}$ is a sample drawn from $P_{l-1}$ to perform gradient descent during optimization in each iteration.\nMoreover, to ensure the parameters $ \\mu$ and $ \\sigma$ can be updated, we use indirect sampling by first sampling a multivariate standard normal distribution $ \\epsilon_j$. The randomness of the parameter $ \\theta_i$ is then defined as:\n$ \\theta_i = \\mu + \\sigma \\odot \\epsilon_j, \\epsilon_j \\sim N(0, I_d)$. (9)\nAccording to Equation (9), the parameter $ \\theta_i$ is multi- variate normal distributed with $ \\theta_i \\sim N(\\mu, \\sigma^2)$.\nWe propose a practical EPIC algorithm, called EPICG, as presented in Algorithm 2. In this algorithm, a policy is defined as a Gibbs distribution in a linear combina- exp( \\theta^T \\psi_{s,a})\nexp( \\theta^T \\psi_{s,a})\n\". Here, $ \\theta$ can be replaced by a neural network.\nFor the parameterization of $ \\theta$ using a neural network, we provide the details in Appendix \u00a7E.1. In each iter- ation, the agent samples a set of policies $ {\\theta_jj\\in[M]} \\sim P$ from the \"posterior\" policy distribution for every N tasks (Lines 8-9). It then rolls out a set of trajectories $ \\tau = {\\tau_{ixj} }i\\in[N]\\times[M]$ for each task and estimates the cost (Lines 10-11). More specifically, an action a is sampled\nFor the parameterization of $ \\theta$ using a neural network, we provide the details in Appendix \u00a7E.1. In each iter- ation, the agent samples a set of policies $ {\\theta_jj\\in[M]} \\sim P$ from the \"posterior\" policy distribution for every N tasks (Lines 8-9). It then rolls out a set of trajectories $ \\tau = {\\tau_{ixj} }i\\in[N]\\times[M]$ for each task and estimates the cost (Lines 10-11). More specifically, an action a is sampled\n\"\n    }"}]}