{"title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning", "authors": ["Xinyin Ma", "Guangnian Wan", "Runpeng Yu", "Gongfan Fang", "Xinchao Wang"], "abstract": "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.", "sections": [{"title": "Introduction", "content": "Chain-of-Thought (CoT) reasoning has emerged as a powerful technique for enhancing the reasoning capabilities of large language models, particularly in complex tasks such as mathematics and coding that require multi-step inference. By simulating the process of human-like thought progression, CoT enables models to break down complex problems into sub-questions, improving accuracy and interpretability. Those reasoning abilities have also been tested in different domains, such as image generation and visual understanding. Training reasoning models often involves generating extensive reasoning paths through methods such as sampling, tree search or reinforcement learning to ultimately reach the correct answer. However, these long chains often incorporate redundant intermediate steps that can be unnecessary or too complex, and the redundancy in the reasoning paths for training leads to inefficiencies in token usage and increased inference costs. However, crafting an optimal reasoning chain that omits extraneous details is challenging due to the limited availability of intermediate rewards to guide the process and human annotations. Removing some or all of the intermediate steps and then training or distilling the model will degrade the performance. Alternative approaches employ information-theoretic measures or identify an \"overthinking\" solution in QwQ to evaluate the contribution of each sentence to the final answer.\nWe observe that current reasoning models, such as QwQ and DeepSeek-R1 allocate an excessive number of tokens to simple tasks, while potentially providing insufficient tokens for complex tasks. Thus, a long reasoning path is still essential, while maintaining the ability to compress reasoning paths for simpler questions is equally important. To solve this, our goal is to fine-tune a model capable of generating both long and short reasoning paths, rather than being restricted to a compressed form. We offer a new way to control the length of CoT, which we refer to as Length-Compressible Chain-of-Thought Tuning.\nA central component of the proposed method is to identify an update direction in the parameter space, which, by manipulating it, acts as increasing or decreasing the length of CoT. Taking a large step in this direction leads the model to generate a short sequence, while a small step still produces a long and complex reasoning trajectory. We choose to incorporate this update direction by LORA, enabling it to function as an additional branch that facilitates easy modulation of intensity while imposing minimal extra parameters on the model. We explore methods to identify this direction and demonstrate that it offers superior controllability compared to prompt-based approaches, which enables the generation of short CoT that prompt-based methods are unable to achieve. Besides, we observe that the direction can be extrapolated, allowing the reasoning chains to be extended beyond or shortened to lengths unseen in the training set. Leveraging this compressibility, we construct a dataset that pairs long and short reasoning chains for each question. This dataset is then utilized in two ways: (1) to refine the direction for more precise tuning, and (2) to progressively compress the reasoning path.\nWe evaluate our method across different types of models, ranging from a pre-trained LLM with little reasoning ability, LLaMA-3.1-8B and LLaMA-3.2-1.5B-Instruct, to post-trained reasoning models, QwQ-32B-Preview, and distilled reasoning models, DeepSeek-R1. Our results demonstrate that, with training for one time, our approach enables a model to generate reasoning paths of varying lengths, and we can achieve better results than previous chain compression baselines. Besides, our study highlights several interesting findings: (1) Short reasoning paths can sometimes outperform longer ones, underscoring the significance of CoT-Valve in enhancing model efficiency. (2) Not every reasoning chain, despite all leading to the correct final answer, is conducive to model optimization. Excessively long or short chains complicate the distillation of CoT, posing challenges to the model training.\nIn summary, our contributions are: (1) CoT-Valve: Enables elastic control of length for CoT within the parameter space, allowing a single model to generate CoT from short to long. (2) MixChain Dataset: A dataset with reasoning paths of varying lengths for each question. (3) Improved Tuning & Progressive Compression: Refines the direction-tuning process based on MixChain and introduces progressive compression for inference efficiency. (4) Performance & Controllability: Achieves controllable reasoning generation and state-of-the-art results for compressed CoT."}, {"title": "Related Work", "content": "Chain-of-Thought. Chain-of-thought reasoning has shown promising progress in recent years, especially the success of OpenAi-O1 and Deepseek-R1 models. This introduces the test-time scaling law, apart from the traditional scaling law for training. Several approaches have been proposed to boost the language model to have better problem-solving abilities, including the model has its self-reasoning abilities or use Best-of-N, beam search and Monte Carlo Tree Search to search and refine the solution without further finetune the large language models. The outcome reward model and process reward models are also introduced to evaluate the score for the entire solution, especially the final answer and the quality of the reasoning path\nChain Compression in reasoning model. Due to the high computational cost associated with inference in reasoning models, particularly for long-chain reasoning, chain compression has become a critical area of research. attempts to distill the chain-of-thought into System 1 but fails to observe improvements when intermediate steps are omitted. proposes internalizing reasoning steps within the hidden states of models, while several implicit-based approaches aim to compress token-wise generation by transitioning from language space to hidden space. Other studies focus on skipping intermediate reasoning steps or using summarization techniques to generate shorter reasoning chains. Additionally, addresses the over-thinking issue in QwQ and employs SimPO for optimization. Kimi K1.5 proposes merging long-CoT models with short-CoT models in a training-free manner. O1-Pruner adopts reinforcement learning to shorten responses."}, {"title": "Method", "content": "In this section, we provide an in-depth discussion of our method. Section 3.1 introduces a simple yet effective approach that enables a single tuning process to generate models with CoT with different lengths. This stage also serves as an initial step for subsequent refinements. Next, in Section 3.2, we explore multiple scenarios in which we can apply CoT-Valve to construct the dataset MixChain. In Section 3.3, we propose several advanced methods that take advantage of long-to-short datasets to improve precision and control over the generated reasoning paths in compressible fine-tuning."}, {"title": "Length-Compressible CoT Tuning", "content": "Our primary objective is to achieve a new way to control the length of reasoning paths after training a reasoning model. Existing approaches, such as prompt-based control, explicitly define sequence length in the prompt or utilize summary tokens for guidance. However, these methods offer only limited control over the length of CoT generated. For instance, requesting a sequence of less than 20 tokens may result in the model generating over 350 tokens and these methods struggle to produce answers with very short lengths. To address these limitations, we introduce CoT-Valve for training one model but can adjust the length of reasoning paths.\nConsider a reasoning model defined by the parameter \u03b8. For a given question q in the dataset D, the probability of generating an answer a and its reasoning thoughts {$t_i$}$_{i=1}^n$ given the question q can be described by:\n$$p(a \\mid t_1,..., t_n, q; \\theta) \\prod_{i=1}^n p(t_i \\mid t_{<i}, q; \\theta)$$\nwhere {$t_i$}$_{i=1}^n$ might include errors or unnecessary details. With short synthesized or human-annotated explanations {$t_i$}$_{i=1}^m$ with m < n, the training objective is to adjust the parameter in such a way that the chain is shortened while still yielding the correct answer:\n$$max_{\\Delta \\theta} E_{(q,a)\\sim D} p(a \\mid t_1, ..., t_m, q; \\theta + \\Delta \\theta) \\prod_{i=1}^m p(t_i \\mid t_{<i}, q; \\theta + \\Delta \\theta)$$\nand \u0394\u03b8 denotes the change in the parameter space that steers the model towards generating a more concise chain.\nSince the model, with and without \u0394\u03b8, outputs the same final answer, \u0394\u03b8 can be interpreted as a task vector. The task here is to control the length of the CoT, provided that the only difference in the training set lies in intermediate reasoning steps {$t_i$}$_{i=1}^m$. Those reasoning paths are different in length but ultimately lead to the same final answer. Thus, we can control the task vector to achieve the goal of adjusting the length of CoT. Al is designed within a parameter-efficient space, functioning as an external branch for inference that incurs minimal overhead. Controlling this external branch enables the manipulation of the length of the reasoning path.\nTask Arithmetic: Interpolation and Extrapolation of \u0394\u03b8. To manipulate this update within the parameter space, we can control the magnitude of a \u0394\u03b8 as an arithmetic operation. We use two primary operations on \u0394\u03b8 here: interpolation and extrapolation. Let \u03b1 denote the magnitude of \u0394\u03b8 for LoRA.\nWhen \u03b1 falls within the range of (0,1), the model smoothly transitions between longer and shorter reasoning paths, similar to weight interpolation between two models. When \u03b1 > 1, extrapolation is introduced, further shortening the reasoning path beyond what was observed during training. This enables an exploration of the minimal reasoning length required to arrive at a given answer. Thus, by adjusting \u03b1 at inference, we can modulate the model's behavior, with each value of \u03b1 corresponding to different CoT lengths.\nApplication Unlike prompt-based approaches that can only regulate the overall length of the reasoning process using prompt words, \u0394\u03b8 provides finer granularity control. \u0394\u03b8 is served in the external parameter space. This allows for greater flexibility in adjusting the reasoning trajectory. Specifically, it facilitates the selective retention of long-chain reasoning in certain thoughts while applying stronger compression to simpler reasoning segments. As a result, reductions in chain length can be localized to specific portions of the inference process rather than being uniformly applied across the entire reasoning path. We remain the design of this segment selection in future work."}, {"title": "Construct the MixChain Dataset", "content": "A crucial thing for the above process is the construction of the training dataset, especially the reasoning chain {$t_i$}$_{i=1}^m$. To have reasoning chains with different lengths, previous approaches rely on multiple rounds of sampling, selecting reasoning paths under different random seeds, or using some hand-crafted way to remove parts of the answer.\nWe introduce MixChain, a dataset inherently generated by our method that contains reasoning paths of varying lengths. This dataset is structured such that each question is associated with multiple reasoning paths, with lengths progressively decreasing from long to short. By simply adjusting the parameter \u03b1, our approach avoids the need for repeated sampling and achieves this diverse set of reasoning paths. In contrast to multi-sampling techniques, MixChain enables a more reliable and consistent generation of shorter reasoning paths while simultaneously capturing a spectrum of reasoning lengths. To construct MixChain, we consider two possible scenarios:\n\u2022 If a well-annotated dataset with human-labeled solutions is available, such as GSM8K or PRM800k , it can be leveraged to fine-tune the model for generating shorter reasoning chains as a cold start (\u03b81 \u2192 \u03b8'1 and \u03b82 \u2192 \u03b8'2 in Figure 2).\n\u2022 In the absence of a dataset containing explicit reasoning paths, or when only final answers are available without full explanations, training solely on final answers is unlikely to enable the model to generate reasoning steps. To address this limitation, we propose an alternative method for constructing Mix-Chain. Specifically, we leverage an existing base LLM (e.g., LLaMA-3.1-8B or Qwen-32B-Instruct) as \u03b81 and use its corresponding reasoning model (e.g., DeepSeek-R1-Distill-Llama-8B or QwQ-Preview) to derive \u0394\u03b8."}, {"title": "Improved Tuning for CoT-Valve", "content": "In this section, we present two enhanced variants of CoT-Valve: one aimed at achieving improved controllability and the other focused on optimizing the compression ratio of the reasoning paths.\nA More Precise CoT-Valve Paradigm: CoT-Valve++. In the previously proposed CoT-Valve framework, the training process only constrained \u0394\u03b8 to satisfy the final objective with \u03b1 = 1. However, during inference, we expect all positions along this direction to exhibit reasoning trajectories of varying lengths. This leads to the inconsistency between training and inference. With MixChain, we can explicitly incorporate this requirement during training by introducing an additional constraint, ensuring that the model can adapt to reasoning chains of different lengths across all positions in this direction. For each training sample, in addition to the question, answer, and solution, we have introduced a normalized term \u03b2, which represents the factor for the length of the reasoning path. Under this dataset, our training objective is modified to find a parameter update \u0394\u03b8' such that it satisfies:\n$$max_{\\Delta \\theta'} E_{(q,a)\\sim D'} p(a \\mid t_{<m}, q; \\theta + \\beta \\Delta \\theta') \\prod_{i=1}^m p(t_i \\mid t_{<i}, q; \\theta + \\beta \\Delta \\theta')$$\nWhere D' is the Mixchain dataset. Each sample consists of the question q, the answer a, the solution {$t_i$}$_{i=1}^m$ and \u03b2, where \u03b2 is calculated as:\n$$\\beta = \\frac{m_{1}}{m_{max} - m_{min}} - \\frac{m_{min}}{m_{min}}$$\nHere, $m_{min}$ and $m_{max}$ is the length of the shortest solution and longest solution for this question. Based on synthetic samples, we introduce additional constraints that enable us to better identify the updated parameter \u0394\u03b8', facilitating more precise compressibility and controllability.\nProgressive Chain Compression: CoT-Valve+P. The structure of MixChain, which features progressively shorter reasoning paths for each question, facilitates a progressive chain-length compression strategy. This approach is similar to iterative pruning in model compression. In this process, the model is trained with a shorter reasoning path from the dataset at each iteration, rather than training directly with the shortest reasoning CoT. This gradual compression method allows the model to progressively reduce the length of its reasoning paths."}, {"title": "Experiments", "content": "Experimental Setup\nModels. We evaluate our method under several models: QwQ-32B-Preview, DeepSeek-R1-Distill-Llama-8B, LLaMA-3.1-8B, LLaMA-3.2-1B and Qwen-32B-Instruct with LIMO. We tested different scenarios for CoT-Valve:\n\u2022 (Long to Short CoT) For QwQ-32B-Preview (QwQ for abbreviation) and DeepSeek-R1-Distill-Llama-8B (R1-Distill), we used our method to control and compress the length of the reasoning chain.\n\u2022 (Short to Long CoT) For LLaMA-3.1-8B and LLaMA-3.2-1B-Instruct, we applied our method to distill reasoning abilities from QwQ-32B-Preview and incorporated CoT-Valve in the distillation process.\n\u2022 (Short-Long-Short CoT) We tested another setting to first post-train a short-CoT LLM, Qwen-2.5-32B-Instruct, to generate Long CoT and then compress it to Short CoT. CoT-Valve can be applied in both two stages.\nMetrics. We report both accuracy and the number of tokens in the answer for each experiment. Given the trade-off between reasoning path length, model size, and performance, we use a new metric, Accuracy per Computation Unit(ACU), to better capture this balance and evaluate model efficiency. It is defined as:\n$$ACU = \\frac{Accuracy}{\\#Params \\times \\#Tokens}$$\nSince the ACU value typically falls within the range of $10^{-5}$ to $10^{-2}$, we report it in units of $10^{2}$ for improved readability."}, {"title": "From Long-CoT to Short-CoT.", "content": "Controllable Results. We illustrate the result in Figure 3a. First, using ground-truth samples as a cold start, we develop a model capable of generating reasoning paths of various lengths, as demonstrated in 'CoT-Valve' in Figure 3a. CoT-Valve already matches the performance of prompt-based control but can generate shorter reasoning chains. We then extrapolate A to produce even shorter reasoning paths. Then, building on MixChain-C from this first model, we conduct further training by CoT-Valve++. CoT-Valve++ substantially surpasses the baseline and shows greater generalization capabilities in cases of extrapolation.\nCompression Results. We evaluated our method against previous chain compression approaches, with the results detailed in Table 1, Table 2, and"}, {"title": "From Short-CoT to Long-CoT & Short-Long-Short CoT", "content": "Our method can also be applied if a short-CoT model is distilled or post-trained to be a Long-CoT model. The results are shown in Figure 3b, Table 4 and Table 5. We found that CoT-Valve can also effectively control the length of the chains in this setting. Notably, we observed that shorter chains could achieve higher accuracy on GSM8K. Moreover, if the model is trained using the MixChain-Z dataset, the results are significantly better, whether using CoT-Valve (55.5 to 58.9) or just simply SFT (52.7 to 57.0). Additionally, after training a long-chain model, we can employ the MixChain dataset to reduce the length of its reasoning chains further. As illustrated in Figure 3c, the results suggest that initially training the chains to be long and subsequently compressing them to be shorter (Results with Long-to-Short) can yield better performance than directly using CoT-Valve in the short-to-long stage (Results with Short-to-Long). This demonstrates significant potential for compressing the reasoning chains. We can also surpass the result of Gemini-Flash-Thinking, with the same accuracy but fewer tokens (10810.5 v.s. 8174.8)\nTraining dynamics does not have the same effect as CoT-Valve. We also explore whether intermediate training steps can achieve similar effects. As depicted in Figure 3c, during the early training phases, the length of the CoT increases but does not correspond with the same rapid improvement in performance. As training progresses, the token length begins to decrease while performance improves. CoT-Valve exhibits a distinct pattern, smoothly bridging the gap between the length of CoT and performance."}, {"title": "Observations", "content": "Based on the results from LLaMA-3.1-8B, LLaMA-3.2-1.5B, QwQ, DeepSeek-R1-Distill-Llama-8B and Qwen2.5-32B-Instruct with LIMO, we summarize the following observations:\n\u2022 Longer reasoning chains are not always the best on simple datasets. Across nearly all models, we find that those directly trained on long CoT data typically do not show the best performance. These models often underperform compared to those generated through CoT-Valve, which results in shorter but more accurate reasoning chains. This trend is particularly pronounced in smaller models. For instance, in the LLaMA-3.2-1B model, training on QwQ synthesized data yields an accuracy of 52.69 with 759.3 tokens. However, using CoT-Valve, we can achieve an accuracy of 55.50 with only 267.0 tokens. However, we do not observe this phenomenon in more complex datasets, indicating that while the reasoning model may be redundant for simple datasets, it still requires test-time scaling to effectively handle complex datasets.\n\u2022 Some reasoning chains are difficult for the model to learn, especially for small LLMs. We fine-tuned LLaMA-3.2-1B-Instruct using only one solution from MixChain, where all solutions lead to the same final answer but involve different intermediate reasoning steps. The results, presented in Table 6, indicate that neither the shortest nor the longest chains are optimal for learning. Instead, the model most effectively learns from moderately short chains, achieving the highest accuracy while maintaining a relatively low token count. This phenomenon is particularly evident in smaller models, but it is not observed in larger models. We believe this could be beneficial for the distillation of CoT in small LLMs."}, {"title": "Analysis", "content": "Ablation on Progressive Compression. Table 7 demonstrates the effect of progressive compression."}, {"title": "Conclusion", "content": "In this paper, we propose a method that enables a model to generate reasoning chains of varying lengths instead of the prompt control. Based on this approach, we construct a dataset containing both long and short reasoning chains to further enhance controllability and compression efficiency. Experimental results demonstrate the effectiveness of our method in dynamic reasoning chain control and the compression of CoT. Future research can further explore finer-grained control strategies to improve reasoning efficiency and model controllability."}]}