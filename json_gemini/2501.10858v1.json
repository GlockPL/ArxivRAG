{"title": "Reliable Text-to-SQL with Adaptive Abstention", "authors": ["Kaiwen Chen", "Nick Koudas", "Yueting Chen", "Xiaohui Yu"], "abstract": "Large language models (LLMs) have revolutionized natural language interfaces for databases, particularly in text-to-SQL conversion. However, current approaches often generate unreliable outputs when faced with ambiguity or insufficient context.\nWe present Reliable Text-to-SQL (RTS), a novel framework that enhances query generation reliability by incorporating abstention and human-in-the-loop mechanisms. RTS focuses on the critical schema linking phase, which aims to identify the key database elements needed for generating SQL queries. It autonomously detects potential errors during the answer generation process and responds by either abstaining or engaging in user interaction. A vital component of RTS is the Branching Point Prediction (BPP) which utilizes statistical conformal techniques on the hidden layers of the LLM model for schema linking, providing probabilistic guarantees on schema linking accuracy.\nWe validate our approach through comprehensive experiments on the BIRD benchmark, demonstrating significant improvements in robustness and reliability. Our findings highlight the potential of combining transparent-box LLMs with human-in-the-loop processes to create more robust natural language interfaces for databases. For the BIRD benchmark, our approach achieves near-perfect schema linking accuracy, autonomously involving a human when needed. Combined with query generation, we demonstrate that near-perfect schema linking and a small query generation model can almost match SOTA accuracy achieved with a model orders of magnitude larger than the one we use.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs) has precipitated a paradigm shift in addressing canonical database challenges, encompassing data integration, information retrieval, and query comprehension. These models' sophisticated natural language understanding capabilities facilitate the extraction of structured data from unstructured text with unprecedented semantic fidelity. By leveraging transformer architectures and self-supervised learning on vast corpora, LLMs exhibit remarkable efficacy in discerning complex linguistic patterns and contextual nuances, thereby augmenting traditional database operations with enhanced semantic interpretation. As an example, LLMs can seamlessly integrate disparate data sources [20, 104], enhance the precision of information retrieval systems [60], and improve the understanding and processing of complex queries [13].\nThe application of LLMs to text-to-SQL conversion [34, 40, 42, 69] has emerged as a focal point of research within the database community. Text-to-SQL encompasses the transformation of natural language queries into corresponding SQL statements, leveraging the underlying structure of relational databases. The significance of this research area lies in its potential to democratize data access, enabling non-technical users to interact with databases effectively. Text-to-SQL systems have wide-ranging implications for domains such as interactive data exploration, automated query generation, and natural language interfaces for database systems. However, the task presents several non-trivial challenges, including the inherent ambiguity of natural languages, the rigid syntactic and semantic constraints of SQL, and the necessity for precise schema mapping and entity resolution within the context of the query."}, {"title": "2 Background and Problem Statement", "content": ""}, {"title": "2.1 General Text-to-SQL Framework", "content": "Text-to-SQL translates a natural language question Q into an SQL query for database D. We outline steps and design space of LLM-powered Text-to-SQL frameworks.\nPre-Processing: Pre-processing, links schema to the question, aiding SQL generation. Schema linking is crucial but error-prone in LLM-based systems [75].\nSQL Generation: SQL generation uses LLMs to translate questions into queries. Models take linked schema and questions as input, outputting SQL [34, 75, 80]. New, better-performing models constantly emerge. Prompt engineering enhances fine-tuned models via zero-shot [17] or few-shot learning [34]. Multi-step generation [34, 69, 75, 80, 100], like chain-of-thought reasoning [89], improves accuracy.\nPost-Processing: Post-processing refines generated SQL. Common strategies include:\n(1) Self-Correction: Models review generated SQL to address issues [75].\n(2) Self-Consistency: Multiple valid queries are executed; voting determines the most consistent [69].\n(3) Execution-Guided SQL Selector: Sequentially executes generated SQLs, choosing the first error-free execution [39, 70]."}, {"title": "2.2 Schema Linking", "content": "Schema linking involves precisely identifying all the relevant columns and tables in a large database that are necessary to answer a given natural language query. At a high level, in LLM-based methods, this process involves querying an LLM possibly after supervised fine-tuning\u00b9. This LLM (referred to as the schema linking model) accepts a natural language query as input (along with the database schema and other applicable metadata) and outputs the tables and columns relevant to the query.\nSpecifically, given a database D with tables \\(T = \\{T_1, T_2, . . ., T_v\\}\\), the columns \\(c_i = \\{C_{i1}, C_{i2}, . . ., C_{ik}\\}\\) for each table \\(T_i\\), and a natural language question Q, the goal of schema linking is to:\n(1) Identify the relevant tables T' \u2286 T for forming an answer to Q (referred as table linking)\n(2) Extract the relevant columns c' from \\(c_i\\) for each table \\(T_i \\in T'\\) for forming an answer to Q (referred as column linking)\nThese lists of tables T' and columns c' will be used to formulate an SQL query that answers the question Q."}, {"title": "2.3 Text-to-SQL with Abstention", "content": "Using a schema linking model, we aim to improve token generation during the answer formation process - specifically in predicting tables and columns pertinent to the query - by identifying critical branching points. These branching points are those generated tokens that are erroneous and steer the generation away from the correct answer (alternatively instigate branching off from the correct answer). Moreover, we will establish guarantees on the probability of detecting a branching point during a generation. Detecting branching points forms the basis of our abstention mechanisms.\nNote that we allow the schema linking model to abstain during the pre-processing step, which has two major advantages:\n(1) Computation Efficiency: Enabling the model to abstain during the pre-processing phase significantly reduces unnecessary computations, which are most likely to lead to erroneous SQL queries, thereby enhancing efficiency during SQL generation.\n(2) Seamless Integration with Advanced Models: Our approach facilitates the seamless integration of newly developed text-to-SQL models in the SQL generation phase without extensive re-engineering. We view this as highly important, as SQL generation models are advancing rapidly with new models available frequently. As a design principle, our proposal is agnostic to the SQL generation model, and we can adopt the best performing one. Abstention during the schema linking process ensures that only high-confidence schemas are delivered to the SQL generation models, thereby improving overall performance and reducing errors.\nThe abstention mechanism also safeguards against low-confidence predictions, enhancing the overall reliability of the system. When the model abstains, as we will detail below, alternative strategies such as human-in-the-loop can be employed to ensure the computation progresses with accuracy. This involves a human expert reviewing the model output and possible suggestions, and resolving the issue that instigated abstention, thereby allowing the model to continue generation while maintaining the system reliability."}, {"title": "3 Reliable Text-to-SQL", "content": "In this section, we present our proposed framework, Reliable Text-to-SQL (RTS), detailing all of its components."}, {"title": "3.1 Branching Points During Schema Linking", "content": "In this section, we outline our strategy for developing an abstention mechanism in the Text-to-SQL schema linking process. In the literature, quantifying uncertainty in LLM predictions is an active research topic but is recognized as challenging due to the nuances of language semantics and form [37]. Recent studies have proposed methods to address these challenges for free-form language generation in LLMs [37, 63]. These methods leverage entropy [92], semantic analysis [50], and logits or hidden state information [7, 59, 79] to quantify uncertainty in pre-trained LLMs during generation.\nOur problem of interest differs from free-form language generation in two key aspects. First, our generation task is semi-structured, expecting the model to output only a list of tables and columns present in the given schema. Second, and most importantly, our focus is on a supervised fine-tuned LLM for schema linking. As previously reported, supervised fine-tuned LLMs exhibit over-confidence [101] in their predictions. Namely, during answer generation, the probability distribution of the next generated token is highly skewed, regardless of the correctness of the generation. To demonstrate this, we recorded the softmax probability during schema-linking generation using a supervised fine-tuned Deepseek model on the BIRD development dataset. As illustrated in Figure 3a, the output probabilities are concentrated around 1 for both correct and incorrect schema predictions. This phenomenon renders useless any logit-based method built on the intuitive expectation that an error in the generation is likely to correspond to a token generated with low probability values [92].\nFor these reasons, we develop techniques to identify erroneous token generations tailored to the schema linking problem during Text-to-SQL. To achieve this, we begin by examining the errors made during the schema-linking process (assuming access to ground truth data) which is critical for accurate Text-to-SQL generation. In this section, we will use table linking as an illustrative example, as the methodology for column linking is similar.\nLet T be the set of tables we have to schema link against. Let \\(T_t\\) be the set of tokens in these tables. We constrain the model's token level generation to only generate tokens in \\(T_t\\) utilizing constraint generation [10]. We denote the model's token-level generation for a specific schema linking instance, as \\(x_1,..., \\hat{x}_m\\), while the ground truth tokens are \\(x_1, ..., x_m\\). Each generated token \\(\\hat{x}_i\\) is compared with the corresponding ground truth token \\(x_i\\), and the first token where the model generation deviates from ground truth is referred to as the branching point, denoted as \\(\\hat{x}_b\\) for the generated token"}, {"title": "3.2 Branching Point Predictor (BPP)", "content": "We now present the design of our branching point predictor (BPP) that utilizes the predictors \\(u_1,..., u_n\\) encompassing probabilistic guarantees for the detection of branching points during schema linking token generation. We start by introducing a formalism of conformal prediction [72, 93] 2 that will be utilized in the following sections.\nLet (x, y) be a random pair with joint distribution D on X \u00d7 Y, where X is the feature space and y is the label space. For a given error level \u03b1 \u2208 (0, 1), for any classifier, conformal prediction constructs a prediction set C : X \u2192 2Y such that [72, 93]:\n\\(p(y \\in C(x)) \\geq 1-\\alpha\\)   (1)\nNote that C(x) is a subset of the label space and conformal prediction bounds the probability that the correct label is a member of that set. This guarantee is valid under the assumption of data exchangeability, which is a weaker condition than the independent and identically distributed (i.i.d.) assumption [93]. It remains applicable, albeit in a modified form, even when this assumption is not met [8], thereby extending its generality and applicability. Let \\((x_1, y_1), ..., (x_{N_d}, Y_{N_a})\\) be the held-out calibration data, and \\((X_{test}, Y_{test})\\) be a test point. Conformal prediction proceeds as follows: First, define a nonconformity measure A : (X \u00d7 Y \u00d7 (X \u00d7 Y)^Nd \u2192 R). The intent of A is to quantify how related/unrelated a new point is to the calibration data. Then, for each y \u2208 y, we can compute its nonconformity score Ri as follows:\n\\(R_i = A((x_i, Y_i), (x_j, y_j): j \\neq i), for i = 1, ..., N_d\\)\nand the nonconformity score for the test data point as:\n\\(R_{test} = A ((x_{test}, y), (x_i, y_i) : i = 1, ..., N_d)\\)\nWe utilize the nonconformity score to compute the \u03c0-value for each y:\n\\(\\pi(y) = \\frac{|i: R_i \\geq R_{test} + 1|}{N_d + 1}\\)\nFinally, the prediction set is then defined as:\n\\(C(x_{test}) = \\{y \\in Y : \\pi(y) > \\alpha\\}\\)\nUnder the assumption that X \u00d7 Y is exchangeable, the prediction set satisfies Equation 1. It is important to highlight that this form of guarantee is marginal [93], indicating that the probability is calculated over a random draw from the calibration data and the test point. A similar guarantee can be provided for non-exchangeable distributions [8]."}, {"title": "3.2.2 Conformal Single Layer BPP (sBPP)", "content": "We propose to employ the conformal prediction methodology to construct prediction sets for a classifier \\(u_i\\). This classifier assigns scores to tokens generated at layer i of the schema linking model, indicating the probability (a score) of each token being a branching point, as depicted in Figure 5. The classifier \\(u_i\\) will be trained using data from \\(D_{branch}\\), specifically the i-th hidden state vectors and all tokens from schema linking tasks \\(d_i \\in D_{branch}\\), along with their corresponding ground truth values \\((h_j^i, s_j)  \\le j \\le m, \\forall l, d_i \\in D_{branch}\\). Let \\(D_i\\) denote"}, {"title": "3.2.3 Conformal Multi-Layer BPP (mBPP)", "content": "We repeat the construction in Section 3.2.2 for each of the n classifiers \\(u_1... u_n\\). Let \\(C_i = C_i(x) \\{0, 1\\}\\) denote the conformal prediction set derived by \\(u_i\\) on some input x. This set includes the labels (0, 1, or both) predicted by classifier \\(u_i\\) for the corresponding token. To ease notation, we will assume that for the calibration set of each \\(u_i\\) exchangeability holds, but all discussion applies equally for the non-exchangeable case.\nMajority Vote. We first present an intuitive and robust procedure for aggregating the predictions of the classifiers to declare whether"}, {"title": "3.3 Abstention Mitigation", "content": "Upon identification of a branching point, the model can be explicitly directed to cease further generation. This approach ensures the enforcement of the analytical results presented in Section 3.2. Subsequently, we explore methodologies to mitigate abstentions, either through process self-correction or by soliciting human intervention. Algorithm 2 is initiated following the identification of a branching point and yields a list of tables for inspection. The algorithm accepts three inputs along with the schema linking model: the set of potential tables T for linking, the sequence of tokens generated up to that point, and the identified branching point. It utilizes a decode function that takes as arguments a sequence S of tokens and the set of tables T. The decode function operates by concatenating the tokens in S from left to right to form table names, which are then compared against the tables in T. The list of tables returned is determined by the set difference of the decode result computed on the sequence before and after the branching point. When applying decode in the sequence after the branching point, if a token subsequence of S cannot be matched to any table name (note that such a subsequence will invariably be a suffix of S), we request that the model continues generation until a table name in T is identified by decode.\nSurrogate Filter. Upon identification of a branching point, a potential corrective action involves attempting to rectify the erroneous token prediction. Algorithm 2 yields a set of one or more tables to which the branching point is attributed. These tables may either indicate a source of ambiguity within the schema or suggest that the model erroneously predicts these tables, consequently impeding further generation."}, {"title": "4 Experimental Evaluation", "content": "In this section, we present the results of a detailed experimental evaluation assessing the impact of RTS both in schema linking accuracy and subsequent SQL generation. We detail our experimental environment, benchmark datasets utilized, evaluation metrics, and methodology. We stress that RTS is a framework that can be utilized by any schema linking methodology utilizing a transparent LLM. For that reason, we keep the schema linking model simple and focus on the evaluation of the impact of RTS. In this work, we assume that the training distribution aligns with the testing distribution. Addressing performance consistency under data drift, which may require alternative approaches like transfer learning, is outside the scope of this paper."}, {"title": "4.1 Experimental Methodology", "content": "We perform empirical evaluations using two relational schema-to-SQL query generation benchmarks: Spider [109] and BIRD [58]. Adhering to established experimental protocols from prior literature, we utilize the training corpus to train our model and assess its efficacy on the validation and test datasets. However, our research objective centers on enhancing the robustness of natural language to SQL query translation. Given that our methodology produces output with a non-standard schema (incorporating abstention), incompatible with the test set submission specifications, we are unable to evaluate our approach on the held-out test data. As a result, our primary performance metrics and reliability assessments are conducted on the publicly accessible validation set and test set for Spider 1.0, which allows for more granular analysis. Moreover, training each BPP requires a labeled dataset, which is readily available in benchmarks. In operational database settings, one can enhance database logs-containing at least the SQL query and, potentially, the result-by adding query descriptions through human curation or by generating them directly using an LLM.\nAll the experiments were conducted on a high-performance workstation. The workstation is equipped with a 16-core AMD Ryzen 5955W CPU and a single NVIDIA A100 GPU with 80 GB of GPU RAM."}, {"title": "4.1.2 Baseline Models", "content": "Due to the unique setting of RTS (namely introducing abstention in the text to SQL process) we opt for a simple transparent LLM-based schema linking model. We stress that any such model can be readily utilized with our approach. Thus we focus on how effective and accurate the abstention mechanisms we introduce are and their impact on the ensuing schema linking task. Our primary objective is to enhance the reliability of any schema-linking model by incorporating abstention mechanisms,"}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 Uncertainty Quantification", "content": "LLMs have been known to exhibit over-confidence [101], poorly calibrated [24, 46], and hallucinations [44], leaving the uncertainty quantification a critical problem for real-world applications. While uncertainty quantification has also been actively studied [2, 36, 71] in deep learning, with typical methods including Bayesian methods [32] and ensemble methods [51], such methods are not suitable for LLM inference due to unique characteristics of LLM such as large space and similar semantics of the outputs [36, 37]. Uncertainty quantification for LLM can be divided into two categories: transparent-boxes and opaque-boxes approaches. Transparent-box approaches utilize additional information from within the model, such as logits [68, 92], internal states [7, 59, 79], or semantics [50] to quantify uncertainty. However, these approaches require access to internal information and often necessitate additional supervised training [37]. Opaque-boxes methods, such as linguistic confidence methods [61, 67] are able to output uncertainty in natural language but require additional calibration procedures [67, 101], while consistency methods [66, 87, 97] are effective but come with significant"}, {"title": "5.2 Text-to-SQL", "content": "Text-to-SQL has been studied for decades [47, 48]. Traditional methods primarily rely on parsing trees and human-crafted rules [83], which support only subsets of natural language [73, 98, 110] or require user interactions [54], thus limiting their applications. With advances in deep learning [47], a major line of work utilizes pretrained language models (PLM), such as BERT [25, 105], Grappa [107], T5 [77], approaching the problem as a sequence-to-sequence learning task [88] using encoder-decoder architectures [26, 91]. To enforce the grammatical rules of SQL, rather than generating SQL tokens directly [26, 62, 84, 102, 115], most works utilize grammar-based [12, 16, 27, 33, 41, 82, 95, 108] or sketch-based approaches [22, 31, 52, 55, 65, 103, 106] for decoders. However, such approaches require extensive training corpora and the model capabilities may be limited by their sizes and architectures.\nLLM-based approaches have drawn much attention recently due to the success of LLM [20, 104]. These studies primarily focus on in-context learning settings [13], utilizing LLMs with prompt engineering techniques [34, 58, 64, 89]. While there is some exploration of prompt design in the zero-shot setting [17], most of these works concentrate on few-shot methods [34], where a limited number of demonstrations are available in the context as examples. These in-context learning approaches address text-to-SQL from one or more aspects, including question decomposition [6, 29, 34, 69, 75, 80, 89, 100], demonstration selection [18, 34, 39, 40, 69, 80], prompt design [6, 34, 40, 42, 45, 89, 112], and additional procedures such as ranking [29, 111, 114], self-correction [75], majority voting [69], or utilizing execution results [21, 39, 40, 42, 70, 86]. For instance, DIN-SQL [75] proposes to decompose the problem into subtasks, including schema linking, query classification & decomposition, SQL generation, and self-correction, while PURPLE [80] proposes to focus on demonstration selection with four modules consists of schema pruning, skeleton prediction, demonstration selection, and database adaption. Additionally, other works have explored applying supervised fine-tuning techniques on LLMs, achieving significant generation speedup [49] and quality improvements [34, 56]."}, {"title": "5.3 Schema Linking", "content": "Schema linking [48, 53] has been widely applied due to its pivotal role in improving SQL generation performance [33, 41, 95] and domain generalization [53, 94]. Early works utilize simple heuristics such as string matching to identify columns/tables from natural language [14, 41, 106], mainly as a pre-processing step, which could result in inaccurate linking [28]. To enhance performance, learned methods have been introduced to resolve schema linking either as a separate problem [19, 28, 65], or as a component in the network,"}, {"title": "6 Conclusions", "content": "In this paper, we present the Reliable Text-to-SQL (RTS) framework, which focuses on the schema linking phase and autonomously detects potential errors. We propose the Branching Point Predictor (BPP) that utilizes conformal prediction techniques on the hidden layers of LLM for schema linking with probabilistic guarantees. Through extensive experiments on commonly used benchmarks, we validate the effectiveness of our proposed framework, stressing the potential of combining LLMs with human feedback for robust and reliable applications."}]}