{"title": "ICLR: In-Context Learning of Representations", "authors": ["Core Francisco Park", "Andrew Lee", "Ekdeep Singh Lubana", "Yongyi Yang", "Maya Okawa", "Kento Nishi", "Martin Wattenberg", "Hidenori Tanaka"], "abstract": "Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy \"graph tracing\u201d task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "A growing line of work demonstrates that large language models (LLMs) organize representations of specific concepts in a manner that reflects their structure in pretraining data (Park et al., 2024c;d; Engels et al., 2024; Abdou et al., 2021; Patel & Pavlick, 2022; Anthropic AI, 2024; Gurnee & Tegmark, 2023; Vafa et al., 2024; Li et al., 2021; Pennington et al., 2014). More targeted experiments in synthetic domains have further corroborated these findings, showing how model representations are organized according to the data-generating process (Li et al., 2022; Jenner et al., 2024; Traylor et al., 2022; Liu et al., 2022b; Shai et al., 2024; Park et al., 2024b; Gopalani et al., 2024). However, when a model is deployed in open-ended environments, we can expect it to encounter novel semantics for a concept that it did not see during pretraining. For example, assume that we describe to an LLM that a new product called strawberry has been announced. Ideally, based on this context, the model would alter the representation for strawberry and reflect that we are not referring to the pretraining semantics (e.g., the fruit strawberry). Does this ideal solution transpire in LLMs?\nMotivated by the above, we evaluate whether when provided an in-context specification of a concept, an LLM alters its representations to reflect the context-specified semantics. Specifically, we propose"}, {"title": "2 EXPERIMENTAL SETUP: IN-CONTEXT GRAPH TRACING", "content": "We first define our setup for assessing the impact of context specification on how a model organizes its representations. In the main paper, we primarily focus on Llama3.1-8B (henceforth Llama3) (Dubey et al., 2024), accessed via NDIF/NNsight (Fiotto-Kaufman et al., 2024). We present results on other models-Llama3.2-1B / Llama3.1-8B-Instruct (Dubey et al., 2024) and Gemma-2-2B / Gemma-2-9B (Gemma Team, 2024)-in App. \u0421.2.\nTask. Our proposed task, which we call in-context graph tracing, involves random walks on a predefined graph G. Specifically, inspired by prior work analyzing structured representations learned by sequence models, we experiment with three graphical structures: a square grid (Fig. 1 (a)), a ring (Fig. 2 (a)), and a hexagonal grid (Fig. 10). Results on hexagonal grid are deferred to appendix due to space constraints. To construct the square grid, we randomly arrange the set of tokens in a grid and add edges between horizontal and vertical neighbors. We then perform a random walk on the graph, emitting the visited tokens as a sequence (Fig. 1 (b)). For the ring, we add edges between neighboring nodes and simply sample random pairs of neighboring tokens on the graph (Fig. 2 (b)). Nodes in our graphs, denoted T = {T0, T1, ..., Tn}, are referenced via concepts that the model is extremely likely to have seen during pretraining. While any choice of concepts is plausible, we select random tokens that, unless mentioned otherwise, have no obvious semantic correlations with one another (e.g., apple, sand, math, etc.). However, these concepts have precise meanings associated with them in the training data, necessitating that to the extent the model relies on the provided context, the representations are morphed according to the in-context graph. We highlight that a visual analog of our task, wherein one uses images instead of text tokens to represent a concept, has been used to elicit very similar results with human subjects as the ones we report in this paper using LLMs (Garvert et al., 2017; Whittington et al., 2020; Mark et al., 2020; 2024; Brady et al., 2009). We also note that our proposed task is similar to ones studied in literature on in-context RL, wherein one provides exploration trajectories in-context to a model and expects it to understand the environment and its dynamics (a.k.a., a world model) (Lee et al., 2024b; Laskin et al., 2022)."}, {"title": "3 RESULTS", "content": ""}, {"title": "3.1 VISUALIZING INTERNAL ACTIVATION USING PRINCIPAL COMPONENTS", "content": "Since we are interested in uncovering context-specific representations, we input sequences from our data-generating process to the model and first compute the mean activations for each unique token \\(\\tau\\in\\mathcal{T}\\). Namely, assume a given context \\(\\mathcal{C} := [C_0,..., C_{N-1}]\\), where \\(c_i \\in \\mathcal{T}\\), that originates from an underlying graph \\(\\mathcal{G}\\). At each timestep, we look at a window of \\(N_w\\) (=50) preceding tokens (or all tokens if the context length is smaller than \\(N_w\\)), and collect all activations corresponding to each token \\(T\\in \\mathcal{T}\\) at a given layer \\(l\\). We then compute the mean activations per token, denoted as \\(\\textbf{h}_\\tau^l \\in \\mathbb{R}^d\\). We further denote the stack of mean token representations as \\(H^l(\\mathcal{T}) \\in \\mathbb{R}^{n \\times d}\\). Finally, we run PCA on \\(H^l(\\mathcal{T})\\), and use the first two principal components to visualize model activations (unless stated otherwise). We note that while PCA visualizations are known to suffer from pitfalls as a representation analysis method, we provide a thorough quantitative analysis in Sec. 4 to demonstrate that the model re-organizes concept representations according to the in-context graph structure, and prove in Sec. 5 that the structure of the graph is reflected in the PCA visualizations because of this re-organization of representations. We also provide further evidence on the faithfulness of PCA by conducting a preliminary causal analysis of the principal components, finding that intervening on concept representations' projections along these components affects the model's ability to accurately predict valid next node generations (App. C.4).\nResults. Figs. 1, 2 demonstrate the resulting visualizations for square grid and ring graphs, respectively (more examples are provided in the Appendix; see Fig. 9, 10). Strikingly, with enough exemplars, we find representations are in fact organized in accordance with the graph structure underlying the context. Interestingly, results can be skewed in the earlier layers towards semantic priors the model may have internalized during training; however, these priors are overridden as we go deeper in the model. For example, in the ring graph (see Fig. 2), concepts apple and orange are closer to each other in Layer 6 of the model, but become essentially antipodal around layer 26, as dictated by the graph; the antipodal nature is also more prominent as context length is increased.\nWe also observe that despite developing a square-grid structure when sufficient context length is given (see Fig. 1), the structure is partially irregular; e.g., it is wider in the central regions, but narrowly arranged in the periphery. We find this to be an artifact of frequency with which a concept is seen in the context. Specifically, due to lack of periodic boundary conditions, concepts that are present in the inner 2\u00d72 region of the grid are visited more frequently during a random walk on the graph, while the periphery of the graph has a lower visitation frequency. The representations reflect this, thus organizing in accordance with both structure and frequency of concepts in the context.\nOverall, the results above indicate that as we scale context size, models can re-organize semantically unrelated concepts to form task-specific representations, which we call in-context representations. Intriguingly, these results are broadly inline with theories of inferential semantics from cognitive science as well (Harman, 1982; Block, 1998)."}, {"title": "3.2 SEMANTIC PRIOR VS. IN-CONTEXT TASK REPRESENTATIONS", "content": "Building on results from the previous section, we now investigate the impact of using semantically correlated concepts. Specifically, we build on the results from Engels et al. (2024), who show that representations for days of the week, i.e., {Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday}, organize in a circular geometry. We randomly permute the ordering of these concepts, arrange them on a 7-node ring graph similar to the previous section (see Fig. 3a), and evaluate whether the in-context representations can override the strong pretraining prior internalized by the model.\nResults. Fig. 3 (b, c) demonstrate the resulting visualizations. We find that when there is a conflict between the semantic prior and in-context task, we observe the original semantic ring in the first two principal components. However, the components right after in fact encode the context-specific structure: visualizing the third and fourth principal components shows the newly defined ring structure. This indicates that the context-specified structure is present in the representations, but does not dominate them. In Fig. 14, we report the model's accuracy on the in-context task, finding that the model overrides the semantic prior to perform well on our task when enough context is given."}, {"title": "4 EFFECTS OF CONTEXT SCALING: EMERGENT RE-ORGANIZATION OF REPRESENTATIONS", "content": "Our results in the previous section demonstrate models can re-organize concept representations in accordance with the context-specified semantics. We next aim to study how this behavior arises as context is scaled is there a continuous, monotonic improvement towards the context-specified structure as context is added? If so, is there a trivial solution, e.g., regurgitation based on context that helps explain these results? To analyze these questions, we must first define a metric that helps us gauge how aligned the representations are with the structure of the graph that underlies the context.\nDirichlet Energy. We measure the Dirichlet energy of our graph G's structure by defining an energy function over the model representations. Specifically, for an undirected graph G with n nodes, let \\(A \\in \\mathbb{R}^{n \\times n}\\) be its adjacency matrix, and \\(x \\in \\mathbb{R}^n\\) be a signal vector that assigns a value \\(x_i\\) to each node i. Then the Dirichlet energy of the graph with respect to x is defined as\n\\[E_G(x) = \\sum_{i,j} A_{i,j} (x_i - x_j)^2. \\]  (1)\nFor a multi-dimensional signal, the Dirichlet energy is defined as the summation of the energy over each dimension. Specifically, let \\(X \\in \\mathbb{R}^{n \\times d}\\) be a matrix that assigns each node i with a d-dimensional vector xi, then the Dirichlet energy of X is defined by\n\\[E_G(X) = \\sum_{k=1}^d \\sum_{i,j} A_{i,j} (X_{i,k} - X_{j,k})^2 = \\sum_{i,j} A_{i,j} ||X_i - x_j||^2. \\]  (2)\nOverall, to empirically quantify the formation of geometric representations, we can measure the Dirichlet energy with respect to the graphs underlying our data generating processes (DGPs) and our mean token activations h:\n\\[E_G(H^l (\\mathcal{T})) = \\sum_{i,j} A_{i,j} ||h_i^l - h_j^l||^2, \\]  (3)\nwhere \\(H^l(\\mathcal{T}) \\in \\mathbb{R}^{n \\times d}\\) is the stack of our mean token representations \\(h_i^l\\) at layer l and i, j\u2208 T are tokens from our DGP at a certain context length. We note \\(H^l(\\mathcal{T})\\) is a function of context"}, {"title": "4.1 RESULTS: EMERGENT ORGANIZATION AND TASK ACCURACY IMPROVEMENTS", "content": "We plot Llama3's accuracy at the in-context graph tracing task alongside the Dirichlet energy measure (for different layers) as a function of context. Specifically, we compute the \"rule following accuracy\", where we add up the model's output probability over all graph nodes which are valid neighbors. For instance, if the graph structure is apple-car-bird-water and the current state is car, we add up the predicted probabilities for apple and bird. This metric simply measures how well the model abides by the graph structure.\nResults are reported in Fig. 4. We see once a critical amount of context is seen by the model, accuracy starts to rapidly improve. We find this point in fact closely matches when Dirichlet Energy reaches its minimum value: energy is minimized shortly before the rapid increase in in-context task accuracy, suggesting that the structure of the data is correctly learned before the model can make valid predictions. This leads us to the claim that as the amount of context is scaled, there is an emergent re-organization of representations that allows the model to perform well on our in-context graph tracing task. We note these results also provide a more quantitative counterpart of our PCA visualization results before.\nIs there a Trivial Solution at play? A simple baseline that would exhibit an increase in performance with increasing context involves the model merely regurgitating a node's neighbors by copying them from its context. We call this the memorization solution. While such a solution would not explain the reorganization of representations, we use it as a baseline to show the model is likely engaging in a more intriguing mechanism. Since our accuracy metric measures rule following, this memorization solution will achieve value 1 if the node has been observed in the context and 0 otherwise. Following our data sampling process then, if we simply choose an initial node at random with replacement, we can express the probability of a node existing in a context of length l as:\n\\[P_{\\text{seen1}}(x) = 1 - \\left(1 - \\frac{1}{n}\\right)^l, \\]  (4)"}, {"title": "5 EXPLAINING EMERGENT RE-ORGANIZATION OF REPRESENTATIONS: THE ENERGY MINIMIZATION HYPOTHESIS", "content": "Building on the results from previous section, we now put forward a hypothesis for why we are able to identify such structured representations from a model: we hypothesize the model internally runs an energy minimization process in search of the correct structural representation of the data (Yang et al., 2022), similar to claims of implicit optimization in in-context learning proposed by prior work in toy settings (Von Oswald et al., 2023a;b). More formally, we claim the following hypothesis.\nHypothesis 5.1. Let n be the number of tokens, d be the dimensionality of the representations, and \\(H^{(l,t)}(\\mathcal{T}) \\in \\mathbb{R}^{n \\times d}\\) be the stack of representations for each token learned by the model at layer l and context length t, then \\(E_G(H^{(l,t)}(\\mathcal{T}))\\) decays with context length t."}, {"title": "5.1 MINIMIZERS OF DIRICHLET ENERGY AND SPECTRAL EMBEDDINGS.", "content": "We call the k-th energy minimizer of \\(E_G\\) the optimal solution that minimizes \\(E_G\\) and is orthogonal to the first k 1 energy minimizers. Formally, the energy minimizers \\(\\{z^{(k)}\\}_{k=1}^S\\) are defined as the"}, {"title": "5.2 ENERGY MINIMIZATION AND GRAPH CONNECTIVITY", "content": "Given the relationship between spectral embeddings (i.e., energy minimizers) and the principal components observed in our results (Figs. 1, 2), we claim that the model's inference of the underlying structure is akin to an implicit energy minimization. To further analyze the implication of this claim, we show that the moment at which we can visualize a graph using PCA is the moment at which the model has found a large connected component (i.e., the graph's structure). Specifically, consider an unconnected graph \\(\\hat{\\mathcal{G}}\\, i.e., G has multiple connected components. Then, there are multiple degenerate solutions to the energy minimization problem, which will be found by PCA. Specifically, suppose G has q connected components, with \\(U_i\\) denoting the set of nodes of the i-th component."}, {"title": "In-context emergence: A hypothesis.", "content": "Our results in Fig. 5 showed an intriguing breakpoint that is reminiscent of a second-order phase transition (i.e., an undefined second derivative). As shown in Fig. 8, we in fact find this behavior is extremely robust across graphs of different sizes, and shows a power-law scaling trend with increasing graph size (see App. C.7 for several more results in this vein, including different graph topologies). Given the relationship offered between energy minimization and discovery of a connected component (graph structure) in our analysis above, a possible framework to explain these results may be the problem of bond-percolation on a graph (Newman, 2003; Hooyberghs et al., 2010): in bond-percolation, one starts with an unconnected graph and slowly fills edges to connect its nodes; as edges are filled, there is a second-order transition after which a large connected component emerges in the graph. The nature of the transition observed in our experiments (Fig. 8) and the theoretical connection between energy minimization and existence of a connected component provide some evidence towards the plausibility of this hypothesis. However, we believe the analogy is still loose, for our graph sizes are relatively small (likely causing significant finite-size effects) and the experiments need to corroborate any scaling theory of the transition point from percolation literature would require running graphs with at least 2 orders-of-magnitude difference in their sizes. However, the consistency of the hypothesis with our empirical results and analysis implies that investigating it further may be fruitful."}, {"title": "6 RELATED WORK", "content": "Model Representations. Researchers have recently discovered numerous structured representations in neural networks. Mikolov et al. (2013) suggests that concepts are linearly represented in activations, and Park et al. (2024d) more recently suggests this may be the case for contemporary language models. Numerous researchers have found concrete examples of linear representations for human-level concepts, including \u201ctruthfulness\" (Burns et al., 2022; Li et al., 2023b; Marks &\nTegmark, 2024), \"refusal\" (Arditi et al., 2024), toxicity (Lee et al., 2024a), sycophancy (Rimsky et al., 2024), and even \u201cworld models\" (Li et al., 2022; Nanda et al., 2023). Park et al. (2024c) finds that hierarchical concepts are represented with a tree-like structure consisting of orthogonal vectors. A relevant line of work includes that of Todd et al. (2023) and Hendel et al. (2023). Both papers find that one can compute a vector from in-context exemplars that encode the task, such that adding such a vector during test time for a new input can correctly solve the task. Language models do not always form linear representations, however. Engels et al. (2024) find circular feature representa- tions for periodic concepts, such as days of the week or months of the year, using a combination of sparse autoencoders and PCA. Csord\u00e1s et al. (2024) finds that recurrent neural networks trained on token repetition can either learn an \u201conion\u201d-like representation or a linear representation, depending on the model's width. Unlike such prior work, we find that task-specific representations with a desired structural pattern can be induced in-context. To our knowledge, our work offers the first such investigation of in-context representation learning.\nScaling In-Context Learning Numerous works have demonstrated that in-context accuracy improves with more exemplars (Brown et al., 2020; Lu et al., 2022; Bigelow et al., 2023). With longer context lengths becoming available, researchers have begun to study the effect of many-shot prompting (as opposed to few-shot) (Agarwal et al., 2024; Anil et al., 2024; Li et al., 2023c). For instance, Agarwal et al. (2024) reports improved performance on ICL using hundreds to thousands of exemplars on a wide range of tasks. Similarly, Anil et al. (2024) demonstrate the ability to jail-break LLMs by scaling the number of exemplars. Unlike such work that evaluates model behavior, we study the effect of scaling context on the underlying representations, and provide a framework for predicting when discontinuous changes in behavior can be expected via mere context-scaling.\nSynthetic Data for Interpretability Recent works have demonstrated the value of interpretable, synthetic data generating processes for understanding Transformer's behavior, including in-context learning (Park et al., 2024a; Ramesh et al., 2023; Garg et al., 2023), language acquisition (Lubana et al., 2024; Qin et al., 2024; Allen-Zhu & Li, 2023b), fine-tuning (Jain et al., 2023; Lubana et al., 2023; Juneja et al., 2022), reasoning abilities (Prystawski et al., 2024; Khona et al., 2024; Wen et al., 2024; Liu et al., 2022a), and knowledge representations (Nishi et al., 2024; Allen-Zhu & Li, 2023a). While prior work typically pre-trains Transformers on synthetic data, we leverage synthetic data to study representation formation during in-context learning in pretrained large language models."}, {"title": "7 DISCUSSION", "content": "In this work, we show that LLMs can flexibly manipulate their representations from semanatics internalized based on pretraining data to semantics defined entirely in-context. To arrive at these results, we propose a simple but rich task of graph tracing, wherein traces of random walks on a graph are shown to the model in-context. The graphs are instantiated using predefined structures (e.g., lattices) and concepts that are semantically interesting (e.g., to define nodes), but meaningless in the overall context of the problem. Interestingly, we find the ability to flexibly manipulate representations is in fact emergent with respect to context size-we propose a model based on energy minimization to hypothesize a mechanism for the underlying dynamics of this behavior. These results suggest context-scaling can unlock new capabilities, and, more broadly, this axis may have as of yet been underappreciated for improving a model. In fact, we note that, to our knowledge, our work is to first to investigate the formation of representations entirely in-context. Our study also naturally motivates future work towards formation of world representations Li et al. (2023a) and world models (Ha &\nSchmidhuber, 2018) in-context, which can have significant implications toward building general and open-ended systems, as well as forecasting its safety concerns. We also highlight the relation of our experimental setup to similar tasks studied in neuroscience literature Garvert et al. (2017); Mark et al. (2020; 2024), wherein humans are shown random walks of a graph of visual concepts; fMRI images of these subjects demonstrate the formation of a structured representation of the graph in the hippocampal\u2013entorhinal cortex, similar to our results with LLMs."}, {"title": "Limitations.", "content": "We do emphasize that our work has a few limitations. Namely, PCA, or more broadly, low dimensional visualizations of high dimensional data can be difficult to interpret or sometimes even misleading. Despite such difficulties, we provide theoretical connections between energy minimization and principal components to provide a compelling explanation for why structures elicited via PCA faithfully represent the in-context graph structure. Second, we find a strong, but nevertheless incomplete, causal relationship between the representations found by PCA and the model's predictions. We view the exact understanding of how these representations form, and the exact relationship between the representations and model predictions as an interesting future direction, especially given that such underlying mechanism seems to depend on the scale of the context."}, {"title": "C ADDITIONAL RESULTS", "content": ""}, {"title": "C.1 DETAILED LAYER-WISE VISUALIZATION OF REPRESENTATIONS", "content": "In Figure 9 and Figure 10 we provide additional visualizations per layer for each of our models and each of our data generating processes."}, {"title": "C.2 PCA, DIRICHLET ENERGY, AND ACCURACY RESULTS ON OTHER MODELS", "content": "Here we provide results from other language models, i.e., Llama3-1B (Dubey et al., 2024), Llama3-8B-Instruct, Gemma2-2B (Gemma Team, 2024), and Gemma2-9B. In Figure 11, we plot the 2d PCA projections from the last layer of various models for various data generating processes. In Figure 12, we plot the normalized Dirichlet energy curves against accuracy for various language models on various tasks. Across all models and tasks, we see results similar to the main paper."}, {"title": "C.3 STANDARDIZED DIRICHLET ENERGY", "content": "In Fig. 13, we report Dirichlet energy values computed after standardization of representations, i.e., after mean-centering them and normalizing by the standard deviation. This renders the trivial solution to Dirichlet energy minimization infeasible, since assigning a constant representation to all nodes will yield infinite energy (due to zero variance). As can be seen in our results, the plots are qualitatively similar to the non-standardized energy results (Fig. 12), but more noisy, especially for the ring graphs. This is expected, since standardization can exacerbate the influence of noise, yielding fluctuations in the energy calculation."}, {"title": "C.4 CAUSAL ANALYSIS OF REPRESENTATIONS", "content": "In this section we report preliminary causal analyses of our graph representations. While fully understanding the mechanisms behind the formation of such representations, as well as the relationship between said representations and model outputs are an interesting future direction, this is not the focus of our work and thus we only ran proof-of-concept experiments.\nWith that said, we ask: do the principal components that encode our graph representations have any causal role in the model's predictions?\nTo test this, we attempt to \"move\" the location of the activations for one node of the graph to another by simply re-scaling its principal components. Namely, assume activation \\(\\textbf{h}_i^l\\) corresponding to node i at layer l. Say we wish to \u201cmove\u201d the activation to a different target node j. We first compute the mean representation of node j using all activations corresponding to node j within the most recent \\(N_w\\) (= 200) timesteps, notated as \\(\\textbf{h}_j^l\\). Assuming the first two principal components encode the \"coordinates\u201d of the node, we simply re-scale the principal components of \\(\\textbf{h}_i^l\\) to match that of \\(\\textbf{h}_j^l\\).\nWe view this approach as rather rudimentary. Namely, there are likely more informative vectors that encode richer information, such as information about neighboring nodes. However, we do find that the first two principal components have some causal role in the model's predictions.\nWe test our re-scaling intervention on 1,000 randomly generated contexts. For each context, assuming our underlying graph has n nodes, we test \u201cmoving\u201d the activations of the last token i to all n-1 other locations in the graph. We then report the averaged metric across the resulting 1,000 \u00d7 n 1 testcases.\nWe report 3 metrics: accuracy (Hit@1), Hit@3, and \u201caccumulated probability mass\u201d on valid tokens. Hit@1 (and Hit@3) report the percentage of times at which the top 1 (top 3) predicted token is a valid neighbor of the target node j. For \u201caccumulated probability mass\u201d, we simply sum up the probability mass allocated to all neighbors (i.e., valid predictions) of the target node j.\nTable 1 reports our results for our ring and grid tasks. We include results for re-scaling with 2 or 3 principal components, as well as null interventions and interventions with a random vector. Overall, we find that the principal components have some causal effect on the model's output predictions, but does not provide a full explanation."}, {"title": "C.5 EMPIRICAL SIMILARITY OF PRINCIPAL COMPONENTS AND SPECTRAL EMBEDDINGS", "content": "Theorem 5.1 predicts that if the model representations are minimizing the Dirichlet energy, the first two principal components will be equivalent to the spectral embeddings (z(2), z(3).\nHere we empirically measure whether the first two principal components are indeed equivalent to the spectral embeddings. In Table 2, we measure the cosine similarity scores between the principal components and spectral embeddings."}, {"title": "C.6 ACCURACY OF IN-CONTEXT TASKS WITH A CONFLICTING SEMANTIC PRIOR", "content": "What would happen when an in-context task which contradicts a semantic prior is given to a model? Namely, Engels et al. (2024) show that words like days of the week have a circular representation."}]}