{"title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling", "authors": ["William Yicheng Zhu", "Keren Ye", "Junjie Ke", "Jiahui Yu", "Leonidas Guibas", "Peyman Milanfar", "Feng Yang"], "abstract": "Recognizing and disentangling visual attributes from objects is a foundation to many computer vision applications. While large vision-language representations like CLIP had largely resolved the task of zero-shot object recognition, zero-shot visual attribute recognition remains a challenge because CLIP's contrastively-learned vision-language representation cannot effectively capture object-attribute dependencies. In this paper, we target this weakness and propose a sentence generation-based retrieval formulation for attribute recognition that is novel in 1) explicitly modeling a to-be-measured and retrieved object-attribute relation as a conditional probability graph, which converts the recognition problem into a dependency-sensitive language-modeling problem, and 2) applying a large pretrained Vision-Language Model (VLM) on this reformulation and naturally distilling its knowledge of image-object-attribute relations to use towards attribute recognition. Specifically, for each attribute to be recognized on an image, we measure the visual-conditioned probability of generating a short sentence encoding the attribute's relation to objects on the image. Unlike contrastive retrieval, which measures likelihood by globally aligning elements of the sentence to the image, generative retrieval is sensitive to the order and dependency of objects and attributes in the sentence. We demonstrate through experiments that generative retrieval consistently outperforms contrastive retrieval on two visual reasoning datasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual Genome Attribute Ranking (VGARank).", "sections": [{"title": "1 Introduction", "content": "Understanding attributes associated with objects in an image is essential for many computer vision applications, including content recommendation, visual reasoning, and text-to-image generative models. While supervised learning techniques such as classification [24,33,61], detection [23, 54, 55], and segmentation models [8,56] have made significant progress in object recognition tasks, directly adding a branch for object-agnositic attribute prediction [18,19,46] can result in incorrect and counterfactual outputs since it fails to model the co-dependency"}, {"title": "2 Related Work", "content": "We first introduce studies on attribute learning, which mostly rely on hand-crafted probabilistic models without the use of large language models. Then, we summarize existing works on language modeling to specifically introduce PrefixLM to the attribute learning tasks. Finally, we provide an overview of existing prompt learning techniques to introduce our novel approach of generative retrieval, which distillate information from the pretrained PrefixLM.\nVisual attribute recognition involves identifying the properties of visual objects, such as their color, material or shape. Early works had focused on object description (img\u2192att) using classification [18,19,46] or relative ranking models [9, 31, 45, 67,74] to learn attribute presence independent of object category. Some works use attributes as a foundation [2,25,34] for zero-shot object recognition (img, att\u2192obj; e.g., recognizing \u201czebra\u201d by the attributes \u201cblack\u201d, \u201cwhite\", \"striped\"). These works learn an attribute vector space and use it to recognize unseen visual objects based on the marginal probability. In visual object de-tection, some models [3,77] train additional attribute prediction branches using the Viusal Genome dataset [32] to improve model diversity and to create mod-els with multi-tasking capabilities. These models concatenate the visual feature with the ground-truth object class embedding and feed them into an attribute prediction branch (img, obj\u2192att).\nVector space-based approaches has also been studied for attribute recog-nition. For example, [7, 22, 48] apply the CLIP [49]. They use the CLIP em-bedding to compare visual information against predefined attribute prompts (img\u2194obj,att), to determine if the image contains those attributes. In addition to CLIP, [40-43] allow objects and attributes to be projected into the same feature space, while the decoration of attributes on objects is modeled as an operator (img\u2194obj OP att, operator OP could be \u00b1 or linear transform).\nOur innovation lies in the novel view of treating attribute recognition as a language modeling problem. We integrate probability modeling for image, object class, and attribute prediction in an unified image-text model, while leveraging LLM's foundational pre-training.\nLanguage modeling (LM) estimates the probability of a sequence of words being observed in a sentence. Early language models use dense neural net-works [6] or recursive neural networks [70], while recent large language models (LLMs) [15, 16, 49, 69, 75] are based on the transformer architecture [66] because of its strong generalizability. LM has many applications in both NLP and com-puter vision, including question answering(QA) [51,71], conversational question answering [53], visual captioning [1, 12, 59, 73], and visual question answering [4, 21, 76]. These applications can be categorized into three main types of LM: (1) image-text matching [20], (2) masked language modeling [16], and (3) prefix language modeling [6].\nAttribute recognition is a condensed VQA problem that requires predicting the visual attribute of an query object. The foundational methods proposed in the VQA domain mostly combine image-text matching and masked lan-"}, {"title": "3 Approach", "content": "Our proposed generative retrieval is based on image-conditioned prefix language modeling, i.e. image captioning. Given an image v, we aim to generate the corresponding text x = ($1, ..., Sn) by modeling the probability p(xv) using Eq. 1. This equation factors p(x|v) into the product of conditional probabilities [6,50], where at each time step, the model predicts the next token si based on the visual input v and previous tokens (so, ..., Si-1) (so is the start-of-sentence token \"<s>\")."}, {"title": "3.1 Image-Conditioned Language Modeling", "content": "Our proposed generative retrieval is based on image-conditioned prefix language modeling, i.e. image captioning. Given an image v, we aim to generate the corresponding text x = ($1, ..., Sn) by modeling the probability p(xv) using Eq. 1. This equation factors p(x|v) into the product of conditional probabilities [6,50], where at each time step, the model predicts the next token si based on the visual input v and previous tokens (so, ..., Si-1) (so is the start-of-sentence token \"<s>\").\n\n\nThe factorization provided by Eq.1 is advantageous as it breaks down the word generation process into individual probability factors. In Fig. 1 (left), we show that the model can capture various object-attribute compositions during pre-training. As a result, in downstream attribute-related tasks, we can leverage"}, {"title": "3.2 Generative Retrieval for Attribute Classification", "content": "We formalize the simplest attribute classification task as a common foundation for both generative retrieval and contrastive retrieval. Specifically, given an image v and sentence t(1), . . ., t(C) (C is number of classes), retrieval-based classification involves designing a loss function L(v, t) to measure the cost of aligning image v and text t(i) (1 \u2264 i \u2264 C). Thus, zero-shot classification can be achieved through finding the class label c = argmin1<i<c{L(v,t(i))}.\nContrastive retrieval builds on the fact that paired image-text are pro-jected into the same feature space through contrastive learning during pre-training. Assuming the image is encoded as f(v) and the text is encoded as g(t), the contrastive learning objective aims to maximize the inner product between the matched image-text embeddings while minimizing the unmatched ones. This encourages paired image-text samples to have a high similarity while pushing unpaired samples apart. Under the common assumption of unit norm in the embeddings [26, 49, 62], this can be equivalently represented by using the L2 loss to measure the distance between image and text, denoted as L(con) (v, t) = ||f(v) - g(t)||2.\nGenerative retrieval is our proposed approach for visual attribute recog-nition, which utilizes cross-entropy to evaluate the image-text alignment loss, represented as L(gen) (v,t) = - \u2211i=1N p(ti) log qo (v, tj|j<i). Here, p(ti) \u2208 R1\u00d7V (1 \u2264 i \u2264 N, N is the length) represents the one-hot representation of the i-th token of sentence t. To generate the information at the i-th step, the model qe relies on the image v and all previous text tokens tj|j<i to produce a probability distribution qe(v, tj|j<i) \u2208 RV\u00d71 over the vocabulary V. The term -p(ti) log qo (v, tj|j<i) \u2208 R\u00b9 represents the cross-entropy between the i-th token in the sentence t and the model's prediction at the i-th step. Fig. 3 (middle) provides a visual representation of this equation."}, {"title": "3.3 Modeling the Conditional Dependence", "content": "In generative retrieval, we can build different probabilistic models for visual at-tribute recognition by changing word ordering in the to-be-measured sentence (see Fig. 2). Our key contribution to the community is proposing and showcasing its versatility in enabling the design of arbitrary probabilistic model by engineer-ing different structure for the measured sentence. Below, we use {A} to indicate attribute, and {0} to indicate object.\nSentence \"{A}\u201d. This sentence models the simplest dependency for predict-ing attribute based on the image. In this dependency model, we focus on the cross-entropy of classifying the image as having a specific attribute, which can be achieved through a simple classification model. This approach aligns with early"}, {"title": "3.4 Finetuning on Attribute Tasks", "content": "Since the attribute class names have similar lengths, their cross-entropy scores L(gen)(v, t) with the image are expected to fall within a similar range of values (see Fig. 4). Therefore, an intuitive way to adapt the knowledge in a few-shot manner is to learn to \"rescale\" the retrieval scores to adapt to the new dataset priors during finetuning. Specifically, if t(c) is the sentence for class c, we introduce learnable parameters bias \u03bc\u03b5 and scaling factor \u03c3\u03b5 to adjust the L(gen) (v, t(c)), resulting in a transformed probability pe = sigmoid( - L(gen) (v,t(c))-\u03bc\u03b5). This probability pe represents the likelihood of the image-object pair being associ-ated with attribute c. During finetuning, pe can be optimized using cross-entropy loss. In Sec. 4.2, we also provide the baseline results of finetuning the contrastive retrieval, using the same approach (but with loss score L(con) instead of L(gen)).\nFor the additional parameters \u03bc\u03b5 and \u03c3\u03b5, we initialize \u03bc\u03b5 using -15.0 and \u03c3\u03b5 using 0.5, inspired by the values we observed in Fig. 4 (which shows -L(gen) (v, t) for sorting purpose). The initial values roughly transform the logits pe to zero mean and scale the standard deviation to 6.0. To finetune the model, we use a batch size of 4, a maximum text length of 16, a weight-decay of 0.01. We use the Adafactor optimizer with B\u2081 = 0.9 and \u03b22 = 0.999, and a learning rate of le-5 linearly decayed to zero in 100k training steps, which are roughly 1.8 training epochs. All experiments are conducted on single machine with TPUv3 with the average time to finetune a model being 7 hours."}, {"title": "4 Experiments", "content": "We use CoCa [75] pretrained on the LAION [58] as the foundation model. CoCa combines multimodal contrastive learning with image-conditioned prefix lan-guage modeling, as illustrated in Fig. 3. Its text decoder consists of (1) a uni-modal text decoder trained on a contrastive learning objective with an image encoder, and (2) a multimodal text decoder trained to generate image captions by cross-attending to the image encoder. We adopt CoCa as the foundation model as it allows for performing both contrastive and generative retrieval with one model trained on the same image-text data, ensuring a fair comparison. In our experiments, we use the CoCa Base model, which consists of a ViT [30] im-age encoder with 12 transformer layers, a unimodal text decoder with 6 layers,"}, {"title": "4.1 Implementation Details", "content": "We use CoCa [75] pretrained on the LAION [58] as the foundation model. CoCa combines multimodal contrastive learning with image-conditioned prefix lan-guage modeling, as illustrated in Fig. 3. Its text decoder consists of (1) a uni-modal text decoder trained on a contrastive learning objective with an image encoder, and (2) a multimodal text decoder trained to generate image captions by cross-attending to the image encoder. We adopt CoCa as the foundation model as it allows for performing both contrastive and generative retrieval with one model trained on the same image-text data, ensuring a fair comparison. In our experiments, we use the CoCa Base model, which consists of a ViT [30] im-age encoder with 12 transformer layers, a unimodal text decoder with 6 layers,"}, {"title": "4.2 Results on the VAW Dataset", "content": "First, we show that generative retrieval is better than contrastive retrieval, then analyze various conditional models, and finally compare our results to the state-of-the-art and analyze in particular its much superior performance on the less frequently seen categories.\nGenerative v.s. contrastive retrieval. The VAW dataset and the follow-ing metrics were used: rank (average rank of the correct predictions out of all 620 choices), mR@15 (mean recall over all classes at top 15 predictions for each instance), and mAP (mean average precision over all classes). We use average rank as the primary metric as it is more direct and comprehensive at describing overall ranking performance in a large candidate space.\nTab. 1 and 2 show the results of the zero-shot and fine-tuning settings, re-spectively. Generative retrieval outperforms contrastive retrieval in both settings, demonstrating a stronger ability to model fine-grained associations between ob-"}, {"title": "4.3 Results on the VGARank Dataset", "content": "Generative vs Contrastive Retrieval. We make similar observations as on the VAW dataset, shown in Tab. 4 and 5. We observe that generative retrieval sentence variations significantly outperformed the contrastive counterparts on both datasets. The best generative retrieval sentence template on VGARank-Attribute is \"{A}{O} is {A}\", achieving a rank of 12.0, while the best one on VGARank-Object is \"{A}{O}\", achieving a rank of 5.8. This again verifies our claim that generative retrieval is more optimal for attribute recognition than contrastive retrieval."}, {"title": "5 Conclusion and Broader Impact", "content": "Our work reformulates attribute learning as a probabilistic modeling problem and a knowledge extraction process from pretraining to downstream tasks, and we in turn propose the generative retrieval method on a vision-based prefixLM foundation. By leveraging the knowledge on complex word dependencies cap-tured by the foundation model during pretraining, our work enables the ex-plicit modeling of various object-attribute dependencies in downstream attribute tasks. We also showcase the method's flexibility in emulating various conditional probabilistic dependencies, and we envision that vision-based prefixLM, through the proposed generative retrieval method, can serve as a universal framework to construct meta-models for representing and measuring complex logical relations.\nAs our method studies the visual attribute recognition problem in the con-text of large pretrained models, advancements in large vision-language models will directly translate to stronger performance on this domain. This work also benefits the broader community on image generative models by providing a bet-ter metric than CLIP for image-text alignment at a fine-grained, attribute level. This additional metric can lead to the creation of cleaner text-image datasets that has higher standards for caption details on object-attribute correctness, which would greatly benefit the training of generative models in the community."}, {"title": "A. Limitations", "content": "One limitation to our proposed method is its increased computational cost. Generative retrieval has n autoregressive text decoding steps, where n is the length of the retrieval template sentence, while contrastive retrieval has one text encod-ing step. Given the short and fixed-length sentence templates in the attribute learning context, the computational complexity of generative retrieval is nx constrastive (n = 2 to 4). In addition, the text-only attribute embeddings in contrastive retrieval can be precomputed and cached in advance, which would make contrastive retrieval take 0 encoding steps at inference time. This is not possible for generative retrieval, as it is not possible to precompute a part of the likelihood of generating a image-object-attribute triple. Another limitation to the generative retrieval approach is that is is specifically designed for tasks where the assumed lengths of answers or prompts are similar. Since the sum of log probabilities in L(gen) is influenced by the length of the text, the approach is biased towards shorter answers. In the context of attribute prediction tasks, the assumption of similar lengths holds true, allowing us to treat attribute prompt optimization as joint probability optimization in a graph model. This task for-mulation sets it apart from VQA tasks, which typically involve multiple-choice questions with answers of varying lengths. It is worth noting that this limita-tion does not undermine our main contribution, which is the development of a novel formulation and framework that connects knowledge from large-scale pre-fixLM pre-training to the method of generative retrieval for attribute recognition problems."}, {"title": "B. Additional Qualitative Examples", "content": "We provide more examples to compare our zero-shot retrieval methods, we also include the results from the fully-supervised method SCONE [14] trained on the VAW dataset. Fig. 5 at the end of the supplementary material shows the results. Some interesting observations can be made. First, VAW is still a closed domain dataset, lacking in the coverage of long-tailed attributes. In example (2), our generative retrieval predicts \u201cdecorative\u201d, \u201cantique\u201d, and \u201cbamboo\u201d, which are vi-sually salient and grammatically correct. However, the ground-truth annotation does not include these two options. Second, compared to others, generative re-trieval can surface some of the most significant attributes in the examples. For example, \"in the background\", \"decorative\", \"worn\", or \"closed\". However, many predictions of the contrastive retrieval method are visually imperceptible or in-correct, such as arch-shaped, standing, partially-eaten, water."}, {"title": "C. Additional Evaluation Results", "content": "We include additional results on the VAW experiments in Tab. 6, including the less comparable metrics of mR@15 and F1@15, which were omitted in the main text due to space constraints. Our method achieves the second place only slightly"}]}