{"title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling", "authors": ["William Yicheng Zhu", "Keren Ye", "Junjie Ke", "Jiahui Yu", "Leonidas Guibas", "Peyman Milanfar", "Feng Yang"], "abstract": "Recognizing and disentangling visual attributes from objects is a foundation to many computer vision applications. While large vision-language representations like CLIP had largely resolved the task of zero-shot object recognition, zero-shot visual attribute recognition remains a challenge because CLIP's contrastively-learned vision-language representation cannot effectively capture object-attribute dependencies. In this paper, we target this weakness and propose a sentence generation-based retrieval formulation for attribute recognition that is novel in 1) explicitly modeling a to-be-measured and retrieved object-attribute relation as a conditional probability graph, which converts the recognition problem into a dependency-sensitive language-modeling problem, and 2) applying a large pretrained Vision-Language Model (VLM) on this reformulation and naturally distilling its knowledge of image-object-attribute relations to use towards attribute recognition. Specifically, for each attribute to be recognized on an image, we measure the visual-conditioned probability of generating a short sentence encoding the attribute's relation to objects on the image. Unlike contrastive retrieval, which measures likelihood by globally aligning elements of the sentence to the image, generative retrieval is sensitive to the order and dependency of objects and attributes in the sentence. We demonstrate through experiments that generative retrieval consistently outperforms contrastive retrieval on two visual reasoning datasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual Genome Attribute Ranking (VGARank).", "sections": [{"title": "1 Introduction", "content": "Understanding attributes associated with objects in an image is essential for many computer vision applications, including content recommendation, visual reasoning, and text-to-image generative models. While supervised learning techniques such as classification [24,33,61], detection [23, 54, 55], and segmentation models [8,56] have made significant progress in object recognition tasks, directly adding a branch for object-agnositic attribute prediction [18,19,46] can result in incorrect and counterfactual outputs since it fails to model the co-dependency\nbetween attributes and the objects. Other existing attribute learning methods\nrely heavily on human annotations [2, 3, 25, 34,77] to address this dependency,\nbut this makes them expensive and hard to scale. All things considered, how to\nproperly establish object-attribute relationship at scale remains an open prob-\nlem.\nLarge-scale image-text foundation models such as CLIP [49] and ALIGN [26]\ninspired us to explore their potential for attribute learning. These models have\nlearned from a vast amount of noisy image-text pairs from the web, adequately\nutilizing self-supervised learning to benefit from easily accessible data sources.\nThey have shown exceptional performance in zero-shot object recognition [38,\n39, 48, 52, 60, 68, 72, 78] through image-text similarity measurement, a method\nwhich we refer to as \"contrastive retrieval\".\nHowever, naively applying contrastive retrieval to attribute prediction tasks\nis suboptimal due to its two inherent problems. First, treating input text as an\nunstructured whole to be aligned with images results in insufficient learning on\nattributes if the object alone is distinguishable enough in the image to match the\nimage-text pair, as often is the case in CLIP training data. This creates a dis-\ncrepancy between the pre-training and the downstream tasks: the model learned\nto primarily differentiate between objects but is later asked to understand finer\nattributes. Second, contrastive prompting cannot model the co-dependency be-\ntween objects and attributes. Since contrastive pre-training does not capture\nword sequence order, as opposed to language model pre-training (Fig. 1 (left)),\nthe model is unable to correctly measure the likelihood of counterfactual combi-\nnations such as \u201cbell-shaped sky\u201d or \u201cgraffitied sky\u201d (see Fig. 4). These challenges\nemphasize the necessity for better methods in modeling object-attribute depen-\ndency in the context of large image-text foundation models.\nThis paper presents a novel approach to address the two aforementioned\nproblems in applying image-text foundation models to attribute learning. The\napproach consists of two parts: prefix language modeling (prefixLM) [6,69] as\nthe pre-training foundation, and a novel, sentence generation-based formulation\nof attribute retrieval that allows for the extraction of pre-training knowledge for\nstructural reasoning (see Fig. 1). During pre-training, the prefixLM is trained to\npredict the next token based on visual inputs and previous text tokens, which\ninherently captures diverse combinations of object-attribute dependencies in the\nsentence. In the downstream attribute recognition task, we measure the object-\nattribute alignment in an image by evaluating the probability of generating a\nsentence capturing the relations. We refer to this approach as \"generative re-\ntrieval\". In particular, this method enables flexible retrieval for a wide range of\nattribute relations (associative, possessive, or further modified with temporal\nwords like \"currently\" or \"already\") through building arbitrary conditional de-\npendency models for downstream tasks at inference time (Fig. 1 (right)), which\nare effectively \"meta-models\" .\nThere are two immediate applications for the proposed prefixLM + genera-\ntive retrieval framework: (1) describing objects through their visual appearance,\nstate of being, or relationship to other objects in the image. And conversely,\n(2) recognizing objects based on their various visual attributes such as color,\nshape, size, and so on. In addition, our method can be further applied towards\nmany other visual tasks that require structural reasoning. We summarize the\ncontributions as follow:\n1. We formally reframe the attribute recognition problem as a task of\nlearning and modeling the image-object-attribute conditional probabilities in\na large visual-language modeling setting.\n2. We establish the effectiveness of using prefixLM as a foundational model for\ncapturing complex object-attribute relationships in pretraining, and pro-\npose the generative retrieval method to flexibly distill pretraining knowl-\nedge for downstream attribute recognition tasks.\n3. We demonstrate the limitations of purely using contrastive learning for at-\ntribute recognition and show the superior zero-shot and finetuning perfor-\nmance of our method.\n4. We introduce Visual Genome Attribute Ranking (VGARank), a novel bench-\nmark combining attribute and object recognition tasks into an unified, and\ntherefore directly comparable setting, to demonstrate the generalizability of\nthe proposed approach."}, {"title": "2 Related Work", "content": "We first introduce studies on attribute learning, which mostly rely on hand-\ncrafted probabilistic models without the use of large language models. Then,\nwe summarize existing works on language modeling to specifically introduce\nPrefixLM to the attribute learning tasks. Finally, we provide an overview of ex-\nisting prompt learning techniques to introduce our novel approach of generative\nretrieval, which distillate information from the pretrained PrefixLM.\nVisual attribute recognition involves identifying the properties of visual\nobjects, such as their color, material or shape. Early works had focused on object\ndescription (img\u2192att) using classification [18,19,46] or relative ranking mod-\nels [9, 31, 45, 67,74] to learn attribute presence independent of object category.\nSome works use attributes as a foundation [2,25,34] for zero-shot object recog-\nnition (img, att\u2192obj; e.g., recognizing \u201czebra\u201d by the attributes \u201cblack\u201d, \u201cwhite\",\n\"striped\"). These works learn an attribute vector space and use it to recognize\nunseen visual objects based on the marginal probability. In visual object de-\ntection, some models [3,77] train additional attribute prediction branches using\nthe Viusal Genome dataset [32] to improve model diversity and to create mod-\nels with multi-tasking capabilities. These models concatenate the visual feature\nwith the ground-truth object class embedding and feed them into an attribute\nprediction branch (img, obj\u2192att).\nVector space-based approaches has also been studied for attribute recog-\nnition. For example, [7, 22, 48] apply the CLIP [49]. They use the CLIP em-\nbedding to compare visual information against predefined attribute prompts\n(img\u2194obj,att), to determine if the image contains those attributes. In addition\nto CLIP, [40-43] allow objects and attributes to be projected into the same\nfeature space, while the decoration of attributes on objects is modeled as an\noperator (img\u2194obj OP att, operator OP could be \u00b1 or linear transform).\nOur innovation lies in the novel view of treating attribute recognition as a\nlanguage modeling problem. We integrate probability modeling for image, object\nclass, and attribute prediction in an unified image-text model, while leveraging\nLLM's foundational pre-training.\nLanguage modeling (LM) estimates the probability of a sequence of words\nbeing observed in a sentence. Early language models use dense neural net-\nworks [6] or recursive neural networks [70], while recent large language models\n(LLMs) [15, 16, 49, 69, 75] are based on the transformer architecture [66] because\nof its strong generalizability. LM has many applications in both NLP and com-\nputer vision, including question answering(QA) [51,71], conversational question\nanswering [53], visual captioning [1, 12, 59, 73], and visual question answering\n[4, 21, 76]. These applications can be categorized into three main types of LM:\n(1) image-text matching [20], (2) masked language modeling [16], and (3) prefix\nlanguage modeling [6].\nAttribute recognition is a condensed VQA problem that requires predicting\nthe visual attribute of an query object. The foundational methods proposed\nin the VQA domain mostly combine image-text matching and masked lan-\""}, {"title": "ArtVLM: Attribute Recognition Through Vision-Based PrefixLM", "content": "guage modeling. Examples include LXMERT [63], UNITER [13], OSCAR [35],\nVinVL [77], ViLT [29], VLMo [5].\nDifferent from these works, we show that prefix language modeling (pre-\nfixLM) [6,10,11,69,75]) can approximate masked language modeling (see Sec. 3.3)\nin the attribute tasks. With a novel prompting scheme, prefixLM exhibits even\ngreater expressive power than MLM, making it a powerful tool for deep visual\nreasoning [65, 72].\nPrompt learning originates in the field of natural language processing\n(NLP), where tasks like question-answering are frequently formulated as a \"fill-\nin-the-blank\" problem. Notable examples include BERT [16] which employs\nmasked language modeling, and GPT [50] that uses prefix language modeling.\nWhile large language models (LLMs) [15,44,64] have been widely explored in\nNLP for fact-based reasoning, their application in the computer vision domain\nis relatively unexplored.\nPrompt learning in computer vision has gained attention following the success\nof CLIP [49]. Numerous works [38, 39, 48, 52, 60, 68, 72, 78] have focused on de-\nsigning CLIP-prompts or utilizing the pre-trained CLIP checkpoint. Approaches\nsuch as [79,80] learn the prompting vectors instead of manually designing text\nprompts.\nOur approach focus on its application towards attribute learning, which the\naforementioned contrastive learning based methods are ill-suited for. Our pro-\nposed generative prompting is based on image-conditioned prefix language mod-\neling [69, 75], which takes sequence ordering into consideration and is therefore\nwell-suited for modeling the dependence between visual objects and attributes.\nThe proposed method has potential applications in other visual reasoning prob-\nlems such as visual relation detection [37] or scene graph generation [28]."}, {"title": "3 Approach", "content": "3.1 Image-Conditioned Language Modeling\nOur proposed generative retrieval is based on image-conditioned prefix language\nmodeling, i.e. image captioning. Given an image v, we aim to generate the cor-\nresponding text x = ($1, ..., Sn) by modeling the probability p(xv) using Eq. 1.\nThis equation factors p(x|v) into the product of conditional probabilities [6,50],\nwhere at each time step, the model predicts the next token si based on the vi-\nsual input v and previous tokens (so, ..., Si-1) (so is the start-of-sentence token\n\"<s>\").\n$$p(x|v) = \\prod_{i=1}^{n}p(S_i|v, s_1, ..., S_{i\u22121})$$\nThe factorization provided by Eq.1 is advantageous as it breaks down the\nword generation process into individual probability factors. In Fig. 1 (left), we\nshow that the model can capture various object-attribute compositions during\npre-training. As a result, in downstream attribute-related tasks, we can leverage"}, {"title": "3.2 Generative Retrieval for Attribute Classification", "content": "We formalize the simplest attribute classification task as a common foundation\nfor both generative retrieval and contrastive retrieval. Specifically, given an image\nv and sentence t(1), . . ., t(C) (C is number of classes), retrieval-based classification\ninvolves designing a loss function L(v, t) to measure the cost of aligning image v\nand text t(i) (1 \u2264 i \u2264 C). Thus, zero-shot classification can be achieved through\nfinding the class label c = argmin1<i<c{L(v,t(i))}.\nContrastive retrieval builds on the fact that paired image-text are pro-\njected into the same feature space through contrastive learning during pre-\ntraining. Assuming the image is encoded as f(v) and the text is encoded as g(t),\nthe contrastive learning objective aims to maximize the inner product between\nthe matched image-text embeddings while minimizing the unmatched ones. This\nencourages paired image-text samples to have a high similarity while pushing\nunpaired samples apart. Under the common assumption of unit norm in the\nembeddings [26, 49, 62], this can be equivalently represented by using the L2\nloss to measure the distance between image and text, denoted as L(con) (v, t) =\n||f(v) - g(t)||2.\nN\nGenerative retrieval is our proposed approach for visual attribute recog-\nnition, which utilizes cross-entropy to evaluate the image-text alignment loss,\nrepresented as L(gen) (v,t) = -\u2211=1p(ti) log qo (v, tj|j<i). Here, p(ti) \u2208 R1\u00d7V\n(1 \u2264 i \u2264 N, N is the length) represents the one-hot representation of the\ni-th token of sentence t. To generate the information at the i-th step, the\nmodel qe relies on the image v and all previous text tokens tj|j<i to produce\na probability distribution qe(v, tj|j<i) \u2208 RV\u00d71 over the vocabulary V. The term\n-p(ti) log qo (v, tj|j<i) \u2208 R\u00b9 represents the cross-entropy between the i-th token\nin the sentence t and the model's prediction at the i-th step. Fig. 3 (middle)\nprovides a visual representation of this equation."}, {"title": "3.3 Modeling the Conditional Dependence", "content": "In generative retrieval, we can build different probabilistic models for visual at-\ntribute recognition by changing word ordering in the to-be-measured sentence\n(see Fig. 2). Our key contribution to the community is proposing and showcasing\nits versatility in enabling the design of arbitrary probabilistic model by engineer-\ning different structure for the measured sentence. Below, we use {A} to indicate\nattribute, and {0} to indicate object.\nSentence \"{A}\u201d. This sentence models the simplest dependency for predict-\ning attribute based on the image. In this dependency model, we focus on the\ncross-entropy of classifying the image as having a specific attribute, which can be\nachieved through a simple classification model. This approach aligns with early\nmethods [9, 18, 19, 31, 45, 46, 67, 74] that describe attributes rather than naming\nthe objects.\nSentence \"{0} is {A}\". This sentence template models the prediction of at-\ntributes based on both an image and an object, approximating p(\u201c{A}\u201d | v, \u201c{O}", "{O} is\" (e.g., \"cat\nis orange\", \"cat is fluffy\", \"cat is cute\", etc.). Therefore, the only factor that mat-\nters in generative retrieval becomes -\u00ee(\u201c{A}\u201d)qe(v, \u201c{O}\u201d, \u201cis\u201d), which quantifies\nthe loss associated with classifying an attribute given the image and object. This\ndependency model characterizes recent attribute works such as [3,47,48,77].\nSentence \"{A}{0}\". This sentence is similar to Masked Language Model-\ning (MLM) [16] as it involves filling in the blank in a sentence like \u201can image\nof a [MASK] cat": "However, there are two key distinctions: (1) p(\u201c{A}\u201d | \u03c5)\nrequires the attribute must be easily recognizable from the image, and (2)\np(\"{0}\" | \u03c5, \u201c{A}", "{O} is {A}\"). The probabilis-\ntic modeling derived from the sentence \u201c{A}{O}\\\" closely resembles the approaches\nin [2,25,34], where attributes were utilized for object recognition.\nSentence \\\"{A}{O} is {A}\\\". This sentence produces unconventional sen-\ntences such as \\\"fluffy cat is fluffy\\\". We highlight this sentence template to show-\ncase the versatility of generative retrieval. In essence, this likelihood formula-\ntion includes all three previously discussed conditional probability terms: (1)\np(\u201c{A}": "v) \u2013 classification; (2) p(\u201c{O}\u201d | \u03c5,\u201c{A}", "\u03c5,\u201c{O}": "attribute prediction based on image and\nobject. We present an approximate probability graph in Fig. 2 (right), where we\nduplicate both the attribute and object nodes. With the duplicated \"{A}\" in the\nsentence, the resulting modeling accounts for the co-dependency between object\nand attribute. For example, attributes preceding objects: \"red car\", \"blue sky\";\nand objects preceding attributes: \u201ckid is smiling\u201d, \u201ccat is lying\u201d. This construction\nfurther bridges the gap between pre-training and zero-shot inference.\nDiscussion. Our proposed generative retrieval is novel for two reasons.\nFirstly, from the language modeling perspective, we offer a new solution for"}, {"title": "ArtVLM: Attribute Recognition Through Vision-Based PrefixLM", "content": "training using prefixLM, enabling the model to mimic MLM or more advanced\nLM in a zero-shot manner for downstream tasks. Secondly, our generative re-\ntrieval produce dependency models at inference time that serves as meta-models\nfor attribute recognition, since we can flexibly modify the probabilistic model-\ning and conditional dependence through changes in sentence templates. In our\nexperiments, we show the results for the four different probabilistic attribute\nmodels (Fig. 2)."}, {"title": "3.4 Finetuning on Attribute Tasks", "content": "Since the attribute class names have similar lengths, their cross-entropy scores\nL(gen)(v, t) with the image are expected to fall within a similar range of values\n(see Fig. 4). Therefore, an intuitive way to adapt the knowledge in a few-shot\nmanner is to learn to \"rescale\" the retrieval scores to adapt to the new dataset pri-\nors during finetuning. Specifically, if t(c) is the sentence for class c, we introduce\nlearnable parameters bias \u03bc\u03b5 and scaling factor \u03c3\u03b5 to adjust the L(gen) (v, t(c)),\nresulting in a transformed probability pe = sigmoid( - L(gen) (v,t(c))-\u03bc\u03b5). This\nprobability pe represents the likelihood of the image-object pair being associ-\nated with attribute c. During finetuning, pe can be optimized using cross-entropy\nloss. In Sec. 4.2, we also provide the baseline results of finetuning the contrastive\nretrieval, using the same approach (but with loss score L(con) instead of L(gen)).\n\u03c3\u03b5\nFor the additional parameters \u03bc\u03b5 and \u03c3\u03b5, we initialize \u03bc\u03b5 using -15.0 and \u03c3\u03b5\nusing 0.5, inspired by the values we observed in Fig. 4 (which shows -L(gen) (v, t)\nfor sorting purpose). The initial values roughly transform the logits pe to zero\nmean and scale the standard deviation to 6.0. To finetune the model, we use a\nbatch size of 4, a maximum text length of 16, a weight-decay of 0.01. We use the\nAdafactor optimizer with B\u2081 = 0.9 and \u03b22 = 0.999, and a learning rate of le-5\nlinearly decayed to zero in 100k training steps, which are roughly 1.8 training\nepochs. All experiments are conducted on single machine with TPUv3 with the\naverage time to finetune a model being 7 hours."}, {"title": "4 Experiments", "content": "4.1 Implementation Details\nWe use CoCa [75] pretrained on the LAION [58] as the foundation model. CoCa\ncombines multimodal contrastive learning with image-conditioned prefix lan-\nguage modeling, as illustrated in Fig. 3. Its text decoder consists of (1) a uni-\nmodal text decoder trained on a contrastive learning objective with an image\nencoder, and (2) a multimodal text decoder trained to generate image captions\nby cross-attending to the image encoder. We adopt CoCa as the foundation\nmodel as it allows for performing both contrastive and generative retrieval with\none model trained on the same image-text data, ensuring a fair comparison. In\nour experiments, we use the CoCa Base model, which consists of a ViT [30] im-\nage encoder with 12 transformer layers, a unimodal text decoder with 6 layers,\nand a multimodal text decoder with an additional 6 layers. The image resolution\nis set to 224 x 224 pixels with a patch size of 16 \u00d7 16 pixels. All transformer\nlayers have hidden dimensions of 768 and MLP size of 3,072.\nThe following two datasets are used for evaluation:\nVisual Attribute in the Wild (VAW) [47] is a large dataset of images\nwith explicitly labeled positive and negative attributes. The associated task re-\nquires a model to predict a set of visual attributes given an object's name and\nthe image it is on. VAW contains 216,790 objects from 58,565 images for train-\ning, 12,286 objects from 3,317 images for validation, and 31,819 objects from\n10,392 images for testing. We use the test set to report results after validating\nthe model.\nVisual Genome Attribute Ranking (VGARank) is a modified version\nof the Visual Genome (VG) dataset [32] also designed to evaluate a model's abil-\nity to recognize visual attributes. The proposed dataset is different from VAW\nin that it is 1) an open-vocabulary ranking task, instead of a fixed vocabu-\nlary domain classification task, and 2) has two variants, VGARank-Attribute or\nVGARank-Object focusing on either attribute recognition given an object or ob-\nject recognition given an attribute. This allows us to investigate how pretraining\nknowledge differs between attribute concepts and object concepts.\nFor VGARank-Attribute, each ranking problem is formulated with respect to\none anchor object, with N ground truth attributes paired with that object in Vi-\nsual Genome's annotations and (50-N) additional false attributes. VGARank-O\nmirrors this design, but is formulated with respect to an anchor attribute present\non the image. To make the problem challenging, these false pairings are se-\nlected in accordance to the dataset's conditional probability P(object|attribute)\nor P(attribute object), i.e. for a given object, we select the attributes most likely\nto co-occur with the object in the Visual Genome distribution but which are not\ntrue for the given bounding box. Additionally, if any of the selected fake pair-\ning exists on the current image as part of another bounding box instance, we\nwould not include it in the set of (50 - N) fake pairings. In the case where\nthe given object or attribute does not appear often enough in Visual Genome\nand there is not enough fake pairing candidates from the conditional probability\nP(object attribute) or P(attribute object) to make up the set of (50 \u2013 N), we"}, {"title": "ArtVLM: Attribute Recognition Through Vision-Based PrefixLM", "content": "would select fake object or attribute according to the dataset prior P(object)\nor P(attribute) to fill the rest of the choices. We obtain a dataset with 770,721\nranking problems for training, 7,997 for validation, and 32,299 for testing, and\nthe dataset is available on our GitHub page.\n4.2 Results on the VAW Dataset\nFirst, we show that generative retrieval is better than contrastive retrieval, then\nanalyze various conditional models, and finally compare our results to the state-\nof-the-art and analyze in particular its much superior performance on the less\nfrequently seen categories.\nGenerative v.s. contrastive retrieval. The VAW dataset and the follow-\ning metrics were used: rank (average rank of the correct predictions out of all\n620 choices), mR@15 (mean recall over all classes at top 15 predictions for each\ninstance), and mAP (mean average precision over all classes). We use average\nrank as the primary metric as it is more direct and comprehensive at describing\noverall ranking performance in a large candidate space.\nTab. 1 and 2 show the results of the zero-shot and fine-tuning settings, re-\nspectively. Generative retrieval outperforms contrastive retrieval in both settings,\ndemonstrating a stronger ability to model fine-grained associations between ob-"}, {"title": "ArtVLM: Attribute Recognition Through Vision-Based PrefixLM", "content": "jects and attributes. Generative retrieval achieves a rank of 56.0 with its best\nsentence template, compared to 95.1 (\u2193 lower is better) for contrastive retrieval in\nthe zero-shot setting (Tab. 1) and similarly achieves 10.6 vs 12.2 in the finetuning\nsetting. (Tab. 2). As previously mentioned, there are two underlying reasons for\ngenerative retrieval's superiority. First, it captures true visual attributes, while\ncontrastive retrieval may learn superficial connections through object identities\n(as shown in Tab. 1, adding object hints in contrastive retrieval makes it perform\nworse). Second, it explicitly models the object-attribute relations through their\ndependencies and interactions, which eliminates counterfactual attribute-object\npairs. Fig. 4 shows some qualitative examples of differences between generative\nretrieval and contrastive retrieval. In the top-left example, the contrastive re-\ntrieval ranks highly the attributes \"sky is bell shaped\" and \"sky is graffitied\",\nwhich are strongly associated with other objects present in the bounding box\nbut which are not applicable to the entity of sky. This shows that contrastive\nretrieval can surface attributes based on image-level associations acquired from\ncontrastive pre-training, which is highly undesirable for attribute recognition.\nConditional dependence modeling. In Tab. 1 and 2, we also investigate\nthe four types of graphical models (see Fig. 2) that generative retrieval approxi-\nmate. As finetuning results shows similar trends, we present the zero-shot results\nin Tab. 1 (bottom).\nThe simple classification sentence template \"{A}\" does not model the impor-\ntant object prior and achieves the worst rank of 82.1. The PrefixLM sentence\ntemplate \"{A}{O}\" produces a better model with a rank of 63.9, as it first clas-\nsifies attributes then checks whether the attributes fit the \"[MASK] {0}\", and\nthe MLM sentence template \"{O} is {A}\" has a similar rank of 61.9. We want\nto highlight that while all baselines on the VAW in Tab. 3 are analogous to\nthis formulation, it is not the best among the four graphical models. Therefore,\nimproving the probability modeling in these SOTA methods can potentially fur-"}, {"title": "ArtVLM: Attribute Recognition Through Vision-Based PrefixLM", "content": "ther improve their performance, and our generative retrieval offers an easy way\nto do so. Finally, the hybrid sentence template \"{A}{O} is {A}\" performs the best\nwith an average rank of 56.0. This is because it jointly considers three important\nfactors: p(\u201c{A}\u201d | v), p(\u201c{O}\u201d | v, \u201c{A}", "{O}\"), all captured by\nthe proposed generative retrieval. In particular, it is difficult for MLM to capture\nthe latter two factors simultaneously, and this shows how our proposed prefixLM\n+ generative retrieval is a more flexible approach.\nComparing to the SOTA. We compared our fine-tuned model to the state-\nof-the-art methods in Tab. 3 using the following metrics from [47": "mAP (mean\naverage precision over all classes), mA (mean balanced accuracy over all classes),\nmR@15 (mean recall over all classes at top 15 predictions in each instance),\nand F1@15 (overall F1 at top 15 predictions). The table below focuses on fine-\ngrained mAP metrics as the best comparison for retrieval performance. Results\non mR@15 and F1@15 can be found in the supplemental. The following base-\nlines were considered: ResNet-Bas.-CE [3,27], ResNet-Bas. [46], LSEP [36], [57],\nPartialBCE + GNN [17], ML-GCN [14], SCONE [47], and TAP [48]. We thank\nthe authors from [47] for reimplementing/adapting all the baselines.\nBy leveraging vision-based PrefixLM pretraining, our method ranks\nfirst among methods without in-domain pretraining on the target\nevaluation dataset. Even compared to TAP with in-domain pretraining, our\nmethod ranks only slightly behind TAP in overall metric and outperforms TAP\non most long-tail categories."}, {"title": "ArtVLM: Attribute Recognition Through Vision-Based PrefixLM", "content": "Compared to SCONE and TAP, our method focuses on cross-domain knowl-\nedge extraction instead of task-specific modules or training procedures to fit\nto the dataset domain. The SCONE and TAP work both rely on object mask\nsupervision and large custom datasets during training, incorporates specialized\nmodules, and are trained and tested in the same domain. In particular, the main\nmetric gain in the TAP work is through in-domain pretraining on LSA [48].\nOur method, on the other hand, is designed to be generalist and compatible\nto large-scale pretraining, while at the same time incorporating flexible prob-\nabilistic modeling in the architecture. One major advantage of our method is\nits significantly better performance in the distribution long tail the rarer at-\ntributes of the Medium (72.0% mAP) and Tail (60.6% mAP) attribute classes,\nas shown in table 3. While SCONE and TAP's has higher overall mAP and Head\nmAP, it does not necessarily mean they are better models overall, since these\nnumbers are biased towards frequently observed attributes where it is easy for\nmodels to directly fit to the dataset, especially when it is in the same domain.\nOur model's strong performance on the long tail demonstrates its ability infer\non rarely seen attributes and indicates that it fits to the priors carried by the\nfoundation models, instead of to the dataset distribution.\nTable 4: Zero-shot results on VGARank-\nAttribute. Generative retrieval vs. con-\ntrastive retrieval, across different sentence\ntemplates/dependency meta-models.\ncon-\nTable 5: Zero-shot results on VGARank-\nObject. Generative retrieval vs.\ntrastive retrieval, across different sentence\ntemplates/dependency meta-models.\n4.3 Results on the VGARank Dataset\nGenerative vs Contrastive Retrieval. We make similar observations as on\nthe VAW dataset, shown in Tab. 4 and 5. We observe that generative retrieval\nsentence variations significantly outperformed the contrastive counterparts on\nboth datasets. The best generative retrieval sentence template on VGARank-\nAttribute is \"{A}{O} is {A}\", achieving a rank of 12.0, while the best one on\nVGARank-Object is \"{A}{O}\", achieving a rank of 5.8. This again verifies our\nclaim that generative retrieval is more optimal for attribute recognition than\ncontrastive retrieval."}, {"title": "Conditional dependence modeling", "content": "Conditional dependence modeling. Tab. 4 and 5 show the results on\nVGARank-A and VGARank-O. We boldface the targets to be predicted, which\nare \"{A}\" for VGARank-A, and \"{0}\" for VGARank-O.\nAs in VAW, the classification template \"{A}\" or \"{0}\" is the least effec-\ntive, with a rank of 14.0 on VGARank-A and 6.1 on VGARank-O. The Pre-\nfixLM template \"{A}{O}\" or \"{0} is {A}\" performs better with rank of 13.0 and\n6.0, which is expected as it first classifies the target token then checks whether\nthe target token fits the context. However, the more optimally ordered MLM\ntemplate \"{0} is {A}\" or \"{A}{O}\" mostly outperforms the previous approach,\nwhich aligns with conclusions drawn by earlier works like [2, 25, 34"}]}