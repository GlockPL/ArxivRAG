{"title": "R2GEN-MAMBA: A SELECTIVE STATE SPACE MODEL FOR RADIOLOGY REPORT\nGENERATION", "authors": ["Yongheng Sun", "Yueh Z. Lee", "Genevieve A. Woodard", "Hongtu Zhu", "Chunfeng Lian", "Mingxia Liu"], "abstract": "Radiology report generation is crucial in medical imaging,\nbut the manual annotation process by physicians is time-\nconsuming and labor-intensive, necessitating the develop-\nment of automatic report generation methods. Existing\nresearch predominantly utilizes Transformers to generate\nradiology reports, which can be computationally intensive,\nlimiting their use in real applications. In this work, we present\nR2Gen-Mamba, a novel automatic radiology report genera-\ntion method that leverages the efficient sequence processing\nof the Mamba with the contextual benefits of Transformer\narchitectures. Due to lower computational complexity of\nMamba, R2Gen-Mamba not only enhances training and in-\nference efficiency but also produces high-quality reports.\nExperimental results on two benchmark datasets with more\nthan 210,000 X-ray image-report pairs demonstrate the ef-\nfectiveness of R2Gen-Mamba regarding report quality and\ncomputational efficiency compared with several state-of-the-\nart methods. The source code can be accessed online.", "sections": [{"title": "1. INTRODUCTION", "content": "Radiology report generation is crucial in medical imaging, of-\nfering key information necessary for diagnosing and manag-\ning patient conditions. Traditionally, these reports are man-\nually annotated by physicians, which is time-consuming and\nlabor-intensive. This challenge is further exacerbated by the\never-increasing volume of medical image data, making it dif-\nficult for radiologists to meet the demands for timely and ac-\ncurate reporting. There has been a growing interest in de-\nveloping automatic report generation methods that can alle-\nviate the burden on medical professionals while maintaining\nthe high standards required in clinical settings.\nNumerous approaches have been introduced for automatic\nradiology report generation [1-3]. Most existing studies rely\non Transformer models [4] that have demonstrated impressive\nperformance in a variety of natural language processing tasks\nsuch as image captioning and text generation. Transformers\nleverage self-attention mechanisms to model long-range de-\npendencies, making them particularly well-suited for generat-\ning coherent and contextually relevant reports from complex\nmedical images. However, Transformer models are often\ncriticized for their high computational complexity, limiting\ntheir use in real applications. Recently, the Mamba model [5],\ndesigned to reduce computational complexity without com-\npromising performance, has attracted increasing attention.\nMamba's efficient sequence processing capabilities make it\nan attractive alternative to Transformers, but its potential for\nradiology report generation has not yet been fully explored.\nIn this work, we propose a novel radiology report gen-\neration method, called R2Gen-Mamba, which leverages the\nstrengths of both Mamba and Transformer architectures.\nSpecifically, R2Gen-Mamba leverages Mamba with low com-\nputational complexity as the encoder, and Transformer as the\ndecoder retaining powerful contextual processing capability.\nBy combining these complementary models, R2Gen-Mamba\nprovides a new pathway for reducing the computational\nburden in radiology while ensuring high-quality, contextu-\nally relevant reports. Experimental results on two bench-\nmark datasets IU X-Ray [6] and MIMIC-CXR [7], suggests\nthat R2Gen-Mamba outperforms traditional Transformer-\nbased models regarding report quality and computational\nefficiency. Compared with state-of-the-art (SOTA) studies,\nR2Gen-Mamba provides a more resource-efficient solution\nfor automatic radiology report generation."}, {"title": "2. METHODOLOGY", "content": "Radiology report generation can be framed as a sequence-\nto-sequence problem, where the input image patch features\nserve as the input sequence and the corresponding report as\nthe target sequence. Typically, the input patch feature se-\nquence X = {X1,X2,...,Xs}, where S is the number of\npatches, each x \u2208 Rd, consists of visual features extracted\nfrom the image patches using pre-trained visual extractor like\nconvolutional neural networks. The output sequence Y =\n{\u04231, \u04232,..., \u0423\u0442}, where T is the maximum length of reports,\neach yt is a token from a predefined vocabulary, represents\nthe generated report. This sequence-to-sequence framework\nis optimized through maximum likelihood of generating the\ncorrect report given the input image. Our R2Gen-Mamba\ncontains three major parts (i.e., visual extractor, Mamba en-\ncoder, and Transformer decoder), which are outlined in sub-\nsequent subsections."}, {"title": "2.1. Visual Extractor", "content": "To produce radiology reports, we begin by extracting vi-\nsual features from the radiology images using convolutional\nneural networks such as VGG or ResNet. As illustrated\nin Fig. 1, the image is passed through the Visual Extrac-\ntor to extract the feature map. Each spatial pixel in the\nfeature map corresponds to a patch in the original image.\nThese spatial pixels are flattened to obtain a sequence rep-\nresentation that serves as the input sequence for subsequent\nMamba encoder. This process is formally represented as:\n{X1,X2,...,XS} = fv(Img), where fr(\u00b7) is the visual ex-\ntractor, and Img is the input image."}, {"title": "2.2. Mamba Encoder", "content": "To extract contextual semantic information, we use Mamba\nas the encoder. Mamba is designed to process sequence\ndata. Compared with Transformers that have quadratic\ncomputational complexity, Mamba has linear complexity\nfor the number of tokens. Provided the input sequence\n{X1,X2,..., XS}, the output sequence Z is obtained by\n{Z1, Z2,...,ZS} = fe(X1,X2,...,xs), where fe denotes\nthe Mamba encoder. As for the core state space model (SSM)\nof Mamba, given the input sequence U, the output sequence\nV is obtained by {V1, V2, ..., vs} = SSM(u1, u2, . . ., us).\nSpecifically, as illustrated in Fig. 1, ut, t \u2208 {1,2,...,S}\nis fed into linear layers to obtain continuous parameters:\nBt, Ct, At = Project(ut). Then discretization is per-\nformed by zero-order hold (ZOH): At = exp(\u2206tA); \u0392t =\n(\u0394tA)-1(exp(\u2206tA) \u2212\u0399)\u00b7 \u2206tBt, where A is a learnable em-\nbedding. Finally, the sequence-to-sequence transformation is\nachieved in two stages: ht = \u0100tht\u22121 + Btut; vt = Ctht."}, {"title": "2.3. Transformer Decoder", "content": "In the proposed R2Gen-Mamba, the decoder is built upon the\nstandard Transformer architecture. The decoding procedure is\nformulated as: yt = fd(Z1, Z2, ..., ZS, Y1, ..., Yt-1), where\nfa() is the Transformer decoder. As noted in [4], the decoder\nneeds to rely on the generation results of the previous step due\nto its auto-regressive nature and requires additional attention\nmechanisms, so we repeat the decoder layer Na times. In our\nexperiments, we set Na to 3."}, {"title": "2.4. Objective Function", "content": "The overall generation process in R2Gen-Mamba can be\nmathematically framed as a recursive implementation of\nthe chain rule, where the probability of the target sequence\n{\u04231, \u04232, ..., \u0423\u0442} provided the input image Img is expressed\nT\nt=1\nas: p(Y | Img) = \u041f\u0440(Yt | Y1, ..., Yt\u22121, Img). The model\nis trained by maximizing the likelihood of the target sequence\nconditioned on the input image:\n0* = argmax  \\u2211 log p(yt | Y1,..., Yt\u22121, Img; 0) (1)\n\u03b8\nt=1\nwhere 0* represents the parameters of R2Gen-Mamba. This\noptimization process ensures that the model learns to accu-\nrately generate the report text based on the visual features ex-\ntracted from the input image. During inference, we use the\nbeam search strategy to sample predictions. To facilitate re-\nproducible research, we have shared the source code to the\npublic through GitHub."}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Experimental Setup", "content": "We perform experiments on two benchmark datasets: IU X-\nRay [6] and MIMIC-CXR [7]. The IU X-Ray dataset in-\ncludes 7,470 chest X-ray images paired with 3,955 reports,\nwhile MIMIC-CXR comprises 473,057 images and 206,563\nreports. Following prior studies [1-3], we exclude samples\nwithout reports. We use a 70%/10%/20% split for training,\nvalidation, and testing on IU X-Ray, and the official split for\nMIMIC-CXR, as detailed in Table 1. Two evaluation metrics\nare employed: traditional natural language generation (NLG)\nmetrics (BLEU [8], METEOR [9], and ROUGE-L [10]) and\nclinical efficacy (CE) metrics. For CE metrics, we use the\nCheXbert [11] tool to automatically label generated reports,\ncomparing them to ground truths across 14 thoracic disease\ncategories using precision, recall, and F1 score."}, {"title": "3.2. Implementation Details", "content": "Following [1-3], we use two images per patient for IU X-Ray\nand one image for MIMIC-CXR as input. The visual extrac-\ntor utilizes a ResNet101 model pre-trained on ImageNet, with\npatch features projected to a dimension of 512. The Mamba\nencoder is set to a dimension of 512, with an SSM state ex-\npansion factor of 16, a local convolution width of 4, and a\nblock expansion factor of 2. The Transformer decoder also\nhas a dimension of 512, with 3 layers, 8 heads, and a dropout\nrate of 0.1. We use the Adam optimizer and set learning rates\nof 5 \u00d7 10-5 for the visual extractor and 1 \u00d7 10\u20134 for other pa-\nrameters, decayed by 0.8 per epoch. The model that achieved\nthe best BLEU-4 score on the validation sets is selected, with\na beam size of 3 for inference to balance between generation\nquality and computational efficiency."}, {"title": "3.3. Visual and Quantitative Results", "content": "To evaluate the effectiveness of our R2Gen-Mamba, we\nperformed a comparative analysis against existing SOTA\nmethods, namely R2Gen [1], R2Gen-CMN [2], and R2Gen-\nRL [3]. Using the same data, R2Gen and R2Gen-CMN were\nimplemented using their released code and checkpoints for\ninference, and R2Gen-RL was retrained from scratch using\ntheir released code. Several typical reports generated by dif-\nferent methods are shown in Fig. 2. It can be seen from this\nfigure that the report generated by R2Gen-Mamba contains\nmore precise information, providing superior results than the\ncompeting methods in accuracy and clarity. The quantitative\nresults regarding NLG and CE metrics are summarized in\nTable 2, from which we have several key findings.\nFirstly, our R2Gen-Mamba, which incorporates Mamba\nand Transformer, outperforms existing approaches in most\ncases, suggesting the advantages of Mamba for report gen-\neration and the feasibility of combining Mamba with Trans-\nformer. Secondly, R2Gen-Mamba slightly under-performs\nR2Gen on BLEU-1 and BLEU-2 metrics for MIMIC-CXR\nbut surpasses it on BLEU-3, BLEU-4, METEOR, and ROUGE-\nL. BLEU-1 and BLEU-2 measure the overlap of single words\nand word pairs, reflecting basic vocabulary matching. BLEU-\n3 and BLEU-4 measure triples and quadruples, capturing\nlonger context dependencies. Higher BLEU-3 and BLEU-\n4 scores indicate R2Gen-Mamba generates text with bet-\nter grammatical and semantic structures, reflecting stronger\ncontext modeling and grammatical consistency. METEOR\ncombines lexical matching, word order, and morphological\nchanges, while ROUGE-L assesses the longest common sub-\nsequence between generated and reference texts. Our R2Gen-\nMamba's better performance on these metrics demonstrates\nstronger vocabulary choice, grammatical structure, and align-\nment with reference text. Thirdly, R2Gen-Mamba demon-\nstrates superior performance on clinical efficacy (CE) met-\nrics, suggesting that the generated reports offer more valuable\nclinical information for diagnosis and decision-making. This\nhighlights the clinical relevance and utility of our R2Gen-\nMamba compared with the competing methods."}, {"title": "3.4. Computation Complexity Analysis", "content": "With the Mamba encoder in the proposed R2Gen-Mamba\nframework, we can significantly reduce model complexity,\nwith only 594.944 K parameters and incurring a computa-\ntional load of 58.216 M floating-point operations (FLOPs).\nThis represents a substantial improvement over the Trans-\nformer encoder utilized in the SOTA R2Gen model, which\ncomprises 4.728 M parameters and incurs a computational\ncomplexity of 462.422 M FLOPs. The considerable reduction\nin both parameter count and computational cost highlights the\nefficiency of the Mamba encoder, making it more suitable for\nresource-constrained environments while maintaining supe-\nrior performance in radiology report generation."}, {"title": "4. CONCLUSION", "content": "This paper presents R2Gen-Mamba, a novel radiology report\ngeneration model that leverages Mamba's efficient sequence\nprocessing and Transformer's contextual strengths. R2Gen-\nMamba reduces computational complexity while producing\nhigh-quality radiology reports. Experiments on two datasets\nshow that R2Gen-Mamba surpasses existing methods in both\nnatural language generation and clinical efficacy metrics. Our\nfindings highlight the effectiveness of merging Mamba with\nTransformer techniques for radiology report generation."}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research was conducted retrospectively using human\nsubject data made available in open access by IU X-Ray and\nMIMIC-CXR. Ethical approval was not required as confirmed\nby the license attached with the open-access data."}]}