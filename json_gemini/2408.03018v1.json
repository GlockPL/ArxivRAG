{"title": "Integrating Controllable Motion Skills from Demonstrations", "authors": ["Honghao Liao", "Zhiheng Li", "Ziyu Meng", "Ran Song", "Yibin Li", "Wei Zhang"], "abstract": "The expanding applications of legged robots require their mastery of versatile motion skills. Correspondingly, researchers must address the challenge of integrating multiple diverse motion skills into controllers. While existing reinforcement learning (RL)-based approaches have achieved notable success in multi-skill integration for legged robots, these methods often require intricate reward engineering or are restricted to integrating a predefined set of motion skills constrained by specific task objectives, resulting in limited flexibility. In this work, we introduce a flexible multi-skill integration framework named Controllable Skills Integration (CSI). CSI enables the integration of a diverse set of motion skills with varying styles into a single policy without the need for complex reward tuning. Furthermore, in a hierarchical control manner, the trained low-level policy can be coupled with a high-level Natural Language Inference (NLI) module to enable preliminary language-directed skill control. Our experiments demonstrate that CSI can flexibly integrate a diverse array of motion skills more comprehensively and facilitate the transitions between different skills. Additionally, CSI exhibits good scalability as the number of motion skills to be integrated increases significantly.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing prevalence of legged robots, the demand for their enhanced capabilities is expected to grow continuously. One significant trend is the expectation for legged robots to manage a diverse array of motion skills to potentially cope with a wide range of tasks in real-world applications. Recently, RL-based approaches have reaped considerable success in the task of multi-skill integration for legged robots, e.g., impressive quadruped parkour [1], omnidirectional bipedal locomotion [2], or wonderful bipedal robots football match [3]. However, these RL-based approaches do not adequately fulfill this objective. One of the most hindering problems is the complex reward engineering required for skill learning using related methods. More frustratingly, the rewards designed for different skills are typically not generalizable, complicating the integration of multiple motion skills.\nIntegrating imitation learning (IL) with RL provides a feasible solution to these problems. Recently, a plethora of ongoing research in the field of character animation has demonstrated the effectiveness of this paradigm. By allowing agents to track the reference motion trajectories [4], [5], [6], or aligning with motion style from imitating reference motion [7], [8], [9], controllers trained via IL can perform motion skills naturally. As demonstration data serves as a reference for policy learning, RL approaches integrated with imitation learning significantly simplify reward engineering. Additionally, the design of rewards necessary for various motion skills becomes more standardized.\nRecently, some methods in character animation have been adapted to the multi-skill integration for legged robots [9], [10], [11], [12], [13]. However, these approaches generally exhibit limited flexibility. Most of them rely on extra well-defined task objectives [9], [10], [11] to integrate a set of similar task-related skills, thereby constraining their applicability and the variety of motion skills they can integrate. Alternatively, other approaches [12], [13] attempt to integrate each different motion skill with the help of additional networks or training stages, which makes the corresponding training costs increase when the number of skills to be integrated increases. Overall, these approaches are limited in the range or number of motion skills that can be integrated.\nIn this work, we propose CSI, a flexible framework designed for legged robots to integrate multiple motion skills from reference motion clips into a single controller. CSI is built upon Generative Adversarial Imitation Learning (GAIL) [14], an IL framework that obviates the need for skill-specific reward engineering. Furthermore, by incorporating key designs such as Conditional Imitation Learning and Condition-Aware Loss, CSI can use skill labels as a control interface for integrated motion skills, which makes it possible to access some external knowledge like natural language for skill control. Our experiments validate the effectiveness of CSI and demonstrate its notable ability to support language-directed skill control through the incorporation of high-level NLI modules.\nIn summary, the primary contributions of this paper are reflected in the following three aspects:\n\u2022\tWe propose CSI, a flexible multi-skill integration framework. CSI enables legged robots to acquire versatile and controllable motion skills by effectively imitating reference motion capture data.\n\u2022\tOur approach provides a more controllable interface, enabling the flexible and easy leveraging of heuristic knowledge to improve the efficiency of skill execution.\n\u2022\tDetailed experiments and analyses of our approach are carried out on different datasets as well as on different robots, which validates the effectiveness and adaptability of our work."}, {"title": "II. RELATED WORK", "content": "A. Multi-Skill Integration for Legged Robots\nIn recent years, methods built upon the RL paradigm have achieved promising performance in integrating multiple motion skills for legged robots. Rodriguez et al. [2] introduced deep reinforcement learning (DRL) method to enable the bipedal robot NimbRo-OP2X [15] to learn agile omnidirectional locomotion, including walk forward, walk backward and steering. Zhuang et al. [1] proposed a two-stage RL training approach for the quadruped robots Go1 [16] and A1 [17] to acquire complex dexterous parkour maneuvers, such as creep forward and jumping. Tuomas et al. [3] employed DRL techniques to impart agile soccer skills on the small bipedal robot OP3 [18].\nHowever, RL-based methods typically require meticulous reward engineering, which can be both labor-intensive and time-consuming, especially for multi-skill integration. To address this problem, the introduction of IL has proven effective in alleviating the need for explicit reward design, thereby offering considerable advantages for integrating multiple motion skills. This beneficial impact has been demonstrated by related works in the field of character animation. Won et al. [19] proposed a tracking-based method to integrate various motion skills from a large-scale open-source motion capture dataset [20] into a few controllers, and characters can switch between controllers to perform different motion skills. Peng et al. [21] and Zhu et al. [22] incorporated a shared embedding space within the learning process, enabling the integration of multiple skills into one single policy. Furthermore, this embedding space also serves as an interface to call integrated skills for subsequent training of downstream tasks.\nThese methods have also witnessed initial explorations on legged robots in recent studies. [9], [10], [11] incorporated velocity command tracking objectives during the training process to integrate a set of similar walking and running maneuvers into one controller. Vollenweider et al. proposed MultiAMP [12], which assigns an additional discriminator for each skill that needs to be integrated to aid learning. Han et al. [13] instead adopted a multi-stage training process, training specific Vector Quantized-Variational AutoEncoder [23] policy for each different skill, then integrating these policies into a single one by distillation.\nB. Controllable Motion Skills\nHow to give controllability to the integrated skills is an important issue for the integration of multiple motion skills in legged robots. [9], [10], [11], [24] added additional task objectives such as tracking velocity command, to the training objective as a way to achieve controllability of the integrated motion skills through command input. This approach is constrained by the need for well-defined task objectives and is also limited by the specific task requirements that determine which motion skills can be integrated. For example, when the task involves velocity tracking, it becomes more difficult to integrate dance movements into the policy. Vollenweider et al. [12] applied one-hot skill code as input, each code corresponds a skill discriminator, through leanring to switch between these codes, the policy can adjust the output motion skill according to the input skill. Although this approach allows for a less restricted range of integrable motion skills, the training cost increases proportionally with the number of skills that need to be integrated.\nIn this paper, we propose CSI, a flexible framework that enables the integration of diverse motion skills into a single controller. CSI uses skill labels as control signals to enable controllability through integrated motion skills, which can be further combined with a pre-trained NLI module to achieve preliminary language-directed skill control. Unlike existing approaches, CSI eliminates the need for additional task objectives and multi-stage training to establish a control interface. We anticipate that our work will serve as a valuable reference for future multi-skill integration applications in legged robots."}, {"title": "III. METHOD", "content": "A. Preliminary\nIn this work, our goal is to enable legged robots to obtain versatile and controllable motion skills. To achieve this, we formulate the problem as a goal-conditioned [25] Markov Decision Process (MDP) $(S,A,c,R, p_0,\\gamma)$, where S is the state space, A denotes the action space, $c\\in C$ signifies the input condition, $R = r(S_t, S_{t+1},c)$ represents the reward for each time step, $p_0$ is initial state distribution, and $\\gamma\\sim (0,1]$ denotes the discount factor. During the training process, we employ the RL algorithm to optimize the parameters of policy: $\\pi_\\theta : S \\rightarrow A$, aiming to maximize the expected return of the discounted episode reward $J(\\pi) = E_{c\\in C,\\pi_\\theta}[\\Sigma_{t=0}^T \\gamma^t]$, where T represents the horizon length of an episode.\nTo enable legged robots to generally learn motion skills by imitating from demonstration motion data, our method is built upon GAIL [14]. In GAIL's framework, a generator is trained in an adversarial manner with a discriminator. The discriminator's role is to distinguish between real samples from expert demonstrations and fake samples generated by the generator. The feedback from the discriminator to the generator serves as a reward signal, guiding the generator to produce data that closely resembles the expert demonstrations. Typically, GAIL requires state-action pairs $(s_t, a_t)$ as input, where expert demonstration data provides both the state and the corresponding action. In our approach, we utilize motion capture data as the source of expert demonstration. Motion capture data primarily records the skeletal states at each frame but does not include the explicit actions taken by the expert. To address this limitation, we adopt the paradigm of GAILfO [26] by utilizing state transitions $(s_t, s_{t+1})$ instead of state-action pairs $(s_t, a_t)$. This adaptation enables the generator to learn from the evolution of states, even in the absence of explicit action conducted by the expert.\nB. Conditional Imitation Learning\nTo achieve controllability over the integrated motion skills, we propose that all networks within the framework should be guided to operate according to some kind of instruction. Therefore, we introduce Conditional Imitation Learning (CIL) to guide the networks to the specified motion skills during the training process. For the discriminator, we add skill labels to the original samples input, which requires it to be able to judge the authenticity of given samples by taking into account the skill label information. For the policy and the value function, we introduce a simple encoder network to map the skill label to a latent vector z. z will be used as part of the input to the policy and the value function, which requires them to be able to respond according to the condition z. Based on the background of CIL, we design the basic training objective of the discriminator as follows:\nConditional Imitation Loss. In the framework of the vanilla GAN [27], a variational approximation of the Jensen-Shannon divergence [28] is commonly applied to achieve the adversarial training objective. Building on the concept of conditional probability, we introduce conditions based on the original objective:\n$\\mathcal{L}_1 = -E_{(s_t,s_{t+1})\\in d_M} [log(D(s_t, s_{t+1}|c))]$\n(1)\n$- E_{(s_t,s_{t+1})\\in d_r} [log(1 \u2013 D(s_t, s_{t+1}|c))]$\nwhere $d_M$ and $d_r$ represent the state transition distributions of the reference motion skills and those generated by the policy, respectively. c denotes the corresponding skill label. Given a motion state transition $(s_t, s_{t+1})$ combined with the corresponding skill label c, the output of the discriminator is expressed as $D(s_t, s_{t+1}|C)$.\nGradient Penalty. We also introduce gradient penalty, which has been proven effective in reducing the destabilizing effects of adversarial training [7], [29], [30]:\n$\\mathcal{L}_{GP} = E_{(s_t,s_{t+1})\\in d_M} [|| \\triangledown D(s_t, s_{t+1})||^2]$\n(2)\nNote that we calculate the gradient penalty with respect to all real samples, irrespective of the conditions corresponding to those samples.\nIn this way, we can preliminarily guide the generator to mimic the specified motion skills during the training process. However, some practical considerations deserve to be noticed. For example, since training directly based on the above training objectives is still an unsupervised paradigm, input conditions are frequently disregarded by the networks during training, which results in controllers that exhibit only a limited set of uncontrollable motion skills. We add some additional designs to the training objectives of the discriminator to cope with these problems:\nCondition Aware Loss. Due to the unsupervised learning nature of GAN, despite the conditional imitation loss described above, the discriminator still tends to ignore the input conditions during the training process, especially when the number of reference motion skills increases. To enhance the discriminator's sensitivity to motion skill labels, we construct mismatched samples that carry mismatched motion skill labels, alongside real and fake samples. In our setting, the mismatched samples should be judged as negative by the discriminator:\n$\\mathcal{L}_{CA} = -E_{(s_t,s_{t+1})\\in d_M} [log(1 \u2013 D(s_t, s_{t+1}|\\hat{c}))]$\n(3)\nwhere $\\hat{c}$ denotes the skill labels that do not match the input samples. Subsequent experiments demonstrate that $\\mathcal{L}_{CA}$ significantly mitigates mode collapse and enables the controller to comprehensively master the integrated motion skills.\nWeight Decay. For multi-skill integration, the reference sample capacity is typically limited, rendering the GAN network prone to overfitting and leading to a relative lack of diversity in the skills generated by the final trained controllers. To alleviate this issue, weight decay is introduced for the discriminator:\n$\\mathcal{L}_{WD} = \\Sigma||\\Theta_D||^2$\n(4)\nwhere $\\Theta_D$ denotes the weight parameters of the discriminator network. This improvement enables the discriminator to focus more on the general features of each skill, thereby increasing the diversity of the generated skills.\nFinally, our training objective for the discriminator is defined as:\n$\\mathcal{L}_D = \\omega_l\\mathcal{L}_1+ \\omega_{ca}\\mathcal{L}_{CA} + \\omega_{wd}\\mathcal{L}_{WD} + \\omega_{gp}\\mathcal{L}_{GP}$\n(5)\nwhere  $\\omega_l$, $\\omega_{ca}$, $\\omega_{wd}$ and $\\omega_{gp}$ are hyperparameters to balance each item of the training objective.\nC. Reward Setting\nSimilar to GAIL, the reward feedback in CSI is also derived from the discriminator. To encourage the generated motion skills to resemble the reference motion capture data, while maintaining alignment between the generated motions and the given conditions, we define conditional style reward [7] as follows:\n$r_s = -log[1 - D(s_t, s_{t+1}|c)]$\n(6)\nConditional style reward provides great regularization for learning motion skills under the distribution of reference motion skill dataset. However, transitions between different motion skills that are not represented in the reference dataset often result in unnatural phenomena, such as jittering. To alleviate this, we introduce some additional regularization terms:\n$r_v = \\sum \\vert\\vert \\dot{q}_t - q_{t+1}\\vert\\vert^2$\n$r_{ep} = \\sum \\vert\\vert\\tau\\vert\\vert$\n(7)\n$r_a = \\vert\\vert a_t - a_{t-1}\\vert\\vert^2$\n$r_t = \\sum\\tau$\nwhere q and $ \\tau$ represent joint rotation angles and torque, respectively. $\\dot{q_t}$ denotes the angular velocity of each joint at time step t, a means action output by policy. All symbols $\\Sigma$ denote the summation over every Degree of Freedom (DoF) of the robot. These regularization terms lead to a smoother overall performance of the motion skills generated by the policy.\nFinally, the total reward is computed as the weighted sum of the style reward and all regularization terms:\n$r_r = w_sr_s + w_vr_v + w_{ep}r_{ep} +w_ar_a + w_tr_t$\n(8)\nwhere $w_s, w_v, w_{ep}, w_a,$ and $w_t$ are weights used to balance each component of the rewards.\nD. Language-Directed Skill Control\nThe low-level controller trained using CSI provides a skill control interface to leverage external knowledge. To implement language-directed skill control, we first manually bind an additional caption to each skill label as skill caption label, e.g. \"Walk Forward\", \"Sprint\", \"Jump\", etc. Then we align the output of the NLI module with those skill caption labels in a zero-shot manner:\nGiven a text input $T_i$ as the premise, and a finite set of skill caption labels $L = {L_1,L_2,L_3,\u2026\u2026\u2026,L_n}$ as a set of hypotheses, where n corresponds to the number of motion skills integrated by the low-level controller. The high-level NLI module is acquired to determine the textual entail-ment relationship between the premise $T_i$ and each skill caption label hypothesis $L_i \\in L$: entailment (positive), contradiction (negative) or neutral.\nAfter the above process, the NLI module will output the entailment scores corresponding to each skill caption label, where the skill label corresponding to the highest-scoring skill caption label will be taken as the condition input for the low-level controller. As demonstrated in the Section IV, this zero-shot classification paradigm decouples the high-level and the low-level modules, enabling a flexible combination between these two modules.\nE. Implementation Details\nModel representation. The policy $ \\pi$, the value function V(st,c) and the discriminator D($S_t$, $S_{t+1}$,c) are all parameterized as shallow MLP networks, each with hidden layers of size [512,256] and rectified linear unit (ReLU) activation functions. The encoder network is a MLP network of size [128, 128], with the size of latent vector z being set to fixed 8-dimensional.\nObservation space. An appropriate observation representation can guide policy training effectively. In our framework, the observation of the discriminator can be represented as {$S_t$, $S_{t+1}$,c}, where $s_t$ denotes the motion state of the robot at time t and c denotes motion skill label. Specifically, motion state s is defined in terms of a relatively complete state representation:\n$s = {h,R_r, v_r, w_r,q_j, \\dot{q}_j, P_i}$\n(9)\nwhere h denotes the height of the root relative to the ground, and the root is roughly located near the center of the pelvis for humanoid robots and the geometric center of the torso for quadruped robots; R, represents the orientation of the root; $v_r$ and $w_r$ mean the linear velocity and angular velocity of the root, respectively; $q_j$ and $\\dot{q}_j$ are joint position and joint velocity of the j-th joint respectively; $p_i$ means the position of the i-th end-effector expressed in the local coordinate frame of the root.\nFor the policy and the value function, the observation can be represented as {$a_{t-1},S_{propri},z$}, where $a_{t-1}$ means the action of the last time step, z is the vector obtained by mapping the skill condition c by the encoder network, $S_{propri}$ denotes proprioceptive states, which is defined as:\n$S_{propri} = {V_r,g_{pro},q_j,\\dot{q}_j}$\n(10)\nwhere $g_{pro}$ is the projected gravity vector, which contains information about the robot's orientation.\nAction space. The action space of the policy is defined by the target joint rotation angles. A PD position controller translates the output of the policy into the motor torques, following the equation $\\tau= k_p(a_t \u2013 \\theta_t) \u2013 k_d\\dot{\\theta}$. In our settings, the policy is queried at a frequency of 50 Hz, and the PD position controller operates at a frequency of 200 Hz.\nMotion retarget. For humanoid robots, we employ a retargeting method similar to the one provided in IsaacGymEnvs [31], and for quadruped robots, we utilize the processing flow described in [4]. The retargeted motion capture data are collected as reference dataset $D^M$ for the subsequent training. State transitions sampled from $D^M$ are treated as real samples for training the discriminator."}, {"title": "IV. EXPERIMENT", "content": "In this section, we conduct detailed experiments on our CSI method. To demonstrate the adaptability of CSI on different robots, we select three different legged robots for our experiments including quadruped robot AlienGo [34], small humanoid robot BRUCE [35] and full-size humanoid robot Unitree H1 [36]. Factors such as morphology, mass, and varying numbers of DoF differ across these three robots, posing significant challenges for the controllable multi-skill integration. The following four methods are used to comparatively validate our method:\n\u2022\tConditional Adversarial Motion Prior (CAMP): Based on AMP [7], [9], one-hot skill labels are used as conditional inputs to the policy, the value function and discriminator, which enables the integration of several different motion skills into a single controller.\n\u2022\tConditional Adversarial Latent Models (CALM) [38]: state-of-the-art multi-skill integration method in the field of character animation, which achieves integration and controllability of multiple motion skills by introducing additional motion encoder and latent space.\n\u2022\tBaseline-I: Our method without Condition Aware Loss.\n\u2022\tCSI (ours): The methodology presented in this paper.\nD. Generality Analysis\nThis section briefly analyzes the generality of CSI. As shown in Fig. 7, CSI can be applied to different humanoid robots BRUCE and H1 as well as quadruped robot AlienGo, and they share the same training process. For other humanoid robots, CSI is also theoretically applicable, but the difficulty lies in how to kinematically retarget the motion capture data for the robot to obtain the corresponding reference motion dataset. Additionally, the motion capture data of quadrupedal creatures is more difficult to obtain than that of human beings, so the source of the reference motion skill is a major obstacle to the application of CSI on quadruped robots. In this regard, we believe that future work is necessary to further expand more sources of reference motions, such as extracting reference motions from videos [44], [45], or obtaining reference motions from generative models [46], [47]. These expanded reference motion data sources will further enhance the generality of CSI.\nAnother point is that CSI requires less correlation between motion skills that need to be integrated. The reference motion skills that can be integrated by previous work [9], [10], [11] are limited by the task objective. For example, in locomotion tasks, policies can usually only integrate task-relevant motion skills such as walking, standing, steering, and running. Integrating additional jumping skills would require further adjustments to the task objective. CSI does not have any obvious relevance requirements for this, as shown in Fig. 7, where similar motion skills, such as trot and pace, as well as very different motion skills, such as dance and zombie, can be integrated, highlighting the generality of CSI for multi-skill integration.\nE. Initialization\nWe adopt a mixed initialization strategy to accelerate motion skill learning. At the beginning of each episode, a set of initialization state and skill label pairs s,c are selected to initialize each agent, where 70% of s are sampled from $d^M$, the remaining 30% is set to default state, and all skill labels c are randomly sampled from skill label space C. Note that c and s may derive from different reference motion skills, which would be beneficial for learning to switch between different motion skills.\nF. Skill labelling\nIn this work, we crop all the reference motion clips used so that they contain only one motion skill. We then artificially assign a unique skill label to each clip. Actually, for unstructured reference motion data, such as motion capture data clips with a mixture of multiple motion skills, skill labels can be considered to be acquired by pre-trained skeleton-based action recognition networks like [48], [49], and similar practice has been reported in [43]."}, {"title": "V. CONCLUSION", "content": "In this work, we introduce CSI, a flexible framework that enables legged robots to acquire a wide range of controllable and diverse motion skills directly from demonstration motion data. This technology enables the rapid integration of multiple motion skills into a single controller. We believe this capability is advantageous for applications that require legged robots to possess a diverse set of skills. One future work is to deploy our work on real legged robots to validate its feasibility in real robotics applications. Another future work will focus on implementing more detailed motion skill control, such as controlling the direction or the velocity of motion skills. This will further improve the usefulness of our approach."}, {"title": "APPENDIX", "content": "A. Hyperparameters\nAdam is used as optimizer for the policy, the value function, and the discriminator in this work, with a fixed learning rate during training. Detailed hyperparameter settings for CSI are shown in . We find that these hyperparameter combinations are suitable for training all tasks in our experiment.\nB. Baseline Settings\nFor a fair comparison, all baselines share the same observation space and action space, and for the same robot, the PD controller parameters used are also identical. In addition, the policies, the value functions, and discriminators used in all baselines are set to the same parameter sizes. Some specific training settings for CAMP and CALM are briefly described below.\nCAMP The overall architecture of CAMP is similar to that of AMP [7], but one-hot coding of motion skill labels is added to the observation inputs, and the training objective of the discriminator is modified to a conditional loss:\n$L = - E_{(s_t,s_{t+1})\\in d_M} [D(s_t, s_{t+1}|c) \u2013 1]^2$\n$- E_{(s_t,s_{t+1})\\in d_r} [D(s_t, s_{t+1}|c) + 1]^2$\n+ \\omega_{gp}E_{(s_t,s_{t+1})\\in d_M} [|| \\triangledown D(s_t, s_{t+1})||^2]\n(13)\nAccordingly, conditional style reward in CAMP is defined as:\n$r_s = max[0,1 \u2013 0.25(D(s_t, s_{t+1}|c) \u2013 1)^2]$\n(14)\nFinally, the hyperparameter settings used for CAMP are the same as that of CSI.\nCALM With motion encoder and latent space, after the pre-training stage, the controller trained by CALM [38] can control a specific integrated skill by providing a short clip of corresponding reference motion data as motion encoder's input. Therefore, in our experimental setup, baseline CALM is only subjected to the pre-training stage. Except for some hyperparameter modifications, we have preserved as much as possible the default settings in the open-source code of CALM. Some key hyperparameter settings used for CALM are shown in. It is also worth noting that the motion state representation used for the CALM's motion"}, {"title": "C. Scalability Analysis", "content": "This section demonstrates the scalability of CSI when the number of reference motion skills increases. We significantly increase the number of motion skills required to be integrated into H1 by constructing a dataset containing 25 different reference motion skills. This dataset contains all the reference datasets used by the humanoid robots in this paper, as well as a set of similar mirror motion skills (wave left hand and wave right hand, left leg kicking and right leg kicking, left hand shake and right hand shake). Additionally, we keep the size of each network unchanged and only extend the training duration to 2.5 times of the original. illustrates the coverage of motion skills. CSI demonstrates excellent scalability by mastering all motion skills despite a significant increase in the number of skills to be integrated and the inclusion of a set of similar motion skills."}, {"title": "D. Generality Analysis", "content": "This section briefly analyzes the generality of CSI. As shown in Fig. 7, CSI can be applied to different humanoid robots BRUCE and H1 as well as quadruped robot AlienGo, and they share the same training process. For other humanoid robots, CSI is also theoretically applicable, but the difficulty lies in how to kinematically retarget the motion capture data for the robot to obtain the corresponding reference motion dataset. Additionally, the motion capture data of quadrupedal creatures is more difficult to obtain than that of human beings, so the source of the reference motion skill is a major obstacle to the application of CSI on quadruped robots. In this regard, we believe that future work is necessary to further expand more sources of reference motions, such as extracting reference motions from videos [44], [45], or obtaining reference motions from generative models [46], [47]. These expanded reference motion data sources will further enhance the generality of CSI.\nAnother point is that CSI requires less correlation between motion skills that need to be integrated. The reference motion skills that can be integrated by previous work [9], [10], [11] are limited by the task objective. For example, in locomotion tasks, policies can usually only integrate task-relevant motion skills such as walking, standing, steering, and running. Integrating additional jumping skills would require further adjustments to the task objective. CSI does not have any obvious relevance requirements for this, as shown in Fig. 7, where similar motion skills, such as trot and pace, as well as very different motion skills, such as dance and zombie, can be integrated, highlighting the generality of CSI for multi-skill integration.\nE. Initialization\nWe adopt a mixed initialization strategy to accelerate motion skill learning. At the beginning of each episode, a set of initialization state and skill label pairs s,c are selected to initialize each agent, where 70% of s are sampled from $d^M$, the remaining 30% is set to default state, and all skill labels c are randomly sampled from skill label space C. Note that c and s may derive from different reference motion skills, which would be beneficial for learning to switch between different motion skills.\nF. Skill labelling\nIn this work, we crop all the reference motion clips used so that they contain only one motion skill. We then artificially assign a unique skill label to each clip. Actually, for unstructured reference motion data, such as motion capture data clips with a mixture of multiple motion skills, skill labels can be considered to be acquired by pre-trained skeleton-based action recognition networks like [48], [49], and similar practice has been reported in [43]."}]}