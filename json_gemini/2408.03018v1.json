{"title": "Integrating Controllable Motion Skills from Demonstrations", "authors": ["Honghao Liao", "Zhiheng Li", "Ziyu Meng", "Ran Song", "Yibin Li", "Wei Zhang"], "abstract": "The expanding applications of legged robots require their mastery of versatile motion skills. Correspondingly, researchers must address the challenge of integrating multiple diverse motion skills into controllers. While existing reinforcement learning (RL)-based approaches have achieved notable success in multi-skill integration for legged robots, these methods often require intricate reward engineering or are restricted to integrating a predefined set of motion skills constrained by specific task objectives, resulting in limited flexibility. In this work, we introduce a flexible multi-skill integration framework named Controllable Skills Integration (CSI). CSI enables the integration of a diverse set of motion skills with varying styles into a single policy without the need for complex reward tuning. Furthermore, in a hierarchical control manner, the trained low-level policy can be coupled with a high-level Natural Language Inference (NLI) module to enable preliminary language-directed skill control. Our experiments demonstrate that CSI can flexibly integrate a diverse array of motion skills more comprehensively and facilitate the transitions between different skills. Additionally, CSI exhibits good scalability as the number of motion skills to be integrated increases significantly.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing prevalence of legged robots, the demand for their enhanced capabilities is expected to grow continuously. One significant trend is the expectation for legged robots to manage a diverse array of motion skills to potentially cope with a wide range of tasks in real-world applications. Recently, RL-based approaches have reaped considerable success in the task of multi-skill integration for legged robots, e.g., impressive quadruped parkour [1], omnidirectional bipedal locomotion [2], or wonderful bipedal robots football match [3]. However, these RL-based approaches do not adequately fulfill this objective. One of the most hindering problems is the complex reward engineering required for skill learning using related methods. More frustratingly, the rewards designed for different skills are typically not generalizable, complicating the integration of multiple motion skills.\nIntegrating imitation learning (IL) with RL provides a feasible solution to these problems. Recently, a plethora of ongoing research in the field of character animation has demonstrated the effectiveness of this paradigm. By allowing agents to track the reference motion trajectories [4], [5], [6], or aligning with motion style from imitating reference motion [7], [8], [9], controllers trained via IL can perform motion skills naturally. As demonstration data serves as a reference for policy learning, RL approaches integrated with"}, {"title": "II. RELATED WORK", "content": "In recent years, methods built upon the RL paradigm have achieved promising performance in integrating multiple motion skills for legged robots. Rodriguez et al. [2] introduced deep reinforcement learning (DRL) method to enable the bipedal robot NimbRo-OP2X [15] to learn agile omnidirectional locomotion, including walk forward, walk backward and steering. Zhuang et al. [1] proposed a two-stage RL training approach for the quadruped robots Go1 [16] and A1 [17] to acquire complex dexterous parkour maneuvers, such as creep forward and jumping. Tuomas et al. [3] employed DRL techniques to impart agile soccer skills on the small bipedal robot OP3 [18].\nHowever, RL-based methods typically require meticulous reward engineering, which can be both labor-intensive and time-consuming, especially for multi-skill integration. To address this problem, the introduction of IL has proven effective in alleviating the need for explicit reward design, thereby offering considerable advantages for integrating multiple motion skills. This beneficial impact has been demonstrated by related works in the field of character animation. Won et al. [19] proposed a tracking-based method to integrate various motion skills from a large-scale open-source motion capture dataset [20] into a few controllers, and characters can switch between controllers to perform different motion skills. Peng et al. [21] and Zhu et al. [22] incorporated a shared embedding space within the learning process, enabling the integration of multiple skills into one single policy. Furthermore, this embedding space also serves as an interface to call integrated skills for subsequent training of downstream tasks.\nThese methods have also witnessed initial explorations on legged robots in recent studies. [9], [10], [11] incorporated velocity command tracking objectives during the training process to integrate a set of similar walking and running maneuvers into one controller. Vollenweider et al. proposed MultiAMP [12], which assigns an additional discriminator for each skill that needs to be integrated to aid learning. Han et al. [13] instead adopted a multi-stage training process, training specific Vector Quantized-Variational AutoEn-"}, {"title": "A. Multi-Skill Integration for Legged Robots", "content": "In recent years, methods built upon the RL paradigm have achieved promising performance in integrating multiple motion skills for legged robots. Rodriguez et al. [2] introduced deep reinforcement learning (DRL) method to enable the bipedal robot NimbRo-OP2X [15] to learn agile omnidirectional locomotion, including walk forward, walk backward and steering. Zhuang et al. [1] proposed a two-stage RL training approach for the quadruped robots Go1 [16] and A1 [17] to acquire complex dexterous parkour maneuvers, such as creep forward and jumping. Tuomas et al. [3] employed DRL techniques to impart agile soccer skills on the small bipedal robot OP3 [18].\nHowever, RL-based methods typically require meticulous reward engineering, which can be both labor-intensive and time-consuming, especially for multi-skill integration. To address this problem, the introduction of IL has proven effective in alleviating the need for explicit reward design, thereby offering considerable advantages for integrating multiple motion skills. This beneficial impact has been demonstrated by related works in the field of character animation. Won et al. [19] proposed a tracking-based method to integrate various motion skills from a large-scale open-source motion capture dataset [20] into a few controllers, and characters can switch between controllers to perform different motion skills. Peng et al. [21] and Zhu et al. [22] incorporated a shared embedding space within the learning process, enabling the integration of multiple skills into one single policy. Furthermore, this embedding space also serves as an interface to call integrated skills for subsequent training of downstream tasks."}, {"title": "B. Controllable Motion Skills", "content": "How to give controllability to the integrated skills is an important issue for the integration of multiple motion skills in legged robots. [9], [10], [11], [24] added additional task objectives such as tracking velocity command, to the training objective as a way to achieve controllability of the integrated motion skills through command input. This approach is constrained by the need for well-defined task objectives and is also limited by the specific task requirements that determine which motion skills can be integrated. For example, when the task involves velocity tracking, it becomes more difficult to integrate dance movements into the policy. Vollenweider et al. [12] applied one-hot skill code as input, each code corresponds a skill discriminator, through leanring to switch between these codes, the policy can adjust the output motion skill according to the input skill. Although this approach allows for a less restricted range of integrable motion skills, the training cost increases proportionally with the number of skills that need to be integrated.\nIn this paper, we propose CSI, a flexible framework that enables the integration of diverse motion skills into a single controller. CSI uses skill labels as control signals to enable controllability through integrated motion skills, which can be further combined with a pre-trained NLI module to achieve preliminary language-directed skill control. Unlike existing approaches, CSI eliminates the need for additional task objectives and multi-stage training to establish a control interface. We anticipate that our work will serve as a valuable reference for future multi-skill integration applications in legged robots."}, {"title": "III. METHOD", "content": "In this work, our goal is to enable legged robots to obtain versatile and controllable motion skills. To achieve this, we formulate the problem as a goal-conditioned [25] Markov Decision Process (MDP) $(S,A,c,R, p_0,\\gamma)$, where S is the state space, A denotes the action space, $c \\in C$ signifies the input condition, R = r(St, St+1,c) represents the reward for"}, {"title": "A. Preliminary", "content": "In this work, our goal is to enable legged robots to obtain versatile and controllable motion skills. To achieve this, we formulate the problem as a goal-conditioned [25] Markov Decision Process (MDP) $(S,A,c,R, p_0,\\gamma)$, where S is the state space, A denotes the action space, $c \\in C$ signifies the input condition, R = r(St, St+1,c) represents the reward for"}, {"title": "B. Conditional Imitation Learning", "content": "To achieve controllability over the integrated motion skills, we propose that all networks within the framework should be guided to operate according to some kind of instruction. Therefore, we introduce Conditional Imitation Learning (CIL) to guide the networks to the specified motion skills during the training process. For the discriminator, we add skill labels to the original samples input, which requires it to be able to judge the authenticity of given samples by taking into account the skill label information. For the policy and the value function, we introduce a simple encoder network to map the skill label to a latent vector z. z will be used as part of the input to the policy and the value function, which requires them to be able to respond according to the condition z. Based on the background of CIL, we design the basic training objective of the discriminator as follows:\nConditional Imitation Loss. In the framework of the vanilla GAN [27], a variational approximation of the Jensen-Shannon divergence [28] is commonly applied to achieve the adversarial training objective. Building on the concept of conditional probability, we introduce conditions based on"}, {"title": "C. Reward Setting", "content": "Similar to GAIL, the reward feedback in CSI is also derived from the discriminator. To encourage the generated motion skills to resemble the reference motion capture data, while maintaining alignment between the generated motions and the given conditions, we define conditional style reward [7] as follows:\n$r_s = -log[1 - D(S_t, S_{t+1}|c)]$\nConditional style reward provides great regularization for learning motion skills under the distribution of reference motion skill dataset. However, transitions between different motion skills that are not represented in the reference dataset often result in unnatural phenomena, such as jittering. To alleviate this, we introduce some additional regularization terms:\n$r_v = \\sum ||\\dot{q}_t - q_{t+1}||^2$\n$r_{ep} = \\sum |\\tau_t|$\n$r_a = ||a_t - a_{t-1}||^2$\n$r_t = \\sum \\tau$\nwhere q and $\\tau$ represent joint rotation angles and torque, respectively. $\\dot{q}_t$ denotes the angular velocity of each joint at time step t, a means action output by policy. All symbols $\\sum$ denote the summation over every Degree of Freedom (DoF) of the robot. These regularization terms lead to a smoother overall performance of the motion skills generated by the policy.\nFinally, the total reward is computed as the weighted sum of the style reward and all regularization terms:\n$r_r = w_s r_s + w_v r_v + w_{ep} r_{ep} + w_a r_a + w_t r_t$\nwhere ws, wv, Wep, Wa, and w\u2081 are weights used to balance each component of the rewards."}, {"title": "D. Language-Directed Skill Control", "content": "The low-level controller trained using CSI provides a skill control interface to leverage external knowledge. To implement language-directed skill control, we first manually bind an additional caption to each skill label as skill caption label, e.g. \"Walk Forward\", \"Sprint\", \"Jump\", etc. Then we align the output of the NLI module with those skill caption labels in a zero-shot manner:\nGiven a text input Ti as the premise, and a finite set of skill caption_labels_L = {L1,L2,L3,\u2026\u2026\u2026,Ln} as a set of hypotheses, where n corresponds to the number of motion skills integrated by the low-level controller. The high-level NLI module is acquired to determine the textual entailment relationship between the premise T; and each skill"}, {"title": "E. Implementation Details", "content": "Model representation. The policy $\\pi$, the value function V(st,c) and the discriminator D(St, St+1,c) are all parameterized as shallow MLP networks, each with hidden layers of size [512,256] and rectified linear unit (ReLU) activation functions. The encoder network is a MLP network of size [128, 128], with the size of latent vector z being set to fixed 8-dimensional.\nObservation space. An appropriate observation representation can guide policy training effectively. In our framework, the observation of the discriminator can be represented as {St, St+1,c}, where s\u2081 denotes the motion state of the robot at time t and c denotes motion skill label. Specifically, motion state s is defined in terms of a relatively complete state representation:\ns = {h,Rr, vr, Wr,qj,\u0121j, Pi}\nwhere h denotes the height of the root relative to the ground, and the root is roughly located near the center of the pelvis for humanoid robots and the geometric center of the torso for quadruped robots; R, represents the orientation of the root; v, and or mean the linear velocity and angular velocity of the root, respectively; qj and \u0121j are joint position and joint velocity of the j-th joint respectively; pi means the position of the i-th end-effector expressed in the local coordinate frame of the root.\nFor the policy and the value function, the observation can be represented as {at\u22121,Spropri,z}, where at\u22121 means the action of the last time step, z is the vector obtained by mapping the skill condition c by the encoder network, Spropri denotes proprioceptive states, which is defined as:\n$S_{propri} = {Vr,8_{pro},q_j,\\dot{q}_j}$\nwhere gpro is the projected gravity vector, which contains information about the robot's orientation.\nAction space. The action space of the policy is defined by the target joint rotation angles. A PD position controller translates the output of the policy into the motor torques, following the equation \u03c4= kp(a\u2081 \u2013 \u03b8\u2081) \u2013 k\u0105\u0117\u06c1. In our settings, the policy is queried at a frequency of 50 Hz, and the PD position controller operates at a frequency of 200 Hz.\nMotion retarget. For humanoid robots, we employ a retargeting method similar to the one provided in Isaac-GymEnvs [31], and for quadruped robots, we utilize the processing flow described in [4]. The retargeted motion capture data are collected as reference dataset DM for the"}, {"title": "IV. EXPERIMENT", "content": "In this section, we conduct detailed experiments on our CSI method. To demonstrate the adaptability of CSI on different robots, we select three different legged robots for our experiments including quadruped robot AlienGo [34], small humanoid robot BRUCE [35] and full-size humanoid robot Unitree H1 [36]. Factors such as morphology, mass, and varying numbers of DoF differ across these three robots, posing significant challenges for the controllable multi-skill integration.\nThe following four methods are used to comparatively validate our method:\nConditional Adversarial Motion Prior (CAMP): Based on AMP [7], [9], one-hot skill labels are used as conditional inputs to the policy, the value function and discriminator, which enables the integration of several different motion skills into a single controller.\nConditional Adversarial Latent Models (CALM) [38]: state-of-the-art multi-skill integration method in the field of character animation, which achieves integration and controllability of multiple motion skills by introducing additional motion encoder and latent space.\nBaseline-I: Our method without Condition Aware Loss.\nCSI (ours): The methodology presented in this paper."}, {"title": "A. Versatile Skills Integration", "content": "CSI can be applied to various legged robots to integrate different controllable motion skills. Fig. 2 in the supplementary qualitatively illustrates some of the motion skills integrated by the controllers for different tasks. It can be seen that CSI does not necessitate a high degree of stylistic similarity among the integrated motion skills. Both similar skills (e.g., pace and trot) and more distinct skills (e.g., dance and wave hello) can be seamlessly integrated by CSI. Furthermore, due to the incorporation of the IL paradigm, CSI does not require specific reward engineering for each motion skill. These characteristics enhance the generality of CSI for multi-skill integration tasks in legged robots.\nThe following experiments will quantitatively evaluate the performance of CSI's multi-skill integration. As an integrated skills library, it should encompass all reference motion skills. Therefore, we first validate the policy trained using CSI in terms of motion skill coverage. Specifically, a motion trajectory is obtained by conditioning the policy $\\pi_\\theta$ with a randomly selected skill label c\u2208 C. For each state transition pair (St, St+1) in trajectory \u03c4, we apply motion matching to identify the motion clip m* that contains the most similar motion transition pairs from the reference motion capture dataset DM:\nm* = arg min min ||St - 5t||2+ ||St+1 - St+1||2\nMEDM (St,St+1)\u2208m\nThe process is repeated for each motion state transition pair in the motion trajectory \u03c4. The motion category that receives the highest number of matches is considered as the category of the motion trajectory \u03c4. In this experiment, a total of 2000 motion trajectories are collected and used to validate the motion skill coverage of different methods, with each skill label sampled with equal probability.\nFig. 3 shows the skill coverage of each method across different tasks. Compared to Baseline-I, CSI demonstrates more comprehensive and balanced mastery of all skills in each task, highlighting the effectiveness of the supervised learning paradigm for multi-skill integration tasks. CAMP also achieves extensive skill coverage across all tasks, attributed to the advantage of the Least Square GAN [39] in mitigating mode collapse compared to the vanilla GAN. Additionally, CALM exhibits significant degrees of mode collapse across all tasks, failing to integrate some skills into the controller. We attribute this primarily to its unsupervised learning paradigm."}, {"title": "B. Language-Directed Skill Control", "content": "In this experiment, we qualitatively demonstrate language-directed skill control task that CSI can accomplish when combined with the pre-trained NLI module. Based on the hierarchical combination approach described in Section III-D, we select the controller trained in the H-Interaction task as the low-level module. Text descriptions from the reference motion dataset used in this task are directly employed as skill caption labels. Further details can be found in TABLE I in the supplementary. For the high-level module, we adapt bart-large-mnli, a BART [40] model trained on MultiNLI dataset [41], specifically for NLI tasks.\nIn each experiment, we first issue textual commands (1) to bart-large-mnli, and then switch to textual commands (2) after 200 time steps. Fig. 5 presents three qualitative results on the language-directed skill control task. It can be seen that bart-large-mnli guides the low-level controller to generate semantically compliant skills based on the given"}, {"title": "C. Ablation Study", "content": "Weight Decay. As illustrated in Section III-B, the introduction of weight decay can increase the diversity of the generated motion skills. To quantitatively evaluate the enhancement brought by weight decay, we adopt the Average Pairwise Distance (APD) [42], [43], which measures the diversity within a set of generated motion sequences. Specifically, given a set M containing N generated motion trajectories, each comprising a fixed length of L frames, the APD score of M is defined as:\nAPD(M) = $\\frac{1}{N(N-1)} \\sum_{i=1}^{N} \\sum_{j=1}^{N} [ \\frac{1}{L} \\sum_{t=1}^{L} (||s_{t}^{i}-s_{t}^{j}||_2)^2 ]^{\\frac{1}{2}}$\nwhere s represents the i\u2013th state of trajectory mi. A higher APD score indicates greater diversity among the motion states of the generated motion trajectories in M. For each task, we set the sampling probability of different skill labels to be equal and collect a total of 2000 motion trajectories, each with a fixed length of 200 steps. For each experiment, we perform 10 different samples and calculate the average APD as shown in TABLE II. Our method without weight decay is called Baseline-II. Evidently, the introduction of weight decay improves the diversity of generated motion skills on each task, which proves that weight decay effectively mitigates the overfitting of the discriminator."}, {"title": "D. Generality Analysis", "content": "This section briefly analyzes the generality of CSI. As shown in Fig. 7, CSI can be applied to different humanoid robots BRUCE and H1 as well as quadruped robot AlienGo, and they share the same training process. For other humanoid robots, CSI is also theoretically applicable, but the difficulty lies in how to kinematically retarget the motion capture data for the robot to obtain the corresponding reference motion dataset. Additionally, the motion capture data of quadrupedal creatures is more difficult to obtain than that of human beings, so the source of the reference motion skill is a major obstacle to the application of CSI on quadruped robots. In this regard, we believe that future work is necessary to further expand more sources of reference motions, such as extracting reference motions from videos [44], [45], or obtaining reference motions from generative models [46], [47]. These expanded reference motion data sources will further enhance the generality of CSI.\nAnother point is that CSI requires less correlation between motion skills that need to be integrated. The reference motion skills that can be integrated by previous work [9], [10], [11] are limited by the task objective. For example, in locomotion tasks, policies can usually only integrate task-relevant motion skills such as walking, standing, steering, and running. Integrating additional jumping skills would require further adjustments to the task objective. CSI does not have any obvious relevance requirements for this, as shown in Fig. 7, where similar motion skills, such as trot and pace, as well as very different motion skills, such as dance and zombie, can be integrated, highlighting the generality of CSI for multi-skill integration."}, {"title": "E. Initialization", "content": "We adopt a mixed initialization strategy to accelerate motion skill learning. At the beginning of each episode, a set of initialization state and skill label pairs s,c are selected to initialize each agent, where 70% of s are sampled from dM, the remaining 30% is set to default state, and all skill labels c are randomly sampled from skill label space C. Note that c and s may derive from different reference motion skills, which would be beneficial for learning to switch between different motion skills."}, {"title": "F. Skill labelling", "content": "In this work, we crop all the reference motion clips used so that they contain only one motion skill. We then artificially assign a unique skill label to each clip. Actually, for unstructured reference motion data, such as motion capture data clips with a mixture of multiple motion skills, skill labels can be considered to be acquired by pre-trained skeleton-based action recognition networks like [48], [49], and similar practice has been reported in [43]."}, {"title": "V. CONCLUSION", "content": "In this work, we introduce CSI, a flexible framework that enables legged robots to acquire a wide range of controllable and diverse motion skills directly from demonstration motion data. This technology enables the rapid integration of multiple motion skills into a single controller. We believe this capability is advantageous for applications that require legged robots to possess a diverse set of skills. One future work is to deploy our work on real legged robots to validate its feasibility in real robotics applications. Another future work will focus on implementing more detailed motion skill control, such as controlling the direction or the velocity of motion skills. This will further improve the usefulness of our approach."}]}