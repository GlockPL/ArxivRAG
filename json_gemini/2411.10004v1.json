{"title": "EyeDiff: text-to-image diffusion model improves rare eye disease diagnosis", "authors": ["Ruoyu Chen", "Weiyi Zhang", "Bowen Liu", "Xiaolan Chen", "Pusheng Xu", "Shunming Liu", "Mingguang He", "Danli Shi"], "abstract": "The rising prevalence of vision-threatening retinal diseases poses a significant burden on the global healthcare systems. Deep learning (DL) offers a promising solution for automatic disease screening but demands substantial data. Collecting and labeling large volumes of ophthalmic images across various modalities encounters several real-world challenges, especially for rare diseases. Here, we introduce EyeDiff, a text-to-image model designed to generate multimodal ophthalmic images from natural language prompts and evaluate its applicability in diagnosing common and rare diseases. EyeDiff is trained on eight large-scale datasets using the advanced latent diffusion model, covering 14 ophthalmic image modalities and over 80 ocular diseases, and is adapted to ten multi-country external datasets. The generated images accurately capture essential lesional characteristics, achieving high alignment with text prompts as evaluated by objective metrics and human experts. Furthermore, integrating generated images significantly enhances the accuracy of detecting minority classes and rare eye diseases, surpassing traditional oversampling methods in addressing data imbalance. EyeDiff effectively tackles the issue of data imbalance and insufficiency typically encountered in rare diseases and addresses the challenges of collecting large-scale annotated images, offering a transformative solution to enhance the development of expert-level diseases diagnosis models in ophthalmic field.", "sections": [{"title": "Introduction", "content": "The increasing prevalence of vision-threatening retinal diseases has significantly strained the global healthcare system.\u00b9 Multimodal ophthalmic images, such as color fundus photographs (CFP), optical coherence tomography (OCT), fundus fluorescein angiography (FFA), etc. provide complementary information on ocular abnormalities.2-4 Deep learning (DL) techniques have shown great potential in automatically identifying lesions in these images, aiding disease diagnosis. 5-7 In this data-centric domain, the effectiveness of DL algorithms relies heavily on the availability of well-curated medical images with high-quality annotations. 8,9 However, accessing large and diverse datasets encounters several real-world obstacles, such as complex image collection procedures, concerns about patient privacy, the labor-intensive nature of human annotation, and challenges in cross-center data sharing.10,11 Additionally, acquiring class-balanced data, particularly for rare diseases, remains a significant hurdle.\nTo address data scarcity and imbalance, various data augmentation techniques have been explored.12 Traditional random oversampling rebalances classes by duplicating minority examples, 13 but this can lead to overfitting and affect model accuracy.14 Recently, though generative adversarial networks (GANs) have been used to synthesize realistic images and improve disease diagnosis, such as diabetic retinopathy (DR) and age-related macular degeneration (AMD), most efforts have focused on unimodal images.15-19 Besides, self-supervised learning has been explored in ophthalmology, as demonstrated by the RETFound model, to enhance data usability.20 Nevertheless, this approach did not fully address data imbalance or the scarcity of rare disease images.\nThe stable diffusion (SD) model, a novel text-to-image generation approach, offers a promising solution by creating realistic images from natural language prompts.21 In general domain, the SD can generate images that perform as well as or better than real images in training self-supervised models.22 This approach provides a practical way to obtain diverse"}, {"title": "Methods", "content": "EyeDiff was trained on large-scale collection of ophthalmic image-text pairs, covering 14modalities and over 80 disease categories from eight datasets. This extensive training enabled the model to accurately capture the relationships between image distributions and their corresponding textual descriptions across a broad spectrum of diseases. We then assessed the clinical value of EyeDiff by using its generated images to augment minority classes across 10 external validation datasets from various global regions, and thoroughly evaluated its performance in downstream disease diagnosis tasks.\nTraining datasets\nA brief overview of the eight training datasets is as follows: (1) Retinal Image Bank: An open-access collection by the American Society of Retinal Specialists, containing over 29,000 multimodal images and descriptions covering various retinal diseases (Supplementary Table 1). (2) EyePACS: Comprising 88,702 fundus images from diverse populations with varying degrees of DR, commonly used for developing and testing DR screening models.23 (3) OCTDL: An open-source OCT dataset with over 2,000 OCT images labeled with disease groups and retinal pathologies, including age-related macular degeneration (AMD), diabetic macular edema (DME), epiretinal membrane (ERM), retinal artery occlusion (RAO), retinal vein occlusion (RVO), and common vitreomacular interface diseases. 24 (4) REFUGE: Provides 1,200 fundus images with ground truth segmentations and glaucoma labels, representing the largest dataset for evaluating automated glaucoma assessment methods.25 (5) ORIGA: Contains 650 retinal images annotated by professionals, focusing on disc and cup segmentation and the cup-to-disc ratio (CDR).26 (6) RIM-ONE: An open retinal image database for optic nerve evaluation, including 313 fundus images from normal subjects and 172 fundus images from glaucoma patients, with manual segmentation by a glaucoma expert.27 (7) DRISHTI: consists of 101 fundus images from both normal and glaucomatous eyes, with optic disc and cup segmented by four experts.28 (8) GAMMA: a multimodal image dataset for glaucoma grading, consisting of fundus and OCT images from 300 patients, annotated with glaucoma grade, macular fovea coordinates, and optic disc/cup segmentation masks.29\nDatasets for downstream tasks\nTo evaluate the effectiveness of EyeDiff-generated images in optimizing retinal disease diagnosis, we used RETFound as the baseline model and employed nine open-access ophthalmic image datasets.20 (1) DR diagnosis: The APTOS-2019 (India), IDRiD (India), and MESSIDOR-2 (France) datasets were used, with labels based on the International Clinical Diabetic Retinopathy Severity Scale, indicating five stages: no DR, mild non-proliferative DR (NPDR), moderate NPDR, severe NPDR, and proliferative DR. (2) Glaucoma diagnosis: The PAPILA (Spain) and Glaucoma Fundus (South Korea) datasets were used, including three labels: non-glaucoma, early glaucoma (suspected glaucoma), and advanced glaucoma. (3) Multi-category eye diseases diagnosis: The JSIEC (China), Retina, OCTID and OCTDL datasets were applied. JSIEC consists of 1,000 images, including 39 common fundus diseases and conditions. The Retina dataset has labels for normal, glaucoma, cataract, and retinal diseases. OCTID includes 572 OCT scans labeled as normal, macular hole, age-related macular degeneration (AMD), central serous chorioretinopathy (CSCR), and DR. OCTDL includes labels for normal, AMD, DME, ERM, RVO, etc. (4) Rare diseases diagnosis: Images collected from the Retinal Image Bank between 2019 and 2023 were applied. We created a custom dictionary to select rare disease cases, such as birdshot retinochoroidopathy, cone dystrophy, Stargardt disease, etc. Rare diseases categories can be retrieved from authoritative databases such as the American Academy of Ophthalmology, Orphanet, and the National Organization for Rare Disorders. A total of 2339 images representing 17 rare diseases were included.\nWell-designed text prompts for guiding image generation\nTo obtain semantically rich text guidance, we conducted an information fusion process across multifaceted descriptions of ophthalmic images. We built a custom dictionary to map different expressions of diseases into a structured format using keyword matching with regular expressions. The mapped labels include image modality, lesion or disease annotations, and disease severity (such as mild NPDR, moderate NPDR, etc.). We excluded non-routine retinal examination images, such as histology and pathology images, and conditions with fewer than 50 occurrences.\nModel development\nEyeDiff was based on SD v1-5. In our training process, we utilized text prompts as input and the corresponding images as the ground truth to train EyeDiff. Among various text-to-image models, SD has garnered significant attention for its impressive performance in generating high-quality images and its cost-effective fine-tuning. Its denoising process operates in a latent space, akin to diffusion models, resulting in final images that are highly consistent with the input text. This makes SD an excellent tool for text-guided image editing.21,30 EyeDiff is designed to learn the distribution of ophthalmic images from multiple modalities, conditioned on the corresponding text prompt which contains multifaceted information. During training, images are resized to 512\u00d7512 resolution and encoded through an encoder, which turns images into latent representations. The text prompts are encoded through a CLIP text encoder"}, {"title": "Image quality evaluations", "content": "We applied novel VQAScore31 to objectively measure the alignment between generated images and the corresponding texts in downstream tasks. The VQAScore applied a visual-question -answering (VQA) model to generate an alignment score by calculating the probability of a \u201cYes\u201d answer to the question \u201cDoes this figure show \u2018[text]'?\". The VQAScore represents a simple and effective metrics that outperforms prior art, 33 demonstrating strong agreement with human judgements. This metric ranges from 0 to 1, with higher scores indicating better alignment.\nHuman evaluation\nFifty images were randomly selected for visual quality evaluation and Turing test by two experienced ophthalmologists (R.C. and X.C.).\nQualitative Turing test: The Turing test was conducted using ophthalmic images without the annotations of \u201creal\u201d or \u201cgenerated\". We asked the ophthalmologists (R.C. and X.C.) to determine whether the image was collected from a real patient or was synthesized by our stable diffusion model prompted by a text.\nVisual quality evaluation: The ophthalmologists evaluated the generated images subjectively using a five-point scale, considering the integrity of generated structures and\""}, {"title": "Downstream diagnosis task using generated images", "content": "We evaluated the applicability of EyeDiff-generated images in augmenting minority classes and enhancing overall disease diagnosis. Using the Vision Transformer (ViT), we investigated whether adding EyeDiff-generated images could improve the diagnosis accuracy of multiple retinal diseases. RETFound, a foundation model known for its high accuracy in retinal disease diagnosis, served as our baseline 20 We compared three models, all utilizing the same hyperparameters for classifying retinal diseases: (1) Original real images (RETFound), (2) Original real images + oversampling regular images (Oversample), and (3) Original real images + images generated by our text-guided model (EyeDiff). All models were initialized with pretrained weights from RETFound. Specifically, features from different images were extracted by a ViT large model into 1024-dimensional embeddings. These embeddings were processed through an attention-based feature fusion module, which assigns different weights to each embedding using a multi-head attention mechanism. The weighted features were then aggregated to form a single, fused representation, which was passed through a fully connected layer and a softmax layer to obtain the final diagnosis output.\nThe performance of retinal disease diagnosis was evaluated through various metrics, such as"}, {"title": "Results", "content": "We finally enrolled 42,048 images from eight datasets for model development and 14,530 images from 10 datasets for downstream common and rare diseases diagnosis. \nQuantitative evaluation\nWe finally enrolled 77 categories of texts and the corresponding generated images in downstream datasets for objective quality evaluation. The VQAScore results for text-image alignment in downstream tasks are presented in Table 2. The average VQAScore in datasets for OCT-based disease detection, CFP-based multi-category eye disease diagnosis and multimodal imaging-based rare disease diagnosis is 0.822, 0.776 and 0.670 respectively.\nHuman evaluations\nThe examples of text prompts and the corresponding generated images are shown in Figure 2. Our model achieved high fidelity in generating detailed structures and lesions in corresponding image modalities according to the text prompts.\nVisual quality evaluation: Fifty generated images were assessed for visual quality by two experienced ophthalmologists (R.C. and X.C.) using a five-point scale (1 = completely matches the text prompts; 5 = does not match the text prompts). The visual scores were 1.940 \u00b1 1.085 as evaluated by the first grader and 2.080 \u00b1 1.055 by the second grader, with a Kappa value of 0.870.\nTuring test: Among fifty generated images, ophthalmologists mistook 62% to 66% of them for real images. Most of the generated images shared high similarity with real images. The"}, {"title": "EyeDiff improves DR and glaucoma disease diagnosis", "content": "Integrating EyeDiff-generated images with original real images significantly improved the diagnosis accuracy for DR and glaucoma. In the DR diagnosis task using the IDRiD dataset, the AUROC improved from 0.826 (95% CI: 0.821-0.832) at baseline to 0.837 (95% CI: 0.833-0.840) with generated images. The AUPR rose from 0.502 (95% CI: 0.483-0.520) to 0.518 (95% CI: 0.509-0.527), outperforming oversampling-augmented images. For glaucoma diagnosis using the Glaucoma Fundus dataset, the AUROC was 0.950 (95% CI: 0.937-0.964) for original real images, 0.959 (95% CI: 0.945-0.973) for oversampling-augmented images, and 0.959 (95% CI: 0.945-0.973) for EyeDiff-generated images. The AUPR improved from 0.876 (95% CI: 0.841-0.911) to 0.893 (95% CI: 0.855-0.931) with EyeDiff-generated images. These improvements were statistically significant."}, {"title": "EyeDiff improves multi-class disease diagnosis", "content": "In multi-class disease detection tasks using JSIEC and Retina datasets, the addition of EyeDiff-generated images significantly improved accuracy. For the JSIEC dataset, the AUROC increased from 0.990 (95%CI: 0.989-0.992) to 0.996 (95%CI: 0.995-0.997), and the AUPR rose from 0.887 (95%CI: 0.871-0.891) to 0.967 (95%CI: 0.957-0.978). For the Retina dataset, the AUROC improved from 0.857 (95%CI: 0.831-0.873) to 0.892 (95%CI: 0.867-0.918), and the AUPR increased from 0.720 (95%CI: 0.688-0.761) to 0.779 (95%CI: 0.731-0.826). These differences were statistically significant. In OCT-based disease detection tasks using the OCTID and OCTDL datasets, the addition of generated images also significantly improved accuracy. For the OCTID dataset, the AUROC increased from 0.993 (95% CI: 0.987-0.999) to 0.995 (95% CI: 0.992-0.997), and the AUPR improved from 0.980 (95% CI: 0.967-0.993) to 0.982 (95% CI: 0.969-0.994). For the OCTDL dataset, the AUROC was 0.982 (95% CI: 0.972-0.992) to 0.996 (95% CI: 0.995-0.997) and the AUPR increased"}, {"title": "EyeDiff improves rare disease diagnosis", "content": "Adding EyeDiff-generated images to the Retina Image Bank significantly enhanced the classification performance of 17 rare diseases. The AUROC was 0.871 (95%CI: 0.863-0.891), 0.893 (95%CI: 0.872-0.923) and 0.919 (95%CI: 0.882-0.931) and the AUPR improved from 0.439 (95%CI: 0.401-0.462) to 0.530 (95%CI: 0.497-0.550)."}, {"title": "EyeDiff-generated images enhance disease classification in minority classes", "content": "Data imbalanced issue exists in these ten downstream datasets and the specific minority classes were demonstrated in Table 4. Supplementing EyeDiff-generated images significantly enhance disease diagnosis performance in these imbalanced classes. (1) IDRID dataset: The AUROC for mild retinopathy increased from 0.772 (95%CI: 0.733~0.811) to 0.817 (95%CI: 0.780~0.864). (2) APTOS 2019 dataset: The AUROC of severe retinopathy was increased from 0.867 (95%CI: 0.829~0.906) at baseline to 0.914 (95%CI: 0.903~0.934). (3) MESSIDOR2 dataset: The AUROC for proliferative retinopathy increased from 0.960 (95%CI: 0.937~0.988) to 0.980 (95%CI: 0.967~0.994). (4) Glaucoma Fundus dataset: The AUROC of early glaucoma increased from 0.860 (95%CI: 0.827~0.873) to 0.927 (95%CI: 0.919~0.934). (5) PAPILA dataset: The AUROC of glaucoma increased from 0.754 (95%CI: 0.742~0.778) to 0.795 (95%CI: 0.756~0.813). (6) JSIEC dataset: The AUROC of fundus neoplasm increased from 0.739 (95%CI: 0.737~0.756) to 0.875 (95%CI: 0.855~0.877). (7) Retina dataset: The AUROC of cataract increased from 0.951 (95%CI: 0.912~0.954) to 0.961 (95%CI: 0.923~0.977). (8) OCTID dataset: The AUROC of AMD increased from 0.921 (95%CI: 0.890~0.946) to 0.954 (95%CI: 0.947~0.969). (9) OCTDL dataset: The AUROC of RVO increased from 0.992 (95%CI: 0.967~1.011) to 0.993 (95%CI: 0.983~1.051) (10) Retina Image Bank dataset (rare diseases only): The AUROC of optic nerve hypoplasia increased"}, {"title": "Discussion", "content": "In this study, we developed EyeDiff, a text-to-image diffusion model, to synthesize multimodal ophthalmic images from text prompts. EyeDiff demonstrated robust performance in generating key lesions of various abnormalities based on text prompts, significantly improving the accuracy of classifying both common and rare diseases on top of a well-established foundation model. This data augmentation method outperformed the conventional oversampling methods, offering a promising solution for overcoming challenges in collecting rare, annotated images and facilitating the development of expert-level disease detection models through balanced data generated from simple text cues.\nDespite the rarity of individual rare diseases, their collective burden is substantial, affecting over 900 eye abnormalities and leading to lifelong vision impairment in many individuals.34 Developing accurate DL models for rare eye diseases screening is promising but hindered by the lack of extensive annotated data.. Even RETFound, a powerful generalist model, excels primarily in common disease diagnosis.20 Previous foundation models like EyeFound and EyeCLIP showed potential in detecting rare eye diseases by learning from unlabeled multimodal retinal images.35,36 EyeDiff further removes the barriers to collecting rare diseases images, such as macular dystrophy, acute posterior multifocal placoid pigment epitheliopathy, etc. by generating diverse, high-quality images from large-scale multimodal datasets, covering multiple imaging modalities and ocular diseases.\nDiffusion models, such as SD, excel in generating realistic images by modeling complex data distributions through forward and reverse diffusion processes.37 SD integrates text embeddings with image features using a cross-attention mechanism, ensuring generated images accurately reflect text prompts.21 This approach offers more diverse image resources"}]}