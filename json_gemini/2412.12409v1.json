{"title": "Improving Cooperation in Language Games with Bayesian Inference and the Cognitive Hierarchy", "authors": ["Joseph Bills", "Diego Blaylock", "Christopher Archibald"], "abstract": "In two-player cooperative games, agents can play together effectively when they have accurate assumptions about how their teammate will behave, but may perform poorly when these assumptions are inaccurate. In language games, failure may be due to disagreement in the understanding of either the semantics or pragmatics of an utterance. We model coarse uncertainty in semantics using a prior distribution of language models and uncertainty in pragmatics using the cognitive hierarchy, combining the two aspects into a single prior distribution over possible partner types. Fine-grained uncertainty in semantics is modeled using noise that is added to the embeddings of words in the language. To handle all forms of uncertainty we construct agents that learn the behavior of their partner using Bayesian inference and use this information to maximize the expected value of a heuristic function. We test this approach by constructing Bayesian agents for the game of Codenames, and show that they perform better in experiments where semantics is uncertain.", "sections": [{"title": "1 Introduction", "content": "Many real-world problems require cooperation with AI or human agents for success. Cooperative games provide an important setting for the development and evaluation of cooperative AI techniques. Agents generally utilize some form of communication to cooperate. Some settings define the semantics of the communication actions which exist only within the game. However, agents can differ in their use of these actions in practice to communicate information. One challenge faced by some reinforcement learning approaches to these problems is that agents develop their own way of using actions to communicate during training. When these agents interact with other agents who do not use the actions in the same way, performance can suffer greatly.\nIn this paper, we focus on a specific type of cooperative game: cooperative language games. The rules of these games require agents to communicate using natural language. One might expect this solves the difficulty agents have communicating, but this is not necessarily the case. Because AI agents might use different language models to represent relationships between words, they could use language in the game in different ways. Even when agents use the same language model, they may have a different understanding as to what action should be performed in response to an utterance, especially if the meaning of an utterance is ambiguous due to restrictions on the amount of information which can be communicated. This illustrates the distinction between the semantic and pragmatic understandings of an utterance, and differences in either understanding can impede communication between agents.\nOur goal is to design an agent for a cooperative language game able to cooperate successfully with any teammate agent, regardless of the semantics of the teammate's language model or the pragmatics of their use of language. We approach this using a Bayesian framework, where instead of being restricted to a single language model, our agent is given a set of possible language models over which it can reason probabilistically. Bayes' rule is used to update the agent's beliefs as it observes its teammate acting. This is, to our knowledge,"}, {"title": "2 Background", "content": "Much previous research has focused on the task of agents who can cooperate with arbitrary teammates, which has been called ad hoc teamwork [24, 33]. The different approaches proposed include some from a reinforcement learning perspective where the goal is to determine a representative set of potential teammates strategies that can be used during training [8, 28], while others instead focus on modifying behavior during play with a specific teammate [3, 25]. An example of a game that has been widely used as an ad hoc domain is Hanabi. Hanabi is a cooperative card game where players communicate using game-defined actions to gain information about their own cards, so that they can successfully play their cards and benefit the group [2, 8, 7, 12]. This is an example where the semantics of the communication are defined by the rules of the game, and so agents can be assumed to have the same understanding of them. The major uncertainty with an unknown teammate comes in the pragmatics of how that agent will use the actions. In contrast, Codenames involves natural language used outside the game. This means that agents might differ in both their semantic interpretation and representation of words as well as in the pragmatics of how they use the words in the game.\nBayesian statistics provides a framework for reasoning about uncertainty [5], which has been widely applied to multi-agent settings enabling AI agents to reason about teammates and opponents. As some examples, Bayesian reasoning has been used within the game of Hanabi [12, 7], n-player competitive games [34] and auctions [1], and to determine which of an agent's strategies will work best against the current opponent in poker [3]. In each case the set of objects over which probabilistic beliefs are held, the way those beliefs are updated upon observations of events in the game, and how the current beliefs are utilized to determine the best action must be designed specifically for that domain. We explore the application of similar ideas to Codenames, which to our knowledge involves the first case of having a probability distribution over a set of language models in a cooperative language game.\nIn addition to utilizing Bayesian reasoning to reason about pragmatics we design our proposed agents to fit within a hierarchy. The model we use is similar to the cognitive hierarchy introduced by Stahl [31, 32]. Our model differs in that best-responses are only approximate and that in addition to a distribution of lower"}, {"title": "Codenames", "content": "Codenames is a board game which explicitly involves language and concepts of similarity between words [9]. In the game, there are 25 cards with a single word on them laid out on the board. Each board card belongs to a hidden category. The possible categories are red, blue, bystander, and assassin. Two teams (red and blue) of at least two players each, compete to be the first to identify all of their team's board cards. The players take one of two roles on each team: spymaster and guesser. The spymaster is aware of the hidden category of each word on the board, but only the guesser can pick a board card and reveal its category assignment. Each turn, the spymaster gives a clue to the guesser. Each clue consists of a single word and a number. The clue word is related in meaning to some of the cards on the board, while the clue number is generally the number of board cards the spymaster intends to connect to the clue word. The guesser must guess at least one card, revealing its hidden category, and picks cards until either 1) a card is guessed that doesn't belong to their team, 2) they have guessed one card more than the clue number, or 3) they choose to end their turn. A team wins when all of their cards have been revealed, ending the game. A team loses instantly if they guess the assassin card.\nA single-team variant of Codenames, where the goal is to guess all the team's cards in as few turns as possible, was used in a Codenames AI competition, and has been used in subsequent AI research. on Codenames [35, 18]. The best AI Codenames agents to date have used word embeddings. While alternatives have been explored, such as large language models [10], they have not yet been as effective as the embedding approaches. Embedding approaches have a modular design, making it easy to separate and interchange the semantic and pragmatic elements of an agent. An agent's word embedding defines the relationships between words, and basic guessers essentially guess board cards in the order of distance from the clue word, according to their word embedding. Spymasters give the clue that maximizes the number of board cards correctly identified by a basic guesser using the same word embedding. These basic strategies result in agents that play very well together when using the same word embedding, but performance is typically very poor when embeddings differ [18]. Subsequent work generally expanded the explored set of language models and began evaluating with a small set of humans [17]. One agent explored in that work used a n\u00e4ive Bayes filter to classify words, but it did not use Bayesian inference to adapt to the behavior of its teammate. Other focused on improving clues for play with humans, but didn't use the actual game of Codenames [19]. All the previous work focuses on identifying the single language model and/or strategy that will work best with a given population of teammate agents, oftentimes humans. However, all of the proposed agents are static, as they do not adjust or adapt their behavior based on the interaction with their current teammate. The baseline"}, {"title": "3 Foundations", "content": "In this section we introduce the foundational concepts and a general overview of our proposed Bayesian approach for cooperative language agents. In particular we will detail how uncertainty regarding both semantics and pragmatics will be represented within the framework. Codenames is used throughout as a concrete example of a cooperative language game, but the same concepts should be applicable to similar settings, albeit with some necessary adaptation."}, {"title": "The Deductive Hierarchy", "content": "The deductive hierarchy is an organization of agents for playing Codenames initially proposed in [4]. The foundation of the hierarchy (level 0) is a static guesser, or a guesser that only considers the current clue and unrevealed board cards in determining its guess. The hierarchy then consists of alternating spymasters and guessers, designated so that a level k spymaster (Sk) approximates a best response to a level k guesser (Gk), while a level k+1 guesser (Gk+1) similarly approximates a best response to Sk. Roughly speaking, Sk assumes it is playing with Gk, simulating Gk's response to any clue it could give, and choosing the clue that results in the maximum revealed team cards. Gk assumes that the clues it receives are generated by Sk\u22121.\nGk maintains beliefs over all possible states of the world (board card assignments), removing those that are inconsistent with given clues and revealed information. Board card identities are deduced when they are true in every remaining possible state. When Gk has deduced that a board card is not on its team, it will skip over that card when guessing, and when it deduces a card is on its team it will use the extra guess to guess it. This behavior allows hierarchical agents on the same level to gain the most information possible from each clue and win the game in fewer turns. From the perspective of this hierarchy, the basic Codenames AI agent framework initially described in [18] consists of a level 0 guesser and a level 0 spymaster. Agents from higher levels in the hierarchy are dynamic, as the clues and guesses they produce will depend upon the entire history of the game to that point. The deductive hierarchy is fragile and hierarchical agents perform poorly when assumptions about teammates are incorrect. Our proposed Bayesian framework can be viewed as an extension of the hierarchy to reason probabilistically and be more robust with all teammates."}, {"title": "Modeling Uncertainty: Pragmatics and Semantics", "content": "One of the core ideas of any Bayesian framework is to explicitly model and account for sources of uncertainty. The main sources of uncertainty in cooperative language games like Codenames are semantics and pragmatics. We represent uncertain semantics by a probability distribution over a set of different word embeddings. Uncertain pragmatics can be represented by a probability distribution over different levels of the just described deductive hierarchy. Both of these sources of uncertainty can be captured by having a set of possible teammates, each with a word embedding and level in the deductive hierarchy. In order to account for the possibility of partnering with an unknown agent, we also add a noise model to the word embeddings. This means that any clue or guess has some non-zero probability of being generated by any agent model. Based"}, {"title": "Bayesian Approach Overview", "content": "We now provide an overview of the proposed Bayesian approach for Codenames agents. Each agent will have a set of possible teammate models M, and each m\u2208 M should differ by the word embedding it is using (semantics) or its position in the hierarchy (pragmatics). Beliefs over these teammate models will be maintained in the form of a probability distribution P(m). When a teammate action a is observed, the agent will update its beliefs using Bayes rule as $P(m | a) \\propto P(a|m)P(m)$, where the posterior $P(m | a)$ will be used as the prior $P(m)$ for the next update.\nThe distribution P(a | m) corresponds to the specifics of how teammate model m acts in the game, as determined by its strategy and word embedding. The Bayesian approach will fail if P(a | m) = 0 for all teammate models when action a is observed since it will result in the posterior P(m) being set to zero for all models, preventing any future inference. To avoid this problem and ensure all models may continue to be used for inference despite faults in the approximation, each teammate model should have a non-zero probability of generating any action. All previous Codenames agents of which we are aware have been deterministic: for the same state of the game, they would generate the same guess or clue. Thus, these previous agent designs must be modified or adapted to be stochastic before they can be used effectively as teammate models in our proposed Bayesian approach. This can be done by adding a random perturbation the embeddings of words, and details of this process will be provided in later sections."}, {"title": "Heuristic Utility Function", "content": "When making a decision in the game, Bayesian agents will use their beliefs to select the action that maximizes expected utility. The Bayesian agents will utilize the following heuristic utility function which will provide a utility for a sequence of cards to be guessed on one turn, based on the identity of revealed cards. The spymaster, with knowledge of the true world state, can use the actual card identities, while the guesser will instead use a possible world state. The utility of a turn is the sum of the values of any cards revealed that turn, minus 1. The value for the card types are as follows, assuming the agents are on the red team: u(red) = 1, u(blue) = \u22121, u(bystander) = 0, u(assassin) = \u2212|R|, where |R| is the total number of red team cards.\nThis heuristic calculates the marginal contribution to the score at the end of the game from the current turn assuming a particular variant of the solitaire Codenames where an opponent card must be revealed each turn. Further explanation is found in the appendix. The heuristic utility function could easily be replaced by another, where motivated.\nWe now describe the Bayesian spymaster, followed by the Bayesian guesser. Due to space restrictions, the full details of each agent will not be given here, but details are included in the appendix and source code. The experimental evaluation of the agents will be given in Section 6."}, {"title": "4 The Bayesian Spymaster", "content": "The Bayesian spymaster aims to deduce which guesser it is playing with, so that it can give more effective clues. The Bayesian spymaster maintains beliefs over a set of guesser models M, represented by a probability distribution that is updated after each turn and thus incorporates all information obtained by previous guesses. Initially, these beliefs are set to be the uniform distribution over M. Given observation of a guess g, the update is $P(m|g) \\propto P(g|m)P(m)$. The conditional probability P(g|m) cannot be computed exactly for arbitrary guessers, so it is estimated with a multinomial distribution, using Monte Carlo sampling to"}, {"title": "5 The Bayesian Guesser", "content": "The Bayesian guesser will have a set of spymaster models, M, over which it will maintain beliefs. It also maintains a history of previous clues and is parameterized with thresholds for guessing and skipping cards. After receiving a clue, the guesser will first sample possible world states consistent with the current observed state of the board using the method described in [4]. For each clue lt in the history, it will calculate the likelihood the clue was given for each combination of spymaster m and world state w in the sample. This likelihood is given exactly by $\\int_{v(l)} N(m_t(w), \\sigma,x)dx$ where v(l) is the Voronoi region centered around l, $m_t(w)$ is the clue model m \u2208 M would give at turn t assuming w was the true state of the world, and $N(m_t(w), \\sigma, x)$ is the multivariate symmetric Gaussian centered at mt(w) with \u03c3 covariance where x is an arbitrary point in the embedding space. The Gaussian distribution is used here since it is the noise added to the embeddings in our experiments, but it could be replaced by another distribution. The entire history's likelihood is calculated as the product of the likelihoods for each turn's clue. The likelihoods of the clue history for each model and world state can be separately marginalized out. If there is a nonzero likelihood of the current clue occurring then the beliefs about model probabilities are updated according to the posterior distribution. Otherwise this step is skipped, mirroring the behavior of the level-k guesser [4]. If beliefs are not updated then the clue is also not added to the history.\nNext, the posterior beliefs of each world state are calculated assuming they were equally likely a priori. The probability of each unknown card on the board belonging to any category is then estimated using a n\u00e4ive Bayesian filter. Board cards are first filtered using the skip threshold. Any card with a probability of being on the guesser's team equal to or below the skip threshold is removed from consideration for boosting it's probability beyond what a strict Bayesian interpretation would infer. Among the remaining board cards, the n closest to the clue have their probability of being their team's card boosted, or increased, to one, where n is the numerical component of the clue and distance is measured according to the word embedding of the model spymaster with the highest posterior probability.\nBoard cards are then filtered using the belief threshold. Among the cards where the probability of being on the team is at least equal to the belief threshold, the card with the highest positive expected utility according to the heuristic function of Section 3 is selected. If no cards have a utility greater than zero, then the card with the highest expected utility among all cards is selected instead, since the rules require at least one card to be guessed.\nTo complete the remainder of the guess, the guesser assumes the first guessed card belongs to the guesser's"}, {"title": "Skip and Belief Thresholds", "content": "To ensure the Bayesian Guesser is a strict generalization of the level-k guesser, it is parameterized using aforementioned skip and belief thresholds. In the deductive hierarchy, proximity to the clue is a signal used to signify which cards are on a player's team, but clues which have been deduced to not be on the player's team are skipped when updating beliefs using this signal. This behavior is not directly modeled in the Bayesian ideal and is missing in some edges cases, so it is explicitly modeled in the Bayesian Guesser by having the probability of close cards be boosted. As such, the skip threshold describes the posterior probability where close cards are considered to not have be signalled because they are likely not on the player's team. The belief threshold describes how confident the guesser must be that the card is on the team before it is guessed. These parameters are designed so that when belief = 0 and skip = 1 the model behaves uses the ideal Bayesian reasoning described, but as skip \u2192 0 and belief \u2192 1 it relies less on the heuristic and Bayesian reasoning and more on an approximation of logical deduction. When skip = 0 and belief = 1, there is no noise, and the set of modeled guessers is a singleton of the level-k \u2013 1 guesser, it behaves identically to the level-k guesser. Details for how these parameters are integrated into the model are included in the appendix."}, {"title": "Cognitive Hierarchy", "content": "A cognitive hierarchy can be constructed by assuming Bayesian agents have prior distributions over other Bayesian agents. To define this behavior, whenever a reference is made to the word embedding of a Bayesian agent, the word embedding of its highest posterior probability agent becomes the assumed word embedding, with a defined ordering of agents to break ties. This implies the word embedding of a level k agent is the same as the level 0 guesser it is founded on. This allows arbitrary cognitive hierarchy models to be defined using Bayesian agents."}, {"title": "6 Experimental Evaluation in Codenames", "content": "We now describe the details of the Codenames experiments that were carried out to evaluate the Bayesian Codenames agents just described at level k = 1 in the hierarchy."}, {"title": "Varied Semantics: Word Embeddings", "content": "The word embeddings used in the experiments include:\n\u2022 Word2Vec (w2v) \u2013 trained using a word context windows [23].\n\u2022 Dict2Vec (d2v) \u2013 similar to w2v but trained on cleaned dictionary entries with an improvement on semantic similarity tasks [36].\n\u2022 FastText (ftxt) \u2013 Uses bags of character n-grams with weighting by position [22].\n\u2022 GloVe (g1, g3) \u2013 trained on pre-computed statistical co-occurrence probabilities for words in a corpus [26]. We used embeddings with dimensions 100 and 300."}, {"title": "Experiment Setup", "content": "For the Bayesian Spymaster, we used two models referred to as $SMB$ and $SMB$ the first assuming zero perturbation noise, and the second assuming a noise value of 1.0.\n6 different Bayesian guessers were evaluated: $DGB$, $DGB$, $BGB$, $BGB$, $MGB$, and $MGB$. Each of the three types again had a version without embedding perturbation noise and a version with a noise value of 1.0. The three types differ in their skip and belief threshold parameters with $DGB$ being set to $skip = 0$ and $guess = 1$, $BGB$ set to $skip = 1$ and $guess = 0$, and finally $MGB$ set to $skip = 0.5$ and $guess = 0.5$.\nAll the Bayesian agents included the same set of word embeddings (w2v, g3, cnnb, and d2v) in the model set M. These are called the internal word embeddings. A set of static non-Bayesian agents using these word embeddings was included in each experiment as a baseline. In addition, the following word embeddings were also used in the experiments, although they were not part of the Bayesian agents: g1, ftxt, wg, and elmo. This set of word embeddings will be referred to external.\nTo more efficiently calculate the set of possible clues, the 300 nearest neighbours of each word were precomputed. The probability that a perturbed vector would fall in the Voronoi region for any clue was precomputed using 1000 samples at each noise levels. This was done by perturbing a model's word embedding using a standard normal distribution with mean at the embedding and then finding the closest word among the 500 closest neighbours. The Bayesian spymasters all used 10 samples, and the Bayesian guessers used 1000 or 10,000 samples.\nEach of the Bayesian spymasters as well as the static spymasters for both internal and external word embeddings were evaluated in environments both with and without the addition of stochastic embedding perturbations to all communicated clue words. These are called stochastic and deterministic environments respectively. Each spymaster played against all of the guessers. The guessers consisted of all the Bayesian guessers, as well as static guessers using both internal and external word embeddings, in both stochastic and deterministic environments. In stochastic environments noise was added to the clue embedding for the guesser in the Bayesian spymaster experiments, while for the Bayesian guesser experiments noise was added to the spymaster's word embedding and then transformed to the closest clue in the vocabulary before being passed to the guesser. Each pairing played 500 games. The results report the win rate for each pair, which is the fraction of solitaire games the pairing is able to successfully win."}, {"title": "7 Discussion", "content": "In general, the Bayesian spymaster that assumed noise performed better than any other spymaster whenever they were partnered with out-of-distribution models or in stochastic environments. We believe this is because the spymaster rapidly learns which of its model guessers is the best fit for the guesser it is playing with, and then performs better than the corresponding spymaster for that word embedding because it hedges its clues around uncertainty from noise or unmmodeled behavior.\nSurprisingly, the Bayesian spymaster performed better with the guesser using d2v than the corresponding"}, {"title": "8 Conclusions", "content": "Bayesian inference and cognitive hierarchies can improve cooperation in language games. We demonstrated the first use of Bayesian reasoning to adapt to teammates in Codenames. The Bayesian spymaster was shown to be especially successful with out-of-distribution teammates, which has been a key difficulty with previous Codenames AI approaches. Effective implementation requires game-specific optimizations. While the theoretical description allows for arbitrarily large cognitive hierarchies, practical difficulties in implementing higher levels in the hierarchy made experimentation beyond the first level out of scope for this study. We hope to overcome that limitation in the future. Additionally, all experiments to date have involved only simulated language models. We would like to see how agents perform with human subjects, who display both complex semantic and pragmatic reasoning. We hope work with multiple semantic and pragmatic models in simulated environments will lead to creating agents that can better communicate with people."}, {"title": "Appendix A The Bayesian Guesser", "content": "In this section we give the complete algorithm for the Bayesian Guesser First, Algorithm 1 is called at beginning of game to initialize the Bayesian guesser."}, {"title": "Appendix B The Bayesian Spymaster", "content": "In this section we give the complete algorithm for the Bayesian Spymaster, which has the following variables and parameters:\n\u2022 Set of possible guessers \u011c"}, {"title": "Appendix C Additional Details", "content": "This section provides some additional details on a few aspects of the Bayesian agent framework."}, {"title": "Utility Heuristic", "content": "The following utility heuristic is used:\n$v(\\gamma, A) = \\sum v(A(\\gamma_i)) \u2013 1$\nv(r) = 1, v(b) = \u22121, v(y) = 0, v(a) = \u2212|R|\nThe rational behind this heuristic is that it calculates the marginal contribution to the score at the end of the game from current term assuming a particular variant of the solitaire Codenames. In this version, the game is played until all red cards are guessed, and then the score is given as the total number of red cards minus the number of blue cards guessed, minus the total number of red cards if the assassin was guessed, and minus the number of turns.\nIn cases where the game is won, this gives the same score as the solitaire variant included in the official rules of Codenames. The difference between this variant and the official rules is that in the official rules, the codemaster chooses a blue card to remove each turn, while in this variant the blue cards remain unless guessed and the impact of turns taken is only calculated at the end of the game. Any positive score corresponds with a won game, while negatives and zero are lost games."}, {"title": "Estimating Likelihood", "content": "To estimate likelihood, we assume a symmetric Dirichlet distribution with a pseudo-count of one for all guesses as the prior for the likelihood. Note the guesses here are the guesses observed by the codemaster, not the guesses intended by the guesser, as the guesser will not be able to execute their intended guess if they guess a card that is not on their team. The estimate used will be the mean of the posterior distribution.\nTo calculate this, a map is defined from observed guesses to pseudo-counts. When ever a guess is observed during sampling, the pseudo-count is increased by one, with the value in the map being initialized to two if the guess was not already defined as a key value. When updating the codemaster's beliefs, the pseudo-count is used as the likelihood since it does not need to be normalized, with one being used when the key value was not found. We can summarize the implementation with the following formula:\n$P_t(g) = h_t(g, 1, n)$"}, {"title": "Optimization", "content": "Without uncertainty, if there is a clue (l, n) so that (l, n) results in guessing only cards guessed on the team but (l, n + 1) results in guessing a card not on the team, then (l, n) will have a greater or equal expected value to all (l, m) where m > n. This allows search to be optimized by only stopping testing increasing numbers for a clue once n has been found. However, with uncertainty such an n is no longer defined. In particular, empirically finding an n where a card not on the team was guessed does not imply the clue with n+1 will result in a card not on the team being guessed. This is because either the guesser may change, or the clue will change when noise is added. With this in mind we propose the following filter for optimization. Test (l, n) without adding noise and see what each guesser would guess. If all guessers guess a card not on the team, stop increasing n and move on to the next clue vector. This optimization is preferred first because we need to ensure a clue is returned, and it's possible that even if there is individually a clue closest for each card on the players' team for each guesser, that there is none that is closest simultaneously for all guessers, which would result in all clues being skipped if the clue had to return a guess with no cards on the other team for all guessers. Second, if the clue without noise leads to cards not on the team being guessed for some guessers but not others, then increasing the number may increase the number of cards guessed on the team for guessers were only cards on the team were guessed without leading to any change for the other guessers, increasing the expected utility. Finally, while it is possible that adding noise can move a clue away from the card not on the team that would have been guessed in order to increase the number of cards on the team guessed, this outcome is unlikely since adding noise is as likely to perturb the clue in the direction of that card as to perturb it in the opposite direction. We found that in practice including this optimization results in a much shorter running time without impacting results, so we have included it in the experiments we run. Algorithm 15 depicts all of the details."}]}