{"title": "Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses", "authors": ["Hung-Ting Su", "Ya-Ching Hsu", "Xudong Lin", "Xiang-Qian Shi", "Yulei Niu", "Han-Yuan Hsu", "Hung-yi Lee", "Winston H. Hsu"], "abstract": "Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting have shown significant multi-step reasoning capabilities in factual content like mathematics, commonsense, and logic. However, their performance in narrative reasoning, which demands greater abstraction capabilities, remains unexplored. This study utilizes tropes in movie synopses to assess the narrative reasoning abilities of state-of-the-art LLMs and uncovers their low performance. We introduce a trope-wise querying approach to address these challenges and boost the F1 score by 11.8 points. Moreover, while prior studies suggest that CoT enhances multi-step reasoning, this study shows CoT can cause hallucinations in narrative content, reducing GPT-4's performance. We also introduce an Adversarial Injection method to embed trope-related text tokens into movie synopses without explicit tropes, revealing CoT's heightened sensitivity to such injections. Our comprehensive analysis provides insights for future research directions.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023) have shown their significant reasoning skills in few-shot manners with billion-scale parameters. Chain-of-thoughts (CoT) (Wei et al., 2022) extends LLMs' reasoning capabilities by introducing stepwise reasoning. This approach resembles \u201cslow thinking\" in cognitive science (Kahneman, 2011), enabling LLMs to break down complex tasks into a sequence of simpler tasks, facilitating progressive processing and integration of results. It enhances performance across various tasks, including arithmetic calculations (Cobbe et al., 2021) and domain-specific reasoning (Liu et al., 2023c).\nUnlike existing LLM analyses, which primarily focus on factual reasoning and have demonstrated significant advances in LLMs and CoT processes for factual reasoning, this work presents the first investigation of LLMs using tropes in movie synopses. It reveals that LLMs and CoT lack narrative reasoning reasoning skills, as shown in Figure 1. Unlike factual reasoning, which is based on logical deductions and objective data (Zhang et al., 2020), narrative reasoning presents distinct challenges by requiring a deep understanding of event sequences and extensive world knowledge (Jiayang et al., 2024). Since narratives encapsulate human behavior, beliefs, and motivations beyond displayed contexts (Piper et al., 2021), the trope in movie synopses task introduces several novel challenges and perspectives beyond previous work, as shown in Figure 2.\nFirst, trope understanding requires LLMs to understand concepts that are not physically present or directly observable. For example, \u201cHeroic Sacrifice\" involves a character choosing to give up their own life or well-being for the greater good or to protect others. The concept goes beyond the physical act of sacrifice to encompass the thematic implications of altruism, selflessness, and the value of individual life versus collective well-being. This trope is often portrayed as the death of a character, but the death of the character does not entail the trope (it could even be the exact opposite, as shown in Figure 2). Existing benchmarks such as math reasoning (Cobbe et al., 2021) or natural language inference (NLI) (Wang et al., 2019) provide concrete and observable context. For math reasoning, concepts and operations are transparent. While NLI involves abstract thinking to some degree to infer relationships between sentences and understand implied meanings, the scope is confined to linguistic and logical reasoning within specific textual contexts. NLI tasks do not demand the same level of thematic interpretation, symbolic analysis, or connection of ideas across disparate narrative elements that trope understanding does.\nSecond, comprehending tropes requires LLMs to make connections between seemingly unrelated ideas. \"Heroic Sacrifice\" requires viewers or readers to connect the act of sacrifice with broader themes or messages of the narrative, such as freedom, love, or redemption. These themes and messages not only are hard to be observed but also appear to be completely unrelated. For example, a character might subtly express their love for freedom in a casual conversation, which later informs their ultimate sacrifice. However, randomly grasping several \"seemingly unrelated ideas\" without carefully reasoning between the ideas could result in hallucination. For example, \"a character A loves B\" and \"the character A dies in an accident in front of B\" could be unrelated to Heroic Sacrifice despite seeming to have the elements of death and love. Meanwhile, existing benchmarks do not require such a capability. For example, while logic reasoning (Liu et al., 2023a) and commonsense reasoning (Talmor et al., 2021) assess AI's logical reasoning and commonsense knowledge, they do not require the nuanced thematic interpretation and narrative analysis needed to understand tropes like Heroic Sacrifice. \"Heroic Sacrifice\" involves integrating complex narrative themes and character motivations, a level of abstract thinking and interpretation beyond the structured challenges of logical reasoning and the common knowledge queries of commonsense reasoning.\nExploring narrative reasoning capabilities is beneficial for both LLM research and application development. It challenges modern LLMs by requiring abstraction from narrative contexts, a realm beyond previous research. This exploration sheds light on LLM behaviors under specific circumstances and encourages the development of more reliable applications to mitigate hallucinations. In light of this, we first investigate LLMs' reasoning capability by revisiting an existing Trope in Movie Synopses (TiMoS) dataset (Chang et al., 2021) and discover that it remains a very challenging task even for current LLMs with a well-engineered prompting pipeline. As Figure 1 shows, advanced LLMs such as GPT-4, ChatGPT, and fine-tuned LLaMa-2 perform poorly, only achieving F1 scores at the level of random guessing even when they are equipped with CoT (Wei et al., 2022) prompts. This suggests that state-of-the-art LLMs, despite dominating various benchmarks, do not carry reasoning skills for challenges in trope understanding tasks.\nFurthermore, we address the challenge by reframing the TiMoS task as trope-wise querying, where each LLM query inputs a single trope, significantly enhancing LLM performance. As a result, performance improves by 11.8 points on the F1 score, surpassing the supervised state-of-the-art results proposed by TiMoS (Chang et al., 2021). This strategy opens a new pathway for tackling complex reasoning tasks by decomposing multiple concepts into a single concept within an LLM query.\nIn addition to assessing trope reasoning capabilities, we reveal CoT's tendency for hallucination and show that it does not always enhance reasoning compared to vanilla prompting. Furthermore, CoT can increase LLMs' susceptibility to adversarial inputs. Specifically, inspired by prior reading comprehension studies (Jia and Liang, 2017), we devise an Adversarial Injection that inserts trope-related text tokens without explicit trope introduction into a movie synopsis, aimed at gauging whether LLMs are misled. The propensity for hallucination tendency is underscored by: (1) a stark decline in LLM precision when employing CoT, (2) Adversarial Injection significantly misleads LLMs through keyword and pattern recognition, especially when CoT is equipped, and CoT generates accurate responses with erroneous rationales. We also provide a comprehensive analysis, highlight the challenges associated with TiMoS, and offer insights for future LLM research and applications."}, {"title": "Related Work", "content": "Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023) have demonstrated their dominant power in various NLP tasks. Compared to traditional \u201csmaller\u201d language models (Devlin et al., 2018; Radford et al., 2019; Raffel et al., 2020) that require fine-tuning to adapt to downstream tasks, LLMs carry much more parameters (100 billion scale) and do not require task-specific fine-tuning to tackle downstream tasks. Without the requirement of massive human annotation, LLMs have reached supervised state-of-the-art or even human-level performance. When equipped with in-context learning, where LLMs implicitly learn from few examples without updating parameters, and advanced prompting techniques such as CoT (Wei et al., 2022), LLMs even dominate tasks that were considered challenging and require new paradigms to conquer.\nNatural Language Understanding (NLU) (Wang et al., 2018, 2019) is a collection of tasks that examine machines' capability of understanding language in general, flexible, and robust manners. Commonsense reasoning (Talmor et al., 2019; Talmor et al.; Ismayilzada et al., 2023) requires not only understanding the context but also referring to commonsense knowledge. Both NLU and commonsense reasoning are challenging for task-specific supervised models, even with pre-trained knowledge. A recent research (Singh et al., 2023) reported that GPT-4 (OpenAI, 2023) surpassed human performance on SuperGLUE (Wang et al., 2019) NLI benchmark and shortened the gap between human and machine to 5.0 accuracy on CommonsenseQA (Talmor et al., 2019) commonsense reasoning dataset. Besides NLI and commonsense, some recent research also indicates that LLMs can serve as domain-specific experts. (Liu et al., 2023c) demonstrated GPT-4+CoT's reasoning skills that surpass previous supervised state-of-the-art by 10 F1 score in medical domain reasoning (Miura et al., 2021). (Wei et al., 2022) showed that CoT significantly boosted GPT-3's (Brown et al., 2020) performance and outperformed supervised state-of-the-art by 2.0 points on math dataset (Cobbe et al., 2021). (Liu et al., 2023b) examined ChatGPT and GPT-4's skills in logical reasoning and reported that GPT-4 significantly outperformed supervised state-of-the-art by 23.5 of accuracy.\nIn recent years, tropes have received attention from the research community for developing multimedia content creation tools (Smith et al., 2017; Chou et al., 2023) or serve as a testbed of machine reasoning skills (Chang et al., 2021; Su et al., 2021). Previous work (Chang et al., 2021; Su et al., 2021) confirmed that understanding tropes in movies requires deeper cognition skills compared to existing ones and tested supervised models such as BERT (Devlin et al., 2018) or graph neural network-based multi-step reasoning model (Palm et al., 2018). A significant gap was observed between the state-of-the-art supervised model (25 F1) and the human (64 F1) on Tropes in Movie Synopses (TiMoS) (Chang et al., 2021) dataset. This study employs it as a testbed for modern LLMs, which have excelled in multiple challenging NLP tasks across various domains. We investigate the narrative reasoning capabilities of LLMs and CoT, uncovering their tendency for hallucination."}, {"title": "Narrative Reasoning with TiMoS", "content": "TiMoS (Chang et al., 2021) is a multi-label classification task where an LLM inputs a movie synopsis and predicts if specific tropes exist. An LLM is given an article and a set of tropes, and the model needs to sift through the content and select"}, {"title": "LLMs Struggle Reasoning TiMoS", "content": "The first block of Table 1 demonstrates LLMs' trope understanding results (fifth to last row) on TiMoS (Chang et al., 2021) compared to traditional supervised baselines (first to fourth row).\nwhere we tasked the model with predicting all relevant tropes, reasons, and related paragraphs at once, just like how human and supervised models do. Surprisingly, we observed significantly low results with LLMs. Even dominating GPT-4+CoT still obtained an 15.33 F1 score. This highlights the inherent narrative reasoning challenges LLMs encounter in trope understanding, necessitating the decomposition of plot elements and stepwise reasoning to integrate results. In addition, ChatGPT faces greater difficulty compared to its successor, GPT-4, despite being considered extremely powerful and already tackled various factual reasoning benchmarks. In addition, while expressing ideas clearly without adjusting prompts, ChatGPT tended to generate responses outside the choices. This highlights the necessity for meticulous prompt engineering to ensure comprehension for ChatGPT, especially when GPT-4 already exhibits sufficient understanding. Moreover, while the performance of LLaMa-2 fine-tuning remains unsatisfactory, it surpassed that of GPT, indicating potential scarcity in multi-label task corpora for LLMs."}, {"title": "Trope-wise Querying Improves LLMS", "content": "As demonstrated in the last block of Table 1, trope-wise querying, where we query the LLM for a single trope, significantly boosts LLM performance and enables GPT-4 to surpass the state-of-the-art supervised model. This is because it no longer needs to handle various concepts that require focusing on different parts of the plot and different comprehension paths. This finding indicates that LLMs do not carry the reasoning capability of processing multiple different concepts at once."}, {"title": "Challenges of Chain-of-Thoughts (CoT)", "content": "Many works of literature, such as Program CoT (Gao et al., 2022), Symbolic Reasoning (Suzgun et al., 2022), or Math reasoning (Cobbe et al., 2021), suggest that chain-of-thought prompting and its variants significantly improve reasoning tasks that require stepwise comprehension. We conduct an experiment equipping ChatGPT and GPT-4 with CoT, and the results are shown in Table 2. Different from previous work (Yao et al., 2023; Del and Fishel, 2022), we observe that chain-of-thought, while remarkably improving ChatGPT performance, does not work well on GPT-4 here. As chain-of-thoughts mainly boosts recall and slightly degrades precision as a trade-off, and ChatGPT has much lower recall than GPT-4 with base prompting, it might reveal that GPT-4 implicitly knows stepwise reasoning for tropes.\nResults in Table 2 also suggests that there is room for improvement in narrative reasoning, even more than the widely used CoT approach. Additionally, as CoT boosts the recall at the price of precision, it could over-conceive the synopses and aggravate hallucination issues, which should be carefully inspected for truth-worthy applications. Furthermore, as illustrated in Figure 3, we note that the distributions of ChatGPT Base and GPT-4 Base closely align, as do those of ChatGPT CoT and GPT-4 CoT. This indicates that Base and CoT represent two distinct modes of thinking. Regardless of the mode employed, neither effectively addresses the problem. The consistent tendency of both models to predict specific tropes suggests a potential blind spot in GPT models regarding tropes, implying that certain tropes may be consistently overlooked or inadequately emphasized.\nWe propose Adversarial Injection to further test the hallucination behavior of LLMs. This is inspired by earlier research (Jia and Liang, 2017) which highlighted the vulnerability of supervised reading comprehension systems to adversarial attacks, specifically by injecting a random sentence from another article. This investigation revealed that supervised reading comprehension models lack robustness. The experiment centered around a hypothetical scenario in which a specific trope required the presence of multiple elements to be deemed valid. For instance, the trope \u201cBig Bad\u201d necessitates the presence of both an antagonist and a story-driving element to be considered valid. Consequently, injecting a segment of a movie synopsis that only depicts the antagonist element without conveying the story-driving element from another synopsis should not result in the addition of the trope to the latter synopsis. Hence, we conducted an adversarial attack on LLMs by inserting a segment from another plot that includes only a partial element of a trope, as demonstrated in Tables 3 and 9. While this injection introduces several keywords related to the trope, it does not actually add the trope to the modified synopsis.\nRemarkably, the SOTA LLM, GPT-4, whether with or without CoT, succumbed to this attack, much like supervised reading comprehension models, as depicted in Table 3. This observation implies that LLMs heavily rely on keywords and patterns to identify tropes. Notably, while CoT enhances performance across various tasks, it exacerbates the issue of hallucination and makes the models more vulnerable to attacks. This analysis aligns with the findings presented in Sections 3.5 and 3.4.1.\nFurthermore, GPT-4's inclination to overinterpret and extract fragments out of context may contribute to inaccuracies in trope judgments for certain texts. These discoveries underscore the necessity of enhancing GPT-4's capacity to grasp the holistic context. Inaccuracies in trope recognition could result from an incomplete understanding of the relationships between elements in a given text."}, {"title": "CoT Generates Flawed Thoughts", "content": "While the accurate definitions of tropes derived from the CoT results suggest that GPT possesses a solid understanding of the meaning of tropes, there seems to be a limitation in fully grasping the deeper, abstract concepts that underlie certain tropes. Notably, in 13 out of 30 (43.3%) of true-positive cases, we observe instances where LLMs do not exhibit sound reasoning. This highlights the possibility that LLMs may not genuinely comprehend certain concepts and might generate predictions based on hallucination, even when they produce correct answers."}, {"title": "Additional Analyses", "content": "Table 5 showcases the 5 most challenging and easiest tropes for GPTs based on F1 scores. Several tropes characterized by distinct visual or emotional patterns that yield high F1 scores, such as \u201cDriven to Suicide\" or \"EyeScream\", appear to be trivial for GPTs as they carry explicit actions or outcomes. On the other hand, tropes that necessitate a more nuanced grasp of context throughout the plot, such as \"Stealth Pun\" (employing a pun without explicit statement) or \"Jerkass Has a Point\u201d (where an unlikeable character may make a valid argument without explicitly acknowledging it), demand the comprehension of implicit elements or the integration of elements across the plot. This implies that GPT might face challenges with tropes that encompass complex intricacies, requiring a deeper level of contemplation where the genuine essence surpasses shallow semantic patterns. This observation reflects a potential limitation in comprehending the profound layers of meaning within textual content.\nFigure 4 illustrates the performance comparison between GPT-4 and MulCom (Chang et al., 2021), a state-of-the-art supervised model. We can observe that, when compared to the supervised state-of-the-art model, GPT-4 excels in performance for several visually conceivable tropes like \u201cThe Alcoholic\", \"Cool Car\u201d, or \u201cOff with His Head!\u201d These tropes can be identified within a single scene, showcasing GPT-4's ability to grasp concepts from the descriptions. This dual-edged skill has the potential to enhance creativity in content creation but also raises concerns about generating inaccurate information. Conversely, GPT-4 exhibits decreased performance on several tropes that necessitate comprehension across different segments of a plot, such as \u201cBrick Joke\", \"Hypocritical Humor\u201d, or \u201cChekhov's Gun-man.\"\""}, {"title": "Conclusion", "content": "We examined LLMs' reasoning using movie synopsis tropes. Despite previous research highlighting LLMs' strengths in complex tasks, including GPT-4, our study reveals challenges in narrative reasoning, with LLMs achieving only random guessing performance on the TiMoS dataset. We addressed these challenges by utilizing Trope-wise Querying, which significantly improved performance. Additionally, we found that CoT diminishes GPT-4's performance and proposed Adversarial Injection to assess LLMs' hallucination tendencies, discovering that CoT exacerbates this issue. These findings underscore the gap in LLMs' capabilities and raise concerns about their safety and trustworthiness. We are optimistic that our findings will pave new paths for future LLM research and applications."}, {"title": "Limitations", "content": "This work examines and points out directions for LLM research for narrative reasoning, which brings the risk of abusing LLMs. Experimental results of ChatGPT and GPT-4, being closed-source models accessed via API, may not be fully reproducible due to potential updates in the background. This work and the associated dataset might contain offensive or sensitive content as they originate from movie narratives. This paper was written with the assistance of ChatGPT."}, {"title": "Piloting LLMs' Ability", "content": "At the initial stage of this work, we carefully explored various ways to prompt LLMs, fully acknowledging that prompt engineering can significantly impact performance. Therefore, we tested various prompting strategies and conducted a pilot analysis focusing on (1) ensuring and aiding LLMs in understanding query formats, and (2) confirming and assisting LLMs in grasping the concept of tropes:\nWe tested various strategies, including ranking the relevance of each trope for a given plot and outputting the top 10 most likely tropes. However, these attempts encountered issues such as indiscriminately outputting all tropes or only specific ones.\nWe evaluated LLMs' understanding of tropes by asking for definitions (e.g., \"What is the definition of the trope 'Big Bad'?\") and requesting examples. This revealed that LLMs possess a certain level of pre-existing knowledge about tropes.\nBuilding on the pilot analysis, we conducted further experiments incorporating trope explanations into prompts using GPT-3.5-turbo. These explanations includ a general definition of trope along with definitions for individual tropes. However, as shown in Table 7, these additional explanations did not significantly impact performance, suggesting that LLMs possess a base understanding of tropes, as observed in the pilot analysis."}]}