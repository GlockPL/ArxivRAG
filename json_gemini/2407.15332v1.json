{"title": "Robust personalized pricing under uncertainty of purchase probabilities", "authors": ["Shunnosuke Ikeda", "Naoki Nishimura", "Noriyoshi Sukegawa", "Yuichi Takano"], "abstract": "This paper is concerned with personalized pricing models aimed at maximizing the expected revenues or profits for a single item. While it is essential for personalized pricing to predict the purchase probabilities for each consumer, these predicted values are inherently subject to unavoidable errors that can negatively impact the realized revenues and profits. To address this issue, we focus on robust optimization techniques that yield reliable solutions to optimization problems under uncertainty. Specifically, we propose a robust optimization model for personalized pricing that accounts for the uncertainty of predicted purchase probabilities. This model can be formulated as a mixed-integer linear optimization problem, which can be solved exactly using mathematical optimization solvers. We also develop a Lagrangian decomposition algorithm combined with line search to efficiently find high-quality solutions for large-scale optimization problems. Experimental results demonstrate the effectiveness of our robust optimization model and highlight the utility of our Lagrangian decomposition algorithm in terms of both computational efficiency and solution quality.", "sections": [{"title": "Introduction", "content": "In recent years, the rapid advancement of information and communication technologies has fostered a business environment conducive to the collection and retention of vast amounts of data. Within this context, the significance of utilizing consumer and item information has grown substantially, leading to increased interest in personalized pricing strategies (Elmachtoub et al., 2021). If the purchasing intentions of each consumer can be accurately predicted, it is possible to maximize revenues and profits by setting personalized prices based on these predictions.\nIn response to this trend, numerous industries have conducted experiments with personalized pricing, including grocery chains, department stores, and airlines, among others (Elmachtoub et al., 2021; Golrezaei et al., 2014). Furthermore, personalized pricing has already become an established practice in various sectors, such as professional graduate programs, financial and insurance items, services, and information goods (Feldman et al., 2015; Waldfogel, 2015).\nThe framework of personalized pricing consists of two phases: demand forecasting and price optimization. The demand forecasting phase involves constructing a model to predict the probabilities that each consumer will purchase an item at various price points. Subsequently, the price optimization phase entails solving an optimization problem to determine the price that maximizes a given objective function (e.g., total revenue or profit) based on the constructed prediction model for each consumer.\nHowever, the predicted purchase probabilities inherently include prediction errors, and the uncertainty associated with these errors can significantly reduce the effect of price optimization (Duchi et al., 2021; Delage and Ye, 2010). Consequently, improving the robustness of optimal prices against prediction errors remains a critical challenge in the implementation of personalized pricing strategies."}, {"title": "Related work", "content": "Numerous prior studies have been conducted on personalized pricing for a single item (Rossi et al., 1996; Allenby and Rossi, 1998; Den Boer and Zwart, 2014; Kallus and Zhou, 2021; Elreedy et al., 2021). While many of these studies have assumed parametric models for the demand function, it is desirable to use more expressive models without such shape restrictions because the accuracy of demand forecasting directly impacts the effect of price optimization.\nSeveral studies have employed demand forecasting models that do not assume a specific functional form (Elmachtoub et al., 2021; Biggs et al., 2021; Subramanian et al., 2022; Amram et al., 2022; Chen et al., 2023b). Elmachtoub et al. (2021) provided lower and upper bounds on the ratio between the profits of an idealized personalized pricing strategy and a single price strategy. Biggs et al. (2021), Subramanian et al. (2022), and Amram et al. (2022) proposed prescriptive trees to improve the interpretability of optimal prices. Furthermore, Chen et al. (2023b) introduced a stochastic optimization model focusing on the fairness of price setting.\nThiele (2006) studied demand uncertainty in the pricing of a single item. However, this research focused on demand uncertainty associated with time series rather than in the context of personalized pricing. Robust optimization methods have been proposed for handling demand uncertainty for personalized pricing of multiple items (Chen et al., 2023a; Jin et al., 2021). These studies focused on assortment-specific problem structures and had difficulty coping with constraints for a large number of consumers.\nA more general framework for prescriptive analytics based on distributionally robust optimization methods has been proposed by Bertsimas and Van Parys (2022), and Wang et al. (2021a). However, these methods are not readily applicable to price optimization problems where purchase probabilities depend on both consumer covariates and item prices (i.e., decision variable). Consequently, there is a need for a more comprehensive framework to cope with demand uncertainty in the context of personalized pricing for a single item."}, {"title": "Our contribution", "content": "The goal of this paper is to enhance the performance of personalized pricing by employing robust optimization techniques. Specifically, we propose a robust optimization model for personalized pricing that accounts for the uncertainty of predicted purchase probabilities, where the uncertainty set of purchase probabilities is estimated using calculated using the bootstrap method. Our optimization model is formulated as a mixed-integer linear optimization problem, which can be solved exactly using mathematical optimization solvers. Additionally, we develop a Lagrangian decomposition algorithm combined with line search to efficiently find high-quality solutions for large-scale optimization problems."}, {"title": "Personalized pricing model", "content": "In this section, we provide a concise overview of the existing framework for personalized pricing of a single item. The framework consists of two phases: (i) constructing a model to predict each consumer\u2019s purchase probabilities for an item, and (ii) solving a price optimization problem based on purchase probabilities predicted by the constructed model.\nLet \\(p_i\\) denote the price of an item offered to consumer \\(i \\in I\\), and \\(x_i\\) represent the covariate vector of consumer \\(i\\). The function \\(q(x_i, p_i)\\) represents the purchase probability, taking the covariates \\(x_i\\) and the offered price \\(p_i\\) as arguments. We assume that the price offered to one consumer does not affect the purchase probability of other consumers, which reduces the complexity of the optimization problem. Due to the presence of unique covariate vectors \\(x_i\\) for a large number of consumers \\(i \\in I\\), nonlinear and highly accurate prediction models, such as gradient boosting decision trees, are often employed to predict the purchase probabilities.\nWe define the price vector as \\(p := (p_i)_{i \\in I} \\in \\mathbb{R}^{|I|}\\) and denote the feasible region of the price vector \\(p\\) by \\(P \\subset \\mathbb{R}^{|I|}\\). The optimization model for personalized pricing can generally be formulated as follows:\n\\[\\begin{aligned}\n& \\underset{p}{\\text{maximize}} & \\sum_{i \\in I} p_i q(x_i, p_i) \\\\\n& \\text{subject to} & p \\in P,\n\\end{aligned}\\]\nwhere the objective function in Eq. (1) represents the expected total revenue earned from all consumers."}, {"title": "Robust optimization model for personalized pricing", "content": "In this section, we present a robust optimization model designed for personalized pricing that effectively addresses the uncertainty associated with the purchase probabilities. Additionally, we describe the procedure of estimating the uncertainty of the purchase probabilities using the bootstrap method."}, {"title": "Problem formulation", "content": "For the consumer \\(i \\in I\\) offered the price \\(P_{ij}\\), let \\(q_{ij}\\) denote the purchase probability with uncertainty, and \\(\\hat{q}_{ij}\\) represent the predicted value of the purchase probability. Let \\(\\Delta_{ij}\\) also denote the magnitude of the uncertainty in the purchase probability. Additionally, \\(\\gamma_i\\) is a decision variable indicating the extent to which uncertainty is considered for consumer \\(i\\), and \\(\\Gamma\\) represents the maximum number of consumers affected by uncertainty."}, {"title": "Uncertainty estimation of purchase probabilities", "content": "We here describe the procedure for calculating the parameter \\(\\Delta_{ij}\\), which indicates the magnitude of uncertainty in the purchase probability when the price candidate \\(P_{ij}\\) is offered to the consumer \\(i\\), using the bootstrap method (Efron, 1992).\nSpecifically, we construct multiple purchase probability prediction models using training data replicated through the bootstrap method. Then, \\(\\Delta_{ij}\\) is calculated based on the standard deviation of the predicted values obtained from these prediction models. This approach enables the assessment of the magnitude of uncertainty associated with any prediction model, providing a robust foundation for our personalized pricing strategy.\nLet \\(I^{\\text{train}}\\) denote the set of consumers in the training data used to construct the prediction model of purchase probabilities. Let \\(N^{\\text{bs}}\\) represent the number of trials in the bootstrap method, and let \\(q_{ij}^{(nbs)}\\) represent the predicted purchase probability for consumer \\(i\\) if price \\(P_{ij}\\) is offered in each trial \\(nbs \\in \\{1, 2, ..., N^{\\text{bs}}\\}\\). The standard deviation of the predicted purchase probabilities is denoted by \\(\\hat{\\sigma}_{ij}\\).\nFurthermore, \\(\\kappa \\in \\mathbb{R}_+\\) is a parameter that indicates the ratio to the calculated standard deviation \\(\\hat{\\sigma}_{ij}\\), and it defines the magnitude of uncertainty \\(\\Delta_{ij}\\) in the purchase probability. For example, when \\(\\kappa = 1\\), the magnitude of uncertainty \\(\\Delta_{ij}\\) in the purchase probability is equal to the estimated standard deviation.\nThe detailed procedure for calculating the parameter \\(\\Delta_{ij}\\) is presented below."}, {"title": "Calculation procedure for \u0394ij", "content": "Step 1: Extract \\(|I^{\\text{train}}|\\) data instances with replacement from the training data \\(I^{\\text{train}}\\) to generate a bootstrap sample. This process is repeated \\(N^{\\text{bs}}\\) times to produce \\(N^{\\text{bs}}\\) bootstrap samples.\nStep 2. Construct prediction models for each of the \\(N^{\\text{bs}}\\) bootstrap samples generated in Step 1.\nStep 3. Calculate the standard deviation of the predicted purchase probabilities for the target consumer \\(i \\in I^{\\text{test}}\\) and the price candidate index \\(j \\in J\\) as follows:\n\\[q_{ij} = \\frac{1}{N^{\\text{bs}}} \\sum_{nbs=1}^{N^{\\text{bs}}} q_{ij}^{(nbs)}, \\forall i \\in I^{\\text{test}}, \\forall j \\in J,\\]\n\\[\\hat{\\sigma}_{ij} = \\sqrt{\\frac{1}{N^{\\text{bs}}} \\sum_{nbs=1}^{N^{\\text{bs}}} \\left( q_{ij}^{(nbs)} - \\bar{q}_{ij} \\right)^2 }, \\forall i \\in I^{\\text{test}}, \\forall j \\in J.\\]\nStep 4. Calculate \\(\\Delta_{ij}\\) based on the following equation. The minimum value with is selected to ensure that \\(q_{ij} \\ge 0\\).\n\\[\\Delta_{ij} = \\min( \\kappa \\hat{\\sigma}_{ij}, \\bar{q}_{ij} ), \\forall i \\in I^{\\text{test}}, \\forall j \\in J.\\]"}, {"title": "Heuristic algorithm for large-scale problems", "content": "In this section, we design a scalable heuristic algorithm for efficiently finding high-quality solutions to a large-scale problem, combining the Lagrangian relaxation method (Geoffrion, 2009; Guignard, 2003) and the golden section search (Kiefer, 1953).\nBy decomposing the original problem (12)\u2013(16) into small optimization problems for consumers, our algorithm achieves fast and high-quality solutions even as the number of consumers increases. The original problem is structured to be indivisible due to the constraints (5) and the decision variable \\(\\nu\\), which depend on the decision variables \\(z\\)."}, {"title": "Lagrangian relaxation method", "content": "First, to address the constraints (5), we introduce Lagrangian multipliers as \\(\\Lambda := (\\lambda_k)_{k \\in K} \\in \\mathbb{R}^{|K|}\\) and consider a Lagrangian relaxation problem by relaxing the constraints (5) as follows:\n\\[\\begin{aligned}\n& \\underset{\\zeta, \\mu, \\nu}{\\text{maximize}} & \\sum_{i \\in I} \\left(\\sum_{j \\in J} P_{ij} \\hat{q}_{ij} z_{ij} - \\mu_i \\right) - \\Gamma \\nu - \\sum_{k \\in K} \\lambda_k f_k(z) \\\\\n& \\text{subject to} & \\text{Eqs. (4), (6), (14)\u2013(16)}.\n\\end{aligned}\\]\nThe optimal value of the Lagrangian relaxation problem serves as an upper bound for the optimal value of the original problem.\nTo minimize the upper bound, we consider the following Lagrange dual problem.\n\\[\\underset{\\Lambda}{\\text{minimize}} \\underset{\\zeta, \\mu, \\nu}{\\text{maximize}} \\sum_{i \\in I} \\left(\\sum_{j \\in J} P_{ij} \\hat{q}_{ij} z_{ij} - \\mu_i \\right) - \\Gamma \\nu - \\sum_{k \\in K} \\lambda_k f_k(z) \\text{Eqs. (4), (6), (14)\u2013(16)}\\sum_{i \\in I} \\left(\\sum_{j \\in J} P_{ij} \\hat{q}_{ij} z_{ij} - \\mu_i \\right) - \\Gamma \\nu - \\sum_{k \\in K} \\lambda_k f_k(z)\\]\nDue to the structure of the Lagrangian dual problem, which embeds a maximization problem within a minimization problem, it is difficult to directly solve the problem.\nTherefore, we employ the projected subgradient method (Boyd et al., 2003), to update the Lagrange multipliers \\(\\Lambda\\). With the updated multipliers, we then solve the Lagrangian relaxation problem. By alternately repeating this process until the termination conditions are satisfied, it becomes possible to approximately solve the Lagrangian dual problem.\nAt the t-th iteration, let the step size parameter be \\(\\delta^{(t)}\\), the solutions be \\(z^{(t)}\\), and the Lagrange multipliers be \\(\\Lambda^{(t)}\\). By denoting the objective function"}, {"title": "Golden section search", "content": "Second, to address the decision variable \\(\\nu\\), we employ the golden section search. By fixing the decision variable \\(\\nu\\) as a constant, the Lagrangian relaxation problem (Eqs. (19) and (20)) can be decomposed into optimization problems for each consumer i as follows:\n\\[\\begin{aligned}\n& \\underset{z_i, \\mu_i}{\\text{maximize}} & \\sum_{j \\in J} P_{ij} \\hat{q}_{ij} z_{ij} - \\mu_i - \\frac{\\Gamma \\nu}{|I|} - \\sum_{k \\in K} \\lambda_k f_k(z_i) \\\\\n& \\text{subject to} & \\mu_i + \\nu - \\sum_{j \\in J} P_{ij} \\Delta_{ij} \\gamma_{ij} z_{ij} \\ge 0, \\\\\n& & \\mu_i \\ge 0, \\\\\n& & \\sum_{j \\in J} z_{ij} = 1, \\\\\n& & z_{ij} \\in \\{0, 1\\}, \\forall j \\in J,\n\\end{aligned}\\]\nwhere \\(z_i := (z_{ij})_{j \\in J} \\in \\{0, 1\\}^{|J|}\\). This optimization problem can be solved to select prices from the set of candidate prices for each consumer i so as to maximize the objective function while satisfying the constraints. This approach allows to obtain solutions with a computational complexity of \\(O(|I||J|)\\).\nSince these optimization problems can be solved independently for each consumer i, it is possible to accelerate the process through parallel processing.\nGiven that the constant \\(\\nu\\) is a one-dimensional value, we adopt the golden section search (Algorithm 1) to iteratively solve the optimization problem (24)-(28) and thus identify the appropriate value for \\(\\nu\\). For a given \\(\\nu\\), let the optimal value of Eq. (24) denote \\(\\pi_i(\\nu)\\), and we define \\(\\pi^*(\\nu) := \\sum_{i \\in I} \\pi_i(\\nu)\\)."}, {"title": "Algorithm procedure", "content": "By integrating the Lagrangian relaxation method (Section 4.1) with the golden section search (Section 4.2), the original problem (12)-(16) can be decomposed into optimization problems for each consumer i, allowing to obtain fast and of high-quality solutions.\nLet \\(\\nu^{(t)}\\) denote \\(\\nu\\) at iteration t, and Algorithm 2 gives a detailed procedure of our proposed algorithm. Our algorithm first initializes the Lagrangian multipliers \\(\\Lambda^{(t)}\\) and the step size \\(\\delta^{(t)}\\) (lines 2\u20133). It then calculates \\(\\nu^{(t)}\\) using the golden section search (line 5). Subsequently, the algorithm computes the subgradient by solving the optimization problem (24)-(28) given \\(\\Lambda^{(t)}\\), \\(\\nu^{(t)}\\) and update Lagrange multipliers \\(\\Lambda^{(t)}\\) and the step size \\(\\delta^{(t)}\\) (lines 6\u20138). These steps are repeated until the termination conditions are satisfied (lines 4\u20139).\nRecall that our algorithm can be accelerated through parallel implementation, as the computations can be performed independently for each consumer i."}, {"title": "Experimental results", "content": "In this section, we evaluate the effectiveness of our proposed method through simulation experiments using synthetic and real-world datasets. All experiments were performed on a Mac OS 12.6 computer equipped with an Apple M1 chip (8 cores) and 8 GB RAM. To solve our optimization problems exactly, we used Gurobi Optimizer 10.0.2 1, a state-of-the-art commercial solver for mathematical optimization."}, {"title": "Synthetic datasets", "content": "In the numerical experiments with synthetic datasets, we generated six synthetic datasets from various purchase probability models, following the previous work (Biggs et al., 2021). By using the true purchase probability model in these experiments, we can accurately calculate counterfactual outcomes following price changes and assess expected revenues."}, {"title": "Purchase Probability Model", "content": "Let the generated price and covariates denote P and \\(X := (X_1, X_2, ..., X_n)\\), respectively. Furthermore, let g(X) and h(X) represent any transformation functions applied to the covariates X and denote the error as \\(\\epsilon\\). The purchase probability q(X, P) for generating synthetic datasets was thus defined as follows:\n\\[q(X, P)^* = g(X) + h(X) P + \\epsilon, \\quad q(X, P) = \\begin{cases} 1 & \\text{if } q(X, P)^* > 0, \\\\ 0 & \\text{if } q(X, P)^* \\leq 0. \\end{cases}\\]"}, {"title": "Evaluation procedure", "content": "We evaluated the revenues in synthetic datasets according to the following process. The set of candidate prices was defined as nine different values ranging from the 10th to the 90th percentile of the prices observed in the training data.\n1. Generate \\(|I^{\\text{train}}|\\) training data samples based on the purchase probability model and train the purchase prediction model;"}, {"title": "Real-world datasets", "content": "In numerical experiments with real-world dataset, as in the previous studies (Biggs et al., 2021), we use purchase data from grocery stores to evaluate our proposed method."}, {"title": "Grocery pricing", "content": "We used publicly available data from Dunnhumby titled \u201cThe complete journey\u201d2, which includes two years of transactional grocery purchase data from 2,500 households frequently shopping at grocery stores, along with their demographic information. In this dataset, available household information includes consumers\u2019 discretized age, discretized household income, whether they are a homeowner or renter, and household composition. Household composition categories are single male, single female, 2 adults no kids, 2 adults with kids, 1 adult with kids, and unknown.\nConsistent with the prior work (Biggs et al., 2021), we followed established procedures for preprocessing data and building prediction models of purchase probabilities. Each record represents a purchase opportunity, detailing household information and the unit price of strawberries ranging from $1.99 to $5.00, mostly in $0.50 increments, along with the outcome of whether strawberries were purchased. If strawberries were not purchased, the price information is missing and we imputed it using the average of the prices of the last three purchases.\nThe dataset was divided into 50% training data and 50% testing data. The training data was used to build a prediction model of purchase probabilities, and purchase probabilities were predicted for each price in the testing data to determine optimal pricing. Since the true purchase probability model"}, {"title": "Evaluation of robust optimization", "content": "First, we present the experimental results and discussion on both synthetic and real-world datasets to verify the effectiveness of the proposed robust optimization method (12)\u2013(16). To investigate the effect of varying the number of consumers affected by the uncertainty, we introduced a parameter \\(\\alpha \\in [0, 1]\\) representing the ratio of consumers affected to all consumers, and set \\(\\Gamma = \\alpha |I^{\\text{test}}| \\). When \\(\\alpha = 0\\), the scenario assumes no effect of uncertainty, while \\(\\alpha = 1\\) corresponds to a situation where all consumers are affected by uncertainty. Additionally, the number of bootstrap iterations in our method was set to \\(N^{\\text{bs}} = 20\\)."}, {"title": "Comparative method", "content": "Biggs et al. (2021) employed LightGBM as a teacher model and introduced the Greedy Student Prescriptive Tree (SPT), which demonstrated superior performance compared to other competing prescriptive tree methods, such as those proposed by Kallus (2017) and Athey and Imbens (2016). Building upon this work, Amram et al. (2022) proposed Optimal Policy Trees (OPT), enhancing performance by optimizing the branches of the SPT. Subramanian et al. (2022) also extended this approach to a multiway tree that incorporated constraints, achieving performance surpassing that of OPT.\nWhile the performance of these prescriptive tree methods has improved over time, previous approaches have not accounted for uncertainty in predicted values. Moreover, the most recent method (Subramanian et al., 2022) has demonstrated performance nearly equivalent to a comparative method called LGBM, which determines prices for consumers based on purchase probabilities predicted by LightGBM.\nOur proposed method can be positioned as an extension of LGBM, which has served as a comparative method in previous studies. Notably, when \\(\\alpha = 0\\), our method is identical to LGBM. Therefore, in this numerical experiment, we adopted LGBM (equivalent to our proposed method when \\(\\alpha = 0\\) as the comparative method, consistent with previous studies."}, {"title": "Result for synthetic datasets", "content": "We analyze the results of purchase probability predictions and the expected revenue resulting from price optimization. These results were based on \\(|I^{\\text{test}}| = 500\\), and the computation time for price optimization was limited to 600 seconds. For the purchase probability prediction model, default hyperparameter values were used. The training process used 20% of the training data as a validation data and was terminated when no improvement in the area under the curve (AUC) was observed on the validation set for 10 consecutive iterations.\nFirst, we discuss the results of the purchase probability predictions. Table 2 shows the AUC of the purchase probability prediction model for each synthetic dataset. From this table, we observe that as the number of consumers in the training data, \\(|I^{\\text{train}}|\\), increased, the AUC for the testing data also improved. However, in the case of Dataset2, the AUC value remained below 0.6, indicating a difficult prediction environment."}, {"title": "Result for real-world dataset", "content": "Fig. 3 shows the average revenues for each method on \u201cThe complete journey.\u201d This figure demonstrates that the revenues can be improved by accounting for the uncertainty of the predicted purchase probabilities. The accuracy of the prediction model for the purchase probabilities was AUC = 0.836 for the testing data. Regarding the computation time of the optimization problem, the exact solution could be obtained in a few seconds for all values of \\(\\alpha\\) for \\(\\kappa = 1\\) and \\(\\kappa = 2\\)."}, {"title": "Evaluation of heuristic algorithm", "content": "Lastly, we evaluate the computational performance of our solution methods: MILO formulation (12)-(16) and our decomposition algorithm (Algorithm 2) on synthetic datasets (Dataset1\u20136), since the synthetic datasets can be easily evaluated by varying to the target number of consumers \\(|I^{\\text{test}}|\\)."}, {"title": "Experimental setup", "content": "Our heuristic algorithm allows for splitting the original problem into sub-problems for any constraint \\(f_k(z) \\leq 0\\) (\\(k \\in K\\)) on the price assignment variables z. In this experiment, we considered the following upper bound constraint on the number of price changes (Wang et al., 2021b; Bitran and Caldentey, 2003), which has been widely used in the context of price optimization:\n\\[\\sum_{i \\in I^{\\text{test}}} \\sum_{j \\in J} w_{ij} z_{ij} < \\beta |I^{\\text{test}}|\\]\nwhere \\(w_{ij} \\in \\{0, 1\\}^{|I^{\\text{test}} \\times J|}\\) is a constant that is set to 1 if price \\(P_{ij}\\) is available to consumer i and 0 otherwise. Additionally, let \\(\\beta \\in [0, 1]\\) be a parameter that specifies the number of price changes. We assumed that the number of consumers who could set a price above the average of the price candidates was limited to less than 10% of the total number of subjects. Specifically, we set \\(w_{ij} = \\{0, 0, 0, 0, 0, 1, 1, 1, 1\\}_{ (i, j) \\in I^{\\text{test}} \\times J }\\) and \\(\\beta = 0.1\\).\nThe termination condition of our algorithm was as follows:\n\\[\\frac{||\\nabla_{AP}(\\lambda)||}{t} < \\epsilon^p, \\quad \\nabla_{AP}(\\lambda) \\ge 0,\\]\nwhere \\(\\epsilon^p\\) represents the tolerance error, and we set \\(\\epsilon^p = 0.01\\). The second condition was added to ensure feasibility when comparing the objective function values in feasible solutions. The step size for updating the Lagrange multipliers was set as follows:\n\\[\\delta^{(t)} = \\frac{1}{||\\nabla_{AP}(\\lambda)|| \\cdot \\sqrt{t}}.\\]\nThe parameter for the termination condition of the golden section search used in the proposed algorithm was set to \\(\\epsilon^g = 0.01\\). The initial value of the upper bound was set to \\(\\max\\{P_{ij} \\Delta_{ij} | i \\in I^{\\text{test}}, j \\in J\\}\\), and the initial value of the lower bound was set to 0.\nThe other parameters for the experiments were set as shown in Table 3, and the time limit of the mathematical optimization solver was set to 300 seconds."}, {"title": "Computational results", "content": "Table 4 presents the objective function values for the exact and heuristic algorithms. These objective function values were calculated based on the obtained solution z using the objective function of the original problem (Eq. (12)). As shown in Table 4, when the number of target consumers \\(|I^{\\text{test}}|\\) was small (i.e., \\(|I^{\\text{test}}| = 100\\) and \\(|I^{\\text{test}}| = 1000\\)), the exact algorithm yielded slightly better objective function values than the heuristic algorithm. However, the differences were minor, and the heuristic algorithm also achieved objective function values close to those of the exact algorithm. Furthermore, when \\(|I^{\\text{test}}| = 10000\\), the heuristic algorithm showed better objective function values in Dataset1, Dataset3, Dataset4, and Dataset6. This indicates that the revenue performance of the exact and heuristic algorithms is competitive when the number of consumers is large.\nTable 5 shows the computation time results for the exact and heuristic algorithms. In this table, the computation time is noted as \u201c> 300\u201d if the optimal solution was not obtained within the time limit of 300 seconds in at least one of the ten trials. The results indicate that the exact algorithm struggled to consistently find the optimal solution within the time limit when \\(|I^{\\text{test}}| > 1000\\). In contrast, the heuristic algorithm completed within 300 seconds for all synthetic datasets, even when \\(|I^{\\text{test}}| = 10000\\). Recall that our algorithm can be processed in parallel, allowing for further computational speedup. Therefore, our algorithm has proven to be fast and capable of obtaining high-quality solutions to large-scale problems."}, {"title": "Conclusion", "content": "We proposed a robust optimization model for personalized pricing that accounts for the uncertainty of predicted purchase probabilities for a single item. Specifically, we calculated the uncertainty of predicted purchase probabilities using the bootstrap method and formulated the optimization problem as a mixed-integer linear optimization problem, which can be solved exactly using mathematical optimization solvers. Moreover, we developed a scalable heuristic algorithm to efficiently obtain high-quality solutions for large-scale problems.\nWe conducted numerical experiments using both synthetic and real-world datasets and confirmed the improvement in expected revenues by considering the uncertainty of the predicted purchase probabilities. Furthermore, we demonstrated our heuristic algorithm can quickly obtain high-quality solutions for large-scale problems.\nFuture directions for this study include extending the pricing model to multiple items by considering the estimation of cross-elasticity among items (Ito and Fujimaki, 2017; Ikeda et al., 2023a) and applying appropriate price ranges (Ikeda et al., 2023b, 2024) for each consumer. A further possible extension is to develop a method for dealing with uncertainty, similar to portfolio optimization approaches (Yanagi et al., 2024; Uehara et al., 2024)."}]}