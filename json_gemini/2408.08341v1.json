{"title": "Exploring Latent Space for Generating Peptide Analogs Using Protein Language Models", "authors": ["Po-Yu Liang", "Xueting Huang", "Tibo Duran", "Andrew J. Wiemer", "Jun Bai"], "abstract": "Generating peptides with desired properties is crucial for drug discovery and biotechnology. Traditional sequence-based and structure-based methods often require extensive datasets, which limits their effectiveness. In this study, we proposed a novel method that utilized autoencoder shaped models to explore the protein embedding space, and generate novel peptide analogs by leveraging protein language models. The proposed method requires only a single sequence of interest, avoiding the need for large datasets. Our results show significant improvements over baseline models in similarity indicators of peptide structures, descriptors and bioactivities. The proposed method validated through Molecular Dynamics simulations on TIGIT inhibitors, demonstrates that our method produces peptide analogs with similar yet distinct properties, highlighting its potential to enhance peptide screening processes.", "sections": [{"title": "I. INTRODUCTION", "content": "Biologists often seek peptides for various purposes, such as aiding in the comprehension of biological mechanisms and addressing societal challenges in healthcare. Immunotherapy, for example, relies on immune checkpoint inhibitors to block checkpoint proteins from binding with target proteins, thereby enhancing immune cell activity against cancer cells. The process of peptide discovery, traditionally laborious, has seen acceleration in recent years due to the fast development of computational methods [1], [2]. These computational methods, including virtual screening, molecular docking, machine learning/deep learning, have revolutionized peptide discovery by various functionalities such as predicting peptide-protein interactions, optimizing peptide sequence, and identifying novel peptide candidates [3]\u2013[5]. Among these methods, deep learning stands out as the most advanced computational method [6]-[8] which focus on reduced part of the vast compound space related to peptides, allowing for the prediction of peptides with a certain degree of accuracy. By leveraging large datasets and sophisticated neural network architectures, the advanced models can uncover bioactive peptides for various applications. Deep learning algorithms, such as various data structured discrimination models [9], [10] and generative models [11], [12], have shown remarkable success identifying an optimizing peptides with desired properties. This success has paved the way for the research focused on the generation of peptides sequence with specific properties.\nGenerating peptides sequence with specific properties recently become a key focus in computer-aided peptide and drug discovery [13]. Researchers are concentrating on two main approaches: sequence-based method and structure-based method. The sequence-based methods [14], [15] learn patterns from existing peptides to predict new ones. These methods often heavily rely on known dataset with desired properties or bioactivities. One significant challenge of sequence-based methods is determine the desired properties for new peptide. In other words, the limited availability of known peptide sequences with these desired properties can reduce the effectiveness of these methods. The structure-based models [16], [17] generates peptides with known three-dimensional structures. These methods require known structures of peptides with target receptors, which can be difficult to obtain, especially for peptides with desired properties or bioactivities. Additionally, structure-based methods face limitations when dealing with peptides that have disordered or unstable structures.\nOur study addresses the challenges of generating peptides analogs without relying on large datasets and structures. We proposed a novel method to generates peptide analogs by exploring the embedding space, requiring only a single peptide sequence. Our proposed method could streamline the peptide discovery process, making it more efficient and less resource-intensive. Leveraging pre-trained protein language models, our method eliminates the need for extensive datasets of similar sequences. We validated the effectiveness of our method"}, {"title": "II. RELATED RESEARCH", "content": "A. Lab Experiment Based Method\nA variety of approaches can be used to identify and optimize peptide-based inhibitors. Rational design is a classic approach to develop peptide analogs based on identification of key residues contributing to the natural ligand:target interaction. While it allows precise modifications, a limitation is that it requires sequence and/or structure information of both the target protein and its ligand [18]. Obtaining this information requires significant time and effort, including extensive mutagenesis scanning and/or generation of a crystal structure. Even with structural data available, only a limited number of peptide analogs can be generated by rational design, potentially missing optimal sequences. Phage display is a powerful tool that enables high-throughput screening of peptides by virtue of a DNA-encoded phage-displayed library and rapid phage amplification [19]. However, binding affinity and function of the selected peptides are not guaranteed, requiring further lead optimization to improve the target selectivity, potency and efficacy. Directed evolution is an advanced method for evolving peptide sequences by generating a library using random mutations and conducting iterative cycles of mutations and selections [20]. Although it explores a larger sequence space [21], it requires resources for screening, and its success rate is dependent upon the initial library. Moreover, it can be time-consuming due to the iterative mutations and screenings.\nB. Deep Learning Based Method\nIn the past decade, numbers of researches studied generating peptides with desired properties using deep learning methods. Some studies focus on the amino acid sequences of proteins. Greener et al. [15] utilized a conditional variational autoencoder to generate metalloproteins based on a known dataset of amino acid sequences. Gupta et al. [14] applied generative adversarial networks combined with a pre-trained sequence function prediction model to generate DNA sequences of proteins with desired properties. Biswas et al. [22] used evolutionarily related homolog sequences, retrieved using a hidden markov model, to fine-tune an embedding model, which was then employed to predict functional characteristics. They subsequently combined this functional prediction model with the Markov chain Monte Carlo (MCMC) method to generate similar proteins. Goverde et al. [16] attempted to search for sequences with desired three-dimensional structures by inverting the AlphaFold2 structure prediction network [23] through the MCMC method. They further extended this research to validate the effectiveness of their method, with some modifications, on membrane proteins [17]."}, {"title": "III. METHOD", "content": "In this research, we proposed a new method to generate peptide sequences by exploring the protein latent space.\nHypothesis Our study posits that peptides with similar embedding are likely to share higher property similarities, even if their sequence expressions differ. This hypothesis is inspired by the field of word embedding studies, where vector abstract feature representations learned from deep learning model capture semantic meaning [24]. In our research, we proposed that the vector abstract feature representations derived from data distributions encapsulate the bioactivety implications of peptide sequences. By utilizing computational techniques similar to those used in word embedding methodologies, we aim to elucidate the structural and functional relationships between peptides, facilitating the generation of peptide analog sequences without the need for extensive datasets by exploring the protein latent space.\nDefinition Our proposed method employs an autoencoder shaped model to learn the feature embedding. We define our dataset as \\(X = \\{X_0, ..., X_i, ..., X_n\\}\\), where \\(x_i\\) is the amino acid sequence of a protein. We define our method as \\(\\hat{y}_i = g(f(x_i) + \\delta_i)\\), where \\(\\hat{y}_i\\) is the generated amino acid sequence of protein analog at step \\(\\tau\\), function \\(f(\u00b7)\\) is a model projecting a protein sequence into the latent space, \\(\\delta_i\\) represents the noise added to the protein embedding at step \\(\\tau\\), and \\(g(\u00b7)\\) project the noised embedding back to the sequence.\nA. Overview of the Proposed Method\nThe proposal method overall structure is showing in the Figure 1. First, the embedding step projects peptide sequences from a discrete space into a continuous latent space. This transformation is crucial as it allows us to manipulate the sequences in a more flexible and informative way. Second, we explore the latent space by introducing noise into the embeddings. This step is performed in a systematic manner, starting with lower levels of noise and gradually increasing to higher levels. By doing so, we can generate sequences that are progressively different from the original, enabling us to discover a wide range of similar peptide analogs. Finally, in the decoding step, the noised embeddings are processed by a decoder, which converts the embeddings back into peptide sequences. This step is essential to ensure that the manipulated embeddings are interpretable and usable.\nTo validate our method, we utilized two embedding models: ProtT5 [25] and ESM [26]. Both embedding models, ProtT5 and ESM-2, are transformer-based and incorporate layer normalization modules that standardize hidden states layer-by-layer to enhance model robustness to noise. ProtT5 is an encoder-decoder based embedding model, which allows us to directly use its decoder module for the final step. On the other hand, ESM is a mask-based embedding model, requiring us to train a new decoder model to transform the embeddings back into sequences. This dual-model approach enables us to demonstrate the robustness and versatility of our method across different embedding architectures.\nB. Embedding\nIn our method, we utilized two state-of-the-art models, as peptide sequence projection function \\(f(\u00b7)\\), to embed peptide sequences: ProtT5 and ESM-2. For both models, we use pre-trained versions to leverage their advanced capabilities in understanding protein sequences. We employed token-level embedding instead of sequence-level embedding for both models, where each token is obtained using one-hot encoding of amino acid. This approach allows the decoder module to reverse the embedding back into sequences effectively, ensuring a seamless transition from embedding to decoding.\n1) ProtT5 Embedding: We employed the \"Prot-T5-XL-Ur50\" as ProtT5 model, which is trained on the UniRef50 dataset [27]. This model provides an embedding size of 1024 to capture features of peptide sequences. In the ProtT5 model, the first embedding corresponds to the initial \\<pad> token, and the last embedding corresponds to the end token \\<s>. Therefore, we removed these two embedding to focus on the meaningful representations of the peptide sequences.\n2) ESM-2 Embedding: We utilized the ESM-2 models's version with 150 million parameters to match the ProtT5 [28], making it a fair comparison. In the ESM-2 model, we used the output from the last layer of the sequence module as the peptide embeddings, which have an output size of 640. This layer captures the features of the peptide sequences, providing a foundation for the subsequent steps in our method. By incorporating these two different types of embedding models encoder-decoder based (ProtT5) and mask-based (ESM-2) we aim to evaluate the performance and versatility of our method across various embedding architectures. This dual-model approach ensures a comprehensive understanding of how our method functions with different embedding strategies.\nC. Noise\nIn this step, we introduce noise \\(\\delta\\) into the peptide embeddings to explore the latent space.This noise \\(\\delta\\) is drawn from a uniform distribution \\(Z ~ U(a,b)\\), where \\(Z\\) range in [-1,1]. To ensure the noise is not neutralized by normalization modules, we add a random shift \\(\\beta\\), sampled from a uniform distribution within the range [-1,1], to the noise. This adjustment maintains the effectiveness of the noise and facilitates latent space exploration. The noise can be represent by: \\(\\delta = U(-1,1)^{m \\times n} + \\beta\\), where \\(\\beta = U(-1,1)\\), \\(m \\times n\\) represent the size of embedding matrix \\(f(x_i)\\).\nWith lower noise levels, there is a higher likelihood of encountering sequences identical to the original. Conversely, excessively higher noise levels may produce sequences that diverge significantly from the original. To address this, we employ a strategy that gradually increases the noise level if no new sequences are discovered after a specified number of trials \\(T_{thresh}\\). This adaptive approach strikes a balance between finding sequences that are similar to and those that are diverse from the original. The final noise applied to the embedding is given by: \\(\\delta_{\\tau} = (\\alpha + \\lfloor \\tau / T_{thresh} \\rfloor) * \\delta\\) where \\(\\alpha\\) (\\(\\alpha = 0.5\\) for initial noise) represents the noise level, and \\(\\tau\\) represent the current number of trials. For each trial, new noise and a random shift are generated. The trial thresholds \\(T_{thresh}\\) vary for each model according to their computational time, ensuring trials are completed efficiently: \\(T_{thresh} = 50\\) for ProtT5 and \\(T_{thresh} = 2000\\) for ESM-2. This differentiation accounts for the different sizes of the models' parameters, with ProtT5 having approximately 2.8 billion parameters and ESM-2 having around 150 million parameters, thus requiring different amounts of time to complete one trial. Additionally, we found that ProtT5 usually identifies new peptide analogs in fewer trials compared to ESM-2 at the same level of noise.\nD. Decoder\nThe final step in our method involves transforming the noised embedding back into peptide sequences through function \\(g(.)\\). We employed different functions \\(g(\u00b7)\\) for ProtT5 and ESM-2 based on distinct architectures.\n1) ProtT5 Decoder: We utilize a pre-trained decoder to project the transformed embeddings. The decoding process begins with the same initial token, \\<pad>, which initiates the transformation of the embedding back into a sequence. This allows for a straightforward and efficient decoding process, leveraging the capabilities of the pre-trained ProtT5 model.\n2) ESM-2 Decoder: The ESM-2 does not have a decoder module, therefore, we trained our own decoder module. The ESM-2 decoder is designed to be symmetric with it encoder's architecture but with fewer layers and followed by a linear function \\(\\hat{y} = h(z_{t-1}W)\\) that maps the hidden state to the sequence token, where \\(z_{t-1}\\) represent the hidden state from last layer of decoder module and \\(W\\) is learnable weight matrix. Thefunction \\(h(.)\\) ensures that the decoder can effectively transform the embedding back into sequences. The ESM-2 decoder module is fine-tuned on the UniProtKB [29] dataset using cross entropy loss. The parameters of the ESM-2 encoder remain frozen to ensure the embeddings are consistent with the original model. The frozen strategy maintains the integrity of the embeddings while allowing the decoder to learn the mapping from embeddings to sequences effectively."}, {"title": "IV. DATA & EXPERIMENT SETUP", "content": "A. Data Source and Filtering\nIn this study, we employed an open-source protein-ligand dataset: BioLip [30] to test our method. BioLip contains a large volume of data, encompassing various ligand types. The recent update includes the sequences of peptides. The entire BioLip database contains 781,684 protein-ligand interactions, out of which 35,167 are protein-peptide interactions. After removing duplicate sequences, we are left with 9,027 unique peptide sequences. Further filtering out sequences containing non-standard amino acids reduces this number to 7,347 sequences. We then filter out sequences longer than 20 amino acids and shorter than 5 amino acids, resulting in a final dataset of 4,758 unique peptide sequences.\nTo train the decoder module for ESM-2, we used the UniProtKB/Swiss-Prot dataset which comprises 571,282 sequences [29]. UniProtKB/Swiss-Prot (Universal Protein Resource Knowledgebase) is a comprehensive database that offers detailed information on protein sequences and functions. It is curated manually, focusing on proteins that are generally more well-researched. For sequences longer than 256, we truncated them from the head to ensure compatibility with the model. The number of sequences that are shorter than or equal to 256 is 245,539.\nB. Baseline Models\nIn this study, we compare our proposed method with two baseline approaches. Random generated sequence and BLOSUM generated sequence. This comparison aims to highlight the efficacy and improvements offered by our proposed approach over traditional random sequence generation and selection based on global alignment metrics.\n1) Random Generated Sequence: The first baseline method, random generated sequence, generates completely random sequences of the same length as the original sequences. This approach is commonly used in high-throughput lab experiments II-A to test if some sequences display the desired properties. By using this method, we can evaluate whether our model's results are meaningful or if they could be attributed to random chance.\n2) BLOSUM Generated Sequence: The second baseline involves a two-step process: initially generating 10,000 random sequences, followed by selecting the sequences with the highest global alignment scores using Needleman-Wunsch algorithm [31] with BLOSUM62 matrix [32].\nC. Evaluation Metrics\nTo evaluate the similarity between original and generated peptide sequences, we use three different indicators: Morgan Fingerprints [33], RDKit Descriptors [34], and QSAR descriptors [35] based on peptide amino acid sequences. By employing these descriptors and their respective similarity measures, we ensure a comprehensive evaluation from structural, physico-chemical, and sequence-based perspectives.\n1) Morgan Fingerprint: We use the Morgan Fingerprint to represent the 2D structure of the molecules. To calculate the Morgan Fingerprint, we first transform the amino acid sequences into SMILES format using the molconvert tool from ChemAxon [36]. The Morgan Fingerprint captures the 2D structural features of the molecules by hashing these features into a fixed-size binary vector. For our evaluations, we choose a fingerprint size of 2048 bits. The similarity between two fingerprints is calculated using Tanimoto similarity, which measures the proportion of shared elements between two sets compared to their combined total.\n2) RDKit Descriptors: RDKit [34] provides a variety of physico-chemical descriptors for molecules. To utilize RDKit for calculating these descriptors, we transform the amino acid sequences into SMILES format using the molconvert tool [36]. We use all the descriptors provided by RDKit to represent the peptides from a molecular aspect. The similarity between RDKit descriptors is calculated using cosine similarity. Before calculating the similarity, the descriptors are normalized to ensure accurate comparison.\n3) QSAR Descriptors: QSAR (Quantitative Structure-Activity Relationship) descriptors for peptides encompass comprehensive properties and indices. These descriptors are calculated using the peptide.py package [35] developed by European Molecular Biology Laboratory. Similar to RDKit descriptors, the QSAR descriptors are scaled and normalized before calculating similarity using cosine similarity.\nD. Comparative Analysis\nTo validate our proposed method, we applied it to peptide ligands of the TIGIT receptor, which have been identified through wet-lab experiments. We selected two peptide sequences: one with a strong affinity to the receptor as positive example and one with a weak affinity as negative example. For each peptide, we generated three similar sequences using our method. Please refer to the Supplementary Material for the exact sequences.\nFor a more accurate evaluation and further validation of the machine learning models, we employed MD Simulation. The process began with predicting the initial structure of each peptide sequence using AlphaFold2 [23]. The predicted initial structures were then placed with TIP3P water molecules into an MD system with a box size of 4 nm cubic box that mimics the experimental environment. The system was simulated for 1 ms with three repeats. We calculated the Root Mean Square Deviation (RMSD) between the last frames of the three repeats and then selected the repeat with the lowest RMSD compared to the other two. The structure of the TIGIT monomer was obtained from the Protein Data Bank (PDB ID: 3Q0H [37]). We created a system containing the TIGIT receptor and the peptide which was extracted from the peptide simulation for docking simulation, positioning the peptide around the pocket by a center of mass (COM) distance of 3 nm. The docking simulation was run for 500 ns with three repeats. The results were analyzed using three metrics: peptide RMSD, which shows the average positional deviation"}, {"title": "V. RESULT AND DISCUSSION", "content": "A. Overall Result\nWe evaluated the performance of the proposed method against the two baseline models described in Section IV-B in terms of average similarities for generating three, five, and 10 new sequences and the peptides with different length (shorter than 10, between 10 to 15, and longer than 15). As shown in Table I, the proposed method outperforms all baseline models in terms of Morgan fingerprint, RDKit descriptor and sequence QSAR similarities. Specifically, ProtT5 exhibits higher average similarity for RDKit descriptor similarity indicating that ProtT5 may generate analogs with more similar physico-chemical properties, while ESM-2 shows higher average similarity for Morgan fingerprint and sequence QSAR similarities indicating that ESM-2 could generate analogs with more similar chemical structure and potentially similar bioactivities based on amino acid sequences. The BLOSUM and random generated sequence showed sub-optimal performance across all three similarity measures. Compare to random generate sequence, the BLOSUM generated sequence showed slightly better performance, however, it is still markedly underperformed compared to our proposed method. The underperformance of BLOSUM may be due to limitations in the BLOSUM matrix, which is based on a limited dataset and does not account for the context of amino acids, focusing only on position-specific substitutions [39]. Additionally, instances where miscalculated BLOSUM matrices outperformed correctly calculated ones suggest a gap between theoretical assumptions and practical performance [40].\nAs shown in Figure 2, 3, for Morgan fingerprint and sequence QSAR similarities, sequences generated by the ESM-2 model have the highest similarities to the original sequences, followed by those generated by ProtT5. There is a large gap between these and the similarities achieved by the BLOSUM method, with random sequences showing the lowest similarities. Specifically, the ESM-generated sequences consistently demonstrate higher similarity metrics, indicating their effectiveness in maintaining key sequence properties.\nIn contrast, in Figure 4, for RDKit descriptor similarities, ProtT5-generated sequences outperform those generated by ESM-2. Both ProtT5 and ESM-2 models outperform the BLOSUM and random methods, suggesting that our approach yields peptides with more desirable molecular properties.\nTo further understand the effectiveness of our method, we compared the distribution of similarities with alignment score differences using the BLOSUM matrix. The density plot Figure 5 shows that our method can identify sequences with high property similarities while having significant differences in amino acid sequences compared to the baseline methods. This indicates that our approach can explore a diverse sequence space while maintaining crucial properties. When comparing ESM and ProtT5, ProtT5 shows more potential to search a larger sequence space with high similarity. On the other hand, ESM generally generates sequences with higher property similarities compared to ProtT5 but searches within a relatively smaller sequence space. Interestingly, in the RDKit descriptor similarities density plot, ESM occasionally generates sequences with lower similarity than the baseline methods. However, this phenomenon is rare and not significant enough to affect the overall performance.\nB. Physics Modeling Validation\nTo further validate our proposed method, we evaluated the performance using physics model MD simulation with wet-lab experiments generated sequences. The generated sequence is obtained from ProtT5 due to it's ability to search for a broader range of sequences while maintaining high property similarity, making it a suitable choice for further exploration and validation in experimental settings. For our MD simulation analysis, we focus on the close interaction between TIGIT and peptide ligands, comparing positive and negative (the explanation of those pairs is detailed in section IV-D) examples. Among the three generated sequences, we selected the one with interactions most similar to the original peptide for further analysis. As shown in Figure 6, the minimum distance between pocket residues and the peptide exhibits similar behavior for both the positive and negative examples. We consider there to be an interaction between the peptide and a pocket residue if the minimum distance between them is less than 0.4 nm [41]. In the positive example, all pocket residues that interact with the example peptide also interact with the generated peptide. In the negative example, of the five residues interacting with the example peptide, only Cys-69 differs between the example and generated sequences. This difference results from the strict 0.4 nm cut-off, with the minimum distance difference being just 0.08 nm. Detailed residue-to-residue distances are included in Supplementary Material Figures S3 and S4.\nBased on the vdW & COM distance plots (Figure 8 and Figure 9), both positive and negative generated sequences show slightly higher vdW energy compared to the original sequences (15.8 kJ/mol for positive and 17.9 kJ/mol for negative). Additionally, for the negative example, the generated sequence exhibits a much lower COM distance (0.5 nm), indicating closer interaction with the pocket compared to the original negative peptide. This suggests that the generated sequence might bind more tightly to the pocket, potentially leading to improved function and higher efficacy.\nIn Figure 7, stability analysis through RMSD shows that the generated peptide in the positive example is more stable than the example peptide, with an average RMSD of 0.08 nm compared to 0.35 nm. Additionally, the generated peptide exhibits less fluctuation. Comparing this plot with Figure 8, we observe that the peptide quickly begins strongly interacting with the receptor and maintains its structure throughout the simulation. In the negative example, the generated peptide is less stable than the example peptide, with an average RMSD of 0.53 nm compared to 0.22 nm. Comparing this plot with Figure 9, around 400 ns, the peptide's structure changes, causing the RMSD to increase as it slightly shifts from the interacting residues. It then engages with another set of pocket residues, leading to a subsequent drop in RMSD.\nTo evaluate the overall affinity, we utilized umbrella sampling. The results pf free energy, as shown in Figure S1 and Figure S2 in the Supplementary Material, reveal that in the positive example, the generated peptide demonstrates comparable affinity to the original peptide (52.00 kJ/mol for the generated peptide versus 58.11 kJ/mol for the original peptide). In the negative example, the generated peptide shows a much higher overall affinity (59.81 kJ/mol) compared to the original peptide (31.85 kJ/mol). These findings showcase the ability of our model not only to generate peptide analogs with similar behavior but also to potentially improve the affinity of existing sequences."}, {"title": "VI. CONCLUSION", "content": "In this research, we have addressed the challenge of generating peptides with desired properties by proposing a novel method that efficiently produces peptide analogs. Traditional approaches in this domain often require large amounts of data, which can be a significant limitation. Our proposed method leverages the inherent capabilities of autoencoder models to explore the protein embedding space, relying solely on pre-trained protein language models. This allows our approach to generate new peptides using only a single sequence of interest, without the necessity for additional sequences with known properties or structures. Our results demonstrate that the proposed method significantly outperforms baseline models across three different similarity indicators. To validate the robustness of our approach, we employed MD simulations on positive and negative examples of TIGIT inhibitors identified through wet lab experiments. These simulations revealed that our method successfully identified peptide analogs exhibiting behavior similar to the original positive and negative examples. Our findings suggest that the proposed method can significantly accelerate the peptide screening process by narrowing the search space. Future work will focus on testing our method in actual wet lab experiments to further validate its effectiveness."}]}