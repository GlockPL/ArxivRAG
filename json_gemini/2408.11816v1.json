{"title": "Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction", "authors": ["Anthony GX-Chen", "Kenneth Marino", "Rob Fergus"], "abstract": "In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher level of state abstraction to pixels, and attribute change at a higher level of temporal abstraction to primitive actions. This abstraction simplifies the transition dynamic by making specific future states easier to predict. We make use of this to propose a fully model-based algorithm that learns a discriminative world model, plans to explore efficiently with only a count-based intrinsic reward, and can subsequently plan to reach any discovered (abstract) states.\nWe demonstrate the model's ability to (i) efficiently solve single tasks, (ii) transfer zero-shot and few-shot across item types and environments, and (iii) plan across long horizons. Across a suite of 2D crafting and MiniHack environments, we empirically show our model significantly out-performs state-of-the-art low-level methods (without abstraction), as well as performant model-free and model-based methods using the same abstraction. Finally, we show how to reinforce learn low level object-perturbing policies, as well as supervise learn the object mapping itself.", "sections": [{"title": "1 Introduction", "content": "Since the inception of reinforcement learning (RL), the problems of exploration and world model learning have been major roadblocks. RL requires an agent to learn both basic motor abilities, and explore long sequences of interactions in the environment. Currently, we have made tremendous progress on problems of low-level control for short-horizon problems. With well defined single tasks, given enough samples (or demonstrations), there are well-developed ways of training low level policies reliably (Schulman et al., 2017; Hafner et al., 2023).\nPerhaps a better way to explore complex environments is to treat the high-level exploration problem separately. We increasingly have the ability to get semantically rich abstractions: via representation learning (Locatello et al., 2020), object segmentation (Kirillov et al., 2023), and visual-language models (Alayrac et al., 2022); even in difficult control settings such as robotics (Liu et al., 2024). In addition, with the recent explosion in the field of natural language processing, people are increasingly interested in addressing RL environments on a semantic level (Andreas et al., 2017; Jiang et al., 2019; Chen et al., 2021). Thus, we ask a timely question: if the agent has a good ability to perceive objects, what more can we do other than tabula rasa learning at the level of pixels and primitive motor actions?\nIn this work, we focus on exploration and world modelling at a semantic level. We explore a simple yet flexible abstraction: in addition to the agent's raw observations (e.g. pixels), it sees an abstract representation of a set of items and their attributes (i.e. object states). We find that to navigate these two levels effectively, it is best to construct a proxy decision process (Ab-MDP) with state and temporal abstractions at the level of objects, with a handful of competent object-perturbing low"}, {"title": "2 Problem Setting", "content": "We begin by considering a (reward free) \"base\" Markov Decision Process (MDP) as the tuple (S, A, P), with state space S, primitive action space A, and transition probability function P: S \u00d7 A \u00d7 S \u2192 [0, 1] (Puterman, 1994). Below we construct the \u201cabstracted\u201d space."}, {"title": "2.1 Abstract states and behaviours", "content": "We give the agent a deterministic object-centric mapping M: S \u2192 X from low level observations to an abstract state X \u2208 X. The abstract state is a discrete set of items, each having form (item identity, item attribute). Denote the i-th item in this (ordered) set as x(i), each x(i) = (\u03b1(\u03af), \u03b5(\u03af)) consists of an item's identity a(i) (e.g. \u201cpotion\u201d), and attribute g(i) (e.g. \u201cin inventory\u201d). In practice, each X is represented as a N \u00d7 (diden + dattr) matrix containing N items (including empty items) each having a diden dimensional identity embedding and dattr dimensional attribute embedding.\nDenote a space B of object perturbation policies which we call behaviours. Each behaviour has an associated \u03c0\u044c : S \u2192 A. A behaviour b = (a(i), \u03be') describes a single item id a(i) and a desired new attribute. For N objects and m attributes, there are N \u00d7 m single item behaviours.\u00b9 Not all behaviours changes a(i) to have \u03be' (due to the change being impossible from current X, or having a bad \u03c0\u044c). A behaviour is competent if executing behaviour policy \u03c0\u03b9 from state S, M(S) = X, result in arriving at state S', M(S') = X', (\u03b1(i),\u03be') \u2208 X', within k steps with high probability. See Appendix B.1 for more details. Executing competent behaviours lead to predictable changes.\nFor the main experiments we assume access to (learned or given) abstract states and behaviours and build on top. We address learning both in Section 4.3."}, {"title": "2.2 Abstracted Item-Attribute MDP", "content": "We now define an Abstracted Item-Attribute MDP using the abstract state X and behaviour B spaces, which we refer to in brief as Ab-MDP. The abstract mapping M provides an state abstraction. We"}, {"title": "3 Methods", "content": "While any standard RL methods can be used to solve the Ab-MDP, we develop a method to more fully make use of the item-attribute structure and item-perturbing behaviours. Specifically, we design a model-based method that efficiently explores and learn an accurate full world model. Our method, Model-based Exploration of abstracted Attribute Dynamics (MEAD) can be broken down into three components: (i) A forward model fo (X, b), (ii) A reward function r(X, b), (iii) A planner which maximizes the reward function.\nOur forward model takes in the current abstract state X and behaviour b, and predicts the probability of success of the change proposed by the abstract behaviour. We describe the procedure for training the forward model in Section 3.1.\nWe use a different reward function and planner at training versus inference time. During training, the agent learns an accurate (abstract) world model without rewards. We use an intrinsic reward function rintr(X, b) that encourages infrequently visited states, and plan using Monte-Carlo Tree Search (MCTS) toward these states. Section 3.2 describes the exploration phase in detail.\nAt inference time, our aim is to reach a particular abstract state. We define our reward function goal as 1 for the goal abstract states and 0 for all others. Because our forward model has been trained to accurately estimate state transitions, we simply run Dijkstra's algorithm using fe and rgoal to maximize expected reward. Section 3.3 describes this."}, {"title": "3.1 Forward model", "content": "A forward model takes in a current abstract state and behaviour to model the distribution over next states, Xt+1 ~ p(Xt+1|Xt, bt). Typically, this distribution has support over the full state space X. We observe that by construction, Ab-MDP considers competent policies as ones that change single item-attributes. Thus, we make the modelling assumption that an abstract transition would result in either (i) a single object's attribute changing (as specified by the executed behaviour), or (ii) the abstract state remaining unchanged. It is of course possible for multiple items' attributes to change at once, or for an unexpected item's attribute to change we show with a competent set of policies and re-planning our model is robust to this in practice in Appendix B.2.2.\nGiven an abstract transition (Xt, bt, Xt+1), we consider this transition successful if the next abstract state contains the attribute change proposed by the behaviour: (\u03b1(\u03b9), \u03be\u02b9) \u2208 Xt+1 for bt = (\u03b1(i),\u03be').\nExecuting a competent policy has a high probability of leading to a successful transition.\nWe model this success probability directly,\nq(Xt,bt) = Pr(x' \u2208 Xt+1|Xt, bt), (1)\nwhere x' = b\u2081 = (\u03b1(i),\u03be') .\nWe refer to this way of modelling as discriminative world modelling, which models the probability that the state change specified by a behaviour is successful. As behaviours are by construction item perturbations, we can view this as asking a question about allowable item changes (\u201ccan I change this item to have this attribute?", "what are all possible outcomes of my action in this state\"). We find this inductive bias leads to more data efficient learning and multi-step planning (Section 4.4.1).\nWe can model the next state after a successful transition as,\u00b2\nX' = \u2206(X,b) = X \\ x(i) \u222a (\u03b1(i), \u03be'), for any b = (a(2), \u03be'). (2)\nThen, given success probability function q (Equation 1), we model the next state distribution as:\nPr(Xt+1/Xt, bt) = { Xt with probability 1 \u2013 q(Xt, bt) . (3)\"\n    },\n    {\n      \"title\": \"3.2 Planning for exploration\",\n      \"content\": \"We want the agent to explore indefinitely to discover how the world works and represent it in its model fo. We use a count-based intrinsic reward introduced in Zhang et al. (2018) which encourages the agent to uniformly cover all state-behaviours (details in Appendix C.3):\nrintr(X,b) = \u221aT/ (N(X, b) + \u20acT), (4)\nwhere N(X, b) is a count of the number of times (X, b) is visited, T is the total number of (abstract) time-steps, and \u20ac = 0.001 a smoothing constant.\nWe use Monte-Carlo Tree Search (MCTS) to find behaviours to reach states that maximizes Equation 4. This is done within the partially learned model's imagination and allow the agent to plan toward novel states multiple steps in the future. The agent takes the first behaviour proposed. Further, since behaviours do not always succeed, one needs multiple state-visitations to estimate q(X, b) well. We found MCTS exploration discovers more novel transition than exploring randomly (Section 4.4.2).\"\n    },\n    {\n      \"title\": \"3.3 Planning for goal\",\n      \"content\": \"Once the agent has sufficiently explored the environment and is well-fitted, the model fe usually contains a small set of high success probability edges between abstract states (see Figure 3b; most item-attribute changes are generally impossible). We can use this model fe to plan to reach any desired (abstract) world state. As our model is not trained to maximize external rewards, we simply give it a function that tells it when it is in a desired state: rgoal(X) = 1if X is a goal state, and 0 otherwise.\nWe use Dijkstra's algorithm which finds the shortest path in a weighted graph. We turn our world model into a graph weighted by success probabilities, using a negative log transform:\nd(X, A(X, b)) = - log fo (X, b), with A(X, b) defined in Equation 2. (5)\nThus, finding the shortest path between current and goal state returns an abstract behaviour sequences that maximizes the probability of successfully reaching the goal.\"\n    },\n    {\n      \"title\": \"4 Results\",\n      \"content\": \"We use two sets of of environment. One is 2D crafting, which requires agent to craft objects following a Minecraft-like crafting tree with each crafting stage requiring multiple low level actions (Chen et al., 2020). Second is MiniHack (Samvelyan et al., 2021), which contains a series of extremely difficult exploration games (Henaff et al., 2022). We detail the environments further in Appendix D.\nFor baselines, we use strong model-free (asynch PPO, (Schulman et al., 2017; Petrenko et al., 2020)) and model-based (Dreamer-v3, (Hafner et al., 2023; Sekar et al., 2020)) baselines. More details in Appendix E. All the methods are trained in the Ab-MDP. We report low level steps and plot 95 confidence interval. See Appendix G for other experimental details.\"\n    },\n    {\n      \"title\": \"4.1 Learning from scratch in single environments\",\n      \"content\": \"Figure 4 shows the from-scratch Ab-MDP training results for strong baselines and our method. We see that Dreamer-v3, a state-of-the-art model-based method, is highly sample efficient. PPO, a model-free method, stably optimizes the reward, but at a data budget of more than two orders of magnitude\"\n    },\n    {\n      \"title\": \"4.2 Transfer and Compositions\",\n      \"content\": \"Next, since the Ab-MDP abstraction models object identities and shared attributes, we evaluate if the abstract level models can transfer object knowledge to new settings. Here we exclusively use the MiniHack environments due to their rich repertoire of object types and higher difficulty. Note we run each transfer experiments in multiple environments; as the result is similar, we report a single illustrative environment in Figure 5 for brevity and provide full results in the Appendix Figure 16.\nWe first place the agent in a \u201csandbox\\\" environment where all objects are spawned with some probability (Appendix D.2.1). The agent is allowed to interact with the environment with only an exploration reward (no task-specific reward) for 200k frames. We then fine-tune the agent in each of the environments. For our model, we continue to train fo within the new environment. For Dreamer-v3, we transfer only the world model (encoder, decoder and dynamics function) and re-initialize the policy and reward functions.\u00b3 Results for all five environments follow similar trends, thus we show a representative example in Figure 5a and present the full set in Figure 16a.\nSurprisingly, Dreamer-v3 trained with both count- and disagreement-based intrinsic rewards fails to get any meaningful rewards in the allocated fine-tuning budget (Figure 5a, red & orange curves), although training for longer seems to show a some reward-indicating negative transfer (see Appendix F.1.2). Similarly, the model-free PPO policy shows negative transfer-reaching lower performance with the same data budget as compared to training from scratch (results in Appendix F.1.1 since x-axis in different order of magnitude). A possible explanation is while the agent observe all object types, the data distribution is dominated by seeing more objects on average in the training environment than in the evaluation environments, thus presenting a distribution shift. In contrast, our model exhibits good zero-shot performance, and reaches optimal performance efficiently.\nTo help the Dreamer-v3 baseline further, we give it a reward function to \\\"use": "he items seen in the pre-training sandbox environment. This is a kind of privileged information which biases the model representation to better model the evaluation-time task. We observe that this variant of Dreamer-v3 exhibits positive transfer, hinting at Dreamer's representatin reliance on the reward function. Nevertheless, it does not exhibit the same degree of zero-shot or fine-tuning capability as our model."}, {"title": "Transfer across object classes", "content": "We further evaluate the ability of the models to transfer to new object types. For example, would a model learn to use a potion better, after it has learned to use a freeze wand, since the abstract level interactions of these objects are similar (the agent can stand on top of these objects, pick it up, and use them)? We take 200k frame checkpoints in the Freeze Random environment and re-train the model in the other 4 environments for an additional 50k frames.\nWe again show one representative comparison in Figure 5b and present the full 4-environments result in Figure 16b. We see our model exhibits reasonable zero-shot performance and improves further through fine-tuning. Similar to the sandbox environment, we observe that exploration-based training of both PPO (results plotted in Appendix F.1.1 due to x-axis being in different order of magnitude) and Dreamer-v3 exhibits negative transfer in our setting, with both methods eventually achieving better rewards but slower than training from scratch. We again pre-train Dreamer-v3 with privileged reward information that matches the downstream task, but still requires generalization to novel objects. We see that in general this way of training dreamer resulted in positive transfer \u2013 it learns to reach a higher reward with fewer samples compared to training from scratch.\nIt is somewhat surprising that our model transfer so well zero-shot to completely new objects in Levitate-Boots and Levitate-Potion. We analyze this further in Appendix F.5: for (item id, item attribute) vectors with unseen ids, we find the model internally cluster them by object attributes, which generalizes from Freeze objects to Levitate objects."}, {"title": "Compositional planning environment", "content": "Finally, we designed an environment similar to the 5 difficult Minihack exploration levels, but with the required planning depth twice as long. This is an even more challenging task where the agent needs to pick up both a freeze and a levitate object, make itself levitate (using the levitate object), then cast a freezing spell to terminate the episode. Further, the levitate spell must be cast after all the objects have been picked up (since the agent cannot pickup objects while levitating), but before the freeze spell is cast. This is a challenging environment requiring precise sequences of abstract behaviours.\nWe observe in Figure 5c that when pre-trained on Freeze objects, our model trains more stably in this environment (but no longer shows good zero-shot results), succeeding majority of times. Our from scratch model solves this task with better rewards. Our method also learns a highly complex yet still interpretable world graph, with a simplified version shown in Figure 31. In contrast, all other methods, with and without pre-training, do not find the correct solution at all, but instead are stuck at a 0 reward local minima."}, {"title": "4.3 Learning object perturbing policies and object-centric map", "content": "Up to this point we have assumed the existence of an object-centric mapping and competent behaviours (Sec. 2.1), which can be combined with our method for efficient exploration and world model learning. We now demonstrate how one might learn both components."}, {"title": "4.3.1 Learning low level policies", "content": "We observe low level policies can be learned with RL if an object-centric map exists. Specifically, we reward the agent for achieving an abstract transition successfully: from Xt, if executing behaviour b = (a(i), \u03be') until an abstract transition results in Xt+1 containing item (a(i), \u03be'), the associated policy \u03c0\u044c gets a reward of +1 (else -1). We train low level policies to replace all behaviours in the MiniHack single task environments, and observe the success probability can be stably optimized with"}, {"title": "4.4 Ablations", "content": "We ablate major design choices below, with additional ablations in Appendix F.3."}, {"title": "4.4.1 Generative vs. Discriminative Models", "content": "A common way of learning a forward model is to model the next state generative-ly, i.e. Xt+1 ~ f(Xt,bt), with support over the entire state space X. In MEAD, we opted to constrain the possible future to either be the same as the current time-step, or one where there is a single item change (Fig. 2). This has the disadvantage of being less flexible, but the advantage of being easier to model. To isolate only the effect of discriminative modelling, we design an experiment where we fit models to the same expert dataset, with the only difference being the model type (fit to either predict success probability in the discriminative case, or to predict the distribution over next item-attribute sets), while keeping all other variables constant. Indeed, given the same (expert) data budget, a discriminative model learns a better model for Ab-MDP planning in the lower data regime (Figure 7, experimental details in Appendix F.3.1)."}, {"title": "4.4.2 Count-based exploration", "content": "We observe using the count-based intrinsic reward (Equation 4) with MCTS was important for exploration. Figure 8 (right) shows count-based exploration discovering many more valid object attribute transitions compared to a randomly exploring agent. Correspondingly, this leads to a model fe that is better at achieving goals (Figure 8, left)."}, {"title": "4.4.3 Parametric vs. Non-Parametric Models", "content": "Instead of fitting a discriminative model to success probability, we can directly use the empirical count-based estimate from a dataset (a non parametric model). This is done to learn the model in Zhang et al. (2018). We compare this choice in Appendix F.1.3 and show our parametric model has better sample efficiency, demonstrating the generalization benefits of our parametric forward model."}, {"title": "5 Related Work", "content": "Hierarchical RL Our Ab-MDP is a hierarchical state and temporal abstraction formalism and can be interpreted as Options (Appendix B.1). Within the Options Framework (Sutton et al., 1999; Precup, 2000), Option discovery remains an open and important question. Existing methods typically do option discovery in a \u201cbottom-up\u201d fashion by learning structures from their training environments (Bacon et al., 2016; Machado et al., 2017; Ramesh et al., 2019; Machado et al., 2021). In contrast, our Ab-MDP uses \"top-down\" prior knowledge in the form of object-perception, and define options that perturb objects' attributes.\nPlanning with abstract representations Abstraction is a fundamental concept (Abel, 2022) and we select a subset of most relevant works. The Ab-MDP can be viewed as a more structured MDP"}, {"title": "6 Discussion", "content": "In this work we introduce the Ab-MDP, which uses human understandable object-attribute relations to build an abstract MDP. This enables us to learn a discriminitive model-based planning method (MEAD) which explores and learns this semantic world model without extrinsic rewards.\nLimitations The major limitation of the work is that the construction of Ab-MDP requires the existence of an object-centric mapping. While we show how these components can be learned in Section 4.3, learning the object mapping still requires a labelled dataset. Promisingly, the field as a whole is moving in a direction where abstractions are becoming more easily obtainable. For instance, object-centric abstract maps can be acquired using segmentation models (Kirillov et al., 2023) and visual language models (Alayrac et al., 2022). Similarly, the short-horizon (low level) manipulation policies can be acquired more efficiently through imitation learning. In the robotics setting it's even been found that using off-the-shelf models for these perception and skill policies is possible (Liu et al., 2024). Nevertheless, a method to automatically discover object-centric representations remains an open question."}, {"title": "A Broader Impact", "content": "Our contributions are fundamental reinforcement learning algorithms, and we hope our work will contribute to the goal of developing generally intelligent systems. However, we do not focus on applications in this work, and substantial additional work will be required to apply our methods to real-world settings.\nRegarding compute resources, we use an internal clusters with Nvidia A100 and H100 GPUs. All experiments use at most one GPU and are run for no more than two days. Running baselines along with randomization of seeds requires multiple GPUs at once."}, {"title": "B Problem Setting Details", "content": "Here we re-iterate our framework and discuss it in greater details. We first define a low level (reward-free) Markov Decision Process (MDP, Puterman (1994)) as the tuple (S, A, P), with (low level) state space S, primitive action space A, and transition probability function P : S\u00d7A\u00d7S \u2192 [0, 1]. Suppose the agent interacts with the environments at discrete time-steps u = 0, 1, 2, ..., then P(su+1|Su, au) describes the probability of transitioning to state su+1 \u2208 S after one time-step when choosing action au \u2208 A in the current low level state su.\nTo construct an Ab-MDP, we start with a surjective deterministic mapping M from low to abstract level: M : S \u2192 X. That is, any low level state S \u2208 S maps onto some abstract state X \u2208 X (multiple S can and do map to the same X).\nDefine behaviours B. Each b = (a(i), \u03be') \u2208 B describes a single item id a(i) and a desired new attribute, along with an associated low level policy \u03c0\u03b9 : S \u2192 A which tries to set the specified item to have the new attribute. A behaviour can be competent or incompetent. Executing competent behaviours lead to the specified item-attribute changes with high probability.\nA behaviour may be incompetent because (i) the proposed attribute change is impossible within the rules of the world (e.g. item is not a craft-able object) (ii) the attribute change is not possible from the current state (e.g. do not have the correct raw ingredients in inventory), or (iii) the behaviour is not sufficiently trained to perform the specified change.\nWe build a simple abstraction that allow us to learn purely at the abstract level while still being able to influence actions at the low level. Notice the surjective map M give us state abstraction. We build temporal abstraction by defining abstract state transitions as happening at a slower time-scale than the low level time-step u.\nAn abstract state transition has occurred at (low level) time-step u if: (i) the abstract state has changed, M(su) \u2260 M(su\u22121); or (ii) the abstract state has not changed in the past k steps, M(su) = M(Su-1) = ... = M(Su-k).\nAn (max-k) Ab-MDP is defined as the tuple (X, B, Tk), with abstract states X, abstract behaviours B, and transition probability function Tk : X \u00d7 B \u00d7 X \u2192 [0, 1]. Th(X'X, b) describes the probability of being in abstract state X' when starting from X and executing behaviour b until an abstract state transition occurs (Definition B.1).\nIn the main text, we write Ab-MDP and denote the transition function as T for brevity (instead of writing max-k Ab-MDP and transition function Tk). k = 8 was chosen for baseline methods as it worked slightly better. MEAD works just as well with k = 8 and k = 16."}, {"title": "B.1.1 Interpretation as Options", "content": "It is natural to interpret our hierarchical framework within the Options framework (Sutton et al., 1999). An option consists of a policy \u03c0 : \u03a9 \u00d7 A \u2192 [0, 1], a termination function \u03b2 : \u03a9 \u2192 [0, 1], and an initiation set of states I C S. N is used to denote the set of all possible historical states."}, {"title": "B.1.2 A note on semi-MDP", "content": "The decision process that selects amongst options is a semi-MDP (Theorem 1, (Sutton et al., 1999)). The general way to model semi-MDP dynamic requires considering the amount of low level timesteps that passes between each abstract level decision as a random variable (Sutton et al., 1999; Barto and Mahadevan, 2003). Instead, we chose to simply treat the abstracted level as having a fixed time interval in between steps. This is chosen for simplicity as our focus is with learning efficiency on the abstract level, and this formulation allow us to directly apply efficient algorithms developed to solve MDPs. We also see empirically that given our M and B this choice does not lead to a less optimal policy compared to algorithms that directly solve the low level MDP."}, {"title": "B.1.3 Multi-Item Behaviours", "content": "In the Ab-MDP, we consider abstract states as an (ordered) set of items X = {x(1), x(2), ...}, where each item consist of an identity and attribute x(i) = (a(i), g(i)). A behaviour is a description of which item(s) to change (Section 2.1). For instance, the single-item behaviour b = {(a(i), \u00bf')} specifies a change of the i-th item to a new attribute \u03be'. We can similarly specify multi-item behaviours, such as a two-item behaviour b = {(a(i), \u03be'), (a(i), \u00bf')}. This behaviour is competent if after an abstract state change, the i-th item takes on attribute \u03be' and the j-th item takes on attribute \u03be\" with high probability.\nFor N objects, m attributes, and I items changes to consider in behaviours,\nNumber of behaviours = (NI) \u00d7 mI . (6)\nFor instance, for single item change behaviours (I = 1), there are (NI) \u00d7m\u00b9 = N\u00d7m behaviours. For two-item change behaviours (I = 2), there are (2) \u00d7 m\u00b2 behaviours, and so on. In the extreme, one can choose to model a all possible item attribute changes (I = N), which results in mN behaviours that is exponential in the number of items considered."}, {"title": "B.2 Analysis of Assumptions", "content": "One may worry that the abstraction specified by Ab-MDP is overly specific to a particular problem and an appropriate abstraction would not exist for a different problem. To this end, we make the following observation that any sparse reward RL task can be described by a Ab-MDP with a single item,\nFor any MDP with a reward function R : S \u2192 R, where,\nR(S) = 1 if S \u2208 G and 0 otherwise,\nfor S\u2208 G terminal goal state(s), we can describe this as an Ab-MDP with a single item a and binary attributes {\u00a71, \u00a72}. The abstract map M : S \u2192 X can be constructed in the following way,\nM(S) = {(\u03b1, \u03be2)} if S\u2208 G, {(\u03b1, \u03be1)} otherwise. (7)\nThe proxy problem of transition from X = {(\u03b1, \u03be1)} to X = {(\u03b1, \u03be2)} solves the sparse reward task in the base MDP and a competent behaviour for this transition is a good policy is the base task."}, {"title": "B.2.2 Discriminative modelling with single item change", "content": "In our forward model, we made the assumption to only model single item's attribute change (Section 3.1 and Equation 3), and that if the attribute does not change then the abstract state Xt+1 stays the same as Xt. We analyze this assumption more deeply in this section."}, {"title": "C MEAD: Extended Methods", "content": "Here we provide more details about our methods"}, {"title": "C.1 Model Fitting", "content": "Given a dataset of (abstract) trajectories collected via the exploration strategy outlined in Section 3.2, D = {(X1,b1, X2, b2, ...)i}i=1,...,n, we can estimate the empirical success probability q of all observed item-attribute pairs by counting their occurrences:\nq(X, b) \u2248 p(X, b) = Number of (Xt, bt, Xt+1) transitions where (\u03b1(\u2170), \u03be\u02b9) \u2208 Xt+1 + \u20ac Number of (Xt, bt) + 2\u20ac , (8)\nwhere e is a small smoothing constant (usually 1e-3). This can be efficiently implemented as a hash map, via hashing (Xt, bt) as keys, and the number of successful and total transitions (starting from (Xt, bt)) as values.\nTo train our discriminative model fo : X \u00d7 B \u2192 [0, 1], we minimize binary cross entropy loss with p as the target,\n0* = arg min E [p(X, b) \u00b7 log (fo(X, b)) + (1 \u2212 p(X, b)) \u00b7 log (1 \u2013 fe(x,b))], (9)\n\u03b8\nwhere the expectation is estimated by uniformly sampling unique (X, b) pairs from dataset.\nAppendix C.2 contains architectural details for fo. All in all, we found that our sampling method and loss function lead to a stable, simple-to-optimize objective. Moreover, it is possible to directly use p to do planning, as is done in Zhang et al. (2018). However, we show in Section F.1.3 that it is more efficient to use our parametric modelling approach as it generalizes better to new objects."}, {"title": "C.2 Model Architecture", "content": "We use a small key-query attention architecture that takes in sets of vectors to produce a binary prediction, see Figure 11."}, {"title": "C.3 Exploration", "content": "We use a reward function that encourages indefinitely exploring all states and behaviours,\nrintr (X, b) = T / N(X, b) + \u20acT , (10)\nwhere N(X, b) is a count of the number of times (X, b) is visited, T is the total number of abstract time-steps, and \u20ac = 0.001 is a smoothing constant.\nIntuitively, a state with many visits (large N(X, b)) will have lower reward, while T prevents the intrinsic reward from shrinking to zero (so the agent explores indefinitely). Indeed, in the bandits setting, this reward function is maximized when all states are visited uniformly (Zhang et al., 2018).\nWe emphasize that all of our model training is done with this intrinsic reward, with no task-specific information injected at training time.\nWe use MCTS to find high (intrinsic) reward states. MCTS iteratively expands a search tree and balances between exploring less-visited states (using an upper confidence tree) and exploiting rewarding states. In our case, MCTS generates child nodes of abstract state X by proposing behaviours b and checking if fo(X,b) > 0.5. We also introduce a small degree of randomness so all behaviours have a small chance of being selected.\nWe modify the MCTS algorithm to return the the maximum rintr(X, b) encountered along each path during the back-propagation stage and set a fix expansion depth without a simulation stage from the leaf node. This is done to prevent loops and encourage the agent to go toward frontier states with the least amount of visitations."}, {"title": "D Environments", "content": "We adopt three crafting environments proposed in Chen et al. (2020), which contain Minecraft-like crafting logic. The agent needs to traverse a grid world, collect resources, and build increasingly more complex items by collecting and crafting ingredients (e.g. getting an axe, chopping tree to get wood, before being able to make wooden planks and furniture that depend on planks). Primitive actions consist of movement in the four cardinal directions (N,E,S,W), toggle (of a switch to open doors), grab (e.g. pickaxe), mine (e.g. ores), craft (at crafting bench of specific items). The low level"}, {"title": "D.2 MiniHack", "content": "We also use the MiniHack environment (Samvelyan et al., 2021), which"}]}