{"title": "Efficient Exploration and Discriminative World Model\nLearning with an Object-Centric Abstraction", "authors": ["Anthony GX-Chen", "Kenneth Marino", "Rob Fergus"], "abstract": "In the face of difficult exploration problems in reinforcement learning, we study\nwhether giving an agent an object-centric mapping (describing a set of items and\ntheir attributes) allow for more efficient learning. We found this problem is best\nsolved hierarchically by modelling items at a higher level of state abstraction to\npixels, and attribute change at a higher level of temporal abstraction to primitive\nactions. This abstraction simplifies the transition dynamic by making specific\nfuture states easier to predict. We make use of this to propose a fully model-based\nalgorithm that learns a discriminative world model, plans to explore efficiently\nwith only a count-based intrinsic reward, and can subsequently plan to reach any\ndiscovered (abstract) states.\nWe demonstrate the model's ability to (i) efficiently solve single tasks, (ii) transfer\nzero-shot and few-shot across item types and environments, and (iii) plan across\nlong horizons. Across a suite of 2D crafting and MiniHack environments, we\nempirically show our model significantly out-performs state-of-the-art low-level\nmethods (without abstraction), as well as performant model-free and model-based\nmethods using the same abstraction. Finally, we show how to reinforce learn low\nlevel object-perturbing policies, as well as supervise learn the object mapping itself.", "sections": [{"title": "1 Introduction", "content": "Since the inception of reinforcement learning (RL), the problems of exploration and world model\nlearning have been major roadblocks. RL requires an agent to learn both basic motor abilities, and\nexplore long sequences of interactions in the environment. Currently, we have made tremendous\nprogress on problems of low-level control for short-horizon problems. With well defined single\ntasks, given enough samples (or demonstrations), there are well-developed ways of training low level\npolicies reliably.\nPerhaps a better way to explore complex environments is to treat the high-level exploration problem\nseparately. We increasingly have the ability to get semantically rich abstractions: via representation\nlearning, object segmentation, and visual-language\nmodels; even in difficult control settings such as robotics. In\naddition, with the recent explosion in the field of natural language processing, people are increasingly\ninterested in addressing RL environments on a semantic level. Thus, we ask a timely question: if the agent has a good ability to perceive objects,\nwhat more can we do other than tabula rasa learning at the level of pixels and primitive motor actions?\nIn this work, we focus on exploration and world modelling at a semantic level. We explore a simple\nyet flexible abstraction: in addition to the agent's raw observations (e.g. pixels), it sees an abstract\nrepresentation of a set of items and their attributes (i.e. object states). We find that to navigate\nthese two levels effectively, it is best to construct a proxy decision process (Ab-MDP) with state\nand temporal abstractions at the level of objects, with a handful of competent object-perturbing low\nlevel policies. By construction, dynamics on this semantic level becomes structured, and we make\nuse of this to design a discriminative model-based RL method that learns to predicts if a specific\nobject perturbation will be successful. Our method, Model-based Exploration of abstracted Attribute\nDynamics (MEAD), is fit with a simple objective to model semantic level transitions, resulting in\nstable model improvements with no need for auxiliary objectives. It is fully online: the agent plans to\nexplore without an extrinsic reward to learn about the world it inhabits using a count-based objective.\nThe same model can be used at any point to plan to solve downstream tasks when given a goal\nfunction, without the need to train policy or value functions.\nThe main results of this work investigates decision making at the semantic level as defined by the\nAb-MDP, assuming access to an object centric map and object perturbing policies. We empirically\nevaluate our method in a set of 2D crafting games, and MiniHack environments that are known to\nbe very difficult to explore. We then show how to learn components of the\nAb-MDP when it is not given: by reinforcement learning object perturbation policies given an object\nmapping, and supervised learning of the object centric mapping itself.\nThe main contributions of this work are:\n\u2022 We define the Ab-MDP: a simple human-understandable hierarchical MDP. The structure of\nthis abstraction allows for pre-trained or from-scratch visual encoders to be used. It also\nallows our method (MEAD) to use a discriminative objective for world-model learning.\n\u2022 We introduce our model-learning method (MEAD). MEAD uses a stable discriminative\nobjective to model item attribute changes, which is more efficient at learning than generative-\nbased baselines and also allows for efficient inference-time planning.\n\u2022 We empirically show MEAD extracts an interpretable world model, accurately plans over\nlong horizons, reaches competitive final performance on all levels with greater sample\nefficiency to strong baselines, and transfers well to new environments.\n\u2022 We demonstrate how to learn parts of the Ab-MDP when it is not given: by reinforcement\nlearning object-perturbing policies, and supervised learning the object-centric mapping."}, {"title": "2 Problem Setting", "content": "We begin by considering a (reward free) \"base\" Markov Decision Process (MDP) as the tuple\n$(S, A, P)$, with state space S, primitive action space A, and transition probability function $P:$\n$S \u00d7 A \u00d7 S \u2192 [0, 1]$. Below we construct the \u201cabstracted\u201d space."}, {"title": "2.1 Abstract states and behaviours", "content": "We give the agent a deterministic object-centric mapping $M: S \u2192 X$ from low level observations\nto an abstract state $X \u2208 X$. The abstract state is a discrete set of items, each having form (item\nidentity, item attribute). Denote the i-th item in this (ordered) set as $x^{(i)}$, each $x^{(i)} = (\u03b1^{(i)}, \u03b5^{(i)})$\nconsists of an item's identity $\u03b1^{(i)}$ (e.g. \u201cpotion\u201d), and attribute $\u03b5^{(i)}$ (e.g. \u201cin inventory\u201d). In practice,\neach X is represented as a N \u00d7 ($d_{iden}$ + $d_{attr}$) matrix containing N items (including empty items)\neach having a $d_{iden}$ dimensional identity embedding and $d_{attr}$ dimensional attribute embedding.\nDenote a space B of object perturbation policies which we call behaviours. Each behaviour has\nan associated $\u03c0_b : S \u2192 A$. A behaviour b = $(\u03b1^{(i)}, \u03be')$ describes a single item id $\u03b1^{(i)}$ and a desired\nnew attribute. For N objects and m attributes, there are N \u00d7 m single item behaviours. Not all\nbehaviours changes $\u03b1^{(i)}$ to have \u03be' (due to the change being impossible from current X, or having\na bad $\u03c0_b$). A behaviour is competent if executing behaviour policy $\u03c0_b$ from state S, $M(S) = X$,\nresult in arriving at state $S', M(S') = X', (\u03b1^{(i)},\u03be') \u2208 X'$, within k steps with high probability. See\nAppendix B.1 for more details. Executing competent behaviours lead to predictable changes.\nFor the main experiments we assume access to (learned or given) abstract states and behaviours and\nbuild on top. We address learning both in Section 4.3."}, {"title": "2.2 Abstracted Item-Attribute MDP", "content": "We now define an Abstracted Item-Attribute MDP using the abstract state X and behaviour B spaces,\nwhich we refer to in brief as Ab-MDP. The abstract mapping M provides an state abstraction. We\nfurther define the Ab-MDP to have a coarser temporal resolution than the low level MDP: an abstract\ntransition only occurs when an abstract state changes, or if the state stays the same but k low level\nsteps elapses (see Definition B.1 for more details).\nSpecifically, the Ab-MDP is the tuple (X, B, T), with abstract states X, abstract behaviours B, and\ntransition probability function $T : X \u00d7 B \u00d7 X \u2192 [0,1]$. $T(X'|X, b)$ describes the probability of\nbeing in abstract state X' when starting from X, executing behaviour b, and waiting until an abstract\ntransition occurs. The Ab-MDP can be interpreted as Options, which we\ndiscuss in Appendix B.1.1.\nThere is an intuitive relationship between the competence of a behaviour and the transition probability\nfunction T. For instance, executing an incompetent behaviour b from abstract state X will likely\nlead to the next abstract state being itself, $X_{t+1} = X_t$, or some hard-to-predict distribution over next\nstates. Executing a competent behaviour b lead to a predictable next state that contains the proposed\nattribute change with high probability.\nIn any case, we can treat the Ab-MDP as a regular MDP use standard RL algorithms to solve it. This\ncan be viewed as solving a proxy problem of the low level base MDP. For the remainder of this\npaper, all methods are trained exclusively within the Ab-MDP unless stated otherwise."}, {"title": "3 Methods", "content": "While any standard RL methods can be used to solve the Ab-MDP, we develop a method to more\nfully make use of the item-attribute structure and item-perturbing behaviours. Specifically, we design\na model-based method that efficiently explores and learn an accurate full world model. Our method,\nModel-based Exploration of abstracted Attribute Dynamics (MEAD) can be broken down into three\ncomponents: (i) A forward model $f_\u03b8(X, b)$, (ii) A reward function r(X, b), (iii) A planner which\nmaximizes the reward function.\nOur forward model takes in the current abstract state X and behaviour b, and predicts the probability\nof success of the change proposed by the abstract behaviour. We describe the procedure for training\nthe forward model in Section 3.1.\nWe use a different reward function and planner at training versus inference time. During training,\nthe agent learns an accurate (abstract) world model without rewards. We use an intrinsic reward\nfunction $r_{intr}(X, b)$ that encourages infrequently visited states, and plan using Monte-Carlo Tree\nSearch (MCTS) toward these states. Section 3.2 describes the exploration phase in detail.\nAt inference time, our aim is to reach a particular abstract state. We define our reward function $r_{goal}$\nas 1 for the goal abstract states and 0 for all others. Because our forward model has been trained\nto accurately estimate state transitions, we simply run Dijkstra's algorithm using $f_\u03b8$ and $r_{goal}$ to\nmaximize expected reward. Section 3.3 describes this."}, {"title": "3.1 Forward model", "content": "A forward model takes in a current abstract state and behaviour to model the distribution over next\nstates, $X_{t+1} \u223c p(X_{t+1}|X_t, b_t)$. Typically, this distribution has support over the full state space X.\nWe observe that by construction, Ab-MDP considers competent policies as ones that change single\nitem-attributes. Thus, we make the modelling assumption that an abstract transition would result\nin either (i) a single object's attribute changing (as specified by the executed behaviour), or (ii) the\nabstract state remaining unchanged. It is of course possible for multiple items' attributes to change at\nonce, or for an unexpected item's attribute to change we show with a competent set of policies and\nre-planning our model is robust to this in practice in Appendix B.2.2.\nDiscriminative modelling Given an abstract\ntransition $(X_t, b_t, X_{t+1})$, we consider this tran-\nsition successful if the next abstract state con-\ntains the attribute change proposed by the be-\nhaviour: $(\u03b1^{(i)}, \u03be\u02b9) \u2208 X_{t+1}$ for $b_t = (\u03b1^{(i)},\u03be')$.\nExecuting a competent policy has a high prob-\nability of leading to a successful transition.\nWe model this success probability directly,\n$q(X_t,b_t) = Pr(x' \u2208 X_{t+1}|X_t, b_t)$,                                                (1)\nwhere $x' = b_t = (\u03b1^{(i)},\u03be')$.\nWe refer to this way of modelling as discriminative world modelling, which models the probability\nthat the state change specified by a behaviour is successful. As behaviours are by construction\nitem perturbations, we can view this as asking a question about allowable item changes (\u201ccan I\nchange this item to have this attribute?", "what are all possible\noutcomes of my action in this state\"). We find this inductive bias leads to more data efficient learning\nand multi-step planning (Section 4.4.1).\nForward Prediction We can model the next state after a successful transition as,\n$X' = \u2206(X,b) = X \\ x^{(i)} \u222a (\u03b1^{(i)}, \u03be'), for any b = (\u03b1^{(i)}, \u03be')$.                                       (2)\nThen, given success probability function q (Equation 1), we model the next state distribution as:\n$Pr(X_{t+1}/X_t, b_t) =  \\substack{ Xt  & with probability 1 \u2013 q(X_t, b_t).  }$                                                                        (3)\nSee Figure 2 for an example. As mentioned above, our model is robust to cases where multiple items\nchange through re-planning (Appendix B.2.2).\nModel Learning We use model function class $f_\u03b8 : X \u00d7 B \u2192 [0, 1]$ with trainable parameters \u03b8,\nand fit it to approximate success probability (Equation 1). We can estimate the empirical success\nprobability from a dataset of trajectories simply by counting, and fit a binary cross entropy loss. We\nfound this to be a stable, simple-to-optimize objective. More details in Appendix C.1.\"\n    },\n    {\n      \"title\": \"3.2 Planning for exploration\",\n      \"content\": \"We want the agent to explore indefinitely to discover how the world works and represent it in its\nmodel $f_\u03b8$. We use a count-based intrinsic reward introduced in Zhang et al. (2018) which encourages\nthe agent to uniformly cover all state-behaviours (details in Appendix C.3):\n$r_{intr}(X,b) = \\sqrt{T/ (N(X, b) + \u03f5_T)}$,                                               (4)\nwhere N(X, b) is a count of the number of times (X, b) is visited, T is the total number of (abstract)\ntime-steps, and \u03f5 = 0.001 a smoothing constant.\nWe use Monte-Carlo Tree Search (MCTS) to find behaviours to reach states that maximizes Equation 4.\nThis is done within the partially learned model's imagination and allow the agent to plan toward\nnovel states multiple steps in the future. The agent takes the first behaviour proposed. Further, since\nbehaviours do not always succeed, one needs multiple state-visitations to estimate q(X, b) well. We\nfound MCTS exploration discovers more novel transition than exploring randomly (Section 4.4.2).\"\n    },\n    {\n      \"title\": \"3.3 Planning for goal\",\n      \"content\": \"Once the agent has sufficiently explored the environment and is well-fitted, the model $f_\u03b8$ usually\ncontains a small set of high success probability edges between abstract states (see Figure 3b; most\nitem-attribute changes are generally impossible). We can use this model $f_\u03b8$ to plan to reach any desired\n(abstract) world state. As our model is not trained to maximize external rewards, we simply give it a\nfunction that tells it when it is in a desired state: $r_{goal}(X) = 1$if X is a goal state, and 0 otherwise.\nWe use Dijkstra's algorithm which finds the shortest path in a weighted graph. We turn our world\nmodel into a graph weighted by success probabilities, using a negative log transform:\n$d(X, \u2206(X, b)) = - log f_\u03b8 (X, b)$, with $\u2206(X, b)$ defined in Equation 2.                                      (5)\nThus, finding the shortest path between current and goal state returns an abstract behaviour sequences\nthat maximizes the probability of successfully reaching the goal.\"\n    },\n    {\n      \"title\": \"4 Results\",\n      \"content\": \"We use two sets of of environment. One is 2D crafting, which requires agent to craft objects following\na Minecraft-like crafting tree with each crafting stage requiring multiple low level actions. Second is MiniHack, which contains a series of extremely\ndifficult exploration games. We detail the environments further in Appendix D.\nFor baselines, we use strong model-free (asynch PPO, and model-based (Dreamer-v3,) baselines. More details\nin Appendix E. All the methods are trained in the Ab-MDP. We report low level steps and plot 95\nconfidence interval. See Appendix G for other experimental details.\"\n    },\n    {\n      \"title\": \"4.1 Learning from scratch in single environments\",\n      \"content\": \"Figure 4 shows the from-scratch Ab-MDP training results for strong baselines and our method. We\nsee that Dreamer-v3, a state-of-the-art model-based method, is highly sample efficient. PPO, a model-\nfree method, stably optimizes the reward, but at a data budget of more than two orders of magnitude\"\n    },\n    {\n      \"title\": \"4.2 Transfer and Compositions\",\n      \"content\": \"Next, since the Ab-MDP abstraction models object identities and shared attributes, we evaluate if\nthe abstract level models can transfer object knowledge to new settings. Here we exclusively use the\nMiniHack environments due to their rich repertoire of object types and higher difficulty. Note we\nrun each transfer experiments in multiple environments; as the result is similar, we report a single\nillustrative environment in Figure 5 for brevity and provide full results in the Appendix Figure 16.\nSandbox transfer We first place the agent in a \u201csandbox\\\" environment where all objects are\nspawned with some probability (Appendix D.2.1). The agent is allowed to interact with the environ-\nment with only an exploration reward (no task-specific reward) for 200k frames. We then fine-tune the\nagent in each of the environments. For our model, we continue to train $f_\u03b8$ within the new environment.\nFor Dreamer-v3, we transfer only the world model (encoder, decoder and dynamics function) and\nre-initialize the policy and reward functions. Results for all five environments follow similar trends,\nthus we show a representative example in Figure 5a and present the full set in Figure 16a.\nSurprisingly, Dreamer-v3 trained with both count- and disagreement-based intrinsic rewards fails\nto get any meaningful rewards in the allocated fine-tuning budget (Figure 5a, red & orange curves),\nalthough training for longer seems to show a some reward-indicating negative transfer (see Ap-\npendix F.1.2). Similarly, the model-free PPO policy shows negative transfer-reaching lower perfor-\nmance with the same data budget as compared to training from scratch (results in Appendix F.1.1\nsince x-axis in different order of magnitude). A possible explanation is while the agent observe all\nobject types, the data distribution is dominated by seeing more objects on average in the training\nenvironment than in the evaluation environments, thus presenting a distribution shift. In contrast, our\nmodel exhibits good zero-shot performance, and reaches optimal performance efficiently.\nTo help the Dreamer-v3 baseline further, we give it a reward function to \"use": "he items seen in the\npre-training sandbox environment. This is a kind of privileged information which biases the model\nrepresentation to better model the evaluation-time task. We observe that this variant of Dreamer-\nv3 exhibits positive transfer, hinting at Dreamer's representatin reliance on the reward function.\nNevertheless, it does not exhibit the same degree of zero-shot or fine-tuning capability as our model."}, {"title": "Transfer across object classes", "content": "We further evaluate the ability of the models to transfer to new\nobject types. For example, would a model learn to use a potion better, after it has learned to use a\nfreeze wand, since the abstract level interactions of these objects are similar (the agent can stand\non top of these objects, pick it up, and use them)? We take 200k frame checkpoints in the Freeze\nRandom environment and re-train the model in the other 4 environments for an additional 50k frames.\nWe again show one representative comparison in Figure 5b and present the full 4-environments result\nin Figure 16b. We see our model exhibits reasonable zero-shot performance and improves further\nthrough fine-tuning. Similar to the sandbox environment, we observe that exploration-based training\nof both PPO (results plotted in Appendix F.1.1 due to x-axis being in different order of magnitude)\nand Dreamer-v3 exhibits negative transfer in our setting, with both methods eventually achieving\nbetter rewards but slower than training from scratch. We again pre-train Dreamer-v3 with privileged\nreward information that matches the downstream task, but still requires generalization to novel objects.\nWe see that in general this way of training dreamer resulted in positive transfer \u2013 it learns to reach a\nhigher reward with fewer samples compared to training from scratch.\nIt is somewhat surprising that our model transfer so well zero-shot to completely new objects in\nLevitate-Boots and Levitate-Potion. We analyze this further in Appendix F.5: for (item id, item\nattribute) vectors with unseen ids, we find the model internally cluster them by object attributes,\nwhich generalizes from Freeze objects to Levitate objects."}, {"title": "Compositional planning environment", "content": "Finally, we designed an environment similar to the 5\ndifficult Minihack exploration levels, but with the required planning depth twice as long. This is\nan even more challenging task where the agent needs to pick up both a freeze and a levitate object,\nmake itself levitate (using the levitate object), then cast a freezing spell to terminate the episode.\nFurther, the levitate spell must be cast after all the objects have been picked up (since the agent cannot\npickup objects while levitating), but before the freeze spell is cast. This is a challenging environment\nrequiring precise sequences of abstract behaviours.\nWe observe in Figure 5c that when pre-trained on Freeze objects, our model trains more stably in\nthis environment (but no longer shows good zero-shot results), succeeding majority of times. Our\nfrom scratch model solves this task with better rewards. Our method also learns a highly complex yet\nstill interpretable world graph, with a simplified version shown in Figure 31. In contrast, all other\nmethods, with and without pre-training, do not find the correct solution at all, but instead are stuck at\na 0 reward local minima."}, {"title": "4.3 Learning object perturbing policies and object-centric map", "content": "Up to this point we have assumed the existence of an object-centric mapping and competent behaviours\n(Sec. 2.1), which can be combined with our method for efficient exploration and world model learning.\nWe now demonstrate how one might learn both components."}, {"title": "4.3.1 Learning low level policies", "content": "We observe low level policies can be learned with RL if an object-centric map exists. Specifically, we\nreward the agent for achieving an abstract transition successfully: from $X_t$, if executing behaviour\n$b = (\u03b1^{(i)}, \u03be')$ until an abstract transition results in $X_{t+1}$ containing item $(\u03b1^{(i)}, \u03be')$, the associated\npolicy $\u03c0_b$ gets a reward of +1 (else -1). We train low level policies to replace all behaviours in the\nMiniHack single task environments, and observe the success probability can be stably optimized with"}, {"title": "4.3.2 Learning object-centric map", "content": "If we wish to also learn the object-centric mapping, we can do so using supervised learning. We\ngenerate a dataset of 100k transitions using a random policy in MiniHack's Freeze-Horn environ-\nment, and use the hand-crafted mapping to provide ground truth abstract state labels. We train a\nneural net to produce the abstract state X from S. We compare planning performance using the\nlearned object-centric mapping and observe it reaches similar performance as the ground\ntruth mapping, even though the encoder is not perfect in predicting the abstract states. We discuss\nalternative way of getting this mapping in Section 6."}, {"title": "4.4 Ablations", "content": "We ablate major design choices below, with additional ablations in Appendix F.3."}, {"title": "4.4.1 Generative vs. Discriminative Models", "content": "A common way of learning a forward model is to model\nthe next state generative-ly, i.e. $X_{t+1} \u223c f(X_t,b_t)$, with\nsupport over the entire state space X. In MEAD, we opted\nto constrain the possible future to either be the same as the\ncurrent time-step, or one where there is a single item change\n(Fig. 2). This has the disadvantage of being less flexible, but\nthe advantage of being easier to model. To isolate only the\neffect of discriminative modelling, we design an experiment\nwhere we fit models to the same expert dataset, with the\nonly difference being the model type (fit to either predict\nsuccess probability in the discriminative case, or to predict\nthe distribution over next item-attribute sets), while keeping\nall other variables constant. Indeed, given the same (expert) data budget, a discriminative model\nlearns a better model for Ab-MDP planning in the lower data regime (Figure 7, experimental details\nin Appendix F.3.1)."}, {"title": "4.4.2 Count-based exploration", "content": "We observe using the count-based intrinsic reward (Equation 4)\nwith MCTS was important for exploration. Figure 8 (right) shows\ncount-based exploration discovering many more valid object at-\ntribute transitions compared to a randomly exploring agent. Corre-\nspondingly, this leads to a model $f_\u03b8$ that is better at achieving goals\n(Figure 8, left)."}, {"title": "4.4.3 Parametric vs. Non-Parametric Models", "content": "Instead of fitting a discriminative model to success probability, we\ncan directly use the empirical count-based estimate from a dataset\n(a non parametric model). This is done to learn the model in Zhang\net al. (2018). We compare this choice in Appendix F.1.3 and show\nour parametric model has better sample efficiency, demonstrating\nthe generalization benefits of our parametric forward model."}, {"title": "5 Related Work", "content": "Hierarchical RL Our Ab-MDP is a hierarchical state and temporal abstraction formalism and\ncan be interpreted as Options (Appendix B.1). Within the Options Framework", "bottom-up": "ashion by learning structures from their training environments\n. In contrast", "top-down\" prior knowledge in the form of object-perception, and define options\nthat perturb objects' attributes.\nPlanning with abstract representations Abstraction is a fundamental concept and\nwe select a subset of most relevant works. The Ab-MDP can be viewed as a more structured MDP\nformulation in which learning and planning is simplified. This relates to OO-MDP, which describes state as a set of object and object states which interact through\nrelations. Also related is MaxQ and AMDP, which provides\na decomposition of a base MDP into multi-level sub-MDP hierarchies to plan more efficiently.\nTransition dynamics can also be defined as symbolic domain specific language. Above works require pre-specifying additional interaction rules, functions, and object relations to\nsimplify planning, whereas we only assume the ability to see objects and their attributes, and learn\nthe forward model as a neural network. More distantly related is Veerapaneni et al. (2019) which\nfocus more on learning good object-centric representations rather than effective usage of an abstract\nlevel. Nasiriany et al. tackles sub-policy learning and planning with abstraction, focusing on\nimplicitly representing abstractions rather than using semantically meaningful ones. The DAC-MDP\nclusters low level states into \"core states\" to solve by value iteration, with a\nfocus on getting good policies in an offline RL setting rather than abstract world model learning.\nMost similar is Zhang et al. (2018), which does model-based planning over abstract attributes. We\ndiffer in having a more expressive abstraction framework": "modelling a flexible set of objects and\nshareable attributes, rather than a fixed, binary set of attributes only. We also use a parametric model\nto more efficiently learn and is able to transfer object information to new environments. We compare\nagainst their non-parametric approach and show better sample efficiency in Section 4.4.3.\nRL in semantic spaces As language models have grown in interest and sophistication, there has been\nan increasing interest in semantic and linguistic abstractions in RL. Early works such as Andreas et al.\n(2017) looked at handcrafted abstract spaces such as we do here to show that creating abstractions\nallows for better learning and exploration. Other works such as Jiang et al. (2019); Chen et al. (2021)\nspecifically use natural language as the abstract space. In others, the language space abstraction is\nused as a way of guiding exploration."}]}