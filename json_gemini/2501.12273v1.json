{"title": "Condor: Enhance LLM Alignment with Knowledge-Driven\nData Synthesis and Refinement", "authors": ["Maosong Cao", "Taolin Zhang", "Mo Li", "Chuyu Zhang", "Yunxin Liu", "Haodong Duan", "Songyang Zhang", "Kai Chen"], "abstract": "The quality of Supervised Fine-Tuning (SFT)\ndata plays a critical role in enhancing the\nconversational capabilities of Large Language\nModels (LLMs). However, as LLMs become\nmore advanced, the availability of high-quality\nhuman-annotated SFT data has become a sig-\nnificant bottleneck, necessitating a greater re-\nliance on synthetic training data. In this work,\nwe introduce Condor, a novel two-stage syn-\nthetic data generation framework that incor-\nporates World Knowledge Tree and Self-\nReflection Refinement to produce high-quality\nSFT data at scale. Our experimental results\ndemonstrate that a base model fine-tuned on\nonly 20K Condor-generated samples achieves\nsuperior performance compared to counter-\nparts. The additional refinement stage in Con-\ndor further enables iterative self-improvement\nfor LLMs at various scales (up to 72B), validat-\ning the effectiveness of our approach. Further-\nmore, our investigation into the scaling for syn-\nthetic data in post-training reveals substantial\nunexplored potential for performance improve-\nments, opening promising avenues for future\nresearch.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are experienc-\ning rapid advancements, with proprietary mod-\nels such as like GPT (Achiam et al., 2023) and\nGemini (Team et al., 2023), alongside open-source\ncounterparts such as LLaMA (Dubey et al., 2024),\nQwen (Yang et al., 2024), Mistral (Jiang et al.,\n2023a), and Deepseek (Liu et al., 2024a) evolv-\ning at an unprecedented pace. However, this rapid\niteration comes with a significant challenge: the\ndepletion of existing high-quality data for Super-\nvised Fine-Tuning (SFT). Moreover, the internet is\nThis work is done when Taolin Zhang and Chuyu Zhang\nare on internship at Shanghai AI Laboratory, * means equal\ncontribution, \u2020 means corresponding author, \u2021 means project\nlead.\nincreasingly inundated with synthetic data of vary-\ning and often questionable quality, making it ever\nmore difficult to construct and filter higher-quality\ntraining data for LLMs.\nEmpirical studies on SFT training have con-\nclusively shown that both data quality and quan-\ntity play crucial roles in enhancing model perfor-\nmance (Shen, 2024). While scaling laws suggest\nthat models can achieve extraordinary capabilities\nwhen trained on large datasets, high-quality data\ncan yield comparable results even at smaller scales.\nThis highlights the importance of generating sub-\nstantial amounts of high-quality data to further en-\nhance large-scale model capabilities. Recent re-\nsearch introduce various methods for synthesizing\nSFT training data (Wang et al., 2024a; Ding et al.,\n2023; Xu et al., 2023; Yuan et al., 2024; Tang et al.,\n2024). However, these approaches face several lim-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Synthesizing Instruction Tuning Data", "content": "With the rapid advancement of Large Language\nModels (LLMs) (Achiam et al., 2023; Team et al.,\n2023; Dubey et al., 2024; Yang et al., 2024; Jiang\net al., 2023a; Liu et al., 2024a) and Large Multi-\nModality Models (LMMs) (Radford et al., 2021;\nZhu et al., 2023; Zhang et al., 2024a,b; Liu et al.,\n2024b; Chen et al., 2024), the demand for high-\nquality training data continues to grow. As a result,\nsynthetic data generation has become increasingly\ncrucial for model development. While high-quality\nsynthetic data can significantly enhance model per-\nformance, low-quality synthetic data may lead to\nmodel degradation or collapse.\nWizardLM (Xu et al., 2023) introduces an Evol-\nInstruct scheme, which expands an initial question\ndataset by generating new questions through tech-\nniques such as deep thinking, adding constraints,\nand reconstruction. However, the quality and diver-\nsity of the generated data are strongly influenced\nby the characteristics of the original dataset. Simi-\nlarly, Self-Reward (Yuan et al., 2024) builds on a\nseed dataset, enabling the model to generate ques-\ntions via few-shot prompting and produce multiple\ncorresponding responses. These responses are sub-\nsequently evaluated by a reward model and labeled\nas training data for reinforcement learning with\nhuman feedback (RLHF).\nMagpie(Xu et al., 2024) explores leveraging\nmodels themselves to generate synthetic data by uti-\nlizing chat templates as prompts to directly produce\ndialogues. However, this approach is not univer-\nsally applicable to all model types, and therefore\nlacks generalization ability. Additionally, the syn-\nthetic data generated often lacks stability, contain-\ning significant noise that requiring extensive filter-\ning to maintain quality. Agent-based data synthesis\nmethods, such as MATRIX-Gen (Tang et al., 2024),\nsimulate multiple world scenarios using a multi-\nagent framework and collect dialogue data from\ninteractions within these scenarios. While effec-\ntive in some cases, this method is time-consuming,\nlabor-intensive, and limited by the need for multi-\nple models to operate concurrently.\nIn contrast to these methods, Condor eliminates\nthe dependence on a seed dataset, requiring only\ntags as seeds for expansion and generation. This\nenables exceptional diversity and scalability in data\nproduction. Furthermore, Condor handles all data\ngeneration tasks with a single model, significantly\nreducing costs while maintaining high efficiency."}, {"title": "2.2 LLM Self-Iteration", "content": "Recent studies have explored methods for large\nlanguage models (LLMs) to improve themselves\nthrough iterative self-enhancement. For in-\nstance, I-SHEEP (Liang et al., 2024) and Self-\nInstruct (Wang et al., 2023) utilize seed data and\nLLMs to generate new instructions. However, these\nmethods often produce instructions that closely re-\nsemble those in the seed dataset, limiting the di-\nversity and novelty of the generated data. (Tao\net al., 2024) categorizes existing self-iteration meth-"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Pipeline Overview", "content": "As shown in Figure 2, the pipeline of Condor is\ndivided into two stages: data synthesis and data\nrefinement. In the data synthesis stage, we begin\nby introducing the World Knowledge Tree, which\nserves as a foundation of tags for data generation.\nNext, we apply task and difficulty expansion to\nenhance the diversity and complexity of questions\nunder each tag, leading to the creation of the initial\nsynthetic QA dataset Dv. In the data refinement\nstage, we employ a Self-Reflection Refinement\nstrategy, enabling the model to iteratively optimize\nthe generated responses and yield DR. Notably,\nwe utilize a single model during the entire pipeline\nfor better efficiency."}, {"title": "3.2 Condor Void", "content": "In the data synthesis stage, we generate the syn-\nthetic data based on existing world knowledge.\nThis stage consists of two key components: (1)\nWorld Knowledge Tree and (2) Q&A pair gen-\neration. The dataset produced from this stage is\nreferred to as Dv."}, {"title": "3.2.1 World Knowledge Tree", "content": "To construct a comprehensive multi-level tag sys-\ntem that encapsulates diverse aspects of world\nknowledge, we propose a methodology that en-"}, {"title": "3.3 Condor Refine", "content": "In this stage, we enable the model to engage in\nself-reflection and critical examination of its own\nresponses, followed by revisiting the questions to\ngenerate higher-quality replies. The resulting re-\nfined dataset is referred as DR.\nThe initial QA pairs contain responses directly\ngenerated by the model, which are inherently lim-\nited by the model's current capabilities. Therefore,\nwe implement data refinement to further improve\nthe quality of the responses. We carefully develop\na set of fixed templates that guide the model in\nidentifying both strengths and weaknesses in its\nresponses, leading to specific suggestions for im-\nprovement. Subsequently, we utilize these self-\ngenerated improvement suggestions to prompt the\nmodel to produce enhanced responses. The model\nis tasked with preserving the effective elements of\nits original responses while addressing identified\nweaknesses to generate superior responses. Com-\nplete examples of QA pairs that have undergone\nthe entire Condor pipeline are provided in the Ap-\npendix. Finally, we obtain 200k high-quality refine-\nment datasets from Dy, i.e., |DR| \u2248 200,000."}, {"title": "4 Experiments and Results", "content": ""}, {"title": "4.1 Training and Evaluation Settings", "content": "Training Settings. We use xTuner (Contributors,\n2023b) as the training framework. To ensure fair-\nness in comparison, we set the initial learning rate\nto 2e-5 and train for 3 epochs in all experiments.\nUnless specifically stated, we start training from\nthe base model and compare it with the official\nRLHF model.\nEvaluation Settings. We use OpenCom-\npass (Contributors, 2023a) and employ greedy\ninference to uniformly evaluate all models, ensur-\ning fairness across various datasets. We selected 8\nhuman-preference benchmarks for evaluation the\nchat capabilities of the models. We evaluate the\nresults and reports the average normalized score\nat the percentage scale. Additionally, due to the\nhigh cost of conducting subjective evaluations with\na paid API model, we use GPT4o as the Judge\nModel for main results only, and we judge with\nthe open-source CompassJudger-1-32B (Cao et al.,\n2024) in ablation study and scaling experiments.\nFurthermore, we also select a range of knowledge-\nbased Q&A datasets for groundtruth-based\nevaluations."}, {"title": "4.2 Data Synthesis Settings", "content": "Only one model is required for data synthesis\nand data refinement throughout the entire pipeline.\nIn the main experiments, we use 200k data syn-\nthesized by Qwen2.5-72B for model training.\nAdditionally, for SOTA comparisons and self-\ninteraction experiments, we also generate 200k data\npoints using Qwen2.5-7B to test the effectiveness\nof the Condor pipeline."}, {"title": "4.3 Main Results", "content": "Using Condor data generated by Qwen2.5-72B-\nInstruct, we first train both the Base and Instruct\nversions of Qwen2.5-7B and compare their perfor-\nmance against the official model on both Chat and\nKnowledge benchmarks.\nHuman-preference Evaluation. We select sev-\neral widely-used human-preference benchmarks\nand calculate the final normalized average scores\non these datasets (detailed calculation logic is pro-\nvided in AppendixA.1). We use GPT-40 as the\nJudge Model, and the average results are reported\nin Table 1. As shown in the table, the results of\ntraining the Base model with DR surpassed the of-"}, {"title": "4.4 Ablation Study", "content": "We further investigate the impact of model type,\nsize, and question difficulty on Condor training\nthrough a series of ablation experiments using Con-\ndor data generated by Qwen2.5-72B-Instruct. For\nthe type ablation, we evaluate models of different\nfamilies, including Qwen, InternLM, and Llama.\nFor the size ablation, we test Qwen2.5 at 7B, 14B,\nand 32B parameter scales. Additionally, in the\ndifficulty ablation, we train models with datasets\ncontaining questions of varying difficulty levels.\nIn all experiments, we utilize CompassJudger as\njudger for subjective evaluation.\nModel Type. From Table 4, it is evident that\ntraining on DR consistently improves performance\nacross different models. Almost all models demon-\nstrate significant enhancements on all subjective\nevaluation datasets. Compared to the official model,\nQwen2.5-7B shows an improvement more than 6%\n(56.9% to 63.3%) after training on Condor Refine.\nInternLM\u00b2 and Llama exhibit an even larger im-\nprovement by about 10%. These results highlight\nthat the quality of the base models plays a crucial\nrole in determining the conversational performance\nof the final Instruct model.\nModel Size. We further evaluate the adaptability\nof models of different sizes to DR. Table 5 show\nresults aligning with the observations from Table 4.\nInterestingly, we find that the improvement in per-\nformance for the 14B and 32B models is notably\ngreater compared to the 7B variant. This suggests\nthat larger models demonstrate a greater capacity to\nlearn and benefit from the refined data and achieve\nbetter performance.\nTask Difficulty. Table 6 presents the perfor-\nmance of models trained with Condor data of vary-\ning difficulty levels. The results show that tasks\nof increasing difficulty contribute to greater per-\nformance improvements, with more challenging\ntasks yielding higher average scores. Additionally,\ncombining all three difficulty levels during train-\ning further enhances performance, resulting in an\naverage improvement of 0.34%."}, {"title": "4.5 Scaling of Condor Data", "content": "The performance of models within the Condor\npipeline improves with respect to the increasing\n\u00b2We apply the SFT to an internal version of InternLM2.5-\n7B, which is enhanced with instruction data focused on math-\nematics and coding."}, {"title": "4.6 Self Iteration", "content": "We conduct self-iteration experiments using Con-\ndor data generated by Qwen2.5-7B and Qwen2.5-\n72B to evaluate whether a single model can en-\nhance its chat capabilities by itself. We train the\nBase model and then report human-preference per-"}, {"title": "5 Analysis and Discussion", "content": ""}, {"title": "5.1 Which capability is improved the most?", "content": "We evaluate the scores for each benchmark across\ndifferent sub-capability dimensions and observe im-\nprovements in sub-capabilities after Condor train-\ning (detailed information in Appendix A.2). As\nshown in Figure 6, the model demonstrates im-\nprovement across all sub-capabilities, with the most\nsignificant score increases observed in the tasks\nof Creation, QA, and Chat. These areas align\nclosely with the core aspects of the model's human-\npreference capability, highlighting Condor's effec-\ntiveness in enhancing key conversational skills."}, {"title": "5.2 How large is the synthetic data coverage?", "content": "We extract the embeddings of questions from Con-\ndor and Magpie and use t-SNE for dimensionality\nreduction and visualization. To ensure a fair and\nbalanced comparison, we randomly sample 200k\nquestions from each dataset for this analysis. Note\nthat according to Magpie's claimed method, it is ca-\npable of feedbacking the model's own training data,\nwhich means that the data distribution obtained by\nMagpie is somewhat close to the model's original\nSFT data. As shown in Figure 7, Condor has a\nsimilar or even broader distribution compared to\nMagpie, which indicates that the questions gener-\nated by Condor have good diversity."}, {"title": "5.3 What does SFT actually enhance?", "content": "A comprehensive analysis of the experimental re-\nsults reveals important insights, particularly from\nthe knowledge-based evaluation results (Table 2).\nThe findings indicate that the SFT phase has min-\nimal impact on the model's intrinsic knowledge\ncapabilities. This suggests that the vast majority\nof knowledge acquisition occurs during the pre-\ntraining phase. These observations suggest that\nthe SFT phase of Condor contributes very little to\nenhancing the model's foundational knowledge. In-\nstead, its primary focus is on refining the model's\nability to utilize existing knowledge to answer ques-\ntions effectively, which can be achieved with a rel-\natively small amount of high-quality data."}, {"title": "6 Conclusion", "content": "In this paper, we propose Condor, a two-stage\ndata synthesis engine to generate high quality data\nfor supervised fine-tuning. Extensive experiments\ndemonstrate that with the high quality data gener-\nated by Condor, the performance of the fine-tuned\nmodel surpasses many existing methods and the\nofficial RLHF models using a small amount of syn-\nthetic data. We also explore the scaling of synthetic\ndata and self-iteration experiments, demonstrating\nthat models can achieve self-iteration through syn-\nthetic data."}, {"title": "7 Limitations", "content": "Despite the significant improvements brought by\nCondor in human preference performance, there\nare still many experiments that require further ex-\nploration, such as the use of multi-round iterative\nsynthetic data and how to further enhance the diver-\nsity of the synthetic data. Additionally, the halluci-\nnations produced by LLMs in synthetic data could\nalso become a potential risk. These issues need to\nbe further addressed and improved, which will in\nturn enhance the quality of the synthetic data."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Detailed Calculation Method for the Subjective Evaluation Scores.", "content": "Since the metrics for statistical scoring of various subjective chat datasets are not the same, and the\nscoring ranges are not all 0-100, we use the following mapping relationships when calculating the average\nsubjective chat score for the model:\n\u2022 The scoring ranges for CompassArena, AlpacaEvalv2, and ArenaHard are 0-100, and no special\ntreatment is needed when calculating the overall mean score.\n\u2022 For FoFo and Followbench, the scoring range is 0-1, and we multiply by 100 when calculating the\noverall mean score.\n\u2022 For AlignBenchv1.1 and MTBench101, the scoring range is 0-10, and we multiply by 10 when\ncalculating the overall mean score.\n\u2022 For WildBench, the scoring range is -100 to 100, and we add 100 to the score and then divide by 2\nfor mapping.\n\u2022 The final calculated total Average score is the mean score of these datasets after they have been\nmapped to the 0-100 range."}, {"title": "A.2 Detailed Information for Sub-capabilities Improvement of Condor", "content": "Based on the subscore for each capability dimension provided by each dataset, we aggregated the model's\nscores by dimension, thereby obtaining the model's scores on each capability dimension across these\nsubjective test datasets. Specifically, according to the different subscores for various capability dimensions\nprovided by different datasets, we aggregated the scores in the following manner:\n\u2022 Math: The math and reasoning capabilities of model, we aggregate the sub-score from AlignBench,\nCompassArena.\n\u2022 Task: The task problem svoling capability of model, we aggregate from AlignBench, ArenaHard and\nMTBench101.\n\u2022 Creation: The ability of the model to create various types of content as required is aggregated from\nAlignBench, CompassArena, MTBench101, and WildBench.\n\u2022 Role-play: The role-playing capability of the model is aggregated from AlignBench, AlpacaEval,\nand WildBench.\n\u2022 QA: The knowledge-based question-answering capability of the model is aggregated from Align-\nBench and CompassArena.\n\u2022 Chat: The daily chat capability of the model is aggregated from AlignBench, AlpacaEval, ArenaHard,\nMTBench101, and WildBench.\n\u2022 IF: The instruction following capability of the model is aggregated from FoFo and FollowBench.\n\u2022 Language: The language understanding and processing capability of the model is aggregated from\nAlignBench and CompassArena."}]}