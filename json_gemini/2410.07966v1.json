{"title": "Neural Reasoning Networks: Efficient Interpretable Neural Networks With Automatic Textual Explanations", "authors": ["Stephen Carrow", "Kyle Harper Erwin", "Olga Vilenskaia", "Parikshit Ram", "Tim Klinger", "Naweed Aghmad Khan", "Ndivhuwo Makondo", "Alexander Gray"], "abstract": "Recent advances in machine learning have led to a surge in adoption of neural networks for various tasks, but lack of interpretability remains an issue for many others in which an understanding of the features influencing the prediction is necessary to ensure fairness, safety, and legal compliance. In this paper we consider one class of such tasks, tabular dataset classification, and propose a novel neuro-symbolic architecture, Neural Reasoning Networks (NRN), that is scalable and generates logically sound textual explanations for its predictions. NRNs are connected layers of logical neurons which implement a form of real valued logic. A training algorithm (R-NRN) learns the weights of the network as usual using gradient descent optimization with backprop, but also learns the network structure itself using a bandit-based optimization. Both are implemented in an extension to PyTorch (available on GitHub) that takes full advantage of GPU scaling and batched training. Evaluation on a diverse set of 22 open-source datasets for tabular classification demonstrates performance (measured by ROC AUC) which improves over multilayer perceptron (MLP) and is statistically similar to other state-of-the-art approaches such as Random Forest, XGBoost and Gradient Boosted Trees, while offering 43% faster training and a more than 2 orders of magnitude reduction in the number of parameters required, on average. Furthermore, R-NRN explanations are shorter than the compared approaches while producing more accurate feature importance scores.", "sections": [{"title": "1 Introduction", "content": "Classifying tabular data has long been a fundamental task in ML/AI and recent advances in AI have led to increased adoption in various industries, but the complexity of these systems has made them black boxes that are difficult to understand and interpret (Adadi and Berrada 2018; Linardatos, Papastefanopoulos, and Kotsiantis 2021; Barredo Arrieta et al. 2020). This lack of transparency is a major obstacle to the adoption of AI in sensitive domains such as healthcare, and increasingly practitioners and researchers aim to explain black-box model predictions (Rudin 2019). A variety of tools have been designed towards that end (Guidotti et al. 2018), however popular methods such as SHAP (Lundberg and Lee 2017) and LIME (Ribeiro, Singh, and Guestrin 2016) have been shown to be susceptible to adversarial attacks (Yuan and Dasgupta 2023; Slack et al. 2020) and fail to effectively explain neural networks and generalized additive models (GAMs) (Carmichael and Scheirer 2023). As a result, these post-hoc explainers can produce misleading explanations, especially when users misunderstand or over trust them. Recent work even considers the use of black-box models to explain other black-box models, such as with a pre-trained LLM (Kroeger et al. 2023) or Diffusion model (Madaan and Bedathur 2023), further obfuscating the explanation generation process.\nOne potential solution to address these drawbacks is to use inherently interpretable models, which can produce their own explanations that faithfully represent the model's computation (Rudin 2019). This approach is explored in, for example, Neural Additive Models (NAM) (Agarwal et al. 2021), and CoFrNet (CFN) (Puri et al. 2021), which have performance that is statistically similar to deep-tree-ensemble-based methods that are known to perform well for tabular classification.\nHowever, inherently interpretable models are not all equal in their explanation quality, predictive accuracy, or practical utility such as training speed or parameter counts. For example, our experiments show that the best NAM and CFN models after hyper-parameter tuning have approximately 861K and 37K parameters respectively. In this paper, we introduce the NRN architecture and the R-NRN supervised classification algorithm. NRNs use Weighted Lukasiewicz Logic, which forms the foundation of interpreting the resulting net-"}, {"title": "2 Related work", "content": "In this section we review related works and discuss how they are positioned within the landscape of AI/ML algorithms used for supervised tabular classification, as well as their relation to explainable AI (XAI).\nWe begin with post-hoc explanation of a trained AI/ML model, as this is a highly common (Rudin 2019) and flexible approach to XAI in practice. Application of post-hoc explainers like SHAP and LIME are traditionally used to explain non-interpretable models. Well-known instances of non-interpretable models are Random Forests (RF) (Breiman 2001), Gradient Boosted Trees (GBT) (Friedman 2001), eXtreme Gradient Boosting (XGB) (Chen and Guestrin 2016), and Multilayer perceptron (MLP). More recently, research has focused on development of neural-methods for tabular data leveraging modern architectures like Transformer. A recent example is the FTTransformer (FTT) (Gorishniy et al. 2021a). FTT is a simple adaptation of the Transformer architecture for tabular data, transforming all features (categorical and numerical) to embeddings and applying a stack of Transformer layers to the embeddings. FTT outperforms other deep learning methods according to the authors.\nChen et al. proposed another non-interpretable neural method for learning from tabular data. They contribute a flexible neural component for tabular data called the Abstract Layer, which learned to explicitly group correlative input features and generate higher-level features for semantics abstraction (Chen et al. 2022). This approach is somewhat similar to self-attention in that it learns sparse feature selections. The authors also proposed Deep Abstract Networks (DANets) which use these Abstract Layers. The authors show that DANets are effective and have superior computational complexity compared to competitive methods.\nMany more neural methods for learning from tabular data exist. An extensive review of all such literature is outside the scope of this paper and other works such as (Borisov et al. 2022) provide such surveys.\nAnother branch of research focuses on inherently interpretable models. These methods should themselves produce faithful explanations representing the model's computation"}, {"title": "3 Neural Reasoning Networks", "content": "In this section, we motivate our architecture, Neural Reasoning Network (NRN), and the benefits of our approach to building inherently interpretable AI systems. We then show how NRN is used to instantiate a supervised classification algorithm for learning on tabular data and close by detailing how this trained network is explained.\nNeuro-symbolic AI techniques, such as (Riegel et al. 2020), have demonstrated capable of producing interpretable models that use logic. LNN showed excellent results for Inductive Logic Programming (Sen et al. 2022), Entity Linking (Jiang et al. 2021), KBQA (Kapanipathi et al. 2020), and reinforcement learning (Kimura et al. 2021). However, LNN lacks a method for logic induction from data, and cannot leverage GPU acceleration. We therefore leverage Weighted Lukasiewicz Logic activations functions introduced in (Riegel et al. 2020) to construct a Neural Reasoning Network with \"blocks\" of Modified Weighted Lukasiewicz Logic, which are implemented as PyTorch modules, to produce an interpretable neuro-symbolic network of connected layers that is differentiable, leverages AutoGrad, GPU acceleration, and distributed GPU computation.\nNRN Intuitively, a NRN is similar to a traditional Neural Network (NN), except that each node in the network is interpreted as either an \"And (Conjunction), or \u201cOr\u201d (Disjunction) operation as shown in Figure 1, which depicts a two-layer Neural Reasoning Network with alternating Conjunction and Disjunction blocks, although different architectures can be easily developed. This interpretation is achieved by representing each node in the network with Weighted Logic that models those operations. Weighted Logic, including Weighted Lukasiewicz Logic introduced in (Riegel et al. 2020), are a form of fuzzy real valued logic that produce a confidence estimation that the given logical operation is True.\n\n$f(\\beta-\\sum_j w_j [ m_j x_j + (1 - m_j) (1-x_j)] )$\n\n$f (1-3 (1-\\beta+\\sum_j w_j| [ m_j x_j + (1 - m_j) (1 - x_j)])$"}, {"title": "Bandit Reinforced Neural Reasoning Network (R-NRN)", "content": "We now proceed to describe our supervised classification algorithm, R-NRN, that induces logic used for prediction from tabular data using NRN. In addition, we will share a"}, {"title": "4 Results", "content": "In this section we present the results of an extensive empirical study comparing R-NRN to tree-based algorithms, traditional deep learning, and specialized deep learning models designed for tabular data. In addition, we present example explanations from R-NRN and evaluate their quality.\nPredictive performance To evaluate our method, we train R-NRN, as well as one or more representative models from each class of models described in the related work, on the benchmark for tabular datasets proposed by (Grinsztajn, Oyallon, and Varoquaux 2022) detailed in Appendix D. The"}, {"title": "5 Conclusion", "content": "Limitations R-NRN shows good performance on the benchmark datasets for tabular classification, but it is not yet clear if this approach will work for regression or other fundamental problems in AI such as with more complex domains like Computer Vision or Natural Language Processing. Furthermore, the flexibility to modify the initialized structure is somewhat limited in R-NRN, raising questions as to if this approach can be used on such complex domains. Also, while we present examples of explanations produced by R-NRN as well as quantitative analysis of their quality, we do not perform a user-study that would give more insight into the usefulness of such explanations in practice.\nConclusion NRNs are used to construct a supervised classification algorithm, R-NRN, with performance on tabular classification similar to RF, XGB, and GBT. R-NRN explanations are smaller and more accurate compared to feature importance based approaches including RF and NAM. Future work could leverage NRNs to develop inherently interpretable algorithms for NLP, computer vision, and reasoning domains."}, {"title": "G GPU Scaling", "content": "In addition to minimizing the number of parameters, we achieve scalability with NRNs by using GPUs to broadcast tensor operations of conjunction and disjunction layers. Denote \u2227 and \u2228 as the vector operations for weighted conjunction (Equation 2) and disjunction (Equation 3) respectively.\nA logical block is defined as a broadcast tensor operation with an input tensor to a block of size (C, O, I, 1) where C is the number of channels, O is the block's output size, and I is the blocks input size. The logical block contains a weight tensor of size, (C, O, I, 1). Each channel c\u2208 C of a Weighted Conjunction Block (Ablock Equation 4) and Weighted Disjunction Block (Vblock Equation 5) contains O weight vectors w \u2208 I \u00d7 1, and takes as input a tiled vector x \u2208 I \u00d7 1, thus producing tensors of the described shapes. In this work, channels of an NRN are leveraged for each class in a multi-class setting, though their use is dictated by the problem to be solved. An additional tensor dimension for batches of samples can be added to the operation."}]}