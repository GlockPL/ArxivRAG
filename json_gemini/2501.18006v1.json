{"title": "Topological Signatures of Adversaries in Multimodal Alignments", "authors": ["Minh Vu", "Geigh Zollicoffer", "Huy Mai", "Ben Nebgen", "Boian Alexandrov", "Manish Bhattarai"], "abstract": "Multimodal Machine Learning systems, particularly those aligning text and image data like CLIP/BLIP models, have become increasingly prevalent, yet remain susceptible to adversarial attacks. While substantial research has addressed adversarial robustness in unimodal contexts, defense strategies for multimodal systems are underexplored. This work investigates the topological signatures that arise between image and text embeddings and shows how adversarial attacks disrupt their alignment, introducing distinctive signatures. We specifically leverage persistent homology and introduce two novel Topological-Contrastive losses based on Total Persistence and Multi-scale kernel methods to analyze the topological signatures introduced by adversarial perturbations. We observe a pattern of monotonic changes in the proposed topological losses emerging in a wide range of attacks on image-text alignments, as more adversarial samples are introduced in the data. By designing an algorithm to backpropagate these signatures to input samples, we are able to integrate these signatures into Maximum Mean Discrepancy tests, creating a novel class of tests that leverage topological signatures for better adversarial detection.", "sections": [{"title": "1. Introduction", "content": "In recent years, the rapid advancement of artificial intelligence (AI) has led to the development of increasingly complex multimodal systems that integrate diverse streams of data, including text, images, audio, and graphs. As these technologies become more pervasive, they also face significant vulnerabilities, particularly from adversarial attacks. Adversarial attacks exploit inherent weaknesses in machine learning (ML) models by introducing subtle perturbations that can maliciously alter the system's outputs (Goodfellow, 2014; Brendel et al., 2017). While substantial research has focused on safeguarding AI models against adversarial attacks in unimodal contexts, the challenges presented by the multimodal landscape remain understudied. Notably, although adversarial attacks in multimodal settings have advanced rapidly (Zhang et al., 2022; Zhou et al., 2023), defense strategies have yet to fully consider the unique characteristics of multimodal systems, which are crucial for enhancing their robustness against such threats.\nIn this study, we focus on multimodal alignments, typically appearing in CLIP (Radford et al., 2021) and BLIP (Li et al., 2022), where the image and text data embeddings are aligned for downstream predictions. Based on the Manifold Hypothesis (Goodfellow et al., 2016), in many instances\u2014such as natural images\u2014the support of the data distribution stays on a low-dimensional manifold embedded within Euclidean space. Many interesting features and patterns of those data can be captured through the topological properties of that manifold as well as the manifold of their neural network's embeddings. Particularly, through extensive experiments based on persistent homology, an emerging technique in topological data analysis (TDA) to study those manifolds, we show that adversarial attacks alter the image-text topological alignment and introduce distinctive topological signatures. We further demonstrate that the presence of adversarial examples in an image batch can be more effectively detected with those signatures.\nThe key results of this work are highlighted in Fig. 1. Using the ImageNet (Deng et al., 2009) and CIFAR-10 (Krizhevsky, 2009) datasets with CLIP-ViT-B/32 and CLIP-ViT-L/14@336px, respectively, we demonstrate our proposed Total Persistence (TP) loss $L_{TP}$ and Multi-scale Kernel (MK) loss $L_{MK}$ under varying proportions of adversarial samples in the data batch. Intuitively, these losses capture the mismatch in the corresponding topological signatures between the image and text embeddings. The results clearly show a proportional change in these losses as the percentage of adversarial data increases, emphasizing the sensitivity of our topological measures to adversarial perturbations. In the third and fourth columns, we demonstrate how the Maximum Mean Discrepancy (MMD) (Grosse et al., 2017; Gao et al., 2021) tests, based on topological features derived from the TP and MK losses (TPSAMMD and MK-SAMMD), enhance the test power and control Type-I error in existing MMD solutions, highlighting the effectiveness of our topological approach in detecting multimodal adversarial attacks. This work's main contributions are:\n\u2022 We introduce two novel Topological-Contrastive (TC) losses, $L_{TP}$ and $L_{MK}$, based on Total Persistence (Divol & Polonik, 2019) and Multi-scale Kernel (Reininghaus et al., 2014), measuring the topological differences between image embeddings and text embeddings in multimodal alignment. We provide extensive experiments showing the presence of adversaries results in a clear distinction in the TC losses, i.e., in most settings, the TC losses monotonically change when more adversarial data is in the data batch. We also provide a theoretical justification for the increase of TP via Poisson Cluster Process modeling.\n\u2022 We integrate both TC losses into state-of-the-art (SOTA) MMD tests that differentiate between clean and adversarial data, creating a novel class of MMD tests based on TC features. Specifically, we design an algorithm to back-propagate the TC losses to the input samples, resulting in the TC features capturing how much each sample contributes to the global topological distortion in the data batch. These features are then used to enhance the MMD test.\n\u2022 We conduct extensive experiments in 3 datasets (CIFAR-10, CIFAR-100, and ImageNet), 5 CLIP embeddings (ResNet50, ResNet101, ViT-B/16, ViT-L/14, and ViT-L/14@336px), 3 BLIP embeddings (ViT-B/14, ViT-B/129, and ViT-B/129-CapFilt-L), and 6 adversarial generation methods (FGSM, PGD, AutoAttack, APGD, BIM, and Carlini-Wagner (CW)) to demonstrate the advantages of the two above findings.\nOur paper is organized as follows. Sect. 2 provides the background and related work of this study. Sect. 3 formulates our proposed TC losses and demonstrates their capabilities in monitoring alignment of multimodal adversaries. Sect. 4 shows how we can leverage the TC losses for MMD-based adversarial detection. Sect. 5 reports our experimental results, and Sect. 6 concludes this paper."}, {"title": "2. Related Work and Preliminaries", "content": "The Two-sample test and the MMD: Given samples from two distributions P and Q, a two-sample test assesses whether to reject the null hypothesis that $P = Q$ based on a test statistic that measures the distance between the samples. One such test statistic is the MMD, which quantifies the distance between the embeddings of the probability distributions into a reproducing kernel Hilbert space (RKHS) (Gretton et al., 2012):\n$MMD(P, Q; H_k) := \\sup_{f \\in H, ||f|| \\leq 1} |E[f(x)] - E[f(Y)]| \\qquad (1)$\nHere, k is a bounded kernel associated with the RKHS $H_k$ (i.e., $|k(\\cdot,\\cdot)| < \\infty$), $|\\cdot| := |\\cdot|_{H_k}$ and X and Y are random variables sampled from P and Q, respectively. Gretton et al. demonstrated that the MMD equals zero if and only if $P = Q$, indicating that the MMD can be used to determine whether two distributions are identical.\nAdversarial Detection in Unimodal/Multimodal ML: In"}, {"title": "3. Topological Signatures of Adversaries against Multimodal Alignment", "content": "Our analysis of multimodal adversaries is based on the intuition that the adversarial perturbations in one data stream can potentially alter or destroy the alignment between that data and another data stream. The problem is in how to capture this behavior, analyze, and leverage it for better adversarial detection. Measuring multimodal alignment is not trivial because the data streams not only come from different domains, have different representations, and semantic meanings, but also lack index alignment. We tackle that question through the lens of TDA: we first capture the topological structures from the point clouds of representations of each data stream. We then compare the extracted topological features to measure the alignment. In particular, we rely on the total persistence (Divol & Polonik, 2019; Edelsbrunner & Harer, 2022) and the Multi-scale Kernel (Reininghaus et al., 2014), two concepts of persistence homology, to quantify topological information.\nThis section first describes our proposed topological con-"}, {"title": "3.1. Topological Contrastive Losses", "content": "This study focuses on two homology signatures commonly employed in TDA: while TP measures the cumulative importance of topological features across all dimensions, serving as a holistic summary of a dataset's topology, the MK facilitates comparisons between datasets by analyzing their topological features at multiple scales. We choose these two homologies for this study for their distinctive behaviors with adversaries (Subsect.3.2) and their capability to be back-propagated for adversarial detection (Sect.4).\nTP Loss: For a given dimension i, the a-total persistence of dimension i is computed on the persistence diagram $D_i(X)$ (Divol & Polonik, 2019):\n$Pers_i^{\\alpha}(X) := \\sum_{(b,d)\\in D_i(X)} (d-b)^{\\alpha} \\qquad (2)$\nThe TP loss of order a between two point clouds is the summation of the difference at all homology groups:\n$L_{TP}^{\\alpha}(X,Y) = \\sum_i |Pers_i^{\\alpha}(X) - Pers_i^{\\alpha}(Y)| \\qquad (3)$\nThe TP (Eq.2) can be considered as a fundamental quantity for persistence homology analysis as it is closely related to the Wasserstein distance between point clouds. We would refer readers to (Divol & Polonik, 2019) for more details on how the loss can be used to capture topological information.\nMK Loss: The loss is formulated based on the Multi-scale kernel $k_{\\sigma} : D \\times D \\rightarrow \\mathbb{R}$ introduced by Reininghaus et al., acting on persistence diagrams of point clouds X and Y:\n$k_{\\sigma}(D_i(X), D_i(Y)) :=\n\\frac{1}{8\\pi \\sigma} \\sum_{p \\in D_i(X), q \\in D_i (Y)} e^{-\\frac{\\|p-q\\|^2}{8\\sigma}}- e^{-\\frac{\\|p-\\hat{q}\\|^2}{8\\sigma}}\\qquad (4)$\nwhere p and q are the birth-death pairs from the corresponding persistence diagrams, and $\\hat{q} = (d, b)$ denotes the mirror of q = (b, d) through the diagonal. Notably, the MK is proved to be 1-Wasserstein stable (Theorem 2. Reininghaus et al.), and is a robust summary representation of data's topological features. For our purpose, we define the MK loss of scale \\sigma between two point clouds by:\n$L_{MK}^{\\sigma} (X,Y) = \\sum_i k_{\\sigma} (D_i(X), D_i(Y)) \\qquad (5)$\nTopological Contrastive Losses for Multimodal Alignment: Fig. 2 describes how we use our TC losses $L_{TC} \\in {L_{TP}, L_{MK}}$ to analyze multimodal alignment. The forward left-to-right arrows indicate the computation of TP and MK losses. In CLIP and BLIP, the images and text representations are aligned in a shared embedding space. We first extract the those logit/embeddings before the alignment step, treating these embeddings as point clouds. Next, the corresponding VR filtrations $\\{R_\\epsilon\\}_{\\epsilon}$ are constructed, followed by the generation of persistence diagrams $\\{D_i\\}_i$. The TC losses are then computed using Eq. 3 for the TP and Eq. 5 for the MK losses, with X and Y representing the image and text embeddings, respectively."}, {"title": "3.2. Monotonic Behaviors of Adversarial TC losses", "content": "We now present a key finding of this work. Through extensive experiments utilizing the proposed TC losses conducted across models, adversarial attacks, and datasets, we observe"}, {"title": "3.3. Modeling Total Persistence of Adversaries", "content": "The final part of this section presents a theoretical explanation for the observed overall increase in TP of adversaries (see Table 1). Our assumption is that, since the primary goal of adversarial attacks is to change the top logit, they do not maintain the structure of the new label cluster. This leads to a more scattered and less organized representations. We call this the adversarial scattering assumption. Under that assumption, our hypothesis is that the scattering behavior of adversarial logit leads to a higher TP.\nPCP modeling: To test the above hypothesis, we start with modeling the logits as points generated by a latent Poisson Cluster Process (PCP) (Daley & Vere-Jones, 2003), with clusters centered at the vertices of a K-dimensional simplex. For the following discussion, it is sufficient to understand that the PCP is governed by two parameters: $a_s$ and a bias ratio r. Intuitively, a larger r encourages the centers of clusters to be closer to the vertices of the simplex, and a larger $a_s$ results in generated points being more concentrated around these centers. This relationship is visualized in Fig. 4. We provide a rigorous formulation of the PCP in Appx. B.\nTP of PCP: We utilize the PCP to investigate how the scattering of logits affects the TP. Due to the high complexity, we focus on connected components (0-dimensional homology), which are the most straightforward to analyze $Pers_i^{\\alpha}(X) = \\sum_{(b,d)\\in D_0(X)} d^0$, which is actually the length of the minimum spanning tree (MST) for the graph on X, and the edges corresponding to the birth-death pairs in $D_0(X)$ constitutes a MST in X (Koyama et al., 2023). Thus, to determine $Pers_0^{\\alpha}(X)$, we can alternatively focus on the MST and avoid the costly constructions of VR filtrations.\nAlthough $Pers_i^{\\alpha}(X)$ is the length of the MST, calculating it theoretically is still very challenging (Aldous & Steele, 1992), and the impact of the scattering of the data on the MST remain theoretically unclear. Consequently, we opt to use Monte Carlo simulations to investigate how the scattering of the logit impact its 0-th order TP $Pers_0^{\\alpha}(X)$.\nFig. 5 illustrates the simulated MST length for 500 and 1,000 points in a 10-dimensional simplex as the parameters $a_s$ and the ratio r vary. In fact, when the PCP is more scattered (i.e., lower $a_s$ and r), the length of the MST increases. Thus, the adversarial scattering assumption implies the adversaries reduce the logits' concentration and results in a higher $Pers_0^{\\alpha}(X)$. This is consistent with results in Sect. 3.2 and facilitates the use of TP to detect adversaries."}, {"title": "4. Maximum Mean Discrepancy with Topological Features", "content": "In this section, we demonstrate how to utilize the topological signatures identified in Sect. 3 for adversarial detection. Following (Gao et al., 2021; Grosse et al., 2017), we aim to address the Statistical adversarial detection (SAD) problem:\nLet $X \\subseteq \\mathbb{R}^d$ and let P be a Borel probability measure on X. Consider a dataset $S_x = \\{x_i\\}_{i=1}^n \\sim P^n$ composed of i.i.d. samples drawn from P, and let $f : \\mathbb{R}^d \\rightarrow \\mathbb{C}$ denote the true labeling mapping for samples in P, where $\\mathbb{C}$ is the set of labels. Suppose that adversaries have access to a classifier f trained on Sx and i.i.d. samples S'x from P. The objective is to determine whether a dataset $S_y = \\{Y_i\\}_{i=1}^m$ is originated from the distribution P. We assume that Sx and S'x are independent and no prior information about the attackers is available. Sy may consist of either i.i.d. samples from P or non-i.i.d. samples generated by attackers.\nIn SAD, given a threshold a, when Sy is drawn from P, we want to accept the null hypothesis $H_0$ (Sx and Sy are from the same distribution) with probability 1 a. Conversely, if Sy includes adversarial samples, our objective is to reject $H_0$ with a probability approaching 1. It is important to note that the classifier f examined in our study is a zero-shot classifier based on multimodal alignment, rather than conventional feed-forward neural networks. We focus on the zero-shot problem because it is closely related to the alignment between modalities in multimodal systems.\nTopological Features: Our approach utilizing $L_{TC}$ for SAD is to compute sample-level features derived from the topological loss $L_{TC}$, i.e., the topological features are the gradients of the TC loss to each input features $Y = \\nabla_Y L_{TC}(Y, T)$, where Y represents the image's logits and T denotes the text embedding. Intuitively, the gradients capture how each image's feature contributes to the changes in the topological image-text alignment. The gradients are computed by back-propagating Eq. 3 and 5 via Pytorch's implementations of the homologies (AidosLab, 2023)."}, {"title": "5. MMD Experimental Results", "content": "Settings and baselines: We validate the advantage of topological features for MMD in the settings stated in Subsect. 3.2. We compare TPSAMMD and MKSAMMD tests with 4 existing two-sample tests: 1) SAMMD (Gao et al., 2021), 2) Mean Embedding (ME) test (Jitkrittum et al., 2016); 3) Smooth Characteristic Functions (SCF) test (Chwialkowski et al., 2015); and 4) Classifier two-sample test (C2ST) (Liu et al., 2020).\nEach MMD test is conducted on two disjoint subsets of clean and adversarial samples, each containing 50 images for CIFAR10 and CIFAR100, and 100 images for ImageNet. The sizes of the holdout data Z for the topological features computation (Eq. 6) are 1000 and 3000 for CIFAR10/100 and ImageNet, respectively. Notably, this is significantly more challenging than existing experiments (Gao et al., 2021), which differentiate between sets of 500 samples. Each test was conducted over 100 trials with Type-I error controlled at a = 0.05. Due to the page limit, results on CIFAR100 and more in-depth experimental results are provided in Appx. E."}, {"title": "6. Conclusion and Future Work", "content": "This study studies the vulnerability of multimodal ML systems, such as CLIP and BLIP, to adversarial attacks by exploring the topological disruptions in text-image alignment. We introduced two novel TC losses to identify distinctive signatures of adversaries, and demonstrate that these losses exhibit consistent monotonic changes across various attacks. By integrating them into MMD tests, we developed new adversarial detection methods that significantly enhance detection accuracy. This approach not only deepens our understanding of multimodal adversarial attacks but also provides practical tools to strengthen their resilience.\nFuture work will explore these topological analysis to other multimodal configurations, and demonstrate the potential of topological methods in enhancing the robustness and reliability of multimodal ML systems."}, {"title": "Impact Statement", "content": "This work addresses a gap in the defense of multimodal ML systems against adversarial attacks. By introducing novel Topological-Contrastive losses and leveraging persistent homology, this study not only deepens our understanding of how adversarial attacks disrupt text-image alignment but also provides practical methods to detect such disruptions with high accuracy. As Multimodal ML systems are increasingly used in sensitive domains, enabling more precise detection will enhance the security and resilience of systems that directly impact public safety, health, and trust in technology. Furthermore, the monotonic properties of the proposed losses provide interpretable insights into adversarial behavior, promoting transparency and trustworthiness in adversarial defense strategies.\nWhile this work provides powerful tools to detect and mitigate adversarial threats, it also necessitates careful ethical considerations. The methods developed here could potentially be misused to generate more advanced adversarial attacks, exacerbating the arms race between attackers and defenders. To mitigate this risk, we advocate for responsible dissemination of these techniques and call for collaboration across the research community to ensure their ethical application. Future directions include extending these topological methods to other multimodal configurations, such as video-text and audio-text systems, further solidifying the role of topological insights in enhancing the robustness of multimodal ML systems. This research demonstrates the transformative potential of topology-based approaches in securing the next generation of ML systems and sets a strong foundation for future advancements in the field."}, {"title": "A. Experimental settings and results on Total Persistence and Multi-scale Kernel", "content": "Our experiments were conducted on a cluster with nodes featuring four NVIDIA Hopper (H100) GPUs each, paired with NVIDIA Grace CPUs via NVLink-C2C for rapid data transfer essential for intensive computational tasks. Each GPU is equipped with 96GB of HBM2 memory, ideal for handling large models and datasets.\nWe evaluated five CLIP models, six attack methods, and three datasets. We employed torch-attack (Kim, 2020) to generate adversarial perturbations with magnitudes $\\epsilon$ of $1/255$, $2/255$, $4/255$, and $8/255$. The Square attack for ImageNet was excluded due to its high computational complexity, which hinders generating a sufficiently large dataset for reliable topological data analysis. Comprehensive results are presented in Figs 10, 11, 12, 13, and 14. Specifically, we report the TP and MK losses as defined in Eq.3 and Eq.5, respectively, against the adversary ratio. For each experiment, we processed all 10,000 test samples to generate adversarial examples, retaining only those that successfully disrupted CLIP alignment. Starting with the full set of clean samples, we randomly replaced samples with adversaries according to each adversarial ratio (x-axis), computed the corresponding TC losses, and presented the results in the figures.\nUnlike the main manuscript, which presents results only for an attack magnitude of $\\epsilon = 4/255$, we provide comprehensive results across various models and attack magnitudes. Except for FGSM, the TC loss shows monotonic behavior across different models, attack methods, and magnitudes. We hypothesize that FGSM's simplicity leads to fewer successful samples and larger distortions, resulting in unrealistic samples and disrupting the original point clouds differently than more sophisticated attacks. This simplicity affects the statistical outcomes of our computations, distinguishing FGSM from more refined attack methods. Additionally, we observe a trend that warrants future research: more sophisticated attack methods and advanced models exhibit more pronounced monotonic behavior in TC losses. This is evident when comparing TC losses of the ViT-L family to ViT-B, ResNet101 to ResNet50, and PGD/AA to FGSM."}, {"title": "B. Modeling Logits as Poisson Cluster Process", "content": "In Subsect. 3.3, we propose to use Poisson Cluster Process to model the logits resulting from the embedding modules in multimodal alignments. We now provide the details formulation of our PCP modeling.\nPoisson Cluster Process: Our PCP modeling is constructed from a Parent Simplex. Then, the actual point cloud is generated by sampling the Children Clusters surrounding the vertices of that simplex. In particular, the process is described as follows:\n\u2022 Parent Simplex: A K-dimensional simplex is the convex hull of K + 1 affinely independent points (called parents points) in $R^K$. Let those parents points of the simplex be V = $\\{v_0, v_1,..., v_K\\}$, the simplex is given as:\n$S = \\{x \\in R^K \\qquad | \\qquad x = \\sum_{i=0}^K \\lambda_i v_i,  \\sum_{i=0}^K \\lambda_i = 1\\}$\nAny point x inside the simplex is a convex combination of its vertices.\n\u2022 Children Clusters: The logits are modeled as points clustering around the simplex vertices. For vertex $v_i$, $N_i$ points are generated using coefficients $\\lambda_i$ sampled from a Dirichlet distribution parameterized by $\\alpha_i = \\{\\alpha_{i,j}\\}_{j=0}^K$:\n$f_{\\alpha_i}(\\lambda_0, \\lambda_1,...,\\lambda_K) = \\frac{1}{B(\\beta)} \\prod_{j=0}^K  \\lambda_j^{\\alpha_{i,j}-1}$\nwhere $B(\\beta)$ is the multivariate Beta function $B(\\beta) = \\prod_{j=0}^K \\Gamma(\\beta_j)/\\Gamma (\\sum_{j=0}^K \\beta_j)$ and $\\Gamma (.)$ is the gamma function. Each generated point z is then computed as $x = \\sum_{j=0}^K \\lambda_j v_i$.\nThis construction captures the clustering behavior of logits of K + 1 labels and provides a basis for understanding how adversarial perturbations might increase topological complexity by introducing distortions in the cluster distribution. Intuitively, for each vertex i, a larger $\\alpha_{i,i}$ and smaller $\\alpha_{i,j}(j \\neq i)$ increases the probability that $\\lambda_i$ is close to 1 and encourages points of cluster i stay nearer to $v_i$. Additionally, a larger overall $\\alpha_{i,j}$ will increase the concentration of $\\Lambda$ at the"}, {"title": "C. Computation of Topological Feature", "content": "Algorithm 1 outlines the pseudocode for computing the TP and MK losses. Given a point cloud Y and a reference point cloud T, the algorithm calculates the topological feature $Y = \\nabla_Y L_{TC}(Y \\cup Z, T)$ as defined in Eq. 6. In our MMD test, Y and Z represent the logits or embeddings of the examined images (both clean and adversarial) and a hold-out image from the clean data, respectively. On the other hand, T denotes the embeddings of the label classes for the corresponding datasets: the 1000 class labels of ImageNet, 100 class labels of CIFAR100, or 10 class labels of CIFAR10."}, {"title": "D. Semantic-Aware Maximum Mean Discrepancy", "content": "Although MMD(P, Q; Hk) is a perfect statistic to check if P and Q are the same, its empirical test power depends significantly on the used kernels (Sutherland et al., 2016; Liu et al., 2020). Semantic-Aware Maximum Mean Discrepancy is a MMD test introduced by (Gao et al., 2021), which utilizes the following kernel acting on semantic features extracted by a well-trained classifier on clean data:\n$k_w (X_{input}, Y_{input}) = [ (1 - \\epsilon_0) \\kappa_f (X_{input}, Y_{input}) + \\epsilon_0] \\kappa_{G} (X_{input}, Y_{input})$\nwhere $sf(X_{input}, Y_{input}) = \\kappa(\\phi_f (X_{input}), \\phi_f (Y_{input}))$ is a deep kernel function that assesses the similarity between $X_{input}$ and $Y_{input}$ using features extracted from the last fully connected layer of the classifier f. Here, $\\kappa$ denotes the Gaussian kernel with bandwidth $\\sigma_{\\phi_f}$, $\\epsilon_0 \\in (0,1)$, and  $\\kappa_{G} (X_{input}, Y_{input})$ represents the Gaussian kernel with bandwidth $\\sigma_q$. Given that kernel, the SAMMD is proposed to measure the discrepancy between natural and adversarial data:\nSAMMD(P, Q) = E [kw(X, X') + kw(Y,Y') \u2013 2kw(X,Y)],\nwhere X, X' ~ P and Y, Y' ~ Q. The U-statistic estimator used to empirically approximate SAMMD(P, Q) is given by:\nSAMMD^2 (S_X, S_Y ; k) = \\frac{1}{n(n - 1)} \\sum_{i \\neq j} H_{ij},\nwhere\nHij = kw(xi, xj) + kw(Yi, Yj) \u2013 kw(Xi, Yj) \u2013 kw(Yi, xj)."}, {"title": "E. MMD Experimental Results", "content": "This appendix provides the complete results of our Maximum Mean Discrepancy (MMD) experiments on ImageNet (Fig. 16 and 17), CIFAR-10 (Fig. 18 and 19), and CIFAR-100 (Fig. 20 and 21). Specifically, we report the test power and Type-I error rates of various methods for detecting adversarial samples in batches. Each test batch consists of 50 clean or adversarial samples.\nNotably, this setting is significantly more challenging than existing works (Gao et al., 2021), which use test batches of size 500. As a result, several methods, such as ME and SCF, perform considerably worse than previously reported. We chose this more demanding setting because the state-of-the-art method, SAMMD, already achieves 100% test power with 500-sample batches. By reducing the batch size to 50, we emphasize the advantages of our topological features in a more challenging scenario.\nThe results demonstrate that SAMMD-related methods achieve highly competitive test power across a wide range of models and attack methods. This outcome aligns with previous findings in unimodal settings (Gao et al., 2021). This consistency motivated us to integrate our topological features into SAMMD, showcasing the advantages of our approach in practical scenarios. As shown, TPSAMMD consistently performs as well as or better than SAMMD in nearly all settings, particularly against PGD and AA attacks and within more complex Vision Transformer models. On the other hand, although MKSAMMD performs slightly worse than TPSAMMD, it still offers improvements over SAMMD in many configurations.\nThe code used in this study is currently under review for release by the organization. We are awaiting approval, and once granted, the code will be made publicly available."}]}