{"title": "DOES CHATGPT HAVE A MIND?", "authors": ["Simon Goldstein", "B.A. Levinstein"], "abstract": "This paper examines the question of whether Large Language Models (LLMs) like ChatGPT possess minds, focusing specifically on whether they have a gen- uine folk psychology encompassing beliefs, desires, and intentions. We approach this question by investigating two key aspects: internal representations and dis- positions to act. First, we survey various philosophical theories of representation, including informational, causal, structural, and teleosemantic accounts, arguing that LLMs satisfy key conditions proposed by each. We draw on recent inter- pretability research in machine learning to support these claims. Second, we explore whether LLMs exhibit robust dispositions to perform actions, a neces- sary component of folk psychology. We consider two prominent philosophical traditions, interpretationism and representationalism, to assess LLM action dis- positions. While we find evidence suggesting LLMs may satisfy some criteria for having a mind, particularly in game-theoretic environments, we conclude that the data remains inconclusive. Additionally, we reply to several skeptical challenges to LLM folk psychology, including issues of sensory grounding, the \"stochastic parrots\u201d argument, and concerns about memorization. Our paper has three main upshots. First, LLMs do have robust internal representations. Second, there is an open question to answer about whether LLMs have robust action dispositions. Third, existing skeptical challenges to LLM representation do not survive philosophical scrutiny.", "sections": [{"title": "1 Introduction", "content": "Recent developments in AI are stunning. Large language models like ChatGPT can generate text that is fluent, accurate, and responsive to human questions. These advances have sparked a fundamental question: Do these AI systems possess minds?\nTo address this broad question, we focus on a more specific aspect of the mental: folk psychology. The key question we explore is whether LLMs have beliefs, desires, and intentions. In other words, do LLMs have goals about what to do, a perspective on what the world is like, and plans for achieving their goals given what the world is like?\nWhy care about LLM folk psychology? There are at least three reasons. First, folk psychology is the primary lens that humans use to understand the behavior of agents. When we interact with one another, we consistently try to explain what is happening in terms of beliefs and desires (Hutto and Ravenscroft (2021)). As LLMs become increasingly capable, we will increasingly interact with them. It is worth figuring out whether we can genuinely use beliefs and desires to understand these interactions, or whether instead beliefs and desires would be at best an elaborate metaphor. Second, folk psychology is relevant for moral patiency. When we ask what it takes to be the kind of entity that can be harmed, one important answer appeals to the satisfaction of desires (Heathwood (2016)). More may be required too, such as full-fledged consciousness; but folk psychology will play a role. Third, folk psychology is relevant to AI safety. As AI systems become more powerful, many have worried that they may systematically pursue goals that conflict with humanity (Russell (2019)). But much of this discussion implicitly assumes that AI systems will have goals and a perspective about how to achieve those goals. If there are important barriers for AI systems to possess a folk psychology, this may complicate our understanding of what it would take for AI systems to be safe.\nOur approach is simple. The question of folk psychology has two key aspects: internal represen- tations and dispositions to act. We'll explore in detail whether LLMs possess each aspect of a folk psychology.\nThe first key question is whether LLMs have robust internal representations of the world. Here, our strategy in \u00a72 will be to survey various philosophical theories of representation, and see whether LLMs satisfy the theories. We'll look at a range of conditions on representation, including: (i) that the system has internal states that carry information about the world; (ii) that these internal states are causally effective at producing the system's behavior; (iii) that these internal representation satisfy folk patterns of reasoning; (iv) that the structure of these internal representations mirrors the structure of what they represent; and (v) that the information-carrying capacity of these representations emerged from some kind of selective, evolutionary process. In each case, we'll draw on recent research in machine learning about LLM interpretability, which suggests that these systems satisfy the relevant condition on mental representation.\nBut internal representations alone are not sufficient for a full-blown folk psychology. In order to possess beliefs and desires, the second key question is whether LLMs have robust dispositions to perform actions. If LLMs have both internal representations and action dispositions, then they will have a folk psychology. In particular, nearly every theory of belief and desire will explain these mental states in terms of some combination of internal representations and dispositions to act. In \u00a74, we explore two of the most prominent traditions of theorizing about belief and desire, interpretationism and representationalism. In each case, we suss out what kinds of action dispositions the theory requires of LLMs. Then we critically assess whether LLMs satisfy the relevant condition. The crucial question for LLMs will be whether their linguistic outputs are stable enough to be best explained as promoting goals. Our conclusion in \u00a74 will be tentative. We argue that the behavior of LLMs in game environments is suggestive of the kinds of rich plans of action required for belief/desire psychology. But the data is not decisive.\nOur third goal of the paper, in \u00a73, will be to refute some of the existing skeptical challenges to LLM folk psychology. Here, we'll engage directly with three challenges. The first challenge, symbol grounding, raises the following question: if the only inputs to a language model are strings of text, rather than rich perceptual experiences or feedback from motor actions, how can language models understand prompts about the external world? The second challenge is that LLMs do not represent the world because they are not trained to do so; instead, they are merely stochastic parrots, trained to predict the next word. The third challenge is that LLMs do not represent the world because their behavior can be fully explained by an alternative theory, according to which"}, {"title": "1.1 Large Language Models", "content": "At the heart of modern LLMs lies the transformer architecture (Vaswani et al. (2023)). Although many variants exist, we'll here focus on the mechanics of decoder-only models such as GPT 3.5.1\nWhen you feed a prompt to a model, it makes a prediction about what comes next. For example, if you feed it \u201cThe cat sat on the,\u201d it will assign a probability to each possible next token.2 It will assign some probability to \u201caardvark,\u201d some probability to \u201cbanana,\u201d some probability to \u201cmat,\u201d and so on.\nTo compute these probabilities, each token is converted into an initial embedding\u2014a vector, or long list of numbers, which encode \"The,\u201d \u201ccat,\u201d \u201csat,\u201d \u201con,\u201d and \u201cthe.\u201d The initial embedding carries information about the corresponding token, but it doesn't at first carry any information about surrounding tokens. For example, the initial embedding for \"cat\" does not encode the fact that \"The\" precedes it.\nThe initial embeddings for each token are then transformed and updated across a large number of layers. At each layer, the embedding for a given token is first updated through the mechanism of self-attention. Self-attention allows the embedding for a given token to \u201cattend to\u201d itself and earlier tokens in the sequence. For example, the token for \u201csat\u201d might attend to \u201ccat\u201d at a given layer. The embedding for \u201csat\u201d could then be updated to represent the fact that \u201ccat\u201d was the immediately preceding token or, perhaps, to encode somehow that \u201ccat\u201d was the subject. In other words, after paying attention to the information of earlier tokens and itself, the embedding for \"cat\" is updated to include contextual information about the surrounding tokens in the prompt. We call this new embedding a contextual embedding. These new embeddings are then refined further using something like a multi-layer perceptrons (MLPs) before being fed forward into a new layer.\nAfter the embeddings are passed through all the layers of the model, the model uses the final contextual embedding for a token to predict what the next token will be. To generate more and more text, we can then select some token assigned relatively high probability, tack it onto the initial prompt, and then feed the new augmented sequence to the model again. For an illustration, see fig. 1.\nTransformer models get extremely good at generating plausible text via training. Training for retail models like ChatGPT comes in two main phases. In the first phase, we take lots of pre-existing text, feed an initial segment of it to the model, and then have the model generate probabilities about what comes next. We then tweak the parameters of the model via gradient descent to make more accurate predictions. For instance, if the model is fed \u201cHappy families are all alike; every"}, {"title": "2 Theories of representation", "content": "Can LLMs represent the world? In this section, we explore this question by examining various philosophical theories of representation. We aim to demonstrate that LLMs meet many of the criteria set by these theories. In particular, we'll highlight a series of recent studies in AI inter- pretability research and related behavioral research that connect closely to various philosophical theories of representation.\nWe focus primarily on naturalistic theories of representation. These theories explain representation in terms of physical processes. They differ from other theories, such as those proposing that representation is primitive (Boghossian (1990)) or dependent on primitive phenomenal properties (Graham et al. (2007)). While non-naturalist theories may allow for LLM representation, it is difficult to say whether AIs could have primitive representational or phenomenal properties.\nOur focus is on whether LLMs have internal states that represent the world by having truth conditions. A representation can truly depict the world if the world is a certain way and falsely if it is another. We will also consider if LLMs have internal states that refer to objects or properties in the world.\nImportantly, our main interest is not the text produced by LLMs but whether LLMs have mental states that represent the world. Our hypothesis is that the activations of LLMs\u2014the patterns of internal neural activity within the network as it processes information-refer to objects in the world and have truth conditions. These activations, which can be thought of as the temporary, computation-specific states of the network, are distinct from the more permanent weights that encode the model's learned knowledge. This question of representation is an important first step in determining whether LLMs have a robust folk psychology with beliefs and desires.\nWith these questions in mind, we will survey leading theories of mental representation to see if LLMs meet their criteria. We draw on existing surveys, including Adams and Aizawa (2021) and Schulte (2023). Our goal is to show that, according to these leading theories, there is strong evidence that LLMs indeed represent the world.\nWe will consider five key conditions on mental representations, and argue that LLMs satisfy each one:\n\u2022 Information carrying: Informational theories posit that mental representation requires internal states to carry information about the external world, typically through probabilis- tic connections. We demonstrate how recent advances in AI interpretability, particularly in probing techniques, provide compelling evidence that LLM internal embeddings carry such world-relevant information.\n\u2022 Causal efficacy: Fodorian theories demand that representational states be causally effective in generating system behavior. We present evidence from recent interpretability studies showing that LLM outputs indeed depend counterfactually on their internal embeddings, satisfying this causal requirement."}, {"title": "2.1 Information", "content": "A long tradition of work on representation has appealed to the concept of carrying information. Smoke carries information about fire, mumps carry information about measles, and thermometers carry information about temperature. Dretske (1981) and others have argued that a system can only represent the world if that system carries information about it.\nPhilosophers have disagreed about how exactly to define carrying information, but in general different analyses all appeal to probabilistic concepts. According to Dretske, a state carries the information that p if and only if the probability of p given the state is 1, provided that various background conditions obtain. Some theorists have instead focused on states raising the probability of p, rather than making it certain (Usher (2001)). Other theorists have focused on more general conditions involving entropy.4\nHow can we tell whether LLMs carry information about the world? Harding (forthcoming) argues there is a close connection between informational approaches to representation and recent work on probing in AI interpretability research (Alain and Bengio (2018)). In probing, researchers train a separate classifier to take activations as input and make a prediction. The probe takes in some activations and predicts features of the input. For example, in a visual AI system, a probe might predict whether the system is looking at a cat based solely on activations, without access to the original input.5\nA compelling example of using probes to discover LLM representation comes from Li et al. (2022). We'll use this example as a case study below. Li et al trained an LLM on sequences of moves in the 8x8 board game Othello, using only lists of moves (like F5 D6 C3 D3 C4 F4 E3) without describing the rules or the board. Othello is a simpler game than chess, but there are far too many possible moves in general for an LLM simply to memorize all legal game states. Despite this, the trained LLM, dubbed Othello-GPT, tended to output legal moves with high probability.\nTo understand how, the authors used probes to identify possible internal representations of the board. By comparing the model's internal states to the actual board, they trained probes to guess whether each of the sixty-four squares was black, white, or blank. The probes achieved remarkable accuracy with an error rate of only 1.7%. Similar probing techniques have been used"}, {"title": "2.2 Causal powers", "content": "Fodor (1975) imposed a series of conditions on genuine representations, one of which is that they must have genuine causal powers. The key question here is whether LLM outputs are robustly caused by the activations discovered by probes or whether these apparent representations are actually epiphenomenal.\nTo make this concern concrete, return to Othello-GPT. Suppose Othello-GPT itself does not represent the state of the board at all. However, a clever probe could read off from its activations alone what the history of moves was. For instance, if F5 D6 C3 D3 C4 F4 E3 were fed to Othello-GPT, the probe could learn just from its internal state that F5 D6 C3 D3 C4 F4 E3 was the input. If the probe also learned (via its own training) what the rules of Othello were, it could"}, {"title": "2.3 Folk patterns of reasoning", "content": "So far, we've argued that LLM activations carry information and causally influence LLM outputs. But this alone may not be enough for genuine representation. For example, Fodor (1975) argued that genuine mental representations have to have special kinds of causal powers: the representations have to influence the system's behavior in ways that match the laws of folk"}, {"title": "2.4 Structure", "content": "Structuralist theories of mental representation propose that mental states represent the world only if their internal structure mirrors the structure of the external world (Opie and O'Brien (2004)). This mirroring creates a network of relationships within the mental states that correspond to the relationships between the objects or properties they represent. As an analogy, maps represent geographic areas by containing a series of symbols whose physical relations on the page mirror the physical distances between locations.\nStructuralist theories of mental content are particularly relevant to large language models. In- terpretability researchers have explored the activations of LLMs to see whether they stand in patterns of relations with one another that are isomorphic to worldly features.\nPatel and Pavlick (2022) tested GPT-3's concepts of direction and color. In the case of cardinal directions, they exposed GPT-3 to a series of gridworlds, consisting of matrices filled with Os and a single 1. The task was to identify the direction of the 1 symbol in the gridworld: left,"}, {"title": "2.5 Teleosemantics", "content": "Another potential necessary condition on representation comes from 'teleosemantic' theories of mental content. According to teleosemantic theories, the function of the underlying system generating a mental state determines its content (Millikan (1984)). For example, whether a given mental state counts as a representation of a goldfinch depends on the function of the system that created that mental state. On standard teleosemantic views, the function of a state is determined by evolution: the state has whatever function explains why it was selected for via natural selection (see (Schulte 2023, p. 35)).\nTeleological conditions can be combined to causal and informational conditions on representation. For example, Stampe (1977)'s causal account of content appealed to the idea of causal correlations that obtain in normal conditions. This can be fleshed out in terms of evolution: a perceptual state represents a feature of the world if that state being reliably caused by the feature helps explain why the organism was selected for in natural selection. Similarly, Neander (2013, 2017) argues that perceptual states have content when they have the function of being reliably produced by features of the environment. For example, toads have evolved perceptual states that fire in response to \u201csmall, dark, moving\u201d objects. These states were selected for by natural selection, because toads with these states were more successful at hunting flies. For this reason, these perceptual states represent small, dark, moving objects. Similarly, informational accounts can be supplemented with the condition that various information-carrying channels were selected for (see Dretske (1981)).\nWe argue that teleological constraints on meaning are broadly compatible with AI representation. Although LLMs are not products of biological evolution, they undergo a form of artificial selection during their training process. This selection process optimizes the model's parameters\u2014"}, {"title": "3 Responding to Skeptical Challenges", "content": "At this point in the paper, we've laid out our positive claims about LLM mental representation. In short, we think that there is a strong case that LLMs possess robust internal representations of the world.\nIn this section, we'll respond to three skeptical challenges to LLM folk psychology. Each skeptical challenge targets the claim that LLMs represent the world. (In the next section, we turn to challenges to LLMs possessing robust action dispositions.) In each case, we'll identify potential gaps in the skeptical challenge, and consider how the skeptical challenge is potentially relevant to questions about LLM folk psychology.\nWe'll consider three challenges:\n\u2022 Sensory grounding: This challenge says that LLM inputs are meaningless because LLMs have no connection to the external world. In response, we'll argue that (i) the challenge relies on overly simplistic causal constraints on mental representation; (ii) the challenge ignores the possibility that LLMs form hypotheses about aspects of the world that are not directly observable to them; and (iii) the challenge is fragile, because it relies on properties of LLMs (such as the lack of perceptual inputs) that are not shared by all models.\n\u2022 Stochastic parrots: This challenge says that LLMs do not represent the world because they aren't trained to do so. In response, we'll argue that (i) the challenge overgeneralizes to threaten human cognition; (ii) the challenge ignores that LLMs could represent the external world as a means to predicting the next word; (iii) the challenge ignores the possibility that representation is an emergent capability of LLMs; and (iv) the challenge"}, {"title": "3.1 Sensory Grounding", "content": "One skeptical challenge concerns sensory grounding. This skeptic says that LLMs don't represent the world because they lack sensory grounding and merely use text: \u201ca system that is trained only on form [such as an LLM] would fail a sufficiently sensitive test [for intelligence], because it lacks the ability to connect its utterances to the world\u201d (Bender and Koller (2020) p. 5188). Pure (text-only) LLMs only see patterns of text. They do not have any outside sensory input. Therefore, according to this challenge, they can't \"break the syntactic circle\" and connect any of the symbols they see to the outside world. As Harnad (1990) defined it, the symbol grounding problem is one of how the semantic interpretation of symbols can be \u201cintrinsic to the system, rather than just parasitic on the meanings in our heads.\"\nOne version of the symbol grounding problem involves causal theories of mental representation. For some theories of mental representation, the right sort of causal connection between the outside world and internal states is required for such states to count as representational in the first place. According to these theories (including Stampe (1977) and Fodor (1987)), mental states represent the world when they are causally connected to the world in the right way.12 For example, my perceptual experiences of cats tend to be caused by cats, and therefore represent them. In addition, my desire to eat ice cream tends to cause me to eat ice cream, and therefore represents ice cream. Causal theories of representation can explain, for example, why a photograph of an identical twin represents one twin and not the other, despite resembling each twin perfectly.\nNaive causal conditions may make trouble for LLM representation. LLMs have activations that are related to cats. But these activations are not directly caused by cats in the way that our perceptual system directly responds to cats in our environment. Instead, LLMs have learned about cats through training on text, and this text itself has been caused by cats.\nThere are two ways to address causal skepticism about LLM representation. The first option is to appeal to long causal chains. The second option is to appeal to pluralism about causal structure. Let's consider each in turn."}, {"title": "3.2 Stochastic Parrots", "content": "Another skeptical challenge to LLM representation is associated with the \u201cstochastic parrots\" argument, originally proposed by Bender et al. (2021). This view contends that LLMs, despite their impressive outputs, are merely sophisticated statistical pattern matchers rather than systems capable of genuine understanding or representation. The core claim of the stochastic parrots argument is that LLMs are trained solely to predict the next word in a sequence, without any true comprehension of the content they generate. According to this view, LLMs don't represent or reason about the world; instead, they simply reproduce patterns from their training data in a statistically sophisticated but fundamentally meaningless way. Proponents of this view often point to cases where LLMs produce fluent but nonsensical or contradictory outputs. For instance, an LLM might confidently assert a false statement or agree with contradictory premises in different conversations. These behaviors, they argue, reveal that LLMs lack genuine understanding and are merely \"parroting\u201d patterns from their training data.\nOne way to understand the stochastic parrot challenge is in terms of the teleosemantic conditions on representation we discussed earlier. Those conditions required that the internal states of LLMs"}, {"title": "3.3 Memorization", "content": "Another skeptical challenge concerns memorization. According to this challenge, we don't need to posit genuine reasoning in LLMs, because we can explain their behavior in another way. Instead of reasoning about questions, LLMs simply memorize great quantities of data and then use shallow heuristics to generalize to new prompts. They are trained on billions of sentences, and in the course of this training, they simply store the answers to a large number of questions. These answers are then returned in response to prompts, without any genuine reasoning taking place.\nWhether memorization threatens LLM representation depends on what is required for representa- tion. If representation is simply a matter of carrying information, then memorization is perfectly compatible with genuine LLM representation. On the other hand, if representation requires the presence of robust models or of reasoning about the world that matches folk psychology, then memorization may rule out representation.\nIn fact, there is a lot of evidence that LLMs do not merely memorize the answers to questions they are asked. Instead, LLMs seem to possess the ability to generalize robustly from their training data and then make correct predictions about new questions. They do not simply use shallow heuristics.\nReturn to the case of modular addition. As we discussed above, Nanda et al. (2023) found that during training, the LLM learns modular addition in multiple steps: an initial period of overfitting, based on memorization, followed by a transition to a general solution to the problem. In the initial phase of training, the LLM does engage in memorization. But after using memorization, the LLM learns a more general algorithm for computing the answer. After this algorithm is learned, the LLM then removes its memorization components. As evidence for this claim, Nanda et al found that in the initial period of training, the model achieved 100% accuracy on the training data, but low accuracy on the testing data. After 10,000 epochs of training, the model learned how to actually perform the task, and achieved high accuracy on the testing data. Overall, this suggests that LLM skeptics are missing out on much of the rich structure of LLM reasoning.\nSimilar abilities to generalize were found in the Othello experiment. Every game of Othello starts with one of four moves. Each initial move creates a 'quadrant' of Othello game space. To train Othello, the experiments only included training data from 3 of the 4 quadrants. But they found that the model was equally successful at playing Othello in the omitted quadrant of game space."}, {"title": "4 Action and Folk Psychology", "content": "While we've argued that LLMs have mental representations, the question remains: do they have a robust folk psychology? To address this, we need to consider whether LLMs have beliefs about the world, desires they aim to satisfy, and intentions that guide their actions. This question is complex, involving issues of stability, coherence, and goal-directed behavior.\nTo approach this question, we'll examine two influential philosophical perspectives on folk psychology: interpretationism and representationalism. Each offers a different framework for understanding mental states and presents different challenges when applied to LLMs."}, {"title": "4.1 Interpretationism", "content": "The most radical view is interpretationism. The idea behind interpretationism is that folk psychol- ogy is for explaining behavior. All that is required to have a folk psychology is for the system to behave in sufficiently complex ways best explained by appeal to folk psychological states. When this happens, the system has beliefs and desires: the system's desires are the goals promoted by its actions, and its beliefs are the views about the world that are required to be true in order for its actions to promote its goals. Whether it has internal states of a certain kind is not directly"}, {"title": "4.2 Representationalism", "content": "Representationalism, advocated by philosophers like Jerry Fodor and Fred Dretske, holds that having mental states requires having internal representations with appropriate functional roles. On this view, to have a belief that p, a system must have an internal state that represents p and plays the right causal role in the system's cognitive economy.\nThere is cause for optimism on the representationalist picture. As we've argued at length, some LLM mental states can have truth-conditions. Furthermore, LLMs even appear to have some sorts of world models and structured representations of conceptual domains. 17\nHowever, even on representationalist pictures like Fodor's, beliefs have to play the right role in the larger system and generally cannot be divorced entirely from desires. Even if LLMs have rich internal representations, it's not clear that these play the right kind of role in generating behavior to count as beliefs or desires. The issue of instability resurfaces here\u2014if internal representations don't stably guide behavior across different prompts, can they really count as beliefs?\nUltimately, we think matters are easier for the representationalist than the interpretationist. While we have some behavioral evidence in the case of LLMs, behavioral evidence is much more limited for LLMs than it is for humans. However, we have perfect internal access to LLMs, and we have much to discover about the role various representations play in LLMs' cognition. Therefore, as we come to understand more about how LLMs think, it will become more obvious for representationalists whether they have folk psychological mental states. Some key open questions for attributing folk psychological states on either picture, then, include:\n\u2022 Action and planning: How can we best understand LLM \"action\" given their limited affordances?\n\u2022 Stability: How can we reconcile the apparent instability of LLM outputs with the need for stable beliefs and desires?\n\u2022 Goal-directedness: Do LLMs have anything analogous to enduring goals or values?\nAddressing these questions will require a combination of philosophical analysis and empirical investigation. While there's evidence that LLMs have sophisticated internal representations and can exhibit complex, apparently goal-directed behavior, significant questions remain about whether they possess full-fledged folk psychological states. Resolving these questions will be crucial for understanding the capabilities, limitations, and potential moral status of these increasingly ubiquitous AI systems."}]}