{"title": "Resilient UAV Trajectory Planning via Few-Shot Meta-Offline Reinforcement Learning", "authors": ["Eslam Eldeeb", "Hirley Alves"], "abstract": "Reinforcement learning (RL) has been a promising essence in future 5G-beyond and 6G systems. Its main advantage lies in its robust model-free decision-making in complex and large-dimension wireless environments. However, most existing RL frameworks rely on online interaction with the environment, which might not be feasible due to safety and cost concerns. Another problem with online RL is the lack of scalability of the designed algorithm with dynamic or new environments. This work proposes a novel, resilient, few-shot meta-offline RL algorithm combining offline RL using conservative Q-learning (CQL) and meta-learning using model-agnostic meta-learning (MAML). The proposed algorithm can train RL models using static offline datasets without any online interaction with the environments. In addition, with the aid of MAML, the proposed model can be scaled up to new unseen environments. We showcase the proposed algorithm for optimizing an unmanned aerial vehicle (UAV) 's trajectory and scheduling policy to minimize the age-of-information (AoI) and transmission power of limited-power devices. Numerical results show that the proposed few-shot meta-offline RL algorithm converges faster than baseline schemes, such as deep Q-networks and CQL. In addition, it is the only algorithm that can achieve optimal joint Aol and transmission power using an offline dataset with few shots of data points and is resilient to network failures due to unprecedented environmental changes.", "sections": [{"title": "I. INTRODUCTION", "content": ""}, {"title": "A. Context and Motivation", "content": "Recent progress towards intelligent wireless networks em- braces efficient and fast decision-making algorithms. Machine learning / artificial intelligence (ML/AI) has gained further interest in 5G-beyond and 6G systems due to their powerful decision-making algorithms that can adapt to large and com- plex wireless networks [1], [2]. Reinforcement learning (RL) is one family of ML/AI that is known as the algorithm of decision-making. In RL, an agent observes the environment, makes decisions, and receives an award that evaluates how good the decision is in the current environment observation. A policy in RL describes what decisions to select at each ob- servation. RL aims to find the optimum policy that maximizes the received awards [3].\nTo this end, RL has shown great promise in a wide range of applications, such as radio resource management (RRM) [4], network slicing [5], unmanned aerial vehicle (UAV) [6] networks, connected and autonomous vehicle (CAV) net- works [7]. RL's power relies on RL algorithms' ability to handle model-free systems, where it is tough to formulate an efficient closed-form model to the system [8]. This applies to 5G-beyond and 6G systems, which are often very complex, have large dimensions, and are full of uncertainties. These characteristics of the wireless systems make RL algorithms fit most of the problems the wireless systems face [9]. In addition, the breakthrough in deep RL enables solving extraordinarily complex and large systems by combining deep neural networks (DNNs) with traditional RL algorithms [10].\nOne example where RL and deep RL show superior benefits is autonomous UAVs' trajectory optimization. UAVs provide remarkable flexibility for gathering data from remote sensors and enhance communication by flying closer to the sensors, thereby increasing the likelihood of line-of-sight (LoS) com- munication [11]. Moreover, remote sensors, such as those used in smart agriculture networks, often have limited power supplies and are difficult to access for battery replacement, par- ticularly during adverse weather conditions [12]\u2013[14]. UAVs play a crucial role in conserving sensor power by reducing the distance between them. Scalable RL algorithms can optimize UAV trajectory and scheduling policies, even in dynamic and rapidly changing network environments [15], [16].\nDespite its high applicability to the wireless environment, RL and deep RL still face significant difficulties in real- world wireless systems [17]. First, almost all RL and deep RL algorithms designed for wireless communication applications are online. Online RL counts on continuous interaction with the environment to update the learned policies until converging to the optimum policy. However, online interactions might not be feasible in real-world scenarios. For instance, online interactions might be unsafe in some applications, such as UAV and CAV networks, where bad decisions can lead to hazardous consequences. In addition, online interactions might be costly and time-consuming in some applications, such as RRM and network slicing, where the algorithm spends large time intervals through a massive amount of online interaction to reach the optimum policy.\nSecond, RL algorithms are not scalable to multiple problems and dynamic environments. For example, optimizing an RL algorithm in a network with a specific number of devices can not be utilized in another network with a different number of devices [18]. Similarly, changing the characteristics of the environment, such as channel model, number of access points, and environment dimension, requires retraining the RL model from scratch, which wastes time and resources. Therefore,"}, {"title": "B. Offline RL and Meta-Learning", "content": "Offline RL [17] is a family of RL that was proposed to over- come the problems of online RL in real-world applications. It suggests using an offline static dataset collected previously using a known behavioral policy to find the optimum policy. However, deploying existing RL and deep RL algorithms offline using static datasets without online interaction with the environment often fails to converge. This happens due to a distributional shift between the existing actions in the offline dataset and the learned actions. This problem is known as out-of-distribution (OOD) problem and leads to overestimating the learned policies. This problem is solved in online RL by selecting the OOD actions and correcting their overestimation.\nTo solve the distributional shift problems in offline RL, the authors in [21] propose conservative Q-learning (CQL). CQL builds over existing RL algorithms by adding a regularization parameter to the conventional optimization update to bound the influence of the OOD actions in the optimized policies. In contrast, this parameter does not affect in-distribution actions. In addition to its simple implementation over existing deep RL frameworks, such as deep Q-networks (DQNs), the CQL algorithm shows promising performance converging to the optimum policy without requiring any online environment visiting [22].\nApart from offline RL, current wireless systems suffer from scalability problems. Meta-learning is a family of learning algorithms that enable learning adaptability over adaptive and changing tasks. The most famous meta-learning algorithm is model-agnostic meta-learning (MAML) [23], which utilizes learning across multiple tasks to find the initialization parame- ters (initial neural network weights) that enable fast adaptation to new tasks through a few training iterations. This overcomes the problem of retraining the model from scratch whenever the environment changes or unpredictable conditions occur. In addition, few-shot MAML corresponds to performing fast adaptation on new tasks using a few shots of training data. This is useful when a limited amount of data is available for a task. MAML has been widely addressed in the wireless domain, such as in channel estimation, channel coding, symbol modulation and demodulation."}, {"title": "C. Related Work", "content": "Many works in the literature have recently adopted RL and deep RL in the wireless domain, specifically in UAV networks. Among these, the authors in [24] optimize the UAV optimal path to maximize the data collected from IoT devices. In [6], the authors propose a DQN algorithm that can jointly minimize AoI and transmission power of limited-power devices. The authors in [25] compare centralized and decentralized deep RL techniques using soft actor-critic (SAC) for trajectory plan- ning in integrated sensing and communications UAV network, where the work in [26] formulates a multi-objective deep RL algorithm for trajectory planning and beamforming design.\nApart from RL and deep RL, meta-learning has been fundamental in ensuring scalability in recent wireless appli- cations. Many works have leveraged meta-learning techniques in the wireless domain. For example, the work in [27] was among the first to investigate meta-learning approaches for wireless communication. It proposes a MAML algorithm for fast training an autoencoder designed for transmitting and receiving data over fading channels. The authors in [28] design fast downlink beamformers using transfer learning and meta- learning. The authors in [29] exploit meta-learning techniques with graph neural networks (GNNs) for fast wireless indoor localization. In [30], deep RL is combined with meta-learning to enhance the fast adaptability of the optimizing the allocation policy a dynamic V2X network, where [31] proposes a multi- agent meta-RL algorithm for trajectories design of multiple UAVs. The authors in [32] jointly minimize IoT devices' Aol and transmission power using a meta-RL algorithm that adapts quickly to environments with adaptive objectives.\nAlthough most existing RL-related works rely on online RL, offline RL has begun to get further attention in the wireless domain. The work in [33] was the first to introduce offline to the wireless domain. The authors formulate a distributional and offline multi-agent RL algorithm for planning the trajectories"}, {"title": "D. Main Contributions", "content": "This work proposes a novel meta-offline MARL framework tailored for adaptive and resilient decision-making in dynamic wireless environments. The main contributions of this paper are summarized as follows.\n\u2022 We consider the problem of optimizing the trajectory and the scheduling policy of a UAV serving limited power sensor nodes. We formulate the problem as a joint optimization problem to minimize the Aol and the transmission power. Different tasks are defined using the trade-off between Aol and transmission power.\n\u2022 We develop a meta-offline RL framework that integrates CQL with MAML to enhance sample efficiency and generalization, enabling rapid adaptation to new environ- ments with limited training data.\n\u2022 Using the MAML algorithm, we find the set of optimum initial parameters used for the CQL algorithm to be trained using a few shots of offline data points using a few stochastic gradient descent (SGD) steps.\n\u2022 Our proposed framework's training is resilience-aware enabling the agent to adapt to unpredictable network disruptions. Our framework ensures robust performance even in environments with adverse conditions, such as harsh weather-related link failures.\n\u2022 Simulation results show that the proposed model out- performs traditional deep RL approaches regarding the resulting reward function. The proposed algorithm con- verges faster than the CQL algorithm with random weight initialization. In addition, it achieves the minimum pos- sible Aol and transmission power combined compared to baseline models.\nThis is the first work to combine meta-learning with offline RL for the wireless communication domain. The rest of the paper is organized as follows. Section II formulates the problem model. Section III describes preliminaries, whereas Section IV proposes the meta-offline RL model. Simulation results are elucidated in Section V and Section VI concludes the article."}, {"title": "II. SYSTEM MODEL", "content": "Real-world applications in smart agriculture and environ- mental monitoring inspire the system model. Thus, consider the wireless network shown in Fig. 1 and assume a set K = {1,2,\u2026\u2026\u2026, K} of K limited-power, randomly deployed, IoT devices that monitor the agricultural/environmental pro- cess(es). We divide the network into L \u00d7 L cells, where each device is positioned in the center of a cell whose length is r, with a coordinate lk = (xk, Yk). The devices uplink their data to a fixed-velocity rotary-wing UAV flying with a velocity U at height h. The position of the UAV is projected to the 2D plane, whose coordinates, at time step t, are lu(t) = (xu(t), yu(t)). At each time step t, the UAV serves one of the devices, where w(t) = k indicates that the UAV chooses to serve device k. In addition, the UAV updates its current position using the movement vector v(t)\n$l_u(t+1) = \\begin{cases} l_u(t) + (0,r), & v(t) = (0, 1), \\\\ l_u(t) \u2013 (0,r), & v(t) = (0,-1), \\\\ l_u(t) + (r, 0), & v(t) = (1,0), \\\\ l_u(t) \u2013 (r, 0), & v(t) = (-1,0), \\\\ l_u(t), & v(t) = (0,0). \\end{cases}$  (1)\nWe assume a LoS between the UAV and the devices [37]. Thus, the channel gain between device k and the UAV is\ngk,u(t) = \\frac{g_0}{h^2 + ||l_k \u2013 l_u(t)||^2}, (2)\nwhere go is the channel gain at 1 m reference distance and ||lk \u2013 lu(t)||2 is the euclidean distance between device k and the UAV [38]. Then, the transmit power of device k at time t is formulated using the LoS channel gain as\nPk(t) = \\frac{\\sigma^2 M}{gk,u(t)} = \\frac{\\sigma^2 M}{g_0} (2^{\\frac{M}{B}} - 1) (h^2+||l_k-l_u(t)||^2), (3)\nwhere M is the size of the transmitted data, B is the band- width, and o\u00b2 is the noise power. Hence, the transmission power is directly related to the position of the UAV.\nWhen the UAV enters the heavy rain area, it is affected by an attenuation factor which influences the path loss [39], [40]. We follow the heavy rain attenuation model described in [41], which calculates the attenuation due to rain as YR = \u00a2 R\u00b0, where R is the rainfall intensity set as 12.5 mm/h. In addition, and are fitting parameters set as in [42].\nThe Aol quantifies how fresh the transmitted information is. It is measured as the time difference between the arrival time of a packet at a destination and its generation time at the source. Then, the Aol of device k at time t is updated as\nAk(t) = \\begin{cases} 1, & \\text{if } w(t) = k, \\\\ min{A_{\\text{max}}, A_k (t \u2212 1) + 1}, & \\text{otherwise}; \\end{cases} (4)\nwhere Amax is the maximum Aol in the system set to bound the complexity of the network. Hence, the Aol vector of all devices at time t is A(t) = [A1(t), A2(t),\u2026\u2026, \u0410\u043a(t)]."}, {"title": "A. Problem Definition", "content": "Often, precision agriculture applications require UAVs to perform tasks such as surveying large and/or remote areas under dynamic environmental conditions. RL-enabled UAVs can optimize their trajectories and data collection strategies to achieve application-specific goals.\nTherefore, our main objective is to find the optimum policy to jointly optimize the freshness of the data collected and energy consumption. In other words, we aim to minimize the devices' weighted-sum Aol and transmission power by optimizing the UAV movement and its scheduling policy. In addition, to perform the optimization, we assume the availability of an offline dataset D with few data points collected from other networks without any online interaction with the optimized network. Hence, the optimization problem is formulated as\nP1:\n$\\underset{\\text{(min)}}{\\text{(min)}} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{k=1}^{K} \\delta_k ( \\lambda A_k(t)+P_k(t)),$  (5a)\ns.t.\n$\\xu(t), yu(t) <L,$ (5b)\n$|D| \u2264 D_{const},$ (5c)\nwhere \u03b4\u03b5 is a device-related importance weight and Ais a user-chosen parameter that controls the trade-off between the Aol and the transmission power. Small A values indicate we minimize the AoI over the transmission power. Choosing X = 0 eliminates the power component from the optimization problem. In contrast, large \u5165 values prioritize the transmission power over the AoI. Asymptotically, X \u2192 \u221e eliminates the Aol component from the optimization problem. The con- straint (5b) ensures that the UAV does not fly outside the network borders. The constraint (5c) bounds the size of the offline dataset D\u2758 to be greater than or equal to a certain threshold size Dconst.\nThe optimization problem in (5a) is a non-linear optimiza- tion problem, which can be solved using deep RL frameworks, such as DQNs. However, relying only on offline static data points without online interaction poses serious challenges. In this work, we overcome these challenges by combining offline RL with meta-learning. Offline RL solves the optimization problem using historical datasets collected from prior UAV operations, eliminating the need for costly and risky online training. Meanwhile, meta-RL ensures that the system can quickly adapt to new objectives by utilizing similar networks to optimize the use of a few (online) data points. In addition, meta-learning ensures the resilience of the RL algorithm to adapt to unpredictable conditions."}, {"title": "III. BACKGROUND", "content": "This section introduces the basics of deep RL, offline RL, and meta-learning. These preliminaries will help present the proposed meta-offline RL algorithm in the next section."}, {"title": "A. Deep Reinforcement Learning", "content": "In the previous section, we formulated the policy optimiza- tion problem of minimizing the IoT devices' average Aol and transmission power. This problem can be viewed as a Markov decision process (MDP). Generally, MDPs compose of the tuple (s(t), a(t), r(t), s(t+1)), where s(t) is the current state, a(t) is the selected action, r(t) is the immediate reward, and (t + 1) is the next state resulted from taking action a(t) at state s(t). In RL, the goal is to find the optimum policy \u03c0* that maximizes the accumulative rewards.\nIn our UAV problem, the MDP can be elucidated as follows:\n\u2022 States: At time instant t, the state space consists of the UAV location lu(t) and the individual Aol of each device A(t). Hence, the detailed state space is s(t) = [xu(t), yu(t), A1(t), A2(t),..., \u0410\u043a(t)], whose length is (2 + K).\n\u2022 Actions: At time instant t, the action space consists of the movement direction of the UAV v(t) and the chosen device to serve w(t). The detailed action space is a(t) = [v(t), w(t)], where the dimension of all possible available actions is (5 \u00d7 K).\n\u2022 Rewards: The reward function is formulated in a way to serve the optimization problem in (5)\nr(t) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{k=1}^{K} \\delta_{ik} (t)+\\frac{\\lambda}{K}P_k(t), (6)\nwhere the negative sign in the reward function ensures jointly minimizing the Aol and the transmission power by maximizing the set of received rewards.\nQ-learning is a famous RL algorithm that efficiently solves MDPs iteratively. It utilizes the state-action value function (Q-function) Q(s,a), which is an evaluation function that evaluates the set of available actions at each state by computing the expected accumulative rewards (return). The Q-learning iteratively finds the optimum Q-function Q*(s, a), which cor- responds to the optimum policy, as follows\nQ(s, a) = Q (s, a)+n_{QL} (r+y max Q (s', a)-Q (s, a)), (7)"}, {"title": "B. Offline Reinforcement Learning", "content": "The aforementioned DQN algorithm is an off-policy online DRL algorithm as it relies heavily on online interaction with the environment besides sampling offline data from the replay buffer. However, online interaction with the environment might not be feasible due to the expensive online data collection, nor safe due to the uncertainties in the environment. Offline RL solves this problem by suggesting collecting offline datasets from previous behavioral policies \u03c0\u03b2 (or even a random policy) to be used to find the optimum policy without any interaction with the environment [43].\nHowever, deploying the presented DQN algorithm offline with a static dataset poses many problems. First, the lack of online interaction introduces out-of-distribution (OOD) ac- tions, which come from the differences between the available and learned policies. Second, offline RL algorithms usually overestimate the quality of the optimized actions due to the limited amount of data. These problems are solved in off- policy online DRL algorithms (DQNs) by exploring experi- ences online. Therefore, deploying the DQN algorithm using only offline datasets generally fails.\nTo this end, Conservative Q-learning (CQL) [21] is an offline RL algorithm that adjusts the traditional Q-learning algorithm for offline training. It overcomes the distributional shift problem by adding a regularization term to the bellman update. The regularization term penalizes the OOD distribu- tion, forcing the selected actions to be as close as possible to the offline data set D collected using a behavioral policy \u03c0\u03b2. The CQL loss is formulated as\nLCQL (Q, Q) = LDON (Q, Q) +\\frac{\\alpha}{W}E_{s \\in D}\\left[log \\sum_{a} exp(Q(s,a)) - Q(s_a)\\right], (9)\nwhere a > 0 is a user-chosen parameter called the conser- vative parameter and a ensures that all possible actions are evaluated. Algorithm 2 illustrates the CQL algorithm."}, {"title": "C. Meta-Learning", "content": "Meta-learning has been a fundamental element in many wireless communication domains in recent years due to its ability to find a scalable, transferable, and resilient solution. The most famous definition for meta-learning is learning to learn, as it utilizes learning across different tasks to enhance the convergence rate of new tasks. In our problem, for exam- ple, selecting different values for A in (5) corresponds to a different set of functions as A controls the trade-off between Aol and transmission power. Hence, each time we change \u5165, we change the objective of the problem. To this end, meta- learning can learn from environments with different \u5165 values to speed up the learning convergence of the UAV when selecting a new value for \u5165 [44]. Another example is a sudden environ- mental condition, such as areas with poor communication links due to weather conditions. Meta-learning can ensure resilient performance by transferring learning across different tasks to adapt to new unpredictable sources quickly.\nModel agnostic meta-learning (MAML) [23] is a well- known meta-learning algorithm that utilizes learning across different tasks to find the set of initial weights that can lead"}, {"title": "IV. FEW-SHOT META-OFFLINE RL", "content": "This section presents the proposed few-shot meta-offline RL algorithm for UAV policy optimization. We combine the of- fline CQL algorithm with the MAML algorithm to train a UAV agent with limited offline data utilizing offline learning across similar tasks. We generate different tasks, each corresponding to a unique environment with a unique \u00e0 value. We randomly initialize the Q-network weights w\u00ba during meta-offline RL training. Then, we update these weights internally for each task using the CQL loss as follows\nW \u2190 W \u2212 \u03b7inner\u2207W LCQL(Q, Q; Ti). (13)\nSimilar to (11)\nLmeta-RL \u2190 \\sum_{i=1}^{T} LCQL (Q, Q; Ti), (14)\nwhich is used to update the initial weights of the Q-network\nW \u2190 W \u2212 \u03b7outer\u2207W Lmeta-RL. (15)\nDuring the meta-offline RL testing phase, we use the con- verged initial parameters on a new sampled task with a new \u03bb value and perform a few SGD steps using the traditional CQL algorithm. The proposed few-shot meta-offline RL algorithm is presented in Algorithm 4."}, {"title": "V. NUMERICAL RESULTS", "content": "In this section, we compare the performance of the proposed few-shot meta-offline RL algorithm to the baselines designed for optimizing the UAV trajectory and its scheduling policy to jointly minimize the Aol and the transmission power of limited-power devices. First, we show the implementation and key simulation metrics. Second, we evaluate the proposed algorithm's scalability by changing the problem's objective. Then, we consider the algorithm's resilience in sudden heavy rain conditions."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we designed a novel and resilient few-shot meta-offline RL algorithm to plan the UAV trajectory and its scheduling policy to jointly minimize the Aol and the trans- mission power of ground IoT devices. In summary, We com- bined offline RL using the CQL algorithm with meta-learning using the MAML algorithm to train the UAV using a few shots of experiences stored in an offline static dataset without interacting with the environment in new unseen environments. The proposed algorithm is the only model that converges to the optimum policy through a few SGD steps. The size of the data set and the number of meta-tasks influence the convergence speed, where the convergence is enhanced by increasing the size of the dataset or/and increasing the number of meta- tasks used in training. In addition, the proposed algorithm outperforms traditional schemes, such as DQN, meta-DQN, and CQL, regarding the joint achieved Aol and transmission power. Adapting meta-learning with multi-agent RL (MARL) and assessing its robustness, resiliency, and scalability are open research directions for future works."}]}