{"title": "Seeing the Forest for the Trees:\nA Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs", "authors": ["Jungsoo Park", "Junmo Kang", "Gabriel Stanovsky", "Alan Ritter"], "abstract": "The surge of LLM studies makes synthesizing\ntheir findings challenging. Meta-analysis can\nuncover important trends across studies, but its\nuse is limited by the time-consuming nature\nof manual data extraction. Our study presents\na semi-automated approach for meta-analysis\nthat accelerates data extraction using LLMs.\nIt automatically identifies relevant arXiv pa-\npers, extracts experimental results and related\nattributes, and organizes them into a structured\ndataset. We conduct a comprehensive meta-\nanalysis of frontier LLMs using an automati-\ncally extracted dataset, reducing the effort of\npaper surveying and data extraction by more\nthan 93% compared to manual approaches. We\nvalidate our dataset by showing that it repro-\nduces key findings from a recent manual meta-\nanalysis about Chain-of-Thought (CoT), and\nalso uncovers new insights that go beyond it,\nshowing for example that in-context examples\nbenefit multimodal tasks but offer limited gains\nin mathematical tasks compared to CoT. Our\nautomatically updatable dataset enables contin-\nuous tracking of target models by extracting\nevaluation studies as new data becomes avail-\nable. Through our scientific artifacts and em-\npirical analysis, we provide novel insights into\nLLMs while facilitating ongoing meta-analyses\nof their behavior.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Large Language Mod-\nels (LLMs) (Brown et al., 2020; Chowdhery et al.,\n2022; Achiam et al., 2023; Touvron et al., 2023;\nTeam et al., 2023; Anthropic, 2024) has led to a\nproliferation of empirical studies. Recent \u201csurveys\nof surveys\" (Lv et al., 2024; Ruan et al., 2024)\nindicates the overwhelming growth of LLM re-\nsearch publications, surpassing what individual re-\nsearchers can manually review. The growing num-\nber of evaluations\u2014each using different models,\ndatasets, and prompting configurations\u2014makes\nit challenging to analyze and synthesize findings\nacross studies, underscoring the urgent need for\nsystematic analysis.\nMeta-analysis, which examines data from multi-\nple independent studies to identify overall trends,\noffers a solution (Reiter, 2018; Hupkes et al.,\n2023). However, conducting such analysis is\ntime-consuming (Yun et al., 2023, 2024). For in-\nstance, as shown in Fig. 1, understanding the ef-\nfects of combining Chain-of-Thought (CoT) with\nin-context examples requires identifying relevant\npapers, extracting specific experimental results,\nand aggregating the data. Furthermore, meta-\nanalyses are likely to become quickly outdated,\nin a fast-moving field, motivating the need for au-\ntomated analyses that can be continuously updated\nas new results are shared on preprint servers such\nas arXiv.org.\nIn this study, we conduct the most extensive\nquantitative meta-analysis of frontier LLMs to date.\nOur dataset comprises 18,127 experimental records\nextracted from 1,737 papers, along with 359 ref-\nerenced papers describing datasets used in these\nexperiments. Our dataset can also be dynamically\nupdated with minimal effort to incorporate results\nfrom new models or studies as they are published.\nThis comprehensive dataset enables us to present\nnew insights into LLM performance, a first in the\nfield where previous studies on extracting leader-\nboards from machine learning papers have primar-\nily focused on improving data extraction accuracy\n(Kardas et al., 2020; Yun et al., 2024).\nTo achieve this, we experiment with an auto-\nmated approach to data extraction that efficiently\nprocesses research literature. This approach uses an\nLLM to scan the arXiv database, identify relevant\npapers with experimental results on target models,\nand extract experimental results along with perti-\nnent attributes. This reduces manual effort needed\nfor surveying and extraction by more than 93%\ncompared to expert manual data extraction. The\napproach employs schema-driven extraction (Bai\net al., 2023) and context augmentation from paper\ncontents to capture essential attributes for a com-\nprehensive understanding of evaluation results. Ad-\nditionally, the process creates dataset descriptions\nthat summarize key characteristics of the dataset\nassociated with the evaluation record, enriching the\ndepth of dataset utility in meta-analysis.\nOur analysis shows that our dataset can\nefficiently replicate a previous manual meta-\nanalysis (Sprague et al., 2024), which found that\nCoT reasoning primarily benefits mathematical\nand symbolic reasoning tasks. We also identify\nthree novel insights from the dataset on prompt-\ning configurations. First, in-context examples en-\nhance multimodal tasks but offer limited benefits\nfor mathematical tasks compared to a zero-shot\nsetup. Second, our qualitative analysis identifies\ncertain dataset characteristics linked to negative\nimpacts when using CoT or in-context learning\n(ICL). Lastly, CoT with demonstrations (i.e., ICL)\ngenerally outperforms CoT without them. How-\never, CoT's improvement over direct (i.e. standard)\nprompting stays consistent with the same number\nof demonstrations, whether zero-shot or few-shot.\nTo further contribute to the understanding of fron-\ntier LLMs' capabilities, we will open-source our\ndataset and code, streamlining meta-analyses with\nautomated and continuous study integration."}, {"title": "2 Data Extraction", "content": "Automated data extraction is essential for scaling\nand improving meta-analysis efficiency. This sec-\ntion details our pipeline for extracting experimental\nresults and model attributes from arXiv sources.\nWe define target models and attributes (\u00a72.1) and\nintroduce our LLM-powered extraction process,\nwhich includes three stages: preprocessing & filter-\ning (\u00a72.2), extraction & augmentation (\u00a72.3), and\ndescription generation (\u00a72.4). We utilize GPT-40\n(Hurst et al., 2024) throughout the pipeline."}, {"title": "2.1 Defining Target Models and Attributes", "content": "Target Models We analyze four leading propri-\netary models in the NLP/AI field: GPT-4 (Achiam\net al., 2023), GPT4-0 (Hurst et al., 2024), Claude3\nOpus (Anthropic, 2024), and Gemini 1.0 Pro (Team\net al., 2023). Our analysis focuses on proprietary\nLLMs accessible via APIs that are not possible to\nfine-tune.2 We chose to exclude fine-tuned models\nfrom our study because comparing diverse fine-\ntuning methods would make controlled cross-study\ncomparisons much more challenging. We also ex-\nclude recently released advanced reasoning models\n(GPT4-01 (OpenAI, 2024) and Deepseek-R1 (Guo\net al., 2025)) due to very limited published results\nto date. In the future, our pipeline can readily ex-\ntract experimental data for these models along with\nthose of other target models, as more studies are\npublished. For further discussion, see \u00a77.\nTarget Attributes To enable thorough analy-\nsis across different studies, we extract various\nperformance-related fields from proprietary mod-\nels. The targeted attributes include: Dataset Name,\nSubset, Model Name, Prompting Method, Number\nof In-Context Examples (shots), Metric Name, and\nPerformance. Note that the subset refers to any\ndivision of the dataset, such as a subtask, domain,\nsplit, or language pair. For instance, as shown in\nFig.2, the arXiv paper with ID 2301.08721 (Cheng\net al., 2023) presents experimental results for GPT-\n4 (Achiam et al., 2023) on the SVAMP dataset (Pa-\ntel et al., 2021). The evaluation used Batch Prompt-\ning with CoT (Wei et al., 2022; Cheng et al., 2023)\nand 12 in-context samples (Brown et al., 2020),\nachieving an accuracy score of 95.0."}, {"title": "2.2 Preprocessing & Filtering", "content": "To extract experimental results of target models\nand attributes, we used arXiv sources from Jan-\nuary 2023 to December 2024, focusing on machine\nlearning papers (cs.CV, cs.AI, cs.CL, cs.LG). We\ndownloaded LaTeX source files and applied regex-\nbased methods to extract structured data like ta-\nbles (Bai et al., 2023). Before extraction, we fil-\ntered tables to reduce time and API costs by se-\nlecting those with target model results using key-\nwords like \u201cgemini\" and \"gpt\". We concentrated\non leaderboard tables presenting main benchmark\nresults, as described in Kardas et al. (2020), and\nused Llama3.1-70B (Dubey et al., 2024) for cost-\nefficient filtering.\""}, {"title": "2.3 Extraction & Augmentation", "content": "Based on the selected tables, we extract records\nfrom tables and paper contents. Using schema-\ndriven extraction, we obtain records matching tar-\nget attributes from tables, focusing only on relevant\nmodel data to save costs while maintaining accu-\nracy. This selective approach significantly reduces\nAPI costs compared to extracting full results from\neach table (Bai et al., 2023). After table extrac-\ntion, we augment records with context from the\nentire paper, adding experimental specifics. Dur-\ning this stage, we also gather BibTeX references\nfor datasets to link and retrieve the original papers\ndescribing these datasets from arXiv."}, {"title": "2.4 Dataset Description Generation", "content": "In addition to extracting structured representations\nof experiments, we also generate summary descrip-\ntions of the relevant tasks and datasets (Fig. 3) that\ncan aid in-depth meta-analysis, such as classify-\ning records by research subarea. We use a two-\nstage approach to create these descriptions. Ini-\ntially, the LLM generates descriptions using its\ninternal knowledge based on the dataset name and\nsubset, which is cost-effective. When the LLM is\nuncertain, especially with lesser-known datasets or\ndue to a knowledge cut-off, we prompt it to extract\ndescriptions using the full content of the source\npapers as a reference. The source papers can be\neither the paper containing the table or the origi-\nnal dataset paper cited within it, which is retrieved\nusing the BibTex references obtained from \u00a72.3."}, {"title": "3 Comprehensive Analysis of the Dataset", "content": "Table 1 presents sampled instances of our dataset,\nwhile the dataset's statistics are summarized in Ta-\nble 2. A total of 18,127 experimental records of\nfour target models were extracted from scanning\nover 300k arXiv source papers. GPT-4 (Achiam\net al., 2023) and GPT-40 (Achiam et al., 2023) dom-\ninate the results, while Claude 3 Opus (Anthropic,\n2024) and Gemini 1.0 Pro (Team et al., 2023) have\nsignificantly fewer entries, highlighting a bias fa-\nvoring certain proprietary models. Many prompt-\ning configurations have missing values, confirmed\nby human verification (\u00a73.1) as unreported by the\noriginal authors, likely indicating the default setup\n(standard prompting & zero-shot). The majority\nof dataset descriptions are generated referencing\nthe source paper of the table, as these papers often\ndescribe datasets when presenting benchmarks or\ndetailing experimental sections."}, {"title": "3.1 Quality and Efficiency Assessment", "content": "Our human evaluation assessed extraction accuracy\nand description quality across 40 records (280 total\nattributes) from different papers. Two NLP Ph.D.\nstudents verified field extraction correctness (Bai\net al., 2023) and rated descriptions on a 5-point\nscale from (1) unrelated to (5) fully relevant and\naccurate (Amidei et al., 2019; Liu et al., 2023).\nMissing values were marked correct if the informa-\ntion was genuinely unavailable in the source papers.\nMore details are in appendix B.\nTable 3 shows strong extraction accuracy and\ndescription quality of our dataset. GPT-40 (Hurst\net al., 2024), which we used during the data ex-\ntraction, demonstrated effective long-context in-\nformation extraction, showcasing its potential for\nscientific literature synthesis. Moreover, high val-\nidation scores suggest missing values stem from\nunreported setups. Cohen's Kappa scores of 0.68\n(extraction) and 0.57 (description) indicate substan-\ntial inter-annotator agreement (McHugh, 2012).\nDuring the study, we recorded the time ex-\nperts spent annotating target attribute information"}, {"title": "3.2 Categorization by Required Skills", "content": "We categorize experimental records from our\ndataset by required skills to enable comprehensive\nmeta-analyses and offer researchers a searchable re-\nsource. Expanding on Tulu3's core skills (Lambert\net al., 2024), we defined 10 categories: Knowledge,\nReasoning, Math, Coding, Multimodality, Instruc-\ntion Following, Safety, Multilinguality, Tool Use\n(Agent Framework), and Other. Records were clas-\nsified using an LLM API (Hurst et al., 2024) based\non dataset names, subsets, and descriptions. Since\ndatasets can fit multiple categories, we applied\nmulti-label classification, allowing multiple tags\nper record. This categorized information is used\nthroughout the meta-analyses in \u00a74.1 and \u00a74.2.\nFigure 4 shows the log-scale frequency of skill\ncategories evaluated every three months based on\narXiv publication dates. Over time, experimental\nresults for all skills have increased due to rising\ninterest in LLMs. Reasoning tasks are the most\npopular and continue to grow, while Multimodality\nhas shown recent growth. Knowledge, Multilingual-\nity, and Math have steadily increased in evaluation\nrates. Frequency is based on the unique count of\ndataset-subsets aggregated by paper. The most fre-\nquently used datasets for each category are listed\nin appendix E."}, {"title": "4 Meta Analysis of Prompting Behavior", "content": "Our extracted dataset enables partially auto-\nmated meta-analyses of LLM prompting behav-\niors through its structured attributes. We validate\nour semi-automated approach to meta-analysis by\nreplicating Sprague et al. (2024)'s manual analysis,\nconfirming CoT's advantages over direct prompt-\ning in mathematical and symbolic reasoning tasks\n(\u00a74.1). We then show that our dataset enables a\nfine-grained analysis, showing that in-context ex-\namples boost performance in multimodal tasks but\nnot in mathematical ones (\u00a74.2). We qualitatively\nanalyze dataset characteristics, such as required ex-\npert knowledge, that may affect the performance\nof CoT and ICL. Finally, we analyze CoT and ICL\ninteractions, finding that while demonstrations en-\nhance CoT performance, they do not affect CoT's\nrelative improvement over standard direct prompt-\ning (\u00a74.4)."}, {"title": "4.1 Which Tasks Benefit from CoT?", "content": "Motivation and Setup CoT (Wei et al., 2022),\na prompting technique for eliciting reasoning, has\nattracted considerable attention, with extensive lit-\nerature on the subject (Wang et al., 2023; Yao et al.,\n2024) Recently, Sprague et al. (2024) conducted\na manual meta-analysis, concluding that CoT im-\nproves over standard direct prompting primarily in\nmathematical and symbolic reasoning tasks. We\nextract instances from our dataset that meet the\ncriteria for replicating their investigation.\nThis identification is achieved through simple\nfiltering using the dataset's attributes. We focus\non instances from the same source paper and ta-\nble, identifying experiments that feature both CoT\nand standard direct prompts under the same con-\nditions (model, dataset, subset, metric, and few-\nshot setup). Human input is used only to iden-\ntify whether a prompt is a CoT prompt or a stan-\ndard direct prompt based on the prompting method\nattribute; this is the only manual effort required\nfor the meta-analysis aside from implementing the\nanalysis itself. We exclude CoT variations like\n\"xx of Thoughts\" (Yao et al., 2024) and CoT-SC\n(Wang et al., 2022). We then calculate CoT's perfor-\nmance improvement over standard direct prompt-\ning, namely delta, where a positive delta indicates"}, {"title": "4.2 Which Tasks Benefit from ICL?", "content": "Motivation and Setup Previous research has\nsystematically explored ICL behavior (Min et al.,\n2022; Agarwal et al., 2024; Wei et al., 2023;\nBertsch et al., 2024). In line with our study in \u00a74.1,\nwe analyze the improvement of using in-context\nexamples compared to not using any. To achieve\nthis, we extract instances from the same papers and\ntables that compare few-shot and zero-shot setups\nunder identical conditions (i.e., model, dataset, sub-\nset, metric, and prompting method). In this case, no\nmanual labor was needed beyond implementation,\nas the number of in-context examples is an integer\nvalue that can be easily computed for filtering. We\nmeasure performance deltas between few-shot and\nzero-shot setups, where positive values indicate\nfew-shot advantages and negative values indicate\nzero-shot advantages.\nAnalysis Figure 7 presents the results. In contrast\nto CoT's strong performance in mathematical rea-\nsoning, ICL shows only modest benefits for math\ntasks (mean delta 11.1 vs mean delta 2.6). How-\never, ICL demonstrates more substantial improve-\nments in multimodal applications, despite showing\nconsiderable performance variability across differ-\nent cases. This variance may be due to the broad\nscope of multimodal tasks, including speech and\nimage processing. ICL shows more uniform im-\nprovements over zero-shot performance across dif-\nferent categories compared to the study in Fig. 5\n(std: 8.3). Note that we did not account for the\nnumber of in-context examples used; we only dis-\ntinguished between the use of in-context examples\nand zero-shot examples. We also plot the perfor-"}, {"title": "4.3 Which Dataset Characteristics Negatively Affect CoT and ICL?", "content": "Motivation and Setup To comprehend the pat-\nterns of performance degradation, we analyze cases\nwhere performance declined: specifically, where\nCoT resulted in worse outcomes than direct prompt-\ning, and where few-shot learning performed worse\nthan zero-shot learning (i.e. deltas below zero). Ta-\nble 4 summarizes the key dataset characteristics\nassociated with performance declines under both\napproaches based on analyzing the descriptions.\nAnalysis Out of performance decline cases, tasks\nrequiring expert-level knowledge showed the high-\nest ratio for both cases, approximately 31%, indi-\ncating that knowledge-intensive tasks do not bene-\nfit much from different prompting configurations\nalone. Sprague et al. (2024) notes a similar point\nthat apparent performance gains in knowledge tasks\nmainly stem from \"reasoning\" or \"math\" compo-\nnents in datasets like MMLU (Hendrycks et al.,"}, {"title": "4.4 How do CoT and ICL Interact?", "content": "Setup and Motivation Building on previous\nstudies \u00a74.1 and \u00a74.2, our analysis uncovers how\nCoT reasoning interacts with demonstrations in\nprompting, highlighting their complementarity. We\nextract instances where CoT is used with varying\nlevels of demonstrations, as well as instances that\ncompare CoT and direct prompting under different\ndemonstration levels (zero-shot vs. few-shot), all\nwhile keeping other attributes constant.\nAnalysis From Table 5, demonstrations consis-\ntently enhance CoT performance: few-shot CoT\noutperforms zero-shot CoT by 3.0 points, and in-\ncreasing the number of demonstrations yields a\nsimilar 3.1 points improvement. These comparable\ngains suggest that the mere presence of demon-"}, {"title": "5 Related Work", "content": "Information Extraction Previous works have fo-\ncused on extracting basic result tuples (e.g., task,\ndataset, metric) from scientific literature (Singh\net al., 2019; Hou et al., 2019; Kardas et al., 2020;\nYang et al., 2022; Bai et al., 2023; Singh et al.,\n2024; \u015eahinu\u00e7 et al., 2024; Kabongo et al., 2024).\nOur extraction pipeline improves upon this ap-\nproach in two significant ways: it extracts enriched\ntuples that include prompting-related attributes and\ngenerates detailed dataset descriptions by leverag-\ning LLM and automatically linked source papers.\nHence, unlike previous works that primarily com-\npiled leaderboard tables, our enhanced extraction\npipeline enables deeper meta-analysis, contribut-\ning to the broader goal of AI-driven scientific dis-\ncovery (Xu et al., 2021; Majumder et al., 2024;\nM. Bran et al., 2024).\nLLM & Prompting Our study focuses on ex-\ntracting experimental results of frontier proprietary\nLLMs (Achiam et al., 2023; Anthropic, 2024; Team\net al., 2023), with a specific emphasis on target at-\ntributes that incorporate information about prompt-\ning methods (Brown et al., 2020; Wei et al., 2022;\nMin et al., 2022). In the context of prompting, prior\nstudies have systematically analyzed the mecha-\nnisms behind prompting methods, focusing either\non the use of in-context examples (Min et al., 2022;\nLampinen et al., 2022; Weber et al., 2023; Zhang\net al., 2022) or techniques that elicit reasoning,\nsuch as CoT prompting (Wei et al., 2022; Shaikh\net al., 2023; Wang et al., 2023; Turpin et al., 2024;\nSprague et al., 2024; Liu et al., 2024). In contrast,\nwe analyze the model's behavior through meta-\nanalysis which aggregates data from scientific liter-\nature, identifying trends and linking resources for\ndeeper investigation.\nMeta Analysis Meta-analysis systematically ag-\ngregates and examines data from multiple indepen-\ndent studies on a given topic to derive more precise\nand reliable conclusions. It has been widely applied\nin the biomedical domain for identifying target ma-\nterials or clinical records (Bao et al., 2019; Yun\net al., 2024). In the NLP domain, meta-analysis\nhas been used for metric standardization (Reiter,\n2018), literature review (Santu et al., 2024; Du\net al., 2024), and assessing evaluation criteria for\nspecific domains (Ostheimer et al., 2023). In con-\ntrast, our work employs a meta-analysis approach\nto evaluate the behavior of LLMs. In the con-\ntext of LLMs, Asai et al. (2024) utilizes retrieval-\naugmented language models to synthesize scientific\nliterature. However, this approach can only process\na limited number of documents during retrieval to\nsynthesize. The work by Sprague et al. (2024) is\nperhaps the most closely related to ours. They con-\nducted a meta-analysis through a literature survey\nto examine the effectiveness of CoT prompting for\nLLMs. However, their study is primarily focused\non CoT prompting, conducted on a limited scale,\nand relies on manual extraction methods."}, {"title": "6 Conclusion", "content": "Our study streamlines meta-analysis by using an\nLLM for dataset extraction, demonstrating that au-\ntomatically extracted datasets can yield novel find-\nings and replicate manual analyses. We confirm our\ndataset's quality by replicating a key finding from\nSprague et al. (2024) on CoT. Our analysis pro-\nvides insights into prompting configurations, such\nas the benefits of in-context examples, the com-\nbined effects of CoT and in-context examples, and\nthe dataset characteristics on performance declines\nwith CoT or demonstrations. Overall, our resources\nsupport ongoing meta-analyses, enhancing the un-\nderstanding of LLM behaviors and trends."}, {"title": "7 Limitations", "content": "Target Model Scope Our study focuses on\nfour leading proprietary LLMs, selected for their\nwidespread adoption and extensive documentation\nin existing literature. We excluded newer models\nlike GPT4-01 and Deepseek-R1 due to limited pub-\nlished results, though our analysis pipeline can eas-\nily accommodate them as more experimental data\nbecomes available. While most analyzed models\nare not fine-tunable, this limitation presents an op-\nportunity for future research combining fine-tuning\nand prompting methods analysis.\nFurther Validation Our meta-analysis aims to\nidentify trends by aggregating information across\nmultiple studies, which generates potential hy-\npotheses but requires systematic validation. While\nwe report findings and patterns observed in the\naggregated scientific literature, we did not indepen-\ndently validate each claim or finding. This limita-\ntion suggests the need for future work to rigorously\ntest the hypotheses emerging from our analysis.\nAttributes' Descriptiveness We frequently ob-\nserved that the extracted attributes were not descrip-\ntive enough, which can hinder the dataset's utility\nfor further analysis. Techniques like Batch COT,\nfor example, would benefit from more detailed de-\nscriptions. Additionally, the dataset descriptions\ncould be enhanced to better differentiate between\nvarious dataset characteristics. Our attempts to fur-\nther generate the \u201ccollection process\" of the dataset\nresulted in numerous inaccuracies. Moreover, ef-\nforts to automatically link descriptions to actual\ndataset instances also encountered technical chal-\nlenges, necessitating extensive manual intervention.\nFuture work should aim to develop more effective\nmethods for comprehensive dataset characteriza-\ntion.\nDataset Canonicalization Cross-study analysis\nrequires standardizing dataset names and formats.\nWhile we implemented strict rules for dataset\ncanonicalization, our approach likely missed po-\ntential matches. Alternative matching techniques\nwe explored using LLM produced too many false\nnegatives, whereas linking to the PaperswithCode\ndataset ontology\u00b3 was limited by its incomplete\ncoverage of datasets.\"\n    }"}]}