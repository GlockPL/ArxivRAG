{"title": "Graph Structure Learning with Bi-level Optimization", "authors": ["Nan Yin"], "abstract": "Currently, most Graph Structure Learning (GSL) methods, as a means of learning graph structure, improve the robustness of GNN merely from a local view by considering the local information related to each edge and indiscriminately applying the mechanism across edges, which may suffer from the local structure heterogeneity of the graph (i.e., the uneven distribution of inter-class connections over nodes). To overcome the cons, we extract the graph structure as a learnable parameter and jointly learn the structure and common parameters of GNN from the global view. Excitingly, the common parameters contain the global information for nodes features mapping, which is also crucial for structure optimization (i.e., optimizing the structure relies on global mapping information). Mathematically, we apply a generic structure extractor to abstract the graph structure and transform GNNs in the form of learning structure and common parameters. Then, we model the learning process as a novel bi-level optimization, i.e., Generic Structure Extraction with Bi-level Optimization for Graph Structure Learning (GSEBO), which optimizes GNN parameters in the upper level to obtain the global mapping information and graph structure is optimized in the lower level with the global information learned from the upper level. We instantiate the proposed GSEBO on classical GNNs and compare it with the state-of-the-art GSL methods. Extensive experiments validate the effectiveness of the proposed GSEBO on four real-world datasets.", "sections": [{"title": "1 Introduction", "content": "Based on the homophily assumption of \"like to associate with like\" [McPherson et al., 2001; Yin et al., 2023c; Shou et al., 2023], Graph Neural Network (GNN) [Scarselli et al., 2009; Yin et al., 2024a; Yin et al., 2024b; Ju et al., 2024] has become the promising solution for node classification. However, a large portion of edges are inter-class connections [Pandit et al., 2007], and representation propagation over such connections can largely hinder the GNN from obtaining class-separated node representations, hurting the performance.\nExisting GSL methods are roughly categorized into attentive mechanism, noise detection, and probabilistic mechanism. Attentive mechanism, which calculates weights for edges to adjust the contribution of different neighbors during representation propagation [Veli\u010dkovi\u0107 et al., 2018; Wang et al., 2021; Jiang et al., 2019; Chen et al., 2020b; Zhao et al., 2021a]. These methods can hardly work well in practice for two reasons: (1) the mechanism may not generalize well to all nodes with different local structures (cf. Figure 1(a)); and (2) the attention cannot be easily trained well due to the limited labeled data [Knyazev et al., 2019]. Noise detection, which incorporates an edge classifier to estimate the probability of inter-class connection for each edge [Zheng et al., 2020; Zhao et al., 2021b; Luo et al., 2021; Kazi et al., 2020; Franceschi et al., 2019; Li et al., 2018]. Although it can be better trained owing to the supervision of edge labels, the edge classifier also suffers from local structure heterogeneity and lacks consideration of global information. Probabilistic mechanism, which models connection from a global view which assumes a prior distribution of edge and estimates GNN parameters with Bayesian optimization to overcome the impact of inter-class edges [Zhang et al., 2019; Elinas et al., 2020; Wang et al., 2020; Wu et al., 2020]. Although the edge specific parameterization largely enhances the model representation ability, it is hard to accurately access the prior distribution.\nDespite the achievements of the existing methods, there still exists some common cons: (1) Edge modeling method, the existing methods model edges with parameter sharing mechanism, which may suffer from the local structure heterogeneity problem; (2) Local optimization, the local optimization problem focuses on optimizing the parameters with the information of neighbor nodes, which ignores the impact from the global view. Therefore, we come up with the key considerations for GSL: (1) modeling graph connection in an edge-specific manner instead of a shared mechanism; and (2) optimizing the corresponding parameters with a global objective of accurately classifying all nodes. The edge-specific modeling can overcome the local structure heterogeneity, i.e., handling nodes with different properties (e.g., node 1 and node 9 in Figure 1(a)) via different strategies. Besides, blindly removing the inter-class edges will increase the risk of misclassifying the target nodes (in dash circle) due to cutting off their connections to the labeled neighbors in"}, {"title": "2 Related Work", "content": "Attentive mechanism. The attentive mechanism methods adaptively learn the weights of edges and adjust the contributions of neighbor nodes [Pang et al., 2023; Yin et al., 2022a; Yin et al., 2023d]. MAGNA [Wang et al., 2021] incorporates multi-hop context information into every layer of attention computation. IDGL [Chen et al., 2020b] uses the multi-head self-attention mechanism to reconstruct the graph, which has the ability to add new nodes without retraining. HGSL [Zhao et al., 2021a] extends the graph structure learning to heterogeneous graphs, which constructs different feature propagation graphs and fuses these graphs together in an attentive manner. However, those methods suffer from different local structures and are difficult to train.\nNoise detection. The noise detection methods leverage the off-shelf pre-trained model to induce node embeddings or labels and incorporate an edge classifier to estimate the probability of each edge [Yin et al., 2023b; Yin et al., ]. NeuralSparse [Zheng et al., 2020] considers the graph sparsification task by removing irrelevant edges. GAUG [Zhao et al., 2021b] utilizes a GNN to parameterize the categorical distribution instead of MLP in NerualSparse. PTDNet [Luo et al., 2021] prunes task-irrelevant edges by penalizing the number of edges in the sparsified graph with parameterized networks. Even though the noise detection methods can be well trained with the supervision of edge labels, the edge classifier also suffers from local structure heterogeneity and lacks consideration of global information.\nProbabilistic mechanism. This type of methods assume the prior distribution of graph or noise and estimate the parameters through observed values, then resample the edges or noise to obtain a new graph [Yin et al., 2023a; Zhang et al., 2019; Yin et al., 2022b]. BGCN [Zhang et al., 2019] estimates the parameter distribution of edges and communities by sampling edges from graph, and resample new graphs with the estimated parameters for prediction. VGCN [Elinas et al., 2020] trains a graph distribution parameter similar to the original structure through ELBO, and resample graphs for prediction. However, both of BGCN and VGCN models are sampled from noisy graph, the estimated parameters also contain noise. DenNE [Wang et al., 2020] assumes the observed graph is composed of real values and noise and the prior distribution of features and noise is known. With a generative model, the likelihood is used to estimate the representation of"}, {"title": "3 Methodology", "content": "We first introduce the essential preliminaries for GNN, and then elaborate the graph convolution operator and bilevel optimization algorithm of the proposed GSEBO.\n3.1 Preliminary\nLet $G = (V,E,X)$ represents a graph with N nodes and M edges, where $V = {v_1, v_2,..., v_N}$ and $E = {e_1,e_2,\u2026, e_M}$ denote the set of nodes and edges respectively. $X = [x_1,x_2,\u2026, x_N] \\in \\mathbb{R}^{N\\times C}$ are nodes features, where $x_i \\in \\mathbb{R}^C$ is the i-th row of X, corresponds to node $v_i$ in the form of a C-dimensional vector. The adjacency matrix $A\\in {0,1}^{N\\times N}$ indicates the connectedness of node pairs.\nNode classification. This task aims to learn a classifier $f(A, X;\\theta)$ from a set of labeled nodes to forecast the remaining nodes labels, where $\\theta$ denotes model parameters. Assuming there are M labels, we index them from 1 to M without loss of generality. Formally, $Y = [Y_1, Y_2,\u2026, Y_N] \\in \\mathbb{R}^{N}$ are the labels of nodes, where $Y_i \\in \\mathbb{R}$ is the label of node i. The target is achieved by optimizing the model parameter $\\theta$ over the labeled nodes, which is formulated as:\n$\\min \\limits_{\\theta} \\sum \\limits_{i<M} l(f(A, X)_i, Y_i; \\theta) + \\lambda ||\\theta||,$"}, {"title": "3.2 Generic Structure Extraction with Bi-level Optimization", "content": "To optimize the graph structure, the key consideration lies in (1) decoupling the graph structure from the GNNs to account for the edge-specific modeling and (2) learning the graph structure from the global information in $\\theta$.\nGeneric structure extractor. Towards the first purpose, the core idea is to decompose the graph structure information into connectedness (the edges in the adjacency matrix) and the strength of connection (the latent variable). In general, there are two ways to model the connection strength regarding whether relying on the inductive bias of translation invariant or not. On one hand, attentive mechanisms or noise detection models are translation invariant, which decode the connection strength of each edge from its local features. However, with the consideration of the local structure heterogeneity issue in most real-world graphs [Xu et al., 2018], it is risky to rely on the translation invariant bias. On the other hand, probabilistic mechanisms separately model the connection strength for each edge, where each edge corresponds to a specific distribution. However, it is non-trivial to set a proper prior in practice. According to these pros and cons, we summarize two considerations for extending the graph convolution: (1) edge-specific modeling; and (2) optimization without prior. Towards this end, we model the connection strength as a parameter matrix Z with the same size as A. Formally, the generic structure extractor (GSE) is abstracted as:\nGSE(Z) : = $\\sigma(Z) \\odot A$,\n$H^{(k)} = COM(H^{(k-1)},AGG(H^{(k-1)}, GSE(Z))$,\nwhere $\\sigma(\\cdot)$ is a non-negative activation function since the value of strength is always positive. COM and AGG are the combination and aggregation functions respectively. Noteworthy, different from GNNs, GSE decouples the graph structure from GNNs and treats it as a learnable objective. Besides, GSE is a generic extractor, which can be instantiated over most existing graph convolutions (cf. Appendix"}, {"title": "4 Experiments", "content": "In this section, we conduct experiments on four datasets to answer the following research questions: (1) how does the performance of the proposed GSEBO compared with the state-of-the-art methods? (2) how robust is the proposed GSEBO? (3) to what extent does the proposed GSEBO decrease the impact of inter-class connections? (4) what are the factors that influence the effectiveness of the proposed GSEBO? We present the results of questions (3) and (4) on Appendix D and E due to space limitations.\n4.1 Experimental Setup\nDataset. We select four widely used node real-world classification benchmark datasets with graph of citation networks (Cora and Citeseer [Kipf and Welling, 2017]), social networks (Terrorist) [Zhao et al., 2006], and air traffic (Air-USA) [Wu et al., 2019]. We summarize the statistics of the datasets and describe in detail on Appendix F. We adopt the same data split of Cora and Citeseer as [Kipf and Welling, 2017], and a split of training, validation, testing with a ratio of 10:20:70 on other datasets [Zhao et al., 2021b].\nCompared methods. We apply GSEBO on 4 representative GNN architectures: GCN [Kipf and Welling, 2017], GAT [Veli\u010dkovi\u0107 et al., 2018], GraphSAGE [Hamilton et al., 2017] and JK-Net [Xu et al., 2018]. For each GNN, we compare GSEBO with the vanilla version, and three variants with state-of-the-art connection modeling methods: AdaEdge [Chen et al., 2020a], DropEdge [Rong et al., 2020] and GAUG [Zhao et al., 2021b]. In addition, we compared the"}, {"title": "4.2 Performance Comparison", "content": "Table 1 shows the performance of GSEBO instantiate with classical GNNs, and Tabel 2 presents the results comparison of GSEBO and state-of-the-art GSL methods. From Table 1 and Table 2, we have the following observations:\nImprovement over baselines. GSEBO outperforms the baselines in most cases. Considering the average performance over four datasets, the improvement of GSEBO over the baselines is in the range of [3.0%, 12.5%], which validates the effectiveness of the proposed method. In particular, (1) Probabilistic mechanisms. The performance gain of BGCN and VGCN over the vanilla GCN are limited, which might be-cause of the unsatisfied assumption of the prior distribution. This result shows the rationality of relaxing the prior assumption and modeling the connection strength with parameters directly. (2) Connection modeling. DropEdge, AdaEdge, GAUG, and PTDNet achieve better performance than vanilla"}, {"title": "4.3 Robustness Analysis", "content": "We investigate the robustness of GSEBO under the different inter-class levels. In particular, we follow [Luo et al., 2021] and construct graphs based on Cora by randomly adding 1,000, 3,000, 5,000, 10,000, and 20,000 inter-class edges, respectively. On the synthetic datasets, we compare GSEBO with Vanilla, GAUG, DropEdge, and AdaEdge. Figure 5 shows the performance on the five GNN architectures. From the figures, we have the following observations:\n\u2022 The margins between GSEBO and vanilla GNN on the synthetic datasets are larger than the original Cora. For example, when adding 20,000 inter-class edges, GSEBO improves the accuracy by 17.6%, 3.1%, 40.6%, and 4.4% over GCN, GAT, GraphSAGE and JKNet. This result indicates the robustness of GSEBO's structure learning ability.\n\u2022 In most cases, GSEBO outperforms the baselines at different noisy levels, which further justifies its robustness.\n\u2022 GAUG and AdaEdge utilizes different strategies to update the structure of the graph, which also consistently perform better than vanilla GNN. However, their gaps to GSEBO on the synthetic data are larger than the original Cora. We postulate that the reason is their objectives are affected by the intentionally added noise.\n\u2022 DropEdge shows worse performances than the vanilla GNN on the synthetic datasets. The comparison shows that randomly dropping edges fails to enhance GNN robustness when the noisy level is high."}, {"title": "5 Conclusion", "content": "In this work, we propose a novel GSEBO, which utilizes the global information to learn the graph structure. To better optimizes the strength of each edge and the parameters of feature mapping, we decompose the optimization problem into two mutually constrained optimization objectives, i.e., the inner and outer objective based on a a universal graph convolution operator. Extensive experiments demonstrate the effectiveness and robustness of GSEBO on both benchmark and synthetic datasets.\nAlthough the impressive results GSEBO achieved, there are still some limitations: GSEBO cannot be effectively applied to large graphs, which requires implementation of mini-batches. Besides, we evaluate GSEBO in the transductive setting, when new nodes add to the graph after training, GSEBO has to retrain the entire model. For future researches, we would like to explore solutions to the above limitations."}]}