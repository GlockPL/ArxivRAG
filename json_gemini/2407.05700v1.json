{"title": "InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct", "authors": ["Yutong Wu", "Di Huang", "Wenxuan Shi", "Shihao Liu", "Ziyuan Nan", "Kaizhao Yuan", "Wei Wang", "Lingzhe Gao", "Zidong Du", "Qi Guo", "Yewen Pu", "Rui Zhang", "Xishan Zhang", "Dawei Yin", "Xing Hu", "Yunji Chen"], "abstract": "Recent advancements in open-source code large language models (LLMs) have demonstrated remarkable coding abilities by fine-tuning on the data generated from powerful closed-source LLMs such as GPT-3.5 and GPT-4 for instruction tuning. This paper explores how to further improve an instruction-tuned code LLM by generating data from itself rather than querying closed-source LLMs. Our key observation is the misalignment between the translation of formal and informal languages: translating formal language (i.e., code) to informal language (i.e., natural language) is more straightforward than the reverse. Based on this observation, we propose INVERSE-INSTRUCT, which summarizes instructions from code snippets instead of the reverse. Specifically, given an instruction tuning corpus for code and the resulting instruction-tuned code LLM, we ask the code LLM to generate additional high-quality instructions for the original corpus through code summarization and self-evaluation. Then, we fine-tune the base LLM on the combination of the original corpus and the self-generated one, which yields a stronger instruction-tuned LLM. We present a series of code LLMs named InverseCoder, which surpasses the performance of the original code LLMs on a wide range of benchmarks, including Python text-to-code generation, multilingual coding, and data-science code generation.", "sections": [{"title": "Introduction", "content": "Code generation, which aims to generate code that satisfies the user's intent from inputs/outputs or natural language, has been a significant challenge in computer science. Recently, closed-source LLMs like GPT-3.5 and GPT-4 [35] have enabled the generation of general-purpose code (like Python) based on natural language, making them broadly applicable in the fields of programming assistance [30], computer vision [16, 44], science [32], and embodied intelligence [24, 29, 45, 49].\nTo develop high-performance open-source models, researchers have leveraged these closed-source LLMs to generate datasets of instructions and code, then distilled these datasets into smaller, open-source code LLMs via instruction tuning [28, 55, 59, 63]. For example, Code Alpaca [6] was fine-tuned on 20K instruction-code pairs generated based on GPT-3.5 with SELF-INSTRUCT [51]. Luo et al. [28] used Evol-Instruct [57], a method that creates a diverse set of instruction data\nThis paper explores how to improve an instruction-tuned code LLM by querying itself (rather than querying a closed-source LLM). We make two observations: (1) While generating multiple code implementations from the same instruction can often lead to incorrect programs, generating multiple instructions from the same code often results in a diverse set of correct instructions. (2) For a fine-tuned code LLM, there is a mismatch in translation ability: translating codes into instructions (i.e., in natural language) is more straightforward than translating instructions into codes. Therefore, given a fine-tuning corpus for code generation, it is possible to use the fine-tuned LLM to generate additional high-quality instructions from the same corpus, obtaining additional fine-tuning data with an increased diversity of instructions.\nWe develop INVERSE-INSTRUCT, a simple yet effective instruction tuning approach based on self-generating multiple instructions from code snippets (See Figure 1). INVERSE-INSTRUCT starts with an instruction-code corpus, and a code LLM fine-tuned on it. We first clean and extract code snippets from the corpus, then let the code LLM translate these code snippets into new instructions. Then, we use the code LLM to evaluate and filter consistent instruction-code pairs from the newly generated data. Finally, the filtered dataset is combined with the original instruction dataset, and the base model is fine-tuned on the combined dataset. Using INVERSE-INSTRUCT, we develop InverseCoder, a series of fine-tuned code LLMs that achieve SOTA results.\nEvaluation of InverseCoder on a wide range of benchmarks, including HumanEval (+) [7, 26], MBPP (+) [1, 26], MultiPL-E [5], and DS-1000 [19]. Results show that InverseCoder series surpasses the base models by exploiting the base models' own capability. Specifically, InverseCoder-DS-6.7B achieves 76.8% on HumanEval+, 69.0% on MBPP+, 62.6% on MultiPL-E, 44.2% on DS-1000, which are SOTA results across four benchmarks among fully open-source (both model and dataset) models with only 6.7B parameters (See Table 1 for a summary).\nThis paper makes the following contributions:\n\u2022 We introduce INVERSE-INSTRUCT, a simple yet effective instruction tuning approach exploiting the mismatch of code-generation and instruction-generation.\n\u2022 We make thorough analysis on INVERSE-INSTRUCT, including the component of generated dataset, the impact of data size, etc. We find that the self-consistency between the code generation and summarization is predictive of the effectiveness of INVERSE-INSTRUCT prior to training."}, {"title": "Related Work", "content": "After being pre-trained on a large amount of code, large language models have demonstrated im-pressive code generation capabilities. Recently, AI code assistants have become one of the most important applications of large language models. Technology companies such as OpenAI and Google have developed and publicly released large language models, including Codex[7], GPT-4[35], PaLM[9], and Gemini[47], which have achieved outstanding performance on code generation benchmarks. In addition to closed-source models, there are also some available open-source models, such as CodeGen[33], PanGu-Coder series[10, 39], CodeGeeX[62], AlphaCode[23], CodeT5 series [20, 52, 53], InCoder[13], StarCoder series[21, 27], CodeLlama[37], DeepSeek-Coder[15], CodeQwen[2]. These open-source code models have shown notable advancements in code-related tasks, but there is still a gap compared to the most advanced code LLMs. Reinforcement Learning can also be applied to large language models to enhance code generation quality by using execution feedback to explore the policy model's output space. CodeRL[20] utilizes unit test signals as rewards and employs actor-critic methods to improve the model's code generation capabilities. PPOCoder[40] refines CodeRL by incorporating Proximal Policy Optimization, while RLTF[25] offers fine-grained rewards by identifying error locations. StepCoder[12] introduces a curriculum of code completion tasks to enhance RL-based code generation. Our approach can further improve the performance of open-source code models."}, {"title": "Instruction tuning methods", "content": "Instruction tuning is a method to fine-tune LLMs on instructional prompts and corresponding outputs. T5[36], FLAN[54], and FLAN-T5[11] have employed instruction tuning to augment the generalization and instruction-following capacities of large language models. For code tasks, OctoPack[31] and Shypula et al. [41] extracted high-quality instruction datasets from human-written code. Fine-tuning on these high-quality instruction datasets has significantly enhanced the program generation capabilities of the base models.\nHowever, obtaining high-quality instruction datasets is usually laborious. Researchers have attempted to employ neural models for generating labeled datasets, including Betker et al. [4] and Gu et al. [14]. Utilizing large language models for generating synthetic instruction data has emerged as a novel"}, {"title": "Inverse-Instruct: Instruction Tuning with Code Summarization", "content": "In this section, we will introduce INVERSE-INSTRUCT, an instruction tuning method that can obtain more high-quality instruction data through the model's own capabilities. The overall illustration of Inverse-Instruct is shown in Figure 1. Then central to INVERSE-INSTRUCT is the misalignment between formal and informal languages: (1) Converting formal language (i.e., code) into informal language (i.e., natural language) is more straightforward than the reverse process. (2) The same code can be considered as a response to different instructions, which increases the diversity of the instruction data.\nThe whole data generation process contains three steps: code preprocessing, code summarization, self-evaluation and data selection. In code preprocessing, we preprocess the code data by filtering clean code snippets {y} from an off-the-shelf instruction tuning dataset (e.g., evol-codealpaca-v1) {(xi, Yi)}. Subsequently, in code summarization, we prompt an instruction fine-tuned code LLM M (e.g., WizardCoder-GPT4-CL) to summarize the clean code snippets {y} filtered before for new instructions (denote as x5). Note that ay may correspond to multiple summarized instructions {x}. Then, in self-evaluation and data selection, we use the same code LLM M to select high-quality instruction data {x;}. This step merely filters out poor data through the LLM's own capabilities and does not generate any new data. The filtered instructions are combined with the code snippets to construct a new instruction tuning dataset {(x, y)}. Finally, we fine-tune the base code LLM with the instruction data {(x, y)} \u222a {(xi, yi)} to obtain a stronger code LLM (i.e. InverseCoder). Details of the three steps are illustrated below."}, {"title": "Code Preprocessing", "content": "The first step is to preprocess the existing code data and get clean code snippets {y}. This is because the summarization capabilities of code LLMs can only be demonstrated on clean code data, whereas the response data {y} in the original dataset typically contains a lot of noise, such as natural language responses.\nWe select data with code snippet {y} from the original {y} with the following two steps:\n1. Filtering responses. We first collect responses that contain the marker of the code block (i.e. ```), which indicates that there are code snippets in the response. The remaining data might contain clean code without any code markers, so then we collect the responses that can pass syntax checking.\n2. Extracting code. After filtering responses with code snippets, we remove the natural language surrounding the code snippets to make it easier for the model to summarize. If there are multiple parts of code in the original response, we only keep the first part, since the following parts are usually test cases or using examples. Detailed examples are shown in Appendix B.\nAt the end of code preprocessing, we obtain clean code snippets {y} for summarization."}, {"title": "Code Summarization", "content": "After filtering, we employ the code LLM M to generate a certain number of corresponding instructions x for each code snippet y by summarizing its functionality. During the summarization process, we randomly choose different instruction prefixes for the prompt to enhance the diversity of the instructions. The summarization prompt and the instruction prefixes are shown in Appendix B.\nIn this way, we have obtained new pairs of natural language and code {(xj, Y)}."}, {"title": "Self-evaluation and Data Selection", "content": "We notice that code LLM M may make mistakes during the code summarization process. An example is shown in Appendix B, where M hallucinates invalid instructions. Therefore, it is necessary for us to utilize M itself to evaluate {(x, y)} and select high-quality instruction data.\nData selection is typically performed by powerful LLMs such as GPT-3.5/4 because these models possess excellent instruction-following capabilities, enabling them to understand and execute complex filtering rules [50]. However, the instruction-following capabilities of code LLMs are often weaker, making it difficult to conduct effective selection.\nInspired by Zhang et al. [61], we use the pseudo-probability of the YES token given by the code LLM M as an indicator of the instruction quality rather than a score in textual format. Specifically, we concatenate the generated instruction 21 and the original code snippet y as a problem-answer pair. Then, we ask M to evaluate the correctness of the answer under the given problem and calculate the pseudo-probability of YES using the logits of the first token given by M. The selection prompt and the formula for calculating probability are shown in Appendix B and Equation 1.\n$LM\\text{-}Score(\\cdot) = \\frac{exp(logit('YES'))}{exp(logit('YES')) + exp(logit(\u2018NO\u2019))}$ (1)"}, {"title": "Implementation Details", "content": "The original instruction tuning dataset. In this work, we mainly use evol-codealpaca-v1 as our original instruction tuning dataset {(xi, Yi)}, which is widely used for instruction tuning of code LLMs [55, 59, 63]. It contains 110K instruction-response pairs generated by Evol-Instruct using GPT-4 in various programming languages. Following Wei et al. [55], evol-codealpaca-v1 is decontaminated by removing data that contain docstrings or solutions from HumanEval [7], MBPP [1], MultiPL-E [5], and DS-1000 [19], which are used to evaluate InverseCoder. We apply the same decontamination method to the newly generated instruction data {(x,y)}.\nTraining for original Code LLM. We take CodeLlama-Python-7B and DeepSeek-Coder-Base-6.7B as base models. To obtain the beginning code LLM M (hereinafter called WizardCoder-GPT4), we fine-tune the base models on evol-codealpaca-v1 for 2 epochs using 8 NVIDIA A100-40GB SMX GPUs through the Distributed Data Parallel (DDP) module from PyTorch. We set the initial learning rate at 5e \u2013 5 with 15 warmup steps and a linear learning rate scheduler. We use Adafactor [38] as our optimizer and choose a batch size of 512 with a sequence truncation length of 1024.\nInstruction data collection. We use the vLLM inference framework [18] for code summarization and instruction selection on NVIDIA A100-40GB SMX GPUs. We generate 10 instructions {Xij}}01 for each code snippet {yi} with temperature = 0.8, repetition_penalty = 1.1 and max_tokens = 2048 in the code summarization stage. For each instruction-response pair, the self-evaluation and data selection process is conducted by prompting the beginning code LLM M with greedy decoding. We choose the instruction with the highest pseudo-probability that the first token given by LLM is YES as the best-generated instruction for each response.\nTraining for InverseCoder. We first fine-tune the base models on synthetic data {(x, y)} generated through INVERSE-INSTRUCT for 1 epoch, then we continue to fine-tune the models with the original instruction tuning dataset {(xi, Yi)} for 2 epochs to obtain InverseCoder models. The hyperparameters and hardware configuration are the same as the training process for the beginning code LLM M. The instruction tuning prompt is the same as Magicoder, which is shown in Appendix B."}, {"title": "Experiments", "content": "In this section, we will show a series of experimental results to demonstrate the effectiveness of INVERSE-INSTRUCT: (1) Main results. We evaluate InverseCoder on four benchmarks widely used for code LLMs, including Python text-to-code generation, multilingual coding, and data-science code generation. (2) Self-consistency on generation and summarization. We attempt to quantify the mismatch between code generation and code summarization, and demonstrate how INVERSE-INSTRUCT bridges this gap. Additionally, we show that self-consistency serves as a proxy evaluation to indicate when our method is effective. (3) Ablation study on code summarization and data selection step in our method. (4) Further analysis on dataset features and the impact of data scaling (see Appendix C)."}, {"title": "Main Results", "content": ""}, {"title": "HumanEval+ and MBPP+: Python Text-to-Code Generation", "content": "We use HumanEval+ and MBPP+ [26], the enhanced counterparts of two Python code generation benchmarks [1, 7], to evaluate the Python code generation capability of InverseCoder. Each benchmark offers a collection of tasks with a natural language description as the prompt for code LLM to generate function-level code, which is then validated by test cases prepared in advance.\nWe compare the performance of InverseCoder based on CodeLlama-Python-7B and DeepSeek-Coder-Base-6.7B with the original model (i.e. WizardCoder-GPT4) and other available code LLMs. For InverseCoder and WizardCoder-GPT4, we report the results whose training and evaluation process are both in our environment to fairly evaluate the improvement of our models. For other code LLMs including both closed-source models such as the GPT series [34] [35] and open-source models [28, 37, 55, 59], we report the results in EvalPlus [26] leaderboard. We use pass@1 [7] metric to compare the code generation capability among different models.\nThe results shown in Table 2 demonstrate that InverseCoder makes a significant improvement over WizardCoder-GPT4 on HumanEval/HumanEval+ and MBPP/MBPP+, which indicates that the Python code generation capability enhancement of the model benefit from INVERSE-INSTRUCT. Furthermore, InverseCoder-DS-6.7B has an outstanding performance in HumanEval/HumanEval+, which surpasses all open-source models with a similar scale of weights."}, {"title": "MultiPL-E: Multi-Language Programming", "content": "Besides Python, we evaluate the code generation capabilities of other six mainstream programming languages for InverseCoder on MultiPL-E benchmark [5]. We use bigcode-evaluation-harness framework [3] to generate and evaluate code of different programming languages under the inference prompt format aligned with the prompt we used in the training process.\nTable 3 shows the results of InverseCoder and other models on MultiPL-E. The results reveal that the capabilities of InverseCoder to generate code in most of the mainstream programming languages are improved over WizardCoder-GPT4."}, {"title": "DS-1000: Data Science Code Generation", "content": "To show the capability of InverseCoder for complex programming problems in realistic applications, we evaluate it on DS-1000 benchmark [19], which comprises 1000 different data science workflows across seven libraries and provides unit tests to validate each problem. Following Wei et al. [55], we evaluate our model only on the completion mode.\nThe results in Table 4 show that the average performance of InverseCoder-DS in the generation tasks for the seven data science libraries is enhanced, which implies that INVERSE-INSTRUCT can help to improve the code generation capability of the original model in realistic tasks beyond basic programming problems."}, {"title": "Self-Consistency on Generation and Summarization", "content": "We attempt to quantify the mismatch between the LLM's ability of code generation and code summarization by measuring self-consistency. That is, whether the model can generate equivalent code after summarizing the code it generates before. Self-consistency is defined as\n$1(M(M^{-1}(M(x))) = M(x)),$ (2)\nwhere M(\u00b7) denotes the LLM's code generation process, M^{-1}(\u00b7) denotes the LLM's code summarization process, 1(\u00b7) is the indicator function, and = denotes functional equivalence. Specifically, self-consistency is measured by three steps: Firstly, we prompt the code LLM M with MBPP+ problems xi to generate code yi = M(xi). Then, code LLM summarizes the code yi to get new instructions x = M^{-1}(yi). Next, we let the code LLM generate new code y based on xi, i.e. y = M(x). Finally, we evaluate if y and y\u2081 are functional equivalent by measuring their outputs given the same inputs taken from MBPP+ benchmark. The result is calculated as the pass@1 of new code responses y taking the original code responses yi as groundtruth solutions. Note that we remove the problems whose inputs will cause a runtime error when applied to yi.\nThe results are shown in Figure 5. InverseCoder has better self-consistency than its base model, which indicates that the base model has a larger gap between generation and summarization, and the performance improvement of InverseCoder may come from bridging this gap.\nFurthermore, we notice that self-consistency can serve as a proxy evaluation of our method before training. That is, one can predict the performance improvement of INVERSE-INSTRUCT by evaluating the model's self-consistency without training. For example, DeepSeek-Coder-Base-6.7B has better self-consistency than CodeLlama-Python-7B, which means that INVERSE-INSTRUCT is less effective on DeepSeek-Coder-Base-6.7B than on CodeLlama-Python-7B."}, {"title": "Ablation Study", "content": "We conduct a series of ablation experiments to analyze the utility of code summarization and data selection steps in our method. The ablation experiments are in three aspects:\nGeneration + Evaluation We regenerate 10 responses {yij}}01 for each instruction {x;} in the original instruction tuning dataset (i.e. evol-codealpaca-v1) and apply the same self-evaluation method to select the best responses, which aims to compare code summarization with the trivial forward data generation method (i.e. from instruction to response).\nPreprocessing We replace the new instructions dataset {(x, y)} with {(xi, y)} to reveal the improvement only from preprocessing, where {x} is the original instructions and {y} is the corresponding preprocessed code snippets to {x}."}, {"title": "Self-improving for Base Models", "content": "Although we still need an original instruction tuning dataset to obtain a stronger instruction following model for better code summarization and self-evaluation in INVERSE-INSTRUCT, we notice that base models already have certain summarization and self-evaluation capabilities under a well-designed prompt in completion form. Therefore, we conduct an experiment to validate that by using high-quality unlabeled source code as responses, the base model can improve itself without any human annotations.\nWe first use the preprocessed code responses {y} of the original dataset {y} as the high-quality unlabeled source code. Then, we apply code summarization and self-evaluation on it simply by prompting a base model to obtain an instruction-tuning dataset. Finally, we fine-tune the base model with the instruction data generated by itself.\nThe evaluation results of self-improving experiments for CodeLlama-Python-7B are shown in Table 7. The enhanced performance reveals that it is useful to apply INVERSE-INSTRUCT for the base model in the situation with adequate high-quality unlabeled data but insufficient human-annotated instruction data."}, {"title": "Conclusion", "content": "In conclusion, this paper presents a novel approach to enhancing the capabilities of open-source code LLMs by leveraging self-generated data for instruction tuning, rather than relying solely on data from powerful closed-source LLMs like GPT-3.5 and GPT-4. Our proposed method, named INVERSE-INSTRUCT, capitalizes on the inherent asymmetry in translating between formal and informal languages. By reversing the conventional process, INVERSE-INSTRUCT generates high-quality natural language instructions from code snippets via summarization and self-evaluation techniques. The effectiveness of this methodology is demonstrated through the development of InverseCoder, a new series of code LLMs that not only outperform their predecessors in traditional benchmarks but also show significant improvement across diverse coding tasks."}, {"title": "Limitations", "content": "Our research is subject to two primary limitations. Firstly, the performance of our approach is contingent upon the base model's capacity to accurately summarize code. A promising avenue for future exploration is to investigate the disparities in the model's capabilities across different modalities (e.g., code-to-code, code-to-instruction, instruction-to-code), which may enable the development of more advanced language models that surpass current performance ceilings. Secondly, the generation of synthetic data relies on access to high-quality code snippet datasets. Future work should focus on reducing the required code snippet volume to enhance efficiency and feasibility."}, {"title": "Broader Impacts", "content": "Compared to directly distilling data from powerful LLMs, INVERSE-INSTRUCT makes the very first attempt to leverage the mismatch between the model's code generation and summarization to self-improve. Through extensive experiments, we have demonstrated the feasibility of this approach. INVERSE-INSTRUCT offers the potential to further enhance the code generation capabilities of code LLMs.\nHowever, similar to other code LLMs, InverseCoder may produce code that does not align with user intent and could be misused. These issues have been thoroughly discussed by Chen et al. [7], and we refer readers to the broader impacts and hazard analysis section of that work. Security concerns are not uncommon. For instance, the generated code might delete system files or the model itself. Thus, we recommend users create a virtual execution environment, such as a sandbox, to mitigate these risks before running the generated code."}, {"title": "Method Details", "content": "The prompts we used in INVERSE-INSTRUCT are shown in Figure 2."}, {"title": "Further Analysis", "content": "To explore the application of INVERSE-INSTRUCT in realistic situations, we further analyze the different features and length distributions between generated instructions and original instructions and the impact of data scaling."}, {"title": "Dataset Analysis", "content": "Following Wei et al. [55], we use the text embeddings generated by INSTRUCTOR [42] to analyze categories of the instructions generated by INVERSE-INSTRUCT. We calculate the ratios of 10 coding-related categories of the instructions in evol-codealpaca-v1 and the dataset generated by CodeLlama-Python-7B. The results are illustrated on Figure 5 and Figure 6. They show that the generated instructions have different distributions from its original dataset, which improves the diversity of our training data.\nFurthermore, we depict the length distribution by counting the token for the instructions and responses in evol-codealpaca-v1 and the dataset with instructions generated by CodeLlama-Python-7B."}, {"title": "Data Scaling", "content": "With 110K instruction tuning dataset for general programming tasks, evol-codealpaca-v1, the performance of InverseCoder is greatly improved by INVERSE-INSTRUCT. However, there may be a limited amount of labeled code data in some specific areas (e.g., embodied intelligence). How much does the performance gain from INVERSE-INSTRUCT in different amounts of data? We conduct a series of experiments to simulate the data-limited scenarios by using 25K, 50K, 75K instruction-response pairs randomly selected from the original dataset. We fine-tune the base model with three subsets of data as three weaker original models and apply INVERSE-INSTRUCT for them respectively.\nThe performance enhancements on HumanEval+ of the model fine-tuned with the three subsets and the whole dataset are illustrated in Figure 10. It is shown that the performances of the original models are all improved by INVERSE-INSTRUCT at different scales of data."}, {"title": "Generation Examples", "content": "Table 8 9 are some examples of the responses to programming instructions given by InverseCoder."}]}