{"title": "MaPPing Your Model: Assessing the Impact of Adversarial Attacks on LLM-based Programming Assistants", "authors": ["John Heibel", "Daniel Lowd"], "abstract": "LLM-based programming assistants offer the promise of programming faster but with the risk of introducing more security vulnerabilities. Prior work has studied how LLMs could be maliciously fine-tuned to suggest vulnerabilities more often. With the rise of agentic LLMs, which may use results from an untrusted third party, there is a growing risk of attacks on the model's prompt. We introduce the Malicious Programming Prompt (MaPP) attack, in which an attacker adds a small amount of text to a prompt for a programming task (under 500 bytes). We show that our prompt strat-egy can cause an LLM to add vulnerabilities while continuing to write otherwise correct code. We evaluate three prompts on seven common LLMs, from basic to state-of-the-art commercial mod-els. Using the HumanEval benchmark, we find that our prompts are broadly effective, with no customization required for different LLMs. Furthermore, the LLMs that are best at HumanEval are also best at following our malicious instruc-tions, suggesting that simply scaling language models will not prevent MaPP attacks. Using a dataset of eight CWEs in 16 scenarios, we find that MaPP attacks are also effective at implement-ing specific and targeted vulnerabilities across a range of models. Our work highlights the need to secure LLM prompts against manipulation as well as rigorously auditing code generated with the help of LLMs.", "sections": [{"title": "1. Introduction", "content": "One of the most popular applications of large language models (LLMs) is assisting programmers in writing code. For example, GitHub Copilot had over 1.3 million paid"}, {"title": "2. Background", "content": ""}, {"title": "2.1. LLMs for Code Generation", "content": "LLMs are now being explicitly trained for code generation alongside natural language generation. In some cases, language models can even be specialized for certain programming languages like Python."}, {"title": "2.2. Safety of Code Generation", "content": "Prior work evaluates the security of GitHub Copilot under normal, non-adversarial usage over a set of test cases designed around some of the most common CWEs (common weakness enumeration). They find that LLMs often generate known security vulnerabilities found in their training data. Similar systematic testing shows that even highly capable models will generate security vulnerabilities across a range of languages. Other previous work tested the prevalence of security vulnerabilities when users are assisted by LLM code generation systems. Some user studies show that LLM coding assistants have minimal impact on security, while others show a large security impact in similar settings."}, {"title": "2.3. Adversarial Attacks on LLMs", "content": "Like many other systems, LLMs will inevitably have actors working against the interests of the users and developers. Prior work considers how an attacker could intentionally influence the output of an LLM. These attacks vary based on their assumptions about the adversary's knowledge and ca-pabilities. Given the ability to train or fine-tune the weights of an LLM, adversaries can use multiple different methods to compromise their security. For example, show that LLMs are more likely to generate insecure code if maliciously-crafted \"poison\" examples are added to their training data. In other attacks, the LLM only gen-erates malicious code when a particular trigger phrase is"}, {"title": "3. MaPP: Malicious Programming Prompt", "content": ""}, {"title": "3.1. Threat model", "content": "We consider an LLM-based programming assistant with a non-malicious user and a malicious attacker or adversary. The user's goal is to write correct and secure code using an agentic LLM, but they may not check the LLM-generated code very carefully. The attacker's goal is to induce vul-nerabilities in the user's code via the LLM. The attacker can insert a short string (under 500 characters) into the prompt through some means. Depending on the specific setting and the attacker's level of access, this could be done by directly editing the system prompt, manipulating a pro-cess used by the agentic LLM, or performing an indirect prompt attack through RAG. In the general purpose attack, the attacker succeeds if their prompt insertion causes the LLM to generate code with security vulnerabilities that also passes basic code correctnesss tests. The latter condition increases the chances of the user accepting and using the vulnerable code as-is. In the narrow attack, the attacker suc-ceeds if their prompt insertion causes the LLM to generate code with the scenario's security vulnerability as rated by GitHub CodeQL or human analysis."}, {"title": "3.2. General security vulnerabilities", "content": "Coming up with a general measure of overall vulnerability is difficult, because the settings in which these models are"}, {"title": "3.3. CWE vulnerabilities", "content": "In addition to general security vulnerabilities, we also eval-uate the effectiveness of MaPP attacks at creating specific vulnerabilities in narrow and more specific contexts. CWE (Common Weakness Enumeration) is a categorization of common security vulnerabilities that are often created natu-rally by programmers with no adversarial influences. Previous work from tested GitHub Copilot's propensity to naturally generate common security vulnera-bilities relevant to high-risk CWEs. To test common context-dependent vulnerabilities, we use the dataset from , henceforth referred to as the \"Asleep at the Keyboard\" dataset in reference to title of the original paper. We use 8 CWEs and 16 total scenarios that can be automati-cally checked in Python from the Asleep at the Keyboard dataset."}, {"title": "4. Evaluation", "content": "We now evaluate the effectiveness of our MaPP prompts on seven LLMs using the HumanEval coding benchmark and the Asleep at the Keyboard dataset."}, {"title": "Prompt construction", "content": "When evaluating our malicious prompts, we assume a generic, non-malicious system prompt as well as a user prompt that may have malicious instructions inserted into it. The system prompt contains"}, {"title": "4.1. General-purpose attacks", "content": "MaPP Effectiveness Rate We use all 164 test scenarios from the HumanEval dataset with the prompt structure as described in Figure 2. Each scenario (three attacks plus one non-malicious) is run for 10 samples with a model temperature of 1.0, generating a total of 6560 programs for each model. We take the average pass rate and average MaPP effectiveness rate among each test scenario. We do not adjust the parameters or prompting between different"}, {"title": "Pass Rate", "content": "The pass rate for each scenario is usually de-pendent on the complexity of the instructed MaPP. The more steps the model has to take to implement an attack, the lower score it will get on the given HumanEval task due to additional possibilities for mistakes. Weaker models struggle with the MemLeak test in particular, as they often forget to define the global variable and try to incorporate the malicious insert as part of the answer. However, more capa-ble models find a much smaller drop compared to control from the MemLeak test. There are performance drops in the MaPP tests compared to control, but for stronger mod-els the actual negative impacts to pass rate were fairly low. In some models, the ExFil and RandSeed attacks actually generate functional code more often than our control."}, {"title": "4.2. CWE attacks", "content": "We used a subset of scenarios from Asleep at the Keyboard's dataset that were both in Python and supported automatic vulnerabity evaluation using GitHub CodeQL. Each sce-nario was run with and without the MaPP attack at one sample each, for a total of 32 tests per model, and 224 test"}, {"title": "5. Discussion", "content": "LLMs are often fine-tuned to avoid bad behavior such as cre-ating vulnerabilities. However, none of the models have any problem following our instructions to generate malicious code. Part of this is the fact that the instructions themselves are completely reasonable instructions in some contexts; setting a random seed or repeatedly modifying a list are normal operations. The problem is in introducing them in the wrong context and without user awareness or consent. No \"jailbreak\" is required to obtain this behavior. This suggests that current approaches to LLM safety, including"}, {"title": "6. Conclusion", "content": "As LLMs become increasingly equipped with tools, inte-grated into developer applications, and placed within agentic frameworks, there are security concerns that need to be ad-dressed. Empirically, an attacker who inserts text into the prompt can induce vulnerabilities with a high success rate and minimal impact on the functional correctness of the code. In spite of attempts to make LLMs safe, the risk is highest with the largest, most capable models.\nMuch work remains to be done on making LLM-based programming systems more secure. Since our attacks rely on prompt modifications that the user never sees, the best defense is to make LLM systems more transparent."}, {"title": "7. Acknowledgements", "content": "This work was supported by a grant from the Defense Ad-vanced Research Projects Agency (DARPA), agreement HR00112090135. This work benefited from access to the University of Oregon high-performance computer, Talapas."}, {"title": "Social Impacts Statement", "content": "In this paper, we discuss vulnerabilities and implementation techniques for malicious attacks. There is an inherent risk of informing or encouraging potential attackers to perform these attacks. However, our tested vulnerabilities are quite simple in design and can be easily detected from a code review. Consequently, we believe the marginal risk posed by our paper to be minimal. By shedding light on the possibility of MaPP attacks, we hope to advance safety research and encourage safer practices and proactive measures before these and other attacks become prevalent in the wild."}]}