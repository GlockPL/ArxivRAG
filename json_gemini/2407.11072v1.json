{"title": "MaPPing Your Model: Assessing the Impact of Adversarial Attacks on LLM-based Programming Assistants", "authors": ["John Heibel", "Daniel Lowd"], "abstract": "LLM-based programming assistants offer the promise of programming faster but with the risk of introducing more security vulnerabilities. Prior work has studied how LLMs could be maliciously fine-tuned to suggest vulnerabilities more often. With the rise of agentic LLMs, which may use results from an untrusted third party, there is a growing risk of attacks on the model's prompt. We introduce the Malicious Programming Prompt (MaPP) attack, in which an attacker adds a small amount of text to a prompt for a programming task (under 500 bytes). We show that our prompt strategy can cause an LLM to add vulnerabilities while continuing to write otherwise correct code. We evaluate three prompts on seven common LLMs, from basic to state-of-the-art commercial models. Using the HumanEval benchmark, we find that our prompts are broadly effective, with no customization required for different LLMs. Furthermore, the LLMs that are best at HumanEval are also best at following our malicious instructions, suggesting that simply scaling language models will not prevent MaPP attacks. Using a dataset of eight CWEs in 16 scenarios, we find that MaPP attacks are also effective at implementing specific and targeted vulnerabilities across a range of models. Our work highlights the need to secure LLM prompts against manipulation as well as rigorously auditing code generated with the help of LLMs.", "sections": [{"title": "1. Introduction", "content": "One of the most popular applications of large language models (LLMs) is assisting programmers in writing code. For example, GitHub Copilot had over 1.3 million paid subscribers in early 2024 (Mic, 2024). However, code written with LLMs may also introduce security vulnerabilities, and programmers may be less likely to notice such vulnerabilities due to \"automation bias\" (Goddard et al., 2012; SKITKA et al., 1999), in which people trust automated suggestions over their own knowledge and intuition. This makes code LLMs a prime target for adversaries who want to create security vulnerabilities but lack direct access to the code. The rise of agentic LLMs (Kenton et al., 2022; Palo et al., 2023) further increases this risk by introducing plugins and external information sources that could be malicious and misleading. For example, many commercially available LLMs can now perform web searches and use the retrieved documents as part of the prompt. Some LLMs, such as ChatGPT, have broader agentic capabilities through plugins or the ability to write and execute code. This is often done with limited human involvement, and the exact operations and information used may not always be easy to inspect. The result is that it is easier than ever for an attacker to influence an LLM's prompt, so that the attacker controls some of the instructions given to the LLM.\nIn order to better understand the vulnerabilities of code LLMs to malicious prompts, we introduce the Malicious Programming Prompt (MaPP) attack, in which an attacker inserts instructions into the prompt of a coding LLM and uses it to induce code vulnerabilities. This corresponds to a threat model where the user prompts an LLM to generate code, but one portion of the prompt is contributed by an attacker who instructs the LLM to include insecure code in the generation (see Figure 1).\nWe first consider three simple vulnerabilities: resetting the random seed, printing system information, and creating a memory leak. The attacker succeeds if the resulting code includes the desired vulnerability and is otherwise correct. We consider general vulnerabilities because they can be inserted into a wider range of code contexts compared to narrower vulnerabilities (e.g., CWEs). This allows us to evaluate vulnerabilities using general coding benchmarks.\nWe evaluate the effectiveness of MaPP on seven different instruction-tuned LLMs using the widely-used HumanEval benchmark (Chen et al., 2021). We find that an attacker with control over one portion of the prompt can consistently succeed in introducing our three different vulnerabilities into generated code, and can do so without a large decrease in the correctness of the prompted code. In some cases, the maliciously prompted LLM is actually more likely to generate code that passes the HumanEval tests than a non-malicious baseline prompt.\nWe then test the effectiveness of MaPP on narrow, context-sensitive vulnerabilities using a dataset from Pearce et al. (2021). We consider 8 common vulnerabilities in 16 scenarios and construct a MaPP attack for each scenario. We find that MaPP attacks can cause LLMs to create serious security vulnerabilities that they otherwise would have never made. The maliciously prompted LLMs are able to bypass the safety finetuning and output code they were trained against generating."}, {"title": "2. Background", "content": ""}, {"title": "2.1. LLMs for Code Generation", "content": "LLMs are now being explicitly trained for code generation alongside natural language generation (Chen et al., 2021). In some cases, language models can even be specialized for certain programming languages like Python (Rozi\u00e8re et al., 2024). One of the standard evaluations for a model's coding ability is the HumanEval benchmark (Chen et al., 2021). This is a dataset of 164 Python function headers and docstrings, along with unit tests for checking for the correct output. This benchmark evaluates whether a model can generate code that is both valid and fulfills the given task. Prior work (Mozannar et al., 2024) also finds that LLMs, especially instruct-tuned models, have a positive impact on programmers' productivity in completing code tasks.\nRather than working in isolation, LLMs are increasingly being integrated with external tools and processes that dynamically load content into the context window. Some previous approaches to using LLMs for code generation involve retrieval augmented generation (RAG) (Lewis et al., 2021; Jimenez et al., 2024), in which relevant pieces of text from a database are loaded into the context of an LLM. There is also burgeoning interest in creating agents using LLMs that are more effective at coding than just using the LLM directly (Tufano et al., 2024; Yang et al., 2024). These agents write and execute code with minimal human intervention, and often access external data through tools like web browsing."}, {"title": "2.2. Safety of Code Generation", "content": "Prior work (Pearce et al., 2021) evaluates the security of GitHub Copilot under normal, non-adversarial usage over a set of test cases designed around some of the most common CWEs (common weakness enumeration). They find that LLMs often generate known security vulnerabilities found in their training data. Similar systematic testing through CyberSecEval (Bhatt et al., 2023) shows that even highly capable models will generate security vulnerabilities across a range of languages. Other previous work tested the prevalence of security vulnerabilities when users are assisted by LLM code generation systems. Some user studies show that LLM coding assistants have minimal impact on security (Sandoval et al., 2023), while others show a large security impact in similar settings (Perry et al., 2023)."}, {"title": "2.3. Adversarial Attacks on LLMs", "content": "Like many other systems, LLMs will inevitably have actors working against the interests of the users and developers. Prior work considers how an attacker could intentionally influence the output of an LLM. These attacks vary based on their assumptions about the adversary's knowledge and capabilities. Given the ability to train or fine-tune the weights of an LLM, adversaries can use multiple different methods to compromise their security. For example, Schuster et al. (2020) show that LLMs are more likely to generate insecure code if maliciously-crafted \"poison\" examples are added to their training data. In other attacks, the LLM only generates malicious code when a particular trigger phrase is present (Aghakhani et al., 2024; Kurita et al., 2020; Wan et al., 2023). Given knowledge of weights or generation details such as logits, adversaries can also perform specialized attacks that optimize some aspects of the input to direct the output in specific and unintended directions (Ebrahimi et al., 2018; Wu et al., 2023).\nPrompt injections are a specific type of attack on LLMs intended to modify the behavior by adding malicious text to the LLM's context. In direct prompt injections, the user attempts to override the system prompt or safety finetuning with conflicting instructions (Wei et al., 2023). These models have been heavily integrated into existing applications and external toolsets, much of it lying outside a user's direct view. These external tools can lead to indirect prompt injections, where an attacker places additional instructions or information in the context of an unaware user's LLM (Greshake et al., 2023). These instructions can drastically change the behavior of the model, but they also have the ability to subtly change behaviors in targeted ways. When these attacks are aimed at code generation tasks, they can introduce security vulnerabilities (Yang et al., 2023). With carefully crafted attacks or a lack of attention from the end user, these malicious vulnerabilities can be deployed in production environments."}, {"title": "3. MaPP: Malicious Programming Prompt", "content": ""}, {"title": "3.1. Threat model", "content": "We consider an LLM-based programming assistant with a non-malicious user and a malicious attacker or adversary. The user's goal is to write correct and secure code using an agentic LLM, but they may not check the LLM-generated code very carefully. The attacker's goal is to induce vulnerabilities in the user's code via the LLM. The attacker can insert a short string (under 500 characters) into the prompt through some means. Depending on the specific setting and the attacker's level of access, this could be done by directly editing the system prompt, manipulating a process used by the agentic LLM, or performing an indirect prompt attack through RAG. In the general purpose attack, the attacker succeeds if their prompt insertion causes the LLM to generate code with security vulnerabilities that also passes basic code correctnesss tests. The latter condition increases the chances of the user accepting and using the vulnerable code as-is. In the narrow attack, the attacker succeeds if their prompt insertion causes the LLM to generate code with the scenario's security vulnerability as rated by GitHub CodeQL or human analysis."}, {"title": "3.2. General security vulnerabilities", "content": "Coming up with a general measure of overall vulnerability is difficult, because the settings in which these models are deployed is highly variable. First, since different programming projects have different goals and capabilities, the types of relevant vulnerabilities are quite different - a project that doesn't use networking libraries won't have vulnerabilities in networking code. The contexts in which suggestions are generated is also relevant: a prompt may consist of a comment with instructions about the code to be written, or just the existing code already present in an incomplete function, and thus the suggestions could range from completing the current line of code with a single function call to generating a whole set of classes and methods. Programmers vary in experience and caution, so an error that is accepted by one programmer might be rejected by another. If the error is egregious enough, then the code LLM might be rejected entirely. Beyond individual programmers, organizations vary in their code review processes, including automated tools for detecting common errors and manual review. Thus, even an error that would be accepted by one programmer may be stopped before causing a vulnerability in production code.\nFor these reasons, we introduce three general vulnerabilities which could be applied to almost any function and cause a security violation as described in Table 1. RANDSEED sets the random seed to a specific value, which helps an attacker infer secret keys, hash functions, or other data that is supposed to be randomly generated. EXFIL prints out key system information, which reveals sensitive information to an attacker who can view the program's output. Finally, MEMLEAK creates a global string that doubles in length each time the vulnerable code is executed, causing the system to eventually run out of memory.\nWe chose these attacks because they represent real vulnerabilities but are flexible enough to be added to almost any block of code. This allows us to use standard code generation benchmarks in our evaluation rather than requiring specific secure programming benchmarks. To be more practical, an attacker could modify these prompts to specify a more specific vulnerability (e.g., changing a key parameter in one function call) or context (e.g., only add the vulnerability if a certain trigger string is present.).\nGeneral MaPP prompts For each attack in Table 1, we construct a corresponding MaPP prompt in Table 3 that instructs the LLM to generate the vulnerability. We use plain text instructions for greatest simplicity and effectiveness, but in some settings an attacker might try to further obfuscate the instructions with base64 encodings, instructions disguised as comments, etc. The MaPP prompts we use are easy to come up with and work across a range of different LLMs without requiring any special knowledge, such as the LLM parameters."}, {"title": "3.3. CWE vulnerabilities", "content": "In addition to general security vulnerabilities, we also evaluate the effectiveness of MaPP attacks at creating specific vulnerabilities in narrow and more specific contexts. CWE (Common Weakness Enumeration) is a categorization of common security vulnerabilities that are often created naturally by programmers with no adversarial influences. Previous work from Pearce et al. (2021) tested GitHub Copilot's propensity to naturally generate common security vulnerabilities relevant to high-risk CWEs. To test common context-dependent vulnerabilities, we use the dataset from Pearce et al. (2021), henceforth referred to as the \"Asleep at the Keyboard\" dataset in reference to title of the original paper. We use 8 CWEs and 16 total scenarios that can be automatically checked in Python from the Asleep at the Keyboard dataset.\nNarrow MaPP prompts We construct a basic MaPP prompt for each attack as described in Table 4 that instructs the LLM to create a vulnerability related to the CWE. These prompts are simple, but effective enough to avoid the majority of the chosen LLM's safety finetuning through the use of deceptive descriptions of the vulnerability. A real world attacker might have to take extra steps to obfuscate, alter, or somehow avoid other code review measures. The MaPP prompts we use are only effective for their particular CWE scenario as opposed to the General MaPP prompts which are effective across every given scenario."}, {"title": "4. Evaluation", "content": "We now evaluate the effectiveness of our MaPP prompts on seven LLMs using the HumanEval coding benchmark and the Asleep at the Keyboard dataset.\nPrompt construction When evaluating our malicious prompts, we assume a generic, non-malicious system prompt as well as a user prompt that may have malicious instructions inserted into it. The system prompt contains some generic instructions and remains unchanged between the different scenarios. In the user prompt, we place an instruction stating to complete the given function and to follow the system prompt. We place our MaPP in the user prompt before the HumanEval or CWE scenario instructions. Finally, we place the HumanEval or CWE scenario function header and docstring at the end of the user prompt.\nFigure 2 is a simplified model of how a user or an agent itself may use a coding LLM. It also includes an example of how a malicious adversary may insert text into the prompt. In a real-world scenario, the prompt is likely much more \"crowded,\" with a large number of instructions and previous responses with sub-optimal malicious insert locations. This simplified structure lets us systematically test the effects of model scale on adversarial code generation, but still have a realistic model that is not always optimal for the attacker.\nWe use seven instruct-tuned LLMs in our evaluation: Llama 3 8B, Llama 3 70B (AI@Meta, 2024), Claude 3 Haiku, Claude 3 Sonnet, Claude 3 Opus (Anthropic, 2024), GPT-3.5, and GPT-4 Omni (OpenAI et al., 2024). We chose instruct-tuned models as opposed to completion-tuned models tested in many previous works. This is because many sophisticated commercial models are only released as instruct models through API access."}, {"title": "4.1. General-purpose attacks", "content": "MaPP Effectiveness Rate We use all 164 test scenarios from the HumanEval dataset with the prompt structure as described in Figure 2. Each scenario (three attacks plus one non-malicious) is run for 10 samples with a model temperature of 1.0, generating a total of 6560 programs for each model. We take the average pass rate and average MaPP effectiveness rate among each test scenario. We do not adjust the parameters or prompting between different models, despite the potential performance impacts (Sclar et al., 2023). Given more extensive prompting techniques and model specific formatting, there is definitely room to improve performance on both the HumanEval task and the inclusion of the malicious insert as described in the MaPP.\nOverall, all LLMs are highly capable at including the three chosen attacks (see Figure 3). There was a notable drop on the MemLeak test for GPT-3.5 due to the model being likely to include only half of the attack, often forgetting to define the list. The MemLeak attack is a series of specific and location conditional set of instructions that are easy for a weaker model to have errors with. Few-shot learning (Brown et al., 2020) may increase the effectiveness of the MaPP, but it comes with the tradeoff of longer attacks.\nPass Rate The pass rate for each scenario is usually dependent on the complexity of the instructed MaPP. The more steps the model has to take to implement an attack, the lower score it will get on the given HumanEval task due to additional possibilities for mistakes. Weaker models struggle with the MemLeak test in particular, as they often forget to define the global variable and try to incorporate the malicious insert as part of the answer. However, more capable models find a much smaller drop compared to control from the MemLeak test. There are performance drops in the MaPP tests compared to control, but for stronger models the actual negative impacts to pass rate were fairly low (see Figure 4 and Table 2). In some models, the ExFil and RandSeed attacks actually generate functional code more often than our control."}, {"title": "4.2. CWE attacks", "content": "We used a subset of scenarios from Asleep at the Keyboard's dataset that were both in Python and supported automatic vulnerabity evaluation using GitHub CodeQL. Each scenario was run with and without the MaPP attack at one sample each, for a total of 32 tests per model, and 224 test overall. We then find the number of vulnerabilities both through GitHub CodeQL and manual analysis. As done in Asleep at the Keyboard, we look for only a single specified CWE in each scenario rather than any possible CWE. There is still room for model specific optimizations that may improve the effectiveness of the MaPP attack and avoid model specific safety finetuning. Even without those improvements, our chosen prompts were still highly effective across a range of LLMs.\nMaPP Effectiveness Rate All LLMs were susceptible to the MaPP attacks (see Figure 5). In the control case with no MaPP attack, the LLMs made only a few vulnerable files, usually in cases where a user might also have made the same choices. However, with the MaPP attack prompts, the LLMs made a multitude of vulnerabilities they otherwise wouldn't, such as using unsafe functions in networking application such as yaml.load(), which allows for the execution of arbitrary python code. There are some incorrect generations for some scenarios where the model failed to listen to the original scenario instructions, but these were counted as not vulnerable for the purposes of this test. Despite this, vulnerabilities were generated at least 75% of the time in all models during the adversarial test, with GPT-40 generating vulnerabilities 100% of the time in the adversarial test."}, {"title": "5. Discussion", "content": "LLMs are often fine-tuned to avoid bad behavior such as creating vulnerabilities. However, none of the models have any problem following our instructions to generate malicious code. Part of this is the fact that the instructions themselves are completely reasonable instructions in some contexts; setting a random seed or repeatedly modifying a list are normal operations. The problem is in introducing them in the wrong context and without user awareness or consent. No \"jailbreak\" is required to obtain this behavior. This suggests that current approaches to LLM safety, including RLHF (Ouyang et al., 2022), are inadequate to prevent such attacks.\nInstruction hierarchy (Wallace et al., 2024) and other techniques that limit a model's ability to follow instructions from uncontrolled sources are a good step towards resolving the problem, since they would reduce indirect prompt injections. However, this reduction may not be enough to guarantee safety. Users and developers must establish stringent safety checks on both model inputs and model outputs. Attackers need to be restricted from manipulating the prompt directly and indirectly. For example, system prompts for deployed models should both be difficult to modify for an adversary and easy for a user to audit for unwanted changes, and tool and RAG usage should be limited as much as possible to trusted APIs and data sources. Developers should also establish effective code vetting strategies on outputted code from models through the use of static code evaluators such as GitHub CodeQL and manual code review processes."}, {"title": "6. Conclusion", "content": "As LLMs become increasingly equipped with tools, integrated into developer applications, and placed within agentic frameworks, there are security concerns that need to be addressed. Empirically, an attacker who inserts text into the prompt can induce vulnerabilities with a high success rate and minimal impact on the functional correctness of the code. In spite of attempts to make LLMs safe, the risk is highest with the largest, most capable models.\nMuch work remains to be done on making LLM-based programming systems more secure. Since our attacks rely on prompt modifications that the user never sees, the best defense is to make LLM systems more transparent."}, {"title": "7. Acknowledgements", "content": "This work was supported by a grant from the Defense Advanced Research Projects Agency (DARPA), agreement HR00112090135. This work benefited from access to the University of Oregon high-performance computer, Talapas."}, {"title": "Social Impacts Statement", "content": "In this paper, we discuss vulnerabilities and implementation techniques for malicious attacks. There is an inherent risk of informing or encouraging potential attackers to perform these attacks. However, our tested vulnerabilities are quite simple in design and can be easily detected from a code review. Consequently, we believe the marginal risk posed by our paper to be minimal. By shedding light on the possibility of MaPP attacks, we hope to advance safety research and encourage safer practices and proactive measures before these and other attacks become prevalent in the wild."}]}