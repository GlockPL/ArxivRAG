{"title": "Predicting Word Similarity in Context with Referential Translation Machines", "authors": ["Ergun Bi\u00e7ici"], "abstract": "We identify the similarity between two words in English by casting the task as machine translation performance prediction (MTPP) between the words given the context and the distance between their similarities. We use referential translation machines (RTMs), which allows a common representation for training and test sets and stacked machine learning models. RTMs can achieve the top results in Graded Word Similarity in Context (GWSC) task.", "sections": [{"title": "1 Grading the Similarity of Words within Context", "content": "Graded Word Similarity in Context (GWSC) task (Armendariz et al., 2020) is about the similarity of two words graded in continuous scale in two different contexts c\u2081 and c\u2082 defined by 60 words on average and the change in the word pair similarity (wps) when the context is changed from C2 to C1. The subtasks are about unsupervised prediction of wps in both c\u2081 and c\u2082 and of the change in wps, which need not be found directly subtracting the two wpses. GWSC provides only the two words and the two contexts. The word pairs used are from SimLex999 dataset (Hill et al., 2015) in English, Croatian, Finnish, and Slovenian and we only participate in the task in English.\nGWSC is a reverse engineering task in the sense that the actual wps in different contexts are being predicted as if read, scored, and evaluated by humans. SimLex999 scores are in the range [0, 10] where 10 means the words are similar, 0 dissimilar, and 5 neutral. Additionally, with unsupervised learning, the task involves optimization before for obtaining the target scores that mimic wps and approach SimLex999 type similarity scores and after for obtaining consistent scores that fit in the score range and distance or the change in wps, which is a subtask of the task.\nWe obtain features to measure the inter and intra similarity of the two contexts, c\u2081 and C2. The intra similarity divides each context into 4 regions and builds an averaged similarity matrix using the semantic similarity databases to obtain the similarity of word pairs among words appearing in different regions. The inter similarity divides each context into 3 regions and again builds an averaged similarity matrix but computing for word pairs appearing in paired regions of the two contexts. Therefore, in the inter similarities, both the contexts and the words are paired. Intra word pair context is used for modeling the wps and inter word pair context is used for modeling the change in the similarity in two contexts. Both model semantic polarity or attraction and Figure 1 depicts both.\nIntra context wps (intra-cwps) features measure the positive or negative semantic attraction between the regions surrounding the word pair. In language modeling, a similarly named feature, statistical lexical attraction (Yuret, 1998), refers to the gain in the information to encode a sequence of words by bits using the mutual information of the probability that some paired words occur more likely in the same relative positions. Statistical lexical attraction is about the relatedness of words, which can be turned into a binary classification task, related or not, but GWSC is about wps, which can be turned into a ternary classificaion task: similar, dissimilar, and neutral. The averaged intra-cwps scores approach the SimLex999 type of similarity.\nInter context wps (inter-cwps) features measure the contextual changes in the semantics surrounding the target word pair in the two contexts and they are used to model the change in wps even if both contexts have the same score. As mentioned, semantic relatedness and similarity are different and word pairs are"}, {"title": "1.1 wps Features", "content": "We prepare 145 features for modeling semantic similarity (Table 3). Edit distance is the Levenshtein distance between strings, length is measured in characters, and maximum or minimum is between all"}, {"title": "1.2 Unsupervised Learning of Word Pair Similarity", "content": "Due to the unsupervised nature of the task, we are not provided the labels for the word pairs, which are actually from the SimLex-999 dataset we used to derive the features. Even though we did not use the wps scores directly from the dataset, their scores are indirectly included as a component averaged when calculating the intra and inter cwps scores. In the end, we find an artificial wps score that mimics the actual wps scores according to WPSIMDAT and to our contextual model of score averaging for wps and its difference in two different contexts. We refer to this score as awpss, artificial wps score. The scores of the training and test data are similar with mean, max, min on the training set are 5.7355, 10, 0.0686 and on the test set are 5.3704, 9.8, 0.417. The change in all of the 360 word pairs including the practice set is on average -0.55 and 0.94 in absolute terms in contrast, the change in the practice data is 1.159 on average and 1.771 after taking the absolute values of the change. The wps scores we obtain in the practice set is 5.725, which are close where we achieve the best scores. Even though SimLex-999 wps values are used within the awpss computations, they were also not included specifically for the"}, {"title": "1.3 Predicting the Intensity of the Structure and Content in Tweets", "content": "Affect in tweets 1 task (Task 1) (Mohammad et al., 2018) is about predicting the intensity of the emotion expressed for tweets within sadness, joy, fear, or anger emotion categorizations or the valance (sentiment). The emotion within tweets is about how the tweeter wrote and valence or sentiment is about what the tweeter wrote. Intensity scores are obtained with best-worst scaling (bws) (Mohammad and Kiritchenko, 2018), which counts only the number of times a tweet is labeled as best or worst among 4 tweets where each is annotated by multiple workers. The scores are obtained with the percentage of counts scaled to [\u22121, 1], which is later scaled to [0, 1] for the task. bws can decrease the annotation effort to obtain the set of binary comparisons used to obtain reliably agreed labels.2\nWe model the task as MTPP of the tweets to the emotions to answer questions like \u201cto what degree is this tweet showing the emotion of\". Since a single emotion word need not provide enough context for semantic discrimination, we use sets of words for each emotion that express the same meaning using a subset of the WordNet affect emotion lists (Strapparava and Valitutti, 2004). The lexicon used for English is in Appendix A. We obtained their translations to Arabic and Spanish using web translation sites 3 to obtain the corresponding lexicon. We use the whole set of words corresponding to the emotion instead of the emotion word to translate to. For valence intensity tasks, we used both of the sets of words from emotions joy and sadness as a single sentence to translate to for the tweet's MTPP. Task 1 predicts the emotion and valence intensity in Arabic, English, and Spanish tweets where the evaluation metric is Pearson's correlation. 4\""}, {"title": "1.4 Predicting the Attributes that Discriminate the Semantics", "content": "Capturing discriminative attributes task (Task 10, SemEval-2018) (Paperno et al., 2018) is looking at whether an attribute (e.g. red) can be used to discriminate between two other words (e.g. apple and banana) to complement semantic similarity efforts. The answer to whether the attribute can be used to discriminate the two words can be useful for semantic similarity with contextual dependency. The task is posed as a binary classification task: \\(f(w1,w2, a) \\rightarrow 0 ? 1\\) where F\u2081 is used for evaluation of target predictions that are either 0 or 1 showing whether the attribute can be used for discrimination for the given context. RTMs are used via casting the task as MTPP between the words and the attribute and building predictors that use the distance between the predictions. We assume that the discriminative power increase when the attribute is similar to words with significant difference. We apply similar approach in GWSC where we use the correponding contexts instead of the attribute and add a row for each: w\u2081 \u2192 C1, W2 \u2192 C2."}, {"title": "2 Stacked RTM Models for Predicting the Discriminative Power of Attributes", "content": "We use referential translation machine (RTM) models (Bi\u00e7ici and Way, 2015) for predicting wps in GWSC, which use parfda (Bi\u00e7ici, 2016a) to select both parallel and monolingual data close to the task instances selected specifically for the task, which are referred as interpretants, to derive features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets using machine translation performance prediction system (MTPPS) (Bi\u00e7ici et al., 2013; Bi\u00e7ici, 2022) to build prediction models. Interpretants provide context and text for feature derivation to link translation source and target and training and test sets. Interpretants are selected from the corpora distributed by the news translation task of WMT (WMT, 2017; Bojar et al., 2019) and they consist of monolingual sentences used to build the LM and parallel sentence pair instances used by MTPPS to derive the features. We built RTM models using:\n\u2022 250 thousand sentences for training data\n\u2022 5 million sentences for LM\nRTMs are applicable in different domains and tasks and in both monolingual and bilingual settings. Figure 2 explains RTMs' model building process where machine learning models including ridge regression (RR), support vector regression (SVR), AdaBoost (Freund and Schapire, 1997), and extremely randomized trees (TREE) (Geurts et al., 2006) in combination with feature selection (FS) (Guyon et al., 2002) and partial least squares (PLS) (Wold et al., 1984) are used. We use averaging of scores from different models for robustness (Bi\u00e7ici, 2017). Model implementations use scikit-learn. 5 We optimize \u03bb for RR, \u03b3, C, and e for SVR using grid search, minimum number of samples for leaf nodes and for splitting an internal node for TREE, the number of features for FS, and the number of dimensions for PLS. We use 500 estimators in the TREE model and also for AdaBoost. We evaluate with Pearson's correlation (r), mean absolute error (MAE), relative absolute error (RAE), MAER (mean absolute error relative), and MRAER (mean relative absolute error relative) (Bi\u00e7ici and Way, 2015). We use 7-fold cross-validation on the training set to rank models.\nRTMs generate features for the training and the test set to map both to the same space where the total number of features in Task 1 (Mohammad et al., 2018) becomes 492 and Task 10 becomes 117 (Paperno et al., 2018). The difference is due to the smaller context the attribute word provides and most of the sentence-level features become not useful including the sentence structure parsing features or word alignment features."}, {"title": "3 Conclusion", "content": "Referential translation machines obtain automatic prediction of semantic similarity using MTPP. We presented encouraging results with stacked RTM models for GWSC with our novel MTPP modeling for translation to context, for predicting the intensity of the structure and content in text with MTPP modeling for translation to WordNet emotion lists, and for the discriminative power of attributes using stacked RTM models. Our results also enable comparisons of prediction results of RTMs in different natural language processing tasks."}]}