{"title": "Evaluating Language Model Context Windows: A \"Working Memory\" Test and Inference-time Correction", "authors": ["Amanda Dsouzat", "Christopher Glaze\u2020", "Changho Shin", "Frederic Salato"], "abstract": "Large language models are prominently used in real-world applications, often tasked with reasoning over large\nvolumes of documents. An exciting development in this space is models boasting extended context capabilities,\nwith some accommodating over 2 million tokens. Such long context model capabilities remain uncertain in\nproduction systems, motivating the need to benchmark their performance on real world use cases. We address this\nchallenge by proposing SWiM, an evaluation framework that addresses the limitations of standard tests. Testing\nthe framework on eight long context models, we find that even strong models such as GPT-4 and Claude 3 Opus\ndegrade in performance when information is present in the middle of the context window (lost-in-the-middle\neffect). Next, in addition to our benchmark, we propose medoid voting, a simple, but effective training-free\napproach that helps alleviate this effect, by generating responses a few times, each time randomly permuting\ndocuments in the context, and selecting the medoid answer. We evaluate medoid voting on single document\nQA tasks, achieving up to a 24% lift in accuracy.", "sections": [{"title": "Introduction", "content": "Real-world applications increasingly use large language models (LLMs) to analyze extensive document collections.\nAlthough new models can ingest extended contexts, some accommodating up to 2 million tokens, their actual\ncapabilities whether they can effectively utilize long context input in practical applications - remain unclear.\nEvaluating their performance in real-world scenarios is important to enabling these models to operate in production\nsettings.\nEvaluating long context capabilities of LLMs has so far been restricted to the popular \"needle in a haystack\" (NIAH)\ntest [9], its variants [8], or through academic benchmarks [2, 12]. While these may serve as useful starting points\nfor evaluating new model releases, their relevance and applicability to real-world problems are unclear for several\nreasons:\n\u2022\n\u2022 Unrelated Tasks: The NIAH test often uses unrelated toy needles, which do not capture the intricacies of\ninformation retrieval in documents, where context and relevance are crucial.\nLimited Applicability: Academic benchmarks are often based on datasets that do not represent the nuances\nand complexities of documents found in real-world scenarios, such as financial reports, legal documents, or\ncustomer feedback.\n\u2022 Varying model performance: Model performance can vary significantly depending on the task and data. For\ninstance, experiments by [8] showed that even on standard tasks/datasets, GPT-4 accuracy went from 95-100%\non NIAH / retrieval tasks to 79.7% on word extraction tasks to 59.0% on QA tasks."}, {"title": "SWiM: Long Context Evaluation Framework for Real-world Tasks", "content": "To address these problems, we propose the Snorkel Work-\ning Memory Test (SWiM), an evaluation framework which\nmeasures long context capabilities of any language model on\nuse-case specific documents and task pairs. The test and its\nname are inspired by a wealth of research on human cognition\nsuggesting that short-term memory buffering (as is required\nin simple recall tasks such as the NIAH) is just one component\nof a more complex working memory network engaged in using\nbuffered information for more executive functions such as\nreasoning and goal-directed behavior [1, 5-7].\nThe SWIM test is executed with a four-step process (Figure 2)\nof task generation, task validation, task completion and task\nevaluation. Directly testing on the relevant data and tasks\nallows a more realistic assessment of a model's long context\ncapabilities for unique applications. Our code is available at\nhttps://github.com/snorkel-ai/long-context-eval.\nWe use SWiM to test eight long context models (including\nfrom Open AI, Anthropic, Google, and Mistral), for their\neffective context length, and the effect of position. Most long\ncontext models are ineffective at retrieving information in the\nmiddle of the context window (confirming \"lost-in-the-middle\"\neffect [10]). To mitigate this effect, we additionally propose a\nsimple algorithmic approach, medoid voting, a straightforward yet effective training-free method that improves\nperformance.\nThe framework takes the following four steps.\n1. Task generation: In order to benchmark long context models on use-case specific data, we automate the creation"}, {"title": "Task generation", "content": "In order to benchmark long context models on use-case specific data, we automate the creation"}, {"title": "Task validation", "content": "Using language models to replace human intensive tasks such as dataset creation and evaluation\nhas become commonplace as strong models continue to be released. They are however, prone to errors and\nwithout a validation step allows these inaccuracies to trickle down the pipeline, leading to compounding errors\nand ultimately influencing reported results.\nWhile prompting and adding guardrails help to some extent, we find that the validation step cannot be\ncircumvented, and a human-in-the-loop approach is best at ensuring good data at the input and output."}, {"title": "Task Completion", "content": "SWiM benchmark supports document QA retrieval, and tests specifically for (a) the effect\nof position on retrieval accuracy, (b) the effect of context size (the level of noise) on retrieval performance.\n(a) Effect of position: [10] showed that models are better at retrieving information from the top or bottom of\na long context, while performance greatly reduces when it is contained in the middle. They refer to this\nphenomenon as the \"lost-in-the-middle\" effect. In a similar vein, the popular \"needle-in-a-haystack\" test\nmeasures how position affects model performance by injecting a synthetic needle into a set of essays.\nWe conduct a similar test to measure how positioning the document within the context window affects\nretrieving the answer from it. This is similar to the \"needle-in-a-haystack\" test except over user documents\nand realistic needles, rather than the synthetic test. This also effectively tests the \"lost-in-the-middle\" effect,\nalthough the experiment setup is slightly different. Liu et al. test on a constant number of 10, 20 and 30\ndistractor documents (documents that do not contain the answer, but are in the same domain as the answer\ndocument), that may not reach the full context window of the model, as well as present the distractor\ndocuments in order of decreasing relevance. In contrast, SWiM tests a model on its full context window,\nwith distractor documents shuffled at random. To measure the effect of position on retrieval performance,\nwe follow the procedure in 1. In our experiments, we test varying depths of the true response at 0, 25, 50, 75,\nand 100% depths.\n(b) Effect of context size: Starting with just the answer document in context (0% noise), distractors are\nsuccessively added, increasing the capacity to 25, 50, 70 and 100% of the context window and model responses\ngenerated to test the effective context length of the model in the presence of distractors. To measure the\neffect of context window used on retrieval performance, we follow the procedure in 2."}, {"title": "Evaluating responses", "content": "We use LLM-as-a-judge to evaluate the responses. On the Single Document QA task,\nwe use the following prompt.\nAs with the task generation step, evaluation with a language model is error prone. While the task is a relatively\nsimple one (check if both answers are correct in semantics), there are cases where we find the LLM judge predicts\nthe wrong score. In some cases, the responses contain additional detail, which requires validating the additional"}, {"title": "Medoid Voting: a simple solution to lost-in-the-middle effect", "content": "We propose an efficient solution to the lost-in-the-middle effect. Since we know models are generally less effective at\nretrieving information from some positions than others, it would help if we positioned the answer documents so\nthat they are in the right places. Of course, we do not know what the right place is a priori. A possible solution to\nthis problem is to run the task completion a few times, each time randomly permuting documents in the context,\nand then use a selection criteria to pick the best response. We consider the medoid response (response with the\nleast dissimilarity to all other responses) in the embedding space as our selection criteria. The detailed procedure is\ndescribed in Algorithm 3."}, {"title": "Experimental Results", "content": "For our experiments, we create QA pairs using GPT-4, from the synthetically generated Huggingface Cosmopedia\nstory_forum dataset [3], treating each instance as a document. We use SWiM to test long context models by Open\nAI, Anthropic, Google, and Mistral on the Document QA task. Specifically, we evaluate LLMs' robustness to\ndistractor documents and \"lost-in-the-middle\" effects. Finally, we validate the effectiveness of medoid voting."}, {"title": "Evaluation on robustness to distractor documents", "content": "We evaluate robustness to distractor documents given in the context. This investigates how well LLMs can extract\nthe necessary information without being distracted to irrelevant context.\nSetup. We test single document QA performance with an increasing number of documents that do not contain the\nanswer (distractor documents). Starting with only the answer document, distractors are added to fill up the context\nwindow to 25, 50, 75 and 100% of its capacity\u00b2, and the set is shuffled within the context window for response\ngeneration.\nResults. Figure 3 (left) shows the experiment results. Unsurprisingly, as the number of distractors increase,\nperformance degrades. But this degradation is not uniform across models. Among models with a long context\nwindow (1M tokens), Gemini-1.5-Pro does extremely well to handle noise. It is also unsurprising that smaller context\nlength models (such as GPT-3.5 Turbo and Mistral-8x7B-Instruct) are more effective at using their context lengths\ncompared to larger context length ones. Analyzing incorrect responses, we find that many of these are due to how\ndocuments are positioned in the context, which we discuss next."}, {"title": "Evaluation on robustness to document position", "content": "We evaluate long-context LLMs' robustness to document position. It is well known that position affects the retrieval\nperformance of long context LLMs, but the effect varies depending on benchmarks and models [10, 12]. We expect\nthat lost-in-the-middle effect is common to long context models.\nSetup. We use SWiM to test eight models on the single document QA task. We place the answer document at\nthe 25, 50, 75 and 100% positions in the context length and fill up the rest of the context window with randomly\nshuffled distractor documents. Note that since we first compute the number of tokens of documents to fit the context\nwindow of a model, the number of documents used for a model may differ based on the model's context window\nsize. Across the position tests for a given model, the set of documents is kept fixed, to reduce confounding effects of\ndistractors on the performance.\nResults. Figure 3 (left) shows the experiment results. Most models exhibited a degradation in performance at the\n25%~75% depth and showed their best performance when the answer is located at 0% depth or 100% depth. The\nobserved performance degradation of long context models in the middle poses significant implications for real world\napplications.\nAdditionally, this result implies that the NIAH test, while a good way to quickly test new models, is often not a\ngood indicator of real world performance. This is evident in Claude-2.1's results, where NIAH results showed strong\nperformance when the needle was at the top and bottom of the context, but we do not find the same behavior when\nthe document is at the top of a long context."}, {"title": "Medoid voting", "content": "Setup. We tested medoid voting on two models that observed the lost-in-the-middle effect, GPT-4-Turbo and\nGPT-3.5-Turbo-16k. To isolate the impact of the medoid voting, we conducted a control experiment that generates\nmultiple runs keeping document position constant at the unfavorable 25% document depth, with model stochasticity\nas the only source of variation. We used temperatures of 0.7 (default) and 1.4 (a high temperature to induce higher\nvariance in responses). This lets us test the specific effect that varying position has over other forms of variation in"}, {"title": "Related Work", "content": "Existing long context benchmarks. As the effectiveness\nof in-context learning [4] has been studied, benchmarks to evaluate long-context capacity of LLMs have been actively\ndeveloped. Long Range Arena [11] evaluates LLMs with the context length from 1K to 16K tokens, including various\ndata types and modalities such as text, images, and mathematical expressions. The needle in a haystack [9] (NIAH)\ntest evaluates LLMs' capability to retrieve answers from the given context (essays). RULER benchmark [8] expands\nupon the NIAH test to include more variations such as multi-hop tracing and aggregation. LongBench [2] provides a\nbenchmark with single-doc QA, multi-doc QA, summarization, few-shot learning, code completion, and synthetic\ntasks in English and Chinese. \u221eBench [12] provides datasets and pipelines to evaluate LLMs with 100K+ context\nin English and Chinese. While these have been useful for evaluating long-context capability of LLMs, they lack\ncustomizability their tasks might not be directly relevant to the problems encountered in business applications.\nOur framework, instead, provides an end-to-end customizable evaluation pipeline."}, {"title": "Conclusion", "content": "We propose the SWiM framework, which enables users to create a personalized benchmark to evaluate long context\nmodels on their data with their tasks. Experimental results suggest that SWiM can identify patterns of errors that\nNIAH cannot, so we strongly recommend usage of SWiM prior to any development work for specific applications.\nHowever, there is more experimental work required to draw general conclusions about the language models used in\nresearch reported here. While we saw that some models such as Gemini-1.5-Pro do extremely well handling long\ncontexts in the setting of single document QA, they may be less effective in more complex scenarios. These complex\nscenarios include relevant documents that have been combined with many other similar documents; tasks requiring\nreasoning over multiple documents; and tasks requiring citations to specific sources within documents. Future\ndevelopment of the SWiM framework should address these complex scenarios with more fine grained evaluations\n(e.g. including hallucination detection), and on a wider array of tasks."}]}