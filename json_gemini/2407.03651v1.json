{"title": "Evaluating Language Model Context Windows:\nA \"Working Memory\" Test and Inference-time Correction", "authors": ["Amanda Dsouzat", "Christopher Glaze", "Changho Shin", "Frederic Salato"], "abstract": "Large language models are prominently used in real-world applications, often tasked with reasoning over large\nvolumes of documents. An exciting development in this space is models boasting extended context capabilities,\nwith some accommodating over 2 million tokens. Such long context model capabilities remain uncertain in\nproduction systems, motivating the need to benchmark their performance on real world use cases. We address this\nchallenge by proposing SWiM, an evaluation framework that addresses the limitations of standard tests. Testing\nthe framework on eight long context models, we find that even strong models such as GPT-4 and Claude 3 Opus\ndegrade in performance when information is present in the middle of the context window (lost-in-the-middle\neffect). Next, in addition to our benchmark, we propose medoid voting, a simple, but effective training-free\napproach that helps alleviate this effect, by generating responses a few times, each time randomly permuting\ndocuments in the context, and selecting the medoid answer. We evaluate medoid voting on single document\nQA tasks, achieving up to a 24% lift in accuracy.", "sections": [{"title": "Introduction", "content": "Real-world applications increasingly use large language models (LLMs) to analyze extensive document collections.\nAlthough new models can ingest extended contexts, some accommodating up to 2 million tokens, their actual\ncapabilities whether they can effectively utilize long context input in practical applications - remain unclear.\nEvaluating their performance in real-world scenarios is important to enabling these models to operate in production\nsettings.\nEvaluating long context capabilities of LLMs has so far been restricted to the popular \"needle in a haystack\" (NIAH)\ntest [9], its variants [8], or through academic benchmarks [2, 12]. While these may serve as useful starting points\nfor evaluating new model releases, their relevance and applicability to real-world problems are unclear for several\nreasons:\n\u2022 Unrelated Tasks: The NIAH test often uses unrelated toy needles, which do not capture the intricacies of\ninformation retrieval in documents, where context and relevance are crucial.\n\u2022 Limited Applicability: Academic benchmarks are often based on datasets that do not represent the nuances\nand complexities of documents found in real-world scenarios, such as financial reports, legal documents, or\ncustomer feedback.\n\u2022 Varying model performance: Model performance can vary significantly depending on the task and data. For\ninstance, experiments by [8] showed that even on standard tasks/datasets, GPT-4 accuracy went from 95-100%\non NIAH / retrieval tasks to 79.7% on word extraction tasks to 59.0% on QA tasks."}, {"title": "SWiM: Long Context Evaluation Framework for Real-world Tasks", "content": "To address these problems, we propose the Snorkel Work-\ning Memory Test (SWiM), an evaluation framework which\nmeasures long context capabilities of any language model on\nuse-case specific documents and task pairs. The test and its\nname are inspired by a wealth of research on human cognition\nsuggesting that short-term memory buffering (as is required\nin simple recall tasks such as the NIAH) is just one component\nof a more complex working memory network engaged in using\nbuffered information for more executive functions such as\nreasoning and goal-directed behavior [1, 5-7].\nThe SWIM test is executed with a four-step process (Figure 2)\nof task generation, task validation, task completion and task\nevaluation. Directly testing on the relevant data and tasks\nallows a more realistic assessment of a model's long context\ncapabilities for unique applications. Our code is available at\nhttps://github.com/snorkel-ai/long-context-eval.\nWe use SWiM to test eight long context models (including\nfrom Open AI, Anthropic, Google, and Mistral), for their\neffective context length, and the effect of position. Most long\ncontext models are ineffective at retrieving information in the\nmiddle of the context window (confirming \"lost-in-the-middle\"\neffect [10]). To mitigate this effect, we additionally propose a\nsimple algorithmic approach, medoid voting, a straightforward yet effective training-free method that improves\nperformance.\nThe framework takes the following four steps.\n1. Task generation: In order to benchmark long context models on use-case specific data, we automate the creation"}, {"title": "Medoid Voting: a simple solution to lost-in-the-middle effect", "content": "We propose an efficient solution to the lost-in-the-middle effect. Since we know models are generally less effective at\nretrieving information from some positions than others, it would help if we positioned the answer documents so\nthat they are in the right places. Of course, we do not know what the right place is a priori. A possible solution to\nthis problem is to run the task completion a few times, each time randomly permuting documents in the context,\nand then use a selection criteria to pick the best response. We consider the medoid response (response with the\nleast dissimilarity to all other responses) in the embedding space as our selection criteria. The detailed procedure is\ndescribed in Algorithm 3."}, {"title": "Experimental Results", "content": "For our experiments, we create QA pairs using GPT-4, from the synthetically generated Huggingface Cosmopedia\nstory_forum dataset [3], treating each instance as a document. We use SWiM to test long context models by Open\nAI, Anthropic, Google, and Mistral on the Document QA task. Specifically, we evaluate LLMs' robustness to\ndistractor documents and \"lost-in-the-middle\" effects. Finally, we validate the effectiveness of medoid voting."}, {"title": "Evaluation on robustness to distractor documents", "content": "We evaluate robustness to distractor documents given in the context. This investigates how well LLMs can extract\nthe necessary information without being distracted to irrelevant context.\nSetup. We test single document QA performance with an increasing number of documents that do not contain the\nanswer (distractor documents). Starting with only the answer document, distractors are added to fill up the context\nwindow to 25, 50, 75 and 100% of its capacity\u00b2, and the set is shuffled within the context window for response\ngeneration.\nResults. Figure 3 (left) shows the experiment results. Unsurprisingly, as the number of distractors increase,\nperformance degrades. But this degradation is not uniform across models. Among models with a long context\nwindow (1M tokens), Gemini-1.5-Pro does extremely well to handle noise. It is also unsurprising that smaller context\nlength models (such as GPT-3.5 Turbo and Mistral-8x7B-Instruct) are more effective at using their context lengths\ncompared to larger context length ones. Analyzing incorrect responses, we find that many of these are due to how\ndocuments are positioned in the context, which we discuss next."}, {"title": "Medoid voting", "content": "Setup. We tested medoid voting on two models that observed the lost-in-the-middle effect, GPT-4-Turbo and\nGPT-3.5-Turbo-16k. To isolate the impact of the medoid voting, we conducted a control experiment that generates\nmultiple runs keeping document position constant at the unfavorable 25% document depth, with model stochasticity\nas the only source of variation. We used temperatures of 0.7 (default) and 1.4 (a high temperature to induce higher\nvariance in responses). This lets us test the specific effect that varying position has over other forms of variation in"}, {"title": "Related Work", "content": "Existing long context benchmarks. As the effectiveness\nof in-context learning [4] has been studied, benchmarks to evaluate long-context capacity of LLMs have been actively\ndeveloped. Long Range Arena [11] evaluates LLMs with the context length from 1K to 16K tokens, including various\ndata types and modalities such as text, images, and mathematical expressions. The needle in a haystack [9] (NIAH)\ntest evaluates LLMs' capability to retrieve answers from the given context (essays). RULER benchmark [8] expands\nupon the NIAH test to include more variations such as multi-hop tracing and aggregation. LongBench [2] provides a\nbenchmark with single-doc QA, multi-doc QA, summarization, few-shot learning, code completion, and synthetic\ntasks in English and Chinese. \u221eBench [12] provides datasets and pipelines to evaluate LLMs with 100K+ context\nin English and Chinese. While these have been useful for evaluating long-context capability of LLMs, they lack\ncustomizability their tasks might not be directly relevant to the problems encountered in business applications.\nOur framework, instead, provides an end-to-end customizable evaluation pipeline."}, {"title": "Conclusion", "content": "We propose the SWiM framework, which enables users to create a personalized benchmark to evaluate long context\nmodels on their data with their tasks. Experimental results suggest that SWiM can identify patterns of errors that\nNIAH cannot, so we strongly recommend usage of SWiM prior to any development work for specific applications.\nHowever, there is more experimental work required to draw general conclusions about the language models used in\nresearch reported here. While we saw that some models such as Gemini-1.5-Pro do extremely well handling long\ncontexts in the setting of single document QA, they may be less effective in more complex scenarios. These complex\nscenarios include relevant documents that have been combined with many other similar documents; tasks requiring\nreasoning over multiple documents; and tasks requiring citations to specific sources within documents. Future\ndevelopment of the SWiM framework should address these complex scenarios with more fine grained evaluations\n(e.g. including hallucination detection), and on a wider array of tasks."}]}