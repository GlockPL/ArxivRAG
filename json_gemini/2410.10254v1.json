{"title": "LOLCATS: On Low-Rank Linearizing of Large Language Models", "authors": ["Michael Zhang", "Simran Arora", "Rahul Chalamalas", "Alan Wu", "Benjamin Spector", "Aaryan Singhal", "Krithik Ramesh", "Christopher R\u00e9"], "abstract": "Recent works show we can linearize large language models (LLMs)-swapping the quadratic attentions\nof popular Transformer-based LLMs with subquadratic analogs, such as linear attention-avoiding the\nexpensive pretraining costs. However, linearizing LLMs often significantly degrades model quality, still\nrequires training over billions of tokens, and remains limited to smaller 1.3B to 7B LLMs. We thus propose\nLow-rank Linear Conversion via Attention Transfer (LOLCATS), a simple two-step method that improves\nLLM linearizing quality with orders of magnitudes less memory and compute. We base these steps on two\nfindings. First, we can replace an LLM's softmax attentions with closely-approximating linear attentions,\nsimply by training the linear attentions to match their softmax counterparts with an output MSE loss\n(\"attention transfer\"). Then, this enables adjusting for approximation errors and recovering LLM quality\nsimply with low-rank adaptation (LoRA). LOLCATS significantly improves linearizing quality, training\nefficiency, and scalability. We significantly reduce the linearizing quality gap and produce state-of-the-art\nsubquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading to 20+ points of improvement on\n5-shot MMLU. Furthermore, LOLCATs does so with only 0.2% of past methods' model parameters and\n0.4% of their training tokens. Finally, we apply LOLCATS to create the first linearized 70B and 405B\nLLMs (50x larger than prior work). When compared with prior approaches under the same compute\nbudgets, LOLCATS significantly improves linearizing quality, closing the gap between linearized and\noriginal Llama 3.1 70B and 405B LLMs by 77.8% and 78.1% on 5-shot MMLU.", "sections": [{"title": "1 Introduction", "content": "\"Linearizing\" large language models (LLMs)\u2014or converting existing Transformer-based LLMs into attention-\nfree or subquadratic alternatives-has shown promise for scaling up efficient architectures. While many such\narchitectures offer complexity-level efficiency gains, like linear-time and constant-memory generation, they\nare often limited to smaller models pretrained on academic budgets [4, 5, 24, 39, 63]. In a complementary\ndirection, linearizing aims to start with openly available LLMs-e.g., those with 7B+ parameters trained on\ntrillions of tokens [2, 28]\u2014and (i) swap their softmax attentions with subquadratic analogs, before (ii) further\nfinetuning to recover quality. This holds exciting promise for quickly scaling up subquadratic capabilities.\nHowever, to better realize this promise and allow anyone to convert LLMs into subquadratic models, we\ndesire methods that are (1) quality-preserving, e.g., to recover the zero-shot abilities of modern LLMs; (2)\nparameter and token efficient, to linearize LLMs on widely accessible compute; and (3) highly scalable,\nto support linearizing the various 70B+ LLMs available today [56, 57].\nExisting methods present opportunities to improve all three criteria. On quality, despite using motivated\nsubquadratic analogs such as RetNet-inspired linear attentions [35, 54] or state-space model (SSM)-based\nMamba layers [24, 60, 64], prior works significantly reduce performance on popular LM Evaluation Harness\ntasks (LM Eval) [21] (up to 23.4-28.2 pts on 5-shot MMLU [26]). On parameter and token efficiency, to\nadjust for architectural differences, prior methods update all model parameters in at least one stage of"}, {"title": "2 Preliminaries", "content": "To motivate LOLCATS, we first go over Transformers, attention, and linear attention. We then briefly\ndiscuss related works on linearizing Transformers and Transformer-based LLMs.\nTransformers and Attention. Popular LLMs such as Llama 3 8B [3] and Mistral 7B [28] are decoder-\nonly Transformers, with repeated blocks of multi-head softmax attention followed by MLPs [58]. For one\nhead, attention computes outputs $y \\in \\mathbb{R}^{l \\times d}$ from inputs $x \\in \\mathbb{R}^{l \\times d}$ (where $l$ is sequence length, $d$ is head\ndimension) with query, key, and value weights $W_q, W_k, W_v \\in \\mathbb{R}^{d \\times d}$. In causal language modeling, we\ncompute $q = xW_q, k = xW_k, v = xW_v$, before getting attention weights $a$ and outputs $y$ via\n$y_n = \\sum_{i=1}^n a_{n,i} v_i$, for $n$ in $[1, ..., l]$\n(1)\nMulti-head attention maintains inputs, outputs, and weights for each head, e.g., $x \\in \\mathbb{R}^{h \\times l \\times d}$ or $W_q \\in \\mathbb{R}^{h \\times d \\times d}$\n($h$ being number of heads), and computes Eq. 1 for each head. In both cases, we compute final outputs\nby concatenating $y_n$ across heads, before using output weights $W_o \\in \\mathbb{R}^{hd \\times hd}$ to compute $y_nW_o \\in \\mathbb{R}^{l \\times hd}$.\nWhile expressive, causal softmax attention requires all ${k_i, v_i}_{i<n}$ to compute $y_n$. For long context or\nlarge batch settings, this growing KV cache can incur prohibitive memory costs even with state-of-the-art\nimplementations such as FlashAttention [15]."}, {"title": "Linear Attention", "content": "To get around this, Katharopoulos et al. [30] show a similar attention operation, but\nwith linear time and constant memory over generation length (linear time and space when processing inputs).\nTo see how, note that softmax attention's exponential is a kernel function $K(q_n, k_i)$, which in general can\nbe expressed as the dot product of feature maps $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d'}$. Swapping $exp(q_nk_i/\\sqrt{d})$ with $\\phi(q_n)^T \\phi(k_i)$\nin Eq. 1 gives us linear attention weights and outputs:\n$\\hat{a}_{n,i} = \\frac{\\phi(q_n)^T \\phi(k_i)}{\\sum_{i'=1}^n \\phi(q_n)^T \\phi(k_i)}$,\n(2)\n$\\hat{y}_n = \\sum_{i=1}^n \\hat{a}_{i n} V_i$\nRearranging terms via matrix product associativity, we get\n$\\hat{y}_n = \\frac{\\sum_{i=1}^n \\phi(q_n)^T \\phi(k_i) v_i}{\\sum_{i'=1}^n \\phi(q_n)^T \\phi(k_i)} = \\frac{\\phi(q_n)^T \\sum_{i=1}^n \\phi(k_i) v_i}{\\phi(q_n)^T \\sum \\phi(k_i)}$.\n(3)\nThis lets us compute both the numerator $s_n = \\sum_{i=1}^n \\phi(k_i)v_i$ and denominator $z_n = \\sum_{i=1}^n \\phi(k_i)$ as recurrent\n\"KV states\". With $s_0 = 0, z_0 = 0$, we recurrently compute linear attention outputs as\n$\\hat{y}_n = \\frac{\\phi(q_n)^T s_n}{\\phi(q_n)^T z_n}$ for $s_n = s_{n-1} + \\phi(k_n) v_n$ and $z_n = z_{n-1} + \\phi(k_n)$\n(4)\nEq. 3 lets us compute attention over an input sequence of length n in $O(ndd')$ time and space, while Eq. 4\nlets us compute n new tokens in $O(ndd')$ time and $O(dd')$ memory. Especially during generation, when\nsoftmax attention has to compute new tokens sequentially anyway, Eq. 4 enables time and memory savings\nif $d' <$ (prompt length + prior generated tokens)."}, {"title": "Linearizing Transformers", "content": "To combine efficiency with quality, various works propose different $\\phi$, (e.g.,\n$\\phi(x) = 1 + ELU(x)$ as in [30]). However, they typically train linear attention Transformers from scratch.\nWe build upon recent works that swap the softmax attentions of existing Transformers with linear attention\nbefore finetuning the modified models with next-token prediction to recover language modeling quality. These\ninclude methods proposed for LLMs [35], and those for smaller Transformers-e.g., 110M BERTS [19])\nreasonably adaptable to modern LLMs [29, 34, 66]."}, {"title": "3 Method: Linearizing LLMs with LoLCATS", "content": "We now study how to build a high-quality and highly efficient linearizing method. In Section 3.1, we\npresent our motivating framework, which aims to (1) learn good softmax attention approximators with\nlinear attentions and (2) enable low-rank adaptation for recovering linearized quality. In Section 3.2, we\nfind that while this attention transfer works surprisingly well for low-rank linearizing with existing linear\nattentions, on certain tasks, it still results in sizable quality gaps compared to prior methods. We also\nfind that attention-transfer quality strongly corresponds with the final linearized model's performance. In\nSection 3.3, we use our learned findings to overcome prior issues, improving attention transfer to subsequently\nimprove low-rank linearizing quality."}, {"title": "3.1 A Framework for Low-cost Linearizing", "content": "In this section, we present our initial LOLCATS framework for linearizing LLMs in an effective yet efficient\nmanner. Our main hypothesis is that by first learning linear attentions that approximate softmax, we can\nthen swap these attentions in as drop-in subquadratic replacements. We would then only need a minimal\namount of subsequent training-e.g., that is supported by low-rank updates to recover LLM quality in a\ncost-effective manner effectively. We thus proceed in two steps.\n1. Parameter-Efficient Attention Transfer. For each softmax attention in an LLM, we aim to learn a\nclosely-approximating linear attention, i.e., one that computes attention outputs $\\hat{y} \\approx y$ for all natural"}, {"title": "Low-rank Adjusting", "content": "After training the linearizing layers, we replace the full-parameter training of\nprior work with low-rank adaptation (LoRA) [27]. Like prior work, to adjust for the modifying layers and\nrecover language modeling quality, we now train the modified LLM end-to-end over tokens to minimize\na sample next-token prediction loss $l_{xent} = -\\sum log P_{\\Theta}(u_{t+1} | u_{1:t})$. Here $P_{\\Theta}$ is the modified LLM, $\\Theta$ is\nthe set of LLM parameters, and we aim to maximize the probability of true $u_{t+1}$ given past tokens $u_{1:t}$\n(Fig. 1 right). However, rather than train all LLM parameters, we only train the swapped linear attention\n$W_q, W_k, W_w, W_o$ with LoRA. Instead of full-rank updates, $W' \\leftarrow W + \\Delta W$, LORA decomposes $\\Delta W$\nas the product of two low-rank matrices $BA$, $B\\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times d}$. For parameter efficiency, we aim to\npick small $r < d$."}, {"title": "Baseline Study: Attention Transfer and Low-rank Linearizing", "content": "We now aim to understand if attention transfer and low-rank adjusting are sufficient for linearizing LLMs.\nIt is unclear whether these simple steps can lead to high-quality LLMs, given that prior works default\nto more involved approaches [35, 60, 64]. They use linearizing layers featuring GroupNorms [61] and decay\nfactors [54], or alternate SSM-based architectures [16, 24]. They also all use full-LLM training after swapping\nin the subquadratic layers. In contrast, we find that simple linear attentions can lead to viable linearizing,\nwith attention transfer + LORA obtaining competitive quality on 4 / 6 popular LM Eval tasks.\nExperimental Setup. We test the LOLCATS framework by linearizing two popular base LLMs, Llama 3\n8B [2] and Mistral 7B v0.1 [28]. For linearizing layers, we study two feature maps used in prior work (T2R [29]\nand Hedgehog [66], Table 2). To support the rotary positional embeddings (ROPE) [53] in these LLMs, we\napply the feature maps $\\phi$ after ROPE, i.e., computing query features $\\phi_q(q) = f(RoPE(q)W_q + b))$. For"}, {"title": "LoL SAD: Limitations of Low-Rank Linearizing", "content": "At the same time, we note quality limitations with\nthe present framework. While sometimes close, low-rank linearized LLMs perform worse than full-parameter\nalternatives and original Transformers on 5 / 6 LM Eval tasks (up to 42.4 points on 5-shot MMLU; Table 4).\nTo understand the issue, we study whether the attention transfer stage can produce high-fidelity linear\napproximations of softmax attention. We note four observations:"}, {"title": "3.3 LoLCATs: Improved Low-rank Linearizing", "content": "We now introduce two simple improvements in architecture (Section 3.3.1) and linearizing procedure (Sec-\ntion 3.3.2) to improve low-rank linearizing quality."}, {"title": "3.3.1 Architecture: Generalizing Learnable Linear Attentions", "content": "As described, we can apply our framework with any linear attentions with learnable $\\phi$ (e.g., T2R and Hedge-\nhog, Figure 3). However, to improve attention-matching quality, we introduce a hybrid $\\phi$ parameterization\ncombining linear attention and sliding window attention. Motivated by prior works that show quality im-\nprovements when combining attention layers with linear attentions [4, 37], we combine short sliding windows\nof softmax attention [6, 67] (size 64 in experiments) followed by linear attention in a single layer. This allows"}, {"title": "3.3.2 Training: Layer (or Block)-wise Attention Transfer", "content": "We describe the LOLCATS training approach and provide a simplified model to show its cost-quality trade-\noffs. Based on the limitations of low-rank linearizing identified in the prior section, we perform attention\ntransfer at a block-by-block granularity for the large scale models. The MSE loss function is scale-sensitive\nand our study showed that the later layers consistently result in larger MSEs (Fig. 6b). Thus, instead of\ncomputing the training loss (Eq. 6) over all $m \\in [M]$ for a model with $M$ layers, we compute the loss over\n$k$-layer blocks, and train each block independently:\n$MSE^{block} = \\frac{1}{KH} \\sum_{m=i}^{i+k} \\sum_{h=1}^{H}MSE$\n(8)\nwhere the training data for the block starting at layer $i$ is the hidden states outputted by prior block starting\nat layer $i - k$, for each sequence in the training corpus. After attention transfer, we arrange the blocks back\ntogether in their original order. Like before, we then do end-to-end LoRA finetuning to recover quality.\nTradeoffs between quality and efficiency. We show that the block-wise\napproach improves linearization quality at large model scales and improves the\nuser's ability to flexibly balance compute and memory efficiency tradeoffs. We\ncompare (1) joint (k = 126) training, where we load the full model once and\ncompute the loss as the sum of layer-wise MSEs, and (2) block-wise (k = 9)\ntraining, where we break the model into blocks and train each independently."}, {"title": "4 Experiments", "content": "Through experiments, we study: (1) if LOLCATS linearizes LLMs with higher quality than existing sub-\nquadratic alternatives and linearizations, and higher generation efficiency than original Transformers (Sec-\ntion 4.1); (2) how ablations on attention transfer loss, subquadratic architecture, and parameter and token\ncounts impact LLM downstream quality (Section 4.2); (3) how LOLCATs' quality and efficiency holds up\nto 70B and 405B LLMs, where we linearize and compare model quality across the complete Llama 3.1 family\n(Section 4.3)."}, {"title": "4.1 Main Results: LoLCATs Efficiently Recovers Quality in Linearized LLMs", "content": "In our main evaluation, we linearize the popular base Llama 3 8B [2] and Mistral 7B [28] LLMs. We first test if\nLOLCATS can efficiently create high-quality subquadratic LLMs from strong base Transformers, comparing\nto existing linearized LLMs from prior methods. We also test if LOLCATS can create subquadratic LLMs\nthat outperform modern Transformer alternatives pretrained from scratch. For space, we defer linearizing\ntraining details to App. A.\nCompared\nto recent linearizing methods, LOLCATs significantly improves quality and training efficiency across tasks\nand LLMs. On quality, LOLCATS closes 79.8% and 86.6% of the Transformer-linearizing gap for Mistral\n7B and Llama 3 8B respectively, notably improving 5-shot MMLU by 60.9% and 40.9% over next best\nfully subquadratic models (17.2 and 9.6 points). On efficiency, we achieve these results while only training"}, {"title": "LoLCATs Component Properties and Ablations", "content": "We next validate that LOLCATS linearizing enable subquadratic efficiency, and study how each of LOL-\nCATS' components contribute to these linearizing quality gains."}, {"title": "Subquadratic Generation Throughput and Memory", "content": "We measure the generation\nthroughput and memory of LOLCATS LLMs,\nvalidating that linearizing LLMs can signifi-\ncantly improve their generation efficiency. We\nuse the popular Llama 3 8B Hugging Face\ncheckpoint3, and compare LOLCATS imple-\nmented in HuggingFace Transformers with the\nsupported FlashAttention-2 (FA2) implemen-\ntation [15]. We benchmark on a single 80GB\nH100 and benchmark two LOLCATs imple-\nmentations with the Hedgehog feature map\nand (linear + sliding window) attention in FP32 and BF16. In Fig. 8a and Fig. 8b, we report the ef-\nfect of scaling batch size on throughput and memory. We measure throughput as (newly generated tokens\n\u00d7 batch size / total time), using 128 token prompts and 4096 token generations. As batch size scales,\nLOLCATS-linearized LLMs achieve significantly higher throughput than FA2. We note this is primarily due\nto lower memory, where FA2 runs out of memory at batch size 64. Meanwhile, LOLCATS supports up to\n3000 tokens / second with batch size 2048 (Fig. 8a), only maintaining a fixed \u201cKV state\u201d as opposed to the\ngrowing KV cache in all attention implementations (Fig. 8b)."}, {"title": "4.3 Scaling Up Linearizing to 70B and 405B LLMs", "content": "We finally use LOLCATS to scale up linearizing to Llama 3.1 70B and 405B models. In Table 7, we find\nthat LOLCATS provides the first practical solution for linearizing larger LLMs, achieving significant quality\nimprovements over prior linearizing approaches of swapping in attentions and finetuning [35]. Controlling\nfor the same linear + sliding window layer, for Llama 3.1 70B we achieve a 39.0 point improvement in 5-shot\nMMLU accuracy. For Llama 3.1 405B, LOLCATS similarly achieves a 38.3 point improvement. These results\nhighlight LOLCATS' ability to linearize large-scale models with greater efficiency and improved performance,\nshowing for the first time that we can scale up linearizing to 70B+ LLMs."}, {"title": "5 Conclusion", "content": "We propose LOLCATS, an efficient LLM linearizing method that (1) trains attention analogs\u2014such as linear\nattentions and linear attention + sliding window hybrids to approximate an LLM's self-attentions, before\n(2) swapping the attentions and only finetuning the replacing attentions with LoRA. We exploit the fidelity\nbetween these attention analogs and softmax attention, where we reduce the problem of linearizing LLMs to\nlearning to approximate softmax attention in a subquadratic analog. Furthermore, we demonstrate that via\nan MSE-based attention output-matching loss, we are able to train such attention analogs to approximate the\n\"ground-truth\" softmax attentions in practice. On popular zero-shot LM Evaluation harness benchmarks and\n5-shot MMLU, we find this enables producing high-quality, high-inference efficiency LLMs that outperform"}, {"title": "Limitations and Future Work", "content": "While we focus on studying how to enable high quality yet highly efficient LLM linearizing with simple linear\nattentions, we note several areas for additional evaluation in both subquadratic capabilities and architectures.\nOn subquadratic capabilities, by replacing each attention layer alternative, we eliminate the need to manage\ngrowing key-value (KV) caches and their associated memory overheads. However, it remains to be seen what\nkinds of capabilities we can enable with this cheaper inference, e.g., if linearized models can exploit quality-\nimproving inference scaling laws suggested by recent works [8, 51]. Under a different motivation, while layers\nlike linear attention achieve greater efficiency gains over softmax attention when processing longer contexts,\nwe leave studying how low-rank linearizing applies to such long context scenarios as a motivated direction\nfor future work. Finally, while we stick to \"vanilla\" linear + sliding window attentions in LOLCATSs, many\nmore recent subquadratic architectures improve linear attention quality with additional factors such as decay\nterms [54] and additional gating [63]. Studying whether attention transfer and low-rank linearizing can help\nscale up these additional attention analogs is an interesting line of future work."}, {"title": "Ethics Statement", "content": "Our work deals with improving the efficiency of open-weight models. While promising for beneficial appli-\ncations, increasing their accessibility also raises concerns about potential misuse. Bad actors could leverage\nour technique to develop LLMs capable of generating harmful content, spreading misinformation, or enabling\nother malicious activities. We focus primarily on base models, but acknowledge that linearizing could also\nbe used on instruction-tuned LLMs; research on whether linearizing preserves guardrails is still an open\nquestion. We acknowledge the risks and believe in the responsible development and deployment of efficient\nand widely accessible models."}, {"title": "Reproducibility", "content": "We include experimental details in Appendix A, and further implementation details with sample code for\nlinearizing architectures and training in Appendix C.1. Our code is also available at https://github.com/\nHazyResearch/lolcats"}, {"title": "A Experimental Details", "content": "A.1 Main Results, Linearizing 7B and 8B LLMs\nSetup. We describe our setup for linearizing Mistral 7B (v0.1) [28], Llama 3 8B [3], and Llama 3.1 8B [20].\nFor linearizing layers, we replace softmax attentions with hybrid linear + sliding window analogs (Sec-\ntion 3.3.1), using Hedgehog's feature map for its prior quality [66].\nFor the sliding window implementation, we con-\nsidered two options: a standard sliding window\nwhere w is the same for all tokens, and a \"terraced\"\nwindow where w changes based on token index (Fig-\nure 9). While we found both comparable in qual-\nity (Table 13), the latter lets us exploit the new\nThunderKittens (TK) DSL's [52] primitives for im-\nplementing fast CUDA kernels. Here we prefer con-\ntiguous blocks of size w = 64, which can quickly\nbe computed in parallel on modern GPUs. We use\nthis \"terrace\" implementation in our main results,\nand include further implementation details in Ap-\npendix C.2.\nFor linearizing data, we use the Alpaca lineariz-\ning data setup in Section 3.2 unless otherwise noted.\nWe also tried a more typical pretraining corpus (a subset4 of RedPajama [14]), but found comparable per-\nformance when controlling for number of token updates (Appendix B.2.1). To linearize, we simply train all\nfeature maps in parallel for two epochs with learning rate 1e-2, before applying LoRA on the attention pro-\njection layers for two epochs with learning rate 1e-4. By default, we use LoRA rank r = 8, and scale LORA\nupdates by 2 ($\\alpha$ = 16 in HuggingFace PEFT5), amounting to training <0.09% of all model parameters. For\nboth stages, we train with early stopping, AdamW optimizer [33], and packing into 1024-token sequences\nwith batch size 8. We evaluate the best checkpoints based on validation set perplexity.\nHyperparameters. We list all model and training hyperparameters in Table 8.\nCompute Resources. For each linearizing run we use one NVIDIA 40GB A100 GPU. With batch size 1\nand gradient accumulation over 8 batches, attention transfer takes $\\approx$ 2 hours and post-swap finetuning takes\n$\\approx$ 4.5 hours, i.e., 6.5 total GPU hours to linearize an 8B LLM."}, {"title": "A.2 Linearizing Llama 3.1 70B", "content": "We provide experimental details corresponding to the 70B parameter results reported in Table 7.\nSetup. We compare the quality of two linearization approaches to the quality of the original Llama 3.1\n70B model, including (1) the baseline linearization without attention transfer, which is representative of\nthe approach used in prior work [35, 60, 64] and (2) our approach, LOLCATS. For both the baseline\nand LOLCATS, we start with Llama 3.1 70B and replace the softmax attentions with the linear attention\narchitecture defined in Section 3.3.1, Equation (7). The training procedure involves:\n\u2022 Baseline: We introduce LoRA parameters to the attention $W_q, W_k, W_v, W_o$ projection matrices. We\ntrain the linear attention feature maps, learnable mixing term $\\gamma$, and the LoRA parameters during the\nfine-tuning adjustment stage of the linearization process.\n\u2022 LOLCATS: We first perform layer-wise attention transfer following Equation (8), with k = 80 (i.e., we\noptimize over all layers together). We then introduce LoRA parameters to the attention $W_q, W_k, W_v, W_o$"}, {"title": "A.3 Linearizing Llama 3.1 405B", "content": "Setup. We compare the quality of two linearization approaches to the quality of the original Llama 3.1\n405B model, including (1) the baseline linearization without attention transfer, which is representative of the\napproach used in prior work [35] and (2) our approach, LOLCATS. For both the baseline and LOLCATS, we\nstart with Llama 3.1 405B and replace the softmax attentions with the linear attention architecture defined\nin Section 3.3.1, Equation (7). The training procedure involves:"}, {"title": "B Additional Experiments", "content": "To better understand LOLCATS' properties and performance for high quality yet parameter and training-\nefficient linearizing, we now study how different amounts of parameter training, different data sources, and\ndifferent amounts of training data affect LOLCATS quality."}, {"title": "B.1 Study on Parameter-Efficient Training", "content": "In our main results, we found that simple default initializations (e.g., rank 8, applied to all attention\nprojection layers) could recover high quality linearizing while only updating <0.2% of model parameters. In\nthis section, we study how changing different aspects of low-rank adaptation and sliding window size impact\nlinearizing performance."}, {"title": "Effect of LoRA Rank", "content": "We study the effect of LORA rank in post-swap finetuning for zero-shot linearized LLM performance. Fol-\nlowing standard implementations [27], we consider two factors: rank r, which determines the rank of the"}, {"title": "Effect of LoRA Projection Layer", "content": "We next compare performance when applying LoRA to different weight matrices of the linear attention\nlayers. With the same training and evaluation setup as Appendix B.1.1, but fixing r = 8, $\\alpha$ = 16, we now\napply LoRA to different combinations of $W_q, W_k, W_w, W_o$ weights after swapping in attention-transferred\nlinear attentions. We use the same combination for each layer."}, {"title": "Effect of Window Size", "content": "We now compare model performance using different window sizes in the LOLCATS linear + sliding window\nattention layer. With the standard sliding window implementation (Fig 9), we compare LM Eval performance"}, {"title": "B.2 Study on Linearizing Data", "content": "While most of our work focuses on architecture and training procedure for improving LLM linearizing quality,\nwe now study how data selection affects LOLCATS performance."}, {"title": "Data Source: Alpaca versus RedPajama", "content": "We study the effect of linearizing data for downstream LLM performance. While we initially found that\njust using the ~ 50K samples of a cleaned Alpaca dataset [55] could lead to surprisingly high performance\non popular zero-shot LM Eval tasks, prior linearizing works [35] use more typical pretraining datasets to\nlinearize such as RefinedWeb [38]. We thus also try linearizing with a random subset of RedPajama [14] to\nevaluate how LOLCATS works with pretraining data, albeit without any special curation. For both setups,\nwe pack samples into 1024 token sequences and randomly subsample the RedPajama data so that we use the\nsame number of training tokens (20M) for both attention transfer and finetune stages (40M tokens overall).\nWe use the setup as described in Appendix A.1 for all other hyperparameters."}, {"title": "B.3 Study on Linearizing Token Budget", "content": "We further study how varying the number of tokens used for both attention transfer and low-rank adaptation\nimpacts LOLCATS linearizing quality.\nTo first test how efficient we can be with attention transfer, we linearize\nLlama 3 8B with varying numbers of attention transfer steps (0 - 1800), before low-rank adjusting for up\nto 2000 steps. We use the Alpaca dataset and the same packed random sampling as our main experiments,\nand measure evaluation perplexity on validation samples both in-distribution (held-out Alpaca samples)\nand out-of-distribution (RedPajama validation samples) over different combinations of steps (Figure 12)."}, {"title": "C Implementation Details", "content": "C.1 Code Implementation\nBelow we provide further details on implementing LOLCATS with PyTorch-like code and example demon-\nstrations from the HuggingFace Transformers library.\nLearnable Linear Attention. To start, we simply replace the softmax attentions in an LLM with a\nlinear attention. We define such a class below."}, {"title": "C.2 Hardware-aware implementation of LoLCATs sliding window", "content": "Despite the theoretical efficiency of linear attention, existing implementations have long underperformed\nwell-optimized attention implementations (e.g., FlashAttention) in wall clock speed [17]. To translate the\nbenefits of LOLCATS to wall clock speedups, we develop a custom hardware-aware algorithm for LOLCATS\nprefill using the ThunderKittens CUDA framework. We first briefly review the GPU execution model and\nthen detail our algorithm.\nC.2.1 GPU execution model\nGPUs workloads are executed by independent streaming multiprocessors (SMs), which contain warps, groups\nof 32 threads, that operate in parallel."}, {"title": "Memory hierarchy", "content": "ML workloads involve moving large tensors (weights, activations) in and out of\nmemory to perform computation. GPUs have a memory hierarchy, which includes global memory (HBM),\nshared memory (SRAM), and registers. Reading from and writing data to memory, referred to as I/O\noperations, takes time. There is a large amount of HBM, which has high I/O costs, and a small amount of\nSRAM and registers have much costs. All SMs access"}]}