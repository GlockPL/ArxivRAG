{"title": "Federated Continual Graph Learning", "authors": ["Yinlin Zhu", "Xunkai Li", "Miao Hu", "Di Wu"], "abstract": "In the era of big data, managing evolving graph data poses substantial challenges due to storage costs and privacy issues. Training graph neural networks (GNNs) on such evolving data usually causes catastrophic forgetting, impairing performance on earlier tasks. Despite existing continual graph learning (CGL) methods mitigating this to some extent, they predominantly operate in centralized architectures and overlook the potential of distributed graph databases to harness collective intelligence for enhanced performance optimization. To address these challenges, we present a pioneering study on Federated Continual Graph Learning (FCGL), which adapts GNNs to multiple evolving graphs within decentralized settings while adhering to storage and privacy constraints. Our work begins with a comprehensive empirical analysis of FCGL, assessing its data characteristics, feasibility, and effectiveness, and reveals two principal challenges: local graph forgetting (LGF), where local GNNs forget prior knowledge when adapting to new tasks, and global expertise conflict (GEC), where the global GNN exhibits sub-optimal performance in both adapting to new tasks and retaining old ones, arising from inconsistent client expertise during server-side parameter aggregation. To tackle these, we propose the POWER framework, which mitigates LGF by preserving and replaying experience nodes with maximum local-global coverage at each client and addresses GEC by using a pseudo prototype reconstruction strategy and trajectory-aware knowledge transfer at the central server. Extensive evaluations across multiple graph datasets demonstrate POWER's superior performance over straightforward federated extensions of the centralized CGL algorithms and vision-focused federated continual learning algorithms. Our code is available at https://github.com/zyl24/FCGL_POWER.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph neural networks (GNNs) are an effective approach for leveraging the relational data in graph databases for learning tasks, achieving remarkable results across diverse real-world scenarios, such as recommendation [1]\u2013[3], biomedical [4]-[6], and finance [7]-[9]. However, in the era of big data, graph structures often evolve as new entities and relationships emerge, while storing or accessing massive historical node profiles (e.g., features and labels) and topologies is impractical considering database capacity and security [10]-[12]. As a result, GNNs often experience unacceptable performance degradation on past tasks (i.e., graphs prior to changes) when adapting to current tasks, known as the catastrophic forgetting problem, a consequence of a weak stability-plasticity tradeoff [13]-[16]. To tackle this issue, inspired by the success of continual learning in vision tasks [17]\u2013[19], various continual graph learning (CGL) studies have been proposed in recent years, all of which have demonstrated satisfactory performance [20]-[24]."}, {"title": "A. Overlooked Potential: CGL with Collective Intelligence", "content": "Despite their effectiveness, current CGL methods are based on the assumption of centralized data storage, where a single institution collects and manages the evolving graph. However, this assumption does not always hold in real-world applications. In many practical scenarios, graph-structured data is distributed across multiple decentralized sources, each capturing and responding to local changes in nodes and topology over time.\nMotivating Scenario. In the e-commerce domain, various platforms maintain private evolving product networks, where nodes represent products and edges capture association rela-tionships (e.g., co-purchases or shared reviews), both evolving as new products or associations are introduced.\nIn such decentralized settings, existing CGL methods face inherent limitations, as each institution can only train its GNN independently, relying solely on knowledge from its own evolving graph. This isolated learning contrasts with how humans learn, as humans benefit from collective intelligence by leveraging knowledge within groups (e.g., social media groups, academic seminars) [25]. Obviously, if an institution could leverage knowledge shared by others, it would be better positioned to mitigate catastrophic forgetting and enhance task performance. However, constraints like data privacy and commercial competition make directly aggregating multiple evolving graphs unfeasible, presenting significant challenges to achieving effective CGL with collective intelligence."}, {"title": "B. First Exploration: Federated Continual Graph Learning", "content": "Building on the success of federated graph learning (FGL) [26], we introduce federated continual graph learning (FCGL) as the first practical framework for collaborative CGL in decentralized environments that adheres to storage and privacy limitations. As shown in Fig. 1, FCGL employs a multi-round communication process. The clients first train local GNNs on their private evolving graphs, and a central server then aggregates these models to form a global GNN, which has improved performance across diverse local graphs. The global model is subsequently broadcast to the clients, enabling the leveraging of collective intelligence without directly sharing massive and sensitive evolving graphs. Notably, although various federated continual learning methods [27]\u2013[30] have demonstrated effectiveness in vision tasks, their algorithms rely on augmentation strategies and network architectures tailored specifically for image data, rendering them inapplicable to graph data. Furthermore, for FCGL, the characteristics of multi-client evolving graphs, feasibility, and potential challenges impacting the GNN performance remain unexplored."}, {"title": "C. Our Efforts: Establishing an Effective FCGL Paradigm", "content": "In order to establish an effective FCGL paradigm, we have undertaken the following efforts and contributions.\nE1: In-depth Empirical Investigation of FCGL. We conduct in-depth empirical investigations of FCGL (Sec. III). From the Data perspective, we explore the data distribution of multi-client evolving graphs and identify the phenomenon of divergent graph evolution trajectories, which leads each client to develop expertise in specific classes at different stages, forming the foundation of their collective intelligence. From the Feasibility perspective, we observe that even the simplest implementation of FCGL outperforms isolated CGL methods in decentralized scenarios, demonstrating its ability to harness collective knowledge from multi-client evolving graphs. From the Effectiveness perspective, we identify two non-trivial chal-lenges impacting GNN performance: Local Graph Forgetting (LGF), where local GNNs forget knowledge from previous tasks when adapting to new ones, and Global Expertise Conflict (GEC), arising from the multi-client graph knowledge conflicts, during the parameter aggregation across clients with divergent graph evolution trajectories. This results in sub-optimal global GNN performance on both past and current tasks.\nE2: Novel and Effective FCGL Framework. Building on the insights from E1, we propose the first FCGL framework, named POWER (graPh evOlution Trajectory-aware knowledge TransfER). Specifically, POWER is designed with two primary objectives: (1) Addressing LGF at local clients: POWER selects experience nodes from past tasks with maximum local-global coverage and replays them during local training for future tasks; and (2) Tackling GEC at central server: POWER introduces a novel pseudo prototype reconstruction mechanism, enabling the server to capture multi-client graph evolution trajectories. Leveraging this, a trajectory-aware knowledge transfer process is applied to restore the lost knowledge of global GNN during parameter aggregation."}, {"title": "Our Contributions", "content": "(1) Problem Identification. To the best of our knowledge, this is the first exploration of the FCGL paradigm, offering a practical solution for continual graph learning with collective intelligence under decentralized graph data management. (2) In-depth Investigation. (Sec. III) We conduct in-depth empirical investigations of FCGL from the perspectives of Data, Feasibility and Effectiveness, providing valuable insights for its development. (3) Novel Framework. (Sec. IV) We present POWER, an effective FCGL framework that tackles LGF by replaying experience nodes from past tasks with maximum local-global coverage, and handles GEC with an innovative pseudo prototype reconstruction mechanism and graph evolution trajectory-aware knowledge transfer process. (4) State-of-the-art Performance. (Sec. V) Extensive experiments on eight datasets demonstrate that POWER consistently outperforms the federated extension of centralized CGL algo-rithms and vision-based federated continual learning algorithms."}, {"title": "II. PRELIMINARIES AND RELATED WORKS", "content": "In this section, we first introduce static graphs and GNNs from a data flow perspective. We then formalize the problem definition for FCGL. Finally, we briefly review two pertinent research domains, including CGL and FGL."}, {"title": "A. Notations and Problem Formalization", "content": "Static Graph and GNNs. Consider an undirected static graph $G = (V, E)$ with $V$ as the node-set and $E$ as the edge-set. Each node $v_i \\in V$ has an attribute vector $x_i \\in R^F$ and a one-hot label $y_i \\in R^C$, where $F$ is the dimension of feature and $C$ is the number of classes. The adjacency matrix (including self-loops) is $A \\in R^{N \\times N}$, where $N = |V|$ is the number of nodes. $N(v_i)$ represents the neighbor nodes set of node $v_i$. Most GNNs can be subsumed into a framework named Message Passing Neural Network (MPNN) [31], which obtains the representation of a specific node by recursively aggregating and transforming the representations of its neighbors. Specifically, the representation of node $v_i$ at the $l$-th layer is denoted as $h_i^{l+1}$, which can be formalized as follows:\n$h_i^{l+1} = UPD(h_i^l, AGG(\\lbrace h_j^l : v_j \\in N(v_i) \\rbrace)),$ (1)\nwhere $h_i^0 = x_i$, $h_i^l$ is the representation of node $v_i$ in the $l$-th (previous) layer, $AGG(\\cdot)$ aggregates the neighbor representa-tions, and $UPD(\\cdot, \\cdot)$ updates the representation of the current node using its representation and the aggregated neighbor representation at the previous layer. The semi-supervised node classification task is based on the topology of labeled set $V_L$ and unlabeled set $V_U$, and the nodes in $V_U$ are predicted based on the model supervised by $V_L$.\nProblem Formalization of FCGL. For FCGL, there is a trusted central server and $K$ clients. Specifically, for the $k$-th client, we use $T_k = \\{T_k^1, T_k^2, ..., T_k^M\\}$ to denotes the set of its tasks, where $M$ is the number of tasks. Then, the local private evolving graph with $M$ tasks can be formalized as follows:\n$G_k = \\{G_k^1, G_k^2, ..., G_k^M \\};$\n$G_k^t = G_k^{t-1} + \\Delta G_k^t,$ (2)"}, {"title": "B. Continual Graph Learning", "content": "CGL introduces a novel paradigm where GNNs learn from dynamically evolving graphs while retaining prior knowledge. The primary challenge in CGL, catastrophic forgetting, mani-fests as significant performance degradation on historical tasks as the GNN adapts to new data. Mainstream CGL research is categorized into three primary approaches: (1) Experience Replay focuses on selecting and storing a minimal set of experiential data from previous tasks. Its core objective is to devise strategies that efficiently capture and retain representative data within constrained memory limits for future task training. This involves deciding which data to replay and how to utilize these samples effectively for subsequent re-execution. Pioneering efforts in this field include ERGNN [20], which stores experience data at the node level, devises node selection strategies such as coverage and social influence maximization and incorporates node classification losses during new task train-ing. DSLR [23] enhances node selection with a coverage-based diversity strategy for class representativeness and intra-class diversity and refines node topology through graph structure learning. Additionally, recent studies [24], [34], [35] propose the experience data selection strategy at the subgraph or ego-network level, which explicitly preserves topological structure information for replay and achieves satisfactory performance. (2) Parameter Regularization alleviates catastrophic forgetting by introducing a regularization term during the training of new tasks to preserve the parameters crucial for previous tasks. For instance, TWP [21] introduces a novel regularization term that preserves critical GNN parameters for node semantics and topology aggregation, considering both node-level and topology-level knowledge from previous tasks. Moreover, SSRM [22] introduces a novel regularization-based technique to mitigate the catastrophic forgetting caused by structural shifts. (3) Architecture Isolation assigns a dedicated set of GNN parameters to each task, ensuring that learning a new task does not interfere with the parameters of the old task. Studies in this line [36], [37] effectively acquire knowledge from new graphs by expanding the GNN architecture when its capacity is insufficient to meet the design requirements."}, {"title": "C. Federated Graph Learning", "content": "Motivated by the success of federated learning in computer vision and natural language processing [38] with the demands for distributed graph learning in the real world, FGL has received growing attention from researchers. From the data and task perspectives, mainstream FGL studies can be divided into two settings: (1) Graph-FL: In this setting, each client independently collects multiple graphs to address graph-level downstream tasks, such as graph classification collaboratively. The main challenge of Graph-FL is to avoid information interference between graph datasets from different clients, particularly when handling multi-domain data. For example, GCFL+ [39] introduces a GNN gradient pattern-aware tech-nique for dynamic client clustering, which mitigates knowledge conflicts caused by structural and feature heterogeneity across clients' graph data. (2) Subgraph-FL: In this setting, each client holds a subgraph of an implicit global graph, aiming to solve node-level tasks such as node classification. The main challenges of Subgraph-FL include subgraph heterogeneity (i.e., performance degradation due to node and topology variations across subgraphs) and missing edges (i.e., lost connections between subgraphs due to distributed storage). To address subgraph heterogeneity, Fed-PUB [40] improves local GNNs by computing similarities from random graph embeddings for weighted server aggregation, with clients using personalized sparse masks to selectively update subgraph parameters. FedGTA [41] encodes topology into smoothing confidence and graph moments to improve model aggregation. Moreover, other studies [42]-[45] also achieve satisfactory results on this challenge. To address missing edges, FedSage+ [26] integrates node representations, topology, and labels across subgraphs while training a neighbor generator to restore missing links, achieving subgraph-FL with robust and complete graph information. Several other works [46]-[48] also show strong performance in addressing this challenge. Further detailed insights into FGL research can be found in comprehensive surveys [49]-[51] and extensive benchmark studies [52]-[54]."}, {"title": "III. EMPIRICAL INVESTIGATION", "content": "In this section, we present a thorough empirical investigation of the proposed FCGL paradigm, structured around three key questions from different perspectives. From the Data perspective, to better understand the complexities of graph data in FCGL contexts, we answer Q1: What patterns and trends emerge in data distribution across multi-client evolving graphs in FCGL settings? From the Feasibility perspective, to demonstrate the advantages of FCGL using collective intelligence, we answer Q2: Compared to isolated CGL, does a naive FCGL approach (i.e., CGL integrated with FedAvg) achieves better performance for decentralized evolving graphs? From the Effectiveness perspective, to reveal the bottleneck of naive FCGL and realize efficient FCGL paradigm, we answer Q3: What non-trivial challenges limit the performance of GNNs within the FCGL framework? Details for the described experimental setup, algorithms, and metrics refer to Sec. V-A. To address Q1, we simulate multiple decentralized evolving graphs using a two-step approach. First, we partition the Cora dataset [55] into three subgraphs using the Louvain algorithm [56], which is widely utilized in various FGL studies [26], [41], [48]. These subgraphs are distributed among three different clients. Second, following the standard experimental settings of CGL [20], [21], [23], each client's local subgraph is further divided into three class-incremental tasks. Each task comprises two classes of nodes, discarding any surplus classes and removing inter-task edges while preserving only intra-task connections. For each client, we present its tasks with node label distributions in Fig. 2 (a).\nObservation 1. Although all these clients eventually learn across six classes, they follow divergent graph evolution trajectories, varying both in sample quantity (e.g., Clients 1 and 2) and in the sequence of class learning (e.g., Clients 1 and 3). Intuitively, these clients tend to develop expertise in specific classes at different stages, forming the basis of the collective intelligence harnessed by the FCGL paradigm.\nTo address Q2, we assess the performance of three CGL algorithms, including ERGNN [20], TWP [21], and DSLR [23], under two scenarios: isolated training (referred to as Iso-ERGNN, Iso-TWP, and Iso-DSLR) and federated training using the FedAvg [33] algorithm (referred to as Fed-ERGNN, Fed-TWP, and Fed-DSLR). In the isolated setting, no client-server communication occurs, while the federated variants implement naive FCGL strategies that simply aggregate param-eters from local GNNs. We assessed their performance using two metrics: the accuracy mean (AM) and the forgetting mean (FM), where higher AM values denote better adaptation to new tasks, and lower FM values denote less forgetting of previously learned tasks. The training protocol for each task includes 10 communication rounds, with 3 local epochs per round. As depicted in Fig. 2 (b), the federated variants consistently outperformed their isolated counterparts in both AM and FM metrics. This enhanced performance is attributed to FCGL's capability to leverage diverse knowledge from evolving graph data across clients, thereby reducing task forgetting through effective cross-client knowledge transfer (e.g., alleviating the knowledge forgetting of Client 1 Task 1 and Client 2 Task 1 through insights gained from Client 3 Task 3).\nObservation 2. Even a basic implementation of FCGL can sub-stantially enhance the performance of existing CGL algorithms on decentralized evolving graphs.\nTo address Q3, we evaluate the naive FCGL algorithm's performance across Client 1's three tasks during the training of Task 3. For readability, we present the results obtained from Fed-ERGNN, as similar trends are also shown in Fed-TWP and Fed-DSLR. Our analysis is further concentrated on rounds 4 and 5. From the Single GNN aspect, we analyze performance variations in Client 1's local GNN, particularly during the critical phases of global parameter reception and local training, motivated by findings in existing literature [21], [28], which suggest that shifts in parameter values play a significant role in causing knowledge forgetting. This perspective enables a closer examination of how individual GNNs react to external updates. As shown in Fig. 2 (c), each global parameter reception phase is marked with scatter points, while lines represent the 3-epoch local training phases. From the Multiple GNNs aspect, we compare the performance of all local GNNs and the aggregated global GNN. The results are depicted in Fig. 2 (d)."}, {"title": "IV. POWER: THE PROPOSED FRAMEWORK", "content": "In this section, we introduce POWER, the first FCGL framework designed for learning on multiple decentralized evolving graphs. We first provide a structured overview of POWER in Fig. 3. Afterward, we delineate the architecture of POWER based on its two design objectives. Specifically, in Sec. IV-A, we detail the maximum local-global coverage-based experience node selection and replay mechanisms that POWER employs to address LGF at local clients. In Sec. IV-B, we expound on the pseudo prototype reconstruction mechanism and the graph evolution trajectory-aware knowledge transfer process, which is employed to tackle GEC at central server."}, {"title": "A. Addressing Local Graph Forgetting", "content": "To tackle the challenge of LGF, POWER employs a strategic selection and storage of experience nodes from prior tasks, enabling targeted replay during local training on new tasks. We begin by introducing the experience node selection strategy.\nExperience Node Selection. To ensure that selected experience nodes capture knowledge from previous tasks, we build upon the Coverage Maximization (CM) strategy [20] with further modifications tailored to FCGL settings. The conventional CM is based on the intuitive assumption that nodes within the same category share similar properties in the embedding space, with many nodes of the same class naturally gathering near their class's representative nodes. As for FCGL, we can utilize the embeddings from both the local and global GNNs to fully consider both local and global knowledge. Specifically, consider the $k$-th client training on its $t$-th local task $T_k^t$; the local graph is denoted as $G_k^t$. At the last training round, we calculate the local-global node embeddings $Z$ as follows:\n$H = GNN(X, A | \\Theta_k^t),$\n$H^g = GNN(X, A | \\Theta^\\star),$\n$Z = \\alpha H + (1 - \\alpha) H^g,$ (3)\nwhere $H$ and $H^g$ are node embeddings from local and global GNNs with parameters $\\Theta_k^t$ and $\\Theta^\\star$, respectively. $\\alpha$ serves as a trade-off factor for combining local and global embeddings. The local-global coverage for each labeled node $v_i$ in the training set $V^{lbl}$ is denoted as $C(v_i)$, defined as:\n$C(v_i) = |\\{v_j | v_j \\in V^{lbl}, y_i = y_j, d(z_i, z_j) < \\epsilon E(v_i) \\}|,$ (4)\nwhere $d(.,.)$ measures the Euclidean distance between the embeddings of $v_i$ and $v_j$, $\\epsilon$ serves as a threshold distance, and $E(v_i)$ denotes the average pairwise distance between embeddings of training nodes in the same class as $v_i$.\nSubsequently, for each class $c$ in the current task, we select $b$ experience nodes from the set of nodes in class $c$ with maximum local-global coverage. The experience node set of class $c$ is denoted as $B_c$, which is formulated as follows:\n$B_c = \\underset{\\{v_{c,1}, ..., v_{c,b} | v_{c,1}, ..., v_{c,b} \\in V^{lbl}\\_c \\}}{\\operatorname{argmax}} \\sum\\_{i=1}^{b} C(v_{c,i}),$ (5)\nwhere $V^{lbl}\\_c = \\{v_i | v_i \\in V^{lbl}, y_i = c\\}$, $b$ represents the buffer size allocated per task and class, constrained by the storage capacity budget. In our experiments, we set $b = 1$ as the default. Eq. 5 poses an NP-hard challenge, which we address using a greedy algorithm. At each iteration, the algorithm picks the node with the highest local-global coverage from the remaining unselected nodes in $V^{lbl}\\_c$, repeating until $b$ nodes are selected. Finally, for each class $c$ in task $T_k^t$, the experience node set $B_c$ is stored into the local buffer $B$ for replay (i.e., $B = B \\cup B_c$).\nExperience Node Replay. POWER's local training process pursues two primary objectives. First, to adapt to the current task $T_k^t$, the local GNN minimizes the cross-entropy loss over the labeled training set $V^{lbl}$, as formulated below:\n$\\hat{Y} = GNN-CLS(X, A | \\Theta_k^t),$\n$\\mathcal{L}\\_{new} = - \\sum\\_{v\\_i \\in V^{lbl}} y\\_i \\log \\hat{y}\\_i + (1 - y\\_i) \\log(1 - \\hat{y}\\_i),$ (6)\nwhere $\\hat{Y}$ denotes the soft labels predicted by the local GNN for the current task nodes, $\\Theta_k^t$ represents the local GNN parameters, and $y_i$ is the ground-truth label for a specific training node $v_i$. When the current task is not the first task (i.e., $t \\neq 1$), the client also optimizes a second objective, aiming at retaining knowledge from previously learned tasks. To achieve this, the client replays experience nodes stored in the local buffer $B$, which captures representative samples from prior tasks. The replay objective is the cross-entropy loss over these stored nodes, which can be calculated as follows:\n$\\hat{Y}\\_B = GNN-CLS(X\\_B, I | \\Theta\\_k^t),\\mathcal{L}\\_{old} = - \\sum\\_{B} y\\_i \\log \\hat{y}\\_i + (1 - y\\_i) \\log(1 - \\hat{y}\\_i),$ (7)\nwhere $\\hat{Y}\\_B$ represents the soft labels predicted by the local GNN for the buffer nodes, while $X\\_B$ denotes the feature matrix of nodes in the local buffer, and $y\\_i$ is the ground truth for a specific buffer node $v\\_i$. Notably, since we store only the node features without including any topological structures, the adjacency matrix for buffer nodes is set as the identity matrix $I$, meaning that each buffer node is predicted in isolation. Finally, the loss function of local training is defined as:\n$\\mathcal{L} = \\beta \\mathcal{L}\\_{new} + (1 - \\beta) \\mathcal{L}\\_{old},$ (8)\nwhere $\\beta$ denotes the trade-off parameter to flexibly balance adaptation to new tasks with a replay of previous tasks."}, {"title": "B. Tackling Global Expertise Conflict", "content": "As discussed in Sec. III, GEC arises from knowledge conflicts during the aggregation of parameters from local GNNS trained on divergent graph evolution trajectories. Consequently, for certain classes, the global model exhibits sub-optimal performance compared to clients with expertise in those classes. Motivated by this, POWER employs two modules to address GEC: (1) Pseudo Prototype Reconstruction, which enables the server to discern each client's evolution trajectory and expertise, and (2) Trajectory-aware Knowledge Transfer, which utilizes multi-client trajectories and expertise to recover the lost knowledge of the global GNN.\nPseudo Prototype Reconstruction Mechanism. First, to capture the expertise of each client, POWER calculates the prototype (i.e., averaged node feature) for each class on the client side. Specifically, consider the $k$-th client on its first communication round of $t$-th local task $T_k^t$. We denote the labeled node set as $V^{lbl}$, with $V^{lbl} = \\{v_i | v_i \\in V^{lbl}, y_i = c\\}$ representing the subset of nodes labeled as class $c$. The prototype of class $c$ is denoted as $P_c$, which is defined as:\n$P\\_c = \\frac{1}{|V\\_c^{lbl}|} \\sum\\_{v\\_i \\in V\\_c^{lbl}} X\\_i,$ (9)\nHowever, directly transmitting class prototypes to the server incurs substantial privacy risks. To mitigate this, POWER adopts a privacy-preserving strategy wherein clients send proto-type gradients instead, allowing the server to reconstruct pseudo prototypes via gradient matching. This approach enhances security for two key reasons: (1) the prototype gradients are derived from a randomly initialized network, making it challenging to extract meaningful information from gradients alone without additional context, and (2) the pseudo prototypes produced on the server only mimic the gradient patterns of the true prototypes, lacking identifiable numerical features that could be recognized by humans. Specifically, the client feeds the computed prototype $P\\_c$ into an $L$-layer randomly initialized gradient encoding network $\\Gamma$ with parameters $\\{\\Omega\\_i\\}\\_{i=1}^{L}$ to obtain predictions. We then backpropagate the cross-entropy loss to compute the gradient $\\nabla \\Gamma P\\_c$, whose $i$-th element $\\eta\\_i$ is formulated as:\n$\\nabla_{\\Omega \\Gamma} = \\eta_i (-y_c log \\hat{y}_c + (1 - y_c) log(1 - \\hat{y}_c)),$ (10)\nwhere $\\hat{y}\\_c = \\Gamma(P\\_c | \\{\\Omega\\_i\\}\\_{i=1}^{L})$ denotes the predicted soft label and $y\\_c$ is the one-hot ground-truth vector representing class $c$."}, {"title": "V. EXPERIMENTS", "content": "In this section, we present a comprehensive evaluation of POWER. We first introduce 8 benchmark graph datasets across 6 domains and the simulation strategy for the FCGL scenario. Then, we present 10 evaluated state-of-the-art baselines. Moreover, we provide detailed metrics description and experiment settings (Sec. V-A). After that, we aim to address the following questions: Q1: Compared with state-of-the-art baselines, can POWER achieve better predictive performance under FCGL scenario? (Sec. V-B) Q2: Does POWER demonstrate superiority in terms of efficiency? (Sec. V-C) Q2: How much does each module in POWER contribute to the overall performance? (Sec. V-D) Q4: Can POWER remain overall robust under changes in hyperparameters? (Sec. V-E) Q5: How does POWER perform under sparse settings like missing features/edges/labels and low client participation rate? (Sec. V-F)\nDatasets and Simulation Strategy. We evaluate POWER on 8 public benchmark graph datasets across 6 domains, including three citation networks (Cora, Citeseer [55] and OGB-arxiv [57]), one co-purchase network (Computers [58]), one co-authorship network (Physics [58]), one wiki-page network (Squirrel [59]), one image network (Flickr [60]), and one article syntax network (Roman-empire [61]). Our strategy for simulating multiple decentralized evolving graphs has been described in detail in Sec. III Q1. Notably, for each dataset, based on the number of nodes and categories, we define varying numbers of clients, task-specific category counts, and the proportions for the training, validation, and test sets. For more detailed statistical information, please refer to Table. I.\nBaselines. We summarize 10 baselines into 3 categories as follows: (1) FL/FGL Fine-tuning, including FedAvg [33], FedSage+ [26], Fed-PUB [40] and FedGTA [41], which simply replace the local client training process of the FL/FGL algorithm with fine-tuning on sequential tasks; (2) Federated CGL, including Fed-ERGNN, FedTWP and Fed-DSLR, which combines the naive FedAvg algorithm with three representative CGL methods, ERGNN [20], TWP [21] and DSLR [23], respectively; (3) Federated Continual Learning for CV, which includes GLFC [27], TARGET [29] and LANDER [30].\nEvaluation Metrics. We adopt two primary metrics commonly used in continual learning benchmarks [20], [21], [23] to eval-uate model performance across sequential tasks: (1) Accuracy Mean (AM), defined as $AM = \\frac{1}{T} \\sum\\_{i=1}^{T} A\\_{T,i}$, and (2) Forgetting Mean (FM), given by $FM = \\frac{1}{T-1} \\sum\\_{i=1}^{T-1} (A\\_{i,i} - A\\_{T,i})$, where $T$ represents the total number of tasks. In standard CGL, $A\\_{i,j}$ denotes the accuracy on task $j$ following the completion of task $i$. However, within our FCGL framework, each client's local GNN independently predicts its own task, and we compute a weighted average of $A\\_{i,j}$ across clients, proportional to their sample counts. AM measures the average accuracy achieved on each task after training on the final task, where higher values indicate stronger overall performance. FM, by contrast, captures the mean reduction in performance on each task as additional tasks are learned, with lower values suggesting greater retention."}, {"title": "VI. CONCLUSION", "content": "This paper pioneers the exploration of federated continual graph learning (FCGL) for node classification, bridging the gap between the idealized experimental setups in previous centralized continual graph learning (CGL) studies and the challenges encountered in real-world decentralized deploy-ments. We introduce a practical solution to advance graph continual learning by harnessing collective intelligence across distributed graph databases. Through empirical analysis, we investigate the data characteristics, feasibility, and effectiveness of FCGL, identifying two critical challenges: local graph forgetting (LGF) and global expertise conflict (GEC). To address these challenges, we propose a novel FCGL framework that selectively replays experience nodes with maximum local-global coverage to mitigate the LGF, while employing a pseudo-prototype reconstruction process and a trajectory-aware knowledge transfer technique to resolve GEC. A promising avenue for future research involves developing more effective methods to identify compatibility and conflict relationships of expertise across evolving client subgraphs, facilitating improved global aggregation. This could further minimize the forgetting of old tasks and enhance adaptation to new tasks, advancing the potential of FCGL in dynamic environments."}]}