{"title": "Enhancing Topic Interpretability for Neural Topic Modeling through Topic-wise Contrastive Learning", "authors": ["Xin Gao", "Yang Lin", "Ruiqing Li", "Yasha Wang", "Xu Chu", "Xinyu Ma", "Hailong Yu"], "abstract": "Data mining and knowledge discovery are essential aspects of extracting valuable insights from vast datasets. Neural topic models (NTMs) have emerged as a valuable unsupervised tool in this field. However, the predominant objective in NTMs, which aims to discover topics maximizing data likelihood, often lacks alignment with the central goals of data mining and knowledge discovery which is to reveal interpretable insights from large data repositories. Overemphasizing likelihood maximization without incorporating topic regularization can lead to an overly expansive latent space for topic modeling. In this paper, we present an innovative approach to NTMs that addresses this misalignment by introducing contrastive learning measures to assess topic interpretability. We propose a novel NTM framework, named ContraTopic, that integrates a differentiable regularizer capable of evaluating multiple facets of topic interpretability throughout the training process. Our regularizer adopts a unique topic-wise contrastive methodology, fostering both internal coherence within topics and clear external distinctions among them. Comprehensive experiments conducted on three diverse datasets demonstrate that our approach consistently produces topics with superior interpretability compared to state-of-the-art NTMs.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realm of data mining and knowledge discovery, topic modeling stands as a fundamental unsupervised statistical technique. Its primary aim is to unveil latent semantic topics within a corpus of textual documents. One prominent model in this domain is Latent Dirichlet Allocation (LDA)[1]. LDA relies on a generative framework grounded in the Dirichlet prior and employs variational Bayesian algorithms for approximate inference. Crucially, topic interpretability plays a pivotal role in the context of data mining, as it empowers researchers and users to extract meaningful insights, comprehend underlying patterns, and effectively employ the identified topics for diverse applications[2], [3], [4], [5], [6], [7].\nRecent years have borne witness to an unprecedented surge in the scale of corpora requiring analysis through topic modeling. Nonetheless, generating high-quality topics on substantial corpora using LDA and its extensions has been recognized as a formidable challenge [8], [9]. To address these challenges and enhance adaptability and scalability, researchers have integrated Auto-encoding Variational Bayes [10] into the inference process. This integration has given rise to a class of models known as neural topic models (NTMs)[8], [9], [11]. However, despite these advancements, some NTMs have faced criticism for generating topics that are less interpretable to humans[12].\nThe main obstacle hindering NTMs from generating highly interpretable topics stems from the inconsistency between the current objective, which focuses on discovering topics that max-imize the likelihood of observed data and the users' intention in topic modeling, which includes discovering topics with high interpretability that facilitate human comprehension of large corpora. By solely focusing on likelihood maximization without incorporating any regularization, the latent space for topic modeling may become excessively large to comprehensively explore. Traditional topic models like LDA implicitly maintain topic interpretability by leveraging their Dirichlet prior, which promotes sparsity in topic-word distribution to restrict the latent space. Nevertheless, devising appropriate and accurate reparameterization tricks for the Dirichlet distribution poses a challenging task, leading researchers to employ various approximations in NTMs, which ultimately compromises the topics' interpretability. Consequently, we propose to integrate measurements of topic interpretability into the objective to regulate the latent space and steer NTMs toward uncovering better topics.\nAt present, the evaluation of topic interpretability typically revolves around two aspects: topic coherence and topic diversity, where coherence measures the degree of semantic consistency among the most relevant words within a topic, while diversity gauges the distinctiveness of the most relevant words across different topics as shown in Figure 1. In order to constrain the latent space, we propose an idea that explicitly incorporates topic interpretability as a regularizer into the objective function of NTMs. While the idea appears intuitive, several challenges persist that require further investigation.\nChallenge 1: Developing a computationally friendly regularizer. One approach is to directly incorporate interpretability-based evaluation metrics as regularizers. However, these metrics, e.g., topic coherence, rely on look-up operations in a large reference corpus, making them non-differentiable and computationally intensive [13]. Such non-differential operations may make it challenging to calculate gradients and perform backpropagation. Even though methods like policy gradient can address the non-differentiability issue, they usually introduce higher computational complexity and gradient variance. A preferable solution would involve designing differentiable surrogates that capture human cognition and encompass various aspects of topic interpretability.\nChallenge 2: Addressing the consideration of multiple aspects in topic interpretability? Drawing from relevant literature [14], [11], [15], [16], [17], it is widely recognized that topic interpretability necessitates the generation of topics that are both semantically coherent and distinctive to human understanding. Neglecting the aspect of distinctiveness may result in redundant topics with overlapping global themes, while overlooking coherence can result in superficially distinctive topics lacking a coherent semantic meaning. Balancing coher-ence and distinctiveness among topics remains a challenging task. To address this, we propose a topic-wise contrastive term that different from previous document-wise contrastive terms in NTMs. The core concept of contrastive learning involves promoting similarity between positive sample pairs while discouraging similarity between negative sample pairs. In the context of Contratopic, positive samples consist of words sampled from the same topic, while negative samples encompass words from different topics. Therefore, encouraging positive samples promotes coherence within a topic, while discouraging negative samples fosters distinctiveness across topics.\nWithin this paper, we propose an innovative framework for NTMs that introduces topic interpretability during the training phase through the integration of a topic-wise contrastive learning regularizer. This regularizer treats words associated with the same topic as positive samples and words associated with different topics as negative samples, thus enhancing the coherence and distinctiveness of the topic-word distribution. We additionally utilize a novel sampling technique that leverages the Gumbel-Softmax trick [18], enabling us to efficiently draw top-k related words from the topic-word distribution in a differentiable manner, generating pairs of positive and negative samples without replacement. To evaluate the effectiveness of our regularizer in improving topic interpretability, we conducted extensive experiments on three datasets. The results demonstrate the superior performance of our proposed ContraTopic approach compared to all baseline methods.\nThe contribution can be summarized as follows:\n\u2022 To mitigate the inconsistency between the current objective of NTMs and the intent usage of topic modeling, we propose an innovative framework for NTMs, named ContraTopic.\n\u2022 Based on the multiple aspects of topic interpretability, we devise a differentiable regularizer via topic-wise con-trastive learning, promoting coherence within individual topics and diversity across different topics.\n\u2022 Through experiments conducted on three datasets, we demonstrate that ContraTopic consistently generates topics with significantly higher levels of interpretability compared to all baseline approaches."}, {"title": "II. RELATED WORKS", "content": "In this section, we will first briefly introduce the development and main branches of the NTMs based on their architectures. Then we focus on NTMs that also leverage contrastive learning but with different goals and methodologies. Last but not least, we introduce NTMs that also attempt to improve the interpretability of generated topics.\n\nA. Neural Topic Models\nA major challenge in applying conventional topic models and developing new methods was the complex inference algorithms of the posterior distribution. The employment of neural networks and black-box inference methods has enabled topic models to automatically solve the inference process via backpropagation, which led to the thriving of NTMS. NVDM [8] and ProdLDA [9] leveraged autoencoding variational Bayes [10] as the basic architecture and respectively applied Gaussian and logistic normal distribution in approximations of the Dirichlet prior in the original LDA. Subsequently, various constructions of the prior distributions have been proposed [19], [15], [20], [11] aiming for a better approximation of the Dirichlet prior. On top of variational autoencoders (VAE), other famous architectures, such as generative adversarial networks [21], [22] and graph neural networks [23], [24], have also been applied into NTMs. Recently, many researchers have proposed novel NTMs via the theory of optimal transport (OT). By naturally incorporating word embeddings into the cost function of the optimal transport distance, their models [12], [25] were able to achieve a better balance between obtaining good document representation and generating topics with high quality. Recently, ECRTM [26] proposes a novel embedding clustering regularization that avoids the collapsing of topic embeddings.\n\nB. Contrastive Learning in NTMs\nThe idea of contrastive learning is to measure the similarity relations between samples by contrasting positive pairs against negative pairs. There have been various efforts to study contrastive methods to learn meaningful representation in a self-supervised way. It has been widely explored in enormous fields including image classification [27], [28], object detection [29], [30], adversarial training [31], and sequence modeling [32]. In recent years, contrastive learning also has been applied in topic modeling to leverage the relations among documents [33]. Nguyen et al. [34] proposed CLNTM with a novel contrastive objective that captured the mutual information between the document prototypes and their positive samples by modeling the relations among augmented samples. TSCTM [35] was proposed to sufficiently model the relations between documents against data sparsity via a new contrastive learning method with efficient sampling strategies.\nA pivotal distinction between our method and other topic models that incorporate contrastive terms stems from the foundational insights underlying each approach. Other methods, such as CLNTM, employ a document-wise contrastive term, with their guiding principle centered around encouraging similar document-topic distribution among similar documents, which will \"implicitly\" benefit the topic-word distribution and thereby contribute to topics with higher quality. In stark contrast, our approach adopts a topic-wise contrastive term, which, in a more direct and unequivocal manner, fosters coherence and diversity of the topic-word distribution, leading to an enhancement in the quality of topics. Empirical experiments corroborate this distinction, revealing that our topic-wise contrastive term eclipses the quality achieved by CLNTM, thus underscoring the merits of directly optimizing the topic-word distribution. More details about the differences can be found in subsection G of section Methodology.\n\nC. Improving Topic Interpretability in NTMs\nThere have been lots of works aiming at improving the topic interpretability of NTMs. Some works [8], [9], [19], [15] focused on finding better approximations for Dirichlet distribution, which encourages the sparsity among topics and implicitly improves the topic interpretability. Besides, another popular way to improve topic interpretability is to incorporate other external meta information into topic modeling, such as seed words [36], [37] and document labels [22], that are closely related to topics. Different from these methods, we do not in-corporate such external information, which means that there are no extra human-annotation costs and expertise in ContraTopic. Apart from incorporating external information, an alternative straightforward solution is to incorporate topic interpretability on the specific corpus into the objectives. Gui et al. [38] borrow the idea of reinforcement learning and incorporate topic coherence measures as rewards to guide the NTMs. However, their approach updates the topic-word distribution using rigid pre-defined rules, and the intricate complexity of the states poses challenges for achieving convergence. Ding et al. [13] also propose a method to incorporate topic coherence based on word embeddings that is differentiable and computation-efficient, but their method only focuses on topic coherence and ignores other aspects of topic interpretability such as topic diversity. In contrast, our approach uses the pre-computed NPMI in the given corpus to provide extra supervision and introduces a topic-wise contrastive regularizer that incorporates multiple facets of topic interpretability."}, {"title": "III. PRELIMINARY", "content": "A. Problem Formulation\nConsider a corpus consisting of D documents, where the vocabulary contains V distinct terms. Each document is represented as a bag-of-word vector $x \\in \\mathbb{R}^V$. Our goal is to derive K topics, which are set of distributions over words noted as $\\beta_1,..., \\beta_K \\in \\Delta^V$, and latent topic distribution for each document noted as $\\theta_1, ...\\theta_D \\in \\Delta^K$, where $\\Delta^K$ is a K \u2212 1 dimensional simplex.\n\nB. Neural Topic Modeling\nThe common generative story of NTMs is similar to [9], where the Dirichlet prior is approximated via a logistic normal distribution. The generative story is summarized as follows, where \u03b1 is the parameter for prior distribution:\n1. Draw topic distribution $\\theta \\sim LN(\\mu_\\theta(\\alpha),\\sigma_\\theta(\\alpha))$;\n2. For $w_{dn}$ in this document:\na. Draw topic $z_{dn} \\sim Cat(\\theta)$;\nb. Draw word $w_{dn} \\sim Cat(\\beta_{z_{dn}})$.\nThe subscripts like $w_{dn}$ on the third page indicate the n-th word in the d-th document of the corpus, following conventions from seminal topic model papers [1], [11]. The $LN(.)$ denotes the logistic-normal distribution, and $Cat(.)$ denotes the multinomial distribution. To leverage the pre-trained word embeddings, ETM [11] uses the word embeddings for the vocabulary $p \\in \\mathbb{R}^{V \\times e}$, where e is the dimension of embedding, and the assigned topic embedding $t_{z_{dn}} \\in \\mathbb{R}^e$ to draw the observed word from the assigned topic $z_{dn}$, noted by $\\mathcal{B}_{z_{dn}} = softmax(p t_{z_{dn}}/\\tau_\\beta)$, where $\\tau_\\beta$ is a temperature forcing the distribution to be sharp.\nBased on the generative story, the variational inference is used to approximate the posterior distribution of latent variables $\\rho, \\theta_\\alpha, Z_\\alpha = \\{z_{d1}, z_{d2},..., z_{dN_d}\\}$ and $t = \\{t_1, t_2,...,t_K\\}$ to maximize the likelihood of observed data. The evidence lower bound (ELBO) can be derived as\n\n$\\mathcal{L}(w) = E_{q(\\theta,z|w)} [log (p(w|\\theta, z; t))] - E_{q(\\theta,z|w)} log \\frac{q(\\theta, z|w)}{p(\\theta, z)},$\n\n$\\mathcal{L}(w) = \\mathcal{L}_{rec} + \\mathcal{L}_{kl}$.\n\nThe first term noted as $\\mathcal{L}_{rec}$ encourages variational distribution q to favor information that is good at explaining the observed words, and the second term noted as $\\mathcal{L}_{kl}$ encourages q to match the prior distribution.\nSpecifically, for the encoder $q(\\theta|w)$, we have $\\pi = MLP(w)$, $\\mu(w) = \\Pi_1(\\pi)$, $log \\sigma(w) = \\Pi_2(\\pi)$ and finally $\\theta(w) = softmax(\\mu + \\sigma \\odot \\epsilon)$, where $\\epsilon \\sim N(0, I)$. The functions $\\Pi_1$ and $\\Pi_2$ are linear transformations. For the decoder network, we collapse z and compute $p(w|\\theta, \\rho,t) = \\theta^T\\mathcal{B}$."}, {"title": "IV. METHODOLOGY", "content": "A. Topic-wise Contrastive Regularizer\nAs shown in the ELBO, the current objective of NTMs lacks consideration of topic interpretability during the training process of VAE-based NTMs, making it difficult to control the topic quality. To incorporate topic interpretability, we propose a topic-wise contrastive regularizer that simultaneously controls the coherence and distinction of generated topics. Figure 1 shows the fundamental insight of ContraTopic.\nFollowing [28], we first introduce the contrastive loss with multiple positive samples and negative samples. For a set of M samples $S = s_1, ....s_M$, the contrastive loss can be calculated as Eq.2.\n\n$\\mathcal{L}_{con} = -log \\sum_{p \\in P(i)} exp(\\mathcal{K}(i,p)) / \\sum_{a \\in I} exp(\\mathcal{K}(i,a))$,\n\nwhere $I = \\{1 ... M \\}$ is the full set of the indices of all samples, $P(i)$ is the set of indices of all positive samples for anchor i.\nConcretely, the full set of samples S consists of K sets of words that are sampled from each topic, noted by $S = \\cup_{k=1}^K T_k$, where $T_k$ denotes the most $v$ related words in the k-th topic, $v \\ll V$. $P(i)$ denotes the indices of samples from the same topic as $s_i$, noted by $P(i) = \\{j|s_i \\in T_k, s_j \\in T_k, and i \\neq j\\}$. The function $\\mathcal{K}(\\cdot)$ measures the similarity between two words. It can be implemented with dot product of word embeddings or the pre-computed Normalized Point-wise Mutual Information (NPMI) in the corpus. By employing the pre-computed NPMI in the corpus for $\\mathcal{K}(\\cdot)$, the positive samples in our regularizer serve to optimize topic coherence directly during training, while the negative samples aim to encourage the sampling of words with low NPMI from distinct topics.\nThe choice of NPMI in $\\mathcal{K}(\\cdot)$: It is indeed a subject of ongoing discourse to ascertain whether these automatic evalution metrics such as NPMI or $C_v$ genuinely align with the intricacies of human evaluation criteria [4], [39], [14], challenging whether the target of interpretability in Contratopic is ambiguous. However, there are also several works pointing out that these automated coherence metrics are still meaningful. For instance, recent research [40] attested to the significance of coherence metrics like NPMI across a spectrum of datasets, encompassing both general corpora such as Wiki and specialized ones like ArXiv and Pubmed. Their findings substantiated that NPMI remains a meaningful automatic coherence metric, showcasing a substantial correlation with human cognition.\nThe rationale behind our selection of NPMI lies in its established status as a widely employed coherence metric within topic modeling. Moreover, the incorporation of mutual information estimation resonates with our contrastive term's objectives: minimizing mutual information between distinct topics while simultaneously maximizing mutual information within individual topics.\n\nB. Sampling Strategy\nThe remaining problem is how to sample $T_k$ from topic k, given the topic-word distribution $\\mathcal{B}_k$ calculated by the word embeddings $p$ and topic embedding $t_k$. However, sampling from a discrete distribution is non-differential. The Gumbel-softmax trick [18] provides a simple and efficient way to parameterize a discrete distribution and allow for the propagation of gradients in the sampling process. In detail, the Gumbel-softmax estima-tor gives an approximate one-hot sample $y = \\{y_1, y_2, .., y_n\\}$ with Eq.3.\n\n$y_i = exp((\\text{log} \\omega_i + g_i) / T_g) / \\sum_{j=1}^V exp((\\text{log} \\omega_j + g_j) / T_g)$, \n\nwhere $T_g$ is the temperature and $\\text{log}(\\omega_i)$ are logits for a softmax distribution $p(x_i|\\omega) = \\omega_i/Z$ and Z is the partition function. $y_i$ denotes the probability of random variable $x_i$ being sampled from the discrete distribution in the Gumbel-Softmax estimator. $g_i = -\\text{log}(-\\text{log }u_i)$ is called a Gumbel random variable and $u_i \\sim Uniform (0,1)$. Since the randomness $g$ is independent of $\\omega$, the reparameterization trick can be leveraged to optimize the model with back-propagation.\nAs we are interested in sampling a subset of words from the topic-word distribution, i.e., drawing samples without replace-ment, we employ a relaxed subset sampling algorithm proposed by [41]. Given the topic-word distribution $\\mathcal{B}$, consisting of $\\beta_k$, and Gumbel variables $g_k$, a Gumbel-max key is computed for each word as $r_k = \\text{log }\\beta_k + g_k$, where $r_k, \\beta_k,\\text{and }g_k \\in \\mathbb{R}^V$. A relaxed subset sample of the items can be drawn by applying a relaxed top-v procedure directly on $r_k$. The procedure defines\n\n$\\mathcal{P}(r_i^+ = 1) = 1 - \\text{log}(1 - \\mathcal{P}(r_i^+ = 1))$,\n\nwhere $r_i^+ := r_k$ and\n\n$\\mathcal{P}(r_i^+ = 1) = \\text{exp}(r_i^+/T_v) / \\sum_{m=1}^V \\text{exp}(r_m^+/T_v)$.\n\nFinally, a relaxed $v$-hot vector for topic k, representing the $v$ words sampled can be computed as $y_k = \\sum_{i=1}^V \\mathcal{P}(r_i^+ = 1)$.\nBalance between Positive and Negative Samples: To maintain the simplicity of our method, we abstain from introducing any components aimed at balancing the quantity of positive and negative samples. Nevertheless, our approach does entail the sampling of \"v\" top words from each topic, which results in $k\\cdot C_2^v$ positive word pairs and $v^2 \\cdot C_k^2$ negative word pairs in all the possible word pairs. The balance between these pairs can be modulated by tuning v, thereby achieving a proportionate ratio. Of course, other methods such as incorporating a hyper-parameter to balance the weights of negative word pairs can also be considered if necessary."}, {"title": "Algorithm 1 The training procedure.", "content": "Input: the input corpus $\\mathcal{D}$, topic number K, total epoch number T, hyperparameter \u03bb, v, temperatures $T_g$, $T_\\beta$\nOutput: K topic-word distributions $\\mathcal{B}_k$, D document-topic distribution $\\theta_d$\nInitialize model and variational parameters\nfor epoch from 1 to T do\nfor a random batch of B documents do\n$\\mathcal{L}_{batch} \\leftarrow 0$;\n$\\mathcal{B} \\leftarrow softmax(p t/\\tau_\\beta)$\nfor each document d in the batch do\n$\\theta_d \\leftarrow q(\\theta|w_d)$\n$p(\\omega|\\theta, \\rho, t) \\leftarrow \\theta^T\\mathcal{B}$.\n$\\mathcal{L}_{batch} \\leftarrow \\mathcal{L}_{batch} + (\\mathcal{L}_{rec} + \\mathcal{L}_{kl})$ by Eq.1;\nend for\nfor k from 1 to K do\nrk $\\leftarrow \\text{log }\\mathcal{B}_k + g_k$\nfor j from 1 to v do\nCompute $\\mathcal{P}(r_i^+ = 1)$ by Eq. 4 and Eq. 5\nend for\nyk \\leftarrow \\sum_{j=1}^V \\mathcal{P}(r_j^+ = 1)$\nGenerate the subset Tk with the $v$-hot vector yk\nend for\n$\\mathcal{L}_{batch} \\leftarrow \\mathcal{L}_{batch} + \\lambda\\mathcal{L}_{con}$ by Eq.2;\nUpdate model parameters with $\\nabla\\mathcal{L}_{batch}$\nend for\nend for\n\nC. Training Objective\nOur final training objective is\n\n$\\mathcal{L}_{tr} = \\mathcal{L}_{rec} + \\mathcal{L}_{kl} + \\lambda\\mathcal{L}_{con}$,\n\nwhere \u03bb is a hyperparameter. The training procedure is described in Algorithm 1.\nA possible concern on Contratopic could be whether op-timizing evaluation metrics in training objectives is valid or perfectly fair. Our objective function encompasses not solely the contrastive term, intricately tied to the evaluation metrics, but also integrates the conventional KL-divergence term and reconstruction term. The efforts to make the evaluation metrics to be incorporated in the training objectives are also parts of our contribution. Moreover, optimizing an objective closely aligned with the evaluation metrics is not rare in the domain of machine learning. For instance, the ubiquitous cross-entropy objective function maintains a significant correlation with diverse evaluation metrics, including AUC scores and accuracy measurements. Besides, there are also works in image classification that focus on directly optimizing the AUROC and AUPRC scores during the training stage [42], [43].\n\nD. From Mutual Information Estimation Perspective\nThe loss function of contrastive learning in our ContraTopic is similar to most mutual information neural estimation methods such as MINE [44]. However, there are still some differences during the optimization. MINE uses a lower bound to the mutual information based on the Donsker-Varadhan representation of the KL-divergence and chooses $\\mathcal{K}_\\phi$ to be the family of functions $K_\\phi: X \\times Z \\rightarrow \\mathbb{R}$ parameterized by a deep neural network with parameters $\\phi \\in \\Phi$,\n\n$I(X; Z) \\geq I_\\phi(X; Z) := \\text{sup}_{\\Phi \\in \\Phi} E_{Pxz} [K_\\phi] - \\text{log} (E_{Px \\otimes Pz} [e^{K_\\phi}])$, \n\nwhere X and Z are random variables and $P_{xz}$ is the joint distribution and $P_x \\otimes P_z$ is the product of their marginal distribution. Given the joint and marginal distribution, MINE optimizes the $K_\\phi$ to maximize the objective, which measures the similarities between samples from X and Z.\nIn most mutual information neural estimation scenarios, the marginal distribution of variables X and Z is typically available, while the similarity measure network $\\mathcal{K}(\\cdot)$ is unknown and necessitates parameterization and optimization. However, in the case of ContraTopic, the similarity measure network $\\mathcal{K}(\\cdot)$ is pre-defined as the NPMI score between two given words and cannot undergo optimization during training. In contrast, we parameterize the marginal distribution of X and Z and optimize their distributions to maximize the objective. In summary, to enhance the coherence and diversity of topics, we employ parameterization and optimization techniques by optimizing the topic-word distribution based on provided similarity measure networks and NPMI scores, thereby maximizing the mutual information within a single topic and minimizing that across different topics.\n\nE. Differences between ContraTopic and Other methods\nA pivotal distinction between our method and other topic models that incorporate contrastive terms stems from the foun-dational insights that underscore each approach. As delineated in related works, other methods, exemplified by CLNTM [34], employ a \"document-wise\" contrastive term, with their guiding principle centered around nurturing similar document-topic distribution among similar documents, which will implicitly benefit the topic-word distribution and thereby contribute to topics with higher quality. In stark contrast, our approach adopts a \"topic-wise\" contrastive term, which, in a more direct and unequivocal manner, fosters coherence and diversity of the topic-word distribution, leading to an enhancement in the quality of topics. Empirical experiments corroborate this distinction, revealing that our topic-wise contrastive term eclipses the quality achieved by CLNTM, thus underscoring the tangible merits of optimizing the topic-word distribution to improve topic quality.\nCompared with CLNTM, our definitions of samples, noted as i or a in eq(2), are different. In Contratopic, we treat predominant words from topics as samples while CLNTM treats documents as samples. Such differences mainly result from the different fundamental insights discussed above.\nBesides, the definitions of similarity functions, noted as $\\mathcal{K}(\\cdot)$ in eq(2), are also different. In Contratopic, we use the precomputed NPMI scores to measure the similarity between words while CLNTM leverages dot product among document representations to measure the similarity between documents."}, {"title": "V. EXPERIMENTS", "content": "A. Datasets\nOur experiments are conducted on three widely-used datasets", "45": "is a dataset that contains around 20", "46": "is a larger dataset that contains nearly 150", "6": 4.0, "metrics": "topic coherence", "12": "and report the average NPMI scores in different percentages of topics. The proportions of the selected topics vary from 10% to 100%. Following previous works [12", "25": "K_{TC"}, "is set to 10 and $K_{TD}$ is set to 25.\nFor the evaluation of document-topic distributions, we perform document clustering tasks. We report the purity and Normalized Mutual Information (NMI) on 20NG and Yahoo, where the document labels are available. Given the document-topic distribution as the document representation, we apply the KMeans algorithm on test data and report the scores of the KMeans clusters (denoted by km-Purity and km-NMI),\nC. Experimental Settings\nWe compare the performance of our proposed model with the following baselines: 1) conventional topic models, LDA [1"], "factors": "the sampling process and the pair-wise computation of NPMI in $\\mathcal{K"}