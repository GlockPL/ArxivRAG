{"title": "Membership Inference Attacks against Large Vision-Language Models", "authors": ["Zhan Li", "Yongtao Wu", "Yihang Chen", "Francesco Tonin", "Elias Abad Rocamora", "Volkan Cevher"], "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR\u00e9nyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.", "sections": [{"title": "Introduction", "content": "The rise of large language models (LLMs) [9, 60, 45, 11] has inspired the exploration of large models across multi-modal domains, exemplified by advancements like GPT-4 [1] and Gemini [59]. These large vision-language models (VLLMs) have shown promising ability in various multi-modal tasks, such as image captioning [33], image question answering [13, 35], and image knowledge extraction [26]. However, the rapid advancement of VLLMs also causes user concerns about privacy and knowledge leakage. For instance, the image data used during commercial model training may contain private photographs or medical diagnostic records. This is concerning since early work has demonstrated that machine learning models can memorize and leak training data [3, 56, 63]. \u03a4\u03bf mitigate such concerns, it is essential to consider the membership inference attack (MIA) [23, 53], where attackers seek to detect whether a particular data record is part of the training dataset [23, 53]. The study of MIAs plays an important role in preventing test data contamination and protecting data security, which is of great interest to both industry and academia [24, 19, 44].\nWhen exploring MIAs in VLLMs, one main issue is the absence of a standardized dataset designed to develop and evaluate different MIA methods, which comes from the large size [16] and multi-modality of the training data, and the diverse VLLMs training pipelines [66, 35, 18]. Therefore, one of the main goals of this work is to build an MIA benchmark tailored for VLLMs.\nBeyond the need for a valid benchmark, we lack efficient techniques to detect a single modality in VLLMs. The closest work to ours is [30], which performs MIAs on multi-modal CLIP [46] by detecting whether an image-text pair is in the training set. However, in practice, it is more common to detect a single modality, as we care whether an individual image or text is in the training set. Therefore, we aim to develop a pipeline to detect the single modality from a multi-modal model. Moreover, existing literature on language model MIAs, such as Min-K% [52] and Perplexity [62], mostly are target-based MIAs, which use the next token as the target to compute the prediction probability. However, we can only access the image embedding instead of the image token in VLLMs, and thus only target-free MIAs [48] can be directly applied.\nTherefore, we first propose a cross-modal pipeline for individual image or description MIAs on VLLMs, which is distinguished from traditional MIAs that only use one modality [61, 62]. We feed the VLLMs with a customized image-instruction pair from the target image or description. We show that we can perform the MIA not only by the image slice but also by the instruction and description slices of the VLLM's output logits, see Figure 1. Such a cross-modal pipeline enables the usage of text MIA methods on image MIAs. We also introduce a target-free metric that adapts to both image and text MIAs and can be further modified to a target-based way.\nOverall, the contributions and insights can be summarized as follows.\n\u2022 We release the first benchmark tailored for the detection of training data in VLLMs, called Vision Language MIA (VL-MIA) (Section 4). By leveraging Flickr and GPT-4, we construct VL-MIA that contains two images MIA tasks and one text MIA task for various VLLMs, including MiniGPT-4 [66], LLaVA 1.5 [35] and LLaMA-Adapter V2 [18].\n\u2022 We perform the first individual image or description MIAs on VLLMs in a cross-modal manner. Specifically, we demonstrate that we can perform image MIAs by computing statistics from the image or text slices of the VLLM's output logits (Figure 1 and Section 5.1).\n\u2022 We propose a target-free MIA metric, MaxR\u00e9nyi-K%, and its modified target-based ModR\u00e9nyi (Section 5.2). We demonstrate their effectiveness on open-source VLLMs and closed-source GPT-4 (Section 6). We achieve an AUC of 0.815 on GPT-4 in image MIAs."}, {"title": "Related work", "content": "Membership Inference Attack (MIA) aims to classify whether a data sample has been used in training a machine learning model [53]. Keeping training data confidential is a desired property for"}, {"title": "Problem setting", "content": "In this section, we introduce the main notation and problem settings for MIAs.\nNotation. The token set is denoted by $\\mathcal{V}$. A sequence with $L$ tokens is denoted by $X := (x_1, x_2, ..., x_L)$, where $x_i \\in \\mathcal{V}$ for $i \\in [L]$. Let $X_1 \\oplus X_2$ be the concatenation of sequence $X_1$ and $X_2$. An image token sequence is denoted by $Z$. In this work, we focus on the VLLM, parameterized by $\\theta$, where the input is the image $Z$ followed by the instruction text $X_{ins}$, and the output is the description text $X_{des}$. We use $\\mathcal{D}_{des}$ and $\\mathcal{D}_{image}$ to represent the description training set and image training set, respectively. Detailed notations are summarized in Table 5 of the appendix.\nAttacker's goal. In this work, the purpose of the attacker is to detect whether a given data point (image $Z$ or description $X_{des}$) belongs to the training set. We formulate this attack as a binary classification problem. Let $A_{image}(Z; \\theta) :\\rightarrow {0, 1}$ and $A_{des}(X; \\theta) :\\rightarrow {0,1}$ be two binary classification algorithms for image and description respectively, which are implemented by comparing the metric $\\text{Score}(Z \\oplus X_{ins} \\oplus X_{des}; \\theta)$ with some threshold $\\lambda$.\nWhen detecting image $Z$, we feed the model with the target image with a fixed instruction prompt such as \"Describe this image in detail\", denoted as $X_{ins}$. The model then generates the description text $X_{des}$. The algorithm $A_{image}(Z; \\theta)$ is defined by\n$A_{image} (Z; \\theta)=\\begin{cases}1 & (Z \\in \\mathcal{D}_{image}), \\text{ if } \\text{Score}(Z\\oplus X_{ins} \\oplus X_{des}; \\theta) < \\lambda, \\\\0 & (Z \\notin \\mathcal{D}_{image}), \\text{ if } \\text{Score}(Z\\oplus X_{ins} \\oplus X_{des}; \\theta) \\geq \\lambda.\\end{cases}$ \nWhen detecting a description sequence $X_{des}$, we feed the model with an all-black image, denoted as $Z_{ept}$, as the visual input, followed by an empty instruction $X_{ept}$. The algorithm $A_{des}(X_{des}; \\theta)$ is defined by\n$A_{des}(X_{des}; \\theta) = \\begin{cases}1 & (X_{des} \\in \\mathcal{D}_{des}), \\text{ if } \\text{Score}(Z_{ept} \\oplus X_{ept} \\oplus X_{des}; \\theta) < \\lambda, \\\\0 & (X_{des} \\notin \\mathcal{D}_{des}), \\text{ if } \\text{Score}(Z_{ept} \\oplus X_{ept} \\oplus X_{des}; \\theta) \\geq \\lambda.\\end{cases}$"}, {"title": "Dataset construction", "content": "We construct a general dataset: Vision Language MIA (VL-MIA), based on the training data used for popular VLLMs, which, to our knowledge, is the first MIA dataset designed specifically for VLLMs. We present a takeaway overview of VL-MIA in Table 1. We also provide some examples in VL-MIA, see Table 16 in the appendix. The prompts we use for generation can be found in Table 6."}, {"title": "Method", "content": "A cross-modal pipeline to detect image\nVLLMs such as LLaVA and MiniGPT project the vision encoder's embedding of the image into the feature space of LLM. However, a major challenge for image MIA is that we do not have the ground-truth image tokens. Only the embeddings of images are available, which prevents directly transferring many target-based MIA from languages to images. To this end, we propose a token-level image MIA which calculates metrics based on the output logit of each token position.\nThis pipeline consists of two stages, as demonstrated in Figure 1. In generation stage, we provide the model with an image followed by an instruction to generate a textual sequence. Subsequently, in inference stage, we feed the model with the concatenation of the same image, instruction, and generated description text. During the attack, we correspondingly slice the output logits into image, instruction, and description segments, which we use to compute various metrics for MIAs. Our pipeline considers the information from the image, the instructions and the descriptions following the image. In practice, even if there is no access to the logits of the image feature and instruction slice, we can still detect the member image solely from the model generation. We visually describe a prompt example with different slice notations presented in Appendix A.2.\nOur pipeline operates on the principle that VLLMs' responses always follow the instruction prompt [60], where the images usually precede the instructions and then always precede the descriptions. For causal language models used in VLLMs that predict the probability of the next token based on the past history [45], the logits at text tokens in the sequence inherently incorporate information from the preceding image."}, {"title": "MaxR\u00e9nyi MIA", "content": "We propose our MaxR\u00e9nyi-K%, utilizing the R\u00e9nyi entropy of the next-token probability distribution on each image or text token. The intuition behind this method is that if the model has seen this data before, the model will be more confident in the next token and thus have smaller R\u00e9nyi entropy.\nGiven a probability distribution $p$, the R\u00e9nyi entropy [47] of order $\\alpha$, is defined as $H_{\\alpha}(p) = \\frac{1}{1-\\alpha} \\log\\left(\\sum_j (p_j)^\\alpha\\right), 0 < \\alpha < \\infty, \\alpha \\neq 1$. $H_{\\alpha}(p)$ is also further defined at $\\alpha = 1, \\infty$, as $H_{\\alpha}(p) = \\lim_{\\gamma \\rightarrow \\alpha} H_{\\gamma}(p)$ by\n\u2022 $H_1(p) = - \\sum_j p_j \\log p_j$, \u2022 $H_{\\infty}(p) = - \\log \\max p_j$.\nTo be more specific, given a token sequence $X := (x_1, x_2, ..., x_L)$, let $p^{(i)}(\\cdot) = P(\\cdot | x_1, ..., x_i)$ be the probability of next-token distribution at the i-th token. Let Max-K%(X) be the top K% from the sequence X with the largest R\u00e9nyi entropies, the MaxR\u00e9nyi-K% score of X equals\n$\\text{MaxR\u00e9nyi-K\\%(X)} = \\frac{1}{|\\text{Max-K\\%(X)}|} \\sum_{i \\in \\text{Max-K\\%(X)}} H_{\\alpha}(p^{(i)})$.\nWhen K = 0, we define the MaxR\u00e9nyi-K% score to be $\\max_{i \\in [L-1]} H_{\\alpha}(p^{(i)})$. When K = 100, the MaxR\u00e9nyi-K% score is the averaged R\u00e9nyi entropy of the sequence X.\nIn our experiments, we vary $\\alpha = 1, 1, 2$, and $+\\infty$; K = 0, 10, 100. As $\\alpha$ increases, the top percentile of distribution p will have more influence on $H_{\\alpha}(p)$. When $\\alpha = 1$, $H_1(p)$ equals the Shannon entropy [50], and our method at K = 100 is equivalent to the Entropy [48]. When $\\alpha = \\infty$, we consider the most likely next token probability [31]. In contrast, Min-K% [52] only deals with the target next token probability. When the sequence is generated by the target model deterministically, i.e., when the model always generates the most likely next token, our MaxR\u00e9nyi-K% at $\\alpha = \\infty$ is equivalent to the Min-K%.\nWe also extend our MaxR\u00e9nyi-K% to the target-based scenarios, denoted by ModR\u00e9nyi. We first consider linearized R\u00e9nyi entropy, $\\bar{H}_\\alpha(p) = \\frac{1}{\\alpha-1}(\\sum_j (p_j)^\\alpha - 1), 0 < \\alpha < \\infty, \\alpha \\neq 1$. $\\bar{H}_\\alpha(p)$ is also further defined at $\\alpha = 1$, as $\\bar{H}_1(p) = \\lim_{\\alpha \\rightarrow 1} \\bar{H}_\\alpha(p) = H_1(p)$. Assuming the next token ID is y, recall that a small entropy value or a large $p_y$ value indicates membership, we want our modified entropy to be monotonically decreasing on $p_y$ and monotonically increasing on $p_j$, $j \\neq y$. Therefore, we propose the modified R\u00e9nyi entropy on a given next token ID y, denoted by $H_\\alpha(p, y)$:\n$\\begin{aligned}H_\\alpha(p, y) &= \\frac{1}{\\alpha-1} \\bigg(\\frac{\\left(1-p_y\\right) p_y^{\\alpha-1}}{\\sum_{j \\neq y}\\left(1-p_j\\right) p_j^{\\alpha-1}} -\\frac{\\left(1-p_y\\right)}{\\sum_{j \\neq y}\\left(1-p_j\\right)}\\bigg).\n\\end{aligned}$\nLet $\\alpha \\rightarrow 1$, we have $H_1(p, y) = \\lim_{\\alpha \\rightarrow 1} H_\\alpha(p,y) = -\\sum_{j\\neq y} p_j \\log(1 - p_j) - (1 - p_y) \\log p_y$, which is equivalent to the Modified Entropy [58]. In addition, our more general method does not encounter numerical instability in Modified Entropy as $p_j \\rightarrow 0, 1$ at $\\alpha \\neq 1$. For simplicity, we let the ModR\u00e9nyi score be the averaged modified R\u00e9nyi entropy of the sequence."}, {"title": "Experiments", "content": "In this section, we conduct MIAs across three target models using various baselines, MaxR\u00e9nyi-K%, and ModR\u00e9nyi. Experiment setup is provided in Section 6.1. The results on text MIAs and image MIAs are present in Section 6.2 and Section 6.3, respectively. In Section 6.4, we show that the proposed MIA pipeline can also be used in GPT-4. Ablation studies are present in Section 6.5. The versions and base models of VLLMs we use are listed in Table 7 of the appendices."}, {"title": "Experimental setup", "content": "Evaluation metric. We evaluate different MIA methods by their AUC scores. AUC score is the area under the receiver operating characteristic (ROC) curve, which measures the overall performance of a classification model in all classification thresholds \u5165. The higher the AUC score, the more effective the attack is. In addition to the average-case metric AUC, we also include the worst-case metric, the True Positive Rate at 5% False Positive Rate (TPR@5%FPR) in Appendix D suggested by [5].\nBaselines. We take existing metric-based MIA methods as baselines and conduct experiments on our benchmark. We use the MIA method from [37], which compares the feature vectors produced by the original image with the augmented image. We use KL-divergence to compare the logit distributions and term it Aug-KL in this paper. We also use Loss attack [62], which is perplexity in the case of language models. Furthermore, we consider ppl/zlib and ppl/lowercase [4], which compare the target perplexity to zlib compression entropy and the perplexity of lowercase texts respectively. [52] proposes Min-K% method, which calculates the smallest K% probabilities corresponding to the"}, {"title": "Image \u039c\u0399\u0391", "content": "We first conduct MIAs on images using VL-MIA/Flickr and VL-MIA/DALL-E in three VLLMs. For the image slice, it is not possible to perform target-based MIAs, because of the absence of ground-truth token IDs for the image. However, our MIA pipeline presented in Figure 1 can still handle target-based metrics by accessing the instruction slice and description slice.\nAs demonstrated in Table 2, MaxR\u00e9nyi-K% surpasses other baselines in most scenarios. An a value of 0.5 yields the best performance in both VL-MIA/Flickr and VL-MIA/DALL-E. As a increases, performance becomes erratic and generally deteriorates, though it remains superior to all target-based metrics. Overall, target-free metrics outperform target-based metrics for image MIAs. Another interesting observation is that instruction slices result in unstable AUC values, sometimes falling below 0.5 in target-based MIAs. This can be partially explained by the fact the model is more familiar with the member data. As a result, after encountering the first word \"Describe\", the model is more inclined to generate the description directly than generating the following instruction of Xins, i.e., \"this image in detail\u201d. This is an interesting phenomenon that we leave to future research.\nThe performance of the image MIA model is influenced by its training pipelines. Recall that MiniGPT-4 only updates the parameters of the image projection layer in image training, and LLaMA Adapter v2 applies parameter-efficient fine-tuning approaches. In contrast, LLaVA 1.5 training updates both the parameters of the projection layer and the LLM. The inferior performance of MIAs on MiniGPT-4 and LLaMA Adapter compared to LLaVA 1.5 is therefore consistent with [52] that more parameters' updates make it easier to memorize training data.\nWe find that VL-MIA/DALL-E is a more challenging dataset than VL-MIA/Flickr, reflected in the AUC being closer to 0.5. In VL-MIA/DALL-E, each non-member image is generated based on the description of a member image. Therefore, member data have a one-to-one correspondence with non-member data and depict a similar topic, which makes it harder to discern."}, {"title": "Text MIA", "content": "Text member data might be used in different stages of VLLM training, including the base LLM model pre-training and the later VLLM instruction-tuning. We hypothesize that after the last usage of the member data in its training, the more the model changes, the better the target-free MIA methods compared to target-based ones, and vice-versa. The heuristic is that if the model's parameters have changed a lot, target-free MIA methods, which use the whole distribution to compute statistics, are more robust than target-based methods, which rely on the probability at the next token ID. On the other hand, if the member data are seen in recent fine-tuning, the next token will convey more causal relations in the sequence remembered by the model, and thus target-based ones are better."}, {"title": "Image MIA on GPT-4", "content": "In this section, we demonstrate the feasibility of image MIAs on the closed-source model GPT-4. Our experiments use two image datasets: VL-MIA/Flickr and VL-MIA/DALL-E, detailed in Section 4. We choose GPT-4-vision-preview API, which was trained in 2023 and likely does not see the member data in either dataset. We randomly select 200 images per dataset and prompt GPT-4 to describe them in 64 words. We then apply MIAs based on the generated descriptions. Since GPT-4 can only provide the top-five probabilities at each token position, we can not directly use the proposed MaxR\u00e9nyi-K% that requires the whole probability distribution. To address this issue, we assume the size of the entire token set is 32000 and the probability of the remaining 31995 tokens are uniformly distributed. The AUC results are present in Table 4. We omit the result of perplexity and Min-K% since they are equivalent to MaxR\u00e9nyi-K% with a = \u221e in the greedy-generated setting, as discussed in Section 5.2. Surprisingly, we observe that in VL-MIA/DALL-E, the best-performed method MaxR\u00e9nyi-K% (a = 0.5) can achieve an AUC of 0.815. This indicates a high level of effectiveness for MIAs on GPT-4, demonstrating the potential risks of privacy leakage even with closed-source models."}, {"title": "Ablation study", "content": "Does the length of description affect the image MIA performance? We conduct ablation experiments on LLaVA 1.5 targeting the length of generated description texts with MaxR\u00e9nyi-10%. In the generation stage, we restrict the max_new_tokens parameter of the generate function to (32, 64, 128, 256) to obtain description slices of different lengths. As presented in Figure 2a, when the length of the description increases, the AUC of the MIA becomes higher and enters a plateau when max_new_tokens reaches 128. This may be because a shorter text contains insufficient information about the image, and in an excessively long text, words generated later tend to be more generic and not closely related to the image, thereby contributing less to the discriminative information that helps discern the membership.\nCan we still detect corrupted member images? The motivation is to detect whether sensitive images are inappropriately used in VLLM's training even when the images at hand may get corrupted. We leverage ImageNet-C [21] to generate corrupted versions of member data in VL-MIA/Flickr: Snow, Brightness, JPEG, and Motion_Blur, with the parameters in Table 8. The corrupted examples and corresponding model output generations are demonstrated in Appendix C Table 17 and Table 18. We take MaxR\u00e9nyi-K% (a = 0.5) as the attacker and the results of LLaVA are presented in Figure 2b. Corrupted member images make MIAs more difficult, but can still be detected successfully. We also observe that reducing model quality (JPEG) or adding blur (Motion_Blur) degrade MIA performance more than changing the base parameter (Brightness) or overlaying texture (Snow).\nCan we use different instructions? We conduct image MIAs on VL-MIA/Flickr with LLaVA through three different instruction texts: \u201cDescribe this image concisely.\u201d, \u201cPlease introduce this painting.", "Tell me about this image.": "We present our results in Table 14 of the appendix. Our pipeline successfully detects member images on every instruction, which indicates robustness across different instruction texts."}, {"title": "Conclusion", "content": "In this work, we take an initial step towards detecting training data in VLLMs. Specifically, we construct a comprehensive dataset to perform MIAs on both image and text modalities. Additionally, we uncover a new pipeline for conducting MIA on VLLMs cross-modally and propose a novel method based on R\u00e9nyi entropy. We believe that our work paves the way for advancing MIA techniques and, consequently, enhancing privacy protection in large foundation models."}, {"title": "Supplementaries to the main text", "content": "Detailed notation\nWe summarize the notations of this work in Table 5."}, {"title": "Explanation and visualization of slices", "content": "We give an example of the different slices in the MiniGPT-4 prompt in Figure 3."}]}