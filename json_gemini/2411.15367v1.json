{"title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage", "authors": ["Soumil Datta", "Shih-Chieh Dai", "Leo Yu", "Guanhong Tao"], "abstract": "Text-to-image diffusion models, such as Stable Diffusion, have shown exceptional potential in generating high-quality images. However, recent studies highlight concerns over the use of unauthorized data in training these models, which may lead to intellectual property infringement or privacy violations. A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features. In this paper, we examine the robustness of various watermark-based protection methods applied to text-to-image models. We observe that common image transformations are ineffective at removing the watermark effect. Therefore, we propose RATTAN, that leverages the diffusion process to conduct controlled image generation on the protected input, preserving the high-level features of the input while ignoring the low-level details utilized by watermarks. A small number of generated images are then used to fine-tune protected models. Our experiments on three datasets and 140 text-to-image diffusion models reveal that existing state-of-the-art protections are not robust against RATTAN.", "sections": [{"title": "1. Introduction", "content": "In the rapidly evolving landscape of artificial intelligence (AI), generative AI has emerged as one of the most transformative areas [24], with Text-to-Image (T2I) models such as Stable Diffusion [43] and DALL-E [42] gaining popularity. These models have made significant strides in generating highly realistic images [9, 36, 56], sometimes to the extent that even humans struggle to distinguish AI-generated content from actual photographs [4, 51]. Advancements in these models offer substantial benefits, including enhanced creative flexibility [17, 54] and a reduction in manual effort [15, 26, 41].\nThe success of these T2I models relies heavily on the large amounts of data available for training. For example, widely used datasets, such as the LAION dataset, contain more than 5 billion image-text pairs [46]. However, these datasets may also include images that are intellectual property or private, which might be unintentionally incorporated into the model or even intentionally exploited by an adversary [1, 30, 30, 50]. For instance, an attacker might maliciously collect artwork from an artist without proper permission and train a T2I model on these samples. Once trained, the attacker could sell the generated images, which would be visually similar to the original artworks [8, 18]. This significantly raises concerns regarding unauthorized data usage, as it leads to intellectual property infringement or privacy violations [3, 59].\nTo address this problem, existing research has introduced various detection and protection approaches. Membership inference attacks [6, 33, 49] were originally designed to extract private information from a machine learning model by determining whether a given private input was part of its training data. These techniques can also be adapted to detect unauthorized data usage, as they share a similar goal [14, 25, 37]. However, it has been shown that membership inference attacks are less effective for T2I diffusion models [13], achieving only around a 66% detection rate on Stable Diffusion v1.5 [43].\nAnother line of research leverages a watermark as a secret key to modify protected images [11, 27, 32, 57]. Any T2I models trained on these samples will also \u201cmemorize\" the watermark. During inference, the T2I model will generate images containing the watermark, which can thus be successfully detected. DIAGNOSIS [53], a state-of-the-art watermark-based protection method, applies a stealthy coating to protected images using a specialized function. Models trained on these coated images will produce outputs with a similar coating effect, thereby flagging the use of unauthorized data. Note that other watermarking methods aim to watermark AI-generated images [22, 31, 44, 60]. These methods differ from the focus of this paper, as they are applied to images already generated by models and aim to identify AI-generated content. In contrast, our work focuses on preventing unauthorized data usage in T2I diffusion models, where the images in question are real (not AI-generated) and need protection from being illegitimately learned by such models.\nIn this paper, we investigate the robustness of state-of-the-art watermarking frameworks for T2I diffusion models. Specifically, we apply a set of common image transformations to watermarked images and observe that these transformations are largely ineffective at removing the watermark effect. This is due to the design of these approaches, which aim to preserve the fine-grained details of inputs precisely the space utilized by watermarks. However, the primary goal of training on protected data is to generate images with similar high-level features. Hence, we propose to extract high-level coarse-grained features from protected images while ignoring low-level details. Particularly, we introduce RATTAN (Removing wATermarks in Text-to-image diffusioN models), which applies a controlled image generation process to protected images. The bottom part of Figure 1 illustrates the pipeline of RATTAN. Specifically, we leverage an off-the-shelf T2I model to generate a new version of the input based on both the protected image and the corresponding text. RATTAN takes advantage of the diffusion process to preserve key input features while removing unnecessary low-level details. The generated images are then used to fine-tune the model. Our evaluation on three datasets and 140 text-to-image diffusion models demonstrates that RATTAN can significantly reduce the detection rate of existing protections to 50%, equivalent to random guessing."}, {"title": "2. Background and Related Work", "content": ""}, {"title": "2.1. Text-to-Image Diffusion Model", "content": "Text-to-image diffusion models have gained popularity for their ability to generate high-quality images from textual descriptions [12, 45, 58]. Stable Diffusion [43], like other models, is an open-source generative model that iteratively transforms noisy data into coherent images through a diffusion process, producing highly detailed and diverse outputs [23, 38].\nThe diffusion process can be intuitively understood as a procedure of sequentially adding and removing Gaussian noise. Starting with an input image, random noise sampled from a normal distribution is iteratively added until the image is transformed into pure Gaussian noise. The goal of training a diffusion model is to learn a neural network that reverses this process, iteratively removing noise until the original input is recovered [5, 10, 55]. Formally, in the forward process of adding noise, given an initial input x, its value at time step t is:\n$$x_t = \\alpha_t \\cdot x_0 + \\sigma_t \\cdot z, z \\sim N(0, I),$$\nwhere $\\alpha_t$ is a constant derived from t that denotes the magnitude of xt, and ot is a constant derived from t that determines the magnitude of noise z added to the image. The noise z is sampled from a standard normal distribution. The training objective of diffusion models is, therefore, to minimize the difference between the initial input x\u00ba and the denoised output obtained from \u00e6t after processing it through the model multiple times.\n$$\\min_\\theta ||\\hat{x}_{t-1} - x^0||^2,$$\n$$x_{t-1} = x^t + \\epsilon \\cdot f_\\theta(x_t, t) + \\epsilon \\cdot z,$$\nwhere \\hat{x} starts from xt. Here, e is a constant derived from \u03c3, and fe represents the diffusion model, which takes the input \u00eet and the current denoising time step t.\nStable Diffusion incorporates information extracted from text into the denoising process. Specifically, fe takes an additional feature vector representing the text features, allowing it to generate images that align with the descriptions provided in the input text.\nHowever, training these models from scratch requires substantial computational resources. Consequently, most research focuses on fine-tuning pre-trained models [16, 35, 48], enabling them to adapt to specific tasks or datasets with a fraction of the computational power otherwise required."}, {"title": "2.2. Detecting Unauthorized Data Usage", "content": "Numerous research efforts have focused on detecting the use of unauthorized data in training and on mitigating models' tendencies to memorize sensitive or copyrighted information. Membership inference attacks [13, 19] are a feasible method to potentially determine if models have memorized data from the training set. Other defense mechanisms [28, 47, 52] introduced perturbations to images so that the model would be unable to learn specific features or styles from the image, thus preventing it from reproducing the image later. Similarly, backdoor watermarking techniques perturb the input training set with a watermark, such as injected steganography-based approaches that can embed binary strings into the image [32, 57], or backdoor image-transformation-based methods [53], so the model can learn and produce the watermark in its generated images. DIAGNOSIS [53] is a state-of-the-art watermark-based protection against unauthorized data usage in text-to-image diffusion models. It considers two watermark scenarios: the unconditional watermark, where the watermark is always present regardless of the text prompt, and the trigger-conditioned watermark, which appears only when a specific trigger sequence is included in the prompt."}, {"title": "3. Threat Model", "content": "In this work, we focus on evaluating the performance of watermark-based protections against unauthorized data usage in text-to-image diffusion models. This scenario involves two parties: the data owner or a third-party data protector, who acts as the defender, and the unauthorized model developer, who is the adversary.\nData Owner or Third-Party Data Protector (Defender). The defender's goal is to prevent unauthorized usage or abuse of specific images that are either intellectual property or private. To achieve this, the data owner or a third-party data protector can intentionally embed imperceptible watermarks in these images, which serve as a secret key known only to the defender. Once text-to-image diffusion models are trained on such watermarked data, they will generate images containing the watermark. Thus, the defender's goal is to detect whether a given model has been trained on unauthorized data by inspecting its generated images. The data owner or protector has full access to the protected data but not to other training data used by the diffusion model. They do not have access to the model itself or the training process and can only query the trained model to obtain generated images. This setup simulates scenarios involving model API providers, such as Midjourney [2].\nUnauthorized Model Developer (Adversary). The adversary, or unauthorized model developer, aims to use protected images to train their text-to-image diffusion model so that it can generate similar images or images with specific features. However, the adversary wants to avoid the detection of unauthorized data usage in their trained model. To achieve this, they may first clean the training data, such as applying image transformations, before using it to train the text-to-image model. They might also inspect or modify the model post-training to remove any embedded watermarks. The adversary does not know whether or which images contain watermarks, nor do they have access to the watermark detector developed by the defender to determine if the model has been trained on unauthorized data."}, {"title": "4. Method", "content": "The main characteristic of watermark-based protections is that the added coatings or perturbations are visually imperceptible. This ensures that the watermark, or \"secret key,\u201d is known only to the data owner and remains hidden from data consumers, such as text-to-image model developers. Given this, our goal is to evaluate the robustness of these watermarks - for example, to determine if they can be removed or degraded by various image transformations."}, {"title": "4.1. Image Transformation", "content": "Since watermark-based protections add small perturbations to protected images, a straightforward approach to evaluate their robustness is to apply various image transformations to these images. We leverage three commonly used transformation methods: Gaussian blur, JPEG compression, and color jittering. Image transformations fail to remove the watermark effect because they cannot significantly alter the input image"}, {"title": "4.2. Our Solution", "content": "The primary function of text-to-image models is to generate images that align with input text descriptions. When unauthorized data is used in training, the model's capability extends to generating images with features similar to those in the protected images. Existing watermark-based protections assume that for a model to generate similar images, it must be trained on the exact content of the protected images. This process would lead to the watermark being embedded into the model. However, this assumption does not always hold. As long as the model learns the key features of the protected data, it can generate similar images without directly replicating the original content (e.g., the watermark).\nWe propose to leverage the generative capabilities of diffusion models to construct data samples that share key features with protected images. Instead of directly using the protected images in training, we employ a technique similar to zero-shot learning. Here, a diffusion model is prompted to generate an image based on a text description along with a reference (protected) image. In this way, the diffusion model has the freedom to create images without focusing on details such as the watermark embedded in the protected images.\nFigure 1 presents an overview of our method, RATTAN, for bypassing watermark-based protections. The top part of the figure illustrates how existing watermark-based protection methods operate, which involve two steps. The first step is embedding a watermark onto protected images. This watermark can be either a sequence of pixel value bits or an image transformation function. The second step involves inspecting the generated images from text-to-image diffusion models. If the model has been trained on watermarked data, the generated images will also contain this watermark. Consequently, existing protections can flag the trained model.\nThe bottom part of Figure 1 shows the pipeline of our technique, RATTAN. The adversary's goal is to train a text-to-image model on unauthorized data. Without knowing whether these protected images are watermarked, RATTAN selects a subset of the data (e.g., 10 images) and their corresponding text descriptions. It then employs an off-the-shelf Stable Diffusion [43] model to perform controlled image generation based on both the input image and its corresponding text. The generated images are intended to retain the high-level key features of the protected data but be free of watermarks. RATTAN then uses this small set of images to fine-tune the potentially watermarked text-to-image model. Below, we detail the two main components of RATTAN: controlled image generation and watermark removal.\nControlled Image Generation. As discussed above, our goal is to obtain images that preserve key features of protected data without copying the fine-grained details used by watermarks. Our idea is to leverage an off-the-shelf diffusion model to perform controlled image generation. Specifically, the model has a certain freedom to create an image based on the given text and the protected input.\nFor diffusion models, the generation process typically starts from random Gaussian noise, as described in Equation 3, and then iteratively removes the noise by passing it through the model. In our scenario, we aim to generate an image that retains the major features of the protected input, such as structures and outlines. To achieve this, similar to existing work [34], rather than starting the generation process from random Gaussian noise, RATTAN uses a starting point obtained from the protected image.\nSpecifically, given the projected image Xprotected, we first apply Equation 1 to it, which essentially adds random noise to the image.\n$$x_{guide}^t = \\alpha_t \\cdot x_{protected}^0 + \\sigma_t \\cdot z, z\\sim N(0, I),$$\n$$x_{guide}^{t-1} =  \\alpha_t.x_{guide}^t$$\nIn the standard diffusion process, the output from the above equations after t iterations becomes Gaussian noise, and the diffusion model aims to recover the original input. This is why, after training, the diffusion model can generate images from random noise. Here, our goal is to preserve the key high-level features of the protected input, such as structures, outlines, color schemes, etc., so that the diffusion model can recover these coarse features while filling in fine-grained details. Thus, we do not add noise to Xprotected until it becomes Gaussian noise but rather stop at a certain step. Suppose the total number of iterations needed to transform an image to random noise is t; our diffusion process (Equation 4) only applies for yt iterations. Empirically, we choose y = 0.6 as it provides the best trade-off between the quality of generated images and the evasion rate. The results of using different y values are discussed in Section 5.5.\nAfter obtaining the diffused Xprotected, i.e., Xguide, we pass it through a standard diffusion model to generate a new image, as illustrated in Equation 3. Note that we use an off-the-shelf diffusion model (not the model trained on the protected data) in RATTAN, with its weight parameters frozen.\nIn addition to the original protected image, we also include its paired text description as a reference. This is because the final text-to-image model is trained on both the image and the text, with the text providing guidance on which parts the model should focus on during training. Thus, we leverage the text in our controlled image generation as well. This approach follows the standard Stable Diffusion [43] inference process, where the text embedding from a text encoder is incorporated into the cross-attention layers during the denoising process. More details can be found in the original paper [43].\nThe last column (f) in Figure 2 shows the result after applying RATTAN to the watermarked input (b). Observe that the generated image has smooth boundary lines, effectively removing the watermark effect present in (b). Additionally, the Pok\u00e9mon's teeth are no longer visible, and the color tone differs from the original image. This is due to the controlled generation process, which preserves high-level features while disregarding low-level details.\nWatermark Removal. Since our generated images from protected inputs do not contain watermarks, a straightforward approach is to apply controlled image generation to all the training data. However, there are two issues with this method. First, the training set could be extensively large, and applying the diffusion model generation to all samples can be computationally expensive and substantially increase costs. Second, as discussed earlier, the controlled generation retains the key high-level features of protected images while ignoring low-level details. Although this helps to eliminate watermarks, it also removes fine-grained features necessary for training text-to-image models.\nThe watermarked model has already been trained on protected images with fine-grained details, including the watermark. We only need to remove the watermark without affecting the fine-grained content features. To achieve this, we propose to fine-tune the watermarked model on a small set of our generated images. Note that the text-to-image model is trained on text-image pairs. The original watermarked model has learned the correspondence between the text and the watermarked image. We use the same text but pair it with our generated image a slightly different version of the protected image. This guides the model to ignore the watermark and focus on the main content features, both coarse-grained and fine-grained. Our evaluation in Section 5.2 shows that with as few as 10 images, RATTAN can effectively eliminate the watermark effect."}, {"title": "5. Evaluation", "content": "This section discusses the evaluation on RATTAN and a few image transformations, as well as ablations studies to understand the impact of different components in RATTAN."}, {"title": "5.1. Experimental Setup", "content": "Datasets. We utilize three popular datasets: Pokemon [40] (833 text-image pairs), Naruto [7] (1221 text-image pairs), and CelebA [29]. For the CelebA dataset, we use the first 1000 text-image pairs to be consistent with the experimental setting in the DIAGNOSIS paper.\nWatermarking Methods. We use three watermark-based protection methods for evaluation, namely the works by Luo et al. [32], Yu et al. [57], and DIAGNOSIS [53]. Luo et al. and Yu et al. employ a bit string as the watermark and DIAGNOSIS uses a warping function. DIAGNOSIS supports both unconditional and trigger-conditioned watermarks. In the unconditional setting, the watermark is activated for any text prompt. In the trigger-conditioned setting, the watermark appears only when the prompt contains a specific text trigger or a predefined token sequence at the beginning.\nModels and Fine-tuning. We primarily use Stable Diffusion v1.4 [43] along with the Low-Rank Adaptation of Large Langauge Models (LoRA) fine-tuning method [21] for our experiments. The evaluation on other diffusion mod-"}, {"title": "5.2. Evading Watermark-based Protections", "content": "In this section, we evaluate the performance of RATTAN on the works by Luo et al. [32], Yu et al. [57], and DIAGNOSIS [53]. For each watermarking method, we trained 10 models: 5 models with the watermark in the training set and 5 benign models where the training set is unmodified. As shown in the table, the watermarks implemented by Luo et al. and Yu et al. are not effectively memorized by the diffusion model, making them unreliable for detecting malicious behavior. Luo et al. and Yu et al.'s methods implement fingerprinting of images using binary strings, with detection conducted by measuring the bit accuracy of generated samples. The average bit accuracy for raw images (images with the watermark directly added) produced by Luo et al. was approximately 68.11%, while for Yu et al., it was around 46.93%. This demonstrates the limited effectiveness of these methods in embedding watermarks. In contrast, DIAGNOSIS performs significantly better, with a 100% detection accuracy. However, for all three watermarking methods, RATTAN successfully reduces their detection accuracy to 50%, equivalent to random guessing.\nResults on DIAGNOSIS. As observed above, since the other two watermarking methods result in poor memorization of watermarks in diffusion models, we mainly focus on DIAGNOSIS in the following evaluation.\nFor our experiments, we adopt a similar setting to the one used in the DIAGNOSIS paper. We use 50 different text prompts to generate images from the fine-tuned models and report the FID scores and memorization strengths. We train 20 models for each case: 10 models using unauthorized data and 10 models without unauthorized data. Our goal is to shift True Positives towards False Negatives, achieving an overall accuracy of 50%, equivalent to random guessing."}, {"title": "5.3. Performance of Image Transformations", "content": "As discussed in Section 4.1, one straightforward idea to remove watermarks is to apply image transformations. We have shown earlier that Gaussian blur, JPEG compression, and color jittering cannot remove the watermark embedded by DIAGNOSIS. Here, we evaluate six more image transformations, including saturation increase, using 8-bit quantization, adding a green hue, increasing the contrast, cropping by a factor of 1.5 on each side, and increasing the brightness. The results are shown in Table 3.\nFrom the table, we observe that DIAGNOSIS is highly effective against most image transformations. However, its performance declines when contrast is increased in the training set. The strong performance of DIAGNOSIS against popular image transformation-based mitigation strategies underscores the need for RATTAN to provide a more robust evaluation."}, {"title": "5.4. Visualization of RATTAN Generated Images", "content": "Figure 2 illustrates the image generated by RATTAN based on the protected input. The bottom row provides a zoomed-in view of a specific region of the image. The first column shows the original image before undergoing DIAGNOSIS's watermarking process. In the second column, DIAGNOSIS applies a watermark, noticeable as slight wobbliness along the edges. This edge perturbation introduces a feature that can be subtly learned by the diffusion model while remain-"}, {"title": "5.5. Ablation Study", "content": "In this section, we present the results of our ablation study on the performance of RATTAN. First, we investigate how fine-tuning with different numbers of cleaned samples affects the efficacy of our method. Next, we examine the impact of varying the number of fine-tuning epochs. Finally, we evaluate the effectiveness of our approach across various diffusion models. Additionally, we consider training a model from scratch using RATTAN-generated images. The results of each ablation study on the Pokemon dataset are shown in Table 3. Since the protection method does not misclassify benign models (those not trained on unauthorized data) as malicious, we only report the detection rate for watermarked models after applying RATTAN in Table 3.\nImpact of RATTAN-Generated Samples. We first examine the effect of fine-tuning on different numbers of RATTAN-generated images, ranging from as few as 5 samples to the entire training set of 783 samples. The results in Table 3 indicate that fine-tuning with a smaller subset of cleaned samples results in relatively lower FID scores. This occurs because RATTAN-generated images primarily preserve high-level features while disregarding low-level details. Fine-tuning on a larger amount of such data may impact the model's ability to generate high-quality images. On the other hand, using the full dataset results in a few mali-cious detections, suggesting that training on a larger number of samples might increase the risk of overfitting to the watermarked features.\nImpact of Fine-tuning Epochs. In this experiment, we examine the impact of varying the fine-tuning epochs in the RATTAN pipeline. By testing a range of epochs from 5 to 100, we evaluate the trade-off between image quality, as measured by the FID score, and the degree of memorization. The results in Table 3 indicate that the number of fine-tuning epochs has a relatively minor impact on the FID score. However, memorization strength decreases signifi-cantly as the number of epochs increases, dropping from 0.336 at 5 epochs to 0.168 at 100 epochs. This decline suggests that extended fine-tuning progressively reduces the retention of watermarked content, resulting in more effective watermark removal. However, it is worth noting that longer fine-tuning periods are computationally expensive. To achieve a balance between watermark removal efficacy and computational efficiency, RATTAN employs 30 epochs for fine-tuning.\nImpact of y. The parameter y in the image generation pipeline controls the degree of transformation applied to the input image. Larger y values place greater emphasis on the textual prompt, while smaller values result in outputs that closely align with the initial protected image. For larger y values, the diffusion model introduces more noise to the input, allowing for greater divergence from the original protected image. As shown in Table 3, smaller y values (e.g., 0.2) result in a high false negative rate, likely because the original image features are preserved sufficiently to retain the watermark. Conversely, the largest y value tested (1.0) also shows an increased malicious detection rate. This is because the generated images diverge significantly from the original inputs, failing to preserve essential content. Such samples, when paired with the text, do not effectively guide the T2I models to disregard the learned watermark.\nImpact of Diffusion Models on Controlled Image Generation. We evaluated the effectiveness of various Stable Diffusion models within the RATTAN pipeline during the controlled generation process. Each model processed watermarked images to minimize the visual artifacts introduced by the watermark. The results are summarized in Table 3 and visualized in Figure 3.\nOur experiments show that Stable Diffusion (SD) v1.4 achieves the lowest average FID scores, indicating the closest alignment with the original data distribution, followed by SD v2.0 and SD v3 Medium. Across all models, the detection rate is 0%, demonstrating that no detectable watermarks remain after fine-tuning on the cleaned images, regardless of the model version.\nFigure 3 visualizes the controlled generation results across different models. Although the differences are subtle, SD v1.4 produces images that appear slightly closer to the original inputs. This ablation study suggests that SD v1.4 performs marginally better in both FID score and memorization reduction, making it a better choice for RATTAN when high fidelity to the original data is desired.\nTraining with RATTAN-Generated Images from Scratch. All the above experiments involve fine-tuning the watermarked model on RATTAN-generated images. Since RATTAN-generated images are already free of the watermark effect, an alternative approach is to directly train a model from scratch using all the generated images. However, the results show that the model trained in this manner has a high FID score of 269.64 and a detection rate of 10%. As discussed in Section 4.2, the controlled generation retains the key high-level features of protected images while ignoring low-level details. Although this helps to eliminate watermarks, it also removes fine-grained features necessary for training text-to-image models. Therefore, the trained model has a much higher FID score compared to fine-tuning on a small subset."}, {"title": "6. Conclusion", "content": "This paper investigates watermark-based protections against unauthorized data usage in text-to-image diffusion models. We find that common image transformations are largely ineffective at nullifying watermark effects. To address this, we propose RATTAN, a framework that effectively erases a model's memorization of watermarks, rendering watermark-based protections non-robust. Our approach requires as few as 10 images to successfully remove watermarks across various datasets and protection methods. This work highlights the limitations of existing watermark-based techniques for intellectual property protection and underscores the need to develop more robust defense strategies.\nLimitations. While RATTAN offers an intuitive approach to evaluate the robustness of watermark-based protections in diffusion models, there are certain limitations that need further investigation. First, the controlled generation process may not fully recover all original image features, which could lead to a slight degradation in image quality compared to the unaltered training data. Our results indicate that in most cases, RATTAN-trained models achieve FID scores comparable to those of the original models. Second, this paper primarily focuses on the issue of unauthorized data usage in text-to-image diffusion models. Similar challenges exist in large language models (LLMs), but it is unclear whether RATTAN would be effective in this con-text. Since the design of RATTAN is general and relies only on an off-the-shelf model for controlled generation, it could potentially be adapted for LLMs. We leverage experimental exploration to future work."}, {"title": "7. Dataset Details", "content": "In this section, we provide more information about the datasets utilized in this work.\n\u2022 Pok\u00e9mon [39]: This dataset consists of 833 text-image pairs. The captions for the images were generated using the BLIP model.\n\u2022 Naruto [7]: This dataset contains 1,121 text-image pairs. Similar to the Pok\u00e9mon dataset, the captions were generated using the BLIP model.\n\u2022 CelebA [29]: This dataset includes images of celebrities' faces paired with captions generated by the LLAVA model. While the full dataset contains 36,646 text-image pairs, we selected 1,000 pairs to ensure consistency with the experimental setup in DIAGNOSIS [53]."}, {"title": "8. Evaluation on Different Models", "content": "The experiments in Section 5 of the main text are conducted on Stable Diffusion v1.4. In this section, we evaluate the efficacy of RATTAN against other popular models, including Stable Diffusion v2.0 and Stable Diffusion v2.1.\nThe results, reported in Table 4, demonstrate that DIAGNOSIS successfully embeds watermarks across all models with a detection rate over 95%. However, it is not resilient to RATTAN, which effectively removes the watermark from every model. RATTAN achieves a 100% evasion of detection on watermarked models by DIAGNOSIS, converting all true positives into false negatives while leaving benign models unaffected.\nThe results demonstrate that RATTAN's performance is consistent across various text-to-image models, showing no degradation in its ability to mitigate watermarks, regardless of the model architecture or version differences. RATTAN not only removes watermarks but also ensures that the benign attributes of the models remain unaffected."}, {"title": "9. Visualizations", "content": "In this section, we present visualizations of images generated during the controlled generation process of RATTAN, along with visualizations of images produced by the trained text-to-image models."}, {"title": "9.1. Controlled Generation Diffusion Process", "content": "RATTAN utilizes the diffusion process to generate a new image based on the original protected image and its corresponding text. This process involves several steps to progressively denoise the added Gaussian noise. We use 60 steps as the default setting and show the intermediate images produced during this process. The results are illustrated in Figure 4 and Figure 5, with y = 0.6 and y = 1.0, respectively.\nWith y = 0.6, noise is not added to the original protected image until it fully becomes Gaussian noise; instead, the process is stopped at 60% of the noise-adding stage, as discussed in Section 4.2. From Figure 4(b), it can be observed that the image retains the high-level key features of the protected input shown in (a). The artifacts from the watermark are largely removed by the introduced noise. The subsequent denoising steps gradually refine the image's details, culminating in the final output in (f). The final image retains all the key features of the watermarked input in (a) but is free from the watermark.\nFigure 5 illustrates the intermediate results with a higher y value. The stronger noise significantly obscures the high-level features, as seen in (b). As a result, the subsequent denoising process struggles to retain these features, instead generating an image primarily based on the model's inherent generation capabilities. In the final output, shown in (f), the features are very different from those in (a), and the generated image no longer resembles the original input. Therefore, a smaller y is preferred in RATTAN to preserve the main features better."}, {"title": "9.2. Effect of y", "content": "In this section, we visually examine the controlled generation results using various y values tested in the ablation study presented in the main paper. Figure 6 displays the images generated with different parameters, demonstrating the influence of y on output quality and the effectiveness of watermark removal.\nAs expected, increasing the y value introduces more noise into the initial input image, leading to a more significant divergence from the original input. This effect is particularly evident in our results, especially in (e) and (f), where the images exhibit significant degradation and divergence from the original watermarked input image (a). Consequently, models trained with these settings tend to have a higher FID score (indicating lower generated image quality), as reported in Table 3 in the main text.\nOn the other hand, a smaller y value ensures that the controlled generation closely follows the original input, thereby preserving high-level features. However, this also means that watermark artifacts are retained, as shown in (b) and (c). Models trained on these generated images can still be"}, {"title": "9.3. Generated Images", "content": "In this section, we present examples of images generated under three scenarios: a benign model fine-tuned on the Pok\u00e9mon dataset without any watermarks, a watermarked model produced by DIAGNOSIS, and a watermark-removed model by RATTAN.\nThe results are presented in Figure 7. The first row displays images generated by a benign model. The second and third rows show images generated by DIAGNOSIS-watermarked models using an unconditional watermark and a trigger-conditioned watermark, respectively. The final two rows depict images generated by watermark-cleaned models from RATTAN. The results show minimal visual quality loss in the images between DIAGNOSIS and RATTAN. Most images successfully reproduce similar subjects, retaining key attributes such as colors, creature types, and positioning within the image. This highlights that RATTAN effectively removes watermarks without compromising the generative performance of cleaned models. These findings indicate that RATTAN effectively targets and removes watermark-related artifacts while preserving the underlying image distribution. This ensures that the model's utility remains intact for the adversary, enabling it to generate content in the style of copyrighted material without facing the associated consequences. This underscores the critical need for developing more robust and effective methods to protect intellectual property and private data."}]}