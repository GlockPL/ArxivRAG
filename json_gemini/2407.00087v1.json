{"title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "authors": ["Ju-Seung Byun", "Jiyun Chun", "Jihyung Kil", "Andrew Perrault"], "abstract": "Large Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs by aligning them with specific preferences. These methods primarily use ranking-based feedback for entire generations. With advanced AI models (Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of detailed feedback that are expensive for humans to provide. We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT). First, we request the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT). This sentence-level feedback allows us to consider individual valuable segments, providing more granular rewards for the RL procedure. Second, we ask the Teacher to correct the wrong reasoning after the RL stage. The RL procedure requires massive efforts for hyperparameter tuning and often generates errors like repetitive words and incomplete sentences. With the correction feedback, we stabilize the RL fine-tuned model through SFT. We conduct experiments on multi-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of our proposal. ARES rationale reasoning achieves around 70% win rate against baseline models judged by GPT-4o. Additionally, we observe that the improved rationale reasoning leads to a 2.5% increase in inference answer accuracy on average for the multi-modal datasets.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) and Large Multimodal Models demonstrate remarkable performance across diverse language and multi-modal tasks (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; Zhang et al., 2022a; Liu et al., 2023a; Goyal et al., 2023). However, these Large Models (LMs) often generate toxic and biased content (Gehman et al., 2020; Tamkin et al., 2021) because LMs are primarily trained to predict the next token based on extensive corpus datasets. To align LM behavior more closely with user preferences, previous works (Glaese et al., 2022; Ouyang et al., 2022) fine-tune their models using Reinforcement Learning from Human Feedback (RLHF). Furthermore, with advancements in LMs, an advanced LM feedback start replacing costly human feedback, yielding Reinforcement Learning from AI Feedback (RLAIF) (Lee et al., 2023; Bai et al., 2022; Yuan et al., 2024).\nHowever, RLHF and RLAIF encounter two significant challenges. First, both methods often utilize ranking-based feedback (Ouyang et al., 2022), which orders the preferences of generated samples. For instance, if sample A is preferred over sample B, the model is fine-tuned to generate more outputs like A and fewer like B. However, if B contains certain valuable parts, these parts are often disregarded. To alleviate this issue, Lightman et al. (2023); Luo et al. (2024) propose sentence-level feedback, applying it solely to the reward model without Reinforcement Learning (RL), called the Process-supervised Reward Model (PRM). It demonstrates its potential through search algorithms on the PRM, such as best-of-N or Monte Carlo Tree Search (MCTS). Furthermore, Wang et al. (2024) demonstrate the effectiveness of RL with sentence-level feedback by heuristically scoring each sentence in math problems, where evaluating the predicted answer is straightforward. Thus, sentence-level feedback exhibits significant promise compared to existing ranking-based feedback. Nonetheless, acquiring sentence-level feedback is more costly than ranking-based feedback.\nSecond, the RL process is inherently unstable and requires extensive hyperparameter tuning (Eimer et al., 2023). This instability often results in the generation of repetitive words and truncated sentences. Hyperparameter tuning becomes an enormous burden as the model size increases, making exhaustive tuning seemingly impossible, especially for individuals. The existing RLHF method (Ouyang et al., 2022) recycles the dataset used in pretraining within the loss function with Proximal Policy Optimization (PPO) (Schulman et al., 2017) to mitigate this degeneration problem. However, this approach prevents the model from fully maximizing the sum of rewards through RL and may limit the opportunity for diverse improvements, which differ from the pretraining distribution.\nIn this work, we aim to address the two challenges mentioned above through various types of feedback using an advanced AI model as a Teacher. Many advanced AI models, including GPT-4 and Claude 3 Opus, are already used as evaluators for many tasks and generate reliable human-level answers (Liu et al., 2023b; Sottana et al., 2023). 1) We request a score from Teacher for each sentence, ranging from 0.0 to 1.0. Each score indicates how much a sentence contributes to solving the problem. This provides detailed reward feedback to the training model and can be applied to both mathematical and more general multi-modal Chain-of-Thought (CoT) (or rationale reasoning) problems. 2) We ask the Teacher to identify and correct minor errors in the RL results, such as incorrect or cut-off parts. With this corrected dataset, we fine-tune the model using Supervised Fine-Tuning (SFT). This stage allows the model to maximize the rewards while properly deviating from the pretraining distribution. In summary, We propose a hybrid algorithm ARES that Alternates REinforcement Learning and Supervised Fine-Tuning.\nTo evaluate how much rationale reasoning can be improved through the ARES framework, we use the ScienceQA and A-OKVQA datasets, which are large-scale, multi-modal datasets that include rationale reasoning data. We use Multimodal-CoT (MM-CoT) (Zhang et al., 2023b) as the baseline. MM-CoT employs two separate models: one model is responsible for generating rationale, and the other model, an inference model, processes the concatenated input (problem and generated rationale). This distinct framework enhances performance, even with relatively smaller models like Flan-Alpacabase (Chia et al., 2023) (251M) and Flan-AlpacaLarge (790M) with ViT feature (Dosovitskiy et al., 2021). We perform ARES on the rationale reasoning model of MM-CoT. We compare ARES rationale reasoning with that of MM-CoT through GPT-4o, determining which rationale is better and computing the win rate. Additionally, we check whether the improved rationale reasoning leads to better answer accuracy. Our results show that our rationale reasoning outperforms the baselines with around 70% win rate and demonstrates 2.5% increase in inference answer accuracy on average for the different model sizes and the multi-modal tasks.\nIn summary, our key contributions are:\n\u2022 We propose ARES, leveraging diverse types of feedback from advanced AI (Teacher) to enhance CoT reasoning for multi-modal tasks.\n\u2022 ARES stabilizes an RL fine-tuned model through SFT and properly reflects the direction RL wants to change without massive hyperparameter tuning.\n\u2022 ARES generates better rationale chains compared to baselines judged by GPT-4o and improves inference answer accuracy for multi-modal tasks."}, {"title": "2 Methodology", "content": "This section briefly introduces the preliminaries in Section 2.1 and present our two-stage hybrid algorithm ARES that Alternates REinforcement Learning and Supervised Fine-Tuning. 1) We request a score for each sentence in the Chain-of-Thought (CoT) from the advanced AI model (Teacher) to determine how much it contributes to solving the problem (Section 2.2). We perform Reinforcement Learning (RL) with the score feedback on our training model. 2) Teacher corrects minor errors such as truncated or slightly incorrect sentences, thereby performing Supervised Fine-Tuning (SFT) (Section 2.2).\n2.1 Preliminaries\nFor an input $x \\in X$, a transformer based (Vaswani et al., 2023) model $\\pi_{\\theta}(\\cdot|x)$ parameterized by $\\theta$ generates the output $y$ composed of sentences {$s_0, s_1, ..., s_k$}.\n$$\n\\pi_{\\theta}(Y | x) = \\prod_{t=0}^k \\pi_{\\theta}(y_t | x, y_{<t}),\n$$\nwhere $y_{<t}$ indicates previous tokens. To proceed with RL finetuning, Ouyang et al. (2022) train an outcome-supervised reward model (ORM) using ranking-based feedback. With more fine-grained feedback like sentence-level, Lightman et al. (2023); Wang et al. (2024) train a process-supervised reward model (PRM). Instead of training a reward model, we request score feedback $r(x \\cup s_{<t}, s_t)$ for each sentence from an advanced AI such as GPT-4 where $s_{<t}$ is previous sentences.\n2.2 Reinforcement Learning\nReinforcement Learning (RL) fine-tunes our model $\\pi_{\\theta}$ to maximize sum of sentence rewards from an advanced AI such as GPT-4 and Claude 3 Opus. The RL objective is as follows:\n$$\n\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} E_{x\\sim X} [\\sum_{s_t\\sim \\pi_{\\theta}(\\cdot|x, s_{<t}) \\newline i=0}^k \\gamma r(x \\cup s_{<t}, s_t)]\n$$\nwhere $\\gamma \\le 1.0$ is a discount factor. We use Proximal Policy Optimization (PPO) (Schulman et al., 2017) to achieve this RL objective, treating sentences as actions (Equation 3).\n$$\n\\underset{\\theta}{\\operatorname{maximize}} E_t [\\frac{\\pi_{\\theta}(s_t|x \\cup S_{<t})}{\\pi_{\\theta old}(s_t|x \\cup S_{<t})} A]\ns.t. \\newline E_t [KL(\\pi_{\\theta}(\\cdot|x \\cup S_{<t}), \\pi_{\\theta old}(\\cdot|x \\cup S_{<t}))] \\le \\delta\n$$\nwhere $\\pi_{\\theta old}$ is the original policy (baseline model) and $A_t$ is an advantage estimator at timestep t. PPO is commonly leveraged in Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and AI Feedback (RLAIF) (Bai et al., 2022). PPO's conservative update prevents the training model from deviating too far from the original model, thus avoiding degeneration.\nSentence-Level Nuanced Feedback: We request a score between 0.0 and 1.0 for each sentence in CoT through the advanced AI for RL. The closer the score is to 1.0, the more relevant and helpful it is to solving the problem. We additionally shift the reward distribution by -0.5 to center it at 0 (Zheng et al., 2023). Therefore, the actual range is from -0.5 to 0.5. Using these nuanced scores, the RL fine-tuned model exhibits emergent behaviors (please refer to Section 4). This allows us to understand the direction in which the model wants to change through RL.\nAdvantages of Using Advanced AI for Score Feedback: Although calling the API has disadvantages, such as incurring costs or facing usage limits, there exist several advantages of using the advanced AI for feedback. First, there is no need to train a reward model. Second, as the RL fine-tuned model begins to generate out-of-distribution outputs that differ from the data used to train the reward model, it becomes challenging for the trained reward model to provide accurate rewards. However, this out-of-distribution problem is effectively addressed with the advanced AI.\nRL Challenge: One of the challenging factors for RL is hyperparameter tuning (Eimer et al., 2023). This often results in generating repetitive words and truncated sentences (Ouyang et al., 2022). Additionally, as the model size increases, finding working hyperparameters becomes infeasible for individuals. To alleviate this issue, we utilize correction feedback from the advanced AI as the second stage (Section 2.3), and proceed with the supervised fine-tuning to stabilize the RL fine-tuned model.\n2.3 Correction: Supervised Fine-Tuning\nThe RL fine-tuning procedure makes model changes to maximize the reward sum, such as correcting mistakes or explaining why other options cannot be the answer. However, without highly tuned hyperparameters (Eimer et al., 2023), the model after the RL phase may often result in errors such as repeated sentences, truncated sentences, or incorrect content for some data points. (See examples in Appendix D.)\nCorrection Feedback: Given the success of LLMS"}, {"title": "2.4 Algorithm Detail", "content": "We propose a hybrid algorithm Alternating between REinforcement learning and Supervised fine-tuning (ARES).  First, we prepare a model with a given training dataset and generate rationale reasoning composed of several sentences for input. For the RL procedure to align the training model with a preference, we request scores for each sentence. The RL result may include some incorrect parts , but it aims to maximize the rewards provided. Next, we request correction feedback and create a corrected dataset ."}, {"title": "3 Experimental Setup", "content": "Data: We first evaluate our proposed method on the ScienceQA (Lu et al., 2022a) dataset, a large-scale, multi-modal science dataset designed to assess multi-hop reasoning abilities. We choose ScienceQA because it contains reasoning chains to derive the answer. Each problem consists of a question, multiple options, multi-modal contexts, a correct answer, and an annotated lecture or solution chain (note that around 9.5% lack the solution chain). In addition, we conduct experiments on A-OKVQA (Schwenk et al., 2022), a knowledge-based multi-modal benchmark with a diverse set of challenging questions paired with rationales, demanding non-trivial commonsense knowledge (see Appendix B).\nBaselines: We mainly compare our method with Multimodal-CoT (MM-CoT) (Zhang et al., 2023b) as the baseline because it utilizes reasoning chains to solve multi-modal tasks. MM-CoT leverages two distinct models: the first generates rationale for a given problem, and the second, an inference model, takes the concatenated input (problem and generated rationale). This separated framework shows improved performance, even for relatively small models such as Flan-Alpacabase (Chia et al., 2023) (251M) and Flan-AlpacaLarge (790M). We use the rationale model provided by MM-COT for ScienceQA and retrain the rationale model ourselves for A-OKVQA because there is no provided model.\nPrompts for Feedback: Since our proposed ARES requests different types of feedback for each stage, a corresponding prompt exists separately. We use Claude 3 Haiku for all training to get feedback because it is about 20 times cheaper than the top competing models, yet still demonstrates decent performance. We first request scores ranging from 0.0 to 1.0 for each sentence in CoT to proceed with the RL stage. To obtain reasonable scores, we let Haiku consider the starting point of thought, the process of elimination, or true statements. \nIn order to collect the corrected dataset for the SFT stage, we let Haiku refer to the given problem and correct the answer as the prompt. We ask Haiku to maintain the format of the existing rationale chains as much as possible and correct only the parts that require correction. The RL stage often makes the training model generate repetitive sentences. This repetition is not easily removed even by GPT-4 when the repetitive sentence exists in the middle of rationale reasoning. To reduce the burden of feedback, we simply hard-code the removal of repetitive sentences before adding the generated rationale to the prompt.\nTraining Details: For the ARESBase RL stage, we use a learning rate of 2e-5 and 10 epochs for PPO with a batch size of 8 for both ScienceQA and A-OKVQA. The learning rate for ARESLarge is 2e-5 with 5 epochs for PPO and a batch size of 2 for both tasks. We proceed with 2 rounds of our pipeline for ARESBase and 2 rounds for ARESLarge for ScienceQA. For A-OKVQA, we proceed with 1 round for both model sizes. For the SFT stage for correction, we follow the hyperparameters used in MM-CoT for both model sizes. Additionally, we replace MM-CoT's inference model, which is the same size as the rationale model, with a LoRA adapter added to the rationale model. The LoRA adapter effectively refers to the rationale model's features with a small number of weights and infers answers.\nEvaluation Metrics: We use two main metrics to test how our pipeline (ARES) improves rationale reasoning quality. First, we evaluate ARES's rationale reasoning quality against baseline models since we enhance our model based on them. For two different model sizes (Flan-AlpacaBase and Flan-AlpacaLarge) and two tasks (ScienceQA and A-OKVQA), rationale reasoning quality is evaluated by GPT-4o-2024-05-13 and the win rate is calculated (Section 4.3). The GPT-4 series is actively used as an evaluation metric, replacing human judgment for various domains (Liu et al., 2023b; Sottana et al., 2023). Second, we assess how the improved rationale reasoning impacts answer accuracy . This evaluation is also performed on both model sizes and tasks. Additionally, we analyze how the RL stage fine-tunes the training model and maximizes the sum of rewards in Section 4.1."}, {"title": "4 Experimental Results", "content": "In this section, we evaluate our pipeline ARES that Alternates REinforcement Learning and Supervised Fine-Tuning by requesting diverse types of feedback for an advanced AI model (Teacher) (Claude 3 Haiku). The goal of ARES is to improve the rationale reasoning quality. We demonstrate how ARES enhances rationale reasoning in the following sections.\n4.1 Emergent Behavior Through RL\nThrough RL, a training model is aligned to a specific preference. Essentially, the model increases the probability of helpful sentences receiving good rewards and reduces the probability of incorrect or meaningless sentences. However, this process produces some interesting additional results.\nFirst, it supplements rationale reasoning for some problems where rationale reasoning is insufficient. In particular, 9.5% of problems in ScienceQA have empty rationale reasoning (solution) data. The model generates nothing before the RL stage for these problems but starts generating reasoning chains afterward . We observe this especially when utilizing PPO's advantage normalization or when the learning rate is large.\nSecond, the training model begins to explain why other options are not the answer . The process of elimination is a useful method for deriving answers when options are given.\n4.2 Guide RL with Correction\nDespite the benefits of RL, hyperparameter tuning often requires massive effort. Without meticulous tuning, the RL fine-tuned model may produce errors such as repetitive or incomplete sentences. To address these issues, we include a supervised fine-tuning (SFT) stage after RL to correct these errors. SFT is more stable than RL. We evaluate how well the SFT stage corrects errors caused by the RL stage for various RL hyperparameters. We test various RL hyperparameters such as learning rate = {5e-6, 1e-5, 2e-5, 5e-5}, batch size = {2, 4, 8, 16, 32}, and PPO epoch = {5, 10, 15}. As a result of RL, we observe that some of the sentences in rationale chains are repetitive or truncated. The SFT stage, with correction feedback, reflects what RL wants to achieve and appropriately guides it. However, excessive RL learning rates or epochs cause serious degeneration of the model, such as producing no output or generating strange words, and the results of correction feedback are also unreasonable.\n4.3 Rationale Reasoning Comparison\nWe check whether ARES improves the quality of rationale reasoning compared to the baseline model. GPT-4o evaluates which rationale chain is better between the rationale generated by ARES and the rationale generated by the baseline model. We randomly shuffle the rationale chains and provide them as Option A and Option B for a fair evaluation (Yu et al., 2023). We conduct our experiments with two different model sizes, Flan-Base and Flan-Large with ViT feature, on ScienceQA and A-OKVQA.  ARES achieves around 70% win rate against each corresponding baseline model for both datasets.\n4.4 Inference Accuracy\nWe investigate whether the improved rationale also contributes to answer inference accuracy.. We evaluate our base model against the MM-CoT baseline. ARESBase achieves a 2.79% improvement compared to the corresponding baseline (MM-CoTBase). The large model (ARESLarge) shows some minimal improvement compared to the corresponding baseline. However, it's worth noting that despite this seemingly small gain, ARESLarge beats to 13B LLaVA (Liu et al., 2023a). This minimal improvement may be due to the 9.5% of ScienceQA problems needing more rationale reasoning (around 9.5% problems have empty rationale reasoning). The RL stages can only eliminate some empty rationale reasoning, which requires numerous ARES pipeline rounds. Above all, our main"}, {"title": "5 Related Work", "content": "Chain-of-Thought (CoT) is a multi-step reasoning method for problem-solving that encourages LLMs to consider the intermediate reasoning steps. Zero-Shot-CoT (Kojima et al., 2023) promotes CoT by using prompts such as \"Let's think step by step\" for LLMs. For Few-Shot-CoT (Zhang et al., 2022b; Wei et al., 2023), a few examples with reasoning processes are provided, allowing the model to refer to these examples and understand how to perform CoT. Wei et al. (2023) reveal that this CoT technique positively impacts the performance of large models (> 100B), but has minimal effect on smaller models. MM-CoT (Zhang et al., 2023b) suggest that CoT is beneficial even for relatively small models, such as 200M, if the model that generates intermediate reasoning and the model that infers the answer are separated. We find that simply adding a LoRA adapter (Hu et al., 2021) to the"}, {"title": "6 Conclusion", "content": "We propose a hybrid algorithm, ARES, which Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT) to enhance multimodal rationale reasoning for ScienceQA and A-OKVQA. ARES leverages two types of feedback: 1) ARES requests a score from a Teacher (we used Claude 3 Haiku) for sentence-level nuanced feedback and proceeds with RL. 2) ARES requests the Teacher to correct rationale chains after RL, stabilizing the RL fine-tuned model with SFT. ARES is designed to aid the RL procedure without massive hyperparameter tuning while properly reflecting the desired changes from RL. We evaluate the improvement in rationale reasoning produced by ARES compared to baselines using GPT-4o, and assess how much the improved rationale chains enhance inference accuracy for the two multi-modal tasks. We hope our work inspires further research on utilizing various types of AI feedback.\nLimitations\nAlthough we address general multi-modal rationale models beyond mathematical problems, receiving feedback from AI models still needs to be more reliable for more complex tasks such as graduate-level math or expert-level knowledge. For instance, some A-OKVQA problems even contain challenging questions requiring external knowledge beyond the image alone. This challenge highlights the necessity for future research to develop methods that can effectively incorporate external knowledge sources into the model. Additionally, if the model is not publicly available for free, using the API incurs costs, and there are daily usage limits."}, {"title": "A Prompts", "content": "A.1 Prompt for Sentence-Level Nuanced Feedback\nThe prompt for obtaining sentence-level nuanced feedback by Claude 3 Haiku is illustrated in . Each reasoning sentence is assigned a value between 0.0 and 1.0.\n\u2022 Values close to 0.0 indicate completely incorrect rationales.\n\u2022 A value of 0.5 represents a neutral rationale, such as an initial thought process or true statements that aid in guiding guesses towards the correct answer.\n\u2022 Values close to 1.0 denote a correct or highly relevant rationale.\nThese scores enable our model to discern the direction of changes through Reinforcement Learning (RL), reflecting the extent to which a sentence aids in resolving the problem.\nA.2 Prompt for Correction Feedback\nDue to the challenges mentioned in Section 2.2, we adopt the correction feedback approach. The following are the specific instructions for obtaining correction feedback using Claude 3 Haiku. We have established the following seven rules for obtaining correction feedback using Claude 3 Haiku: The prompt is presented as a Table 7.\nA.3 Prompt for Win Rate Evaluation\nWe prompt GPT-4o (2024-05-13) to choose which generated rationale is better for solving the question because we don't have gold rationales. Given two generated rationales (e.g., MM-CoTBase and ARESBase), we ask GPT-4o: \"You are given two rationale options (A or B). Your job is to select the better rationale between A and B for solving the given problem with the given image, choices, hint, and answer. Please output only 'A' or 'B'.\" . Yu et al. (2023) find that ChatGPT is skewed towards choosing option A, so we randomly swap options A and B for each evaluation to avoid bias.\nB Difficulties with External Knowledge in A-OKVQA\nThe A-OKVQA dataset includes challenging questions paired with rationales that demand knowledge beyond the information available in the image. These questions cannot be answered simply by querying a knowledge base, as they require a deeper understanding and integration of external knowledge.\nOur model faces difficulties with problems that cannot be resolved using only the information from the image. While our approach is designed to improve rationales by addressing grammatical errors and incomplete or incorrect statements, it struggles with questions that necessitate external knowledge.\n illustrates an example where the question requires knowledge about the typical PSI range for bicycle tires. This information is not visually apparent from the image of the bicycle alone. To answer this question correctly, one needs external knowledge about standard bicycle maintenance practices and the recommended PSI ranges for different types of bicycle tires."}, {"title": "C Training Details", "content": "ScienceQA has 21K multi-modal problems, with 12K for training, 4K for validation, and 4K for testing. It also includes various difficulty levels from elementary to high school, covering domains like natural science, language science, and social science. In addition, we conduct experiments on A-OKVQA (Schwenk et al., 2022), a knowledge-based multi-modal benchmark with a diverse set of 25K questions A-OKVQA includes 25K questions (17K for training, 1K for validation, and 6K for testing).\nC.1 Reinforcement Learning\nFor the ARESBase and ARESLarge training on the ScienceQA and A-OKVQA dataset, we employ the following settings:\nCommon Settings: We use top-k sampling with $k = 50$ and and sample 4 actions. The initial coefficient for the Kullback-Leibler (KL) divergence is set to 0.0001. The range for clipping the probability ratios in PPO is 0.2. The discount factor is set to 1.0. Token length is constrained to 512. We train the model using 4 NVIDIA A100 80GB GPUs.\nARESBase Specific Settings: We use a learning rate of 2e-5 and 10 epochs for PPO with a batch size of 8. Advantage normalization is applied for ARESBase, and gradient accumulation steps are set to 8.\nARESLarge Specific Settings: The learning rate for Flan-AlpacaLarge is 2e-5 with 5 epochs for PPO and a batch size of 2 for both tasks. Advantage normalization is not used and gradient accumulation steps are set to 16.\nC.2 Supervised Fine-Tuning\nWe use a batch size of 8 and train for 20 epochs with a learning rate of 8e-5 for ARESBase, following . For ARESLarge, we use a batch size of 2 and train for 50 epochs with a learning rate of 5e-5. The output length is set to 64 tokens. Training for ARESBase utilizes 1 A100 GPU, while training for ARESLarge utilizes 4 A100 GPUs. In the MM-CoT paper , because the final_eval setting was not consistent, we retrained the base model with final_eval=true and the large model with final_eval=false for consistency.\nToken Cleanup: In order to collect the corrected dataset, we need to identify tokens representing the end of each sentence, such as periods, question"}, {"title": "C.3 LoRA Adapter Training", "content": "MM-CoT utilizes two identically sized models for reasoning and inference tasks. In our approach, we replace the inference model with a LoRA adapter , which is added to the rationale model and consists of only one-tenth of the weights.\nFor LoRA adapter training for ScienceQA and A-OKVQA, we use a LoRA rank of r = 64, a LoRA a = 128, and a LoRA dropout rate of 0.05. The learning rate is set to 8e-5 for both ARES Base and ARES Large. The batch size is 16 for ARESBase and 4 for ARES Large on the ScienceQA dataset. For the A-OKVQA dataset, the batch size is 4 for both ARESBase and ARES Large.\nD Comparison of Generated Rationales\nAs mentioned in Section 2.3 and Section 4.1, because RL increases the probability of sentences receiving positive rewards and reduces the probability of sentences receiving negative rewards, the trained model often exhibits specific phenomena. It tends to generate repetitive and incomplete sentences (Table 10 and Table 11). Before the RL steps, the model couldn't produce rationales, but after RL steps, it starts generating meaningful rationale reasoning (Table 12). Furthermore, it begins to generate reasons why other options are not the answer .As illustrated in we compare the solutions from the ScienceQA original dataset, the rationales generated by the baseline model (MM-CoTBase), the rationales from the baseline model with correction feedback applied, and the rationales generated by our model (ARESBase). The first example, \"Which property do these three objects have in common?\" illustrates that the baseline model generates incorrect rationales such as \"The lemon is not (yellow)\" and \"All three objects are rough. The property that all three objects have in common is rough.\" However, when we apply correction feedback to the rationales generated by the baseline model and compare it to our proposed method, we see that our approach generates correct rationales that include the correct answer and provide explanations on why other options are not the answer. The second example also shows that our method improves rationale reasoning."}]}