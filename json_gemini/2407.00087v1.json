{"title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "authors": ["Ju-Seung Byun", "Jiyun Chun", "Jihyung Kil", "Andrew Perrault"], "abstract": "Large Multimodal Models (LMMs) excel at comprehending human instructions and demonstrate remarkable results across a broad spectrum of tasks. Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs by aligning them with specific preferences. These methods primarily use ranking-based feedback for entire generations. With advanced AI models (Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of detailed feedback that are expensive for humans to provide. We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT). First, we request the Teacher to score how much each sentence contributes to solving the problem in a Chain-of-Thought (CoT). This sentence-level feedback allows us to consider individual valuable segments, providing more granular rewards for the RL procedure. Second, we ask the Teacher to correct the wrong reasoning after the RL stage. The RL procedure requires massive efforts for hyperparameter tuning and often generates errors like repetitive words and incomplete sentences. With the correction feedback, we stabilize the RL fine-tuned model through SFT. We conduct experiments on multi-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of our proposal. ARES rationale reasoning achieves around 70% win rate against baseline models judged by GPT-4o. Additionally, we observe that the improved rationale reasoning leads to a 2.5% increase in inference answer accuracy on average for the multi-modal datasets.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) and Large Multimodal Models demonstrate remarkable performance across diverse language and multi-modal tasks (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; Zhang et al., 2022a; Liu et al., 2023a; Goyal et al., 2023). However, these Large Models (LMs) often generate toxic and biased content (Gehman et al., 2020; Tamkin et al., 2021) because LMs are primarily trained to predict the next token based on extensive corpus datasets. To align LM behavior more closely with user preferences, previous works (Glaese et al., 2022; Ouyang et al., 2022) fine-tune their models using Reinforcement Learning from Human Feedback (RLHF). Furthermore, with advancements in LMs, an advanced LM feedback start replacing costly human feedback, yielding Reinforcement Learning from AI Feedback (RLAIF) (Lee et al., 2023; Bai et al., 2022; Yuan et al., 2024).\nHowever, RLHF and RLAIF encounter two significant challenges. First, both methods often utilize ranking-based feedback (Ouyang et al., 2022), which orders the preferences of generated samples. For instance, if sample A is preferred over sample B, the model is fine-tuned to generate more outputs like A and fewer like B. However, if B contains certain valuable parts, these parts are often disregarded. To alleviate this issue, Lightman et al. (2023); Luo et al. (2024) propose sentence-level feedback, applying it solely to the reward model without Reinforcement Learning (RL), called the Process-supervised Reward Model (PRM). It demonstrates its potential through search algorithms on the PRM, such as best-of-N or Monte Carlo Tree Search (MCTS). Furthermore, Wang et al. (2024) demonstrate the effectiveness of RL with sentence-level feedback by heuristically scoring each sentence in math problems, where evaluating the predicted answer is straightforward. Thus, sentence-level feedback exhibits significant promise compared to existing ranking-based feedback. Nonetheless, acquiring sentence-level feedback is more costly than ranking-based feedback.\nSecond, the RL process is inherently unstable and requires extensive hyperparameter tuning (Eimer et al., 2023). This instability often results in the generation of repetitive words and truncated sentences. Hyperparameter tuning becomes an enormous burden as the model size increases, making exhaustive tuning seemingly impossible, especially for individuals. The existing RLHF method (Ouyang et al., 2022) recycles the dataset used in pretraining within the loss function with Proximal Policy Optimization (PPO) (Schulman et al., 2017) to mitigate this degeneration problem. However, this approach prevents the model from fully maximizing the sum of rewards through RL and may limit the opportunity for diverse improvements, which differ from the pretraining distribution.\nIn this work, we aim to address the two challenges mentioned above through various types of feedback using an advanced AI model as a Teacher. Many advanced AI models, including GPT-4 and Claude 3 Opus, are already used as evaluators for many tasks and generate reliable human-level answers (Liu et al., 2023b; Sottana et al., 2023). 1) We request a score from Teacher for each sentence, ranging from 0.0 to 1.0. Each score indicates how much a sentence contributes to solving the problem. This provides detailed reward feedback to the training model and can be applied to both mathematical and more general multi-modal Chain-of-Thought (CoT) (or rationale reasoning) problems. 2) We ask the Teacher to identify and correct minor errors in the RL results, such as incorrect or cut-off parts. With this corrected dataset, we fine-tune the model using Supervised Fine-Tuning (SFT). This stage allows the model to maximize the rewards while properly deviating from the pretraining distribution. In summary, We propose a hybrid algorithm ARES that Alternates REinforcement Learning and Supervised Fine-Tuning.\nTo evaluate how much rationale reasoning can be improved through the ARES framework, we use the ScienceQA and A-OKVQA datasets, which are large-scale, multi-modal datasets that include rationale reasoning data. We use Multimodal-CoT (MM-CoT) (Zhang et al., 2023b) as the baseline. MM-CoT employs two separate models: one model is responsible for generating rationale, and the other model, an inference model, processes the concatenated input (problem and generated rationale). This distinct framework enhances performance, even with relatively smaller models like Flan-Alpacabase (Chia et al., 2023) (251M) and Flan-AlpacaLarge (790M) with ViT feature (Dosovitskiy et al., 2021). We perform ARES on the rationale reasoning model of MM-CoT. We compare ARES rationale reasoning with that of MM-CoT through GPT-4o, determining which rationale is better and computing the win rate. Additionally, we check whether the improved rationale reasoning leads to better answer accuracy. Our results show that our rationale reasoning outperforms the baselines with around 70% win rate and demonstrates 2.5% increase in inference answer accuracy on average for the different model sizes and the multi-modal tasks.\nIn summary, our key contributions are:\n\u2022 We propose ARES, leveraging diverse types of feedback from advanced AI (Teacher) to enhance CoT reasoning for multi-modal tasks.\n\u2022 ARES stabilizes an RL fine-tuned model through SFT and properly reflects the direction RL wants to change without massive hyperparameter tuning.\n\u2022 ARES generates better rationale chains compared to baselines judged by GPT-4o and improves inference answer accuracy for multi-modal tasks."}, {"title": "2 Methodology", "content": "This section briefly introduces the preliminaries in Section 2.1 and present our two-stage hybrid algorithm ARES that Alternates REinforcement Learning and Supervised Fine-Tuning. 1) We request a score for each sentence in the Chain-of-Thought (CoT) from the advanced AI model (Teacher) to determine how much it contributes to solving the problem (Section 2.2). We perform Reinforcement Learning (RL) with the score feedback on our training model. 2) Teacher corrects minor errors such as truncated or slightly incorrect sentences, thereby performing Supervised Fine-Tuning (SFT) (Section 2.2).\n2.1 Preliminaries\nFor an input $x \\in X$, a transformer based (Vaswani et al., 2023) model $\\pi_{\\theta}(\\cdot|x)$ parameterized by $\\theta$ generates the output $y$ composed of sentences {$s_0, s_1, ..., s_k$}.\n$\\pi_{\\theta}(Y | x) = \\prod_{t=0}^{k} \\pi_{\\theta}(y_t | x, y_{<t}),$ (1)\nwhere $y_{<t}$ indicates previous tokens. To proceed with RL finetuning, Ouyang et al. (2022) train an outcome-supervised reward model (ORM) using ranking-based feedback. With more fine-grained feedback like sentence-level, Lightman et al. (2023); Wang et al. (2024) train a process-supervised reward model (PRM). Instead of training a reward model, we request score feedback $r(x \\cup s_{<t}, s_t)$ for each sentence from an advanced AI such as GPT-4 where $s_{<t}$ is previous sentences.\n2.2 Reinforcement Learning\nReinforcement Learning (RL) fine-tunes our model $\\pi_{\\theta}$ to maximize sum of sentence rewards from an advanced AI such as GPT-4 and Claude 3 Opus. The RL objective is as follows:\n$\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} \\mathbb{E}_{x \\sim X} [\\sum_{t=0}^{k} \\gamma^t r(x \\cup s_{<t}, s_t)]$ (2)\n$\\pi_{\\theta}(s_t | x, s_{<t})$\nwhere $\\gamma \\leq 1.0$ is a discount factor. We use Proximal Policy Optimization (PPO) (Schulman et al., 2017) to achieve this RL objective, treating sentences as actions (Equation 3).\n$\\underset{\\theta}{\\operatorname{maximize}} \\mathbb{E}_t [\\frac{\\pi_{\\theta}(s_t | x \\cup s_{<t})}{\\pi_{\\theta_{old}}(s_t | x \\cup s_{<t})} A_t]$\ns.t. $\\mathbb{E}_t [KL(\\pi_{\\theta}(\\cdot|x \\cup s_{<t}), \\pi_{\\theta_{old}}(\\cdot|x \\cup s_{<t}))] \\leq \\delta$ (3)\nwhere $\\theta_{old}$ is the original policy (baseline model) and $A_t$ is an advantage estimator at timestep $t$. PPO is commonly leveraged in Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) and AI Feedback (RLAIF) (Bai et al., 2022). PPO's conservative update prevents the training model from deviating too far from the original model, thus avoiding degeneration.\nSentence-Level Nuanced Feedback: We request a score between 0.0 and 1.0 for each sentence in CoT through the advanced AI for RL. The closer the score is to 1.0, the more relevant and helpful it is to solving the problem. Table 5 presents the prompt format. We additionally shift the reward distribution by -0.5 to center it at 0 (Zheng et al., 2023). Therefore, the actual range is from -0.5 to 0.5. Using these nuanced scores, the RL fine-tuned model exhibits emergent behaviors (please refer to Section 4). This allows us to understand the direction in which the model wants to change through RL.\nAdvantages of Using Advanced AI for Score Feedback: Although calling the API has disadvantages, such as incurring costs or facing usage limits, there exist several advantages of using the advanced AI for feedback. First, there is no need to train a reward model. Second, as the RL fine-tuned model begins to generate out-of-distribution outputs that differ from the data used to train the reward model, it becomes challenging for the trained reward model to provide accurate rewards. However, this out-of-distribution problem is effectively addressed with the advanced AI.\nRL Challenge: One of the challenging factors for RL is hyperparameter tuning (Eimer et al., 2023). This often results in generating repetitive words and truncated sentences (Ouyang et al., 2022). Additionally, as the model size increases, finding working hyperparameters becomes infeasible for individuals. To alleviate this issue, we utilize correction feedback from the advanced AI as the second stage (Section 2.3), and proceed with the supervised fine-tuning to stabilize the RL fine-tuned model.\n2.3 Correction: Supervised Fine-Tuning\nThe RL fine-tuning procedure makes model changes to maximize the reward sum, such as correcting mistakes or explaining why other options cannot be the answer. However, without highly tuned hyperparameters (Eimer et al., 2023), the model after the RL phase may often result in errors such as repeated sentences, truncated sentences, or incorrect content for some data points. (See examples in Appendix D.)\nCorrection Feedback: Given the success of LLMS and LMMs in a wide range of areas (Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022a), we do not need to be restricted to requesting feedback in the form of scores. We request correction feedback from advanced AI (Teacher) for sentences containing errors after the RL process, and obtain a corrected dataset $X_{corrected}$. Since the supervised fine-tuning is more stable and finding appropriate hyperparameters is easier than RL, we proceed with supervised fine-tuning using $X_{corrected}$ exactly as in common autoregressive model (Vaswani et al., 2023) training to stabilize the RL fine-tuned model. This reduces the burden of RL's exhaustive hyperparameter tuning and properly guides the direction in which the training model wants to change.\nHow Correction Feedback Helps RL: RL basically increases the probability of positively rewarded actions (or sentences) and decreases the probability for negative rewards. The direction of learning is determined by the reward (scalar) value. However, the opposite direction of the reward is sometimes required. For example, suppose there is a truncated sentence $S_{truncated}$ in CoT. $S_{truncated}$ gets a negative score because it is an incomplete sentence (Table 11). If there is no correction stage, the probability of $S_{truncated}$ is simply reduced. What if $S_{truncated}$ contains some valuable part? This valuable part is ignored, and its probability decreases. To alleviate this issue, we instead receive the corrected sentence as feedback and encourage the training model to generate complete sentences, which is very challenging to achieve with only RL.\nAdditionally, RL is primarily fine-tuned through PPO (Schulman et al., 2017) to prevent the model from deviating too much from the original model. The KL divergence penalty further prevents deviation. However, this penalty often causes the model's degeneration. As a solution, InstructGPT (Ouyang et al., 2022) proposes PPO-ptx, where the supervised fine-tuning term with the pretraining dataset is included in the loss function. While this aims to align the training model with specific preferences, it tends to anchor the model to the pre-training dataset. Instead, we perform supervised fine-tuning through the Teacher's correction feedback to allow the training model to more freely adapt and meet specific preferences without degeneration.\n2.4 Algorithm Detail\nWe propose a hybrid algorithm Alternating between REinforcement learning and Supervised fine-tuning (ARES).  First, we prepare a model with a given training dataset and generate rationale reasoning composed of several sentences for input. For the RL procedure to align the training model with a preference, we request scores for each sentence. The RL result may include some incorrect parts (colored as the red 4th sentence in the RL result box), but it aims to maximize the rewards provided. Next, we request correction feedback and create a corrected dataset (colored as the blue 3rd sentence"}, {"title": "3 Experimental Setup", "content": "Data: We first evaluate our proposed method on the ScienceQA (Lu et al., 2022a) dataset, a large-scale, multi-modal science dataset designed to assess multi-hop reasoning abilities. We choose ScienceQA because it contains reasoning chains to derive the answer. Each problem consists of a question, multiple options, multi-modal contexts, a correct answer, and an annotated lecture or solution chain (note that around 9.5% lack the solution chain). In addition, we conduct experiments on A-OKVQA (Schwenk et al., 2022), a knowledge-based multi-modal benchmark with a diverse set of challenging questions paired with rationales, demanding non-trivial commonsense knowledge (see Appendix B).\nBaselines: We mainly compare our method with Multimodal-CoT (MM-CoT) (Zhang et al., 2023b) as the baseline because it utilizes reasoning chains to solve multi-modal tasks. MM-CoT leverages two distinct models: the first generates rationale for a given problem, and the second, an inference model, takes the concatenated input (problem and generated rationale). This separated framework shows improved performance, even for relatively small models such as Flan-Alpacabase (Chia et al., 2023) (251M) and Flan-AlpacaLarge (790M). We use the rationale model provided by MM-COT for ScienceQA and retrain the rationale model ourselves for A-OKVQA because there is no provided model.\nPrompts for Feedback: Since our proposed ARES requests different types of feedback for each stage, a corresponding prompt exists separately. We use Claude 3 Haiku for all training to get feedback because it is about 20 times cheaper than the top competing models, yet still demonstrates decent performance. We first request scores ranging from 0.0 to 1.0 for each sentence in CoT to proceed with the RL stage. To obtain reasonable scores, we let Haiku consider the starting point of thought, the process of elimination, or true statements. (See Table 5.)\nIn order to collect the corrected dataset for the SFT stage, we let Haiku refer to the given"}, {"title": "4 Experimental Results", "content": "In this section, we evaluate our pipeline ARES that Alternates REinforcement Learning and Supervised Fine-Tuning by requesting diverse types of feedback for an advanced AI model (Teacher) (Claude 3 Haiku). The goal of ARES is to improve the rationale reasoning quality. We demonstrate how ARES enhances rationale reasoning in the following sections.\n4.1 Emergent Behavior Through RL\nThrough RL, a training model is aligned to a specific preference. Essentially, the model increases the probability of helpful sentences receiving good rewards and reduces the probability of incorrect or meaningless sentences. However, this process produces some interesting additional results.\nFirst, it supplements rationale reasoning for some problems where rationale reasoning is insufficient. In particular, 9.5% of problems in ScienceQA have empty rationale reasoning (solution) data. The model generates nothing before the RL stage for these problems but starts generating reasoning chains afterward (See Table 12). We observe this especially when utilizing PPO's advantage normalization or when the learning rate is large.\nSecond, the training model begins to explain why other options are not the answer (See Table 13). The process of elimination is a useful method for deriving answers when options are given.\n4.2 Guide RL with Correction\nDespite the benefits of RL, hyperparameter tuning often requires massive effort. Without meticulous tuning, the RL fine-tuned model may produce errors such as repetitive or incomplete sentences. To address these issues, we include a supervised fine-tuning (SFT) stage after RL to correct these errors. SFT is more stable than RL. We evaluate how well the SFT stage corrects errors caused by the RL stage for various RL hyperparameters. We test various RL hyperparameters such as learning rate = {5e-6, 1e-5, 2e-5, 5e-5}, batch size = {2, 4, 8, 16, 32}, and PPO epoch = {5, 10, 15}. As a result of RL, we observe that some of the sentences in rationale chains are repetitive or truncated (see Table 11 and 10). The SFT stage, with correction feedback, reflects what RL wants to achieve and appropriately guides it. However, excessive RL learning rates or epochs cause serious degeneration of the model,"}, {"title": "6 Conclusion", "content": "We propose a hybrid algorithm, ARES, which Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT) to enhance multimodal rationale reasoning for ScienceQA and A-OKVQA. ARES leverages two types of feedback: 1) ARES requests a score from a Teacher (we used Claude 3 Haiku) for sentence-level nuanced feedback and proceeds with RL. 2) ARES requests the Teacher to correct rationale chains after RL, stabilizing the RL fine-tuned model with SFT. ARES is designed to aid the RL procedure without massive hyperparameter tuning while properly reflecting the desired changes from RL. We evaluate the improvement in rationale reasoning produced by ARES compared to baselines using GPT-4o, and assess how much the improved rationale chains enhance inference accuracy for the two multi-modal tasks. We hope our work inspires further research on utilizing various types of AI feedback.\nLimitations\nAlthough we address general multi-modal rationale models beyond mathematical problems, receiving feedback from AI models still needs to be more reliable for more complex tasks such as graduate-level math or expert-level knowledge. For instance, some A-OKVQA problems even contain challenging questions requiring external knowledge beyond the image alone. This challenge highlights the necessity for future research to develop methods that can effectively incorporate external knowledge sources into the model. Additionally, if the model is not publicly available for free, using the API incurs costs, and there are daily usage limits."}]}