{"title": "Can Large Language Models Predict the Outcome of Judicial Decisions?", "authors": ["Mohamed Bayan Kmainasi", "Ali Ezzat Shahroor", "Amani Al-Ghraibah"], "abstract": "Large Language Models (LLMs) have shown excep-tional capabilities in Natural Language Processing (NLP) acrossdiverse domains. However, their application in specialized tasks such as Legal Judgment Prediction (LJP) for low-resource lan-guages like Arabic remains underexplored. In this work, we address this gap by developing an Arabic LJP dataset, collected and preprocessed from Saudi commercial court judgments. We benchmark state-of-the-art open-source LLMs, including LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as zero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a comprehensive evaluation framework combining quan-titative metrics (BLEU and ROUGE) and qualitative assessments (Coherence, legal language, clarity). Our results demonstrate that fine-tuned smaller models achieve comparable performance to larger models in task-specific contexts while offering significant resource efficiency. Furthermore, we investigate the effects of prompt engineering and fine-tuning on model outputs, providing insights into performance variability and instruction sensitivity. By making the dataset, implementation code, and models publicly available, we establish a robust foundation for future research in Arabic legal NLP.", "sections": [{"title": "I. INTRODUCTION", "content": "LLMs have transformed NLP, achieving state-of-the-art (SOTA) performance across diverse tasks in multilingual and multitask settings [1]. Advanced LLMs like Generative Pre-trained Transformer 4 (GPT-4) and Gemini demonstrate remarkable reasoning, comprehension, and problem-solving capabilities, enabling applications in specialized domains such as law [2], medicine [3], [4], and education [5].\nHowever, their closed-source nature raises concerns about data privacy, customization, and accessibility, especially for organizations seeking domain-specific solutions [6], [7]. Open-source models like Large Language Model Meta AI (LLaMA)-3 [8] and Philosophically Integrated-4 (Phi-4) [9] provide flexible alternatives, offering fine-tuning and in-context learning capabilities that often rival proprietary systems and exceed baseline results [10], [11].\nA key application of LLMs is LJP, which predicts judicial outcomes based on factual case details [12]. Currently, legal experts rely on extensive training to identify relevant laws, determine charges, and issue judgments [13]. Legal decision-making is inherently complex, requiring analysis of vast case-specific information while ensuring consistency and fairness [14]. This process remains labor-intensive, time-consuming, and prone to bias [14].\nAdvances in NLP are automating aspects of LJP, enabling efficient and consistent data-driven predictions [12]. Early LJP approaches relied on manually extracted features, which were costly and inefficient [15]. LLM integration reduces the need for manual feature engineering, improves prediction accuracy, and simplifies complex legal reasoning.\nDespite these advancements, LJP remains underexplored in Arabic due to unique linguistic challenges like morphological richness, dialectal variation, and complex syntax [16]. LLMs, successful in high-resource languages like English, often underperform in Arabic, especially with Arabic-prompted instructions [17]. The lack of publicly available Arabic datasets tailored for LJP further exacerbates this gap, hindering model training and evaluation.\nTo address these challenges, this work develops a tailored solution for Arabic Legal Judgment Prediction. Specifically, we:\n1) Collect and preprocess Arabic legal case data from the Saudi commercial court\u00b9, creating a domain-specific dataset.\n2) Craft 75 diverse Arabic instructions of varying lengths and styles to simulate user-centric prompting scenarios.\n3) Benchmark open-source LLMs, including LLaMA-3.2-3B-instruct and LLaMA-3.1-8B-instruct, under zero-shot, one-shot, and fine-tuning settings using QLoRA.\n4) Evaluate performance using traditional metrics like BLEU and ROUGE, complemented by qualitative analysis using LLaMA-3.1-8B-instruct to assess responses against ground truth judgments.\nThis study addresses the lack of Arabic legal NLP datasets by providing a curated dataset, benchmarking results, implementation code, and trained models. Additionally, it introduces an evaluation pipeline combining quantitative and qualitative"}, {"title": "II. RELATED WORK", "content": "Recent years have seen significant interest in LJP, particu-larly with the rise of Machine Learning (ML) and NLP tech-niques. Early approaches to LJP primarily relied on rule-based systems and manually crafted features. While these methods were foundational, they were constrained by scalability and efficiency challenges [18]. The introduction of transformer-based models, such as the Bidirectional Encoder Representations from Transformers (BERT) model [19], marked a turning point by enabling models to capture complex semantic and syntactic relationships in legal texts. For example, Imran et al. [20] demonstrated BERT's effectiveness in classifying European Court of Human Rights cases, showcasing its capacity to process intricate legal documents. Building on the success of transformers, researchers turned to ML-based approaches that emphasized data-driven learning, further enhancing the ability of models to handle unstructured legal texts. Similarly, ALJP [21] focused on Arabic legal texts, addressing challenges like morphological complexity and demonstrating that deep learning approaches outperformed traditional classifiers in this domain.\nAletras et al. [22] conducted one of the first studies on predicting case outcomes for the European Court of Human Rights (ECHR). Using a Support Vector Machine (SVM) [23] trained on N-grams and topic features, they achieved 79% accuracy in determining whether a human rights article had been violated. This pioneering work demonstrated the potential of using textual data for judicial decision-making and inspired further research into automated legal outcome prediction. However, the reliance on manual features in such approaches highlighted the need for more robust and scalable methods."}, {"title": "A. Transformer-Based Models in Legal NLP", "content": "Transformer-based models revolutionized NLP by enabling the modeling of complex semantic and syntactic relationships. The BERT model [19] and its variants have significantly advanced legal text analysis by capturing semantic richness.\n[20] demonstrated BERT's effectiveness in classifying European Court of Human Rights cases, highlighting its ability to handle intricate legal documents. Similarly, ALJP: An Arabic Legal Judgment Prediction tackled challenges in Arabic legal texts, such as morphological richness and syntax complexity, showing that deep learning approaches, including transformers, outperformed traditional classifiers like SVM and Random Forest [21]. AraBERT, a transformer model fine-tuned for Arabic, has also delivered strong performance across Arabic NLP tasks, emphasizing its value for domain-specific applications like LJP [24]. Collectively, these studies demonstrate how transformer models have driven significant advancements in legal NLP, particularly in case classification and legal document understanding."}, {"title": "B. ML-Based Approaches to LJP", "content": "As ML methodologies progressed, researchers shifted their focus toward data-driven methods leveraging increasingly ac-cessible law-related datasets. This shift represented a crucial evolution in LJP research, allowing models to learn directly from extensive legal document collections rather than relying on manually crafted features.\nCui et al. [25] explored the impact of NLP techniques on processing unstructured legal documents, facilitating tasks like legal text classification, summarization, and case outcome prediction. Additionally, specialized transformer architectures like LegalBERT [26] and GPT have proven particularly ef-fective in handling the intricate structure of legal texts, en-abling advancements in legal-specific tasks. However, most of these models focus on English-language datasets, with limited research on applying such techniques to Arabic's unique linguistic challenges [27]."}, {"title": "C. Text-Generation Techniques for Legal NLP", "content": "Text-generation techniques have become transformative in legal NLP, supporting tasks like document summarization, automated contract drafting, and case argument generation. Large pre-trained language models like GPT-3 and GPT-4 have made generating coherent and contextually accurate legal text increasingly feasible. For instance, [28] evaluated GPT-3's performance on the Uniform Bar Examination (UBE), a standardized test for legal professionals, showcasing its ability to comprehend legal reasoning and provide contextually relevant answers in multiple-choice sections. Similarly, transformer-based models like GPT have demonstrated their capacity to process and generate structured, long-form legal text, highlighting their potential in specialized applications such as drafting judgments."}, {"title": "D. LLM-Based Performance for Low-Resource Languages", "content": "LLMs excel in high-resource languages like English but face challenges in low-resource languages, such as Arabic, due to pre-training on English-heavy datasets. Conneau et al. [29] demonstrated that multilingual models like XLM-R can generalize across languages, handling cross-lingual tasks with limited data. Similarly, Pires et al. [30] showed that multilin-gual BERT effectively transfers knowledge across languages, achieving strong results in low-resource tasks through cross-lingual embeddings.\nInterestingly, English-written instructions often yield strong performance even for non-English tasks. For instance, [17] found that non-native English instructions surpassed native Arabic ones in certain Arabic NLP tasks, highlighting the adaptability of LLMs when leveraging high-quality, generalized instruction data in low-resource settings."}, {"title": "E. Research Gap in Arabic Legal NLP", "content": "Despite advancements in legal NLP and the rise of transformer-based models, notable gaps persist in Arabic LJP. A major obstacle is the absence of publicly available Arabic"}, {"title": "III. METHODOLOGY", "content": "This section provides a detailed overview of the dataset construction process, the models used, the prompting strategies implemented, and the fine-tuning approach adopted. Fig. 1 illustrates the complete end-to-end workflow of our study."}, {"title": "A. Dataset Collection and Preparation", "content": "In this research, we constructed a domain-specific dataset for Arabic LJP by collecting court judgments from the publicly accessible Saudi Ministry of Justice Judgment Publication Platform\u00b2. This platform hosts a substantial repository of legal cases, particularly within the domain of commercial law. Fig 2 presents a sample data point.\nTo compile the dataset, we employed web scraping tech-niques to extract judgments, followed by rigorous parsing and preprocessing steps to organize the data into a structured for-mat. The resulting dataset facilitates the generation of diverse instruction-based examples suitable for fine-tuning language models.\nAfter shuffling the dataset to ensure randomization, we sam-pled a smaller version of the dataset and split it into training (3752 samples) and testing (538 samples) sets for training and evaluation. We created 75 diverse Arabic instructions using GPT-40 varying in length and complexity. These instructions were uniformly distributed across the dataset's data points to ensure broad coverage.\nThe dataset is stored as JSON files, with each data point containing several fields: a unique identifier (id), a reference to the original legal case (original_id), the concatenated facts and reasons from the case (input), the court's final judgment (output), the dataset name (e.g., LJP), the task type (Legal Judgment Prediction), the language (Ar), a detailed task instruction (Instruction), and a numerical vector representation of the input (embedding).\nThe embeddings were generated using a BERT-based sentence embedding model from Sentence Transformers [31], specifically the paraphrase-multilingual-MiniLM-L12-v2"}, {"title": "B. QLORA Fine-Tuning", "content": "To fine-tune our model efficiently, we adopt QLORA (Quan-tized Low-Rank Adaptation) [32], a parameter-efficient fine-tuning technique that enables the adaptation of large pre-trained models without a significant increase in computational cost. QLORA combines two key principles: quantization and low-rank adaptation. It quantizes the model's parameters into 4-bit precision, significantly reducing memory usage while preserving performance. This allows fine-tuning on resource-limited hardware, making it viable for large-scale applications.\nThe core mechanism of QLORA builds upon LoRA (Low-Rank Adaptation) [33], which factorizes the weight matrix \\(W_o \\in \\mathbb{R}^{d \\times k}\\) into two low-rank matrices, as follows:\n\\(h = W_o x + \\Delta W x = W_o x + B A x,\\)\nwhere \\(B \\in \\mathbb{R}^{d \\times r}\\), \\(A \\in \\mathbb{R}^{r \\times k}\\), and \\(r\\) is the rank of the factor-ization, with \\(r \\ll d\\) and \\(r \\ll k\\). Here, \\(\\Delta W = B A\\) represents the low-rank modification to \\(W_o\\), allowing parameter-efficient updates.\nDuring training, the original weights \\(W_o\\) remain frozen and do not receive gradient updates, ensuring that the pre-trained knowledge is preserved. Only the low-rank matrices \\(A\\) and \\(B\\) are trainable. To control the contribution of the LORA modifications, \\(\\Delta W x\\) is scaled by a factor \\(a/r\\), where \\(a\\) is a scaling hyperparameter. Higher values of \\(a\\) amplify the influence of the low-rank updates, enabling more expressive adaptations.\nThe experimental setup uses a batch size of 2, gradient accumulation steps of 2, a maximum sequence length of"}, {"title": "C. Evaluation Metrics", "content": "We evaluated the model using both quantitative and qual-itative metrics.\nFor quantitative evaluation, we employed ROUGE-F [35] and BLEU [36], which are standard metrics for text generation tasks:\n*   ROUGE-F: Measures the F1-score (harmonic mean of precision and recall) for overlapping n-grams, capturing content similarity and informativeness between the generated text and the reference text.\n*   BLEU: Evaluates n-gram precision with a brevity penalty, assessing fluency and alignment with the reference.\nFor qualitative evaluation, we used the LLaMA3.1-8B-Instruct model to rate the generated responses against the ground truth on a scale of 1 to 10 across eight dimensions:\n1) Accuracy: Alignment with factual and legal details.\n2) Relevance: Appropriateness to the legal question.\n3) Coherence: Logical organization and consistency.\n4) Brevity: Conciseness with sufficient detail.\n5) Legal Language: Use of formal legal terminology.\n6) Faithfulness: Preservation of ground truth facts and principles.\n7) Clarity: Ease of understanding.\n8) Consistency: Absence of contradictions.\nOur use of LLMs as evaluators is inspired by previous studies [37], [38]."}, {"title": "D. Prompting Strategies", "content": "1) Zero-shot Prompting: In zero-shot prompting, the LLM generates a legal judgment based on an instruction and the provided case details. Formally:\n\\(f \\text{(Instruction, Facts + Reasons)} \\rightarrow \\text{Judgment}\\)\nwhere:\n*   \\(f\\): The LLM.\n*   Instruction: A prompt specifying the task, e.g., \"Based on the facts, analyze the reasons and extract the final judgment text.\"\n*   Facts + Reasons: The factual circumstances of the case, along with the legal reasoning and principles applied.\n*   Judgment: The model's output decision.\n2) Few-shot Prompting: Few-shot prompting provides the LLM with examples from the training data to guide its judgment. In our case, we used only a single example. The formalization remains the same as in zero-shot prompting, with the addition of an example:\n\\(f \\text{(Instruction, Example, Facts + Reasons)} \\rightarrow \\text{Judgment}\\)"}, {"title": "E. Model Inference", "content": "The models were deployed for inference using VLLM [39] on two NVIDIA A16 GPUs. The maximum sequence length was set to 2048 tokens. To ensure reproducibility and deterministic outputs, the temperature parameter was fixed at zero."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Table I presents a comparison of the performance of smaller and larger versions of LLaMA, evaluated under 1-shot learning and fine-tuning settings, across both subjective and objective evaluation metrics."}, {"title": "A. Subjective Metrics", "content": "The LLaMA-3.1-8B-Instruct achieves the highest scores in coherence (5.49), legal language (6.69), and faithfulness (5.99), demonstrating the advantages of increased parameter size. The LLaMA-3.2-3B-Instruct-Finetuned performs com-petitively, excelling in clarity (5.94) and consistency (6.16), underscoring the benefits of task-specific fine-tuning. The one-shot model improves over the base model across all metrics but lags behind the fine-tuned and larger models."}, {"title": "B. Objective Metrics", "content": "For ROUGE-1, ROUGE-2, and BLEU, the LLaMA-3.2-3B-Instruct-Finetuned achieves the highest scores (0.34, 0.25, 0.14), demonstrating strong alignment with reference outputs. The one-shot model performs moderately well, while the LLaMA-3.1-8B-Instruct underperforms, indicating that fine-tuning provides more significant gains than scale alone for these metrics."}, {"title": "C. Comparative Insights", "content": "The LLaMA-3.1-8B-Instruct emerges as the strongest general-purpose model, but the LLaMA-3.2-3B-Instruct-Finetuned achieves comparable or superior performance in task-specific contexts with reduced computational cost. While prompting improves performance, it remains less effective than fine-tuning for domain-specific tasks.\nFine-tuning smaller models can surpass larger models in specialized tasks, offering a resource-efficient alternative."}, {"title": "V. DISCUSSION", "content": "This section examines the performance of LLM configurations in LJP, focusing on the effects of prompt engineering, fine-tuning, and in-context learning. We analyze model behaviors, highlight strengths and limitations, and discuss trade-offs in adapting LLMs for this specialized task."}, {"title": "A. Prompt Engineering Effect for LJP", "content": "Statistical analyses using one-way ANOVA and the Kruskal-Wallis test were conducted to assess how prompt instructions affected model performance in LJP. The goal was to determine if significant variability existed across different instructions.\nThe Llama-3.2-3b-1shot model displayed no significant differences across instructions (ANOVA: F = 0.652, p = 0.988; Kruskal-Wallis: H = 48.701, p = 0.990), indicating consistent responses regardless of prompt variations. In contrast, Llama-3.2-3b-finetuned showed significant differences in ANOVA (F = 1.377, p = 0.028) but not in the Kruskal-Wallis test (H = 89.055, p = 0.112), suggesting fine-tuning increases sensitivity to instruction variability.\nThe Llama-3.2-3b-instruct model showed marginal evidence of inconsistency (ANOVA: F = 1.271, p = 0.075; Kruskal-Wallis: H = 85.083, p = 0.178). Although not statistically significant, this highlights the subtle effects prompt variability may have on instruction-tuned models [40]. On the other hand, the Llama-3.1-8b-instruct model demonstrated robust consistency (ANOVA: F = 0.968, p = 0.557; Kruskal-Wallis: H = 80.320, p = 0.288), likely due to its larger size and diverse instruction pre-training, aligning with scaling laws for language models [41].\nThese findings reveal key patterns in prompt engineering effects. Smaller models, such as Llama-3.2-3b-1shot, perform consistently but exhibit lower overall performance. Fine-tuning smaller models, as with Llama-3.2-3b-finetuned, significantly improves performance, comparable to larger models, but increases sensitivity to instruction structure. Larger models, like Llama-3.1-8b-instruct, perform better overall and show reduced dependence on prompt quality.\nEffective prompts that are specific, structured, and con-textually aligned with legal reasoning consistently enhance model performance, enabling accurate and coherent outputs. Conversely, vague or overly broad prompts lead to less relevant and disjointed responses. These results highlight the critical role of prompt clarity and structure in optimizing LJP tasks."}, {"title": "B. Effect of Fine-tuning and In-Context Learning", "content": "Table I shows significant ROUGE-2 and BLEU score im-provements for the LLaMA-3.2-3B model with 1-shot learning and fine-tuning compared to the base model (LLaMA-3.2-3B-Instruct). Specifically, 1-shot learning, as an in-context learning approach, achieves an 850% improvement in ROUGE-2 (0.02 to 0.19) and a 1000% improvement in BLEU (0.01 to 0.11). While enhancing response quality, style, and correctness, it falls short of fine-tuning. Fine-tuning delivers greater gains, with a 1150% increase in ROUGE-2 (0.02 to 0.25) and a 1300% increase in BLEU (0.01 to 0.14), enriching task-specific knowledge and generating more accurate, relevant responses. These results highlight fine-tuning's effectiveness in task adaptation and the lighter, albeit less impactful, benefits of in-context learning."}, {"title": "C. Limitations", "content": "While this study provides valuable insights into the effects of prompt engineering and fine-tuning on LLM performance in LJP, several limitations should be noted:\n*   Reliance on LLM-Based Evaluation:\nWe relied on an LLM to score model outputs against the ground truth, However, the evaluation's accuracy and reliability were not systematically validated, risking potential biases or misinterpretations.\n*   Limited Model Size:\nComputational constraints limited us to smaller models, thoughLarger models are known to yield better performance across multiple NLP tasks. Exploring the impact of scaling remains a future direction.\n*   Restricted Number of Shots:\nWe used a 1-shot setting for in-context learning to balance performance and efficiency. Increasing the shots could improve performance but was not feasible due to resource constraints.\n*   QLORA Fine-Tuning:\nFine-tuning was performed on a 4-bit quantized model using QLoRA, which is efficient but may under-perform compared to full-model fine-tuning.\nAddressing these limitations in future work will deepen understanding of LLM strengths and weaknesses in LJP."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduced the first Arabic instruction-following dataset for the LJP task and benchmarked LLMs on it. We evaluated approaches such as zero-shot, one-shot, and fine-tuning. Results show that smaller LLMs face challenges with this complex task, with minimal performance gains from prompt engineering. However, fine-tuning yielded significant improvements, demonstrating its effectiveness in adapting to the task. In-context learning (one-shot) also enhanced performance, achieving results close to fine-tuning, albeit slightly lower.\nFuture work could expand on this study by testing larger models to assess scaling effects, experimenting with advanced prompting methods, and exploring alternative fine-tuning strategies for further optimization. Additionally, the dataset provided here offers opportunities for the research community to develop new methodologies and applications in LJP for Arabic and other low-resource languages."}]}