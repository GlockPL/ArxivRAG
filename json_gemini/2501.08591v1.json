{"title": "OpenMLDB: A Real-Time Relational Data Feature Computation System for Online ML", "authors": ["Xuanhe Zhou", "Wei Zhou", "Liguo Qi", "Hao Zhang", "Dihao Chen", "Bingsheng He", "Mian Lu", "Guoliang Li", "Fan Wu", "Yuqiang Chen"], "abstract": "Efficient and consistent feature computation is crucial for a wide range of online ML applications. Typically, feature computation is divided into two distinct phases, i.e., offline stage for model training and online stage for model serving. These phases often rely on execution engines with different interface languages and function implementations, causing significant inconsistencies. Moreover, many online ML features involve complex time-series computations (e.g., functions over varied-length table windows) that differ from standard streaming and analytical queries. Existing data processing systems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for these computations, making them unsuitable for real-time online ML applications that demand timely feature updates. This paper presents OpenMLDB, a feature computation system deployed in 4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB first employs a unified query plan generator for consistent computation results across the offline and online stages, significantly reducing feature deployment overhead. Second, OpenMLDB provides an online execution engine that resolves performance bottlenecks caused by long window computations (via pre-aggregation) and multi-table window unions (via data self-adjusting). It also provides a high-performance offline execution engine with window parallel optimization and time-aware data skew resolving. Third, OpenMLDB features a compact data format and stream-focused indexing to maximize memory usage and accelerate data access. Evaluations in testing and real workloads reveal significant performance improvements and resource savings compared to the baseline systems. The open community of OpenMLDB now has over 150 contributors and gained 1.6k stars on GitHub\u00b9.", "sections": [{"title": "Introduction", "content": "Relational data feature computation, which extracts ML features from raw data tables, is essential for many online applications leveraging machine learning models (e.g., credit card anti-fraud, online advertising [29, 56, 60]). However, this task is inherently complex. For instance, the product recommendation model at Vip-shop requires over 600 distinct features [36]. Many of these features demand specialized ML functions, such as statistical functions (e.g., top-N frequency counts) and feature signatures (e.g., export format labeling for high-dimension features like product items [47]), as well as computation patterns (e.g., multi-table windows) distinct from those in standard streaming and analytical queries [65, 67]. To illustrate the complexity, consider the product recommendation scenario that generates personalized advertisements based on the user's most recent actions. As shown in Figure 1 (a), the feature computation process begins by computing window tables. For instance, we retrieve tuples from Actions and Orders that match the incoming tuple's user ID and fall within a short time interval (e.g., three seconds). These matched tuples, together with the incoming tuple, form a window table (w_union_3s). Next, we compute various window features like (i) counting the number of product types purchased by the user and (ii) calculating the average price of products within specific categories, subject to conditions like \u201cpurchased more than one\u201d. To support recommendation at scale, feature computation must handle extremely high concurrency (e.g., more than 200 million requests per minute) and still deliver up-to-date features within milliseconds. In current practice, a feature computation workflow has two separate stages. As shown in Figure 1 (b), first, data scientists develop and compute offline features with batch processing engines (e.g., SparkSQL), iterating until the model's training performance meets their objectives. Afterward, to deploy these offline feature scripts in online production, the engineering team must refactor and optimize them for online computation (with streaming engines like Flink). However, the two stages are handled by different teams and systems, which can cause inconsistencies. For instance, in a real-world de-fraud case at Varo [12], the online application defines the \"account balance\" feature as the current real-time balance, while offline data scientists define it as the previous day's balance. Such discrepancies cause mismatches between offline and online results, making extensive and costly verification necessary before the final deployment. Based on our experience (e.g., at Akulaku [15]), consistency verification can take several months or even one year. Moreover, existing data processing systems, both online [1, 10, 11, 24-26, 37, 53, 55] and offline [2, 9, 40, 54, 59, 63], as well as specialized feature stores [5-7, 28], struggle to meet the performance demands (see Section 2.2). For instance, streaming execution engines like Apache Flink [37], and Spark Streaming [53] lack critical optimizations for feature computation, such as handling long window operations and aggregating data across multiple table windows, making it difficult to achieve sub-second online response time. Similarly, widely used storage engines (e.g., MySQL [25], PostgreSQL [26], Cassandra [1]) are not designed for rapid ingestion or efficient ordering of time-series data, both of which are essential for online feature storage. On the offline side, the execution engines (e.g., Spark [63], Hive [59], ClickHouse [54]) face issues including limited operation-level parallelism, data skew, and frequent RPC calls during distributed computation, which limit their throughput and scalability for offline feature computation. Lastly, existing feature stores like Feast [5] primarily focus on fast data retrieval rather than real-time feature computation, forcing users to combine multiple tools for feature computation and thus introducing additional synchronization and operational overhead. To address these problems, we propose OpenMLDB, an industrial-level feature computation system. Specifically, OpenMLDB first ensures online-offline feature computation consistency by providing a unified query plan generator that supports extended SQL syntax and compilation-level optimizations (e.g., cycle binding, compilation caching). Second, OpenMLDB achieves ultra-high feature computation performance through a suite of optimization techniques for both online and offline computations. For online computation, it reuses pre-aggregation results to enhance long-window computations and employs a self-adjusting strategy to optimize multi-table window unions. For offline computation, it supports multi-window parallel optimization and mitigates data skew by repartitioning tuples based on utilized columns and data distribution. Finally, OpenMLDB offers a compact in-memory data format and a stream-focused data structure, which pre-ranks data by key and timestamp to enable rapid online data access."}, {"title": "Background and Related Work", "content": "Given a set of data tables R, the goal of relational data feature computation is to produce feature values, as specified in a feature script Q, that may include both discrete and continuous features. These feature computations often extend beyond standard relational operations (see Section 4.1) and present unique challenges. For example, they may require specialized windowed aggregations (e.g., top-frequency keys, conditional averages, time-series indicators), advanced string parsing, window unions, last-join operations, and direct output in ML-friendly formats. Based on distinct performance requirements, it can be further divided into online and offline stages. Online Feature Computation processes features for continuously arriving data tuples and provides up-to-date features for online requests. The computations draw on both recent data (often spanning windows of different time lengths) and static reference data (e.g., user profiles). Many commercial scenarios demand stringent real-time performance. For example, among the target clients of OpenMLDB, Akulaku (a fintech company) requires about 4-ms response time for risk control, and bank's real-time anti-fraud checks must be completed under 20 ms [17]. Similarly, companies such as UBiX and Vipshop need sub-100 ms latency for online advertising. Offline Feature Computation processes features in batch mode against periodically updated tables (e.g., historical orders). The primary objective is to efficiently handle large batches of computation requests while maximizing system throughput. For instance, in consumer-facing scenarios like banks and Yum Holdings (a restaurant company), daily active users reach tens of millions, and achieving higher throughput helps reduce machine costs. In a similar vein, when Akulaku adopted OpenMLDB for offline risk control, they saved 20 servers (each with 128GB RAM), totaling around $275,000."}, {"title": "Existing Data Systems", "content": "General-purpose stream processing systems [37, 53] are insufficient for millisecond-level online ML services. Apache Flink [37] incurs significant overhead due to frequent context switching and lacks advanced optimizations for stateful aggregations. For instance, with sliding windows, Flink needs to select and discard the oldest data during each window operation. Since it lacks a robust state retention mechanism, Flink has to re-sort the data to identify the oldest entries for eviction, increasing the time complexity from O(1) to O(log n). Additionally, its Java-based implementation lacks specific low-level optimizations, resulting in high latency for individual requests. Similarly, Spark Streaming [53] relies on multi-node computation and requires frequent RPC calls to complete operations, making it challenging to achieve latencies in the tens-of-milliseconds range. Relational databases such as MySQL [25], MariaDB [11], and PostgreSQL [26], even when configured for in-memory storage engines, often exhibit inefficiencies in standard indexing for time-based queries (e.g., rapid data insertion). They also lack native time-ordering capabilities essential for real-time analytics and sequential data processing. Distributed storage systems like HBase [10], Cassandra [1], DynamoDB [55], and MongoDB [24] do not support standard ANSI SQL and lack optimizations for time-series data, which increases usage complexity for data scientists. While NewSQL systems (TiDB [44], CockroachDB [57], OceanBase [62]) simplify usage via SQL support, they are also not optimized for time-series data. Meanwhile, time-series databases (e.g., InfluxDB [22], TDengine [19], OpenTSDB [18]) cannot fully manage data in memory space (primarily on disk), limiting their ability to support high-performance feature extraction. Existing data processing systems [40, 59, 63] lack specific optimizations for feature extraction. For instance, Spark's window aggregation does not support Whole-Stage Java Code Generation and struggles to parallelize many window operations without overlaps. Similarly, Apache Hive [59], built on top of MapReduce [40], suffers from high job startup latency and disk-based operations, rendering it unsuitable for high-throughput iterative feature extraction. Other MPP systems, including Apache Doris [2], ClickHouse [54], GreenPlum [9], Apache Druid [3], and StarRocks [27], are primarily designed for general analytical workloads rather than time-series feature computations. Furthermore, existing C++ execution engines have their own limitations. Intel OAP [14] does not extensively exploit advanced code generation techniques for complex transformations and aggregations. Databricks Photon [21] and NVIDIA Spark RAPIDS [13] enable vectorized execution but lack specialized optimizations for unique feature computation patterns, such as efficient multi-window processing and custom aggregations. Similarly, Alibaba EMR [20] provides limited support for code generation in window functions, further restricting its applicability to feature extraction tasks. Existing online feature stores such as Feast [5] focus on rapid feature retrieval but lack mechanisms for on-the-fly, low-latency feature computation. Others, like Hopsworks [7], Tecton [28], and Feathr [6], provide limited optimization for complex feature pipelines, especially those involving multiple data streams"}, {"title": "Overview of OpenMLDB", "content": "As shown in Figure 2, OpenMLDB comprises a unified query plan generator, online real-time execution engine, offline batch execution engine, and compact time-series data and memory management. To ensure consistency between offline and online feature computation, first our plan generator supports standard SQL for compatibility (e.g., MySQL queries) and SQL extensions to simplify the implementation of common features (see Section 4). Second, our plan generator uses LLVM and JIT [43, 48, 50] to transform the SQL queries into efficient machine code, where we adopt optimizations like cycle binding and compilation caching. This machine code includes direct calls to the C++ library functions shared by the offline and online execution engines. We developed the online engine from scratch, which is several orders of magnitude faster than common stream processing and database systems. The underlying techniques include (1) pre-aggregation for enhancing functions over windows with extremely long time intervals (e.g., for years) or hotspot data and (2) dynamic data adjusting for efficient stream unions over multiple tables (e.g., with out-of-order stream data [66]). Note that we also adopt Zookeeper (a distributed framework [45]) to ensure high system availability and fault tolerance. Compared to engines like Spark, the offline engine in OpenMLDB (1) improves the resource usage by parallelizing the execution of window operations over the same tables"}, {"title": "Execution Modes", "content": "As illustrated in Figure 3, OpenMLDB supports multiple execution modes for offline and online feature computation. These modes share the same SQL syntax but impose distinct execution constraints. (1) Offline Execution Mode: In this mode, historical data is synchronized with the offline engine, where data scientists develop feature scripts and execute large-scale computations. Certain SQL commands (e.g., LOAD DATA) are executed asynchronously as background tasks. (2) Online Preview Mode: This mode enables testing newly developed feature scripts on a limited set of online data, minimizing any negative impact on online services. To ensure efficiency, it retrieves results from a data cache and constrains query complexity (e.g., limiting the number of key columns). (3) Online Request Mode: In this mode, each incoming request tuple is virtually inserted into the online data table. The online engine then applies the previously deployed offline script and returns a single feature result for each tuple."}, {"title": "Unified Query Plan Generator", "content": "SQL is one of the most popular data processing languages. Compared to other declarative languages like Python, most relational databases like MySQL and data processing systems like Spark support SQL, making it a common choice for data processing and feature extraction in ML workflows. Additionally, the inherent constraints of SQL (e.g., execution plans without arbitrary loops) enable more targeted optimizations [32, 34]."}, {"title": "SQL Compilation", "content": "We compile SQL into efficient C++ machine code by applying optimizations in three main aspects. (1) Parsing Optimization: OpenMLDB first identifies the identical column references and similar window definitions within the deployed SQL. Instead of generating separate code segments for each instance, it merges them into a unified code block. For instance, when multiple windows share the same computation template (e.g., in the form like \"PARTITION BY key 1 ORDER BY ts ROWS ...\"), they can be merged into a single window definition, preventing redundant window construction and reducing repetitive code generation. Besides, OpenMLDB applies index optimizations to critical information (e.g., data provider) in WINDOW and LAST JOIN operations (see Section 7.2), so as to accelerate the execution process. (2) Cyclic Binding: When multiple aggregate functions appear in the same window, OpenMLDB applies a cyclic binding strategy. First, it compiles the minimal intermediate states required by simpler aggregates like sum, min, and max. These intermediate results are then reused to compute more complex aggregates such as avg. (3) Compilation Cache: OpenMLDB employs a caching mechanism that stores previously compiled code segments, allowing subsequent deployments of similar SQLs (feature scripts) to bypass the full compilation process. When a new SQL shares code patterns with an existing cached segment, OpenMLDB retrieves the relevant compiled portion and directly applies it. This reuse of compiled artifacts accelerates computation by reducing the time spent on repeated code generation and optimization."}, {"title": "Online Feature Computation", "content": "Beyond compiling optimization, OpenMLDB's online execution engine offers specific optimization techniques for time-consuming operations in OpenMLDB SQL, including pre-aggregation for long window computation in Section 5.1 and self-adjusting computation for multiple window table unions in Section 5.2."}, {"title": "Long Window Pre-Aggregation", "content": "We first explain how to resolve the performance bottleneck caused by window functions involving extremely large data volumes. Such cases arise (i) when the time intervals are extremely long (e.g., spanning multiple years), which is uncommon in typical streaming scenarios, or (ii) when containing hotspot data. We refer to them as long-window features. On the one hand, as data volumes grow, the computation time for these long-window features increases proportionally, making it difficult to achieve real-time performance. On the other hand, processing overlapped long-window features will cause significant resource wastes. To address these problems, we propose a long-window pre-aggregation technique. As illustrated in Figure 4, we partially compute aggregation results during data insertion. Then, for real-time requests involving long-window features, we simply compute by merging these pre-aggregated values, avoiding to process all raw tuples in the windows. Specifically, it includes three main steps. We first initialize multi-level aggregators based on the long-window features in the deployed scripts. Each level corresponds to a specific time granularity and builds upon the pre-aggregated results of finer-grained levels. We determine the optimal aggregator hierarchy by considering factors such as query frequency, time window sizes, data distribution, and maintenance overhead. Meanwhile, we utilize data structures like segment trees [39] to efficiently manage historical computation results of these aggregators, based on which we compute statistics such as query frequencies for aggregator hierarchy enhancement. For an online request, the query optimizer identifies long-window computations and replaces expensive raw data scans with efficient lookups in the aggregator hierarchy. As illustrated in Figure 4, the long-window operation is transformed into the merging of (i) pre-aggregated features agg2 and agg4 from the daily aggregator (first level), (ii) the pre-aggregated feature agg3 from the monthly aggregator (second level), and (iii) real-time computed features from raw data agg1 and agg5. Based on the evolving query patterns, OpenMLDB can adaptively adjust the hierarchy by adding or removing aggregation levels, avoiding unnecessary computational or storage costs. For instance, to compute a feature over the past year, we can adopt aggregators at daily and monthly levels if hourly aggregators are seldom queried. To maintain up-to-date aggregators without negatively impacting online data insertion, we employ an asynchronous updating strategy. First, aggregator updates are designed by assuming the \"binlog_offset\" increases monotonically. To ensure this, all updates are protected within the replicator lock, which prevents other operations, such as concurrent Put requests, from inserting conflicting updates in the middle of the sequence. By coordinating updates through the binlog, we decouple pre-aggregation computations from the main data insertion pipeline. Besides, to support failure recovery during aggregation updates with binlog, we encapsulate the aggregator update logic within update_aggr closure, ensuring atomic updates within the binlog sequence. Updates are appended to the binlog (\"replicator->AppendEntry(entry, &closure)\"), which not only appends the updates but also triggers asynchronous execution."}, {"title": "Self-Adjusted Window Union", "content": "Next, we explain how to optimize multi-table window unions (Window Union), which are one of the most complex operations in online feature computation. Window Union aims to match tuples from multiple stream tables over a shared time window and partitioned by common keys. For example, as shown in Figure 1, a 3-second Window Union (w_union_3s) returns tuples arriving within the last three seconds to the current tuple. A standard approach, used by Flink [37], employs a static key-based distribution strategy, where data tuples are routed to threads solely based on their hashed key columns. This rigid approach struggles to handle workload shifts effectively. To address this shortcoming, OpenMLDB incorporates a self-adjusting technique that dynamically refines task allocation and applies incremental computations, ensuring low response time under various workloads. It includes two main steps. (1) On-the-Fly Load Balancing. Our prior work [66] found that if the set of distinct keys is limited, static key-based distribution often encounters severe load imbalances. To alleviate this problem, we introduce a dynamic scheduler that periodically adjusts the mapping from keys to worker threads. By gathering runtime metrics on processing overhead, We estimate each worker thread's load and reassign specific keys at runtime, keeping the workload even across all threads. Furthermore, multiple workers can collaborate on the same key subset if needed, improving flexibility and overall throughput. (2) Incremental Computation. Large windows commonly create overlapping intervals of data, making computations repetitive and resource-intensive. To avoid recalculating results from scratch, we utilize a Subtract-and-Evict approach [58]. As an outdated tuple leaves the window, we subtract it from the running aggregator. Then, we incorporate any newly arriving tuples incrementally."}, {"title": "Offline Feature Computation", "content": "The first optimization in our offline engine focuses on multi-window parallel optimization, which enhances the performance of queries involving different window functions over the same table. Consider a query that involves two window functions without dependencies: (1) Window 1 (w1) partitions rows by \"name\" and orders them by \"age\". (2) Window 2 (w2) partitions rows by \"age\" and orders them by \"age\". In traditional data processing systems, such queries are serially computed, even when the windows have no dependency relations. This sequential processing is inefficient and presents significant optimization opportunities. OpenMLDB parallelizes these window operations by introducing two additional node types into the execution plan: \"Concat Join\" and \"Simple Project\". The \"Concat Join\" node concatenates features computed for the same data rows across different windows, marking the end of a parallel optimization segment. The \"Simple Project\" node represents the target columns without additional computations, marking the start of a parallel optimization segment. Realizing parallel window computation is challenging because the order of data tuples can differ in windows using different partition keys (e.g., \"name\" for w1 and \"age\" for w2). This discrepancy makes it difficult to align and concatenate the results from parallelized windows based on their natural order. To overcome this, we introduce an index column to the input data, assigning a unique identifier to each data tuple. This index serves as a consistent join key across all window operations, ensuring correct alignment during concatenation regardless of the partitioning scheme."}, {"title": "Time-Aware Data Skew Resolving", "content": "The second optimization aims to resolve the data skew problem in distributed feature computation. In the distributed framework of OpenMLDB, when window computation encounters partition skew, and an imbalance is observed with excessive data in a particular partition, traditional data skew optimization methods like adding a random prefix to keys before repartitioning (known as \"salting\") [31, 64] cannot be applied. Randomly prefixing keys can assign data tuples with the same group key to different partitions, leading to out-of-order processing and even inaccurate results. To tackle this challenge, OpenMLDB introduces a data-aware parallel computation strategy that dynamically adjusts data partitions based on workload and data distribution with the following steps. Given the historical data, we first evaluate the overall data to understand the distribution of the partition keys (e.g., Gender). We calculate statistical metrics to determine appropriate partition boundaries: (1) Quantile determines how to split the data into n equal parts. For example, setting Quantile to 4 aims to divide the data into four partitions; (2) Percentile (PERCENTILE_i): Based on the Timestamp column (e.g., used in the ORDER BY clause), we calculate boundary values that define the ranges for each partition. Data where Timestamp falls within (PERCENTILE_i, PERCENTILE_i+1] belongs to the i-th partition. Note we adopt HyperLogLog [41] algorithm to approximate the percentile distribution to avoid full data scan. Based on the distribution analysis, we tag each data tuple with: (1) PART_ID: A new partition identifier that, along with the original partition key (e.g., Gender), determines the new partitions after repartitioning. (2) EXPANDED_ROW: A boolean flag indicating whether the row is an original data row (false) or an expanded window row (true). In this way, we can control the repartitioning process without altering the original keys, thus preserving the continuity required for accurate window computations. To ensure that window computations produce correct results after repartitioning (origin tuples in the same window may be of different partitions), we expand each partition's data by including additional data tuples from preceding partitions that are necessary for computing the window functions. The process involves: (1) Identify Required Data: For each partition (except the first), we identify data tuples from earlier partitions that fall within the window frame needed for computations in the current partition. (2) Mark Expanded Data: We set the EXPANDED_ROW flag to true for these additional data"}, {"title": "Compact Time-Series Data Management", "content": "OpenMLDB uses a compact row encoding format that efficiently stores various data types while reducing memory usage. The data format combines fixed-size and variable-size data structures (see Figure 5). Each row consists of four parts. (1) Header (6 bytes): The row begins with a header that encodes essential metadata, including field version, schema version, and total row size. With fewer than 64 versions, each version requires only one byte and a 32-bit value stores the row's size. (2) BitMap: After the header, the BitMap indicates which columns contain NULL values. Allocated in byte units, it marks NULL fields by setting specific bits, thus avoiding the direct storage of NULL values. Its size depends on the number of columns and the presence of NULLs. (3) Basic Type Data Fields: Next come the basic data types (e.g., integers, floats, timestamps), stored contiguously in memory at variable lengths. For integer data, OpenMLDB uses a more compact offset calculation approach, speeding up offset determination and reducing storage costs. (4) Variable-length Fields: Finally, variable-length fields (e.g., strings) are stored by their offsets rather than embedding actual values. The string length is the difference between its current and previous offsets, and OpenMLDB avoids unnecessary fixed-size (32-bit) length fields. Instead, it allocates precisely as much memory as the string requires, further optimizing space utilization. Moreover, we utilize JNI to invoke Java's UnsafeRow data format [35], which can directly read data Rows in C, access pointers,"}, {"title": "In-Memory Data Structure", "content": "For online feature computation, traditional data structures like hash tables in Redis [8] and balanced trees [33, 46] often encounter bottlenecks due to lock contention and rebalancing overhead when handling numerous simultaneous time-series insertions and complex feature computations. Redis's hash tables also perform periodic rehashing to expand or shrink the table size, which can introduce significant latency spikes and impede real-time performance. To address these challenges, we adopt the refined skiplist data structure [51], where nodes in the first layer are sorted by key values (e.g., item ID), and each key node in the second layer points to a linked list (or a secondary skiplist) that contains all data tuples associated with that key ordered by timestamps. We support lock-free reads and writes over the refined data skiplist (with multiple levels of linked lists that can perform insertions and deletions independently) by utilizing atomic operations for pointer updates. Specifically, when a thread inserts or deletes a node, it uses atomic compare-and-swap (CAS) instructions to update pointers, i.e., a memory location is updated only if it contains an expected value. For instance, during insertion, the thread identifies the position where the new node should be placed and attempts to update the next pointer of the preceding node using CAS. If another thread has modified the pointer in the meantime, the CAS operation fails, and the thread retries the operation. Managing outdated data efficiently is essential for maintaining optimal performance and ensuring that OpenMLDB does not process stale information that has surpassed its time-to-live (TTL). Here, we provide an efficient outdated data removal strategy: (1) Timestamp Ordering: By ordering nodes based on their timestamps, the skiplist allows quick identification of outdated tuples. Since the nodes are sorted, all outdated nodes are located contiguously at the beginning or end of the list, depending on the sorting order. (2) Batch Deletion: To remove all data before a specific timestamp T, we traverse the skiplist from the head node and delete nodes until it reaches a node with a timestamp \u2265 T."}, {"title": "On-Disk Data Structure", "content": "For online data requiring persistence, we adopt RocksDB as the on-disk storage engine, used in databases like Redis [8] and TiDB [44]. First, we assign multiple indexes when creating tables for columns frequently used in operations like order-by and partition-by. Each index corresponds to a Column Family in RocksDB. Different column families have separate Sorted String Table (SST) files and individual data eviction policies, but they share a single Memory Table (memtable). This means that while each Column Family manages its on-disk storage and data eviction independently, they all write to the same in-memory data structure before flushing data to disk. In our implementation, we utilize the refined data skiplist as the Memory Table, where a key and a timestamp (ts) are concatenated to form a composite key. The RocksDB keys are pre-sorted, so data with the same key is grouped together, facilitating data queries over a specified time range. We support data eviction in RocksDB by parsing the keys and timestamps in the skiplist and removing data whose timestamp is out-of-date."}, {"title": "Memory Management Mechanisms", "content": "OpenMLDB workloads often require substantial memory resources, particularly when dealing with large-scale feature computations. To guide users in choosing appropriately sized memory configurations, we provide an empirical memory estimation model. Inspired by established practices in feature computation [16], we use the following model to estimate total memory usage:\n$mem_{total} = \\sum_{i=1}^{n_{table}}  \\Big( n_{replica_i} \\times \\sum_{j=1}^{N_{index}} (N_{pk_{ij}} \\times (|pk_{ij}| + 156)) + K \\times N_{index_i} \\times N_{row_i} \\times C +  N_{index_i} \\times N_{row_i} \\times |row_i| \\Big)$\nwhere $n_{table}$ is the total number of tables, and $n_{index}$ is the number of indexes per table. For each table i, $n_{replica_i}$ is the number of replicas (full table data copies), $N_{pk_{ij}}$ is the number of unique primary keys on the jth index column, and $|pk_{ij}|$ is the average key column length. The parameters $N_{index_i}$ and $N_{row_i}$ denote the number of indexes and tuples in table i, respectively, while $|row_i|$ is the average tuple length. The factor C depends on the table type: set C = 70 for \"latest\" (storing recent data for a given key) or \"absorlat\" (with additional logic like combining recent entries) tables, and C = 74 for \"absolute\" (data keyed by a timestamp or an absolute reference) or \"absandlat\" (e.g., data accessible by absolute timestamps) tables. The parameter K, representing the number of data copies stored, varies between 1 and $n_{index_i}$. For instance, suppose we have a \"latest\" table with 1 million rows, average row length 300 bytes, two indexes, and replicas equal 2. Each primary key is 16 bytes. Using C = 70 and K = 1, the memory usage is about 1.568 GB. Note memory consumption by pre-aggregation are typically negligible compared to the main memory consumption and so is not reflected here. With the estimated memory usage, users can flexibly assign storage engines to tables in OpenMLDB. For instance, if the estimated memory usage for a table fits within available memory resources and ultra-low latency (around 10 milliseconds) is required, the in-memory storage engine is recommended. If memory resources are limited or the estimated memory usage exceeds available memory, and a latency of 20-30 milliseconds is acceptable, the disk-based"}, {"title": "Runtime Memory Management", "content": "Moreover, to ensure high stability and prevent memory exhaustion, which can lead to tablets being terminated by the operating system and result in service downtime, OpenMLDB introduces two key features: (1) Memory Resource Isolation provides a tablet-level configuration parameter (max_memory_mb) that limits the maximum memory usage of a tablet. When memory usage exceeds this limit, write operations fail but read operations continue, keeping the service online. Users can address memory shortages through scaling, shard migration, or other methods with minimal impact on production systems. (2) Memory Alerting Mechanism: An alerting mechanism notifies relevant personnel or systems when memory usage surpasses a preset threshold."}, {"title": "Evaluation", "content": "We evaluate OpenMLDB using a micro-benchmark (MicroBench) and various real-world production workloads. The micro-benchmark measures feature computation performance prior to deploying [4]. For real workloads, we consider the publicly available TalkingData dataset from Kaggle [23], as well as two production use cases at Akulaku [15]: an Item Ranking service (RTP) and a Geographical Location Querying service (GLQ). We implemented a Java-based testing tool employing the OpenMLDB Java SDK and Jedis to compare performance and memory consumption. Due to differences in supported data types and storage layouts, the data insertion procedure for each system was slightly adjusted. To reflect realistic usage, we designed time-series-based tests (with three stream tables) with multiple adjustable parameters (e.g., number of windows, number of LAST JOIN operations). The open TalkingData dataset (covering around 200 million clicks over four days [23]) contains common column types (e.g., strings, number values, timestamps). Since the dataset is time-series-based and many data tuples share the same ip key, we created a table indexed by the ip column, allowing us to calculate the memory usage savings by OpenMLDB and the baselines. The RTP service at Akulaku is an item-ranking framework with a data warehouse exceeding 1,000 TB. It supports various online machine learning models, such as Logistic Regression (LR) and Gradient Boosted Decision Trees (GBDT), as well as diverse feature formats. This framework powers multiple online ML tasks, including user risk ranking, device fingerprinting, and facial recognition. The GLQ workload consists of 10 billion geospatial tuples linked to GPS coordinates. It involves complex, full-scale geospatial queries (e.g., proximity searches, route optimization, clustering), which are extremely resource-intensive and can cause out-of-memory (OOM) errors in traditional systems. For MicroBench, we use four servers of identical configurations: each with 512GB of memory, dual Intel Xeon E5-2630 v4 CPUs (2.20GHz), a 1Gbps network card, and running CentOS-7. Three servers run the OpenMLDB service, and the fourth"}, {"title": "Overall Performance", "content": "We first compare OpenMLDB with three commonly used baselines for in-memory online analytics: (1) We pair Redis (a popular in-memory data store [8", "38": "to handle SQL queries over Redis data. (2) We test MySQL configured with the MEMORY storage engine to compute online features over the stored relational data [25", "52": "designed for online analytical processing. As shown in Figure 6, OpenMLDB achieves significant performance improvement in both latency and throughput compared to all the baselines on MicroBench. In terms of latency, OpenMLDB outperforms MySQL (in-mem) by over 68.4%, DuckDB by 87.7%, and Trino+Redis by over 96%, respectively. OpenMLDB also achieves throughput gains by over 17 times higher than the baselines. OpenMLDB employs a C++-based compilation framework that specializes window operations across multiple granularities (e.g., daily, weekly, and monthly) in a single integrated pass. OpenMLDB also uses LLVM-based Just-In-Time (JIT) compilation to fuse and streamline aggregate functions (e.g., sum and avg). Instead, MySQL (in-mem) relies heavily on interpreted SQL execution, and DuckDB does not incorporate aggressive cross-operation optimizations. Moreover"}]}