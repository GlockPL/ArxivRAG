{"title": "Exploiting Formal Concept Analysis for Data Modeling in Data Lakes", "authors": ["Anes Bendimerad", "Romain Mathonat", "Youcef Remil", "Mehdi Kaytoue"], "abstract": "Data lakes are widely used to store extensive and heteroge-neous datasets for advanced analytics. However, the unstructured nature of data in these repositories introduces complexities in exploiting them and extracting meaningful insights. This motivates the need of exploring efficient approaches for consolidating data lakes and deriving a common and unified schema. This paper introduces a practical data visualiza-tion and analysis approach rooted in Formal Concept Analysis (FCA) to systematically clean, organize, and design data structures within a data lake. We explore diverse data structures stored in our data lake at Infologic, including InfluxDB measurements and Elasticsearch indexes, aiming to derive conventions for a more accessible data model. Lever-aging FCA, we represent data structures as objects, analyze the con-cept lattice, and present two strategies-top-down and bottom-up-to unify these structures and establish a common schema. Our methodology yields significant results, enabling the identification of common concepts in the data structures, such as \"resources\" along with their underlying shared fields (timestamp, type, usedRatio, etc.). Moreover, the number of distinct data structure field names is reduced by 54% (from 190 to 88) in the studied subset of our data lake. We achieve a complete coverage of 80% of data structures with only 34 distinct field names, a significant improvement from the initial 121 field names that were needed to reach such coverage. The paper provides insights into the Infologic ecosystem, problem formulation, exploration strategies, and presents both qualita-tive and quantitative results. The source code and datasets of this work are made available: https://zenodo.org/records/10589722", "sections": [{"title": "1 Introduction", "content": "Organizations increasingly rely on data lakes [25] as versatile repositories to store vast and heterogeneous datasets for advanced analytics. The flexibility and scala-bility offered by data lakes have positioned them as a bedrock for managing mas-sive volumes of raw, unstructured, and heterogeneous data. As defined in [17], a data lake is a massive collection of datasets that: (1) may be hosted in different storage systems; (2) may vary in their formats; (3) may not be accompanied by any useful metadata or may use different formats to describe their metadata; and\n(4) may change autonomously over time. At Infologic [5], our data lake serves as a key component in the scope of our predictive maintenance system [9], enabling seamless aggregation of continuously collected data from diverse sources at a low cost. New data collections can be easily added to the data lake by different teams, without the need of defining a priori the data schema. Nevertheless, the unstructured and heterogeneous nature of the data stored in data lakes poses a significant challenge, hindering the full exploitation of their inherent value [17]. It becomes difficult to have a clear understanding of the content of the data and to implement data pipelines that generalize well. Furthermore, executing some common analytics operations between data structures, such as merges and joins, becomes cumbersome, as these structures may use different names for the same fields. Particularly at Infologic, we were using two storage systems, InfluxDB [4] and Elasticsearch [3], each adhering to distinct conventions. This combination of two storage systems were motivated by the effectiveness of InfluxDB in handling metrics and time series, while Elasticsearch is more efficient as a search engine, especially in textual data and JSON documents.\nWe aim to explore the data structures within our data lake, and extract a set of conventions to consolidate our data model. The data structures under exami-nation include the schemas of InfluxDB measurements and Elasticsearch indexes. Deriving a common schema is a problem that has interested many practitioners. Notably, the Elasticsearch community has proposed the ECS [2] (Elastic Com-mon Schema) to define a common set of fields to be used when storing events data in Elasticsearch. The Common Event Format [7] (CEF) has been designed to propose standard naming conventions for logs in network and security devices and computer systems. Inspired by these well-established standards, we seek to derive a tailored data model that not only aligns with industry practices, but also accommodates the specificities of our business at Infologic. Such specifici-ties concern the architecture that governs our Copilote ERP software, as well as conventions that are already followed in the Relational database that is used to store Copilote critical business data.\nThis paper addresses this challenge through a comprehensive exploration of a novel approach grounded in Formal Concept Analysis (FCA) [12, 27], aimed at systematically cleaning, structuring, and designing the data within our data lake. We perform interactive data analysis, leveraging the concept lattice as a central tool. FCA has been exploited to address various challenges in both soft-ware engineering and data engineering, such as mining functional dependencies for SQL data refractoring [8,15], creating and merging of ontology top-levels [13], and fault localization in software [10]. However, our paper represents the first attempt to employ the concept lattice as a visual tool for consolidating struc-tures in data lakes. We represent data structures, including tables, measure-ments, and indexes, as objects within a formal context. Each of these objects is described by Boolean attributes that indicate whether a field is present in the related data structure. Subsequently, we derive and analyze the concept lat-tice from this formal context. In our exploration of this lattice, we present two distinct strategies-top-down and bottom-up that leverage visual insights to"}, {"title": "2 Background", "content": "2.1 Infologic\nInfologic [5] is a leading provider of enterprise resource planning (ERP) solutions for the agri-food, health nutrition, and cosmetic sectors in France. Its flagship product, Copilote, is an ERP software designed to optimize and automate a large panel of business processes, including sales tracking, supply chain and cus-tomer relationship management. INFOLOGIC provides maintenance of the ERP instances and infrastructure in operation for its clients. As the proper functioning of their businesses depends heavily on the reliable performance and accessibility of the ERP, it is crucial to ensure high availability and excellent maintenance for Copilote. To this aim, Infologic has made substantial investments in a pre-dictive maintenance project [9, 19-23]. In [9], the architecture of this project\n2.2 Data lake\nDuring the period of our study, the primary components of our data lake com-prised InfluxDB [4] and Elasticsearch [3], serving as repositories for the contin-uous collection and storage of diverse datasets. In Figure 2, we present a subset of data structures contained in our data lake, which constitutes the focus of our analysis. This figure depicts our data as a hierarchy whose leaves represent specific data structures, such as Machine, Tomcat, Storage, among others. The internal nodes in the figure denote cohesive groups of structures belonging to distinct domains. For instance, Lucene forms a group encompassing four In-fluxDB tables (measurements) designed for storing Lucene monitoring data [16], including PausedIndex, WaitingDoc, Current Job, and IndexSize. Time series data were stored in InfluxDB, whereas Elasticsearch was dedicated to textual data and JSON documents. The examined subset comprises 32 data structures"}, {"title": "3 Problem Formulation", "content": "3.1 Data model with FCA\nWe formalize the dataset describing our data structures based on Formal Con-cept Analysis (FCA) [12, 27]. We define the formal context [12] as a triple K = (G,M,I) comprising two sets G and M and an incidence relation I be-tween G and M. Elements of G are called objects, and elements of M are called attributes. To signify that an object g\u2208 G has an attribute m \u2208 M, we use the notation gIm. The incidence relation I is visually depicted by crosses in the table, and it represents the fact that a data structure contains a field. Notably, some fields in data structures of Table 1 convey similar meanings but are designated by dis-tinct names, such as time and timestamp, or serviceName and name. The goal of our study is to analyze a comprehensive set of data structures, and identify groups of akin field names that manifest recurrently and signify the same un-derlying notion. Subsequently, we aim to generalize these field names uniformly, establishing a cohesive and unified schema.\nTwo fundamental operators, namely extent and intent, are defined on a for-mal context K = (G, M, I). The extent operator, denoted ext, associates to each subset of attributes B \u2286 M the set of objects g\u2208 G possessing all attributes in B, that is, ext(B) = {g \u2208 G | (\u2200m \u2208 B) gIm}. Dually, the intent operator, de-noted int, associates to each subset of objects A \u2286 G the set of attributes m \u2208 M shared among the objects in A, that is, int(A) = {m \u2208 M | (\u2200g \u2208 A) g Im}. It is noteworthy that, for B \u2286 M and A \u2286 G, the following relationships hold: ext(B) = \u2229m\u2208 B ext({m}) and int(A) = \u2229g\u2208 A int({g}). A key theorem in FCA (Proposition 10 in [12]) is:\nTheorem 1. The pair of functions (ext, int) form a Galois connection between the power set lattices (29,\u2286) and (2M,\u2286). That is, ext \u25cb int and int \u25cb ext are closure operators on (29, \u2286) and, (2M, \u2282) respectively.\n3.2 Concept lattice\nWe construct the concept lattice (B(K), \u2264) from our formal context K. Vari-ous tools can be used to visualize the concept lattice given any formal context stored in some specific format [24], such as a CSV file. In our study, Concept Explorer [1] was utilized for this purpose. Using such data visualization, our aim is to analyze concepts in order to derive relevant unifica-tion of fields and structures. These transformations result in the unified field names ascending in the direction of the top of the lattice. An alternative method"}, {"title": "4 Exploiting the Concept Lattice", "content": "4.1 Analyzing the initial lattice\nWe generate the concept lattice from the dataset depicted in Figure 2, com-prising 32 objects (data structures) and 191 attributes (field names). The root represents the formal concept that covers all the objects of the dataset. Since there is no field name that is present in all the data structures, the root node in Figure 5 has an empty intent. Below the root node, we observe a few nodes that sit in this second level of the lattice, such as the concept with the intent {instanceType, instanceCode, time}. This concept reflects a set of field names that are present in every data structure of InfluxDB, but they are not used in Elasticsearch. Other examples of concepts as children of the root include those with the intent {rank} and {user}. In Figure 4, we notice that nearly all objects (data structures) reside in the penultimate level. Notably, this lattice exhibits a small height and a large width due to the limited number of common field names between data structures, attributed to the absence of a standardized naming convention. Consequently, there is a limited number of internal formal concepts representing shared attributes. This lattice serves as the basis for our analysis, where we explore two distinct data analysis approaches outlined in this section: the top-down approach and the bottom-up approach.\n4.2 The top-down approach\nThis approach consists in starting from the root (the top-node) of the hierarchy and find akin field names that can be unified. In this section, we illustrate this top-down approach with two interesting findings that were made possible thanks to this data exploration.\nUnifying generic fields from InfluxDB and Elasticsearch. As depicted in Figure 6 (a), the concept lattice visualization provides a clear view of the root having two children covered by similar sets of fields. The first group, com-prising {typeInstance, codeInstance, @timestamp}, is uniformly present in all data structures stored in Elasticsearch. Here, typeInstance signifies the Copilote ERP instance type (production, test, development, deployment), instanceCode is a unique identifier for Copilote instances, and @timestamp denotes the cre-ation time of the collected event or metric. Simultaneously, the second group {instanceType, instanceCode, time} is utilized across all InfluxDB measure-ments. These two groups have been consolidated into the unified field names {timestamp, instanceType, instanceCode}.\nUnifying the user field. Another compelling application of the top-down approach involves the consolidation of the field describing the user identifier, as illustrated in Figure 7 (a). Initially, we observed distinct field names referencing the user identifier, namely usr, user, and username. Each of them is covered by several data structures, which makes them visible in the upper layers of the lattice. We have unified these identifiers under a standardized field name, user, as depicted in Figure 7 (b).\n4.3 The bottom-up approach\nAnother method to exploit concept lattice visualization is to inspect the bottom layers to discern a substantial set of distinct fields that are used in a relatively limited set of data structures but represent the same notion. Unlike the top-down approach, which unifies groups of fields already used in many data structures, the bottom-up approach has another focus. It targets the unification of fields generally employed in unique data structures and, consequently, found only in"}, {"title": "5 Results", "content": "In this section, we begin by reporting and analyzing the final lattice derived from the dataset after performing our data structure consolidation. We highlight the distinctions between this final lattice and the one generated from the original dataset. Then, we perform a quantitative study to analyze the distribution of field names, their coverage of the data structures, and measure the improvement achieved from this perspective.\n5.1 Final lattice\nFigure 11 illustrates the concept lattice generated from the final dataset after applying all the transformations related to the proposed data model. The illus-tration also includes the objects associated to each of the formal concepts rep-resented in the lattice. Some data structures have moved upward in the lattice, as they use only common fields without any specific field names. Three field names appear in the root of the lattice timestamp, instanceType, and instanceCode-as they are covered by all the data structures. In the second layer, we find generic fields such as user, type, duration, code, used. Notably, the concept covered by used represents resources whose consumption is measurable, and the field used is em-ployed to store the quantity of utilization of the related resource. For example, if the resource is the swap memory, used indicates the number of bytes con-sumed by the swap. Furthermore, the concept covered by used has a child that is extended with the fields max and usedRatio, to represent resources whose capacity is limited and known. Another interesting observation in the lattice is that, while duration sits in the second layer, the fields startTimestamp and endTimestamp are separated in other concepts that come lower in the lattice. A possible further consolidation of our model is to add the fields startTimestamp and endTimestamp to all the \"events\" data structures that are described by a duration. This would unify the three fields duration, startTimestamp and endTimestamp in a same formal concept. Following our data structure consoli-dation, the number of distinct field names has decreased from 190 to 88. However, the number of formal concepts has increased from 44 to 72, due to more sets of fields common between data structures. Consequently, the lattice height has increased from initially 4 to 6 after performing the data structure consolidation.\n5.2 Analyzing the number of attributes\nOur objective is to assess the extent of data structures coverage by the new fields names, and compare it with their coverage before applying our transfor-mations. To achieve this, we sort the field names in descending order based on the frequency of their utilization in data structures. Subsequently, we measure the proportion of completely covered data structures by a given number of top fields ranked with respect to their utilization frequency. the same number of field names covered less than 50% of the data structures. Moreover, we can cover the entire dataset with 88 field names instead of original 190 field names, reducing them by 54%."}, {"title": "6 Conclusion", "content": "In conclusion, our application of Formal Concept Analysis (FCA) to the explo-ration of data lake structures at Infologic has proven highly effective. By system-atically analyzing diverse data structures and leveraging FCA's concept lattice, we successfully reduced the number of distinct attributes by 54%, from 190 to 88, and covered 80% of data structures with only 34 distinct field names. This approach not only addresses specific challenges at Infologic, but also provides a valuable framework for organizations navigating the complexities of data lakes. We believe that concept lattices are effective visual tools that are accessible even to individuals who are not data analysis experts. They can read and understand the presented concepts and make informed decisions accordingly. Moving for-ward, the insights gained pave the way for a cleaner and a more exploitable data lake. Moreover, the resulting unified schema can serve as a model for our Electronic Data Interchange (EDI [18]) module, a generic Copilote component serving as an interface for the exchange of data between Copilote instances and other systems. As a part of future work, a promising avenue is to incorporate advanced tools to enhance and automate our manual data analysis methods used in this paper. Integrating Natural Language Processing (NLP) and graph min-ing techniques will empower us to identify groups of similar fields, enabling the mapping of disparate fields to a unified name. Additionally, leveraging analogy-based reasoning will assist in identifying corresponding fields across different tables. For example, the following analogy would make it possible to unify the fields a and b: \"the field a is to the table A as the field b is to the table \u0412\". Thanks to these techniques, our approach would be able to scale to larger data lakes with a higher number of data structures to consolidate. Another possibility to improve the scalability of our approach is to exploit AOC-posets [11], which are smaller and more concise subsets of concept lattices."}]}