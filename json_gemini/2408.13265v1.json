{"title": "Exploiting Formal Concept Analysis for Data\nModeling in Data Lakes", "authors": ["Anes Bendimerad", "Romain Mathonat", "Youcef Remil", "Mehdi Kaytoue"], "abstract": "Data lakes are widely used to store extensive and heteroge-\nneous datasets for advanced analytics. However, the unstructured nature\nof data in these repositories introduces complexities in exploiting them\nand extracting meaningful insights. This motivates the need of exploring\nefficient approaches for consolidating data lakes and deriving a common\nand unified schema. This paper introduces a practical data visualiza-\ntion and analysis approach rooted in Formal Concept Analysis (FCA)\nto systematically clean, organize, and design data structures within a\ndata lake. We explore diverse data structures stored in our data lake at\nInfologic, including InfluxDB measurements and Elasticsearch indexes,\naiming to derive conventions for a more accessible data model. Lever-\naging FCA, we represent data structures as objects, analyze the con-\ncept lattice, and present two strategies-top-down and bottom-up-to\nunify these structures and establish a common schema. Our methodology\nyields significant results, enabling the identification of common concepts\nin the data structures, such as \"resources\" along with their underlying\nshared fields (timestamp, type, usedRatio, etc.). Moreover, the number\nof distinct data structure field names is reduced by 54% (from 190 to 88)\nin the studied subset of our data lake. We achieve a complete coverage\nof 80% of data structures with only 34 distinct field names, a significant\nimprovement from the initial 121 field names that were needed to reach\nsuch coverage. The paper provides insights into the Infologic ecosystem,\nproblem formulation, exploration strategies, and presents both qualita-\ntive and quantitative results. The source code and datasets of this work\nare made available: https://zenodo.org/records/10589722", "sections": [{"title": "1 Introduction", "content": "Organizations increasingly rely on data lakes [25] as versatile repositories to store\nvast and heterogeneous datasets for advanced analytics. The flexibility and scala-\nbility offered by data lakes have positioned them as a bedrock for managing mas-\nsive volumes of raw, unstructured, and heterogeneous data. As defined in [17], a\ndata lake is a massive collection of datasets that: (1) may be hosted in different\nstorage systems; (2) may vary in their formats; (3) may not be accompanied by\nany useful metadata or may use different formats to describe their metadata; and\n(4) may change autonomously over time. At Infologic [5], our data lake serves as\na key component in the scope of our predictive maintenance system [9], enabling\nseamless aggregation of continuously collected data from diverse sources at a\nlow cost. New data collections can be easily added to the data lake by different\nteams, without the need of defining a priori the data schema. Nevertheless, the\nunstructured and heterogeneous nature of the data stored in data lakes poses a\nsignificant challenge, hindering the full exploitation of their inherent value [17].\nIt becomes difficult to have a clear understanding of the content of the data and\nto implement data pipelines that generalize well. Furthermore, executing some\ncommon analytics operations between data structures, such as merges and joins,\nbecomes cumbersome, as these structures may use different names for the same\nfields. Particularly at Infologic, we were using two storage systems, InfluxDB [4]\nand Elasticsearch [3], each adhering to distinct conventions. This combination of\ntwo storage systems were motivated by the effectiveness of InfluxDB in handling\nmetrics and time series, while Elasticsearch is more efficient as a search engine,\nespecially in textual data and JSON documents.\nWe aim to explore the data structures within our data lake, and extract a set\nof conventions to consolidate our data model. The data structures under exami-\nnation include the schemas of InfluxDB measurements and Elasticsearch indexes.\nDeriving a common schema is a problem that has interested many practitioners.\nNotably, the Elasticsearch community has proposed the ECS [2] (Elastic Com-\nmon Schema) to define a common set of fields to be used when storing events\ndata in Elasticsearch. The Common Event Format [7] (CEF) has been designed\nto propose standard naming conventions for logs in network and security devices\nand computer systems. Inspired by these well-established standards, we seek to\nderive a tailored data model that not only aligns with industry practices, but\nalso accommodates the specificities of our business at Infologic. Such specifici-\nties concern the architecture that governs our Copilote ERP software, as well as\nconventions that are already followed in the Relational database that is used to\nstore Copilote critical business data.\nThis paper addresses this challenge through a comprehensive exploration of\na novel approach grounded in Formal Concept Analysis (FCA) [12, 27], aimed\nat systematically cleaning, structuring, and designing the data within our data\nlake. We perform interactive data analysis, leveraging the concept lattice as a\ncentral tool. FCA has been exploited to address various challenges in both soft-\nware engineering and data engineering, such as mining functional dependencies\nfor SQL data refractoring [8,15], creating and merging of ontology top-levels [13],\nand fault localization in software [10]. However, our paper represents the first\nattempt to employ the concept lattice as a visual tool for consolidating struc-\ntures in data lakes. We represent data structures, including tables, measure-\nments, and indexes, as objects within a formal context. Each of these objects\nis described by Boolean attributes that indicate whether a field is present in\nthe related data structure. Subsequently, we derive and analyze the concept lat-\ntice from this formal context. In our exploration of this lattice, we present two\ndistinct strategies-top-down and bottom-up that leverage visual insights to"}, {"title": "2 Background", "content": "2.1 Infologic\nInfologic [5] is a leading provider of enterprise resource planning (ERP) solutions\nfor the agri-food, health nutrition, and cosmetic sectors in France. Its flagship\nproduct, Copilote, is an ERP software designed to optimize and automate a\nlarge panel of business processes, including sales tracking, supply chain and cus-\ntomer relationship management. INFOLOGIC provides maintenance of the ERP\ninstances and infrastructure in operation for its clients. As the proper functioning\nof their businesses depends heavily on the reliable performance and accessibility\nof the ERP, it is crucial to ensure high availability and excellent maintenance\nfor Copilote. To this aim, Infologic has made substantial investments in a pre-\ndictive maintenance project [9, 19-23]. In [9], the architecture of this project"}, {"title": "2.2 Data lake", "content": "During the period of our study, the primary components of our data lake com-\nprised InfluxDB [4] and Elasticsearch [3], serving as repositories for the contin-\nuous collection and storage of diverse datasets. In Figure 2, we present a subset\nof data structures contained in our data lake, which constitutes the focus of\nour analysis. This figure depicts our data as a hierarchy whose leaves represent\nspecific data structures, such as Machine, Tomcat, Storage, among others. The\ninternal nodes in the figure denote cohesive groups of structures belonging to\ndistinct domains. For instance, Lucene forms a group encompassing four In-\nfluxDB tables (measurements) designed for storing Lucene monitoring data [16],\nincluding PausedIndex, WaitingDoc, CurrentJob, and IndexSize. Time series\ndata were stored in InfluxDB, whereas Elasticsearch was dedicated to textual\ndata and JSON documents. The examined subset comprises 32 data structures"}, {"title": "3 Problem Formulation", "content": "3.1 Data model with FCA\nWe formalize the dataset describing our data structures based on Formal Con-\ncept Analysis (FCA) [12, 27]. We define the formal context [12] as a triple\nK = (G,M,I) comprising two sets G and M and an incidence relation I be-\ntween G and M. Elements of G are called objects, and elements of M are called\nattributes. To signify that an object $g\\in G$ has an attribute $m \\in M$, we use the\nnotation gIm. Table 1 reports a formal context (G, M, I) where objects in G\nrepresent data structures from the data lake, and attributes in M denote fields\nwithin these data structures. The incidence relation I is visually depicted by\ncrosses in the table, and it represents the fact that a data structure contains a\nfield. For instance, we have \u201cStorage I used\" that can be read as: \"the Storage\ndata structure contains the field used\". In total, the data structure Storage is\ncharacterized by the following fields: time, used, max, path. Notably, some fields\nin data structures of Table 1 convey similar meanings but are designated by dis-\ntinct names, such as time and timestamp, or serviceName and name. The goal\nof our study is to analyze a comprehensive set of data structures, and identify\ngroups of akin field names that manifest recurrently and signify the same un-\nderlying notion. Subsequently, we aim to generalize these field names uniformly,\nestablishing a cohesive and unified schema.\nTwo fundamental operators, namely extent and intent, are defined on a for-\nmal context K = (G, M, I). The extent operator, denoted ext, associates to each\nsubset of attributes $B \\subseteq M$ the set of objects $g\\in G$ possessing all attributes in\nB, that is, $ext(B) = \\{g \\in G | (\\forall m \\in B) gIm\\}$. Dually, the intent operator, de-\nnoted int, associates to each subset of objects $A\\subseteq G$ the set of attributes $m \\in M$\nshared among the objects in A, that is, $int(A) = \\{m \\in M | (\\forall g \\in A) g Im\\}$. It\nis noteworthy that, for $B \\subseteq M$ and $A \\subseteq G$, the following relationships hold:\n$ext(B) = \\bigcap_{m\\in B} ext(\\{m\\})$ and $int(A) = \\bigcap_{g\\in A} int(\\{g\\})$. A key theorem in FCA\n(Proposition 10 in [12]) is:\nTheorem 1. The pair of functions (ext, int) form a Galois connection between\nthe power set lattices $(2^G,\\subseteq)$ and $(2^M,\\subseteq)$. That is, ext \u25cb int and int \u25cb ext are\nclosure operators on $(2^G,\\subseteq)$ and, $(2^M, \\subset)$ respectively."}, {"title": "3.2 Concept lattice", "content": "We construct the concept lattice (B(K), \u2264) from our formal context K. Vari-\nous tools can be used to visualize the concept lattice given any formal context\nstored in some specific format [24], such as a CSV file. In our study, Concept\nExplorer [1] was utilized for this purpose. Figure 3 (a) shows the concept lattice\ngenerated from the toy dataset in Table 1. Each node of this lattice represents\na formal concept $(A, B) \\in 2^G \\times 2^M$ such that $A = ext(B)$ and $B = int(A)$.\nFor example, the right child of the root corresponds to the formal concept\n(\\{Storage, DBTablespace\\}, \\{time, used, max\\}). Subsequently, the right child of\nthe latter formal concept is (\\{Storage\\}, \\{time, used, max, path\\}), which is a for-\nmal concept covering only the object (data structure) Storage. Using such data\nvisualization, our aim is to analyze concepts in order to derive relevant unifica-\ntion of fields and structures. In the toy dataset, we can unify the field names\ntime and timestamp, renaming both as time. We can also consolidate the field\nnames serviceName, name, and path into a unified label, such as name. These\ntransformations result in the unified field names ascending in the direction of\nthe top of the lattice. Figure 3 (b) illustrates the final lattice after applying\nthese transformations on the toy formal context of Table 1. The two fields name\nand time have ascended to the root of the lattice since they are covered by all\nthe objects Storage, DBTablespace, and ServiceCall. An alternative method"}, {"title": "4 Exploiting the Concept Lattice", "content": "4.1 Analyzing the initial lattice\nWe generate the concept lattice from the dataset depicted in Figure 2, com-\nprising 32 objects (data structures) and 191 attributes (field names). Figure 4\npresents the complete lattice, highlighting the objects associated with each for-\nmal concept, while Figure 5 displays the attributes within each concept. The\nroot represents the formal concept that covers all the objects of the dataset.\nSince there is no field name that is present in all the data structures, the root\nnode in Figure 5 has an empty intent. Below the root node, we observe a few\nnodes that sit in this second level of the lattice, such as the concept with the\nintent \\{instanceType, instanceCode, time\\}. This concept reflects a set of field\nnames that are present in every data structure of InfluxDB, but they are not\nused in Elasticsearch. Other examples of concepts as children of the root include\nthose with the intent \\{rank\\} and \\{user\\}. In Figure 4, we notice that nearly all\nobjects (data structures) reside in the penultimate level. Notably, this lattice\nexhibits a small height and a large width due to the limited number of common\nfield names between data structures, attributed to the absence of a standardized\nnaming convention. Consequently, there is a limited number of internal formal\nconcepts representing shared attributes. This lattice serves as the basis for our\nanalysis, where we explore two distinct data analysis approaches outlined in this\nsection: the top-down approach and the bottom-up approach."}, {"title": "4.2 The top-down approach", "content": "This approach consists in starting from the root (the top-node) of the hierarchy\nand find akin field names that can be unified. In this section, we illustrate this\ntop-down approach with two interesting findings that were made possible thanks\nto this data exploration.\nUnifying generic fields from InfluxDB and Elasticsearch. As depicted\nin Figure 6 (a), the concept lattice visualization provides a clear view of the\nroot having two children covered by similar sets of fields. The first group, com-\nprising \\{typeInstance, codeInstance, @timestamp\\}, is uniformly present in all\ndata structures stored in Elasticsearch. Here, typeInstance signifies the Copilote\nERP instance type (production, test, development, deployment), instanceCode\nis a unique identifier for Copilote instances, and @timestamp denotes the cre-\nation time of the collected event or metric. Simultaneously, the second group\n\\{instanceType, instanceCode, time\\} is utilized across all InfluxDB measure-\nments. These two groups have been consolidated into the unified field names\n\\{timestamp, instanceType, instanceCode\\}. As shown in Figure 6 (b), this group"}, {"title": "4.3 The bottom-up approach", "content": "Another method to exploit concept lattice visualization is to inspect the bottom\nlayers to discern a substantial set of distinct fields that are used in a relatively\nlimited set of data structures but represent the same notion. Unlike the top-down\napproach, which unifies groups of fields already used in many data structures,\nthe bottom-up approach has another focus. It targets the unification of fields\ngenerally employed in unique data structures and, consequently, found only in"}, {"title": "5 Results", "content": "In this section, we begin by reporting and analyzing the final lattice derived from\nthe dataset after performing our data structure consolidation. We highlight the\ndistinctions between this final lattice and the one generated from the original\ndataset. Then, we perform a quantitative study to analyze the distribution of\nfield names, their coverage of the data structures, and measure the improvement\nachieved from this perspective."}, {"title": "5.1 Final lattice", "content": "Figure 11 illustrates the concept lattice generated from the final dataset after\napplying all the transformations related to the proposed data model. The illus-\ntration also includes the objects associated to each of the formal concepts rep-\nresented in the lattice. Some data structures have moved upward in the lattice,\nas they use only common fields without any specific field names. For example,\nResource emerges as one of the most generic data structures, positioned at one of\nthe initial layers of the lattice. Figure 12 presents the same lattice but with a view\non field names (attributes of the formal context). Three field names appear in\nthe root of the lattice timestamp, instanceType, and instanceCode-as they\nare covered by all the data structures. In the second layer, we find generic fields\nsuch as user, type, duration, code, used. Notably, the concept covered by used\nrepresents resources whose consumption is measurable, and the field used is em-\nployed to store the quantity of utilization of the related resource. For example,\nif the resource is the swap memory, used indicates the number of bytes con-\nsumed by the swap. Furthermore, the concept covered by used has a child that\nis extended with the fields max and usedRatio, to represent resources whose\ncapacity is limited and known. Another interesting observation in the lattice\nis that, while duration sits in the second layer, the fields startTimestamp and\nendTimestamp are separated in other concepts that come lower in the lattice. A\npossible further consolidation of our model is to add the fields startTimestamp\nand endTimestamp to all the \"events\" data structures that are described by\na duration. This would unify the three fields duration, startTimestamp and\nendTimestamp in a same formal concept. Following our data structure consoli-"}, {"title": "5.2 Analyzing the number of attributes", "content": "Our objective is to assess the extent of data structures coverage by the new\nfields names, and compare it with their coverage before applying our transfor-\nmations. To achieve this, we sort the field names in descending order based on\nthe frequency of their utilization in data structures. Subsequently, we measure\nthe proportion of completely covered data structures by a given number of top\nfields ranked with respect to their utilization frequency. Figure 13 reports the\nresults. In the final context, we achieve a coverage of 75% of data structures\nwith 25 field names, marking a significant improvement compared to the initial"}, {"title": "6 Conclusion", "content": "In conclusion, our application of Formal Concept Analysis (FCA) to the explo-\nration of data lake structures at Infologic has proven highly effective. By system-\natically analyzing diverse data structures and leveraging FCA's concept lattice,\nwe successfully reduced the number of distinct attributes by 54%, from 190 to\n88, and covered 80% of data structures with only 34 distinct field names. This\napproach not only addresses specific challenges at Infologic, but also provides a\nvaluable framework for organizations navigating the complexities of data lakes.\nWe believe that concept lattices are effective visual tools that are accessible even\nto individuals who are not data analysis experts. They can read and understand\nthe presented concepts and make informed decisions accordingly. Moving for-\nward, the insights gained pave the way for a cleaner and a more exploitable\ndata lake. Moreover, the resulting unified schema can serve as a model for our\nElectronic Data Interchange (EDI [18]) module, a generic Copilote component\nserving as an interface for the exchange of data between Copilote instances and\nother systems. As a part of future work, a promising avenue is to incorporate\nadvanced tools to enhance and automate our manual data analysis methods used\nin this paper. Integrating Natural Language Processing (NLP) and graph min-\ning techniques will empower us to identify groups of similar fields, enabling the\nmapping of disparate fields to a unified name. Additionally, leveraging analogy-\nbased reasoning will assist in identifying corresponding fields across different\ntables. For example, the following analogy would make it possible to unify the\nfields a and b: \"the field a is to the table A as the field b is to the table \u0412\".\nThanks to these techniques, our approach would be able to scale to larger data\nlakes with a higher number of data structures to consolidate. Another possibility\nto improve the scalability of our approach is to exploit AOC-posets [11], which\nare smaller and more concise subsets of concept lattices."}]}