{"title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference", "authors": ["Xing Li", "Zeyu Xing", "Yiming Li", "Linping Qu", "Hui-Ling Zhen", "Wulong Liu", "Yiwu Yao", "Sinno Jialin Pan", "Mingxuan Yuan"], "abstract": "KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we thoroughly analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 38.3% compared with KV8 quantization over various context lengths.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) and multi-modality large models can comprehend and generate text, audio, image, and video like humans, showing the strong capability of assisting and interacting with humans. LLM inference efficiency such as throughput and latency is critical to enhance user experience and reduce cost. To improve the inference efficiency of LLMs, previously processed KV tokens are cached to avoid redundant recomputation. However, the memory usage of the KV cache linearly grows with the number of batch size and sequence length, so the KV cache becomes the new bottleneck of LLM serving systems with large batching requests and long context. Valuable long context generation applications include multi-turn dialogues, long document understanding,"}, {"title": "Background", "content": ""}, {"title": "Transformer and KV Cache", "content": "In LLMs, there are multiple intermediate transformer layers stacked and executed to generate final output responses. For the $l$-th transformer layer, given $i$-th D-dimensional input hidden state $x_i^l \\in \\mathbb{R}^D$, the $l$-th query, key, and value feedforward neural network layers generate $q_i^l = W_q^l x_i^l$, $k_i^l = W_k^l x_i^l$ and $v_i^l = W_v^l x_i^l$ with the corresponding weight matrices $W_q^l, W_k^l,$ and $W_v^l$, respectively. Then the self-attention scores $a_i^l$ are computed with the current query embedding and all key embeddings until the $i$-th step. Finally, the $l$-th self-attention layer generates the output state $o_i^l$, which is forwarded to downstream sub-layers in the $l$-th transformer layer, with the softly weighted value embeddings $V^l$ using the attention scores $a_i^l$:\n\n$a_i^l = \\text{softmax} \\left(\\frac{q_i^l K_i^l}{\\sqrt{D}}\\right), o_i^l = a_i^l V^l,$\n\nwhere $K^l = \\text{concat}(K^{l-1}, k_i^l)$ and $V^l = \\text{concat}(V^{l-1}, v_i^l)$ are the key and value embeddings generated in the prefilling and decoding stage in $l$-th transformer layer until $i$-th step. They will still be re-used in subsequent generation steps for self-attention computation. Therefore, we need to store them as KV cache in each layer independently to remove the additional computational cost of KV cache re-computation."}, {"title": "KV Cache Quantization", "content": "Although storing KV cache can reduce the re-computation cost, the KV cache may become the new inference memory and latency bottleneck in the large batch size and long context scenario. KV cache quantization can effectively address these problems. The round-to-nearest B-bit quantization and dequantization along the channel or token dimension to input $X \\in \\mathbb{R}^{S \\times D}$ are defined as\n\n$\\begin{aligned} Q(X) &= \\text{round} \\left( \\frac{X - z}{s} \\right), \\\n\\hat{X} &= Q(X) \\cdot s + z, \\end{aligned}\n\nwhere the offset $z = \\min(X)$ and the scale $s = \\frac{\\max(X) - \\min(X)}{2^{B-1}}$. We measure the relative KV cache and attention output errors and the absolute attention score error as $e_k = \\text{mean} \\left( \\frac{\\left| K' - \\hat{K} \\right|}{K'} \\right)$, $e_v = \\text{mean} \\left( \\frac{\\left| V' - \\hat{V} \\right|}{V'} \\right)$, $e_a = \\text{mean}(|a' - \\hat{a}|)$, and $e_o = \\text{mean}(|o' - \\hat{o}|)$, where the attention score with dequantized key cache $\\hat{a}' = \\text{softmax} \\left( q(\\hat{K}')^T \\right)$ and the attention output with dequantized KV cache $\\hat{o}' = \\hat{a}'V'$."}, {"title": "Observation", "content": ""}, {"title": "Error Accumulation", "content": "Due to the sequential nature of LLMs along both the model layer and token sequence dimensions, the previous layer output with KV cache quantization errors is the input of the current layer and the previous step model output token with errors is the input of the input and subsequent transformer layers. Therefore, KV cache quantization leads to two-dimensional error accumulation. The error in the $l$-th layer and $i$-th token $e_i^{1:L}$ depends on previous $1 \\sim l - 1$ layers and $1 \\sim i - 1$ steps, as defined in\n\n$e_i^{1:L} = f_e(e_i^{(l-1)}, ..., e_{i-1}^{1:L})$."}, {"title": "Sensitivity to Quantization Mode and Precision", "content": "KV cache quantization errors strongly correlate with the quantization mode and precision as in Table 4. In terms of relative key error $e_k$, the per-channel-asym quantization mode consistently outperforms the per-token-asym counterpart under the same precision for key cache, because key cache has strong channel-wise outliers Liu et al. (2024f); Hooper et al. (2024), more detailed experiment results can be found in Table 8. Therefore, for specific KV cache, the quantization mode modification may lead to the shift of importance of key and value to attention output errors. As shown in Table 4, the Pareto-optimal intra-layer KV cache quantization precision pairs significantly differ between these two modes. Therefore, the KV cache precision pairs need to be adapted to quantization modes. More detailed experimental settings and results are available in Appendix A.2 and A.4 due to space limitations."}, {"title": "Why Key Cache Is More Important?", "content": "We discover the diverse model and transformer layer sensitivity to KV cache quantization mode and pairs, which is mainly caused by attention distribution shift as in Figure 1. In this section, we thus analyze the reason why key cache is normally more important than value cache from both the empirical and theoretical perspectives."}, {"title": "Intermediate attention errors.", "content": "Following the settings in Table 8, we visualize the simulated layer-wise attention score errors of Llama-3.1-8B-Instruct with the per-token-asym KV cache quantization mode in Figure 2. More results of diverse LLMs and datasets are available in Appendix A.9. Decreasing the key cache quantization precision from 8-bit to 4-bit and from 4-bit to 2-bit leads to 13.9\u00d7 and 4.6\u00d7 average attention score error degradation in Figure 2, respectively. It may result in attention distribution shift in the token levels of specific sensitive heads as in Figure 1 and thus degrade the final accuracy. A similar phenomenon occurs in the final output token probability when implementing KV cache eviction Adnan et al. (2024)."}, {"title": "KV Quantization Errors vs Attention Patterns", "content": "As shown in Figure 3, heads with high KV cache quantization errors typically exhibit non-sparse attention patterns. The sparsity patterns of the attention heads are correlated with the head-wise and layer-wise sensitivity to KV cache quantization, Highly sparse streaming heads are generally more robust to KV cache quantization than retrieval heads. The proof of Lemma 1 is available in Appendix A.1."}, {"title": "Layer-wise Sensitivity to KV Cache Quantization", "content": "According to the layer-wise attention score and relative output errors of different prompts and KV cache quantization precision pairs of Llama-3.1-8B-Instruct in Figure 2 and 13, transformer layers sensitive to KV cache quantization remain consistent across different input prompts. The observed shifts in layer-wise error distribution primarily stem from variations in key cache quantization precision. Both Qwen2.5-7B-Instruct and Mistral-7B-Instruct-v0.3 exhibit similar behavioral patterns in this respect. Further analysis results can be found in Appendix A.9. We can thus conclude that layer-wise sensitivity to KV cache quantization is an inherent characteristic of LLMs.\nKV cache quantization errors are accumulated over both the model layer and generation sequence dimensions, and the sensitive layer will further amplify errors and lead to dramatic model performance degradation. We can perform an offline search to identify the optimal coarse-grained KV cache quantization configuration, determining the most effective precision pairs for each layer, particularly for sensitive layers, to achieve a balance between memory reduction and generation efficiency without incurring any overhead during online inference."}, {"title": "Method", "content": "KVTuner is an adaptive tuning framework for hardware-friendly mixed-precision KV cache quantization. It optimizes layer-wise KV precision pairs by considering their inherent sensitivity properties, aiming to achieve a better trade-off between inference efficiency and model accuracy.\nInstead of making online decisions about fine-grained token or page-level KV cache quantization precision for improved model accuracy, we conduct offline search to identify the Pareto-optimal quantization precision settings for coarse-grained KV cache in each transformer layer using multi-objective optimization algorithms. Here, we refer to the entire low-bit KV cache being quantized with a specific precision pair, such as K8V4 or K4V2. This approach ensures that no additional overhead is introduced during dynamic quantization and online inference. Due to the flexibility introduced by layer-wise KV cache quantization precision tuning, KVTuner is able to accommodate more hardware and accuracy constraints of different deployed LLMs compared to uniform 8-bit or even lower precision quantization. Moreover, KVTuner accelerates LLM inference and reduces memory footprint, while still maintaining lossless or slightly lossy final model generation."}, {"title": "Problem Formulation", "content": "The offline layer-wise KV precision pair tuning problem can be formulated as a discrete combinatorial optimization task, considering hardware limitations and accuracy loss constraints. It can be solved using multi-objective optimization algorithms. We aim to minimize the quantized KV cache memory usage across all transformer layers while minimizing the final model accuracy loss, subject to the maximum M memory and \u0394A accuracy loss constraints:\n\n$\\min_{P} (f_m(P), f_a(P)) \\text{ s.t. } f_m(P) \\leq M, f_a(P) \\leq \\Delta A,$\n\nwhere the search space $P \\in S^L$ is the KV cache precision pairs in $L$ layers. The layer-wise search space $S$ is defined as the KV cache precision pair $(P_k^l, P_v^l)$ in the $l$-th layer. $f_m(P) = \\sum_{l=1}^{L} size(P_k^l)$ captures the average equivalent quantization bits of all KV cache, $f_a(P) = A_{LLM}(KV_{half}) - A_{LLM}(KV_P)$ measures the final LLM accuracy loss with the KV precision as P compared with LLM inference using 16-bit half precision KV cache. For instance, we can limit the average KV cache quantization precision to 2.5-bit, while optimizing the equivalent quantization precision and inference accuracy."}, {"title": "Framework", "content": "To reduce the overhead of online fine-grained KV cache mixed-precision quantization tuning, we propose offline calibration of the optimal coarse-grained KV cache quantization precision pairs for each layer or head using multi-objective optimization algorithms Akiba et al. (2019); Zhang & Li (2007). These pre-calibrated settings are then directly applied during online quantization. The efficiency of offline calibration is crucial for practical applications due to the large combinatorial search space of KV cache quantization pairs across multiple transformer layers. Therefore, as demonstrated in Figure 4, we propose the intra-layer and inter-layer search space pruning algorithms to accelerate the search process while preserving optimization opportunities. After the efficient preprocessing, the final LLM inference accuracy is utilized to search the Pareto optimal layer-wise KV precision pairs P capturing complex dependencies of the nonlinear error accumulation."}, {"title": "Automatic KV Cache Quantization Precision Pair Search", "content": "As analyzed in Section 4.2 and 4.3, the model-wise and layer-wise sensitivity to KV cache quantization mode and precision is the inherent model property and is independent of the input prompts. Therefore, we can search for the optimal layer-wise KV cache quantization precision pairs offline to eliminate the additional online decision-making overhead with high generalization. If the candidate layer-wise"}, {"title": "Intra-layer KV Cache Quantization Precision Pair Pruning", "content": "KV cache quantization errors in each layer accumulate across both the model layers and generation token dimensions. Therefore, we must control the layer-wise error by pruning KV cache quantization pairs to limit the final model error. For all candidate KV cache quantization pairs in each layer, we prune those that are not part of the Pareto frontier, considering both the equivalent KV cache quantization precision and the relative attention output errors. For example, the precision pairs KV8, K8V4, KV4, K4V2, and KV2 are Pareto efficient for most layers in Llama-3.1-8B-Instruct in Figure 13, except for the 0-th layer, where K4V8 results in smaller errors than K8V4."}, {"title": "Inter-layer Clustering", "content": "Although the above intra-layer pruning already significantly reduces the search space to $S_p^L$ such as $5^{32} \\approx 2.3 \\times 10^{22}$ in Llama-3.1-8B-Instruct, it is still too computationally costly for searching. Therefore, we further propose the inter-layer clustering algorithm based on relative attention output errors and the pruned candidate KV quantization pairs to $S_p^G$ such as $5^6 = 15625$. The initial step involves partitioning layers based on distinct candidate sets of pruned KV cache quantization precision pairs. These candidate sets serve as indicators of how individual layers respond differently to specific KV cache quantization precision configurations. The subsequent step involves clustering layers that share the same candidate set, using quantization sensitivity as the clustering metric. This sensitivity is quantified with the relative attention output errors produced by the pruned precision pairs."}, {"title": "Calibration Dataset Design", "content": "To effectively evaluate different quantization settings, we develop an approach that amplifies KV cache quantization error accumulation and distinguishes the performance of KV precision pairs during the calibration process. This approach utilizes dequantized KV cache for self-attention computation during the prefilling stage, enabling error accumulation across model layers. Furthermore, we utilize long-context generation and challenging calibration datasets such as mathematical reasoning. In these tasks, minor errors propagating in decoding steps may result in intermediate generation token flipping and substantial mistakes in final answers as demonstrated by Table 1."}, {"title": "Experimental results", "content": "The detailed experimental settings are available in Section A.3. The intra-layer and inter-layer KV precision pairs pruning results of various LLMs are available in Appendix A.4. The proposed pruning algorithm can significantly reduce the search space to $S_p^G$ and speedup convergence of MOO search. The final model accuracy on mathematical reasoning datasets and the throughput improvement validate the effectiveness of KVTuner."}, {"title": "Pareto-optimal KV Cache Precision Pair Search", "content": "KIVI. The mixed precision KIVI quantization mode can maintain high accuracy. As shown in Figure 5a, KVTuner with KIVI effectively maintains Llama-3.1-8B-Instruct performance while reducing the equivalent quantization precision to 3.06-bit. In addition, KVTuner also finds out four settings including lower-precision 4.91-bit in the Pareto frontier whose memory usage and accuracy are better than KV8. Most sampled settings are close to the Pareto frontier, indicating that Llama-3.1-8B- Instruct is more robust to low-precision KV quantization. These demonstrate that KVTuner increases the flexibility of KV cache quantization and can achieve lower precision and even better precision than uniform KV precision."}, {"title": "LLM Final Mathematical Generation Accuracy", "content": "Apart the in-context few-shot GSM8K datasets, we also utilize them as the internal reasoning steps in a multi-turn way to imitate Openai ol like reasoning systems in Table 5. KIVI-2 and KIVI-4 result in dramatic accuracy loss in Qwen2.5-{3B, 7B}-Instruct due to their high sensitivity to low-precision KV quantization. KVTuner with KIVI can nearly losslessly quantizate KV cache to 3.25-bit, 3.17-bit, and 5.96-bit of the three models, respectively, further reducing the memory footprint compared with KIVI-4 and KIVI-8. In addition, we find out an interesting observation: KVTuner enables longer context and lower KV precision for better CoT and multi-turn mathematical reasoning accuracy than"}, {"title": "Throughput", "content": "We measure the maximum throughput and the corresponding batch size under specific input prompt length with the implementation of the KIVI GPU kernel. The Llama2-7B model and the output length 128 are used to align with KIVI. As shown in Table 7, for context length ranging from 512 to 4096, the high accuracy mode KVTuner-C6 can achieve 26.7% ~ 38.3% throughput improvement and the high efficiency KVTuner-C3 can achieve 58.3% ~ 76.4% throughput improvement than KV8."}, {"title": "Detailed Analysis", "content": "By analyzing the detailed configurations in the Pareto frontier identified for Llama-3.1-8B-Instruct, we observe that:\n\n\u2022 In most cases, all layer groups adopt a quantization configuration where the precision of the key is higher than the precision of the value. This supports our earlier observation from uniform quantization that the key plays a more critical role in quantization.\n\u2022 In other cases, in certain specialized layer groups, the value is set at a higher precision than the key for certain specialized layer groups. This aligns with the patterns identified in Table 4, which highlight specific layer groups may require higher precision for values.\n\u2022 KVTuner tends to allocate higher precision to groups with larger quantization errors. Reducing the quantization precision of the key for a crucial group of layers can significantly degrade the performance. For instance, in Llama-3.1-8B-Instruct, the layer group [8 ~ 11, 14 ~ 17, 20, 30] is particularly sensitive to the reduction of the key precision, and if the precision of the key is reduced from 4-bit to 2-bit, the performance would drop from 0.67 to 0.495."}, {"title": "Ablation Studies", "content": "According to Figure 6, when using the per-token-asym quantization mode on the Llama-3.1-8B-Instruct model, the search results deteriorate significantly if the proposed intra-layer and inter-layer search space pruning algorithms are not applied. In comparison with the counterpart with search space pruning as pre-processing in Figure 11a, this highlights search space pruning is helpful for MOO search convergence and maintaining quantization performance."}, {"title": "Conclusion", "content": "KVTuner enables efficient and adaptive layer-wise mixed-precision KV cache quantization via sensitivity-aware optimization techniques. It systematically reduces KV cache quantization errors by prioritizing key cache precision while balancing memory efficiency and inference accuracy. Experimental results demonstrate that KVTuner achieves nearly lossless compression at 3.25-bit for Llama-3.1-8B-Instruct and 4-bit for sensitive Qwen2.5-7B-Instruct. KVTuner also demonstrates that employing longer CoTs with lower and mixed precision KV cache quantization yields superior performance compared to shorter CoTs utilizing higher precision KV cache. This improvement is evident in both memory efficiency and accuracy, particularly in the context of mathematical reasoning tasks. KVTuner also greatly narrows the performance difference between the simple per-token-asym and accurate KIVI quantization modes, even when using overall similar low-precision settings."}, {"title": "Appendix", "content": ""}, {"title": "Proof of Lemma 1", "content": "Lemma 1 claims that only attention heads with sparse and concentrated patterns demonstrate consistent robustness to low-precision KV cache quantization. Its proof is below.\nProof. Given the query token $q \\in \\mathbb{R}^{1\\times D}$ and key cache $K \\in \\mathbb{R}^{D \\times S}$, the attention score without errors is $a_i = \\frac{\\exp(qK_i)}{\\sum_{j=1}^S \\exp(qK_j)}$. The key asymmetric uniform quantization error $\\Delta K \\in \\mathbb{R}^{S \\times D} \\sim \\mathcal{N}(0, \\sigma^2)$ follows normal distribution, where $\\sigma = \\frac{\\max(K) - \\min(K)}{2^{B-1}}$. Therefore, low precision quantization leads to exponential larger quantization errors. Then, the $i$-th attention score with key errors is\n\n$\\begin{aligned} \\hat{a}_i &= \\frac{\\exp(q(K_i + \\Delta K_i))}{\\sum_{j=1}^S \\exp(q(K_j + \\Delta K_j))} \\\\ &= \\frac{\\exp(qK_i) \\exp(q\\Delta K_i)}{\\sum_{j=1}^S \\exp(qK_j) \\exp(q\\Delta K_j)} \\\\ &= \\frac{\\frac{\\exp(qK_i)}{\\exp(q\\Delta K_i)}}{\\sum_{j=1}^S \\frac{\\exp(qK_j)}{\\exp(q\\Delta K_j)}}. \\end{aligned}$\n\nIf the key quantization error vector $\\Delta K_i$ with low quantization precision $B$ is noticeable, the inner product of query and error vector $q\\Delta K_j$ is also not ignorable. There are two cases where $\\hat{a}_i$ equals to $a_i$ for all tokens. In other words, the attention distribution before and after key quantization are identical.\n\nCase 1) $\\frac{\\exp(qK_i)}{\\exp(q\\Delta K_i)} = 1$, where each key token quantization errors have the same inner product result with the query token $q\\Delta K_i = q\\Delta K_j$ which normally does not happen.\n\nCase 2) There is a dominating key token i. If $j \\neq i, \\exp(qK_i) > \\exp(qK_j)$ and $\\frac{\\exp(qK_j)}{\\exp(q\\Delta K_j)} \\approx 0$, then\n\n$\\hat{a}_i = \\frac{\\exp(qK_i)}{\\sum_{j=1}^S \\exp(qK_j)} \\frac{\\exp(q\\Delta K_i)}{\\exp(q\\Delta K_i)} = 1.$\n\nOther dominated key token thus has the attention score $\\hat{a}_j = 0$. The exactly identical attention distribution with a dominating key token may be a special case, but it indicates that attention heads with a small amount of dominated key tokens, which have highly attention scores and result in sparse and concentrated attention patterns, are consistently robust to low-precision KV cache quantization."}, {"title": "Effects of KV Cache Quantization Mode and Precision", "content": "In this section, we analyze the effects of KV cache quantization mode and precision. We collect the full precision query tensor in the decoding phase and KV cache in both prefilling and decoding stages of the Llama-3.1-8B-Instruct model when processing the first 20 mathematical GSM8K zero-shot prompts without KV cache quantization. After that, we quantize KV cache along the channel or token dimension with uniform precision to compute errors of KV cache and attention score and output vectors of each self-attention layer as defined in Section 3.2, caused by KV cache quantization without any error accumulation. Finally, we average the simulated errors over different prompts and all layers in Table 8 to study the inherent sensitivity of KV cache to quantization mode and precision.\nThe non-accumulated relative attention output errors $e_o$ of INT8 KV cache quantization with the per-token-asym or per-channel-asym are lower than 3%. Minor single-token errors may cause slight shifts in intermediate attention patterns and final output distributions, yet these shifts are typically insufficient to alter the generated output tokens. However, when implementing extremely low-precision 2-bit KV2 cache quantization, the relative key quantization error $e_a$ increases to 40.1% or 77.5%, which may lead to substantial attention distribution shift for non-sparse retrieval heads as demonstrated in Figure 3. $e_o$ increases dramatically to 81.4% with the per-channel-asym mode even 96.2% with the per-token-asym mode. The noticeable errors may thus lead to noticeable token flipping and generation errors as in Table 1.\nThe relative key error $e_k$ of the INT8 per-token-asym key quantization is 0.012280, which is 2.5\u00d7 larger than the per-channel-asym counterpart 0.004869. Dynamically asymmetric quantization along"}, {"title": "Correlation of Model- and Layer-wise KV Cache Quantization Sensitivity with Attention Patterns", "content": "According to the layer-wise attention score errors of Llama-3.1-8B-Instruct in Figure 2 and Qwen2.5-7B-Instruct in Figure 16, we can observe the clear layer-wise difference in the same LLM. In this"}]}