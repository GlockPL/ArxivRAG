{"title": "OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in Videos", "authors": ["Debidatta Dwibedi", "Yusuf Aytar", "Jonathan Tompson", "Andrew Zisserman"], "abstract": "We introduce a dataset of annotations of temporal repetitions in videos. The dataset, OVR (pronounced as over), contains annotations for over 72K videos, with each annotation specifying the number of repetitions, the start and end time of the repetitions, and also a free-form description of what is repeating. The annotations are provided for videos sourced from Kinetics and Ego4D, and consequently cover both Exo and Ego viewing conditions, with a huge variety of actions and activities. Moreover, OVR is almost an order of magnitude larger than previous datasets for video repetition. We also propose a baseline transformer-based counting model, OVRCounter, that can localise and count repetitions in videos that are up to 320 frames long. The model is trained and evaluated on the OVR dataset, and its performance assessed with and without using text to specify the target class to count. The performance is also compared to a prior repetition counting model. The dataset is available for download at: https://sites.google.com/view/openvocabreps/", "sections": [{"title": "1 Introduction", "content": "Whether it is cutting an onion or a morning workout, activities with temporal repetitions are ubiquitous in our daily lives. Even solutions to our most basic needs such as navigation, communication and nourishment are handled by repetitive behaviours such as walking, talking, and eating. Moreover, many dexterous human activities involve deliberately repeating certain procedures to reach a goal one step at a time (e.g. carving wood, painting, playing guitar, stirring tea, cleaning surfaces etc.).\nIn this paper we introduce a new, large, open-vocabulary, video dataset for temporal repetition counting. We developed this Open-Vocabulary Repetitions (OVR) dataset to enable algorithms to be trained and their performance evaluated for more general counting capabilities than are possible at the moment. Our aim is a high quality diverse dataset. We achieve this by not restricting the vocabulary to a small set of classes. We ensure diversity by annotating existing datasets (Kinetics [17] and Ego4D [13]) that have a huge variety of actions and activities. Since our aim is diversity and scale in the number of video clips, rather than scale in the duration of the repetition, the annotations cover up to 10s of video providing free-form textual description and the temporal extent of the repetition.\nWhy is a new dataset required? As has been demonstrated time after time in many tasks (e.g. image understanding and video action recognition), developing a new capability is only possible with proper datasets and benchmarks guiding the way. For open vocabulary repetition counting, the dataset must be large-scale and diverse, as these are the dominant factors for determining the performance of a model on a given task. However, the existing temporal repetition counting datasets are quite limited in size and diversity; See Table 1. QUVA [30] and UCFRep [42], both of which are noticeably diverse repetition counting datasets, but have only 100 and 526 videos, respectively. The RepCount [14] dataset increases the amount of annotated data to roughly 1.5K, at the expense of decreasing the diversity in the data distribution by limiting repetitive activities to sports and workout domains. By sub-sampling video clips from Kinetics [17] action recognition benchmark, the Countix [11] dataset"}, {"title": "2 Related Work", "content": "Temporal Repetition Counting. Earlier literature in this space [3, 4, 7, 28, 36, 37] worked on understanding repetitions from the signal period using several types of crafted features, and were mainly studied in limited diversity settings. More recent work [11, 19, 30, 42] increased diversity and studied temporal repetition counting more in the wild settings. While [30] applied optical flow based methods, many others [11, 19, 42] employed deep learning methods. [19] learned from synthetic sequences using CNNs. [42] introduced a network that utilizes temporal context at multiple scales and trained on a small annotated dataset (UCFRep). RepNet [11], a deep network architecture that utilizes temporal self-similarity matrices as bottlenecks, learned from synthetic repetitions generated from Kinetics videos, and finetuned the network on the limited data (Countix) that is annotated for temporal repetitions. Most recently [43] utilizes sound in addition to frames, [20] employs temporal convolution networks over offline extracted features, and TransRAC [14] introduces a deep learning model which processes data at multiple scales thereby achieving better performance. Improvements in performances are also achieved through specialized models that use human body poses (e.g. PoseRAC[40]) but they may not generalize to in-the-wild settings where humans don't exist or are partially visible.\nTemporal Repetition Counting Datasets. Earlier datasets [19, 30] in this domain are quite small (100 videos) and only used for evaluation purposes. Later on UCFRep [42] and Countix [11], with 526 and 8.7K videos respectively, are used for training deep models but their scale is still limited such that they require carefully designed network architectures and synthetic data for training. RepCount [14], with roughly 1.5K videos, increased the length of videos but limited the diversity to sports and workout videos. The summary statistics of these datasets can be viewed in Table 1. Compared to existing datasets, our dataset offers a lot more diversity and scale, together with free-form text annotations.\nSemantics in Temporal Repetition Counting Datasets. Temporal repetition counting in the wild is mostly explored in a class-agnostic setting [11, 30, 42] where the target is counting repetitive behaviours independent of its semantics. In fact, a few recent datasets [11, 14, 42] already have category-level labels as they are generated from subsets of existing action recognition benchmarks such as Kinetics[17] and UCF101[34]. Although category labels create opportunities to study class-specific repetition counting methods, they are not rich enough to enable open-vocabulary understanding of repetitive activities in the wild. With OVR dataset we open that path where less engineered deep models can be trained with open-vocabulary semantics for understanding the semantic context of repetitions and their attributes.\nCounting Repetitions in Images. The related task of counting 'repeating' objects in images, e.g. counting cells or people, has seen a more rapid growth in challenge and interest, than that of temporal repetition counting. Early works had datasets for only a single class of objects, and models that were only able to count a single class such as cells [18] or cars [24]. This progressed to models that were 'class-agnostic', able to count any class, where the class was specified by providing 'image exemplars' (image patches from the target image covering the object of interest) [10, 12, 21\u201323, 25, 27, 29, 31, 39, 41]. Research in this area was encouraged by the release of datasets, such as FSC-147 [29], that covered a large variety of classes. Recent work has moved onto class-agnostic models where the target class is specified by a text description [1, 15, 16, 38]. These recent models at a more detailed level will enable machines to better understand the subtle temporal nuances in human actions.\nAnother significant benefit is that it allows text-conditioned counting models to be developed. We train a powerful baseline model, OVRCounter, that can be conditioned on the text description of a repeating action. Unlike class-agnostic repetition counting models like RepNet [11] and TransRAC [14] which count any repeating action, we can now control the counter to focus on only the specified action of interest. This unlocks the new capability of counting multiple classes of actions in the same video either spatially or temporally by using different text prompts.\nOur main contribution in this paper is the OVR dataset and benchmark that is large scale, diverse and annotated with open-vocabulary text descriptions. We also train a powerful baseline model, OVRCounter, for text-conditioned counting which can also operate in a class-agnostic setting."}, {"title": "3 OVR Dataset", "content": "In this section, we introduce OVR, a large scale video repetition counting dataset with text descriptions. We first overview the statistics and content of the dataset, providing qualitative examples to illustrate its diversity. Then in Section 3.2, we describe how the dataset was built.\n3.1 Dataset overview\nOur dataset is composed of two subsets: OVR-Ego4D [13] and OVR-Kinetics [5, 17], capturing both first-person and third-person views of human activities.\nOVR-Ego4D. This subset is generated from publicly available Ego4D [13] videos. These videos are comprised of egocentric videos recorded by humans in a wide range of geographical locations and contextual settings. The details of the curation process are explained in Section 3.2 which results in a dataset of 50, 665 videos with free-form text descriptions. Table 2 displays the detailed train/test split statistics. Figure 2(a) demonstrates example repetitive video clips and their text annotations. Figure 2(c) presents the detailed statistics of repetition counts and durations. Note that there are a significant number of temporal repetitions with two cycles, which brings an additional challenge as recognising a repetitive pattern with only two cycles could be much harder compared to more cycles. Figure 2(b) visualises the word-cloud generated from the text descriptions which displays the diversity of human behaviours captured in our dataset. \"Person\", \"man\", \"woman\", \u201cmoving\" words and stop-words are removed from the word-cloud to flush out details in the dataset.\nOVR-Kinetics. This subset is generated from the publicly available Kinetics [33] dataset similar to the Countix [11] dataset. However, instead of using a small number of categories with higher chance of repetitive behaviour (i.e. Countix), we annotate samples from the entire Kinetics dataset, hence the diversity is even more increased in our dataset. This subset has 21, 887 videos with free-form text descriptions. The train/test statistics are displayed in Table 2, and visual examples, word-clouds and statistics of repetition counts and durations are demonstrated in Figure 3 (a), (b), and (c), respectively.\nComparing word-clouds in the two subsets, we can recognise different word distributions where OVR-Ego4D focuses more on close-up human hand behaviours (e.g. painting, cutting, cleaning), tools (e.g. brush, hammer) and objects (e.g. food, wood, dough) whereas OVR-Kinetics has more emphasis on full-body human activities viewed with certain distance (e.g. walking, exercise, swimming, running) and directions (e.g. forward, right, left).\n3.2 Dataset Curation\nIn this section we describe the curation process: how candidate video clips were obtained from Ego4D and Kinetics, and then the processing pipeline that was used to select and annotate the candidates and clean up the dataset. The overall curation process is illustrated in Figure 4.\nStage 1: Obtaining candidate video clips\nAs repetitions are pervasive in daily activities, we obtain clips with repetitions from existing video datasets - there is no need to collect a new dataset of videos that just have repetitions. In particular, instances of this repetition in training; Second, we use a low threshold (0.25) on the repetition classifier score to encourage diversity. We then apply manual filtering to confirm valid repetitions. We find that this filtering step results in ~ 54 candidate repeating videos for every 100 clips in the Kinetics dataset (see Table 3a). While RepNet is a reliable repetition detector for non-egocentric videos (like the ones present in Kinetics), it is not as effective on egocentric videos of the Ego4D dataset. It often counts head and camera movements as repetitions.\nLLM based candidates. For the Ego4D videos, we use a language model to select candidate videos which may have repetitions. Ego4D provides a human supplied narration at a given timestamp. We choose a temporal window of 10 seconds around the narration timestamp as a candidate clip. We use only the narrations as input for the LLM. The task of the LLM is to predict from the narration alone whether there might be a repeating motion or not. We use FlanT5 [6] model for this step. We also ignore any walking motion to focus more on tasks being performed by the person with their hands as a disproportionately large number of clips of Ego4D involve walking. The prompt provided to the LLM is described as follows: \"Narration: <EGO4D Narration>. Q: Does the action described in above narration require any repeating motion? If the action is walking say no. A:\" Some example narrations classified as having repetitions are: 1. C cuts the tree with the chainsaw in his right hand. 2. The woman Y operates a phone with her hands. 3. C peels the potato with the knife. 4. The man Y climbs down the stairs. 5. C mixes the paint on the paint box with the brush. Some example narrations classified as not having repetitions are: 1. The man Y drops the boots on the floor. 2. C looks around the room. 3. C opens the garage door. 4. The man Z passes a bag to his left hand. 5. C enters a room. We find that this steps results in ~ 18 candidate repeating videos for every 100 clips in the Ego4D dataset (see Table 3a).\nStage 2: Filtering valid candidates\nThe objective of this stage is to determine if the candidate clips actually contain repetitions or not according to human annotators. We ask them not to label very common repetitions like walking, breathing, etc. Beyond that the annotators were free to choose which repetition to focus on so that we can get a diverse set of actions. We prepared a web-based annotation tool that displays one video clip at a time to an annotator, and asked them if the clip had a repeating action in it or not. To further increase robustness we show each video to two different annotators and check the agreement between them. This stage is important as both the LLM and RepNet have their own sets of false positives. Since the LLM based filtering does not use the video and only the text, a common pitfall is that while the action in the narration generally involves repetitions at some point (using a phone), such repetitions were not there or are not clear in the video. Common false positives from RepNet include clips with walking or repetitive camera movements.\nStage 3: Annotating the repetitions\nThe objective of this stage is to annotate the repetition in detail in the valid candidates. Our web-based annotation tool enables users to smoothly go back and forth in the video and enter answers to four questions: (a) What is the repeating action? (b) When does the repetition start? (c) When does the repetition end? (d) How many times does the repetition happen?\nThrough these questions we obtain the temporal interval of the contiguous repetitions, the repetition count, and a free-form text description of the repetition. For this stage also, initially we use two annotators. If multiple repetitions happen in a video we ask the annotators to annotate only the earliest repetition in the clip. This is done to ensure consistent labeling.\nStage 4: Quality assurance and final curation\nFinally in the last stage, we resolve disagreements through a third annotator. For any video if the existing two annotators agreed on the temporal repetition segment (\u2265 50% IOU between the two segments) but disagreed in the repetition count by more than 1, we re-annotate the video again with a new annotator. If there is agreement between any two of the three annotators we include the sample in the final dataset. We retain all annotations. We call the videos with at least 2 agreeing annotators as consistent. All videos in the test split are consistent while a subset of the videos in the train split are consistent. We do not get rid of the non-consistent in case training on more data with noisy labels is helpful in the future. We then create train/test splits from these videos. We use a split of 80% train and 20% test. The test split of OVR-Kinetics is from the test split of Kinetics-700. We hope the large number of test videos is useful in future evaluations."}, {"title": "4 OVRCounter: A Conditional Counting Model", "content": "Our objective is to train a baseline model to show the effectiveness of the collected data in training counting models. We train a single model that can perform temporal repetition counting with or without text conditioning. Below we will discuss all the components of our model shown in Figure 5.\nInput. The model takes a video \\(V = [f_1, f_2, ... f_n]\\) as input where N is the number of frames and \\(f_i\\) denotes a single frame. In this work, consider processing N = 320 frames at once. This exceeds the maximum length of a video in the OVR which is of 300 frames.\nSliding Window Video Encoder. First, we pass non-overlapping segments of length K frame samples at a stride of S of the video through a video segment encoder. This produces \\(\\frac{N}{KXS}\\) segments. Say each segment passed through the video segment encoder produces n tokens. Since, we want to keep all the spatio-temporal tokens for processing, we end up with \\(\\frac{N}{KXS} n\\) tokens. Specifically we use a Kinetics-400 pre-trained ViViT [2] model that can take K = 16 frames at a stride of S = 4 frames as input pre-trained with Masked Auto-encoder [35] loss. For videos of length N = 320 where each frame is of size 224 \u00d7 224 \u00d7 3, we end up with \\(\\frac{320}{16x4}\\) = 5 segments and 5 \u00d7 1568 = 7840 tokens.\nFurther processing using all 7840 spatio-temporal tokens with self-attention transformer layers would result in extremely large attention matrices. Hence we use a video resampler module (described below) using cross-attention layers which are more memory efficient.\nVideo Resampler. We add a segment position encoding to the spatio-temporal tokens to let the transformer know the temporal ordering of the segments. We then pass the spatio-temporal tokens with the position embedding to 2 cross-attention transformer decoder layers. The goal of these layers is to perform two objectives: i) perform temporal reasoning ii) reduce the number of tokens required for further processing. We learn N latent vectors multiplied with positional embeddings as queries for the cross-attention layers. Each token corresponds to a single frame in the video. This correspondence is established through the loss function that is applied at a per-frame level.\nConditional Counter. We use an Adaptive LayerNorm (AdaLN) [26] self-attention transformer layer as our counting module. This decoder composed of 2 layers takes per-frame video tokens as input queries and a conditioning token used to modulate the LayerNorm outputs used in the transformer layers. The conditioning token can either be the the CLS token output of the Video Resampler in which case the model performs class-agnostic counting, or alternatively we can also use text to condition the counter to only count actions if they are relevant to the text used in the description. We use text tokens from a frozen BERT [9] model as our textual conditioning. Conditional Counter is expected to produce a per-frame density with peaks at the temporal mid-points of each period of the relevant repeating action and zeros elsewhere. Summing up the per-frame densities give us the count for the whole video.\nLosses. We mainly have two loss functions: (a) a counting loss that minimizes the difference between predicted and expected per frame densities (i.e. mean squared error (MSE) loss), and (b) a text-video contrastive loss which trains the CLS token to be used instead of text when text is not available. For every training sample, we minimize the counting loss in three different scenarios: (i) use only video, where CLS token is used in Conditional Counter; (ii) use text and video, where text token is used in Conditional Counter, (iii) use video with a mismatched text, where text token is used in Conditional Counter and the Ground Truth density is set to a zero vector. We also train a text-video contrastive loss (CLIP style) in order to align the CLS token output (from the Video Resampler) with the text token (from the Text Encoder). This provides a semantic supervision to the Video Resampler and enables the CLS token to be used instead of text token (when not available) in Conditional Counter."}, {"title": "5 Experiments", "content": "We consider two tasks: class-agnostic counting, and text-conditioned counting. Class-agnostic counting refers to the counting of any repetitions in a video, whereas in text-conditioned only repetitions with the class specified by the text are counted, and other repetitions are ignored. In addition to the counting task, we also assess the task of repetition segmentation, which is to determine when the contiguous repetitions start and end.\nEvaluation Metrics. We use the following metrics in order to evaluate different tasks related to repetition counting: 1. Counting. As is common in repetition counting literature we use off-by-one error (OBOE) and mean absolute error (MAE) to measure if the model can accurately count repetitions or not. We also report off-by-zero error (OBZE) and root mean squared error (RMSE) which are newer repetition counting metrics introduced in [32]. 2. Repetition Segmentation. In order to measure if models are aware of when the repetition is happening in a video we measure the intersection over union (IOU) between ground truth and predicted repeating segment. In case there are multiple repeating segments in one video we consider only the first/earliest segment to calculate this metric.\nResults. We report the results in Table 4. We find that a prior model RepNet on class-agnostic counting is unable to produce accurate counts or repetition segments on our dataset. This is mainly due to RepNet being trained on a cleaner data where videos mostly contain repeating actions. However, in our dataset only some parts of the video has repeating actions. Our baseline model OVRCounter that is trained on OVR dataset performs much better in all metrics. There is still much room for improvement as the harder counting metric Off-by-Zero-Error is 0.75 for OVR-Kinetics and 0.71 for OVR-Ego4D. We also find a significant increase in the ability of the model to localize repeating segments in videos. For OVR-Kinetics, average IOU increased from 0.3 to 0.45 and for OVR-Ego4D average IOU increased from 0.26 to 0.45.\nThe second setting we study is the text-conditioned setup where the action of interest is provided as input to the model. We report the results in Table 4. We find our model is able to count without any performance drop when conditioned with text. This shows the flexibility of the model in being used in two different settings. We also study what happens when a mismatched text is provided as conditioning input. On the OVR-Kinetics dataset the off-by-zero error is 0.30 and on OVR-Kinetics dataset is 0.29. This indicates that the model does pay attention to the text conditioning. However, more work is needed to improve the OBZE metric when there is a text mismatch.\nWe show examples of OVRCounter's predictions in Figure 6. On the left two columns (green), we show two examples where the model correctly counts and localizes the repetitions. Note how when we provide a mismatched text (bottom row) the density predictions are close to zero indicating the model is ignoring those mis-matched repetitions. On the right two columns (red) we provide examples of incorrect predictions. In the third column the model is counting repetitions in a different part of the video. In the fourth column, the model is not able to count properly. We also observe it is producing counts with a mismatched text conditioning."}, {"title": "6 Limitations and Broader Impact", "content": "While we aim to scale the diversity of repetition counting datasets, we limit ourselves to clips that are 10s long as ensuring consistency of count and localization labels between annotators with such short clips was found to be challenging. Text-conditioned repetition counting in videos has broad implications for sports and health analytics, and vision based industrial process counting. However, there are concerns if the model is used for unethical surveillance or used without mitigating biases."}, {"title": "7 Conclusion", "content": "We introduce a large dataset of temporal repetitions in videos annotated with free-form text. This dataset contains videos captured from both egocentric and exocentric viewpoints. We also train a baseline model OVRCounter that is able to count repetitions in videos in an end to end manner. We hope this dataset and the model will help the community in developing better temporal reasoning visual models."}, {"title": "A Dataset Release Details", "content": "The dataset is being released as 2 JSON files available to download at these links hosted on Google Cloud: OVR-Ego4D and OVR-Kinetics. Both datasets have the following structure:\n1.  description - Description of the activity that is being repeated.\n2.  start_time - Time in seconds when repetition starts in the video. (Time is calculated as offset from the chosen clip)\n3.  end_time - Time in seconds when repetition ends in the video. (Time is calculated as offset from the chosen clip)\n4.  count - The number of times the action was repeated in the video.\n5.  split - Part of Train or Test split"}]}