{"title": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models", "authors": ["Yuan Sui", "Yufei He", "Tri Cao", "Simeng Han", "Bryan Hooi"], "abstract": "Large Language Models (LLMs) increasingly rely on prolonged reasoning chains to solve complex tasks. However, this trial-and-error approach often leads to high computational overhead and error propagation, where early mistakes can derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a framework that dynamically optimizes inference-time reasoning by enabling LLMs to \"think about how to think.\" Drawing inspiration from human meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic advisor, decoupling high-level guidance from step-by-step generation. It employs contextual multi-armed bandits to iteratively evaluate reasoning progress, and select optimal strategies (e.g., backtrack, clarify ambiguity, restart from scratch, or propose alternative approaches), and reallocates computational resources toward the most promising paths. Our evaluations on mathematical reasoning and puzzles highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.", "sections": [{"title": "1 Introduction", "content": "ol-like reasoning chains allow Large Language Models (LLMs) to \"think for an extended period\" before actually solving a problem. This shows impressive performance on challenging tasks, such as logical problems puzzles (Lei et al., 2024; Yao et al., 2023), math questions (Patel et al., 2024; Lightman et al., 2023), logical reasoning (Han et al., 2024), and science questions (Rein et al., 2023), which often pose difficulties for even the most advanced models (Gandhi et al., 2024; Sui et al., 2024c; He et al., 2024).\nHowever, the trial-and-error nature of ol-like reasoning often incurs substantial computational overhead (Snell et al., 2024; Manvi et al., 2024) and is prone to error propagation where early flaws in a reasoning chain can compound and derail subsequent steps (Lei et al., 2024; Yao et al., 2023; Gandhi et al., 2024). While related iterative approaches (Gandhi et al., 2024; Li et al., 2025) have explored techniques like partial revision or backtracking, they typically address errors in an ad-hoc manner for a narrow span of reasoning steps and lack a systematic way to assess whether an entire line of reasoning remains viable. Thus, models remain vulnerable to getting \u201cstuck\u201d in less promising reasoning trajectories, continuously expending computational resources on unpromising paths rather than recognizing when a major strategic shift is needed. A critical challenge, therefore, is to enable LLMs to allocate their reasoning budget more effectively, prioritizing promising avenues while adapting or discarding ineffective strategies.\nTo overcome these challenges, we propose Meta-Reasoner, a specialized module that operates alongside the LLM to enhance its reasoning capabilities. The meta-reasoner serves as an \"advisor\", dynamically evaluates the reasoning process, offering high-level guidance and strategic redirection when progress stalls. Unlike the LLM, which focuses on more specific stepwise generation, the"}, {"title": "2 Related Works", "content": "Complex Reasoning in LLMs The introduction of CoT reasoning has revolutionized how LLMs approach problem-solving, allowing them to break tasks into intermediate steps (Lee et al., 2025). The recent LLMs like 01, 03 from OpenAI and Deepseek-v3 from Deepseek have achieved state-of-the-art results in diverse domains using CoT-like reasoning (Manvi et al., 2024; Li et al., 2025; Kudo et al., 2024; Sui et al., 2024b). However, CoT's sequential dependency limits its robustness, as errors in earlier steps can cascade through the process (Snell et al., 2024) and also when facing complex reasoning tasks (Sui et al., 2024a,c), CoT-like reasoning may stuck in the infinite loop of reasoning (Lee et al., 2025). These issues motivate us to propose Meta-Reasoner to assess and adapt the overall reasoning strategy based on the progress of CoT reasoning. Unlike the LLMs, which focuses on more specific each step generation, the meta-reasoner focus on the border perspective and evaluates the overall progress and strategy of the reasoning process. It can provide a global oversight to avoid LLMs getting stuck or wasting resources on unproductive lines of thoughts.\nBacktracking and Error Correction Addressing cascading errors in multi-step reasoning remains a central challenge. Recent approaches have focused on backtracking and self-verification (Yao et al., 2023; Besta et al., 2023; Gandhi et al., 2024). For instance, Weng et al. (2023) have shown that incorporating a self-verification stage\u2014where the model re-checks its conclusions using the very chain of thought it generated\u2014can dramatically boost performance by catching missteps early. Similarly, Ling et al. (2023) propose not only generate multiple candidate reasoning chains but also employs a verifier mechanism to identify and backtrack on erroneous steps. These techniques go beyond post-hoc validation by introducing dynamic strategy adjustments during inference (Lightman et al., 2023), thereby reducing the impact of errors propagating through long reasoning chains. Following these useful efforts, we initiate our Meta-Reasoner with the instructions like (1) restart from scratch and propose alternative strategies; (2) backtracking to the point where the error occurred; and (3) continue and provide specific suggestions. The detailed strategy can be found in \u00a74.3.\nMeta-Cognition & Dual-Process Systems Meta-cognition in human reasoning involves higher-order processes that allow individuals to monitor, evaluate, and adjust their cognitive strategies (Gao et al., 2024; Yoran et al., 2024). This reflective thinking\u2014often seen as System 2 processes in dual-process theories (Havrilla et al., 2024) is vital for tasks that require careful deliberation and error correction (Didolkar et al., 2024). Drawing on these insights, our Meta-Reasoner can be considered analogous to dual-process systems, where LRM for generating CoT steps parallels System 1 and Meta-Reasoner"}, {"title": "3 Preliminary", "content": "A central challenge in complex reasoning tasks is choosing the most effective strategy among multiple valid options. This challenge naturally aligns with the contextual multi-armed bandit (MAB) framework, which is designed to balance the exploration of new strategies with the exploitation of strategies known to perform well.\nIn this framework, an agent observes a context xt that characterizes the current state of the environment at every time step t and selects an arm st from a finite set S. Upon selecting arm st, the agent receives a reward r(st, xt) that reflects both the chosen arm and the context. The primary objective of MAB is to maximize the cumulative reward over T\ntime: \\(R(T) = \\Sigma_{t=1}^{T}r(s_t, x_t)\\). A central challenge in the contextual MAB problem is balancing exploration (trying different arms to gather information about their rewards) with exploitation (selecting the arm that has yielded high rewards in similar contexts in the past). This balance ensures that while the agent leverages known profitable actions, it also continues to search for potentially better options. This principle is central to our motivation behind Meta-Reasoner, which aims to automatically select the most efficient strategy to guide the reasoning process during inference time.\nA widely used algorithm in the contextual setting is LinUCB (Li et al., 2012), which models the expected reward as a linear function of the context. Specifically, for an arm s given context xt, the expected reward is modeled as \\(E [r (s, x_t)] \\approx x_t^T\\theta_s\\), where \\( \\theta_s \\) is an unknown parameter vector associated with arm s. To manage uncertainty in its estimates, LinUCB maintains for each arm an estimate \\( \\theta_s \\) and an associated covariance matrix \\(A_s\\). The algorithm then selects the arm according to:\n\\(s_t = arg \\max_{s \\in S} \\left[ x_t^T\\theta_s + c\\sqrt{x_t^T A_s^{-1} x_t} \\right]\\)\nwhere c is a constant that controls the exploration level. Here, the term \\(c\\sqrt{x_t^T A_s^{-1} x_t}\\) serves as a confidence bound on the reward estimate, encouraging"}, {"title": "4 Methods", "content": "Based on the intuition to allow the LLMs to focus on their computation on more promising lines, we are motivated by two research questions: (1) How can we enable language models to dynamically allocate resources during inference to optimize for reasoning and planning? (2) What architecture allows for effective separation between the reasoning process in LRM and the meta-level guidance of that process in Meta-reasoner? To address them, we propose a new framework, Meta-Reasoner, to equip the LLMs with the capabilities to \u201cthinking about how to think\". It supervises the reasoning process of the LLMs and provides dynamic strategies to enable the LLMs to focus on more promising reasoning paths instead of iterative \u201ctrial-and-error\". This framework also addresses limitations of the current sequential generation of the reasoning paths which may get stuck in suboptimal trajectories by balancing \"higher-order\u201d thinking.\nThe meta-reasoning framework operates iteratively as shown in Figure 2. At each round t, the reasoning process comprises three steps: (1) CoT generation by the LLM, (2) Progress Reporting to summarize the reasoning progress so far (i.e., this is partly for efficiency, and partly to help the meta-reasoner focus on its main goal of \u201cadvising\u201d rather than being distracted by the details in the CoT), and (3) Strategy Generation by the meta-reasoner to optimize subsequent steps. The selection of the strategy is almost exactly corresponds to the well-studied problem of contextual multi-armed bandits learning. Each strategy can be seen as an arm for the bandit, and the reward of each strategy can be evaluated by the progress of LLM reasoning after applying the strategy. We analogy the process of executing and evaluating each strategy as the act of \"pulling\" each arm. The overall goal of our meta-reasoner is to find the best arm (i.e., strategy with highest cumulative rewards) with as few pulls as possible. The complete algorithm of Meta-Reasoner is appended in Algorithm 1."}, {"title": "4.1 Chain-of-Thought (CoT) Generation", "content": "In the first step, the LLM generates a reasoning step to extend its CoT reasoning based on the user query. Starting with its reasoning history \\(C_{t\u22121}\\) and the guidance \\(G_{t-1}\\) provided by the meta-reasoner in the previous round, the LRM M produces a new reasoning step \\(s_t\\). This step is then appended to the current CoT, forming \\(C_t = C_{t\u22121}\u222a \\{s_t\\}\\). This incremental process allows the LRM to iteratively build a structured reasoning path. By keeping track of the full reasoning trajectory at each round, the model creates a coherent foundation for evaluation and further refinement. This process is alike the demonstration in o1-like models, which generate a long-term thinking process. However, the issue of this reasoning is that its more like a process of \"trial-and-error\", which may waste some of the inference costs on unnecessary/useless paths. In addition, due to the sequential generation process, it may easily get stuck in suboptimal solutions."}, {"title": "4.2 Progress Reporting", "content": "Once the LRM has updated its CoT, we summarize the reasoning history \\(C_t\\) into a concise progress report \\(P_t\\). This summary captures the key aspects of the reasoning trajectory, such as how much progress has been made toward the task goal, the consistency of the reasoning, and any significant updates so far. The summarization function f abstracts the detailed CoT into a simpler, more focused representation. This step is designed to be both computationally efficient and informative, ensuring that the meta-reasoner can focus on evaluating high-level progress without being overwhelmed by the granular details of every reasoning step. Even this step is more like an engineering"}, {"title": "4.3 Meta-reasoner Strategy Generation", "content": "In the next step, the meta-reasoner evaluates the progress report \\(P_t\\) and selects proper strategy \\(G_t\\) for LLM reasoning (the complete process can be found in Algorithm 1). We formulate the generation of strategy as a multi-armed bandits problem and consider two settings below: (1) our approach begins with a fixed-strategy formulation, where the meta-reasoner selects from a predefined set of strategies using a contextual bandit algorithm. We then extend this architecture to (2) an advanced setting in which the meta-reasoner is itself an LLM-based agent and can introduce or refine new strategies on the fly. In both cases, the meta-reasoner uses the same partial-feedback principle of multi-armed bandits to adaptively choose which strategy to deploy based on a reward function. The reward function evaluates the quality of the given reasoning progress after applying the meta-reasoner strategy. We demonstrate the contextual bandit pair (i.e., diagnosis of the current state from the progress report and the corresponding strategy) in Table 1.\nProgress Evaluation. A central goal of our evaluation mechanism is to measure how effectively the model's current reasoning is advancing toward the task objective (e.g., solving a complex problem) while also monitoring computational expenditure to encourage efficiency. Concretely, we implement a reward function that tracks both solution"}, {"title": "5 Experiments", "content": "In this section, we first introduce the experiment settings including the dataset, baselines, and the backbone models. We then present the main results of Meta-Reasoner with some other analysis from perspectives like efficiency, rewards accumulation, and qualitatively assess meta-reasoner output."}, {"title": "5.1 Experiments Setup", "content": "Datasets. We consider the tasks requiring complex reasoning and the proper solutions naturally composed of long reasoning steps. We evaluate Meta-Reasoner on several challenging datasets: the 24-point game proposed by Yao et al. (2023), college-level scientific problem from SciBench (Wang et al., 2024) and theorem-driven math question in TheoremQA (Chen et al., 2023). For the SciBench, we only consider the math-related subset for testing which covers the diff, stat, and calc (the detailed clarification of each subset collection can be found in Wang et al. (2024) and we provide the demonstration for each subset in Figure 9). For the TheormQA, we consider the mathematics subset that involves logical reasoning for our testing. We follow the experimental setup in MACM (Lei et al., 2024) to conduct the corresponding analysis on these datasets.\nBaselines. We consider several established prompting methods as baselines as follows:\n\u2022 Chain-of-thought (CoT) (Wei et al., 2022): A prompting technique that encourages models to generate intermediate reasoning steps to enhance problem-solving capabilities.\n\u2022 Self-Consistent Chain of Thought (SC-CoT) (Wang et al., 2022): An extension of CoT that improves reasoning consistency by generating multiple reasoning chains and selecting the most consistent answer.\n\u2022 Multi-Chain Reasoning (MCR) (Yoran et al., 2024): enhances SC-CoT by having another LLM to assess and integrate content among the sampled reasoning chains to generate the final consistent answer.\n\u2022 Tree of Thoughts (ToT) (Yao et al., 2023): A method that explores multiple reasoning paths in a tree structure, allowing the model to consider various possibilities before arriving at a conclusion by tree search algorithms.\n\u2022 Reflexion (Shinn et al., 2024): A framework that enables models to reflect on their reasoning process, iteratively refining their answers based on feedback.\n\u2022 MACM (Lei et al., 2024): A multi-agent system to refine the reasoning based on iterative condition mining.\nBackbone Models. We consider both LLMs and the recent LRMs for our experiments. For the LLMs, we consider the closed-source models like GPT-40, GPT-40-mini (between Nov 2025 to Jan 2025) from OpenAI, and open-sourced models"}, {"title": "5.2 Main Results", "content": "We compare the accuracy of different prompting methods across different backbone models on SciBench (as shown in Table 2), 24-points game (as shown in Table 3) and TheoremQA (as shown in Table 4). We found that basic prompting strategies, such as CoT and SC-CoT, show limited ef-"}, {"title": "5.3 Ablation Study", "content": "In this section, we conduct an ablation study to analyze each component contribution of Meta-Reasoner. In specific, we consider the following setup: (1) w/o progress report: we replace the progress reporting process with directly considering the entire CoT history without summarization; (2) w/o MAB: instead of using MAB to select the proper strategy, we directly leverage an LLM to the decision making to provide the proper strategy for LRM reasoning. In Table 5, we show that when removing progress reporting (\"w/o Progress Report\"), the overall performance moderately degrades and we hypothesize it is due to the concise intermediate summarizations can help the Meta-reasoner only consider the high-level strategy instead of being confused with too much details of the reasoning process. We also find that removing the MAB brings a more pronounced effect, especially when strategy selection falls back to a direct chain-of-thought approach (\u201cw/o MAB (CoT)\"). It verifies the effect of our meta-reasoner module to help the model stay on track for getting an optimal solution. In Table 6, we compare fixed and dynamic bandit variants on the game of 24 and theoremQA. We find that using a fixed set of strategies (e.g., K = 3 and K = 5) yields lower performance compared to the dynamic approach which adaptively explores more strategies (shown by larger unique strategies). The results highlight the benefit of flexibly allocating diverse reasoning strategies using LLM in-context learning capabilities.\""}, {"title": "5.4 Analysis", "content": "Effectiveness of Meta-reasoner. In Figure 4, we demonstrate the cumulative rewards across iterations to analyze the effectiveness of the our meta-reasoner module. We compare our MAB-based method with a baseline which directly prompts an LLM to select an arm (\"strategy\"). We refer to this baseline method as Baseline (Direct Arm Selec-"}, {"title": "6 Conclusion", "content": "In this work, we introduce Meta-Reasoner, a meta-reasoning framework designed to enhance the reasoning capabilities of LRMs and optimize the inference-time reasoning efficiency. By operating as an \"advisor\u201d, meta-reasoner dynamically evaluates the reasoning process and provides high-level strategic guidance, addressing key limitations of ol-like reasoning chains, such as compounding errors and inefficiency in inference computing. Unlike conventional reasoning approaches, Meta-Reasoner focuses on global oversight rather than granular step-by-step processes, enabling LRMS to avoid unproductive lines of thought and better allocate computational resources. The experiments highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.\nLimitations\nOur proposed Meta-Reasoner framework, while effective at improving inference-time reasoning, has a few key limitations that may affect its applicability. First, it relies on a carefully designed reward function to guide strategy selection: if the reward signal does not accurately reflect correctness or progress, the meta-reasoner may persist with in-"}]}