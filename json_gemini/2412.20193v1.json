{"title": "Imitation Learning from Suboptimal Demonstrations via Meta-Learning An Action Ranker", "authors": ["Jiangdong Fan", "Hongcai He", "Paul Weng", "Hui Xu", "Jie Shao"], "abstract": "A major bottleneck in imitation learning is the requirement of a large number of expert demonstrations, which can be expensive or inaccessible. Learning from supplementary demonstrations without strict quality requirements has emerged as a powerful paradigm to address this challenge. However, previous methods often fail to fully utilize their potential by discarding non-expert data. Our key insight is that even demonstrations that fall outside the expert distribution but outperform the learned policy can enhance policy performance. To utilize this potential, we propose a novel approach named imitation learning via meta-learning an action ranker (ILMAR). ILMAR implements weighted behavior cloning (weighted BC) on a limited set of expert demonstrations along with supplementary demonstrations. It utilizes the functional of the advantage function to selectively integrate knowledge from the supplementary demonstrations. To make more effective use of supplementary demonstrations, we introduce meta-goal in ILMAR to optimize the functional of the advantage function by explicitly minimizing the distance between the current policy and the expert policy. Comprehensive experiments using extensive tasks demonstrate that ILMAR significantly outperforms previous methods in handling suboptimal demonstrations.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning has achieved notable success in various domains, such as robot control [Levine et al., 2016], autonomous driving [Kiran et al., 2022] and large-scale language modeling [Carta et al., 2023]. However, its application is significantly constrained by a carefully designed reward function [Hadfield-Menell et al., 2017] and the extensive interactions with the environment [Garc\u00eda and Fern\u00e1ndez, 2015].\nImitation learning (IL) emerges as a promising paradigm to mitigate these constraints. It derives high-quality policies from expert demonstrations, thus circumventing the need for a predefined reward function, often in offline settings where interaction with the environment is unnecessary [Hussein et al., 2017]. However, to alleviate the compounding error issue where errors accumulate over multiple predictions, leading to significant performance degradation\u2014substantial quantities of expert demonstrations are required [Ross and Bagnell, 2010]. Unfortunately, acquiring additional expert demonstrations is often prohibitively expensive or impractical.\nCompared with expert demonstrations, suboptimal demonstrations can often be collected in large quantities at a lower cost. However, a distributional shift exists between suboptimal and expert demonstrations [Kim et al., 2022]. Standard imitation learning algorithms [Pomerleau, 1991; Ho and Ermon, 2016], which process expert and non-expert demonstrations indiscriminately, may inadvertently learn the deficiencies inherent in suboptimal demonstrations, potentially degrading the quality of the learned policies. Current approaches to addressing this issue often require manual annotation of the demonstrations [Wu et al., 2019] or interaction with the environment [Zhang et al., 2021], both of which are expensive and time-consuming. Another category of methods, which has shown considerable promise, utilizes a small-scale expert dataset along with a large-scale supplementary dataset sampled from one or more suboptimal policies [Kim et al., 2022; Xu et al., 2022; Li et al., 2023]. This paper is focused on exploring this particular setup.\nPrevious studies often train a discriminator to distinguish between expert and non-expert demonstrations, performing weighted imitation learning on the supplementary dataset. However, during the training of the discriminator, labeled expert demonstrations are assigned a value of 1, while the demonstrations from the supplementary dataset are assigned a value of 0 and discarded. Given the limited scale of the labeled expert dataset, the supplementary dataset may contain a substantial amount of unlabeled expert demonstrations, leading to a positive-unlabeled classification problem [Elkan and Noto, 2008]. Furthermore, supplementary dataset often includes many high-quality demonstrations that, although not optimal, could improve policy performance when selectively leveraged, especially in cases of insufficient expert demonstrations [Xu et al., 2022]. Thus, these weighting imitation learning methods based on the expert distribution tend to discard high-quality non-expert demonstrations, failing to fully"}, {"title": "2 Related Work", "content": "Imitation Learning with Suboptimal Demonstrations In imitation learning, a large number of expert demonstrations are typically required to minimize the negative effects of compounding errors. However, obtaining expert demonstrations is often expensive or even impractical in most cases. Therefore, researchers have turned to using suboptimal demonstrations to enrich the dataset [Li et al., 2023; Kim et al., 2022]. Traditional imitation learning methods, such as behavioral cloning (BC) [Pomerleau, 1991] and generative adversarial imitation learning (GAIL) [Ho and Ermon, 2016], often treat all demonstrations uniformly, which can lead to suboptimal performance.\nTo address this issue, BCND [Sasaki and Yamashina, 2021] employs a two-step training process to weight the suboptimal demonstrations using a pre-trained policy. However, this method performs poorly when the proportion of expert demonstrations in the suboptimal dataset is low. CAIL [Zhang et al., 2021] ranks demonstrations by superiority and assigns different confidence levels to suboptimal demonstrations, but this approach requires extensive interaction with the environment. ILEED [Beliaev et al., 2022] leverages demonstrator identity information to estimate state-dependent expertise, weighting different demonstrations accordingly. The most similar studies to ours are DWBC [Xu et al., 2022], DemoDICE [Kim et al., 2022] and ISW-BC [Li et al., 2023], which weight suboptimal demonstrations by leveraging a small number of expert demonstrations along with supplementary demonstrations. However, these methods discard a substantial portion of high-quality suboptimal demonstrations within the supplementary dataset by focusing solely on distinguishing expert demonstrations from non-expert demonstrations. Our method is based on the advantage function, which assigns weights by comparing demonstrations with the learned policy. This strategy can effectively leverage these high-quality, non-expert data.\nImitation Learning with Meta-Learning Meta-imitation learning is an effective strategy to address the lack of expert demonstrations [Duan et al., 2017; Finn et al., 2017b]. It typically involves acquiring meta-knowledge from other tasks in a multi-task setting, enabling rapid adaptation to the target task [Finn et al., 2017a; Gao et al., 2022]. Although our approach utilizes a meta-learning framework, it is designed for a single-task setting and uses meta-goal to optimize the model. The study most similar to ours is meta-gradient reinforcement learning [Xu et al., 2018], which uses meta-gradients to obtain optimal hyperparameters. In our approach, the use of meta-goal allows the model to assign appropriate weights, leading to the development of a satisfactory policy."}, {"title": "3 Problem Setting", "content": null}, {"title": "3.1 Markov Decision Process", "content": "We formulate the problem of learning from suboptimal demonstrations as an episodic Markov decision process (MDP), defined by the tuple $M = (S, A, P, R, H, p_0, \\gamma)$, where S is the state space, A is the action space, H is the episode length, $p_0$ is the initial state distribution, P is the"}, {"title": "3.2 IL with Supplementary Demonstrations", "content": "In imitation learning, it is typically assumed that there exists an optimal expert policy $\u03c0_E$, and the goal is to have the agent make decisions by imitating this expert policy. To mitigate the issue of compounding errors, substantial quantities of expert demonstrations are typically required. A promising solution is to supplement the dataset with suboptimal demonstrations.\nWe use the expert policy to collect an expert dataset $D_E = {\\tau_1, ..., \\tau_{N_E}}$, consisting of $N_E$ trajectories. Each trajectory is a sequence of state-action pairs $\u03c4 = {s_1, a_1, ..., s_H, a_H}$. The supplementary dataset $D_S = {\\tau_1, ..., \\tau_{N_S}}$ is collected using one or more policies, where $N_S$ is the number of supplementary trajectories. In general, there is no strict quality requirement for the policies used to collect the supplementary dataset. These trajectories may originate from a wide range of policies, spanning from near-expert level to those performing almost randomly. Therefore, it is crucial to develop algorithms that can effectively discern useful demonstrations from the supplementary dataset of varying quality to learn a good policy. We combine the expert dataset $D_E$ with the supplementary datasets $D_S$ to form the full dataset D.\nWeighted behavior cloning is a classical approach to tackle this challenge. It seeks to assign weights that reflect the expert level of the demonstrations and then perform imitation learning on the reweighted dataset. The optimization objective of weighted behavior cloning is as follows:\n$\\min_\\pi E_{(s,a)\\sim D} [-w(s, a) \\log \\pi(a|s)]$,\nwhere w(s, a) is an arbitrary weight function, and s and a are the state and action in demonstrations. When w(s,a) = 1 for all (s, a) \u2208 D, weighted behavior cloning degenerates to vanilla BC. If w(s, a) is the weight assigned by a discriminator that distinguishes expert demonstrations, this objective aligns with the optimization goal of ISW-BC [Li et al., 2023]. DWBC [Xu et al., 2022] expands this by incorporating the policy into discriminator training. The primary goal of weighted BC is to filter out low-quality demonstrations and selectively learn from valuable suboptimal demonstrations in the supplementary dataset."}, {"title": "4 Method", "content": "In this section, we present a novel offline imitation learning algorithm called imitation learning from suboptimal demonstrations via meta-learning an action ranker. Our goal is to train a discriminator capable of evaluating the relative benefit of the learned policy and the demonstration policy, allowing us to fully leverage suboptimal demonstrations in the supplementary dataset to improve the learned policy. We propose a bi-level optimization framework to enhance the performance of weighted behavior cloning method, enabling the discriminator to automatically learn how to assign appropriate weights to the demonstrations in order to achieve high-performance policies. We also provide an explanation of the weights assigned by our discriminator, which offers an intuitive understanding of why our approach is effective."}, {"title": "4.1 Imitation Learning via Learning An Action Ranker", "content": "In our framework, we utilize a model parameterized by \u03b8 as the policy \u03c0 and another model parameterized by \u03c8 as the discriminator C, which evaluates the relative benefit of demonstrations compared with the learned policy.\nIt is evident that we can avoid learning low-quality demonstrations if we have access to an advantage function. The optimization objective of policy \u03c0 can be written as:\n$\\min_\\pi E_{(s,a)\\sim D} [-A^\\pi(s, a) \\log \\pi(a|s)]$,"}, {"title": "4.2 Meta-Goal for Weighted Behavior Cloning", "content": "To enhance weighted behavior cloning for learning from suboptimal datasets, we propose the meta-goal approach. In weighted behavior cloning, the discriminator should assign weights to the suboptimal dataset such that the resulting policy closely resembles the expert policy, effectively recovering the expert distribution. This goal can be instantiated using the Kullback-Leibler (KL) divergence:\n$\\min_\\pi,C D_{KL} (\\pi_E||\\pi)$,\nwhere $D_{KL} (\\pi_E||\\pi) = E_{s\\sim d^{\\pi_E}} [D_{KL} (\\pi_E(\\cdot | s)||\\pi(\\cdot | s))]$ and $d^{\\pi_E}$ is the stationary distribution of the expert policy. We cannot directly optimize the discriminator using this objective. Inspired by the meta-gradient methods [Xu et al., 2018], we adopt a bi-level optimization framework. Specifically, drawing on the ideas of expectation-maximization (EM), we proceed as follows: in the inner optimization loop, we perform weighted behavior cloning using the current discriminator to update the policy; in the outer optimization loop, we then adjust the discriminator parameters based on the resulting difference between the learned policy and the expert policy. Through this nested optimization process, the meta-goal method effectively guides the discriminator to assign weights that lead to a policy closely resembling the expert."}, {"title": "4.3 Theoretical Results", "content": "When updating the discriminator network with meta-goal, ILMAR employs a bi-level optimization framework, where the policy network is updated in the inner loop and the discriminator network is updated in the outer loop. The convergence"}, {"title": "5 Experiments", "content": "In this section, we conduct experiments to evaluate and understand ILMAR. Specifically, we aim to address the following questions:\n1. How does ILMAR perform compared with previous suboptimal demonstrations imitation learning algorithms?\n2. How does the proportion of suboptimal demonstrations affect the performance of ILMAR?\n3. Is meta-goal compatible with other algorithms, and does it enhance their performance?"}, {"title": "5.1 Comparative Evaluations", "content": "Datasets In this section, we evaluate the effectiveness of ILMAR by conducting experiments on the MuJoCo continuous control environments [Todorov et al., 2012] using the OpenAI Gymnasium framework [Towers et al., 2023]. We conduct experiments on four MuJoCo environments: Ant-v2, Hopper-v2, Humanoid-v2 and Walker2d-v2. We collect expert demonstrations and additional suboptimal demonstrations and conduct the evaluation as follows. For each MuJoCo environment, we follow prior dataset collection methods [Wu et al., 2019; Zhang et al., 2021]. Expert agents are trained using PPO [Schulman et al., 2017] for Ant-v2, Hopper-v2 and Walker2d-v2, and SAC [Haarnoja et al., 2018] for Humanoid-v2. We consider three tasks (T1, T2, T3) with a shared expert dataset $D_E$ consisting of one expert trajectory. The supplementary datasets include expert and suboptimal trajectories at ratios of 1:0.25 (T1), 1:1 (T2), and 1:4 (T3). Each supplementary suboptimal dataset $D_S$ contains 400 expert trajectories and 100, 400, or 1600 suboptimal trajectories generated by four intermediate policies sampled during training, with equal contributions from each policy. We visualized the distributional differences between the additional datasets and the expert demonstrations under three task settings on Ant-v2. The detailed methodology for the visualization is provided in the supplementary materials. As shown in Figure 3, the distributional differences between expert and supplementary datasets increase as the proportion of expert demonstrations decreases, challenging the algorithm with more suboptimal data.\nBaselines We compare ILMAR with five strong baseline methods in our problem setting including BC, BCND [Sasaki and Yamashina, 2021], DemoDICE [Kim et al., 2022], DWBC [Xu et al., 2022], and ISW-BC [Li et al., 2023]. For these methods, we use the hyperparameter settings specified in their publications or in their codes. The training process is carried out for 1 million optimization steps. We evaluate the performance every 10,000 steps with 10 episodes. More experimental details are provided in the supplementary material."}, {"title": "5.2 Ablation Studies", "content": "In this section, we conduct ablation studies to analyze the effects of the different components of the loss function.\nTable 2 presents the results for ILMAR when evaluated using only the naive loss or only the meta loss across the MuJoCo experiments. First, ILMAR outperforms the best baseline even without using meta-goal. This validates that weighting suboptimal demonstrations based on advantage function"}, {"title": "5.3 Meta-goal Used in Other Algorithms", "content": "In this section, we apply meta-goal to ISW-BC and DemoDICE to evaluate its compatibility with other algorithms. Specifically, we update the discriminator of ISW-BC and DemoDICE using both the meta loss and the discriminator loss of original algorithm, as shown in Eq. (10). Table 3 presents the results of employing meta-goal in ISW-BC and DemoDICE across the MuJoCo experiments. We observe that incorporating meta-goal leads to significant performance improvements for both DemoDICE and ISW-BC in nearly all environments. This further validates the effectiveness and robustness of meta-goal, demonstrating its compatibility with other algorithms."}, {"title": "5.4 Additional Analysis of Results", "content": "To further understand ILMAR, we explore the relationship between the weights learned by ILMAR and the actual reward values. The results indicate a clear monotonic positive correlation between the weights assigned by ILMAR and the true rewards. Specifically, we calculate the Spearman rank correlation coefficient, which measures the strength and direction of the monotonic relationship between two variables, between the weights and true rewards in the task setting T3 for Ant-v2 and Humanoid-v2. The formula is given by:\n$p = 1- \\frac{6 \\Sigma d_i^2}{n(n^2 - 1)}$\nwhere:\n* $d_i = rank(x_i) \u2013 rank(y_i)$ is the difference between the ranks of corresponding values $x_i$ and $y_i$ from the two variables.\n* n is the number of observations.\nThis coefficient provides a robust measure of the monotonic relationship, making it suitable for evaluating non-linear dependencies in the data. The Spearman rank correlation coefficients between the weights and true rewards for Ant-v2 and Humanoid-v2 are 0.7862 and 0.7220, respectively. These results strongly validate that the weights assigned by ILMAR effectively represent the superiority of demonstration actions, enabling the model to learn from suboptimal demonstrations."}, {"title": "6 Conclusion", "content": "We propose ILMAR, an imitation learning method designed for datasets with a limited number of expert demonstrations and supplementary suboptimal demonstrations. By utilizing a functional of the advantage function, ILMAR avoids directly discarding high-quality non-expert demonstrations in the supplementary suboptimal dataset, thereby improving the utilization of the supplementary demonstrations. To further enhance the updating of our discriminator, we introduce the meta-goal method, leading to performance improvements. Experimental results show that ILMAR achieves performance that is competitive with or superior to state-of-the-art IL algorithms. One potential direction for future exploration is to investigate the use of a functional of the advantage function weighting for processing suboptimal demonstrations with reward information. Another direction is to apply the meta-goal method to semi-supervised learning scenarios."}, {"title": "A Experimental Details", "content": null}, {"title": "A.1 Implementation Detail", "content": "In this section, we provide detailed descriptions of our experimental setup to ensure the reproducibility of our results. We evaluate the performance of various imitation learning algorithms across four motion control tasks in the MuJoCo suite [Todorov et al., 2012]: Ant-v2, Humanoid-v2, Hopper-v2, and Walker2d-v2.\nTo train the expert policies, we use the proximal policy optimization (PPO) [Schulman et al., 2017] and soft actor-critic (SAC) [Haarnoja et al., 2018] algorithms. After comparing the results, we select the best-performing models as the expert policies: Ant-v2 is trained with PPO for one million steps, Humanoid-v2 with SAC for two million steps, Hopper-v2 with PPO for three million steps, and Walker2d-v2 with PPO for two million steps. We select four intermediate policies with varying levels of optimality by evaluating the policies every 100,000 steps, using the best-performing policy as the expert policy. All algorithmic dependencies in our code are based on the implementation provided by CAIL [Zhang et al., 2021], available at https://github.com/Stanford-ILIAD/ Confidence-Aware-Imitation-Learning. The training curves of the expert agents are presented in Figure 5.\nWe select four suboptimal policies, each with performance rewards evaluated every 10,000 steps with 5 episodes, approximating 80%, 60%, 40%, and 20% of the optimal policy, respectively. Table 4 illustrates the average performance of the collected trajectories.\nAll three task settings share the same expert dataset $D_E$, which consists of only one expert trajectory. The supplementary dataset for each task is composed of a mixture of expert and suboptimal trajectories at different ratios: 1:0.25 (T1), 1:1 (T2), and 1:4 (T3). Specifically, the supplementary sub-"}, {"title": "A.2 Additional Experimental Results", "content": "Ablation Test on Hyperparameters a and \u03b2\nAs we can notice in Eq. (10), ILMAR introduces hyperparameters \u03b1 and \u03b2, which control the relative influence of the meta loss and the vanilla loss on the updates of the discriminator. In this section, we aim to study how does the choice of \u03b1 and \u03b2 affect the training processes and performance of our algorithm. We conduct a grid search over \u03b1\u2208 {0,0.1,0.3,0.7,1.0} and \u03b2\u2208 {0,0.01,0.05, 0.5, 1.0} in the task setting T3.\nFigure 7 demonstrates that meta-goal significantly enhances policy performance. We observe that when using the meta-goal method alone, ILMAR performs suboptimally. However, when combined with the vanilla loss, the performance of ILMAR is often significantly improved. Notably, in scenarios where only meta-goal is used, the performance exhibits higher sensitivity to random seeds.\nThis observation supports our previous theoretical analysis, which suggests that the explicitly designed vanilla loss provides prior knowledge by regularizing the output of the discriminator. This prior knowledge assists the model in learning how to effectively weight the demonstrations using meta-goal, ultimately yielding a policy that closely resembles the expert policy."}, {"title": "B Theoretical Derivation", "content": null}, {"title": "B.1 Derivation of The Gradient $\\frac{\\partial L_{meta}}{\\partial \\psi}$", "content": "Eq. 9 establishes that the gradient $\\frac{\\partial L_{meta}}{\\partial \\psi}$ can be expressed as:\n$\\frac{1}{D} \\frac{\\partial L_{meta}}{\\partial \\theta_{t+1}} = \\sum_{(s,a) \\in D} \\frac{\\partial^2 w(s, a, \\pi_{\\theta_t})}{\\partial \\psi \\partial \\theta_t} log \\pi_{\\theta_t}(a|s)$"}, {"title": "B.2 Proof of Theorem 1", "content": "Proof. By Lemma 2 in Zhang et al. [2021] (with proof on Page 12), the function f(x) is Lipschitz-smooth with constant L, then the following inequality holds:\n$f(y) \\le f(x) + \\nabla f(x)^T(y - x) + \\frac{L}{2} ||y - x||^2,  \\forall x, y$.\nThus, we have:\nProof.\n$L_c (\\theta_{t+1}) \u2013 L_c (\\theta_{t})$\n$\\le \\nabla L_c (\\theta_{t})^T (\\theta_{t+1} \u2013 \\theta_{t}) + \\frac{L}{2} ||(\\theta_{t+1} \u2013 \\theta_{t})||^2$ (11)\n$= \u2212\u00b5 \\nabla L_c (\\theta_{t+1})^T \\nabla_{\\theta} L_{actor} (\\theta_t, \\psi_t)$\n$+ \\frac{L}{2} \u00b5^2 ||\\nabla_{\\theta} L_{actor} (\\theta_t, \\psi_t)||^2$\n$= \\frac{L}{2} (\u00b5K \u2212 \\frac{L}{2} \u00b5^2) ||\\nabla_{\\theta} L_{actor} (\\theta_t, \\psi_t)||^2$ (12)\n$\\le 0$ (13)"}]}