{"title": "Cog-GA: A Large Language Models-based Generative Agent for\nVision-Language Navigation in Continuous Environments", "authors": ["Zhiyuan Li", "Yanfeng Lu", "Yao Mu", "Hong Qiao"], "abstract": "Abstract-Vision Language Navigation in Continuous En-\nvironments (VLN-CE) represents a frontier in embodied AI,\ndemanding agents to navigate freely in unbounded 3D spaces\nsolely guided by natural language instructions. This task intro-\nduces distinct challenges in multimodal comprehension, spatial\nreasoning, and decision-making. To address these challenges,\nwe introduce Cog-GA, a generative agent founded on large\nlanguage models (LLMs) tailored for VLN-CE tasks. Cog-GA\nemploys a dual-pronged strategy to emulate human-like cognitive\nprocesses. Firstly, it constructs a cognitive map, integrating\ntemporal, spatial, and semantic elements, thereby facilitating\nthe development of spatial memory within LLMs. Secondly, Cog-\nGA employs a predictive mechanism for waypoints, strategically\noptimizing the exploration trajectory to maximize navigational\nefficiency. Each waypoint is accompanied by a dual-channel\nscene description, categorizing environmental cues into 'what'\nand 'where' streams as the brain. This segregation enhances\nthe agent's attentional focus, enabling it to discern pertinent\nspatial information for navigation. A reflective mechanism\ncomplements these strategies by capturing feedback from\nprior navigation experiences, facilitating continual learning and\nadaptive replanning. Extensive evaluations conducted on VLN-\nCE benchmarks validate Cog-GA's state-of-the-art performance\nand ability to simulate human-like navigation behaviors. This\nresearch significantly contributes to the development of strategic\nand interpretable VLN-CE agents.", "sections": [{"title": "I. INSTRUCTION", "content": "Vision Language Navigation (VLN) plays a pivotal role\nin robotics, where an embodied agent carries out natural\nlanguage instructions inside real 3D environments based on\nvisual observations. Traditionally, the movements of agents in\nVLN environments are processed by a pre-prepared navigation\ngraph that the agent traverses. Recognizing this, Krantz et\nal. [15] introduced an alternative approach known as Vision-\nLanguage Navigation in Continuous Environments (VLN-CE).\nUnlike traditional methods, VLN-CE eliminates the need\nfor navigation graphs, enabling agents to move freely in 3D\nspaces. This framework has gained prominence for its realistic\nand adaptable approach to robotic navigation, allowing agents\nto respond effectively to verbal commands. Previous works\nsuch as ERG [24], VLN-Bridge [10], and CKR model [8]\nprimarily focus on reinforcement learning methods. However,\nreinforcement learning requires lots of interactive data.\nLarge language models (LLMs) have recently illustrated\nremarkable performance in various fields. Several recent stud-\nies have explored the versatility of LLMs in interpreting and\nnavigating complex digital environments, demonstrating their\nremarkable performance in various fields. For instance, Velma\n21] adopts the LLM in Street View VLN tasks. Esc [28]\nand LFG [22] focus on zero-shot object navigation(ZSON)\ntasks. ProbES [16] further enhances the generalization of\nLLMs in REVERIE tasks. We aim to leverage the wealth\nof prior knowledge stored in LLMs to construct an agent\nwith better generalization abilities for VLN-CE tasks. This\nagent receives dual input from visual and language modalities.\nIt summarizes the key information from the two modalities\nthrough its abstract knowledge structures powered by LLMS,\nbridging sensory modalities and establishing abstract concepts\nand knowledge structures.\nTo this end, we propose Cog-GA (Cognitive-Generative\nAgent), a LLM-based generative agent for vision-language\nnavigation in continuous environments. One of the key\nchallenges in building an efficient VLN agent with LLMs is\ntheir lack of inherent spatial memory abilities, as LLMs are\ntrained on flattened text input, lacking the ability to model 3D\nspatial environments natively. To address this, we introduce\nthe cognitive map, which maintains spatial information\nrelated to scene descriptions and landmark objects at each\nnavigation step as a graph. These recorded spatial memories\nare then retrieved and utilized in subsequent navigation\nsteps. Another core challenge is that valuable waypoints for\ndecision-making by LLMs are often sparsely distributed in\nthe environment. To construct a more reasonable and efficient\nsearch space for the agent, we employ the waypoints predictor\n. For each waypoint, we adopt the dual-channel theory\n[12], [17] to describe the observed scene efficiently, which\ndivides scene descriptions into the \"what\" stream related\nto landmark objects and the \"where\" stream concerning\nspatial characteristics of indoor environments. This division\naligns well with the navigation task of reaching objects\nin different environmental contexts. Since the instructions\nreceived by the agent can be separated into sub-instructions\ncorresponding to reaching objects and switching environments,\nthe LLM can effectively focus on the current target. We further\nintroduce a reflection mechanism with a waypoint instruction\nmethod to enable the agent to abstract new knowledge from\ninteractions with the environment. The LLM then combines\nthese past experiences with the spatial information from the\ncognitive map to perform more informed navigation planning\nand facilitate continuous learning and adaptation. Cog-GA\nemploys the LLM to fuse perception results and historical\ninformation by maintaining temporal, spatial, and descriptive\nmemories in a cognitive map. Each navigation step optimizes\nthe search space using predicted waypoints, abstracting scene\ndescriptions through dual \"what\" and \"where\" channels to\nemphasize relevant objects and spatial contexts. The system\nlearns from experience and adapts its policy, with a reflection\nmechanism capturing navigation feedback via the LLM.\nExtensive experiments on the VLN-CE dataset confirm that\nour Cog-GA agent achieves promising performance with\npsychologically human-like behavioral simulation. This work\nlays a foundation for developing more intelligent, human-like\nvision-language navigation agents that can strategically adapt\nto new environments while leveraging prior knowledge from\nlanguage models. Our key contributions can be summarized\nas follows:\n\u2022 We propose the Cog-GA framework, a generative agent\nbased on large language models (LLMs) for vision-\nlanguage navigation in continuous environments (VLN-\nCE), simulating human-like cognitive processes, includ-\ning cognitive map construction, memory retrieval, and\nnavigation reflection. Experiments demonstrate that Cog-\nGA achieves a 48% success rate comparable to the\nstate-of-the-art on the VLN-CE dataset.\n\u2022 We introduce a cognitive map-based memory stream\nmechanism that stores spatial, temporal, and semantic\ninformation, providing contextual knowledge to the LLM\nto facilitate navigation planning and decision-making.\n\u2022 We introduce a waypoints predictor and a dual-channel\n(\"what\" and \"where\") scene description approach that\noptimizes the search space, enabling the LLM to focus\non current goals. This method significantly improves the\nnavigation success rate."}, {"title": "II. RELATED WORK", "content": "A. Vision-Language Navigation\nVisual language navigation (VLN) is an emerging inter-\ndisciplinary field that has garnered significant attention from\nnatural language processing, computer vision, and robotics\ncommunities. Anderson et al. first proposed the VLN task in\n2018, introducing the Room-to-Room (R2R) dataset based\non real-world environments [2]. Following the release of the\nR2R dataset, various expansions of VLN tasks emerged, such\nas the outdoor visual language navigation Touchdown [5] and\nRemote Embodied Visual Referring Expression in Real Indoor\nEnvironments (REVERIE) [18]. Numerous methods have\nbeen proposed to address these tasks. A notable advancement\nis the Reinforced Cross-Modal Matching (RCM) approach,\nwhich has outperformed baseline methods by 10%\nThe History Aware Multimodal Transformer (HAMT) has\nset a new benchmark in long-term navigation, demonstrating\nthe importance of incorporating historical context in naviga-\ntion tasks [7].\nB. VLN in Continuous Environments\nIn traditional VLN tasks, agents navigate through a\nrestricted graph, an unrealistic assumption for real-world\nnavigation robots. Krantz et al. [15] extended the discrete\nR2R VLN task setup to continuous environments, where\nagents make navigation decisions in freely traversable 3D\nspaces. The VLN-CE framework introduces new challenges\nand brings the task closer to real-world navigation scenarios.\nZhang et al. [27] highlighted the significant impact of visual\nappearance features on agent performance, underscoring\nthe need for models that generalize better across diverse\nenvironments. Similarly, Wang et al. [25] introduced the Re-\ninforced Cross-Modal Matching (RCM) approach, achieving\nnotable improvements in navigation performance. Guhur et\nal. [9] demonstrated that in-domain pretraining using the\nBnB1 dataset significantly enhances generalization to unseen\nenvironments.\nTo address the complexities of the VLN-CE task, Hong\net al. [10] showed that agents navigating with predicted\nwaypoints perform significantly better, reducing the discrete-\nto-continuous gap. Wang et al. [24] proposed an Environment\nRepresentation Graph (ERG) that strengthens the relationship\nbetween language and environment, leading to improved\nperformance in VLN-CE tasks. Chen et al. [6] introduced\nDNA, a direction-guided navigator agent that integrates\ndirectional cues from instructions into the encoder-decoder\nframework. However, current VLN-CE methods require\nmassive training to obtain prior knowledge.\nC. Large Language Models Guided Navigation\nLeveraging their powerful information processing capabili-\nties and extensive prior knowledge, large language models\n(LLMs) have emerged as a novel approach for reasoning in\nnavigation tasks. The VELMA model [21] uses LLMs to\npredict movement directions based on landmark information\nfrom language instructions. The Esc model [28] considers\ncorrelations with target objects at both the object and\nroom levels. For zero-shot object navigation, LFG [22]\nintroduced chain-of-thought (CoT) reasoning in LLMs to\navoid navigating to irrelevant areas. Cai et al. [3] extended\nthe use of CoT by clustering panoramic images into scene\nnodes and employing CoT to decide whether to explore"}, {"title": "III. MATERIAL AND METHODS", "content": "We leverage the LLM to stimulate the cognitive process of\nnavigation, including creating the cognitive map, instruction\nunderstanding, and the reflection mechanism. By introducing\nLLM, the VLN agent can obtain tremendous prior knowledge,\nwhich enables the agent to process tasks effectively. We\nconstruct a graph-based cognitive map as external memory to\naddress the LLM lack of long-term and spatial memory. That\nallows the LLM-based agent to understand and remember\nthe continuous environment.\nA. Generative Agent for VLN-CE Tasks\nWe divide VLN-CE tasks into three phases: generating\nthe search space, high-level target planning, and low-level\nmotion generation. Constructing the search space, a crucial\npreprocessing step, segments the continuous environment\ninto waypoints, simplifying navigation to point selection\nand improving efficiency. We use a planner based on large\nlanguage models (LLMs) for high-level target planning. It\nselects a waypoint as the next target based on current sub-\ninstructions using spatial memories from the cognitive map\nof the memory stream. The target waypoint is then sent to\nthe motion generator for action execution.\nWe propose a generative agent for VLN-CE tasks, including\nthe waypoint predictor, memory stream, instruction process-\ning, high-level planner, and reflection module. The instruction\nprocessing module divides the task into shorter-range sub-\ninstructions. The waypoint predictor uses panoramic obser-\nvations to construct the waypoint search space at each step.\nA scene describer characterizes the observed scene, dividing\ndescriptions into the \"what\" stream (landmark objects) and the\n\"where\" stream (spatial characteristics). These descriptions\nadjust the sub-instruction for alignment with the environment,\nallowing the planner to focus on the current target: cognitive\nmaps and reflection memories from the memory stream guide\nthe high-level planner. The planner uses scene descriptions,\nmemories, and the sub-instruction to compose a prompt for the\nLLM. The target waypoint index is conveyed to the low-level\nactuator. During movement execution, a reflection generator\nassesses the navigation results, constructing feedback to\nevaluate each navigation impact of step.\nB. Cognitive Map based Target Inference\nHumans and animals create cognitive maps to code, store,\nand retrieve information about their environments' relative\nlocations and attributes. Introduced by Edward Tolman"}, {"title": "in 1948 [23], this concept explains how rats learn maze\nlayouts and apply them to humans for navigation and spatial\nawareness. We use the cognitive map for LLM-based agents\nand VLN-CE tasks.", "content": "A significant challenge for LLMs is the lack of long-term\nand spatial memory, making external memory crucial [11],\n[30]. We address this by introducing a graph-based cognitive\nmap as external memory, which builds and stores spatial\nmemory to help the LLM understand and remember the\nenvironment.\nThe cognitive map starts as an undirected graph G(E,N)\nwith nodes $N_p$ for traversed spaces and $N_o$ for observed\nobjects. $N_o$ nodes connect to their corresponding $N_p$ nodes\nwith 1-weight edges $E_{p,o}$. Connections between $N_p$ nodes\n($E_p$) are weighted to represent distance and angle between\nwaypoints, ranging from 0.25 to 3 for distance and 1 to 8\nfor direction. Each $N_p$ node also has a time step label t. The\ncognitive map graph is represented as:\n$G({E_{p,o}, E_p}, {N_p,N_o})$\nRetrieving the cognitive map is crucial for target inference.\nAs shown in Figure 3, we define two retrieval methods: the\nhistory and observation chains. The history chain focuses\nprimarily on navigated nodes, providing planners an abstract\nview of the current path. In contrast, the observation chain\nfocuses on potential targets between the current and previous\npositions, offering a broader view of past decisions.\nFigure 4 outlines the target inference process. After the\nwaypoint predictor segments the panorama, the LLama-\nbased scene describer processes the waypoint image into\n'where' and 'what' related words. These waypoints update\nthe cognitive map in the memory stream. The history\nchain, environment descriptions, reflection memory, and sub-\ninstruction form a unified prompt input to the LLM-based\nplanner. The planner outputs the target waypoint index, which\nis stored in the memory stream for the cognitive map. The\nactuator then extracts distance and angle information for the\nagent's action."}, {"title": "C. Instruction Rationalization based Instruction Processor", "content": "For VLN-CE agents, handling instructions is nontrivial.\nUsing unprocessed instructions directly confuses the planner,\ncausing it to perform meaningless actions. To solve this,\nwe propose an instruction rationalization mechanism. We\nbreak the instruction into several sub-instructions using LLMS\nto guide the agent. However, the original sub-instructions\noften lack context. For example, the sub-instruction \"Exit the\nliving room.\" might confuse the agent about its current target,\ncausing it to repeat routes. Therefore, we include current\nenvironment information and the unprocessed instruction to\nadjust the sub-instruction. This process can be expressed as\n$I_{i,1} = R(I_{i,0}|D,I) \\rightarrow ... \\rightarrow I_{i,n} = R(I_{i,n-1}|D,I)$\nwhere $I_{i,0}$ is the original sub-instruction, and $I$ is the\nunprocessed instruction. As the agent moves through the\nenvironment, sub-instructions are continuously rationalized.\nIf a sub-instruction is completed, the agent moves to the\nnext one until all sub-instructions are finished. For example,\nthe rationalized sub-instruction \"Find the door of the living\nroom and look for the sign to the kitchen\" is more effective\nfor the agent. At the start of the navigation task, the agent\nbreaks down the natural language instruction into multiple\nsub-instructions. Each sub-instruction is updated based on\nobservations at each time step as the agent moves, a process\nwe call instruction rationalization. Detailed discussions of\ninstruction rationalization will be provided in the appendix.\nD. Generative Agent with Reflection Mechanism\nThe concept of a generative agent, which blends AI with\nhuman-like simulation, represents a significant advancement.\nThese agents mimic human behaviors based on interactions\nwith the environment and past experiences. VLN-CE closely\nmimics real-world navigation, making it an ideal applica-\ntion for simulating psychological processes during human\nnavigation."}, {"title": "IV. EXPERIMENTS", "content": "To verify the performance of our agent, we deployed our\nmethod in VLN-CE environments. This section outlines our\nexperimental setup and implementation details and compares\nour performance against standard VLN-CE methods. We\nalso highlight several notable features of LLM agents that"}, {"title": "could inform future research directions. Finally, we assess\nthe impacts of our core methods and provide visual analyses.", "content": "A. Experimental Setup\nWe conducted experiments on the VLN-CE dataset [15],\nwhich includes 90 Matterport3D [4] scenes. Due to the\nextended response time of LLaMA, we randomly selected 200\ntasks in unseen validation environments for our experiments.\nFollowing the methodologies of [13], [15], we used five\nevaluation metrics [1]: Navigation Error (NE), Trajectory\nLength (TL), Success Rate (SR), Oracle Success Rate (OSR),\nand Success Rate weighted by Path Length (SPL), with SR\nbeing the primary metric.\nB. Implementation Details\nWe utilize Vicuna-7b [29] as the scene describer to\nalign visual modality information with natural language\ninformation. For path planning, considering the balance\nbetween performance and response time, we adopted GPT-3.5.\nAs used in [10], the Waypoint Predictor is employed with\na candidate waypoint number set to 7. Our experiment is\nimplemented in PyTorch, utilizing the Habitat simulator [20],\nLangChain, and trained on two NVIDIA RTX 4090 GPUs.\nC. Comparison with Previous VLN-CE Methods\nIn line with previous research, we compare our agent with\nfive previously published VLN-CE methods: Waypoint [13],\nCMA [15], BridgingGap [10], LAW [19], and Sim2Sim [14].\nAll experiments were conducted using the same setup. The\nresults are presented in Table I. Our Cog-GA demonstrated a\nnotable advantage in Success Rate (SR) and Oracle Success\nRate (OSR), indicating that the LLM-based agent performs\nbetter and effectively transfers its prior knowledge. However,\nit is essential to note that the trajectory length is significantly\nhigher than other methods. That is attributed to the agent's\nconservative stopping mechanism, which prefers to get as\nclose to the target point as possible.\nD. Ablation Experiments\nTo verify the effectiveness of each component of our\nmethod, we conducted ablation experiments based on the\nvalidation setup in unseen environments. These experiments\nfocused on the influence of each element on Trajectory\nLength (TL), Success Rate (SR), and Oracle Success Rate\n(OSR). Specifically, we examined the reflection mechanism,\nthe instruction rationalization mechanism, and the cognitive\nmap. The results of the ablation experiments are presented\nin Table II."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce a generative agent for VLN-CE\nthat demonstrates the powerful ability of natural language\nto represent. By mimicking human navigation processes,\nthe agent excels in performance. The simulation of brain\nnavigation could bring an advantage for VLN-CE tasks. The\ncognitive map-based external memory enables the LLM agent\nto memorize spatial information. However, communication\nspeed with LLMs is a significant hurdle when using these\nagents in robotic systems. Future efforts will aim to create a\nmore efficient, high-performing generative agent and improve\nmultimodal large models for vision-language navigation."}, {"title": "APPENDIX", "content": "Algorithm 1 Cog-GA\nInput: Instruction I, Environment P\nInitialization:\nSub-Instruction Set I = {$I_{1,0}$, $I_{2,0}$, ..., $I_{n,0}$}\nCognitive Map G(E,N), Memory Stream M\ni = 1, t = 1\nrepeat\nW = {$o_1$, $o_2$, ...$o_m$}, $o_k$ \u2208 waypoint(P)\n$D_k,r_k$ = Discriber($o_k$) k \u2208 1,2, ..m\ntarget = Planner($I_{i,j}$, G, M, $D_k,r_k,k\u22081,2,..m$)\ny = T(target)\nUpdate P(y)\nUpdate G(E, N)\nexp = Reflection(y, y*, G, M, $I_{i,j}$)\nUpdate M(exp)\n$I_{i,j+1}$ = R($I_{i,j}$ $D_k,r_k,k\u22081,2,..m$,I)\nif Complete($I_{i,0}$) is true then\ni=i+1\nend if\nt=t+1\nif Complete(I) is true then\nbreak\nend if\nuntil i = n\n1. Task Setup\nIn Vision-Language Navigation in Continuous Environ-\nments (VLN-CE) [15], agents must navigate through un-\nseen 3D environments to specific target positions based on\nlanguage instructions. These environments are considered\nas continuous open spaces. The agent selects a low-level\naction from an action sequence library at each step, given\nthe instruction I and a 360\u00b0 panoramic RGB-D observation\nY. Navigation is successful only if the agent selects a stop\nwithin 3 meters of the target location.\nRecent VLN-CE solutions [10], [14] have adopted a high-\nlevel waypoint search space approach. During navigation, the\nagent utilizes a Waypoint Predictor to generate a heatmap\ncovering 120 angles and 12 distances, highlighting navigable\nwaypoints. Each angle increment is 3 degrees, and the\ndistances range from 0.25 meters to 3.00 meters, with\n0.25-meter intervals corresponding to the turning angle and\nforward step size in the low-level action space. This approach\ntranslates the problem of inferring low-level controls into\nselecting an appropriate waypoint.\n2. Optimal Prompt Mechanism for LLMs in Navigation\nTasks\nDuring the development of the agent, we observed several\nintriguing features. The structural context is crucial for\nnavigation tasks. To direct the LLM's focus toward navigation-\nrelated information, we categorized the information into\nthree distinct types: objects, room types, and directions.\nConsequently, the context should be structured in the format\n'Go (direction), Is (room type), See (objects).' Maintaining\nconcise context is essential, as complex and miscellaneous\ncontexts can disrupt the LLM's performance.\nFor waypoint selection, the clarity of surrounding environ-\nment descriptions also plays a significant role. We format the\nenvironment descriptions as 'In (direction), See (objects),\nIs (room type).' Clear and concise information reduces\nunnecessary processing burdens for LLMs and minimizes\nthe risk of irrational outputs due to redundant input. However,\na fully structured prompt alone is insufficient for VLN-CE\ntasks. Original instructions often involve multiple steps, and\nstructural division of sub-instructions can lead to information\nloss and misdirection. Therefore, we introduced a guidance\nmechanism to provide structural information for the current\ntarget while supplementing sub-instructions. As detailed in\nsection III, we divided sub-targets into 'where' and 'what.'\nThe 'where' targets involve switching environments based\non room type, and the 'what' targets involve finding specific\nobjects in the current environment. Thus, we constructed\nthe structural guidance as 'You should try to go (where)'\nand 'You should try to find (what).' This guidance updates\nsimultaneously with the rationalized sub-instruction to ensure\ncoherence.\n3. The Influence of Instruction Quality and Construct-\ning Better Sub-Instructions\nOur experiments highlighted the critical importance of\ninstruction quality on navigation outcomes. This section\nanalyzes how to enhance instruction quality during navigation\nand explores its implications for future work. As described\nin section III-C, splitting instructions into multiple steps and\ncontinuously rationalizing each step has proven effective. For\nexample, the rationalized sub-instruction 'Find the living room\ndoor and look for a sign to the kitchen.' yielded better results\nthan the unprocessed sub-instruction 'Exit the living room.'.\nThis finding reveals an interesting phenomenon: for an agent\nperforming a task, the sequence of separated steps should\nmaintain instructional coherence and constantly adapt the\ndescription of the target to the practical environment. Similar\nphenomena may also occur in human cognitive processes.\nFurthermore, performance improvements observed before\nand after splitting the original instruction demonstrate that\nLLMs have limited capacity to process long-term descriptive\ninstructions. Long-term instructions can easily confuse the\nLLM by presenting multiple potential targets.\n4. The evaluation metric of reflection memory\nWe define three parameters for each reflection memory: op-\ntimal distance, proximity, and repeatability. Optimal distance\nis the dynamic time warping (DTW) between the current\nand ground-truth navigation sequences. Repeatability counts\nhow often similar memories occur, and proximity is the time\nbetween the memory and the current step. If a new memory\nis identical to an existing one, it won't be stored, and the\ncurrent memory's proximity and repeatability will be updated.\nThe score of each reflection memory is defined as follows:\n$Scorem = \\frac{|d_m - \\delta|}{\\delta} + \\frac{t_m}{T} + \\frac{r_m}{max_{r_n} \\epsilon R r_n}$\nwhere $d_m$ is the optimal distance, $\\delta$ is the threshold parameter"}, {"title": "of the optimal distance, $t_m$ is proximity, T is the current time\nstep, $r_m$ is repeatability, and R is the set of repeatability of\nreflection memories. The forgetting process will eliminate\nreflection memories with scores in the bottom 10%.", "content": "5. Vision-Language Navigation Task Sample\nFor the following figures, the left part is the first view\nimage chosen by the agent, and the right part is the map\nfor the task environment. The 'Action,' 'In,' and 'See' are\nthe linguistic observations and movements for the first view\nimage chosen by the agent.\nTASK INSTRUCTION: Exit the living room and turn\nright into the kitchen. Turn left at the end of the counter and\nwait in the room across the hallway slightly to the left."}]}