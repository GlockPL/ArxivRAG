{"title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models", "authors": ["Ruiyi Zhang", "Yufan Zhou", "Jian Chen", "Jiuxiang Gu", "Changyou Chen", "Tong Sun"], "abstract": "Large multimodal language models have demonstrated impressive capabilities in\nunderstanding and manipulating images. However, many of these models strug-\ngle with comprehending intensive textual contents embedded within the images,\nprimarily due to the limited text recognition and layout understanding ability. To\nunderstand the sources of these limitations, we perform an exploratory analysis\nshowing the drawbacks of classical visual encoders on visual text understanding.\nHence, we present LLaVA-Read, a multimodal large language model that utilizes\ndual visual encoders along with a visual text encoder. Our model surpasses existing\nstate-of-the-art models in various text-rich image understanding tasks, showcasing\nenhanced comprehension of textual content within images. Together, our research\nsuggests visual text understanding remains an open challenge and an efficient visual\ntext encoder is crucial for future successful multimodal systems.", "sections": [{"title": "Introduction", "content": "Instruction tuning [1, 2] has demonstrated remarkable generalization abilities across unseen tasks,\ncontributing to the increasing adoption of large language models (LLMs) such as GPT-4 [3]. Recently,\nmultimodal language models have benefitted from visual instruction fine-tuning [4, 5, 6, 7, 8], leading\nto significant successes in real-world applications. These models utilize visual encoders such as\nCLIP-ViT [9, 10] to imbue LLMs with image comprehension capabilities. However, challenges\npersist in comprehending textual information within images, likely stemming from the prevalence of\nnatural images in training datasets such as Conceptual Captions [11] and COCO [12]), as highlighted\nby [13]. To address this, [14] proposed improving end-to-end visual instruction-tuned models by\nintroducing noisy Optical Character Recognition (OCR) annotations to improve vision language\nalignment. Additionally, low-resolution visual encoders pose challenges as a minimum of nine pixels\nare required to recognize a word. Previous works [15, 16, 17] have explored various methods to\nimprove encoder resolution, resulting in significant performance gains in various downstream tasks.\nHowever, it is worth noting that high-resolution encoders typically require more resources for image\nencoding and produce more visual tokens for language models to process, leading to inefficiencies in\ntraining and inference. [18, 19] have proposed methods such as visual token merging and smarter\narchitecture designs to mitigate these challenges and enhance model performance.\nDocument images often comprise text-rich content, with the visual components typically being simple\nwhile the textual parts are densely packed. A pertinent inquiry arises regarding the proficiency of\nexisting visual encoders in encoding visual text and generating visual tokens for language models.\nTo address this, we conducted synthetic experiments to assess visual encoders' performance in\ntext recognition and compare it with open-source Optical Character Recognition (OCR) tools. Our\nanalyses reveal that OCR tools exhibit superior efficiency and accuracy in encoding large text blocks,\nwhereas popular visual encoders excel in recognizing smaller and shorter words and phrases. In\naddition, OCR tools can seamlessly scale up to process high-resolution images at minimal cost.\nMotivated by these findings, we propose a novel architecture named LLaVA-Read that integrates\nmultiple visual encoders. Our rationale dictates that a visual encoder should efficiently capture"}, {"title": "Related Work", "content": "Multimodal Instruction Tuning Multi-modal instruction tuning, including image [4, 20, 8], video\n[21, 22], and audio [23, 24] settings, has been an active research topic. Most efforts aim to integrate\nvisual representations, which are obtained through an independent visual encoder, into large language\nmodels. MiniGPT-4 [7] uses ChatGPT to generate high-quality instruction-following data, while\nLLaVA [4] generates such data by prompting GPT-4 with captions and bounding boxes. Previ-\nous works [25, 26] generate more than 1M high-quality data for multimodal LLM training via\nprompting OpenAI GPT-4V. LLaMA-Adapter [27, 28] aligns text-image features using COCO data,\nand mPLUG-owl [29] combines extensive image-text pairs for pretraining and a mixture of data\nfor fine-tuning. InstructBLIP [20] addresses this by transforming 13 vision language tasks into an\ninstruction-following format. mPLUG-Owl [30, 29] apply multitask instruction funetuing using\nexisting document datasets. Previous works [31, 15, 16, 17, 32, 33] have investigated different ways\nto improve encoder resolution, receiving great improvement in various downstream tasks. A compre-\nhensive survey is available [34]. Despite this, many models struggle with visual text understanding\ntasks [13]. The proposed LLaVA-Read aims to improve the text-rich image understanding ability,\nwhere both visual objects and visual texts understanding can be done simultaneously.\nVisual Document Understanding There have been efforts to boost Multimodal large language\nmodels (LMMs) to better comprehend text-rich images, including document images. Among these,\nLLaVAR [14] uses GPT-4 to collect fine-tuning data without human annotations using OCR and\ncaptioning tools. It discovered that resolution plays a significant role in recognizing textual infor-\nmation and explored several options. TGDoc [35] improves LLaVAR and explores text-grounding\nfor multimodal LLMs. Monkey [18] performed a surgery between simple text labels and high input\nresolution, enabling remarkable performance in visually-rich document images with dense text.\nTextMonkey [36] has implemented shifted window attention to filter out similar tokens effectively.\nMeanwhile, DocPedia [37] and HRVDA [38] have focused on enlarging input resolution to reduce\nthe disparity between multimodal LLMs and visual document understanding. Recent works consider\nfigures from academic papers as the input, which are composed of text and figures [39, 29]. InternLM-\nXComposer2 [17] scales up the visual encoder's resolution to 4,096. OCR-based methods have been\ncriticized for inducing more errors [40], which can now be alleviated with the help of large language\nmodels and visual encoders. LLaVA-Read uses PaddleOCR as a visual-text encoder because of its\ngood generalization ability, and it can also use other visual encoders with great generalization ability.\nVisual Text Understanding Humans are incredibly robust to a variety of text permutations [41]\nbecause they can leverage the graphical information in text [42]. Previous work on visual language\nmodeling aims to handle unseen out-of-vocabulary (OOV) words to overcome the drawback of a\nfixed vocabulary, which may lead to performance degradation [43]. PIXEL [44] achieved comparable\nperformance with BERT [45], but it can only perform natural language understanding tasks. Pixar [46]\nproposed the first pixel-based autoregressive LLM that performs text generation. [47] developed"}, {"title": "LLaVA-Read: Enabling LLaVA to Read", "content": "LLaVA-Read is designed to enhance the comprehension of textual information within images,\nparticularly in text-rich images. An overview of the model is shown in Figure 1. LLaVA-Read\ncomprises multiple visual encoders, a visual-text encoder, and a large language model (LLM) serving\nas the decoder. Given an input image \\(X\\), the visual encoders generate visual features \\(Z_v = f_v(X)\\),\nwhere \\(f_v\\) consists of two visual encoders. Subsequently, we employ a multi-layer perceptron\n(MLP) projection \\(g\\) to transform \\(Z_v\\) into visual tokens \\(H_v = g(Z_v)\\) for the large language model.\nNotably, \\(H_v\\) shares the same embedding dimensions as the text tokens used by the LLM tokenizer.\nDifferent from the conventional architecture of multimodal large language models [4], LLaVA-Read\nincorporates a visual-text encoder \\(f_t\\) to better capture textual and layout information, along with a\nhigh-resolution encoder for finer visual details. The objective of the visual-text encoder is to extract\ntext from an image, yielding visual-text tokens \\(H_t = f_t(X_v)\\). Subsequently, we concatenate \\(H_v, H_t\\),\nand \\(H_q\\), feeding them into the large language model to generate the desired response \\(Y\\).\nIn designing LLaVA-Read, we hold the belief that a visual encoder should specialize in processing\nvisual objects, while a lightweight visual-text encoder should focus on extracting text within images.\nThis approach, we believe, enhances the efficiency of the visual components, as text recognition\npresents distinct patterns compared to visual object detection. Although high-resolution visual\nencoders can capture finer details, they also generate a larger number of visual tokens. To mitigate\nadditional computational costs associated with employing two visual encoders in LLaVA-Read, we\nmerge the output of these encoders while maintaining the same visual tokens as in LLaVA. More\ndetails on architectural design are elaborated in Section 3.1. In essence, LLaVA-Read offers a\nmultimodal LLM framework that leverages multiple visual encoders to improve visual token learning\nand conversion efficiency. To foster enhanced collaboration between multiple visual encoders, we\npropose layout-aware tuning during the two-stage training of LLaVA-Read, as discussed in Sections\n3.2 and 3.3."}, {"title": "Model Architecture", "content": "Visual-Text Encoder Successful commercial visual-text extractor solutions typically have much\nsmaller sizes compared to visual object detection models [48, 49, 50]. Increasing the resolution of\nthe visual encoder for visual text recognition often incurs unnecessary computational costs, resulting\nin training and inference inefficiencies. While visual encoders excel at comprehending visual object\ninformation and scene texts, they often struggle with processing large chunks or paragraphs of visual\ntext (further details in Section 4.1). Solutions such as Donut [40] and LayoutLM [51] offer neat\napproaches, but their generalization abilities are limited due to constraints in the pretraining dataset\ndomains. Therefore, we consider employing open-source OCR tools as an alternative encoder to\nextract text and layout information. LLaVAR [14] initially utilized PaddleOCR 1 to construct a noisy\npretraining dataset to enhance text recognition capabilities. Consequently, we integrate the lightweight\nPaddleOCR as our visual-text encoder. One major concern with the use of OCR-based methods is\nthe potential for induced errors. However, collaboration between the visual encoder and the large\nlanguage model mitigates this drawback. Additionally, more robust visual-text encoders (including\nboth OCR-based and OCR-free ones) can replace PaddleOCR in our framework, potentially offering\nenhanced performance. We use PaddleOCR in our paper as an example to verify our conviction on\nvisual-text encoders. Furthermore, it demonstrates high efficiency in converting visual texts into text\ntokens for LLMs with excellent generalization ability.\nWe customized a customized OCR tokenizer to effectively encode both words and their respective\nlocations (i.e., text bounding boxes). This tokenizer comprises a layout recovery module \\(f_r(\u00b7)\\) and\na standard LLM tokenizer \\(f_q(\u00b7)\\). Upon receiving OCR results from a text-rich image, the layout\nrecovery module \\(f_r\\) processes the input by inserting spaces and line breaks, as described in [52].\nThe layout recovery process follows a heuristic approach: i) Text boxes in the same row with\ndetected words are identified and rearranged in top-to-bottom and left-to-right order based on their\ncoordinates. ii) The average character width is calculated for each row based on its width and word\ncount. Placeholders are then inserted based on the horizontal distance between two text boxes in the\nsame row, resulting in the extraction of single-row texts. iii) Newline characters are inserted for each\nrow, reconstructing the page layout. Figure 8 in the appendix provides an example of how the OCR\ntokenizer operates. Once the plain text with layout information is obtained, it serves as part of the\nLLM prompts in both training and inference: \\(H_t = f_t(X_v) = f_q(f_r(f_{ocr}(X_v)))\\).\nVisual Encoders LLaVA with low-resolution visual encoders has demonstrated significant success\n[4], and the integration of a higher resolution encoder typically leads to performance improve-\nments [33]. However, high-resolution encoders tend to generate a larger number of visual tokens,\nand methods such as similarity-based token merging or compression may sacrifice details. Ideally, a\nhigh-resolution encoder should focus on question-related details without significantly increasing the\nnumber of visual tokens for language models. To address this, we propose a novel approach to merge\ndetails from high-resolution encoders to low-resolution encoders. Specifically, we utilize the pre-\ntrained OpenCLIP model ConvNext-L/32-320 as the high-resolution encoder \\(f_h\\) and the pretrained\nCLIP model ViT-L/14-336 as the low-resolution encoder \\(f_s\\). The high-resolution visual encoder\nwith an image patch size of 32 can accommodate approximately 2.3 times higher resolution images\ncompared to the low-resolution encoder with a patch size of 14. For example, if the low-resolution\nencoder takes the image \\(X_v\\) with dimensions 336 \u00d7 336, then the high-resolution encoder processes\nthe image \\(X_{vn}\\) with dimensions 768 \u00d7 768. Position embedding interpolation will be applied for\nencoders if the resolution is higher than 768.\nTo prevent the generation of additional visual tokens, we combine the visual features of both visual\nencoders as follows: \\(f_v(X_v) = f_h(X_{vn}) + f_s(X_{vo})\\), where \\(f_h\\) and \\(f_s\\) are two fully connected layers\nwith the same input and output dimensions as \\(X_v\\). This operation ensures that the resulting features\nare of the same size as the low-resolution ones [33]. This straightforward merging strategy proves to\nbe effective in text-rich image understanding, as demonstrated by empirical results in Section 4."}, {"title": "Layout-aware Pretraining for Feature Alignment", "content": "Starting with the LAION-5B dataset, we selectively retained images prominently featuring text.\nFrom the filtered LAION-5B, a random sample of 10,000 images was clustered into 50 groups\nbased on CLIP-ViT-B/32 visual features [53]. After careful examination of the clustering results,"}, {"title": "Layout-aware Finetuning for Instruction Following", "content": "Jointly understanding both visual texts and objects is crucial to efficiently analyzing text-rich images.\nTo enhance the model's visual object understanding, we performed finetuning using the natural\nimage finetuning dataset from LLaVA. Although scaling up the dataset could potentially further\nimprove visual object understanding, we did not explore this direction in this paper. To improve"}, {"title": "Experimental Results", "content": "We first perform a visual text understanding analysis, which inspires us to propose the visual-text\nencoder branch in LLaVA-Read. Then, we evaluate the performance of LLaVA-Read on classical\ntext-rich image benchmarks and OCRBench [13]. We pretrain our model for 1 epoch to obtain\nprojection layers with a batch size of 128, a context window size of 2048, and a learning rate of 2e-3.\nWe further fine-tune LLaVA-Read on the 425k instruction tuning set for 3 epochs with a learning rate\nof 2e-5 with a batch size of 32 and a context window size of 4096. We use Vicuna-1.5 13B as the\nbase language model. All experiments were performed on NVIDIA A100s."}, {"title": "Visual Text Understanding Analysis", "content": "Settings Following previous work [44, 46, 47], we generate synthetic data to evaluate the text\nrecognition ability of different visual encoders by varying font sizes and number of words, as shown\nin Figure 2. We use PaddleOCR as a simple and effective visual-text encoder and OpenAI CLIP\nplus trained projection layers to inspect the text recognition ability of visual encoders. We use\nmultiple fonts to render text-rich images and use the OCR accuracy as a metric. For PaddleOCR\nand multimodal LLM, accuracy means that the rendered ground-truth words can be exactly found\nin the outputs. For CLIP with projection, we first obtain the model outputs, which are visual token\nembeddings, and then perform similarity-based ranking with words from the language model's\nvocabulary. If the ground truth words can be found in the top-3 words, we count these are detected by\nthe model. We list a few research questions to help the reader better understand our experimental\nresults. Please note that we removed stop-words from the NLTK [64] package as many repeat\nstop-words exist in text paragraphs.\nRQ1: How many pixels do we need to recognize words? We first investigate the performance\nof different modules on text recognition ability with different font sizes. In Figure 2a, all text-rich\nrendered images have a plain white background, which is similar to the scan document images or\nscreen shots. In Figure 2b. All rendered text-rich images are rendered with a random selected image\nas the background, corresponding to the scene text and poster settings. In both scenarios, we use the\nterms of machine learning as the texts to recognize, each phrase containing no more than four words.\nWe measure the font size with its vertical heights. CLIP with projection can recognize texts with a"}, {"title": "Main Results", "content": "We evaluate the LLaVA-Read and its baselines on OCRBench and other text-rich image benchmarks 2\nin Table 2 and Table 4(a). LLaVA-Read shows state-of-the-art performance in the OCR bench\namong open-source models and comparable performance with Gemini and GPT-4v. Compared\nwith other baselines, LLAVA-Read with low-resolution encoders can beat Text-Monkey, the best\nopen-source model with a large gap, showing the benefits of adding visual-text encoders. Specifically,\nperformance on KIE and other classical document VQA is greatly improved, where large chunks of"}, {"title": "Limitation and Broader Impact", "content": "LLaVA-Read uses PaddleOCR as its visual text encoder, which relies on the accuracy of PaddleOCR.\nAlthough the language model and visual encoder can mitigate this issue, it may still negatively\naffect model performance if there are errors or inaccuracies in text extraction. Training a visual-text\nencoder on a large corpus with a similar architecture to Donut [40] should further enhance LLaVA-\nRead performance. In addition, LLaVA-Read still requires computational resources for training and\ninference, which may limit its practical applicability in resource-constrained environments or on\ndevices with limited processing power. For a wider impact, LLaVA-Read can enhance accessibility\nfor people with visual impairments by providing accurate and efficient text extraction from images.\nFurthermore, LLaVA-Read can significantly reduce manual efforts in tasks such as data entry,\ninformation retrieval, and document analysis, leading to increased productivity and efficiency."}, {"title": "Visual Text Understanding Analysis Details", "content": "Figure 4: Different length of dense texts with plain background."}, {"title": "", "content": "puter Vision\nComputer Vision\nComputer Vision\nComputer Vision\nComputer Vision\nComputer Vision\nComputer Vision\nComputer Vision Computer\nVision\nFigure 5: Different font sizes with natural image background."}, {"title": "", "content": "Computation Computer\nand Language Science\nComputer\nVision\nData\nStructures\nAlgorithms\nFormal\nLanguages\nAutomata\nTheory\nGraphics\nLogic in\nComputer\nScience\nMachine\nLearning\nOther\nComputer\nScience\nPerformance\nSocial\nInformation\nNetworks\nSoftware\nEngineering\nFigure 6: Different ML terms with plain background."}, {"title": "B Training Data Details", "content": "B.1 Pretraining Data Examples\nWe present pretraining instruction templates of Task II in Table 7, Task III in Table 6 and Task IV in\nTable 8. Pretraining examples randomly selected are shown in Figure 7 and 8.\nB.2 Finetuning Data Examples\nThe finetuning examples randomly selected are shown in Figure 7 and 8.\nNo. User Instruction\n1 Could you locate the text in the image and furnish the coordinates [xmin, ymin, xmax, ymax] for\neach text block?\n2 Please recognize all the text within the image and supply the coordinates [xmin, ymin, xmax, ymax]\nfor each text element.\n3 Can you identify and extract all the text from the image, and include the coordinates [xmin, ymin,\nxmax, ymax] for each text block?\n4 I would like you to recognize the text within the image and provide the bounding box [xmin, ymin,\nxmax, ymax] for each piece of text.\n5 Kindly identify and extract text from the image, and supply the coordinates [xmin, ymin, xmax,\nymax] for each text portion.\n6 Can you recognize all the text present in the image and provide the corresponding bounding boxes\nor coordinates [xmin, ymin, xmax, ymax]?\n7 I'm looking for you to detect and list all text within the image, accompanied by their bounding box\ncoordinates [xmin, ymin, xmax, ymax].\n8 Please analyze the image for text, and for each text segment, provide the bounding box coordinates\n[xmin, ymin, xmax, ymax].\n9 I'd appreciate it if you could identify and provide the coordinates [xmin, ymin, xmax, ymax] for all\ntext found in the image.\n10 Kindly pinpoint the text in the image and provide the coordinates [xmin, ymin, xmax, ymax] for\neach text block.\nTable 6: Task II: Text Localization Templates\nCan you identify and extract all the text from the image, and include the\ncoordinates [xmin, ymin, xmax, ymax] for each text block?\nObjectives [0.352, 0.042, 0.648, 0.128] Understand current research\nregarding FSLA [0.154, 0.289, 0.861, 0.344] Outline best practices for\nFSLA [0.158, 0.404, 0.643, 0.448] Identify tools of recovery [0.158,\n0.516, 0.541, 0.568] Making Advances: A Comprehensive Guide for\nTreating Female Sex and Love Addicts [0.023, 0.932, 0.697, 0.966]\nwww.sash.net/makingadvances [0.723, 0.935, 0.961, 0.961]\nCan you recognize all the text present in the image and provide the\ncorresponding bounding boxes or coordinates [xmin, ymin, xmax, ymax]?\nFactory [0.345, 0.159, 0.418, 0.201] Customers [0.747, 0.250, 0.957, 0.310]\nWarehouse [0.113, 0.359, 0.217, 0.388] Warehouse [0.595, 0.349, 0.700, 0.378]\nSuppliers [0.094, 0.760, 0.231, 0.815]\nCan you recognize all the text present in the image and provide the\ncorresponding bounding boxes or coordinates [xmin, ymin, xmax, ymax]?\nContinuous Business Planning [0.344, 0.117, 0.556, 0.193] Collaborative\nCustomer Feedback& Optimization [0.077, 0.279, 0.236, 0.422] Collaborative\nDevelopment [0.641, 0.310, 0.803, 0.388] DevOps [0.344, 0.458, 0.540, 0.560]\nContinuous Monitoring [0.101, 0.633, 0.243, 0.711] Continuous Testing [0.654,\n0.628, 0.794, 0.698] Continuous Release and Deployment [0.341, 0.820,\n0.573, 0.891]\nFigure 7: Pretraining Examples for Task II, which is produced by PaddleOCR."}, {"title": "", "content": "Given OCR-based Page Parser Results:\nWhat is [0.093, 0.113, 0.617, 0.207].\n14: A generated example of text grounding on screenshot."}, {"title": "Conclusion", "content": "In this paper, we first analyze the visual text understanding ability of multimodal LLMs, demonstrating\nthe essential need for integrating extra visual text encoders. Then we propose LLaVA-Read, a model\narchitecture that enhances the reading ability of multimodal large language models by integrating\nlayout information and using multiple visual encoders. Through a comprehensive evaluation on\ntext-rich image understanding tasks, LLaVA-Read outperforms existing state-of-the-art models,\ndemonstrating the effectiveness of incorporating layout information and utilizing multiple visual\nencoders in improving the comprehension of textual content situated in images. This work contributes\nto the advancement of multimodal language models and provides valuable insights for further research\nin enhancing the reading ability of such models."}]}