{"title": "Prompting with Phonemes: Enhancing LLM Multilinguality for non-Latin Script Languages", "authors": ["Hoang Nguyen", "Khyati Mahajan", "Vikas Yadav", "Philip S. Yu", "Masoud Hashemi", "Rishabh Maheshwary"], "abstract": "Multilingual LLMs have achieved remarkable benchmark performance, but we find they continue to underperform on non-Latin script languages across contemporary LLM families. This discrepancy arises from the fact that LLMs are pretrained with orthographic scripts, which are dominated by Latin characters that obscure their shared phonology with non-Latin scripts. We propose leveraging phonemic transcriptions as complementary signals to induce script-invariant representations. Our study demonstrates that integrating phonemic signals improves performance across both non-Latin and Latin languages, with a particularly significant impact on closing the performance gap between the two. Through detailed experiments, we show that phonemic and orthographic scripts retrieve distinct examples for in-context learning (ICL). This motivates our proposed Mixed-ICL retrieval strategy, where further aggregation from both leads to our significant performance improvements for both Latin script languages (up to 12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL retrieval.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable multilingual capabilities across various natural language processing (NLP) tasks. The increase in model parameters and rise of instruction datasets have led to the emergent capability of LLMs to perform tasks in few to zero shots (Brown et al., 2020; Wei et al., 2022; Nguyen et al., 2023a) or adapt efficiently to new tasks through in-context learning (ICL) during inference (Zoph et al., 2022). However, these capabilities remain disparate across languages (Lai et al., 2023a), with one particular axis being along non-Latin versus"}, {"title": "2 Background and Related Work", "content": "Phonemes are considered the smallest units of speech distinguishing one word (or word element) from another. Since LLMs are trained on text, they might implicitly encode some information from the phonemes present in the text; however, investigative research on the phonemic awareness in language modeling is very limited. We discuss related work in phonemic awareness in NLP and other approaches to mitigating the performance divide of scripts., motivating the need for our in-depth exploration of phonemic integration with LLMs."}, {"title": "Leveraging Phonemes in Text NLP", "content": "Training text-based neural networks on both phonemic and orthographic information has given downstream task performance improvements in same- and cross-language NLP and speech tasks (Chen et al., 2014; Bharadwaj et al., 2016; Liu et al., 2024). However, such work was mostly limited to smaller LMs or task-specific models (<1B parameters) where parameter constraints remain major potential obstacles for effective integration (Wang et al., 2020). On the other hand, as LLMs scale up to many billions of parameters, training-based schemes face limited orthographic-phonemic data and high computational costs; hence, motivating our work on phonemes integration with LLMs via prompting and enhanced ICL."}, {"title": "Performance Gaps in Multilinguality", "content": "It was noted during the rise of early LLMs (<1B parameters) such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) that performance varied widely across languages (Wu and Dredze, 2020), an observation that has persisted to modern LMs (see references in Section 1). Some other script-related gaps include the greater difficulty of adapting to languages with non-Latin scripts (Muller et al., 2021; Pfeiffer et al., 2021).\nNon-phonemic approaches to bridging multilingual gaps via finetuning involved training schemes, translation, and contrastive presentations (Zheng et al., 2021; Kumar et al., 2022; Yang et al., 2022). With regards to script, while existing works explored improving transfer capability (Fujinuma et al., 2022; Nguyen et al., 2023b; Liu et al., 2024), they also remained limited to small LMs (<2B parameters). Bridging the gap in prompting and ICL performance gaps involved cross-lingual chain of thought and demonstrations via translation (Qin"}, {"title": "3 Is the Written Script Sufficient for Multilinguality?", "content": "While the multilingual discrepancy performance when comparing non-Latin and Latin languages have been studied for smaller Transformer-based LMs (<1B parameters), these studies may not fully apply to recent advancements in contemporary LLMs (> 7B parameters), thus requiring additional in-depth investigations.\nTo empirically measure the gap in performance on Latin versus non-Latin scripts as a baseline for our work, we conduct an extensive and explicit (i.e.,"}, {"title": "4 Prompting with Phonemes", "content": "Motivated by insights from our pilot study (Section 3), we now conduct experiments on representative task categories where different LLM families struggle to achieve similar performance between Latin and non-Latin languages, including NLG (AYA-WIKI), MT (FLORES), and question answering (AYA-MLQA) and compare on the same metrics. Due to the lack of publicly available multilingual corpora with orthographic-phonemic alignments, we adopt the approach of Bharadwaj et al. (2016); Nguyen et al. (2023b) to construct our own aligned dataset for evaluation. Specifically, we use Epitran (Mortensen et al., 2018), a linguistically-crafted tool, to generate IPA transcriptions for the orthographic-only multilingual datasets, setting up the foundation for our phonemic integration explorations with LLMs.\nIn this work, we explore a series of experiments on phonemic integration with text-based LLMs and aim at improving LLMs' inference performance without the need for pre-training or fine-tuning. In our study, we cover two contemporary mid-sized multilingual LLMs, including Llama3-8B-Instruct (Dubey et al., 2024) and Qwen2-7B-Instruct (Yang et al., 2024), chosen for their strong performance in our pilot study (Figure 2). More specifically, we explore two prompting approaches:"}, {"title": "4.1 Phonemic Integration via Direct Prompting", "content": "We first study the most straightforward approach to inject phonemic information: direct prompting. More specifically, under the assumption that LLMs might be able to surface internal knowledge about correspondences between script and phonemes effectively, we append the phonemic transcription in the prompt as additional auxiliary information for the textual script inputs. We conduct experiments with 0-shot and 3-shot prompting with random sampling. Additional details of prompt templates and variations are provided in Appendix A.2.\nAs shown in Table 2, concatenating IPA information with orthographic inputs consistently improves performance across various tasks and evaluation settings, except in the 3-shot setting for FLORES. Given that 0-shot performance on FLORES is already high with standard prompting, we hypothesize that text-based LLMs can effectively handle FLORES without much benefit from adding IPA information through in-context examples."}, {"title": "4.2 Phonemic Integration via ICL Retrieval", "content": "Besides directly injecting IPA information through prompting, we explore the use of phonemic IPA information to enhance ICL retrieval. Since LLMs are highly sensitive to various prompt formats (Lu et al., 2024), based on our observations in 4.1, we maintain a consistent Script+IPA prompt format for our experiments, using different ICL retrieval strategies in a fixed 3-shot setting.\nAdditionally, we compare the impact of few-shot example retrievals based on phonemic and orthographic similarity, against a baseline of randomly retrieved examples (Random). The retrieval methods include orthographic-based matching (Script-ICL), IPA-based matching (IPA-ICL), and our proposed mixed strategy (Mixed-ICL). In the Mixed-ICL approach, matching scores are calculated separately with Script and IPA, then averaged for each sample. The top 3 samples are selected after re-ranking by the averaged scores, leveraging both orthographic and phonemic similarity information. Additional in-depth comparisons with other mixing strategies are further explored in Appendix B.4. Due to a lack of phonemic transcription embedders, we focus our studies on using lexical retrieval (namely BM25 sparse retrieval; Trotman et al., 2014; Luo et al., 2023) to compute matching scores across all of our ICL variants unless stated otherwise. Our similar study on a dense retriever variant (Appendix B.3) further validates"}, {"title": "4.3 Reducing the Performance Gap between Latin versus non-Latin Languages", "content": "We observe that the inclusion of IPA information in ICL leads to improved performance for both Latin and non-Latin script languages, and especially contributes to better performance for non-Latin script languages, helping reduce the previously observed performance gap (Table 4). In particular, we find that the Mixed-ICL strategy contributes to the most gains across all tasks, with the exception of FLORES on Latin languages, where the IPA-ICL strategy achieves stronger performance gain; refer to Appendix C for absolute performance details. This reinforces our main finding: integrating phonemic information in ICL prompting leads to improved performance for not just non-Latin languages, but also Latin languages, with non-Latin languages seeing higher gains. This study also reiterates the essentials of leveraging IPA as phonemic representation for universal language support since romanization does not exist"}, {"title": "5 How do Script vs IPA vs Mixed Work for ICL?", "content": "To gain a better intuitive understanding of different ICL retrievers, we conduct additional qualitative studies by (1) probing the generated output differences when promping with different ICL retriever methods, and (2) investigating the top retrieved examples from different retrievers. Observing the LLM generations when prompted with different ICL approaches (Figure 3), the Mixed-ICL strategy shows a more comprehensive answer than its Script-ICL and IPA-ICL counterparts, resulting in the answer closest to the ground truth target among the three compared.\nAdditionally, when comparing the top retrieved examples from different ICL approaches (Figure 4), we observe the top retrieved example from Mixed-ICL covers the most similar concepts with the original input query (i.e. Drexel University vs Texas A&M University (university), Philadelphia vs San Antonio (location), political science vs law (governance)). This showcases its closer semantic connections with input query sample as compared to the other two variants, where fewer concepts were captured. These observations support our motivations in synthesizing the knowledge from both Script and IPA information via ICL, helping explain the performance improvements observed in Table 3."}, {"title": "6 Analysis", "content": "For consistency and conciseness, our analysis results are reported as averages over the aforementioned non-Latin languages and Latin languages (if applicable) for the targeted tasks unless stated otherwise."}, {"title": "Does IPA-ICL retrieval vary with different tokenizers?", "content": "Since text-based LLM tokenizers might not have been trained with processing IPA inputs, we investigate the potential impact of different tokenizers on IPA-ICL. As observed in Table 7, most tokenizers show benefits of IPA-ICL over Random baseline. However, we observe that na\u00efve CS and WS tokenizers typically perform worse than tokenizers from pre-trained LLMs (14.44% vs 10.75% relative performance gain on AYA-WIKI with Llama3-8B-Instruct and WS tokenizers respectively). On FLORES, WS and CS tokenizers even hurt task performance, resulting in the relative performance decrease of 0.51% and 0.16% when compared to the Baseline.\nOverall, our analyses confirm our findings: IPA information is an effective tool, even in different settings, towards better inference-time performance-especially for non-Latin languages."}, {"title": "7 Conclusion", "content": "We investigate the effect of integrating phonemic information towards improving the multilingual abilities of large-scale language models at inference time. Our pilot study demonstrates that the performance gap between Latin and non-Latin script languages remains high even on latest state-of-the-art LLMs (with an approximate average 23% difference in performance) for certain tasks, especially on generation tasks (approx. 65%).\nMotivated by these observations, we propose introducing phonemic integration in prompting with LLMs. We explore two incorporation mechanisms: direct prompting and ICL retrieval. While we observe performance gains with direct prompting, our empirical study demonstrates that the ICL retrieval provides an even more effective way to improve downstream task performance. The Mixed-ICL retrieval strategy captures diverse and similar ICL examples beyond the textual scripts, leading to the best overall performance across multiple tasks, also evidenced in the case studies we present.\nWe contribute an extensive empirical study on the effect of integrating phonemic information towards improving the performance of large-scale LLMs, reducing the performance gap between Latin and non-Latin performance. We show that incorporating phonemic information as IPA with few-shot ICL retrieval prompting is an effective method to improve multilingual performance for languages with differing written scripts."}, {"title": "Limitations", "content": "We conduct multiple studies and analyses towards providing a comprehensive report on how phonemic integration with orthographic prompting can improve performance for non-Latin script languages, especially towards reducing the performance gap between their Latin counterparts. However, our study has its limitations.\nFirst, we rely on the external resources for both multilingual evaluation datasets and IPA generation tools to generate the phonemic text input, and thus rely on their quality. To the best of our knowledge,"}, {"title": "Ethical Considerations and Societal Impact", "content": "Our study is mainly empirical in nature, and does not involve studies with humans. We also utilize publicly available datasets, which are reportedly non-toxic. We make our unique IPA-orthographic aligned data available publicly, and hope that along with our study, it motivates further exploration and research into the potential of including prompting with phonemic information towards better performance.\nWe believe our study could have a positive impact by reducing the performance gap of non-Latin script languages compared to Latin script languages. We hope our work will encourage the exploration of new methods in (and beyond) phonemic information integration, towards further reducing the performance gap and improving access for all.\nOur work only generates the IPA transcription from the multilingual benchmark datasets. In other words, we do not generate any new contents besides the transcriptions of the given textual scripts; hence, our work and its generated data do not pose any potential risks."}, {"title": "A Additional Setup Details", "content": "In this section, we provide details regarding our Pilot Study setup, separated into 3 main sections: (1) Task Coverage, (2) Language Coverage, (3) Experimental Setup"}, {"title": "A.1 Pilot Study Details", "content": "In this section, we provide details regarding our Pilot Study setup, separated into 3 main sections: (1) Task Coverage, (2) Language Coverage, (3) Experimental Setup"}, {"title": "Language & Task Coverage", "content": "As mentioned in the 3, the objective of our Pilot Study is to compare and contrast performance of contemporary LLMs (\u22657B parameters) on non-Latin and Latin languages. The upper bound performance is considered English (ENG) performance. Details of covered languages together their corresponding written scripts are presented in Table 8.\nWith the goal of conducting a thorough evaluation of contemporary LLMs, we cover a wide"}, {"title": "A.2 Details of main experimental setup", "content": "Prompt templates by task. In our experiments we observe that slight prompt variability can affect"}, {"title": "Dataset Statistics", "content": "Considering the future extensions of our work towards more languages, we purposely conduct our evaluations on the multilingual datasets covering a wide range of languages. For Machine Translation task, we leverage FLORES dataset (Team et al., 2024b). For AYA-WIKI and AYA-MLQA, we extract from Aya Collection Dataset (Singh et al., 2024) covering 114 languages. However, as Aya leveraged NLLB translation tool (NLLB Team et al., 2022) to generate translations, the translation data might be noisy due to the potential low-quality generations such as <unk> tokens, etc. Therefore, we conduct further quality control steps to filter out the noisy samples. For each task, we conduct evaluations on 500 randomly sampled examples to form the testing set ($D_{test}$). We also construct an example pool of 10,000 samples from the train_set ($D_{pool}$) for ICL retrieval where $D_{pool} \\cap D_{test} = \\emptyset$. The"}, {"title": "Implementation and Hyperparameters", "content": "Similar to our pilot study presented in Section A.1, for mid-sized LLMs, we conduct our empirical study via Ethereum AI evaluation framework. The hyperparameters are set similarly to the default configuration for each individual task. We conduct evaluation on Llama3-8B-Instruct and Qwen2-7B-Instruct models on 2xA100 GPUs 80GB. Each suite of experiments across all evaluated tasks will consume approximately 2 hours, resulting in total of 10 hours for one LLM type.\nFor Supervised Fine-tuning (SFT) presented in Section 6, following Maheshwary et al. (2024), we conduct full FT on M2Lingual dataset. However, for direct evaluation on the impact of in-language SFT, we extract and FT Llama3-8B-Instruct model only on samples of the targeted non-Latin languages. Our SFT training takes around 4 hours on 5xA100 (80GB) GPUs."}, {"title": "B Additional Experimental Explorations", "content": "As previously mentioned, romanization is a viable phonemic signal. Despite appealing characteristic of leveraging roman characters to transliterate and/or capture pronunciation for the target languages, romanization is not standardized and heavily language-specific, resulting in various potential romanization schemes for given languages. More importantly, since romanization is only available in non-Latin languages, it cannot be utilized to capture phonemic information for Latin languages,"}, {"title": "B.1 IPA versus Romanization", "content": "As previously mentioned, romanization is a viable phonemic signal. Despite appealing characteristic of leveraging roman characters to transliterate and/or capture pronunciation for the target languages, romanization is not standardized and heavily language-specific, resulting in various potential romanization schemes for given languages. More importantly, since romanization is only available in non-Latin languages, it cannot be utilized to capture phonemic information for Latin languages,"}, {"title": "Direct Prompting", "content": "For Direct Prompting, as observed in Table 10, in most cases, similar to Script+IPA, Scrip+Roman gains improvements over the Script-only prompting approach. However, it is unclear whether IPA or romanization serves as a better phonemic signal for this approach. For instance, on AYA-WIKI task, Script+IPA tends to perform better than Script+Roman, but the opposite might exist on FLORES task. Hence, we further study ICL based approaches with Romanization as well."}, {"title": "ICL retrieval benefits with IPA", "content": "We evaluate the impact of IPA-ICL retriever and Roman-ICL"}, {"title": "B.2 Experiments with GPT-4 and Mixtral", "content": "We study proprietary, large-scale production LLMs like GPT-4 (Achiam et al., 2023) and Mixtral-8x22B-Instruct , selected for their widespread use in industry, towards understanding whether IPA integration helps. As seen in Table 13 on 100 samples, the performance with these proprietary and production models is not consistent. While further investigation is necessary, it is difficult to ascertain data contamination with these models for all the tasks we study (Deng et al., 2024; Li and Flanigan, 2024), and the performance changes indicate that these models might have data contamination for the tasks we experiment on. We also find some samples moderated by the API, however upon further manual examination, the 6 total samples (out of the 400) focus on factual news topics such as a"}, {"title": "B.3 Dense ICL retrieval also benefits with IPA", "content": "Due to space constraints, without the loss of generality, we focus our studies on BM25 retrieval methods in the main paper, and report dense retrieval results here. For dense ICL retrieval, we select paraphrase-xlmr-multilingual-v1 (Reimers and Gurevych, 2019) as the Encoder for input query and individual samples in the ICL pools. The max_context_length is set to 512. The sentence representation is leveraged for cosine similarity computation between query and all pooling examples to select the final top-k ICL examples. Results are reported in Table 12.\nConsistent with the sparse BM25 Retrieval results reported in Section 4.2, we observe that our Mixed strategy outperforms Script and IPA based ones on all downstream tasks. Similar to observed performance with open-source LLMs, GPT4 and Mixtral 8x22B also show a trend where the Mixed strategy outperforms most others. These observations imply that ICL retrieval benefits from looking at both orthographic and phonemic information for better example selection, guiding the LLMs to-wards desired generation output for downstream"}, {"title": "B.4 Impact of different mixing strategies on ICL", "content": "Besides our Mixed strategy, there exist different approaches towards aggregating information from Script and IPA ICL retrieval. We specifically consider 4 different approaches: (1) using the Harmonic Mean to calculate the mixed score for each pool example (2) Split-Half: We retrieve top-(k//2) examples from Script and IPA separately, then aggregate them together to form the final top-k ICL samples. Within this approach, we evaluate 3 different potential ordering to aggregate ICL examples from different sources, including (a) Script+IPA, (b) IPA+Script, (c) Random Shuffle. This approach requires even k-shot samples, (3) Divide-Conquer: After sorting and retrieving top-k samples for both IPA and Script, we concatenate the corresponding scores to form top-2k samples. These samples are then ranked and filtered down to the top-k samples again based on their corresponding BM25 scores, (4) Append: We simply concatenate the scores from the two approaches and retrieve the top-k highest score as the final selected examples. Unlike previous approaches, this approach can possibly result in similar ICL examples being selected twice in the top-k samples.\nBased on our empirical study in Table 15, our Mixed strategy yields the best performance across our evaluated tasks. Split-Half aggregation is limited to even-shot ICL samples and can potentially suffer from ordering sensitivity, leading to variable evaluation performance (Lu et al., 2022)."}, {"title": "B.5 Possible Future Studies with SFT and ICL", "content": "For better understanding of the enhanced ICL as compared to continual pre-training or Supervised Fine-tuning (SFT), we conduct additional studies in which we fine-tune Llama3-8B-Instruct model with additional multilingual M2Lingual dataset (Maheshwary et al., 2024). Further implementation"}]}