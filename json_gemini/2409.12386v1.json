{"title": "Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust Speech Recognition", "authors": ["Chien-Chun Wang", "Li-Wei Chen", "Cheng-Kang Chou", "Hung-Shin Lee", "Berlin Chen", "Hsin-Min Wang"], "abstract": "While pre-trained automatic speech recognition (ASR) systems demonstrate impressive performance on matched domains, their performance often degrades when confronted with channel mismatch stemming from unseen recording environments and conditions. To mitigate this issue, we propose a novel channel-aware data simulation method for robust ASR training. Our method harnesses the synergistic power of channel-extractive techniques and generative adversarial networks (GANs). We first train a channel encoder capable of extracting embeddings from arbitrary audio. On top of this, channel embeddings are extracted using a minimal amount of target-domain data and used to guide a GAN-based speech synthesizer. This synthesizer generates speech that faithfully preserves the phonetic content of the input while mimicking the channel characteristics of the target domain. We evaluate our method on the challenging Hakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving relative character error rate (CER) reductions of 20.02% and 9.64%, respectively, compared to the baselines. These results highlight the efficacy of our channel-aware data simulation method for bridging the gap between source- and target-domain acoustics.", "sections": [{"title": "I. INTRODUCTION", "content": "Automatic speech recognition (ASR) has become an indispensable technology, powering applications ranging from virtual assistants to transcription services. Recent advancements in deep learning, particularly with architectures such as convolutional neural networks (CNNs) [1], long short-term memory networks (LSTMs) [2], [3], Transformers [4]\u2013[7], and Conformers [8]\u2013[10], have significantly improved ASR accuracy across various conditions. However, these architectures often remain susceptible to performance degradation caused by channel mismatch a discrepancy in acoustic characteristics between training and test data due to differences in recording equipment.\nThis vulnerability is particularly evident in scenarios like teleconferencing, where diverse microphones, ranging from professional condenser microphones to built-in webcams, introduce significant variations in signal quality. This mismatch can drastically degrade performance. For instance, as shown in Table I, using WhisperTiny [7] fine-tuned on Condenser data from the Hakka Across Taiwan (HAT) [11] and Taiwanese Across Taiwan (TAT) [12] corpora results in drastically increased character error rates (CERs) when evaluated on other microphone types, highlighting the urgent need for more channel-robust ASR systems.\nTo address this challenge, researchers have explored domain adaptation techniques [13]\u2013[18] that aim to bridge the gap between training and test distributions. While these techniques have shown promise, they often involve complex training procedures or may not fully exploit the underlying relationship between domains. Recently, data simulation has emerged as an alternative approach [19]\u2013[21], generating synthetic target-domain data from source-domain data to facilitate model adaptation without requiring paired samples. However, existing data simulation approaches primarily focus on"}, {"title": "II. PROPOSED METHOD", "content": "Fig. 1 illustrates the architecture of our proposed CADA-GAN, which comprises three key components: a generator (G), a discriminator (D), and a channel encoder (E). The process begins with the channel encoder, which extracts a channel embedding ($C^T$) from a target-domain spectrogram ($X^T$). This embedding encapsulates the distinct acoustic characteristics of the target recording environment. The generator then utilizes this embedding alongside a source-domain spectrogram ($X^S$) to synthesize a simulated spectrogram ($X^G$) that mimics the target-domain channel characteristics while preserving the phonetic content of the source speech. Finally, the discriminator distinguishes between real target spectrograms and simulated spectrograms, providing feedback to the generator during training.\n**A. Generator and Discriminator**\nThe generator (G) is designed to transform a source-domain spectrogram ($X^S$) into a simulated target-domain spectrogram ($X^G$). It achieves this by first processing the source spectrogram through two 2D downsampling convolutional layers (kernel size: 3\u00d7 3, stride: 2 \u00d7 2), followed by nine residual blocks to capture deep, hierarchical representations. Each residual block consists of two convolutional layers (kernel size: 3\u00d7 3, stride: 1 \u00d7 1) and a dropout layer to prevent overfitting. Finally, two transposed convolutional layers (kernel size: 3 \u00d7 3, stride: 2 \u00d7 2) upsample the learned representations to generate the simulated spectrogram.\nThe discriminator (D) plays a crucial role in ensuring the authenticity of the generated spectrograms. It distinguishes between real target spectrograms ($X^T$) and simulated ones using five 2D convolutional layers (kernel size: 4 \u00d7 4) with Leaky ReLU activation functions. The stride is set to 2 x 2 for the first three layers and 1 x 1 for the last two, gradually increasing the receptive field. The adversarial loss employed during training is defined as follows:\n$L_{adv}(G,D, X^T, X^S, C^T) = E_{x \\sim X^T}[log D(x)] + E_{x \\sim X^S, c \\sim C^T}[log(1 \u2013 D(G(x, c)))].$  (1)\nThis adversarial loss encourages the generator to produce spectrograms that closely resemble real target spectrograms, while the discriminator learns to identify subtle differences that distinguish real from simulated data. This adversarial training process compels the generator to continuously improve its ability to generate realistic and domain-specific speech.\n**B. Channel Encoder**\nDrawing inspiration from recent advancements in \"aware\" techniques [17], [22], [23], we introduce a dedicated channel encoder (E) to extract channel embeddings ($C^T$) from the final layer of a pre-trained model. Unlike conventional approaches that directly utilize raw spectrograms as channel information, our channel encoder focuses on capturing high-level, discriminative channel characteristics. Specifically, our channel encoder leverages a MFA-Conformer model [24], which is pre-trained on the HAT corpus [11]. The training data consists of recordings from speakers uttering identical content using different microphones at the same time, ensuring that the model is unaffected by speech content or speaker identity. By excluding both the source and target channels used in the main experiment, we enhance the model's ability to classify various channels based purely on their acoustic properties. This strategy enables the channel encoder to effectively capture detailed channel characteristics without incorporating phonetic information, leading to more robust and generalizable channel embeddings.\nThe channel embeddings are integrated to the generator using Feature-wise Linear Modulation (FiLM) [25]. The embeddings undergoes separate linear transformations to produce weights and biases. These are used to modulate the output features from specific layers in the generator. To further ensure that the generated spectrograms accurately reflect the target-domain channel characteristics, we introduce a channel reconstruction loss:\n$L_{ch}(G, X^S, C^T) = E_{x \\sim X^S, c \\sim C^T} [||c \u2013 E(G(x,c))||_1].$ (2)\nThis loss function encourages the generator to synthesize spectrograms that, when processed by the channel encoder, yield embeddings highly similar to the original target channel embeddings. This reinforces the channel awareness of the generator, ensuring that the generated speech accurately captures the subtle nuances of the target recording environment.\n**C. Patch-wise Contrastive Learning**\nTo maintain linguistic consistency between the simulated and source speech, we apply patch-wise contrastive learning [26]. This approach maximizes mutual information, particularly shared speech content, between source and simulated spectrograms. Specifically, we utilize the generator to extract deep features from both spectrograms. A small patch from the simulated representation serves as the \"query\", with the corresponding patch from the source as the \"positive\" sample. We select 256 patches from the source as \"negative\" samples. These patches are projected into a lower-dimensional space using two linear layers with 256 units each and ReLU activation. The contrastive loss, computed across five generator layers, measures the cross-entropy loss between the \"query\" patch and both positive"}, {"title": "and negative patches. This encourages high similarity between corresponding patches in the source and simulated spectrograms, while distinguishing them from random patches. The loss is defined as:", "content": "$L_{pcl}(G, X) = \\sum_{l=1}^{L} \\sum_{i=1}^{I} -log(\\frac{e^{(z_i \\cdot z_i^+ / \\tau)}}{\\sum_{j=1}^{J}e^{(z_i \\cdot z_j^- / \\tau)}}).$ (3)\nwhere $z_i$ represents the ith positive patch from source representations at the Ith layer of the generator, i denotes the corresponding patch from simulated representations, and $z_j$ refers to the jth negative patch from simulated representations at the same layer. The temperature parameter $T$ regulates the contrastive learning process. This loss function is applied to both source ($L_{pcl}(G,X^S)$) and target ($L_{pcl}(G,X^T)$) spectrograms to maintain consistent speech content and minimize unnecessary changes.\n**D. Training Objective and Adaptation Process**\nThe training objective is to optimize GAN using a comprehensive loss function that includes the adversarial loss, patch-wise contrastive learning losses for both source and target spectrograms, and the channel reconstruction loss. The total loss function is defined as:\n$L_{total} =L_{adv}(G, D, X^T, X^S, C^T) + L_{pcl}(G,X^S) + L_{pcl}(G,X^T) + \\lambda_{ch} L_{ch}(G, X^S, C^T),$ (4)\nwhere $\\lambda_{ch}$ weights the channel reconstruction loss.\nTo address limited target-domain data, an equal amount of speech is randomly sampled from the source domain. The model is trained on this unpaired dataset, optimizing with the total loss function $L_{total}$ as specified in (4). After training, the generator serves as a domain converter $F_{S \\sim T}$, transforming $X^S$ to $X^T$, with the pre-trained channel encoder aiding in channel simulation. This simulation uses plentiful source speech and randomly selected target speech from training. The augmented data enhances the fine-tuning of ASR models without requiring additional transcriptions."}, {"title": "III. EXPERIMENTS", "content": "We conducted extensive experiments on two benchmark datasets to evaluate the efficacy of our proposed CADA-GAN method for domain-adaptive ASR.\nHAT [11]: The HAT corpus comprises hundred thousands sets of recordings, where each set was uttered by the same speaker with identical speech content using eight different microphones, reflecting diverse recording conditions. The recorders include an iPhone, an Android phone, a Webcam, a professional Condenser microphone, a Lavalier microphone, a cheap PC microphone (PC-Mic), and an X-Y stereo microphone (ZOOM-X and ZOOM-Y). The dataset contains 97,385 training sets (779,080 utterances in total) and 4,559 test sets (36,472 utterances in total). We selected recordings from Condenser as the source domain and those from Webcam as the target domain due to their significant acoustic mismatch. To train our GAN, we randomly sampled 40 utterances from each domain, demonstrating the method's effectiveness with limited target-domain data.\nTAT [12]: To further validate that our channel encoder does not inadvertently capture phonetic information, we conducted additional experiments on the TAT corpus. This dataset is similar to HAT but excludes recordings from Webcam and PC-Mic. We used Condenser recordings as the source domain and Android recordings as the target domain for this evaluation. There is no information indicating that HAT and TAT use the same type and brand of devices."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "We present the results of our experiments, comparing the performance of CADA-GAN with the baseline models on both HAT and TAT corpora. Additionally, ablation studies were conducted to analyze the contribution of individual components in our method.\n**A. Main Results on HAT and TAT**\nTable II presents the CERs achieved by all ASR models on the HAT and TAT corpora. CADA-GAN demonstrates substantial improvements over the baseline approaches, achieving a remarkable 20.02% relative CER reduction on the HAT corpus and a 9.64% relative CER reduction on the TAT corpus compared to the Vanilla ASR model. These results highlight the effectiveness of incorporating the pre-trained channel encoder within the domain adaptation framework.\nFurthermore, the consistent performance gains observed on both HAT and TAT, despite the channel encoder being trained solely on the HAT corpus, underscores its ability to capture and leverage channel-specific features effectively while remaining agnostic to phonetic"}, {"title": "B. Ablation Studies", "content": "To delve deeper into the contribution of each component within CADA-GAN, we conducted ablation studies, summarized in Table III. Removing the channel reconstruction loss (- $L_{ch}$) during training resulted in a marginal performance decline, suggesting that while this loss aids in maintaining channel fidelity, its overall impact on speech recognition accuracy is relatively small.\nConversely, omitting the channel embeddings (- Embeddings) during the generation process led to a significant drop in performance. This substantial decrease emphasizes the critical role channel embeddings play in accurately capturing and transferring channel-specific characteristics from the target domain, even though our channel encoder was trained solely on the HAT corpus. These embeddings are essential for enhancing the model's robustness and enabling accurate speech recognition across different channel conditions."}, {"title": "C. UMAP Visualization of Channel Embeddings", "content": "To gain further insights into the workings of CADA-GAN, we visualized the learned channel embeddings and evaluate the perceptual quality of the generated speech. Uniform Manifold Approximation and Projection (UMAP) [28] was employed to visualize the channel embeddings extracted from the HAT and TAT corpora, as shown in Fig. 2. In Fig. 2 (a), a clear separation between different channel types in the HAT corpus is observed. This distinct clustering demonstrates the effectiveness of our pre-trained channel encoder in capturing unique acoustic characteristics associated with each microphone. While the separation is less pronounced in Fig. 2 (b) for the TAT corpus (where the encoder was not specifically trained), similar channel types are still effectively grouped together. This observation highlights the generalization ability of our channel encoder across different languages, reinforcing its capacity to learn channel-specific features rather than language-dependent patterns."}, {"title": "D. MOS Evaluation on Simulated Data", "content": "To assess the perceptual realism of the generated speech, we conducted a Mean Opinion Score (MOS) evaluation focused specifically on channel characteristics. Ten participants rated the similarity of the perceived recording channel between generated audio samples"}, {"title": "E. Analysis of Validation Loss and Embedding Distance", "content": "To quantitatively assess the channel discrimination capability of our encoder, we analyzed the evolution of both the validation loss and the average pairwise Euclidean distance between channel embeddings during training. The pairwise distance, computed using the validation set, was averaged between embeddings of the same utterance set (same speaker and content) recorded across different channels. As shown in Fig. 3, the validation loss (red curve) decreases steadily over 30 epochs, while the pairwise distance (blue curve) increases, indicating the encoder is effectively learning distinct channel characteristics. This positive trend supports the critical role of the channel encoder in the success of CADA-GAN."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This study presents CADA-GAN, a novel method for channel compensation in robust ASR. By integrating a channel encoder with a GAN architecture, CADA-GAN effectively addresses channel mismatch, improving ASR generalization to unseen conditions. Experiments on the HAT and TAT corpora demonstrate that CADA-GAN significantly outperforms strong baselines, achieving substantial CER reductions and higher MOS scores. These results underscore the efficacy of our method in improving both the accuracy and perceptual quality of speech recognition across diverse channel environments.\nFuture work will involve further validating the effectiveness of CADA-GAN with more advanced ASR models like WhisperLarge and extending the evaluation to a wider range of challenging datasets. Additionally, we aim to explore integrating CADA-GAN with other domain adaptation techniques to address multiple sources of variability in speech data."}]}