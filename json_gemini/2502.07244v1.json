{"title": "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting", "authors": ["Jiecheng Lu", "Shihao Yang"], "abstract": "Autoregressive attention-based time series forecasting (TSF) has drawn increasing interest, with mechanisms like linear attention sometimes outperforming vanilla attention. However, deeper Transformer architectures frequently misalign with autoregressive objectives, obscuring the underlying VAR structure embedded within linear attention and hindering their ability to capture the data generative processes in TSF. In this work, we first show that a single linear attention layer can be interpreted as a dynamic vector autoregressive (VAR) structure. We then explain that existing multi-layer Transformers have structural mismatches with the autoregressive forecasting objective, which impair interpretability and generalization ability. To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model. Then, we propose Structural Aligned Mixture of VAR (SAMOVAR), a linear Transformer variant that integrates interpretable dynamic VAR weights for multivariate TSF. By aligning the Transformer architecture with autoregressive objectives, SAMOVAR delivers improved performance, interpretability, and computational efficiency, comparing to SOTA TSF models. The code implementation is available at this link.", "sections": [{"title": "1. Introduction", "content": "In recent years, autoregressive decoder-only Transformers have made significant strides (Vaswani, 2017; Radford, 2018), powering Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023) capable of handling complex sequential data. Their core mechanism, autoregressive self-attention, computes attention weights between the current token and all preceding tokens during prediction. However, using softmax to compute the N \u00d7 N attention map results in large O(N^2) time complexity as the sequence length grows. To address the efficiency bottleneck, researchers have developed efficient variants like Linear Transformers (Katharopoulos et al., 2020; Hua et al., 2022). By replacing softmax with a linearizable kernel, linear attention reduces complexity from O(N^2) to O(N) by maintaining a 2d-dimensional hidden state instead of forming the full N \u00d7 N attention map (Choromanski et al., 2021; Hua et al., 2022; Sun et al., 2023). Although it often underperforms vanilla attention in complex tasks like NLP, studies show that in simpler tasks, such as time series forecasting (TSF), linear attention can outperform vanilla attention (Patro & Agneeswaran, 2024; Lu et al., 2024a; Behrouz et al., 2024).\nAutoregressive modeling has a long history in time series forecasting. Traditional methods like ARIMA handle univariate series through autoregression, differencing, and moving averages (Winters, 1960; Holt, 2004), while Vector Autoregression (VAR) (Stock & Watson, 2001; Zivot & Wang, 2006) extends this to multivariate settings by capturing cross-variable lag dependencies. Although widely used in fields like economics and climate due to their interpretability and theoretical guarantees (Burbidge & Harrison, 1984; Pretis, 2020), these models' linear assumptions and fixed lag orders limit their ability to capture complex patterns. With growing data scale and complexity, deep learning-based TSF models, especially attention-based approaches, have outperformed traditional AR/VAR methods (Li et al., 2019; Zhou et al., 2021; Nie et al., 2022; Liu et al., 2024).\nPrevious research offers various perspectives on linear attention, viewing it as an RNN with linear state updates, a dynamic temporal projection, or fast weight programming (Katharopoulos et al., 2020; Schlag et al., 2021). In this paper, we show that linear attention naturally contains a VAR structure. While a single-layer linear attention module can be directly interpreted as a VAR model, stacking multiple layers introduces structural mismatches with the time series generative process, reducing its effectiveness for TSF. We demonstrate that by reorganizing the input-output flow, multi-layer linear attention can fully align with a VAR model. Further, we propose Structural Aligned Mixture of VAR (SAMOVAR), which enhances linear Transformers for TSF, improving both accuracy and interpretability.\nThe main contributions are summarized as follows:\n1) We provide a new perspective by interpreting a single-layer linear attention module as a VAR structure, where the key represents the observation and the outer product of value and query forms dynamic VAR weights.\n2) We analyze how the designs of existing Transformers lead to misalignments with a VAR model's time series generative objective, including mismatched losses, inconsistent residual streams, and unbalanced observation weighting.\n3) We show that properly arranging the input-output flow in a linear Transformer allows multi-layer linear attention to act as a expressive dynamic VAR model. With l layers, each past step's influence on future steps is captured through a \"temporal influence path\" involving up to l \u2013 1 intermediate nodes, enhancing interpretability.\n4) Based on this aligned structure, we propose SAMOVAR for TSF. Experiments demonstrate that it surpasses previous TSF models in accuracy, interpretability, and efficiency."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Time Series Forecasting", "content": "Time Series Forecasting (TSF) aims to predict future values in a multivariate sequence $S \\in \\mathbb{R}^{L \\times C}$, split into a historical part $S_1 \\in \\mathbb{R}^{L_1 \\times C}$ and a future part $S_p \\in \\mathbb{R}^{L_p \\times C}$, where $L = L_1 + L_p$ are the series lengths, and C is the number of channels. The task is to learn a function $f : \\mathbb{R}^{L_1 \\times C} \\rightarrow \\mathbb{R}^{L_p \\times C}$ that generates $\\hat{S_p} = f(S_1)$, given the input $S_1$."}, {"title": "2.2. Preliminaries: Attention Mechanisms", "content": "Previous studies have examined autoregressive attention mechanisms from various angles, emphasizing their common feature of dynamic weights (Katharopoulos et al., 2020; Hua et al., 2022; Sun et al., 2023). For an input sequence of length N and dimension d, represented as $X \\in \\mathbb{R}^{N \\times d}$, with each token denoted by $x_t \\in \\mathbb{R}^{1 \\times d}$, a single-head autoregressive attention layer is formulated as:\n$\\text{Attn}(X) = \\sigma (M(QK^T))VW_o,$\n$\\text{with } Q, K, V = XW_q, XW_k, XW_v,$\n$X := X + \\text{Attn}(LN(X))$\\tag{1}\nHere, $W_q, W_k, W_v, W_o \\in \\mathbb{R}^{d \\times d}$ are the projection matrices for query, key, value, and output. The causal mask $M \\in \\mathbb{R}^{N \\times N}$ ensures autoregressive behavior, with $M_{ij} = 1\\{i > j\\} - \\infty \\cdot 1\\{i < j\\}$, allowing only current and past positions. Attn(\u00b7) and LN(\u00b7) denote the attention and layer normalization functions. When \u03c3 is softmax, the mechanism becomes vanilla attention. Replacing \u03c3 with an identity mapping simplifies it to linear attention. Adding an MLP layer after the attention layer, as $X := X + MLP(LN(X))$, forms a standard autoregressive Transformer block. We will explore the attention from multiple perspectives (P.). An attention layer dynamically computes a temporal mapping weight matrix $QK^T \\in \\mathbb{R}^{N \\times N}$ for a sequence of length N. For each input step t, it generates an attention map over all N tokens as dynamic weights. In vanilla attention, these weights are softmax-normalized to sum to 1. In autoregressive attention, a lower-triangular mask M ensures each step only attends to positions up to t. Thus, the attention layer functions as a variable-length dynamic linear layer on the input value sequence V(Vaswani, 2017; Katharopoulos et al., 2020; Yang et al., 2024).\nP2: Recurrent Form and Autoregression\nAutoregressive attention can be viewed as a step-by-step generative process with a recurrent formulation. For the input $x_t$ at step t, the output $o_t$ is: $o_t = \\frac{\\sum_{i=1}^{t}\\sigma(q_t, k_i)v_i}{\\sum_{i=1}^{t}\\sigma(q_t, k_i)}$, where $q_t, k_t, v_t \\in \\mathbb{R}^{1 \\times d}$ are query, key, and value vectors at step t. When $\u03c3(q_t, k_i) = \\exp(q_tk_i)$, this represents vanilla attention, relying on all previous keys $k_{\\{1,...,t\\}}$ and values $v_{\\{1,...,t\\}}$. If $\u03c3(q_t, k_i)$ is derived from a kernel feature map $k(q_t, k_i) = \\phi(q_t)^\\intercal\\phi(k_i)$, the computation is linearized as: $o_t = \\frac{\\phi(q_t) \\sum_{i=1}^{t} \\phi(k_i)v_i}{\\phi(q_t) \\sum_{i=1}^{t} \\phi(k_i)^\\intercal}$. This avoids the full N \u00d7 N attention map by aggregating past information into a hidden state. Ignoring the denominator, the simplified form is: $o_t = q_t \\sum_{i=1}^{t} k_i v_i$. Here, attention acts like an RNN with a 2D hidden state $k_i v_i \\in \\mathbb{R}^{d \\times d}$ and identity state updates. Studies have shown comparable performance without normalization, so we use this simplified form (Zhai et al., 2021; Mao, 2022; Qin et al., 2022; Sun et al., 2023; Yang et al., 2024). In this view, attention is a dynamic autoregressive model with shared weights $w_{t,i} = g(q_t, k_i)$ across all d channels: $o_t = \\sum_{i=1}^{t} w_{t,i}v_i$.\nP3: Fast Weight Programming Fast weight programming (FWP) refers to the process of dynamically determining a set of linear predictor weights for each step in the input sequence, i.e., $W_{\\text{FWP},t} = g(x_1,...,x_t) \\in \\mathbb{R}^{d \\times d}$. In linear attention, this is achieved using a summation aggregator to combine past weight information (Schlag et al., 2021): $W_{\\text{FWP},t} = \\sum_{i=1}^{t}(k_i)^\\intercal v_i$, which serves as a dynamic linear predictor for $q_t$. The final output at step t is then $o_t = q_tW_{\\text{FWP},t}$."}, {"title": "2.3. Linear Attention as VAR", "content": "Recent TSF research shows that linear attention\u2014without softmax\u2014often outperforms vanilla attention (Patro & Agneeswaran, 2024; Lu et al., 2024a; Behrouz et al., 2024). While it can be viewed as an RNN or a form of fast weight programming (FWP), these perspectives do not directly link to the autocorrelation or generative nature of TSF data. In this section, we demonstrate that linear attention naturally forms a dynamic VAR structure, making it well-suited for modeling TSF data generation.\nP4: Vector Autoregression A classic Vector Autoregressive model VAR(p) with lag p uses parameter matrices $A_j \\in \\mathbb{R}^{d \\times d}$ to model dependencies on p previous time steps:\n$y_t^T = A_1y_{t-1}^T + A_2y_{t-2}^T + \\cdots + A_py_{t-p}^T + u_t,$\nwhere $y_j \\in \\mathbb{R}^{1 \\times d}$ represents the observations and $u_t \\in \\mathbb{R}^{1 \\times d}$ is the residual. In the RNN and FWP views, the parameter matrices depend directly on $k_i$ and $v_i$ and act on the query $q_t$ rather than sequentially applying to previous steps as in VAR. However, we can reformulate linear attention to reveal its VAR structure. Since $q_tk_i$ is a scalar, rearranging terms gives:$o_t = \\sum_{i=1}^{t} k_iq_t^T v_i$. By transposing this expression, we obtain the VAR form of a single-layer linear attention mechanism:\n$o_t^T = \\sum_{i=1}^{t} A_{t,i}k_i, A_{t, i} = v_iq_t^T$\\tag{2}\nThis defines a VAR(t) structure with dynamic weights, where the observations are $k_i$ and the rank-1 weight matrices $A_{t,i} = v_iq_t^T$ are dynamically generated for each step t. Unlike RNNs or FWP, where weights propagate across steps, these matrices are independently generated at each time step. Thus, autoregressive linear attention forms its own VAR(\u00b7) structure at each step, as shown in Figure 1(a)."}, {"title": "3. Aligning the Objective of Autoregressive Attention with Autoregressive Forecasting", "content": "In this section, we show that while a single linear attention layer naturally exhibits a dynamic VAR structure, the design of current multi-layer Transformers diverges from the VAR training objective. As a result, these models lose the beneficial VAR properties for time series forecasting (TSF).\nTime series data naturally show temporal dependence. A classic VAR model captures both autocorrelation and cross-correlation in the data generation process, with weight matrices $A_j$ decoupling how past values influence future outcomes. Although standard attention and Transformers are effective for modeling complex sequence relationships (e.g., in NLP), their architecture conflicts with VAR's goal of explicitly representing lag-based dependencies. We outline the key sources of this misalignment below.\nVAR Loss and Position Shifting A VAR model is designed to directly capture the relationship between past observations and future values. Suppose we rewrite Eq. (2) into a strict VAR model form:\n$k_{t+1}^T = o_t^T + u_t = \\sum_{i=1}^{t} v_iq_t^Tk_i + u_t,$\n$\\hat{A}_{t,i} = v_iq_t^T + u_t,$\\tag{3}\nwhere $u_t \\in \\mathbb{R}^{1 \\times d}$ is the residual not explained by the dynamic VAR system. To align linear attention with a VAR model, minimizing $u_t$ should be part of the training objective. Adding a loss term for each layer's residual $o_{t-1}$ might partially achieve this but would conflict with the overall Transformer objective.\nIn a VAR model, the weights are learned to perform forward shift for each input position. With l attention layers, enforcing this VAR loss would result in l shifts, whereas the Transformer's autoregressive objective only requires a single-step shift at the output. As a result, each layer would need to perform only a fractional shift to stay aligned.\nResidual Stream Next, we focus on how each attention layer operates. In decoder-only Transformers, a common pre-normalization design (Radford, 2018) includes a residual shortcut from input to output, with each block learning the difference between the current and the next step. The attention layers gather information from previous steps, gradually refining this difference and adding it to the shortcut.\nIf all attention layers are disabled, the model reduces to a local MLP predictor that maps the current input to the next-step output. With patch tokenization, where each token covers Lp time steps, the model effectively becomes an MLP-based TSF predictor mapping Lp input steps to Lp outputs. Adding an attention layer adjusts the input pattern using past tokens to better match the next-step difference. Recent studies of in-context learning (Zhang et al., 2023; Aky\u00fcrek et al., 2022) suggest that with more layers, the model can dynamically approximate predictors like gradient descent, ridge regression, or shallow MLPs. Thus, attention layers prioritize refining local predictors through context, rather than explicitly modeling step-by-step generation considering raw observations.\nInput and Output Comparing the attention output in Eq. (1) with the VAR output in Eq. (3), we see that attention outputs must align with the residual shortcut space, while VAR outputs correspond to the key observations $k$. VAR represent the data generation process, so their outputs should not be treated as residuals. However, by adjusting the weight matrix at each step, we can align the dynamic VAR structure of linear attention with the residual-based objective for transitioning observations to the next step. We call this a \"key shortcut,\" which adds an identity matrix to the dynamic VAR weights, guided by an indicator for the output step t.\n$k_{t+1}^T = k_t^T + o_t^T + u_t = \\sum_{i=1}^{t} A_{t,i} k_i + u_t,$ $A_{t, i} = v_iq_t^T + I \\cdot 1[i=t]$\\tag{4}\nHowever, in a pre-normalization setup, the attention layer only processes the transformed input LN(xt) and lacks direct access to the original signal xt needed to model the residual. As a result, it is not possible to establish a strict VAR recurrence involving $o_t^T$ and the original signal, regardless of adjustments to $k_t$ or the weight matrices $A_{t,i}$.\nBalanced Weights of Observations In a VAR model, all lag positions are initially treated equally, with their influence on future steps determined by learned weights, free from positional bias. In contrast, a multi-layer Transformer requires each token to perform two roles: (1) gather information via attention to predict its next step and (2) serve as context for future tokens. As layers deepen, accumulated residual updates cause the representation $f_{rep}(x_1, ..., x_i)$ to drift away from the original observation semantically. This drift complicates the VAR-like stepwise shift and leads to uneven weighting of original observations in a linear attention-based VAR framework."}, {"title": "4. Structural Aligned Mixture of VAR", "content": "We show that the misalignments between linear attention and VAR-based forecasting can be resolved by reorganizing the MLP and attention layers in a linear Transformer. By redesigning the input-output flow, we can enable multi-layer linear attention to maintain a VAR structure, improving its ability to model the generative processes of time series data. For a single linear attention layer in Eq. (2), the Transformer naturally aligns with the VAR objective for one-step shifting, as position-wise operations are confined within the layer. The VAR weights remain balanced across past lags when viewed in the original key observation space. To maintain this structure, the output and input signals must follow the same recursive equation, and the residual shortcut should share the same normalization as the attention layer's key inputs, avoiding typical pre-normalization shortcuts seen in standard Transformers."}, {"title": "4.1. Multi-layer Linear Attention as VAR", "content": "Using a single linear attention layer preserves a clear VAR structure but limits the Transformer's expressive power. Each outer product weight $v_iq_t^T$ forms a rank-1 matrix, and unlike RNNs or fast weight programming, this VAR formulation cannot increase rank through timestep summation. Below, we show that when multiple linear attention layers are stacked without MLP layers in between, and $o_t$ is directly fed into the next layer, the attention layers can still function as a dynamic VAR model. This model uses the first layer's key input $k_i^{(1)}$ as the observation, with explicit weight matrices that remain aligned with the autoregressive forecasting objective.\nLet us denote the output of the first linear attention layer at step t by $o_t^{(1)}$ (omitting residuals, normalization, and setting $W_o = I$). In the single-head case: $o_t^{(1)T} = \\sum_{j=1}^{t} (v_i^{(1)}q_i^{(1)T}) k_i^{(1)}$, where $q_i^{(1)} = x_tW_q^{(1)}, k_i^{(1)} = x_tW_k^{(1)}, v_i^{(1)} = x_tW_v^{(1)}$. Denote the first layer's key input by $k_i^{(1)} = k_i$, and let $B_j^{(1)}$ be the weight matrix directly acting on $k_j$. Then:\n$o_t^{(1)T} = \\sum_{j=1}^{t} B_{t,j}^{(1)} k_j^{(1)}, B_{t,j}^{(1)} = (v_i^{(1)}q_i^{(1)T})$.\nNow take $o_t^{(1)T}$ as input to the second layer. The second-layer output becomes: $o_t^{(2)T} = \\sum_{j=1}^{t} (v_i^{(2)}q_i^{(2)T}) k_i^{(2)}$, where $A_{t,j}^{(2)}$"}, {"title": "5. Experimental Results", "content": "In this section, we introduce SAMOVAR (Structural Aligned Mixture of VAR), which reorganizes MLP and linear attention layers, serving as a drop-in replacement for standard linear Transformers in time series forecasting (TSF), as shown in Figures 1 (c) and (d).\nA l-layer SAMOVAR Transformer includes: a) MLP Component: Consists of l MLP layers that learn optimal representations for VAR observations. After MLP processing, layer normalization ensures VAR outputs align with the transformed input signals. b) SAMoVAR Attention: Comprises l linear attention layers, parameterized with the robust path pruning methods discussed earlier. A unified residual shortcut across all layers stabilizes training. The overall architecture is shown in Figures 1 (d) and (e).\nPatch-based ARX Tokenization Prior research highlights the importance of preserving univariate dependencies for effective multivariate time series forecasting (TSF) (Zeng et al., 2023; Nie et al., 2022; Lu et al., 2024b). For multivariate inputs S1, we adopt an autoregressive (ARX) tokenization strategy, which captures univariate relationships while treating other series as exogenous inputs to model multivariate dependencies. We partition a time series of length L\u2081 into non-overlapping patches of size Lp. If needed, zero-padding P is added so that L\u2081 + P is divisible by Lp, resulting in N = $\\frac{L_I+P}{Lp}$ patches $S[i:i+Lp] \\in \\mathbb{R}^{LP \\times C}$. For each series j, a linear projection $W_{tok} \\in \\mathbb{R}^{d \\times LP}$ transforms its patch $S[i:i+Lp,j]$ into an autoregressive token $(W_{tok} S[i:i+Lp,j])^T \\in \\mathbb{R}^{1 \\times d}$. This token is used to predict the next Lp steps for series j, which can be viewed as a PatchTST-style tokenization (Nie et al., 2022) with an autoregressive loss.\nInspired by vector autoregressive with exogenous variables (VARX), $y_t = \\sum_{m=1}^{p} A_my_{t-m} + \\sum_{n=0}^{q} B_ne_{t-n} + U_t$, where $e_{t-n}$ represents exogenous factors, we model all other series as exogenous when forecasting series j. Specifically, a linear projection $W_{ex} \\in \\mathbb{R}^{C \\times C}$ mixes each channel independently to generate the exogenous token $(W_{tok} S[i:i+LP,:])^T \\in \\mathbb{R}^{1 \\times d}$. As shown in Figure 2, we combine autoregressive and exogenous tokens along the sequence dimension to form the input tokens $X_{input} \\in \\mathbb{R}^{C \\times 2N \\times d}$. The channel dimension is treated as part of the batch for independent computation, with each exogenous token placed before its corresponding target token. Trainable position embeddings based on token positions and channel indices are added. The SAMOVAR Transformer processes the sequence, and the outputs corresponding to target tokens are projected using $W_{out} \\in \\mathbb{R}^{LP \\times d}$ to generate the next-step ARX predictions."}, {"title": "5.1. Synthetic Tasks", "content": "Model Setup We use the SAMOVAR architecture described in Fig. 1(d) along with the ARX tokenization from Fig. 2 to train our TSF models. Additionally, we construct a baseline model (LinTrans) based on the classic linear Transformer structure shown in Figure 1(c) to highlight the improvements of SAMOVAR. To demonstrate the impact of dynamic VAR weights, we replace SAMOVAR's mixture of VAR modules with a fixed-weight VAR layer (FixedVAR).\nVAR Generalization To test SAMOVAR's ability to learn and generalize to the underlying data generation process, we generate training and test data using random VAR(p) models with varying lag orders. For training, $p \\in \\{1, 2, 3\\}$ with coefficients between -0.5 and 0.5. For validation, $p \\in \\{3,4,5\\}$"}, {"title": "Ablation Studies", "content": "In Table 2, we present ablation studies to validate the effectiveness of components within SAMOVAR: 1) Reintroducing key projection weights: Based on our analysis in \u00a74.1, introducing Wk negatively impacts the scaling control of temporal influence paths and increases numerical instability due to the additional weight matrices. The results show a significant performance drop and training instability when Wk is added. 2) Removing the inverse matrix D-1: This matrix, corresponding to Wo, plays a critical role in controlling the output space. Without it, performance degrades, as expected. 3) Removing RMSNorm from queries and values: As discussed in \u00a74.1, norm control over queries and values is essential for learning effective VAR path weights. Removing RMSNorm significantly degrades performance and causes training issues. 4) Increasing the number of attention heads: When the hidden dimension is fixed, using more attention heads reduces the dimension per head and passively disables more influence paths during initialization, leading to performance degradation. This also reduces the parameter capacity of dynamic VAR weights, further hurting performance. 5) Varying the number of Transformer layers l: The layer depth l determines the number of intermediate points in the temporal influence paths included in the final VAR weights. The results show that l = 1 yields the worst performance due to the lack of intermediate points. For real-world TSF datasets, l = 3 or l = 4 (2 or 3 intermediate points) is sufficient for good performance, while larger l values lead to overfitting risks.\nComputational Costs SAMOVAR introduces no additional computational overhead compared to the vanilla linear Transformer and even reduces the key projection step. This gives it efficiency advantages over other TSF models. Detailed comparisons are shown in Table 8."}, {"title": "6. Conclusion and Limitation", "content": "This work bridges the gap between linear attention Transformers and VAR models for time series forecasting. We demonstrate that single-layer linear attention inherently captures dynamic VAR structures, while standard multi-layer architectures misalign with autoregressive objectives. By structurally aligning the input-output flow and MLP layers, we propose SAMOVAR, a multi-layer linear Transformer that integrates dynamic VAR weights through temporal influence paths. SAMoVAR achieves superior accuracy, interpretability, and efficiency compared to state-of-the-art models across synthetic and real-world benchmarks. As for limitation, we have not yet tested larger SAMOVAR models on large-scale general TSF tasks to evaluate their potential as foundation models. Additionally, we have not explored applying SAMOVAR to general sequence modeling tasks to assess whether the learned dynamic VAR weights are effective beyond TSF tasks."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Workflow of the SAMOVAR Attention Module", "content": "We present the details of the attention module in the SAMOVAR Transformer, as shown in Algorithm 1."}, {"title": "A.2. Experimental Datasets", "content": "Our multivariate time series forecasting experiments employ twelve real-world benchmark datasets. The original dataset names and their key details are summarized as follows:\nWeather Dataset\u00b3 (Wu et al., 2021) This dataset records 21 meteorological indicators (e.g., temperature, humidity) at 10-minute intervals throughout 2020, collected from the weather station of the Max Planck Institute for Biogeochemistry in Germany.\nSolar Dataset\u2074 (Lai et al., 2018) Comprising solar power generation data from 137 photovoltaic plants, this dataset captures energy production values sampled every 10 minutes during 2006.\nElectricity Dataset(Wu et al., 2021) Containing hourly electricity consumption records of 321 clients, this dataset spans a three-year period from 2012 to 2014.\nETT Dataset(Zhou et al., 2021) The Electricity Transformer Temperature (ETT) dataset monitors operational parameters (including load and oil temperature) from power transformers, recorded at 15-minute (ETTm1/ETTm2) and hourly (ETTh1/ETTh2) resolutions between July 2016 and July 2018. Each subset contains seven critical operational features.\nTraffic Dataset (Wu et al., 2021) This dataset provides hourly road occupancy rates from 862 highway sensors in the San Francisco Bay Area, collected continuously between January 2015 and December 2016.\nPEMS Dataset(Li et al., 2017) A standard benchmark for traffic prediction, the PEMS dataset includes California freeway network statistics recorded at 5-minute intervals. Our experiments utilize four widely adopted subsets: PEMS03, PEMS04, PEMS07, and PEMS08."}, {"title": "A.3. Hyper-parameter Settings and Implementation Details", "content": "This section explains the hyper-parameter settings for SAMOVAR attention, linear (AR) attention, and fixed VAR models used in the experiments.\nAttention Module: We use 3 layers, consisting of 3 MLP layers and 3 attention layers for SAMOVAR attention. For linear attention, we also use 3 Transformer blocks. For fixed VAR, we first use 3 MLP layers and replace the final mixture of VAR modules in SAMOVAR with a single-layer VAR structure with fixed weights. This VAR structure shares the same architecture as linear attention, but the query and key vectors are replaced with a set of fixed position vectors, similar to trainable positional embeddings.\nHidden Dimension: For all Transformers, we set the hidden dimension as d = 32[\u221aC], where C is the number of multivariate time series. For linear attention and fixed VAR, we use 8 attention heads. For SAMOVAR, the number of heads is determined to ensure each head dimension is 16. For example, for the ETT datasets (C = 7), d = 64, and the number of heads is 16. For the Weather dataset (C = 21), d = 128, and the number of heads is 8.\nInitialization: All linear layers are initialized using a normal distribution with a mean of 0 and a standard deviation of 0.02. Embedding layers are zero-initialized. For projection layers in MLPs, we use GPT-2-style initialization with a scale factor of $ \\sqrt{\\frac{2}{l}}$, where l is the number of MLP/attention layers. The MLP structure follows the standard 2-layer Transformer design with an expansion ratio of 4. The dropout rate is set to 0.1 uniformly.\nLayer Normalization: We add RMSNorm after query and key projections for SAMOVAR. For the other layer normalization modules, experiments showed no significant difference between traditional layer normalization and RMSNorm, so we opted for RMSNorm due to its lower computational cost.\nInput Preprocessing: For input time series of shape (C, LP), where Lp is the input length, we concatenate timestep embeddings along the channel dimension if available (as described in works like Autoformer (Wu et al., 2021) and DLinear (Zeng et al., 2023)). The time series is divided into N non-overlapping patches of size $L_P$. Zero padding is applied if $L_P$ does not divide $L_I$ evenly. The resulting input tokens have shape (C, N, LP). We apply RevIN (Kim et al., 2022) to normalize each token by subtracting its mean and dividing by the standard deviation of the entire input.\nWe then create an additional set of tokens by projecting the channel dimension using C \u00d7 C linear weights. These exogenous tokens are interleaved with the univariate tokens, resulting in an ARX input of shape (C, 2N, LP). The patch size dimension Lp is projected to the hidden dimension d, resulting in input tokens of shape (C, 2N, d). We add d-dimensional channel and position embeddings to the input tokens.\nTransformer Module: The input tokens are layer-normalized and passed through the Transformer module. The output has shape (C, 2N, d). We select the outputs corresponding to the original univariate tokens, resulting in (C, N, d). This is followed by layer normalization and projection back to dimension Lp, producing output tokens of shape (C, N, LP). The RevIN reverse process restores the outputs by multiplying with the previously calculated standard deviation and adding the mean. We compute the MSE loss between the output and the next timestep's univariate token values.\nTraining Setup: All experiments are conducted on a single Nvidia RTX 4090 GPU with a batch size of 32. For datasets that cause memory overflow, the batch size is reduced to 16 or 8, with 2-step or 4-step gradient accumulation to maintain an effective batch size of 32. The optimizer is AdamW with a weight decay of 0.1 and \u00df values of (0.9, 0.95). All baseline models are retrained under the same settings."}, {"title": "Dataset Splits and Preprocessing", "content": "Following Nie et al. (2022); Liu et al. (2024), we use a train-validation-test split ratio of 0.7, 0.1, and 0.2. Input data is standardized using the mean and standard deviation calculated from the training set.\nTraining Procedure: We apply early stopping with a patience of 12 epochs and a maximum of 100 epochs. For the first 5 epochs, we use a warm-up learning rate, gradually increasing it from 0.00006 to 0.0006, followed by linear decay until the maximum epoch is reached."}]}