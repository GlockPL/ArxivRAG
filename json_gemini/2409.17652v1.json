{"title": "FACTORSIM: Generative Simulation via Factorized Representation", "authors": ["Fan-Yun Sun", "S. I. Harini", "Angela Yi", "Yihan Zhou", "Alex Zook", "Jonathan Tremblay", "Logan Cross", "Jiajun Wu", "Nick Haber"], "abstract": "Generating simulations to train intelligent agents in game-playing and robotics from natural language input, from user input or task documentation, remains an open-ended challenge. Existing approaches focus on parts of this challenge, such as generating reward functions or task hyperparameters. Unlike previous work, we introduce FACTORSIM that generates full simulations in code from language input that can be used to train agents. Exploiting the structural modularity specific to coded simulations, we propose to use a factored partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation. For evaluation, we introduce a generative simulation benchmark that assesses the generated simulation code's accuracy and effectiveness in facilitating zero-shot transfers in reinforcement learning settings. We show that FACTORSIM outperforms existing methods in generating simulations regarding prompt alignment (i.e., accuracy), zero-shot transfer abilities, and human evaluation. We also demonstrate its effectiveness in generating robotic tasks.", "sections": [{"title": "1 Introduction", "content": "Simulations hold significant potential for training agents to perform real-world tasks where data collection is costly, dangerous, or infringes on individual privacy. A major bottleneck in harnessing the potential of simulations at scale for agent training is the cost of designing and developing them, especially when we need a distribution of simulations that meet detailed design specifications to train more generalized policies. In this paper, we aim to generate coded simulations given text specifications. Code provides a natural interface for users to inspect, modify, and debug the simulation. It also allows us to craft diverse environments for Reinforcement Learning (RL) purposes.\nGenerating full simulations in code to train agents from a text prompt is an under-explored challenge. Previous works focus on parts of this challenge, including reward function design [22], hyperparameter tuning [24], and task configuration while relying on an existing simulator [35]. These methods use large language models (LLMs) to generate the components of simulations specified as code. However, when faced with large and detailed contexts, LLMs often generate simulations that ignore or fail to adhere to parts of the input prompt [21]. This issue is not solely due to the limitations of existing LLMs but also suggests that some form of decomposition is always critical as we scale up the number of components in simulations. We ask the question: can we exploit the inherent structure (e.g., having a game loop that handles agent actions, updates internal game states accordingly, and displays the game states to the users through a rendering process) of coded simulations to generate them better?\nWe propose FACTORSIM, a framework that takes an arbitrary language specification as input and outputs a full simulation that can be used to train RL agents. The key idea of FACTORSIM is to decompose the input prompt into a series of steps and then use a factored Partially Observable Markov Decision Process (POMDP) representation to reduce the context needed for each generation step. To realize FACTORSIM, we use the model-view-controller software design pattern to structure the generation process. Consider generating a coded simulation of WaterWorld; see Figure 1. The game consists of an agent (blue circle) traveling in a 2d world, capturing food (green circle) while avoiding enemies (red circle). Our method first decomposes the game description into multiple steps to be implemented. For example, a step instruction could be \u201cIntroduce red dot enemies that can be controlled with arrow keys. Give the player a -1 reward when the agent collides with an enemy\". We first select the context needed for this functionality to be implemented, e.g., positions of existing agents. Subsequently, FACTORSIM generates (at most) three functions: one to handle player input (i.e., handle_key_press, the controller component), one to implement the collision logic (i.e., collision_logic, the model component), and one to update the rendering function (i.e., render_red_dot, the view component). Limiting the context during each step of the simulation generation process allows FACTORSIM to focus on the task at hand while avoiding hallucinating non-existent functions or modifying code not meant to be changed.\nTo evaluate the task of full simulation generation, we propose a new Generative Simulation\u2074 benchmark with accompanying success metrics. One set of success metrics is the pass rate in automated system tests. Commonly used in game development, these system tests programmatically assess whether the behavior of the generated simulation adheres to the specifications given in the input prompt. The second success metric assesses the value of the generated simulations for transfer learning in an RL setting. This evaluates how well agents trained on a set of generated simulations can generalize to held-out environments that satisfy the design specifications provided in prompts. Generalization to unseen environments is crucial for many applications, including transferring robotics"}, {"title": "2 Related Work", "content": "We aim to generate simulations for training agents to generalize to previously unseen environments. Recent work has investigated this in the context of learned neural world models and LLM-generated code for simulations.\nWorld models simulate the dynamics of a given environment and use this as a proxy environment for agent training, rather than interacting with a ground truth simulator [8]. Several approaches have demonstrated the value of learning world models as part of general algorithms that can learn to play a variety of games (AlphaZero [30], Muesli [13], and DreamerV3 [9]). Other efforts use a large set of offline data to learn a world model that is subsequently used for agent training, including for autonomous vehicle driving (GAIA-1 [14]), robotic manipulation (UniSim [40]), and 2D platformer games (Genie [5]). We generate world models as code as they are more interpretable, modular, and easily modified or extended by humans-key advantages we believe are important for their use in authoring large-scale or complex simulations.\nLLMs have generated many parts of simulations for game playing and robotics. In (RL) games, LLMs have been used to generate game levels [34, 31], to choose parameters for an existing simulator [44], and to assist humans in creating full games [3]. In robotics, LLMs have been used to generate reward functions, task specifications, and specific components like scene configurations within robotics tasks. Many works such as RoboGen [37], Holodeck [41], and Gen2Sim [16] build on top of existing simulators and use a series of prompts to generate interactable 3D environments to train agents. GenSim [35] starts from a human task library and iteratively generates and tests new tasks to generate robotic manipulation tasks. Other efforts have focused on generating reward functions for tasks [22, 19, 23]. Eureka [22] uses feedback from agent training to refine reward function specification. Our approach is able to generate both the simulator dynamics and reward functions and can be applied to both robotics and games.\nAs noted above, LLMs can struggle to handle complex tasks: this has prompted research into different ways to structure LLM reasoning. Chain-of-Thought (CoT) prompting demonstrated LLM performance can be substantially boosted by prompting the LLM to break a single task into multiple steps with either few-shot examples [38] or zero-shot [18]. Subsequent work has developed a variety of techniques to improve LLM reasoning through multi-step reasoning prompts: checking for consistency among multiple reasoning paths [36], interleaving reasoning and tool use (ReACT [42]), using tree data structures to guide the LLM reasoning process (Tree-of-Thought [43]), or formulating reasoning as a tree search process [12, 46]. Approaches for general code generation include decomposing the task into functions to subsequently generate (Parsel [45]), generating code to reach a series of intermediate execution states (ExeDec [28]), and using a multi-agent framework to generate, test, and refine code (AgentCoder [15]). Other efforts optimize the prompts for given tasks, using evolutionary search (EvoPrompt [7]) or defining generalized declarative programming frameworks with modular optimization algorithms [17]. Our approach generates code by leveraging a factorized representation specific to simulations to reduce the input context needed for different reasoning steps; it can be used in conjunction with approaches for general code generation, such as generating tests as a form of self verification."}, {"title": "3 FACTORSIM: Generating Simulations via Factorized Representation", "content": "A simulation is a structured system of modules connected by events and responses. Our framework, FACTORSIM, generates code using LLMs by exploiting this structure to construct a simulation progressively. Our key insight is that, by generating a simulation step-by-step while only selecting the relevant context information needed for each step, we can effectively reduce the reasoning capacity needed for each step, leading to simulations that adhere more closely to the input requirements.\nIn this section, we describe our method for generating Turing-computable simulations. First, we describe simulations that can be modeled as a Partially Observable Markov Decision Process (POMDP). Second, we use Chain-of-Thought (CoT) to decompose an input prompt describing the desired full simulation into a series of prompts describing different components to be implemented. Third, we introduce a factorized POMDP representation that exploits the inherent modularity of coded simulations. Refer to Algorithm 1 and Figure 2 for an overview of FACTORSIM alongside an illustrative example."}, {"title": "3.1 Modeling Simulation as POMDP", "content": "A Partially Observable Markov Decision Process (POMDP) is used to represent a coded simulation. Formally a POMDP is represented as a tuple M = (S, A, O, \u03a4, \u03a9, R) where S is a set of states, A is a set of actions, O is a set of observations, \u03a4 : S \u00d7 A \u2192 \u2206(S) is a transition probability distribution, \u03a9 : S \u2192 \u2206(0) is an observation function, and R : S \u00d7 A \u00d7 S' \u2192 R is the reward model \u2075.\nWe aim to generate a simulation from a prompt Qtext. In this paper, we are particularly interested in the case where Qtext comprises detailed design specifications such that the resulting simulation could be used to train agents, though our method applies to any prompt for defining a simulation. In our experiments, Qtext is a paragraph of text around 10 sentences specifying this simulation."}, {"title": "3.2 Chain of Thought", "content": "We first decompose the prompt Qtext into a series of steps using Chain of Thought [38], each describing a module of the simulation to be implemented. Following similar formulation as in [26], this can be thought of as marginalizing over a step-by-step plan variable (q\u2081, ..., qk) using N Monte Carlo samples:\np(M'|Q_{text}) = \\frac{1}{N} \\sum_{i=1}^{N} p(M'|q^{(i)}_{1}, ..., q^{(i)}_{K}), where (q^{(i)}_{1}, ..., q^{(i)}_{K}) \\sim p(q_{1}, ..., q_{K}|Q_{text}), (1)\np is a probability estimation model (i.e., an LLM in our experiments), and M' is the resulting code that fully specifies a simulation. In practice, we only produce a single plan N = 1.\nIntuitively, this process breaks the prompt into sub-tasks. After we sample such a plan of K steps, we generate the simulation progressively. Given an existing POMDP M and a natural language specification q, we update the POMDP to reflect the changes specified.\np(M_{K+1}|q_{1},...,q_{K}) \\approx \\prod_{k=1}^{K} [p(M_{k+1}|M_{k}, q_{k}) (2)\nwhere Mk+1 is the POMDP (simulation as code) after the k-th step is implemented, and MK+1 is the final simulation. While Chain-of-Thought prompting allows LLMs to avoid having to generate code for all simulation logic at once, the complexity of each step still grows with k due to the expanding codebase. This task remains challenging because LLMs must comprehend the code and accurately identify where modifications are needed. Acknowledging the limited reasoning ability of LLMs, we ask: can we further decompose the p(Mk+1|Mk, qk) into simpler distributions to reduce the complexity of each prompt?"}, {"title": "3.3 Decomposition by Factorized Representation", "content": "Naively, we could further decompose a step of the generation into several steps, each focused on generating a different component of the POMDP:\np(M_{k+1}|M_{k}, q_{k}) = p(S_{k+1}|M_{k}, q_{k}). (3)\np(T_{k+1}|S_{k+1}, M_{k}, q_{k}). (4)\np(R_{k+1}|S_{k+1},T_{k+1}, M_{k}, q_{k}). (5)\np(\u03a9_{k+1}|S_{k+1}, T_{k+1}, R_{k+1}, M_{k}, q_{k}) (6)\nHowever, this still requires the LLMs to take the entire simulation (Mk) as context, which could be over hundreds of lines of code in our experiments. Empirically, we observe that many failed generations can be attributed to LLMs attending to or modifying parts of the input context unrelated to the prompt.\nTo reduce the input context needed for each generation step, we propose to use a factored POMDP representation to remove the dependence on the full previous POMDP as context. For instance, given an existing simulation Mk of red, green, and blue agents, to implement the kth-step instruction qk: respawn the red agent when it collides with the blue agent, we only need context regarding the respawn logic of the red agent and the positions of the red and blue agents. Code regarding the green agent or the rendering logic would be unnecessary context.\nTo formalize our approach, we first introduce notation common to the literature [25, 32]. Suppose we have a POMDP with a state space factored into n state variables S = S[1] \u00d7 ... S[n] and Z is a subset of indices Z \u2286 {1, 2, ..., n}, we define the scope set S[Z] := \u2297iez S[i] as the state space spanned by the subset of state variables. For example, if Z = 1,3, 4, then S[Z] defines a state space defined by S[1] \u00d7 S[3] \u00d7 S[4]. We denote a state in the scoped state space S[Z] as s[Z]. Below, let us formally define a factored POMDP.\nDefinition 3.1. A factored POMDP is a POMDP with both factored transition distribution and factored reward function. A transition probability distribution T of a POMDP with discrete action space is factored over its state space S = S\u2081 \u00d7 ... Sn with scopes Z1, ..., Zm if, for all s \u2208 S, a \u2208 A\nT(s'|s, a) = \\prod_{i=1}^{m} T_{i}(s[i]|s[Z_{i}], a). (7)\nA reward function R of a POMDP is factored over S = S\u2081 \u00d7... Sn with scopes Z1,..., Z\u0131 if, for all s \u2208 S, a \u2208 A there exist some {R}=1 in the space of all possible reward functions on the state space S and action space A, such that,\nR(s, a) = \\sum_{i=1}^{l} R_{i}(s[Z_{i}],a). (8)\nA factored POMDP can be represented as a factor graph \u2076 with two types of nodes: state variables (i.e., Si) and factors (i.e., T\u2081 or Ri), functions of (state) variables. Our idea is to reduce context dependence by structuring the code using a factored POMDP representation and treat each generation step as expanding a factored POMDP with new state variables and new factors. During every step qk, we first select a set of relevant state variable indices Zk. Then, we select existing factors that have overlapping scope with the selected set of state variables as context, which we denote as T[Zk] and R[Zk]. That is, we can reduce the dependence on the previous simulation Mk and rewrite Equation 3-6 to the following:\np(M_{k+1}|M_{k}, q_{k}) \\approx p(S_{k+1}|S_{k}, q_{k})\u00b7update state space (9)\np(S[Z_{k}]|S_{k+1}, q_{k})\u00b7 identify relevant state variables (10)\np(T_{k+1}|T[Z_{k}], S[Z_{k}], A, q_{k})\u00b7 update state transition function (11)\np(R_{k+1}|R[Z_{k}], S[Z_{k}], A, q_{k})\u00b7update reward function (12)\np(\u03a9_{k+1}|S[Z_{k}], q_{k}). update partial observation function (13)\nNote that Zk can only consist of state variable indices in the state space Sk+1. In practice, we achieve this by encouraging the LLM to select a minimal set of relevant states Zk in the prompt.\nWe find that the term 11 is most prone to error, likely because the most complicated functions of a simulation are state transitions. Motivated by this observation, we propose to adopt the model-view-controller design pattern for structuring these prompts. Instead of prompting LLMs to update the state transition function first and then update the reward function, we prompt the LLMs to update the"}, {"title": "4 Experiments", "content": "In this paper, we consider two types of simulations: 2D Reinforcement Learning (RL) games and robotics tasks in a physics engine. We also introduce a new benchmark to evaluate generative simulation methods. Our experiments are designed to test three hypotheses. First, FACTORSIM generates simulations with better prompt alignment, which we evaluate through system tests and human evaluations. Second, FACTORSIM enables better zero-shot transfer by training RL agents in the simulated generated environments. Third, FACTORSIM's strengths in generating robotic tasks."}, {"title": "4.1 RL Game Generation", "content": "To answer our first two hypotheses,\nwe propose a new benchmark that in-\ncludes all 2D games from the PyGame\nLearning Environment \u2077 [33]: Flappy\nBird, Catcher, Puckworld, Pixelcopter,\nPong, Snake, Waterworld, and Mon-\nster Kong. For each RL game, the in-\nput prompt consists of the game's on-\nline documentation. Since most game\ndocumentation is incomplete, we man-\nually supplement them with additional\ndetails (see Appendix). This ensures\nthat our method and the baselines do\nnot hallucinate any missing game in-\nformation, allowing for a fair evalua-\ntion across all methods.\nFollowing common practices in game development, we design system tests to verify that the generated simulations follow the specified logic programmatically. These tests simulate actions like key presses and mouse clicks and check if the game states are updated correctly. Refer to the Appendix for more details."}, {"title": "5 Conclusion & Future Work", "content": "We have proposed FACTORSIM as an approach to generate full simulations as code that can train agents while adhering to detailed design requirements specified as a text prompt. We also introduce a benchmark suite of eight RL environments to evaluate generative simulation methods.\nGenerating complex simulations in code is challenging, and we anticipate numerous opportunities to extend the simulation generation process. There is substantial room to address larger-scale, more complex games, and robotics environments that require code bases beyond what can be used effectively in the context window of existing LLMs. We also see great potential to accelerate RL agent training by generating code that can be accelerated on GPU devices. Our robotic simulation results will benefit from further investigations to demonstrate transfer to real-world environments. We have only addressed single-agent simulations, leaving the extension of our method to multi-agent settings to future work. In the future, we also plan to incorporate information from the agent training process to automatically modify the generated simulation environment for enhanced agent learning and generalization. Taken together, we believe the generation of full simulations as code will be an"}, {"title": "A Societal Impact", "content": "This work can be applied broadly to many types of simulations, including robotics, autonomous vehicles, and other autonomous systems. Such systems have the potential for both positive and negative societal impact (e.g., harmful dual use). As researchers, we must critically evaluate such applications and promote beneficial ones. In this work we have focused on simulations with potential positive social impact, particularly in supporting the development of robots able to operate in human environments like households or manufacturing facilities.\nThe methods we present generate simulations that can be used to train agents to perform tasks. One risk with generated simulations is for training agents in an unintended manner. By generating simulations specified as code we mitigate this concern by making the behavior of the simulation explicit and inspectable by humans. Further, our approach is better able to guide LLMs to generate code that matches input design specifications compared to baseline methods, reducing the risk of LLMs inadvertently producing undesirable functionality. We believe this can help enhance the reliability of generated simulations while offering strong editing and control capabilities to humans.\nThe potential negative environmental impact of the compute for using our technique is small. We have shown our technique consumes less tokens than comparable methods to yield equally good results. Thus our method can be seen as a way to reduce computational needs when using LLMs for tasks like creating simulations. Compared to systems that use a neural world model our approach benefits from the relatively lower computational costs of running simulations in code compared to running large neural models for simulation."}, {"title": "B Additional details of our experiments", "content": "All experiments are done on a workstation with 8 Nvidia A40 GPUs and 1008G of RAM. For our code generation experiments, one generation (i.e., generation of one training environment) takes around 30 seconds to 5 minutes. For our Reinforcement Learning experiments, one trial of training (i.e. training on a set of environments for 10M steps in total) takes around 3-5 hours to complete. In all of our experiments, GPT-4 refers to the OpenAI's \"gpt-4-1106-preview\" model, GPT-3.5 refers to OpenAI's \"gpt-3.5-turbo\" model, and Llama-3 refers to the open-sourced \"meta-llama-3-70b-instruct\" model that can be found on huggingface.\nFor the zero-shot transfer RL experiment, we supply all methods with the reference \"controller\" (i.e., the same key press/mouse click leads to the same thing). We do this because language descriptions of such can be very ambiguous (e.g., a description \"a key press leads the bird to flap its wings\" can imply a change in position, velocity, or acceleration). In our experiments, we generate 10 environments and filter out those that cannot be properly executed with a random policy.\nAll the RL experiments are implemented in RLLib [20] 8. The PPO agent is trained with a batch size of 10,000, and an SGD minibatch size of 2048. Our agent used a fully connected network with hidden layers of sizes (4, 4) and post-FCNet hidden layers of size 16, all employing ReLU activation functions. The policy network uses an LSTM with a cell size of 64 to incorporate previous actions but not previous rewards. Over the course of the 10 million training steps, 20 checkpoints were saved, with the best zero-shot performance on the testing environment reported."}, {"title": "C Additional details of FACTORSIM", "content": "We provide code in the supplementary material. Here we provide the prompts used in FACTORSIM.\nListing 1: The first decompositional prompt used in FACTORSIM.\nGiven an unstructured game description, decompose the game's specification into a set of steps or modules. Each step or module should contain at most one input event handling, one state transitional logic, and one rendering logic.\nIf we model the game as a Markov Decision Process (MDP), the steps, or modules of the game, should share as little state variables as possible.\nPlease provide the response in the following format:\n\"\"\"json\nhttps://docs.ray.io/en/latest/rllib/index.html"}, {"title": "D Additional details and results for the robotics task generation experiment", "content": "In this section, we provide the prompts we used for FACTORSIM in the robotics task generation experiment. The prompts for the baselines can be found in the GenSim paper\u00ba [35].\nTo conduct human evaluation, we begin by observing the oracle agent attempting to solve the task. If the oracle agent successfully completes the task, we then assess whether the resulting goal states align with the input task prompt. If the oracle agent fails to solve the task, we investigate the reason for the failure. Often, the cause is apparent, such as the target container being too small or not having the right color of objects for the task. These are marked as failures. For cases where it is clear that the limitation lies in the oracle agent's ability, or when the reason for failure is not immediately apparent, we manually inspect the code for the task specification and base our decision on both the code and our observation of the oracle agent's attempt at solving the task."}, {"title": "E Additional details for the proposed generative simulation benchmark", "content": "This section provides the prompts for all 8 RL games in the benchmark.\nListing 9: The prompt for the game Catcher.\nCreate a catcher character, represented as a rectangle, positioned at the bottom and the middle of the screen.\nAllow the player to control the catcher's horizontal movement using the left and right arrow keys on the keyboard.\nThere should always be exactly one ball on the screen at all times. The ball should be visually distinct and easily recognizable.\nMake the ball move downwards at a steady pace towards the catcher. The speed can be constant or increase gradually as the game progresses.\nDetect collisions between the catcher and the ball. When the catcher catches a ball, increment the player's score, spawn a new ball, and display this score in the top-left corner of the screen.\nGive the player a 3 lives. Each time a ball is missed by the catcher and reaches the bottom of the screen, decrease the player's life count by one.\nEnd the game when the player's lives reach zero. Display a \"Game Over!\" message and temporarily halt gameplay but dont terminate the game.\nOver\" screen is displayed.\nProvide an option for the player to click the screen to restart the game after the \"Game Continuously generate new balls after each catch or miss, ensuring endless gameplay.\nOptionally, increase the game's difficulty gradually by speeding up the ball's fall or reducing the size of the catcher as the player's score increases.\nListing 10: The prompt for the game Flappy Bird.\nCreate a bird character, visually represented as a simple rectangle within the game window.\nIntroduce gravity, causing the bird to continuously fall slowly.\nAllow the bird to jump or accelerate upwards in response to a player's mouse click, temporarily overcoming gravity.\nPeriodically spawn pairs of vertical pipes moving from right to left across the screen. Each pair should have a gap for the bird to pass through, and their heights should vary randomly.\nIf the bird makes contact with the ground, pipes or goes above the top of the screen the game is over.\nImplement the following scoring system: for each pipe it passes through it gains a positive reward of +1. Each time a terminal state is reached it receives a negative reward of -1.\nWhen the game ends, display a \"Game Over!\" messagea and stop all the motion of the game. Show the current score in the top-left corner of the screen during gameplay.\nEnsure the game has no predefined end and that new pipes continue to generate, maintaining consistent difficulty as the game progresses.\nListing 11: The prompt for the game Snake.\nCreate a snake character represented as a series of connected pixels or blocks. Initially the snake should be a single block (i.e. the head) that moves in a specific direction within the game window.\nAllow the player to control the snake's movement using arrow keys. The snake should be able to turn left or right, but it should not be able to move directly backward. Eg if its moving downwards it cannot move up.\nThe movement of the snake should be continuous in the current direction until the player provides new input. Ensure that the snake moves one grid unit at a time.\nImplement a basic food system where one food item appears randomly on the screen. When the snake consumes the food by moving over or colliding with it, the snake's length increases, and the player earns points. It recieves a positive reward, +1, for each food the head comes in contact with. While getting -1 for each terminal state it reaches.\nIf the head of the snake comes in contact with any of the walls or its own body, the game should end.\nIncorporate a scoring system, displaying the current score on the screen during gameplay. The score should increase each time the snake consumes food.\nEnsure that the game has no predefined end, allowing the snake to continue growing and the difficulty to increase over time. New food items should appear after the snake consumes one.\nProvide an option for the player to restart the game after it ends. Display a \"Restart\" option on the game over screen to allow the player to play again.\nListing 12: The prompt for the game Pixelcopter.\nCreate a copter character represented as a large white square that remains fixed horizontally but can ascend and descend vertically within the game window."}, {"title": "F Additional details for the human study experiment", "content": "In this section, we first provide details for the experiment and then the instructions we gave to human participants in our human study, along with the user interface. We also provide the detailed results of this evaluation for all games in Figure 9.\nHuman participants were asked to play and evaluate the generated games given the prompt while excluding factors such as aesthetics or difficulty. They rated the games on a scale of 1 to 4, where 4 indicates a fully playable game, 3 is a playable game with some bugs or flaws that hinder gameplay experience, 2 is an unplayable game (i.e., no interactivity) with correctly rendered UI, and 1 is a game that crashes or fails to launch.\nListing 17: The instructions we give to human participants to our human study.\nWelcome to your user study! Your task is to evaluate AI-generated games.\nSelect the game you want to generate and click the button \"Generate\" to generate games. You might have to wait for the game to load for 5-10 seconds.\nNote that the game is intentionally slowed down, making it easier for you to evaluate them!\nThus, when you click or press a key/button, the \"character\" might react slower than you expected.\nPlease click \"Random Generate\" to generate a game.\nIf the game doesn't load (black screen), select \"1 - unplayable.\"\nPlease do not consider the difficulty of the game.\nPlease don't take aesthetics into account.\nYou want to assess whether the UI elements are rendered accurately for gameplay purposes while excluding considerations related to aesthetics, overlapping, or duplicated UI components.\nGiven the prompt as shown, please judge the playability of the game.\n4: Fully Playable: the game is generally playable from start to finish without significant bugs.\n3: Playable but with some flaws: the game is somewhat playable (interactable), but there are issues (inaccurate logic or glitches) that impair the gameplay.\n2: Not playable: no interactivity but the UI seems to be rendered correctly.\n1: Unplayable: the game cannot be started, or it crashes immediately upon launch."}]}