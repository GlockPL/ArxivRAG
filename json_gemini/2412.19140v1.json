{"title": "SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis", "authors": ["Senbin Zhu", "Chenyuan He", "Hongde Liu", "Pengcheng Dong", "Hanjie Zhao", "Yuchen Yan", "Yuxiang Jia", "Hongying Zan", "Min Peng"], "abstract": "In recent years, fine-grained sentiment analysis in finance has gained significant attention, but the scarcity of entity-level datasets remains a key challenge. To address this, we have constructed the largest English and Chinese financial entity-level sentiment analysis datasets to date. Building on this foundation, we propose a novel two-stage sentiment analysis approach called Self-aware In-context Learning Correction (SILC). The first stage involves fine-tuning a base large language model to generate pseudo-labeled data specific to our task. In the second stage, we train a correction model using a GNN-based example retriever, which is informed by the pseudo-labeled data. This two-stage strategy has allowed us to achieve state-of-the-art performance on the newly constructed datasets, advancing the field of financial sentiment analysis. In a case study, we demonstrate the enhanced practical utility of our data and methods in monitoring the cryptocurrency market. Our datasets and code are available at https://github.com/NLP-Bin/SILC-EFSA.", "sections": [{"title": "1 Introduction", "content": "The importance of sentiment analysis in the financial domain has increasingly become apparent. As early as 1970, Fama recognized the potential of sentiment analysis in finance and introduced the concept of the Efficient Market Hypothesis (EMH) (Fama, 1970). The EMH suggests that stock prices respond to unexpected fundamental information, supporting the use of sentiment analysis in finance. With the rapid growth of the internet and the financial sector, numerous stock reports, research papers, and investor opinions have become valuable for assessing companies and events, playing a key role for both investors and regulators.\nCurrently, most sentiment analysis corpora in the financial domain use sequence-level annotation."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Entity-level Sentiment Analysis of Financial Texts", "content": "NLP techniques have gained widespread adoption in financial sentiment classification (Kazemian et al., 2016; Yang et al., 2022; Chuang and Yang, 2022; Xing et al., 2020). Recent studies have achieved state-of-the-art performance in SemEval 2017 Task 5 and FiQA Task 1 (Du et al., 2023). With advances in NLP, sentiment analysis has shifted from coarse- to fine-grained approaches (Du et al., 2024). However, entity-level sentiment analysis in financial texts, a key fine-grained task, remains in its early stages (Zhu et al., 2020) and faces several challenges (Tan et al., 2023).\nExisting financial sentiment classification datasets, such as Financial Phrase Bank (Malo et al., 2014), SemEval2017 (Cortis et al., 2017), AnalystTone dataset (Huang et al., 2023), Headline News dataset (Sinha and Khandait, 2021), and Trillion Dollar Words (Shah et al., 2023a), are based on entire text sequences (sentences or articles). FiQA\u00b9 is an open challenge dataset with aspect-level sentiment. However, it does not include entity annotations. For financial entity annotation datasets, FiNER (Shah et al., 2023b) and FNXL (Sharma et al., 2023) have been created for financial entity recognition and numerical span annotation, respectively, but both lack sentiment annotations. The FinEntity dataset is a dataset with entity spans and sentiment information.\nGururangan et al. (2020); Zhang et al. (2023a) suggest that re-training general PLMs on domain-specific corpora enhances performance on specialized tasks. However, entity-level financial sentiment analysis requires further research due to the unique complexity of financial entities compared to general text entities (Zhang et al., 2023b). Tang et al. (2023) have achieved preliminary results in entity-level sentiment analysis tasks using a combination of FinBERT and CRF."}, {"title": "2.2 LLMs in Finance", "content": "Large Language Models (LLMs) are considered a technological breakthrough in the field of natural language processing, as exemplified by GPT-3 and GPT-4 (Brown et al., 2020). LLMs have been applied to various tasks in the financial domain (Dredze et al., 2016; Araci, 2019; Bao et al., 2022; DeLucia et al., 2022; Konstantinidis et al., 2024; Ahmed et al., 2024), from predictive modeling to generating insightful narratives from raw financial data.\nAn early example of a financial LLM is BloombergGPT (Wu et al., 2023). Lan et al. (2024) further apply large models to specific tasks in enterprise alert systems and constructed the FinChina SA dataset, achieving meaningful results. Chen et al. (2024) obtain desirable performance on event-level datasets using their proposed four-hop reasoning chains. However, it is worth noting that experiments have shown that the CoT (Chain-of-Thought) framework negatively impacts entity-level financial sentiment analysis tasks, likely due to the complex reasoning processes involved.\nSelf-correction techniques aim to improve the accuracy of LLM outputs by enabling models to revise their initial predictions (Pan et al., 2023; Kamoi et al., 2024). The fundamental problem with existing methods is that LLMs cannot reliably assess the correctness of their inferences (Huang et al., 2024a). Recent studies have shown that incorporating examples with feedback into the context can improve response quality (Xu et al., 2024). These findings highlight the significant research potential for correction strategies in entity-level sentiment analysis using LLMs."}, {"title": "3 Datasets Construction", "content": "The FinEntity dataset provides annotations for financial entity spans and their associated sentiments within text sequences, with sentiment labels categorized as positive, neutral, or negative (Tang et al., 2023). It treats entity-level sentiment classification as a sequence labeling task using the BILOU annotation scheme. The dataset contains 979 example sentences, with 2,131 entities in total. Additionally, approximately 60% of the financial texts contain multiple entities. However, due to the small size of this dataset, we construct two additional datasets-one in English and one in Chinese-to provide a more comprehensive analysis of this task. Detailed information about the constructed dataset is shown in Table 1, with 20% of the data randomly selected as the test set for experiments."}, {"title": "3.1 English Dataset", "content": "SEntFiN is a manually annotated dataset designed for fine-grained financial sentiment analysis of news headlines, with sentiment labels linked to financial entities (Sinha et al., 2022). The sentiment labels are defined as positive, neutral, and negative. In total, the dataset includes 14,404 entity and sentiment annotations.\nWe apply a rule-based approach to add entity location tags to the annotations, aligning the label format with the FinEntity dataset for ease of subsequent work. This restructured dataset is named SEntFiN-Span.The reconstructed dataset remains relatively balanced in terms of sentiment distribution."}, {"title": "3.2 Chinese Dataset", "content": "To address the shortage of Chinese financial sentiment analysis datasets, Chen et al. (2024) pioneer a new task called Event-Level Financial Sentiment Analysis, which involves predicting a five-tuple (company, department, coarse-grained event, fine-grained event, sentiment). To support this task, they construct a large-scale publicly available dataset containing 12,160 news articles and 13,725 five-tuples.\nTo adapt this dataset for entity-level sentiment analysis tasks, we first keep data containing only single-type entity label information. Such data amount to 10,832 instances, accounting for 89% of the dataset. We then use a rule-based approach to annotate the spans of financial entities while ignoring event labels, simplifying the task to purely entity-level financial sentiment analysis. The processed dataset has an average text length of 145.23 words, with 75.65% of the data containing a single entity and 24.35% containing multiple entities. This restructured dataset is named FinEntCN."}, {"title": "4 Methodology", "content": "The methodology of our study consists of two stages: base model fine-tuning and error correction model training. The methodological framework is illustrated in Figure 2."}, {"title": "4.1 Stage 1: Initial Response Generation", "content": "In the first stage, we aim to fine-tune the base LLMs for entity-level sentiment analysis in the financial domain. We use two models for different datasets: LlaMA2-7b-hf-finance for the English dataset and Baichuan2-7b for the Chinese dataset.\nDuring the fine-tuning process, we develop multiple versions of instruction templates to ensure their general applicability. Each instruction is designed to comprehensively describe and clearly convey the task requirements, including financial entity recognition, sentiment classification, and the details of result annotation. In the output phase, the model must accurately identify entity names, clearly delineate their boundaries within the text, and classify their sentiment polarity. To enhance the effectiveness of the fine-tuning, we incorporate three manually selected fixed examples into the instructions. These examples are carefully chosen to cover a range of scenarios within the task, providing strong representativeness and comprehensiveness. The instruction template for this stage is shown in Figure 6 in Appendix. After the fine-tuning process, we perform predictions on both the training and test sets, generating initial labels, which we refer to as pseudo-labels.\nThe problem can be formalized as follows: Let Dtrain and Dtest represent the training and test datasets, respectively. For each text t \u2208 Dtrain \u222a Dtest, we want to predict the sentiment label ys for each entity e within the text. The fine-tuned model"}, {"title": "4.2 Stage 2: Self-correction Steps", "content": "The second stage focuses on training an error correction model to identify and rectify errors in the pseudo-labels generated during the first stage. The specific steps are as follows: We begin by filtering the pseudo-labeled data Dtrain. Let Ccorrect and Cincorrect represent the correctly and incorrectly labeled samples, respectively. To emphasize the model's attention on erroneous samples, we filter the training data by retaining all incorrectly predicted samples Cincorrect, while removing a portion of the correctly predicted samples Ccorrect:\n\nDfiltered = Cincorrect \u222a S(Ccorrect)\n\nwhere S is a sampling function that selects a subset of correctly predicted samples. Next, we fine-tune an error correction model Mcorrect using the filtered dataset Dfiltered.\nWe reference the GNN-based context example retriever proposed by Yang et al. (2024), as shown in Figure 2. The GNN example retriever uses a graph attention network (GAT) as the base model, with two GAT layers designed for linguistic and sentiment features. It outputs feature representations rich in linguistic and sentiment information, along with sentence-level average representations. During training, the GNN model employs contrastive learning, where linguistic features (such as syntactic dependencies and part-of-speech tags) and sentiment features (such as sentiment polarity) are extracted from the training set based on heuristic rules for comparison. This encourages the GNN's output to be optimized in both linguistic and sentiment dimensions. By encoding the dataset using the trained GNN model, three feature representations enriched with linguistic and sentiment characteristics are obtained. Finally, for prediction, approximate nearest neighbor search is used to retrieve the most similar examples from the encoded training set.\nWe incorporate the retrieved examples into the context of the fine-tuning instructions, providing information on whether the pseudo-labels are correct. This allows the model to learn how to judge the accuracy of pseudo-labels. Since the model from the first stage may produce similar errors on similar examples, this retrieval method helps the correction model identify and correct these errors. The trained error correction model is then used to detect and correct the pseudo-labels generated by the first stage on the test set. By employing this two-stage approach, we further enhance prediction accuracy and reliability. The corrected-labels are formally consistent with pseudo-labels.\nIt is important to note that during the fine-tuning of the error correction model, we do not include additional instructions related to entity-level sentiment analysis in the financial domain but focused solely on the error correction task. This design avoids potential negative impacts on model performance from the comprehensiveness of task instructions, ensuring that the model remains concentrated on the error correction task itself. The correction task prompt template can be found in Figure 7 in Appendix."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Baselines", "content": "FinBERT-CRF: FinBERT is a BERT variant for finance, and FinBERT-CRF adds a CRF layer for token label dependencies (Yang et al., 2020; Tang et al., 2023).\nSpanABSA: SpanABSA is a span-based extract-then-classify framework (Hu et al., 2019).\nInstruct ABSA: InstructABSA (Scaria et al., 2024) is an aspect-based sentiment analysis method based on instruction learning.\nT5: T5 is a general text generation model proposed by Google Research (Raffel et al., 2020).\nCh_finT5_base: Pre-trained Language Model in the Chinese financial domain (Lu et al., 2023)."}, {"title": "5.2 Experimental Settings", "content": "The detailed experimental settings of our method and parameter configurations of methods such as SpanABSA, BGCA, and InstructABSA can be found in Appendix B."}, {"title": "5.3 Overall Performance", "content": "Table 2 and Table 3 present the main experimental results, demonstrating that our proposed method outperforms all benchmark models on most metrics across three datasets. On the FinEntity dataset, our approach improves the F1 score by 5.1% compared to the previous best method, and it also performs well on the other two datasets. In our comparative study, we explore pre-trained models that have been successfully applied to aspect-based sentiment analysis tasks. The results show that the fine-tuning approach for LLMs demonstrates excellent"}, {"title": "5.4 Ablation Study", "content": "At this stage, we explore the contributions of various components within our framework. Table 4 presents the results of different model variants.\nTo evaluate the effectiveness of the second stage, we compare the experimental results with those from the first stage, which involves only basic fine-tuning. The results demonstrate that the correction strategy improves performance across all three datasets. Furthermore, we examine the role of the GNN-based example retriever in the second stage. We replace the in-context examples with the same number of fixed examples and report the corresponding results. Overall, the absence of any feature typically results in a decrease in F1 scores compared to the complete model."}, {"title": "5.5 Key Parameters Analysis", "content": null}, {"title": "5.5.1 The Number of In-context Examples", "content": "Figure 3 presents the experimental results of fine-tuning the LlaMA2-7B model on the FinEntity dataset using different numbers of randomly selected in-context examples. Compared to the task fine-tuning template without examples, the inclusion of such examples leads to a significant improvement in model performance. As the number of in-context examples increases, the results indicate that the model achieves optimal performance with three examples. This finding provides a basis for selecting three examples in the above main experiments."}, {"title": "5.5.2 The Positive-to-negative Sample Ratio", "content": "After obtaining the pseudo-labeled data from the first stage, we filter the training set and retain different proportions of correct examples to investigate the impact of the positive-to-negative sample ratio on the performance of the correction model in the second stage. The experimental results are shown in Figure 4. As the proportion of correct samples retained increases, the performance of the fine-tuned correction model initially improves and then declines. This indicates that when training a correction model, an excessively high or low ratio of correct to incorrect samples in the training set can negatively affect correction performance, potentially leading to worse outcomes."}, {"title": "5.6 Case Study", "content": "To better demonstrate the effectiveness of our framework, we conduct case studies for both English and Chinese texts, as shown in Table 7 in Appendix.\nIn the English case 1, the model fine-tuned in the first stage accurately identifies two financial entities in the text: Twitter Inc and Tesla Inc. The description of Twitter Inc's stock increase is correctly classified as positive sentiment. However, the model incorrectly classifies the sentiment for Tesla Inc as positive as well, which is a common issue in multi-entity sentiment analysis where the model erroneously assumes uniform sentiment across multiple entities. In the second stage, we introduce three correction examples to guide the model in evaluating and adjusting the pseudo-labels, which leads to successful results. Table 8 in Appendix presents the experimental results of our method, showing the F1 scores for entity and sentiment classification. Both scores improved after our correction stage, with a greater increase in the sentiment polarity score.\nAdditionally, we observe that LLMs also produce significant errors similar to those in the Chinese case 1, likely due to the lack of financial domain optimization in the Chinese base models. Our correction strategy proves effective in these instances as well. English case 2 and Chinese case 2 demonstrate the limited ability of our method to correct the model's persistent misjudgments regarding entity boundaries and categories."}, {"title": "6 Case Application: Cryptocurrency Market Prediction", "content": "Studies have shown a positive contemporaneous correlation between Bitcoin prices and entity-level sentiment scores, with the maximum information coefficient (MIC) between cryptocurrency prices and sentiment indicating a moderate positive correlation. Furthermore, entity-level sentiment demonstrates higher correlations than sequence-level sentiment (Tang et al., 2023), suggesting that market sentiment plays a positive role in regulating price volatility."}, {"title": "7 Conclusions", "content": "Our research focuses on the task of entity-level sentiment analysis in the financial domain, for which we have constructed the largest English and Chinese datasets. Moreover, we propose an innovative strategy called \"Self-aware In-context Learning Correction\" (SILC). The SILC framework consists of two stages and significantly improves the accuracy by enabling the model to learn correction examples relevant to the current instance. Experimental results demonstrate that the proposed SILC strategy effectively enhances model performance, achieving state-of-the-art results. Additionally, the"}, {"title": "Limitations", "content": "The proposed method involves multiple training stages, which, while enhancing model refinement, also increase training time and computational requirements. This could impact scalability and resource use. To mitigate these issues, optimization techniques such as model pruning and knowledge distillation, as well as cloud computing and distributed training, can be employed."}, {"title": "Ethics Statement", "content": "This study adheres to the ACL Code of Ethics. The data collected comes entirely from publicly accessible datasets. Our constructed datasets do not disseminate personal information and do not contain content that could potentially harm any individual or community."}, {"title": "4.2 Stage 2: Self-correction Steps", "content": "Dfiltered = Cincorrect \u222a S(Ccorrect)"}, {"title": "B Experimental Parameter Settings", "content": "Our method involves multiple stages. In the first stage of supervised fine-tuning, the learning rate and number of epochs are set to 8 \u00d7 10-5 and 5, respectively. For the GNN training phase, we define different heuristic rules for Lig and Osen to distinguish between linguistic and sentiment feature similarities. The linguistic (@Lig) and sentiment (@sen) parameters are illustrated in Table 6. We use a BERT-based uncased tokenizer as the token encoder, with an initial learning rate of 1 \u00d7 10\u22124,"}]}