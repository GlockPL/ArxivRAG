{"title": "Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models", "authors": ["Yunpu Zhao", "Rui Zhang", "Junbin Xiao", "Changxin Ke", "Ruibo Hou", "Yifan Hao", "Qi Guo", "Yunji Chen"], "abstract": "Large Vision-Language Models (LVLMs) have shown significant capability in vision-language understanding. However, one critical issue that persists in these models is sycophancy, which means models are unduly influenced by leading or deceptive prompts, resulting in biased outputs and hallucinations. Despite the progress in LVLMs, evaluating and mitigating sycophancy is yet much under-explored. In this work, we fill this gap by systematically analyzing sycophancy on various VL benchmarks with curated leading queries and further proposing a text contrastive decoding method for mitigation. While the specific sycophantic behavior varies significantly among models, our analysis reveals the severe deficiency of all LVLMs in resilience of sycophancy across various tasks. For improvement, we propose Leading Query Contrastive Decoding (LQCD), a model-agnostic method focusing on calibrating the LVLMs' over-reliance on leading cues by identifying and suppressing the probabilities of sycophancy tokens at the decoding stage. Extensive experiments show that LQCD effectively mitigate sycophancy, outperforming both prompt engineering methods and common methods for hallucination mitigation. We further demonstrate that LQCD does not hurt but even slightly improves LVLMs' responses to neutral queries, suggesting it being a more effective strategy for general-purpose decoding but not limited to sycophancy.", "sections": [{"title": "Introduction", "content": "Large Vision-Language Models (LVLMs) have attracted significant attention for their ability to understand and generate multimodal data by integrating textual and visual information. These models have been applied in diverse domains, showcasing impressive capabilities that push the boundaries of artificial intelligence (Zhang et al. 2024; Cui et al. 2024). Despite their success, LVLMs are widely criticized for the hallucination problem (Liu et al. 2024b; Xiao et al. 2024), of which sycophancy (Sharma et al. 2024; Perez et al. 2023) is a key factor but has rarely been analyzed. Sycophancy refers to the phenomenon where a model tends to agree with the statement in input queries in unwanted ways (Sharma et al. 2024; Perez et al. 2023). It is a sort of model susceptibility, meaning that models are unduly influenced by text interference or deceptive prompts. As such, sycophancy often results in performance decreasing, bias and hallucination (Cui et al. 2023; Qian et al. 2024), as shown in Figure 1. In practice, sycophancy can lead to AI violating human ethics under deliberate inducement, causing biased and discriminatory output and severely impact the widespread real-world application of LVLMs. Thus, the need to evaluate and mitigate sycophancy in LVLMs is paramount. While existing benchmarks focus on evaluating various types of hallucination of LVLMs, they typically employ neutral queries, and thus fail to assess the model's robustness against sycophancy (Li et al. 2023b, 2024; Yu et al. 2024). Although some research has examined sycophancy in LLMs and find that finetuning and alignment techniques based on human feedback likely bring sycophancy in models (Sharma et al. 2024; Cui et al. 2023; Qian et al. 2024), studies about sycophancy in LVLMs are missing. Sycophancy in LVLMs is more complex than in LLMs due to the integration of visual knowledge. The additional visual and cross-modal fusion module often increase the instability of the model's output and can also interfere with the results generated by the language module. For instance, the model's attention to visual information often differs from the attention to texts, which adds the complexity of analyzing sycophantic behavior. Additionally, there is a notable absence of work for mitigating sycophancy in LVLMs. This work thus conducts the first systematic analysis to"}, {"title": "Related Work", "content": "Inspired by the significant achievements of LLMs, researchers have developed a series of LVLMs by integrating them with visual pretraining models. A typical architecture consists of a visual encoder, an LLM, and a modality alignment module. The visual encoder often employs popular visual pretraining models (Han et al. 2022), while the LLM can utilize various popular models like Vicuna and LLaMA. The modality alignment module is finetuned using paired image-text data, endowing the model with visual dialogue capabilities. Some of the representative models that have garnered widespread attention include Qwen-VL, LLaVA, and CogVLM (Wang et al. 2023b; Liu et al. 2024a; Bai et al. 2023), and these are also baselines in our experiments. Due to issues related to network architecture, the quality of training data, and inherent problems within LLMs themselves, LVLMs face numerous challenges.\nHallucination of LVLMs and Sycophancy\nAmong the many flaws of LVLMs, the most prominent is the hallucination problem, which also exists in LLMs and is particularly severe in LVLMs. Existing research has analyzed the causes of hallucination (Liu et al. 2024b). Numerous new benchmarks are also proposed to evaluate the hallucination in LVLMs (Li et al. 2023b; Wu et al. 2024; Guan et al. 2024). Different types of hallucination exhibit different manifestations and require various mitigation strategies. Currently, research on alleviating hallucination in LVLMs focuses on three primary approaches: the first is prompt engineering (M\u00fcndler et al. 2024; Lee et al. 2023), the second is enhancing the model itself (Liu et al. 2023a; Jiang et al. 2023), and the third is post-processing of the results. This post-processing could involve new decoding strategies or using external tools like GroundingDINO (Liu et al. 2023b) to detect and eliminate hallucinations (Leng et al. 2024; Huang et al. 2024; Chen et al. 2024b). Sycophancy is a significant cause of model hallucination. Some studies have reported that leading queries or prompts that are biased can interfere with the responses of LVLMs, indicating that the models often do not verify the prior statements in the queries and tend to agree with users (Cui et al. 2023; Qian et al. 2024). However, evaluating and mitigating for sycophancy are rarely studied. It remains unknown whether the aforementioned methods that have shown effective for alleviating object/attribute hallucinations, are capable of solving hallucinations in the context of leading queries. This gap thus underscores the need on related investigation, which is also the goal of this work.\nContrastive Decoding\nThe main idea of contrastive decoding is a commonly employed in image and text generation. For instance, in classifier-free diffusion models, a contrastive objective is used to estimate diffusion models (Ho and Salimans 2022). In text generation, there is often a significant performance difference between the two models being compared, such as an expert LLM and an amateur LLM (Li et al. 2023a). This"}, {"title": "Dataset Construction", "content": "In this section, we construct specifically datasets to comprehensively and systematically evaluate sycophancy phenomenon in LVLMs. We select five popular Visual Question Answering (VQA) datasets, and then edit and extend the question text to inject leading and deceptive information for sycophancy evaluation.\nData Source. We select and extend five datasets to evaluate sycophancy in LVLMs, as shown in Table 1. Considering that sycophancy is an factor of hallucination, we select two popular datasets POPE (Li et al. 2023b) and AMBER (Wang et al. 2023a) which are proposed to evaluate hallucination. Meanwhile, we also select three VQA datasets RealworldQA (x.ai 2024), ScienceQA (Lu et al. 2022) and MM-Vet (Yu et al. 2024) to evaluate the effect of sycophancy on commonsense comprehension and complex multimodal tasks. POPE (Li et al. 2023b) is designed to assess object hallucination in LVLMs through yes-or-no questions regarding the presence of objects in images. POPE is divided into three settings: random, popular, and adversarial, indicating different methods of sampling hallucination objects. AM-BER (Wang et al. 2023a) is a comprehensive benchmark designed to evaluate the hallucination performance of LVLMs. In our experiments, AMBER is mainly used for analyzing the attribute hallucination and relation hallucination. Real-worldQA (x.ai 2024) is designed to evaluate the real-world spatial understanding capabilities of large multimodal models. ScienceQA (Lu et al. 2022) is designed for evaluating multimodal reasoning in science question answering. MM-Vet (Yu et al. 2024) is designed to assess the integrated capabilities of LVLMs based on 16 complex multimodal tasks, focusing on six core vision-language capabilities: recognition, optical character recognition (OCR), knowledge, language generation, spatial awareness, and math.\nLeading Query Generation. Queries in these above-mentioned datasets are neutral without explicit misleading cues. Thus, the neutral queries are incapable of assessing"}, {"title": "Leading Query Contrastive Decoding", "content": "In this section, we propose a method called Leading Query Contrastive Decoding to alleviate sycophancy. Sycophancy can be problematic as it biases the output, potentially leading to less accurate or relevant results. Leading query can aggravate sycophancy since a leading query essentially amplifies language priors in a certain fixed direction, thereby reducing the model's sensitivity to visual perception. Thus, to mitigate sycophancy in LVLMs, we need to calibrates the model's over-reliance on language priors of leading queries. To achieve this goal, it is necessary to suppress the token probability of incorrect answers caused by sycophancy. Inspired by (Li et al. 2023a; Leng et al. 2024), we designed this suppress mechanism based on contrastive decoding, and propose Leading Query Contrastive Decoding. During decoding stage of LVLMs, LQCD contrasts output token distributions of generated neutral query and leading query, as shown in Figure 3. Therefore, the over-reliance on language priors of leading queries can be removed, so as to suppress the token probability of sycophancy answers and highlight the expected neutral answers.\nContrastive Objective\nSpecifically, given a textual leading query \\(x_l\\) and a visual input \\(v\\), we firstly generate transformed neutral query \\(x_n\\) corresponding to \\(x_l\\) by using LLM. Then LVLM model generates two distinct output distribution \\(logit_{\\theta}(y|x_l, v)\\) conditioned on the leading query \\(x_l\\) and \\(logit_{\\theta}(y|x_n, v)\\) conditioned on the transformed neutral query \\(x_n\\). The contrastive probability distribution \\(P_{lqcd}\\) of LQCD is computed by exploiting the differences between the two obtained distributions, formulated as:\n\\[P_{lqcd} (y|x_n, x_l, v) = softmax[(1 + \\alpha)logit_{\\theta} (y/x_n, v) - \\alpha logit_{\\theta} (y|x_l, v)]\\]\nwhere \\(\\alpha\\) is a weight hyperparameter, \\(y\\) is the resulting token distribution, \\(\\theta\\) denotes the model parameters. Larger \\(\\alpha\\) value signifies a greater amplification of disparity between the two distributions (where \\(\\alpha = 0\\) corresponds to regular decoding). The objective enhance token probability favored by the neutral query output and suppress token probability favored by leading query output.\nAdaptive Plausibility Constraint\nHowever, leading query output are not always mistaken: except the hallucination caused by sycophancy, the output still capture many simple aspects of grammar and common sense. Thus, penalizing all tokens from leading query output indiscriminately would penalize these simple aspects that are correct (For example, due to tokenization, the probability of the word \"is\" in \"The color is red\" is close to 1 under both neutral and leading query output, but the contrast may be very closed to 0, which is much lower than bad continuation). In order to promote the generation of plausible tokens, we truncate the vocabulary:\n\\[V_{head}(Y_{<t}) = \\{y_t \\in V : \\po(Y_t|v, x, Y_{<t}) \\geq \\beta \\max_{w}po(w|v, x, y_{<t})\\},\\Plqcd(Y_t|x_n, x_l, v) = 0, if y_t \\notin V_{head}(Y_{<t}),\\]\nwhere \\(V\\) is the output vocabulary of LVLMs and \\(\\beta\\) is a hyperparameters in [0, 1] for controlling the truncation of the next token distribution. Larger \\(\\beta\\) entails more aggressive truncation and smaller \\(\\beta\\) allows tokens of lower probabilities to be generated. Combining the above two formulations, we can obtain the full formulation:\n\\[Y_t \\sim softmax[(1 + \\alpha)logit_{\\theta} (y|x_n, V, Y_{<t}) - \\alpha logit_{\\theta} (y x_l, v, y_{<t})], subject to y_t \\in V_{head}(Y_{<t}).\\]\nIt is worth mentioning that the decoding method presented in this paper can be directly applied to regular prompts, not just limited to leading prompts that induce sycophancy. Considering that regular prompts are often neutral by nature, the distribution obtained from the transformed neutral prompts will be consistent with or close to the original regular prompts, thus not interfering with the decoding result for the original regular query."}, {"title": "Experiments", "content": "Our experiments answer three research questions: Q1: To what extent are the current LVLM's capabilities interfered by sycophancy? Q2: Do different models exhibit same behavior under the influence of sycophancy? Q3: How effective is the proposed LQCD for mitigating sycophancy? Related code of the experiments can be found in supplementary materials.\nLVLMs. We select five popular LVLMs to perform the experiments, including Qwen-VL (Bai et al. 2023), CogVLM2 (Wang et al. 2023b), InternVL (Chen et al. 2024a), LLaVA-NeXT (Liu et al. 2024a) and mplug-Owl (Ye et al. 2024). These LVLMs vary in parameter sizes from 7B to 34B, and cover different architectures of vision encoders, LLM module and modality fusion architecture, as shown in Table 2. The chosen models are diverse and represent a broad spectrum of architectures. For all five models, we conducted evaluations on the aforementioned five extended datasets.\nExperimental Settings. Throughout our experiments, we set \\(\\alpha = 0.1, \\beta = 0.1\\) for all baselines and datasets. The hyperparameter settings used in these experiments are empirical selected. For a consistent comparison, our baseline decoding strategy follow standard protocol (num_beams is set to 1 and do_sample is set to True)."}, {"title": "Results and Analysis", "content": "Q1:To what extent are the current LVLM's capabilities interference by sycophancy? As shown in Figure 4 and left of Figure 5, the leading queries in both POPE and AM-BER severely damage the performance of all LVLMs. The accuracy of all models on the POPE dataset decreased by 12% to 42%, while the F1 score showed a more significant decline, ranging from 15% to 88%. The results indicate that all models exhibited varying degrees of sycophancy, leading to performance degradation and exacerbating the issue of hallucinations. We also evaluated sycophancy on datasets related to Real-world Understanding and Science Knowledge on RealworldQA and ScienceQA. As shown in middle of Figure 5, almost all model performance deteriorate significantly on both datasets. An exception is CogVLM on ScienceQA, which performance does not shrink but even improved slightly. Similar experimental results can be observed on MM-Vet which can measure different integrated capabilities. As shown in right of Figure 5, almost all models have a performance drops on all six capabilities. Except that CogVLM did not exhibit sycophancy effects in the Knowledge domain, which is consistent with our observations in ScienceQA. And LLaVA does not influenced by sycophancy in Math ability either. In summary, sycophancy is common in existing LVLMs, making models easily influenced by deceptive prompts in various capabilities, which alters their originally correct judgments. Specifically, LLaVA and InternVL exhibit slightly better resistance to sycophancy, with less performance degradation compared to Qwen and mPLUG-Owl. It may be because these two models have larger parameter sizes, making them relatively less susceptible to sycophancy. Additionally, CogVLM did not exhibit sycophancy effects in the Knowledge domain in both ScienceQA and MM-Vet. We believe this is due to CogVLM's use of LLaMA-3-8B, which conduct specialized adversarial training for knowledge-related content.\nQ2: Do different models exhibit same behavior under the influence of sycophancy? In this section, we conduct a more detailed analysis of sycophancy based on dissecting the flipped predictions. By comparing the original results from neutral queries with those from the leading queries, for a given sample in the dataset, if sycophancy occurs, it can result in four possible outcomes: true positive changes to false negative (TP2FN), true negative changes to false positive (TN2FP), false negative changes to true positive (FN2TP), and false positive changes to true negative (FP2TN). Based on this, we define the following metrics:\n1. Consistency Transformation Rate (CTR). This metric measures the consistency of the model's predictions between the original and leading queries. A higher CTR indicates that the model's performance is less stable and sycophancy is more severe. N refers to total sample size of the dataset.\n\\[CTR = \\frac{TP2FN + FP2TN + TN2FP + FN2TP}{N}\\]\n2. Error Introduction Rate (EIR). This metric measures the proportion of new errors introduced by the model under the leading query settings. A higher EIR indicates that the model is more prone to making incorrect predictions when influenced by sycophancy.\n\\[EIR = \\frac{TP2FN + TN2FP}{TP + TN}\\]\n3. Error Correction Rate (ECR). This metric measures the proportion of incorrect predictions that the model corrects under the leading query settings, i.e. sycophancy accidentally leading to correct predictions. This could be due to intentional adversarial training on leading questions during the model's training phase. Higher values essentially indicate the model's instability.\n\\[ECR = \\frac{FP2TN + FN2TP}{FP + FN}\\]\n4. Prediction Imbalance Rate (PIR). This metric measures whether the shifts in answers from Yes to No and No to Yes under the influence of sycophancy are balanced. When PIR deviates from 0.5, it suggests that the prediction results are imbalanced and the model's performance exhibits bias.\n\\[PIR = \\frac{FP2TN + TP2FN}{FP2TN + TP2FN + FN2TP + TN2FP}\\]\nThe results of evaluation are shown in Table 3. First, From the CTR and EIR metric, it can be seen that different models exhibit varying degrees of sensitivity to sycophancy,"}, {"title": "Q3: How effective is the proposed LQCD for mitigating sycophancy?", "content": "As illustrated in Figure 3, when asked, \"What color is the traffic light in this scene?\" the probability of the correct token \"green\" significantly decreases under the influence of a leading question, leading to the erroneous output of the token \"red\" and causing a hallucination. By applying contrastive decoding of the leading question and the neutral question, we obtain a calibrated probability, which allows the model to correctly output the intended result. Additional examples can be found in the supplementary materials. We compared our proposed LQCD with several other commonly used techniques. First is Chain-of-Thought (CoT) (Wei et al. 2022), a method that involves generating intermediate reasoning steps to enhance model performance. The second is to use more detailed and specific guiding prompts (Qian et al. 2024). We also evaluated Volcano, a self-feedback-based hallucination mitigation method (Lee et al. 2023). Due to the potential for errors in the generated process of reverting leading queries to neutral queries, we also conducted Oracle experiments which directly used the original neutral queries and leading queries for contrastive decoding, representing the upper limit of our method's performance. As shown in Table 4, the experimental results demonstrate that LQCD method is highly effective in addressing the sycophancy problem. Whether in the oracle setting or the generated setting of LQCD, the performance can approach or even exceed that of the model in the neutral setting. It can also be observed that, in In RealworldQA and ScienceQA, more complex queries impose higher demands on prompt engineering, resulting in a significant performance gap between the Oracle results and those based on prompt engineering. More cases and analysis can be found in supplementary material. Additionally, the experiments reveal the deficiency of common techniques oriented for hallucination in addressing sycophancy. Some test reveal that, they may even exacerbate the sycophancy phenomenon. For POPE dataset, compared to the leading query, CoT, detailed prompt, and Volcano methods do not significantly mitigate the model's sycophancy, with CoT only slightly enhancing the model's accuracy and F\u2081 score, but not substantially. Moreover, different methods exhibit significant performance variations across different models, resulting in large performance variance. The proposed method effectively mitigates the sycophancy phenomenon, bringing the performance in the leading query setting close to that in the neutral setting."}, {"title": "Robustness Analysis and Ablation Experiments", "content": "We conduct robustness analysis and ablation experiments to validate two aspects. First, we analyze whether LQCD causes significant performance degradation when addressing neutral problems. As shown in Figure 6, by comparing the results of \"Neutral\" and \"Neutral+LQCD\", it can be observed that using LQCD for neutral queries does not result in a performance decline. This aligns with the theoretical foundation of our method and demonstrates its generalizability, proving it to be effective beyond merely addressing sycophancy issues. Second, to ensure that the LQCD is the key factor in performance enhancement, we conduct experiments by removing LQCD and solely using prompt engineering to transform the leading query to a neutral query. Comparing the results of \"Leading+PE\" and \"Leading+PE+CD\", the proposed LQCD reduces the variance of results while improving the performance in terms of accuracy and F\u2081 score, demonstrating its effectiveness."}, {"title": "Conclusion", "content": "We systematically analyzed the behavior of various LVLMs when faced with sycophantic prompts in a range of perception and reasoning challenges, highlighting the severe inadequacy of these models in mitigating sycophantic hallucinations. In response, we introduced Leading Query Contrastive Decoding (LQCD), a training-free decoding method that effectively reduces sycophancy by adjusting the models' reliance on language priors corresponding to the leading cues. We conduct extensive experiments to validate the effectiveness of LQCD, showcasing its strength over existing relevant techniques. We hope our work provide a solid foundation for evaluating and mitigating sycophancy in LVLMs."}]}