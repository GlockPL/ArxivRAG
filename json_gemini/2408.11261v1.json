{"title": "Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models", "authors": ["Yunpu Zhao", "Rui Zhang", "Junbin Xiao", "Changxin Ke", "Ruibo Hou", "Yifan Hao", "Qi Guo", "Yunji Chen"], "abstract": "Large Vision-Language Models (LVLMs) have shown significant capability in vision-language understanding. However, one critical issue that persists in these models is sycophancy, which means models are unduly influenced by leading or deceptive prompts, resulting in biased outputs and hallucinations. Despite the progress in LVLMs, evaluating and mitigating sycophancy is yet much under-explored. In this work, we fill this gap by systematically analyzing sycophancy on various VL benchmarks with curated leading queries and further proposing a text contrastive decoding method for mitigation. While the specific sycophantic behavior varies significantly among models, our analysis reveals the severe deficiency of all LVLMs in resilience of sycophancy across various tasks. For improvement, we propose Leading Query Contrastive Decoding (LQCD), a model-agnostic method focusing on calibrating the LVLMs' over-reliance on leading cues by identifying and suppressing the probabilities of sycophancy tokens at the decoding stage. Extensive experiments show that LQCD effectively mitigate sycophancy, outperforming both prompt engineering methods and common methods for hallucination mitigation. We further demonstrate that LQCD does not hurt but even slightly improves LVLMs' responses to neutral queries, suggesting it being a more effective strategy for general-purpose decoding but not limited to sycophancy.", "sections": [{"title": "Introduction", "content": "Large Vision-Language Models (LVLMs) have attracted significant attention for their ability to understand and generate multimodal data by integrating textual and visual information. These models have been applied in diverse domains, showcasing impressive capabilities that push the boundaries of artificial intelligence (Zhang et al. 2024; Cui et al. 2024). Despite their success, LVLMs are widely criticized for the hallucination problem (Liu et al. 2024b; Xiao et al. 2024), of which sycophancy (Sharma et al. 2024; Perez et al. 2023) is a key factor but has rarely been analyzed. Sycophancy refers to the phenomenon where a model tends to agree with the statement in input queries in unwanted ways (Sharma et al. 2024; Perez et al. 2023). It is a sort of model susceptibility, meaning that models are unduly influenced by text interference or deceptive prompts. As such, sycophancy often results in performance decreasing, bias and hallucination (Cui et al. 2023; Qian et al. 2024), as shown in Figure 1. In practice, sycophancy can lead to AI violating human ethics under deliberate inducement, causing biased and discriminatory output and severely impact the widespread real-world application of LVLMs. Thus, the need to evaluate and mitigate sycophancy in LVLMs is paramount. While existing benchmarks focus on evaluating various types of hallucination of LVLMs, they typically employ neutral queries, and thus fail to assess the model's robustness against sycophancy (Li et al. 2023b, 2024; Yu et al. 2024). Although some research has examined sycophancy in LLMs and find that finetuning and alignment techniques based on human feedback likely bring sycophancy in models (Sharma et al. 2024; Cui et al. 2023; Qian et al. 2024), studies about sycophancy in LVLMs are missing. Sycophancy in LVLMs is more complex than in LLMs due to the integration of visual knowledge. The additional visual and cross-modal fusion module often increase the instability of the model's output and can also interfere with the results generated by the language module. For instance, the model's attention to visual information often differs from the attention to texts, which adds the complexity of analyzing sycophantic behavior. Additionally, there is a notable absence of work for mitigating sycophancy in LVLMs. This work thus conducts the first systematic analysis to"}, {"title": "Related Work", "content": "Inspired by the significant achievements of LLMs, researchers have developed a series of LVLMs by integrating them with visual pretraining models. A typical architecture consists of a visual encoder, an LLM, and a modality alignment module. The visual encoder often employs popular visual pretraining models (Han et al. 2022), while the LLM can utilize various popular models like Vicuna and LLaMA. The modality alignment module is finetuned using paired image-text data, endowing the model with visual dialogue capabilities. Some of the representative models that have garnered widespread attention include Qwen-VL, LLaVA, and CogVLM (Wang et al. 2023b; Liu et al. 2024a; Bai et al. 2023), and these are also baselines in our experiments. Due to issues related to network architecture, the quality of training data, and inherent problems within LLMs themselves, LVLMs face numerous challenges."}, {"title": "Hallucination of LVLMs and Sycophancy", "content": "Among the many flaws of LVLMs, the most prominent is the hallucination problem, which also exists in LLMs and is particularly severe in LVLMs. Existing research has analyzed the causes of hallucination (Liu et al. 2024b). Numerous new benchmarks are also proposed to evaluate the hallucination in LVLMs (Li et al. 2023b; Wu et al. 2024; Guan et al. 2024). Different types of hallucination exhibit different manifestations and require various mitigation strategies. Currently, research on alleviating hallucination in LVLMs focuses on three primary approaches: the first is prompt engineering (M\u00fcndler et al. 2024; Lee et al. 2023), the second is enhancing the model itself (Liu et al. 2023a; Jiang et al. 2023), and the third is post-processing of the results. This post-processing could involve new decoding strategies or using external tools like GroundingDINO (Liu et al. 2023b) to detect and eliminate hallucinations (Leng et al. 2024; Huang et al. 2024; Chen et al. 2024b). Sycophancy is a significant cause of model hallucination. Some studies have reported that leading queries or prompts that are biased can interfere with the responses of LVLMs, indicating that the models often do not verify the prior statements in the queries and tend to agree with users (Cui et al. 2023; Qian et al. 2024). However, evaluating and mitigating for sycophancy are rarely studied. It remains unknown whether the aforementioned methods that have shown effective for alleviating object/attribute hallucinations, are capable of solving hallucinations in the context of leading queries. This gap thus underscores the need on related investigation, which is also the goal of this work."}, {"title": "Contrastive Decoding", "content": "The main idea of contrastive decoding is a commonly employed in image and text generation. For instance, in classifier-free diffusion models, a contrastive objective is used to estimate diffusion models (Ho and Salimans 2022). In text generation, there is often a significant performance difference between the two models being compared, such as an expert LLM and an amateur LLM (Li et al. 2023a). This"}, {"title": "Dataset Construction", "content": "In this section, we construct specifically datasets to comprehensively and systematically evaluate sycophancy phenomenon in LVLMs. We select five popular Visual Question Answering (VQA) datasets, and then edit and extend the question text to inject leading and deceptive information for sycophancy evaluation."}, {"title": "Data Source", "content": "We select and extend five datasets to evaluate sycophancy in LVLMs, as shown in Table 1. Considering that sycophancy is an factor of hallucination, we select two popular datasets POPE (Li et al. 2023b) and AMBER (Wang et al. 2023a) which are proposed to evaluate hallucination. Meanwhile, we also select three VQA datasets RealworldQA (x.ai 2024), ScienceQA (Lu et al. 2022) and MM-Vet (Yu et al. 2024) to evaluate the effect of sycophancy on commonsense comprehension and complex multimodal tasks. POPE (Li et al. 2023b) is designed to assess object hallucination in LVLMs through yes-or-no questions regarding the presence of objects in images. POPE is divided into three settings: random, popular, and adversarial, indicating different methods of sampling hallucination objects. AM-BER (Wang et al. 2023a) is a comprehensive benchmark designed to evaluate the hallucination performance of LVLMs. In our experiments, AMBER is mainly used for analyzing the attribute hallucination and relation hallucination. Real-worldQA (x.ai 2024) is designed to evaluate the real-world spatial understanding capabilities of large multimodal models. ScienceQA (Lu et al. 2022) is designed for evaluating multimodal reasoning in science question answering. MM-Vet (Yu et al. 2024) is designed to assess the integrated capabilities of LVLMs based on 16 complex multimodal tasks, focusing on six core vision-language capabilities: recognition, optical character recognition (OCR), knowledge, language generation, spatial awareness, and math."}, {"title": "Leading Query Generation", "content": "Queries in these above-mentioned datasets are neutral without explicit misleading cues. Thus, the neutral queries are incapable of assessing"}, {"title": "Leading Query Contrastive Decoding", "content": "In this section, we propose a method called Leading Query Contrastive Decoding to alleviate sycophancy. Sycophancy can be problematic as it biases the output, potentially leading to less accurate or relevant results. Leading query can aggravate sycophancy since a leading query essentially amplifies language priors in a certain fixed direction, thereby reducing the model's sensitivity to visual perception. Thus,"}, {"title": "Contrastive Objective", "content": "Specifically, given a textual leading query $x_{\\iota}$ and a visual input $v$, we firstly generate transformed neutral query $x_n$ corresponding to $x_{\\iota}$ by using LLM. Then LVLM model generates two distinct output distribution $logit_{\\theta}(y|x_{\\iota}, v)$ conditioned on the leading query $x_{\\iota}$ and $logit_{\\theta}(y|x_n, v)$ conditioned on the transformed neutral query $X_n$. The contrastive probability distribution $P_{lqed}$ of LQCD is computed by exploiting the differences between the two obtained distributions, formulated as:\n\n$P_{lqcd} (y|x_n, x_\\iota, v) = softmax[(1 + \\alpha)logit_{\\theta} (y/x_n, v) - \\alpha logit_{\\theta} (y|x_{\\iota}, v)]$\n\nwhere $\\alpha$ is a weight hyperparameter, $y$ is the resulting token distribution, $\\theta$ denotes the model parameters. Larger $\\alpha$ value signifies a greater amplification of disparity between the two distributions (where $\\alpha = 0$ corresponds to regular decoding). The objective enhance token probability favored by the neutral query output and suppress token probability favored by leading query output."}, {"title": "Adaptive Plausibility Constraint", "content": "However, leading query output are not always mistaken: except the hallucination caused by sycophancy, the output still capture many simple aspects of grammar and common sense. Thus, penalizing all tokens from leading query output indiscriminately would penalize these simple aspects that are correct (For example, due to tokenization, the probability of the word \"is\" in \"The color is red\" is close to 1 under both neutral and leading query output, but the contrast may be very closed to 0, which is much lower than bad continuation). In order to promote the generation of plausible tokens,"}, {"title": "Experiments", "content": "Our experiments answer three research questions: Q1: To what extent are the current LVLM's capabilities interfered by sycophancy? Q2: Do different models exhibit same behavior under the influence of sycophancy? Q3: How effective is the proposed LQCD for mitigating sycophancy? Related code of the experiments can be found in supplementary materials."}, {"title": "LVLMs", "content": "We select five popular LVLMs to perform the experiments, including Qwen-VL (Bai et al. 2023), CogVLM2 (Wang et al. 2023b), InternVL (Chen et al. 2024a), LLaVA-NeXT (Liu et al. 2024a) and mplug-Owl (Ye et al. 2024). These LVLMs vary in parameter sizes from 7B to 34B, and cover different architectures of vision encoders, LLM module and modality fusion architecture, as shown in Table 2. The chosen models are diverse and represent a broad spectrum of architectures. For all five models, we conducted evaluations on the aforementioned five extended datasets."}, {"title": "Experimental Settings", "content": "Throughout our experiments, we set $\\alpha$ = 0.1, $\\beta$ = 0.1 for all baselines and datasets. The hyperparameter settings used in these experiments are empirical selected. For a consistent comparison, our baseline decoding strategy follow standard protocol (num_beams is set to 1 and do_sample is set to True)."}, {"title": "Results and Analysis", "content": "As shown in Figure 4 and left of Figure 5, the leading queries in both POPE and AM-BER severely damage the performance of all LVLMs. The accuracy of all models on the POPE dataset decreased by 12% to 42%, while the F1 score showed a more significant decline, ranging from 15% to 88%. The results indicate that all models exhibited varying degrees of sycophancy, leading to performance degradation and exacerbating the issue of hallucinations. We also evaluated sycophancy on datasets related to Real-world Understanding and Science Knowledge on RealworldQA and ScienceQA. As shown in middle of Figure 5, almost all model performance deteriorate significantly on both datasets. An exception is CogVLM on ScienceQA, which performance does not shrink but even improved slightly. Similar experimental results can be observed on MM-Vet which can measure different integrated capabilities. As shown in right of Figure 5, almost all models have a performance drops on all six capabilities. Except that CogVLM did not exhibit sycophancy effects in the Knowledge domain, which is consistent with our observations in ScienceQA. And LLaVA does not influenced by sycophancy in Math ability either. In summary, sycophancy is common in existing LVLMs, making models easily influenced by deceptive prompts in various capabilities, which alters their originally correct judgments. Specifically, LLaVA and InternVL exhibit slightly better resistance to sycophancy, with less performance degradation compared to Qwen and mPLUG-Owl. It may be because these two models have larger parameter sizes, making them relatively less susceptible to sycophancy. Additionally, CogVLM did not exhibit sycophancy effects in the Knowledge domain in both ScienceQA and MM-Vet. We believe this is due to CogVLM's use of LLaMA-3-8B, which conduct specialized adversarial training for knowledge-related content."}, {"title": "Discussion", "content": "with larger models being less sensitive to sycophancy. Second, it can be observed that the PIR values significantly vary among different models. Among them, Qwen-VL is easily guided to answer No, while mPLUG-Owl2.1, on the contrary, is more susceptible to being influenced to answer Yes, whereas InternVL-1.5 and LLaVA-NeXT are relatively balanced. Models with relatively balanced PIR may have been intentionally trained with a balanced approach to positive and negative samples. Third, InternVL-1.5 and LLaVA-NeXT exhibited high ECR values. ECR indicates a phenomenon opposite to sycophancy where the model originally answered incorrectly, but chose the opposite of the leading query after introducing a leading query, and happened to answer correctly. We suspect this is due to the intentional introduction of adversarial samples during the training phase of InternVL-1.5 and LLaVA-NeXT. In contrast, CogVLM2 achieved the lowest ECR value, indicating that it has a high level of confidence in its own correct answers, suggesting that there may have been special design during its training. On this basis, we performed sentiment analysis on the leading queries. We divided the leading query texts into two groups based on whether sycophancy occurred in each sample. A ROBERTa-based language model was used to predict the sentiment intensity of the leading texts, yielding corresponding probability values. We then applied the Mann-Whitney U Test to these two sets of values to conduct hypothesis testing, determining whether there is a significant difference in sentiment between the leading queries that led to sycophancy and those that did not. The p-values from the Mann-Whitney U Test are reported in the table, with a p-value of less than 0.05 indicating a significant difference. It can be observed that Qwen, CogVLM, and mPLUG-Owl2 show significant differences between the two groups, indicating that for these models, stronger sentiment in the tone of the leading queries makes them more prone to sycophancy. These models may be developed on biased data, leading to significant deviations in handling leading queries with different sentiments. We suspect that some models might prioritize accuracy while neglecting other issues, leading to biases in sycophancy."}, {"title": "Proposed LQCD for Mitigating Sycophancy", "content": "As illustrated in Figure 3, when asked, \"What color is the traffic light in this scene?\" the probability of the correct token \"green\" significantly decreases under the influence of a leading question, leading to the erroneous output of the token \"red\" and causing a hallucination. By applying contrastive decoding of the leading question and the neutral question, we obtain a calibrated probability, which allows the model to correctly output the intended result. Additional examples can be found in the supplementary materials. We compared our proposed LQCD with several other commonly used techniques. First is Chain-of-Thought (CoT) (Wei et al. 2022), a method that involves generating intermediate reasoning steps to enhance model performance. The second is to use more detailed and specific guiding prompts (Qian et al. 2024). We also evaluated Volcano, a self-feedback-based hallucination mitigation method (Lee et al. 2023). Due to the potential for errors in the generated process of reverting leading queries to neutral queries, we also conducted Oracle experiments which directly used the original neutral queries and leading queries for contrastive decoding, representing the upper limit of our method's performance. As shown in Table 4, the experimental results demonstrate that LQCD method is highly effective in addressing the sycophancy problem. Whether in the oracle setting or the generated setting of LQCD, the performance can approach or even exceed that of the model in the neutral setting. It can also be observed that, in In RealworldQA and ScienceQA, more complex queries impose higher demands on prompt engineering, resulting in a significant performance gap between the Oracle results and those based on prompt engineering. More cases and analysis can be found in supplementary material. Additionally, the experiments reveal the deficiency of common techniques oriented for hallucination in addressing sycophancy. Some test reveal that, they may even exacerbate the sycophancy phenomenon. For POPE dataset, compared to the leading query, CoT, detailed prompt, and Volcano methods do not significantly mitigate the model's sycophancy, with CoT only slightly enhancing the model's accuracy and F\u2081 score, but not substantially. Moreover, different methods exhibit significant performance variations across different models, resulting in large performance variance. The proposed method effectively mitigates the sycophancy phenomenon, bringing the performance in the leading query setting close to that in the neutral setting."}, {"title": "Robustness Analysis and Ablation Experiments", "content": "We conduct robustness analysis and ablation experiments to validate two aspects. First, we analyze whether LQCD causes significant performance degradation when addressing neutral problems. As shown in Figure 6, by comparing the"}, {"title": "Conclusion", "content": "We systematically analyzed the behavior of various LVLMs when faced with sycophantic prompts in a range of perception and reasoning challenges, highlighting the severe inadequacy of these models in mitigating sycophantic hallucinations. In response, we introduced Leading Query Contrastive Decoding (LQCD), a training-free decoding method that effectively reduces sycophancy by adjusting the models' reliance on language priors corresponding to the leading cues. We conduct extensive experiments to validate the effectiveness of LQCD, showcasing its strength over existing relevant techniques. We hope our work provide a solid foundation for evaluating and mitigating sycophancy in LVLMs."}]}