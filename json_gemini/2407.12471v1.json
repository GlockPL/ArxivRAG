{"title": "CHARACTERIZATION OF POLITICAL POLARIZED USERS ATTACKED BY LANGUAGE TOXICITY ON TWITTER", "authors": ["Wentao Xu"], "abstract": "Understanding the dynamics of language toxicity on social media is important for us to investigate the propagation of misinformation and the development of echo chambers for political scenarios such as U.S. presidential elections. Recent research has used large-scale data to investigate the dynamics across social media platforms. However, research on the toxicity dynamics is not enough. This study aims to provide a first exploration of the potential language toxicity flow among Left, Right and Center users. Specifically, we aim to examine whether Left users were easier to be attacked by language toxicity. In this study, more than 500M Twitter posts were examined. It was discovered that Left users received much more toxic replies than Right and Center users.", "sections": [{"title": "Introduction", "content": "Social media has become an indispensable daily element of contemporary social life [1]. When social media platforms bring people freedom of communication, studies identified language toxicity emerging across platforms, such as Twitter (Now, X) [2, 3], Facebook [4, 5], Reddit [6], YouTube [7], Telegram [8], and Whisper [9]. These studies have identified various forms of toxic content, including violence, obscenity, threats, insults, and abusive language. Toxic language in online social networks is prevalent between users with no connection than between mutual friends, and mildly offensive terms are used more frequently to express hostility between these two groups[10]. The nature and extent of toxic language can vary by platform. For instance, Reddit has been found to contain a higher frequency of posts with insults, identity attacks, threats of violence, and sexual harassment[11]. Additionally, the research indicates that while most studies on offensive language detection have focused on English, the toxic contents have been identified in other languages, such as Greek and Indonesian, as well [12, 13]. In addition, the presence of toxic language on Twitter can have significant negative impacts on individuals and communities, even affecting mental health[14].\nIn political scenarios, online conversations during U.S. presidential elections can indeed exhibit toxicity [15]. While social media platforms are praised for enhancing democratic discussions, the presence of social bots can distort political discourse, potentially influencing public opinion and election integrity negatively[16, 17]. The behaviour of social bots aggravates the propagation of toxic content. The toxicity in online political talk is often linked to incivility, challenging the perception that it is beneficial for the elections. Moreover, the study of online chatter surrounding elections is crucial for ensuring evidence-based political discourse and free and fair elections [18]. Therefore, while online conversations can provide a platform for political discussions, the toxicity and manipulation through bots underscores the importance of monitoring and studying these interactions to safeguard the election process.\nHowever, the political process is severely affected by polarization. Polarization is popular on Twitter, as the platform serves as a significant space for political discourse, which can influence public opinion and democratic processes. Studies have shown that Twitter can both facilitate cross-ideological exchanges and contribute to the clustering of users around shared political views, potentially reinforcing partisan loyalties and contributing to polarization [19].\nThe impact of Twitter on political polarization is also significant in fragmented political systems, where the platform's role in shaping communication among political entities can affect collaboration between parties and the overall political"}, {"title": "Data & Methods", "content": "It is well known that the COVID-19 pandemic is a worldwide healthcare crisis, during which political polarization was intensified [30, 31, 32]. Such a catastrophic global situation provides a time window to examine the association between political polarization and online language toxicity.\nIn this study, 542,212,429 English tweets were collected from February 20 2020 to May 30 2022 by querying COVID- 19-related keywords: \u201ccorona virus\u201d, \u201ccoronavirus", "covid19": "2019-nCoV", "SARS-CoV-2\", \\\"wuhanpneumonia": "sing the Twitter Search API. A total of 25,370,268 replies of English tweets were used for this study."}, {"title": "User annotation", "content": "A politically-leaning URL domain list of news websites was then obtained by requesting from Allsides\u00b9 for academic research purposes, which contains 160 Left and Lean Left URLs, 98 Right and Lean Right URLs and 180 Center URLs. Based on the list, each reply was labelled as Right if its domain of the Twitter URL object was identified in the Right or Lean Right domain list; the other replies were labelled as Left and Center, accordingly.\nTo examine the degree to which a user engages with labelled replies, we categorized users according to their replies' domain labels. For example, the Right user category includes users whose reply URL objects contain the Right domains, exclusively. It happens that a reply does not contain any URLs. Please, keep in mind that this study only looked at replies that met two criteria:\n\u2022 The user to whom were replied (\u201cin_reply_to_screen_name\u201d in the standard Twitter object \u00b2) is not a \u201cNull\u201d value.\n\u2022 The reply contains at least, one domain in the Twitter URL object.\nMeanwhile, the study further considered the frequency with which each user was replied to in each politically-leaning category. For example, if the Left domains occurred in a replied-to user's reply URL object three times without Center"}, {"title": "Toxicity Calculation", "content": "The Perspective API \u00b3 is considered suitable for toxicity calculation due to its machine learning-based approach to detecting and moderating toxic content on social media platforms [33, 34, 35, 36, 27, 7, 28]. It has been adopted for content moderation, monitoring, and research purposes. It aligns well with human ratings of toxicity [26] and disrespectfulness, especially for highly toxic comments [37], indicating the capability of language toxicity measurement for Perspective API is robust.\nFor the text input into the Perspective API, a probability score scaling within [0, 1] is calculated. The higher the score is, the more toxic the input text is. Some research uses a threshold for classifying \u201ctoxic\u201d and \u201cnontoxic\u201d texts. Here, this strategy was not adopted, as I need to characterise the toxicity of all users. To measure the toxicity of each user, the replied texts for each user were aggregated, and then sent to Perspective API. Since each category of users possesses various statistical indicators for toxicity, here, the analysis for maximum and median toxicity scores of Left, Right and Center was reported."}, {"title": "Results", "content": "The overall negative correlation between maximum toxicity and the replied times was identified in this study(Figure.1)."}, {"title": "The Left received much more toxic replies.", "content": "The maximum toxicity is the highest value of language toxicity of the category with specific replied times. Figure 1 illustrates the maximum toxicities of each category replied at different times, indicating that more-replied-to users were less likely to receive replies with high toxicity. The Right and Center categories replied-to users shared a similar maximum toxicity distribution (Kolmogorov-Smirnov test, p > 0.05), while the Left category showed a different distribution (p < 0.05). However, the statistical difference does not change the overall trend of the three categories.\nIn general, more frequently replied-to users shared lower maximum toxicities, regardless of user category. The Left category differs from the others, possibly due to the higher toxicity values of several outliers. For instance, some Left category outliers (indicated by arrows in Figure 1) shared larger toxicities and some of them even reached more than 0.8. The outliers could be top toxic repliers. By contrast, the Right category users' maximum toxicities were less than 0.4, when they were replied more than, approximately 1,000 times. This reveals that the maximum toxicities of the Left category users are significantly higher than those of the other two categories (Mann-Whitney U test, p < 0.005)."}, {"title": "The Left and Center outliers received much more toxic replies.", "content": "The median can be used to represent the centre tendency of a dataset. In contrast to the maximum scenario, the level of median toxicity did not exhibit a negative correlation with replied times.\nMost of the median toxicity values were concentrated between around 0.05 to 0.4. This overall tendency showed that the toxicity of replies was less aggressive, but fluctuated as the replied times increased. Specifically, when we looked at the Right category users, the median toxicities were below 0.5, but the outlier values for Left and Center users reached over 0.7. No statistical significance was identified across the Left, Right, and Center, suggesting the three categories shared a similar distribution for median toxicity (Figure 3), and no significant median toxicity group was identified out of the three categories (Figure 4)."}, {"title": "Discussion", "content": "This study shows that Left users could receive more toxic replies than Right and Center users. This pattern of toxicity propagation is important for understanding misinformation propagation and echo chamber development, as toxicity in online interactions can lead to a decrease in user activity, ultimately impacting the collaborative nature of platforms [38]. Previous research confirmed that the left group was more distant from the neutral group than the right group [27]. However, this study found that Left users were much closer to Right users than the Center user, in terms of maximum toxicities. This \u201ctoxicity distance\" might suggest that right and left users were sending toxicities to each other, but Left users received much more. Although there was no significant difference in the language toxicity across the replied-to users of the Left, Right, and Center categories, the replied users targeted by toxic repliers in each category cannot be neglectable, especially the Left users.\nWhat precautions are necessary to take for protecting users from language toxicity attacks, especially during political discussions, such as U.S. presidential elections should be carefully considered. When users are engaging the Left, it is suggested to pay attention to the toxic comments and replies, which might further pollute the SNS ecosystem and make users more emotional. Future work would be finding out the dynamics of interaction and engagement dynamics for the Left, Right and Center. In addition, more intelligent tools need to be proposed to combat the aggression of the toxic language to keep our SNS ecosystem healthier. This study has implications for other platforms, such as Facebook and Reddit."}]}