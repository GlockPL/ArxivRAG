{"title": "CHARACTERIZATION OF POLITICAL POLARIZED USERS\nATTACKED BY LANGUAGE TOXICITY ON TWITTER", "authors": ["Wentao Xu"], "abstract": "Understanding the dynamics of language toxicity on social media is important for us to investigate\nthe propagation of misinformation and the development of echo chambers for political scenarios such\nas U.S. presidential elections. Recent research has used large-scale data to investigate the dynamics\nacross social media platforms. However, research on the toxicity dynamics is not enough. This\nstudy aims to provide a first exploration of the potential language toxicity flow among Left, Right\nand Center users. Specifically, we aim to examine whether Left users were easier to be attacked by\nlanguage toxicity. In this study, more than 500M Twitter posts were examined. It was discovered that\nLeft users received much more toxic replies than Right and Center users.", "sections": [{"title": "1 Introduction", "content": "Social media has become an indispensable daily element of contemporary social life [1]. When social media platforms\nbring people freedom of communication, studies identified language toxicity emerging across platforms, such as Twitter\n(Now, X) [2, 3], Facebook [4, 5], Reddit [6], YouTube [7], Telegram [8], and Whisper [9]. These studies have identified\nvarious forms of toxic content, including violence, obscenity, threats, insults, and abusive language. Toxic language\nin online social networks is prevalent between users with no connection than between mutual friends, and mildly\noffensive terms are used more frequently to express hostility between these two groups[10]. The nature and extent of\ntoxic language can vary by platform. For instance, Reddit has been found to contain a higher frequency of posts with\ninsults, identity attacks, threats of violence, and sexual harassment[11]. Additionally, the research indicates that while\nmost studies on offensive language detection have focused on English, the toxic contents have been identified in other\nlanguages, such as Greek and Indonesian, as well [12, 13]. In addition, the presence of toxic language on Twitter can\nhave significant negative impacts on individuals and communities, even affecting mental health[14].\nIn political scenarios, online conversations during U.S. presidential elections can indeed exhibit toxicity [15]. While\nsocial media platforms are praised for enhancing democratic discussions, the presence of social bots can distort political\ndiscourse, potentially influencing public opinion and election integrity negatively[16, 17]. The behaviour of social bots\naggravates the propagation of toxic content. The toxicity in online political talk is often linked to incivility, challenging\nthe perception that it is beneficial for the elections. Moreover, the study of online chatter surrounding elections is crucial\nfor ensuring evidence-based political discourse and free and fair elections [18]. Therefore, while online conversations\ncan provide a platform for political discussions, the toxicity and manipulation through bots underscores the importance\nof monitoring and studying these interactions to safeguard the election process.\nHowever, the political process is severely affected by polarization. Polarization is popular on Twitter, as the platform\nserves as a significant space for political discourse, which can influence public opinion and democratic processes.\nStudies have shown that Twitter can both facilitate cross-ideological exchanges and contribute to the clustering of users\naround shared political views, potentially reinforcing partisan loyalties and contributing to polarization [19].\nThe impact of Twitter on political polarization is also significant in fragmented political systems, where the platform's\nrole in shaping communication among political entities can affect collaboration between parties and the overall political"}, {"title": "2 Data & Methods", "content": "2.1 Data\nIt is well known that the COVID-19 pandemic is a worldwide healthcare crisis, during which political polarization\nwas intensified [30, 31, 32]. Such a catastrophic global situation provides a time window to examine the association\nbetween political polarization and online language toxicity.\nIn this study, 542,212,429 English tweets were collected from February 20 2020 to May 30 2022 by querying COVID-\n19-related keywords: \u201ccorona virus\u201d, \u201ccoronavirus", "covid19": "2019-nCoV", "SARS-CoV-2\", \"wuhanpneumonia": "nusing the Twitter Search API. A total of 25,370,268 replies of English tweets were used for this study.\n2.2 User annotation\nA politically-leaning URL domain list of news websites was then obtained by requesting from Allsides\u00b9 for academic\nresearch purposes, which contains 160 Left and Lean Left URLs, 98 Right and Lean Right URLs and 180 Center URLs.\nBased on the list, each reply was labelled as Right if its domain of the Twitter URL object was identified in the Right or\nLean Right domain list; the other replies were labelled as Left and Center, accordingly.\nTo examine the degree to which a user engages with labelled replies, we categorized users according to their replies'\ndomain labels. For example, the Right user category includes users whose reply URL objects contain the Right domains,\nexclusively. It happens that a reply does not contain any URLs. Please, keep in mind that this study only looked at\nreplies that met two criteria:\n\u2022 The user to whom were replied (\u201cin_reply_to_screen_name\u201d in the standard Twitter object 2) is not a \u201cNull\u201d\nvalue.\n\u2022 The reply contains at least, one domain in the Twitter URL object.\nMeanwhile, the study further considered the frequency with which each user was replied to in each politically-leaning\ncategory. For example, if the Left domains occurred in a replied-to user's reply URL object three times without Center"}, {"title": "2.3 Toxicity Calculation", "content": "The Perspective API 3 is considered suitable for toxicity calculation due to its machine learning-based approach to\ndetecting and moderating toxic content on social media platforms [33, 34, 35, 36, 27, 7, 28]. It has been adopted\nfor content moderation, monitoring, and research purposes. It aligns well with human ratings of toxicity [26] and\ndisrespectfulness, especially for highly toxic comments [37], indicating the capability of language toxicity measurement\nfor Perspective API is robust.\nFor the text input into the Perspective API, a probability score scaling within [0, 1] is calculated. The higher the score is,\nthe more toxic the input text is. Some research uses a threshold for classifying \u201ctoxic\u201d and \u201cnontoxic\u201d texts. Here, this\nstrategy was not adopted, as I need to characterise the toxicity of all users. To measure the toxicity of each user, the\nreplied texts for each user were aggregated, and then sent to Perspective API. Since each category of users possesses\nvarious statistical indicators for toxicity, here, the analysis for maximum and median toxicity scores of Left, Right and\nCenter was reported."}, {"title": "3 Results", "content": "3.1 The Left received much more toxic replies.\nThe overall negative correlation between maximum toxicity and the replied times was identified in this study(Figure.1)."}, {"title": "3.2 The Left and Center outliers received much more toxic replies.", "content": "The median can be used to represent the centre tendency of a dataset. In contrast to the maximum scenario, the level of\nmedian toxicity did not exhibit a negative correlation with replied times.\nMost of the median toxicity values were concentrated between around 0.05 to 0.4. This overall tendency showed that\nthe toxicity of replies was less aggressive, but fluctuated as the replied times increased. Specifically, when we looked at\nthe Right category users, the median toxicities were below 0.5, but the outlier values for Left and Center users reached\nover 0.7. No statistical significance was identified across the Left, Right, and Center, suggesting the three categories\nshared a similar distribution for median toxicity (Figure 3), and no significant median toxicity group was identified out\nof the three categories (Figure 4)."}, {"title": "4 Discussion", "content": "This study shows that Left users could receive more toxic replies than Right and Center users. This pattern of toxicity\npropagation is important for understanding misinformation propagation and echo chamber development, as toxicity in\nonline interactions can lead to a decrease in user activity, ultimately impacting the collaborative nature of platforms\n[38]. Previous research confirmed that the left group was more distant from the neutral group than the right group [27].\nHowever, this study found that Left users were much closer to Right users than the Center user, in terms of maximum\ntoxicities. This \u201ctoxicity distance\" might suggest that right and left users were sending toxicities to each other, but Left\nusers received much more. Although there was no significant difference in the language toxicity across the replied-to\nusers of the Left, Right, and Center categories, the replied users targeted by toxic repliers in each category cannot be\nneglectable, especially the Left users.\nWhat precautions are necessary to take for protecting users from language toxicity attacks, especially during political\ndiscussions, such as U.S. presidential elections should be carefully considered. When users are engaging the Left, it is\nsuggested to pay attention to the toxic comments and replies, which might further pollute the SNS ecosystem and make\nusers more emotional. Future work would be finding out the dynamics of interaction and engagement dynamics for the\nLeft, Right and Center. In addition, more intelligent tools need to be proposed to combat the aggression of the toxic\nlanguage to keep our SNS ecosystem healthier. This study has implications for other platforms, such as Facebook and\nReddit."}]}