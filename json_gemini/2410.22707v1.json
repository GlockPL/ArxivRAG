{"title": "Robotic State Recognition with Image-to-Text Retrieval Task of Pre-Trained Vision-Language Model and Black-Box Optimization", "authors": ["Kento Kawaharazuka", "Yoshiki Obinata", "Naoaki Kanazawa", "Kei Okada", "Masayuki Inaba"], "abstract": "State recognition of the environment and objects, such as the open/closed state of doors and the on/off of lights, is indispensable for robots that perform daily life support and security tasks. Until now, state recognition methods have been based on training neural networks from manual annotations, preparing special sensors for the recognition, or manually programming to extract features from point clouds or raw images. In contrast, we propose a robotic state recognition method using a pre-trained vision-language model, which is capable of Image-to-Text Retrieval (ITR) tasks. We prepare several kinds of language prompts in advance, calculate the similarity between these prompts and the current image by ITR, and perform state recognition. By applying the optimal weighting to each prompt using black-box optimization, state recognition can be performed with higher accuracy. Experiments show that this theory enables a variety of state recognitions by simply preparing multiple prompts without retraining neural networks or manual programming. In addition, since only prompts and their weights need to be prepared for each recognizer, there is no need to prepare multiple models, which facilitates resource management. It is possible to recognize the open/closed state of transparent doors, the state of whether water is running or not from a faucet, and even the qualitative state of whether a kitchen is clean or not, which have been challenging so far, through language.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots are expanding their range of activities not only to fixed environments such as factories and laboratories, but also to human living spaces, nursing care facilities, and disaster sites [1]-[3]. For robots that provide daily life support, security, and nursing care, the state recognition of the environment and objects is indispensable. In particular, binary state recognition, e.g. open/closed state of doors and on/off state of lights is most frequently used, which cause the motion switching of robots. So far, this state recognition has been done by human programming to extract features from point clouds or raw images [4], [5], by manually annotating datasets and training neural networks [6], or by preparing a special sensor for the state to be recognized [7]. On the other hand, these methods require programming, annotation, training, or new sensors, which are expensive to construct for various state recognitions.\nTherefore, we propose a method of state recognition using a large-scale vision-language model [8]-[10] that is trained on a large visual and linguistic dataset and is capable of performing a wide variety of tasks without retraining, including Visual Question Answering (VQA) [11], [12], Image-to-Text Retrieval (ITR) [13], [14], etc. By using the spoken language for state recognition, even states that cannot be easily described by a program can be recognized. Although object recognition using vision-language models is common [15], studies extending it to object and envi- ronment state recognition are lacking. Certainly, there are methods that implicitly perform state recognition within the trained reward and policy [16], [17]. However, our aim is to make this process explicit, enabling easy utilization in robot demonstrations. In this study, we perform state recognition utilizing CLIP [13], which is a model capable of ITR. Note that VQA can also be utilized [18], but this takes too much time for inference, so we chose a model capable of ITR. By preparing multiple prompts that represent states of objects and environments in advance, and by calculating their similarity to the current image using CLIP, state recog- nition is possible (Fig. 1). Also, the recognition accuracy can be improved by extracting appropriate prompts from the prepared prompts [19]. Therefore, we prepare a small number of image datasets in advance and optimize the weight of each prompt by using black-box optimization. We also discuss the recognition accuracy according to the different evaluation functions in the black-box optimization. We show that this method is effective in recognizing the open/closed state of transparent doors, whether water is running or not, and even the qualitative state of whether the kitchen is clean or not, which have been challenging without manual"}, {"title": "II. CLIP-BASED ROBOTIC STATE RECOGNITION OPTIMIZED WITH BLACK-BOX OPTIMIZATION", "content": null}, {"title": "A. Pre-Trained Vision-Language Model - CLIP", "content": "We describe the basic usage of CLIP, a pre-trained vision-language model. CLIP is a model that calculates the similarity between an image and predefined texts (called prompts), and can search which prompt is closest to the current image. First, let v be a vector transformed by CLIP from the current image V, and let $P_{1,2,\\ldots,N_p}$ be a vector transformed by CLIP from the set of predefined prompts $P_{1,2,\\ldots,N_p}$ ($N_p$ is the number of prompts). Next, the cosine similarity $a_{1,2,\\ldots,N_p} = v \\cdot p_{1,2,\\ldots,N_p}$ between v and p is computed. Finally, we compute the softmax $s_i$ for $a_i$ ($1 \\leq i \\leq N_p$) and take the most probable prompt $P_i$ as the text that best matches the current image V. This allows us to know, for example, whether the current image is more consistent with an apple or a banana by setting $P_{1,2} = {apple, banana}$. The main concept of this study is to utilize CLIP for robotic state recognition through the spoken language."}, {"title": "B. CLIP-based Robotic State Recognition", "content": "For example, if you want to recognize the open/closed state of a door, you should set $P_{1,2}$ ={\"open door\", \"closed door\"}. Here, we can calculate the softmax $s_i$ and judge that the door is open when $s_1 \\geq s_2$ and closed when $s_1 < s_2$. This is the simplest method, but \"open door\" and \"closed door\" do not necessarily have the similar recognition performance. In that case, for example, $s_1 \\geq s_2$ will always hold regardless of whether the door is open or closed. Therefore, we consider another state recognition method. We prepare a set of prompts $P_{1,2,\\ldots,N_p}$, set the weighted sum $e = \\sum_i w_i a_i$ of the obtained $a_i$ as the evaluation value ($w_i$ is the weight of the prompt $P_i$), and recognize that the door is open if it is above the threshold $C_{thre}$, and that the door is closed if it is below $C_{thre}$. $w_i$ is set to 1 for the prompt indicating that the door is open and -1 for the prompt indicating that the door is closed. Since there is no restriction on how to select prompts in this method, various prompt sets can be considered, and as long as appropriate prompts and threshold values are provided, state recognition is possible and performance can be easily adjusted.\nHere, we consider how to choose the prompt P. In this study, we consider changes in (i) articles, (ii) state"}, {"title": "C. Optimization of CLIP-based Robotic State Recognition Using Black-Box Optimization", "content": "We describe a method to improve the recognition accuracy of state recognition using CLIP, based on the optimization of prompt weights by black-box optimization. For $e = \\sum_i w_i a_i$ in the method described in Section II-B, the weights $w_i$ were set manually to be appropriate. For example, if $P_{1,2} =${\"open door\", \"closed door\"}, then $w_{1,2} = ${1, -1}. On the other hand, if $w_i$ is a continuous value and we can optimize this value, we should be able to construct a state recognizer with higher recognition accuracy. In this study, we optimize $w_i$ as a continuous value in the range [-1, 1].\nThe optimization procedure is described below. First, as a dataset D, we prepare images $V_t$ ($1 \\leq t \\leq T$, where t expresses the time axis, the angle of view, etc., and T is the number of images), the correct response $A_t$ for each image $V_t$, and multiple prompts $P_i$ ($1 \\leq i \\leq N_p$). Here, $A_t \\in$ {1, -1} (e.g., 1 for the open door state and -1 for the closed door state).\nNext, we define the evaluation function E to be optimized. First, with the latent vector of the image $v_t$, the latent vector of the prompt $p_i$, and a given weight $w_i$, the most straight- forward evaluation function $E_1$ that maximizes recognition accuracy when an optimal threshold $C_{thre}$ is set can be"}, {"title": "III. EXPERIMENT", "content": "The representative images used in our state recognition experiments and their prompt combinations are shown in Fig. 2. Specifically, we conduct experiments to recognize whether a room door, an elevator door, a cabinet door, a refrigerator"}, {"title": "A. Room Experiment", "content": "The results of the open/closed state recognition experiment of a room door are shown in Table I. The recognition accuracies of OPT-{1, 2, 3} are all similarly high, and $R_{opt} > R_{eval}$ holds. Also, OPT > ONE > ALL holds for the recognition accuracy, and the accuracy without optimization is lower than that with optimization."}, {"title": "B. Elevator Experiment", "content": "The results of the open/closed state recognition experiment of an elevator door are shown in Table II. The recognition accuracies of OPT-{1, 2, 3}, ONE, and ALL are almost the same, and $R_{opt} > R_{eval}$ holds. Even without optimization, state recognition can be performed with high accuracy."}, {"title": "C. Cabinet Experiment", "content": "The results of the open/closed state recognition experiment of a cabinet door are shown in Table III. The accuracy trend"}, {"title": "D. Refrigerator Experiment", "content": "The results of the open/closed state recognition experiment of a refrigerator door are shown in Table IV. The recognition accuracies of OPT-{1, 2, 3} are similarly high, and $R_{opt} \\approx R_{eval}$ holds. On the other hand, unlike the previous cases, the recognition accuracy of ALL is higher than that of ONE. Moreover, $R_{opt} > R_{eval}$ holds for ALL and ONE."}, {"title": "E. Various Doors Experiment", "content": "The results of the open/closed state recognition experiment of the aforementioned four types of doors using the same prompts are shown in Table V. For OPT-{1, 2, 3}, the states of four different doors are recognized with high accuracy even with the same prompts. Also, $R_{opt} > R_{eval}$ holds, among which OPT-3 > OPT-2 > OPT-1 holds for $R_{eval}$. On the other hand, the recognition accuracies of ONE and ALL are much lower than those of OPT, and ALL > ONE holds for the recognition accuracy."}, {"title": "F. Microwave Experiment", "content": "The results of the open/closed state recognition experiment of a microwave door are shown in Table VI. OPT-2 > OPT-3 > OPT-1 holds for the recognition accuracy, and especially for OPT-3, $R_{eval}$ is much lower than $R_{opt}$. Also, while the accuracy of ONE is close to that of OPT, the accuracy of ALL is significantly lower."}, {"title": "G. Transparent Door Experiment", "content": "The results of the open/closed state recognition experiment of a transparent glass door are shown in Table VII. This task is more difficult than the previous ones, and $R_{opt}$ does not reach its maximum value even after optimization. OPT-2 > OPT-3 > OPT-1 holds for the recognition accuracy, and $R_{opt} > R_{eval}$ holds for OPT-1 and OPT-3, while $R_{opt} = R_{eval}$ for OPT-2, maintaining high accuracy. As with Table VI, the accuracy of ONE is close to that of OPT, while the accuracy of ALL is significantly lower."}, {"title": "H. Light Experiment", "content": "In the following sections, we show the results of state recognition experiments that are qualitatively different from the open/closed door state recognition experiments. The results of the on/off state recognition experiment of a light are shown in Table VIII. While the recognition accuracies of OPT-{1, 2, 3} and ONE are similarly high, that of ALL is lower. Also, $R_{opt} > R_{eval}$ holds."}, {"title": "I. Display Experiment", "content": "The results of the on/off state recognition experiment of a display are shown in Table IX. The recognition accuracies of OPT-{1, 2, 3}, ONE, and ALL are similarly high. Also, $R_{opt} > R_{eval}$ holds."}, {"title": "J. Handbag Experiment", "content": "The results of the open/closed state recognition experi- ment of a handbag are shown in Table X. The recognition accuracies of OPT-{1, 2, 3} are similarly high with respect"}, {"title": "K. Water Experiment", "content": "The results of the state recognition experiment of whether water is running or not are shown in Table XI. The recog- nition accuracies of $R_{opt}$ for OPT-{1, 2, 3} are similarly high, while OPT-3 > OPT-2 > OPT-1 holds for $R_{eval}$. OPT > ONE > ALL holds for the recognition accuracy, and especially $R_{opt}$ of ALL is low. Also, $R_{eval}$ is much lower than $R_{opt}$, and the adaptability to the dataset for evaluation is low."}, {"title": "L. Kitchen Experiment", "content": "The results of the state recognition experiment of whether a kitchen is clean or not are shown in Table XII. OPT-2 > OPT-3 > OPT-1 holds for the recognition accuracy, and perfect recognition is achieved for OPT-2. The performance of ALL and ONE is worse than that of OPT."}, {"title": "IV. DISCUSSION", "content": "We were able to obtain highly accurate state recognizers throughout the experiments, and several characteristics were observed. First, in most cases, OPT has better recognition performance than ALL using all prompts or ONE with only one best prompt, indicating the importance of weighting optimization of prompts. In OPT, the same prompt can be used to recognize a variety of open/closed door states, and can recognize even the state of a transparent door or the qualitative clean/messy state of a kitchen. The performance difference between ALL and ONE varies from task to task and cannot be judged in general. On the other hand, the performance of ALL and ONE is high for simple tasks, and in particular, ALL is the easiest to use because it does not require weighting decisions."}, {"title": "V. CONCLUSION", "content": "In this study, we described a binary state recognition method for robots through the spoken language using CLIP, a pre-trained large-scale vision-language model. By calculating the similarity between multiple prompts and the current image and combining them, the robot can recognize the state of objects and environments such as the open/closed state of a door or the on/off state of a light. By preparing a small number of datasets and optimizing the weighting of the prepared prompts using black-box optimization, a more accurate state recognizer can be constructed. We also discussed the change in recognition accuracy depending on the difference of evaluation functions in the optimization. Higher recognition accuracy can be obtained by maximizing the difference of the means of the evaluation values in two states or minimizing the variance of the evaluation value within each state. We have shown that the robot can easily recognize the open/closed state of a transparent door, whether water is running or not from a faucet, and even the qualitative state of whether a kitchen is clean or not, without manual programming, retraining of neural networks, or the use of dedicated sensors. Since only prompts and their weights need to be prepared for each recognizer, there is no need to prepare multiple models, which facilitates resource management. We believe that this study will revolutionize the recognition"}]}