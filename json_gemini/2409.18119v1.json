{"title": "Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography", "authors": ["Yuexi Du", "John Onofrey", "Nicha C. Dvornek"], "abstract": "Contrastive Language-Image Pre-training (CLIP) shows promise in medical image analysis but requires substantial data and computational resources. Due to these restrictions, existing CLIP applications in medical imaging focus mainly on modalities like chest X-rays that have abundant image-report data available, leaving many other important modalities under-explored. Here, we propose the first adaptation of the full CLIP model to mammography, which presents significant challenges due to labeled data scarcity, high-resolution images with small regions of interest, and data imbalance. We first develop a specialized supervision framework for mammography that leverages its multi-view nature. Furthermore, we design a symmetric local alignment module to better focus on detailed features in high-resolution images. Lastly, we incorporate a parameter-efficient fine-tuning approach for large language models pre-trained with medical knowledge to address data limitations. Our multi-view and multi-scale alignment (MaMA) method outperforms state-of-the-art baselines for three different tasks on two large real-world mammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared with the largest baseline. The code is available at https://github.com/XYPB/MaMA.", "sections": [{"title": "1 Introduction", "content": "Contrastive learning [5, 17, 16] has become one of the most popular self-supervised representation learning paradigms due to its intuitive concept and robust performance. Contrastive learning removes the reliance on a supervised signal by optimizing the semantic distance for similar pairs in the representation space in a contrastive manner. More recently, the introduction of natural language signals to contrastive learning [38] has given rise to modern visual-language models [26, 25, 29].\nContrastive Language-Image Pre-training (CLIP) [38] has also been widely applied in the medical imaging domain [53, 19, 51, 56, 54, 55, 15] and shows promising improvement in medical image understanding when large-scale medical imaging datasets are available [22, 20, 15, 55]. However, the CLIP model in the natural image domain usually demands more than hundreds of millions of image-text pairs to be properly trained [38, 44, 46, 45], which is almost impossible in the medical domain due to privacy and security concerns. Existing medical CLIP methods either build general-purpose CLIP models with multiple anatomical sites and modalities from public online databases [15, 55] or focus on imaging modalities with large-scale (less than a million) datasets, e.g., chest X-ray or pathology images [56, 19, 51, 54, 53, 60, 52, 50, 24]. This means other imaging modalities, such as mammography, have yet to fully benefit from such visual-language pre-trained models.\nMammography is a critical medical imaging modality for breast cancer screening and diagnosis, as breast cancer is one of the most commonly diagnosed cancers globally and a leading cause of cancer-related mortality in women [47]. While visual-language pre-training (VLP) has the potential to improve mammography interpretation, there are two major obstacles: 1) Limited data and annotation: Recent work has introduced a large-scale mammography image and tabular dataset of more than 110,000 patients, i.e., EMBED [21], but no corresponding clinical reports are available. 2) Nature of mammography: Different from the single view natural image or chest X-ray, each mammography study usually contains four high-resolution (~2,000-by-2,000 pixels) views of the same patient: left and right side, each with craniocaudal (CC) and mediolateral oblique (MLO) views. Such multi-view mammography has the critical properties of bilateral asymmetry [12] and ipsilateral correspondence [32]. Bilateral asymmetry means images from different sides of the same patient can contain different information, e.g., density, calcification, and mass findings. Ipsilateral correspondence means different views of the same side share similar information from different viewpoints. Clinicians consider both properties and all four images at once as a cross reference when reading a study. Meanwhile, lesions of interest are often relatively small compared with high-resolution mammograms, which further challenges a model's ability to focus on local details. This pixel-level imbalance compounds the problem of image-level imbalance, in which the vast majority of mammograms will not contain cancer. While one recent work [6] attempts to address these issues by leveraging VLP, they take a fine-tuning approach in which they simply adopt a pre-trained CLIP model and perform supervised fine-tuning to a zero-shot classification task for multi-view mammography, rather than capitalizing on the mammography domain information to perform contrastive language-image pre-training for mammography representation learning [58].\nTo address these challenges, we propose a novel Multi-view and Multi-scale Alignment i.e., MaMA, contrastive language-image pre-training framework that exploits the multi-view property of mammography and aligns multi-scale features simultaneously. We first propose an intuitive method for template-based report construction from tabular data to resolve the lack of clinical reports and enable VLP. The proposed model then optimizes both multi-view image-image and symmetric text-image contrastive loss simultaneously (Fig. 1), learning the correspondence between the multi-view images and image-report relationship from the same mammography study. We then propose a novel symmetric local alignment module that actively learns sentence-patch relationships by computing similarity scores for each image-text pair. We also incorporate parameter-efficient fine-tuned large-language models (LLMs) pre-trained with medical domain knowledge to improve the understanding of the report while addressing data scarcity. We validate our method on two large-scale mammography datasets, EMBED [21] and RNSA-Mammo [4], with multiple settings compared with state-of-the-art medical CLIP methods. The proposed method surpasses all the baselines with a considerable gap with only 52% model size, showing promise on multiple mammography-related tasks."}, {"title": "2 Related Works", "content": "Medical Visual-Language Pre-training Existing medical VLP methods can be divided into two types depending on the training data. The first type is the general-purpose medical CLIP model trained with a large-scale medical-image dataset with multiple anatomical sites and imaging modalities derived from PubMed [15, 55]. This approach mainly focuses on scaling dataset size while using a vanilla CLIP design [38]. These models show promising generalization ability on multiple sites but are often suboptimal compared with modality-specific models due to the lack of a specific design for the individual image modality. The other type of VLP models mainly focuses on"}, {"title": "3 Method", "content": "In this section, we introduce the proposed MaMA (Fig. 2). We begin with the construction of the structured mammography report from the tabular data. We then introduce the multi-view contrastive image-text pre-training framework, followed by the proposed symmetric local alignment (SLA).\n3.1 Structured Report Construction\nDifferent from chest X-ray datasets that provide paired images with corresponding clinical reports, e.g., MIMIC-CXR [22], large-scale mammography datasets with the full report available are rare. Rather, existing datasets in this domain [21, 4, 35] mainly provide a tabular structure annotation including both the anonymized meta information as well as the clinical findings, e.g., breast density type, calcification findings, tumor description, and Breast Imaging Reporting and Data System (BI-RADS) assessment category [41]. Clinical findings serve as cross-validation evidence for the final diagnosis. Using a CLIP-style [58] caption with only the simple class label for cancer will result in a highly simplified caption and limit the model's understanding of the image due to missing details.\nWe propose a template-based caption construction method following the standard clinical report structure [36] (Fig. 2 (a)). We first create a report template with segments describing study procedure, patient meta-information, image meta-information, breast composition, findings, clinical impression and the final overall assessment in a natural language report style. Each segment contains keywords that can be replaced with the corresponding meta-information in the tabular data. By replacing these keywords and concatenating these segments, we can build a complete clinical report for each specific image, and provide more details for language-image contrastive learning. We provide the full template and a few image-caption examples in the appendix.\nMeta-Info Masking The increased information from patient and image-specific meta-data may be memorized by the model during the contrastive training and result in learning shortcuts for the model decision. To focus more on the diagnosis and disease-related information, we propose a data augmentation method that randomly masks each patient or image meta-information keyword with a probability of m when constructing the caption.\n3.2 Multi-view VLP\nWe introduce the multi-view contrastive VLP framework here. Let $D = \\{(x_i, Y_i), i = 0, 1, . . ., N\\}$ be a multimodal dataset, where there are N individual images $x_i$ and corresponding text captions $Y_i$. Our framework optimizes both image-to-image and symmetric image-to-text contrastive loss.\nMulti-view Visual Contrastive Loss We first optimize the contrastive loss within the multi-view images (Fig. 2 (a)). We define a study to include the data from the same imaging session for a patient, including one or more image-text pairs. For a random image-text pair $(x_i, Y_i)$ from the dataset D, we uniformly sample another image $x_{i'}$ from the same study that $x_i$ belongs to as the positive sample of $x_i$. Note that $i'$ could be $x_i$ as the augmented view of the same image is naturally a positive sample. We augment both images with random data augmentation and then feed into the vision encoder $f_v$ and d-dimensional global embedding projection head $g_v$ followed by average pooling to get corresponding visual embedding $v_i, v_i \\in R^d$, i.e., $v_i = avg(g_v (f_v(x_i)))$. We then compute the cosine similarity for each pair of visual embeddings and optimize the InfoNCE [5] loss for $v_i$ in a mini-batch of size B:\n$L_{vv}(v_i, v_{i'}) = log \\frac{exp(sim(v_i,v_{i'})/\\tau_1)}{\\sum_{j=1}^{B}exp(sim(v_i,v_j)/\\tau_1)},$ (1)\nwhere $sim(v_i, v_j) = \\frac{v_i^T v_j}{|| v_i || || v_j ||},$ where $\\tau_1$ is the visual temperature constant and $v_j$ is the j-th visual embedding in the batch. Since two views of the same side of a study have ipsilateral correspondence, it is natural to treat them as positive samples of each other, as the features, like tumors, present in one view, are often present in the other view as well. On the other hand, even if considering bilateral asymmetry for images from different sides, they still share much high-level information such as patient-level features (e.g., global breast shape similarity, age) and similar breast density. Introducing multi-view mammography contrastive learning forces the model to learn semantically similar features from images within the same study. This also provides a stronger self-supervised signal than using random augmented images. Our image-to-image contrastive learning framework follows the design of SimCLR [5] for simplicity."}, {"title": "Symmetric Visual-Text Contrastive Loss", "content": "While existing methods like SLIP [34] also optimize both image-image and image-text contrastive loss, we note there is a potential contradiction between image-image and image-text objectives when computed for different examples (Fig. 1 (b)), i.e., $L_{vv}$ and $L_{vT}$ are independent and the extra image will introduce unnecessary memory cost. To address this, we propose re-using $v_i$ when optimizing $L_{vT}$ and symmetrically optimizing this loss.\nWe feed caption $y_i$ to the tokenizer and text encoder $f_T$ and then the text global projection head $g_T$ with average pooling to get the text embedding $t_i \\in R^d$. We optimize the CLIP [38] loss (Fig. 2 (a)):\n$L_{VT}(v_i, t_i) = -\\frac{1}{2}(log \\frac{exp(sim(v_i, t_i)/\\tau_2)}{\\sum_{j=1}^{B} exp(sim(v_i, t_j)/\\tau_2)} + log \\frac{exp(sim(t_i, v_i)/\\tau_2)}{\\sum_{j=1}^{B} exp(sim(t_i, v_j)/\\tau_2)}),$ (2)\nwhere $\\tau_2$ is the learnable language temperature constant. We compute $L_{VT}$ for both $v_i$ and $v_{i'}$ for the same $t_i$ symmetrically. Namely, we minimize the semantic distance between two images from the same view and the corresponding report simultaneously. We note that even if the information in $Y_i$ is not completely matched with $i'$, e.g., different side and view caption, they still share a large overlap in patient-level information. This encourages the model to mine the high-level shared patient-related features via minimizing $L_{VT}(v_{i'}, t_i)$ while focusing on diagnosis-related information by minimizing $L_{VT}(v_i,t_i)$.\n3.3 Symmetric Local Alignment (SLA)\nMammography usually contains high-frequency details and the region of interest is usually very small. These properties require a higher image resolution for the deep learning method to work properly. It also challenges the model's ability to extract important local information and filter out less meaningful background and tissue unrelated to diagnosis. To address these challenges, we propose a symmetric local alignment (SLA) module. Specifically, the SLA module allows the model to determine the local correspondence relationship between each sentence and image patch (Fig. 2 (b)).\nWe start with extracting local features from input $(x_i, Y_i)$. We feed the image and caption to the vision encoder $f_v$ and text encoder $f_T$ respectively, followed by corresponding local projection head $h_v$ and $h_T$ without pooling to produce output feature sequence $v_{local} \\in R^{N_v \\times d}$ and $t_{local} \\in R^{N_T \\times d}$ where $N_v$ and $N_T$ are the length of visual tokens and text tokens, respectively. We then extract sentence-level features by selecting the embedding corresponding to the [SEP] token, which results in a sequence of sentence embeddings $s_i \\in R^{S \\times d}$, where S is the number of sentences. We extract the image patch-level features by removing the extra functional tokens like [CLS] tokens, resulting in a sequence of patch embeddings $p_i \\in R^{P \\times d}$, where P is the number of patches. We then compute the sentence-patch correspondence matrix $C_{i,i'} \\in R^{S \\times P}$ in the form of cosine similarity, which reveals the relationship between local patches and each sentence in the report. However, we cannot directly supervise the learning of this matrix since we have no access to the local correspondence between the image and the report. Thus, we aggregate the patch-sentence level correspondence matrix $C_{i,i'}$ to an image-report level similarity score. We start by localizing the patch that has the highest correspondence for each sentence. Namely, we find the most relevant region in the image for each sentence. We call this process Visual Localization. We then average the similarity score for each sentence to obtain a correspondence score which describes the similarity of the most relevant patch for the whole report $c_i = \\sum_j max_k C_{i,i'}(j, k)$, where $C_{i,i'}(j, k)$ is the similarity between the j-th sentence and the k-th patch. Similarly, we conduct Text Localization by finding the most similar sentence feature for each patch and averaging it to get a score for the similarity of the most relevant sentence for the whole image $c_{i'} = \\sum_k max_j C_{i,i'}(j, k)$. We compute the aggregated visual and text localization scores for all p and s in the mini-batch and optimize the InfoNCE [17] loss:\n$s_{Llocal}(i) = -\\frac{1}{2}( \\frac{exp(c_i/\\tau_{local})}{\\sum_{j=1}^{B} exp(c_j/\\tau_{local})} + \\frac{exp(c_{i'}/\\tau_{local})}{\\sum_{j=1}^{B} exp(c_{j'}/\\tau_{local})}),$ (3)\nand $L_{local}$ is defined similarly, where $\\tau_{local}$ is the local temperature constant. The final local loss will then be $L_{local} = (L_{local} + L_{local})$. We note that introducing this local loss from the beginning of the training can lead to unstable behavior as the initial visual and language embeddings are not aligned. Thus, we add this loss after k steps of training.\nThe intuition behind this design is to mimic the process of radiologic interpretation of a medical image in the real world. On the one hand, in mammography, the clinician will look for the image"}, {"title": "LLM with PEFT as Text Encoder", "content": "Lastly, we incorporate parameter-efficient fine-tuning (PEFT) of a pre-trained large language model (LLM) as our text encoder (e.g., BioMedLM [3]) rather than use a small pre-trained BERT encoder [2]. Using a pre-trained LLM with strong domain knowledge can help improve the model's understanding of the text caption and provide a more robust supervised signal for the visual-language pre-training. Moreover, PEFT (e.g., LoRA [18]) can greatly reduce the cost of adapting LLM to scenarios with a shortage of computing resources while maintaining a strong performance after fine-tuning. Adapting an LLM with PEFT thus has the potential to greatly improve performance while reducing trainable parameters and GPU memory costs compared to learning the commonly adopted BERT-style encoder."}, {"title": "4 Experiments", "content": "4.1 Pre-training Settings\nDataset We pre-trained our model on the Emory EMBED [21] dataset, which is a large-scale mammography dataset with public access and one of the largest open mammography datasets available. The current release contains 72,768 multi-view mammography studies for 23,356 patients collected from 4 hospitals. We focus on 2D mammography, which has 364,564 individual images in total. The dataset provides tabular annotation about the patient, imaging meta-information, and corresponding image-level findings including breast density, BI-RADS assessment, and calcification findings. We split the dataset by patient into train/validation/test partitions, each with 70%/10%/20% images. All the images are resized and padded to 518 \u00d7 518 without changing the aspect ratio.\nImplementation Details We choose to use DiNOv2-ViT-B-14 [37] and BioMedLM [3] as our image and text encoder respectively. We adapt LoRA [18] to the text encoder to fine-tune it efficiently. We choose DiNOv2 [37] ViT as it is pre-trained with a larger image size which is suitable for mammography. Note that our method does not depend on a specific text encoder design. We also report the performance of our model with a more common BioClincialBERT [2] encoder. The meta masking ratio m is 0.8 during training. We train our model with the AdamW optimizer [33] using a learning rate of 4E-5, weight-decay of 0.1, and cosine annealing scheduler for 40k steps. We also adapt warm-up from 1E-8 for 4k steps. The SLA loss is added after k = 8k steps. We use a batch"}, {"title": "4.2 Downstream Evaluation Settings", "content": "Tasks and Datasets We primarily evaluate our method on the EMBED [21] dataset for both BI-RADS assessment category (7 classes) and breast density (4 classes) prediction tasks. Note that the real-world distribution of labels for both tasks is extremely imbalanced. To demonstrate the behavior of each model in a more realistic scenario, we sample 7,666 images for BI-RADS prediction and 7,301 images for breast density prediction from the test split following the dataset distribution. To avoid insufficient test data and possible bias, we use all the images with BIRADS 5 and 6 in the BIRADS prediction test set. Detailed class distribution is provided in the appendix. We also use the RSNA-Mammo [4] dataset for out-of-domain evaluation for binary cancer detection, which only released a training set with 54k images. We split it into a training set of 85% data and used the remaining as the evaluation. Given the extremely imbalanced distribution of both datasets, we choose to report balanced accuracy and AUC as our primary metrics. We also report the sensitivity and specificity of the RSNA-Mammo cancer detection task. We do not assess zero-shot classification on this dataset since only a binary cancer label is available for all samples.\nEvaluation Settings We evaluate all methods under zero-shot, linear classification, and full fine-tuning settings. For zero-shot classification, we provide patient and imaging meta-information along with the class-wise captions, as this meta-information is readily available without a clinician's diagnosis. For linear classification, we attach a linear classifier and fine-tune it using 1%, 10%, or 100% of the training data. Following [56, 19, 51, 53, 54, 50], we perform this full data efficiency study with linear classification and present as our primary results since this experiment mainly focuses on the quality of the pre-trained embedding and it can best demonstrate the difference between each VLP method. For full fine-tuning, we again attach a linear classifier and fine-tune the whole model using 100% of the training data. Our learning rate is set to 5E-4 and weight decay to 1E-3 using the SGD optimizer with cosine annealing scheduler for 8k steps with batch size 36. A warm-up of 100 steps with a minimum learning rate of 1E-5 is applied. The fine-tuning uses 2 RTX A5000 GPUs.\nBaselines As the very first attempt at full contrastive language-image pre-training for mammography, we choose to compare with the following baselines: 1) ViT [13, 37]: we compare with vision-only baselines with both random initialization and DiNOv2 [37] pre-training. 2) CLIP [39]: the vanilla CLIP model without other additional design; 3) SLIP [34]: a contrastive learning framework that optimizes both image-image and image-text loss; 4) MM-MIL [52]: a CLIP model that learns local image-language relationship via a multiple instance learning paradigm; 5) ConVIRT [56]: one of the first Chest X-ray specific CLIP models; 6) MGCA [51]: one of the SoTA CLIP models for Chest X-ray that applies multi-granularity feature alignment. We pre-train and fine-tune all these baselines with the same settings as our model. We also replaced the original DeiT [48] ViT with DiNOv2 [37] for a fair comparison since DeiT-ViT [48] is only trained with a smaller image size. All the baseline methods use fully fine-tuned BioClinicalBERT [2] as text encoder. While we acknowledge that there are other recent medical VLP methods [19, 54, 50, 53], they either adapt"}, {"title": "4.3 Results", "content": "Linear Classification We report the performance of both EMBED BI-RADS and density classification tasks for each baseline in Tab. 1. We note MaMA achieves the best performance overall under different amounts of training data. Our method shows a non-trivial improvement of more than 4% of balanced accuracy on the BI-RADS prediction task when fine-tuned with full training data. We note that reducing the amount of training data has a greater influence on the BI-RADS prediction task than the density prediction task, as the BI-RADS distribution is more imbalanced, e.g., there are only 6 training images for BI-RADS category 5 and 2 images for category 6 when using 1% training data. However, our method still maintains the best overall performance even when trained with only 1% data on the BI-RADS prediction task. This demonstrates the strong generalization ability and robustness of MaMA. Even if comparing with baselines also with local awareness [52, 51], our method is still the best. We also notice that the DiNOv2 [37]-based models tend to outperform the DeiT [48]-based models even if using the same VLP model design. This is not only because DiNOv2 ViT [37] was trained on more data, but also due to the use of a larger image size, which is critical for high-resolution mammography.\nZero-shot Classification We report the zero-shot classification performance for each of the methods on both EMBED [21] tasks in Tab. 2. While our method still outperforms all the baselines, we highlight the zero-shot performance of the BI-RADS score prediction task, where our model outperforms the best baseline by ~5% in terms of balanced accuracy and more than 7% in AUC score. Compared with baselines using the fully fine-tuned small BioClinicalBERT [2], our method with pre-trained LLM with PEFT shows much better zero-shot performance as the LLM can provide a text-supervised signal with higher quality. Meanwhile, the PEFT helps to prevent the LLM from collapsing during fine-tuning. As a result, our LLM text encoder with PEFT can provide better zero-shot text embedding and improve the zero-shot performance greatly. Meanwhile, we note that the adopted LLM with PEFT encoder only has 2.6 M trainable parameters, which is only 3% of the BioClinicalBERT [2] in terms of size.\nFull Fine-tuning Classification We also report the classification results after full-fine-tuning for EMBED [21] tasks in Tab. 2. We note that while the gap between each method is somewhat reduced due to full fine-tuning, our model still beats all other baselines on both tasks.\nOut-of-Domain Data Analysis We report performance of each method on the out-of-domain RSNA-Mammo dataset in Tab. 3. Since RSNA-Mammo [4] is an extremely imbalanced dataset (48:1 negative to positive ratio), we report the sensitivity and specificity as well. We note our model"}, {"title": "4.4 Ablation Experiments", "content": "Model Design We ablate the influence of each component in Tab. 4. Compared with these baselines, we note each component has an important contribution to the overall model performance, as removing any one resulted in inferior performance. We note that the baseline without PEFT-LLM instead employs BioClinicalBERT [2] and shows a clear drop in zero-shot performance, which validates the importance of using a PEFT-LLM. However, this model still performs well on the linear classification and full fine-tuning tasks, which demonstrates the effectiveness of our other design choices.\nMulti-view Ablation We ablate the multi-view sampling strategy here by using: 1) the same image, 2) an intra-side image, and 3) the complete intra-study image (Tab. 5). We can see that the model trained with only one image loses the multi-view understanding. The model using only intra-side images only considers ipsilateral correspondence and also results in a worse performance.\nCaption Ablation We evaluate the influence of using different caption construction strategies in Tab. 6. We note that a CLIP style caption that only focuses on class labels shows a better zero-shot performance, but degenerates greatly in the linear classification and full fine-tuning tasks. Meanwhile, if simply using the full meta-information during training, the model will fail with zero-shot classification since it may mainly rely on the meta-information during the training and ignore more important clinical information. Our full design of using a structural caption with meta-information masking shows the best performance."}, {"title": "5 Discussion and Conclusion", "content": "In this work, we presented a complete and novel multi-view and multi-scale alignment contrastive language-image pre-training method for mammography. We proposed utilizing the multi-view nature of mammography and providing local image-sentence correspondence to help address the challenges of small ROIs and high image resolution and provide fine-grained visual clues for decisions. The proposed method greatly outperforms multiple existing medical CLIP baselines."}, {"title": "Limitation and Future Work", "content": "As we mainly focus on image representation learning, we have yet to evaluate other downstream tasks like image-text retrieval, object detection, and segmentation. While also limited by accessible data in this domain, our method will be evaluated on more downstream tasks in future work. We plan to extend this current framework to more mammography imaging modalities including C-view and digital breast tomosynthesis to further enhance its understanding of mammography. Meanwhile, we also plan to integrate this pre-trained component into a multi-modal question-answering and grounding model, to further explore the potential of medical VLP."}, {"title": "6 Acknowledgement", "content": "This work was supported by NIH grant R21EB032950."}, {"title": "A Appendix", "content": "In the appendix, we provide more detailed training settings, evaluation settings, model configurations, and additional analysis.\nA.1 Further Limitations\nThe EMBED data comes from the Atlanta, GA region. While the dataset is highly ethnically diverse, the geographic focus could limit generalizability to other populations, e.g., the breast density distribution may differ from data gathered in other regions of the world. Additionally, The caption is created from the templated-based method, which may potentially harm the model due to limited caption diversity. Future works may consider augment the template-based prompt with LLM.\nA.2 Broader Impacts\nThis paper proposed a promising visual language pre-training scheme for mammography that can be used for various downstream tasks. It can also potentially speed up the real-world mammography screening or diagnostic process by filtering out low-risk studies and highlighting high-risk images for the clinician. While the EMBED dataset is one of the largest and most diverse public mammography datasets available, it is notable that the data were collected from four specific hospitals and thus the trained model may have a specific bias towards a specific group of people due to training data composition. Any user who wants to use this model in their own research may need to carefully analyze such bias and their own application and tasks and avoid using the model in real-world clinical trials without further approval."}, {"title": "A.3 Pseudo-Code of SLA Module", "content": "We provide the pytorch pseudo-code of the SLA module in the Algorithm 1 to better illustrate the design of the SLA module."}, {"title": "A.4 Pre-training Implementation Details", "content": "Dataset and Pre-processing As mentioned in Sec. 4.1, we use the EMBED [21] dataset for pre-training. We only use the 2D mammography and split the dataset into 70%/10%/20% for training, validation, and testing at the patient level. We filter out the studies for males or those that have missing BI-RADS or density labels. We provide the detailed distribution of BI-RADS score and"}, {"title": "A.7 Additional Ablation Experiments", "content": "Meta Masking Ratio To better understand the influence of masking the meta-information, we here provide an extra zero-shot evaluation on different mask ratios m during the pre-training stage in Tab. 8. As shown above, the zero-shot performance increases as the meta-information masking ratio increases, which means the model tends to rely more on clinical-related information, and therefore, does better in the zero-shot classification task."}, {"title": "A.8 Benchmark Different Text Encoders", "content": "We evaluate all methods with the same DiNOv2 [37] vision encoders but compare the influence of using different text encoders in Tab. 12.\nText Encoders 1) BioClinicalBERT [2]: The standard text encoder used for previous medical CLIP models [52, 56, 19, 51, 50] and also our baseline methods, which is a BERT [11]-style transformer pre-trained with MIMIC-III [23] clinical report. 2) BioMedLM [3]: A 2.7B level GPT-2 [39] transformer pre-trained with PubMed data, which is also one of the best 3B LLM according to multiple benchmarks [7]. 3) Meditron-7B [7]: A newly released Llama2 [49] model fine-tuned with PubMed papers. 4) Llama3-8B [1]: Recently released, the most robust open-souced LLM, with roughly the same architecture as Llama2 [49] but pre-trained with much more data. All the latter three LLMs are fine-tuned with LoRA [18]\nResults We report the results on linear classification in Tab. 12. We note that even our model with BioClinicalBERT [2] text encoder outperforms all the baselines in this evaluation; this demonstrates the effectiveness of the proposed multi-view mammography pre-training and symmetric local alignment module. Comparing three different LLMs with LoRA [18], we note that BioMedLM [3] and Llama3-8B [1] roughly have a similar level of performance, while the BioMedLM-based model has a smaller GPU memory cost and faster training speed due to its relative size. Meanwhile, we notice that the Meditron [7]-based model is not as good as the other two LLMs, but all these LLM-based methods outperform the model with smaller BERT-style [11] encoder in general. Overall, our choice of BioMedLM [3]-based model has the best balance between performance and model size."}, {"title": "A.9 Local Similarity Map Analysis", "content": "We visualize the learned local patch-sentence similarity map in Fig. 5. As described in Sec. 3.3, the local patch-sentence similarity map indicates the relationship between each region of the image and the corresponding input sentence. We visualize the similarity map for the \u201cImpressions\u201d sentence in the report (see examples in Fig. 6 to Fig. 8), which includes the most important diagnosis information. We also visualize the same similarity map for MM-MIL [52", "52": "model even failed to detect the tumor for patient 4. On the other hand, the variation of our model that optimizes only visual localization loss can"}]}