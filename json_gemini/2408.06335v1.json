{"title": "LOLgorithm: Integrating Semantic, Syntactic and\nContextual Elements for Humor Classification", "authors": ["Tanisha Khurana", "Vikram Pande", "Kaushik Pillalamarri", "Munindar Singh"], "abstract": "This paper explores humor detection through a\nlinguistic lens, prioritizing syntactic, semantic, and contextual\nfeatures over computational methods in Natural Language Pro-\ncessing. We categorize features into syntactic, semantic, and\ncontextual dimensions, including lexicons, structural statistics,\nWord2Vec, WordNet, and phonetic style. Our proposed model,\nColbert, utilizes BERT embeddings and parallel hidden layers\nto capture sentence congruity. By combining syntactic, semantic,\nand contextual features, we train Colbert for humor detection.\nFeature engineering examines essential syntactic and semantic\nfeatures alongside BERT embeddings. SHAP interpretations and\ndecision trees identify influential features, revealing that a holistic\napproach improves humor detection accuracy on unseen data.\nIntegrating linguistic cues from different dimensions enhances\nthe model's ability to understand humor complexity beyond\ntraditional computational methods.", "sections": [{"title": "I. PROBLEM DESCRIPTION", "content": "Humor is a fascinating and puzzling area of study in the field\nof computers understanding human language. In recent times,\ncomputers have transformed from being simple machines that\nfollow instructions to becoming smart agents that interact\nwith people and learn from them. When a computer talks\nto a person, if it can understand and appreciate the humor\nin what the person is saying, it can grasp the real meaning\nof human language better. This, in turn, helps the computer\nmake better decisions to enhance the user's experience. So,\nworking on techniques that allow computers to \"get\" humor\nin human conversations and adjust their responses accordingly\nis something we should pay special attention to.\nH umor Recognition is about figuring out if a sentence is\nfunny in a given situation. It's a tough problem in understand-\ning human language. First, humor is tricky because different\npeople may find different things funny even in the same\nsentence. Second, humor often depends on knowing a lot about\nthe context. For example, think about these sentences: \"The\none who invented the door knocker got a No Bell prize\" and\n\"Veni, Vidi, Visa: I came, I saw, I did a little shopping.\" To find\nthem funny, you need to know a bit about culture and language.\nLastly, there are many types of humor, like wordplay, irony,\nand sarcasm, but we don't have a clear way to categorize them\nall. So, it's almost impossible to create a computer program\nthat can recognize all types of humor, just like humans can't\nalways classify them perfectly either.\nIn this work, we formulate humor recognition as a classifica-\ntion task in which we distinguish between humorous and non-\nhumorous instances. Exploring the syntactical structure in-\nvolves leveraging Lexicons to capture sentiment counts within\na sentence, while Statistics of Structural Elements (SSE) [15]\nencapsulates the statistical insights of Noun phrases, Word\nphrases, and more. Unveiling the semantic layers of humor\ndelves into Word2Vec [18] embeddings, analyzing incon-\ngruity, ambiguity, and phonetic structures within sentences.\nAdditionally, contextual information is harnessed through Col-\nBERT embeddings [3]. For each latent structure, we design a\nset of features to capture the potential indicators of humor."}, {"title": "A. Example Scenario", "content": "Imagine you work for a social media analytics company, and\none of your clients is interested in understanding and quanti-\nfying humor within user-generated content on their platform.\nThey want to identify and categorize humorous posts, as well\nas analyze the types of humor that resonate the most with their\naudience. Humor classification will be extremely beneficial for\nthese scenarios.\nAmazon for example employs humor detection in product\nquestion-answering systems [35] as some products attract\nhumor due to their unreasonable price and their peculiar\nfunctionality. Detecting humorous questions in such systems\nis important for sellers, to better understand user engagement\nwith their products. It is also important to inform users about\nthe flippancy of humorous questions, and that answers for such\nquestions should be taken with a grain of salt."}, {"title": "II. RELEVANT LITERATURE", "content": "Many of the exciting studies on humor recognition are\nformulated as binary classification problems and try to identify\njokes using a set of linguistic features. For example, Zhang\net al. [33] developed various humor-related features based\non humor theories, linguistic rules, and emotions. They used\nthese features in a Gradient Boosting Regression Tree model\nto recognize humor. Yang et al. [32] tried to identify several\nsemantic structures behind humor and design sets of features\nfor each structure, they extracted anchors that enable humor\nin the sentence and then used a computational approach to\nrecognize humor. They explored the types of latent semantic\nstructure behind humor namely incongruity, ambiguity, inter-\npersonal affect, and phonetic style. Liu et al. [15] extended this\nresearch by adding syntactical structures for humor detection\nsuch as statistics of structural elements, production rules, and"}, {"title": "A. DATASET", "content": "We employ two distinct datasets, for training and testing\npurposes:\nColBERT Dataset: The ColBERT dataset [2] has 200,000\nstatements with humor labels and is accessible on Kaggle for\nmodel training and evaluation. We partition it into training and\nvalidation sets.\nFor the unseen test set, we scraped funny one-liners from\nthe Bestlife website to gather jokes. We preprocess the data\nby removing punctuation and lemmas and manually labeling\nthe jokes as humorous or not for unseen datasets for model\nevaluation. Also, we've obtained around 2K samples of sen-\ntences from Reddit, out of which some are funny and some\nare not, which would also be used for further evaluating the\nmodel's performance on unseen data."}, {"title": "B. CODE", "content": "We use libraries such as PyTorch [24], TensorFlow,\nHugging-Face [31], NLTK [4], SpaCy [12], NRCLex [19],\nSklearn [26], and some inherent Python libraries to help in\nthe coding part."}, {"title": "III. PROBLEM IMPORTANCE", "content": "Humor classification is a crucial problem and our solution\ntakes into account the contextual, semantic, and syntactic\nmeaning of the joke. Our problem has various applications in\ncustomer support and chatbot development. Many companies\nutilize conversational services to recommend items and coun-\nsel consumers. There is also a growing demand for emotional\nchatterbots for user's higher satisfaction in various commercial\nfields. Siri could become more 'human' if she had the ability\nto recognize social cues like humor and respond to them with\nlaughter. As described in our example scenario, our problem\nwill be useful in social media analytics and for question-\nanswering systems for products on Amazon[35]. Our problem\nalso has further usage when combined with speech as well. For\nexample, sitcoms are written and performed primarily to cause\nlaughter, so the ability to detect precisely why something will\ndraw laughter is of extreme importance to screenwriters and\nactors [27]."}, {"title": "IV. METHODOLOGY", "content": "The aim of the project was to work more with linguis-\ntic features rather than exploring computational methods in\nthe area of Natural Language Processing. For the features\nwe chose to work with, we've categorized them into three\ntypes for experimentation: syntactic, semantic, and contextual\nfeatures. Syntactic features help understand how words func-\ntion structurally and influence model predictions. Semantic\nanalysis delves into word and sentence meanings. Exploring\ncontextual dependencies within sentences is also crucial. We're\ngenerating these features using methods described later in\nthis section and analyzing these features to identify the most\ninfluential ones driving model predictions using basic SHAP\ninterpretations [16], and visualizing decision trees [5]. We\nthen model these features independently and together by\ncomputational methods."}, {"title": "A. Syntactical information", "content": "1) Lexicons: In some cases, individual words can be inher-\nently funny. Therefore, if a sentence contains amusing words,\nthe entire sentence may be humorous. With this assumption\nin mind, our initial approach involves extracting syntactical\ninformation using lexicons [13] [20]. Specifically, we employ\nthe NRC word emotion lexicons for this purpose. Utilizing\nthis lexicon, we represent words as vectors, where each entry\nsignifies the word's similarity to various emotions such as\nanger, anticipation, disgust, fear, joy, sadness, and surprise, as\nwell as its similarity to the general sentiments of positivity and\nnegativity. By employing these word representations, we aim\nto capture the syntactical information embedded within each\nword. To obtain syntactical embeddings for the entire sentence,\nwe aggregate the vectors of each word in the sentence, thereby\nencapsulating the overall syntactical characteristics of the\nsentence. We make use of the available NRCLex library to\nmeasure the emotional affect of a body of jokes. We get the\nscores for emotions such as fear, anger, anticipation, trust,\nsurprise, positive, negative, sadness, disgust, and joy for each\njoke."}, {"title": "2) Statistics of Structural Elements", "content": "As described by Liu\net al.[15] we follow the methods to acquire syntactic structures\nfor humor recognition. Among these methods, we intend to\nfocus on utilizing statistics related to structural elements to\ngenerate features for subsequent classification. These features\nencompass complexity metrics, phrase length ratio, average\nphrase length, and more. Statistics of structural elements have\nbeen effective in evaluating the linguistic quality of text [21].\nWe implement the following syntactic features:\n\u2022\nComplexity Metrics: Complexity metrics can be calcu-\nlated by measuring the differences from the perspective\nof sentence complexity as humorous and non-humorous\ntexts may differ in the way they express intentions.\nTherefore, the number of noun phrases (NP count), verb\nphrases(VP count), prepositional phrases (PP count), and"}, {"title": "Phrase Length Ratio", "content": "The length ratio (LR) for PP, NP,\nand VP is individually calculated. This ratio represents\nthe number of words in each phrase type divided by the\ntotal sentence length."}, {"title": "Avergae Phrase Length", "content": "The average phrase length is\ndetermined by dividing the total number of words within\neach phrase by the respective number of phrase types.\nIt's essential to note two distinct calculation methods: one\n(AP L1) accounts for nested phrases. For instance, in a\nVP phrase (VP1...(...VP2...)), the VP's length equals the\nsum of the lengths of VP1 and VP2. The other approach\n(AP L2) considers only the maximum phrase length,\nwhere the phrase length is determined by the length of\nVP1."}, {"title": "Ratio of PP or NP within a VP (RP NV)", "content": "If a VP\nencompasses NP or PP, this value corresponds to the\naverage length of NP or PP divided by the length of VP.\nWe make use of the NLTK package to obtain Statistics\nof Structural Elements and these features can be called SSE\nfeatures (SSE)."}, {"title": "B. Semantic information", "content": "Among the 12 humor theories [11], also known as humor\nstructures, [32] we explore the latent semantic structures\nbehind humor in three aspects: (a) Incongruity, (b) Ambiguity\n(c) Phonetic Style."}, {"title": "Incongruity Structure", "content": "Laughter often arises from the union of seemingly con-\ntradictory or incongruous elements, forming a unique\nrelationship or assemblage within the mind (Lefcourt,\n2001) [14]. The crux of humor lies in the incongruity,\nthe separation of one idea from another (Paulos, 2008)\n[10]. Humor often thrives on specific types of incongruity,\nlike opposition or contradiction. We extract two types of\nfeatures to assess the meaning distance between pairs of\ncontent words in a sentence by leveraging Word2Vec [18]\nto gauge the semantic connections within a sentence. We\ndescribe incongruity through the following two features:\nDisconnection: Representing the maximum mean-\ning (semantic) distance among word pairs in a sentence.\nRepetition: Representing the minimum meaning\n(semantic) distance among word pairs in a sentence."}, {"title": "Ambiguity Theory", "content": "Humor and ambiguity often come together when the\nlistener tries to interpret the meaning. Ambiguity arises\nwhen the words in a sentence can be grouped in multiple\nways, resulting in various underlying interpretations. For\nexample:\nI saw the man on the hill with the telescope.\nDifferent possible meanings of words lead to diverse\nunderstandings for readers. To capture this ambiguity\nwithin a sentence, we employ WordNet and assess it as\nfollows:\nSense Combination: This computation involves\nidentifying Nouns, Verbs, Adjectives, and Adverbs\nthrough a POS tagger [4]. Subsequently, we consider the\npotential meanings of these words (wl, w2, ,wk) via\nWordNet [8], calculating the sense combinations as\nlabeleueqn log$\n\\frac{\\Pi \\pi(sense)}{\\text{senses}}$\n) (1)\nSense Farmost: the largest Path Similarity of any\nword senses in a sentence.\nSense Closest: the smallest Path Similarity of any\nword senses in a sentence."}, {"title": "Phonetic Style", "content": "Mihalcea et al. [17] suggests that the phonetic character-\nistics of humorous sentences hold significant importance\nalongside their content. Many one-liners exhibit linguistic\nelements like alliteration, word repetition, and rhyme,\ncreating a comedic effect regardless of whether the joke is\nactually funny. An alliteration chain involves consecutive\nwords starting with the same sound, while a rhyme\nchain comprises words ending with the same syllable.\nTo extract these phonetic features, we utilize the CMU\nPronouncing Dictionary [29] and create four features:\nAlliteration: quantifies the count and maximum\nlength of alliteration chains within a sentence.\nRhyme: measures the count and maximum length\nof rhyme chains.\nThese can be called Human Theory Driven Features (HTF)."}, {"title": "C. Contextual information", "content": "By looking at the general structure of a joke to understand\nthe underlying linguistic features that make a text laughable,\nMany suggested that humor arises from the sudden trans-\nformation of an expectation into nothing. In this way, the\npunchline, as the last part of a joke, destroys the perceiver's\nprevious expectations and brings humor to its incongruity. The\npunchline is related to previous sentences but is included in\nopposition to previous lines in order to transform the reader's\nexpectation of the context.\nThe proposed method for humor detection focuses on the\nstructure of humor in text. It observes that individual sentences\nin a joke may seem normal and non-humorous when read\nseparately but become humorous when considered together\nin context. To capture this, the method employs the model as\ndescribed by Colbert [3] a neural network architecture with\nseparate paths for sentence-level and whole-text features. The\nprocess involves tokenizing and encoding sentences using\nBERT sentence embeddings [7], followed by parallel hidden\nlayers to extract mid-level features for each sentence.\nEach sentence segment is given max_sentence_length as\n20 tokens and a maximum of 5 segments are considered for\nan individual joke. For the whole sentence, 100 tokens are\nconsidered. The model aims to detect relationships between\nsentences, especially the punchline's connection to the rest,\nand examines word-level connections in the entire text.\nThese BERT sentence embeddings [7] are generated by in-\nputting these tokens into the BERT model, resulting in vectors\nof size 768. The model involves eight neural network layers,\nwith each sentence processed through a separate parallel line\nof three hidden layers. These layers are concatenated in the\nfourth layer and continue sequentially to predict a single\ntarget value. The implementation utilizes the huggingface [31]\nlibrary for BERT and keras.tensorflow [1] for the neural\nnetwork. Notably, training occurs solely on the neural network,"}, {"title": "D. Combination of Syntactic, Semantic with ColBERT", "content": "In our modifications we firstly combined all the previously\ndescribed syntactical, semantic, and contextual information\nobtained from NRC, HTF into one feature list of 33 features\nfor 200k inputs. These features were given to parallel hidden\nlayers and then concatenated with the Bert embeddings\nobtained from Colbert data preprocessing passed through the\nDense neural network layers. The model is finalized with\nthree sequential layers of a neural network that combine the\noutputs from all previously hidden layers. These dense layers\noutputs 104 feature vectors from Bert embeddings and 104\nfrom syntactic and semantic were then concatenated together\nand passed through final dense layers.The output was then\npassed through a sigmoid activation function. The model was\ntrained with pre-trained Colbert parameters using our feature\nmodifications. Adam optimizer and Binary Cross entropy loss\nwere used for training and the modified model was trained\nfor 10 epochs with a batch size of 64. These final layers aim\nto determine sentence congruity and detect changes in the\nreader's viewpoint after reading the punchline."}, {"title": "V. ALTERNATIVES AND JUSTIFICATION", "content": "Our proposed method aims to assess whether a sentence\nis humorous or not by considering three types of information\nwithin it. While there exist traditional methods such as SVM\n[9] and XGBoost,[6]and simpler sequence-to-sequence models\nlike LSTM-based ones[25], the complexity of humor detection\nmay challenge their ability to understand the full context. In\ncontrast, more advanced models like BERT can handle this\ncomplexity better. There can also be graph-based approaches\nto find the relationship between sentences and words before\ntraining.\nBERT [7] distinguishes different meanings for particular\nwords based on the context by using contextualized embed-\ndings. For example, even though the word \u201cstick\u201d could be\nboth a noun as well as a verb, normal word embeddings assign\nthe same vector to both meanings. BERT is trained in a self-\nsupervised way by predicting missing words in sentences, and\npredicting if two randomly chosen sentences are subsequent or\nnot. Previously, there was an attempt to use BERT [30][7]for\nhumor detection, and it worked well on the training data.\nHowever, it didn't consider syntactical information. The only\nresult provided was its performance on a Spanish Tweet\nDataset, where it performed significantly worse (about a 15%\ndrop)."}, {"title": "A. Challenges Encountered", "content": "Training the COLBERT model was both interesting and\nchallenging. The biggest hurdle was needing a lot of computer\npower, like high-performance GPUs. The training also took a\nlong time because the model is complex. Making changes to\nCOLBERT for specific tasks required tweaking its structure.\nThese challenges taught us a lot about large language models\nand their model layer specifications. It also showed that\nmanaging costs is crucial. Overall, the project highlighted how\nvital it is for models to be both powerful and flexible.\nWhile creating features for ambiguity, we encountered sev-\neral challenges. The ambiguity structure includes the sense\ncombination score, farmost, and closest path similarity. During\nthe process of determining path similarity using WordNet [8],\nwe grappled with utilizing the Synonym set for a given word.\nWordNet contains synsets for words in various forms like\nnouns, verbs, etc. Our objective was to exclusively identify\nnoun meanings for a given noun, which posed a signifi-\ncant challenge. Obtaining information about path similarity\nturned out to be a time-consuming process. Programming the\nformulas mentioned in previous literature also presented a\nchallenging task. Surprisingly, we anticipated that semantic\nfeatures would yield higher accuracy compared to syntactic\nfeatures, given our focus on understanding the meanings of\nwords and sentences; however, the opposite turned out to be\ntrue.\nWe believe that combining syntactical information with\ncontextual and semantic information can lead to better perfor-\nmance on unseen data. Humor often involves a mix of syntactic\ncues, semantic meaning, and context, and considering all these\naspects should improve the model's accuracy in detecting\nhumor."}, {"title": "VI. EVALUATION", "content": "To evaluate the effectiveness of our approach for humor\ndetection we employed a range of evaluation metrics. Accu-\nracy, F1 score, and ROC-AUC are fundamental metrics to\ngauge the model's overall performance, its balance between\nprecision and recall, and its ability to distinguish between\nhumorous and non-humorous instances. During training we\nemployed different methodologies, for all the models not\ninvolving BERT in any way, the data was split into train and\nval, and the hyperparameters were set using the val data. For\nBERT the whole dataset was used to finetune. To evaluate its\nperformance on unseen data, we scraped data from a website\n[22] and also used a sample of jokes scraped from Reddit by\na third-party [23]."}, {"title": "A. Results", "content": "We primarily assess our hand-crafted features based on\naccuracy and the F1 measure. The accuracy of the combined\nfeatures, encompassing all three types, ranked the highest,\nfollowed by the syntactic features. These outcomes are il-\nlustrated in the table below. Evaluating BERT and Colbert\nmodels based on accuracy, we observed a consistent trend\nwhere the performance on unseen data was lower compared\nto the training data. These findings are also presented in the\ntable below."}, {"title": "VII. MAIN FINDINGS", "content": "We've discovered intriguing nuances in how features rep-\nresenting emotions and semantics help the models in humor\ndetection. Looking at the SHAP and Decision trees it was\nclear that it is easier for machines to detect humor based\non words that exhibit emotions such as positive, anticipation,\nsurprise, and trust. It is quite intriguing to see anticipation\nand surprise as they are two fundamental features that affect\nwhether something is funny or not. Upon looking at the\nstatistics of the syntactical elements, it seems VP_count is\none feature that helps the models to understand if a sentence\nwould be funny or not, and even after combining all the 33\nfeatures and feeding it into the model, it seems that VP_count\nis still what provides more understanding the model. From\nthe features obtained from NRCLex, only the surprise feature\nseems to be present in the top 5 features, which is surprising\nas we expected anticipation to be present as well. But what\nwasn't surprising was the domination of semantic features over\nsyntactic, which is quite understandable as you cannot take\nsomething at its face value, just because a sentence contains\nsyntax that might contain emotions projecting humor, the\nwhole sentence need not be humorous, compared to a sentence\nhaving a funny meaning is highly probable to be funny. But\non standalone terms, embeddings that capture the contextual\ninformation, and are produced by models that are pre-trained\non a huge corpus dominate every other feature. Does it mean\nthis experimentation was a waste of time? No. Combining\nthe above-mentioned features with the embeddings obtained\nfrom models such as BERT, resulted in higher quality which\nperformed better on the downstream classification task, this\ncan be seen even in the results. Overall there is comparable"}, {"title": "C. Bias", "content": "Our model is trained on just a single dataset, while it is a\ncollection of one-liners, they all have a similar pattern. While\nit does perform equally well on unseen data, we believe it is\nbeneficial to train on multiple datasets, and on different types\nof humor for better generalization."}, {"title": "D. Problem Formulation", "content": "Our primary goal is to predict humor based on a given\nsentence. However, it's important to note that humor can\nsometimes be offensive or insensitive, and our current model\nmight not recognize this aspect. It is crucial for the model to\npromptly identify offensive jokes and flag them. This proactive\napproach is intended to prevent any negative repercussions or\nbacklash that may arise from the use of inappropriate content."}, {"title": "E. Explainabilty", "content": "The hand-crafted features can be used to explain part of the\nmodel's high performance, but the inherent black-box nature\nof the BERT/ANNs is still present as the contextual features\ncannot be explained to stakeholders properly why the model\nis succeeding/failing."}, {"title": "IX. PREREQUISITES TO APPLY IN PRACTICE", "content": "We have made the entire codebase open source and ready\nto use. The code is hosted and available to use on GitHub.\u00b9.\nThe only requirement for it to be able to work in practice is\nthat the jokes should be in a text-like format to feed into the\nmodel. This is because, all the features are extracted from the\nsentence, and there is no pre-processing required to be done\non the user's side."}, {"title": "X. FUTURE WORK AND IMPROVEMENTS", "content": "Despite incorporating a range of one-liners from our dataset,\nthe model's performance fell short of our expectations for\nunseen data. Even after experimenting with fine-tuning the\nmodel, ensuring the robustness of new data remained a\nchallenge. This issue could stem from potential overfitting,\nespecially for the BERT and Colbert models, which might not\nadequately handle structures beyond their design. One poten-\ntial solution is to amalgamate multiple datasets and reassess\nthe model's performance. Developing nuanced datasets could\nbe instrumental in addressing this challenge. Additionally, ex-\nploring larger models like GPT [28] could offer an alternative\napproach."}, {"title": "XI. CONCLUSION", "content": "In this study, we tested how well a model performs when we\ncombine hand-crafted features with contextual embeddings.\nBy examining jokes, we created features based on sentence\nstructure and meaning, discovering that these features can\nenhance the model's ability to recognize humor. Our analysis\nrevealed that humorous texts often: 1) use straightforward\nwords in complex sentence structures, 2) include vivid ele-\nments like adverbs, and rhymes 3) relate closely in meaning\nwithin sentences, and 4) combine different word senses. These\nfindings highlight the specific style elements found in humor."}]}