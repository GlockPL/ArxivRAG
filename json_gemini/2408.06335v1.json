{"title": "LOLgorithm: Integrating Semantic, Syntactic and Contextual Elements for Humor Classification", "authors": ["Tanisha Khurana", "Vikram Pande", "Kaushik Pillalamarri", "Munindar Singh"], "abstract": "This paper explores humor detection through a linguistic lens, prioritizing syntactic, semantic, and contextual features over computational methods in Natural Language Pro- cessing. We categorize features into syntactic, semantic, and contextual dimensions, including lexicons, structural statistics, Word2Vec, WordNet, and phonetic style. Our proposed model, Colbert, utilizes BERT embeddings and parallel hidden layers to capture sentence congruity. By combining syntactic, semantic, and contextual features, we train Colbert for humor detection. Feature engineering examines essential syntactic and semantic features alongside BERT embeddings. SHAP interpretations and decision trees identify influential features, revealing that a holistic approach improves humor detection accuracy on unseen data. Integrating linguistic cues from different dimensions enhances the model's ability to understand humor complexity beyond traditional computational methods.", "sections": [{"title": "I. PROBLEM DESCRIPTION", "content": "Humor is a fascinating and puzzling area of study in the field of computers understanding human language. In recent times, computers have transformed from being simple machines that follow instructions to becoming smart agents that interact with people and learn from them. When a computer talks to a person, if it can understand and appreciate the humor in what the person is saying, it can grasp the real meaning of human language better. This, in turn, helps the computer make better decisions to enhance the user's experience. So, working on techniques that allow computers to \"get\" humor in human conversations and adjust their responses accordingly is something we should pay special attention to.\nHumor Recognition is about figuring out if a sentence is funny in a given situation. It's a tough problem in understand- ing human language. First, humor is tricky because different people may find different things funny even in the same sentence. Second, humor often depends on knowing a lot about the context. For example, think about these sentences: \"The one who invented the door knocker got a No Bell prize\" and \"Veni, Vidi, Visa: I came, I saw, I did a little shopping.\" To find them funny, you need to know a bit about culture and language. Lastly, there are many types of humor, like wordplay, irony, and sarcasm, but we don't have a clear way to categorize them all. So, it's almost impossible to create a computer program that can recognize all types of humor, just like humans can't always classify them perfectly either.\nIn this work, we formulate humor recognition as a classifica- tion task in which we distinguish between humorous and non- humurous instances. Exploring the syntactical structure in- volves leveraging Lexicons to capture sentiment counts within a sentence, while Statistics of Structural Elements (SSE) [15] encapsulates the statistical insights of Noun phrases, Word phrases, and more. Unveiling the semantic layers of humor delves into Word2Vec [18] embeddings, analyzing incon- gruity, ambiguity, and phonetic structures within sentences. Additionally, contextual information is harnessed through Col- BERT embeddings [3]. For each latent structure, we design a set of features to capture the potential indicators of humor."}, {"title": "A. Example Scenario", "content": "Imagine you work for a social media analytics company, and one of your clients is interested in understanding and quanti- fying humor within user-generated content on their platform. They want to identify and categorize humorous posts, as well as analyze the types of humor that resonate the most with their audience. Humor classification will be extremely beneficial for these scenarios.\nAmazon for example employs humor detection in product question-answering systems [35] as some products attract humor due to their unreasonable price and their peculiar functionality. Detecting humorous questions in such systems is important for sellers, to better understand user engagement with their products. It is also important to inform users about the flippancy of humorous questions, and that answers for such questions should be taken with a grain of salt."}, {"title": "II. RELEVANT LITERATURE", "content": "Many of the exciting studies on humor recognition are formulated as binary classification problems and try to identify jokes using a set of linguistic features. For example, Zhang et al. [33] developed various humor-related features based on humor theories, linguistic rules, and emotions. They used these features in a Gradient Boosting Regression Tree model to recognize humor. Yang et al. [32] tried to identify several semantic structures behind humor and design sets of features for each structure, they extracted anchors that enable humor in the sentence and then used a computational approach to recognize humor. They explored the types of latent semantic structure behind humor namely incongruity, ambiguity, inter- personal affect, and phonetic style. Liu et al. [15] extended this research by adding syntactical structures for humor detection such as statistics of structural elements, production rules, and dependency relations. Annamoradnejad et al. [3] have lever- aged the power of the transformer's embeddings to capture the contextual information from a given sentence."}, {"title": "A. DATASET", "content": "We employ two distinct datasets, for training and testing purposes:\nColBERT Dataset: The ColBERT dataset [2] has 200,000 statements with humor labels and is accessible on Kaggle for model training and evaluation. We partition it into training and validation sets.\nFor the unseen test set, we scraped funny one-liners from the Bestlife website to gather jokes. We preprocess the data by removing punctuation and lemmas and manually labeling the jokes as humorous or not for unseen datasets for model evaluation. Also, we've obtained around 2K samples of sen- tences from Reddit, out of which some are funny and some are not, which would also be used for further evaluating the model's performance on unseen data."}, {"title": "B. CODE", "content": "We use libraries such as PyTorch [24], TensorFlow, Hugging-Face [31], NLTK [4], SpaCy [12], NRCLex [19], Sklearn [26], and some inherent Python libraries to help in the coding part."}, {"title": "III. PROBLEM IMPORTANCE", "content": "Humor classification is a crucial problem and our solution takes into account the contextual, semantic, and syntactic meaning of the joke. Our problem has various applications in customer support and chatbot development. Many companies utilize conversational services to recommend items and coun- sel consumers. There is also a growing demand for emotional chatterbots for user's higher satisfaction in various commercial fields. Siri could become more 'human' if she had the ability to recognize social cues like humor and respond to them with laughter. As described in our example scenario, our problem will be useful in social media analytics and for question- answering systems for products on Amazon[35]. Our problem also has further usage when combined with speech as well. For example, sitcoms are written and performed primarily to cause laughter, so the ability to detect precisely why something will draw laughter is of extreme importance to screenwriters and actors [27]."}, {"title": "IV. METHODOLOGY", "content": "The aim of the project was to work more with linguis- tic features rather than exploring computational methods in the area of Natural Language Processing. For the features we chose to work with, we've categorized them into three types for experimentation: syntactic, semantic, and contextual features. Syntactic features help understand how words func- tion structurally and influence model predictions. Semantic analysis delves into word and sentence meanings. Exploring contextual dependencies within sentences is also crucial. We're generating these features using methods described later in this section and analyzing these features to identify the most influential ones driving model predictions using basic SHAP interpretations [16], and visualizing decision trees [5]. We then model these features independently and together by computational methods."}, {"title": "A. Syntactical information", "content": "1) Lexicons: In some cases, individual words can be inher- ently funny. Therefore, if a sentence contains amusing words, the entire sentence may be humorous. With this assumption in mind, our initial approach involves extracting syntactical information using lexicons [13] [20]. Specifically, we employ the NRC word emotion lexicons for this purpose. Utilizing this lexicon, we represent words as vectors, where each entry signifies the word's similarity to various emotions such as anger, anticipation, disgust, fear, joy, sadness, and surprise, as well as its similarity to the general sentiments of positivity and negativity. By employing these word representations, we aim to capture the syntactical information embedded within each word. To obtain syntactical embeddings for the entire sentence, we aggregate the vectors of each word in the sentence, thereby encapsulating the overall syntactical characteristics of the sentence. We make use of the available NRCLex library to measure the emotional affect of a body of jokes. We get the scores for emotions such as fear, anger, anticipation, trust, surprise, positive, negative, sadness, disgust, and joy for each joke.\n2) Statistics of Structural Elements: As described by Liu et al.[15] we follow the methods to acquire syntactic structures for humor recognition. Among these methods, we intend to focus on utilizing statistics related to structural elements to generate features for subsequent classification. These features encompass complexity metrics, phrase length ratio, average phrase length, and more. Statistics of structural elements have been effective in evaluating the linguistic quality of text [21]. We implement the following syntactic features:\n\u2022 Complexity Metrics: Complexity metrics can be cal- culated by measuring the differences from the perspective of sentence complexity as humorous and non-humorous texts may differ in the way they express intentions. Therefore, the number of noun phrases (NP count), verb phrases(VP count), prepositional phrases (PP count), and subordinating conjunctions (SBAR count) are counted respectively as features.\n\u2022 Phrase Length Ratio: The length ratio (LR) for PP, NP, and VP is individually calculated. This ratio represents the number of words in each phrase type divided by the total sentence length.\n\u2022 Avergae Phrase Length: The average phrase length is determined by dividing the total number of words within each phrase by the respective number of phrase types. It's essential to note two distinct calculation methods: one (AP L1) accounts for nested phrases. For instance, in a VP phrase (VP1...(...VP2...)), the VP's length equals the sum of the lengths of VP1 and VP2. The other approach (AP L2) considers only the maximum phrase length, where the phrase length is determined by the length of VP1.\n\u2022 Ratio of PP or NP within a VP (RP NV): If a VP encompasses NP or PP, this value corresponds to the average length of NP or PP divided by the length of VP.\nWe make use of the NLTK package to obtain Statistics of Structural Elements and these features can be called SSE features (SSE)."}, {"title": "B. Semantic information", "content": "Among the 12 humor theories [11], also known as humor structures, [32] we explore the latent semantic structures behind humor in three aspects: (a) Incongruity, (b) Ambiguity (c) Phonetic Style.\n\u2022 Incongruity Structure\nLaughter often arises from the union of seemingly con- tradictory or incongruous elements, forming a unique relationship or assemblage within the mind (Lefcourt, 2001) [14]. The crux of humor lies in the incongruity, the separation of one idea from another (Paulos, 2008) [10]. Humor often thrives on specific types of incongruity, like opposition or contradiction. We extract two types of features to assess the meaning distance between pairs of content words in a sentence by leveraging Word2Vec [18] to gauge the semantic connections within a sentence. We describe incongruity through the following two features:\nDisconnection: Representing the maximum mean- ing (semantic) distance among word pairs in a sentence.\nRepetition: Representing the minimum meaning (semantic) distance among word pairs in a sentence.\n\u2022 Ambiguity Theory\nHumor and ambiguity often come together when the listener tries to interpret the meaning. Ambiguity arises when the words in a sentence can be grouped in multiple ways, resulting in various underlying interpretations. For example:\nI saw the man on the hill with the telescope.\nDifferent possible meanings of words lead to diverse understandings for readers. To capture this ambiguity within a sentence, we employ WordNet and assess it as follows:\nSense Combination: This computation involves identifying Nouns, Verbs, Adjectives, and Adverbs through a POS tagger [4]. Subsequently, we consider the potential meanings of these words (wl, w2, ,wk) via WordNet [8], calculating the sense combinations as\n$labeleueqn  log(\\frac{\\Pi_{sense}}{sense Esenses})$(1)\nSense Farmost: the largest Path Similarity of any word senses in a sentence.\nSense Closest: the smallest Path Similarity of any word senses in a sentence.\n\u2022 Phonetic Style\nMihalcea et al. [17] suggests that the phonetic character- istics of humorous sentences hold significant importance alongside their content. Many one-liners exhibit linguistic elements like alliteration, word repetition, and rhyme, creating a comedic effect regardless of whether the joke is actually funny. An alliteration chain involves consecutive words starting with the same sound, while a rhyme chain comprises words ending with the same syllable. To extract these phonetic features, we utilize the CMU Pronouncing Dictionary [29] and create four features:\nAlliteration: quantifies the count and maximum length of alliteration chains within a sentence.\nRhyme: measures the count and maximum length of rhyme chains.\nThese can be called Human Theory Driven Features (HTF)."}, {"title": "C. Contextual information", "content": "By looking at the general structure of a joke to understand the underlying linguistic features that make a text laughable, Many suggested that humor arises from the sudden trans- formation of an expectation into nothing. In this way, the punchline, as the last part of a joke, destroys the perceiver's previous expectations and brings humor to its incongruity. The punchline is related to previous sentences but is included in opposition to previous lines in order to transform the reader's expectation of the context.\nThe proposed method for humor detection focuses on the structure of humor in text. It observes that individual sentences in a joke may seem normal and non-humorous when read separately but become humorous when considered together in context. To capture this, the method employs the model as described by Colbert [3] a neural network architecture with separate paths for sentence-level and whole-text features. The process involves tokenizing and encoding sentences using BERT sentence embeddings [7], followed by parallel hidden layers to extract mid-level features for each sentence.\nEach sentence segment is given max_sentence_length as 20 tokens and a maximum of 5 segments are considered for an individual joke. For the whole sentence, 100 tokens are considered. The model aims to detect relationships between sentences, especially the punchline's connection to the rest, and examines word-level connections in the entire text.\nThese BERT sentence embeddings [7] are generated by in- putting these tokens into the BERT model, resulting in vectors of size 768. The model involves eight neural network layers, with each sentence processed through a separate parallel line of three hidden layers. These layers are concatenated in the fourth layer and continue sequentially to predict a single target value. The implementation utilizes the huggingface [31] library for BERT and keras.tensorflow [1] for the neural network. Notably, training occurs solely on the neural network, not the BERT model. The chosen BERT variant is the smaller- sized BERTBASE, with 12 layers, 768 hidden states, 12 heads, and 110 million parameters, pre-trained on lower-cased English text from BooksCorpus [34] and English Wikipedia. The final layers of the neural network integrate the outputs from all paths to predict the congruity of sentences and identify shifts in the reader's viewpoint after encountering the punchline.\nParallel hidden layers in a neural network process the BERT embeddings [7] for each sentence, extracting mid-level features related to context and sentence type, resulting in a 20-dimensional vector for each sentence."}, {"title": "D. Combination of Syntactic, Semantic with ColBERT", "content": "In our modifications we firstly combined all the previously described syntactical, semantic, and contextual information obtained from NRC, HTF into one feature list of 33 features for 200k inputs. These features were given to parallel hidden layers and then concatenated with the Bert embeddings obtained from Colbert data preprocessing passed through the Dense neural network layers. The model is finalized with three sequential layers of a neural network that combine the outputs from all previously hidden layers. These dense layers outputs 104 feature vectors from Bert embeddings and 104 from syntactic and semantic were then concatenated together and passed through final dense layers.The output was then passed through a sigmoid activation function. The model was trained with pre-trained Colbert parameters using our feature modifications. Adam optimizer and Binary Cross entropy loss were used for training and the modified model was trained for 10 epochs with a batch size of 64. These final layers aim to determine sentence congruity and detect changes in the reader's viewpoint after reading the punchline."}, {"title": "V. ALTERNATIVES AND JUSTIFICATION", "content": "Our proposed method aims to assess whether a sentence is humorous or not by considering three types of information within it. While there exist traditional methods such as SVM [9] and XGBoost,[6]and simpler sequence-to-sequence models like LSTM-based ones[25], the complexity of humor detection may challenge their ability to understand the full context. In contrast, more advanced models like BERT can handle this complexity better. There can also be graph-based approaches to find the relationship between sentences and words before training.\nBERT [7] distinguishes different meanings for particular words based on the context by using contextualized embed- dings. For example, even though the word \u201cstick\u201d could be both a noun as well as a verb, normal word embeddings assign the same vector to both meanings. BERT is trained in a self- supervised way by predicting missing words in sentences, and predicting if two randomly chosen sentences are subsequent or not. Previously, there was an attempt to use BERT [30][7]for humor detection, and it worked well on the training data. However, it didn't consider syntactical information. The only result provided was its performance on a Spanish Tweet Dataset, where it performed significantly worse (about a 15% drop)."}, {"title": "A. Challenges Encountered", "content": "Training the COLBERT model was both interesting and challenging. The biggest hurdle was needing a lot of computer power, like high-performance GPUs. The training also took a long time because the model is complex. Making changes to COLBERT for specific tasks required tweaking its structure. These challenges taught us a lot about large language models and their model layer specifications. It also showed that managing costs is crucial. Overall, the project highlighted how vital it is for models to be both powerful and flexible.\nWhile creating features for ambiguity, we encountered sev- eral challenges. The ambiguity structure includes the sense combination score, farmost, and closest path similarity. During the process of determining path similarity using WordNet [8], we grappled with utilizing the Synonym set for a given word. WordNet contains synsets for words in various forms like nouns, verbs, etc. Our objective was to exclusively identify noun meanings for a given noun, which posed a signifi- cant challenge. Obtaining information about path similarity turned out to be a time-consuming process. Programming the formulas mentioned in previous literature also presented a challenging task. Surprisingly, we anticipated that semantic features would yield higher accuracy compared to syntactic features, given our focus on understanding the meanings of words and sentences; however, the opposite turned out to be true.\nWe believe that combining syntactical information with contextual and semantic information can lead to better perfor- mance on unseen data. Humor often involves a mix of syntactic cues, semantic meaning, and context, and considering all these aspects should improve the model's accuracy in detecting humor."}, {"title": "VI. EVALUATION", "content": "To evaluate the effectiveness of our approach for humor detection we employed a range of evaluation metrics. Accu- racy, F1 score, and ROC-AUC are fundamental metrics to gauge the model's overall performance, its balance between precision and recall, and its ability to distinguish between humorous and non-humorous instances. During training we employed different methodologies, for all the models not involving BERT in any way, the data was split into train and val, and the hyperparameters were set using the val data. For BERT the whole dataset was used to finetune. To evaluate its performance on unseen data, we scraped data from a website [22] and also used a sample of jokes scraped from Reddit by a third-party [23]."}, {"title": "A. Results", "content": "We primarily assess our hand-crafted features based on accuracy and the F1 measure. The accuracy of the combined features, encompassing all three types, ranked the highest, followed by the syntactic features. These outcomes are il- lustrated in the table below. Evaluating BERT and Colbert models based on accuracy, we observed a consistent trend where the performance on unseen data was lower compared to the training data. These findings are also presented in the table below."}, {"title": "VII. MAIN FINDINGS", "content": "We've discovered intriguing nuances in how features rep- resenting emotions and semantics help the models in humor detection. Looking at the SHAP and Decision trees it was clear that it is easier for machines to detect humor based on words that exhibit emotions such as positive, anticipation, surprise, and trust. It is quite intriguing to see anticipation and surprise as they are two fundamental features that affect whether something is funny or not. Upon looking at the statistics of the syntactical elements, it seems VP_count is one feature that helps the models to understand if a sentence would be funny or not, and even after combining all the 33 features and feeding it into the model, it seems that VP_count is still what provides more understanding the model. From the features obtained from NRCLex, only the surprise feature seems to be present in the top 5 features, which is surprising as we expected anticipation to be present as well. But what wasn't surprising was the domination of semantic features over syntactic, which is quite understandable as you cannot take something at its face value, just because a sentence contains syntax that might contain emotions projecting humor, the whole sentence need not be humorous, compared to a sentence having a funny meaning is highly probable to be funny. But on standalone terms, embeddings that capture the contextual information, and are produced by models that are pre-trained on a huge corpus dominate every other feature. Does it mean this experimentation was a waste of time? No. Combining the above-mentioned features with the embeddings obtained from models such as BERT, resulted in higher quality which performed better on the downstream classification task, this can be seen even in the results. Overall there is comparable"}, {"title": "VIII. LIMITATIONS", "content": "A. Absence of Audio Data\nThe task of humor classification, especially when dealing with textual data, is a complex and challenging task due to several factors and One of the main limitations in this project is not having audio along with text. A sentence can have multiple interpretations, some interpretations might be humorous while some might not.\nFor example, take the sentence: \"Did you hear about the guy whose whole left side was cut off? He's all right now.\" The humor here relies on how you say it, and if it's not said right, it might not be funny and could even sound insensitive. This kind of ambiguity is tough for people to navigate, and it's even harder for a model.\nAudio can capture things like tone and pronunciation, making it easier for the model to understand and recognize humor. So, adding audio might just help our model do a better job at figuring out what's funny.\nB. Feature Selection of Hand Crafted Features\nOut of several features mentioned in [15][32]we finalized to work with only 33 features, while more features don't mean better performance, we can't help but wonder if they might perform better than the ones selected.\nWhen we performed feature selection on the 33 features, we used SHAP and Decision Tree Classifier, while both are frequently used practices, further experimentation can be done with methods such as Correlation Analysis, Recursive Feature elimination, Recursive Feature addition, etc.\nC. Bias\nOur model is trained on just a single dataset, while it is a collection of one-liners, they all have a similar pattern. While it does perform equally well on unseen data, we believe it is beneficial to train on multiple datasets, and on different types of humor for better generalization.\nD. Problem Formulation\nOur primary goal is to predict humor based on a given sentence. However, it's important to note that humor can sometimes be offensive or insensitive, and our current model might not recognize this aspect. It is crucial for the model to promptly identify offensive jokes and flag them. This proactive approach is intended to prevent any negative repercussions or backlash that may arise from the use of inappropriate content.\nE. Explainabilty\nThe hand-crafted features can be used to explain part of the model's high performance, but the inherent black-box nature of the BERT/ANNs is still present as the contextual features cannot be explained to stakeholders properly why the model is succeeding/failing."}, {"title": "IX. PREREQUISITES TO APPLY IN PRACTICE", "content": "We have made the entire codebase open source and ready to use. The code is hosted and available to use on GitHub.\u00b9. The only requirement for it to be able to work in practice is that the jokes should be in a text-like format to feed into the model. This is because, all the features are extracted from the sentence, and there is no pre-processing required to be done on the user's side."}, {"title": "X. FUTURE WORK AND IMPROVEMENTS", "content": "Despite incorporating a range of one-liners from our dataset, the model's performance fell short of our expectations for unseen data. Even after experimenting with fine-tuning the model, ensuring the robustness of new data remained a challenge. This issue could stem from potential overfitting, especially for the BERT and Colbert models, which might not adequately handle structures beyond their design. One poten- tial solution is to amalgamate multiple datasets and reassess the model's performance. Developing nuanced datasets could be instrumental in addressing this challenge. Additionally, ex- ploring larger models like GPT [28] could offer an alternative approach."}, {"title": "XI. CONCLUSION", "content": "In this study, we tested how well a model performs when we combine hand-crafted features with contextual embeddings. By examining jokes, we created features based on sentence structure and meaning, discovering that these features can enhance the model's ability to recognize humor. Our analysis revealed that humorous texts often: 1) use straightforward words in complex sentence structures, 2) include vivid ele- ments like adverbs, and rhymes 3) relate closely in meaning within sentences, and 4) combine different word senses. These findings highlight the specific style elements found in humor."}]}