{"title": "Ranking Joint Policies in Dynamic Games using Evolutionary Dynamics", "authors": ["Natalia Koliou", "George Vouros"], "abstract": "Game-theoretic solution concepts, such as the Nash equilibrium, have been key to finding stable joint actions in multi-player games. However, it has been shown that the dynamics of agents' interactions, even in simple two-player games with few strategies, are incapable of reaching Nash equilibria, exhibiting complex and un-predictable behavior. Instead, evolutionary approaches can describe the long-term persistence of strategies and filter out transient ones, accounting for the long-term dynamics of agents' interactions. Our goal is to identify agents' joint strategies that result in stable be-havior, being resistant to changes, while also accounting for agents' payoffs, in dynamic games. Towards this goal, and building on pre-vious results, this paper proposes transforming dynamic games into their empirical forms by considering agents' strategies instead of agents' actions, and applying the evolutionary methodology a-Rank to evaluate and rank strategy profiles according to their long-term dynamics. This methodology not only allows us to identify joint strategies that are strong through agents' long-term interactions, but also provides a descriptive, transparent framework regarding the high ranking of these strategies. Experiments report on agents that aim to collaboratively solve a stochastic version of the graph coloring problem. We consider different styles of play as strate-gies to define the empirical game, and train policies realizing these strategies, using the DQN algorithm. Then we run simulations to generate the payoff matrix required by a-Rank to rank joint strategies.", "sections": [{"title": "1 INTRODUCTION", "content": "Game theory studies agents' strategies not only in terms of opti-mality of performance but also with regard to stability of agents' behavior. Game-theoretic solution concepts, particularly the Nash equilibrium, have played an important role in this research. However, solution concepts do not account for the long-term dynamics of agents' interactions, which are important in dynamic settings. In static games, where payoff matrices are known, studying solution concepts is relatively straightforward. For example, the mixed strat-egy Nash equilibrium for the Rock-Paper-Scissors game shown in Table 1 occurs when both players randomize their choices uniformly across Rock, Paper, and Scissors. Accounting for the dynamics of agents' interactions over time in dynamic settings, we need to an-alyze agents' behavior in terms of their payoffs, identifying joint strategies that result into agents' stable behaviors. Evolutionary approaches have shown great potential towards this aim.\nTo study agents' behavior in multi-agent dynamic settings, re-searchers often train deep learning models to learn joint policies. These models, either in collaborative or competitive settings, are usually trained with the ultimate objective to result into Nash equi-libria, aiming to agents' stability of behavior, where no agent has an incentive to deviate from their joint policy. In complex dynamic settings with long-term dynamics of agents' interactions, there is no guarantee of reaching that objective and there is no way to reveal the reasoning behind the agents' choice of a policy instead of another. Although proposals towards explainability and inter-pretability of models are important, these aim to provide either explanations for the policy as a whole (i.e. agent's style of play) or about individual decisions. In our case, we need a descriptive framework to account for transparency regarding the strength of agents' policies, accounting for long-term dynamics.\nOur goal is to identify agents' individual policies that result in stable (i.e., resistant to changes) behavior while playing with others, accounting for long-term agents' interactions and agents' payoffs, in dynamic games. This is motivated by the need to identify strate-gies of human or software agents that need to act as co-players in a common setting, considering that these agents have policies that have been formed independently: This is in contrast to assuming that agents have been trained to learn a joint policy. We conjec-ture that using a descriptive evolutionary framework approach helps agents select strong (i.e. non-transient) policies when play-ing with or against other agents in dynamic settings. Identifying these policies and understanding their \"superiority\" is important,"}, {"title": "2 BACKGROUND", "content": "In this section, we outline the key concepts necessary to follow the proposed approach.\n2.1 Dynamic Games\nDynamic games describe agent interactions along the time dimen-sion. Unlike static games, where players execute single one-shot actions, dynamic games involve a series of decisions made by each of the players at subsequent points in time. A key property of dynamic games is that the actions taken at any given moment in-fluence the future states of the system and future decisions made. These temporal dependencies require players to consider the long-term consequences of their actions. A dynamic game can be repre-sented as a tuple G = (S, K, A, T, P), where S represents a finite set of states, K is the set of players, and A = (Ak \u00d7 A\u00afk) is the set of joint actions, where Ak corresponds to the set of actions available to the player k. A\u00afk denotes the set of actions available to players other than k. The transition function T describes the dynamics of the setting, determining the next state of the system based on the current state and the actions chosen by the players. Finally, pk : S \u00d7 (Ak \u00d7 A-k) \u00d7 S \u2192 R is the payoff function for player k, given the current joint state, the action chosen by player k and the actions of the other agents, and the resulting state.\nIn this work we focus on stochastic dynamic games, as intro-duced by L.S. Shapley in 1953 [16]. In stochastic games, the outcome of players' actions is influenced by probabilistic events, making future states of the game uncertain. These games are often referred to as Markov games [17]. Therefore, in stochastic games, the tran-sition function T is defined as a probability distribution over next states. Specifically, $T : S \\times A \\rightarrow \\Delta(S)$, where \u0394(S) is a probability distribution over the states, given a state and joint action. For ex-ample, in poker, while players' actions do influence the outcome, the next state of the game also depends on luck, such as drawing a strong hand like a flush or a weak hand like a pair of twos. In such games, players, when planning their actions, must account for both the actions of their opponents and the dynamics of the environment.\nIn dynamic games, players aim to decide on the course of their joint actions through time (joint policy) to maximize their accumu-lated rewards over time:\n$\n\\sum_{(s_t, (a^k_t, a^{-k}_t), S_{t+1})} T(s_t, (a^k_t, a^{-k}_t), S_{t+1}) \\cdot p_k(s_t, (a^k_t, a_{-k}), S_{t+1})\n$\nHere, T specifies the transition probability from state st to the state st+1 given (a, a\u012bk), and pk (st, (ak, a\u2212k), st+1) is the reward the player receives for choosing action ak, given the actions a\u00afk of the other players, at state st, and resulting into state st+1.\n2.2 Empirical Analysis and Empirical Games\nEmpirical Game Theory Analysis (EGTA) provides a framework that uses empirical methods to analyze player interactions within complex game environments [10]. These methods are used to define game components, such as payoff matrices, based on observed interactions, rather than relying on predefined rules. Simulation is one such method, where agents repeatedly play a game, and payoffs are collected based on the outcomes of these interactions. Other techniques include sampling, where a subset of the action space is explored to approximate the payoffs for a wider set of actions, and machine learning methods to identify players' behavior and estimate outcomes based on historical data [27]. Empirical techniques are applied in cases where the action space is too large and complex to define manually, making payoff matrices impossible to generate from simple rules and assumptions.\nAn empirical game, also referred as a meta-game, is a Normal Form Game of the form G = (K, Str, P), where K, Str and P specify players, players' strategies, and payoffs, correspondingly. We define empirical games by abstracting the actions and defining the payoffs of players in an underlying dynamic game. The underlying game represents the actual setting where players interact. In the empirical game representation, K is the same as in the underlying game, i.e. the set of players engaged in strategic interactions. Strategies (i.e. styles of playing the game) in empirical games offer an action ab-straction and can be derived by identifying distinct behaviors during game-play. The strategy space Str consists of distinct agents' styles of play. Strk denotes the strategies of agent k and Str\u2212k the set of strategies of agents other than k. The set of strategy profiles, i.e. agents' joint strategies, is defined to be $SP = \\{S_i|S_i = (str^1, str^2, ..., str^K)\\}$, where $str \\in Str^k$, and i = 1,... the profile index}. The payoff matrix of an empirical game can be generated using empirical analysis techniques. Here, we focus on simulation, where agents engaged in the underlying game act according to policies adhering to specific strategies.\nSubsequently, we use the terms action and policy when speaking about the underlying game, and the term strategies or styles of play when speaking about the empirical game."}, {"title": "2.3 The a-Rank Method", "content": "Evolutionary dynamics studies how agents' interactions in multi-agent settings evolve over time. While single-agent systems have acquired a strong foundation over the years [3], multi-agent systems are more challenging to analyze.\nCurrent literature indicates a growing interest in studying the evolutionary dynamics of multi-agent systems [3] [6] [14]. In the context of games, evolutionary algorithms are widely used to ex-plore game-theoretic concepts, resulting to the Evolutionary Game Theory. Building on work done in this area, a-Rank [13] introduces a novel game-theoretic approach to provide insights into the long-term dynamics of agents' interactions.\na-Rank is an evolutionary methodology designed to evaluate and rank agents' strategies in large-scale multi-agent interactions, introducing a new dynamic solution concept called Markov-Conley chains (MCCs). Given a K-player game, a-Rank considers the empir-ical game with K player slots, called populations, where individual agents correspond to strategies, i.e. to styles of playing the underly-ing game. Populations of agents interact with each other through an evolutionary process following the dynamics of games. The rewards received from these interactions determine how well each strategy performs and, in turn, how often it is adopted by individuals in the populations. Strategies that perform well have a higher probability of being adopted and carried over to the next generation, while those performing poorly are less likely to be adopted. This process leads to the evolution of populations.\nTo facilitate evolution, a-Rank uses the concept of mutation. Ini-tially, populations are monomorphic, meaning all individuals within them choose the same strategy. During K-wise interactions, individ-uals have a small probability of mutating into different strategies or choosing to stick with their current one. The probability that the mutant will take over the population, defined to be the fixation prob-ability function p, depends on the relative fitness of the mutant and the population being invaded. Fitness is a function that computes the expected reward an individual can receive when adopting a particular strategy, given the strategies of the other individuals. The stronger the fitness, the more likely it is for individuals to mutate, whereas the lower the fitness, the more likely it is for the mutant to go extinct. When the mutation rate is small, we can assume that the fitness for any agent k is fk (strk, str-k) = pk (strk, str\u2212k), where P is the empirical game payoff.\nFormally, the probability of a mutant strategy str' fixating in some population where individuals play strategy str is given by:\n$p_{str \\rightarrow str'} = \\frac{1 - e^{-a \\cdot m \\cdot \\Delta f}}{1 - e^{-a \\cdot m \\Delta f}}$\nassuming that \u0394f is non-zero. \u0394f = fk (str', str-k)\u2212fk (str, str-k) represents the difference in fitness between the mutant strategy str' and the resident strategy str in the focal population k, while the remaining K \u2013 1 populations are fixed in their monomorphic strategies str-k. Parameter m is the population size and a is the selection intensity. This adjusts the sensitivity of the system to fitness differences: with higher values of a, even small differences in fitness lead to larger changes in p. The nominator measures the potential of the mutant to \"invade\" the resident population solely based on its fitness advantage. Note that, for example, as Af approaches zero, the probability of the mutant's success decreases.\nThe denominator, on the other hand, normalizes the fixation prob-ability using the population size m, making it more challenging for a mutant to dominate in larger populations. When Af is zero, the fixation probability is equal to 1/m ([13], eq.13), indicating that the mutant strategy has the same probability of taking over as any other strategy in the population. We refer to this probability as the neutral fixation probability, denoted by pm.\nIn the context of K-player games, a-Rank creates a Markov transi-tion matrix over strategy profiles. This is an |Str|\u00d7 |Str| matrix that defines the probability of moving from one strategy profile to another based on how likely each population is to change its strategy.\n$C_{str\\rightarrow str'} = \\begin{cases}\n\\eta \\cdot p_{str \\rightarrow str'} & \\text{if str } \\neq str' \\\\\n\\Sigma_{str}C_{str\\rightarrow str'} & \\text{otherwise}\n\\end{cases}$\nHere, C is the strategy-transition matrix where each entry Cstr\u2192str' represents the probability of transitioning from strategy str to strat-egy str'. The first part of the formula calculates the probability of strategy transition, $p_{str\\rightarrow str'}$ scaled by \u03b7 = \u03a3\u03ba (|Strk|-1), where k indexes populations. The second part of the formula computes the probability of staying with the same strategy str, excluding transitions to all other strategies.\nThis evolutionary process of competition and selection among play-ers' strategies leads to a unique stationary probability distribution \u03c0 of dimensionality |SP|, where the mass assigned to a strategy profile indicates how likely it is to resist being \"invaded\" by other strategies as the dynamics evolve. To evaluate and rank strategy profiles -which is the ultimate goal- the method calculates \u03c0 over the game's Markov chain, using the strategy-transition matrix C. This distribution indicates how often the system is likely to remain in each profile over time, allowing us to identify the most dominant strategies that are expected to prevail in the long run. Formally, \u03c0 can be computed from the following equation:\n$\u03c0C = \u03c0 \u21d2 \u03c0(C \u2013 I) = 0$\nwhere I is the identity matrix. This means we are looking for a probability vector \u03c0 such that when multiplied by the transition"}, {"title": "3 PROBLEM STATEMENT", "content": "As already stated, we aim at identifying (human and software) agents' strong joint strategies, in terms of stability and joint per-formance, to solve problems in dynamic settings, accounting for agents' long-term dynamics of interactions. Stability implies non-transient strong strategies, persisting in time, as they fit better to the objective of the agents given the structure of the game and payoffs received. However, in dynamic games, we need to define the payoff matrix and exploit this to determine strategies stability. Even if we manage to estimate payoffs, the computation of solution concepts like the Nash equilibrium imposes a high computational cost in these settings, does not guarantee convergence, and fails to scale to large games. Beyond identifying stable joint strategies, it is important to transparently justify/describe what makes one joint strategy better than another, providing evidence for the rankings.\nWe could, therefore, consider our problem as follows: Given a dynamic game G with K players, our goal is to identify styles of playing G, and thus, the set of strategy profiles SP, and rank these profiles based on how stable they are over time, considering long-term agents' interactions towards achieving their objectives. Specifically, we aim to define a ranking function R : SP \u2192 R, where R(Si) > R(Sj) (resp. R(Si) \u2265 R(Sj)) indicates that the strategy profile S\u012f is strictly (resp. weakly) preferred over Sj, us-ing a descriptive framework D defined over SP, that provides transparency on how rankings are decided.\nIt must be noted that empirical game strategies are realized by agents' policies adhering to these strategies in the underlying game. Thus, identifying stable joint strategies in the empirical game trans-lates to identifying stable joint policies adhering to these strategies in the underlying dynamic game."}, {"title": "4 PROPOSED METHOD", "content": "To address the challenge of identifying stable joint policies in dy-namic games, we propose an approach that combines concepts from Empirical Game Theory and Evolutionary Dynamics, using a-Rank, providing transparency to rankings of agent's styles of play.\nGiven that the set of agents' policies in dynamic games can be infinitely large we focus on a subset of policies that adhere to concrete and well-defined styles of play. A way to identify styles of play is to observe how players behave in the underlying game or exploit demonstrations of game playing by means of style (mode) -preserving offline or inverse reinforcement learning methods. This may result into a mixture of policies (one per style of play) given that human experts performing a task usually follow a distinct set of specific styles based on well-established practices, preferences and experience. Having determined the game playing strategies, we can transform the dynamic game into its empirical form, defining the meta-game, as specified in Section 2.2: By (a) identifying empirical game strategies, and (b) training policies for agents to play the underlying game according to these strategies, (c) run policies to define the empirical game payoff matrix, through simulations.\nHaving defined the meta-game, we need to define the function R, which ranks joint strategies based on agents' long-term dynamics and objectives. In our approach, we propose using the evolutionary a-Rank methodology to determine these rankings. The rankings are based on each strategy profile's evolutionary success, which is reflected in the probability of that profile being selected over time. This probability is captured by the stationary distribution \u03c0, which a-Rank computes in the limit of infinite ranking intensity a. As demonstrated by [13], a large a limit suffices. Therefore the long-term behavior is captured by the unique stationary distribution \u03c0 under the large a limit. As it is proved in [13], the Markov chain as-sociated with a generalized multi-population model, coincides with the MCC solution concept: MCCs can be identified effficiently in all games by the sink strongly connected components of a response graph, whose vertices correspond to pure strategies' profiles and directed edges from a strategy profile Si to a strategy profile Sj specifies that Sj is weakly a better response than Si for player k.\nTo compute \u03c0 over strategy profiles, a-Rank requires the payoff matrix of the empirical game P. Along with the stationary distribu-tion \u03c0, \u03b1-Rank outputs the fixation probability function PS\u2081\u2192Sj, Si, Sj \u2208 SP, which measures the likelihood of transitioning from one strategy profile Si to another Sj. Thus, a-Rank can be ab-stracted as a function:\n$\u03b1\\text{-Rank}(P) \\rightarrow (\u03c0, \u03c1)$\nWhile the stationary distribution \u03c0 provides valuable insight into the long-term behavior of strategies, it alone does not help us fully understand how strategies transition between one another. The fixation probability function p fills this gap. Based on this, the descriptive framework D can be adequately represented by \u03c0 and p, which are constituents of the response graph that provides a complete view of the empirical game dynamics.\nOverall, building on the a-Rank descriptive framework, the method proposed here for computing strategy profile rankings in dynamic games is as follows:\n(1) Identify players' styles of play.\n(2) Define the strategies of the empirical game based on those styles.\n(3) Train policies realizing the defined strategies.\n(4) Run game simulations to create the empirical payoff matrix P.\n(5) Apply a-Rank to define R and D:\n(a) Calculate the Markov transition matrix C.\n(b) Find the unique stationary distribution \u03c0.\n(c) Rank joint strategies by ordering the masses of \u03c0.\n(d) Describe the rankings through the response graph.\n(e) Study the effect of different a values on \u03c0."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "In this section, we present the experiments and discuss the results obtained from applying the proposed methodology to the Graph Coloring Game using the configuration shown in Figure 1. Appendix C provides results with additional grid configurations."}, {"title": "5.1 The Graph Coloring Problem", "content": "The well-known Graph Coloring Problem (GCP) involves assigning colors to vertices in a graph such that no two adjacent vertices share the same color, and using the minimum number of colors, also known as the chromatic number [25].\nIn this study, we shift our focus from finding the chromatic num-ber across graph configurations to solving the multi-agent problem of assigning colors to vertices of a dynamic graph with respect to the constraints: In doing so, we define the graph coloring problem as a dynamic game that allows us to study the evolutionary dy-namics in multi-agent interactions. This problem setting abstracts settings where agents need to abide to dynamically evolving con-straints while acting jointly. For instance, in traffic, any agent acts to abide to constraints, and these actions, given also the dynamics of the environment, result into new emerging constraints to which agents must adhere to, and so on. Through this problem, we aim to demonstrate how we can gain insights into the effectiveness of playing the dynamic game when individual styles of play are combined.\nWe consider the underlying graph coloring dynamic game to be a two-player game executed in rounds. The graph corresponds to a grid comprising blocks of cells: A block comprises one or more merged cells. Each vertex of the graph corresponds to a block, and the adjacency relation between blocks specifies the edges in the graph. At the beginning of the game, the grid is initialized with a random number of rows and columns (n \u00d7 m). In our experimen-tal setup we assume a 4 \u00d7 5 grid. The environment is initialized by randomly combining cells to create the blocks. The resulting configuration remains the same throughout the entire game. A snapshot of such a configuration with 10 blocks is the one shown in Figure 1, together with the corresponding graph. Merging cells is important as it allows for complex neighboring relationships to be defined, expanding beyond the standard constraints between ad-jacent blocks. Blocks are either (a) colored by the agents, (b) white (free to be colored), or (c) hidden (their colors cannot be observed and they cannot be recolored by the agents). Let B be the set of blocks corresponding to graph vertices and CR be the set of possible colors that an agent can use for coloring blocks in B. The game unfolds over multiple rounds in which agents choose their actions simultaneously. At the beginning of each round, the environment reveals the color of some of the hidden blocks, if any. The number of blocks that get unhidden is random, which implies that the state of the graph is influenced not only by the agents' actions but also by the environment. We therefore consider the game to be stochastic. When all blocks in B are uncovered and colored, the game ends. The set of agents' actions A is defined to be the Cartesian product of the set of blocks B and the set of the available colors CR:\n$A = B \\times CR = \\{(b, c) | b \\in B, c \\in CR\\}$\nTo specify states, let CR* include the elements of CR, and two additional elements representing hidden and white blocks: CR* = CRU {hidden, white}. A state s is as follows:\n$s = \\{(b_i, c_i), i = 1, ..., |B|\\},\ns.t.\\forall b \\in B, \\exists a \\text{ unique } c \\in CR^*, \\text{with } (b,c) \\in s$\nRegarding the reward function, it is a sum of gains, penalties, sanctions, delays and adopted preferences. Given that actions are represented as vectors of shape (b, c) \u2208 B \u00d7 CR, an agent receives a gain point (+1) for each neighbor of b that has a different color than the chosen color c. On the contrary, an agent receives a penalty point (-2) for each neighbor that shares the same color c. Sanction is a big negative reward (-10) that an agent receives when it attempts to color a hidden block or a block that has already been colored. Delay (-1) is a small negative reward that both agents receive when they try to color the same block b, causing a brief pause in the game to determine which agent will eventually color b. Last but not least, there is the preference-adoption reward, which agents receive regardless of whether their action is good, bad, or forbidden. This reward helps agents to be trained so as to adhere to specific preferences, or what we call styles of play. We will elaborate shortly on these in the following section."}, {"title": "5.2 Defining the Empirical Game", "content": "Transforming the underlying dynamic graph-coloring game into its empirical form involves two key steps: (1) identifying agents' strategies and (2) constructing the empirical game payoff matrix.\n5.2.1 Agents' Strategies. Agents' strategies define distinct styles of play, usually revealed by preferences in playing the game. In our experiments, we specify different styles across three main di-mensions: color tone (preference for which colors to use), block difficulty (preference for the types of blocks to choose), and col-oring approach (preference for the number of colors to use), as shown in Table 2. Through the combination of preferences in each of these dimensions, a style can range from complete indifference, where none of the dimensions hold any influence (denoted by \"I\"), to specific preferences in all dimensions.\nPolicies corresponding to specific strategies are represented us-ing convolutional neural networks. To train these policies, we assign specific values in the three dimensions of the game's preference re-ward. These values, range from 1 to 1, where 1 indicates a strong preference for a particular dimension. For example, a value of 0.7 for warm colors indicates a relatively high preference for warm tones. In our experimental setting, we define 11 distinct styles: I, C, W, E, M, L, A, AE, CA, LE and WL, given \"I\" and combinations of preference values specified in Table 2. Assuming no inherent bias"}, {"title": "5.2.2 Training the agents", "content": "All policy models share the same under-lying architecture and training setup. Although hyperparameter tuning is typically recommended, it does not make much differ-ence in this case, as these models are relatively easy to optimize when trained in small settings. Regarding the convolutional neu-ral network architecture, it consists of four convolutional layers, each defined with a kernel size of 3, stride of 1, and padding of 1, meant to extract spatial features from the input. The input tensor has dimensions 10 \u00d7 12, where |B| = 10 represents the number of blocks in the state and |CR* | = 12 represents the number of possi-ble colors a block can have. Each block is encoded using one-hot encoding, meaning that each color is represented as a binary vector of length 12. As shown in Figure 2, the network processes the input through four convolutional layers with respective output sizes 10 \u00d7 12 \u00d7 32, 10 \u00d7 12 \u00d7 64, 10 \u00d7 12 \u00d7 128, and 10 \u00d7 12 \u00d7 256. The final convolutional layer output is flattened to a vector of size 30720. This representation is passed through two fully connected layers: the first reduces the size to 512, and the second produces the final output, which corresponds to a matrix of size |CR| x |B| = 10 \u00d7 10 encoding the action space. Although the architecture seems to be excessive, it aims at efficient policy training and generalization, by extracting meaningful relationships among state components. This claim is justified in Appendix D.\nPolicy models are trained individually (without co-players) in the underlying game using the deep Q-learning reinforcement learning algorithm specified in Algorithm 1. We set y to 0.7. To optimize the model parameters, we use the smooth L1 loss function with \u03b2=1.0 and the Adam optimizer with a learning rate of 5e-4 and weight decay of 1e-5 to prevent over-fitting. To further enhance the learning process, we incorporate experience replay, with a memory that stores up to 10 million experiences [11]. A target network alongside the main policy network, is being used according to the Double-DQN approach [24]. To update the target network we apply a soft update with a factor \u03c4=5e-3. This gradually brings the target network closer to the policy network, balancing learning speed and stability. With a batch size of 64, we train the models for 10000 episodes. All trained agents manage to learn a policy that successfully abides to constraints."}, {"title": "Algorithm 1: Double Deep Q-Learning with Experience Replay", "content": "5.2.3 Empirical Game Payoff Matrix. The empirical payoff matrix is generated by simulating each strategy profile over multiple games. Payoffs represent how well different styles of play perform jointly, according to the game's rules. Here we must note that even the most incompatible pairs of styles violate few constraints, due to the inherent differences between agents' preferences. For instance, the profile (W,C) of compatible strategies scores 0.05% of viola-tions, while the profile of incompatible strategies (C,C) scores 23% violations.\nThe values in the payoff matrix are computed in terms of the delay and the quality of the solution according to the game's con-straints (gain, penalty, and sanction), excluding preferences. This ensures a common ground for distinct strategies, evaluating solu-tions solely based on the game's rules. For each pair of strategies, we simulate the game over 5,000 repeats and calculate the average payoff for each strategy. These values are then organized into the payoff matrix, which is provided in Table 4 (Appendix A). From this matrix, we observe that (L, WL) and its symmetric counterpart (WL, L) both with payoffs of (3.15, 3.21) and (3.21, 3.15) respectively, are the only Nash equilibria. It is important to note here that these equilibria prescribe agents' strategies, given that they do play the game with rational co-players, but they do not capture the overall dynamics of the game, considering the long-term effects of agents' interactions."}, {"title": "5.3 Evaluation and Ranking", "content": "Given the payoff matrix derived from the empirical analysis, we apply the a-Rank method to evaluate the performance of strategy profiles over time in terms of the MCC solution concept. Specifically, we ran the method 1000 times, using values of a within the range [0.1, 10] with step=0.01, while assuming populations of size m = 100."}, {"title": "6 RELATED WORK", "content": "Evaluation and ranking of learned multi-agent strategies is mainly based on game theoretic concepts, while computational social choice has been proposed, as well [9]. A predominant approach is the Elo rating system used to evaluate and rank agents that learn through reinforcement learning ([7], [18], [19], [12]). Elo estimates the prob-ability an agent to win another agent. Although it was designed specifically to rank players in the two-player, symmetric constant-sum game, it has been widely applied to other domains. Recently, it has been used for the evaluation of large language models [28]. However, Elo cannot model intransitive relationships [1], as those in the Rock-Paper-Scissors game, in real-world games [4], and in our game coloring setting. In addition, incorrectness issues have been reported in transitive settings [2]. Nash-based evaluation methods, such as Nash Averaging, can be applied in two-player, zero-sum settings ([1] [21]), but these are not more generally applicable as the Nash equilibrium is intractable to compute and select [5].\nHere, our focus is on (many) agents' long-term interactions in general-sum dynamic settings: Agents need to rank their policy profiles to decide their non-transient individual policy profile, ac-counting for long-term interactions and payoffs.\nEmpirical Game Theory Analysis (EGTA), deploying empirical or meta-games [15] [20], [8], [26], can be used to evaluate learning agents that interact in large-scale multiagent systems [13], [22] [23]. In our case, aiming at strategy profile rankings, we define the empirical game strategies based on agents' styles of play and train policies realizing these strategies. Then the empirical payoff matrix is estimated by means of game-playing simulations, and is used by the a-Rank method to compute strategy profile rankings. a-Rank applies to many-player, general-sum games [13]."}, {"title": "7 CONCLUSIONS", "content": "In this study, we developed a methodology for identifying strong joint-strategies in dynamic multi-agent games, accounting for sta-bility and performance, using the a-Rank evolutionary algorithm. This methodology is applied to a stochastic version of the Graph Coloring Problem, where players collaborate to color a graph while ensuring that neighboring vertices are assigned different colors. According to the methodology, we first transformed the game into its empirical form by defining styles of play. We then designed and trained Deep Q-Learning policy models that realize these styles of play in the underlying game, and run simulations to generate the empirical payoff matrix. Applying a-Rank to this matrix results in a unique stationary distribution over strategy profiles, which defines the empirical game's MCC. a-Rank not only helped us iden-tify stable strategy profiles resistant to changes, but also provided a descriptive framework for understanding why certain profiles prevail in the long run, based on the underlying dynamics of the game. Through this approach, we successfully described a concise methodology for evaluating and ranking agents' joint policies, con-sidering their long-term interactions in dynamic settings, while also explaining how strategy profiles are defined within the MCC.\nFuture work involves (a) applying the methodology in more complex and large-scale settings, accounting for strategy profiles of multiple stakeholders that may collaborate and/or compete, (b) using machine learning methods to identify different styles of play from demonstrations and specifying the empirical game, (c) ex-ploring advanced models able to adapt their strategies based on observed behaviors based on the behavior of co-players, and (d) applying the methodology into real-world settings where agents need to align with human preferences in dynamic settings."}, {"title": "C.2 Cross-configurations policy rankings", "content": "To determine which strategy profiles perform best across different graph configurations, we aggregate data from all three experiments (the graph configuration shown in Figure 1 and the two configurations in Figure 6) and applied the a-Rank method. Data aggregation here involves taking the empirical payoff matrices produced in the different configurations and computing cell-wise average values. This results into a new \"average\" empirical payoff matrix. Ranking policy profiles in this way reduces the risk of overfitting to any specific configuration and provides more generalized and reliable rankings. Having done so, the rankings for the top 6 strategy profiles are presented in Table 6. However, aggregating empirical payoff matrices for different configurations may not be of value in cases these configurations differ largely. This is an aspect that needs more study."}, {"title": "D THE ROLE OF CONVOLUTIONS IN POLICY TRAINING", "content": "We conjecture that the grid-like structure of the environment makes convolutions a natural choice for the policy model architecture: Since the grid includes spatial relationships between blocks, local dependencies, and configurations with multiple blocks, convolutions enable the policy model to efficiently capture these spatial patterns. Thus, incorporating convolutional layers, policy models are enabled to converge more effectively. To validate this claim, we performed an experiment that compares the training of policies with and without convolutional layers. The results are presented in Figure 11.\nSpecifically, we trained a policy for the \"warm\" strategy using the model architecture with convolutional layers (W), and compared its performance to a \"warm\" policy trained with a model of fully connected layers (Wfc). Both policies are trained on the same graph configuration, and their performance is evaluated based on the loss curves. The results show that the policy with convolutional layers converges more quickly, suggesting that convolutions enable the model to learn the spatial relationships between blocks more efficiently. It must be noted that the Wfc curve picks at 2.27, while the W at 0.89. While both policies eventually reach the same level of performance (with the loss after"}]}