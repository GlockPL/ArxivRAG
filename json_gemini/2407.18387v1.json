{"title": "SCALE: SELF-REGULATED CLUSTERED FEDERATED LEARNING\nIN A HOMOGENEOUS ENVIRONMENT", "authors": ["Sai Puppala", "Ismail Hossain", "Md Jahangir Alam", "Sajedul Talukder", "Zahidur Talukder", "Syed Bahauddin"], "abstract": "Federated Learning (FL) has emerged as a transformative approach for enabling distributed machine\nlearning while preserving user privacy, yet it faces challenges like communication inefficiencies and\nreliance on centralized infrastructures, leading to increased latency and costs. This paper presents\na novel FL methodology that overcomes these limitations by eliminating the dependency on edge\nservers, employing a server-assisted Proximity Evaluation for dynamic cluster formation based on\ndata similarity, performance indices, and geographical proximity. Our integrated approach enhances\noperational efficiency and scalability through a Hybrid Decentralized Aggregation Protocol, which\nmerges local model training with peer-to-peer weight exchange and a centralized final aggregation\nmanaged by a dynamically elected driver node, significantly curtailing global communication over-\nhead. Additionally, the methodology includes Decentralized Driver Selection, Check-pointing to\nreduce network traffic, and a Health Status Verification Mechanism for system robustness. Validated\nusing the breast cancer dataset, our architecture not only demonstrates a nearly tenfold reduction in\ncommunication overhead but also shows remarkable improvements in reducing training latency and\nenergy consumption while maintaining high learning performance, offering a scalable, efficient, and\nprivacy-preserving solution for the future of federated learning ecosystems.", "sections": [{"title": "Introduction", "content": "Collaborative model training in FL is achieved by breaking down the training process into local training and model\naggregation stages [1]. Each data owner conducts local training on its specific data partition and shares only intermediate\nresults, such as gradients, for model aggregation. This communication can take place either at a centralized server or\ndirectly among data owners. The version of federated learning that employs a central server for coordinating model\naggregation is referred to as centralized FL. Centralized FL aims to connect thousands of diverse, distributed devices\ninto a cloud-based centralized server, where devices' local models are directly transmitted to the cloud server. This\ntraditional approach is plagued by issues such as limited wireless resources and long transmission distances, leading to\nunpredictable and unreliable communication that can severely impair training efficiency and model accuracy.\nAddressing this critical bottleneck of communication inefficiency, the Hierarchical Federated Learning (HFL) framework\nhas emerged as a promising solution. By decentralizing the process, HFL employs a two-tier aggregation process\nwhere local parameter values are initially aggregated at edge servers, such as base stations, before undergoing global\naggregation at the central server [2]. This goal of significantly reducing communication overhead with the cloud lead to\na client-edge-cloud hierarchical Federated Learning (FL) system, aiming to combine the reduced data access of cloud\nservers with the rapid updates of edge servers [3]. To enhance resource allocation and incentive design at the edge\nservers, decentralized edge intelligence has been proposed [4] that employs a deep learning based auction mechanism.\nHowever, a significant limitation of existing HFL approaches is the dependency on a specialized, and often costly, edge"}, {"title": "Related Works", "content": "Since the inception of federated learning [5], a significant objective has been to reduce the communication costs\nassociated with Federated Learning [6]. However, as pointed out by Li et al. [7], there are limited existing techniques\nthat alter the approach to user selection. Hartmann [8] proposes stratification based on contextual information about\nusers, while Nishio and Yonetani [9] group users based on hardware characteristics. Author zheng in his paper talks\nabout the uplink and downlink during communication between global server and client nodes [10]. We might see\ncommunications as not a big problem, provided if we have a enough bandwidth, but over communications will impact\nbattery life. Communication overload on global server and cost implications are commonly addressed open problems\nin federated learning architecture. on the other side, author Aroju talks about the energy aware systems in federated\nlearning [11].\nIn a recent survey conducted by Zihao, efforts to enhance communication efficiency were explored [12]. Additionally,\nnumerous studies have been undertaken in the past to achieve swift communication establishment and low latency\nbetween global server and client nodes, aiming for expedited data processing [13]. Research advancements in addressing\ncommunication bottlenecks in federated learning include the exploration of communication-efficient algorithms that\nminimize information exchange between the central server and nodes, such as federated optimization algorithms\nemphasizing local updates [14]. Quantization techniques compress model updates before transmission, reducing\nexchanged information size without significant performance loss [15]. Asynchronous federated learning allows\nindependent node updates, reducing dependency on synchronized communication and enhancing scalability [16]."}, {"title": "Methodology", "content": "To accurately identify the driver node within a federated learning framework, we introduce an advanced initial data\npreparation method. This method involves computing several critical components at the client node, which are then\nencrypted and transmitted to the global server. These components include feature variance, and performance indices for\nedge devices. We provide a detailed mathematical and technical explanation for each component below."}, {"title": "Feature Variance", "content": "Feature variance calculation is pivotal for grouping similar datasets, thereby enhancing the federated averaging process's\nefficiency.\nMethod 1: Alphabetical Schema-Based Scoring Feature variance is computed at the client node by analyzing the\nmetadata of input datasets, focusing on columns and their schema. To ensure consistent scoring for identical attributes,\ncolumns are arranged in alphabetical order. This ordering is crucial to avoid discrepancies in feature scoring.\nGiven a feature attribute represented by a string a7, a6, ..., a1, ao, the formula for calculating its score is as follows:\nScore =a7356 + \u03b16355 + 25.354\n+ \u04304.353 + \u0430\u0437 352 + \u04302 351 + a1.35\u00b0\n(1)\nwhere each character in the attribute name is assigned a numeric value based on its position in the English alphabet\n(A=0, B=1, ..., Z=25).\nMethod 2: Combined Metadata Features To further refine the feature variance score calculations, we adopt a method\nthat utilizes combined metadata features. This approach incorporates the alphabetical order of columns and their data"}, {"title": "Performance Index for Edge Devices", "content": "The Performance Index (P.I.) is crucial for evaluating an edge device's suitability as a driver node within a federated\nlearning network. Given the unique constraints and capabilities of edge devices, we derive the P.I. from metrics such as\nenergy efficiency, latency, network bandwidth, and concurrency support.\nMethod 1: Compute Ability Score for Edge Devices The Compute Ability Score serves as a foundational metric for\nthe global server to cluster edge devices in a federated learning environment. By assessing computational power, energy\nefficiency, latency, network stability, and multitasking capabilities, this score enables the global server to categorize\ndevices based on their performance and operational efficiency. This clustering facilitates optimized task allocation,\nensuring devices are grouped and utilized according to their strengths and capabilities, enhancing the overall efficiency\nand effectiveness of federated learning processes.\nThe formula for calculating the Compute Ability Score integrates these attributes, each weighted according to its impact\non the device's overall performance, thus providing a nuanced evaluation metric:\nx' = a + \\frac{(x - min(x))(b \u2212 a)}{max(x) - min(x)}\n(3)\nThese values are scaled to a uniform range via transformation, facilitating a balanced evaluation across diverse hardware\nspecifications and operational conditions. Subsequently, the Compute Ability Score for an edge device is calculated as\na weighted sum of these scaled values, integrating the various metrics into a comprehensive performance index:\nP.Icompute = W1 \u00b7 Cp + W2 \u00b7 Ee + W3. L + W4. Nb + W5. Ci\n(4)\nHere, Cp represents Computational Power, Ee for Energy Efficiency, L indicates Latency, N\u2081 measures Network\nBandwidth, and Ci reflects Concurrency Level. Each metric, scaled and assigned a weight (wi), quantifies each device's\noperational efficiency and suitability to facilitate clustering.\nMethod 2: Operational Efficiency Score The Operational Efficiency Score assesses edge devices' performance in a\nfederated learning setting by examining CPU utilization, energy consumption, network efficiency, and energy efficiency.\nThis comprehensive metric evaluates how effectively a device manages its computational resources (CPU utilization), its\npower usage (energy consumption and energy efficiency), and its capacity to maintain stable communication (network\nefficiency). Leveraging this information, the global server can effectively cluster devices, aligning them based on their\noperational efficiency and resource optimization capabilities through a composite measure:\n\u03c8 = \\frac{1}{\\frac{1}{CPU Utilization. W\u2081} + \\frac{1}{Energy Consumption. W2} + \\frac{1}{Network Efficiency. w3} + \\frac{1}{Energy Efficiency. W4}}\n(5)\nLocal P.I(a) = \\frac{1}{\u03c8/4}\n(6)\nConsidering the varied nature of edge devices, a logarithmic transformation is applied to the performance index scores\nto ensure scalability and manageability before transmission to the global server:\nLocal log P.I = log(a)\n(7)\nThis strategic approach enables the global server to allocate clustering more efficiently, ensuring that devices are utilized\nin a manner that enhances the network's overall performance and sustainability."}, {"title": "Global Server-Assisted Parallel Integration for Cluster Formation", "content": "To optimize the FL process, we propose a global server-assisted mechanism for forming clusters based on parallel\nintegration of data similarity and geographical proximity. Edge devices submit their dataset summaries, performance"}, {"title": "Proximity Evaluation", "content": "This process involves assessing the geographical closeness of nodes to facilitate efficient data communication and\ncomputational collaboration among devices. By leveraging geographical coordinates, the global server can group\ndevices that are physically closer, thereby reducing latency and improving the overall speed and reliability of the\nfederated learning tasks. Additionally, proximity evaluation helps in minimizing network congestion and optimizing\nbandwidth usage, which are essential for the scalability and performance of federated learning systems.\nFor an alternative proximity evaluation, the Equirectangular Approximation calculates distances between nodes with\nthe formula:\ndistance = R. \\sqrt{(\u0394\u03a6)\u00b2 + (cos(\\frac{\u03a61 + \u03a62}{2}). \u0394\u03bb)\u00b2},\n(8)\nwhere R is the Earth's radius, \u2206 and \u2206\u03bb are the differences in latitude and longitude, and $1 and $2 represent the\nlatitudes of the two points. This method offers a simpler calculation for distances, aiding in the efficient clustering of\nnodes based on geographic proximity."}, {"title": "Hybrid Decentralized Aggregation Protocol for Federated Learning", "content": "The FL framework is further refined through a hybrid decentralized aggregation protocol, which combines local model\ntraining and peer-to-peer weight exchange with centralized final aggregation under a dynamically selected driver. This\nprotocol minimizes global communication overhead and maximizes the efficiency of model updates aggregation. The\nselection of a driver node is crucial for coordinating the final aggregation and ensuring the distributed learning process's\ncohesion and progression.\nThe Hybrid Decentralized Aggregation Protocol, introduces an innovative approach to model aggregation within the FL\nframework. It synergizes local model training and peer-to-peer weight exchange with a centralized final aggregation\nphase conducted by a dynamically selected driver. This hybrid protocol is designed to minimize global communication\noverhead while maximizing the efficiency and efficacy of the aggregation process. The mathematical formulation of the\nprotocol's operation is as follows:\nLocal Model Training and Weight Exchange:\ni\n\u2022 Each edge device e\u00bf \u2208 E updates its model weights wt) through local training on its dataset D\u2081.\n\u2022 Subsequently, ei engages in weight exchange with a selected subset of peers Ni, aggregating received weights\nto update wit+1).\nw(t+1) = \\frac{1}{|Ni + 1|} (wt) + \\sum_{j\u2208N} w(t))\n(9)\nCentralized Final Aggregation by Driver:\n\u2022 A driver L is elected through the decentralized driver selection mechanism (Algorithm 4), coordinating the\nfinal aggregation of model weights across the cluster.\n\u2022 The driver L computes the final aggregated model weights Wconsensus by averaging the updated weights from\nall devices in the cluster:\nWconsensus = \\frac{1}{|\u03b5|}\\sum_{i=1} w(t+1)\n(10)"}, {"title": "Decentralized Driver Selection", "content": "To maintain the system's operational continuity, we introduce a decentralized driver selection mechanism that takes\nplace after decentralized weight exchange and averaging. This is also triggered upon the failure of a current driver\nnode, alongside a health status verification mechanism to monitor the vitality of the communication channels. Several\ncritical criteria ensure the elected driver optimizes the aggregation phase's efficiency and effectiveness: Computational\nCapacity, Network Connectivity and Bandwidth, Battery Life or Energy Resources, Reliability and Availability, Data\nRepresentativeness, and Security and Trustworthiness.\nComputational Capacity is essential for processing and aggregating data swiftly, while Network Connectivity and\nBandwidth ensure fast and reliable communication with other devices. Battery Life or Energy Resources guarantee the\ndevice can sustain the increased workload without interruption. Reliability and Availability reflect a device's historical\nuptime, indicating its dependability throughout the learning process. Data Representativeness is crucial for generating a\nglobal model that accurately reflects the collective dataset, and Security and Trustworthiness ensure the integrity of the\ndata and the aggregation process, safeguarding against potential breaches or compromises. Together, these criteria form\na comprehensive framework for selecting a driver capable of efficiently managing the complexities and demands of\nfederated learning in an edge computing environment.\nThe Decentralized Driver Selection mechanism is presented in Algorithm 4. Upon a leadership vacuum, a new driver\nis elected based on predefined criteria P = {P1, P2, ..., p\u0131}, incorporating factors like computational capacity and\nnetwork stability. The election process is mathematically represented as:\nL = arg max  \\sum_{j=1}\u03c9jPji\n(11)\nwhere L designates the new driver, pj,i indicates the jth criterion for device ei, and wj is the weight assigned to the jth\ncriterion. This formula ensures a weighted and consensus-based driver election, enhancing the democratic essence of\nthe selection process."}, {"title": "Experiment", "content": "We initiated our experiment by utilizing the SCALE architecture with the breast cancer data-set, distributed among 100\nclient nodes. These data-sets were shared across all client nodes in both identical and non-identical ways. We evaluated\nour approach in two distinct federated learning scenarios. Initially, we employed the breast cancer data-set within the\ntraditional federated learning framework, and subsequently, we compared the results with the SCALE approach."}, {"title": "Dataset", "content": "The BreastHealth Dataset is a comprehensive and diverse collection of annotated mammogram images designed\nto support research and development in the field of medical imaging and breast cancer prediction, with a focus on\nhealthcare applications. This dataset provides a rich source of mammogram images along with detailed annotations,\nmaking it an invaluable resource for various tasks such as breast cancer detection, classification, and risk assessment.\nWe have gathered data from Breast Cancer Wisconsin [21] and utilized 30 available features to forecast whether breast\ntissue is malignant or benign.\nIn the context of local training and validation of accuracies for traditional federated learning and SCALE architecture,\nwe have conducted our experiments using Support Vector Classifier machine learning algorithm."}, {"title": "Results", "content": "Our empirical investigation systematically evaluates the SCALE approach against traditional federated learning\nframeworks, leveraging diverse performance metrics, communication efficiencies, and cost considerations. Our analysis,\nsupported by comprehensive metrics across model performance, communication efficiency, and cost-effectiveness,"}, {"title": "Model Performance Metrics", "content": "The quantitative analysis of model performance, as mentioned in Table 1, highlights the nuanced improvements\nfacilitated by SCALE. Initially, both methodologies display comparable accuracies. However, as training progresses,\nSCALE demonstrates a subtle yet consistent enhancement in model performance metrics, including F1 scores, precision,\nrecall, and ROC AUC values. This incremental improvement underscores the efficacy of SCALE's distributed learning\nmechanism, which not only accommodates but leverages the diversity of data across nodes to refine model accuracy\neffectively."}, {"title": "Communication Metrics", "content": "A critical examination of the communication overhead, unveils SCALE's strategic reduction of global server updates.\nThe stark reduction from 2850 updates in the federated learning paradigm to a mere 235 under SCALE underscores a\nsignificant optimization in data transmission. This efficiency is pivotal in mitigating network congestion and minimizing\nlatency, thereby enhancing the overall scalability of the federated learning system. Such an approach is not merely a\ntechnical improvement but a strategic reorientation towards more sustainable and efficient federated learning operations."}, {"title": "Processing Latency", "content": "Notably, the introduction of checkpointing mechanisms yields a dramatic reduction in latency, optimizing the aggregation\nprocess at the global server level. This methodological refinement enhances the responsiveness of the learning system,\nfacilitating quicker model updates and enabling more agile adaptations to evolving data landscapes. The implications\nof such efficiency extend beyond mere time savings, suggesting a more dynamic and responsive federated learning\nframework."}, {"title": "Cost Implications", "content": "By significantly reducing the computational demands on the global server and optimizing the efficiency of data\ntransmission, SCALE offers a cost-effective solution for federated learning deployments. This cost efficiency is\nparticularly salient in cloud-based implementations, where computational resources come at a premium. The SCALE\napproach, therefore, not only enhances the technical performance of federated learning systems but also presents a\nfinancially sustainable model for large-scale deployments."}, {"title": "Conclusion", "content": "In conclusion, this paper presents an innovative Federated Learning (FL) methodology that overcomes traditional\nchallenges such as privacy concerns, high latency, and increased costs in distributed machine learning by eliminating\nedge server dependency and implementing a server-assisted Proximity Evaluation for dynamic cluster formation.\nThis is based on data similarity, performance indices, and geographical proximity. Our approach includes a Hybrid\nDecentralized Aggregation Protocol that merges local model training with peer-to-peer weight exchange and centralized\nfinal aggregation, significantly reducing communication overhead. Additionally, Decentralized Driver Selection, Check-\npointing mechanisms, and a Health Status Verification Mechanism enhance network efficiency, system robustness, and\ndata privacy. Tested on a breast cancer dataset, our methodology drastically cuts communication overhead, improves\ntraining latency and energy efficiency, and maintains high learning performance, offering a scalable, efficient, and\nprivacy-preserving framework for advancing federated learning."}]}