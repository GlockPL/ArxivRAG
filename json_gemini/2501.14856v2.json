{"title": "NOISE-CONDITIONED ENERGY-BASED ANNEALED REWARDS (NEAR): A GENERATIVE FRAMEWORK FOR IMITATION LEARNING FROM OBSERVATION", "authors": ["Anish Abhijit Diwan", "Julen Urain", "Jens Kober", "Jan Peters"], "abstract": "This paper introduces a new imitation learning framework based on energy-based generative models capable of learning complex, physics-dependent, robot motion policies through state-only expert motion trajectories. Our algorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR), constructs several perturbed versions of the expert's motion data distribution and learns smooth, and well-defined representations of the data distribution's energy function using denoising score matching. We propose to use these learnt energy functions as reward functions to learn imitation policies via reinforcement learning. We also present a strategy to gradually switch between the learnt energy functions, ensuring that the learnt rewards are always well-defined in the manifold of policy-generated samples. We evaluate our algorithm on complex humanoid tasks such as locomotion and martial arts and compare it with state-only adversarial imitation learning algorithms like Adversarial Motion Priors (AMP). Our framework sidesteps the optimisation challenges of adversarial imitation learning techniques and produces results comparable to AMP in several quantitative metrics across multiple imitation settings. Code and videos available at anishhdiwan.github.io/noise-conditioned-energy-based-annealed-rewards/", "sections": [{"title": "1 INTRODUCTION", "content": "Learning skills through imitation is probably the most cardinal form of learning for human beings. Whether it is a child learning to tie their shoelaces, a dancer learning a new pose, or a gymnast learning a fast and complex manoeuvre, acquiring new motor skills for humans typically involves guidance from another skilled human in the form of demonstrations. Acquiring skills from these demonstrations typically boils down to interpreting the individual features of the demonstration motion \u2013 for example, the relative positions of the limbs in a dance pose \u2013 and subsequently attempting to recreate the same features via repeated trial and error. Imitation learning (IL) is an algorithmic interpretation of this simple strategy of learning skills by matching the features of one's own motions with the features of the expert's demonstrations.\nSuch a problem can be solved by various means, with techniques like behavioural cloning (BC), inverse reinforcement learning (IRL), and their variants being popular choices (Osa et al., 2018). The imitation learning problem can also be formulated in various subtly differing ways, leading to different constraints on the types of algorithms that solve the problem. One notably challenging version of the problem is Imitation from Observation (IfO) (Torabi et al., 2018; 2019; Zare et al., 2024), where the expert trajectories are only comprised of state features and no information about the expert's actions is available to the imitator. This means that learning a policy is not as straightforward as capturing the distribution of the expert's state-action pairs. Instead, the imitator must also"}, {"title": "2 BACKGROUND: ADVERSARIAL IMITATION LEARNING", "content": "Given an expert motion dataset M containing i.i.d. data samples x = (s,s') \u2208 X implying a distribution PD, where X is the space of state transitions, adversarial IL methods aim to learn a differentiable generator (policy) \\pi_{\\theta_{\\varsigma}}(s) : S \u2192 A where S is the state space, A is the action space, and s \u2208 S is a sample drawn from the occupancy measure of the policy \\rho_{\\pi}. Similarly to a standard GAN, the idea here is to learn a differentiable discriminator D_{e_{\\\u266d}}(x) : X \u2192 [0,1] that returns a scalar value representing the probability that the sample x was derived from PD. However,"}, {"title": "3 NOISE-CONDITIONED SCORE NETWORKS (NCSN)", "content": "Score-based generative models are a family of techniques recently popularised for generating realistic images and video samples. They model the unknown data distribution as a Boltzmann distribution and generate samples by an iterative denoising process by traversing along the gradient of the data distribution's log probability (Song & Kingma, 2021). Score-based models approximate the gradient of the log probability (called the score function) through a procedure called denoising score matching (Vincent, 2011). Similarly to GANs, the aim here is to learn a probability distribution PG that closely resembles the data distribution PD, with PG \uc2a5 e^{-E(x)/z} (Boltzmann distribution) and E(x) called the energy function of the distribution. Intuitively, the energy function is a measure of the closeness of a sample to PD while the score (\\nabla_x log p_g(x)) is a vector pointing towards the steepest increase in the likelihood of PD. Learning the score function implicitly also learns the energy function as \\nabla log p_g(x) = \\nabla_x log(e^{-E(x)/z}) = -\\nabla E(x). In this paper, we propose to explicitly learn the energy function to then use the energy of a sample to guide reinforcement learning. To do so, we make modifications to a score-based framework called Noise Conditioned Score Networks (NCSN) (Song & Ermon, 2019; 2020).\nThe underlying idea of NCSN is to learn the score function by a process of artificial perturbation and denoising. Data samples are first perturbed by adding variable amounts of Gaussian noise. The score function is then learnt by estimating the denoising vectors that point from the perturbed data samples to the original ones. Given i.i.d. data samples {x ~ PD \u2208 \\mathbb{R}^D}, NCSN (Song & Ermon, 2019) formulates a perturbation process that adds Gaussian noise \\mathcal{N}(x, \\sigma) to each sample x, where o is the standard deviation representing a diagonal covariance matrix and is sampled uniformly from a geometric sequence {\\sigma_1, \\sigma_2, ..., \\sigma_L}. Following this perturbation process, we obtain a conditional distribution q_{\\sigma}(x'|x) = \\mathcal{N}(x'|x,\u03c3I) from which a marginal distribution q_{\\sigma}(x') can be obtained as \\int q_{\\sigma}(x'|x)p_p(x)dx. Given this perturbed marginal distribution, NCSN attempts to learn a score function s_{\\theta}(x, \\sigma) : \\mathbb{R}^D \u2192 \\mathbb{R}^D that points from the perturbed samples back to the original ones. The idea is to learn a conditional function to jointly estimate the scores of all perturbed data distributions, i.e., \u2200\u03c3 \u2208 {\u03c3_{i}\\}_{i=1}^L : s_\u03c3(x',\u03c3) \u2248 \\nabla_{x'} log q_\u03c3(x'). The score network is learnt via denoising score matching (DSM) (Vincent, 2011) on samples drawn from the conditional distribution q_\u03c3(x'|x).\nBy perturbing individual data samples with Gaussian noise, NCSN essentially creates a perturbed distribution that is a smooth and dilated version of PD \u2013 with the standard deviation o controlling the level of dilation. This perturbation strategy ensures that PD is supported in the whole sample space and not just a low-dimensional manifold in RD , ensuring well-defined gradients and allowing a better score function approximation. It also ensures that the score function is accurately approximated in data-sparse regions in PD by increasing sample density in such regions (Song & Ermon, 2019)."}, {"title": "4 NOISE-CONDITIONED ENERGY-BASED ANNEALED REWARDS (NEAR)", "content": "In NCSN, the perturbed conditional distribution q(x'x) is formulated as a Boltzmann distribution such that q(x'x) = \\frac{e^{-DIST(x',x)}}{z} where DIST() is a function that defines some distance measure"}, {"title": "4.1 LEARNING ENERGY FUNCTIONS", "content": "Given D-dimensional i.i.d. data samples {x ~ P_D \u2208 \\mathbb{R}^D} where P_D is the distribution of state-transition features in the expert's trajectories, NEAR learns a parameterised energy function e_\\theta(x',\u03c3) : \\mathbb{R}^D \u2192 \\mathbb{R} that approximates the energy of samples x' in a perturbed data distribution obtained by the local addition of Gaussian noise \\mathcal{N}(x,\u03c3) to each sample x.\nThe idea here is to jointly estimate the energy functions of several perturbed distributions, i.e., \u2200\u03c3\u2208 {\\sigma_i\\}_{i=1} : e_\u03c3(x',\u03c3) \u2248 DIST_\u03c3(x'). The sample's score is computed by taking the gradient of the predicted energy w.r.t. the perturbed sample, s(x',\u03c3) = \\nabla_{x'}e_\\theta(x', \u03c3). The energy network is learnt via denoising score matching (DSM) (Vincent, 2011) using this computed"}, {"title": "4.2 ANNEALING", "content": "We modify the definition of the perturbed conditional distribution q(x'|x) by flipping the sign of the energy function such that higher energies indicate closeness to pp. This is done to simplify the downstream reinforcement learning such that the predicted energy can be maximised directly. Following the improvements introduced in Song & Ermon (2020), we define e_\\theta(x',\u03c3) = e_o(x')/\u03c3 where eo (x') is an unconditional energy network . This allows us to learn the energy function of a large number of noise scales with a very small sized dataset.\nThe appropriate selection of the noise scale ({\u03c3i}) is highly important for the success of this framework. \u03c3\u2081 must be small enough that the perturbed distribution qo\u2081 () is nearly identical to pD. This ensures that the policy aims to truly generate samples that resemble those in PD. In contrast, \u03c3\u2081 must be sufficiently large such that e_\\theta(x', \u03c3\u2081) is well-defined, continuous, and non-zero for any sample that is generated by the worst-possible policy. This ensures that the agent always receives an informative signal for improvement. Assuming that policy degradation is unlikely, 01 must be such that supp(qo1()) effectively contains the support of the distribution induced by a randomly initialised policy network. In practice, these are dataset-dependent hyperparameters."}, {"title": "5 EXPERIMENTS", "content": "We evaluate NEAR (Algorithm 1) on complex, physics-dependent, contact-rich humanoid motions such as stylised walking, running, and martial arts. The chosen task set demands an understanding of physical quantities such as gravity and the mass/moments of inertia of the character and contains a variety of fast, periodic, and high-acceleration motions. The expert's motions are obtained from the CMU and SFU motion capture datasets and contain trajectories of several motions. For each motion, a dataset of state transitions M = {(s, s')} is created to learn an imitation policy. Rewarding the agent for producing similar state transitions as the expert, incentivises the agent to also replicate the expert's unknown actions."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "We evaluate NEAR (Algorithm 1) on complex, physics-dependent, contact-rich humanoid motions such as stylised walking, running, and martial arts. The chosen task set demands an understanding of physical quantities such as gravity and the mass/moments of inertia of the character and contains a variety of fast, periodic, and high-acceleration motions. The expert's motions are obtained from the CMU and SFU motion capture datasets and contain trajectories of several motions. For each motion, a dataset of state transitions M = {(s, s')} is created to learn an imitation policy. Rewarding the agent for producing similar state transitions as the expert, incentivises the agent to also replicate the expert's unknown actions.\nr(s,a,s',g) = w_{task}r_{task} (s, a, g) + w_{energy}e_\\theta(s, s')\nTo understand the impact of motion data availability on the algorithm, we also train NEAR in a single-clip setting \u2013 using a single expert motion for training \u2013 on challenging motions like mummy-style walking and spin-kick. Further, to understand the composability of the learnt rewards, we train NEAR with both environment-supplied rewards (such as a target reaching reward) and energy-based rewards learnt from different motion styles (to perform hybrid stylised motions). To incorporate the environment-supplied task reward rtask (s, a, g) \u2208 [0, 1], we use the same strategy from Peng et al. (2021) and formulate learning as a goal-conditioned reinforcement learning problem, where the policy is now conditioned on a goal g and maximises a reward r\u0303(s, a, s', g) (Equation (3)). Details of the tasks and goals can be found in Appendix B.1. We also apply an additional reward transformation of tanh ((r-r')/10) where r' is the mean horizon-normalised return received by the agent in the last k = 3 policy iterations, r' = mean({R_{t-i}/horizon}_{i=1}^k). This bounds the unnormalised energy reward to a fixed interval so that changes between the noise levels o are smoother. Additionally, it grounds the agent's current progress in relation to its average progress in the last few iterations. The policy is trained using Proximal Policy Optimisation (Schulman et al., 2017) and we use the following quantitative metrics to measure the performance of our algorithm."}, {"title": "5.2 RESULTS", "content": "We compare Noise-conditioned Energy-based Annealed Rewards (NEAR) with Adversarial Motion Priors (AMP) (Peng et al., 2021) and in both cases only use the learnt rewards to train the policy. AMP is used as a baseline since it is an improved formulation of previous state-of-the-art techniques and has shown superior results in the state-only adversarial IL literature. Each algorithm-task combination is trained 5 times independently and the mean performance metrics across 20 episodes of each run are compared. Both algorithms are trained for a fixed number of maximum iterations.\nFigure 4 shows snapshots of the policies trained using NEAR. We find that NEAR achieves very close imitation performance with the expert's trajectory and learns policies that are visually smoother and more natural. For the quantitative metrics, we use the average performance at the end of training for a fair comparison and find that both NEAR and AMP are roughly similar across all metrics (Table 1). In most experiments, NEAR is closer to the expert in terms of the spectral arc length while AMP has a better pose error. NEAR also outperforms AMP in stylised goal-conditioned tasks, producing motions that both imitate the expert's style while simultaneously achieving the desired global goal (Table 2 and Figure 4 bottom). From the experiments on spatially composed learnt rewards, we find that NEAR can also learn hybrid policies such as waking while waving. Finally, we notice that NEAR performs poorly in single-clip imitation tasks, highlighting the challenges of accurately capturing the expert's data distribution in data-limited conditions. Conversely, AMP is less affected by data unavailability since the discriminator in AMP is simply a classifier and does not explicitly capture the expert's distribution."}, {"title": "5.3 ABLATIONS", "content": "We also conduct ablation experiments (Table 3) that help identify the crucial components of NEAR. The main focus of these experiments is to understand the contributions of annealing and the effects of"}, {"title": "6 LIMITATIONS & CONCLUSIONS", "content": "While NEAR is capable of generating high-quality, life-like motions and also outperforms AMP in several tasks, it is still prone to some limitations. The annealing strategy discussed in Section 4.2 does lead to progressively improving rewards, however, annealing at high noise levels also tends to cause unpredictability in the received rewards. We attribute this unpredictability to the static"}, {"title": "A.1 PROOF OF ENERGY FUNCTION SMOOTHNESS", "content": "In this section, we prove that an energy function learnt via denoising score matching (Vincent, 2011) is smooth and well-defined in the manifold of perturbed samples.\nLemma A.1. Let p\u0189 be a distribution with support contained in a closed manifold M \u2286 Rd. We assume that PD is continuous in this manifold. Let qo be a distribution supported in a closed manifold PC Rd obtained by the addition of Gaussian noise \\mathcal{N}(x,\u03c3) (exp-E(x))/z to each sample x in pp (s.t. qr(x) = \u222bN(x'|x,\u03c3I)pp(x)dx). Then qo is continuous in P and Vlog qo (x) = -\u2207xEo(x) is smooth in P.\nProof. The convolution of a function with a Gaussian kernel results in a smooth function. By the same reasoning, \u2207 log qo (x) is differentiable in P because qo is continuous in P.\nLemma A.1 shows that a perturbed distribution and its score function are both smooth in the manifold of perturbed samples. Given this perturbed distribution, denoising score matching aims to learn a score function s(x, \u03c3) = \u2207xe\u03b8(x, \u03c3) where e\u0473(x, \u03c3) \u2248 Eo(x). From the universal approximation theorem (Cybenko, 1989; Hornik et al., 1989), it follows that a sufficiently large neural network can approximate any continuous function (score function in our case) on a compact domain to arbitrary precision, meaning that Vee(x, \u03c3) is a smooth function.\nTheorem A.2. Given a distribution qo that is supported in a closed manifold P and is also continuous in this manifold, a parameterised energy function learnt via denoising score matching on samples drawn from qo is smooth in P.\nProof. Lemma A.1 implies that the gradient of the score function is smooth in P and can be approximated smoothly by a neural network \u2207xee(x, \u03c3). Since a function with a continuous gradient in a domain is itself continuous in that domain, it follows that ee(x, \u03c3) is also continuous in P.\nContinuity of e(x, \u03c3) in the whole space requires P to be equivalent to Rd. However, the annealing strategy introduced in this paper and a sufficiently large o ensure that the manifold of policy-generated samples always lies in P."}, {"title": "A.2 ANNEALING DISCUSSION", "content": "Annealing not only ensures that the reward signal is well-defined, but it also progressively provides more \u201cfocused\u201d rewards to the agent. Given a score function s(x', \u03c3) = \u2207'e\u0473(x',\u03c3) = (x' \u2013 x)/\u03c32, for any fixed sample x', \u2207'ee(x',\u03c3\u03ba+1) > \u2207'\u03b5\u03b8(x',\u03c3\u03ba). This means that at any given point in training, the increase in received rewards for making positive progress is much higher for the (k + 1)th energy function. This can greatly incentivise the agent to move closer to PD."}, {"title": "A.2.1 WHY ANNEAL IF THE LEARNT ENERGY FUNCTION IS SMOOTH?", "content": "If the learnt energy function ee(., \u03c3\u03ba) is smooth, then simply maximising it should still provide an unambiguous improvement signal to the agent. Why then must we anneal the energy functions?"}, {"title": "A.2.2 WHY DO SOME TASKS SUBSTANTIALLY BENEFIT FROM ANNEALING?", "content": "Ablation experiments from Section 5.3 show that annealing has a significant positive impact on more challenging tasks like crane pose. We hypothesise that the expert data distribution PD is more densely distributed for some tasks while sparsely for others (Figure 6). Assuming that a newly initialised policy always starts in the same manifold, the agent would start at a more informative reward function for a sparsely distributed PD than for a densely distributed PD. This means that a noise level change for a densely distributed PD provides more informative rewards."}, {"title": "B EXPERIMENT DETAILS", "content": null}, {"title": "B.1 TASKS", "content": "The task reward and goal features for each imitation task are described below."}, {"title": "Target Reaching", "content": "In this task, the agent's objective is to navigate towards a randomly placed target. The agent's state is augmented to include a goal gt = x+ where t is the current timestep, and x is the target's position in the agent's local coordinate frame. During training, the target is randomly initialised in a 240\u00b0 arc around the agent within a radius in the range [2.0, 6.0] meters. The agent is rewarded for minimizing its positional error norm and heading error to the target. Here, xt is the agent's position, vt is the agent's velocity vector, d*t is a unit vector pointing from the agent's root to the target, and v* is a scalar desired velocity set to 2.0 for walking and 4.0 for runninng.\nptask = 0.6 (exp(-0.5 ||x - xt||2)) + 0.3 (1/(1 + exp (5 * ((Ut*d*t)/|| Ut ||)\u22122))) + 0.1 (1 \u2013 (||vr|| \u2013 v*)2)\nTarget Reaching & Punching\nIn this task, the agent's objective is to both reach a target and then strike it with a left-handed punch. Here, the goal is a vector of the target's position in the agent's local frame and a boolean variable indicating whether the target has been punched, gt =< xx, punch state >. We use the same target initialisation strategy as before with an arc of 45\u00b0 and an arc radius in [1.0, 5.0] meters. The agent is rewarded using the target location reward when it is farther than a threshold distance from the target and with a target striking reward when it is within this threshold. The striking reward aims to minimise the pose error and heading error between the agent's end effector and the target while simultaneously aiming to achieve a certain end effector velocity and height. The complete reward function is shown below where xeff is the end effector position, vef veff is the end effector velocity vector, heff is the end effector height, and v*eff = 4.0 and h*eff = 1.4 are scalar desired punch speed and height."}, {"title": "B.2 TRAINING & EVALUATION DETAILS", "content": null}, {"title": "B.1", "content": "TASKS"}, {"title": "B.2.1 ARCHITECTURES", "content": "For both NEAR and AMP, the policy is a simple feed-forward neural network that maps the agent's state s to a Gaussian distribution over actions, \u03c0\u03b8\u03c2 = N(\u03bc(s), \u03a3) with the mean \u00b5(s) being returned by the neural network and a fixed diagonal covariance matrix \u03a3. In our experiments, the neural network is a fully-connected network with (1024, 512) neurons and ReLU activations. \u2211 is set to have values of e-2.9 and stays fixed throughout training. The critic (value function) is also modelled by a similar network. The value function is updated with TD(X) (Sutton, 1988) and advantages are computed using generalised advantage estimation (Schulman et al., 2015). When using the environment-supplied task reward, we set wtask = wenergy = 0.5.\nThe NCSN neural network is a fully-connected network with an auto-encoder style architecture. Here, the encoder has (512, 1024) neurons and maps the input to a 2048-dimensional latent space. The decoder has (1024, 512, 128) neurons with the output being the unconditional energy of a sample. We use ELU activations between all layers of the auto-encoder and use Xavier uniform weight initialisation (Glorot & Bengio, 2010) to improve consistency across different independent training runs. Further, we standardise samples before passing them to the network. The NCSN noise scale was defined as a geometric sequence with \u03c3\u2081 = 20, \u03c3\u03b5 = 0.01, and L = 50 following the advice from Song & Ermon (2020). Following Song & Ermon (2020) we also track the exponentially moving average (EMA) of the weights of the energy network during training and use the EMA weight during inference, as it has been shown to further reduce the instability in the sample quality. All models in this paper were trained on the Nvidia-A100 GPU (Choquette et al., 2021)."}, {"title": "B.2.2 REINFORCEMENT LEARNING", "content": "We borrow the experimental setup from Peng et al. (2021) where the agent's state is a 105-dimensional vector consisting of the relative position of each link with respect to the root body and the rotation of each link (represented as a 6-dimensional normal-tangent vector of the link's linear and angular velocities). All features are in the agent's local coordinate system. Similarly to Peng et al. (2021), we do not add additional features to encode information like the feature's phase in the motion, or the target pose. Further, the character is not trained to replicate the phase-wise features of the motion and the learnt rewards are generally only a representation of the closeness of the agent's motion to the expert's data distribution. The agent's actions specify a positional target that is then tracked via PD controllers at each joint."}, {"title": "B.2.3 EVALUATION METRICS", "content": "= . Average Dynamic Time Warping Pose Error: This is the mean dynamic time warping (DTW) error (Sakoe & Chiba, 1978) between trajectories of the agent's and the expert's poses averaged across all expert motions in the dataset. Given a set of j expert motion trajectories ; of arbitrary length L; where each trajectory is a series containing the Cartesian positions of the reference character's joints \u0109i, D = {{xi,1j}}=1Lj,traj where tj = {{xi,tj}}i=1traj , first, we roll out the trained policy deterministically across several thousand random starting-pose initialisations . Then, the k = 20 most rewarding trajectories are selected to form a set of policy trajectories D1 = {{Tm}}=1 where Tm = {{xi,m}}i=1 and each trajectory has an arbitrary length Lm. The average dynamic time warping pose error is then computed as the average DTW score of all Tms across all expert trajectories \u00cej with ||\u00cei \u2212 Xi ||2 as the cost function. To ensure that the pose error is only in terms of the character's local pose and not its global position in the world, we transform each Cartesian position to be relative to the character's root body position at that timestep xitxixroot and XiXi xroot Spectral Arc Length: Spectral Arc Length (SAL) (Beck et al., 2018; Balasubramanian et al., 2011; 2015) is a measure of the smoothness of a motion. The smoothness of the character's trajectory is an interesting metric to determine the policy's ability to perform periodic motions in a controlled manner. The underlying idea behind SAL is that smoother motions typically change slowly over time and are comprised of fewer and low-valued frequency domain components. In contrast, jerkier motions have a more complex frequency domain signature that consists of a lot of high-frequency components. The length of the frequency domain signature of a motion is hence an appropriate indi- cation of a motion's smoothness (with low values indicating smoother motions). SAL is computed by adding up the lengths of discrete segments (arcs) of the normalised frequency-domain map of a motion. In our experiments, we use SPARC Beck et al. (2018), a more robust version of the spectral arc length that is invariant to the temporal scaling of the motion. We track the average SAL of the k = 20 most rewarding trajectories generated by the policy at different training intervals and use the root body Cartesian coordinates to compute the SAL. Note that in this case, we do not transform the positions to the agent's local coordinate system."}, {"title": "B.2.4 HYPERPARAMETERS & TRAINING ITERATIONS", "content": null}, {"title": "B.2.5 REPEATABILITY & DETERMINISM", "content": "Each algorithm was trained 5 times independently on every task with separate random number generator seeds for each run. However, using a fixed seed value will only potentially allow for deterministic behaviour in the IsaacGym simulator. Due to GPU work scheduling, it is possible that runtime changes to simulation parameters can alter the order in which operations take place, as environment updates can happen while the GPU is doing other work. Because of the nature of floating point numeric storage, any alteration of execution ordering can cause small changes in the least significant bits of output data, leading to divergent execution over the simulation of thousands of environments and simulation frames. This means that experiments from the IsaacGym simulator (including the original work on AMP) are not perfectly reproducible on a different system. However, parallel simulation is a major factor in achieving the results in this paper and minor non-determinism between independent runs is hence just an unfortunate limitation. More information on this can be found in the IsaacGymEnvs benchmarks package. Note that this is only a characteristic of the reinforcement learning side of our algorithm. The pretrained energy functions are also seeded and these training runs are perfectly reproducible."}, {"title": "B.3 MAZE DOMAIN DETAILS", "content": "This section provides additional details of the experiments and procedures used to generate Figure 1. In this experiment, the agent is initialised randomly in a small window at the top portion of the L-shaped maze. The agent aims to reach the goal position at the bottom right (the episode ends when the agent's position is within some threshold of the goal). Expert demonstrations were collected, so the expert's trajectory did not reach the target directly but first passed through an L-shaped maze. The agent is expected to learn to imitate this by passing through the maze. We train AMP and NEAR in this domain and visualise the learnt reward functions. In the case of NEAR, we visualise the energy function (ee(\u00b7, \u03c3) with \u03c3 = 20.0) and in the case of AMP we visualise the discriminator. The energy function is trained by training our modified NCSN on the expert state transitions in the maze domain. The discriminator is trained while training the AMP policy. Figure 1 shows two comparisons. We compare rew(s'|s) at a fixed state s in the maze at different training iterations. The energy-based reward function is stationary throughout training and rew(s'|s) only changes with s' for a fixed s. In contrast, the adversarial reward depends on the agent's policy and keeps changing to minimise the prediction for the samples in the policy's distribution PG.\nThere is no environment-provided reward function in this domain and the visualisations are obtained only by using the learnt rewards. Apart from visualisation and domain-related differences, the training regime for both NEAR and AMP in this task is identical to the training regime used in all other experiments in this paper. Please refer to Appendix B.2 for training details."}, {"title": "C EXTENDED RESULTS", "content": "We evaluate NEAR (Algorithm 1) on complex, physics-dependent, contact-rich humanoid motions such as stylised walking, running, and martial arts. The chosen task set demands an understanding of physical quantities such as gravity and the mass/moments of inertia of the character and contains a variety of fast, periodic, and high-acceleration motions. The expert's motions are obtained from the CMU and SFU motion capture datasets and contain trajectories of several motions. For each motion, a dataset of state transitions M = {(s, s')} is created to learn an imitation policy. Rewarding the agent for producing similar state transitions as the expert, incentivises the agent to also replicate the expert's unknown actions."}, {"title": "E ADVERSARIAL IL CHALLENGES & AMP DISCRIMINATOR EXPERIMENTS", "content": "In this section, we elaborate on the challenges of adversarial imitation learning and provide additional empirical results demonstrating instability and non-smoothness in the AMP discriminator. As briefly highlighted in Section 2, the root causes for the challenges of adversarial techniques are the"}, {"title": "E.1 AMP DISCRIMINATOR EXPERIMENTS", "content": null}, {"title": "E.1.1 HIGH DISCRIMINATOR VARIANCE", "content": "We conduct additional experiments to very the high variance in the adversarial motion priors (Peng et al., 2021) discriminator predictions. We train AMP on a humanoid walking task using the loss function from Equation (1) for the discriminator and Proximal Policy Optimisation (PPO) (Schul-man et al., 2017) to train the policy. The discriminator is slightly modified by removing the sigmoid activation at the output layer and instead computing the loss on sigmoid(D()) (same setup as the main experiments in this paper). Training is continued normally until some cut-off point. The cut-off point is varied across runs to obtain varying levels of intersection between supp(PD) and supp(pc). Then, with the policy updates paused, we continue training the discriminator to maintain the learnt decision boundary and visualise the variance in the trained discriminator's predictions on the motions generated by an unchanging policy. We hypothesise that as training continues and the supports of the two distributions get closer, the discriminator is less likely to see samples from a region outside supp(PD) Usupp(pc), meaning that its variance reduces as training progresses. We observe that the discriminator's predictions indeed have quite a high variance and the range of the predictions varies vastly across training levels (Figure 7). Further, the variance indeed reduces over the training level, indicating a gradually increasing intersection between supp(PD) and supp(PG). The adversarial optimisation is likely to get stabilised as the policy gets closer to optimality, however, training for the most part is still rather unstable because of the high variance in the discriminator's predictions."}, {"title": "E.1.2 DISCRIMINATOR NON-SMOOTHNESS", "content": "We also conduct experiments to understand the smoothness of the learnt reward function and its changes over training iterations (Figure 8). To do this, we again train AMP on a humanoid walking task. This time we do not modify the algorithm and simply evaluate the discriminator at gradually increasing distances from the true data manifold at various points in training. We find that the discriminator's predictions on average, decline as we move farther away from the true data manifold. However, again, the predictions are quite noisy and have a fairly large standard deviation."}, {"title": "E.1.3 PERFECT DISCRIMINATION", "content": "Finally, we replicate the experiments from Arjovsky & Bottou (2017) on adversarial IL (Figure 9). We use the same experimental setup as Appendix E.1.1 but now compute the accuracy with the output of the final Sigmoid layer. Here, instead of continuing the discriminator's training, we retrain the discriminator to distinguish between samples in the expert dataset PD and samples in pc (same"}], "equations": ["min_{D} E_{x \\sim p_D} [log D_{\\theta}(x)] + E_{s \\sim \\rho_{\\pi}} [log(1 - D_{\\theta}(W(\\pi_{\\theta_{\\varsigma}}(s))))]\nmax_{\\theta} J \\text{ where } \\nabla_{\\theta} J(\\pi_{\\theta_{\\varsigma}}) = E_{\\pi_{\\theta_{\\varsigma}}} [Q_{\\theta_{\\varsigma}}(s, a)\\nabla_{\\theta} log \\pi_{\\theta_{\\varsigma}}(s, a)]\n\\text{where } Q_{\\theta_{\\varsigma}}(s, a) = E_{\\pi_{\\theta_{\\varsigma}}} [log D_{\\theta}(W(\\pi_{\\theta_{\\varsigma}}(s))))]", "DSM(\\theta) = E_{P_D} E_{P_P}E_{x'\\sim \\mathcal{N}(x,\\sigma)}[ \\frac{||x' -"]}