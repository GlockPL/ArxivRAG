{"title": "Evaluate Summarization in Fine-Granularity: Auto Evaluation with LLM", "authors": ["Dong Yuan", "Eti Rastogi", "Fen Zhao", "Sagar Goyal", "Gautam Naik", "Sree Prasanna Rajagopal"], "abstract": "Due to the exponential growth of information and the need for efficient information consumption the task of summarization has gained paramount importance. Evaluating summarization accurately and objectively presents significant challenges, particularly when dealing with long and unstructured texts rich in content. Existing methods, such as ROUGE (Lin, 2004) and embedding similarities, often yield scores that have low correlation with human judge-ments and are also not intuitively understandable, making it difficult to gauge the true quality of the summaries. LLMs can mimic human in giving subjective reviews but subjective scores are hard to interpret and justify. They can be easily manipulated by altering the models and the tones of the prompts. In this paper, we in-troduce a novel evaluation methodology and tooling designed to address these challenges, providing a more comprehensive, accurate and interpretable assessment of summarization out-puts. Our method (SumAutoEval) proposes and evaluates metrics at varying granularity levels, giving objective scores on 4 key dimensions such as completeness, correctness, Alignment and readability. We empirically demonstrate, that SumAutoEval enhances the understanding of output quality with better human correlation.", "sections": [{"title": "1 Introduction", "content": "The LLMs, e.g. GPT-4, Claude, exceeded the summarization capabilities previously known. These models can generate high-quality summaries which are indistinguishable from human-written texts but also often suffer from missing information, hal-lucinations, misinterpretation of facts and various other forms of inaccuracies which are hard tYesah o identify and measure by both machine and human.\nThe unstructured nature of the text outputs for summarization task makes evaluation a specifically challenging task. Many tools like ROUGE score (Lin, 2004), embedding based cosine similarity have been used in the past but they tend to be non-intuitive score numbers, highly variant and most importantly correlates badly with human judge-ments. In LLM generated summarization world, these metrics are ineffcient in detecting missing details and hallucinations errors. The use of these metrics in critical domain-specific use cases such as medical note summarization can be even more concerning as the model can miss a lot of relevant information and still have good metrics.\nRecently, more LLM-based evaluation tech-niques (Liu et al., 2023) have emerged, aiming to replicate human ratings on summary quality and have even achieved strong correlations with human subjective ratings (Liu et al., 2023). Building on the framework defined by (Kry\u015bci\u0144ski et al., 2019), which includes Coherence, Consistency, Fluency, and Relevance as the four subjective dimensions for summarization evaluation, many subsequent works (Fabbri et al., 2021; Liu et al., 2023) have adopted these criteria. These works typically involve sub-jective ratings from 1 to 5 to assess summarization. However, these metrics are often seen as not suffi-ciently objective and concrete, making it challeng-ing for models to excel and provide mathematically explainable results.\nIn this paper, we propose a new set of metrics that are both machine-readable and reflective of the four key subjective aspects of summarization in a measurable manner. Additionally, we introduce an innovative LLM-based framework designed to evaluate these metrics effectively. Our novel frame-work is designed to objectively evaluate the qual-ity of summarized text based on four key pillars: Completeness, Correctness, Organization, and Readability. Much like the process of scoring an answer sheet according to a precise answer key, our method ensures that each aspect of the text is mea-sured in an objective and quantifiable manner. This approach not only provides a clear understanding"}, {"title": "2 Metrics", "content": "To thoroughly evaluate a summary, we define four key aspects of quality. These metrics en-compass objective criteria that are more machine-understandable as opposed to subjective measures.\nWe use the term source to refer to the text that needs to be summarized, target note to refer to the summary being evaluated, and ground truth note to denote the ground-truth summary of the source, which is accurate, detailed, well-organized, and curated by expert humans.\nCompleteness: This aspect measures the extent to which relevant information from the ground truth note is covered in the target note. It penalizes any relevant information from the ground truth note that is missing in the target note.\nCorrectness: This metric evaluates the accuracy of information presented in the target note. It iden-tifies and penalizes hallucinations, misinterpreta-tions, incorrect facts, and other inaccuracies in the target note.\nAlignment: This criterion assesses how well the information in the target note aligns with that in the ground truth note. It penalizes irrelevant or extraneous information included in the target note. In medical note summarization, this metric is cru-cial because the mere presence of information is insufficient; it must also be placed in the correct section and under the right category. For example, referencing imaging scans like an X-ray is appro-priate and aligned only if it is recorded under the Subjective-Imaging section and not elsewhere in the note.\nReadability: This metric grades the target note on professional writing quality. It is penalized by awkward sentence flow, grammatical errors, inap-propriate language, and spelling mistakes.\nAs defined in SummEval (Fabbri et al., 2021), summarization is broadly evaluated on four key metrics - Consistency, Relevance, Coherence and Fluency. However, in our observation these are not enough and needs to be broken down into more granular and measurable metrics. For instance, Rel-evance handedly covers two key parameters - miss-ing information which we measure in Complete-ness and presence of extra or irrelevant information which is represented by Alignment in our metrics. Similarly, Coherence can be mapped to Readability-Awkward Flow, Fluency to Readability-Grmmar and Spelling Errors and Consistencuy to Correct-ness."}, {"title": "3 Method", "content": "This section introduces the technical details of our evaluation method."}, {"title": "3.1 Entity Extraction", "content": "To extract key entities from a summary accurately, we designed a three-step solution. We define an En-tity as a unit of information, typically a short phrase that is both concise and complete in its meaning. An Entity contains exactly one key concept that is essential to the summary and without which the summary would bre considered incomplete.\n1. Extraction. An initial step to break down the note into short phrases, emphasizing each phrase should have an intact meaning, and also only have one key point. For example, \"A and B are both normal when C happens\" will be two \"A is nor-mal when C happens\" and \"B is normal when C happens\".\n2. Self-Verification. This step enables the model to verify its generated output and merge contextu-ally similar entities. For instance, an isolated date holds little significance unless it is connected to another action or context-related entity.\n3. Reference Sourcing. This prompt assists in identifying the original phrases or sentences to sup-port the entities. It represents the final phase of re-finement, where unsupported entities are removed. Additionally, the phrases or sentences, Entity Refer-ence, which pertains to the word phrases from the original note, is used to calculate correlation with human inputs on entities."}, {"title": "3.2 Metrics Calculation", "content": "With target note and the extracted target note enti-ties, ground truth note, and ground truth note enti-ties, we collect entity level results using a unified prompt template for Completeness, Correctness, and Alignment.\nSystem Message: [Identity] [Goal]\nUser Message:\n[Task Description]\n[Output Options]\n[Guideline]\n[Examples]\n[Reference Materials]\n[Question]\n[Chain of Thoughts Request]\n[Output Formatting]\nBased on different evaluation tasks, Identity and Goal are set differently. For example, for medical note summarization task, Identity is \"You are a help-ful assistant good reviewing medical note\". Task Description gives description about the task, e.g. \"verify the correctness of an entity by referencing the source transcript\". Output Options describes the possible output answers. Guideline and Exam-ple gives concrete interpretation of how to solve the task. Reference Materials are either ground truth note, source or other material that model can use as a reference for answering the question.\nInspired by self-consistency prompt (Wang et al., 2022), to further improve the accuracy of the met-rics, for classification tasks, we design Consistency Prompts, which has multiple different prompts to evaluate from different angles and then aggregate the answers to get more accurate results. The rea-son why we do not use self consistency prompt to improve accuracy here, is from our practical expe-rience, sending the same prompt to LLM, a lot of times they return the exact the same output, which does not improve the accuracy comparing to our Consistency Prompts."}, {"title": "3.2.1\nCompleteness", "content": "To measure the missing ratio of the ground truth rubric entity to reflect note's completeness, we de-signed multiple (currently three) prompts to mea-sure the \"missing\" property of an entity from the ground truth note.\nPrompt 1. Checks whether a given entity \"is present\" in the target note or not.\nPrompt 2. Analyzes the \"concept coverage\" of information from the entity, aka the information within entity is covered in the target note or not.\nPrompt 3. Finds relevant materials from the target note to support the entity, can we find enough mate-rials/information to support the information in the entity or not.\nThese prompts aim the same goal, but guide the model to analyze from different perspective to find out the answer.\nHowever, this approach may incorrectly label information that is inaccurate or contradicts the ground truth as still being complete. To mitigate this issue, we include \"partial\" and \"contradict\" as options in the Output Option. An example of output options definition:\nYes: The concept content is covered in the target note.\nNo: None of the key points in concept is covered in the target note.\nContradict: the target note mentioned rel-evant information related to the concept but contradicts or refutes it.\nPartially: While some elements from the concept are included in the target note, there are also key pieces of information from the concept that have not been inte-grated.\nEach output option is accompanied by a detailed explanation that includes the model's step-by-step reasoning and justification for its answers, specif-ically in relation to each of the given reference entities.\nFor all prompts, we aggregated the answers using a majority voting system for each entity, with specific exception rules. For instance, 'partial + partial +\nyes' is aggregated as 'yes.' If there is no agreement, the answer defaults to 'no.' We award 0.5 points to 'partial' responses and 1 point each for 'yes' responses to calculate the entity score\nAfter all entities are evaluated, we calculate miss-ing entity ratio on ground truth note to get com-pleteness metrics as follows -\n$Completeness Score = \\frac{Entity Score in the target note}{\\#Entities in the ground truth note}$ (1)\nMore advanced technology, like using graphic mod-eling tech to model each prompt's quality and po-tential answer could be used to further improve this scoring metric."}, {"title": "3.2.2 Correctness", "content": "Similar to completeness measurement, multiple prompts are designed to evaluate the target note's correctness from different perspectives. Both the ground truth note and source materials are provided within the prompt. The source materials serve to verify information that may not be covered in the ground truth but is still correct (e.g., additional de-tails that aren't relevant enough to be included in the ground truth note). While the prompt design aligns with that of completeness measurement, the output options differ. They include \"Yes,\" \"No,\" \"Partially,\" and \"Unknown.\" The \"Unknown\" op-tion is specifically designed to allow the model to more accurately capture cases where it cannot find any information to support or refute a given entity. Majority voting is also applied here to get the final entity level answer. Based on these entity-level an-swers, the incorrectness ratio is calculated to derive the correctness score\n$Correctness Score = \\frac{Entity Score in the target note}{\\#Entities in the target note}$ (2)"}, {"title": "3.2.3 Alignment", "content": "To evaluate whether an entity is relevant by deter-mining if it's correctly positioned in the appropriate section, we employ two different prompt designs. The first method involves checking by definition, utilizing the header, section name, or other prede-fined criteria to understand what should be included in that section. The second method involves verify-ing whether the entity appears in the corresponding section of the ground truth note. Both methods aim to identify irrelevant entities. Ideally, one of these prompts should work perfectly on its own, but in the real world, both tend to make mistakes and they compliment each other. Alignment score is generated by the ratio of correctly placed entities."}, {"title": "3.2.4 Readability", "content": "Readability concerns multiple aspects of the writ-ing style of the target note. It includes awk-ward flow, inappropriate language, grammar, and spelling issue. For awkward flow, prompt is de-signed to analyze the relationship among sentences to identify the broken logic. For inappropriate language, grammar, and spelling issue, prompt is designed to directly identify these issues, as it's straightforward for a language model. After an-swer aggregation, we add up all errors to calculate metric scores for readability."}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Baselines", "content": "Rouge. Rouge score measures the n-gram or longest common sequence similarity between two sequences on word level.\nBARTScore (Yuan et al., 2021) It measures the conditional probability of one text generating an-other to evaluate the text similarity based on the BART model.\nG-Eval (Liu et al., 2023) It prompts LLMs, e.g. GPT3.5, GPT-4, to evaluate the quality of summary to input source to give a 1-5 rating on Cosistency, Relevance, Coherence and Fluency."}, {"title": "4.2 Medical Note Summarization", "content": "Medical Note Summarization Dataset. We cu-rated a medical note summarization dataset. The source is de-identified real doctor and patient con-versation transcript data, summary is a note in-cluding Chief Compliant, Symptoms, Medications, Medical History, Family History, Surgical History Social History, Labs / Tests / Imaging etc. This dataset has 30 notes in total, the ground truth note is generated by human experts (with human errors). We use GPT-4-32k to generate note for these con-versations. The generated note (target note) is re-viewed by human against the ground truth note with both entity-level labeling and final review scores, note that this score is still subjective but it's based on the results of entity labeling.\nFirst, we analyze the correlation between solu-tions with human on entity labeling results in Ta-ble 1. Note that Rouge and BARTScore cannot measure Organization and Readability, so the com-parison is done among SumAutoEval variations and G-Eval in Table 1 and 2.\nWe use Cohen's Kappa score (or agreement) to analyze the correlation of human and solution as for each entity it's similar to a classification task. Note that, we don't use precision and recall as hu-man labels actually also have quite some errors. As shown in Table 1, SumAutoEval aligns with human better than Rouge and BARTScore solu-tions, this is because SumAutoEval uses LLM as backend, which has stronger capability in verify-ing entity's existence or correctness. Rouge and BARTScore does not cover the organization and readability as these two requires to do entity under-standing instead of only entity comparison across ground truth and target note. We also observe that GPT-40 model aligns with human much better than Sonnet in completeness, and correctness organiza-tion as these metrics require reasoning capability. But for readability, it's more sensation of word and narratives, Sonnet model is as powerful as GPT-40. Worth to note that, in Readability, the correlation"}, {"title": "4.3 SummEval", "content": "SummEval dataset (Fabbri et al., 2021) is designed specifically for evaluating the performance of auto-matic text summarization systems. It contains hu-man annotations and comprehensive benchmarks that help researchers and developers assess the qual-ity of summaries produced by various algorithms.\nTo effectively evaluate the quality of machine-generated summaries against human-compiled summaries, we employed a structured approach involving rubric creation, expert evaluation, and metric-based analysis.\nExpert Score Consistency and Averaging. We observed a notable variance in expert scores across different machine summaries. To address the incon-sistency in the original scores, we chose a machine summary (from the pool of summaries) where ex-pert evaluations were more consistent. We then averaged these consistent expert scores to create a reliable baseline. This average score was used as a benchmark for future comparisons. We refer to this new subset dataset as SummEval\u2020 Dataset.\nRubric Creation and Entity Filtering. We cre-ated a rubric by aggregating entities from the given human summaries. To ensure the relevance and significance of the entities included in the rubric, we filtered out any entities that were not present in at least five human summaries. This step was critical in focusing our evaluation on consistently important information, as determined by human experts. We used gpt-4 as the base model to run all our prompts.\nMetrics. In Table 3, we report the correlation be-tween experts and our autoEval ratings on several metrics: consistency, relevance, fluency and coher-ence. We also report the scores after excluding the outliers where we observed that the human score was erroneous as described in the Data Error Anal-ysis section below.\nConsistency To measure consistency, we fo-cused on the \"incorrect\" metric, which closely aligns with the definition of consistency. The score was calculated using the formula:\n$Consistency Score = \\frac{Total Entities - Incorrect Entities}{Total Entities}$ (3)\nThis score was then normalized to a scale ranging from 1 to 5.\nRelevance For relevance, we combined the \"missing\" and \"irrelevant\" metrics as the human experts were required to assess both the presence and relevance of entities. The percentage of found entities(partial or complete) in the rubric and the percentage of irrelevant entities in the test note were computed and averaged, providing a compre-hensive measure of relevance.\nFluency and Coherence To evaluate fluency and coherence, we utilized our Writing Issue metric. Fluency was assessed by identifying grammar and spelling errors, while coherence was determined through the presence of awkward flow in sentences. The score was calculated by determining the num-ber of sentences with issues and dividing it by the total number of sentences, thereby capturing the percentage of the note with issues:\n$Writing Issue Score = \\frac{Sentences with Issues}{Total Sentences}$ (4)"}, {"title": "4.3.1 Data Error Analysis", "content": "Relevance During the rubric creation process, we discovered two articles that lacked any common entities appearing in at least five human summaries, prompting us to exclude them from our calcula-tions. Additionally, we observed that in at least 10 instances the expert scores were not accurate. Since the experts were given a single summary cho-sen at random as a reference, we believe it might have contained unique entities not aligning with our methodology, which relies on common entities to identify the main theme of the article. These 10 examples were substantial enough to reduce our metrics by 23%. See Appendix A.1 for an example.\nCoherence We observed a pattern similar to what we found in relevance assessment: a few data sam-ples had ratings that did not match the autoEval results. After further analysis and human evalu-ation, we realized that the experts' ratings often did not accurately reflect the coherence of the sum-maries. To substantiate our findings, we included a reference example in the appendix A.2. Notably, a small number of data points (around 10) were sufficient to reduce the correlation by 20%.\nFluency Upon human analysis, we observed that most ratings disregarded capitalization errors and missing punctuation. Our methodology, developed to mimic a professional writer's standards, strongly penalizes errors in capitalization, lengthy sentences, and other similar issues. However, to maintain con-sistency with the definition of fluency as outlined in (Fabbri et al., 2021), we only included errors where there were serious grammar issues, such as missing subjects or extensive misuse/lack of punctuation, which made the summary difficult to comprehend. See Appendix A.3 for reference.\nConsistency Similar to the analysis above, we also observed that some machine summaries, which contained inaccurate information or misinterpreted facts from the article, were mistakenly rated as 5. An example is provided in the Appendix A.4 to illustrate this."}, {"title": "5 Related Work", "content": "Rouge based Evaluation. ROUGE (Lin, 2004), is a very popular evaluation metric and can be defined such that, ROUGE-N captures the similarity based off of exact overlap of N-grams. ROUGE-L is also often used, where L stands for the longest common sequence.\nEmbedding based Evaluation. Methods like BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) asses the embedding based sim-ilarity of the produced text with the ground truth at different token granularity levels using BERT based embeddings. Sub-Sentence Encoder (Chen et al., 2023) also introduces a method to extract and embed atomic propositions from a paragraph. These embeddings can then be used to infer the extent of information capture. (Zhong et al., 2022) trains specialized classifiers to evaluate each dimen-sion.\nLLM based Evaluation. There has been a lot of work (Gao et al., 2024) in using LLM's for evaluation. (Gao et al., 2023) used ChatGPT to annotate the outputs using Human-like methods such as Likert Scoring, Pyramid and outperformed commonly used model based metrics. (Chan et al., 2023) goes one step beyond to use muti-agent to discuss the evaluation. Research around training models specifically for evaluation has also shown promise - (Wang et al., 2023b) train a model to evaluate model outputs, whereas (Ye et al., 2023) and (Wang et al., 2023a) train a critique LLM. More recent work Prometheus builds a 13B model which has high correlation with GPT4 on evalua-tion scores along multiple preference dimensions. Using GPT4 itself as an evaluator using prompting as also become popular as in Alpaca (Taori et al.,"}, {"title": "6 Conclusion & Future Work", "content": "This paper proposed a fine-granularity evaluation method on summarization output and proves its ad-vantage over existing methods in capturing details and better alignment with humans in several dimen-sions. Later it's worth exploring how to combine the fine-granular way and overall evaluation into a single solution to generate a more comprehensive evaluation of a summarization note."}, {"title": "A Example Appendix", "content": ""}, {"title": "A.1 Outlier Example for Relevance", "content": "Article Id:\nc50d33e9749e7bb484d9b69c4f5fca35 a3a50cb5\nMachine Summary: jacob phillips, 23, was found dead at the end of the disused northern promenade . he ran from the driver, who gave chase at the seaside town of penarth . an inquest heard how the trio got out of the taxi to \u2018use an atm'\nExpert Scores: 4, 4, 4 (avg - 4)\nAutoEvalScore: 2\nReasoning: As per the rubric, the main theme of the article is: 1. Jacob Phillips, a 23-year-old accountant, fell to his death from a 70-foot cliff. 2. The incident occurred in December after a night out with friends. 3. He was running away from a taxi driver to avoid paying the fare. 4. Phillips and his friends did not have enough money to pay for the taxi fare.\nHowever, the given machine summary completely misses the point that the young man died because he was running away from a taxi driver to avoid paying the fare. The summary doesn't clearly explain why Jacob was running from the driver and how he died."}, {"title": "A.2 Outlier Example for Coherence", "content": "Article Id:\n4761dc6d8bdf56b9ada97104113dd1bcf4aed3f1\nMachine Summary: north pacific gray whale swam nearly 14,000 miles -lrb- 22,500 kilometers -rrb-. varvara, which is russian for \u201cbarbara, \" left her primary feeding ground off russia 's sakhalin island. varvara 's journey surpassed a record listed on the guinness worlds records website\nExpert Scores: 4,4,4 (avg -4)\nAutoEvalScore: 1\nReasoning:\nThe summary lacks a coherent flow of information. It jumps from the whale's distance swam to the meaning of its name and then to the record it broke without providing a smooth transition between these points. The summary could be improved by restructuring the sentences to provide a more logical progression of information. For example, it could start by introducing the whale and the record it broke, then explain the distance it swam, and finally mention where it started its journey."}, {"title": "A.3 Outlier Example for Fluency", "content": "Article Id:\ndm-test-4001b252a072ac149c70840b22299cc6cfab3bae\nMachine Summary: radamel falcao has scored four goals all season louis van gaal 's side . united will have to pay # 46million to make falcao 's transfer permanent . united are unlikely to take up that option.\nExpert Scores: 5,5,5(avg - 5)\nAutoEvalScore: 2.3\nReasoning:\nThe sentence \"radamel falcao has scored four goals all season louis van gaal 's side .\" is gram-matically incorrect. It should be rephrased to something like Radamel Falcao, who is part of Louis van Gaal's side, has scored four goals all season. Similarly, in the sentence united will have to pay # 46million to make falcao 's transfer permanent ., the symbol '#' is incorrect. It should be 'f' to represent British Pounds.\""}, {"title": "A.4 Outlier Example for Consistency", "content": "Article Id:\nd75b043ebefc3098aea84d92bb8bec0f509b1563\nMachine Summary: three of the militants were killed by iranian forces in the town of negur . jaish al adal claimed responsibility for the attack . the iranian state media says the militants crossed into the country . the militants have been killed in clashes with pakistan, iranian media says. the sunni muslim group says it is investigating the incident .\nExpert Scores: 5,5,5 (avg - 5)\nAutoEvalScore: 2.6\nReasoning: The statement the militants have been killed in clashes with Pakistan is not accurate according to the article. The article states that \"Eight Iranian border guards have been killed in clashes with militants near the border with Pakistan\" and \"Three of the militants were killed by Iranian forces in the fighting Monday in the southeastern town of Negur\". Therefore, the militants were not killed in clashes with Pakistan, but rather with Iranian forces. Also, the statement the sunni muslim group says it is investigating the incident is incorrect. The group only claimed responsibility for the attack. The investigation is being conducted by the security agencies of Pakistan, not the Sunni Muslim group."}]}