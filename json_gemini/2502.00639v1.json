{"title": "Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer", "authors": ["Tao Ren", "Zishi Zhang", "Zehao Li", "Jingyang Jiang", "Shentao Qin", "Guanghao Li", "Yan Li", "Yi Zheng", "Xinping Li", "Min Zhan", "Yijie Peng"], "abstract": "The probabilistic diffusion model (DM), generating content by inferencing through a recursive chain structure, has emerged as a powerful framework for visual generation. After pre-training on enormous unlabeled data, the model needs to be properly aligned to meet requirements for downstream applications. How to efficiently align the foundation DM is a crucial task. Contemporary methods are either based on Reinforcement Learning (RL) or truncated Backpropagation (BP). However, RL and truncated BP suffer from low sample efficiency and biased gradient estimation respectively, resulting in limited improvement or, even worse, complete training failure. To overcome the challenges, we propose the Recursive Likelihood Ratio (RLR) optimizer, a zeroth-order informed fine-tuning paradigm for DM. The zeroth-order gradient estimator enables the computation graph rearrangement within the recursive diffusive chain, making the RLR's gradient estimator an unbiased one with the lower variance than other methods. We provide theoretical guarantees for the performance of the RLR. Extensive experiments are conducted on image and video generation tasks to validate the superiority of the RLR. Furthermore, we propose a novel prompt technique that is natural for the RLR to achieve a synergistic effect. See our implementation at https://github.com/RTkenny/RLR-Opimtizer.", "sections": [{"title": "1. Introduction", "content": "Probabilistic diffusion model (DM) (Sohl-Dickstein et al., 2015; Ho et al., 2020; Zhang et al., 2024b) has emerged as a transformative framework in high-fidelity data generation, demonstrating unmatched capabilities in diverse applications such as image synthesis (Podell et al., 2023), video generation (Wang et al., 2023a), and multi-modal data modeling. These models operate by iteratively denoising noisy data representations, effectively capturing complex underlying distributions. Despite their success, even fine-tuning DMs remains a daunting challenge due to the computational and caching demands associated with gradient computation during training, especially for large-scale tasks (Clark et al., 2023). This challenge has limited the broader deployment of DM in dynamic and resource-constrained environments.\nIt is a natural way to fine-tune DMs via full backpropagation (BP) through all time steps (Rumelhart et al., 1986), which is theoretically functional, providing precise gradient estimation over the entire diffusion chain. However, the computational and memory overhead of BP scales prohibitively with the model size and the number of diffusion steps (Prabhudesai et al., 2023; Yuan et al., 2024; Clark et al., 2023; Prabhudesai et al., 2024), making full BP impractical for most real-world scenarios. Thus, truncating recursive differentiation becomes a common practice to alleviate memory constraints. But truncated BP suffers from structural bias, as it terminates gradient computation before sourcing to the input, only considering a limited subset of the diffusion chain. Fine-tuning based on a biased gradient estimation can inadvertently impair the optimization performance, resulting in model collapse, i.e., contents generated reducing to pure noise. Moreover, truncated BP fails to capture the multi-scale information across all time steps due to the absence of differentiation on early steps. The hierarchical feature of generation imposes a thorough gradient evaluation to retain fidelity from pixel to structural level.\nReinforcement learning (RL) (Schulman et al., 2017) as a gradient computation trick has enabled another branch of DM fine-tuning (Lee et al., 2023; Black et al., 2023; Wallace et al., 2024; Fan et al., 2024). It typically ignores the differentiable connection between steps and recovers the"}, {"title": "2. Related works", "content": "Diffusion Probabilistic Models. Denoising Diffusion Model (Ho et al., 2020) is one of the strongest models for generation tasks, especially for visual generation (Rombach et al., 2022; Peebles & Xie, 2023; Chen et al., 2024a). Extensive research has been conducted from theoretical and empirical perspectives (Song et al., 2020a; Karras et al., 2022; Song et al., 2020b). It has achieved phenomenal success in muti-modality generation, including image, video, audio, and 3D shapes. The DM is trained on enormous images and videos from the internet (Bain et al., 2021; Wang et al., 2023b; Schuhmann et al., 2022). Empowered by modern architecture (Vaswani, 2017), it has powerful learning capability for Pixel Space Distribution.\nAlignment and Post-training. After pre-training to learn the distribution of the targeted modality (Achiam et al., 2023; Kaplan et al., 2020), post-training is conducted to align the model toward specific preferences or tune the model to optimize a particular objective. RL has been utilized to align the foundation models toward various objectives (Ziegler et al., 2019; Lambert et al., 2022; Black et al., 2023). Supervised learning can also be applied to the post-training phase (Rafailov et al., 2024), either optimizing an equivalent objective (Wallace et al., 2024) or directly differentiating the reward model (Clark et al., 2023; Prabhudesai et al., 2023; 2024). For DM, most methods use a neural reward model to align the pre-trained model, and there has been a continual effort to design better reward models (He et al., 2024; Xu et al., 2024a;b).\nForward Learning Methods. After extensive exploration of model training using forward inferences only (see, e.g., Peng et al., 2022; Hinton, 2022), forward-learning methods (Salimans et al., 2017; Malladi et al., 2023; Chen et al., 2023; Jiang et al., 2024) based on stochastic gradient estimation have recently emerged as a promising alternative to classical BP for large-scale machine learning problems (Zhao et al., 2024; Zhang et al., 2024a). Subsequent research (Ren et al., 2025; Chen et al., 2024b) has further optimized computational and storage overhead from various perspectives, achieving greater efficiency."}, {"title": "3. Diffusion Model under Perturbation-based Analysis", "content": "DMs constituted of the forward process and the backward process work by injecting noises or perturbations into a recursive computation chain to generate data from a given distribution. By analyzing the final output and the inherent intermediate perturbation, a forward learning estimator is a free byproduct without \u201cwasting the noise\u201d."}, {"title": "3.1. Recursive Structure in Diffusion Model.", "content": "In the forward process, data points (e.g., images) are gradually corrupted with Gaussian noise over a series of time steps, resulting in a sequence of noisy samples. The forward process is represented by a stochastic transformation $q(x_t|x_0)$, where $x_0$ is the original data, and $x_t$ is the noisy sample at time t. The ultimate goal is for the model to learn the reverse process, starting from pure noise $x_T$ and progressively recovering the original data $x_0$.\nThe training objective is to learn the reverse diffusion process by predicting the noise added at each time step. This is achieved by minimizing the difference between the predicted noise and the actual noise added in the forward process. The objective is defined as follows:\n$\\min_\\theta E_{q(x_0),t\\sim U(0,T),q(x_t|x_0)} [||\\epsilon_\\theta(x_t, t) - \\epsilon||^2],$ (1)\nwhere $\\epsilon_\\theta(x_t, t)$ is the predicted noise at time $t$, and $\\epsilon$ is the actual noise added during the forward process. The model\u2019s neural network $\\epsilon_\\theta$ is trained to accurately predict this noise, enabling it to progressively denoise data during generation.\nThe backward process of the DM can be generally expressed by a stochastic differential equation (Karras et al., 2022; Song et al., 2020b):\n$dx_t = (h(x_t,t) + g(t)\\epsilon_\\theta(x_t,t))dt + dw_t,$ (2)\nwhere $h(x_t, t)$ and $g(t)$ have analytical forms, and $dw_t$ is the stochastic drift term. So the discretization to Equation (2) can be formulated as\n$x_{t-1} = \\phi_t(x_t; \\theta) + z_t,$ (3)\nwhere $\\phi_t(x_t; \\theta)$ denotes the deterministic part of the discretization, and $z_t = \\sigma t \\epsilon$ stands for the stochastic part for $1 \\leq t \\leq T$. Consider a general form of the sampling procedure:\n$x_{t-1} = \\phi_t(x_t, z_t; \\theta) = \\phi_t(x_t; \\theta) + \\sigma t \\epsilon.$ (4)\nThen, when the backward process is deterministic, we define the diffusion chain as\n$x_0 = \\Phi_{1:T}(x_T, z_{1:T}; \\theta) = \\phi_1 \\circ ... \\circ \\phi_T(x_T, z_{1:T}; \\theta) = \\Phi_1(\\Phi_{2:T}(x_T, z_{2:T}; \\theta), z_1; \\theta) = \\Phi_{1:T-1}(\\phi_T(x_T, z_T; \\theta), z_{1:T-1}; \\theta) = \\Phi_1(\\Phi_{2:T-1}(\\phi_T(x_T, z_T; \\theta), z_{2:T-1}; \\theta), z_1; \\theta).$ (5)\nAfter the pre-training phase, we need to conduct post-training to align the model to generate visually pleasing"}, {"title": "3.2. Perturbation-based Gradient Estimators", "content": "The LR method, also called the score function method, is a perturbation-based gradient estimation approach used in neural network training (Jiang et al., 2024; Ren et al., 2025). It provides an alternative to BP by bypassing recursive computations for the backward chain, making it computationally friendly and memory efficient. Especially, when applied to differentiate a very deep network or long time horizon of RNN, LR serves as an efficient surrogate gradient estimator for BP. There are two types of LR estimators, both of which have been proven to be unbiased in the literature (Glasserman, 1990; Jiang et al., 2024).\nHalf-Order (HO) Estimator The first version borrows the intermediate noise as the perturbation to perform gradient estimation in Equation (3): $x_{t-1} = \\phi_t(x_t, z_t; \\theta) = \\phi(x_t;\\theta) + z_t$, where $z_t$ is the inherent noise of the DM following Gaussian distribution: $z_t = \\sigma t \\epsilon \\sim N(0,\\sigma_t^2 I)$.\nThe gradient estimator for the first version takes the form:\n$R(x_0) \\Sigma_t Dj_{h:j+h-1}(x_{j:h}, z_{j:h};\\theta)\\nabla \\ln f(z_j),$ (7)\nwhere $f(\\cdot)$ denotes the noise density, and $D_\\theta \\phi \\in R^{d\\phi \\times d\\theta}$ denotes the Jacobian matrix of $\\phi \\in R^{d\\phi}$ with respect to $\\theta \\in R^{d\\theta}$. Since this estimator enables a local differentiation chain, $Dj_{h:j+h-1}(x_{j:h}, z_{j:h}; \\theta)$, of length h after the perturbation $z_h$, it is a combination of Zeroth- and First-Order optimization, which we refer to as a Half-Order estimator.\nMoreover, RL in DM training is fundamentally an HO method with a local chain length of 1 (Jiang et al., 2024). Action sampling can also be interpreted as perturbations applied to the model's latent variables. However, it encounters"}, {"title": "Zeroth-Order (ZO) Estimator", "content": "The second version of LR directly injects exogenous noise to the parameters of the targeted module, $x_{t-1} = \\phi_t(x_t, z_t; \\theta) = \\phi(x_t; \\theta + z_t)$.\nThe estimator is then given by\n$\\Sigma_{t=1}^T R(x_0) \\nabla \\ln f(z_t),$ (8)\nwhich is commonly identified as the Zeroth-Order estimator. It is completely blind to all structural information from the forward computation, relying solely on output evaluations to estimate gradients. The well-known Simultaneous Perturbation Stochastic Approximation (SPSA) is covered by this version, when the antithetic variable trick is applied to perform randomized finite differences (Ren et al., 2025).\nFirst-Order (FO) Estimator As a contrasting extreme, BP is an FO method that requires the entire computational process to be pathwise differentiable. It leverages the chain rule of differentiation to compute gradients through the diffusive chain and the reward model, expressed as\n$\\nabla_\\theta R(x_0) = [\\Pi_{i=1}^{T} \\frac{\\partial \\phi_1(x_1, z_1; \\theta)}{\\partial x_0} + \\Sigma_{i=2}^{T} [\\Pi_{j=1}^{i-1} \\frac{\\partial \\phi_i(x_i, z_i; \\theta)}{\\partial (x_j, z_j; \\theta)} ] \\frac{\\partial R(x_0)}{\\partial x_0}$ (9)\nThis approach ensures efficient gradient computation by utilizing the differentiable structure of the model.\nThe unbiasedness of the above estimators is provided in subsequent Theorem 5.1. We will compare their properties and then build a new RLR estimator based on their different merits in the next section."}, {"title": "4. Bias and Variance of Gradient Estimators", "content": "From the perspective of stochastic gradient estimation, there are three types of estimators for fine-tuning DM. BP and RL are covered by the aforementioned estimators, utilizing the differentiability or the inherent noise in the DM. We will further analyze the bias and variance of gradient estimations, motivating the formulation of the RLR optimizer."}, {"title": "4.1. Bias Analysis", "content": "The stochastic nature of the DM results in the variance of the FO estimator. As expected, the variance of the FO estimator is lower than that of the HO and ZO estimators because the differentiation leverages the precise structure of the neural network.\nPerforming full BP over the diffusive chain during training is computationally infeasible due to the large memory requirements. For example, training Stable Diffusion 1.4 with a batch size of 1 and 50 time steps would require approximately 1TB of GPU RAM (Prabhudesai et al., 2023).\nTo alleviate the memory burden of full-step BP, the truncated variant is often employed, where the total number of time steps T in Equation (9) is replaced with a smaller surrogate $T' \\ll T$. However, the truncation introduces a structural bias into the gradient estimator. We have the following proposition to justify this structural bias.\nProposition 4.1. Assume R and $\\phi$ are differentiable almost everywhere, R and $\\phi$ have bounded gradients or Lipschitz constants, then the FO estimator is unbiased. However, the truncate BP estimator $\\nabla_\\theta R(x_0)_{truncated}$ has a structural bias, which can be specified as below:\n$\\nabla E[R(x_0)] - E[\\nabla_\\theta R(x_0)_{truncated}] = E_{Z_{1:T}} [\\Sigma_{i=T'-1}^{T} [\\Pi_{j=1}^{i-1} \\frac{\\partial \\phi(x_i; \\theta)}{\\partial (x_j, z_j; \\theta)} ] \\frac{\\partial R(x_0)}{\\partial x_0}$ where $\\phi (x_j; \\theta)$ denotes $\\phi_j(x_j, z_j; \\theta)$ for simplicity.\nThe bias in the estimator can lead to numerical instability, resulting in suboptimal updates or even training failure, as the truncated gradient may not be a descent direction. The undesired consequences are as follows.\n\u2022 Model collapse: The model may fail to converge, degenerating into generating pure noise due to unstable optimization.\n\u2022 Failure to capture multi-scale information: By limiting the gradient computation to fewer time steps, truncated BP loses essential multi-scale visual information along the diffusive chain."}, {"title": "4.2. Variance Analysis", "content": "We know that FO, HO and ZO estimators are all unbiased. However, BP introduces significant computational and storage overhead. The following proposition demonstrates that this additional cost is, to some extent, justified, as BP leverages accurate internal structural to reduce estimation variance, and therefore the variance of BP is lower than zeroth-order methods.\nProposition 4.2. Under Assumptions (A.1-3) in the Appendix, the variance of FO estimators in Equation (9) is less or equal to ZO estimators in Equation (8), i.e.\n$Var(\\nabla_\\theta R(x_0)) \\le Var(R(x_0) \\nabla \\ln f(z)).$ (10)\nAdditionally, it is straightforward to conclude that the variance of HO estimator is also less or equal to that of ZO estimator, as it is essentially an FO estimator with a perturbation at the end of the chain. This argument is also discussed in (Salimans et al., 2017). The following Table 1 presents all the gradient computation methods discussed above."}, {"title": "5. Recursive Likelihood Ratio Optimizer", "content": "As summarized by Table 1, in DM fine-tuning, the full BP imposes prohibitively high computational and memory requirements, while RL suffers from excessively high variance. In this section, we propose the ZO-informed RLR optimizer, which integrates the above three gradient estimators within a recursive framework. The RLR estimator is an"}, {"title": "Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer", "content": "unbiased gradient estimator that pursues minimal variance under a \"feasible\" computational and memory budget. The RLR estimator is constituted of three parts and takes the form:\n$\\frac{\\partial\\phi(x_1;\\theta)}{\\partial x_0} \\frac{dR(x_0)}{dx_0} + \\Sigma_{i\\in C} R(x_0) \\nabla \\ln f(z_i).$ (11)\nwhere $j \\sim U(1,T \u2013 h) = U({1, 2, ...,T \u2013 h})$, $C = {1,2,...,T} \\backslash {j, j + 1, ..., j + h}$, $z = {z_i, z_j}$ and $z_i \\sim N(0, \\sigma_0^2 I)$, $z_j \\sim N(0, \\sigma_0^2 I)$ independently.\nDifferentiating the reward model. In the first part of the RLR estimator, we differentiate the reward model and the first time-step of the diffusive chain, as shown in Figure 2. With the availability of the reward model structure, it is unnecessary to use zeroth-order methods to bypass BP.\nFixed-horizon half-order optimization. The generation process of the DM follows a coarse-to-fine structure, with each time-step in the chain controlling a different scale of generation. Incorporating precise gradient information from every time-step is essential, but full BP is computationally prohibitive. Truncated BP introduces bias, while zeroth-order optimization leads to high variance by ignoring structural information. To address this, the second part of RLR applies half-order optimization on a randomly selected h-length sub-chain, ensuring unbiasedness while minimizing variance. Specifically, before each optimization step, a time-step $j \\sim U(1,T \u2013 h)$ is uniformly sampled, natural perturbations are applied to the corresponding latent variable, and localized BP is performed over the h-length sub-chain, where the hype-parameter h is chosen based on the available computation budget.\nSurrogate estimator via parameter perturbation. For the remaining times steps, $C = {1,2,...,T} \\backslash {j, j + 1, ..., j + h}$, we inject noise directly into the model's parameters to construct an ZO estimator. This approach does not require caching intermediate latent variables or performing BP, making it computationally cheap.\nOverall, the RLR reorganizes the recursive computation chain by perturbation-based estimation, seamlessly integrating zeroth-order, half-order, and first-order optimization techniques. RLR strikes a balance between computational cost and gradient accuracy, achieving both unbiasedness and"}, {"title": "Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer", "content": "low variance. The following Theorem 5.1 establishes its unbiasedness.\nTheorem 5.1. The RLR estimator is an unbiased estimator:\n$\\nabla_\\theta E[R(\\phi_{1:T}(x_T; \\theta))] = E_{z \\sim f(z), j \\sim U(1,T-h)} [\\frac{\\partial\\phi(x_1;\\theta)}{\\partial x_0} \\frac{dR(x_0)}{dx_0} + \\Sigma_{i\\in C} R(x_0) \\nabla \\ln f(z_i) + R(x)D_{h:j+h}(x_{j+h}; \\theta)\\nabla \\ln f(z_j)$ (12)\nThe variance of the RLR estimator, denoted by $\\sigma_{RLR}^2$, is discussed in the appendix. Under limited computational resources where full BP is infeasible, RLR achieves substantially lower variance compared to other unbiased gradient estimators. Finally, the convergence rate of RLR is provided in the following Theorem 5.2.\nTheorem 5.2. Suppose that the reward function $R(\\cdot)$ is L-smooth. By appropriately selecting the step size, the convergence rate of the RLR is given by\n$\\frac{1}{K+1} \\Sigma_{k=0}^{K} E(||\\nabla R(\\theta_k)||^2) \\le \\frac{8L \\Delta \\sigma_{RLR}^2}{K+1} + \\frac{2L \\Delta}{K+1}$ where $K$ is the number of iterations, $\\theta_k$ is the trainable parameter in the $k$-th iteration, and $\\Delta = |R(\\theta_0) \u2013 R^*|$ is difference between initialization performance and optimal performance."}, {"title": "6. Experiments", "content": "We verify the superiority and applicability of the RLR optimizer against various baselines on two generation tasks: Text2Image and Text2Video. Finally, we propose a novel prompt technique that is natural for the RLR optimizer to further demonstrate the intuition of RLR and applicability."}, {"title": "6.1. Setting", "content": "Prompts dataset. We use Pick-a-Pic (Kirstain et al., 2023) and HPD v2 (Wu et al., 2023) for the Text2Image experiments. We report the performance of the trained model on unseen prompts from the training phase. For the Text2Video task, we prompt ChatGPT to generate a series of prompts that describe motions and train models under the prompts. After the training, we evaluate the model performance on the unseen prompts from the vbench (Huang et al., 2024).\nReward model and benchmark. We adopt multiple human preference reward models to train and test our methods, including PickScore (Kirstain et al., 2023), HPS v2 (Wu et al., 2023), and ImageReward (Xu et al., 2024b). All the models are trained on large-scale preference datasets. We"}, {"title": "6.2. Text2Image Generation", "content": "We evaluate our methods on two DMs: Stable Diffusion 1.4 and 2.1 (Rombach et al., 2022). As shown in Table 2, the RLR methods achieve higher reward scores on the unseen prompts from the test set. The RL-based method have limited improvement with respect to the base model, due to the sample inefficiency nature. Alignprop has considerable improvement over the base model. However, the biased"}, {"title": "6.2.1. SAMPLE EFFICIENCY ANALYSIS", "content": "The compare the sample efficiency and the variance of different methods, we show the reward curves of SD 1.4 when training on the AES and HPS v2 models in Figure 4. The DDPO optimizes the reward in a very slow pace, indicating high variance and low sample efficiency. In the earlier phase, the AlignProp has comparable performance as the RLR. In the later phase, while the RLR can continue to improve the reward, the AlignProp suffers from severe model collapse."}, {"title": "6.3. Text2Video Generation", "content": "We compare our RLR not only with RL and truncated BP but also with a series of open-source or API-based Text2Video"}, {"title": "6.4. Diffusive Chain-of-Thought", "content": "Furthermore, we propose the Diffusive Chain-of-Thought (DCoT), a prompt technique, that is natural for our RLR optimizer to demonstrate the applicability of our method.\nIt is worth noting that the generation process of the DM is essentially a multi-scale or coarse-to-fine process, first generating global structures and then focusing on local details. We propose dividing all the diffusion process steps into three groups: coarse-level, mid-level, and fine-level. The coarse-level chain includes steps adjacent to the initial noise, focusing on generating a rough outline. The fine-level chain includes steps adjacent to the final output, focusing on the fine-grained details. The mid-level chain in between focuses on the geometric structure of the content.\nThe idea of DCoT is shown in Figure 8, which converts the original prompt into multi-scale prompts reflecting the coarse-to-fine nature. Different generation steps should conditioned on different prompts instead of conditioned on the same prompt."}, {"title": "6.5. Ablation Study", "content": "We conduct the ablation study, using SD 1.4 and HPD v2, to verify the contribution of different parts in the RLR optimizer. In Table 5, we evaluate the RLR and its variant (V1: the RLR without HO and ZO; V2: the RLR without ZO; V3: the RLR without HO). The V1 performs the worst since it actually reduces to the truncated BP with only one time-step. The V2 and V3 perform better than the V1. It is worth noting that the V2 is better than the V3. The V2 without HO is actually an unbiased estimator since it takes all time steps into account when estimating the gradient. Even though the V2 rearranges the computational graph by the HO, it still a biased estimator. This phenomena indicates the importance of unbiasness when conducting the fine-tuning task."}, {"title": "7. Conclusion", "content": "In this paper, we propose a novel zeroth-informed optimizer for the alignment of the DM. We give a perturbation-based analysis for the diffusion chain and draw theoretical insight into the bias, variance, and convergence of the RLR estimator and optimizer. The empirical study on Text2Image and Text2Video is conducted. Our RLR outperforms all baselines under various metrics. Furthermore, we propose a prompt technique that is natural for the RLR optimizer to validate the applicability."}, {"title": "A.7. Proofs", "content": "The interchange of derivative and expectation can be justified by the dominated convergence theorem (Rudin, 1987). Furthermore, the condition of uniform integrability, which is necessary to ensure the safe exchange of limit operations with expectation, can be relaxed by employing a Lipschitz condition (Glasserman, 1990).\nBy chain rule and $x_0 (z; \\theta) = \\phi_{1:T}(x_T, z; \\theta) = \\phi_1 (x_1, z_1; \\theta)$, we have"}, {"title": "A.7.1. PROOF OF PROPOSITION 4.1", "content": "$\\frac{\\partial R(x_0(z; \\theta))}{\\partial \\theta} = \\frac{dR(x_0)}{dx_0} \\frac{\\partial x_0(z; \\theta)}{\\partial \\theta}$ = $\\frac{dR(x_0)}{dx_0} \\frac{\\partial \\phi(x_1, z_1; \\theta)}{\\partial \\theta} + \\frac{dR(x_0)}{dx_0} [\\frac{\\partial \\phi(x_1, z_1; \\theta)}{\\partial x_1} \\frac{\\partial x_1(z_2:T; \\theta)}{\\partial \\theta} ]+... +\\frac{dR(x_0)}{dx_0} [\\frac{\\partial \\phi(x_1, z_1; \\theta)}{\\partial x_1} \\frac{\\partial \\phi(x_2, z_2;\\theta)}{\\partial x_2}(\\frac{\\partial x_2(z_3:T; \\theta)}{\\partial \\theta}) ] +... +\\frac{dR(x_0)}{dx_0} [\\Pi_{j=1}^{i-1} \\frac{\\partial \\phi(x_i; \\theta)}{\\partial x_j} ]$\\\\\n=$\n\\frac{dR(x_0)}{dx_0} \\frac{\\partial \\phi(x_1, z_1; \\theta)}{\\partial \\theta} +\\Sigma_{i=2}^{T} [\\Pi_{j=1}^{i-1} \\frac{\\partial \\phi(x_i; \\theta)}{\\partial x_j} ]$.Therefore, we can reach the conclusion that  $E[\\nabla_\\theta R(x_0(z; \\theta))] = E[\\nabla_\\theta R(x_0(z; \\theta))]$. +...+ .\\frac{\\partial R(x_0)}{dx_0} [\\frac{\\partial \\phi(x_1, z_1; \\theta)}{\\partial x_1} \\frac{\\partial \\phi(x_2, z_2;\\theta)}{\\partial x_2}(\\frac{\\partial x_2(z_3:T; \\theta)}{\\partial \\theta}) ] +... +\\frac{dR(x_0)}{dx_0} [\\Pi_{j=1}^{i-1} \\frac{\\partial \\phi(x_i; \\theta)}{\\partial x_j} ]$\\\\\n=$\n[\\frac{\\partial \\phi_i(x_i; \\theta)}{\\partial (x_j, z_j; \\theta)} ] \\frac{\\partial R(x_0)}{\\partial x_0}  (13)which means the unbiasedness of the FO estimator.Furthermore, the truncated BP estimator is $ \\nabla_\\theta E[R(x_0)] truncated=\\frac{\\partial \\phi(x_1, z_1; \\theta)}{\\partial \\theta} + [\\Pi_{i=1}^{T'-1} \\frac{\\partial \\phi_i(x_i; \\theta)}{\\partial x_i}] +\\Sigma_{i=2}^{T} [\\Pi_{j=1}^{i-1} \\frac{\\partial \\phi_i(x_i; \\theta)}{\\partial (x_j, z_j; \\theta)} ]  (14). The structural bias of the truncated BP estimator can be specified by combining the above results: $ \\nabla E[R(x_0)] - E[\\nabla_\\theta R(x_0) truncated]= Ez1:T $\\Sigma_{i=T'-1}^{T} [\\Pi_{j=1}^{i-1} \\frac{\\partial \\phi(x_i; \\theta)}{\\partial (x_j, z_j; \\theta)} ] $ $\\frac{\\partial R(x_0)}{\\partial x_0}  (13).So we end the proof."}, {"title": "A.7.2. PROOF OF PROPOSITION 4.2 AND ADDITIONAL ASSUMPTIONS", "content": "Assumption A.1. Define R(z; \u03b8) as R(x0(z; \u03b8)). Assume that R(z; \u03b8) is differentiable with respect to \u03b8 almost surely, P(R(\u03b8) = r) = 0 for every r \u2208 R, and Lipschitz condition holds for every \u03b81 and \u03b82:|R(z; \u03b81) - R(z; \u03b82)| \u2264 m1(z)|\u03b81 - \u03b82|, where m1(z) is integrable.\nAssumption A.2. For any xt, whose randomness comes from z, the density f(xt; \u03b8) is differentiable with respect to \u03b8, and uniform integrability holds: sup$\\frac{\\partial}{\\partial\u03b8} R(x1;\u03b8) f(x1;\u03b8) \u2264 m2(x1), where m2(xt) is integrable.\nAssumption A.3. R(z; \u03b8) is twice continuously differentiable. The following functions are integrable: m1(\u00b7)2, m2(\u00b7)2, supe |R(\u00b7;\u03b8)| \u00d7 m1(\u00b7), supe |R(\u00b7;\u03b8)| \u00d7 supe |R\u201d(\u00b7;\u03b8)|.\nProof of Proposition 4.2. Since R(z; \u03b8) is the reward function and the random variables z1:T are Gaussian distributions in our case, it is easy to check the above assumptions are satisfied. By applying the Theorem 2 in (Cui et al., 2020), we can reach the conclusion."}, {"title": "A.7.3. PROOF OF THEOREM 5.1", "content": "Proof. The RLR estimator contains three parts: FO estimator terms, HO estimator terms, and ZO estimator terms. For simplicity, we can consider there terms with an FO estimator term, an HO estimator term, and an ZO estimator term.Substituting the specific form of the iteration process, we have $x_0 = \\varphi(x_1, z_1; \\theta)$,  $x_1 = \\varphi(x_2; \\theta + z_2)$,  $x_2 = \\varphi(x_3; \\theta) + z_3$,where $x_1 = \\varphi(x_2; \\theta + z_2)$ is an HO estimator term and $x_2 = \\varphi(x_3; \\"}]}