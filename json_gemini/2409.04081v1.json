{"title": "UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity", "authors": ["Yicheng Fu", "Jianpeng Cheng", "Raviteja Anantha", "Etai Littwin", "Prabal Vashisht"], "abstract": "Generating user intent from a sequence of user interface (UI) actions is a core challenge in comprehensive UI understanding. Recent advancements in multimodal large language models (MLLMs) have led to substantial progress in this area, but their demands for extensive model parameters, computing power, and high latency makes them impractical for scenarios requiring lightweight, on-device solutions with low latency or heightened privacy. Additionally, the lack of high-quality datasets has hindered the development of such lightweight models. To address these challenges, we propose UI-JEPA, a novel framework that employs masking strategies to learn abstract UI embeddings from unlabeled data through self-supervised learning, combined with an LLM decoder fine-tuned for user intent prediction. We also introduce two new UI-grounded multimodal datasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed for few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos across 219 intent categories, while IIT contains ~900 videos across 10 categories. We establish the first baselines for these datasets, showing that representations learned using a JEPA-style objective, combined with an LLM decoder, can achieve user intent predictions that match the performance of state-of-the-art large MLLMs, but with significantly reduced annotation and deployment resources. Measured by intent similarity scores, UI-JEPA outperforms GPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged across two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x reduction in computational cost and a 6.6x improvement in latency in the IIW dataset. These results underscore the effectiveness of UI-JEPA, highlighting its potential for lightweight, high-performance UI understanding.", "sections": [{"title": "1 Introduction", "content": "As the use of smart devices for daily tasks increases, the user interface (UI) remains the primary medium through which humans interact with applications, either directly or via dialogue agents. Accurately perceiving UI actions and predicting user intent can significantly enhance dialog agents, providing valuable feedback on the success or failure of their actions. Moreover, an effective perception model can enable \"Multimodal Intent State Tracking\" (MIST)"}, {"title": "2 Related Work", "content": "Our work with UI-JEPA is focused on advancing UI understanding by significantly improving generalization capabilities and enabling automatic inference of user intent from UI interaction sequences with minimal reliance on annotation, memory, and computational resources during both training and inference. By using JEPA, we strive to achieve performance matching large state-of-the-art MLLMs while utilizing less video-text aligned training data. This work sits at the intersection of UI understanding, MLLMS, and self-supervised learning, pushing the capabilities of what is possible with lightweight MLLMs."}, {"title": "2.1 UI Understanding", "content": "Various machine learning models have been proposed to enhance UI understanding. Early efforts primarily focused on pre-training transformer-based models using large-scale, unlabeled UI data to learn generic feature representations at the UI component level. Other approaches have augmented model training with semantic information and heuristics to improve UI detection. However, these methods often fall short in understanding the concept of a task and fail to learn comprehensive visual representations, as they are limited to individual UI components. Some approaches utilize crawler models tailored to specific tasks, but these models struggle with scalability across a large number of tasks and exhibit limited generalization to unseen tasks. Additionally, methods that integrate image encoders with LLMs are generally confined to basic UI tasks, such as icon recognition and widget listing, and operate on static images, which hinders their ability to learn temporal relationships and the concept of a task.\nIn contrast, our approach processes videos that capture sequences of UI actions during task execution. By using a JEPA-based encoder, we learn video representations through self-supervised learning, and an LLM decoder to textual representations of the user intents. This method captures not only the temporal dynamics of UI interactions, but also offers a more holistic understanding of user tasks."}, {"title": "2.2 Multimodal Large Language Models", "content": "Recent Multimodal Large Language Models (MLLMs) have led to significant advancements in UI understanding, combining various data modalities for more precise intent predictions. Despite these improvements, the high computational demands of MLLMs often necessitate server-side processing, which drives up costs and limits their scalability, while potentially sacrificing privacy and imposing connectivity constraints.\nRecent research has focused on reducing model size while striving to maintain performance. Although these efforts have achieved some success, the resulting models still require substantial data and remain too large for efficient deployment on advanced mobile devices.\nIn comparison, our approach seeks to address these limitations by further compressing the model size and reducing data requirements. We achieve this without compromising performance, demonstrating that our method performs competitively with larger models in both few-shot and zero-shot scenarios, making it more suitable for mobile environments."}, {"title": "2.3 Self Supervised Learning", "content": "Obtaining large-scale labeled datasets for UI understanding, especially those that pair videos of UI actions with user intent labels, is costly and difficult to scale. Moreover, such datasets cannot capture every possible visual variation, making it essential to develop approaches that use unlabeled data and generalize well by learning robust abstract representations.\nSelf-supervised learning (SSL) has emerged as a promising solution to these challenges. For instance, VideoMAE uses SSL to pretrain vision transformers (ViTs) by masking and reconstructing random video cubes. Similarly, Joint Embedding Predictive Architecture (JEPA) and its derivatives, I-JEPA and V-JEPA , focus on learning semantic representations by ignoring irrelevant details and predicting masked spatio-temporal regions.\nOur approach builds on existing JEPA-based methods by employing a temporal masking strategy, where entire frames are masked rather than just patches. Additionally, we integrate a JEPA-based encoder with an LLM decoder, projecting video embeddings from ViT into the LLM input space using a dense layer and generating intent descriptions with a fine-tuned LoRA adapter."}, {"title": "3 The UI-JEPA Framework", "content": "Our goal is to track a mobile user's intent by analyzing their interactions with the UI and representing that user intent as a text summary. We opt for text over a structured format because natural language serves as a general-purpose and scalable semantic representation, which language models excel at generating.\nThis task requires comprehending the textual, spatial, and temporal dimensions of screen activities and transforming their abstract meanings into a coherent text description. To address this, we developed UI-JEPA, a framework consisting of two key components: a video transformer and a decoder-only language model (LM). The video transformer processes videos of continuous screen activities into video embeddings, which are then fed into the decoder-only LM to generate a corresponding text description of the user intent."}, {"title": "3.1 Network Parameterization", "content": "We employ the Vision Transformer (ViT) as our video encoder. To obtain the video embeddings, we process the entire video by sampling 16 evenly spaced frames, resizing them to 384 \u00d7 384 pixels, and dividing them into a 3D grid of spatial-temporal patches. Each patch measures 16 \u00d7 16 pixels and spans 2 frames. These patches, or visual tokens, are fed into the video encoder to generate the video embeddings. As illustrated in Figure 2(b), these embeddings are projected into the LM's input space using a dense layer.\nFor the LM, we use a lightweight variant, Microsoft Phi-3 , with approximately 3 billion parameters. This choice of a lightweight model facilitates on-device experimentation and deployment. The LM processes both the video embeddings and the text embeddings, with positional embeddings applied only to the text inputs. An overview of UI-JEPA inference architecture is presented in Figure 2(b)."}, {"title": "3.2 Training", "content": "We use pre-trained weights from a Vision Transformer (ViT) and an LM to perform fine-tuning in two stages: first, fine-tuning the ViT on unlabeled UI videos, and second, fine-tuning the LM on videos labeled with user intent descriptions.\nIn the initial fine-tuning stage for the ViT, we address the challenge of annotating user intent descriptions by training the ViT on solely videos in a self-supervised manner. Following the approach of V-JEPA , we use a masked prediction task where the unmasked video serves as context, and the masked part is predicted as the target. As illustrated in Figure (x-encoder), a target momentum encoder (y-encoder), and a predictor. The context encoder uses the ViT weights, which are the focus of our fine-tuning. During JEPA fine-tuning, we apply a masking strategy: masked tokens are removed from the input to the x-encoder, but are used as inputs to the y-encoder, along with the unmasked tokens (we discuss the different masking schemes employed in 6.2). The predictor receives a sequence of embeddings from the x-encoder concatenated with learnable mask tokens, which include positional embeddings representing the 3D spatio-temporal positions of the masked tokens. The predictor is then tasked with predicting the y-encoder's embeddings for each mask token. The y-encoder weights start as a deep copy of the x-encoder weights and are updated using an exponential moving average (EMA) of the x-encoder weights.\nIn the second fine-tuning stage, we freeze the ViT weights (the x-encoder from UI-JEPA) and update the adapters of a pre-trained language model. The video embeddings produced by the ViT are projected into the LM's input space via a dense layer. The parameters updated in this stage include the dense projection layer and the LM's LoRA adapter. An overview of the training process is shown in Figure 2(a)."}, {"title": "3.3 UI-JEPA Data Strategy", "content": "There are two key differences in the fine-tuning data between UI-JEPA and its parent V-JEPA . First, we intentionally avoid data augmentation on UI videos. Unlike general video data, UI videos contain both spatial and temporal activities as well as textual information that accurately describes app functionality and user input. Our experiments demonstrate that random data augmentation can result in malformed UI videos, which negatively impacts model performance. Second, while UI-JEPA incorporates the patch-based temporal-spatial masking strategy used in V-JEPA , it also introduces a novel temporal masking approach where entire frames are masked rather than just patches. This new strategy enhances the model's ability to learn frame dependencies, addressing the dramatic changes that often occur in UI videos when users open new apps or navigate between different screens."}, {"title": "4 The UI-JEPA Benchmarks", "content": "Current UI understanding benchmarks fall short in capturing UI actions as sequences where temporal relationships can be learned. Additionally, existing datasets focus primarily on learning multimodal representations from separate images, which may not sufficiently capture the complexity of tasks or enable models to accurately predict user intent. These limitations highlight the need for datasets that record task execution as a sequence of UI actions in video format, with each video labeled to describe the user's intent.\nFor such datasets to be useful practically, they must possess certain characteristics. First, variations in app versions can lead to different presentations and behaviors, and the sequence of UI actions may vary depending on the intended task and app version. Furthermore, to reflect real-world usage and to test model robustness, the dataset should include natural noise, such as irrelevant UI actions in the videos. Regarding challenges, scaling the collection of such a dataset is challenging, requiring a large number of annotators and mobile devices to perform tasks according to instructions, which demands significant investment. Finally, it is impractical to capture all possible visual representation variations of all apps and UI action sequences. To ensure models are built and tested for good generalization, the dataset must support evaluation in both few-shot and zero-shot settings.\nTo address these challenges, we created two multimodal datasets: \"Intent in the Wild\" which captures open-ended sequences of UI actions with ambiguous user intent, and \"Intent in the Tame\" which focuses on more common tasks with relatively clearer intent. We believe these datasets will contribute to the development of more powerful and lightweight MLLMs, as well as training paradigms with enhanced generalization capabilities."}, {"title": "4.1 Intent in the Wild", "content": "To train and evaluate our proposed UI-JEPA model, state-of-the-art MLLMs, and other comparable self-supervised learning approaches on tasks where UI actions are open-ended and user intent is challenging to predict or ambiguous, we developed the \"Intent in the Wild\" (IIW) dataset. The creation of the IIW dataset involved three key steps:\n\u2022 Recording UI Interactions: We manually recorded UI interactions for complex tasks, such as booking a vacation rental, ensuring that some intent categories appear only once. We filtered out any videos shorter than 16 frames to maintain data quality.\n\u2022 Annotating User Intent: We annotated the user intent based on the recorded UI interactions, providing high-level, delexicalized descriptions of intent.\n\u2022 Dataset Splitting: The dataset was split into two settings: a few-shot split with at least two instances of videos for each intent category, and a zero-shot split where each intent category appears only once, with no overlap with the few-shot split.\nThe IIW dataset consists of 219 intent categories, with 135 in the few-shot split and 84 in the zero-shot split. In total, the dataset contains 1.6K videos, each averaging 749 frames and approximately ~25 seconds in duration."}, {"title": "4.2 Intent in the Tame", "content": "The Intent in the Tame (IIT) dataset was developed to record authentic UI interactions executed by individuals in the process of task completion. For scaling the size, while maintaining realism, dataset creation was automated by framing the navigation as a directed graph (V, E) traversal, where set of vertices V represents different state of the screen and set of edges E are the different UI macros (actions). The approach can be summed up in two steps,\n\u2022 Setup: An iOS framework was used to record diverse set of UI macros across apps. An LLM was utilized to synthetically generate staging data for the device. Macros, in conjunction with the data produced by an LLM, were then used to automatically stage the device to a specific state. For instance, recorded macros for the iOS Contacts app were used to create the contact to be edited (with the name and generated by the LLM) for the Edit Contact intent.\n\u2022 Execution: A comprehensive graph was created for each intent which encompassed all potential execution paths. Parameters for the intent were synthetically created via an LLM (to exemplify, the name of the new contact for \"Edit Contact\" intent). Finally, each data point was generated using a staged device and a randomly guided traversal of the graph (algorithm made sure the intent is completed).\nThis approach ensures that the IIT dataset encapsulates four primary characteristics: realism, diversity, intent fulfillment and labeled. In its first version, IIT dataset consists of 914 labeled videos spanning across 10 intent categories. Out of 914 videos, 45 videos were evaluated in a zero-shot setting."}, {"title": "5 Baselines", "content": "We compare UI-JEPA with several baselines, focusing primarily on different model weights for the video encoder, all based on the ViT architecture :\n\u2022 Random Encoder: A ViT encoder initialized with random weights, which are then fine-tuned using labeled data in the second stage.\n\u2022 V-JEPA: A ViT encoder pre-trained on video datasets using a feature prediction objective.\n\u2022 VideoMAE: A ViT encoder pre-trained on video datasets with video tube masking.\nThese baseline video encoders are paired with the same LLM for user intent generation. We also include closed-source models with potentially different end-to-end architectures for video-to-text generation:\n\u2022 GPT-4 Turbo: A closed-source model with multimodal understanding capabilities by OpenAI.\n\u2022 Claude-3.5-Sonnet: A closed-source multimodal model by Anthropic.\nThese closed-source models are prompted to convert the UI test split videos into text summaries of user intents for evaluation.\nFor all baselines, we apply the same data preprocessing technique: sampling 16 evenly spaced frames from the entire video. Since VideoMAE was originally pre-trained with a 224 \u00d7 224 image size, we also report scores for the UI-JEPA encoder using the same 224 \u00d7 224 image size to ensure a fair comparison.\nWe evaluate the outputs using several established metrics: the SBERT (Sentence-BERT) cosine similarity score , and the ROUGE-1, ROUGE-2, and ROUGE-L scores . The SBERT score measures semantic similarity by embedding sentences into a vector space and computing cosine similarity. ROUGE scores evaluate summary quality through unigram overlap (ROUGE-1), bigram overlap (ROUGE-2), and the longest common subsequence (ROUGE-L), reflecting sentence structure. Additionally, we introduce a new metric, Intent Similarity, calculated by averaging the four normalized similarity scores, each scaled to the range [0, 1]. Together, these metrics provide a comprehensive evaluation of both lexical and semantic quality in the generated intents."}, {"title": "6 Results", "content": "UI-JEPA outperforms all other baselines on the few-shot split."}, {"title": "6.1 Main Results", "content": "UI-JEPA outperforms all other baselines on the few-shot split.  Tables 3, 4, 5, and 6 present the performance of UI-JEPA alongside other baselines on the few-shot and zero-shot splits of the IIW and the IIT datasets, respectively. Among all video encoders, UI-JEPA consistently achieves the highest scores in both few-shot and zero-shot scenarios. When comparing intent similarity to the leading closed-source model, Claude 3.5 Sonnet, UI-JEPA demonstrates superior performance on few-shot tasks while showing a performance gap in zero-shot settings especially for the IIT dataset. This indicates that while UI-JEPA excels in tasks involving familiar applications, it faces challenges with unfamiliar ones. However, it is important to note that Claude 3.5 Sonnet comes with significant trade-offs, including costs that are 50.5 times higher and latency that is 6.6 times greater than UI-JEPA as a more lightweight model. Incorporating OCR-extracted text from the final frame enhances performance on the few-shot split of IIT dataset for all JEPA-based models, with UI-JEPA outperforming other baselines. However, closed-source models and the zero-shot split show no improvement from this addition. For UI-JEPA specifically, the OCR extraction step introduces a 13.5% latency increase while delivering a 14.4% performance boost."}, {"title": "6.2 Ablation Studies", "content": "We perform ablation studies to assess the impact of data augmentation, positional embeddings, masking strategy, masking ratio, and JEPA-tuning data size. All experiments are conducted using the Microsoft Phi-3 model and the \"Intent in the Wild\" dataset."}, {"title": "6.2.1 Data Augmentation", "content": "During the pretraining stage of V-JEPA, data augmentation techniques such as random flipping and cropping are applied to each video frame. However, unlike natural video datasets, smartphone screens have a fixed orientation, making image flipping ineffective and potentially introducing noise. Additionally, crucial signals like notifications are often located at the top or bottom of the screen, so cropping risks losing significant information. To evaluate the impact of excluding these less effective data augmentation methods, we conducted experiments (see Table 7).\nOur results show that while flipping slightly improves performance on the few-shot split, it significantly degrades performance on the zero-shot split. This decline could be due to the video encoder's difficulty in learning a consistent orientation within the UI video dataset, which negatively affects generalization to the zero-shot set. Similarly, the cropping method also leads to reduced performance. The combination of flipping and cropping further exacerbates performance degradation in both few-shot and zero-shot scenarios. As a result, we decided to eliminate all data augmentation methods in our subsequent experiments."}, {"title": "6.2.2 Positional Embedding", "content": "During the integration of video and text embeddings before passing these hybrid embeddings to the LM, we examined the impact of adding positional embeddings to the video embeddings, similar to those used in text embeddings. We also considered the alternative of omitting these additional positional embeddings. Given that the video embeddings already contain 3D positional information from the encoder, which represents spatio-temporal positions, we evaluated whether adding extra positional embeddings would offer any advantage (see Table 8).\nOur results show that omitting additional positional embeddings consistently improves performance, while their inclusion tends to degrade it. Based on these findings, we chose not to incorporate extra positional embeddings into the video embeddings in our subsequent experiments."}, {"title": "6.2.3 JEPA-tuning Data Size", "content": "In this section, we examine the effect of varying the size of the JEPA-tuning dataset to assess whether adding more unlabeled UI videos enhances video representation and overall model performance. We experimented with using 25%, 50%, 75%, and 100% of the available data during the JEPA-tuning phase while using the full dataset for fine-tuning. The results are summarized in Figure 7.\nAs the size of the JEPA-tuning data increases, model performance improves consistently in the few-shot scenario. For the zero-shot scenario, performance initially remains stable but shows a significant increase when the full dataset is used for JEPA-tuning. This indicates that even when the number of labeled examples is fixed, increasing the number of unlabeled examples during JEPA-tuning enhances the model's feature extraction capabilities, leading to better performance in downstream tasks."}, {"title": "6.2.4 Masking Strategy", "content": "In the original JEPA pre-training process, optimal results are achieved using a multi-block masking strategy, where random spatio-temporal blocks from the entire video are masked. However, applying additional masking to the last few frames can reduce performance. Given the unique characteristics of our UI-grounded multimodal datasets, these conventional masking strategies may not be ideal. To explore the effects of masking during JEPA-tuning, we address two key research questions:\nQ1: What types of masking yield optimal performance?\nQ2: What ratio and strategy of temporal masking should be used for the best results?\nTo answer Q1, we experimented during the JEPA-tuning stage with three masking settings: (1) short-range masking, (2) short-range + long-range masking (as used in V-JEPA ), and (3) short-range + long-range + temporal masking. Figure 6 provides an overview of these masking strategies, and Figure 8 compares their performance.\nOur results show that progressively adding more masking improves performance in both the few-shot and zero-shot scenarios, indicating that additional temporal masking helps the model better capture serial dependencies between frames, leading to enhanced outcomes.\nFor Q2, we tested two temporal masking strategies during JEPA-tuning: (1) Contiguous Temporal Masking, where a single block of consecutive frames is masked in Figure 9; and (2) Discrete Temporal Masking, where arbitrary frames are selected for masking in Figure 10. Since each patch spans two video frames, masking occurs at the level of these patches, with \"frame\" here referring to a hyper-frame consisting of two video frames.\nFrom our experiments, we observed that in the few-shot scenario, performance improves as more frames are masked. However, in the zero-shot scenario, this trend is less pronounced, with performance remaining relatively stable across different masking ratios. The best results for both the few-shot and zero-shot scenarios are achieved with six masked discrete frames."}, {"title": "7 Conclusion and Applications", "content": "In this work, we introduced UI-JEPA, a framework that uses self-supervised learning to generate abstract UI embeddings and, when combined with a small LLM, can perform user intent prediction. UI-JEPA matches the performance of state-of-the-art MLLMs while reducing computational costs by 50.5x and latency by 6.6x, making it ideal for lightweight, on-device applications. Our newly introduced datasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), establish a benchmark for few-shot and zero-shot UI understanding. These findings highlight UI-JEPA's potential for advancing efficient and privacy-preserving UI understanding."}, {"title": "7.1 Conclusion", "content": "In this work, we introduced UI-JEPA, a framework that uses self-supervised learning to generate abstract UI embeddings and, when combined with a small LLM, can perform user intent prediction. UI-JEPA matches the performance of state-of-the-art MLLMs while reducing computational costs by 50.5x and latency by 6.6x, making it ideal for lightweight, on-device applications. Our newly introduced datasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), establish a benchmark for few-shot and zero-shot UI understanding. These findings highlight UI-JEPA's potential for advancing efficient and privacy-preserving UI understanding."}, {"title": "7.2 Applications of UI-JEPA", "content": "User intent understanding has at least two key applications: User Feedback Learning and Multimodal Intent State Tracking."}, {"title": "7.2.1 User Feedback Learning", "content": "Smartphone users worldwide interact daily with digital assistants, generating a vast array of queries. This data is invaluable for refining the reasoning capabilities of digital assistants and aligning their responses with user preferences. However, privacy and security concerns aside, a significant challenge is that many digital assistants currently struggle to address user requests effectively, resulting in a large volume of disorganized, low-quality data. By using UI-JEPA, we can accurately infer user intent from on-screen activity and assess the effectiveness of the digital assistant's performance. This includes determining whether users continue with the app opened by the assistant or switch to a different one. If UI-JEPA predicts successful execution, the data point can be directly added to our high-quality dataset. Conversely, if the prediction indicates a failure, UI-JEPA can still capture the user's true intent, contributing valuable data to our high-quality dataset, as shown in Figure 11. These high-quality datasets can then be used to further fine-tune UI-JEPA, enhancing its performance and enriching the dataset for future UI understanding research."}, {"title": "7.2.2 Multimodal Intent State Tracking", "content": "Another promising application of UI-JEPA is its integration into an agentic framework designed to actively track user intent across various applications and modalities. In this framework, UI-JEPA functions as the perception agent, capturing user intent at different time points and storing these intents in a memory store. When a user interacts with a digital assistant, the system retrieves the most relevant intent based on the query and generates the appropriate API call to fulfill the user's request, as illustrated in Figure 12."}, {"title": "8 Limitations", "content": "While UI-JEPA has shown promising results on the IIW and IIT datasets, several limitations remain:\n\u2022 Granularity of User Intent Prediction: JEPA embeddings alone often fall short for granular user intent prediction, especially in the IIT dataset, where the encoder primarily captures high-level video representations. This limitation affects tasks requiring detailed text recognition and description. To address this, we incorporate OCR; however, its effectiveness is contingent on the quality of the OCR and the presence of textual information in the frames. Further research is needed to enhance JEPA representations to capture more detailed content.\n\u2022 Pre-training Requirements: Experimental results reveal that JEPA-tuning a randomly initialized video encoder yields poor performance. Effective JEPA-tuning necessitates extensive pre-training of the video encoder, which restricts the applicability of JEPA-tuning to scenarios with ample video data.\n\u2022 Performance in Zero-Shot Scenarios: Although UI-JEPA performs competitively with large MLLMs like Claude 3.5 Sonnet and GPT-4 Turbo in few-shot settings, its performance lags significantly in zero-shot scenarios. This indicates that UI-JEPA's ability to generalize from familiar to unfamiliar apps needs improvement.\n\u2022 Audio Modality: UI-JEPA has not been tested with audio modalities, and its performance in this domain remains unexamined."}]}