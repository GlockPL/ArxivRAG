{"title": "Dying Clusters Is All You Need - Deep Clustering\nWith an Unknown Number of Clusters", "authors": ["Collin Leiber", "Niklas Strau\u00df", "Matthias Schubert", "Thomas Seidl"], "abstract": "Abstract-Finding meaningful groups, i.e., clusters, in high-\ndimensional data such as images or texts without labeled data\nat hand is an important challenge in data mining. In recent\nyears, deep clustering methods have achieved remarkable results\nin these tasks. However, most of these methods require the user\nto specify the number of clusters in advance. This is a major\nlimitation since the number of clusters is typically unknown\nif labeled data is unavailable. Thus, an area of research has\nemerged that addresses this problem. Most of these approaches\nestimate the number of clusters separated from the clustering\nprocess. This results in a strong dependency of the clustering\nresult on the quality of the initial embedding. Other approaches\nare tailored to specific clustering processes, making them hard\nto adapt to other scenarios. In this paper, we propose UNSEEN,\na general framework that, starting from a given upper bound,\nis able to estimate the number of clusters. To the best of our\nknowledge, it is the first method that can be easily combined\nwith various deep clustering algorithms. We demonstrate the\napplicability of our approach by combining UNSEEN with\nthe popular deep clustering algorithms DCN, DEC, and DKM\nand verify its effectiveness through an extensive experimental\nevaluation on several image and tabular datasets. Moreover,\nwe perform numerous ablations to analyze our approach and\nshow the importance of its components. The code is available at:\nhttps://github.com/collinleiber/UNSEEN\nIndex Terms-Deep Clustering, kNN-based Clustering, Esti-\nmating the Number of Clusters", "sections": [{"title": "I. INTRODUCTION", "content": "Identifying clusters in data without supervision is an impor-\ntant task in machine learning. As a result, clustering is a well-\nstudied area of research, and many different approaches have\nbeen proposed [39]. Since traditional methods struggle with\nhigh-dimensional data, like images and texts, many state-of-\nthe-art clustering algorithms are based on deep learning. This\nenables them to learn powerful representations to effectively\ncluster the data without using label information.\nA large group of these so-called deep clustering algorithms\napplies autoencoders (AEs) [4] to learn low-dimensional rep-\nresentations in which to cluster the data effectively [42]. AE-\nbased clustering algorithms are popular because they not only\nwork well with image data but have been successfully applied\nto a vast array of different data domains such as text or tabular\ndata [38]. In contrast to more sophisticated strategies, like\ncontrastive networks, AEs are often easier to apply as they\ndo not rely on augmentations, which can be difficult to define\nfor certain data types such as tabular data. Consequently, AE-\nbased deep clustering procedures can be applied to a broad\nrange of different types of data. Due to their versatility, we\nfocus on AE-based deep clustering algorithms in the following.\nEven though these approaches work on unlabeled data, the vast\nmajority of these algorithms still require the user to specify\nthe number of clusters in advance. However, the number of\nclusters is usually unknown and particularly challenging to\nobtain due to the lack of label information/ground truth.\nTo overcome this issue, a number of procedures have been\ndeveloped for classical (non-deep) clustering algorithms [33].\nHowever, these approaches can be ineffective when applied to\nhigh-dimensional data as they struggle with the curse of di-\nmensionality [5]. Furthermore, they often rely on unsupervised\nmetrics, such as the silhouette score [32], which can become\nmeaningless in a deep learning environment as the data spaces\ncan change significantly in each iteration.\nThus, they are often unsuitable for use in combination with\ndeep clustering algorithms. Some approaches to estimate the\nnumber of clusters have been proposed specifically for deep\nclustering algorithms. These approaches can be divided into\ntwo main categories. The former work in a sequential manner,\ni.e., they first learn an embedding, then estimate the number\nof clusters based on the embedding, and cluster the data [8],\n[30]. However, these approaches are heavily dependent on the\ninitial embedding and can, therefore, be quite unstable. The\nlatter propose specialized loss functions tailored to specific\nalgorithms [23], [31] that are highly specialized and cannot\nbe easily applied to other clustering algorithms.\nTo the best of our knowledge, currently no general frame-\nwork exists to estimate the number of clusters that can be\nstraightforwardly applied to a multitude of deep clustering\nalgorithms. In this paper, we propose UNSEEN (unknown\nnumber of clusters in deep embedings), a general framework\nthat can be applied to many deep clustering algorithms\nincluding prominent examples such as DCN [40], DEC [38], or\nDKM [10] \u2013 without specifying the exact number of clusters.\nOur approach merely requires to specify an upper bound on\nthe number of clusters, which can be more easily obtained.\nFurthermore, we empirically demonstrate that our approach\nachieves good performances, even with upper bounds that\nvastly overestimate the actual number of clusters. Thus, we\nargue that it is not particularly difficult to provide such a loose\nupper bound in most applications."}, {"title": "II. RELATED WORK", "content": "This work addresses the question of how to find high-quality\nclusters in deep clustering if the exact number of clusters\nis unknown. Therefore, two main branches of research are\nrelevant: deep clustering and estimating the number of clusters.\nDeep Clustering: We focus on AE-based deep clustering\napproaches, as they are not restricted to certain data types. This\nperfectly matches our goal of designing a generic framework\nthat can be applied to a majority of algorithms and datasets.\nDeep clustering procedures based on AEs can be divided\ninto three main categories: sequential, alternating, and simul-\ntaneous approaches [42]. In sequential approaches, an embed-\nding embedding is learned first and afterward the clustering\ntask is performed on the resulting embedded data. A well-\nknown approach is to train an AE using the reconstruction\nloss $\\mathcal{L}_{rec}$, often the mean squared error is utilized and\nthen execute k-Means [25]. We denote this strategy as AE+k-\nMeans. Alternating deep clustering algorithms go one step\nfurther by creating an embedding tailored to the clustering task\nat hand. Therefore, they perform a batch-wise optimization\nof a clustering-specific loss function $\\mathcal{L}_{clust}$ together with the\nreconstruction loss. The combined loss for a given batch B\ncan be defined as:\n$\\mathcal{L}_{total} (B) = \\lambda_{1} \\mathcal{L}_{rec}(B) + \\lambda_{2}\\mathcal{L}_{clust}(B)$,\nwhere $\\lambda_{1}$ and $\\lambda_{2}$ are parameters to balance the two loss terms.\nSince alternating approaches often use hard cluster labels, their\nloss function is usually not fully differentiable. Therefore, they\niterate between optimizing the embedding and optimizing the\nclustering result.\nA common way to define $\\mathcal{L}_{clust}$ is using the squared Eu-\nclidean distance between the embedded samples and their\nassociated cluster center, i.e.,\n$\\mathcal{L}_{clust}(B) = \\sum_{i\\in[1,k]} \\sum_{x\\in C_i} ||enc(x) \u2013 \\mu_{i} ||^2$,\nwhere $\\mu_{i}$ is the cluster center of cluster i. Prominent examples\nutilizing this loss function include AEC [34] and DCN [40].\nOther methods use, for example, ideas from subspace cluster-\ning [27] or statistical tests [22].\nIn contrast to alternating deep clustering approaches, simul-\ntaneous algorithms optimize the embedding and the clustering\nparameters simultaneously by using soft cluster assignments.\nThe most well-known representative is DEC [38], where\n$\\mathcal{L}_{clust}$ is based on the Kullback-Leibler divergence to compare\na centroid-based data distribution with an auxiliary target\ndistribution. While DEC ignores $\\mathcal{L}_{rec}$ during the clustering\noptimization, IDEC [12] was proposed as an extension of\nDEC that uses $\\mathcal{L}_{rec}$ also during the clustering optimization.\nDCEC [13] is a variant of IDEC that showcases that the\napplied feedforward AE can be easily substituted by a con-\nvolutional AE. The algorithm DKM [10] proposes a different\nloss function which is more similar to the classic k-Means\nprocedure. Here, the Euclidean loss is extended by a factor\nbased on a parameterized softmax function to simulate hard\ncluster labels. Thereby, the embedding and the cluster centers\nare fully differentiable.\nEstimating the number of clusters: A prominent branch of\nalgorithms that can estimate the number of clusters is density-\nbased approaches. These procedures usually follow the idea\nof DBSCAN [9] and search for areas in the feature space that\ncontain neighborhoods of data points. While such approaches\nwork well in many scenarios it is non-trivial to transfer these\nideas into a deep learning setting. Furthermore, often complex\nparameters have to be set to define the necessary characteristics\nof a neighborhood.\nAnother line of research extends centroid-based clustering\nalgorithms like k-Means [25] so that the number of centers\ncan be estimated. These approaches can be subdivided into\ntop-down processes which start with a low number of clusters\nthat is successively increased and bottom-up processes which\nwork the opposite way. Examples for top-down approaches\ninclude the Gap Statistic [35], X-Means [29], G-Means [14],\nPG-Means [11], and DipMeans [18]. An prominent bottom-\nup approach is proposed in [6]. In both scenarios, a common\nway to decide which clusters to split or merge is to utilize\nconcepts from information theory or statistical tests like An-\nderson-Darling [3] or the dip-test of unimodality [15].\nIn this paper, we integrate ideas from density-based clus-\ntering into a novel bottom-up deep clustering approach by\nleveraging nearest-neighbors.\nDeep Clustering + Estimating the number of clusters:\nAlthough deep clustering algorithms that are able to estimate\nthe number of clusters exist already, they exhibit severe\ndisadvantages. A common paradigm is to define sequential\ndeep clustering approaches that combine the transformation\ncapabilities of a neural network with established or novel k-\nestimation techniques. This is done, for example, by SCDE [8],\nDDC [30], and DeepDPMtwo-step [31]. However, these algo-\nrithms heavily depend on the quality of the initial embedding\nas they can not fix certain errors at a later stage. Furthermore,\nit can be hard to decide beforehand which k-estimation tech-\nnique best matches the resulting embedding.\nOther approaches, like DipDECK [23] or\nDeepDPMend-to-end [31], propose specific loss functions\nwhich are hard to adapt to other cluster definitions. In\ncontrast, our UNSEEN framework is defined in such a way\nthat it can be easily combined with a vast array of deep\nclustering approaches."}, {"title": "III. A GENERAL FRAMEWORK FOR DEEP CLUSTERING\nWITH AN UNKNOWN NUMBER OF CLUSTERS", "content": "In this paper, we build upon the observation that samples\noftentimes change their cluster assignments during training.\nThis is because the learned embeddings of deep clustering\nalgorithms are very flexible. We observed that this can lead to\nclusters becoming essentially empty, a phenomenon we refer\nto as dying clusters. However, even though these clusters only\ncontain very few points, deep clustering algorithms typically\nkeep these clusters because the number of clusters is fixed. Our\napproach leverages these dying clusters and removes them,\nresulting in a final clustering with a lower number of clusters.\nIn the following, we will detail our approach.\nFirst, we define a cluster $C_i$ as the set of samples assigned\nto this cluster. We use the notation $C_i^j$ to refer to cluster i in\nepoch j and $|C_i^j|$ to denote its current size (i.e., the number of\nsamples in this cluster). Additionally, we also define a creation\nsize $C_i^0$ for each cluster. The creation size of a cluster is\ninitialized to the size of the cluster before the first epoch.\nImportantly, the creation size of a cluster is independent of\nits current size and only updated when the cluster receives\nsamples from a dead cluster. When the size of a cluster $|C_i^j|$\nshrinks below a fraction t of its creation size $|C_i^0|$, we refer\nto the cluster as dead and remove it completely. That means,\nthe samples corresponding to the dead cluster are reassigned\nto the remaining active clusters and the creation sizes of the\nactive clusters are updated accordingly. More specifically, the\ncreation size $|C_i^0|$ of the cluster is increased by the number of\nsamples that are reassigned to it from dead clusters. In order to\nremove the dead clusters from the underlying deep clustering\nalgorithm, the parameter k, which specifies the number of\nclusters, is updated to the current number of active clusters $k_j$.\nThe samples of the dead clusters are reassigned to the most\nlikely or closest cluster in accordance with the deep clustering\nalgorithm.\nAfter each epoch, we identify dead clusters and remove\nthem. In contrast to most deep clustering algorithms, where the\nnumber of cluster remains constant, this leads to a decrease in\nthe number of clusters over time. Let us note that this process\nbears some resemblance to [23]. However, in contrast to [23]\nour approach is not tailored to a specific clustering algorithm.\nInstead, our method can be easily combined with various deep\nclustering algorithms.\nOur approach converges to a fixed amount of clusters over\ntime. However, this process and, thus, the final clusters are\nlargely influenced by the initial clustering. In fact, many\nmethods encounter difficulties in overcoming this initial bias,\nas they tend to strongly optimize in the direction of the\ninitial clustering. This may result in a subpar final clustering\nperformance. To counteract the impact of the initial bias on\nthe clustering, we propose to add a nearest-neighbor-based\nloss term, which brings neighboring clusters closer together\nin the latent space. Additionally, we believe that this loss also\npulls samples towards larger clusters, which can enhance the\nclustering quality in balanced datasets. Moreover, we observe\nthat the loss acts as a regularization term that facilitates\nsamples to change their cluster during training. This is because\nwe only select the nearest neighbors from samples within the\nbatch. Consequently, the nearest neighbors are likely to come\nfrom different clusters and, due to the Euclidean distance, are\npulled closer to the other cluster. More formally, our nearest-\nneighbor loss is defined as follows:\n$\\mathcal{L}_{UNSEEN} (B, l) = \\frac{1}{|B|} \\sum_{x \\in B} \\sum_{y \\in nn(B, x, l)} ||enc(x) \u2013 enc(y)||^2$,\nwhere B corresponds to a batch of data, enc(x) is the embed-\nded version of x, nn(B, x, l) returns the l-nearest-neighbors of\nenc(x) in {enc(y) | y \u2208 B} and $|| . ||^2$ denotes the squared\nEuclidean distance. Our empirical findings show that this\nnearest-neighbor loss is effective for sequential deep clustering\napproaches like DCN [40]. However, when combined with\nsimultaneous methods, such as DEC [38] or DKM [10], we\nobserve that the loss can cause the embeddings to collapse.\nIn these algorithms, the clustering parameters are updated\ntogether with the embedding, which can result in significant\nchanges to the clustering structure and, as a consequence,\nthe formation of excessively large individual clusters. To\ninhibit the formation of such oversized individual clusters, we\nnormalize the nearest-neighbor loss by considering all pairwise\ndistances in a batch. This reduces the impact of the change\nin embeddings. Thus, when dealing with a simultaneous deep\nclustering algorithm, we use the following modified nearest-\nneighbor loss:\n$\\mathcal{L}_{UNSEEN}^{simul} (B, l) = \\frac{\\sum_{x \\in B} \\sum_{y \\in B} ||enc(x) \u2013 enc(y)||^2}{|B|(|B|-1)}$\nThis results in the complete loss function\n$\\mathcal{L}_{total}(B, l) = \\lambda_{1} \\mathcal{L}_{rec} (B) + \\lambda_{2} (\\mathcal{L}_{clust}(B, C) + \\mathcal{L}_{UNSEEN} (B, l))$,\nwhere $\\mathcal{L}_{rec} (B)$ denotes the reconstruction loss of the AE,\n$\\mathcal{L}_{clust}(B, C)$ the clustering loss of the underlying deep clus-\ntering algorithm, and $\\lambda_{1}$ as well as $\\lambda_{2}$ are weighting terms.\nOur nearest-neighbors loss term has introduced an addi-\ntional hyperparameter. Namely, the number of nearest neigh-\nbors l to consider. At first glance, selecting a suitable value for\nthis parameter appears to be challenging. Especially, since a\ngood value depends on the number of currently active clusters,\nwhich varies during the optimization process. However, we\npropose to set this parameter based on the following intuition.\nIn the beginning, samples of different clusters are usually\nstill close in the embedding; thus, a large number of nearest\nneighbors would lead to pulling samples from different clus-\nters together, deteriorating the effectiveness of the clustering.\nHowever, later in the clustering process, when many clusters\nhave already died, they are usually more compact and further\nseparated in the latent space. Thus, we want to include more\nneighbors in order to focus more on the neighboring clusters\nand facilitate the combination of clusters by strengthening\nthe connection between these clusters. Therefore, we set the\nnumber of nearest neighbors l dependent on the number of\ncurrently active clusters. Formally, it is defined as follows:\n$l = \\lfloor \\frac{|B|}{k_j} \\rfloor$\nHere, $|B|$ denotes the batch size and $k_j$ the number of active\nclusters in epoch j.\nWe detail the complete process of our framework in Algo-\nrithm 1. In contrast to existing approaches, it is straightforward\nto integrate our method into most deep clustering algorithms.\nMore specifically, our framework consists of two components.\nThe first component is an additional loss term, which is trivial\nto integrate into deep clustering algorithms. The second part\nof our framework keeps track of active clusters and dissolves\ndead clusters. This is done outside of the underlying clustering\noptimization and thereby independent of it."}, {"title": "IV. EXPERIMENTS", "content": "We thoroughly evaluate the effectiveness of our framework\nin various synthetic and real-world benchmarks by comparing\nits performance against popular deep clustering baselines,\nincluding commonly used sequential and simultaneous ap-\nproaches. Furthermore, we provide a detailed analysis of\nUNSEEN's behavior and properties. Finally, we perform ex-\ntensive ablations to empirically examine our design choices\nand parameters. More specifically, we focus on the following\nresearch questions:\n\u2022 RQ1: How good are the clustering results of UNSEEN\nin combination with different deep clustering algorithms\nand how do they compare to the baselines across different\ndatasets?\n\u2022 RQ2: How accurately can UNSEEN estimate the number\nof clusters?\n\u2022 RQ3: What is the impact of the nearest-neighbor loss\n$\\mathcal{L}_{UNSEEN}$ on the performance of UNSEEN?\n\u2022 RQ4: How sensitive is UNSEEN to the value of the dying\nthreshold t?\nDatasets: We perform our experiments on various image\ndatasets, including Optdigits [2], USPS [17], MNIST [21],\nFashion-MNIST (FMNIST) [37], and Kuzushiji-MNIST (KN-\nNIST) [7]. Additionally, we use the tabular dataset Pendig-\nits [1]. Further details regarding the datasets are shown in the\nfirst column of Table I. We pre-process the data in accordance\nwith [23]. This includes a channel-wise z-normalization (0\nmean and standard deviation of 1) and flattening for im-\nage datasets and a feature-wise z-normalization for tabular\ndatasets.\nEvaluation metrics: To evaluate the clustering results,\nwe use the following three commonly used metrics: Nor-\nmalized Mutual Information (NMI) [20], Adjusted Rand\nIndex (ARI) [16], and Unsupervised Clustering Accuracy\n(ACC) [41]. These metrics compare the predicted labels with\na given ground truth. Let us note that a value of 1 equals a\nperfect clustering, and lower values indicate that the clustering\nresult is more similar to that of random clustering.\nExperimental setting: We verify the broad applicability of\nour framework by combining UNSEEN with the alternating\napproach DCN [40] as well as the simultaneous approaches\nDEC [40] and DKM [40], where DKM uses the reconstruction\nloss also during the clustering process and DEC does not. To\nensure comparability and a sound evaluation, our experimental\nsetting follows the standard described in [24]. We use an\nautoencoder (AE) with the encoder layers of size d-500-500-\n2000-10. Here, d denotes the number of input features in the\noriginal dataset. The decoder is a mirrored version of the\nencoder layers. This corresponds to the architecture used in\nmany publications like [38], [12], [40], [22], and [10]. All AEs\nare pre-trained for 100 epochs using the mean squared error\nwith a learning rate of 1e-3, and we use the same pre-trained\nAEs as the basis for all algorithms. Afterward, the clustering\nprocess is executed for 150 epochs with a learning rate of\n1e-4. In all experiments, ADAM [19] is used as the optimizer\nwith a batch size of 256. For DKM and UNSEEN+DKM, we\nset the weights of the reconstruction loss $\\lambda_{1}$ and clustering\nloss $\\lambda_{2}$ to 1 and the a parameter to 1000. For DCN and\nUNSEEN+DCN, we set the weight of the reconstruction loss\n$\\lambda_{1}$ to 1 and the clustering loss $\\lambda_{2}$ to 0.05. As DEC does not\nuse $\\mathcal{L}_{rec}$ during clustering optimization, we do not have to set\nweights. To set the initial number of clusters, we follow the\nproposal in [23] and choose a value of $k_{init} = 35$. Furthermore,\nwe use a default dying threshold of t = 0.5. The baselines\nDKM, DEC, and DCN are given the correct number of ground\ntruth clusters. All implementations are based on ClustPy [24]."}, {"title": "A. RQ1: Clustering Results", "content": "We start our evaluation by comparing the clustering results\nof our approach and its baselines on various datasets in Table I.\nHere, we can observe that UNSEEN almost consistently\noutperforms the baselines across all datasets and metrics. It\nachieves the best result in 16 out of 18 scenarios. Only for\nOptdigits and KMNIST, it is the second best according to the\nACC metric. These results demonstrate the effectiveness of\nUNSEEN. Impressively, in the vast majority of the cases, it\nis able to outperform the underlying deep clustering algorithm\nthat has been trained given the ground truth number of clusters.\nIt is noteworthy that USEEN+DKM beats its baseline in\nall but one and UNSEEN+DCN in all but two scenarios.\nFurthermore, we observe that the standard deviation when"}, {"title": "B. RQ2: Correctness of the Estimated Number of Clusters", "content": "We continue our evaluation by examining how well UN-\nSEEN can estimate the number of clusters. For this, we use a\nsynthetic and a real-world setting based on MNIST.\nSynthetic setting: For the synthetic setting, we use\nthe make blobs\u00b9 functionality of scikit-learn [28] to cre-\nate datasets with a varying number of clusters $k=\n{5, 7, 10, 15, 20, 25, 30}, N = 10,000$ and $d=\n100$. Here,\nall clusters are made out of isotropic Gaussian blobs with\na standard deviation of 1 and contain an equal number of\nsamples, i.e., $\\forall_{1<j<k} : |C_j| = \\frac{N}{k}$.\nThe results are illustrated in Figure 1. Note that the algo-\nrithms start with $k_{init} = 35$ in all experiments and that this\nvalue marks the maximum number of clusters that can be\nreached. It can be observed that UNSEEN+DKM is able to"}, {"title": "C. RQ3: Importance of LunseEN", "content": "One of the core components of UNSEEN is the nearest-\nneighbor loss $\\mathcal{L}_{UNSEEN}$. We perform an ablation study to\nempirically demonstrate the importance of this loss, which\ncan be found in Table II. While the results without $\\mathcal{L}_{UNSEEN}$\nare slightly better in some settings, this loss is crucial for\nthe effectiveness of UNSEEN in other settings. Note that the\nmaximum performance loss when using $\\mathcal{L}_{UNSEEN}$ is 8.8% in\nthe case of the NMI result of UNSEEN+DCN for KMNIST.\nAll other losses are markedly lower. The gains of applying\n$\\mathcal{L}_{UNSEEN}$, however, are up to 36.0% in the case of ACC for\nUNSEEN+DCN and Optdigts. Overall, we recommend includ-\ning $\\mathcal{L}_{UNSEEN}$ since it drastically improves the performance of\nUNSEEN on some datasets while it only slightly decreases\nthe performance on others.\nTo further investigate the influence of $\\mathcal{L}_{UNSEEN}$ on the op-\ntimization, we visualize clustering results of UNSEEN+DEC\nwith and without $\\mathcal{L}_{UNSEEN}$ applied on USPS. For this task,\nwe transform the ten-dimensional embedding into a two-\ndimensional representation using t-SNE [36]. The resulting\ndata distributions are displayed in Figure 3. While the results\nafter the first epoch look rather similar, it can be seen that\nUNSEEN+DEC without $\\mathcal{L}_{UNSEEN}$ compresses the initial clus-\nters more strongly. This separates them more vigorously from\nneighboring larger clusters and results in fewer dead clusters.\nAccordingly, we observe more clusters after epoch 100."}, {"title": "D. RQ4: Impact of Dying Threshold t", "content": "Last, we investigate the impact of the dying threshold t. To\ndo this, we evaluate the performance of UNSEEN regarding\nACC for values of t \u2208 [0.1, 0.9]. We perform this experiment\non the datasets Optdigits, USPS, MNIST, and Pendigits.\nWhen analyzing the plots shown in Figure 4, we see that\nmost results are quite stable regarding the choice of t. Only\nUNSEEN+DCN shows strong fluctuations, where the ACC\nvaries from 0.7 to 0.86 for MNIST, for example. Overall,\nperformance appears to increase with increasing t. However,\nthe value must not be set too high, as otherwise, too many\nclusters will be interpreted as dead, resulting in a crash in\nperformance. From this, it can be concluded that 0.5 represents\na good trade-off between cluster quality and the risk of losing\ntoo many clusters. Hence, we chose this value for all other\nexperiments."}, {"title": "V. CONCLUSION", "content": "To conclude, this paper presents UNSEEN, a framework for\nestimating the number of clusters that can be easily combined\nwith many deep clustering algorithms. We demonstrate its\nperformance using an extensive experimental evaluation that\nincludes several image and tabular datasets. Furthermore, we\nconduct several ablations to justify the design choices and\ninvestigate the importance of UNSEEN's components. Several\npromising avenues for future work exist. For instance, we plan\nto examine how UNSEEN performs on unbalanced datasets.\nAnother interesting direction is to test UNSEEN with a broader\nrange of deep clustering algorithms, particularly those not\nbased on centroids and based on contrastive learning."}]}