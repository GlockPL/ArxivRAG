{"title": "A+Al: Threats to Society, Remedies, and Governance", "authors": ["Don Byrd"], "abstract": "This document focuses on the threats, especially near-term threats, that Artificial Intelligence (AI) brings to society. Most of the threats discussed here can result from any algorithmic process, not just AI; in addition, defining AI is notoriously difficult. For both reasons, it is important to think of \u201cA+AI\u201d: Algorithms and Artificial Intelligence.\nIn addition to the threats, this paper discusses countermeasures to them, and it includes a table showing which countermeasures are likely to mitigate which threats. Thoughtful governance could manage the risks without seriously impeding progress; in fact, chances are it would accelerate progress by reducing the social chaos that would otherwise be likely. The paper lists specific actions government should take as soon as possible, namely:", "sections": [{"title": "I. Introduction", "content": "There is a wide range of opinions on the opportunities and especially on the threats that arise from Artificial Intelligence (AI). This document focuses on the latter, especially on near-term threats. (\u201cNear-term threats\" here means those likely to have serious effects within a year or so.) It consists of a discussion and list of ways in which AI and related algorithms threaten society; a discussion and list of possible ways to protect society; and a table relating the two. To the best of the author's knowledge, this is the first attempt to make such a table. \u201cTASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI\" by two AI researchers, Critch and Russell (2023), is probably closer than anything else to the current paper; it has many valuable features, including an exhaustive decision tree for classifying harms and well-thought-out stories to illustrate how various risks could play out. \u201cAn Overview of Catastrophic AI Risks\" from the Center for AI Safety (Hendrycks et al. 2023) includes much helpful detail, but it's over 50 pages long, and it's organized by sources of risk instead of resulting harm. Both features make it a good resource for a specialist but not a good briefing document. Neither of these documents is addressed to legislators or regulators.\nIt's essential to note, first, that most of the threats discussed here can result from any algorithmic process, not just AI. An \u201calgorithmic process\" is simply a procedure involving an algorithm, like Facebook's first simple methods for automatically suggesting content users might like. The Brennan Center's \"Regulating AI Deepfakes and Synthetic Media in the Political Arena\u201d (n.d.) gives several examples of what might be called \"shallow fakes\u201d whose creation didn't require AI. In addition, defining AI is notoriously difficult. In part this is because it's a moving target. As soon as we know how to do something previously thought to require intelligence\u2014beat an expert chess player, describe a picture in words\u2014it's no longer considered AI. (Hofstadter (1980) calls this \u201cTesler's Theorem\u201d.) For both reasons, everything below should be considered to apply to algorithmic processes of any kind, and future rules or regulations should avoid mentioning Al whenever possible. For the same reasons, the term \u201cA+AI\u201d is used herein instead of just \"\u0391\u0399\".\nBut many threats that don't require AI will be exacerbated by it. In the words of Bruce Schneier, \u201cWhen I survey how artificial intelligence might upend different aspects of modern society, democracy included, I look at four different dimensions of change: speed, scale, scope, and sophistication\u201d (Schneier 2023).\nThe threats of A+AI are very serious but possible to mitigate. Many of the obvious countermeasures -for example, licensing and moratoriums on development\u2014are unlikely to accomplish much, particularly since bad actors can ignore them. However, thoughtful governance could manage the risks without"}, {"title": "Executive Summary", "content": "There is a wide range of opinions on the opportunities that Artificial Intelligence (AI) offers society, and an even wider range of opinions on its threats. This document focuses on the threats, especially near-term threats. Most of the threats discussed here can result from any algorithmic process, not just AI; in addition, defining AI is notoriously difficult. For both reasons, it is important to think of \u201cA+AI\u201d: Algorithms and Artificial Intelligence.\nIn addition to the threats, this paper discusses countermeasures to them, and it includes a table showing which countermeasures are likely to mitigate which threats. Many of the obvious measures-for example, licensing and moratoriums on development are unlikely to accomplish much, particularly since bad actors can ignore them. However, thoughtful governance could manage the risks without seriously impeding progress; in fact, chances are it would accelerate progress by reducing the social chaos that would otherwise be likely.\nTo address the threats of A+AI, specific actions that should be taken as soon as possible (whether by legislation or regulations) include:"}, {"title": "III. Threats", "content": "The list below is intended to be as comprehensive as possible. It includes time frame estimates (in italics) for when the given threat is likely to start having serious effects.\nMany of the near-term threats listed are happening now; the others are almost certain to materialize before long. Many involve a loss of trust, a very serious problem for any democracy. In early 2023, Gary Marcus, the well-known Al researcher and critic, commented \u201cI think we wind up very fast in a world where we just don't know what to trust anymore. I think that's already been a problem for society over the last, let's say, decade. And I think it's just going to get worse and worse.\" (Klein 2023)\nThe few \"long-term\u201d threats may happen or they may not, so how important and/or urgent are they? Some researchers argue that they're urgent because they might happen almost any time and with very little warning. All such claims are speculative; but we might best consider the opinions of Geoffrey Hinton and Yoshua Bengio, Turing Award winners who are universally considered among the godfathers of modern AI. In June 2023, Bengio estimated with 95% confidence that \u201csuperintelligence\" would be developed in 5 to 20 years (Bengio 2023). In February 2024, Hinton estimated that the probability of it being developed in the next 20 years is about 50% (University of Oxford 2024). Bengio's minimum time estimate and Hinton's estimate taken together suggest a superintelligent AI is unlikely to to be created within the next four years, and presumably it would take awhile for such an AI to cause much damage. However, an unaligned superintelligent AI\u2014one whose goals aren't compatible with the wellbeing of humans could be utterly catastrophic, and we have very little idea of how alignment can be achieved; for that matter, thus far, we can't even define what alignment would mean. In view of those facts, we need to start working on all of the threats listed, long- or near-term, as soon as possible.\nThe categories below are adapted from a taxonomy based on the type of harm caused and originally developed as part of the \u201cAI incident database\u201d from the Center for Security and Emerging Technology (CSET), though their database now uses a different taxonomy (\u201cWelcome to the Artificial Intelligence Incident Database\u201d n.d). Note that some sources of harm-for example, disinformation\u2014appear more than once because they can cause harm in more than one way. \u201cSoon\u201d in this list is equivalent to near-term, i.e., very roughly within a year."}, {"title": "T1. Harm to social or political systems.", "content": "Regardless of any other effects, everything in this category is likely to hurt society by reducing trust.\na. (happening now) Disinformation (false information distributed intentionally to mislead people), especially deep audio and video fakes, and especially when used for impersonating well-known persons; and misinformation (false information not intended to mislead). Both are increasingly serious problems. Misinformation from Als is mostly \u201challucinations\u201d (really confabulations, though the term is not well known), where an LLM-a \u201cLarge Language Model\" like GPT-4, Gemini, LLaMa, or Claude\u2014makes a statement that's a garbled version of its training data and is completely wrong. This is surprisingly common. It's becoming a serious problem because the LLM always sounds completely self-confident, and what it says is usually plausible, even to experts in the area in question. As a result, the misinformation is creeping into publications\u2014even scientific literature\u2014more and more (Subbaraman 2024). And if the misinformation is \u201cpumped back into the training data ocean...a future LLM may end up being trained on these very same polluted data.\u201d (McGraw et al. 2024) To minimize arguments over which of the categories (disinformation or misinformation) false information belongs in, countermeasures should not assume bad faith on anyone's part except perhaps in extreme cases. However, some cases of (intentional) disinformation are covered by existing laws as fraud or slander. An example that reached thousands of voters before the 2024 New Hampshire presidential primary: President Biden's voice urges listeners not to go to the polls and to \"save your vote for the November election.\" (Ramer & Swenson 2024) A more recent and, appropriately, more serious example: The immediate cause of the recent anti-immigrant violence in England was a single social media post suggesting the attacker was a Muslim immigrant (Kirka 2024).\nb. (happening now) Improper influencing via social media. A great deal has been written about the way A+AI -generated recommendations have driven social media users to extreme opinions and even extreme actions. There's evidence that \u201csocial media would still be a mess even without engagement algorithms\u201d (Farrell 2024); but there's also evidence that engagement algorithms have led to more extreme opinions and, sometimes, actions. An entire book, Max Fisher's The Chaos Machine (Little, Brown, 2022), is devoted to \u201cthe inside story of how social media rewired our minds and our world\".\nc. (may be happening now; if not, almost certain to happen soon) Directly and improperly influencing legislation and regulations (lobbying in a very general sense). An LLM \u201ccould automatically compose comments to regulators, write letters to the editor for local newspapers, and comment on news articles, blog entries, and social media posts millions of times a day.\" (Sanders & Schneier 2023)\nd. (happening now) Hacking (essentially, finding and exploiting loopholes in) social, economic, and political systems as well as computer systems. For example, hackers are already using AI-driven tools to illegally break into computer systems. Many, many laws are complex enough to have such loopholes, and A+AIs will be able to find them, allowing bad actors to exploit them, far more quickly than they can be closed. \u201cThe Coming AI Hackers\u201d (Schneier 2021) goes into detail about a breathtaking and frightening range of things its author believes are likely to happen, many of them in the near future. (A+Als can also help the \"good guys\u201d find them before bad actors do, but that's far from a complete solution.) Hacking of essential infrastructure is a plausible form of AI-enabled cyberwarfare. This could include damaging power, telecommunications, and water systems.\ne. (may be happening now; if not, almost certain to happen soon) Enabling non-American citizens to directly influence legislation and regulations without being identified as non-"}, {"title": "T2. Psychological harm", "content": "a. (happening now) Disinformation, especially deep audio and video fakes: used for scams via phishing, impersonating people, etc., and for pornography. Many cases of scams are undoubtedly covered by existing laws as fraud, and many of pornography are covered by existing laws for several reasons. But all, legal or not, hurt society by reducing trust. For example, a mother got a call from an unknown number and heard her daughter's voice crying and saying \"Mom, mom, I messed up.\" Then a man got on the line and told her that he had her daughter and demanded a million dollars to let her go unharmed. It turned out the voice wasn't her daughter (Panas 2023). Another example: Pornographic videos in which one person's face is pasted onto another's body-women in the vast majority of cases are becoming more and more common (Compton & Hamlyn 2023). This may well be a factor in the mental-health crisis for American teenage girls, though research on it is hard to find.\nb. (happening now) Improper influencing via social media. There's significant evidence that excessive social media use can lead to feelings of depression and isolation (Calfas 2023), and specifically that social media is a major cause of the mental-health epidemic in American teenage girls, 30% of whom now say that they have seriously considered suicide (Haidt 2024). It's very likely that social-media companies' use of A+AI leads many teen girls to excessive social media use.\nc. (may be happening now; if not, almost certain to happen soon) Miscellaneous actions taking advantage of trust: phishing, identity theft, etc. Many cases will be covered by existing laws as fraud; even those that don't involve fraud hurt society by reducing trust. An example: The quality of deepfakes is rapidly approaching the point where people are likely to wonder if even innocent-sounding calls and messages from close friends and family members really are from the person they appear to be from.\nd. (happening now in some countries; likely to happen elsewhere soon) Massive and deeply intrusive surveillance. An example: China now has hundreds of millions of surveillance cameras. The police have positioned them to capture as much activity as possible, and they're building upon that technology to collect voiceprints from the general public. DNA and iris scans as well as voice prints are being collected indiscriminately from people with no connection to crime. Other authoritarian regimes are likely to follow suit (Qian et al. 2021). Great Britain has moved in this direction, but nowhere near as far as China, and it seems unlikely that it will ever go that far. And given the American public's long-standing distrust of government, it's unlikely that the U.S. government will do anything at all comparable in the foreseeable future; but \u201csurveillance capitalism\u201d\u2014large-scale surveillance of online behavior-by large corporations is already a serious issue here, and there's evidence that government agencies are getting information from the corporations.\ne. (long term). Usurping humanity's role as the species with dominion over the earth. This might imply dethroning homo sapiens as the intellectual and/or creative pinnacle of the universe, or it might simply be a matter of power and control. Among the many relevant fictional accounts are the 1966 novel Colossus and the 1970 movie based on it, Colossus: The Forbin Project. Their story revolves around two supercomputers, one built by the U.S. and one built by the Soviet Union, each of which has been given full control of its country's nuclear weapons. Both prove to be far more intelligent than expected, and they join forces; declare that they can rule humanity better than we can rule ourselves; and threaten unprecedented destruction if their demands aren't met (Jones 1966). It's unlikely that many people would be happy with such a situation."}, {"title": "T3. Harm to physical health/safety", "content": "a. (may be happening now; if not, almost certain to happen soon) Controlling autonomous weapons. This technology obviously has tremendous potential to harm persons other than the intended targets. For example, both the U.S. and China are scrambling to field AI-controlled autonomous weapon systems and seriously considering lethal autonomous weapons (LAWs), systems that can decide on their own to kill people as opposed to merely destroying enemy \"assets\" (drones, ammunition depots, etc.) (Reuters 2023). In May 2024 China's military displayed a machine-gun-equipped robot \u201cdog\u201d; it may not be autonomous yet, but it's easy to see it as a candidate for a LAW (Lendon & Gan 2024). China is also developing onboard Al systems to enable missiles to identify and hit targets while maneuvering to avoid defensive countermeasures (Honrada 2022). (The first known attack by an autonomous weapon actually happened several years ago; but it was a 2020 attack by drones on other drones, not on people (Mizokami 2021).)\nb. (may be happening now; if not, almost certain to happen soon) Hacking (finding and exploiting previously unknown vulnerabilities) of controls of cars, power plants, etc. (Schneier 2021) For example, taking control of cars' steering remotely through software vulnerabilities has already been demonstrated more than once.\nc. (could happen soon) Helping develop weapons, especially difficult-to-control weapons. For example, a January 2024 study from OpenAI suggests that access to a special version of GPT-4 without the standard \u201cguardrails\u201d made it significantly easier for biology experts to complete tasks relevant to creating bioweapons, e.g., synthesizing the Ebola virus. Many articles in the popular press reported that the research showed that unrestricted GPT-4 access did not make a significant difference, but that's not at all clear. What is clear is that LLMs will soon be able to help create bioweapons even if they can't today. See Marcus (2024).\nd. (long term) Destroying civilization or even annihilating humanity. The idea that AI poses an existential risk to humanity has gotten much attention; see \u201cThe case for how and why AI might kill us all\" (Blain 2023) or the Wikipedia article \u201cExistential risk from artificial general intelligence.\" (Wikipedia 2024d). However, physical anthropologist Kevin Hunt observes that human beings are extremely resilient (Hunt 2024), and it's very doubtful that AI could literally make our species extinct, say, by the end of the 21st century. Killing 99% of the world's population or destroying civilization worldwide in the same timeframe would be virtually as serious, and either one seems far more likely.\""}, {"title": "T4. Harm to civil liberties", "content": "a. (happening now) Improper influencing via social media.\nb. (happening now in some countries; likely to happen elsewhere soon) Massive and deeply intrusive surveillance. An example: See the statement about authorities in various countries under T2d, above."}, {"title": "T5. Financial harm", "content": "a. (happening now) Disinformation, especially deep audio and video fakes, used for scams via impersonating people, etc. Cases here are likely to be covered by existing fraud laws; but again, even those that don't involve fraud hurt society by reducing trust. An example: Fraudsters lured a finance worker at a multinational firm into joining a video conference call in which all the other participants were deepfakes, with one posing as the company's chief financial officer. The worker was tricked into paying out $25 million to the fraudsters (Chen and Magramo 2024). In addition, at least two columnists have reported logging into their bank's website with AI-generated simulations of their own voices (Stern 2023).\nb. (may be happening now; if not, almost certain to happen soon) Hacking (finding and"}, {"title": "IV. Approaches to Protection", "content": "(a) Introductory notes\nFirst, it must be noted that two serious problems that several proposed countermeasures are subject to are requiring a definition of AI and requiring evaluation of a claim's truth. As pointed out before, defining AI is extremely difficult; but in most cases, the need for a definition can be sidestepped by referring to \"A+AI\" instead of just \u201cAI\u201d. In other cases, one can refer to \u201cgenerative AI\u201d, for which a reasonable definition is just \u201cSoftware capable of generating text, images, or other media, in response to commands from a human or other AI but without additional intervention.\" Unfortunately, if the truth of claims is relevant, there's no way to avoid evaluating it, and doing that well enough to satisfy most parties in the current political climate would be a tremendous challenge. Problem cases are legion. These days many persons, especially conservatives, would cite the Hunter Biden laptop story (Jenkins 2022) as an example.\nSecond, it's important to distinguish three types of users: (1) bad actors who are highly motivated and have significant resources behind them; (2) bad actors who lack either strong motivation or significant resources; and (3) users who are well meaning but not well-informed. Most countermeasures that would work for threats posed by those in the second and third categories\u2014e.g., penalties of some kind, or requirements to disclose provenance\u2014won't be effective against, say, the North Korean government. But that doesn't mean they're useless! The vast majority of threats are likely to come from the second and third groups. Greatly reducing the problems they cause would let us concentrate on the first group.\nIt must also be acknowledged that, as a technology implemented in software, AI crosses political borders-whether between states, countries, or anything else\u2014easily. One might conclude that restrictions of any kind are useless because there are no restrictions that every government in the world would agree to, and even if there were, bad actors\u2014both criminals and governments willing to ignore international norms-could simply ignore them. These are real problems and they should be taken seriously. But the likely effects of AI on society are very complex, and complete protection against its threats is out of the question. Simply doing our best to minimize Al's undesirable effects will undoubtedly require multiple approaches (what is sometimes called \u201cdefense in depth\"). It may well be worthwhile to limit what bad actors can do, especially in wealthier countries, as best we can. Furthermore, while many measures would not protect against determined and well-resourced adversaries, they would stop less serious troublemakers (groups 2 and 3 from the previous paragraph) so that authorities could concentrate on the others. In addition, many of the companies developing frontier models\u2014Anthropic, Google, Meta, and OpenAI, among others are based in the U.S. state of California; others-for example, Amazon and Microsoft\u2014are based in Washington State. This means that California laws could be relatively effective; U.S. national laws could be even more effective.\nIt'd be possible to regulate the technology itself or specific applications (also called \u201cuse cases\u201d) of the\""}, {"title": "(b) List of measures proposed", "content": "The list below includes every widely discussed category of approaches to protecting society from unintended effects of A+AI. While the list of categories is as complete as the author could make it, a great deal more could be written about every category and many proposals. The goal here is just to convey concisely the general idea of each category and key current proposals. Note that the appearance of a category or proposal here does not mean the author supports it! In fact, many of these are impractical and/or unlikely to be very effective; see Section IV(c) for discussion.\nIn the list below, the statement of each approach is followed by the date (to the author's knowledge) when the idea was first publicly proposed unless it's obvious.\nP0. Principles. Of course, principles in themselves won't accomplish anything; they're simply a basis for action. Specific proposals:\n(a) IBM's four principles of \"precision regulation\u201d for different use cases (mentioned in a U.S. Senate hearing of May 2023):\n1. Different rules for different risks: the strongest regulation for use cases with the greatest risk.\n(This principle has been adopted by the EU and, no doubt, others; see proposal P3a below.)\n2. Clearly defining risks.\n3. Transparency: consumers should know when they're interacting with A+AI, and have recourse to interact with a real person if they want.\n4. Showing the impact: for higher risk use cases, require companies to conduct risk assessments.\nHowever, note that the last two principles are so specific they're effectively regulations, and they're repeated in the section on regulations in this list. 2023 or earlier.\n(b) The White House \u201cBlueprint for an AI Bill of Rights for the U.S.\u201d (White House Office of Science and Technology Policy 2022) lists five \u201cprinciples\u201d: Safe and Effective Systems; Algorithmic Discrimination Protections; Data Privacy; Notice and Explanation; and Human Alternatives, Consideration, and Fallback.\n(c) The paper \"Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc\" (Lenat and Marcus 2023) proposes 16 \"Desiderata for a Trustworthy General AI \u201d: 1. Explanation: able to recount its line of reasoning behind any answer it gives. 2. Deduction: able to perform the same types of"}, {"title": "(c) What protective measures are likely to be effective?", "content": "With the ideas of Section IV(a) in mind, it's clear that many of the proposals listed above are impractical and/or unlikely to be very effective. One example is Category P5, a worldwide pause or even an indefinite freeze in research on and development of AI. Eliezer Yudkowsky, the author and best-known proponent of the latter argues that even nuclear war is preferable to allowing unfettered work on AI to continue (Yudkowsky 2023). This is quite difficult to believe, but even if he's right, world leaders are very far from taking the danger of AI seriously enough to support an indefinite freeze.\nCategory P1, wrappers, is a second example: they may work well in specific applications where what prompts users are allowed to give can be severely restricted, but in normal situations where users can ask anything, even the built-in wrappers of LLMs have been defeated over and over (Murgia n.d.). A third example: requiring social-media companies to verify facts or provenance, which probably violates the First Amendment. As Section IV(a) points out, its practicality is also questionable. Requiring those companies to use certain controls brings up the same objections but considerably less strongly, so that might or might not be workable.\nIt has been argued that licensing requirements would benefit only \u201cincumbents\u201d and would hurt newcomers (Kapoor and Narayanan 2023). In addition, courts have held that software is a form of speech, suggesting that requiring licenses for software written by Americans is a violation of the First Amendment. For both reasons licensing AI systems is unlikely to work.\nFinally, watermarking of A+AI output appears at first to be yet another hopeless approach. Bad actors will undoubtedly look for ways to remove watermarks, and they may well succeed even if the watermarks are maximally indelible. If they succeed, of course society will look for another way to do the watermarking. But experts have expressed doubts as to whether it's possible to stay ahead in this race (Knibbs 2023), and the history of attempts to watermark at least one medium-digital audio\u2014is not encouraging\nBut this does not mean requiring watermarking\u2014more generally, provenance information\u2014is hopeless, and it's so valuable it's worth fighting for, so to speak. Watermarking could be made reasonably effective by the means the Digital Millennium Copyright Act used to protect weakly encrypted digital audio recordings, namely making it a crime to circumvent the encryption The equivalent in this case would be removing the watermark, or using a system\u2014perhaps one built by the adversary from open source that does not add watermarks. This would not protect against determined and well-resourced adversaries, but it would stop less serious troublemakers (groups 2 and 3 from Section IV(a), and probably the vast majority) so authorities could concentrate on the others. It's desirable to require both a prominent notice (which would make the document's provenance obvious but would be relatively easy to remove) and a watermark (which would probably not be detectable except by computer and would be much more difficult to remove). It's also desirable that the watermark be easily detectable by computer to facilitate automating responses to incoming documents. If watermarking is mandated, bad actors are likely to try to confuse citizens by adding watermarks to documents not produced or modified by A+AI, so consideration should be given to outlawing that behavior.\nNote that how provenance information is added and how it's shown is necessarily medium-specific. It's usually thought of in terms of images, but similar considerations apply to audio, video, and even text-though the difficulty of doing so varies greatly; it's especially hard to see how text could be watermarked effectively.\nSocial media platforms have a great deal of control over how and how widely user postings are disseminated. One idea for reducing harm is via regulations that target the dissemination via social"}, {"title": "V. Legislation and/or regulations to enact now", "content": "Speaking to the danger of future Als, Yuval Noah Harari has written (Harari 2023):\nWon't slowing down public deployments of Al cause democracies to lag behind more ruthless authoritarian regimes? Just the opposite. Unregulated AI deployments would create social chaos, which would benefit autocrats and ruin democracies.\nWe should put a halt to the irresponsible deployment of AI tools in the public sphere, and regulate Al before it regulates us. And the first regulation I would suggest is to make it mandatory for Al to disclose that it is an AI. If I am having a conversation with someone, and I cannot tell whether it is a human or an AI-that's the end of democracy.\nHow AI will develop in the future is unclear in many ways. As a result, AI researcher Jack Clark's argument is equally important (Clark 2024):\nI've come to believe that in policy \"a little goes a long way\" \u2013 it's far better to have a couple of ideas you think are robustly good in all futures and advocate for those than make a confident bet on ideas custom-designed for one specific future.\nOf the current ideas for making A+AI safer, those that should be put into legislation or regulations as soon as possible include:\n* Require all social media platforms accessible within the U.S. to offer users verification that their accounts are owned by U.S. citizens, and to prominently display every account's verification status on everything associated with the account. Citizenship must be verified by presentation of strong identification, perhaps with an in-person option that would be displayed.\n* Establish regulations on A+AI in three regards:\n    * Harari's \"first regulation\": All products created or significantly modified with A+AI\u2014not just generative AI\u2014must be clearly labeled as such. This applies to AIs that converse with humans, even for mundane purposes like tech support, as well as to all pre-generated media. The details necessarily vary with the medium, but for media for which it makes sense both a prominent notice (to make the document's provenance obvious, though it would be relatively easy to remove) and a watermark (which would probably not be detectable except by computer and would be much more difficult to remove) are required. There are stiff penalties for violations. (Omitting a label intentionally for a nefarious purpose should probably also be a serious crime.)\n    * Generative AI may not be used to create a likeness of a minor and may not be used to create a likeness of an adult without the advance permission of that adult.\n    * Creators of generative AI software must disclose all materials used to train their software and must compensate the creators of any protected material (copyright, trade secret, trademark, or other) used in the training of the AI.\n* Fund a crash project of research on mitigating the threats, with one or more of a variety of goals. Most important in the near term: making LLMs and other AIs more trustworthy, reliable (in the sense of avoiding \u201challucinations\u201d), and explainable. On a larger time scale, alignment of AI agents\u2014AIs that actually have their own goals\u2014is vital. Other worthwhile goals include improved watermarking of all media, and improved detection of AI-generated content, especially deepfakes. The project should be under international control, for example via the nascent international network of AI safety institutes.\n* Fund large-scale educational campaigns to raise awareness of the threats among election workers and among citizens in general.\nLegislative and regulatory actions on new and newly discovered threats to society will naturally evolve over time. But all of the above ideas could be implemented today and seem clearly to be constitutional, and they are likely to ameliorate quite a bit of the negative effects of A+AI that we can already see. In addition, none requires a definition of AI or judging the truth of claims. And, while AI as a whole is very difficult to define, \u201cgenerative Al\u201d is not"}, {"title": "Author Biography", "content": "Don Byrd received his Ph.D. in Computer Science from Indiana University in 1984; his dissertation supervisor was the well-known cognitive scientist Douglas Hofstadter. Byrd is noted for his contributions to music information retrieval\u2014an AI-related field he helped to found\u2014and music information systems. He has also worked both inside and outside academia on text information retrieval, information visualization, user-experience design, and math education, among other things. His work outside academia was for three high-tech startups, including one of his own. Byrd is the author of the open source music notation editor Nightingale. Now retired, he spends some time on music, but more working with Braver Angels (https://braverangels.org), a grassroots national organization dedicated to reducing the toxic polarization of American politics. He is one of Braver Angels' state coordinators for Indiana, and he co-chairs a Braver Angels \u201ctask force\u201d to combat AI-driven polarization."}]}