{"title": "Sing-On-Your-Beat: Simple Text-Controllable Accompaniment\nGenerations", "authors": ["Quoc-Huy Trinh", "Minh-Van Nguyen", "Trong-Hieu Nguyen Mau", "Khoa Tran", "Thanh Do"], "abstract": "Singing is one of the most cherished forms\nof human entertainment. However, creating\na beautiful song requires an accompaniment\nthat complements the vocals and aligns well\nwith the song's instruments and genre. With ad-\nvancements in deep learning, previous research\nhas focused on generating suitable accompani-\nments but often lacks precise alignment with\nthe desired instrumentation and genre. To ad-\ndress this, we propose a straightforward method\nthat enables control over the accompaniment\nthrough text prompts, allowing the generation\nof music that not only complements the vocals\nbut also aligns with the song's instrumental\nand genre requirements. Through extensive ex-\nperiments, we successfully generate 10-second\naccompaniments using vocal input and text con-\ntrol. Additionally, our method demonstrates ro-\nbust control over the generated accompaniment\nbased on input prompts, improving alignment\nwith the song's instrumental and genre needs. A\nlink to our work is available at https://songgen-\nai.github.io/llambada-demo/.", "sections": [{"title": "1 Introduction", "content": "Music is a fundamental aspect of human culture.\nIn recent years, with the rapid advancement of\ndeep learning, several methods for non-vocal mu-\nsic generation have been proposed, such as Au-\ndioLM (Borsos et al., 2023), MusicGen (Copet\net al., 2024), and Stable Audio (Evans et al., 2024).\nThese methods have shown promising results in\naddressing music generation challenges. In addi-\ntion to non-vocal music, vocal music generation\nhas become a significant task today, with various\nreal-world applications. However, generating vocal\nmusic requires both the singing voice and instru-\nmental accompaniment, as they play crucial roles\nin the process.\nNaturally, in singing, the vocals and melody\nalign with the beat, essential to forming a com-\nplete song. For this reason, SingSong (Donahue\net al., 2023) and Fast-SAG (Chen et al., 2024) are\ntwo of the first systems designed to generate in-\nstrumental audio that accompanies vocal record-\nings. Regarding SingSong, they propose a system\nthat adapts AudioLM (Borsos et al., 2023) with\nthe Transformer model to train an \"audio-to-audio\"\nmodel for the accompaniment generation via the\nvocal input. With FastSAG, they propose a Non-\nAutoregressive diffusion-based framework that is\nconditioned based on the vocal signal to directly\ngenerate the Mel-Spectrogram of the accompani-\nment. From their published results, the quality of\nthe output audio is superior and their results are\npromising and have the potential to enhance music\nproduction.\nSince the beat and accompaniment reflect the\ncreativity of the producer, users often want con-\ntrol over these elements, including the specific\ninstruments, beat type, rhythm, and tempo. De-\nspite the promising audio quality of previous works,\nthey have struggled to generate accompaniment\nthat meets users' creative demands. To address\nthis challenge, we propose Llambada, a simple,\ntext-controllable system for accompaniment gen-\neration. Llambada allows users to define their de-\nsired accompaniment by inputting their require-\nments through a text prompt, enabling the input\nvocal sings along to their custom beats. This\nmodel incorporates AudioLM (Borsos et al., 2023),\nthe Audio-Language Alignment Model (CLAP)\n(Elizalde et al., 2023), Encodec (D\u00e9fossez et al.,\n2022), and Mert (Li et al., 2023). These compo-\nnents process text prompt tokens alongside vocal\nsemantic tokens-representing the general rhythm\nand harmony of the accompaniment\u2014to generate\naccompaniment that is both in sync with the vocal\ninput and text-controllable. To our knowledge, our\nsystem is one of the first systems that support the\naccompaniment generation with text control.\nMoreover, the challenge of this task is the lack\nof a dataset. Previous works focus on the dataset"}, {"title": "2 Related work", "content": "Audio generation is one of the most challenging\ntasks in AI. It involves generating audio from var-\nious inputs, such as speaker identity or reference\nspeakers. One of the pioneering works in this field\nis WaveNet (Van Den Oord et al., 2016), which in-\ntroduced a groundbreaking unconditional model to\ngenerate audio. WaveNet has been widely applied\nin text-to-speech systems. In the domain of music\ngeneration, it has also been used to produce piano\nand general music audio. Building on this, Sam-\npleRNN (Mehri et al., 2016) was proposed, which\nutilizes RNNs at different scales of the model to\ncapture long-term dependencies, addressing the\nlimitations of previous works that struggled with\nmaintaining coherence and rhythm in the generated\naudio.\nIn recent years, the rapid development of\nattention mechanisms and Transformer models\n(Vaswani, 2017) for sequence-to-sequence tasks\nhas significantly advanced audio generation. One\nof the most notable contributions is Jukebox (Dhari-\nwal et al., 2020), which employs a VQ-VAE archi-\ntecture and an autoregressive Transformer (AR)\nto generate high-fidelity music. A key innovation\nof Jukebox is its conditioning mechanism, which\nallows for input prompts such as vocal style and\nunaligned lyrics, resulting in highly coherent vocal\nperformances and songs. In addition to Jukebox,\nrecent models like AudioLM (Borsos et al., 2023),\nMusicLM (Agostinelli et al., 2023), and MusicGen\n(Copet et al., 2024) have also gained popularity\nfor generating high-quality music based solely on\ntext prompts. These models introduced the con-\ncepts of semantic and coarse tokens, which have\nlaid the groundwork for subsequent advancements\nin music generation, such as SingSong (Donahue\net al., 2023), and have influenced tools for music\nproduction, including SUNO and Udio.\nInspired by these works and their proven effec-\ntiveness, Llambada is designed with two stages.\nThe first stage, semantic generation, produces se-\nmantic tokens that represent the general structure\nand rhythm of the song, controlled by vocal and\nprompt inputs. The second stage, coarse generation,\nproduces acoustic tokens, which are decoded into\nmusic audio using the Encodec model (D\u00e9fossez\net al., 2022)."}, {"title": "2.2 Accompaniment Generation", "content": "Accompaniment generation is the task of creating\ninstrumental music that complements input vocals.\nSingSong (Donahue et al., 2023) and FastSAG\n(Chen et al., 2024) are among the first methods\nto successfully generate high-fidelity instrumental\naudio without relying on symbolic representations.\nTheir results have been widely praised for their"}, {"title": "3 Task Definition and Method", "content": "In this work, we propose an accompaniment gen-\neration model as a conditional model, with the in-\nputs are vocal waveform $X_{vocal} \\in R^{f \\times T}$, and text\nprompt $X_{prompt}$ and the output is the accompa-\nniment music waveform $Y \\in R^{f \\times T}$, with given\nsample rate s and time frame T, we want to model\na distribution $P(Y|X_{vocal}, X_{prompt})$. The effec-\ntiveness of the text control is assessed in Section 4."}, {"title": "3.2 Modelling through a proxy distribution", "content": "As observed in previous works like Jukebox and\nWaveNet, modeling the distribution directly in the\nwaveform domain requires significant computa-\ntional resources and large datasets to generate high-quality audio, due to the complex structure of wave-\nforms. To address this challenge, modeling the\ndistribution in a proxy domain using discrete au-\ndio codes-has proven more efficient. This ap-\nproach simplifies training by constraining the prob-\nlem to a finite set of audio codes. Specifically, we\ndefine the encoded vocal and accompaniment as\n$X_{vocal} = Enc_{vocal}(X)$ and $Y = Enc(Y)$, where\n$X_{vocal} \\in V^{f \\times T}$ and $\\hat{Y} \\in V^{f \\times T}$ (with V is the fi-\nnite vocabulary set with size c). These encoded\nrepresentations can be decoded back into X and\nY through the Dec(.) function, where both the en-\ncoder and decoder are parametric functions. In\nterms of prompt input, we also do the transform\nfrom the text prompt embedding to the same dis-\ncrete code domain of the audio, the audio code\ndenotes as $X_{prompt} \\in V^{f \\times T}$."}, {"title": "3.3 Audio and Text representation", "content": "Inspired by the MusicLM (Agostinelli et al., 2023),\nwe extract the semantic, and acoustic information\nof the audio by two public models are MERT (Li"}, {"title": "3.4 Overview of Llambada", "content": "As mentioned in the previous sections for the rea-\nson why we should do the modeling through a\nproxy distribution, Llambada is proposed to gener-\nate the acoustic tokens that can be decoded to the\naccompaniment waveform, which is illustrated in\nFigure 1. The first stage is the semantic modeling\n(input: vocal and prompt, output: semantic tokens\nof the accompaniment), and the Coarse Acoustic\nModeling (input: accompaniment semantic tokens\nand the vocal coarse tokens). Let $X_{vocal} = A$, and\ntext prompt $X_{prompt} = B$. From the task defini-\ntion, we can decompose the modeling of the proxy\ndistribution as the following Equation 1.\n$P(Y|A, B) \\approx P(\\hat{Y}|\\hat{A}, B)$\n$= P(Coarse(Y)|Sem(Y), Coarse(A))$\n$.P(Sem(Y)|Sem(A), CLAP(B))$\nFrom the Y, we can employ the Encodec (D\u00e9fossez\net al., 2022) decoder model to decode the acous-\ntic to the waveform which is the accompaniment\noutput."}, {"title": "3.5 Accompaniment Semantic generation\nstage", "content": "As shown in Figure 1, during the accompaniment\nsemantic generation stage, the vocal mono-audio\nwaveform with dimensions T \u00d7 1 (where T repre-\nsents the number of audio time frames) is encoded\ninto an audio embedding using the MERT model\n(Li et al., 2023), resulting in an embedding with di-\nmensions $T_e\\times768$ (where $T_e$ is the MERT timestep,\ndepending on the audio input length). Through a\nquantization process, a code sequence with dimen-\nsions $T_e \\times 5$ is obtained. Similarly, the text prompt\nis encoded by the CLAP model (Elizalde et al.,"}, {"title": "3.6 Accompaniment Acoustic generation stage", "content": "In the generation of accompaniment acoustic to-\nkens, the input vocal mono-audio waveform with\nshape T \u00d7 1 is processed by the Encodec model\n(D\u00e9fossez et al., 2022). These are then encoded\ninto a codec token sequence with shape $T_e \\times N_g$,\nwhere $n_g$ is the number of quantizers, and $T_e$ rep-\nresents the time steps of the Encodec model. This\ntoken sequence is integrated with the accompani-\nment semantic tokens of shape T \u00d7 1024, and a T5\nTransformer Decoder is employed to predict coarse\nacoustic tokens with shape $T_e \\times 3$. These predicted\ntokens can then be decoded into the accompani-\nment waveform, maintaining the same shape as the\nvocal input."}, {"title": "3.7 Llambada Inference Stage", "content": "During the inference stage, users can input the text\nprompt and the vocal input for the accompaniment\ngeneration model. Then the CLAP model, MERT\nencoder, and Encodec model generate the code to-\nkens for the prompt input, the semantic tokens, and\nthe acoustic tokens of the vocal. In the first stage,\nthe Semantic Accompaniment Language model\nuses the clap tokens and semantic tokens to pre-\ndict the accompaniment semantic tokens. Then, in\nthe"}, {"title": "3.8 Pseudo Captioning dataset pipeline", "content": "The community has released several music caption-\ning models such as K2C-Aug (Wu et al., 2023),\nand LP-MusicCaps (Doh et al., 2023) with promis-\ning results. Moreover, the main problem of the\nsong dataset is that, there are no captions and de-\nscriptions for the song, and the annotation cost by\nhumans is costly. For this reason, we leverage LP-\nMusicCaps for prompt generation. Get the input\naudio $X_a$, our target prompt $X_p$ is generated by the\ncaptioning model. Then, to filter out the unused\ndata, and keep the important tags, we use Llama 3\n(AI@Meta, 2024) model to do the tag extracting.\nFrom then, we can have clean prompts for the train-\ning dataset. The prompt output has the same format\nas the normal person's prompt to the system."}, {"title": "4 Experiments", "content": "For the training and testing dataset, we all separate\nthe source music into the two components are the\nvocal and the accompaniment through the state-of-\nthe-art model in the music source separation, De-\nmucs (Rouard et al., 2023; D\u00e9fossez, 2021). Then,\nto assess the performance of the Llambada model\nby reproducing the training and testing Llambada\nwith previous methods are SingSong and Fast-SAG\nin the following dataset setup."}, {"title": "4.1.1 Training dataset", "content": "For the training dataset, we do the training on the\n4400 music hours for both vocals and the accom-\npaniment, including the music some several genres\nsuch as pop, rock, jazz, and ballad and with the\nvariant of the instruments including drum, bass,\nguitar, piano, and organ. For the prompt of the\naudio, we follow the format that includes genre,\ninstruments, and the rhythm of the song, which can\nallow the model to learn and can generate the song"}, {"title": "4.1.2 Test dataset", "content": "For the testing dataset, we follow the baseline of\nthe creation of the training dataset, and we have\ntwo domains of testing.\nIn-domain testing: which uses the vocal and\nprompt from the same genre, with similar instru-\nments from the training dataset (but these songs\nare not used in training) to evaluate the alignment\nand the effectiveness and the accompanied of\nthe beat generated by the Llambada model\nin the accompaniment generation stage. This\ndataset setup contains 53 songs from a variety\nof instruments such as guitar, bass, and drum,\nand several genres such as pop, rock, ballad, and\nromantic. Each song is segmented into several\n10-second segments for the best of comparison.\nOut-of-distribution testing In the out-of-distribution test dataset setting, we reproduce\nthe caption generation and preprocessing by\nthe Musiccap (Agostinelli et al., 2023) for the\nbenchmark matching between the output audio and\nthe prompt. With the out-of-distribution dataset,\nwe use the dataset from MusDB18 (Rafii et al.,\n2017), which is not included in our training dataset.\nFor this testing dataset, we used 150 songs from\nthe original set. This test dataset setting aims to\nevaluate the generalization and the robustness of\nthe model.\nEvaluation: To evaluate audio quality and the\nalignment between the prompt and audio output,\nwe use the Frechet Audio Distance (FAD) metric\n(Kilgour et al., 2018) and the CLAP score (Elizalde\net al., 2023). The FAD metric does not always accu-\nrately reflect audio output quality, so we reproduce\nFAD using two pre-trained models: VGGish (Her-\nshey et al., 2017) and CLAP music (Elizalde et al.,\n2023). We then calculate the mean of these two re-\nsults to enhance the robustness of the comparison."}, {"title": "4.2 Baseline and Implementation Details", "content": "As there are no official implementation and dataset\ndetails of the SingSong and FastSAG, we reproduce\nthese two methods in as same as our dataset format\nfor a fair comparison.\nSingSong (2023): (Donahue et al., 2023) In\nour implementation of SingSong, we build upon\nthe open-musiclm source code. Similar to Musi-"}, {"title": "4.3 Qualitative results", "content": "Table 1 presents a comparison of the Llambada\nmodel with previous works. Thanks to the\ntext-prompt control, Llambada outperforms both\nSingSong and other Llambada variants across all\nfour metrics, with particularly notable improve-\nments in the CLAP score and FADmean, by\n+0.087 and -0.0695, respectively. These results\nindicate higher audio quality and stronger align-"}, {"title": "4.3.2 Out-of-distribution results", "content": "Table 2 illustrates the comparison between the\nLlambada model and previous methods on the\nout-of-distribution testing domain. Thanks to text-prompt control, our method surpasses prior works\non the FADmean and CLAP score metrics, with\nimprovements of -1.537 and +0.12, respectively.\nThese results demonstrate the significant enhance-\nment in Llambada's performance when guided by\ntext prompts, which align the model's output more\nclosely with the ground truth, leading to better\nbenchmark results."}, {"title": "4.4 Ablation Study", "content": "To assess the effectiveness of the text guiding in sev-\neral criteria from the song, we do the experiments\nto validate the matching between the prompt con-\ntrol with each criteria of the music, including the\ninstruments and the genres of the song. However, to\ndo that we have to have the music expert do the eval-\nuation, but it is difficult, and high cost for the music\nexpert's judgment. Thanks to the Llava-Bench (Liu\net al., 2024) idea when they leverage the GPT-4 in\nthe benchmark of the multimodal-LLM, inspired\nby this idea, we employ Qwen-Audio (Chu et al.,\n2023), an Audio-Language model to the judgment\nand scoring between the alignment of the music\ngenerated output and the ground truth in several mu-\nsic criteria. The benchmark based on Qwen-Audio\nis produced three times, and we calculate the mean\nscore of the three-time outputs. The example of the\ntemplates to ask the grading from Qwen-Audio is\nthe following:\n1. Human: Given the <audio input>, with"}, {"title": "4.4.1 Evaluation of the effectiveness of the\ntext prompt in the instrument alignment", "content": "To assess the effectiveness in the alignment be-\ntween the text prompt and the instrumental from\nthe music output, we do the benchmark on the in-\nstrumental characteristic criteria of the song with\nthe instrument mentioned in the accompaniment\nground truth, which is illustrated in Table 3."}, {"title": "4.4.2 Evaluation of the effectiveness of the\ntext prompt in the genre alignment", "content": "To evaluate the alignment in genre between the\ngenerated accompaniment that is controlled by text\nprompt and not controlled by text prompt, we do\nthe evaluation based on the genre criteria, which is\nillustrated in Table 4."}, {"title": "5 Conclusion", "content": "In conclusion, we introduce Llambada, a novel\npipeline for the text prompt control of the accom-\npaniment generation task. Additionally, we also\npropose a pseudio captioning dataset pipeline of\nthe dataset creation for the text prompt from the\naccompaniment, which can serve this task. From\nextensive experimental results, it shows that our\nmethod can surpass in the music quality and the\nbetter alignment with the clients' demands when\ncompare with previous works, which indicate the\npromising results of the model.\nAlthough the promising performance of our\nmethod, there are several rooms should be im-proved. Firstly, the alignment between the text\nprompt with the semantic feature of the song to\nmake the better understanding of the Language\nModel in the relation between inputs tokens. Sec-\nondly, this method leverages two Language models,\nwhich can get the extensive computing resource.\nWe encourage the further researches focus on these\ntwo limitations to improve the model, thus can en-\nhance the application of the music generation."}]}