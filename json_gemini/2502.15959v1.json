{"title": "A Knowledge Distillation-Based Approach to Enhance Transparency of Classifier Models", "authors": ["Yuchen Jiang", "Xinyuan Zhao", "Yihang Wu", "Ahmad Chaddad"], "abstract": "With the rapid development of artificial intelligence (AI), especially in the medical field, the need for its explainability has grown. In medical image analysis, a high degree of transparency and model interpretability can help clinicians better understand and trust the decision-making process of AI models. In this study, we propose a Knowledge Distillation (KD)-based approach that aims to enhance the transparency of the AI model in medical image analysis. The initial step is to use traditional CNN to obtain a teacher model and then use KD to simplify the CNN architecture, retain most of the features of the data set, and reduce the number of network layers. It also uses the feature map of the student model to perform hierarchical analysis to identify key features and decision-making processes. This leads to intuitive visual explanations. We selected three public medical data sets (brain tumor, eye disease, and Alzheimer's disease) to test our method. It shows that even when the number of layers is reduced, our model provides a remarkable result in the test set and reduces the time required for the interpretability analysis.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) is now widely used in various fields, especially deep learning (DL), which advances DL applications in more autonomous directions (Wang et al. 2023a; Al-lQubaydhi et al. 2024). In medical imaging, AI has become integral to healthcare (Chaddad, Wu, and Desrosiers 2024; Amiri et al. 2024). Despite Convolutional Neural Networks (CNNs) performing well in medical classification tasks, they lack transparency, particularly in decision-making (Chaddad et al. 2024; Hassija et al. 2024). This is crucial in medicine because unexplainable AI results cannot convince physicians and may lead to medical accidents (Jin et al. 2023). Explainable artificial intelligence (XAI) addresses this by enhancing model transparency and understandability, helping physicians trust AI results and improving medical decisions (Chaddad et al. 2023a; Van der Velden et al. 2022).\nDL models often have large parameters, complicating interpretation. Knowledge distillation (KD) transfers knowledge from complex to simpler models (Hinton, Vinyals, and Dean 2015), widely used in medical tasks. For instance, (Wang et al. 2023b) proposed a self-supervised method integrating diverse knowledge into KD for skin disease classification, achieving higher performance. The Distilled Student Network, using KD, detects melanoma from dermoscopic images more efficiently than pre-trained models (Khan et al. 2022). Smaller models are computationally cheaper and easier to understand. Our key question is: How to design a simple model with feasible classification accuracy while maintaining high interpretability? This study simplifies CNN architecture through KD, retaining most dataset features and reducing network layers. Our approach involves: 1) training a teacher model such as CNNs and performing KD to obtain a simpler student model, 2) using average feature maps on limited layers for intuitive visualization. We use Kullback-Leibler (KL) divergence as a loss function to preserve useful features while simplifying the model. Feature distillation aims to make the student model learn similar feature representations as the teacher model, enhancing student model performance. We performed a layer-by-layer interpretability analysis, reducing CNN layers without compromising effectiveness. Analyzing each student model layer's feature map identifies key features and decision-making processes. Our goal is to maximize model performance and interpretability while maintaining a lightweight structure. To verify the proposed approach, we used two interpretability methods: 1) Grad-CAM, highlighting image regions contributing to model predictions by visualizing the last convolutional layer (Selvaraju et al. 2017), and 2) SHapley Additive exPlanations (SHAP), assigning each feature a contribution value to the model output (Lundberg and Lee 2017). Our contributions are:\n\u2022 We propose a solution to the interpretability problem using KD in the context of DL, ensuring high performance and quick explanations.\n\u2022 We simulate the proposed model using three medical datasets to demonstrate classifier accuracy with few layers.\n\u2022 We compare the proposed interpretability model with the Grad-CAM and SHAP models using both visual explanations (e.g., heat map) and quantitative metrics (e.g., Fidelity score)."}, {"title": "Related Work", "content": "Knowledge Distillation: Several KD methods are used today in medical applications (Gou et al. 2021). For example, a study proposes a KD strategy for building compact DL models suitable for the classification of chest X-ray images (CXR) with multiple labels in real time. It uses different CNNs and transformers as teacher networks to transfer KD to smaller student networks (Termritthikun et al. 2023). In (Park et al. 2022), distillation for self-supervision and self-training demonstrates higher performance in diagnosing tuberculosis, pneumothorax, and COVID-19 even when using unlabeled data. In (Patel et al. 2023), the study predicted results using the Open Access Serial Imaging Study (OASIS) MRI dataset. The process includes data exploration, data preprocessing, and the hybrid model that integrates both logistic regression and decision tree algorithms. The proposed hybrid model outperforms existing models, with an overall accuracy of 96%. Furthermore, in (Liu et al. 2023), they introduced a dual-branch architecture that improves performance by transferring knowledge from a teacher to a student model, achieved by minimizing Shannon entropy and KL divergence. This method achieved outstanding results in a public data set for the left ventricular endocardium segmentation task.\nExplainable AI: As previously presented, the importance of XAI in medical diagnosis continues play a major role in providing clear interpretability (Chaddad et al. 2023b). For example, in (Raihan et al. 2023), the authors used Extreme Gradient Boosting (XGboost) to predict whether a patient has chronic kidney disease (CKD). SHAP analysis is used to explain the impact of features on the XGBoost model. For example, using SHAP analysis and the Biogeography-Based Optimization (BBO) algorithm, hemoglobin models arebumin contributed mainly to the detection of CKD. In (Alabi et al. 2023), machine learning (ML) models are used with understandable AI to build a prognostic model to group patients with nasopharyngeal cancer (NPC) into two groups based on their survival probabilities, high and low. Local Interpretable Model Agnostic Explanations (LIME) and SHAP models were used to provide interpretability. LIME and SHAP models identify personalized protective and risk factors for each NPC patient, revealing new non-linear relationships between input features and survival odds. Similarly, Grad-CAM introduced a novel XAI framework that enhances feature explainability for decision making in tumor diagnosis using ultrasound data, as proposed in (Song et al. 2023). This framework is capable of identifying regions that are more relevant and feature-associated compared to the traditional Grad-CAM. Currently, an automated approach for detecting and classifying leukemia was demonstrated in a data set independent of the subject, using deep transfer learning supported by Grad-CAM visualization (Abhishek et al. 2023). Unlike previous methods, our approach uses KD to reduce the number of CNN layers and outputs the main features of each layer to improve the interpretability analysis. Furthermore, the model can fully demonstrate the decision-making process while retaining most of the key features."}, {"title": "Method", "content": "Figure 1 illustrates the pipeline of our approach. The KD method is used to train CNN, preserving its essential features, and the average feature map is used to illustrate the decision-making process at each layer, emphasizing the decision-making process within CNN. Algorithm 1 provides a basic procedure of our approach.\nKD. The objective of KD is to have the student model learn to mimic the behavior of the teacher model. This is typically achieved through a combination of the following two types of loss functions. The first one is Hard Loss (denoted as $\\mathcal{L}_{HL}$), the $\\mathcal{l}_{HL}$ function we used is the Cross-Entropy Loss, which for a multi-class classification problem. M represents the total number of classes, N represents the total number of samples, and $y_{s,i,c}$ denotes the predicted probability of the student model for the i-th sample belonging to class c. $Y_{i,c}$ represents that for the i-th sample, the true label for class c. In one-hot encoding, if sample i belongs to class c, then $Y_{i,c} = 1$; otherwise, it is 0. log $(y_{s,i,c})$ is the logarithm of the predicted probability. The log function amplifies the penalty for probabilities below 1, making the model focus more on those samples that are predicted incorrectly. It can be expressed as follows.\n$\\mathcal{L}_{HL} = \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{c=1}^{M}Y_{i,c} \\text{log} (y_{s,i,c})$\nSoft Loss ($\\text{l}_{SL}$) is the loss between the softened output of the\nteacher model and the softened version of the true labels. It\ndiffers from traditional KD by directly comparing the output\nof the teacher model with the adjusted real labels. This guides\nthe student model to achieve the performance of the teacher\nmodel more directly. The temperature parameter T adjusts the"}, {"title": "Experiment", "content": "Brain tumor: This dataset (Nickparvar 2021) is a collection that combines three separate data sets: figshare (Figshare 2017-04-03), the SARTAJ (Bhuvaji 2020), and Br35H (Hamada 2020). A total of 7023 MRI images of the human brain are included. These images are classified into four classes: glioma (n=1621), meningioma (n=1645), no tumor (n=2000), and pituitary (n=1757). Unlike other datasets, official website has predefined test and training sets for brain tumor dataset. Therefore, our training-to-validation ratio is 8:2.\nEyes disease: This dataset consists of 4217 Colour fundus photography (CFP) with four classes: cataract (n = 1038), diabetic retinopathy (n = 1098), glaucoma (n = 1007) and normal images (n = 1074) (Doddi 2023). We randomly divided the images according to training, validation and testing with a ratio of 7:2:1.\nAlzheimer: The dataset consists of MRI images grouped into four classes: Mild (n = 8960), moderate (n = 6464), non- demented (n = 9600), and very mild (n=8960) (Kaggle 2022). The dataset contains both augmented images (processed through random rotations, cropping, color transformations, etc.) and original images. The images are randomly divided into training, validation, and testing ratios of 7:2:1. For detailed information on the training/validation and testing sets, refer to Table 1."}, {"title": "Implementation details", "content": "We used an Intel i9-13900k, an NVIDIA GeForce RTX 4090, and TensorFlow-gpu 2.6.0 for our simulations. We used consistent hyperparameter settings, employed the Adam optimizer, set the learning rate to 1 \u00d7 10\u22124, and configured the batch size to 16. The model we used as a teacher model is DenseNet121 pretrained on ImageNet (Huang et al. 2017). In the training phase of the teacher model, average pooling is used to extract features, with the aim of preserving background information and considering overall characteristics effectively. However, for the student model, we used max pooling to capture the most prominent features within an area. The training of the models is performed on a single GPU, while the interpretability tasks are carried out on the CPU. We used standard classification performance metrics such as accuracy, and F-score.\nFigure 2 shows the confusion matrix obtained from both the teacher model and the student model, which is used for class-wise performance evaluations. We also used the Receiver Operating Characteristic (ROC) curve (Mandrekar 2010) and Decision Curve Analysis (DCA) (Vickers and Elkin 2006) to evaluate the classifier models."}, {"title": "Simulation result", "content": "During the training process of the teacher model, we consistently use Densenet121 as our teacher model across each dataset. After obtaining the teacher model, we experiment with different parameters (\u03b1 and T) to perform KD on it, aiming to achieve optimal performance of the student model. The test accuracy, F1 score and test loss of each model are reported in Table 2.\nBrain Tumor: The test accuracy of the teacher model can reach 0.9676. Furthermore, the student model is able to perform well in the test set. The best model is the S5 model (\u03b1=0.7, T=10) with the best test accuracy of 0.9748 and test loss of 0.0944.\nAlzheimer: As reported in Table 2, the test accuracy of the student model exceeds even that of the teacher model (e.g., 0.9946 vs. 0.9938 using SA1 and TA, respectively). This rare situation may be due to the selection of good temperature parameters, which causes the student model to learn useful information faster. In addition, both TA and all student models can achieve a similar Avg. F1 score (~ 0.99), indicating that the student model can learn rich features from the teacher.\nEyes disease: As illustrated in Table 2, the best student model is S3 (a = 0.4, T = 15) with a test accuracy of 0.9351 and a test loss of 0.1956. Unlike the BT and Alzheimer datasets, the student model demonstrates lower accuracy compared to Te (e.g., approximately 5% decrease). This suggests that the use of shallow networks is limited. Overall, synthesizing performance across the three datasets reveals that KD has enabled the student models to approach the teacher model in terms of validation accuracy. The student model achieved an average F1 score of 0.99, 0.94, and 0.98 in the Alzheimer's disease, Eye disease, and Brain tumor datasets, respectively. Those results are consistent with the confusion matrices of the student and teacher models in all datasets as illustrated in Figure 2."}, {"title": "Interpretability analysis", "content": "An interpretability study was conducted to understand the decision-making mechanisms of the classifier model. We extracted an image from each class in the three datasets, selected the best performing student model for layer-by- layer interpretability analysis, and used the teacher model to perform Grad-CAM and SHAP analysis to verify the effectiveness of the proposed method, as shown in Figure 3. Based on the results in Figure 3, clinicians would focus more on the output of the last layer.\nGrad-CAM: For Brain tumor, in the heatmap for Glioma, Meningioma, and Pituitary tumor classes, red highlights areas predicted as tumors on MRI scans, while dark blue marks the background regions of the brain. For the \"No tumor\" class, there are no designated tumor regions, thus in these heatmaps, the brightest zones are not focused on any specific tumor location due to the absence of an actual tumor. Instead, these highlighted areas may signal different features or regions that the model uses to class the image as \"No tumor\". For the Eyes disease, except for the \"Normal\" class, the focus of the heat map is near the optic disc, which is one of the key parts for diagnosis and is related to many eye diseases. For the Alzheimer's disease, in the \"Non demented\" class, the size of the sulci and ventricles are within the normal range, there is no enlargement and there is no brain atrophy. In \"Very mild demented\", the MRI image shows mild ventricular enlargement, the hippocampus shows a shrinking trend, and the focus of the heat map is around the ventricles. In \"Mild demented\", the sulci may become more pronounced, and the ventricles relatively larger. For the \"Moderate demented\" class, MRI images showed significant brain atrophy that affected a wider range of brain regions.\nSHAP: For understanding, we also performed the SHAP interpretability plots specifically for classes that were predicted accurately. In Brain tumor image, there are many SHAP values with positive weights near the tumors. In Eyes disease images, SHAP positive weights are distributed around the optic disc, which is similar to the result of heatmap as well. For the normal class, the positive weights are evenly distributed throughout the retina. In Alzheimer images, for each class, each feature contributes differently to the prediction of the model, resulting in a wide distribution of SHAP values. In addition, Alzheimer's disease symptoms are associated with many parts of the brain, which can lead the model to identify a variety of influencing factors, resulting in a wide range of SHAP values (Bhattarai et al. 2024). In this case, in addition to being able to accurately determine the class of the image, no effective interpretability analysis results can be obtained.\nCNN features based analysis: Our student model has only five convolutional layers, which can be approximately regarded as the model inference process with high convolutional layers. For Brain tumor, the first few convolutional layers usually focus on the basic elements of the image, such as the basic structure and contours of the brain. As the network level deepens, the middle layers begin to extract more complex features, and from the third layer onward, it gradually changes to learning shapes, specific texture patterns, or components of local objects. Deeper in the network, like the fourth layer, the focus starts to be on regions of the image that are directly related to the predicted class. In the fifth layer, it covers most of the area, including the tumor, which is caused by the maximum pooling layer. Our initial purpose is to reduce the spatial dimension of the feature map while retaining the most important features. This dimensionality reduction operation will lead to the spatial resolution of the feature map. The rate is reduced, so in deep convolutional networks, even small local features may occupy a larger area on the visualized heat map (Wang et al. 2021). For Eyes disease, at the initial layers, the model primarily detects the contours of the optic disc and the eyes. As the network goes deeper, it increasingly focuses on specific regions of the image, indicating the identification of distinct structures or features within the eyes. In the more advanced layers, the focus of the heat map becomes sharper and more defined. These concentrated areas, or hot spots, typically align with the macula, optic disc, and surrounding blood vessels, which are important for diagnosing retinal conditions. In the \"normal\" class, the heat map covers nearly the entire image, which indicates an absence of significant anomalies. For Alzheimer, like with other datasets, the model initially concentrates on the contours of the brain. In the fourth layer, it becomes apparent that multiple areas are influencing the decision-making process of the model. In the fifth layer, only the segments with the highest impact of the features are retained in each region, aligning with the findings we derived from using Grad-CAM."}, {"title": "XAI evaluation", "content": "We used the Fidelity score to evaluate the consistency of the explanations generated by the explanation method with the behavior of the original model. A higher Fidelity Score means that the confidence levels of the adversarial and original samples are more similar, indicating that the adversarial sample does not significantly change the model predictions. The given value $C_{adv} (Y_{true})$ represents the confidence of the model (maximum value in the predicted probabilities) in the adversarial sample for the original class $Y_{true}$. $C_{orig} (Y_{true})$ represents the confidence of the model in the original sample for the original class $Y_{true}$. The Fidelity Score is calculated using the following formula:\n$Fidelity \\ score = \\frac{C_{adv} (Y_{true})}{C_{orig} (Y_{true})}$\nBecause adversarial generation has minimal impact on the normal class, we focus exclusively on the disease classes. Table 3 reports the Fidelity score for the XAI methods. SHAP provides a higher Fidelity score overall, indicating that its interpretative performance is relatively stable and reliable in different cases. Grad-CAM performs well in certain specific classes, but scores lower in others, such as Very Mild Demented. Grad-CAM has relative limitations in detecting minor lesions compared to SHAP, which evaluates the contributions of all features. Although the proposed method achieves feasible performance across classes, on the whole it mimics the impacts of the Grad-CAM and SHAP techniques."}, {"title": "Time-efficient model interpretability", "content": "An additional distinct advantage of the proposed method is that it could reduce the computational effort and the time required. Due to its small number of layers, a quick interpretability analysis can be done on a large number of images. We used two metrics to demonstrate the portability of our model. The first is Floating Point Operations Per Second (FLOPs), which is used to indicate the performance of a computer. It is related to floating-point operations on a computer that can be performed in one second. Another metric is the Mean Execution Time (MET), which indicates the average time it takes for an algorithm to run once. These measurements are reported in Table 4. Specifically, Table 4 shows that the FLOPs value depends solely on the model, with the student model having less than 50% of the teacher model FLOPs. This is important for deploying models in resource- limited environments, like mobile devices, as it requires less power and has faster inference times. Various interpretability methods applied show a significant decrease in MET. Grad- CAM provides immediate feedback, producing results in less than one second, requiring only one-fifth of the time of traditional methods while effectively highlighting important lesion regions. The results of the Grad-CAM and SHAP analysis for the student model are shown in Figure 4.\nThe MET disparity is more pronounced during SHAP analysis. In medical data sets with many features and interactions, SHAP value calculations can be lengthy. Typically, analyzing a single image with SHAP takes about one minute, but the proposed method reduces this to fifteen seconds, important for efficient large-scale image analysis. SHAP ability to easily identify classes aids in quickly diagnosing patients conditions, allowing faster clinical decisions based on interpretable data. Large hospital-managed data sets with hundreds of classes and millions of images require advanced CNN architectures for feature extraction and classification. However, the proposed method enables the student model to learn from the soft labels of the teacher model, capturing nuanced differences between classes. This helps the student model maintain high accuracy despite reduced complexity."}, {"title": "Conclusion", "content": "In this paper, we presented the intersection of KD and XA\u0399 within the CNN model for classification tasks. This approach to simplifying CNN architectures through KD effectively retains essential feature representations while reducing the complexity and size of the model. Experimental results demonstrated that this method maintains high classification accuracy. Furthermore, the use of average feature maps to visualize the focus of the features within each CNN layer allows clinicians to understand the decision-making process of the model. In addition, it reduces computation time and saves computing resources, which is beneficial in high- volume medical image processing. So far, this study presents a faster and more efficient way for healthcare providers to provide better care to patients."}]}