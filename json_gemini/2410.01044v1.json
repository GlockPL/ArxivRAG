{"title": "RATIONALYST: Pre-training Process-Supervision for Improving Reasoning", "authors": ["Dongwei Jiang", "Guoxuan Wang", "Yining Lu", "Andrew Wang", "Jingyu Zhang", "Chuyu Liu", "Benjamin Van Durme", "Daniel Khashabi"], "abstract": "The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.", "sections": [{"title": "1 Introduction", "content": "Rationales play a crucial role in human reasoning and its accuracy (Rips, 1994; Mercier and Sperber, 2011). In reasoning problems, having accurate rationales often correlates with accurate outcomes (Tversky et al., 1982; Davis, 1984). This importance of rationales extends to Large Language Models (LLMs) as well. Wei et al. (2022) were among the first to show that generating chain-of-thought rationales significantly improves LLMs' reasoning performance. Subsequent research has further refined the methods for eliciting rationales, leading to improved performance (Fu et al., 2023; Zhou et al., 2022).\nIn the context of LLM reasoning, these rationales are typically employed through a chain-of-thought process that makes reasoning steps explicit by articulating them as plain-text rationales. In this approach, each subsequent rationale is generated based on rationales produced in preceding steps, effectively using them as a form of supervision. However, the generated reasoning chains might be incomplete, containing potential logical leaps while leaving some rationales implicit (or hidden) during the generation process. These gaps in the reasoning chain can weaken the LLM's reasoning ability throughout the problem-solving process.\nOne reason why chain-of-thought methods might miss implicit steps is that models trained with \u201cnext-token prediction\" often replicate the omissions present in their training data. Implicit rationales-underlying logical connections that are often not explicitly stated-are frequently missing in daily communication and web text. In this example, we see a passage from Harry Potter: \"Harry used magic outside... He is punished to attend...\" The text contains the implicit (unstated) rationale: \u201cWhen someone breaks the rule, he will be punished!\" This implicit rationale is crucial in inferring the causal reasoning that connects the cause (Harry breaking rules) to its effect (punishment), but is also left unstated in the context. As a result, existing LLMs trained to mimic web text will have difficulty surfacing these implicit statements during the reasoning process, which can lead to flawed conclusions, such as erroneously justifying theft as a praiseworthy act when done to support one's family.\nThis paper presents RATIONALYST, a model tailored for process-supervision of reasoning. RATIONALYST is pre-trained on a vast collection of implicit rationales extracted from a mixture of web-scale unlabeled datasets and existing reasoning datasets. Although existing LLMs may miss crucial details in their reasoning, leading to flawed conclusions, integrating these LLMs with RATIONALYST provides an additional supervision mechanism to guide their reasoning processes, resulting in more robust conclusions. RATIONALYST is developed and used in three stages: (1) we employ LLMs to extract implicit rationales from unlabeled text corpora without human annotation. These rationales are subsequently filtered based on their helpfulness in predicting subsequent text (\u00a73.1); (2) we train RATIONALYST to predict those rationales given the preceding context (\u00a73.2); and then (3) as depicted in , during inference, we assume reasoning is done incrementally in a chain-of-thought fashion (Wei et al., 2022) by another agent model, and we use RATIONALYST to provide supervision for the agent model at each reasoning step throughout the reasoning process \u00a73.3. By adopting a data-centric approach, RATIONALYST utilizes abundant unlabelled data to provide process supervision (Lightman et al., 2023) across various reasoning tasks without the need for human annotation.\nOur method extracts 65k implicit rationales from the web-scale unlabelled dataset The Pile (Gao et al., 2020). To adapt the extracted rationales to our tested domain and stabilize training, we additionally extract a much smaller set of 14k implicit rationales from the question-answer pairs in the training sets of two reasoning datasets: GSM8K (Cobbe et al., 2021a) and ECQA (Aggarwal et al., 2021). Our extraction process controls for answer leakage to prevent artificial amplification of performance. Using this curated set of rationales, RATIONALYST is then fine-tuned from LLaMa-3-8B. To assess the effectiveness of our approach, we evaluate RATIONALYST on a diverse set of reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning."}, {"title": "2 Related Work", "content": "Supervising reasoning. Supervision-based approaches have been shown to enhance the reasoning abilities of LLMs. Cobbe et al. (2021b) and Snell et al. (2024) demonstrate that training a \u201cverifier\" to supervise reasoning can be more parameter-efficient than simply expanding the parameters of the \"reasoner\" responsible for solving the reasoning task. Ground-truth feedback from interaction with the environment is an effective form of supervision (Wang et al., 2023), but it works only in controlled environments like simulated world. General-purpose verifiers (Dhuliawala et al., 2023; Weir et al., 2024, 2023; Vacareanu et al., 2024) offer broader applicability utilizing principles like compositional reasoning. However, they don't fully capitalize on the vast amount of unlabelled data in the way a data-driven approach might. Process-based supervision (Lightman et al., 2023) offers supervision at each reasoning step rather than just at the final result. While promising, it requires substantial human annotation for the correctness of intermediate steps, making it resource-intensive. Our work aims to address these challenges by proposing a data-centric process-supervision method without the need for human annotation.\nKnowledge extraction from unlabelled data. LLMs are conventionally trained on extensive web data using autoregressive next-token prediction. While effective, this approach may not fully harness the potential of the pre-training data, as latent information within this data could be better accessed using techniques beyond simple next-token prediction. Recent research has demonstrated several approaches to utilize this latent information to develop more sophisticated language model capabilities. Schick et al. (2023) introduced Toolformer, which autonomously annotates and extracts appropriate positions, names, and inputs for tool use by leveraging supervision from future tokens. Similarly, Cornille et al. (2024) developed a method for learning to plan coherent article writing through self-supervised learning in text. More closely related to our work, Zelikman et al. (2024) proposed Quiet-Star, which applied a comparable technique to uncover underlying rationales in daily communication to enhance reasoning capabilities. Our work adopts a strategy similar to Quiet-Star for extracting rationales in an unsupervised manner. However, our approach diverges in its primary objective: we aim to train a \"supervisor\" that can utilize these rationales to provide process supervision for any \"reasoner.\" This focus enables us to implement a simpler and more reliable method, as we don't need to directly integrate rationale extraction with \"reasoner\" training. Our approach thus offers a novel perspective on leveraging latent information in language models to enhance their capabilities.\nRationales as the basis for reasoning. Various studies have focused on improving the use of rationales to elicit reasoning. Fu et al. (2023) refine rationales for more effective reasoning elicitation, while Li et al. (2023) explore different approaches to leveraging rationales to enhance reasoning. Other works, such as Hwang et al. (2024), examine the verification of rationales produced by LLMs during reasoning to improve performance. Additionally, training LLMs on rationale-rich data is a common strategy for enhancing reasoning skills. As highlighted by Lewkowycz et al. (2022) and Jiang et al. (2024a), LLMs trained on science and math data tend to perform better on reasoning tasks, particularly when CoT prompting is used. In this work, we build on this foundation by using rationales as the core of our method to supervise reasoning."}, {"title": "3 Building RATIONALYST", "content": "We discuss the construction of RATIONALYST and its usage at inference time. First, we describe extracting rationales from unlabeled text (\u00a73.1), then use them to train RATIONALYST (\u00a73.2), and finally, employ RATIONALYST to supervise reasoning during inference (\u00a73.3).\nSetup. As we will be using multiple LLMs throughout the process, we define them here: $M_{ra}$ is the trained rationale generation model (RATIONALYST) that generates rationales and heuristics during inference. $M_{Agent}$ is a general-purpose reasoning agent that produces candidate reasoning steps and incorporates rationales during inference. We use one additional model $M$ for initial rationale extraction, rationale filtration, and probability estimation of potential next reasoning steps during inference. These LLMs can be implemented using various state-of-the-art models, allowing for adaptability to specific research needs and computational resources.\n3.1 Large-scale Rationale Extraction\nImplicit rationales are often embedded in unlabelled text, reflecting natural thought processes in daily communication. Our extraction process, illustrated in Figure 3, aims to make these rationales explicit. Using a pre-trained and aligned language model M, we generate rationales from text and then use M to filter these rationales to retain only those that are useful, akin to the self-supervised \"tool\" learning approach described by Schick et al. (2023). The same M is subsequently used to train RATIONALYST.\nA Extracting rationales from pre-training data. We employ M to generate rationales from the Pile. Due to the size of this dataset, we implement a pre-filtering process to identify reasoning-rich documents by (1) computing the average semantic embedding of representative reasoning training sets using a paragraph embedding model, and (2) selecting documents from unlabelled datasets that exceed a cosine similarity threshold \u03b1 when compared to this average embedding. After pre-filtering, we segment the selected paragraphs into 2000-word segments and instruct M to generate rationales at the end of each sentence, using prompts with demonstrations. Detailed information on the prompts and in-context learning demonstrations used for rationale extraction can be found in Appendix A.\nB Extracting rationales from reasoning datasets. In parallel to A, we also extract rationales from existing reasoning datasets to adapt the extracted rationales to our tested domain and stabilize training. For a given reasoning dataset with pairs of questions and final answers $D = \\{(q_i, a_i)\\}_{i=1}^{n}$, we create a prompt P that instructs M to generate rationales for each reasoning step in the final answer $a_i$. The input of the prompt consists of the entire question and answer, and the output includes implicit rationales that can be inferred from the reasoning process in the answer. Consider the concrete example from existing datasets (bottom) in Figure 3. The solution involves two reasoning steps: \u201cNatalia sold 48 / 2 = 24 clips in May\u201d and \u201cNatalia sold 48 + 24 = 72 chips altogether.\" Here, the implicit rationale that connects the first and second steps, \"Now we should calculate the sum of chips in April and May,\" is implicit yet helpful for the prediction of the second step. These rationales are subsequently filtered and used to train RATIONALYST.\nFiltering extracted rationales. Generated rationales in A and B may not always be accurate or helpful. In reasoning tasks, our objective is for the extracted rationales to effectively aid in future reasoning, which means a good rationale should enhance the likelihood of accurately predicting the following text. Let i be the position of the rationale r in the sequence $x = x_1,...,x_n$. Given a sequence of weights $(w_k)_{k \\in N}$, the weighted cross-entropy loss for future token prediction is defined as:\n$L_i(r) = - \\sum_{j=i}^{n} w_{j-i}log p_M(x_j | r, x_{1:j-1})$\nwhere M, in a different role from its previous use, is employed to estimate the probability over tokens $x_i, ..., x_n$ prefixed by preceding tokens $x_{1:i-1}$ and rationale r. The weight assigned to each future token decreases exponentially by a factor of 0.9 for each step further away it is from the rationale. We compute $L_i = L_i(r_i) \u2013 L_i(\u03b5)$, where \u03b5 represents an empty rationale (i.e. predicting following tokens based only on preceding tokens). A rationale is considered helpful if it makes the prediction of future tokens easier, indicated by $L_i \u2265 \u03c4_f$, where $\u03c4_f$ is a filtering threshold. We retain rationales for which adding the rationale reduces the loss by at least $\u03c4_f$ compared to having no rationale.\nIt's crucial to clarify two key aspects of our rationale extraction process. First, while M extracts rationales from the training sets of reasoning datasets, these training sets are not directly used as targets when training $M_{ra}$ itself. Second, we explicitly instruct M to exclude answers from the extracted rationales. This precaution prevents answer leakage in our prompts, thereby ensuring the integrity of our reasoning process."}, {"title": "3.2 RATIONALYST Training", "content": "The goal of RATIONALYST (denoted by $M_{ra}$) training is to develop a model that can generate implicit rationales to guide stepwise problem-solving during inference time. For web-scale datasets like The Pile, the input context consists of a segment of text from a document. $M_{ra}$ learns to generate an implicit rationale that can guide the prediction of the next segment of text in the document's flow. In the case of structured reasoning datasets such as GSM8K or ECQA, the input context includes the question and any preceding reasoning steps toward the answer. Here, $M_{ra}$ learns to generate rationales that could guide the next step in the problem-solving sequence.\nGiven the appropriate context from either source, the implicit rationales, extracted and filtered as described in \u00a73.1, serve as the target outputs during training. The overall training objective is to minimize the per-token cross-entropy loss between the generated rationales and their ground truth values from the extracted and filtered rationales. By learning to generate appropriate rationales for both free-form text and structured problem-solving data, RATIONALYST develops the ability to provide meaningful guidance across a wide range of contexts during inference."}, {"title": "3.3 Inference with the Help of RATIONALYST", "content": "During inference, a general-purpose LLM (the \"agent model\" or $M_{Agent}$) is employed for reasoning across various problems. Algorithm 1 outlines the procedure.\n$M_{Agent}$ generates reasoning incrementally in a chain-of-thought fashion, producing multiple candidates for the next reasoning step. These steps and the question form a \u201creasoning trajectory\" T that aims to solve the problem, which also serves as input to $M_{ra}$. $M_{ra}$ then generates r, the implicit rationale (line 3) With the help of implicit rationale, we provide supervision for the next reasoning step. Two supervision methods we considered are:\nImplicit supervision. For this supervision, $M_{Agent}$ generates the next reasoning steps conditioned on the trajectory T (line 6). We then use M to estimate the probability of potential next reasoning steps given rationale r and reasoning trajectory T (line 13). This probability-based heuristic aligns with our rationale filtration process used during $M_{ra}$ training. Just as we identified rationales that improved the prediction of future text during filtration, here we use rationales to improve the selection of future reasoning steps. By leveraging the probability estimates as a heuristic, we can effectively discriminate between more and less likely next steps in the reasoning process, guiding the overall trajectory towards more accurate conclusions.\nExplicit supervision. Another approach is to directly incorporate the implicit rationale into the generation of the next reasoning steps. This method makes the previously implicit rationale an explicit part of the reasoning process. To do that, we ask $M_{Agent}$ to generate multiple candidate next steps by temporarily appending r to the trajectory T, and then producing potential continuations based on this augmented context (line 8). Then, we estimate the probability of candidate generations according to $M_{Agent}$ (line 15). This approach allows $M_{Agent}$ to make the final decision on the next reasoning step, as in normal beam search (Snell et al., 2024; Yao et al., 2023), while benefiting from the additional context provided by $M_{ra}$'s rationales."}, {"title": "4 Experimental Setup", "content": "4.1 Setup for Training RATIONALYST\nRationale extraction. As discussed in \u00a73.1 we perform pre-filtering on The Pile, an unlabelled web-scale dataset, to identify documents with extensive reasoning content before rationale extraction. This is achieved by computing the average semantic embedding from the training sets of the reasoning datasets we test, filtering documents that exceed the cosine similarity threshold \u03b1 of 0.3, and keeping only the documents with length under 2000 tokens to fit within LLaMa-3 models' context length. The model we used to calculate these embeddings is MPNet-base (Song et al., 2020).\nFollowing the recipe in \u00a73.1 B, we also extract rationales from existing reasoning datasets. GSM8K (Cobbe et al., 2021b) and ECQA (Aggarwal et al., 2021) were selected for their complementary coverage of mathematical and commonsense reasoning, respectively. This combination ensures RATIONALYST is trained on diverse reasoning patterns, enhancing its versatility across various tasks.\nRationale annotation and filtration. The model M used for rationale extraction and rationale filtering are both LLama-3-8B-Instruct (MetaAI, 2024). On GSM8K and ECQA, we manually annotated 100 pairs of {preceding_context, rationale, following_context} to determine an appropriate filtration threshold. The annotations include 50 positive and 50 negative rationale examples. Since it's straightforward to scale up the extraction of rationales from unlabelled data for filtration, we prioritize maximizing the precision of our filtered rationales, even if it means extracting fewer of them. We set the threshold $\u03c4_f$ to ensure that 95% of the filtered rationales are accurate. On The Pile, we do not perform rationale annotation due to its diverse composition of corpora with varying characteristics. So the filter threshold $\u03c4_f$ for the Pile is set to O for all of its subdomains.\nThe resulting data from extraction/filteration. The results of rationale extraction and filtration on GSM8K, ECQA, and The Pile are presented in Table 1. On GSM8K, our method generates an average of 2.34 rationales per document, while on ECQA, it generates 2.58 rationales per document. The filtration process removes 80.5% of the generated rationales on GSM8K and 42.4% on ECQA.\nFor The Pile, we report the number of rationales per document and the number after filtration for each subdomain. The Pile's documents, being longer than those in GSM8K and ECQA, yield a higher average number of rationales per document. Among the subdomains, StackExchange retains the highest percentage of rationales, likely due to its question-answering format aligning well with our reasoning tasks and containing more inherent reasoning. However, The Pile as a whole contains less reasoning content, making rationale extraction challenging. Setting the threshold to 0 accepts all rationales more helpful than not having them, but the yield remains low. A manual review shows that most filtered rationales describe the preceding context rather than guiding future reasoning.\nIn total, we extracted approximately 14k rationales from GSM8K and ECQA combined, and about 65k from The Pile after filtration.\nRATIONALYST training. RATIONALYST is fine-tuned with LLaMa-3-8B-Instruct as the base model. We use the default hyperparameters as specified in the LLaMa-3 technical report (MetaAI, 2024) for fine-tuning."}, {"title": "4.2 Setup for Evaluating RATIONALYST", "content": "Evaluation tasks and metrics. A summary of reasoning tasks we evaluate is provided in Table 2. We assess our method on the following datasets: GSM8K (Cobbe et al., 2021b) and MATH (Hendrycks et al., 2021) for mathematical reasoning, ECQA (Aggarwal et al., 2021) and HellaSwag (Zellers et al., 2019) for commonsense reasoning, ProofWriter (Tafjord et al., 2021) for logical reasoning, ARC (Clark et al., 2018) for scientific reasoning, and the recently proposed multi-task reasoning dataset MMLU-Pro (Wang et al., 2024) for holistic reasoning across multiple tasks. All tasks are evaluated using exact match as the metric. We apply the postprocessing setups from lm-evaluation-harness2 before exact match calculation where applicable.\nInference setting. The model $M_{Agent}$ used for our baseline inference is also LLaMa-3-8B-Instruct. As mentioned earlier, to incorporate RATIONALYST, we instruct $M_{Agent}$ to reason in a chain-of-thought manner. For procedural reasoning tasks like GSM8K, Math and MMLU_Pro, we provide in-context learning examples that break down the reasoning into individual steps leading to the final answer. For multiple-choice reasoning tasks like ECQA and ARC, we include examples that analyze and compare each answer choice. The content and number of in-context demonstrations align with lm-evaluation-harness or the original paper if available; otherwise, they are adjusted to fit the context window of LLaMa-3-8B-Instruct. Detailed prompts and in-context learning demonstrations are provided in Appendix B.\nFor all experiments, we employ a temperature of 0.7 during inference to facilitate sampling. This approach diverges from the conventional use of temperature 0, yielding improved performance on certain datasets (e.g., ProofWriter) while marginally reducing effectiveness on others (e.g., GSM8K). We set the sampling parameter top_k to 3, allowing $M_{Agent}$ to sample three reasoning steps simultaneously at each inference stage. For the baseline without RATIONALYST, the next reasoning step is chosen randomly from these 3 samples. When using RATIONALYST, the selection of the next step is guided by the rationales generated, as described in \u00a73.3.\nOther verifiers. To evaluate the effectiveness of RATIONALYST's process supervision, we compare it with other approaches. For process supervision with other models, we include LLaMa-3-8B-Instruct and GPT-4 in our comparison. These models are prompted to rerank partial reasoning trajectories as reasoning steps are generated. The prompts and in-context learning demonstrations used for these models on representative datasets are provided in Appendix D. For outcome supervision, we also compare with outcome-based verifiers derived from LLaMa-3-8B-Instruct. These verifiers are fine-tuned on the training sets of each reasoning dataset. Following the approach outlined by Cobbe et al. (2021b), they assess the correctness of the final prediction by directly evaluating the question and final solution. This comparison allows us to assess the performance of RATIONALYST against both process-based and outcome-based supervision methods."}, {"title": "5 Empirical Results", "content": "5.1 Main result: RATIONALYST Improves Performance on Various Tasks\nIn this section, we train RATIONALYST using a combination of rationales extracted from GSM8K and ECQA, as well as from The Pile, as outlined in Table 1. The baseline does not use any verifier. We use implicit supervision for this experiment. The main result is shown in Table 3.\nEvaluation of RATIONALYST shows that training with rationales from GSM8K, ECQA, and The Pile improves performance not only on GSM8K and ECQA, but also on other reasoning tasks (e.g. scientific reasoning, logical reasoning, etc) not directly used in rationale extraction. This supports the idea that rationales can be broadly applicable across different reasoning tasks. In addition, since we use the same model (LLaMa-3-8B-Instruct) for rationale extraction, filtering, RATIONALYST training, and inference, our results do not leverage external knowledge from stronger models like LLaMa-3-70B-Instruct or GPT-4. Future work might change M to stronger models, with the expectation that higher-quality rationales will lead to better performance.\n5.2 Ablation: Web-scale Rationales Enhance Performance Across Tasks\nTo assess the benefit of web-scale rationales, we train another model: RATIONALYST w/o Pile solely on rationales extracted from the training sets of GSM8K and ECQA. We re-ran the experiments on the same reasoning datasets using implicit supervision. The results are detailed in Table 4.\nWe find that training the model on web-scale data results in better performance compared to training only on the rationales extracted from GSM8K and ECQA. This improvement is consistent and particularly significant on MMLU-Pro. Web-scale data likely provides exposure to more diverse reasoning types and content, including specialized knowledge, complex real-world scenarios, and interdisciplinary connections not present in the more focused datasets.\n5.3 Ablation: Implicit Supervision Works Better than Explicit Supervision\nIn this section, we conduct ablation studies to test the effectiveness of different supervision methods. To isolate the impact of supervision methods and minimize confounding variables, we focus on GSM8K and ECQA as representative benchmarks for mathematical and commonsense reasoning, respectively. We train two versions of RATIONALYST: one on rationales extracted from the GSM8K training set (RATIONALYST - GSM8K) and another on rationales from the ECQA training set (RATIONALYST - ECQA). These models are used to supervise $M_{Agent}$ during inference on their respective tasks.\nAs shown in Table 5, implicit supervision outperforms explicit supervision. Our manual analysis revealed that implicit supervision's superior performance stems from its greater robustness to errors. When RATIONALYST generates an imperfect rationale, the probability-based heuristic used in implicit supervision can still provide useful guidance even if the rationale itself is not ideal. This approach is less likely to lead $M_{Agent}$ to produce incorrect next steps. In contrast, explicit supervision directly incorporates potentially flawed rationales into the reasoning process, which can cause $M_{Agent}$ to produce incorrect next steps. Essentially, implicit supervision acts as a softer guide, allowing for some imperfection in rationales, while explicit supervision more strictly adheres to potentially flawed rationales, making it more susceptible to errors.\nRATIONALYST shows superior process-supervision performance than much bigger models like GPT-4: We observe consistent superior performance of RATIONALYST compared to GPT-4's process supervision. We hypothesize that this advantage arises from RATIONALYST's specialized design for providing supervision, in contrast to GPT-4's general-purpose training.\nRATIONALYST surpasses outcome-based verifiers trained using matching data: Notably, our method surpasses the performance of fine-tuned outcome-based verifiers on both GSM8K and ECQA datasets, despite these verifiers being trained on matching data. We attribute this success to the richer feedback provided by process-based supervision compared to outcome-based approaches.\n5.4 RATIONALYST Outperforms Other Verifiers\nTable 6 presents an analysis of RATIONALYST against various verifiers. Our findings reveal several insights:\nRATIONALYST outperforms vanilla LLaMa-3-8B-Instruct using process supervision: RATIONALYST, even without leveraging The Pile dataset, outperforms process-based verifiers using vanilla LLaMa-3-8B-Instruct. A manual examination of reasoning trajectories suggests that LLaMa-3-8B-Instruct faces difficulties in reranking partial reasoning steps. This challenge likely stems from the model's struggle to differentiate among its own generated outputs, a phenomenon observed in recent studies (Jiang et al., 2024b; Huang et al., 2023).\n5.5 RATIONALYST Generates Accurate and Easy-to-understand Rationals\nWe annotate some samples from the test set of Math (Hendrycks et al., 2021) at inference time, which was not part of the rationale sampling datasets. Through manual observation, we find that our model can generate useful rationales that is helpful for understanding LLM's reasoning process on Math (an example is provided in Appendix C). Comparing the rationales generated by RATIONALYST with those generated by Quiet-Star (Zelikman et al., 2024) on the same problems, we find that our method produces more human-understandable rationales. We believe this happens because Quiet-Star optimizes rationales during training using the accuracy of the final prediction as a reward. This approach, while effective for improving task performance, does not explicitly prioritize human interpretability. In addition, this appraoch might inadvertently develop shortcuts or non-intuitive patterns that optimize for accuracy but not necessarily for clarity or human understanding."}, {"title": "6 Discussion", "content": "Scaling up RATIONALYST. Scaling RATIONALYST with stronger models and increased computational resources is a logical next step. Utilizing stronger models, such as LLaMa-3-70B or GPT-4, would enhance the quality of extracted rationales, improve filtration accuracy, and ultimately strengthen RATIONALYST. However, due to computational constraints, we have not pursued this, which remains a limitation of this paper. Additionally, using larger unlabelled datasets with more extensive reasoning content, such as OpenWebMath (Paster et al., 2023), is currently infeasible due to the significant computational and time requirements for pre-filtering and training. These enhancements are planned for future work.\nConnection to research on scaling test-time compute. Recent research has focused on extending computational resources at test-time (Snell et al., 2024; Wu et al., 2024), particularly for complex reasoning tasks. In our experiments, we focus on developing heuristics and employ a straightforward approach of sampling multiple candidates and reranking them based on RATIONALYST's guidance. However, RATIONALYST's framework is compatible with more sophisticated test-time compute techniques. Its heuristics can be integrated into existing algorithms like beam-search or look-ahead search, potentially enhancing their performance without significantly increasing computational cost.\nIs training on extracted rationales necessary? In our approach, we first select a subset of unlabelled data that contains strong reasoning signals, then extract implicit rationales from this data for model fine-tuning. While it has been demonstrated that training on data with robust reasoning signals can enhance reasoning capabilities on its own (Gunasekar et al., 2023; Jiang et al., 2024a), we believe our method offers additional performance benefits for two reasons. First, many language models have already been trained on datasets like The Pile. The value of fine-tuning on previously encountered text is likely lower than the value of fine-tuning on newly incorporated rationales. Second, implicit rationales encapsulate the reasoning process. Pre-training on these rationales enhances reasoning more effectively than focusing on the whole document."}, {"title": "7 Limitations", "content": "One limitation of this work is the comprehensiveness of our experiments. In future research, we plan to extend our experiments to a broader range of reasoning tasks and compare RATIONALYST with other outcome-based and process-based verifiers. We also plan to adjust the combination of rationales used to train RATIONALYST by (1) sampling from different reasoning tasks and (2) altering the mix of rationales in unlabelled web-scale pre-training data to better understand its generalizability."}, {"title": "8 Conclusion", "content": "In this paper, we introduced RATIONALYST, a novel self-supervised model designed to enhance the reasoning capabilities of LLMs by leveraging hidden rationales extracted from unlabeled text. Our approach centers on the effective extraction and utilization of implicit rationales-those underlying thought processes that are not explicitly stated in the text but can be inferred. By capturing these rationales, RATIONALYST provides a mechanism for process supervision during reasoning, enabling LLMs to reason better."}]}