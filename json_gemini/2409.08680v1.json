{"title": "NEST-RQ: Next Token Prediction for Speech Self-Supervised Pre-Training", "authors": ["Minglun Han", "Ye Bai", "Chen Shen", "Youjia Huang", "Mingkun Huang", "Zehua Lin", "Linhao Dong", "Lu Lu", "Yuxuan Wang"], "abstract": "Speech self-supervised pre-training can effectively improve the performance of downstream tasks. However, previous self-supervised learning (SSL) methods for speech, such as HuBERT and BEST-RQ, focus on utilizing non-causal encoders with bidirectional context, and lack sufficient support for downstream streaming models. To address this issue, we introduce the next token prediction based speech pre-training method with random-projection quantizer (NEST-RQ). NEST-RQ employs causal encoders with only left context and uses next token prediction (NTP) as the training task. On the large-scale dataset, compared to BEST-RQ, the proposed NEST-RQ achieves comparable performance on non-streaming automatic speech recognition (ASR) and better performance on streaming ASR. We also conduct analytical experiments in terms of the future context size of streaming ASR, the codebook quality of SSL and the model size of the encoder. In summary, the paper demonstrates the feasibility of the NTP in speech SSL and provides empirical evidence and insights for speech SSL research.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, there has been significant progress in the field of speech self-supervised learning (SSL) technology, which attracts widespread attention from both the academic and industrial communities [1]\u2013[7]. By mining the information from a large amount of unlabeled speech data, speech SSL can provide powerful representations or representation models for downstream speech tasks [8]. SSL is pushing the performance of downstream tasks to new heights. For instance, speech SSL has facilitated many ASR models to achieve state-of-the-art (SOTA) performance on various benchmarks [9], [10]. These studies demonstrate the great potential of speech SSL in speech applications.\nHowever, most popular SSL methods, such as Wav2vec 2.0 [3], HuBERT [5], Data2vec [6], focus on exploring non-causal encoders that attend to bidirectional context, and ignore the downstream streaming tasks. Applying the non-causal encoder to streaming models not only requires modifying the encoder [11], [12] and the training strategy [13]-[16], but may also result in sub-optimal performance. Therefore, it is not easy to directly adapt the popular SSL methods to downstream streaming tasks. Furthermore, there are a limited number of studies on self-supervised learning (SSL) specifically for streaming models in downstream tasks [12], [17]. Although BEST-RQ [17] explores speech self-supervised pre-training for streaming ASR models, some of its conclusions are still unclear and needed to be further investigated due to the adoption of different settings for streaming and non-streaming ASR tasks. Among SSL methods that can adapt to streaming tasks, CPC [1] and APC [2] use the causal encoder as the backbone, and adopt contrastive predictive coding and auto-regressive predictive coding as pre-training objectives, respectively. However, these methods have not been evaluated on current popular end-to-end ASR models, and there is a gap in both structure and performance compared to the current popular SSL methods.\nIn this paper, we propose the NExt token prediction based Speech pre-Training with Random-projection Quantizer (NEST-RQ), a novel speech SSL method that uses next token prediction (NTP) as the pre-training objective. Currently, the large language model (LLM) [18]\u2013[20] that relies on NTP-based SSL has achieved great success, and has been widely explored and applied in many fields [10], [21]\u2013[24]. However, due to the continuous nature of speech, it is not easy to apply NTP to speech SSL. With the help of the random-projection quantizer (RQ) [17], we can convert the speech into the token sequence. Thus, we can introduce the NTP task in speech SSL and further optimize it based on the characteristics of speech. This paper verifies the feasibility of NTP in speech SSL, and brings more inspiration for joint auto-regressive modeling of audio and text. Our contributions are listed as follows:\n1) We propose a novel speech SSL method named NEST-RQ. First, the encoder in NEST-RQ is set to the causal structure. Then, RQ converts the continuous speech features into a sequence of discrete tokens. Finally, the causal encoder takes continuous speech features as inputs and uses the output of the encoder at each frame to predict the tokens of multiple subsequent frames;\n2) We show the effectiveness of NEST-RQ on a large-scale dataset that covers 300,000 hours of unlabeled speech and 30,000 hours of labeled speech. NEST-RQ achieves comparable performance to BEST-RQ on non-streaming ASR task and outperforms BEST-RQ on streaming ASR task. NEST-RQ inherits the simplicity of BEST-RQ while maintaining performance;\n3) We conduct analytical experiments to explore the performance of NEST-RQ in terms of the codebook quality in SSL, encoder size in SSL, and future context size in streaming ASR. Experiments show that NEST-RQ brings consistent improvements on streaming ASR across different settings."}, {"title": "II. METHODOLOGY", "content": "1) SSL Method: BEST-RQ [17] is an effective approach for speech SSL. Specifically, the method masks some segments of speech features and input the speech features into the speech encoder. The encoder learns to predict the masked segments based on the unmasked speech features, and the learning targets are generated by a random-projection quantizer (RQ). The random-projection quantizer projects the speech feature with a randomly-initialized matrix, finds the nearest vector in a randomly-initialized codebook, and uses the index of that vector as the target token. During training, both the projection matrix and the codebook is fixed. Benefiting from the simple quantizer design and the widely-recognized masked prediction task, BEST-RQ shows unique advantages among SSL methods. However, BEST-RQ relies on bidirectional context to predict the tokens of the masked segments, which makes it not easy to adapt to downstream streaming models.\n2) Downstream ASR Model: Factorized neural transducer (FNT) [25], [26] is used in downstream ASR task in this work. By controlling the causality of the encoder, FNT can perform streaming and non-streaming ASR. As shown in Fig.1, FNT consists of a speech encoder, the joint network, and the vocabulary predictor and the blank predictor. FNT predicts the blank token and vocabulary tokens separately, so that the vocabulary predictor can function as a language model (LM). Thus, inspired by studies [10], [22], [27], [28] that apply LLMs to end-to-end ASR models, we directly use a pre-trained LLM to initialize the vocabulary predictor and its fully-connected layer (FC) for projection, and freeze both of them during training. The output sequence of the encoder is downsampled by a factor of 2 using a convolutional layer, and then used to generate logits for connectionist temporal classification (CTC) [29]. Since the LLM keeps frozen, the loss $L_{LLM}$ for LM in original FNT is removed. Thus, the total loss for FNT is the sum of loss $L_{Transducer}$ for transducer and loss $L_{CTC}$ for CTC, and can be written as:\n$L_{FNT} = L_{Transducer} +  \\lambda_{CTC} L_{CTC}$"}, {"title": "B. NEST-RQ", "content": "We propose a novel speech SSL method called NEST-RQ.\nNEST-RQ uses the causal encoder that can only attend to the current frame and the past frames of speech features, and applies NTP as the pre-training task. Both causal encoders and NTP task make NEST-RQ friendlier to downstream streaming models. The training task of NEST-RQ is illustrated in Fig.2. After the feature extraction, raw speech can be transformed into speech features $X = (x_1, x_2, ..., x_t, ..., x_T)$. The convolution layers provides 4 times temporal-dimension reduction for $X$. After down-sampling, the feature sequence can be denoted as $H = (h_1, ..., h_l, ..., h_L)$. Meanwhile, we use the same RQ in BEST-RQ to generate token sequence $K = (k_1,k_2, .., k_l, ..., k_L)$ from $X$. Since the down-sampling convolution layers provides 4 times temporal-dimension reduction, RQ combines every 4 frames for projection. After processing input feature sequence $H$, the conformer module [30] outputs state sequence $O = (o_1, o_2..., o_l, ..., o_L)$. Then, we can use $l$-th state $o_l$ to predict multiple subsequent tokens. Specifically, assuming the current position is $l$, the output state $o_l$ is used to predict the tokens ${k_{l+1},..., k_{l+n}, ..., k_{l+N}}$ of the next $N$ tokens with N different prediction heads. The loss at the $l$-th position can be written as $\\sum_{n=1}^{N} log p (k_{l+n}|h_{\\leq l})$. Finally, the NEST-RQ loss for one sample can be represented as:\n$L_{NEST-RQ} = - \\sum_{l=1}^{L} \\sum_{n=1}^{N} log p (k_{l+n}|h_{\\leq l})$"}, {"title": "C. Encoder Adaptation from SSL to ASR", "content": "Non-causal encoders are usually used as the backbone in past SSL studies [3], [5], [17]. \u201cNon-causal\u201d means that the encoder uses both past and future context, while \u201ccausal\u201d means that the encoder can only use past context. For ASR task, non-causal encoders are suitable for non-streaming models, while causal encoders are suitable for streaming models. In"}, {"title": "III. EXPERIMENTAL SETTINGS", "content": "The SSL of the encoder uses 300,000 hours of in-house unlabeled speech data. The supervised fine-tuning (SFT) of the ASR model uses 30,000 hours of in-house ASR data, and the ASR test sets covers five subsets. All data cover multiple challenging scenarios, including video, live, etc. All input speech features are 80-dimensional log-mel filterbank coefficients, and each frame has the stride of 10ms. The metric used for ASR evaluation is character error rate (CER)."}, {"title": "B. Model", "content": "We use encoders with different sizes: 0.1B, 0.3B, 0.6B. Both 0.1B and 0.6B encoders have almost the same structure with that in [31]. However, the non-causal convolution kernel size of 0.1B is set to 31 (2 \u00d7 m + 1, m = 15) in this work. 0.3B encoder has the same settings with 0.6B encoder but has half number of conformer blocks. Among three encoders, 0.3B is the default choice. For non-streaming ASR, the attention module is NC-A, and the convolution is NC-C. For streaming"}, {"title": "III. Analytical Results in Different Settings", "content": "1) Analysis with Different Codebooks: We conduct analytical experiments with different codebooks. The random-projection quantizer is still represented as RQ. In addition, we introduce two extra quantizers Q1 and Q2 with high-quality codebooks. Specifically, the codebook in Q1 is generated by clustering the outputs of a non-causal encoder, which is trained with BEST-RQ on 3.5 million hours of unlabeled speech, into 1024 centroids. The codebook in Q2 is generated by clustering the outputs of a non-causal encoder, which is trained with BEST-RQ on 7.5 million hours of unlabeled speech, into 32768 centroids. As shown in Table III, with the help of more powerful codebooks, both BEST and NEST can achieve further improvements, verifying the scalability of the NEST method with different codebooks. However, we observe that NEST is worse than BEST on non-streaming ASR task. We conjecture that this is because the high-quality codebooks are derived from the non-causal encoder trained with BEST-RQ, which is more compatible with BEST.\n2) Analysis with Different Encoder Sizes: The experiments are extended to different encoder sizes. We explore 3 different model sizes mentioned in section III. From Table IV, we observe that: 1) The performance of BEST-RQ and NEST-RQ on both streaming and non-streaming models gradually improves as the encoder size increases; 2) Compared to BEST-RQ, NEST-RQ consistently brings relative error rate reduction of 5% to 7% on streaming ASR and maintains comparable non-streaming performance across different encoder sizes.\n3) Analysis with Different Future Context Sizes: In streaming ASR, the attention modules of the bottom 3 conformer blocks attend to the past frames, the current frame and the next frame by default. The more blocks that can attend to the next frame, the larger the receptive field for future context. Therefore, by controlling the number of conformer blocks $M$ that attend to the next frame, we can control the future context size of the encoder. Table V shows the streaming ASR performance when $M$ takes values from {0,1,3,5,7}. In all settings, NEST-RQ shows better performance. As the future context size increases, the model tends to be more non-streaming, resulting in increased recognition latency and decreased performance gain."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel speech SSL method named NEST-RQ. NEST-RQ employs a causal encoder and predicts the tokens of the subsequent positions in the sequence as the SSL task. The causal encoder makes NEST-RQ more friendly to downstream streaming tasks. Experiments on the large-scale dataset show that NEST-RQ achieves comparable performance to BEST-RQ on non-streaming ASR task and outperforms BEST-RQ on streaming ASR task. In the future, we will explore the performance of the NEST-RQ on more downstream tasks and transfer it into the joint auto-regressive modeling of speech and text."}]}