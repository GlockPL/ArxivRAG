{"title": "SCIENTIFIC MACHINE LEARNING IN ECOLOGICAL SYSTEMS: A STUDY ON THE PREDATOR-PREY DYNAMICS", "authors": ["Ranabir Devgupta", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "abstract": "In this study, we apply two pillars of Scientific Machine Learning: Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs) to the Lotka-Volterra Predator-Prey Model, a fundamental ecological model describing the dynamic interactions between predator and prey populations. The Lotka-Volterra model is critical for understanding ecological dynamics, population control, and species interactions, as it is represented by a system of differential equations. In this work, we aim to uncover the underlying differential equations without prior knowledge of the system, relying solely on training data and neural networks. Using robust modeling in the Julia programming language, we demonstrate that both Neural ODEs and UDEs can be effectively utilized for prediction and forecasting of the Lotka-Volterra system. More importantly, we introduce the \"forecasting breakdown point\" \u2013 the time at which forecasting fails for both Neural ODEs and UDEs. We observe how UDEs outperform Neural ODEs by effectively recovering the underlying dynamics and achieving accurate forecasting with significantly less training data. Additionally, we introduce Gaussian noise of varying magnitudes (from mild to high) to simulate real-world data perturbations and show that UDEs exhibit superior robustness, effectively recovering the underlying dynamics even in the presence of noisy data, while Neural ODEs struggle with high levels of noise. Through extensive hyperparameter optimization, we offer insights into neural network architectures, activation functions, and optimizers that yield the best results. This study opens the door to applying Scientific Machine Learning frameworks for forecasting tasks across a wide range of ecological and scientific domains.", "sections": [{"title": "1 Introduction", "content": "Scientific Machine Learning (SciML) is an emerging interdisciplinary field that integrates the strengths of traditional scientific models with the flexibility of machine learning techniques, and it has seen successful applications across various domains such as epidemiology, gene regulation, quantum mechanics, and fluid dynamics [1, 2, 3]. The core of SciML lies in combining the structure and interpretability of physical models, such as ordinary and partial differential equations (ODEs/PDEs) [4, 5], with the representational power of neural networks.\nTwo primary approaches define the landscape of Scientific Machine Learning:"}, {"title": "\u2022 Neural Ordinary Differential Equations (Neural ODEs):", "content": "This method replaces the traditional ODE/PDE system entirely with neural networks. By backpropagating through the system, we can optimize the net-work's parameters and approximate the dynamics of the system. The neural network is used to model the continuous-time dynamics of a system. Neural ODEs treat the hidden layers of a neural network as continuous transformations, modeled by a differential equation, rather than discrete layers as in conventional neural networks. This formulation allows the use of ODE solvers to propagate the system state forward in time. [1, 3].\nThe general form of a Neural ODE is given as:\n$\\frac{du(t)}{dt} = f_\\theta(u(t), t)$ (1)\nwhere u(t) represents the system state at time t, and $f_\\theta$ is a neural network parameterized by $\\theta$. In this formulation, the neural network learns the time derivatives (i.e., the dynamics) of the state directly from the data, without requiring a predefined model for the system's evolution.\nThe continuous transformation of the state variable is modeled as:\n$u(t_1) = u(t_0) + \\int_{t_0}^{t_1} f_\\theta(u(t), t) dt$ (2)\nwhere u(to) is the initial state and u(t\u2081) is the state at time t\u2081. The system state can be propagated forward by solving this integral using numerical ODE solvers such as the Runge-Kutta methods (e.g., Tsit5) or adaptive solvers. The objective is to minimize the difference between the predicted and observed state trajectories by optimizing the parameters $\\theta$ of the neural network.\nThe training process of Neural ODEs typically involves backpropagating through the ODE solver. However, directly backpropagating through the entire integration process can be computationally expensive. To address this, the adjoint sensitivity method is commonly employed. The adjoint method computes gradients with respect to the neural network's parameters $\\theta$ by solving an additional ODE backwards in time, thus allowing efficient memory management during training. This method is summarized as follows:\n$\\frac{dL}{d\\theta} = \\int_{t_0}^{t_1} a(t) \\frac{\\partial f_\\theta(u(t), t)}{\\partial \\theta} dt$ (3)\nwhere a(t) is the adjoint variable, which captures how the loss L changes with respect to the state variables."}, {"title": "\u2022 Universal Differential Equations (UDEs):", "content": "Instead of substituting the entire equation, UDEs replace specific terms of ODEs/PDEs with neural networks, allowing the discovery of unknown components or missing dynamics in existing models [3]. This approach is particularly useful when some physical laws are known but others remain uncertain. While Neural ODEs replace the entire system with a neural network, UDEs retain certain known components of the differential equation and augment the unknown or uncertain parts with neural networks. This method allows the system to capture known physics while also learning unknown dynamics from the data.\nThe general form of a UDE is:\n$\\frac{du(t)}{dt} = f_{known}(u(t), t) + f_{NN}(u(t), t; \\theta)$ (4)\nwhere $f_{known}$ represents the known part of the differential equation derived from physical laws, and $f_{NN}$ is a neural network with parameters $\\theta$ that models the unknown or unmodeled dynamics. The neural network's role is to fill in the gaps where the traditional model is insufficient or where the underlying mechanisms are unknown.\nIn practice, the UDE takes the form:\n$\\frac{du(t)}{dt} = f_{known}(u(t), t) + \\sum_{i=1}^{m} NN_i(u(t), t; \\theta_i)$ (5)\nwhere the neural networks $NN_i$ are used to model specific unknown terms or interactions in the system. Each neural network takes the system state u(t) and possibly time t as inputs, and outputs the corresponding missing term in the equation. The sum allows for multiple neural networks to model different parts of the dynamics."}, {"title": "2 Methodology", "content": "In this work, we aim to model the Lotka-Volterra Predator-Prey system using two advanced approaches: Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs). These methods leverage the flexibility of neural networks [10, 11] to model or learn specific interaction terms within the system.\nThe Lotka-Volterra equations, which describe the interaction between prey and predator populations, are given by the system of first-order ordinary differential equations (ODEs):\n$\\frac{dx}{dt} = \\alpha x - \\beta xy$ (6)\n$\\frac{dy}{dt} = -\\delta y + \\gamma xy$ (7)\nwhere x(t) represents the prey population, y(t) represents the predator population, and \u03b1, \u03b2, \u03b3, and 8 are constant parameters that govern the interaction dynamics."}, {"title": "2.1 Data Generation", "content": "To generate synthetic data for training and testing, we set the parameters of the Lotka-Volterra system to a = 1.5, \u03b2 = 1.0, \u03b3 = 0.5, and d = 2.0, with initial conditions x(0) = 1.0 and y(0) = 1.0. We numerically solve the system over the time span t \u2208 [0, 10] using the Tsitouras 5/4 Runge-Kutta method (Tsit5) with 101 equally spaced time points. [Refer Fig. 1(a)]\nThe solution provides time-series data for both prey and predator populations. For UDE, Gaussian noise with a standard deviation of \u03c3 = 0.05, 0.1, 0.3 was added to simulate real-world data perturbations. [Refer Fig. 1(b)]"}, {"title": "2.2 Neural Ordinary Differential Equation (Neural ODE)", "content": "In the Neural ODE approach, we replace the entire right-hand side of the Lotka-Volterra system with a neural network that learns the underlying dynamics directly from data. The neural network architecture consists of fully connected layers using radial basis functions (RBFs) as activation functions. The structure is as follows:\n\u2022 The input layer takes the 2-dimensional vector [x, y].\n\u2022 Three hidden layers, each with 100 neurons, and the RBF activation function.\n\u2022 The output layer produces a 2-dimensional vector [$\\frac{dx}{dt}, \\frac{dy}{dt}$].\nThe loss function for the Neural ODE is RMSE defined as the sum of squared differences between the predicted and actual prey and predator populations:\n$L(\\theta) = \\sum_{i=1}^{n} ((\\hat{x_i}(\\theta) - x_i^{true})^2 + (\\hat{y_i}(\\theta) - y_i^{true})^2)$ (8)\nwhere $x_i^{true}$ and $y_i^{true}$ are the true prey and predator data, and $\\hat{x_i}(\\theta)$ and $\\hat{y_i}(\\theta)$ are the predicted values from the neural network."}, {"title": "2.3 Universal Differential Equation (UDE)", "content": "In the UDE approach, we retain certain known components of the Lotka-Volterra system (such as prey growth and predator death rates) and replace the interaction terms \u03b2xy and \u03b3xy with neural networks. The UDE system is formulated as:\n$\\frac{dx}{dt} = \\alpha x - NN_1(x, y; \\theta_1)$ (9)\n$\\frac{dy}{dt} = -\\delta y + NN_2(x, y; \\theta_2)$ (10)\nwhere NN\u2081 and NN2 are neural networks that model the unknown interaction terms. Each network takes [x, y] as input and outputs a scalar. The training procedure for UDE is similar to that of the Neural ODE, where the loss function is minimized by comparing predicted and actual prey and predator populations.\nFor UDE the neural network parameters \u03b8\u2081 and \u03b8\u2082 are initialized randomly, and the network is trained by minimizing the difference between synthetic and predicted data. By introducing the Gaussian noise the loss is calculated by of difference noisy synthetic data and the predicted populations from the UDE model. The loss function is defined as the sum of squared errors between the noisy prey and predator data and the UDE-predicted values:\n$L(\\theta) = \\sum_{i=1}^{n} ((\\hat{x_i}(\\theta) - x_i^{noisy})^2 + (\\hat{y_i}(\\theta) - y_i^{noisy})^2)$ (11)\nwhere $x_i^{noisy}$ and $y_i^{noisy}$ are the noisy prey and predator data, and $\\hat{x_i}(\\theta)$ and $\\hat{y_i}(\\theta)$ are the predicted values from the UDE model."}, {"title": "2.4 Model Training and Optimization", "content": ""}, {"title": "2.4.1 Neural Network Architecture and Performance", "content": "The architecture of the neural networks plays a critical role in how well both the Neural ODE and UDE models perform. While both models use neural networks to learn the underlying dynamics of the Lotka-Volterra system, their structural design and depth directly influence the model's ability to generalize and make accurate predictions."}, {"title": "2.4.2 Neural ODE Architecture", "content": "The neural network architecture in the Neural ODE model is relatively deep, comprising three hidden layers with 100 neurons each. Figure 5 shows the impact of different number of hidden units vs Loss. We will discuss this more on section 2.5 in hyperparameters"}, {"title": "2.4.3 UDE Architecture and the Advantage of a Shallow Network", "content": "In contrast, the UDE model leverages a much shallower neural network architecture. Specifically, two neural networks with three hidden layers and only 10 neurons per layer are used. The activation function chosen is the Rectified Linear Unit (ReLU), which is computationally efficient and helps in avoiding vanishing gradient issues commonly seen with deeper networks [12, 13, 14].\nThe shallow architecture of the UDE has several advantages:\n\u2022 Faster Training: The shallow architecture results in fewer parameters to optimize, which leads to faster convergence during training. This reduced complexity allows the UDE model to reach accurate predictions with fewer training iterations compared to the deeper Neural ODE model.\n\u2022 Better Generalization: While deep networks have high capacity, they are more prone to overfitting, especially when trained on noisy or limited data. The shallow network used in the UDE model strikes a balance between model capacity and generalization, making it less likely to overfit to the training data.\nThe key advantage of using a shallow architecture in UDE comes from the fact that the model is not tasked with learning the entire dynamics from scratch. Instead, it only needs to learn the residual terms (e.g., interaction terms like \u03b2xy and \u03b3xy in the Lotka-Volterra system). This partial learning task allows the model to remain simple, robust, and computationally efficient. In contrast, the Neural ODE approach requires a more complex architecture as it must approximate the entire system behavior, leading to the need for a deeper network."}, {"title": "2.4.4 Model Optimization", "content": "Both the Neural ODE and Universal Differential Equation (UDE) models were trained using a two-phase optimization strategy. This combined approach leverages the strengths of different optimizers to first rapidly converge on an approximate solution, followed by a more refined optimization to reach the optimal parameters."}, {"title": "2.4.5 Neural ODE Optimization", "content": "For the Neural ODE model, the optimization was performed in two phases [Refer Fig 2(a)]:\n\u2022 Adam Optimization: The Adam optimizer [15, 16, 17, 18] was employed for 400 iterations with a learning rate of 0.001. Adam was chosen for its adaptability and ability to handle sparse gradients.\n\u2022 BFGS Optimization: After the initial phase with Adam, the parameters were further refined using the BFGS optimization [19, 20, 21] algorithm for 100 iterations. BFGS, being a quasi-Newton method, allows for fine-tuning the parameters to a local minimum with greater precision."}, {"title": "2.4.6 UDE Optimization", "content": "Similarly, the UDE model training also used a two-phase optimization process [Refer Fig 2(b)]:\n\u2022 Adam Optimization: The Adam optimizer was applied for 20,000 iterations with a learning rate of 0.001. Given its effectiveness in adjusting the learning rate dynamically, Adam was well-suited for the UDE model, especially in handling noisy data.\n\u2022 RMSProp Optimization: After the Adam phase, RMSProp [22] was used to further optimize the model for an additional 20,000 iterations. With a learning rate of 0.001, momentum p = 0.9, and e = 1e-8, RMSProp helped fine-tune the parameters, particularly in non-stationary environments like the UDE model."}, {"title": "2.5 Hyperparameters", "content": "Hyperparameter tuning is a critical aspect of the model training process. For both Neural ODE and UDE models, specific hyperparameters have a significant impact on model performance, stability, and convergence Refer Figure 3. In this section, we will explore the key hyperparameters used for each model and how they influence the training process."}, {"title": "2.5.1 Neural ODE Hyperparameters", "content": "The Neural ODE model uses a deeper architecture and thus requires careful tuning of several key hyperparameters to ensure stable and efficient training. The main hyperparameters considered for the Neural ODE are:\n\u2022 Learning Rate: A learning rate of 0.01 was used with the Adam optimizer. The learning rate controls the step size at each iteration while moving towards a minimum in the loss function. For Neural ODEs, a learning rate that is too high can lead to oscillations or divergence, while a very small learning rate can result in slow convergence. The value of 0.01 was chosen as a compromise between these extremes, providing stable convergence without overshooting.\n\u2022 Number of Layers and Neurons: The neural network used in the Neural ODE has 3 hidden layers, each with 100 neurons [Refer Figure 4]. This deep architecture is essential to capture the complex dynamics of the Lotka-Volterra system, but it also introduces a large number of trainable parameters. The depth and width of the network significantly impact the expressiveness of the model. However, deeper networks often require more computational resources and are prone to overfitting, which is why regularization techniques and careful monitoring of the validation loss are critical.\n\u2022 Activation Function: The Radial Basis Function (RBF) was chosen as the activation function for the hidden layers. RBFs are particularly effective in modeling complex, non-linear interactions as they provide a smooth, localized response. However, their complexity adds a computational burden and requires more careful initialization of weights and biases to avoid numerical instability during training.\n\u2022 Optimizers: A two-phase optimization strategy was used:\nAdam Optimizer: Adam, with a learning rate of 0.001, was employed for the first 500 iterations. Adam's adaptability to the varying learning rates during training makes it an excellent choice for initial optimization, especially for complex models with noisy gradients. Adam's momentum terms allow it to navigate complex loss surfaces and escape local minima more efficiently.\nBFGS Optimizer: After the Adam optimization phase, BFGS (Broyden-Fletcher-Goldfarb-Shanno) was used for 100 iterations to refine the parameters. BFGS is a quasi-Newton method and excels at fine-tuning the solution found by Adam by efficiently finding the local minima in smooth and continuous optimization landscapes. Its reliance on second-order approximations of the Hessian matrix allows for more precise parameter adjustments, making it suitable for final parameter refinement.\n\u2022 Epochs/Iterations: In the initial phase, 500 iterations were conducted with the Adam optimizer. This was followed by 100 iterations using BFGS. The number of iterations was chosen to ensure a good balance between convergence speed and model performance.\n\u2022 Weight Initialization: Proper weight initialization is crucial, especially for deep networks. Random initializa-tion using default settings in the Lux package was employed, ensuring that the initial values were not too large (which can lead to exploding gradients) or too small (which can result in vanishing gradients)."}, {"title": "2.5.2 UDE Hyperparameters", "content": "For the UDE model, the neural network architecture is shallower, and the focus is on learning specific interaction terms, which allows for a more streamlined set of hyperparameters. The key hyperparameters for the UDE model include:\n\u2022 Learning Rate: Similar to the Neural ODE, a learning rate of 0.001 was used with the Adam optimizer. However, given the shallow architecture of the UDE, the model is less sensitive to the learning rate compared to the deeper Neural ODE model. The learning rate is crucial in the early stages of training, ensuring that the UDE model can rapidly converge to a reasonable approximation of the predator-prey interactions.\n\u2022 Number of Layers and Neurons: The UDE model employs two separate neural networks with 3 hidden layers and 10 neurons each. This shallow architecture significantly reduces the number of trainable parameters, making the training process more efficient. The reduced number of neurons is sufficient for learning the residual terms (like \u03b2xy and \u03b3xy in the Lotka-Volterra system), allowing the UDE model to generalize better and avoid overfitting. The small architecture also ensures faster convergence and reduces the computational load.\n\u2022 Activation Function: The ReLU (Rectified Linear Unit) activation function was used in the UDE model. ReLU is computationally efficient and helps in avoiding the vanishing gradient problem, which can be an issue in deep networks. In a shallow network like UDE, ReLU provides a good balance between simplicity and effectiveness, allowing the model to learn complex interactions while maintaining numerical stability.\n\u2022 Optimizers: A two-phase optimization strategy was also employed for the UDE model:\nAdam Optimizer: Adam, with a learning rate of 0.001, was used for 20,000 iterations. Given the lower number of parameters in the UDE model, Adam's dynamic adjustment of learning rates enabled rapid convergence to a good solution.\nRMSProp Optimizer: After the Adam phase, RMSProp was applied for an additional 5,000 iterations. RMSProp is particularly useful for non-stationary environments, as it adjusts the learning rate based on recent gradients, helping to avoid overshooting in the later stages of training. With RMSProp, we used a learning rate of 0.001, momentum p = 0.9, and \u20ac = 1 \u00d7 10-8 to stabilize training and ensure smooth convergence.\n\u2022 Epochs/Iterations: The UDE model underwent 20,000 iterations using Adam, followed by 5,000 iterations with RMSProp. The larger number of iterations is justified by the shallow network architecture, which allows for more stable and less computationally expensive optimization steps over a longer period.\n\u2022 Weight Initialization: The Lux package's default initialization strategy was used, with care taken to avoid too large or too small weights that could destabilize training. The small number of parameters in the UDE model made this process more straightforward and less prone to initialization issues compared to the Neural ODE."}, {"title": "2.5.3 Comparison of Hyperparameters between Neural ODE and UDE", "content": "The distinct architectures and roles of Neural ODE and UDE models explain why RBF worked better in Neural ODE, while ReLU was more effective in UDE."}, {"title": "2.6 Forecasting with the Trained Models", "content": "After training, the models are used to forecast the predator-prey populations beyond the training window, over the extended time span t \u2208 [0, 20]. The forecasting performance is evaluated by comparing the predictions with the true populations.\nFor the Neural ODE model, the neural network models the entire system dynamics, while for the UDE model, only the learned interaction terms are forecasted. Both models' forecasts are compared to assess whether retaining known physics (as in the UDE approach) provides an advantage over fully data-driven Neural ODEs."}, {"title": "2.7 Model Validation and Performance", "content": "To evaluate the model's forecasting accuracy, we calculate the root mean square error (RMSE) between the forecasted and actual prey and predator populations. Additionally, we introduce the concept of the forecast breakdown point, the point at which the model's predictions start to diverge significantly from the true values. This breakdown point is used to assess the robustness of UDEs versus Neural ODEs in long-term forecasting tasks.\nThrough this methodology, we demonstrate how UDEs can effectively learn unknown interaction terms while retaining known system dynamics, providing a flexible yet interpretable framework for ecological modeling."}, {"title": "3 Results", "content": "In this section, we analyze the performance of the Neural ODE and UDE models in terms of their ability to forecast the population dynamics of the Lotka-Volterra system as the amount of training data is progressively reduced. The goal is to identify the forecast breakdown point, where the model's ability to predict future population dynamics deteriorates. We assess different cases of training data percentages for both models."}, {"title": "3.1 Neural ODE Forecast Breakdown", "content": "The Neural ODE model was trained on varying amounts of the dataset, and its forecasting performance was evaluated on the remaining unseen data. The following cases illustrate the breakdown points as the training data is reduced:"}, {"title": "3.1.1 Case 1: Training Neural ODE with 90% data and forecasting on the remaining 10%", "content": "In this case, the Neural ODE model was able to forecast with high accuracy, closely matching the ground truth for both prey and predator populations. (See Fig. 4a)"}, {"title": "3.1.2 Case 2: Training Neural ODE with 50% data and forecasting on the remaining 50%", "content": "The model was still able to capture the underlying dynamics with very good accuracy (See Fig. 4b)"}, {"title": "3.1.3 Case 3: Training Neural ODE with 40% data and forecasting on the remaining 60%", "content": "The model was still able to capture the underlying dynamics although slight perturbations start to appear (See Fig. 4c)"}, {"title": "3.1.4 Case 4: Training Neural ODE with 35% data and forecasting on the remaining 65%", "content": "When trained on 35% of the data, the Neural ODE model completely broke down. The forecast deviated significantly from the true population dynamics, indicating that the model could no longer capture the underlying system behavior accurately. (See Fig. 4d)"}, {"title": "3.2 UDE Forecast Breakdown", "content": "The UDE model was also trained on varying amounts of data to evaluate its performance under different training regimes.\nWe started from 35% training data where Neural ODE model started to fail miserably. Notably, the UDE model demonstrated much better performance when trained on small portions of the dataset. Even when the training data was reduced to 35%, the UDE model continued to forecast the population dynamics accurately, unlike the Neural ODE."}, {"title": "3.2.1 Case 1: Training UDE with 35% data and forecasting on the remaining 65%", "content": "Unlike the Neural ODE, the UDE model was able to capture the underlying dynamics with much greater accuracy, even when trained on only 35% of the data. This highlights the UDE model's robustness and ability to generalize with limited data. (See Fig. 5a)"}, {"title": "3.2.2 Case 2: Partial Breakdown at 31% Training Data", "content": "The UDE model began to show signs of breakdown at 31% training data, though it still captured the underlying dynamics reasonably well. This partial breakdown highlights the limitations of the UDE model when trained on extremely small data samples. (See Fig. 5b)"}, {"title": "3.2.3 Case 3: Complete Breakdown at 30% Training Data", "content": "Finally, we identified an edge case where the UDE model began to completely breakdown at 30% training data. (See Fig. 5b)"}, {"title": "3.3 Effect of Gaussian Noise on Performance", "content": "To further test the robustness of Neural ODE and UDE, Gaussian noise with varying standard deviations was added to the training data. Specifically, noise was added with standard deviations of 0.05 and 0.3, representing low and high levels of noise, respectively."}, {"title": "3.3.1 Performance with Mild Gaussian Noise (Standard Deviation = 0.05)", "content": "When introducing Gaussian noise with a standard deviation of 0.05, both Neural ODE and UDE models showed resilience. The noise introduced only minor fluctuations in the data, and both models were able to learn the underlying dynamics without any significant degradation in performance."}, {"title": "\u2022 Neural ODE:", "content": "The model showed resilience and captured the underlying trend. It was able to capture both predator and prey data well."}, {"title": "\u2022 UDE:", "content": "UDE also demonstrated similar robustness under the same conditions. The model remained highly stable, maintaining accurate forecasts with little fluctuation in its predictions."}, {"title": "3.3.2 Performance with High Gaussian Noise (Standard Deviation = 0.3)", "content": "Introducing a higher level of Gaussian noise, with a standard deviation of 0.3, revealed a clear performance difference between Neural ODE and UDE. The added noise significantly distorted the data, making it much harder for the models to learn the dynamics accurately. However, UDE outperformed Neural ODE:\n\u2022 Neural ODE: The performance of the Neural ODE model degraded notably. The loss function increased by over 25%, and the model started to struggle with convergence during training. Forecast accuracy declined sharply, especially for predator data and for extended time horizons, where predictions diverged significantly from the true dynamics. This suggests that Neural ODE is highly sensitive to data corruption from noise and struggles with generalization under such conditions.\n\u2022 UDE: On the other hand, UDE continued to perform relatively robustly despite the high noise levels. While the loss function increased by about 10%, the model remained stable and produced reasonably accurate forecasts. UDE's ability to handle higher noise levels highlights its robustness, especially in real-world applications where data may often be noisy or imperfect."}, {"title": "4 Discussion and Conclusion", "content": "In this work, we evaluated the performance of Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs) in predicting the population dynamics of the Lotka-Volterra system. The primary focus was to identify the forecasting breakdown points as the amount of training data was progressively reduced, and to assess the effectiveness of each approach in terms of model architecture, data requirements, and generalization ability."}, {"title": "4.1 Performance of Neural ODES", "content": "Neural ODEs offer a powerful framework for learning continuous dynamical systems directly from data. In our experiments, the Neural ODE model performed well when trained on a large percentage of the dataset (90% or 50%). The model successfully captured the underlying prey-predator interactions and provided accurate forecasts for future population dynamics.\nHowever, as the amount of training data was reduced, the Neural ODE model began to show signs of deterioration. Specifically, the model started to break down when trained with less than 40% of the data, with a complete failure observed at 35% and below. This breakdown can be attributed to several factors:\n\u2022 Model Complexity: The Neural ODE architecture employed in this study was relatively deep, with three hidden layers of 100 neurons each. While this deep architecture provides expressive power, it also requires a large amount of data to generalize well. With less training data, the model struggled to learn the complex dynamics, leading to overfitting and poor forecasting performance.\n\u2022 Data Dependency: Neural ODEs rely heavily on data for uncovering the complete system dynamics. As the amount of training data decreases, the model lacks the necessary information to learn the continuous trajectories of the system. This is particularly challenging for Neural ODEs, as they must approximate the full dynamics from scratch, requiring both a deep architecture and sufficient data."}, {"title": "4.2 Performance of Universal Differential Equations (UDES)", "content": "In contrast to Neural ODEs, Universal Differential Equations (UDEs) performed significantly better in the same forecasting tasks, particularly when trained on smaller portions of the dataset. Even with as little as 35% training data, the UDE model was able to capture the underlying dynamics of the Lotka-Volterra system with high accuracy. This robustness to data scarcity is one of the key advantages of UDEs.\nSeveral factors contributed to the superior performance of UDES:\n\u2022 Partial Learning of Dynamics: UDEs benefit from integrating known system dynamics (in this case, the Lotka-Volterra equations) with learned residual terms. Instead of learning the entire dynamics from scratch, the UDE model only learns the unknown or hard-to-model components of the system. This reduces the complexity of the learning task, allowing the model to generalize well even with less data. By leveraging prior knowledge, UDEs avoid overfitting and perform better under data constraints.\n\u2022 Shallow Neural Networks: The UDE model used a much shallower architecture compared to the Neural ODE model-specifically, three hidden layers with only 10 neurons per layer. Despite this shallow network, the UDE model achieved better performance, highlighting the efficiency of the UDE framework. Shallow networks are not only computationally more efficient but also less prone to overfitting, particularly when trained on limited data. The ability to train shallow networks while maintaining high accuracy underscores the robustness of UDEs in capturing the system's behavior with fewer parameters.\n\u2022 Robustness to Data Scarcity: One of the most significant advantages of UDEs is their robustness to data scarcity. While Neural ODEs showed a clear breakdown at 35% training data, the UDE model continued to perform well. Even at 31% training data, where some partial breakdown was observed, the model still captured the underlying dynamics reasonably well. This robustness is a result of combining known physics with machine learning, where the UDE framework augments the existing system dynamics with learned corrections. As a result, the model is less dependent on data and more capable of making accurate predictions with limited information.\n\u2022 Generalization Ability: By leveraging the known Lotka-Volterra equations, UDEs focus on learning the specific corrections or interaction terms (\u03b2xy, yxy) that may vary with data, rather than re-learning the entire dynamics. This allows the UDE model to generalize better across different time spans, even when training data is limited. The ability to extend the forecasting window while maintaining accuracy further demonstrates the advantage of UDEs over Neural ODEs in terms of generalization."}, {"title": "4.3 Comparison of Computational Efficiency", "content": "Another important factor to consider is computational efficiency. The Neural ODE model, with its deeper architecture, required more computational resources and time to converge, especially during the initial optimization phases.\nThe UDE model, on the other hand, was not only more accurate with less data, but it was also faster to train due to its shallow network architecture. This efficiency is particularly valuable in scientific machine learning applications, where computational resources may be limited or expensive to scale."}, {"title": "4.4 Conclusion", "content": "In conclusion, while Neural ODEs are a powerful tool for modeling continuous dynamical systems, they are highly data-dependent and prone to breakdown when trained with insufficient data. Their deep architecture, while expressive, makes them less robust when data is limited. Universal Differential Equations (UDEs), on the other hand, provide a more robust, efficient, and accurate alternative. By leveraging known system dynamics and focusing on learning only the unknown components, UDEs perform better with less training data and shallower networks."}, {"title": "Limitations and Future Work", "content": "While our approach using UDEs offers several advantages, such as robustness to noise, data scarcity, and computational efficiency, there are certain limitations that must be addressed for broader applicability. One significant challenge is the training of SciML frameworks on systems with a large volume of data points. As the dataset size increases, the computational cost and memory requirements also grow substantially, making it difficult to train models using higher-order differential equations or models with complex interactions, where each iteration requires significant computational power and memory for solving the differential equations accurately.\nFurthermore, while UDEs can incorporate prior knowledge about the system, the process of defining the right combina-tion of neural network architectures and differential equations remains non-trivial. It requires a deep understanding of both the physical system and the appropriate modeling techniques, making the framework less accessible for practitioners who lack domain expertise.\nFor future work, we see opportunities to enhance the scalability of UDEs through the development of more efficient solvers and optimization algorithms, potentially leveraging parallel computing and GPU acceleration.\nWe also plan to explore the application of UDEs to a wider range of real-world biological and ecological systems, such as multi-species interactions, time-series [9", "23": "sequential data [24"}]}