{"title": "MAMBA FUSION: LEARNING ACTIONS THROUGH QUESTIONING", "authors": ["Zhikang Dong", "Apoorva Beedu", "Jason Sheinkopf", "Irfan Essa"], "abstract": "Video Language Models (VLMs) are crucial for generalizing across\ndiverse tasks and using language cues to enhance learning. While\ntransformer-based architectures have been the de facto in vision-\nlanguage training, they face challenges like quadratic computa-\ntional complexity, high GPU memory usage, and difficulty with\nlong-term dependencies. To address these limitations, we intro-\nduce MambaVL, a novel model that leverages recent advance-\nments in selective state space modality fusion to efficiently cap-\nture long-range dependencies and learn joint representations for\nvision and language data. MambaVL utilizes a shared state tran-\nsition matrix across both modalities, allowing the model to cap-\nture information about actions from multiple perspectives within\nthe scene. Furthermore, we propose a question-answering task\nthat helps guide the model toward relevant cues. These questions\nprovide critical information about actions, objects, and environ-\nmental context, leading to enhanced performance. As a result,\nMambaVL achieves state-of-the-art performance in action recog-\nnition on the Epic-Kitchens-100 dataset and outperforms base-\nline methods in action anticipation.", "sections": [{"title": "1. INTRODUCTION", "content": "Learning visual representations using paired vision-language de-\nscriptions have proven to be a powerful tool in computer vision.\nNatural language supervision [22, 30] has not only pushed the state-\nof-the-art in visual tasks such as classification [23, 22], object detec-\ntion [18], etc., but also across other domains such as audio [28, 5],\nwearable sensor applications [11], etc. Current video-language\ndatasets are typically categorized into third-person and egocentric\n(first-person) videos. While third-person view data has been ex-\ntensively studied, existing methods struggle to adapt to egocentric\nvideos due to their unique characteristics, such as frequent and rapid\ncamera motion, limited field of view, and occlusions caused by\nthe user's body and objects. However, recent egocentric datasets\nlike Ego4D [8] and Epic-Kitchens-100 [3] have facilitated deeper\nexploration into this challenging setting.\nMost Visual Language Models (VLMs) rely on descriptions or\ncaptions, typically describing the contents of the videos [17]. This\nresults in capabilities like cross-modal retrieval and zero shot recog-\nnition across diverse classification tasks [17, 30]. In contrast, we\npropose to generate two questions-one for the verb and one for the\nnoun-each capturing distinct contextual information. This allows\nthe model to better grasp the context of actions in the video, leading\nto enhanced representation learning as questions encourage deeper\nreasoning and understanding relative to captions that describe the\nvideo content. The advantage of such a setup are illustrated using\nthe following example: Imagine a video showing the action \"drink-\ning water\" and you're playing a guessing game where you must ask\nquestions to help identify the action. To predict the verb and noun,\ntwo possible questions could be: (1) \"What should you consume\nto quench your thirst?\" and (2) \"What should you do to the water\nto enjoy its refreshing taste?\". The answers to these questions re-\nveal the action as a verb-noun pair within the scene. This approach\nmirrors how we often guide children toward answers by prompt-\ning them with questions, rather than simply providing the solution.\nDriven by this insight, we explore how a question-driven framework\ncan enhance the task of action recognition in the Epic-Kitchens-100\ndataset.\nFor video-language modeling (including multi-modal learning),\nTransformer-based approaches are the most popular option. They\ncan be broadly categorized into contrastive [23, 2], fusion-based\nlearning [25], or a combination of both [22]. While they are highly\neffective, the underlying attention mechanism requires quadratic\ncomputational complexity as the number of tokens grows. This lim-\nits their efficiency during both training and inference, and reduces\ntheir effectiveness in handling long-term sequence learning. More\nrecently however, Mamba, a state-space model-based approach, has\nemerged as an effective alternative, providing higher efficiency with\nlinear scaling and reduced complexity [9].\nTherefore, in this work, we introduce Mamba VL, a novel selec-\ntive state space fusion model designed to process multi-modal input\nwhile efficiently handling long sequence modeling. The model em-\nploys a shared state transition matrix within the selective state space\nmodels (SSMs) for both the vision and language branches. This\nshared structure enables the SSMs to exchange information across\nmodalities, extending the selection mechanism from single-modality\nmodels as described in [9]. This approach allows the model to effec-\ntively select relevant information across both vision and language\ndomains, ensuring effective fusion - which is highly flexible, and\ncapable of integrating with any number or type of input modalities,\nseamlessly incorporating existing pre-trained models.\nOur contributions are: (1) We propose a new question-answering\ntask for recognizing actions in egocentric videos; (2) We propose a\nnovel vision-language fusion approach based on the selective state-\nspace model, which can be straightforwardly adapted to handle in-\nputs from any modality; and (3) We conduct extensive experimental\nevaluation on the Epic-Kitchens-100 dataset [3], achieving state-of-\nthe-art performance on action recognition and action anticipation."}, {"title": "2. RELATED WORK", "content": "Vision-Language Models. VLMs have seen rapid expansion, pri-\nmarily driven by breakthroughs in image-language pre-training [23,\n17, 22, 7]. Primarily, they are based on Transformers [26], either\nemploying dual encoders that project visual and textual representa-"}, {"title": "3. METHODOLOGY", "content": "We present MambaVL in Figure 1, a vision-language fusion model\nthat uses SSM to learn joint representations from modalities.\n3.1. Preliminaries\nConsider a continuous system (1), which maps a 1-dimensional\nfunction or sequence $x(t) \\in \\mathbb{R} \\rightarrow y(t) \\in \\mathbb{R}$.\n$h'(t) = Ah(t) + Bx(t)$,\n$y(t) = Ch(t)$,\nwhere $A \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times 1}, C \\in \\mathbb{R}^{1 \\times N}$ are the state transition,\ninput projection and output projection matrices, respectively.\nBuilding on this, [10] proposes the structured state space se-\nquence (S4) model defined in the discrete space, where given a\ntimescale parameter $\\Delta$, the continuous matrices $A$ and $B$ are trans-\nformed into discrete matrices $\\overline{A}$ and $\\overline{B}$. The discretization process\nis zero-order hold (ZOH), which has the following form:\n$\\overline{A} = \\exp(\\Delta A)$,\n$\\overline{B} = (\\Delta A)^{-1} (\\exp(\\Delta A) - I) \\cdot \\Delta B$.\nUsing (2), (1) can be rewritten as:\n$h_t = \\overline{A} h_{t-1} + \\overline{B} x_t, y_t = C h_t$.\nBy performing a global convolution in (3), we have\n$K = (C \\overline{B}, C \\overline{A} \\overline{B}, ..., C \\overline{A}^{k-1} \\overline{B}), y = x * K$,\nwhere $K \\in \\mathbb{R}^k$ is a structured convolutional kernel.\n3.2. MambaVL\nFigure 1 shows the overall structure of MambaVL. [9] uses the orig-\ninal Mamba block to process the single modality sequence data,\nwhere, the timescale parameter $A_t$, $B$ and $C$ are directly derived\nfrom the input. As $A$ is not directly dependent on the input, we hy-\npothesize that sharing the state transition matrix $A$ between different\nmodalities enables the model to learn joint representations.\nGiven a video clip $V \\in \\mathbb{R}^{F \\times C \\times \\hat{H} \\times \\hat{W}}$, with $F$ frames, $C$ chan-\nnels, and $(\\hat{H}, \\hat{W})$ frame size, we use a video encoder to extract video"}, {"title": "4. EXPERIMENTS", "content": "4.1. Experimental Setup\nDatasets and Metrics: We evaluate our approach on the Epic-\nKitchens-100 (EK100) dataset on two tasks: Action recognition and\nAction Anticipation. EK100 [3] is an egocentric video dataset cap-\nturing daily activities around the kitchen. Action anticipation and\nclassification task requires each video classifying/predicting one of\nthe 97 verbs and 300 nouns. The highest scoring verb and noun pair\npredicted by the network constitute the action, and top-1 accuracy\nis reported for action recognition whereas Recall@5 is reported for\naction anticipation.\nImplementation Details: We set the input frame length to 16\nframes for all our experiments, and follow the same data processing\npipelines as the specific backbones used. Training is performed\nfor 100 epochs, where we employ cosine annealing with a warmup\nfor 2 epochs, with a base learning rate of $1e-6$, which linearly\nincreases to a peak of $1e-3$, and then gradually decreases to $1e-5$\nfollowing a half-wave cosine schedule.. The Mamba block has a\nhigher peak learning rate, set to 3e-3. We use a batch size of 128,\ndistributed over 8 Nvidia A40 GPUs. For action anticipation, we set\nthe anticipation time $t_a$ as 1 second, and use 16-frame long clips\nat a resolution of 224 x 224, uniformly sampled from an observed\nvideo of 64 frames (~2s in total). For action anticipation, we use\nthe same configuration as ORViT\u00b9 to train the baseline model. We\nuse AVION [30] to recognize actions in the observed frames and\nuse it to generate questions for the action anticipation task. We will\nrelease the code upon acceptance.\nQuestion Generation: We introduce a new annotation framework\nconsisting of question-answer pairs for actions in the EK100 dataset.\nThis process is divided into two stages: first, we enrich the action"}, {"title": "5. CONCLUSION", "content": "In this paper, we introduced MambaVL, a novel approach for fus-\ning visual and language features. Our key innovation is the use\nof a learnable shared state transition matrix within the SSM block\nfor each modality. This approach, along with the selection mecha-\nnism, enables each modality to learn from its own input while also\nconsidering information from other modalities during training. This\nlightweight fusion method is not only flexible enough to accommo-\ndate any number of input modalities, but it is also compatible with\na wide range of pretrained feature extraction models. Our findings\ndemonstrate the effectiveness of MambaVL in cross-modal fusion,\nand we believe it opens new opportunities for further research into\nMamba's application in cross-modal tasks."}]}