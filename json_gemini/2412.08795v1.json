{"title": "Coverage-based Fairness in Multi-document Summarization", "authors": ["Haoyuan Li", "Yusen Zhang", "Rui Zhang", "Snigdha Chaturvedi"], "abstract": "Fairness in multi-document summarization (MDS) measures whether a system can generate a summary fairly representing information from documents with different social attribute values. Fairness in MDS is crucial since a fair summary can offer readers a comprehensive view. Previous works focus on quantifying summary-level fairness using Proportional Representation, a fairness measure based on Statistical Parity. However, Proportional Representation does not consider redundancy in input documents and overlooks corpus-level unfairness. In this work, we propose a new summary-level fairness measure, Equal Coverage, which is based on coverage of documents with different social attribute values and considers the redundancy within documents. To detect the corpus-level unfairness, we propose a new corpus-level measure, Coverage Parity. Our human evaluations show that our measures align more with our definition of fairness. Using our measures, we evaluate the fairness of thirteen different LLMs. We find that Claude3-sonnet is the fairest among all evaluated LLMs. We also find that almost all LLMs overrepresent different social attribute values.", "sections": [{"title": "1 Introduction", "content": "Multi-document summarization (MDS) systems summarize the salient information from multiple documents about an entity, such as news articles about an event or reviews of a product. Typically, such documents are associated with social attributes e.g. political ideology in news and sentiments in reviews. Documents with different social attributes tend to have diverse information or conflicting opinions. A summary for them should fairly represent differing opinions across documents.\nFairness in MDS measures whether a system can generate summaries fairly representing information from documents with different social attribute values. It can be measured at a summary-level- quantifying how fairly an individual summary represents documents with different social attribute values or at a corpus-level-quantifying how fairly a corpus of summaries as a whole represents documents with different social attribute values. Previous works in this area measured fairness in extractive or abstractive settings (Shandilya et al., 2018; Olabisi et al., 2022; Zhang et al., 2023; Huang et al., 2024). These works generally evaluate the fairness of a system as the aggregated summary-level fairness of its generated summaries. It is measured using Proportional Representation\u2014a fairness measure based on Statistical Parity (Verma and Rubin, 2018). It requires that a document sentence being selected as a summary sentence is independent of its originating document's social attribute value. Therefore, the distribution of social attributes among information in a fair summary should be the same as the distribution of social attributes among input documents.\nThe definition of Proportional Representation (PR) suffers from two key problems. The first problem is that Proportional Representation does not consider the redundancy in input documents, common in MDS. For example, in reviews of a hotel with great views, multiple reviews could comment on the view. For input documents with much redundancy, it is difficult to generate a non-redundant, fair summary according to PR. For example, suppose 67% of input reviews are positive and mostly discuss topics A and B, while 33% of reviews are negative and evenly discuss topics D, E, F, and G (Fig. 1a). According to PR, Summary 1 is fairer than Summary 2. However, Summary 2 is clearly better since, unlike Summary 1, it avoids repetition (Summary 1 repeats topic A twice) and almost equally cover the information from both positive and negative reviews. Hence, there is a need to redefine fairness to consider redundancy in MDS. Besides, the idea behind PR can work for measuring fairness in other NLP tasks but not for abstractive summarization because recent LLM-"}, {"title": "2 Related Work", "content": "Shandilya et al. (2018, 2020); Dash et al. (2019) propose to measure the summary-level fairness"}, {"title": "3 Fairness Measures", "content": "In this section, we describe notation (Sec. 3.1) and our proposed measures, Equal Coverage (Sec. 3.2) and Coverage Parity (Sec. 3.3)."}, {"title": "3.1 Notation", "content": "We use G to denote all samples for evaluating the fairness of a system on a social attribute. Each sample (D, S) \u2208 G contains a document set D = {d1,..., dn} and a summary S generated by the system for these documents, where di denotes the i-th document. Each input document di is labeled with an social attribute value a\u017e \u2208 {1, ..., K}."}, {"title": "3.2 Equal Coverage", "content": "Equal Coverage is a summary-level fairness measure for measuring the fairness of a summary, S, for document sets, D. Equal Coverage is based on the principle that the documents with different social attribute values should have equal probabilities for being covered by a summary sentence. We denote the probability that a random document d \u2208 D whose social attribute value a is k is covered by a random summary sentence s \u2208 S as p(d, s|a = k). This is referred to as the coverage probability for the social attribute value k. Similarly, we denote the probability that a random document, d, is covered by a random summary sentence s \u2208 S as p(d, s), which is referred to as the coverage probability for a document. For a fair summary S according to Equal Coverage, coverage of a random document d should be independent of its social attribute value:\n$$p(d, s|a = k) = p(d, s), \\forall k. $$(1)\nHowever, two issues arise with summaries' complex sentence structures. First, a summary sentence can combine information from several documents, making it difficult to attribute the sentence to any single document. Second, sentence lengths vary significantly based on social attribute values. The length difference can skew the coverage probability for different social attribute values p(d, s|a = k). To address these issues, Equal Coverage splits the summary sentences s \u2208 S into multiple simpler sentences by prompting GPT-3.5 motivated by Bhaskar et al. (2023); Min et al. (2023). For example, compound sentences are split into simple sentences that describe a single fact. We denote the j-th summary sentence after split as sj. Further details are in App. A.1.\nEqual Coverage estimates the coverage probability for different social attribute values p(d, s|a = k) and for a document p(d, s) based on the probability p(di, sj) that a document di is covered by a summary sentence sj. The probability p(dj,sk) is estimated as the entailment probability that the document dj entails the summary sentence sk by a textual entailment model (Williams et al., 2018). Motivated by Laban et al. (2022), Equal Coverage divides documents into chunks of M words. The l-th chunk of the document dj is denoted as dj,1. The entailment model estimates the probability p(dj, sk) as the maximum entailment probability p(dj,l, sk) between the chunk dj, and the summary sentence sk:\n$$p(di, sj) = max{p(di,l, sj)|di,l \u2208 di}, $$(2)\nwhere p(di,1, sj) is the probability that the document chunk dit entails the summary sentence sj as per the entailment model. Based on the probability p(di, sj) that the document di is covered by the summary sentence sj, the coverage probability for"}, {"title": "3.3 Coverage Parity", "content": "Coverage Parity is a corpus-level fairness measure designed to measure the fairness of a system of all samples G. Coverage Parity is based on the principle that documents with different social attribute values should have equal chances of being overrepresented or underrepresented by their corresponding summaries.\nFor a fair MDS system, Coverage Parity requires that the average difference between the coverage probability for the document di, p(di, s), and the coverage probability for a document, p(d, s), (Eqn. 4) is close to zero for all documents di whose social attribute value a\u017c is k. The coverage probability for the document di, p(di, s) is estimated as:\n$$p(di, s) = \\frac{1}{|S|} \\sum_{Sj \\in S}p(di, j), $$(6)\nFor simplicity, we denote the coverage probability difference between p(di, s) and p(d, s) as c(di). We collect these coverage probabilities differences c(di) from all input documents of the dataset G whose social attribute value is k into a set Ck. Based on the average of the set of coverage probability differences Ci, Coverage Parity measures the fairness of the MDS system:\n$$CP(G) = \\frac{1}{K} \\sum_{k=1}^{K}E(CK), $$(7)\nwhere E(Ck) denotes the average coverage probability difference of Ck. A high Coverage Parity value CP(G) indicates less fairness since it suggests that the chances of being overrepresented or underrepresented are very different for some social attribute values. Based on whether the average coverage probability differences, E(Ck), is greater or less than zero, we can also identify which social attribute value tends to be overrepresented or underrepresented."}, {"title": "4 Experimental Setup", "content": "We now describe experimental setup to evaluate the fairness of LLMs using our measures."}, {"title": "4.1 Datasets", "content": "We conduct experiments on five different datasets from the three domains: reviews, tweets, and news. These datasets are the Amazon (Ni et al., 2019), MITweet (Liu et al., 2023), SemEval (Mohammad et al., 2016), Article Bias (Baly et al., 2020), and News Stance (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017; Hanselowski et al., 2019) datasets. Tab. 1 shows the statistics of these datasets along with their social attribute values.\nWe observe that for some datasets, the fairness of summaries depends on the distributions of social attribute values in the input documents (Sec. 5.4).\u03a4\u03bf balance social attribute values' impacts on fairness, we perform stratified sampling to collect 300 input document sets, D, for each dataset. Among these sampled sets, input document sets D dominated by different social attribute values have equal proportions. The stratified sampling does not affect the calculation of our fairness measures. More details of preprocessing are in App. \u0410.4."}, {"title": "4.2 Implementation Details", "content": "We evaluate the fairness of six families of LLMs: GPTs (Ouyang et al., 2022; Achiam et al., 2023) (GPT-3.5-0124, GPT-4-turbo-2024-04-09), Llama2 (Touvron et al., 2023) (Llama2-7b, Llama2-13b, Llama2-70b), Llama3.1 (AI@Meta, 2024) (Llama3.1-8b, Llama3.1-70b), Mixtral (Jiang et al., 2023, 2024) (Mistral-7B-Instruct-v0.1, Mixtral-8x7B-Instruct-v0.1), Gemma2 (Team et al., 2024)"}, {"title": "5 Experiment Results", "content": "We now describe experiment results to evaluate the fairness of LLMs using our measures."}, {"title": "5.1 Human Evaluation", "content": "We perform a human evaluation to determine which measure, Equal Coverage or Proportional Representation, aligns more with our definition of fairness, which is to fairly represent information from documents with different social attribute values. For Proportional Representation, we use the BARTScore (Yuan et al., 2021) implementation proposed by Zhang et al. (2023), as it shows the highest correlation with human perception.\nPerforming human evaluation for fairness on summarization is challenging due to the need to read entire input document sets. Therefore, we perform experiments on the Amazon dataset which only contains eight short reviews per input document set (Tab. 1). Besides, it is easier for people without training to judge whether an opinion is positive or negative than to judge if it is left-leaning or right-leaning. To simplify the evaluation, we only consider input document sets with only negative or positive reviews, displayed in two randomized columns. For each input document set, we consider the summary generated by GPT-3.5 since it shows medium-level fairness (Tab. 2, 3). To further simplify the evaluation, we focus on summaries where Equal Coverage and Proportional Representation disagree on their fairness. We randomly select 25 samples containing input document sets and corresponding summaries that meet these criteria. Each sample is annotated by three annotators recruited from Amazon Mechanical Turk. The annotators should be from English-speaking countries and have HIT Approval Rates greater than 98%. More details are in App. A.5.\nFollowing previous practice of performing human evaluation on fairness in summarization (Shandilya et al., 2020), annotators are asked to read all reviews and identify all unique opinions. They then are asked to read the summary and rate its fairness as leaning negative, fair, or leaning positive. The Randolph's Kappa (Randolph, 2005) between annotations of three annotators is 0.42, which shows moderate correlation. Out of these 25 samples, annotations aligns more with Equal Coverage in 17 samples, while aligns more with Proportional Representation in 8 samples. The difference is statistically significant (p < 0.05) using paired bootstrap resampling (Koehn, 2004). It shows that our implementation aligns more with our definition of fairness than Proportional Representation.\nWe compare Coverage Parity with second-order fairness (Zhang et al., 2023) based on Proportional Representation to evaluate their effectiveness in identifying overrepresented sentiments on the group level. For this, we perform bootstrapping to generate 5000 groups of bootstrap samples. We find that Coverage Parity aligns with human annotation in 95% of the groups, while second-order fairness aligns in 5% of the groups. It shows that Coverage Parity aligns more with our definition of fairness than the second-order fairness."}, {"title": "5.2 Summary-level Fairness Evaluation", "content": "To evaluate the summary-level fairness of different LLMs, we report the average Equal Coverage values for all samples in each dataset, EC(G). We also report an Overall score which is the average of normalized EC(G) ([0, 1]) using min-max normalization on all datasets. Results are in Tab. 2.\nFrom the table, we observe that Gemma2-27b is the fairest based on the Overall score. Among smaller LLMs (with 7 to 9 billions parameters), Gemma2-9b is the fairest. We also observe that almost all evaluated LLMs are fairer than COOP on the Amazon dataset, and PEGASUS and PRIMERA on the MITweet and Article Bias datasets. For the comparison within families of GPTs, Llama2, Claude3, Mixtral, and Gemma2, we observe that larger models are generally fairer.\nPrevious works by Zhang et al. (2023) and Lei et al. (2024) evaluate the fairness of LLMs using Proportional Representation. When evaluating the fairness on sentiments in the review domain, our results using Equal Coverage are consistent with these works in the finding that GPT-4 is fairer than GPT-3.5, and both are fairer than COOP. However, they find that Llama2-13b is the fairest, while our results show that Llama2-13b and Llama2-70b are comparably fair. When evaluating the fairness on political ideologies in the tweet domain, our results are consistent with these works on that smaller models are fairer for Llama2. However, they find that GPT-4 is fairer than GPT-3.5 while we find that GPT-3.5 is fairer than GPT-4. We also show that GPT-4 is fairer than Llama2-7b, which contrasts the previous works. As shown in Sec. 1, Equal Coverage considers redundancy common in MDS, suggesting better reflects fairness in MDS."}, {"title": "5.3 Corpus-level Fairness Evaluation", "content": "To evaluate the corpus-level fairness of different LLMs, we report the Coverage Parity, CP(G), on different datasets. For each dataset, we report the most overrepresented and underrepresented social attribute value k whose average coverage probability difference, E(Ck), is the maximum and minimum respectively. We also report an Overall score which is the average of normalized CP(G) ([0, 1]) using min-max normalization on all datasets. The results are in Tab. 3.\nFrom the table, we observe that Llama2-70b is the fairest based on the Overall score. Among smaller LLMs (with 7 to 9 billions parameters), Llama3.1-8b is the fairest. We also observe that evaluated LLMs are less fair than PEGASUS and PRIMERA on the SemEval dataset. While comparing within each family of LLMs, we observe that larger models are fairer for the families of Llama2, Llama3.1, Mixtral, Gemma2, and Claude3. However, for the family of GPTs, GPT-4 is less fair than GPT-3.5, suggesting that larger models are not necessarily more fair on the corpus level. Besides, we observe that the fairness measured by Coverage Parity and Equal Coverage are different on some datasets. The difference indicates that we should consider both summary-level fairness and corpus-level fairness for comprehensively measuring fairness in MDS.\nWe can also observe that most LLMs overrepresent"}, {"title": "5.4 Fainess under Different Distributions of Social Attributes", "content": "We perform experiments to evaluate whether the fairness of LLMs changes under different distribu-"}, {"title": "5.5 LLM Perception of Fairness", "content": "We perform experiments to evaluate which measure, Equal Coverage or Proportional Representation, aligns more with LLMs' perception of fairness. This is an exploratory experiment, and we do not assume the LLMs' perception of fairness as ground truth. We prompt an LLM to generate a summary for an input document set, then prompt it again to generate a fair summary for the same set. The second prompt requires that the summary fairly represent documents with different social attributes (App. A.7). However, it does not provide any other details about fairness, allowing the LLM to decide. The prompt also includes the social attribute value for each document. We compute the Equal Coverage and Proportional Representation for both summaries and consider the relative change in values before and after the LLM is prompted to generate a fair summary. If a measure aligns more with LLM's perception of fairness, the score for the 'fair' summary should be lower. The differences between average relative changes of Equal Coverage and Proportional Representation are in Tab. 6.\nFrom the table, we observe positive differences for most LLMs, suggesting that Equal Coverage decreases more compared to Proportional Representation. It means that Equal Coverage aligns more with LLM's perception of fairness. Specifically, Proportional Representation aligns more with the perception of fairness of Gemma2-27b and Claude3-haiku, while Equal Coverage aligns more with the remaining LLMs."}, {"title": "6 Conclusion", "content": "We propose two coverage-based fairness measures for MDS, Equal Coverage for measuring summary-level fairness and Coverage Parity for measuring corpus-level fairness. Using these measures, we find that Claude3-sonnet is the fairest among all LLMs. We also find that most LLMs overrepresent certain social attribute values in each domain.\nFuture works can explore the effect of training data, especially instruction tuning and preference tuning data, on the fairness of LLMs. Future works can also finetune LLMs based on our measures to develop fairer models."}, {"title": "7 Limitations", "content": "The effectiveness of two proposed measures, Equal Coverage and Coverage Parity, relies on whether the probability that a document entails a summary sentence estimated by the entailment model is accurate. To evaluate the performance of the entailment model for such a task, previous works generally use the accuracy of the entailment model on the fact verification dataset or the correlation between the factuality scores of summaries annotated by humans with the factuality scores estimated by the entailment model on the summarization evaluation benchmark. Although there are several fact verification datasets and summarization evaluation benchmarks in the news domain, there are no such datasets in the reviews and tweets domain to our best knowledge. Therefore, we cannot evaluate the accuracy or perform calibration for the entailment models in these two domains. However, as shown in Sec. A.3, Equal Coverage and Coverage Parity based on different commonly used entailment models are mostly correlated. These entailment models are also widely used for measuring factuality in summarization tasks (Maynez et al., 2020; Laban et al., 2022)."}, {"title": "8 Ethical Consideration", "content": "The datasets we use are all publicly available. We do not annotate any data on our own. All the models used in this paper are publicly accessible. We do not do any training in this paper. For the inference of Llama2-7b, Llama2-13b, Mistral-7b, and Gemma, we use on Nvidia A6000 GPU. For the inference of Llama2-70b and Mixtral-8x7b, we use 4 Nvidia A6000 GPUs. For all other experiments, we use one Nvidia V100 GPU.\nWe perform human evaluation experiments on Amazon Mechanical Turk. The annotators were compensated at a rate of $15 per hour. During the evaluation, human annotators were not exposed to any sensitive or explicit content."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Split and Rephrase Summary Sentences", "content": "Motivated by Bhaskar et al. (2023); Min et al. (2023), we split all summary sentences into simple sentences. The goal of this step is to ensure that each summary sentence after the split only discusses a single fact. Specifically, compound sentences are split into simple sentences, while sentences with compound subjects or objects are split into sentences with simple subjects or objects. After the split, summary sentences are then rephrased to remove the reported speech, like 'documents say what'. For splitting and rephrasing, we prompt GPT-3.5 with demonstrations."}, {"title": "A.2 Document Chunking", "content": "To estimate the probability that a document is covered by a summary sentence (Eqn. 2), we divide the document into chunks of no more than W words. Each chunk contains one or several neighboring sentences of the document. Since LLMs are less prone to factual errors (Goyal et al., 2022; Zhang et al., 2024), the chunk size W is tuned to maximize the proportion of summary sentences whose originating documents are identified by the Roberta-large is the highest. We tune the chunk size W based on the average proportions of summary sentences generated by GPT-3.5 and Llama2-7b on all datasets. From the table, we can observe that when the chunk size is 100, the proportion of summary sentences whose originating documents are identified by the Roberta-large is the highest. The result is consistent with the finds of Honovich et al. (2022). Therefore, we set the chunk size W as 100."}, {"title": "A.3 Choice of Entailment Models", "content": "The implementations of our measures are independent of the choice of entailment model."}, {"title": "A.4 Datasets", "content": "In this section, we describe the reason for choosing these datasets and how we preprocess these datasets."}, {"title": "A.5 Human Evaluation", "content": "For each sample of an input document set and its corresponding summary, annotators are asked to identify all unique negative and positive opinions in the input document set. They then evaluate whether the summary reflects these opinions and classify the summary as leaning negative, fair, or leaning positive."}, {"title": "A.6 Summarization Prompts", "content": "We prompt these LLMs to generate summaries for the input document sets of different datasets. For the SemEval and News Stance datasets, the prompts additionally request that the summaries focus on the social attributes' target since the input documents of these datasets contain unrelated information. We use the default generation hyperparameters for all LLMs."}, {"title": "A.7 Fair Summarization Prompts", "content": "To test LLM perception of fairness, we prompt these LLMs to generate summaries that fairly represent documents with different social attribute values. However, it does not provide any other details about fairness, allowing the LLM to decide."}, {"title": "A.8 Correlation between Equal Coverage and Proportional Representation", "content": "To compare Equal Coverage and Proportional Representation, we report the Spearman correlation between these two measures. Specifically, we report the Spearman correlation between Equal Coverage value EC(D, S) and Proportional Representation based on BARTScore of each summary generated by different LLMs. The results are in Tab. 9."}, {"title": "A.9 Lower and Upper bounds of the measure", "content": "To measure the difficulty of obtaining fair summaries on different datasets, we estimate lower bounds, Lowergre, and upper bounds, Uppergre, for both fairness measures by greedily extracting sentences that minimize or maximize the measures."}, {"title": "A.10 Summary-level Fairness under Different Distributions of Social attributes", "content": "To measure differences of the summary-level fairness measured by Equal Coverage under different distributions, we use maximum differences of equal coverage values EC(Gk) (Sec. 3.2) on sets dominated by different social attribute values Gk."}]}