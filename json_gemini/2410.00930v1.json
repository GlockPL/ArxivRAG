{"title": "ACEV: Unsupervised Intersecting Manifold Segmentation using Adaptation to Angular Change of Eigenvectors in Intrinsic Dimension", "authors": ["Subhadip Boral", "Rikathi Pal", "Ashish Ghosh"], "abstract": "Intersecting manifold segmentation has been a focus of research, where individual manifolds, that intersect with other manifolds, are separated to discover their distinct properties. The proposed method is based on the intuition that when a manifold in D dimensional space with an intrinsic dimension of d intersects with another manifold, the data variance grows in more than d directions. The proposed method measures local data variances and determines their vector directions. It counts the number of vectors with non-zero variance, which determines the manifold's intrinsic dimension. For detection of the intersection region, the method adapts to the changes in the angular gaps between the corresponding direction vectors of the child and parent using exponential moving averages using a tree structure construction. Accordingly, it includes those data points in the same manifold whose neighborhood is within the adaptive angular difference and eventually identifies the data points in the intersection area of manifolds. Data points whose inclusion in the neighborhood-identified data points increases their intrinsic dimensionality are removed based on data variance and distance. The proposed method performs better than 18 SOTA manifold segmentation methods in ARI and NMI scores over 14 real-world datasets with lesser time complexity and better stability.", "sections": [{"title": "I. INTRODUCTION", "content": "A manifold [1] is a topological structure that is locally homeomorphic to a Euclidean space of a specific dimension. A multi-manifold structure is a complex arrangement where data points are distributed across multiple distinct manifolds rather than within a single continuous space. Manifold segmentation is a computational task focused on dividing intersecting and non-intersecting manifolds present in data into meaningful and connected segments, aiming to delineate the surface or interior of the manifold using specific criteria. This segmentation process finds applications in diverse fields, including computer graphics, computer vision, medical imaging, and more. There are primarily two types of manifolds: intersecting and non-intersecting. Intersecting manifolds overlap with other manifolds, while non-intersecting manifolds either exist in separate regions of space or are arranged in a manner that prevents intersection. Intersecting Manifold Segmentation holds significant value in various applications, such as medical image analysis, precise segmentation of anatomical structures, and spectral clustering [2]. The primary job of intersecting manifold segmentation is to learn the structure of very individual manifolds and identify the region where it intersects with other manifolds."}, {"title": "II. RELATED WORKS", "content": "The problem of intersecting manifold segmentation is addressed by various methods, which broadly belong to these categories."}, {"title": "A. Traditional Methods", "content": "Methods that do not consider the structure of the individual manifolds and instead of that, segment depending on the local structure of the data. A comparative detail of few these methods are presented in Table I."}, {"title": "B. Manifold Structure Learning Methods", "content": "Various methods learn the structure of manifolds and they are used for manifold segmentation. Table II holds the details of these methods."}, {"title": "C. Individual Manifold Structure Learning Based Methods", "content": "There are methods dedicated to segment intersecting manifolds with various assumptions and constraints. The brief discussion of these methods are given in Table III."}, {"title": "III. PROPOSED WORK", "content": ""}, {"title": "A. Problem Statement", "content": "Suppose there are n data points represented in D dimensional space and these n data points are lying on m(\u2265 1) non-intersecting manifolds, where li represents ith non-intersecting manifold. Each li consists of qi(\u2265 1) intersecting manifolds and Mij, the jth individual manifold in li has intrinsic dimension dij and 1 \u2264 i \u2264 m,1 \u2264 j \u2264 qi and 1 \u2264 dij \u2264 D. The proposed method first segments those m non-intersecting manifolds and in the second step, it separates each individual manifold Mij, which intersects with other manifolds in li if qi > 1, based on an unsupervised approach. This manuscript proposes an innovative approach to this."}, {"title": "B. Segmentation of non-intersecting manifolds", "content": "The segmentation of non-intersecting manifolds is based on the work [26], which is an unsupervised manifold segmentation mechanism. The method uses a graph-based component analysis to determine the number of components or non-intersecting manifolds present in the data and uses agglomerative clustering to group data points that belong to the same manifold.\nInitially, a k-neighborhood is found for every data point and those k data points are considered adjacent to that data point. Following this mechanism an adjacency matrix is created which resembles a graph. There are n data points and therefore an n\u00d7n adjacency matrix G will be obtained. The method follows the idea that the singular value decomposition of a Laplacian graph matrix will depict the number of disjoint components present in the graph. In other words, the number of eigenvectors with corresponding zero eigenvalues of the Laplacian graph matrix will be the number of components in the graph. Therefore, the number of components present in the graph is found by constructing the adjacency matrix G and corresponding Laplacian graph matrix LG. In other words, as there are m non-intersecting manifolds present in the dataset, the number of components in the graph G will be also m and the number of eigenvectors with corresponding zero eigenvalues of the Laplacian graph matrix will be m. The Laplacian graph matrix LG corresponding to the adjacency matrix G will be a n x n matrix where value of the jth column of the ith row will be\n$X_{ij} =\n\\begin{cases}\n-1 & \\text{if } v_i \\text{ and } v_j \\text{ are adjacent} \\\\\nd(v_i) & \\text{if } i = j \\\\\n0 & \\text{otherwise;}\n\\end{cases}$ (1)\nwhere vi and vj are the i-th and j-th vertex and d(vi) is degree of vi. Then singular value decomposition (SVD) of the Laplacian graph matrix LG is performed and the number of zero eigenvalues is counted.\nNow to find which datapoint is part of which non-intersecting manifold, hierarchical agglomerative clustering [27] is performed on the data. This yields m clusters, where each cluster represents individual non-intersecting manifolds li and the proposed method finds the intersecting manifolds present in each li."}, {"title": "C. Segmentation of intersecting manifolds", "content": "The proposed method introduces a novel unsupervised intersecting manifold segmentation mechanism. In Figures 1a and 1b, data points A and B are attributed to manifold U, data points C and D to manifold V, while data points P, Q and R reside within the intersection of these two manifolds. The primary objective is to identify the data points situated in the intersection region of different manifolds with an approach that relies on the intrinsic dimension [28] of individual manifolds. As depicted in Figures 1a and 1b, manifolds U and V have an intrinsic dimension 2 and this is true for data points A, B, C, and D as their local"}, {"title": "1) Intrinsic Dimension Determination", "content": "Suppose there is a d-dimensional manifold in a D(D > d)-dimensional vector space and the neighbourhood of a data point t from that manifold is considered. Now, the covariance matrix of that neighbourhood is computed and D eigenvalues and associated D eigenvectors are found. The number of eigenvectors corresponding to non-zero eigenvalues for that neighbourhood will be d. This is true because there will be zero data variance in the other (D - d) directions, which means that the associated eigen-\nvalues will be zero. Further in the discussion, eigenvector and eigenvalues, eigenvector and eigenvalues of the datapoint and eigenvector and eigenvalue of the neighbourhood will mean eigenvectors and eigenvalues of the covariance matrix of that data point's neighbourhood. The algorithm determines the directions of the eigenvectors with non-zero eigenvalues for the neighbourhood of t. Suppose, two data points t and f belong to the same manifold, then their neighbourhood structure will be similar and the direction of data variance will be similar and therefore, their corresponding directions of eigenvectors will be similar, i.e., gth principal component of datapoints t and f will be in the same direction. Therefore, the angular gap between two corresponding eigenvectors derived from the neighbourhood of two individual data points is decisive in determining whether those two data points belong to the same local structure or not.\nSo, the angular gaps between the corresponding eigenvectors of the data points are calculated using equation 2 for the data point t and f, where ptg and Pfg are the gth principal component of tth and fth data point respectively.\n$angle\\_di e\\_di f fer_g(t, f) = cos^{-1} \\frac{(P_{tg}, P_{fg})}{||P_{tg}|| ||P_{fg}||}, g = 1,..., D$ (2)\nIn Figure 2, the green circles show the neighbourhoods of data points E and C, while the red circle corresponds to the neighbourhood of datapoint Q. The arrows within each circle denote the direction of non-zero data variance for their respective neighbourhoods, where the arrow length is independent of the amount of data variation. The angular difference between the 1st principal components of datapoints E and C is denoted by a and the same is denoted by \u1e9e for the 2nd principal components as shown in Figure 2. For datapoints E and C, a and B are observed to be nearly zero as the neighbourhood structures are similar. However, for datapoint Q, the angular difference in the third direction is high, as Q is from a region which has an intrinsic dimension of 3. It will have a non-similar 3rd eigenvector"}, {"title": "2) Manifold Structure Learning using Time Series Analysis", "content": "These local structural changes manifest gradually rather than abruptly. To effectively learn the change in angular differences, ACEV employs a neighbourhood-based approach using the Exponential Moving Average (EMA) method [29]. The EMA method is applied through the following equation 3. This strategic use of EMA ensures that the algorithm adapts to the evolving nature of local structures and captures the subtle variations in the angular gaps between eigenvectors. The predicted angular gap is computed as\n$E_d(s) = \u03b1 \u00b7 angle\\_di f fer_d(s, s \u2212 1)+ (1-\u03b1) \u00b7 E_d(s\u2212 1)$ (3)\nwhere Ed(s) predicts the angular difference of dth eigenvector for sth data point, a is the exponential smoothing factor and d = 1 \u2026 D.\nTo segment intersecting manifolds, ACEV initiates traversal from a data point t \u2208 li, where t = min(t1,\u2026\u2026,tn\u2081) for any one dimension in D and ni is the number of unlabelled data points in li which are not included in any individual manifold. t is first considered for the manifold and serves as the root. After determining t, its k-nearest neighbor data points are identified for traversal and probable inclusion as its children. The algorithm aims to include data points within the same manifold that exhibit non-significant angular differences in all directions. It begins by finding the eigenvectors of the neighbourhood for both the parent s - 1 and potential child data points s. Subsequently, it calculates the angular differences angle_differd(s,s \u2013 1) between these vectors using equation 2. ACEV then predicts angular differences Ed(s) for all directions using equation 3. If the difference between Ed(s) and angle_di f ferd(s, s \u2013 1) is insignificant for all D directions, then the potential child is included in the same manifold as the parent.\nThe inclusion process follows a depth-first search method, creating a tree structure with t as the root. The calculation of equation 3 follows the path from the root to the specific data point s for which it is calculated and s \u2013 1 is the parent of s and so on in the tree structure. This inclusion procedure continues until each data point in li is included, or with the current neighbourhood, the probable child s couldn't be included.\nIt is important to mention that the EMA method requires an initial construction phase and therefore, 0.05% of unlabelled data points are included in the tree without constraint. This step has negligible"}, {"title": "3) Intersecting Neighbourhood Filtration", "content": "The unsatisfiability of the inclusion criterion signifies that there are data points in the neighbourhood, for which there is a significant angular difference in one or more directions. This establishes that there is an increase in data variance in these directions, i.e., there is an increase in eigenvalue. It is evident that the probable child is lying in the intersection region and ACEV filters the neighbourhood and considers those data points lying on that particular manifold that it is currently traversing and excludes those data points from other manifolds.\nFor example as shown in Figure 3, suppose X, Y and Z are the directions of data variance of Q, which lies on manifold A, and data points C and E are wrongly included in the neighbourhood of Q and Q couldn't be included in the manifold A. As Q lies in manifold A, there will be zero eigenvalues in the direction of Z; therefore, the predicted angular gap in that direction will be nearly zero.\nNow, consider the distances of Q, C and E from X, Y and Z. It is clear that Q will be very close to X and Y rather than C and E, but may have similar distances from Z. In this scenario, for the removal of C and E from the neighbourhood of Q, eigenvector directions and associated eigenvalues of the covariance matrix of the neighbourhood of (Q - 1), the parent of Q, will be beneficial. Let us consider the modified distance for each data point r in the neighbourhood of Q shown in the equation 4.\n$mod\\_dis(r) = \\sum_{w=1}^{D} dis(e_w, r) \\frac{E_w(r)}{E_w(Q-1)}$ (4)\nwhere dis(ew,r) is the distance of datapoint r from wth eigenvector of neighbourhood of (Q\u22121). Ew(r) is eigenvalue for wth eigenvector of neighbourhood of r and Ew(Q-1) is eigenvalue for wth eigenvector of neighbourhood of (Q \u2013 1). The data points, which are on the same manifold, will have similar $\\frac{E_w (r)}{E_w (Q-1)}$ with $E_w(r)}{E_w (Q-1)}, \\forall w$ and $\\frac{E_w (r)}{E_w (Q-1)}$ will be\nnearly 1. In contrast, the data points which are not on the same manifold will have dissimilar values $\\frac{E_w (r)}{E_w (Q-1)}$ with $E_w(r)}{E_w (Q-1)}$ and $\\frac{E_w (r)}{E_w (Q-1)} (r)$ will be more than 1. For example, in Figure 3, C will have a higher variance in Z direction, i.e., EZ(C) will be higher than EZ(Q-1) and $\\frac{EZ(r)}{E_Z (Q-1)}$ will be greater than 1. This will be the contribution of $\\frac{EZ(r)}{E_Z (Q-1)}$ in demarcation. Similarly, the data points, which are not part of the manifold, may have similar $E_w(r)}{E_w (Q-1)}$ with $\\frac{E_w (r)}{E_w (Q-1)}$ but as they are not from the same manifold the value of dis(ew, r) will be higher than a data point which lies on that manifold. This will be the contribution of dis(ew,r) in demarcation. So, the mod_dis(r) for neighbourhood data points of Q, which are on manifold A will be comparatively lower than data points C and E.\nSo using this intuition to filter the neighbourhood, mod_dis(r) is found for every data point in the neighbourhood of the probable child Q. Now, maintaining a decreasing order of mod_dis(r), the ACEV removes data points from the neighbourhood of Q one by one and calculates the angular differences angle_di f fera(Q, Q \u2212 1), \u2200d using equation 2 with the updated neighbourhood. If the difference between Ed(Q) and angle_di f fera(Q, Q \u2212 1) is insignificant for all directions D, the potential child Q is included in the same manifold as the child. Removal of certain data points will satisfy the criterion, and Q will be included with this updated neighbourhood and the traversal and inclusion of"}, {"title": "IV. PERFORMANCE ANALYSIS AND COMPARATIVE STUDY", "content": ""}, {"title": "A. Experimental Setup and Performance Evaluation Metrics", "content": "The performance analysis and comparative study were carried out on an Intel i5 processor with a clock speed of 4.90 GHz and 16 gigabytes of RAM without a dedicated graphics processing unit. The effectiveness of the proposed method is examined using real-life datasets with diverse sample sizes, dimensions, and classes. Table IV contains the description of each dataset. The exponential smoothing factor a was set to 0.6 and the k-neighborhood value were employed for individual datasets with k = 25."}, {"title": "B. Performance Analysis and Sensitivity Study on Parameters", "content": "The proposed method consists of two components: the first focuses on segmenting non-intersecting manifolds, and the second on intersecting manifolds. Table V highlights the necessity of both parts, with non-intersecting manifold segmentation excelling for datasets Echocardiogram, Zoo, and Seeds, while intersecting manifold segmentation performs better for other datasets. Notably, ACEV consistently outperforms both individual approaches and highlights the necessity of incorporating both intersecting and non-intersecting manifold segmentation to achieve proper segmentation.\nThe efficacy of the ACEV depends on three key factors: k, a, and the learning percentage. To assess its performance across these parameters, Figure 5,"}, {"title": "C. Comparative Analysis", "content": "ARI and NMI scores have been computed for 18 state-of-the-art intersecting manifold segmentation algorithms across real-life datasets. The performance of comparative methods and ACEV are presented in Tables VI and VII. In these tables, the best-performing algorithm is highlighted in green for each dataset, while the second-best is colored"}, {"title": "V. DISCUSSION", "content": "An implicit assumption of the algorithm is that change in the tangent space should be smooth which may not be always satisfied. For example, if there is a concave part on a generally convex surface, then there is an abrupt change in the tangent direction as shown in Figure 7. The proposed algorithm using EMA will learn the structural change in the tangent space shown in red in Figure 7 by learning the local structures A, B, and C gradually and include them in the same manifold. This will depend on the neighbourhood construction because if it creates a neighbourhood like D then there will be a significant difference in the tangent space and it will not be labelled as the same manifold. So the dependency of the algorithm like other existing algorithms on the neighborhood construction is a limitation."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "The proposed two-step intersecting manifold segmentation mechanism (ACEV) learns the intrinsic dimension of individual manifolds and segments them from each other manifolds and demonstrates notable efficiency gain over existing methods with better time complexity. The unsupervised segmentation capability makes ACEV well-suited for practical applications in real-life scenarios. Along with these positivities, the limitation will be reduced for betterment in the future."}, {"title": "D. Algorithm and Time Complexity Analysis", "content": "The algorithm involves several steps: firstly, O(nDlog(n)) is needed to construct the tree-like structure of k-neighborhoods. Subsequently, O(n\u00b3) is required for labeling and segmenting non-intersecting manifolds. Finding the k nearest neighbor of the root data point takes O(klog(n)) time, followed by determining the principal axis, which consumes O(n\u00b3) time involving covariance matrix computation and eigenvector calculation. The angle between vectors is found in O(D) time, where D is the number of dimensions. Since, the algorithm runs recursively for each neighbor, the time required for each recursion is logkn. The overall time complexity of the ACEV is expressed as O(nD log(n)+n\u00b3+logk n(k log(n)+n\u00b3+D)). It's noteworthy that the complexity is expected to decrease over time as manifold determination reduces the number of data points, denoted as n."}]}