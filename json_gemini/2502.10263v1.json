{"title": "LARGE LANGUAGE MODELS AND SYNTHETIC DATA FOR MONITORING DATASET MENTIONS IN RESEARCH PAPERS", "authors": ["Aivin V. Solatorio", "Rafael Macalaba", "James Liounis"], "abstract": "Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.", "sections": [{"title": "INTRODUCTION", "content": "Datasets are fundamental to scientific research, underpinning empirical analysis, model development, and policy decisions (Mooney & Newton, 2012; Stacy et al., 2024). However, tracking how datasets are mentioned and used in academic literature remains a significant challenge (Potok, 2022; Silvello, 2018; Stacy et al., 2024). Unlike traditional bibliographic citations, dataset references are often embedded within text, described inconsistently, or omitted entirely, making it difficult to assess data reuse, transparency, and accessibility (Silvello, 2018; Buneman et al., 2020). The lack of systematic dataset tracking limits efforts to evaluate the impact of datasets, identify underutilized resources, and address data gaps in research (Piwowar & Vision, 2013; Buneman et al., 2021). Without structured metadata and comprehensive monitoring, researchers, funders, and policymakers struggle to make informed decisions about data investments, availability, and governance."}, {"title": "METHODS", "content": "This section presents our methodology for monitoring dataset mentions in research papers, with a focus on climate change literature. To address the scarcity of labeled training data for dataset extraction and classification, we leverage synthetic data generation to create a weakly supervised dataset. We then develop and fine-tune specialized models for classification and extraction, as outlined in Figure 1."}, {"title": "DATA", "content": "Climate change research relies on diverse datasets from various sources, including weather and oceanographic data, socioeconomic indicators, satellite imagery, land-use records, greenhouse gas emissions inventories, and agricultural and biodiversity surveys (Meehl et al., 2007; Camarillo-Naranjo et al., 2019; Hasegawa et al., 2022). A corpus encompassing climate change research, therefore, provides a robust test of the method's generalizability in identifying mentions of datasets across different types and domains.\nTo implement and evaluate our framework, we compile a collection of climate-related research papers from two primary sources: (1) the One Earth corpus, as identified in Sietsma et al. (2024), and (2) climate-related papers from the World Bank's Policy Research Working Papers (PRWP) series. The PRWP series encompasses a broad range of socio-economic development topics, including climate change.\nTo curate the PRWP subset, we use the World Bank's Documents and Reports platform (Bank, 2025), applying climate-related tags to filter relevant documents. Since full-text access is required for dataset mention extraction, we utilize the Semantic Scholar Paper Title Search API to identify open-access papers and retrieve their PDF links. This process yielded 2,123 papers with PDFs from the One Earth corpus and 582 papers from the PRWP collection. Additional information related to data acquisition and processing is described in Annex A.1."}, {"title": "WEAKLY SUPERVISED SYNTHETIC DATA GENERATION FOR PRE-FINE-TUNING", "content": "The lack of publicly available datasets designed to classify dataset mentions by purpose and citation quality presents a major challenge for developing reliable machine learning models for this task. While the Coleridge Initiative's Show US the Data (Potok, 2022) dataset provides examples of dataset mentions, it has two key limitations: (1) it lacks contextual information about dataset usage, making it difficult to determine how datasets are used in research, and (2) it focuses on a limited subset of datasets, introducing systematic bias and reducing generalizability to a wider range of research domains. This scarcity of high-quality training data poses a bottleneck in building scalable and domain-adaptive dataset extraction models.\nTo address this, we leverage LLMs for extractive, classification, and pseudo-reasoning tasks to generate a weakly supervised synthetic fine-tuning dataset from our research corpus. By using LLM-generated synthetic data, we significantly reduce reliance on manually annotated datasets, making it possible to expand dataset mention detection across diverse fields where labeled training data is scarce. To ensure the models get trained with high-quality training data, we strategically sample and manually annotate portions of the weakly supervised dataset, balancing automation with human validation."}, {"title": "ZERO-SHOT EXTRACTION AND CLASSIFICATION", "content": "To construct the weakly supervised pre-fine-tuning dataset, we process each page of research papers using an LLM-based extractor. This model identifies whether a dataset is mentioned and provides structured information, including:\n\u2022\nThe dataset name (if present).\n\u2022\nA classification of its citation quality (e.g., whether the dataset is explicitly named or only generically described)."}, {"title": "LLM-AS-A-JUDGE FOR QUALITY ASSESSMENT", "content": "A manual review of the extracted dataset mentions revealed frequent misclassifications, where non-dataset entities (e.g., institutions, reports, or software) were incorrectly labeled as datasets. To mitigate this issue, we integrate an LLM-as-a-Judge (Gu et al., 2025) mechanism, where a second LLM evaluates the extracted dataset mentions for accuracy and relevance. This secondary assessment improves classification reliability by filtering out false positives before the dataset is used for training.\nWhile this step improves precision, LLM-based judgments are still susceptible to subtle misclassifications, particularly when dataset names resemble organization names or when datasets are ambiguously referenced. Further refinement is needed to enhance classification robustness."}, {"title": "AUTONOMOUS REASONING FOR FILTERING", "content": "Further analysis showed that even after LLM-as-a-Judge validation, many non-dataset references persisted, including reports, conceptual frameworks, and organizations mistakenly classified as datasets. To improve classification accuracy, we introduce a reasoning agent that autonomously develops and executes a structured self-evaluation strategy. This agent systematically reassesses its conclusions, incorporating:\n\u2022\nA \"devil's advocate\" mechanism, challenging its own classifications by considering alternative interpretations.\n\u2022\nA hierarchical decision process, where the agent re-evaluates ambiguous cases by cross-referencing multiple extraction criteria.\n\u2022\nThe ability to override previous LLM-based judgments, provided it justifies any changes.\nThis dynamic self-correction process reduces reliance on implicit assumptions, enforcing stricter classification criteria and improving the reliability of dataset mention identification. The impact of this refinement was significant: out of the 37,225 mentions initially shortlisted by the LLM judge, the reasoning agent filtered out approximately 42%, leaving 21,408 dataset mentions as likely valid.\nAll LLM-based methods in this study use the OpenAI GPT-40-mini (2024-07-18) model. The prompts used for these processes are provided in Appendices A.4.1, A.4.2, and A.4.3, corresponding to the zero-shot extraction, LLM-as-a-Judge, and reasoning agent methods, respectively."}, {"title": "FINE-TUNING DATA", "content": "We sampled 1,000 pages from the output of the previous method and manually annotated them using Doccano (Nakayama et al., 2018) to remove any remaining false positives from the weakly supervised approach. This curated dataset serves as a foundational resource for training and evaluating downstream models specialized in extracting and classifying dataset mentions offline. We split the annotated data into three partitions for training (n=864), validation (n=40), and testing (n=20)."}, {"title": "FINE-TUNING MODELS", "content": "Detecting Data Use. To improve efficiency in dataset mention detection, we fine-tune both BERT (Devlin et al., 2019) and ModernBERT (Warner et al., 2024) models and compare their performance. The objective is to shift the filtering stage to a lightweight encoder-based model, reducing computational overhead when processing large volumes of text. Given that dataset mentions are typically sparse within documents, our approach ensures that only passages identified as containing dataset mentions by the encoder models are passed to the LLM for further processing. This hierarchical filtering strategy optimizes resource usage while maintaining extraction accuracy. Since dataset mentions are first filtered by an encoder model before being passed to the LLM for extraction, there is a possibility that some datasets may not be identified if the encoder misclassifies the passage containing the mention. To mitigate this, we need to optimize and select a model that possesses the highest recall.\nExtracting and Classifying Dataset Mentions. We fine-tune the Phi-3.5-mini instruct model (Abdin et al., 2024) using 16-rank LoRA (Hu et al., 2021) to extract structured information about dataset mentions in text. Our training follows a two-stage fine-tuning approach:\n\u2022\nPre-fine-tuning on synthetic data \u2013 We first pre-fine-tune the model using our large weakly supervised synthetic dataset for 10 epochs with an effective batch size = 16, learning rate = 2e-4, warmup ratio of 1%, and linear scheduler with decay of 0.01, saving a checkpoint every 100 steps and tracking the best-performing model based on validation loss.\n\u2022\nFine-tuning on high-quality annotated data After pre-fine-tuning, we load the best-performing checkpoint and further fine-tune the model on a smaller, manually annotated dataset with the same configurations as the pre-fine-tuning except for it training over 20 epochs, an effective batch size = 2, and learning rate = 3e-5, with checkpoints saved every 50 steps (and created at the end of each epoch) and the model corresponding to the best-performing evaluation loss is saved and loaded after training.\nThis two-stage fine-tuning strategy enables the model to establish a broad representation of dataset mentions using the diverse, albeit imperfect, synthetic corpus before refining its understanding on the manually curated dataset. By progressively adapting to more precise annotations, we expect the model to enhance its ability to distinguish between dataset references and similar entities, improving classification robustness. We perform an ablation study to assess the impact of each fine-tuning stage.\nTo evaluate our approach, we compare performance against NuExtract-v1.5 (Cripwell et al., 2024) and GLiNER-large-v2.1 (Zaratiana et al., 2023), two state-of-the-art models for named entity recognition and zero-shot structured data extraction, serving as baselines."}, {"title": "RESULTS", "content": "Our evaluation assesses both classification and extraction models for detecting dataset mentions in research papers. Table 1 summarizes the performance of the models across different training configurations and baselines."}, {"title": "IMPACT OF FINE-TUNING ON EXTRACTION PERFORMANCE", "content": "To measure the extraction accuracy, we use the Jaccard Similarity-based (Equation 1) F\u03b2 Score, a metric introduced in the Coleridge Initiative's data extraction Kaggle competition (Gupta, 2021). This method evaluates the overlap between predicted and ground-truth dataset mentions, allowing for partial matches rather than requiring exact string matches\u2014an important consideration given the variability in how datasets are referenced in text and how the models select which snippets constitute dataset names."}, {"title": "CLASSIFICATION MODEL PERFORMANCE", "content": "In addition to dataset extraction, we evaluate classification models that determine whether a text passage is likely to contain a dataset mention. This step serves as a filtering mechanism before passing text to the extraction model, improving computational efficiency.\n\u2022\nModernBERT-base achieves perfect precision and recall (100.0 F1-score), making it the preferred choice for classifying dataset mentions.\n\u2022\nBERT-uncased struggles with recall (50.0), leading to an Fl-score of 67.0. This suggests that while BERT can correctly classify some dataset mentions, it frequently fails to detect them, potentially impacting recall in the extraction pipeline.\n\u2022\nOne likely reason for ModernBERT's superior performance is its ability to process 2048-token contexts, which we were able to set, compared to BERT's 512-token limitation. The larger context size allows ModernBERT to better capture dataset mentions, especially when they appear in longer textual discussions."}, {"title": "DISCUSSION", "content": "Our key finding is that relying solely on small, manually curated datasets is suboptimal. However, these curated datasets become significantly more effective when used to fine-tune a synthetic-data-preconditioned LLM for the task. By adopting a two-stage fine-tuning strategy-first with synthetic data, then with curated data\u2014our approach provides a scalable solution for tracking dataset usage in research papers.\nThese results further highlight the benefits of pre-fine-tuning on synthetic data for improving dataset mention detection while maintaining high recall. Additionally, the performance of ModernBERT as a filtering model suggests that context length plays a critical role in classification accuracy, with larger context windows enhancing the model's ability to capture dataset references more effectively.\nIn Annex 2, we provide examples of extracted dataset mentions alongside empirical dataset references, demonstrating the model's effectiveness in identifying relevant datasets."}, {"title": "CONCLUSION", "content": "This paper introduced a machine learning framework for automating dataset mention detection in research papers, combining synthetic data generation with a two-stage fine-tuning approach. Our results demonstrate that pre-fine-tuning on a weakly supervised dataset before manual fine-tuning significantly improves extraction accuracy while maintaining high recall. The fine-tuned Phi-3-mini instruct model outperforms state-of-the-art baselines, highlighting the value of synthetic data in addressing training data scarcity and improving model generalization to unseen dataset mentions. Additionally, our classification experiments show that ModernBERT's larger context window enhances filtering efficiency, reducing computational overhead while ensuring high recall.\nBeyond improving dataset discoverability, this approach contributes to scalable research monitoring and metadata generation, supporting open science initiatives and data-driven decision-making. Future work will include building larger manually annotated data and exploring the hypothesis of potential diminishing returns when pre-fine-tuning with synthetic data is employed. We will also explore improvements to generating the synthetic data as well as adaptive fine-tuning strategies to further refine extraction accuracy and expand the dataset to cover a broader range of research domains, improving the generalizability of dataset mention detection."}, {"title": "APPENDIX", "content": null}, {"title": "DATA SOURCES", "content": "The datasets/corpus used in this study are derived from:\n\u2022\nOne Earth corpus source: The dataset where the One Earth corpus was de-rived from was obtained from Zenodo (https://zenodo.org/records/7893023).\nThe dataset was introduced in (Sietsma et al., 2024), which provides context andmethodological details regarding its creation.\n\u2022\nPRWP corpus source: To identify Policy Research Working Papers (PRWPs)relevant to climate change, a structured filtering approach was applied using theWorld Bank Documents and Reports portal (source).\n\u2013\nThe query parameters in the source URL indicate the applied filters, ensuring that only documents meeting specific criteria were selected. The filteringcriteria included:\n* Document Type: Policy Research Working Papers (PRWP)\n* Query: climate change\n* Language: English\n* Selected Topics:\nClimate Change and Agriculture\nAdaptation to Climate Change\nClimate Change and Environment\nClimate Change Impacts\nClimate Change Mitigation and Greenhouse Gases\nClimate Change and Health\nClimate Change Economics\nInvestment and Investment Climate\nClimate Change Policy and Regulation\nClimate and Meteorology\nScience of Climate Change\nSocial Aspects of Climate Change\nThe process for building the corpus involved ensuring a PDF is available for the paper.This requires the following approach: metadata retrieval, validating for open access, anddownloading of the PDFs."}, {"title": "TEMPLATES AND CLASSIFICATIONS", "content": "Below is the JSON template used for extracting data mentions using the NuExtract-v1.5model."}, {"title": "PROMPTS", "content": null}, {"title": "ZERO-SHOT EXTRACTION AND CLASSIFICATION PROMPT", "content": "Listing 2 System prompt used to extract the initial structured data containing likely datamentions from a given text [1/3].\nYou are an expert in extracting and categorizing dataset mentions fromresearch papers and policy documents. Your task is to **identify andextract all valid dataset mentions**, ensuring they are correctlyclassified based on naming specificity, context, and relevance.\n### **What Qualifies as a Dataset?**\nA dataset is a structured collection of data used for empiricalresearch, analysis, or policy-making. Examples include:\n**Surveys & Census Data** (e.g., LSMS, DHS, national census records)\n**Indicators & Indexes** (e.g., HDI, GFSI, WDI, ND-GAIN, \u0395\u03a1\u0399)\n**Geospatial & Environmental Data** (e.g., OpenStreetMap, Sentinel-2imagery)\n**Economic & Trade Data** (e.g., UN Comtrade, Balance of PaymentsStatistics)\n**Health & Public Safety Data** (e.g., epidemiological surveillance,crime reports)\n**Time-Series & Energy Data** (e.g., climate projections, electricitydemand records)\n**Transport & Mobility Data** (e.g., road accident statistics, smartcity traffic flow)\n**Other emerging dataset types** as identified in the text.\n**Important:**\nIf the dataset does not fit into the examples above, infer the **mostappropriate category** from the context and **create a new`\"data_type\"` if necessary**.\n### **What Should NOT Be Extracted?**\nDo **not** extract mentions that do not clearly refer to a dataset,including, but not limited to:\n1.\n**Organizations & Institutions** (e.g., WHO, IMF, UNDP, \"World Bankdata\" unless it explicitly refers to a dataset)\n2.\n**Reports & Policy Documents** (e.g., \"Fiscal Monitor by the IMF\",\"IEA Energy Report\"; only extract if the dataset itself is referenced)\n3.\n**Generic Mentions of Data** (e.g., \"various sources\", \"surveyresults from multiple institutions\")\n4.\n**Economic Models & Policy Frameworks** (e.g., \"GDP growthprojections\", \"macroeconomic forecasts\")\n5.\n**Legislation & Agreements** (e.g., \"Paris Agreement\", \"General DataProtection Regulation\")"}, {"title": "LLM-AS-A-JUDGE PROMPT", "content": "Listing 5 System prompt used to characterize the LLM-as-a-Judge agent to assessthe quality of the first stage of structured data generation [1/2].\nYou are an expert in dataset validation. Your task is to assess whethereach dataset mention is **valid, invalid, or requires clarification**,ensuring correctness and consistency based on the dataset's **empiricalcontext**.\n### **Dataset Validation Criteria**\nA dataset is **valid** if:\n1.\n**It is structured**|collected systematically for research, policy,or administrative purposes.\n2.\n**It is reproducible**|meaning it consists of collected recordsrather than being derived purely from computations or models.\n**Always Valid Datasets:**\nGovernment statistical and geospatial datasets (e.g., census,official land records).\nOfficial surveys, administrative records, economic transaction data,and scientific research datasets.\n**Invalid Datasets:**\nSet as invalid all `\"raw_name\"` that belong under the followingclasses.\nDerived indicators or computational constructs (e.g., \"wealth score\",\"mine dummy\", \"district total production\").\nStandalone statistical metrics without a clear underlying dataset(e.g., \"average income growth rate\" without source data).\nGeneral organizations, reports, or methodologies (e.g., \"World Bank\",\"UNDP Report\", \"machine learning model\").\n**Uncertain Cases:**\nIf a dataset is **vaguely named but potentially valid**, set it asvalid but return: `\"Potentially valid|needs dataset nameconfirmation.\"`\nIf a dataset reference is **too generic** (e.g., `\"time-varying dataon production\"`), set it as valid but return: `\"Needsclarification|dataset name is too generic.\"`"}, {"title": "REASONING AGENT PROMPT", "content": "Listing 7 System prompt used to characterize the reasoning agent.\nYour task is to review a structured user input that may mention adataset in a text. Please take your time.\nCarefully analyze what the text in the `mentioned_in` field explicitlymeans and in what context the`raw_name` is discussed. Never infer,imply, or assume, so you must exclusively rely on the text as facts. Ifthere are multiple datasets, do the assessment individually.\nPlan a strategy to ensure you can maximize the chances of correctlyjudging and classifying whether the provided input:\nClearly, the `raw_name` falls under the concept of a data/dataset andnot by extension or implicitly.\nWhether the raw_name is actually in the mentioned_in`.\nWhether the harmonized_name (if present) is actually in the`mentioned_in`. If not found, remove it from the output.\nThe `raw_name` is `properly_named`(e.g., DHS, LSMS, etc.),`descriptive_but_unnamed`(administrative school records in Ghana for2020) or `vague_generic` (a survey data). Any of these are valid datamentions. To be sure, elaborate how you interpret these classes and usethat for classifying.\nThe context concerning usage of the dataset is mentioned: is it`primary`, `supporting`, or `background`.\nThen, write down your strategy.\nAfter you write down your strategy, synthesize it to develop a rubricof what qualifies as a dataset, which you must use to base yourjudgment.\nIncorporate a devil's advocate review as part of your strategy. If thereview shows inconsistency, update accordingly. Do not reason based onassumption, inference, or implicit thinking. Relationships do notcount as a dataset; for example, the producer is not a dataset.\nExecute the strategy, **step by step**, and write an analysis of howyou interpret the `raw_name` in the context of the `mentioned_in`.\nIf your analysis results in the `raw_name` being a dataset, set the`valid field to `true`, otherwise, set it to `false`. In both cases,return the result of your analysis focusing on the `raw_name` in the`reason field. If it is invalid, set the `specificity` and `context`to null.\nALWAYS WRITE A DEVIL'S ADVOCATE REVIEW AFTER THE ANALYSIS BEFORECONCLUDING.\nAfter you write your analysis, your output must repeat the input withthe `specificity`, `context`, `valid` and `invalid_reason` valuesreplaced accordingly in the same level as the corresponding `raw_name`.IMPORTANT: the final output must be between these tags<OUTPUTDATA>```json<the output must be here>```</OUTPUTDATA>"}, {"title": "REASONING AGENT EXAMPLE", "content": null}]}