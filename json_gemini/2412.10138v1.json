{"title": "ROUTE: ROBUST MULTITASK TUNING AND COLLABORATION FOR TEXT-TO-SQL", "authors": ["Yang Qin", "Chao Chen", "Zhihang Fu", "Ze Chen", "Dezhong Peng", "Peng Hu", "Jieping Ye"], "abstract": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by large language models (LLMs), the latest state-of-the-art techniques are still trapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which limits their applicability in open scenarios. To address this challenge, we propose a novel Robust mUltitask Tuning and collaboration method (ROUTE) to improve the comprehensive capabilities of open-source LLMs for Text2SQL, thereby providing a more practical solution. Our approach begins with multi-task supervised fine-tuning (SFT) using various synthetic training data related to SQL generation. Unlike existing SFT-based Text2SQL methods, we introduced several additional SFT tasks, including schema linking, noise correction, and continuation writing. Engaging in a variety of SQL generation tasks enhances the model's understanding of SQL syntax and improves its ability to generate high-quality SQL queries. Additionally, inspired by the collaborative modes of LLM agents, we introduce a Multitask Collaboration Prompting (MCP) strategy. This strategy leverages collaboration across several SQL-related tasks to reduce hallucinations during SQL generation, thereby maximizing the potential of enhancing Text2SQL performance through explicit multitask capabilities. Extensive experiments and in-depth analyses have been performed on eight open-source LLMs and five widely-used benchmarks. The results demonstrate that our proposal outperforms the latest Text2SQL methods and yields promising performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Text2SQL has emerged as a popular and practical technology for question answering based on large-scale databases, serving as a crucial link between natural language and database systems (Zhang et al., 2024). Recently, Large Language Models (LLMs) have proven to be an effective solution in Text2SQL (Pourreza & Rafiei, 2024a). Unlike pioneer works (Katsogiannis-Meimarakis & Koutrika, 2021; Xiao et al., 2016; Bogin et al., 2019; Li et al., 2023a;b; Gu et al., 2023), LLM-based methods (Pourreza & Rafiei, 2024a; Gao et al., 2024a; Wang et al., 2023) primarily develop effective prompt engineering techniques, e.g., in-context learning, to motivate LLMs to understand the database schema and generate accurate SQL query. However, accurately aligning entities in natural language questions and databases for SQL generation remains challenging, especially when dealing with complex database schema or semantically complex questions (Li et al., 2024c).\nTo address these challenging scenarios, recent efforts (Lee et al., 2024; Talaei et al., 2024; Li et al., 2024d) have developed various pipelines that enhance the entire SQL generation process and reduce potential error risks. These improvements include techniques such as Schema Linking (Pourreza & Rafiei, 2024a), Self-correction (Wang et al., 2023), Chain-of-Thought (CoT) (Tai et al., 2023), Reliability Voting (Li et al., 2024d), etc. Although these methods have achieved promising results, they often rely on closed-source models (e.g., GPT-4/40 (Achiam et al., 2023)), which can raise potential privacy risks and incur significant overheads when deploying LLMs in practical scenarios. Moreover, while these techniques perform well on GPT-4 or other large-sized LLMs, their effectiveness"}, {"title": "2 RELATED WORK", "content": "Text-to-SQL (Li et al., 2023b; Gao et al., 2024a; Pourreza & Rafiei, 2024a) aims to understand user intent and convert natural language questions into SQL queries. Existing Text2SQL methods can be roughly categorized into Pre-LLM and LLM-based methods. Pre-LLM approaches mainly exploit the rule modeling (Katsogiannis-Meimarakis & Koutrika, 2021), specialized neural networks (Xiao et al., 2016; Bogin et al., 2019) and pre-trained models (Fu et al., 2023; Li et al., 2023a;b; Gu et al., 2023) to parse and improve SQL generation. The latter has made significant progress recently thanks to LLM's unique emergent abilities in developing Text2SQL solutions, including prompt engineering (Rajkumar et al., 2022; Gao et al., 2024a; Pourreza et al., 2024; Maamari et al., 2024) and LLM fine-tuning/pre-training (Li et al., 2024b; Pourreza & Rafiei, 2024b; Yang et al., 2024b). In this paper, we mainly focus on the LLM-based methods.\nPrompt Engineering. To unleash the potential of LLMs in the Text2SQL task, a straightforward attempt is to design effective prompting techniques (Liu et al., 2023; Pourreza & Rafiei, 2024a; Gao et al., 2024a; Wang et al., 2023; Lee et al., 2024; Gao et al., 2024b) to guide LLMs. Recent approaches mainly focus on leveraging closed-source models (e.g., ChatGPT and GPT-4) to design innovative instructions or pipelines through the techniques of chain-of-thought (Zhang et al.), LLMs' agents (Wang et al., 2023), question/task decomposition (Pourreza & Rafiei, 2024a), self-debugging (Wang et al., 2023; Chen et al., 2023), schema linking (Li et al., 2024b), few-shot ex-ample selection (Gao et al., 2024a), etc. For example, DIN-SQL (Pourreza & Rafiei, 2024a) adopts"}, {"title": "3 METHODOLOGY", "content": "In this section, we will delve into our proposed method, Robust Multitask Tuning (ROUTE), designed to enhance SQL generation capabilities. As shown in Figure 1, ROUTE consists of two core stages, i.e., Multitask Supervised Fine-Tuning (MSFT) and Multitask Collaboration Prompting (MCP) for SQL Generation. In the following, we first show the notations and definitions used in this work in Section 3.1, then introduce the details of our ROUTE in Sections 3.2 and 3.3 for Text2SQL."}, {"title": "3.1 \u039d\u039f\u03a4\u0399ONS AND PROBLEM FORMULATION", "content": "Given an instructional Text2SQL dataset D = {(di, qi, Si)}=1, where di is a SQL database, qi is a natural language question that may be accompanied by a question hint, and si is a ground-truth SQL query, the purpose of Text2SQL is to exploit an LLM(M) to generate a SQL query s based on a prompt constructed by di and qi, and the execution results of predicted SQL query are consistent with those of the ground-truth SQL query si. In this paper, in addition to the standard Text2SQL task, our approach incorporates three additional SQL-related tasks in the SFT and SQL generation stages, including schema linking, noise correction, and continuation writing, as shown in Figure 2. All these tasks are defined as follows:"}, {"title": "3.2 MULTITASK SUPERVISED FINE-TUNING", "content": "Existing SFT-based methods (Li et al., 2024b; Yang et al., 2024b) mainly focus on the task of Text2SQL to improve final performance. However, previous prompting-based methods have shown that SQL-related tasks can also effectively improve performance. Unfortunately, due to the lack of specialized pre-training or instruction fine-tuning for these related tasks, it is difficult for open-source LLMs to complete these tasks with high accuracy. In this section, we present the details of our Multitask Supervised Fine-tuning (MSFT) on the dataset D that combines training sets from two wide-used cross-domain datasets, i.e., SPIDER (Yu et al., 2018) and BIRD (Li et al., 2024c).\nNoisy correspondence filtering. Recent empirical evidence (Wretblad et al., 2024; Wretblad & Gordh Riseby, 2024) indicates that even carefully annotated Text2SQL datasets exhibit semantic inconsistencies between the given question and the ground-truth SQL query, as shown in Figure 3, which we call noisy correspondences. It is widely recognized that noise remains a primary factor"}, {"title": "3.3 MULTITASK COLLABORATION PROMPTING", "content": "To fully utilize these specialized capabilities of LLMs, we develop a Multitask Collaboration Prompting (MCP) method to reduce potential risks in schema linking errors or SQL clause errors in SQL generation as shown in Figure 1-(b). Our MCP consists of three core steps corresponding to the defined task in Section 3.1.\nFirst, to streamline the redundant information in the prompt, we propose an enhanced schema linking strategy, which leverages the schema linking capability of LLMs to identify tables and columns relevant to solving the user question, complemented by the pseudo-SQL query (Li et al., 2024d) to simplify the database. Given a database di and a user question qi, the pseudo-SQL refers to the intermediate SQL query generated in advance using the complete schema information, i.e., M(ot(di, qi), di). The final simplified database d\u2081 can be represented as:\ndi = M(os(di, qi)) \u3143 f(M(ot(di, qi), di),                                                                           (2)\nwhere f is the parsing function to extract the tables and columns from SQL queries through fuzzy matching and \u3143 denotes the operation defined for merging tables and columns. Such a merger operation helps to decrease the likelihood of missing potentially related entities (tables or columns) and maximize the information relevant to the question, thereby reducing the potential risks in schema linking errors. After obtaining the tables and columns by schema linking, we conduct SQL generation by s = M(ot(di, qi)) to obtain an intermediate SQL query. Then, to explicitly identify the incorrect SQL queries, we utilize LLMs to check them by combing \u03c3\u03b7(di, qi, s*) with the exception information ei raised by a SQL executor (SQLer(di, s*)), i.e., M(on(di, qi, s*, ei)). Note that ei is empty if the SQL query is executed successfully. If the LLM shows that s cannot answer qi accurately, we take the corrected SQL \u0161\u012b and check \u0161\u012f using the SQL executor (SQLer(di, \u0161i)). If it executes successfully, replace s* with Si.\nFinally, we present a novel strategy stemming from our observations of LLMs on SQL queries after continuation writing. We discovered that continuation writing of a given SQL piece by LLMs can enhance the quality of the generated SQL. In view of this finding, we employ LLMs to continue writing truncated SQL queries to refine and improve complex SQL queries. To achieve this, we categorize the difficulty of the generated SQL query as follows: Simple: involving only one table, Medium: involving two tables, and Hard: involving more than two tables. For ease of presentation, we define a difficulty evaluation function h(si, di) \u2208 {1,2,3}, where 1 ~ 3 correspond to three hardness levels. For all Hard SQL queries, we conduct continuation writing on the incomplete SQL queries started with 'SELECT' for further refinement. To enhance the clarity of our MCP, we offer a detailed explanation of the process in Appendix A.9."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENT SETTINGS", "content": "Benchmarks. To evaluate our method, we conduct extensive experiments on five benchmarks to verify the effectiveness of our method. These benchmark includes two widely-used cross-domain benchmarks, i.e., SPIDER (Yu et al., 2018) and BIRD (Li et al., 2024c), and three robust benchmarks derived from SPIDER, i.e., SPIDER-SYN (Gan et al., 2021a), SPIDER-DK (Gan et al., 2021b), and SPIDER-Realistic (Deng et al., 2020). SPIDER consists of 7,000 Text-SQL pairs in the training set, 1,034 pairs in the development set, and 2,147 pairs in the test set, which covers nearly 200 databases and 138 domains. BIRD is a recently proposed benchmark including 9,428, 1,534, and 1,789 pairs in training, development, and test sets, respectively. Compared with SPIDER, BIRD contains more complex databases, more difficult questions, and external knowledge, making it more challenging. For the derived variants, SPIDER-SYN replaces some keywords in the questions in the SPIDER dev set with synonyms. SPIDER-DK introduces some domain knowledge reasoning challenges, while SPIDER-Realistic removes explicit mentions of column names in the SPIDER development set. These variants all simulate real-world scenarios for a more comprehensive evaluation."}, {"title": "4.2 COMPARISON RESULTS", "content": "Results on SPIDER and BIRD. In Table 1, we report the performance of our method and baselines on the SPIDER development set, test set, and BIRD development set. Due to time limitations, we are unable to provide the results of our ROUTE and MCP on the BIRD test set. From the results, we can see that the prompting-based baselines achieve promising performance with the help of GPT-4, however, their effectiveness is obviously limited in the small-sized open-source LLMs. In contrast, our MCP effectively improves the performance of these small-sized LLMs, e.g., MCP brings more than 5% absolute performance improvement to Qwen2.5-7B on the SPIDER development set. For the fine-tuning group, our ROUTE has achieved the best results in most metrics among the fine-tuning-based methods. Especially on the BIRD dataset, our method (14B) surpasses some existing prompting-based methods (e.g., DIN-SQL, DAIL-SQL, and MAC-SQL) with an EX score of 60.8, greatly narrowing the gap with existing methods using GPT-4.\nResults on SPIDER-variants. Table 5 shows the performance on the benchmarks derived from SPIDER. Except for SPIDER-SYN, our MSFT can slightly improve performance on other variant benchmarks. Despite this, our ROUTE can perform better by inspiring the multi-task capabilities of LLMs with the help of MCP, thus improving performance. In addition, MCP can still have performance gains for the SFT model, demonstrating the necessity of conducting multitask collaboration."}, {"title": "4.3 ABLATION AND ANALYTIC STUDIES", "content": "In this section, we first conduct a comprehensive ablation study on all benchmarks to explore the impact of each key component, verifying the effectiveness of our method. Furthermore, we conduct in-depth analyses to explore the transferability and upper-bound performance of our approach. If not stated, all results are performed on Llama3-8B-Instruct and evaluated on the SPIDER and BIRD development sets using the EX score."}, {"title": "5 LIMITATIONS AND BROADER IMPACTS", "content": "Although our exploration has achieved promising performance, we have to acknowledge the following limitations. First, our solution may bring additional reasoning costs. Although some existing efficient inference frameworks can alleviate this (e.g., VLLM (Kwon et al., 2023)), we still encourage the exploration of more efficient multitask collaboration modes. Second, from the study on the upper bound performance, there is still a large gap between the performance of our method and the upper bound ones, which encourages further study on synthesis data and SFT paradigms of SQL-related tasks to understand and mitigate the biases and risks potentially brought by multi-task collaboration. Finally, we provide the ethics and reproducibility statement in Appendix A.13 to avoid the potential impact and risk."}, {"title": "6 CONCLUSION", "content": "In this paper, we study and propose a robust multitask tuning and collaboration method named ROUTE to stimulate the potential of open-source LLM in Text2SQL, narrowing the gap with existing solutions based on closed-source LLMs, such as GPT-4. Our method minimizes the risk of hallucination in SQL generation by explicitly learning multiple SQL-related tasks and conducting multitask collaboration. We apply our approach to recent LLMs to demonstrate its effectiveness and superiority on multiple benchmarks. The results show that our method has satisfactory transferability and achieves promising execution accuracy on Text2SQL. In the future, we plan to explore more SQL-relevant tasks, larger LLMs, and more efficient collaboration frameworks for robust Text2SQL."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 PERFORMANCE COMPARISON WITH DIFFERENT HARDNESS", "content": "To explore a more fine-grained performance comparison, we follow previous works (Pourreza &\nRafiei, 2024a; Gao et al., 2024a) and report the EX scores on development sets of SPIDER and\nBIRD. From the results shown in Tables 8 and 9, the conclusion of our ROUTE\u2019s overall performance\nis consistent with that of its fine-grained performance, which verifies the superiority of our method."}, {"title": "A.2 THE RESULTS STUDIED ON TRANSFERABILITY", "content": "In this appendix, we provide detailed results of a transferability study in Table 10. Furthermore,\nwe compare our methods with those using the same base LLMs, i.e., SQL-Llama7B (Wang et al.,\n2023), DTS-SQL (Pourreza & Rafiei, 2024b) and SENSE (Yang et al., 2024b). The results show that\nour method has good scalability on various LLMs, whether the base or code-specific ones. This is\nsufficient to verify the superiority and generalization of our ROUTE."}, {"title": "A.3 MORE COMPARISONS WITH RECENT WORKS", "content": "In this appendix, we provide more comparisons with recent works in Appendix A.3, including\nCHASE-SQL (Pourreza et al., 2024), Distillery (Maamari et al., 2024), and CHESS (Talaei et al.,"}, {"title": "A.4 PERFORMANCE ON SCHEMA LINKING", "content": "In this appendix, like (Pourreza & Rafiei, 2024b), we report the performance (Recall and Preci-\nsion) of our schema linking module. The results shown in Table 12 demonstartes several important\nobservations:\n\u2022 After MSFT, the schema linking capability is significantly improved, especially in terms of\nprecision scores.\n\u2022 A higher Recall score generally leads to improved EX performance due to minor information\nloss.\n\u2022 While simplifying the database schema is necessary, ensuring its completeness is more crucial\nfor achieving enhanced performance."}, {"title": "A.5 THE RESULTS ON DR. SPIDER", "content": "In this appendix, we conduct experiments on Dr.Spider (Chang et al., 2023) to have a clearer and\nmore comprehensive understanding of the advantages of our ROUTE. Dr.Spider includes 17 per-\nturbation variants that can comprehensively measure the effectiveness and robustness. The specific\nexperimental results are shown in Tables 13 and 14, including the study on MSFT and on each\ncomponent. In the table, Avg.DB means the average results on DB perturbation test sets, Avg.NLQ\nmeans the average results on NLQ perturbation test sets, Avg.SQL means the average results on SQL\nperturbation test sets, and Avg.all means the average results on all test sets. From the results, the con-\nclusions are consistent with those on SPIDER and BIRD and each component brought performance\nimprovement, which shows that each task has an indispensable contribution to the performance.\nThis further verifies the advantages and robustness of ROUTE."}, {"title": "A.6 THE IMPACT OF SINGLE TASK SFT", "content": "In this appendix, we conduct additional experiments to explore the impact of single/triple-task SFT\nand MSFT on each task. For Text-to-SQL (TS), we report zero-shot EX results on the SPIDER"}, {"title": "A.7 DETAILS OF NOISY CORRESPONDENCE FILTERING", "content": "In this appendix, we mainly clarify some details in noisy correspondence filtering. To obtain more\ndiversity negative examples, we artificially introduce some errors to the ground-truth SQL queries.\nBased on some recent works exploring, we focus on five types of errors, schema linking errors,\nnesting errors, GROUP BY errors, JOIN errors, and symbol errors. More specifically,\n\u2022 Schema linking error refers to the wrong table and column names in the SQL query. We ran-\ndomly introduce such errors by making typos and synonym substitutions for table or column\nnames.\n\u2022 Nesting errors refer to the need to use nested or set operations but not using them (e.g., UNION,\nUNION ALL, INTERSECT and EXCEPT). We destroy the SQL queries by randomly removing\nthe sub-SQLs before or after these keywords.\n\u2022 JOIN errors commonly occur in that the SQL queries using JOIN operation focus on the wrong\ntable or column names. We introduce such errors by randomly replacing the table or column\nnames.\n\u2022 GROUP BY errors commonly occur in that the SQL queries using GROUP BY operation focus\non the wrong column names. We introduce such errors by randomly replacing the column\nnames after GROUP BY.\n\u2022 Symbol errors are some minor errors such as incorrect keywords, missing commas, missing\nparentheses, and confusing function names (such as COUNT, MAX, MIN, etc.).\nTo obtain artificially constructed negative examples, we select a certain type of error according to a\ncertain probability and introduce it into the ground-truth SQL queries. If the SQL query does not\nmeet the type of error introduced, such as no GROUP BY operation, we will randomly select other\ntypes of errors to continue to construct negative examples."}, {"title": "A.8 INNOVATION AND DIFFERENCE DISCUSSION", "content": "In this appendix, we provide more discussion to further elaborate on the innovations of our ROUTE\nand highlight how it differs from existing methods. The valuable insights and significant contribu-\ntions of our work can be summarized as follows:\n\u2022 ROUTE is among the pioneering frameworks under the context of LLMs that explores multi-\ntask tuning and collaborative prompting to improve Text2SQL performance.\n\u2022 We have exhaustively introduced and defined three important tasks in SQL generation, demon-\nstarting that multi-task tuning and collaborative prompting in Schema Linking, Noise Correc-\ntion and Continuation Writing significantly improve SQL generation accuracy. The additionally\nintroduced SQL-related tasks are well integrated during both the training and inference phases.\n\u2022 We have achieved state-of-the-art performance in 7B/14B-sized LLMs on both the widely-\nrecognized SPIDER and BIRD benchmarks, with verified generalization and transferability\nacross various cross-domains benchmarks and LLMs.\nWe highlight the key similarities and differences between our ROUTE and other related works as\nfollows:\n1. Multi-task Supervised Fine-tuning (MSFT): The method most comparable to our ROUTE\napproach is MAC-SQL (Wang et al., 2023), which introduces multiple task agents and\ndemonstrates the effectiveness of fine-tuning through the use of multi-agent instructions on\nCodeLlama-7B (Roziere et al., 2023).\n(a) First, the defined tasks in MSFT for ROUTE differ from those in MAC-SQL. We have\nintroduced a new continuation writing (CW) task to further refine the challenging SQL\nqueries. As demonstrated in Table 15, CW holds significant potential for SQL generation.\nOn SPIDER development set, exploring CW task is able to achieve an impressive EX\nscore of 91.1.\n(b) Second, in MAC-SQL, generating instruction data for SFT involves decomposing com-\nplex questions into multiple sub-questions and constructing corresponding answers. In\ncontrast, our approach, beyond noise correction, allows for the synthesis of SFT data for\nvarious tasks using programming functions. This makes our method more practical for\nlarge-scale multi-task data synthesis for MSFT.\n(c) Third, in terms of performance, our ROUTE is significantly outperforms MAC-SQL based\non the open-source LLM of CodeLlama-7B. The detailed results are presented in Table 10.\n2. SQL-Data Synthesis: Our ROUTE involves the synthesis of SQL-related instruction-following\ndata, which shares similarities with the recent work SENSE (Yang et al., 2024b).\n(a) First, compared to SENSE, the data synthesis pipeline of ROUTE encompasses not only\nText2SQL but also multiply other SQL-related tasks. Our approach focuses on utilizing\nexisting data to synthesize multi-task SFT data, thereby enhancing the capabilities of open-\nsource LLMs to handle various SQL-related tasks. In contrast, SENSE mainly focused on\nSQL generation task, leveraging strong LLMs to increase the diversity of SQL generation\ntraining set and synthesize preference data.\n(b) Besides, our ROUTE achieves comparable performance to SENSE on the SPIDER devel-\nopment set and better performance on the BIRD development set, as shown in Table 10.\n3. Multi-tasking Collaboration: To exploit the potential of multi-task capabilities, we propose a\nmulti-task collaborative prompting strategy (MCP) to improve the final SQL generation. The\nmost similar works are DIN-SQL (Pourreza & Rafiei, 2024a) and MAC-SQL (Wang et al.,\n2023), which both aim to reduce the complexity of the Text2SQL and improve the final perfor-\nmance via self-correction.\n(a) First, compared to them, our MCP efficiently integrates multiple tasks using concise\nprompts across all tasks, which makes it more effective in small-sized LLMs that strug-\ngle with comprehending complex instructions. As shown in the results of Table 1, the\neffectiveness of MAC-SQL and DIN-SQL is constrained by the limited capacity of small-\nsized LLMs to comprehend complex instructions, while our MCP can achieve better and\nimpressive performance."}, {"title": "A.9 ALGORITHM OF MCP", "content": "In this appendix, to make our MCP clearer, we describe the pipeline in detail in Algorithm 1."}, {"title": "A.10 STATISTICS OF MSFT DATA", "content": "Our MSFT data includes the main SFT data for Text2SQL and the SFT data for three other tasks.\nThe former consists of the SPIDER and BIRD training sets after noisy correspondence filtering (898\npairs removed), with a total of 15,530 pairs. The SFT data for other tasks are randomly selected\nfrom the filtered dataset and 10,000 data pairs are constructed. That is to say, our MSFT data has a\ntotal of 45,530 data pairs."}, {"title": "A.11 PROMPT TEMPLATES", "content": "In this appendix, we provide the prompts of all tasks used in our pipeline as shown in Figures 5\nto 8. Among these prompts, only the BIRD dataset provides question hints. Meanwhile, other infor-\nmation, such as few-shot examples and execution exceptions, only provides for non-SFT models to\nguide the output. The prompt used for noisy correspondence filtering is presented in Figure 9, which\nis similar to the NC\u2019s instructions (Figure 6) but does not include the terms of execution exception\nto avoid LLMs focusing only on the wrong pattern instead of the discriminative pattern."}, {"title": "A.12 QUALITATIVE EXAMPLES OF NOISY PAIRS", "content": "In Listings 1 and 2, we provide more identified noisy pairs in SPIDER and BIRD training sets. To\nshow the error visually, we mainly provide noisy examples with obvious semantic inconsistencies."}, {"title": "A.13 ETHICS AND REPRODUCIBILITY STATEMENT", "content": "This paper aims to explore the possibility of improving Text2SQL performance by multitask collab-\noration based on large language models. For this purpose, we use some open-source LLMs as our\ninvestigated models to propose solutions, which may pose potential commercial risks. We pledge\nthat we will only conduct academic research on these LLMs to verify the effectiveness of our pro-\nposed solution and will not use them for other purposes. In addition, to ensure the reproducibility of\nour research, we have made several efforts to ensure that our solution is convincing. First, we pro-\nvide details in Section 3.2 to clarify the data construction pipeline used for MSFT. In addition, the\nused prompt templates are provided in Appendix A.11 to ensure reproducibility. During the training\nstage, we utilized a unified fine-tuning framework, i.e., Llama-Factory, and detailed the settings of\ncore parameters in Section 4.1. During the inference stage, we recommend setting a very low tem-\nperature of 0.01 to ensure the reproducibility of LLMs. Finally, we commit to releasing code and\nsynthetic data for reproducibility, thus further advancing the Text2SQL community."}]}