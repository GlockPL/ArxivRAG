{"title": "Flow-based Domain Randomization for Learning and Sequencing Robotic Skills", "authors": ["Aidan Curtis", "Eric Li", "Michael Noseworthy", "Nishad Gothoskar", "Sachin Chitta", "Hui Li", "Leslie Pack Kaelbling", "Nicole Carey"], "abstract": "Domain randomization in reinforcement learning\nis an established technique for increasing the ro-\nbustness of control policies trained in simulation.\nBy randomizing environment properties during\ntraining, the learned policy can become robust to\nuncertainties along the randomized dimensions.\nWhile the environment distribution is typically\nspecified by hand, in this paper we investigate\nautomatically discovering a sampling distribution\nvia entropy-regularized reward maximization of\na normalizing-flow-based neural sampling distri-\nbution. We show that this architecture is more\nflexible and provides greater robustness than ex-\nisting approaches that learn simpler, parameter-\nized sampling distributions, as demonstrated in\nsix simulated and one real-world robotics domain.\nLastly, we explore how these learned sampling\ndistributions-combined with a privileged value\nfunction-can be used for out-of-distribution de-\ntection in an uncertainty-aware multi-step manip-\nulation planner.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) has proven to be a useful tool\nin robotics for learning control or action policies for tasks\nand systems which are highly variable and/or analytically\nintractable (Luo & Li, 2021; Zhu et al., 2020; Schoettler\net al., 2020). However, RL approaches can be inefficient,\ninvolving slow, minimally parallelized, and potentially un-\nsafe data-gathering processes when performed in real en-\nvironments (Kober et al., 2013). Learning in simulation\neliminates some of these problems, but introduces new is-\nsues in the form of discrepancies between the training and\nreal-world environments (Valassakis et al., 2020).\nSuccessful RL from simulation hence requires efficient and\naccurate models of both robot and environment during the\ntraining process. But even with highly accurate geometric\nand dynamic simulators, the system can still be only con-\nsidered partially observable (Kober et al., 2013)-material\nqualities, inertial properties, perception noise, contact and\nforce sensor noise, manufacturing deviations and tolerances,\nand imprecision in robot calibration all add uncertainty to\nthe model.\nTo improve the robustness of learned policies against sim-to-\nreal discrepancies, it is common to employ domain random-\nization, varying the large set of environmental parameters\ninherent to a task according to a given underlying distribu-\ntion (Muratore et al., 2019). In this way, policies are trained\nto maximize their overall performance over a diverse set\nof models. These sampling distributions are typically con-\nstructed manually with Gaussian or uniform distributions\non individual parameters with hand-selected variances and\nbounds. However, choosing appropriate distributions for\neach of the domain randomization parameters remains a\ndelicate process (Josifovski et al., 2022); too broad a distri-\nbution leads to suboptimal local minima convergence (see\nFigure 3), while too narrow a distribution leads to poor real-\nworld generalization (Mozifian et al., 2019; Packer et al.,\n2018). Many existing methods rely on real-world rollouts\nfrom hardware experiments to estimate dynamics parame-\nters (Chebotar et al., 2019; Ramos et al., 2019; Muratore\net al., 2022). However, for complex tasks with physical\nparameters that are difficult to efficiently or effectively sam-\nple, this data may be time-consuming to produce, or simply\nunavailable.\nAn ideal sampling distribution enables the policy to focus\ntraining on areas of the distribution that can feasibly be\nsolved in order to maximize the overall success rate of the\ntrained policy while not wasting time on unsolvable regions\nof the domain. Automating updates to parameter distri-\nbutions during the training process can remove the need\nfor heuristic tuning and iterative experimentation (Mozi-\nfian et al., 2019; OpenAI et al., 2019; Tiboni et al., 2024).\nIn this paper, we present GoFlow, a novel approach for\nlearned domain randomization that combines actor-critic"}, {"title": "2. Related Work", "content": "Recent developments in reinforcement learning have proven\nthat policies trained in simulation can be effectively\ntranslated to real-world robots for contact-rich assembly\ntasks (Zhang et al., 2024; Tang et al., 2023a; Noseworthy\net al., 2024; Jin et al., 2023). One key innovation that has\ncontributed to the development of robust policies is domain\nrandomization (Chen et al., 2021; Peng et al., 2017), wherein\nenvironment parameters are sampled from a distribution dur-\ning training such that the learned policy can be robust to\nenvironmental uncertainty on deployment.\nSome previously explored learning strategies include min-\nimization of divergence with a target sampling distribu-\ntion using multivariate Gaussians (Mozifian et al., 2019),\nmaximization of entropy using independent beta distribu-\ntions (Tiboni et al., 2024), and progressive expansion of a\nuniform sampling distribution via boundary sampling (Ope-\nnAI et al., 2019). Here, we propose a novel learned domain\nrandomization technique using normalizing flows (Rezende\n& Mohamed, 2015) as a neural sampling distribution, thus\nincreasing flexibility and expressivity.\nIn addition to learning robust policies, such sampling distri-\nbutions can be used as indicators of the world states under\nwhich the policy is expected to succeed. Some previous\nworks have combined domain randomization with informa-\ntion gathering via system identification (Ramos et al., 2019;\nSagawa & Hino, 2024). In this work, we similarly make use\nof our learned sampling distribution as an out-of-distribution\ndetector in the context of a multi-step planning system."}, {"title": "3. Background", "content": ""}, {"title": "3.1. Markov Decision Process", "content": "A Markov Decision Process (MDP) is a mathematical frame-\nwork for modeling decision-making. Formally, an MDP\nis defined as a tuple (S, A, P, R, \u03b3), where S is the state\nspace, A is the action space, $P:S\\times A\\timesS \\rightarrow [0, 1]$ is\nthe state transition probability function, where $P(s' | s, a)$\ndenotes the probability of transitioning to state s' from state\ns after taking action a, $R:S\\times A \\rightarrow R$ is the reward\nfunction, where R(s, a) denotes the expected immediate\nreward received after taking action a in state s, $\u03b3 \u03b5 [0,1)$\nis the discount factor, representing the importance of future\nrewards.\nA policy $\u03c0 : S \\times A \\rightarrow [0, 1]$ defines a probability distribu-\ntion over actions given states, where \u03c0(\u03b1 | s) is the prob-\nability of taking action a in state s. The goal is to find an\noptimal policy \u03c0* that maximizes the expected cumulative\ndiscounted reward."}, {"title": "3.2. Domain Randomization", "content": "Domain randomization introduces variability into the envi-\nronment by randomizing certain parameters during training.\nLet \u039e denote the space of domain randomization parame-\nters, and let \u00a7 \u2208 \u039e be a specific instance of these parameters.\nEach & corresponds to a different environment configuration\nor dynamics.\nWe can define a parameterized family of Markov Decision\nProcesses (MDPs) where each $M_\u03b5 = (S, A, P_E, R_\u03b5, \u03b3)$ has\ntransition dynamics $P_\u025b$ and reward function $R_\u025b$ dependent\non \u00a7. The agent interacts with environments sampled from\na distribution over \u039e, typically denoted as $p(\u03be)$. 1\nThe objective is to learn a policy \u03c0 : S \u2192 A that maximizes\nthe expected return across the distribution environments:\n$J(\u03c0) = \u0395_{\u03be~p(\u03be)} \u0395_{\u03c4~\u03a1_{\u03b5,\u03c0}} [\\sum_{t=0}^\u221e \u03b3^tRe(st, at)]$\nwhere \u0442 = {(50, 40, 81, a1,...)} denotes a trajectory gen-\nerated by policy \u03c0 in environment \u00a7. Domain randomization\naims to find a policy \u03c0* such that: $\u03c0^* = arg max_\u03c0 J(\u03c0)$.\nIn deep reinforcement learning, the policy \u03c0 is a neural net-\nwork parameterized by 0, denoted as \u03c0\u03c1. The agent learns\nthe policy parameters 0 through interactions with simulated\nenvironments sampled from p(\u03be). In our implementation,\nwe employ the Proximal Policy Optimization (PPO) algo-\nrithm (Schulman et al., 2017), an on-policy policy gradient\nmethod that optimizes a stochastic policy while ensuring\nstable and efficient learning.\nTo further stabilize training, we pass privileged information\nabout the environment parameters & to the critic network.\nThe critic network, parameterized by 4, estimates the state-\nvalue function:\n$V_\u03c8(st,E) = \u0395_{\u03c0_\u03b8} [\\sum_{k=0}^\u221e rt+k | st, \u03be]$\nwhere st is the current state, rt+k are future rewards, and\ny is the discount factor. By incorporating \u00a7, the critic\ncan provide more accurate value estimates with lower vari-\nance (Pinto et al., 2017). The actor network $\u03c0_\u03b8(a_t| s_t)$ does\nnot have access to \u00a7, ensuring that the policy relies only on\nobservable aspects of the state."}, {"title": "3.3. Normalizing Flows", "content": "Normalizing flows are a class of generative models that\ntransform a simple base distribution into a complex tar-\nget distribution using a sequence of invertible, differen-\ntiable functions. Let z ~ pz(z) be a latent variable from\na base distribution (e.g., a standard normal distribution).\nA normalizing flow defines an invertible transformation\nfo : Rd \u2192 Rd parameterized by neural network parameters\n4, such that x = fo(z), aiming for x to follow the target\ndistribution.\nThe density of x is computed using the change of variables\nformula:\n$px(x) = Pz (f_\u03c6^{-1}(x)) |det (\\frac{df_\u03c6^{-1}(x)}{dx})|$\nFor practical computation, this is often rewritten as:\n$log px (x) = log pz (z) - log |det (\\frac{df_\u03c6(z)}{dz})|$"}, {"title": "4. Method", "content": "In this section, we introduce GoFlow, a method for learned\ndomain randomization that goes with the flow by adaptively\nadjusting the domain randomization process using normal-\nizing flows.\nIn traditional domain randomization setups, the distribution\np(\u03be) is predefined. However, selecting an appropriate p(\u03be)\nis crucial for the policy's performance and generalization.\nToo broad a sampling distribution and the training focuses\non unsolvable environments and falls into local minima.\nIn contrast, too narrow a sampling distribution leads to\npoor generalization and robustness. Additionally, rapid\nchanges to the sampling distribution can lead to unstable\ntraining. To address these challenges, prior works such\nas (Klink et al., 2021) have proposed a self-paced learner,\nwhich starts by mastering a small set of environments, and"}, {"title": "Algorithm 1 GoFlow", "content": "Require: Initial policy parameters 0, flow parameters 4,\ntraining steps N, network updates K, entropy coeffi-\ncient a, similarity coefficient \u1e9e, and learning rate n\u03c6\nfor n = 1 to N do\nSample {train} =1 ~ P\u2084(f), {test} =1 ~ u(\u03be)\nTrain \u03c0\u03bf with train initializations\nEstimate Jetest (\u03c0\u03bf) via policy rollouts\nSave current flow distribution as Pgold (\u03be)\nfork= 1 to K do\nB\nR\u2190 \u039e\u03a3=1 [P(test) test (\u03c0\u03bf)].\n\u0124 \u2190 \u2212\u00a6\u039e\u00a6 \u00b7 E\u00a3~u(t) [P\u2084(\u00a3) log p$(E)]\nDKL + E\u20ac~P@old (f) [log Polda (5) - log po(f)]\n\u2190 \u00a2 + n\u00a2\u2207\u00a2 (R + \u03b1\u0124 \u2013 BDKL)\nend for\nend for"}, {"title": "5. Domain Randomization Experiments", "content": "Our simulated experiments compare policy robustness in\na range of domains. For full details on the randomization\nparameters and bounds, see Appendix A.3."}, {"title": "5.1. Domains", "content": "First, we examine the application of GoFlow to an illustra-\ntive 2D domain that is multimodal and contains intervariable\ndependencies. The state and action space are in R2. The\nagent is initialized randomly in a bounded x, y plane. An\nenergy function is defined by a composition of Gaussians\nplaced in a regular circular or linear array. The agent can\nobserve its position with Gaussian noise proportional to\nthe inverse of the energy function. The agent is rewarded\nfor guessing its location, but is incapable of moving. This\ntask is infeasible when the agent is sufficiently far from any\nof the Gaussian centers, so a sampling distribution should\ncome to resemble the energy function. Some example func-"}, {"title": "5.2. Baselines", "content": "In our domain randomization experiments, we compare\nto a number of standard RL baselines and learning-based\napproaches from the literature. In our quantitative exper-\niments, success is measured by the sampled environment\npassing a certain performance threshold JT that was se-"}, {"title": "6. Application to Multi-step manipulation", "content": "While reinforcement learning has proven to be a valuable\ntechnique for learning short-horizon skills in dynamic and\ncontact-rich settings, it often struggles to generalize to more\nlong-horizon and open ended problems (Sutton & Barto,\n2018). The topic of sequencing short horizon skills in the\ncontext of a higher-level decision strategy has been of in-\ncreasing interest to both the planning (Mishra et al., 2023)\nand reinforcement learning communities (Nasiriany et al.,\n2021). For this reason, we examine the utility of these\nlearned sampling distributions as out-of-distribution detec-\ntors, or belief-space preconditions, in the context of a multi-\nstep planning system."}, {"title": "6.1. Belief-space planning background", "content": "Belief-space planning is a framework for decision-making\nunder uncertainty, where the agent maintains a probability\ndistribution over possible states, known as the belief state.\nInstead of planning solely in the state space S, the agent\noperates in the belief space B, which consists of all possible\nprobability distributions over S. This approach is partic-\nlarly useful in partially observable environments where\nthere is uncertainty in environment parameters and where it\nis important to take actions to gain information.\nRather than operating at the primitive action level, belief-\nspace planners often make use of high-level actions \u0391\u03c0,\nsometimes called skills or options. In our case, these high-"}, {"title": "6.2. Computing preconditions", "content": "In this section, we highlight the potential application of\nthe learned sampling distribution po and privileged value\nfunction Vy as a useful artifacts for belief-space planning.\nIn particular, we are interested in identifying belief-space\npreconditions of a set of trained skills.\nOne point of leverage we have for this problem is the privi-\nleged value function V\u2084(s, \u00a7), which was learned alongside\nthe policy during training. One way to estimate the belief-\nspace precondition is to simply find the set of belief states"}, {"title": "6.3. Updating beliefs", "content": "Updating the belief state requires a probabilistic state esti-\nmation system that outputs a posterior over the unobserved\nenvironment variables, rather than a single point estimate.\nWe use a probabilistic object pose estimation framework\ncalled Bayes3D to infer posterior distributions over object\npose (Gothoskar et al., 2023). For details on this, see Ap-\npendix A.4.1.\nThe benefit of this approach in contrast to traditional\nrendering-based pose estimation systems, such as those pre-"}, {"title": "6.4. A simple belief-space planner", "content": "While the problem of general-purpose multi-step planning\nin belief-space has been widely studied, in this paper we use\na simple BFS belief-space planner to demonstrate the utility\nof the learned sampling distributions as belief-space precon-\nditions. The full algorithm can be found in Algorithm 2.\nAn example plan can be seen in Figure 4. The goal is to\nassemble the gear box by inserting all three gears (yellow,\npink, and blue) into the shafts on the gear plate. Each gear\ninsertion is associated with a separate policy for each color\ntrained with GoFlow. In addition to the trained policies, the\nrobot is given access to an object-parameterized inspection\naction which has no preconditions and whose effects are\na reduced-variance pose estimate attained by moving the\ncamera closer to the object. The robot is initially uncertain\nof the x, y, and yaw components of the 6-dof pose based\non probabilistic pose estimates. Despite this uncertainty,\nthe robot is confident enough in the pose of the largest and\nclosest yellow gear to pick it up and insert it. In contrast,\nthe blue and pink gears require further inspection to get a\nbetter pose estimate. Closer inspection reduces uncertainty\nalong the x and y axis, but reveals no additional information\nabout yaw dimension due to rotational symmetry. Despite\nan unknown yaw dimension, the robot is confident in the"}, {"title": "7. Conclusion and discussion", "content": "In this paper, we introduced GoFlow, a novel approach to\ndomain randomization that uses normalizing flows to dy-\nnamically adjust the sampling distribution during reinforce-\nment learning. By combining actor-critic reinforcement\nlearning with a learned neural sampling distribution, we\nenabled more flexible and expressive parameterization of\nenvironmental variables, leading to better generalization in\ncomplex tasks like contact-rich assembly. Our experiments\ndemonstrated that GoFlow outperforms traditional fixed and\nlearning-based domain randomization techniques across a\nvariety of simulated environments, particularly in scenarios\nwhere the domain has irregular dependencies between pa-\nrameters. The method also showed promise in real-world\nrobotic tasks including contact-rich assembly.\nMoreover, we extended GoFlow to multi-step decision-\nmaking tasks, integrating it with belief-space planning to\nhandle long-horizon problems under uncertainty. This ex-\ntension enabled the use of learned sampling distributions\nand value functions as preconditions leading to active infor-\nmation gathering.\nAlthough GoFlow enables more expressive sampling dis-\ntributions, it also presents some new challenges. One lim-\nitation of our method is that it has higher variance due to\noccasional training instability of the flow. This instability"}, {"title": "8. Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Reproducibility Statement", "content": "In our supplementary materials, we provide a minimal code\nimplementation of our approach along with all the baseline\nimplementations and environments discussed in this paper.\nIf accepted, we will publish a full public release of the code\non Github."}, {"title": "A.2. Importance Sampling Proofs", "content": "Here we show how we obtain the importance-sampled esti-\nmates for both the reward term R and the entropy term \u0124 in\nGoFlow(Algorithm 1). Specifically, we sample from a uni-\nform distribution u(\u03be) over \u039e, rather than from the learned\ndistribution p(\u00a7) directly, in order to avoid collapse onto\nnarrow regions of the parameter space."}, {"title": "A.2.1. REWARD TERM: R", "content": "We want an unbiased estimate of\n$\u0395_{\u03be~\u03c1\u03c6(\u0395)} [J\u03b5(\u03c0)] = \u222b_\u039e P\u03c6(\u03be) J\u03b5(\u03c0) \u03b1\u03be$.\nLet u(g) be a uniform distribution over \u039e. Since\nP\u03c6(\u03be) J\u03b5(\u03c0) = \\frac{P\u03c6(\u03be)}{u(5)}u(5) J\u03b5(\u03c0), we can rewrite:\n$\u222b_\u039e P\u03c6(\u03be) J\u03b5(\u03c0) \u03b1\u03be = \u222b_\u039e \\frac{P\u03c6(\u03be)}{u(\u03be)}u(\u03be) J\u03b5(\u03c0) \u03b1\u03be$.\nHence, sampling i ~ u(\u03be) and averaging \\frac{\u03a1\u03c6(\u03be)}{\u03ba(\u03be)} J\u03b5; (\u03c0)\ngives an unbiased Monte Carlo estimate of Ep\u2084 [J\u03b5 (\u03c0)].\nIf has finite measure ||, then u(\u03be) = 1/|\u039e|. Thus\n$\\frac{P\u03c6(\u03be)}{\u03ba(\u03be)} = P(\u03be) |\u039e\u0399$.\nTherefore, the empirical estimate becomes\n$R = \\frac{1}{B} \\sum_{i=1}^B [\\frac{P\u03c6(\u03be)}{\u03ba(\u03be\u03b5)}\u039e) J\u03b5; (\u03c0)] = \\frac{|\u039e|}{B}\\sum_{i=1}^B P\u03c6(\u03be\u03b5) J\u03b5; (\u03c0)$.\nwhere {fi}1 ~ u(\u03be). This matches Line 7 in Algorithm 1."}, {"title": "A.2.2. ENTROPY TERM: \u0124", "content": "Similarly, to compute the differential entropy of p(\u03be), we\nhave\n$H(\u03c1\u03c6) = \u222b_\u039e P\u03c6(\u03be) log p (\u03be) \u03b1\u03be$.\nAgain, we apply the same importance sampling trick via\n\u0e19(\u03be). We write:\n$P\u03c6(\u03be) log p\u2084(\u03be) = \\frac{P\u03c6(\u03be)}{u(\u03be)}\\frac{u(\u03be)}{u(\u03be)} log p\u2084(\u03be)$.\nHence\n$\u222b_\u039e P\u03c6(\u03be) log p\u2084(\u03be) \u03b1\u03be = \u222b_\u039e \\frac{\u03a1\u03c6(\u03be)}{u(\u03be)}u(\u03be) log p4 (\u03be) \u03b1\u03be$.\nIf || is the measure of \u2261 under u(\u03be), then u(\u03be) = 1/|\u039e|. So\nthe Monte Carlo estimate for \u222b p$(\u00a7) log p\u2084(\u00a7) d\u0121 becomes\n$\\frac{|\u039e|}{B}\\sum_{i=1}^B P\u03c6(\u03be\u03b5) logo (\u03be\u03b5)$,\nwith \u03be\u2081 ~ u(\u03be). Multiplying by -1 yields the differential\nentropy:\n$H(\u03c1\u03c6) = -\u222b_\u039eP(E) log po (4) de = \\frac{|\u039e|}{B}\\sum_{i=1}^B P\u03c6(\u03be\u03b5) log p\u03c6(\u03be\u03b5)$,\nwhich is exactly what we implement in Line 8 of Algo-\nrithm 1.\nRemark A.1. Using uniform sampling u(\u03be) to approximate\nthese terms provides global coverage of E, helping prevent\nthe learned distribution p$ from collapsing around a small\nsubset of parameter space. By contrast, if one sampled\n\u00a7 from p\u2084(\u03be) itself for these terms, the distribution might\nfail to expand to other promising regions once it becomes\npeaked."}, {"title": "A.3. Domain Randomization Parameters", "content": "Below we describe the randomization ranges and parameter\nnames for each environment. We also provide the reward\nsuccess threshold (JT) and cut the max duration of some en-\nvironments in order to speed up training (tmax). Lastly, we\nslightly modified the Quadruped environment to only take\na fixed forward command rather than the goal-conditioned\npolicy learned by default. Other than those changes, the first\nfour simulated environments official IsaacLab implementa-\ntion.\n\u2022 Cartpole parameters (J\u2081 = 50, tmax = 2s):"}, {"title": "A.4. Multi-Step Planning Details", "content": ""}, {"title": "A.4.1. UPDATING BELIEFS VIA PROBABILISTIC POSE\nESTIMATION", "content": "Updating the belief state b requires a probabilistic state\nestimation system that outputs a posterior over the state\nspace S, rather than a single point estimate. Given a new\nobservation o, we use a probabilistic object pose estimation\nframework (Bayes3D) to infer posterior distributions over\nobject pose (Gothoskar et al., 2023).\nThe pose estimation system uses inference in an probabilis-\ntic generative graphics model with uniform priors on the\ntranslational x, y, and rotational yaw (or rx) components\nof the 6-dof pose (since the object is assumed to be in flush\ncontact with the table surface) and an image likelihood\n$P(Orgbd | rx, x, y)$. The object's geometry and color infor-\nmation is given by a mesh model. The image likelihood\nis computed by rendering a latent image imrgbd with the\nobject pose corresponding to (rx, x, y) and calculating the\nper-pixel likelihood:\n$P(Orgbd | rx, x, y) \u221d \u03a0_{i,jEC} [P_{out} + (1 - P_{out}). Pin(Obd | rx, x, y)]$\n$Pin(orgbd | rx, x, y) \u221d exp ( -\\frac{||0_{rgb} - im_{rgb}||_1}{b_{rgb}} - \\frac{||0_{bd} - im_{bd}||_1}{b_{d}} )$,\nwhere i and j are pixel row and column indices, C is the set\nof valid pixels returned by the renderer, brgb and ba are hy-\nperparameters that control posterior sensitivity to the color\nand depth channels, and pout is the pixel outlier probability\nhyperparameter. For an observation Orgbd, we can sample\nfrom P(rx, x, y | Orgbd) \u00d7 P(Orgbd | rx, x, y) to recover the\nobject pose posterior with a tempering exponential factor a\nto encourage smoothness. We first find the maximum a pos-\nteriori (MAP) estimate of object pose using coarse-to-fine\nsequential Monte Carlo sampling (Del Moral et al., 2006)\nand then calculate a posterior approximation using a grid\ncentered at the MAP estimate.\nThe benefit of this approach in contrast to traditional\nrendering-based pose estimation systems, such as those\npresented in (Wen et al., 2024) or (Labb\u00e9 et al., 2022), is\nthat our pose estimates indicate high uncertainty for distant,\nsmall, occluded, or non-visible objects as well dimensions\nalong which the object is symmetric. A visualization of the\npose beliefs at different points in the multi-step plan can be\nseen in Figure 13 in the Appendix."}, {"title": "A.5. Hyperparameters", "content": "Below we list out the significant hyperparameters involved\nin each baseline method, and how we chose them based\non our hyperparameter search. We run the same seed for\neach hyperparameter and pick the best performing hyper-\nparameter as the representative for our larger quantitative\nexperiments in figure 3. The full domain randomization\n(FullDR) and no domain randomization (NoDR) baselines\nhave no hyperparameters."}, {"title": "A.5.1. GOFLOW", "content": "We search over the following values of the a hyperparam-\neter: [0.1, 0.5, 1.0, 1.5, 2.0]. We search over the follow-\ning values of the \u1e9e hyperparameters [0.0, 0.1, 0.5, 1.0, 2.0].\nOther hyperparameters include number of network updates\nper training epoch (K = 100), network learning rate\n(n = 1e - 3), and neural spline flow architecture hyper-\nparameters such as network depth (l = 3), hidden features\n(64), and number of bins (8). We implement our flow using\nthe Zuko normalizing flow library (Rozet et al., 2022)."}, {"title": "A.5.2. LSDR", "content": "Similary to GoFlow, we search over the following values\nof the ar hyperparameter: [0.1, 0.5, 1.0, 1.5, 2.0]. Other\nhyperparameters include the number of updates per train-\ning epoch (T=100), and initial Gaussian parameters: \u03bc ="}, {"title": "A.5.3. DORAEMON", "content": "We search over the following values of the ED hyperparame-\nter: [0.005, 0.01, 0.05, 0.1, 0.5]. After fixing the best ED for\neach environment, we additionally search over the success\nthrshold AD: [0.005, 0.01, 0.05, 0.1, 0.5]."}, {"title": "A.5.4. ADR", "content": "In ADR, we fix the upper threshold to be the success\nthreshold t+\n= JT as was done in the original pa-\nper and search over the lower bound threshold t=\n[0.1t+, 0.25t+, 0.5t+,0.75t+,0.9t+]. The value used in\nthe original paper was 0.5tH. Other hyperparameters in-\nclude the expansion/contraction rate, which we interpret to\nbe a fixed fraction of the domain interval, \u2206 = 0.1 * [\u00a7max\n\u00c9min], and boundary sampling probability p\u044c = 0.5."}, {"title": "A.6. Coverage vs Range Experiments", "content": "We compare coverage vs. range scale in the ant domain. We\nadjust the parameter lower and upper bounds outlined in\nAppendix A.3 and see how the coverage responds to those\nchanges during training. The parameter range is defined rel-\native to a nominal midpoint m set to the original domain pa-\nrameters: [m-(m-lower)*scale, m+(upper-m)*scale].\nThe results of our experiment are shown in Figure 12"}, {"title": "A.7. Real-world experiments", "content": "In addition to simulated experiments, we compare GoFlow\nagainst baselines on a real-world gear insertion task. In\nparticular, we tested insertion of the pink medium gear over\n10 trials for each baseline. To test this, we had the robot\nperform 10 successive pick/inserts of the pink gear into the\nmiddle shaft of the gear plate. Instead of randomizing the\npose of the gear, we elected to fix the initial pose of the\ngear and the systematically perturb the end-effector pose\nby a random \u00b10.01 meter translational offset along the x\ndimension during the pick. We expect some additional grasp\npose noise due to position error during grasp and object shift\nduring grasp. This led to a randomized in-hand offset while\nrunning the trained insertion policy. Our results show that\nGoFlow can indeed more robustly generalize to real-world\nrobot settings under pose uncertainty."}, {"title": "A.8. Statistical Tests", "content": "We performed a statistical analysis of the simulated results\nreported in Figure 3 and the real-world experiments in Ta-\nble 1. For the simulated results, we recorded the final do-\nmain coverages across all seeds and performed pairwise\nt-tests between each method and the top-performing method.\nThe final performance mean and standard deviation are re-"}, {"title": "Algorithm 2 Belief-Space Planner Using BFS", "content": "Require: Initial belief state bo, goal condition G C B, set\nof skills An, success threshold \u03b7\n1: Initialize the frontier F \u2190 {bo}\n2: Initialize the visited set V\u2190 \u00d8\n3: Initialize the plan dictionary Plan mapping belief states\nto sequences of skills\n4: while Fis not empty do\nDequeue b from F\nif be G then\nreturn Plan [6] \u25b7 Return the sequence of skills\nleading to b\nend if\nfor all skills \u03c0\u2208 \u0391\u03c0 do\nif b \u2208 Pre given \u03b7 then\nb' \u2190 sample(Eff)\nif b' V then\nAdd b' to F and V\nUpdate Plan [b'] \u2190 Plan[b] + [\u03c0]\nend if\nend if\nend for\nend while\nreturn Failure \u25b7 No plan found"}]}