{"title": "EFFICIENT TRAINING OF SPARSE AUTOENCODERS\nFOR LARGE LANGUAGE MODELS VIA LAYER GROUPS", "authors": ["Davide Ghilardi", "Federico Belotti", "Marco Molinari"], "abstract": "Sparse Autoencoders (SAEs) have recently been employed as an unsupervised ap-\nproach for understanding the inner workings of Large Language Models (LLMs).\nThey reconstruct the model's activations with a sparse linear combination of in-\nterpretable features. However, training SAEs is computationally intensive, es-\npecially as models grow in size and complexity. To address this challenge, we\npropose a novel training strategy that reduces the number of trained SAEs from\none per layer to one for a given group of contiguous layers. Our experimental\nresults on Pythia 160M highlight a speedup of up to 6x without compromising the\nreconstruction quality and performance on downstream tasks. Therefore, layer\nclustering presents an efficient approach to train SAEs in modern LLMs.", "sections": [{"title": "INTRODUCTION", "content": "With the significant adoption of Large Language Model (LLM)s in world applications, understand-\ning their inner workings has gained paramount importance. A key challenge in LLMs interpretability\nis the polysemanticity of neurons in models' activations, lacking a clear and unique meaning (Olah\net al., 2020). Recently, SAEs (Huben et al., 2024; Bricken et al., 2023) have shown great promise\nto tackle this problem by decomposing the model's activations into a sparse combination of human-\ninterpretable features.\nThe use of SAEs as an interpretability tool is motivated by two key reasons: the first is the substantial\nempirical evidence supporting the Linear Representation Hypothesis (LRH), or that LLMs exhibit\ninterpretable linear directions in their activation space (Mikolov et al., 2013; Nanda et al., 2023; Park\net al., 2023); the second is the Superposition Hypothesis (SH) (Elhage et al., 2022), which supposes\nthat, by leveraging sparsity, neural networks represent more features than they have neurons. Under\nthis hypothesis, we can consider a trained neural network as a compressed simulation of a larger\ndisentangled model, where every neuron corresponds to a single feature. To overcome superposi-\ntion, SAEs leverage the LRH to decompose model activations into a sparse linear combination of\ninterpretable features.\nHowever, training SAEs is computationally expensive and will become even more costly as the\nmodel size and parameter counts grow. Indeed, one Sparse Autoencoder (SAE) is typically learned\nfor a given component at every layer in a LLM. Moreover, the number of features usually equals\nthe model activation dimension multiplied by a positive integer, called the expansion factor. For\nexample, a single SAEs trained on the Llama-3.1 8B model activations with an expansion factor of\n32 has roughly 40962322 \u2248 1.073B parameters, with a total of more than 32B when the training\neach one of the 32 layers.\nIn this work, we reduce the computational overhead of training a separate SAE for each layer of a\ntarget LLM by learning a single SAE for groups of related and contiguous layers. This approach is\ninspired by the observation that neural network layers often group together in learning task-specific\nrepresentations (Szegedy et al., 2014; Zeiler & Fergus, 2014; Jawahar et al., 2019): shallow layers\ntypically capture low-level features, while deeper layers learn high-level abstractions. Additionally,\nadjacent layers in LLMs tend to encode redundant information, as evidenced by the similarity in"}, {"title": "RELATED WORK", "content": "2.1 THE LINEAR REPRESENTATION AND SUPERPOSITION HYPOTHESES\nSupported by substantial evidence, from the seminal Mikolov et al. (2013) vector arithmetic to the\nmore recent work of Nanda et al. (2023) and Park et al. (2023) on LLMs, the Linear Representation\nHypothesis (LRH) supposes that neural networks have interpretable linear directions in their acti-\nvations space. However, neuron polysemanticity remains an essential challenge in neural network\ninterpretability (Olah et al., 2020).\nRecently, Bricken et al. (2023) explored this issue by relating the Superposition Hypothesis (SH)\nto the decomposition ideally found by a SAE. According to the SH, neural networks utilize n-\ndimensional activations to encode m > n features by leveraging their sparsity and relative impor-\ntance. As a result, we can write the activations x1 in a model as\n$x^{1} = b + \\sum_{i}^{m} f_{i}(x)d_{i}$                    (1)"}, {"title": "SPARSE AUTOENCODERS", "content": "Sparse Autoencoders have gained popularity in LLM interpretability due to their ability to counteract\nsuperposition and decompose neuron activations into interpretable features (Bricken et al., 2023;\nHuben et al., 2024). Given an input activation x \u2208 Rd, a SAE reconstructs it as a sparse linear\ncombination of dsae \u226b d features, denoted as vi \u2208 Rdmodel. The reconstruction follows the form:\n(xf)(x) = Waf(x) + bd                    (2)\nHere, the columns of Wa represent the features vi, ba is the decoder's bias term, and f(x) represents\nthe sparse features activations. The feature activations are computed as\nf(x) = \u03c3(We(x \u2212 bd) + be)                      (3)\nwhere be is the encoder's bias term and o is an activation function, typically ReLU(x) = max(0, x).\nThe training of a SAE involves minimizing the following loss function:\nLsae = ||x \u2212 x||2 + 1||f(x)||1                     (4)\nwhere the first term in Equation 4 represents the reconstruction error, while the second term is an l\u2081\nregularization on the activations f(x) to encourage sparsity.\nTypically, dsae is set as dsae = c \u00b7 d, where c \u2208 {2n|n \u2208 N+}. As the size and depth of the model\nincrease, training SAEs can become more computationally intensive."}, {"title": "SAES EVALUATION", "content": "SAE evaluation in the context of LLMs presents a significant challenge. While standard unsuper-\nvised metrics such as L2 (reconstruction) loss and Lo sparsity are widely adopted to measure SAE\nperformance (Gao et al., 2024; Lieberum et al., 2024), they fall short of assessing two key aspects:\ncausal importance and interpretability.\nRecent approaches, including auto-interpretability (Bricken et al., 2023; Huben et al., 2024; Bills\net al., 2023) and ground-truth comparisons (Sharkey et al., 2023), aim to provide a more holistic\nevaluation. These methods focus on the causal relevance of features (Marks et al., 2024) and evaluate\nSAEs in downstream tasks. Makelov et al. (2024), for instance, proposed a framework for the\nIndirect Object Identification (IOI) task, emphasizing three aspects: the sufficiency and necessity of\nreconstructions, sparse feature steering (Templeton et al., 2024), and the interpretability of features\nin causal terms.\nKarvonen et al. (2024) further contributed by developing specialized metrics for board game lan-\nguage models. Using structured games like chess and Othello, they introduced supervised metrics,\nsuch as board reconstruction accuracy and coverage of predefined state properties, offering a more\ndirect assessment of SAEs' ability to capture semantically meaningful and causally relevant features."}, {"title": "IMPROVING SAES TRAINING", "content": "As SAEs gain popularity for LLMs interpretability and are increasingly applied to state-of-the-art\nmodels (Lieberum et al., 2024), the need for more efficient training techniques has become evident.\nTo address this, (Gao et al., 2024) explored the scaling laws of Autoencoders to identify the optimal\ncombination of size and sparsity.\nRecent work also explored using transfer learning to improve SAE training. For example, Kissane\net al. (2024) and Lieberum et al. (2024) demonstrated the transferability of SAE weights between\nbase and instruction-tuned versions of Gemma-1 (Team et al., 2024a) and Gemma-2 (Team et al.,\n2024b), respectively. On the other hand, Ghilardi et al. (2024) shows that transfer also occurs among\nlayers of a single model, both in forward and backward directions."}, {"title": "EXPERIMENTAL SETUP", "content": "3.1 DATASET AND HYPERPARAMETERS\nWe train SAEs on the residual stream of the Pythia-160M model (Biderman et al., 2023) after the\ncontribution of the MLP. The chosen dataset is a 2B pre-tokenized version of the Pile dataset (Gao\net al., 2020) with a context size of 1024. We set the expansion factor c = 8, x = 1 in Equation 4,\nlearning rate equal to 3e-5, and a batch size of 4096 samples. Following Bricken et al. (2023), we\nconstrain the decoder columns to have unit norm and do not tie the encoder and decoder weights.\nWe use the JumpReLU activation function as specified in Rajamanoharan et al. (2024), and defined\nas JumpReLU(z) = z\u00b7ReLU(z\u22120), with 0 being a threshold learned during training. We fixed the\nhyperparameters for all the experiments conducted in this work. All hyperparameters can be found\nin Table 3."}, {"title": "LAYER GROUPS", "content": "For a model with L layers, the number of possible combinations of k groups of adjacent layers that\ncan be tested is given by (L-1). With this number growing with model depth, we employed an ag-\nglomerative grouping strategy based on the angular distances between layers to reduce it drastically.\nIn particular, we compute the mean angular distances, as specified in Gromov et al. (2024), over 5M\ntokens from our training set:\ndangular (xpost, xpost) =\n1\n\u03c0\narccos(\nxpost Xpost\n|xpost ||2||xpost ||2\n)                     (5)\nfor every p,q \u2208 {1, ..., L}, where xpost are the l-th residual stream activations after the MLP's\ncontribution. From Figure2, it can be noted how the last layer is different from every other layer\nin terms of angular distance. For this reason, we have decided to exclude it from the grouping\nprocedure.\nWe adopted a bottom-up hierarchical clustering strategy with a complete linkage (Nielsen, 2016)\nthat aggregates layers based on their angular distances. Specifically, at every step of the process,"}, {"title": "SAES TRAINING AND EVALUATION", "content": "We train a SAE for every layer of the Pythia-160M model with the hyperparameters specified in Sec-\ntion 3.1 and consider it our baseline. We denote SAE\u00bf as the baseline SAE trained on the activations\nfrom the i-th layer.\nMoreover, for every 1 \u2264 j \u2264 k \u2264 5 with j, k \u2208 N, we define SAEjk as the SAE trained to reconstruct\nthe post-MLP residual stream activations of all layers in the j-th group from a partition of k groups.\nAdditionally, we define [jk] as the set of all the layers that belong to the j-th group in the k groups\npartition. We train a SAEj for every k and j and compare it with the baselines. In particular, we\ncompare every SAEjk against all the baseline SAEs, with s \u2208 [jk].\nSection 4 reports the performance w.r.t. standard reconstruction and sparsity metrics. Addition-\nally, in Section 5 and 6 we show, respectively, the performance achieved on popular downstream\ntasks(Marks et al., 2024; Hanna et al., 2023; Wang et al., 2023), and human interpretability scores."}, {"title": "RECONSTRUCTION EVALUATION", "content": "To assess the quality of the reconstruction of SAEs trained with our grouping strategy, we report\nthree standard reconstruction metrics and one sparsity metric. In particular, the Cross-Entropy Loss\nScore (CELS), defined as CE(C)-CE(\u00d8)\nCE(C)-CE(M), measures the change in Cross-Entropy loss (CE) between\nthe output with SAE reconstructed activations (C) and the model's output (M), relative to the\nloss in a zero-ablated run (5), i.e., a run with the activations set to zero."}, {"title": "DOWNSTREAM TASKS EVALUATION", "content": "While achieving good reconstruction metrics is crucial for a trained SAE, it is insufficient for a\ncomprehensive evaluation of its performance. For instance, unsupervised metrics alone cannot de-\ntermine whether the identified features capture causally influential directions for the model. To\naddress this, following Marks et al. (2024), we applied SAEs to three well-known tasks: Indirect\nObject Identification (IOI), Greater Than, and Subject-Verb Agreement.\nEach task can be represented as a set of counterfactual prompts paired with their respective answers,\nformally denoted as T: {xclean, xpatch, aclean, apatch}. Counterfactual prompts are similar to clean\nones but contain slight modifications that result in a different predicted answer."}, {"title": "HUMAN INTERPRETABILITY", "content": "In addition to achieving excellent reconstruction and downstream performance, SAEs must learn\ninterpretable features. Following Ghilardi et al. (2024), we engaged human annotators to identify"}, {"title": "CONCLUSION", "content": "This work introduces a novel approach to efficiently train Sparse Autoencoders (SAEs) for Large\nLanguage Models (LLMs) by clustering layers based on their angular distance and training a single\nSAE for each group. Through this method, we achieved up to a 6x speedup in training without com-\npromising reconstruction quality or performance on downstream tasks. The results demonstrate that\nactivations from adjacent layers in LLMs share common features, enabling effective reconstruction\nwith fewer SAEs.\nOur findings also show that the SAEs trained on grouped layers perform comparably to layer-specific\nSAEs in terms of reconstruction metrics, faithfulness, and completeness on various downstream\ntasks. Furthermore, human evaluations confirmed the interpretability of the features learned by our\nSAEs, underscoring their utility in disentangling neural activations.\nThe methodology proposed in this paper opens avenues for more scalable interpretability tools, fa-\ncilitating deeper analysis of LLMs as they grow in size. Future work will focus on further optimizing\nthe number of layer groups and scaling the approach to even larger models."}, {"title": "LIMITATIONS AND FUTURE WORKS", "content": "One limitation of our approach is the absence of a precise method for selecting the optimal number\nof layer groups (k). This choice is due to the lack of a clear elbow rule for identifying the correct\nnumber of groups. While our results showed comparable performance across different values of k\non all key metrics, further exploration is needed to determine whether certain configurations could\nyield more optimal outcomes under specific conditions.\nAdditionally, we tested our method primarily on the Pythia-160M model, a relatively small LLM.\nWhile our findings demonstrate significant improvements in efficiency without sacrificing perfor-\nmance, the scalability of our approach to much larger models remains an open question. Future\nwork could explore how the grouping strategy and training techniques generalize to models with\nbillions of parameters, where the computational benefits would be even more pronounced.\nAnother important direction for future research involves understanding how Sparse Autoencoders\n(SAEs) handle the superposition hypothesis when encoding information from multiple layers. While\nour method effectively grouped layers and maintained high performance, how SAEs manage the po-\ntential overlap in feature representation across layers remains unclear. Investigating this aspect could\nlead to a more clear understanding of the trade-offs between sparsity and feature disentanglement in\nSAEs, and inform strategies for improving interpretability without compromising task performance.\nIn summary, while our work represents an efficient step forward in training SAEs for interpretability,\nextending this approach to larger models and exploring the handling of superposition will provide\nvaluable insights for both practical applications and the theoretical understanding of sparse neural\nrepresentations."}]}