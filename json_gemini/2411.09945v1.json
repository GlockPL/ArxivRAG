{"title": "TEESlice: Protecting Sensitive Neural Network Models in Trusted Execution Environments When Attackers have Pre-Trained Models", "authors": ["DING LI", "ZIQI ZHANG", "MENGYU YAO", "YIFENG CAI", "YAO GUO", "XIANGQUN CHEN"], "abstract": "Trusted Execution Environments (TEE) are used to safeguard on-device models. However, directly employing TEEs to secure the entire DNN model is challenging due to the limited computational speed. Utilizing GPU can accelerate DNN's computation speed but commercial widely-available GPUs usually lack security protection. To this end, scholars introduce TEE-shielded DNN partition (TSDP), a method that protects privacy-sensitive weights within TEEs and offloads insensitive weights to GPUs. Nevertheless, current methods do not consider the presence of a knowledgeable adversary who can access abundant publicly available pre-trained models and datasets. This paper investigates the security of existing methods against such a knowledgeable adversary and reveals their inability to fulfill their security promises. Consequently, we introduce a novel partition before training strategy, which effectively separates privacy-sensitive weights from other components of the model. Our evaluation demonstrates that our approach can offer full model protection with a computational cost reduced by a factor of 10. In addition to traditional CNN models, we also demonstrate the scalability to large language models. Our approach can compress the private functionalities of the large language model to lightweight slices and achieve the same level of protection as the shielding-whole-model baseline.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Neural Networks (DNNs) and recent Large Language Models (LLMs) have emerged as a significant category of intelligent software for user devices. These applications are capable of executing a diverse range of complex AI tasks such as voice assistants [152], image recognition [32], and natural language processing [29]. Nevertheless, the deployment of intelligent software on user devices presents a novel attack surface in comparison to cloud-based services: The detailed information of intelligent software (e.g., model weight values) is exposed to potential malicious users of the device. By having access to this white-box information, adversaries can easily achieve high attack accuracy with significantly lower costs for common attacks like Model Stealing (MS) and Membership Inference Attack (MIA) [19, 48, 58, 74, 111, 114]. These attacks pose a serious threat to the security of the intelligent software, its intellectual property, and the sensitive data privacy (e.g. training data privacy) of the software owners. Hence, a primary goal in fortifying on-device intelligent software is to thwart adversaries from obtaining the white-box information, thereby transforming straightforward and effective white-box MS and MIA attacks into black-box (considerably more challenging) scenarios [46, 48, 98, 133].\nIn this paper, we focus on protecting DNN models on clients' devices, which are equipped with Trusted Execution Environments (TEEs) and low-grade commercial GPUs. TEEs are commonly used to safeguard on-device intelligent software [43, 65, 73, 78]. Like traditional software is protected, TEEs ensure that sensitive data (e.g., private keys) are kept separate from the system environment, making it inaccessible to formidable adversaries like malicious operating systems and administrators. Compared to other methods of protection at the algorithmic level, such as Multi-Party Computation (MPC) [61], Homomorphic Encryption (HE) [37], Regularization [103], and Differential Privacy (DP) [33], TEE-based security imposes lower computational overhead on mobile and IoT devices while preserving the accuracy of the secured models [48, 137]. Nevertheless, applying TEEs directly to safeguard entire DNN models poses challenges because low-grade commercial GPUs (e.g. GeForce RTX 4090 and RTX A6000) do not provide the functionality of TEE. Although some recent high- end GPUs (e.g. Nvidia Hopper GPU Architecture [109]) provide the functionality of confidential computing, their prices are too high for ordinary model users. An Nvidia H100 GPU is over 15\u00d7 more expensive than a GeForce RTX 4090. Attempting to shield the complete DNN model within a TEE (shielding-whole-model) could result in a 50x reduction in the model's speed.\nWhile safeguarding an entire deep learning model using TEEs may not be practical for on-device scenarios, recent research suggests safeguarding the privacy-sensitive and critical components of the model to ensure both high utility and security simultaneously. Specifically, a concept known as TSDP has been proposed. This approach involves splitting a large DNN model into two components: a privacy-sensitive component, which is small and contains vital information, and a privacy-insensitive component, which is larger and holds less critical data. The privacy-sensitive part operates within TEEs, while the privacy-insensitive part runs on GPUs [46, 98, 121, 133]. The rationale behind TSDP is akin to securing conventional software with TEEs, where the privacy sensitive portion (e.g., private keys) is compact and can be protected by TEEs, while the larger portion of the software (e.g., the remaining codebase) operates outside of TEEs [70].\nCurrent TSDP approaches generally assume that the portion off-loaded to the GPU does not reveal sensitive information of DNN models. These methods employ retraining techniques to show that even if an attacker uses this portion for MS or MIA, the reconstructed DNN model only achieves a similar accuracy to a black-box baseline [46, 98, 121, 133], which is significantly lower than the white-box accuracy. Previous TSDP studies rely on empirical experiments to prove that the disclosed model components do not leak significantly more information than a black-box interface [46, 98, 121, 133].\nThis paper examines the security promises provided by current TSDP solutions in the presence of a more sophisticated and cunning adversary in the age of large language models. Specifically, we ex- plore a realistic threat scenario where the adversary can leverage readily available public information from the Internet, such as pre-trained models and public datasets [23, 25, 144]. With the prevalence of large language models, it has become common for software developers to utilize publicly accessible models to accelerate the development of proprietary software. Previous studies have demonstrated that these public models can be exploited to compromise private software [129]. To undermine TSDP, attackers can use public information to scrutinize outsourced model components and obtain more information on privacy beyond simply analyzing black-box output, thus undermining the security guarantees of TSDP. However, none of the existing methods has thoroughly assessed their security guarantees in the presence of public information. Therefore, we contend that it is crucial to systematically evaluate the security assurances of TSDP solutions under this threat landscape.\nTo investigate the security of TSDP methods, our initial step involves conducting a comprehensive review of the existing literature on TSDP. We analyze publications released from 2018 to 2023 in reputable conferences such as IEEE S&P, MobiSys, ATC, ASPLOS, RTSS, MICRO, AAAI, ICLR, ICML, PETs, MICRO, and TDSC. Each paper's technical approaches are scrutinized, and we classify them into five distinct categories based on their primary contributions. These categories include fortifying deep layers (\u2460), fortifying shallow layers (\u2461), fortifying high-magnitude weights (3), fortifying intermediate layers (\u2463), and fortifying non-linear layers (5). Subsequently, we choose one exemplary paper for each category and proceed to implement its technical methodology.\nAfter categorizing the existing TSDP approaches, we perform a thorough security assessment using a more powerful adversary that has access to public-pre-trained models. Both Membership Inference (MS) and Model Inversion Attacks (MIA) are carried out against the representative TSDP solutions we reviewed, and the attack accuracy is compared against two baselines: the black-box baseline (shielding-whole-model) offers the highest security assurance but the lowest utility, while the white-box baseline (where the entire Deep Neural Network model is offloaded outside of TEE) provides the highest utility but lacks security protection. The experiment results reveal that current TSDP methods inadvertently expose significant private information to attackers through offloaded model weights, allowing attacks of almost white-box quality against TEE-protected models. The accuracy of MS attacks on existing TSDP solutions is 3.85\u00d7 \u2013 4.56\u00d7 higher than that of the black-box (shielding-whole-model) baseline. On the contrary, the unprotected white-box baseline demonstrates a 4.57\u00d7 higher accuracy compared to the shielding-whole-model configuration. The results for MIA attacks show a similar trend, with existing TSDP methods exhibiting 1.16\u00d7 \u2013 1.36\u00d7 higher MIA accuracy than the shielding-whole-model baseline, while the accuracy for the white-box setup is 1.37\u00d7 higher.\nFurthermore, we found that significant challenges were faced in improving the security of established TSDP methods without fundamentally altering their approaches. For example, we evaluated the effectiveness of MS/MIA attacks using various setups of current TSDP techniques. Identifying an optimal configuration that balances a DNN model's performance with security requirements proved to be particularly challenging. Specifically, achieving a high level of tolerance to attacks requires distinct settings to configure the protected component when protecting different models and datasets. Thus, a thorough empirical process is essential to determine the ideal configuration customized to specific models and datasets within all existing TSDP strategies. However, conducting such empirical analyses is excessively costly due to the large number of possible combinations of models and datasets.\nDuring our literature survey and empirical evaluation, we found that the fundamental weakness of existing TSDP approaches is that they follow a training-before-partition strategy. This involves first training a private model with a public pre-trained model and private data, and then separating the model into two parts: a shielded part that runs in TEEs, and an offloaded part that runs out of TEEs. Since training occurs before model partitioning, privacy-related weights may likely pervade the entire model. Therefore, it is hard for existing TSDP solutions to accurately isolate privacy-related weights, creating potential attack surfaces."}, {"title": "4", "content": "In order to enhance the security of TSDP solutions against the new threat model, we introduce a novel TSDP framework named TEESLICE. This framework effectively separates privacy-sensitive weights from outsourced weights during the inference phase. Unlike the training-before-partition approach used in prior research, TEESLice employs a partition-before-training strategy. This method involves initially dividing a DNN model into a backbone and several private segments, utilizing publicly pre-trained models as the backbone, and then training the segments with private data. Consequently, TEESLICE effectively isolates privacy-related weights from offloaded weights and ensures the protection of all privacy-sensitive weights in TEEs.\nThe primary difficulty in implementing the partition-before-training approach lies in guarantee- ing that individual segments are of a manageable size for execution in TEEs without compromising on accuracy. To address this challenge, we suggest employing a dynamic pruning method. Initially, the private segments are trained with larger sizes to ensure they possess adequate model capacity for achieving high accuracy. Subsequently, the algorithm automatically adjusts the segment sizes to stay below a specified threshold of accuracy loss. Through this process, TEESLICE is able to identify the optimal configuration, or \"sweet spot,\" that minimizes the number of segments (computation) within the TEE while preserving the accuracy level of the non-partitioned model.\nOur evaluation indicates that TEESLICE surpasses existing TSDP methods in terms of both security assurance and utility cost. It is challenging for attackers to extract sensitive information through the analysis of model structures, demonstrating that TEESLICE achieves a security level equivalent to the shielding-whole-model baseline with a computational cost that is 10\u00d7 lower compared to alternative TSDP solutions, in both experimental and real-world scenarios. Additionally, TEESLICE attains a high level of security with minimal trade-offs. Statistical analysis reveals no discernible differences in accuracy between the protected TEESLICE model and the original unpartitioned model. Furthermore, the outsourced public backbone does not enhance the efficacy of attacks. Our evaluation also shows that TEESLICE can effectively protect large language models with LoRA. The contribution of this paper can be summarized as follows:\n\u2022 We systematically evaluate the security guarantee of previous TSDP solutions using two representative attacks, MS and MIA, and reveal the security issues of these solutions.\n\u2022 We illustrate the difficulty in improving the security of previous TSDP approaches without substantially changing their methodologies.\n\u2022 We propose TEESLICE, a novel TSDP solution for DNN inference that isolates privacy from off-loaded model parts to provide a strong security guarantee using TEEs and cryptographic primitives. Our detailed evaluation shows that TEESLICE offers a high security guarantee with moderate overhead and no accuracy loss.\nThis paper is an extended version of a conference paper [155]. The conference paper categorized existing TSDP solutions, evaluated their security on three representative models, and proposed TEESLICE on the CNN models. This paper includes additional content compared with the confer- ence paper. First, this paper conducts a more comprehensive review of existing TSDP solutions, including the scenarios, threat model, design insight, evaluated attacks, outsourced data security, and limitations. Second, this paper includes more experiments on the security evaluation of existing TSDP work and demonstrates the scalability of the observation. Third, this paper proposes an extended approach of TEESLICE that can be applied to large language models. Our evaluation demonstrates the effectiveness of the approach to protect large language models.\nAvailability. The artifacts are available at [5] and [8].\nOverview. In Sec. 2.2, we will introduce the background and the threat model. In Sec. 4, we survey existing TSDP solutions and evaluate their defense effectiveness. Based on the vulnerability in Sec. 4, in Sec. 5, we further reveal that it is difficult to mitigate the vulnerability straightforwardly."}, {"title": "2 BACKGROUND AND THREAT MODEL", "content": "2.1 Background\nTrusted Execution Environment (TEE). A Trusted Execution Environment (TEE) is an isolated hardware enclave that stores and processes sensitive data. The trusted enclave can be implemented as a part of the process address space (Intel SGX [90]), a virtual machine (AMD SEV [62], Intel TDX [57], ARM CCA [10], and HyperEnclave [59]), or a separate system apart from the normal system (TrustZone [9]). TEE provides a high level of security guarantee against privileged attackers. The defendable adversaries include OS-level process and malicious party who has physical access to the platform (e.g. maintenance staff). Evaluating the security guarantee of TEEs in front of various attacks (e.g. side-channel attacks [35, 76, 106, 146]) and enhancing TEE security [84, 110, 125] are important topics in the security community. Recently, Nvidia's Hopper architecture provides a confidential computing mode to protect data privacy [109]. However, such a feature is only available in recent high-end GPUs and is not supported by low-grade commercial GPUs (e.g. Nvidia RTX 4090 and Nvidia Ampere architecture).\nIn this paper, we follow prior work and deem TEE as a secure area on a potential adversary host device (including GPUs) [46, 98, 122, 133]. It means the data, code, and the whole computation process inside TEEs are secure. Although there are side-channel attacks that may leak sensitive data from TEE, they are out of our consideration.\nSecurity of DNN-based Intelligent Software. With the fast development and wide deployment of intelligent software, researchers found that its security and privacy become an important concern. The intelligent software can be vulnerable to various attacks, leading to severe consequences such as privacy leakage [126], intellectual property theft [111], and misbehavior [58]. In this paper, we focus on two representative attacks: Model Stealing (MS) and Membership Inference Attack (MIA). MS aims at the functionality of the target intelligent software, while MIA aims at the private training data. MS builds a surrogate model with a similar functionality as the target software by either directly stealing the model weights [116, 134, 163] or training a local model [58, 111, 122]. MIA predicts whether a sample is in the training dataset of the target software [93, 114].\nAvailability of Public Models and Data. There are various public DNN models on the Internet that can be downloaded for free [38, 40, 94, 141]. Companies upload trained models online to facilitate the development of the AI community [39, 95] and help individual developers to train models with low cost [164]. Such models are usually trained on public datasets [28, 81] and contain no private information of the model owner. Recent research on ML security shows that these public models can be leveraged to improve both ML attacks and defenses [25, 97, 144].\nTSDP Solutions. TSDP aims to make full use of both the security of TEEs and the computation power of GPUs. As shown in Fig. 1, the idea of TSDP is similar to TEE-shielded software partition, which only protects security-sensitive part of the application to reduce the trusted computation base [12, 13, 82, 139]. For traditional software, a common practice to use TEE is to select the privacy-sensitive keys or libraries and shield them inside TEEs, while leaving the rest parts to the untrusted rich OS. In this way, the software can achieve both high security and high performance. Similarly, TSDP solutions select the important DNN layers/weights and shield them inside TEEs, while leaving the rest parts to the untrusted GPU."}, {"title": "2.2 Threat Model", "content": "Defender's Goal. TSDP solutions (and the defenders) aim to provide a black-box label-only protection against MS/MIA by shielding partial DNN models inside TEEs. The motivation is to reduce inference latency of the straightforward black-box protection that shields the whole model inside TEEs (increase latency by up to 50\u00d7 [137]). The security goal of TSDP solutions is to downgrade white-box MS/MIA against on-device models to black-box label-only attacks [46, 98, 121, 133]. Such degeneration is important and practical for deployed DNN models in production environments. For MS, TSDP solutions enforce accurate, cheap (usually taking negligible number of queries) white-box attacks [116, 163] to expensive (usually taking tens of thousands of queries) and inaccurate black-box attacks [111]. For MIA, TSDP solutions provide a deployment framework to guarantee differential privacy requirements with little accuracy sacrifice [48]. We consider the security guarantee of a black-box baseline, where TEE shields the whole DNN model and only returns prediction labels, as the upper bound security protection offered by TSDP approaches [46, 98, 121, 133]. We however, do not aim to mitigate information leakage from TEE completely outputs (i.e., prediction labels).\nAttacker's Goal. The goal of the attacker is to steal the secrets of the model that is deployed on the user's device. Specifically, the attacker wants first to steal the model weights (white-box information) to copy the model functionality and then infer the training data privacy based on the stolen model weights.\nDefender's Capability. We assume the defender (the model owner) can utilize public models and datasets on the Internet to help the defense [25, 113, 144]. The defender can also utilize TEEs on the user's device to protect the sensitive part of the model. The defender can remotely attest that the private model part is successfully deployed in the TEE.\nAttacker's Capability. Similar to the defender, we assume the attacker can utilize public infor- mation to improve the accuracy of the model or attacks, which is a realistic setting for modern on-device learning tasks [25, 28, 38, 40, 94, 95, 144, 164]. The attacker can infer the architecture of the whole protected model, or an equivalent one, based on the public information, such as the inference results or the unprotected model part, with existing techniques [23, 25, 44, 46, 98]. Besides, we assume that the attacker can query the victim model for limited times (less than 1% of the training data), a practical assumption shared by related work [53, 116, 153]. For simplicity, we denote the victim model as $M_{vic}$, the public model as $M_{pub}$, and the surrogate model produced by model stealing as $M_{sur}$. We assume the attacker can utilize malicious software or even control"}, {"title": "3 LITERATURE REVIEW", "content": "Table 1. A summary of TSDP solutions. We covered the top-tier conferences over the last five years across diverse fields, including computer security, mobile computing, machine learning, and computer systems. For each paper, we summarized seven aspects: the scenario, threat model, applied stage, design principles, design scenarios, evaluated attacks, the security of outsourced data, and potential limitations.\nIn this section, we systematically summarize existing literature on TSDP. We covered the top-tier conferences over the last five years across diverse fields, including computer security, mobile"}, {"title": "3.1 Categorization", "content": "We categorize existing solutions into two categories: Single-Point Partition and Multi-Point Partition. Single-Point Partition solutions only partition the DNN model at one point and produce two parts. One part is shielded by the TEE and the other part is deployed on the GPU. The internal feature at the split point is transmissed between the TEE and the GPU. Multi-Point Partition solutions partition the DNN model at multiple points and produce multiple segments. Some segments are shielded by the TEE and other segments are deployed on the GPU. Most multi-point partition solutions shield non-linear layers (e.g. ReLU layers) in the TEE and deploy linear layers (e.g. FC layers and convolutional layers) on the GPU. This is because non-linear layers only occupies marginal computation (less than 5% [137]) and are difficult to be encrypted. We can observe that in early years of this direction (before 2021), most solutions are Single-Point Partition solutions. After 2021, people start to focus on Multi-Point Partition solutions.\nFor single-point partition, the advantage is the straightforward design and implementation. The disadvantage is the lack of guarantee on the input privacy. Although it is heuristically evaluated that the transmissed feature does not leak input privacy or membership information, there is no formal proof. It is difficult to provide a formal proof because the feature is usually a high-dimensional vector and it is challenging to define the privacy leakage associated with such a vector."}, {"title": "3.2 Survey Summary", "content": "In this subsection, we will summarize the observations from the survey from seven aspects: the scenario, threat model, applied stage, design principles, design scenarios, evaluated attacks, the security of outsourced data, and potential limitations.\nScenario. Most papers focus on the cloud scenario for the target scenario, where the DNN model is deployed on the cloud and the client sends the input to the cloud for inference. The model owners can be the same as the cloud servers or third-party owners. In this setting, the main goal is to protect the user input data. This cloud-based setting is a major scenario for ML as a service because many internet companies use their cloud platforms to provide ML services. Some papers focus on the edge scenario, where the DNN model is deployed on the user's device, and users directly use the edge device to run the DNN model. This is because in some scenarios, the input data is highly sensitive, and the data owners don't allow the data to leave their own server. The users' devices can either be mobile (such as smartphones) or a desktop computer located in the user's organization. These two scenarios are the most common scenarios for providing ML services.\nStage. For the applied stage of DNN model, most papers focus on the inference stage, where the DNN model is already trained and the DNN model is used to process the input. The reason that much training data that is used to train the DNN models is publicly available (e.g., Imagenet [28] dataset) is that the training phase was not considered private. However, after several years, the training phase is regarded as private due to two reasons. First, as the size of the model increases, the amount of the public dataset may not be enough to train larger models. Developers need to utilize more prolific private data to train the larger model. Second, DNN model is applied to many personalized tasks where the training data is private. For the training phase, the required amount of computation is larger than the inference because training needs to compute additional gradients and save intermediate feature maps. TSDP is more important for the training stage because the disadvantage of TEE is amplified due to the large amount of computation. The techniques to protect the training phase can also be used to protect the inference phase because the required operations during the inference are a subset of the training's operations.\nThreat Model. The threat model can be categorized into two types: 1) the cloud server in the cloud scenario is malicious, and 2) the users' device is malicious. For the first case, the goal of TSDP is to protect the uploaded user input and sometimes the output label of the input [42]. The users do not want their private data to be leaked to the cloud server when using the remote model. For the second case, the goal of TSDP is to protect the model security on the user's devices. Because the user controls his device, TSDP does not need to protect the input data. On the contrary, the user has the motivation to steal the downloaded model and steal the intellectual property in the model. Besides protecting data privacy, some papers aim to protect the integrity of DNN output in some safety-critical scenarios on the edge [121, 150]. AegisDNN aims to defend against the adversarial fault injection attack in intelligence autonomy such as automatic driving and smart robotics [150]. SOTER aims to output integrity in mission-critical applications such as autopilot navigation and home monitoring [121].\nDesign Principle. Many TSDP solutions are motivated by heuristic observations that some parts of the DNN model are more important for certain attacks. Some papers find that the output features of shallow layers can be used to reconstruct the input sample, so they use TEE to secure the shallow"}, {"title": "4 EVALUATING EXISTING TSDP DEFENSES", "content": "Based on the literature survey, we select the TSDP techniques that can be applied to our threat model introduced in Sec. 2.2. We categorize these techniques into five different types of defenses. Then, we implement a representative technique from each category and evaluate their security via MS and MIA. We assess if they were sufficiently secure against the launched attacks, and we harvest empirical observations to present lessons from the evaluation.\n4.1 Fine-Grained Defense Taxonomy\nAfter inspecting the detailed design of all the prior TSDP solutions, we found that six papers do not meet the requirements under our threat model. Among them three are unable to defend models against MS (DarKnight [44], Slalom [137], GINN [11], and 3LegRace [108]), one has a stronger assumption that requires two TEEs to verify each other (Goten [105]), and the last one decreases DNN accuracy significantly (eNNclave [119]; details in Sec. 7.2). We then divide the remaining ten papers into five categories depending on the TSDP schemes. Fig. 3 schematically illustrates each category using a sample 4-layer DNN model (including two convolution layers and two ReLU layers). We also include our proposed approach (details in Sec. 6) for comparison. The five categories are as follows:\nShielding Deep Layers partitions the DNN according to the layer depth and places the layers close to the output layer in the TEE. In Fig. 3, two deepest layers (Conv2 and ReLU2) are shielded.\nShielding Shallow Layers partitions the DNN according to the layer depth and places the layers close to the input layer in the TEE. In Fig. 3, two shallowest layers (Conv1 and ReLU1) are shielded."}, {"title": "4.2 Representative Defenses", "content": "Scheme Selection. For each of the five TSDP schemes, we select one representative solution for evaluation. For shielding deep layers (\u2460), we choose DarkneTZ because according to related work [44, 97], it is the state-of-the-art (SOTA) solution for protecting training data privacy. For shielding shallow layers (\u2461) and shielding intermediate layers (\u2463), we select Serdab [34] and SOTER [121] because they are the most recent papers published in peer-reviewed conferences. For shielding large-magnitude weights (\u2462), we choose Magnitude [46] because it has practical defense cost. The other solution in this category (NNSplitter [161]) introduces significant training cost and is not suitable for our evaluation. For shielding non-linear layers (5), we choose ShadowNet [133] since they are the only solutions in their respective categories.\nConfiguration Setting. All these schemes require configurations, e.g., for \u2460, we need to configure the exact number of \u201cdeep\u201d layers in the TEE. Overall, we configure each defense scheme according to their papers. In particular, for DarkneTZ (\u2460), we put the last classification layer into TEE. For Serdab (2), the TEE shields the first four layers. For Magnitude (\u2462), the TEE shields 1% weights"}, {"title": "4.3 Evaluated Attacks", "content": "Attack Selection. We consider MS and MIA attacks that can extract confidential model weights and private training data as the security benchmark for existing TSDP approaches. For MS, we employ standard query-based stealing techniques where the attacker trains a model from a set of collected data labeled by the partially-shielded $M_{vic}$. Query-based MS has been widely adopted in literature [58, 111, 112, 122]. We leverage the attack implementation offered by Knockoff Net [1], a SOTA baseline accepted by prestigious publications [58, 112, 122]. For MIA, we chose the transfer attack that is designed against the label-only scenario [79]. Transfer attack builds $M_{sur}$ to imitate the behavior of $M_{vic}$ and infer the privacy of $M_{vic}$ from white-box information of $M_{sur}$ (e.g., confidence scores, loss, and gradients). The intuition is that membership information can transfer from $M_{vic}$ to $M_{sur}$. We chose the standard confidence-score-based algorithm to extract membership from $M_{sur}$. In particular, this process trains a binary classifier, such that given the confidence score of $M_{sur}$, the classifier predicts if the corresponding DNN input is in the training data of $M_{vic}$. Confidence-score- based MIA has been extensively used in previous attacks [79, 103, 114, 117, 126], and we reused the attack implementation from a recent benchmark suite, ML-DOCTOR [2, 85]. Recent work has consistently employed ML-DOCTOR for membership inference [18, 24, 122].\nAttack Pipeline. As in Fig. 4, the attack goal is to get the victim model Muic's functionality and membership information. The attack pipeline consists of three phases: surrogate model initialization (Pi), MS (Pii), and MIA (Piii). The attacker first analyzes the target defense scheme and then conducts P\u00a1 to get an initialized surrogate model (denoted as $M_{init}$). Pii trains $M_{init}$ with queried data and outputs the surrogate model $M_{sur}$ (the recovered victim model). Piii takes $M_{sur}$ as input, uses MIA algorithms and outputs $M_{vic}$'s membership privacy. Specifically, in Piii, the adversary first trains a binary classifier. Then, given an input i and the $M_{sur}$'s prediction confidence score p, the classifier decides if i belongs to the $M_{vic}$'s training data by taking p as its input [18, 126, 154]."}, {"title": "4.4 Evaluation Setting", "content": "Datasets. We use four different datasets and five DNN models to evaluate different defenses. The dataset and model selection refers to prior MS and MIA literatures [58, 85]. In particular, the datasets include CIFAR10, CIFAR100 [67], STL10 [26], and UTKFace [160]. CIFAR10 and CIFAR100 are default choices of prior MS/MIA literatures [18, 58, 85, 112, 154]. STL10 and UTKFace are used by ML-DOCTOR to quantify model vulnerability [85]. When evaluating MS, we use each dataset's default train/test split to avoid possible bias. To evaluate MIA, we follow the setting of ML-DOCTOR to partition each dataset into four splits: the target training dataset, the target testing dataset, the shadow training dataset, and the shadow testing dataset. The model owner uses the target training"}, {"title": "4.5 Attack Results", "content": "We aim to answer the following research question primarily:\nRQ1: How secure are the selected TSDP solutions against model and data privacy stealing?\nWe report the evaluation results over five models (AlexNet, ResNet18, VGG16_BN, ResNet34 and VGG19_BN) over the two major metrics (MS accuracy and MI accuracy) in Table 3 and Table 4 respectively (total 20 cases). As aforementioned, we also report the baseline settings (\u201cNo-Shield\u201d and \"Black-box\u201d) for comparison. To compare the performance between different defenses, for each case, we compute a relative accuracy as the times of the accuracy over the accuracy of the black-box baseline. We report the average relative accuracy in the last row of Table 3 and Table 4. A defense scheme is considered more effective if its corresponding attack accuracy is closer to the black-box baseline (the relative accuracy is closer to 1.00\u00d7). For the white-box baseline (the whole $M_{vic}$ is offloaded to the GPU), the relative accuracies are 4.57\u00d7 for model stealing in Table 3 and 1.37\u00d7 for membership inference in Table 4.\nFrom Table 3 and Table 4, we can observe that the defense effectiveness of all solutions is limited. It is evident that even the lowest attack accuracy (marked in yellow ), indicating the highest defense effectiveness, among each setting, is still much higher than the black-box baseline: the lowest attack accuracies are averagely 3.85\u00d7 higher than black-box for MS and 1.16\u00d7 higher for MIA. Even worse, the highest attack accuracies (marked in red ) are similar to that of the white-box baseline, indicating that the defense schemes are ineffective.\nThe relative accuracy (w.r.t. black-box baselines) of DarkneTZ (\u2460) for MS is 4.27\u00d7 and 1.28\u00d7 for MIA. For Serdab (\u2461), the relative attack accuracies are 4.10\u00d7 and 1.32\u00d7. Since the attack performance toward both Serdab and DarkneTZ is high, we interpret that shielding a limited number of deep"}, {"title": "5 CHALLENGES OF STRAIGHTFORWARD MITIGATIONS", "content": "Our study and observation in answering RQ1 show that straightforward mitigation to the attacks for existing TSDP approaches is to put a larger proportion of a DNN model into TEEs to improve the protection effectiveness. However", "Trade-off": "putting more portions of a DNN model into TEEs boosts security but presumably diminishes utility (e.g.", "sweet spot\\\" configuration (i.e., how large the TEE protected part should be) that satisfies the security requirement while minimizing the utility overhead. This section will address the following research question": "nRQ2: For each of the five TSDP solutions evaluated in Sec. 4, is there a systematic approach to identify its \"sweet spot\""}]}