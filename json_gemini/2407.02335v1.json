{"title": "CALICO: Confident Active Learning with Integrated Calibration", "authors": ["Lorenzo S. Querol", "Hajime Nagahara", "Hideaki Hayashi"], "abstract": "The growing use of deep learning in safety-critical applications, such as medical imaging, has raised concerns about limited labeled data, where this demand is amplified as model complexity increases, posing hurdles for domain experts to annotate data. In response to this, active learning (AL) is used to efficiently train models with limited annotation costs. In the context of deep neural networks (DNNs), AL often uses confidence or probability outputs as a score for selecting the most informative samples. However, modern DNNs exhibit unreliable confidence outputs, making calibration essential. We propose an AL framework that self-calibrates the confidence used for sample selection during the training process, referred to as Confident Active Learning with Integrated Calibration (CALICO). CALICO incorporates the joint training of a classifier and an energy-based model, instead of the standard softmax-based classifier. This approach allows for simultaneous estimation of the input data distribution and the class probabilities during training, improving calibration without needing an additional labeled dataset. Experimental results showcase improved classification performance compared to a softmax-based classifier with fewer labeled samples. Furthermore, the calibration stability of the model is observed to depend on the prior class distribution of the data.", "sections": [{"title": "Introduction", "content": "The growing complexity of modern deep neural networks (DNNs) poses a challenge by demanding a substantial increase in labeled data needed to achieve state-of-the-art performance [8,27]. In real-world applications, obtaining labeled data is a logistically expensive process. This challenge is particularly profound in medical imaging, where images are complex and difficult to interpret, requiring the need for domain experts with clinical experience. This implies that a longer turnaround time is needed to finalize ground-truth annotations [3,27]. Given this time-consuming process, the ratio between labeled and unlabeled data samples"}, {"title": "Background & Related Works", "content": ""}, {"title": "Active Learning", "content": "According to [20], allowing a learnable algorithm to select the data from which it learns can lead to more effective learning. In other words, AL seeks to maximize neural network performance with a smaller selection of data. As shown in Fig. 2, a typical AL scenario involves an active learner (model) continuously seeking new samples from a large pool of unlabeled data and inquiring the oracle (domain expert) for ground-truth annotations. The goal is to achieve a specific target performance, such as accuracy, while minimizing labeling costs. AL has been widely utilized in traditional machine learning tasks [20,6,13], but with the emerging use of DL achieving superior results in various tasks [24,8] comes with increasing dependence on the amount of labeled data gathered. Acquiring labeled data in task-specific or real-world problems is laborious and time-consuming, thus methods such as AL have become more relevant with its use in combination with DL, hence commonly referred to as deep AL [33].\nSample acquisition can be categorized into three main frameworks: membership query synthesis, stream-based sampling, and pool-based sampling. In real-world scenarios, it is common to acquire a large batch of unlabeled data at once, prompting the use of pool-based sampling [20]. The pool-based sampling framework assumes that there is a limited amount of labeled data and a more extensive pool of unlabeled data. To guide the active learner in selecting which data points to request labels for, samples are selected greedily via a query strategy [24]. The labels for the query are obtained by inquiring the oracle, and the data pools are consequently updated. The model is then retrained iteratively until a certain metric reaches a target value or the unlabeled data pool is exhausted. Various common query strategies, such as maximum entropy, margin, least confidence, and mean standard deviation-based approaches, have been established in AL. Moreover, the rise of frameworks like generative adver-"}, {"title": "Energy-Based Models", "content": "An EBM is a type of generative model that directly models a negative log-probability, which is also known as the energy function. The model's probability density function is derived by normalizing the energy function, expressed as $p_{\\theta}(x) = \\exp(-E_{\\theta}(x))/Z_{\\theta}$. In this formulation, $E_{\\theta}$ is the energy function parameterized by $\\theta$, and $Z_{\\theta}$ is the normalizing constant, computed as $Z_{\\theta} = \\int \\exp(-E_{\\theta}(x))dx$. A key challenge in working with EBMs is that this integral for the normalizing term is typically intractable. To estimate it, advanced sampling techniques are often employed, such as using Markov chain Monte Carlo (MCMC) and stochastic gradient Langevin dynamics (SGLD) [26].\nEBMs are utilized in a vast range of applications such as image generation [5], texture generation [28], and text generation [4,2]. EBMs have also applied in the context of dropout and pruning within NNs [19]. Additionally, EBMs can be utilized for the complex problem of continuous inverse optimal control [29]. There have been several approaches to simultaneously training an EBM and a classifier [31,15,32,11], revealing the effectiveness of EBMs for semi-supervised learning, outlier detection, and confidence calibration."}, {"title": "Confidence Calibration", "content": "Formally, considering a dataset with input x and outcome $y \\in \\{1, ..., K\\}$, a neural network is considered perfectly calibrated if:\n\n$p(Y = y | \\hat{c} = c) = c, \\forall c \\in [0, 1] \\qquad(1)$\n\nHere, $\\hat{c}$ represents the probability of a predicted label Y, and y is the ground-truth label. Model calibration is frequently depicted using reliability diagrams [10], where the expected accuracy is plotted as a function of confidence. Perfect"}, {"title": "CALICO: Confident Active Learning with Integrated Calibration", "content": "The proposed CALICO achieves efficient learning by utilizing calibrated confidence when selecting samples to be labeled. Confidence calibration is performed during the AL process by simultaneously estimating the input data distribution with an EBM and the class posterior probabilities with a classifier."}, {"title": "Algorithm of CALICO", "content": "The details of CALICO are described in Algorithm 1. Suppose that we have a limited amount of labeled data $D_l = \\{(x_i, y_i)\\}_{i=1}^{N}$, and a more extensive pool of unlabeled data $D_u = \\{x_i\\}_{i=1}^{M+N}$, where $M < N$, and $y_i \\in \\{1,..., K\\}$ is the class label of input $x_i$. The model $f_{\\theta}$ is trained over both $D_l$ and $D_u$. The use of unlabeled data is enabled by simultaneous learning with an EBM, and this"}, {"title": "Query Strategy", "content": "We utilize a least confidence strategy [24]. The least confidence query strategy is designed to acquire samples with the smallest probability among the maximum activations. Given an unlabeled data pool $D_u$, trained model $f_{\\theta}$, and the query size Q, the strategy $\\alpha_{LC}$ is defined as follows:\n\nCompute the posterior probability $p_{\\theta}(y | x)$ for all $x \\in D_u$ based on (2).\nObtain the corresponding confidence $\\max_y(p_{\\theta}(y|x))$.\nSort the samples in $D_u$ in ascending order with respect to the confidence.\nThe strategy $\\alpha_{LC}$ returns the top Q samples.\n\nBased on this strategy, a set of samples is selected from the unlabeled data pool to query the oracle for annotation."}, {"title": "Joint Learning of a Classifier and an Energy-based Model", "content": "Among various methods proposed for joint learning of a classifier and an EBM [9,31,32,11], we construct our model $f_{\\theta}$ with reference to the structure of JEM [9]. The model consists of a single neural network with a multi-head output for classification and EBM. Given an input $x \\in \\mathbb{R}^D$, the model first outputs a real-valued K-dimensional vector, i.e., $f_{\\theta} : \\mathbb{R}^D \\rightarrow \\mathbb{R}^K$. The vector is then converted into the class posterior probability in the classification head and the probability density of input data in the EBM head. In the classification head, the posterior probability of class $y \\in \\{1, . . ., K\\}$ is calculated through the standard softmax transfer function, as defined below:\n\n$P_{\\theta}(y | x) = \\frac{\\exp(f_{\\theta}(x)[y])}{\\sum_{y'=1}^{K} \\exp(f_{\\theta}(x) [y'])} \\qquad(2)$\n\nwhere $f_{\\theta}(x) [y]$ indicates the logit corresponding to y-th class. In the EBM head, the probability density $p_{\\theta}(x)$ is computed as follows:\n\n$p_{\\theta}(x) = \\frac{\\sum_{y=1}^{K} \\exp(f_{\\theta}(x)[y])}{Z(\\theta)} \\qquad(3)$\n\nwhere $Z(\\theta) = \\sum_{y=1}^{K} \\exp(f_{\\theta}(x) [y])dx$ is the normalizing constant, otherwise known as the partition function."}, {"title": "Experiments", "content": ""}, {"title": "Experimental conditions", "content": "To verify the validity of CALICO, we conducted experiments using medical image datasets. We used five benchmark medical imaging datasets found in the MedMNIST collection [30], namely Blood, Derma, OrganS, OrganC, and Pneumonia. These medical imaging datasets consist of preprocessed 28\u00d728 two-dimensional images, accompanied by their corresponding class labels. The classification tasks within these datasets range from binary to multi-class, serving as benchmarks for foundational models in the medical imaging domain.\nThe model closely followed the setup of [31]. However, we changed the ReLU activation function used by the Wide-ResNet architecture to a Swish activation function for the added stability observed by [5]. As the computational training time of JEM++ is also relatively long, we limited the number of queries for all datasets (more details in Appendix).\nWe evaluated the results based on classification accuracy and calibration errors. Miscalibration can be condensed into a convenient scalar metric, often quantified as the expected calibration error (ECE) [17]. This metric discretizes probability intervals into a fixed number of bins and the calibration error is calculated as the difference between the fraction of correct predictions (accuracy), and the mean of the probabilities in the bin (confidence). ECE computes a weighted average of this error across bins:\n\n$ECE = \\sum_{m=1}^{M_{bin}} \\frac{|B_m|}{N_{data}} |acc(B_m) - conf(B_m) | \\qquad(5)$"}, {"title": "Performance Comparison", "content": "Table 1 shows that CALICO consistently outperformed the baseline accuracy across all evaluated datasets. In parallel with the accuracy improvements, CAL-ICO also showed reduced ECEs, indicating its effectiveness in minimizing miscalibration. We observed that using a softmax-based classifier within an AL paradigm, as opposed to straightforward training methods, resulted in better calibration. However, CALICO demonstrated a more substantial increase in performance when compared to the baseline, suggesting its effectiveness in improving the classifier's performance in an AL paradigm. While CALICO generally achieved lower ECE values across most datasets, the use of a softmax-based classifier in an AL paradigm demonstrated comparable efficacy to CALICO in some cases, such as Derma and OrganS."}, {"title": "Confidence Calibration", "content": "The final reliability diagrams are depicted in Fig. 3. A perfectly calibrated model would align with the red-diagonal dashed line, effectively creating an identity"}, {"title": "Learning Curve Analysis", "content": "Learning curves play a crucial role in assessing how well a model performs in an AL paradigm, particularly by observing a specific metric with respect to the number of labeled samples. In this study, the ECE is of certain importance. Similar to the commonly used accuracy, the goal is to determine whether miscalibration can be minimized with significantly fewer labeled samples through the utilization of JEMs. As illustrated in Fig. 4, the overall calibration trend using CALICO is observed to be lower than that of the softmax-based classifiers. Additionally, it is noteworthy that a lower or comparable ECE can be achieved with fewer samples in comparison to the baseline reference."}, {"title": "Performance Comparison of Test Accuracy and ECE Values with an Equal Class Distribution", "content": "We explored the impact of class distribution balancing on CALICO's performance. While many confidence calibration studies emphasize calibration error within balanced class distributions, achieving such balance is not guaranteed in AL due to sampling methods' greedy nature. Calibration learning curves (Fig.4) exhibited instability, possibly due to overconfidence from uncertainty-based sampling. Additionally, inherent dataset class imbalances can lead to querying uninformative samples, creating a mode collapse problem and miscalibration. To simulate literature setups, CALICO's performance was evaluated with an equal class distribution (named Equal), enforcing limits based on the dataset's least represented class. Varying labels per class allowed for sufficient iterations to analyze learning curves. Experimental setup details are provided in Table 3 of Appendix.\nWe observed instances where having an equal class distribution yielded better calibration or more stable learning curves across the evaluated datasets, such as the results on the PneumoniaMNIST dataset. However, Table 2 also highlights instances where equal class distribution did not result in better calibration compared to the original CALICO. One possible explanation for this disparity is the nature of the datasets; PneumoniaMNIST is an imbalanced binary dataset, while others are relatively balanced multi-class datasets, and balancing the class ratio in sample selection facilitated effective learning of information from the minority class. These findings imply the potential of class distribution balancing to enhance CALICO's performance on imbalanced datasets."}, {"title": "Conclusion", "content": "We proposed an AL method called CALICO, which aimed to use the calibrated confidence outputs as the input for a query strategy in an AL paradigm. CAL-"}, {"title": "Appendix", "content": "Experimental Setup To ensure consistency across all experiments, the computational runtime constraints necessitated limiting each dataset to 4000 labeled samples, with a query size of 250 for each iteration, resulting in a total of 16 iterations. In the ablation study that focused on an equal class distribution, the experimental setup was detailed in Table 3. The number of labeled samples"}]}