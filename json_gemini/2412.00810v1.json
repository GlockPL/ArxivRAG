{"title": "LONG TEXT OUTLINE GENERATION: CHINESE TEXT OUTLINE\nBASED ON UNSUPERVISED FRAMEWORK AND LARGE LANGUAGE\nMODEL", "authors": ["Yan Yan", "Yuanchi Ma"], "abstract": "Outline generation aims to reveal the internal structure of a document by identifying underlying chap-\nter relationships and generating corresponding chapter summaries. Although existing deep learning\nmethods and large models perform well on small- and medium-sized texts, they struggle to produce\nreadable outlines for very long texts (such as fictional works), often failing to segment chapters\ncoherently. In this paper, we propose a novel outline generation method for Chinese, combining an\nunsupervised framework with large models. Specifically, the method first generates chapter feature\ngraph data based on entity and syntactic dependency relationships. Then, a representation module\nbased on graph attention layers learns deep embeddings of the chapter graph data. Using these chapter\nembeddings, we design an operator based on Markov chain principles to segment plot boundaries.\nFinally, we employ a large model to generate summaries of each plot segment and produce the overall\noutline. We evaluate our model based on segmentation accuracy and outline readability, and our\nperformance outperforms several deep learning models and large models in comparative evaluations.", "sections": [{"title": "Introduction", "content": "Well-written stories are often composed of numerous semantically coherent chapters, with each chapter or group of\nchapters centered around a specific theme. An outline can concisely capture the content structure of a document,\nproviding clear guidance for navigation and significantly reducing the cognitive burden of understanding the entire text.\nFurthermore, it helps uncover the underlying thematic structure of the text. Outline generation captures various thematic\nelements of a text, including subtitles, plot points, and other key aspects of the narrative. Additionally, outlines facilitate\na wide range of text analysis applications. They are not only beneficial for traditional downstream NLP tasks, such as\ndocument summarization Xiao and Carenini (2019) and discourse parsing Huber et al. (2022), but also play a crucial\nrole in large language models (LLMs). For example, during retrieval-augmented generation (RAG) in large language\nmodels Shi et al. (2024), it is essential to extract the necessary information from long documents. The paragraph-level\nthematic structure of a document can aid in quickly locating the approximate position of the required content within a\nlengthy text, thereby reducing the search space. The relevant process concept is outlined as follows.\nPrevious work on outline generation Zhang et al. (2019); Zhou et al. (2015a) has primarily focused on short- and\nmedium-length texts, such as news articles and announcements, helping readers quickly grasp the structure of the\ncontent. The vertical domains involved include sociology, psychology and economics Ma et al. (2023). These methods\ntypically generate outlines based on natural paragraphs. However, for fictional works such as Game of Thrones (GoT),\nthe Marvel Comics Universe (MCU), Greek mythology, or epic novels like Leo Tolstoy's War and Peace or Don\nWinslow's The Cartel, the task of outline generation is much more challenging due to their length and intricate semantic\nstructures Hertling and Paulheim (2020). While novels are generally meant for entertainment, some of these works\nalso reflect subcultural trends and capture the zeitgeist of particular eras. Analyzing their narrative structures and\nnetworks is of great interest to scholars in the humanities. For instance, War and Peace is set against the backdrop\nof Napoleon's wars in Russia, while The Cartel trilogy intertwines both fact and fiction concerning drug trafficking.\nTherefore, generating outlines for such long texts not only helps general readers quickly understand the relevant plots\nbut also provides a valuable data foundation for historians, social scientists, media psychologists, and cultural studies\nscholars to conduct deeper analyses of these complex works Chu et al. (2021).\nLong fictional texts are composed of significant plot points, where the focus is less on basic relationships such as\nbirthplaces or spouses, and more on specific events like alliances, betrayals, killings, or clan conflicts. These fictional\nworks often have large fan communities, and search engines frequently receive queries like \"When does Xiao Yan\nobtain the Bone Spirit Cold Flame?\" (Battle Through the Heavens) Hua et al. (2016b). For texts spanning millions of\nwords, it is evidently challenging to locate a particular plot without an outline. Therefore, it is necessary to develop\nan outline generation method tailored to long fictional texts to address this problem. Upon further examination of the\noutline generation challenge, we observe that it actually involves two structured prediction tasks: (1) identifying chapter\nfeatures and plot boundaries, and (2) generating chapter summary titles. These two tasks correspond to predicting\nthe hierarchical relationship between chapters and summarizing individual chapters. While the second task can be\nwell-handled by existing large language models (LLMs), particularly for short- and medium-length texts, LLMs often\nexhibit inaccuracies and increased hallucination issues when applied to longer texts. For instance, when tasked with\nsummarizing a work of over a million words, LLMs may overlook key plot points, preventing readers from fully\ngrasping the narrative [add experiment]. Therefore, we consider whether an ideal outline generation framework could\nextend the strengths of large models to ultra-long texts. The key challenge here lies in accurately identifying the\nsequence and features of chapters to obtain precise plot boundaries.\nIn our work, we propose a new end-to-end architecture to address this challenge. The key idea is to enhance large\nlanguage model (LLM) outputs by guiding them with enriched information, specifically by determining plot boundaries\nthrough a neural network before using them to guide the LLM in generating a detailed outline. We posit that graph\ndata can better represent relationships between entities within chapters, thus reflecting chapter characteristics more\neffectively. Therefore, our method first generates entity nodes through a chapter-level graph data generation module,\nfollowed by constructing the adjacency matrix between nodes based on syntactic dependency relationships. For node\nfeature vectors, we not only select entity word vectors but also expand the feature set to include the tf-idf matrix of\nthe entities, and we incorporate chapter numbers to represent contextual coherence. We then apply an improved graph\nneural network (GNN) based on graph attention layers (GAT) to learn from the chapter graph data. To this, we add\na convolutional module for feature extraction and dimensionality reduction of deep chapter graph embeddings. For\neach chapter embedding, we perform chain-based prediction: specifically, we determine significant plot points and\nboundaries using Markov chains and path dependence based on their potential distances in feature space. Finally, LLMs\nare used to generate the themes and summaries for each plot segment, resulting in the final outline.\nTo facilitate our research, we constructed a new benchmark dataset in Chinese. As we observed, ultra-long texts in\nthe domain of fictional literature often consist of millions of words. In this dataset, we not only provide the original\nliterary works but also manually generated outlines to serve as a reference for evaluating experimental performance. For\nassessment, we compared several state-of-the-art methods to verify the effectiveness of our model, including rule-based\nmodels and large language models. The experimental results demonstrate that our proposed method significantly\noutperforms all baselines. We also conducted a detailed analysis of the proposed model, including an analysis of the\nreadability of the generated outlines, to better understand the learned content structure.\nThe contributions of our work are summarized as follows:\n\u2022 We developed a model for the task of outline generation for ultra-long documents, presenting a novel solution\nthat combines unsupervised learning frameworks with large models.\n\u2022 We established a public dataset for the outline generation (OG) task, which includes multiple ultra-long texts,\neach exceeding a million words, along with corresponding outlines.\n\u2022 Extensive experiments were conducted to validate the effectiveness of the proposed model, and the results\nshow that our method achieves state-of-the-art performance."}, {"title": "Related Works", "content": "To the best of our knowledge, the tasks most closely related to outline generation for ultra-long fictional texts are Named\nEntity Recognition (NER), storyline generation, and outline generation, all of which have been extensively studied over\nthe past few decades."}, {"title": "NER", "content": "Named Entity Recognition (NER) is a classical problem in natural language processing, aimed at automatically\nextracting named entities and their relationships from documents. In our research, NER is used to establish chapter\nnode features. Early work on relationship extraction from text sources employed rules and patterns (e.g., Agichtein and\nGravano (2000); Reiss et al. (2008)). Open Information Extraction (Open IE) methods Mausam (2016); Stanovsky et al.\n(2018) use linguistic cues to infer patterns and triplets collectively, but they lack appropriate SPO (Subject-Predicate-\nObject) parameter normalization. With the recent advancements in pre-trained language models such as BERT, as well\nas ElMo, GPT-3, T-5, and others, the best current NER methods leverage these models for representation learning Cui\net al. (2021); Ghosh et al. (2023); Kim et al. (2024)."}, {"title": "Storyline generation", "content": "Storyline generation aims to summarize the development of certain events and understand how they evolve over time.\nHuang and Huang (2013a) formalized different types of sub-events into local and global aspects. Several studies have\nused Bayesian networks for storyline detection Hua et al. (2016a); Zhou et al. (2015b). Lin et al. (2012) first obtained\nrelevant tweets, and then generated story lines through graph optimization to extract the story plot of events. In Huang\nand Huang (2013b), an evolutionary hierarchy Dirichlet process was proposed to capture the theme evolution pattern\nin the plot summary. The current story line extraction focuses more on multi-modal data, such as Yang et al. (2024)\ngenerating video story lines through structured story lines."}, {"title": "Plot division and outline generation", "content": "Early work primarily used unsupervised methods Choi (2000); Glavas et al. (2016) for topic segmentation. As large-\nscale thematic structure corpora were developed, supervised methods gradually became mainstream, such as sequential\nlabeling models Badjatiya et al. (2018); Lukasik et al. (2020). Only a few studies have focused on Chinese topic\nsegmentation tasks using sequential labeling models following English methodologies Wang et al. (2016); Xing et al.\n(2020) or local classification models Jiang et al. (2021) to predict topic boundaries.\nMost of the existing outline generation methods Sun et al. (2022); Zhang et al. (2019) are for small and medium\ntext, and more attention is paid to English. In our work, we introduce methods related to named entity recognition,\ntitle generation, and storyline generation; however, there are some notable differences. First, the NER task can only\nidentify named entities and their relationships but cannot systematically construct graph data. Second, traditional plot\nsegmentation and outline generation tasks output single document-level titles with coarse-grained semantics, whereas\nour outline generation (OG) task outputs a sequence of plot-level titles with fine-grained semantics and contextual\nidentification. Lastly, storyline generation is based on multiple sub-events along a timeline, whereas the OG task focuses\non multiple sections. Therefore, most of the existing methods for these related tasks may not be directly applicable to\nthe OG task."}, {"title": "Method", "content": "For the overall architecture of our method, we divide it into five steps. These steps will be discussed in detail in this\nchapter."}, {"title": "The text is processed in chapters", "content": "For ultra-long fictional texts, it is necessary to construct an overall outline based on chapter information. Therefore, the\nfirst step is to divide the entire text according to its chapters. We use a regular expression method to match chapter titles,\nwhere each chapter begins with a label such as \"Chapter X.\" This chapter title label allows for an effective segmentation\nof the entire text. In the context of Chinese, since Chinese characters are not formed by alphabet-like symbols, word\ntokenizers do not break words into smaller segments. Instead, they use Traditional Chinese Word Segmentation (CWS)\ntools to split the text into several words. This allows for the application of whole-word masking in Chinese, masking\nentire words instead of individual characters. To implement this functionality, the original whole-word masking code\nmust be strictly followed, without altering other components such as the percentage of word masking. The LTP tool is\nused for Chinese word segmentation to identify word boundaries. Similarly, whole-word masking can be applied to\nROBERTa without using the NSP task."}, {"title": "Construction of chapter level node eigenvector and adjacency matrix", "content": "In text feature extraction, methods such as term frequency are often used, where important entity words represent the\noverall text features Kouissi et al. (2023); Li et al. (2023). For each chapter of the text, we need to construct graph data\nto represent the content of the chapter. To do this, we first select chapter nodes and construct the feature vectors and\nadjacency matrices for these nodes.\nTherefore, for chapter-level text data, we utilize the LTP tool Che et al. (2021) for processing. Its core functionalities\ninclude word segmentation, part-of-speech tagging, named entity recognition (NER), and syntactic dependency analysis,\namong other sub-tasks. Similar to THULAC Li and Sun (2009), LTP is also based on a structured perceptron (SP) and\nuses the maximum entropy principle to model the scoring function of the label sequence Y given the input sequence X.\n$S(Y, X) = \\Sigma_{s} \\theta_{s} (Y, X)$\nHere, $\\theta$ represents the local feature function. The Chinese word segmentation problem is equivalent to solving for the\nlabel sequence Y corresponding to the score function, given the input X.\nThrough the NER task, we can extract named entities. Using LTP for part-of-speech tagging, we select entities with\nnoun tags as chapter nodes, which include key plot information such as the main characters, locations, and items\nwithin the chapter. After obtaining the nodes, the relationships between the entity nodes can be determined based\non syntactic dependency information, allowing us to construct the adjacency matrix for the nodes within the chapter.\n$E(x, y) \\in \\{0,1\\}$\nTo construct node features, we consider using the tf-idf matrix to highlight both node and chapter features. Therefore,\nwe propose a new method for constructing chapter node features, which includes important attributes such as the entity\nname of the node, the chapter number, and the tf-idf value of the entity node.\nFor obtaining the vector for the entity name of a node, we consider using the BERT-WMM model Cui et al. (2021). For\nthe tf-idf[] values of entity nodes, we first filter the top 10 tf-idf values within the chapter, and then assign these tf-idf\nvalues to the chapter nodes. Each entity node corresponds to its respective tf-idf matrix value. If an entity node has one\nof the top 10 tf-idf values, the value is appended; otherwise, the corresponding matrix value is set to 0. This results in a\n10-dimensional tf-idf value matrix, where each row represents a feature vector for an entity node. Finally, the number\nof chapters is combined with the above features."}, {"title": "Chapter deep embedding", "content": "For learning chapter features, we have constructed an unsupervised learning model, a graph autoencoder based on\nGAT Velickovic et al. (2017) layers, to extract chapter features by learning node features and the adjacency matrix. The\ncore idea is to use GAT to focus on each node's neighbors in order to learn the hidden representation of the current\nnode. At the same time, the AE model captures the feature vector attribute $x_{i}$ of node $v_{i}$. The most straightforward\nstrategy for processing the neighbors of a node is to aggregate the node's representation equally with all of its neighbors.\nHowever, the importance of different neighboring nodes varies, which results in different weights being assigned to\nthem. Based on the multi-head attention mechanism, the GAT network effectively strengthens the weights of important\nneighboring nodes while diminishing the weights of irrelevant ones. The computation of the hidden representation of\nthe current node $v_{i}$ is as follows:\n$Z = \\sigma(\\Sigma_{j \\in N_{i}} \\alpha_{ij} WZ^{-1})$\n$\\alpha_{ij}$ represents the attention factor, signifying the importance of neighbor node $v_{j}$ to node $v_{i}$, and $\\sigma$ denotes a nonlinear\nfunction. W is a hyperparameter. Z corresponds to the output representation of node $v_{i}$, and $N_{i}$ refers to the neighbors\nof node $v_{i}$. The subsequent step involves assessing the significance of neighbor node $v_{j}$ with consideration for both\nattribute value and topological distance, ultimately determining the attention coefficient $A_{ij}$.\nIn terms of node attribute values, the $\\alpha_{ij}$ can be expressed as a single-layer feedforward neural network for $x_{i}$ and $x_{j}$ in\nseries with a weight vector of $a \\in R$.\n$d_{ij} = a(W\\vec{x_{i}}, W\\vec{x_{j}})$\nNote that the coefficients are usually normalized between all neighbors $v_{j} \\in N_{i}$ with the softmax function, making\nthem easy to compare between nodes.\n$\\alpha_{ij} = softmax_{j}(d_{ij}) = \\frac{exp(d_{ij})}{\\Sigma_{k \\in N_{i}} exp(d_{ik})}$"}, {"title": "Plot boundary division", "content": "Following this, the representation of the current target node by its neighboring nodes in terms of topology becomes\nessential. GAT, in its original form, concentrates solely on the 1-hop neighboring nodes (first-order) of the current node\nVelickovic et al. (2017). However, given the intricate structural relationships within graphs, there arises a need for\nhigher-order neighboring node relationships. To address this, we stack n layers of GAT, enabling the current node $v_{i}$ to\nretain information about higher-order neighbors. This can be formulated as:\n$H = \\Sigma_{j \\in N} \\alpha_{ijj} = \\Sigma_{j \\in N} \\alpha_{ij} (x_{1}, x_{2}....., x_{n}), x_{i} \\in R^{N \\times d}$\nWe choose to reconstruct the graph structure as part of the decoder, which uses the sigmoid function to map (-8,+8)\nto the probability space. We minimize the reconstruction error by measuring the difference between $A_{i}$ and $\\hat{A}$, where\n$\\hat{A}$ is the reconstructed structure matrix of the graph.\n$L_{r} = \\Sigma_{i=1}^{n} loss(A_{i}, \\hat{A}), \\hat{A} = sigmoid(Z^{T}Z)$\nThen we reduce the learned deep embeddings to 2-dimensional data on the feature space through a pooling layer to\nobtain chapter feature embeddings Z.\nWe propose a new method based on the principles of path dependence and Markov chains Prasad and Nesgos (1974),\nreferred to as DMc. This allows us to further predict plot boundaries based on the learned chapter features. This step\ninvolves dividing the segmented ultra-long document $C_{1}, C_{2}, C_{3}, ..., C_{n}$ into multiple consecutive parts $\\{s_{1}, s_{2},...,s_{N}\\}$\nby predicting the section boundary labels $\\{l_{1}, l_{2}, ..., l_{M} \\}$, where $l_{M} \\in \\{0,1\\}$. If $l_{m} = 0$, then $C_{n}$ is a chapter within a\nplot boundary, and the section prediction continues. If $l_{m} = 1$, then $e_{n}$ is the last chapter of a plot segment, and an\nappropriate title should be generated. Some literature points out that paragraphs are coherent units of information; we\nconsider chapters as sequences of coherent paragraphs, and coherence modeling is inherently non-trivial. The properties\nof Markov chains can help address consistency between contexts and identify paragraph boundaries. However, it is\nundeniable that plot boundaries are still related to content mentioned in the previous chapters. Therefore, we introduce\nthe principle of path dependence and construct an operator mechanism to predict plot boundaries in ultra-long texts.\nThe operator specifically extends the process of examining the previous chapter of the target chapter to considering the\nprevious $\\alpha$ chapters in order to determine whether the chapter is a plot boundary. As shown in Figure 2, we use the\nhidden representations of the current chapter $C_{m}$, the previous $\\alpha$ chapters $C_{m-\\alpha}$, and the next chapter $C_{m+1}$ to predict\nthe segment boundary label $l_{m}$. We set a threshold $s_{t}$, determined by the EU (Embedding Unit) of the previous $\\alpha$\nchapters, with the following formula:\n$s_{t} = \\beta \\cdot mean(EU)$\nwhere $\\beta$ is the learning parameter. If $EU(c_{m+1}) > s_{t}$, then the current chapter $c_{m}$ is considered a plot boundary.\nOtherwise, it is considered to be part of the same plot. Next, we set a safety distance $d_{a}$, which represents the minimum\nnumber of chapters that we consider as part of the ongoing plot to save computational resources. Therefore, the operator\nwill continue searching for the plot boundary after $C_{m+d_{a}}$."}, {"title": "Outline of stories based on LLM", "content": "After the plot content is summarized, chapter boundaries and related instructions are input into the large model to obtain\nthe overall text outline."}, {"title": "Experiments", "content": ""}, {"title": "Dataset", "content": "In this experiment, a Chinese data set containing 31 ultra-long texts for outline generation is constructed for us. The\ntopics covered include adventure, fantasy, fairy, biography, classics five categories. The average text size is 2234.63kb\nand contains an average of 580.8 chapters.As Table 1"}, {"title": "Baseline", "content": "GPT 3.5: GPT-3.5 is a large language model based on the GPT-3.5 architecture that utilizes a network of transformers\nto perform various tasks such as dialogue, text completion, and language translation.\nGPT 4.0: A new generation of GPT models, exceeding 3.5 in both size and performance.\nLama7b: Excellent open source large model based on transformer"}, {"title": "Evaluation Metric", "content": "To evaluate the performance of the proposed method, we used the accuracy, recall rate, and F-score commonly used to\nevaluate information extraction systems. Accuracy is calculated based on the following conditions: whether the division\nof plot boundaries is correct. In addition, we also evaluated the readability of the generated outline from two different\nevaluation indexes. These are CheckEval Framework and Kendall tau."}, {"title": "Result", "content": "We evaluate the model's performance from two aspects: boundary prediction accuracy and the readability of the\ngenerated outline. First, we test boundary prediction accuracy on both our constructed dataset and two publicly available\ndatasets, as shown in Table 2. Additionally, the readability of the generated outline is tested using the CheckEval\nframework and the Kendall Tau correlation, as presented in Fig 3.\nFurthermore, we conduct detailed experiments on each ultra-long text in our constructed dataset, with the relevant\nexperimental data provided in Appendix A. This includes tests for plot boundary prediction accuracy and readability\nanalysis for each book. We also provide several detailed outline examples to demonstrate the readability of our model.\nFrom the above experimental results, it is obvious that our method is better in predicting the accuracy of plot boundaries.\nThis makes our generation outline more accurate. In addition, two index tests on outline readability show that our\ngenerated outline is more readable."}, {"title": "Conclusion", "content": "In this paper, we propose a method based on plot segmentation to guide large models in generating better outlines for\nultra-long texts. First, the chapter graph data effectively captures chapter feature information. Based on the chapter\nembeddings learned by the GAT, we use an improved Markov chain to divide the plot boundaries. Finally, the large\nmodel accurately generates the plot content for each boundary, which is then aggregated into the outline. When"}]}