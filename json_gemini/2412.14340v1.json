{"title": "A Unifying Information-theoretic Perspective on Evaluating Generative Models", "authors": ["Alexis Fox", "Samarth Swarup", "Abhijin Adiga"], "abstract": "Considering the difficulty of interpreting generative model out-put, there is significant current research focused on determin-ing meaningful evaluation metrics. Several recent approachesutilize \"precision\" and \"recall,\" borrowed from the classifica-tion domain, to individually quantify the output fidelity (re-alism) and output diversity (representation of the real datavariation), respectively. With the increase in metric proposals,there is a need for a unifying perspective, allowing for easiercomparison and clearer explanation of their benefits and draw-backs. To this end, we unify a class of kth-nearest-neighbors(kNN)-based metrics under an information-theoretic lens us-ing approaches from kNN density estimation. Additionally,we propose a tri-dimensional metric composed of PrecisionCross-Entropy (PCE), Recall Cross-Entropy (RCE), and Re-call Entropy (RE), which separately measure fidelity and twodistinct aspects of diversity, inter- and intra-class. Our domain-agnostic metric, derived from the information-theoretic con-cepts of entropy and cross-entropy, can be dissected for bothsample- and mode-level analysis. Our detailed experimentalresults demonstrate the sensitivity of our metric components to their respective qualities and reveal undesirable behaviors of other metrics.", "sections": [{"title": "1 Introduction", "content": "Need for Metrics. Deep generative models, including Gener-ative Adversarial Networks (GANs) (Goodfellow et al. 2014),Variational Autoencoders (VAEs) (Kingma and Welling2022), and diffusion models (Sohl-Dickstein et al. 2015),are achieving unprecedented realism in their outputs (Ravuriet al. 2023). Their success highlights an urgent need for robustmethods to evaluate their output quality. Unlike discrimina-tive models, which are evaluated against a clear ground truthin the form of labeled test sets, generative models produce aspectrum of plausible outputs based on the learned data dis-tribution. Hence, evaluating generative models is nontrivial.\nTraditional 1D Metrics. Often, an important goal of gen-erative models is to produce outputs that human observersfind realistic (Stein et al. 2024). While human assessment"}, {"title": "Motivation for 2D Metrics.", "content": "However, as one-dimensionalscores, IS and FID cannot distinguish various shortcom-ings, e.g. when outputs are generated in low-probability re-gions of the real distribution (low fidelity) or insufficientlyrepresent high-probability regions (low diversity) (Sajjadiet al. 2018). A recent set of two-dimensional metrics, whichcan be broadly categorized as versions of \u201cprecision\u201d and \u201cre-call,\" has emerged to differentiate the several types of failuremodes. Sajjadi et al. (2018) introduce the first of a series ofprecision and recall metrics based on decomposing the realand generated distributions into common and unique com-ponents. Subsequent works propose alternatives to precisionand recall, with most utilizing k-Nearest-Neighbor (kNN)methods (Kynk\u00e4\u00e4nniemi et al. 2019; Naeem et al. 2020; Alaaet al. 2022; Cheema and Urner 2023; Park and Kim 2023)."}, {"title": "The Gap.", "content": "Despite the proliferation of evaluation methods,there remains a need for clearer theoretical comparisons andvalidation of these metrics. We address this gap by providingprobabilistic derivations with information-theoretic interpre-tations for a subset of metrics, focusing on Precision RecallCover (Cheema and Urner 2023) and Density & Coverage(Naeem et al. 2020).\nWe also introduce a simple new information-theoretic met-ric that, unlike existing metrics, simultaneously distinguishesbetween the three failure modes visualized in Figure 1. Modeinvention (1b) indicates a loss of precision (fidelity), occur-ring when the model generates implausible outputs that donot align with the real distribution. Mode dropping (1a) repre-sents one type of recall (diversity) loss, where the model lackssufficient coverage of certain real regions. Another recall is-sue, mode shrinkage (1c), happens when a model repeatedlyproduces highly similar points\u2014lacking intra-class diver-sity-that represent a mode \"average.\u201d We summarize ourthree main contributions below."}, {"title": "Unifying Existing Metrics.", "content": "In Section 4, we provideinformation-theoretic derivations for Precision Recall Coverand Density & Coverage. By extending lemmas from Noshadet al. (2017), we derive a bias term for the expected value ofthese metrics, allowing for easier interpretation and compari-son."}, {"title": "Novel Information-Theoretic Metric.", "content": "We propose a newmetric for generative models, composed of precision cross-entropy (PCE), recall cross-entropy (RCE), and recall en-tropy (RE), based on the estimator from Leonenko, Pronzato,and Savani (2008). Unlike previous work, we separate the twotypes of diversity loss. Section 5 details the definition of thesescores as well as how they discern the various failure modesmentioned. We also describe how our population-level metriccan be dissected for both mode- and sample-level analysis,useful for diagnostic purposes."}, {"title": "Experimental Results.", "content": "We list desiderata for generativemodel metrics in Section 5.1. Through our experimental re-sults in Section 6, we show how our metrics align with theseideals, as well as how other existing metrics are less effectivewith respect to one or more of these. Specifically, we analyzethe sensitivity of the measures to the different failure modesmentioned above, as well as examine their correlation withhuman scoring. As with most prior work, we focus on imageevaluation, but our methods can be easily generalized to otherdata modalities."}, {"title": "2 Background and Related Work", "content": "Several metrics have been proposed to quantify the perfor-mance of generative models. Below we summarize relevantmetrics and their respective benefits and shortcomings. Westart with common background information.\nBackground. Calculating distances in raw input spaces,such as image pixels, is often impractical due to the highdimensionality and sensitivity to noise. To address this, manymetrics including ours - embed data into a feature space that provides a more semantic representation, where Eu-clidean distances correlate better with human-perceived simi-larity. While expensive, human perception can be used as a\"ground truth\" for other metrics for realism. One benchmark,"}, {"title": "Human eYe Perceptual Evaluation (HYPE)", "content": "measures humanerror rate when identifying fake and real images, serving as aproxy for realism. (Zhou et al. 2019)."}, {"title": "Classic Metrics.", "content": "The Inception Score and log-likelihoodhave historically been popular metrics but are criticized fortheir poor correlation with human judgment. (Theis, Oord,and Bethge 2015; Kolchinski et al. 2019). Fr\u00e9chet InceptionDistance (FID), which we later compare experimentally, isanother widely used metric for evaluating generated images.FID evaluates images by comparing the Fr\u00e9chet distanceof generated and real images in an Inception-based featurespace, modeled as continuous multivariate Gaussians (Heuselet al. 2017). FID is consistent with human judgment (Steinet al. 2024); however, it makes the assumption of Gaussian-distributed features, which may not always hold (Borji 2022).Additionally, its one-dimensional nature does not let it dis-cern different failure modes, which is one motivation for thefollowing multi-dimensional metrics."}, {"title": "Early Notion of Precision and Recall.", "content": "Sajjadi et al. (2018)introduce the novel Precision-Recall for Distributions (PRD)metric, which compares embedded samples from the realand generated distribution within a shared feature space, us-ing k-means clustering assignments to assess whether thedifferences in their common support should be attributed toprecision (accuracy) or recall (diversity) loss. Recognizingthe limitations of the PRD metric, which relies on relativedensities and yields a continuum of values along a PR-curve,Kynk\u00e4\u00e4nniemi et al. (2019) propose an improved version ofprecision and recall (IP & IR) leveraging kNN distancesof samples embedded in a feature space. They measure pre-cision by assessing if each generated image is within thereal image manifold, estimated with the union of kNN ballscentered on each image. Symmetrically, recall evaluates ifeach real image is within the generated image manifold."}, {"title": "Recent Extensions.", "content": "Several subsequent works can be seenas extensions of the approaches by Sajjadi et al. (2018) andKynk\u00e4\u00e4nniemi et al. (2019). While superficially different,most of these are building on common underlying computa-tional ideas. We focus in particular on the work of Cheemaand Urner (2023), who propose precision coverage and re-call coverage (PC & RC, collectively defined as PRC),which differs from previous works by parameterizing what isconsidered \"sufficient\u201d coverage, and on the work of Naeemet al. (2020), who observed that IP & IR are sensitive to out-lier presence and fail to identify identical distributions. Theydeveloped the more robust density and coverage measures(D&C). We focus on these two because they are either recent,or have particular limitations (on which, more in Section 4).However, other recent approaches can be seen in a similarinformation-theoretic light (Alaa et al. 2022; Djolonga et al.2020; Liu et al. 2021), which we will comment upon brieflyin the discussion.\nWe note that no metric discussed above simultaneouslydistinguishes between inter- and intra-class diversity, which isa strength of our metric detailed in Section 5. The rarity score(Han et al. 2022), outside the precision and recall framework,quantifies the uniqueness of generated samples which, on anaggregate-level, can be viewed as a measure of intra-classdiversity."}, {"title": "3 Preliminaries", "content": "Given a (multivariate) random variable X with distribu-tion $f_X$, let $X = \\{X_1,..., X_{N_X}\\}$ be a set of $N_X$ i.i.d.samples of X, with each $X_i \\in \\mathbb{R}^d$. In this work, wewill frequently compare two distributions, letting X and Y rep-resent two general distributions, while R and G specificallyrepresent the real and generated distributions, respectively.Given sets of samples X and Y of X and Y, Z is the com-bined set of points from X and Y, denoted by $Z := X \\cup Y$."}, {"title": "3.1 Information Theoretic Concepts", "content": "For completeness, we present some foundational conceptsfrom information theory, which we will later integrate intothe analysis of generative metrics. Shannon entropy of arandom variable X is defined as $H(X) = \\mathbb{E}_{x \\sim X} \\log f_X(x)$while cross entropy of X relative to Y is $CE(X,Y) =\\mathbb{E}_{x \\sim X} \\log f_Y(x)$. KL-divergence of X from Y is defined as\n$D_{KL}(X||Y) = \\mathbb{E}_{x \\sim X} \\log \\left(\\frac{f_X(x)}{f_Y(x)}\\right)$.\nR\u00e9nyi divergence, parameterized by order \u03b1 (\u03b1 \u2260 1 and\u03b1 > 0), is a generalization of KL-divergence that similarlyquantifies the informational difference between two distribu-tions (R\u00e9nyi 1961). The formal definition is as follows:\n$D_\\alpha(X||Y) = \\frac{1}{\\alpha - 1} \\log \\int f_X(x)^\\alpha f_Y(x)^{1-\\alpha} dx \\\\= \\frac{1}{\\alpha - 1} \\log J_\\alpha (f_X, f_Y),$\nwhere $J_\\alpha (X, Y) = \\mathbb{E}_{X \\sim Y} [\\left(\\frac{f_X(x)}{f_Y(x)}\\right)^{\\alpha - 1}]$ (Noshad et al. 2017).\nAs \u03b1 \u2192 1, R\u00e9nyi divergence approaches the KL-divergence.\nWe note that \u03b1 = 1 is defined for $J_\\alpha (X, Y)$."}, {"title": "3.2 kNN-based Estimators", "content": "In practical applications, the full distribution of X or Y is of-ten not known, or the integrals required to compute measuressuch as entropy or divergence are intractable. As a result,direct computation of these measures is not feasible, neces-sitating the estimation of these values from available data.Multiple methods (Noshad et al. 2017; P\u00f3czos and Schnei-der 2011; Leonenko, Pronzato, and Savani 2008) have beenproposed that utilize kth-nearest-neighbors of sample pointsto estimate probability density ratios. Below we introducenotation and definitions for such estimators.\nkNN Density Estimation. Let $B_{k, X} (X_i)$ be a hyperspherein $\\mathbb{R}^d$ centered at the point $X_i$, with its radius determinedby the distance to the k-th nearest neighbor in the set X.Both the radius and number of points within a kNN ball canbe used for density estimation. We define $\\#(Y, B_{k, X}(X_i))$as the number of points from set Y found within the ball$B_{k, X}(X_i)$. To accommodate the distance-based nature ofother estimators, we define $D_{k,Y}(X_i)$ as the distance from$X_i$ to its k-th nearest neighbor in the set Y.\nEntropy and Cross Entropy Estimator. Leonenko, Pron-zato, and Savani (2008) propose direct, nonparametric estima-tors for cross entropy and entropy. These estimators leveragekNN distances as a proxy for probability density."}, {"title": "The estimators for entropy, Hk(X), and cross entropy, CE(X, Y), are defined below:", "content": "$H_k(X) = \\frac{1}{N_X} \\sum_{i=1}^{N_X} \\log ((\\Psi - 1) e^{\\Psi(k)} \\Gamma(D_{k,X}(X_i))^d)$\\\\$CE_k(X,Y) = \\frac{1}{N_X} \\sum_{i=1}^{N_X} \\log \\left(\\frac{N_Y}{V} e^{\\Psi(k)} (D_{k,Y}(X_i))^d\\right)$\nHere, \u03a8(z) = \u0393'(z)/\u0393(z) represents the digamma func-tion and is the volume of a unit ball in $R^d$.\nDivergence Estimator. Noshad et al. (2017) define an esti-mator for R\u00e9nyi divergence utilizing nearest neighbor ratios.Their method calculates the ratio of the number of pointsfrom X to that of Y found within the hypersphere, $B_{k,Z} (Y_i)$,to estimate $\\frac{f_X}{f_Y}$. Noshad et al. (2017) derive the followingestimators for $J_\\alpha(X||Y)$ and $D_\\alpha(X||Y)$, with \u03b7 = $N_Y/N_X$:\n$J_\\alpha(X,Y) = \\frac{\\eta^\\alpha}{\\binom{N_Y}{Y}} \\sum_{i=1}^{N_Y} \\frac{\\#(X, B_{k,Z}(Y_i))^{\\alpha - 1}}{\\#(Y, B_{k,Z}(Y_i)) + 1}^{\\alpha - 1}$ \\\\$D_\\alpha(X,Y) = \\frac{1}{1 - \\alpha} \\log J_\\alpha (X, Y)."}, {"title": "4 Unifying Information Theoretic Perspective", "content": "Below we examine the formulae for two metrics\u2014PrecisionRecall Cover and Density & Coverage in order to expressthem within a unified information theoretic perspective. Wethen use our results to motivate our metric."}, {"title": "4.1 Precision Recall Cover", "content": "Cheema and Urner (2023) propose the Precision Recall Cover(PRC) metric, consisting of Precision Coverage (PC) andRecall Coverage (RC). For their population-level analogue,they define two parameters, \u03b1 and \u03b2, which allow for flexibil-ity in determining what is considered \u201csufficiently\u201d coveredfor both generated and real regions. Cheema and Urner (2023)additionally provide a kNN-based empirical definition, whichwe describe below.\nEmpirical (k, k')-PRC Definition. For two positive in-tegers k = Ck' where C is a positive integer as well, let$A = \\frac{R}{N_G}$ and $B = \\frac{k'}{k}$. The (k, k')-Precision Coverage ofsample set R with respect to G is defined by constructing kand k'-nearest neighbor balls over the sample sets:\n$PC_{k,k'} (R, G) = \\frac{1}{N_G} \\sum_{i=1}^{N_G} 1[\\#(R, B_{k,G}(G_i)) \\geq k']$,\nwhere 1[\u00b7] is the indicator variable of the specified event.This method calculates the proportion of kNN balls in G thatcontain at least k' points from R, normalized by the totalpoints in G. (k, k')-Recall Coverage is defined similarly:\n$RC_{k,k'} (R, G) = \\frac{1}{N_R} \\sum_{i=1}^{N_R} 1[\\#(G, B_{k,R}(R_i)) \\geq k']$."}, {"title": "Connecting PRC.", "content": "To set up its definition to be compatiblewith our perspective, we rearrange the expression of PC. Re-sults for RC generalize symmetrically. By definition of #(\u00b7),the quantity $\\#(G, B_{k,G}(G_i))$ refers to the number of pointsfrom G within the k-nearest neighborhood of $G_i$, which isexactly k. The term in (5) can be expressed as:\n$\\#(R, B_{k,G}(G_i)) \\geq k' \\Rightarrow \\frac{\\frac{1}{N_G} \\cdot \\#(R, B_{k,G}(G_i))}{\\frac{1}{N_G} \\cdot \\#(G, B_{k,G}(G_i))} \\geq \\frac{k'}{k}$.\nTherefore, noting that \u03b7 = $\\frac{N_R}{N_G}$,\n$PC_{k,k'} (R, G) = \\frac{1}{N_G} \\sum_{i=1}^{N_G} 1[\\eta \\cdot \\frac{\\#(R, B_{k,G}(G_i))}{\\#(G, B_{k,G}(G_i))} > \\frac{k'}{k}]$.\nBy (3), with \u03b1 \u2192 1 and U := R \u222a G:\n$J_1(R, G) = \\frac{1}{N_G} \\sum_{i=1}^{N_G} \\frac{\\eta \\cdot \\#(R, B_{k,U} (G_i))}{\\#(G, B_{k,U}(G_i)) + 1}$.\nWe define an indicator function version of the above and itscorresponding empirical version as:\n$\\hat{J_1}(R,G) = \\frac{1}{N_G} \\sum_{i=1}^{N_G} 1[\\frac{\\eta \\cdot \\#(R, B_{k,U}(G_i))}{\\#(G, B_{k,U}(G_i)) + 1} > \\frac{k'}{k}]$\n$\\hat{J_1}(R, G) = \\mathbb{E}_{x \\sim G} \\left[1\\left[\\frac{f_R(x)}{f_G(x)}\\right]\\right]$.\nSince the indicator function is not Lipschitz continuous, asrequired by Lemma 3.2 of Noshad et al. (2017), we definea close sigmoid approximation to derive a bias term in theAppendix, where $\\sigma(x; \\theta) = \\frac{1}{1+e^{-\\theta(x-\\tau)}}$, with \u03c4 controllingthe steepness of the sigmoid (higher \u03b8 approaching closer tothe indicator function). As seen above, the definition of PC(Eqn. 8) and the estimator $\\hat{J_1}$ (Eqn. 10) differ in only twoaspects: PC calculates the nearest neighbors within the set Rinstead of U, and PC has no additional 1 in the denominator.In the Appendix, we show how these differences affect thebias. We also define an information theoretic intuition for PCthrough rearrangement of (11) and defining $C_2 := \\log(\\frac{k'}{k})$:\n$\\hat{J_1}(R, G) = \\mathbb{E}_{x \\sim G} \\left[1\\left[\\log\\frac{f_G(x)}{f_R(x)} < C_2\\right]\\right]$.\nThe interior expression checks if the number of extra bitsneeded to encode the generated data point using the optimalreal encoding scheme instead of generated is no greater than$C_2$. By symmetry, we can relate recall coverage to $\\hat{J_1}(G, R)$."}, {"title": "4.2 Density & Coverage", "content": "Naeem et al. (2020) define their metric, called Density (notto be confused with probability density) & Coverage, withthe motivation of building upon the work of Kynk\u00e4\u00e4nniemi et al. (2019), by creating a version of precision & recall moreresistant to outliers. Their definitions are described below."}, {"title": "Density & Coverage Definition.", "content": "Density, used to measurethe fidelity of generated samples, can be defined as:\n$Density = \\frac{1}{N_R} \\sum_{i=1}^{N_R} \\frac{\\#(G, B_{k,R}(R_i))}{k}$\nDensity counts the number of neighborhood spheres of realsamples that contain the generated data point, normalized by$N_G$. Their recall counterpart, coverage, can be defined as:\n$Coverage = \\frac{1}{N_R} \\sum_{i=1}^{N_R} 1[\\#(G, B_{k, R}(R_i)) \\geq 1]$\nNaeem et al. (2020) base their recall measure on the realsample set, justified by its lower outlier presence comparedto generated sets.\"Coverage averages the number of real kNNballs that contain one or more generated samples.\nConnecting Density. We rearrange the expression of den-sity to define it within our framework. Noting that k =$\\#(R, B_{k,R}(R_i))$,\n$Density = \\frac{1}{N_G} \\sum_{i=1}^{N_R} \\frac{\\#(G, B_{k,R}(R_i))}{\\#(R, B_{k,R}(R_i))}$\nWe then define $\\hat{J_1} (G, R)$ and its theoretical value $J_1(G, R)$:\n$\\hat{J_1}(G, R) = \\frac{1}{N_R} \\sum_{i=1}^{N_R} \\frac{\\#(G, B_{k,U}(R_i))}{\\#(R, B_{k,U}(R_i)) + 1}$\n$J_1(G, R) = \\mathbb{E}_{x \\sim R} \\left[\\frac{f_G(x)}{f_R(x)}\\right]$"}, {"title": "In the Appendix, we explain how the differences betweendensity and $\\hat{J_1} (G, R)$, similar to that of PC and $\\hat{J_1} (R, G)$,affect the bias.", "content": "The fact that $J_1(G,R) = 1$ implies that as$N_G$ increases, and with k a growing function of $N_G$ (Noshadet al. 2017), density and $\\hat{J_1} (G, R)$ are both asymptotically 1.Because of differing convergence rates, we note that densitycan still be a meaningful measure when comparing outputwith the same number of drawn samples. However, as ourAppendix experiments show, density is an unreliable measurewhen working with an increasingly large number of samples.\nIntuition. Here, we describe the intuition behind why den-sity metric is asymptotically 1, under appropriate conditions.For clarity, we present definitions of $J_1(G, R)$ and its es-timator $\\hat{J_1}(G, R)$ that are simply rearrangements of theirformulae in Section 3.2:\n$J_1(G, R) = \\frac{\\int \\frac{f_G(x)}{f_R(x)} \\cdot f_R(x) dx}{N_R}$ \\\\$\\hat{J_1}(G, R) = \\frac{\\frac{1}{N_R} \\sum_{i=1}^{N_R} \\#(G, B_{k,U}(R_i))}{N_R \\cdot N_G \\times (\\#(R, B_{k,U}(R_i)) + 1)}$\nMonte Carlo integration $\\frac{1}{N} \\sum_{i=1}^N f_Y(X_i)$ serves as an unbi-ased estimator for $\\int f_Y(x) f_X (x) dx$. Applying this principle,"}, {"title": "$\\frac{1}{N_R} \\sum_{i=1}^{N_R} \\frac{f_G(X_i)}{f_R(X_i)}$ is an unbiased estimator of $J_1 (G, R)$", "content": ""}, {"title": "As demonstrated from our mathematical analysis, these met-rics approximate the general form of a statistical divergence D(X||Y)", "content": "$\\mathbb{E}_{x \\sim X} [g\\left(\\frac{f_X(x)}{f_Y(x)}\\right)]$, where PC, RC, and Cov-erage use $g(z) = 1[z > 17]$, Density uses $g(z) = z$, andKL-divergence is $g(z) = -log(z)$. We note that in the con-text of precision, the form $D(G||R)$ is ineffective due to itsdependence on relative densities. For instance, $\\hat{J_1}(G, R)$ (re-lated to PC) includes a $f_G (x)$ term in the denominator, whichunfairly punishes for when the generated region is dense\u2014an aspect which should be of neutral relevance. In the con-text of recall, the form $D(G||R)$ divergence is more reliable.Regions with high probability in the real distribution shouldcorrespondingly require dense coverage by generated sam-ples; however, dense generated regions should not necessitatedenser real regions to be considered precise.\nWe substantiate our observations, noting $D_{KL}(G||R) =CE(G,R)-H(G)$, by comparing the closed-form expressionfor all three terms when increasing the variance of G. We analyze two multivariate Gaussian distributions, R ~ N(0, I)and $G \\sim N(0,\\sigma^2I)$ in $\\mathbb{R}^{10}$, where 0 represents the zeromean vector and I denotes the identity matrix."}, {"title": "5 Information-theoretic Precision and Recall Metric", "content": "Here, we introduce the three components of our comprehensive metric: Precision Cross-Entropy (PCE), Recall Cross-Entropy (RCE), and Recall Entropy (RE)."}, {"title": "5.1 Desiderata for Generative Model Metrics", "content": "To clarify the optimal behavior of generative model metrics,we first present the following desiderata for precision andrecall measures.\nPrecision should (P1) reflect high fidelity for generatedsamples that are likely within the real distribution, (P2) indi-cate low fidelity for those that are improbable, highlightingcases of mode invention (Borji 2018). For certain domainssuch as image generation, where realism is effectively judgedby human evaluation, precision should (P3) align with humanevaluation (Stein et al. 2024).\nRecall should (R1) reward the case where high-probabilityregions of the real distribution are sufficiently covered bygenerated samples, addressing mode dropping, and where(R2) the generated samples capture the full variation withineach mode, limiting mode shrinkage."}, {"title": "5.2 Theoretical Metric Definitions", "content": "We exploit the asymmetric nature of (cross-)entropy, as de-fined in Section 3.2, to construct an information-theoreticinterpretation of precision and recall. Below, we define ourmetric and justify its use in accordance with the labeleddesiderata in Section 5.1.\nThe three components of our metric are defined below.Note that for all components, the entropy of the real datadistribution, H(fR), serves as a constant baseline, so thatfor identical distributions, PCE = RCE = RE = 0. Be-cause H(fR) is independent of the generated distribution,the quality of the metric scores does not depend on its value.\nPrecision Cross-Entropy (PCE).\nPCE = CE(G, R) \u2013 H(R) \\\\$= \\mathbb{E}_{x \\sim G} \\log \\frac{1}{f_R(x)} - \\mathbb{E}_{x \\sim R} \\log \\frac{1}{f_R(x)}$\nPCE is minimized when generated points are in high-probability regions of the real manifold (P1). Lower valuesof PCE correspond to higher levels of fidelity and less modeinvention (P2).\nRecall Cross-Entropy (RCE).\nRCE = CE(R, G) \u2013 H(R) = $\\mathbb{E}_{x \\sim R} \\log \\frac{f_R(x)}{f_G(x)}$\nMinimizing RCE requires that the regions where the realprobability is large also exhibit large values of the generatedprobability (R1). Low values of RCE indicates high inter-class diversity and therefore limited mode dropping. Notethat RCE is equivalent to DKL(R||G).\nRecall entropy (RE).\nRE = H(G) - H(R) \\\\$= \\mathbb{E}_{x \\sim G} \\log \\frac{1}{f_G(x)} - \\mathbb{E}_{x \\sim R} \\log \\frac{1}{f_R(x)}$"}, {"title": "The more similar generated points are to each other (i.e. loweruniqueness), the lower RE is.", "content": "Low RE coupled with lowPCE indicates mode shrinkage, as points clustered at theaverage of a mode are highly precise, but not unique (R2).RE can be seen as the intra-class diversity complement toRCE. RE and PCE are normalized versions of the twocomponents of DKL(G||R) = CE(G,R) \u2013 H(G)."}, {"title": "5.3 Empirical Definitions", "content": "For calculations on sample sets, we refer back to the kNN-based estimators defined in Section 3.2 and define the empir-ical version of our metric as:\n$PCE_k = CE_k(G, R) \u2013 H_k(R)$ \\\\$RCE_k = CE_k(R, G) \u2013 H_k(R)$ \\\\$RE_k = H_k(G)$\nFrom a high-level perspective, PCEk calculates the averagelog-normalized distance to the kth-nearest-neighbor in thereal set for all generated samples. Similarly, RCEk is mini-mized when the average normalized distance of real points totheir generated neighbors is small. Conversely, REk is sensi-tive to when generated points are clustered together. In theempirical context, Hk (R) can also be seen as a normalizationfactor dependent on the dimension and choice of k (Park andKim 2023). These scores leverage the same principle as otherkNN metrics and FID; distances in a feature space conveymeaningful information, with shorter distances signifyinggreater similarity between points.\nUnlike non-kNN-based metrics, such as FID, our scorescan be dissected for mode- and sample-level analysis, as theestimators are additive in the contributions of different points.Hence, we can see which samples contribute the most tothe overall score and what modes the generative model isstruggling to represent. We note that in cases where PCEk issignificantly negative for a sample, memorization\u2014when themodel generates data (near-)identical to a training data point\u2014may be present. Examples are provided in the Appendix.\nBecause of its favorable sample-level nature, our methodcan be easily related to other works. For instance, generatedsamples can be sorted by contribution to PCEk, and thehighest scoring can be discarded; this is a form of model-auditing (Alaa et al. 2022), where samples are judged forquality and the most unrealistic samples are rejected. We canalso extend our method to Precision-Recall curves (Djolongaet al. 2020), where we can visualize a trade-off betweenPCEk and REk when auditing levels are increased. Whenthe less realistic samples, which likely reside in the fringesof modes, are removed, intra-class diversity decreases whilefidelity increases."}, {"title": "6 Experiments", "content": "Here we analyze the empirical behavior of Density & Cover-age (D & C), Precision Recall Cover (PC & RC), Fr\u00e9chetdistance (FD), and our metric (PCE, RCE, & RE), specif-ically focusing on how well they align with the desideratalisted in Section 5.1. We set k to 5, C = 3, and follow the rec-ommendation of Stein et al. to embed the image vectors with"}, {"title": "the DINOv2-ViT-L/14 encoder (Oquab et al. 2024), which they claim provides a richer representative feature space than the commonly used Inception network, which may unfairly punish diffusion models.", "content": "We extend the work of Stein et al.(2024), who perform both diversity and human perceptioncorrelation experiments for many metrics.\nDataset Descriptions. We use both ImageNet (Deng et al.2009) and CIFAR-10 (Krizhevsky, Hinton et al. 2009) im-age datasets for our analysis. The sampled training set forImageNet contains 1000 classes with 100 images each, whileCIFAR-10 has 10"}]}