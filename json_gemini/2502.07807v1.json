{"title": "CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception", "authors": ["Senkang Hu", "Yihang Tao", "Zihan Fang", "Guowen Xu", "Yiqin Deng", "Sam Kwong", "Yuguang Fang"], "abstract": "Collaborative perception (CP) is a promising method for safe connected and autonomous driving, which enables multiple vehicles to share sensing information to enhance perception performance. However, compared with single-vehicle perception, the openness of a CP system makes it more vulnerable to malicious attacks that can inject malicious information to mislead the perception of an ego vehicle, resulting in severe risks for safe driving. To mitigate such vulnerability, we first propose a new paradigm for malicious agent detection that effectively identifies malicious agents at the feature level without requiring verification of final perception results, significantly reducing computational overhead. Building on this paradigm, we introduce CP-GuardBench, the first comprehensive dataset provided to train and evaluate various malicious agent detection methods for CP systems. Furthermore, we develop a robust defense method called CP-Guard+, which enhances the margin between the representations of benign and malicious features through a carefully designed Dual-Centered Contrastive Loss (DCCLoss). Finally, we conduct extensive experiments on both CP-GuardBench and V2X-Sim, and demonstrate the superiority of CP-Guard+.", "sections": [{"title": "1. Introduction", "content": "The development of collaborative perception (CP) has been driven by the increasing demand for accurate and reliable perception in autonomous driving systems (Chen et al., 2019b;a; Li et al., 2022; Hu et al., 2024f; 2023; 2024b; Fang et al., 2024a; Xu et al., 2022; Hu et al., 2024d;e;a; Tao et al., 2025; Lin et al., 2024c;b). Single-agent perception systems, which rely solely on the onboard sensors of a single vehicle, are restricted by limited sensing range and occlusion. On the contrary, CP systems incorporate multiple connected and autonomous vehicles (CAVs) to collaboratively capture their surrounding environments. Specifically, CAVs in a CP system can be divided into two categories: the ego CAV and helping CAVs. The helping CAVs send complementary sensing information (most methods send intermediate features) to the ego CAV, and the ego CAV then leverages this complementary information to enhance its perception performance (Balkus et al., 2022; Han et al., 2023; Hu et al., 2024c; Wang et al., 2020; Fang et al., 2025; 2024b; Tao et al., 2024; Fang et al., 2024c; Lin et al., 2024f;a;d). For example, the ego CAV can detect occluded objects and extend the sensing range after fusing the received information.\nDespite many advantages of CP outlined above, it also has several crucial drawbacks. Compared to single-agent perception systems, CP is more vulnerable to security threats and easier attack since it requires receiving and fusing information from other CAVs, which expands the attack surface. In particular, malicious agents can directly send intermediate features with adversarial perturbations to fool the ego CAV or a man-in-the-middle who can capture the intermediate feature maps and manipulate them. Figure 1a illustrates the vulnerability of CP to malicious agents. In addition, several attack methods have been designed to fool CP. For example, Tu et al. (Tu et al., 2021) developed a method to generate indistinguishable adversarial perturbations to attack the multi-agent communication in CP, which can severely degrade the perception performance (Lin et al., 2024e; 2025; 2023).\nThe inability for the ego CAV to accurately detect and eliminate malicious agents from its collaboration network poses significant risks to CP, potentially resulting in compromised perception outcomes and catastrophic consequences. For instance, the ego CAV might misinterpret traffic light statuses or fail to detect objects ahead of the road, resulting in severe traffic accidents or even fatalities. Hence, it is crucial to develop a defense mechanism for CP that is resilient to attacks from malicious agents and capable of eliminating them from its collaboration network.\nTo address the security threats in CP, some previous works"}, {"title": "2. Preliminaries", "content": "In this section, we formulate collaborative perception and give the pipeline of our CP system. Specifically, let $\\mathcal{X}^N$ denote the set of N CAVs in the CP system. CAVs in X can be divided into two categories: the ego CAV and helping CAVs. The ego CAV is the one that needs to perceive its surrounding environment, while helping CAVs are the ones that send their complementary sensing information to the ego CAV to help it enhance its perception performance. Thus, each CAV can be an ego one and helping one, depending on its role in a perception process. We assume that each CAV is equipped with a feature encoder $f_{\\text{encoder}}(\\cdot)$, a feature aggregator $f_{\\text{agg}}(\\cdot)$, and a feature decoder $f_{\\text{decoder}}(\\cdot)$. For the i-th CAV in the set X, the raw observation is denoted as $O_i$ (such as camera images and LiDAR point clouds), and the final perception results are denoted as $Y_i$. The CP pipeline of the i-th CAV can be described as follows."}, {"title": "2.1. Formulation of Collaborative Perception", "content": "1.  Observation Encoding: Each CAV encodes its raw observation $O_i$ into an initial feature map $F_i = f_{\\text{encoder}}(O_j)$, where $j \\in \\mathcal{X}^N$.\n2.  Intermediate Feature Transmission: Helping CAVs transmit their intermediate features to the ego CAV: $F_{j\\rightarrow i} = \\Gamma_{ji}(F_j), j \\in \\mathcal{X}^N, j \\neq i$, where $\\Gamma_{j\\rightarrow i}(\\cdot)$ denotes a transmitter that conveys the j-th CAV's intermediate feature $F_j$ to the ego CAV, while performing a spatial transformation. $F_{j\\rightarrow i}$ is the spatially aligned feature in the i-th CAV's coordinate.\n3.  Feature Aggregation: The ego CAV receives all the intermediate features and fuses them into a unified observational feature $F_{\\text{fused}} = f_{\\text{agg}}(F_{0i}, \\{F_{j\\rightarrow i}\\}_{j\\neq i, j\\in \\mathcal{X}^N})$.\n4.  Perception Decoding: Finally, the ego CAV decodes the unified observational feature $F_{\\text{fused}}$ into the final perception results $Y = f_{\\text{decoder}}(F_{\\text{fused}})."}, {"title": "2.2. Adversarial Threat Model", "content": "Our focus is on the operation of an intermediate-fusion collaboration scheme, where an attacker introduces designed adversarial perturbations into the intermediate features to mislead the perception of the ego CAV. Since an attacker participates in the collaborative system with local perception model installation, we assume they have white-box access to the model parameters. The attack procedure in each frame follows four sequential phases.\n1.  Local Perception Phase: All agents, including the malicious one, process their sensing data independently and extract intermediate features using feature encoders.\n\n$F_k = f_{\\text{encoder}}(O_k), k \\in \\mathcal{X}^N$ (1)\n\nThis phase operates in parallel without inter-agent communication.\n2.  Feature Communication Phase: All agents broadcast their extracted features through the network. Malicious agent k collects feature information $\\{F_{j\\rightarrow i}\\}$ from other agents. Feature-level transmission ensures minimal communication overhead compared to raw sensor data exchange.\n3.  Attack Generation Phase: A malicious agent executes the attack by first perturbing its local features and then propagating them through the collaborative perception pipeline described in Section 2.1. The attacker aims to optimize the perturbation $\\delta$ through an iterative process. The optimization objective is formulated as:\n\n$\\arg \\max_\\delta L(Y^\\delta, Y_{\\text{gt}}), \\text{ s.t. } ||\\delta|| \\leq \\triangle$ (2)\n\nwhere $\\triangle$ bounds the perturbation magnitude to maintain attack stealthiness. The total loss function is designed to aggregate adversarial losses over all object proposals, targeting both classification and localization aspects:\n\n$L(Y^\\delta, Y_{\\text{gt}}) = \\sum_{p \\in Y^\\delta} L_{\\text{adv}}(p^\\delta, p)$ (3)\n\nFor each proposal p with the highest confidence class $c = \\arg \\max\\{p_i\\}$, we leverage a class-specific adversarial loss following (Tu et al., 2021):\n\n$L_{\\text{adv}}(p^\\prime, p) = \\begin{cases} - \\log(1 - p^\\prime) \\eta & c \\neq k, p_c > T_1 \\\\ - A p^\\prime \\log(1 - p) & c = k, p_c > T_2 \\\\ 0 & \\text{otherwise} \\end{cases}$\n\nwhere $\\eta$ represents the IoU between perturbed and original proposals to consider spatial accuracy, $T_1$ and $T_2$ are confidence thresholds for different attack scenarios, $A$ balances the importance of different attack objectives, and k denotes the background class.\n4.  Defense and Final Perception Phase: The ego vehicle integrates all received feature information, including potentially corrupted ones, to complete the final object detection task. Note that we focus exclusively on CP-specific vulnerabilities, excluding physical sensor attacks (e.g., LiDAR or GPS spoofing), which are general threats to CAVs. We also assume communication channels are secured with proper cryptographic protection."}, {"title": "3. CP-GuardBench", "content": "To facilitate feature-level malicious agent detection in CP systems, we propose to develop CP-GuardBench, the first benchmark for malicious agent detection in CP systems. It provides a comprehensive dataset for training and evaluating malicious agent detection methods. In this section, we will introduce the details of CP-GuardBench, including the automatic data generation and annotation pipeline in Section 3.1, and the data visualization and statistics in Section 3.2."}, {"title": "3.1. Automatic Data Generation and Annotation", "content": "We build CP-GuardBench based on one of the most widely used datasets in the CP field, V2X-Sim (Li et al., 2022), which is a comprehensive simulated multi-agent perception dataset for V2X-aided autonomous driving. In this section, we introduce the automatic data generation and annotation pipeline of CP-GuardBench. The pipeline is shown in Figure 2. It consists of three steps: 1) intermediate feature generation, 2) attack implementation, and 3) pair generation and saving.\nSpecifically, we first train a robust LiDAR collaborative object detector, which consists of a convolutional backbone, a convolutional decoder, and a prediction head for classification and regression (Luo et al., 2018). As for the fusion method, we adopt the mean fusion method to fuse the intermediate features from different collaborators. Subsequently, the backbone is retained for extracting intermediate features, which are then transmitted and utilized by an ego CAV as supplementary information.\nSecondly, the attacks are implemented and applied to the intermediate features. The detection head and decoder are then frozen to generate the attacked detection results and optimize the adversarial perturbations. As shown in Figure 2, several iterations are required to optimize the perturbations, and the loss function differs for different attack types. In our CP-GuardBench, we consider five types of attacks, including Projected Gradient Descent (PGD) (Madry et al., 2018), Carini & Wagner (C&W) attack (Carlini & Wagner, 2017), Basic Iterative Method (BIM) (Kurakin et al., 2017), Guassian Noise Perturbation (GN), and Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015). The implementation details can be found in Appendix ??.\nIn the generation of attack data, we randomly choose one of the attacks mentioned above and generate the corresponding attack data in each iteration. Finally, the perturbed features will be annotated with the corresponding attack type and saved for later use."}, {"title": "3.2. Data Visualization and Statistics", "content": "We visualize the samples of the generated data in Figures 3(a), (b), (c), and (d). We observe that attacks are so stealthy that it is very hard to see the difference with the naked eye, which poses a great challenge to address the detection of malicious agents.\nTo construct CP-GuardBench, we randomly sample 9000 frames from V2X-Sim and generate 42200 feature-label pairs. The data is then split into training, validation, and test sets with a ratio of 8:1:1. The data statistics are shown in Figures 3(e), (f), and (g). Figure 3(e) illustrates the distribution of the number of collaborators, which is the number of agents that collaboratively perceive environments. The number of collaborators ranges from 3 to 6, with the most common scenario being 4 collaborators, accounting for 46.0% of the total data. 5 and 6 collaborators are also common, accounting for 29.9% and 19.5% of the total data, respectively. Regarding the distribution of attack types, as depicted in Figure 3(f), we observe that the attack types are evenly distributed, with each type accounting for approximately 20% of the total data. This is due to the random selection of one attack type in each iteration. Figure 3(g) illustrates the attack ratio, which represents the ratio of the number of attackers to the total number of agents in a collaboration network. The maximum attack ratio exceeds 0.3,"}, {"title": "4. CP-Guard+", "content": "In this section, we present our CP-Guard+, a tailored defense method for CP scenarios that effectively detects malicious agents. It consists of two techniques: 1) Residual Latent Feature Learning, which learns the residual features of benign and malicious agents, and 2) Dual Centered Contrastive Loss (DCCLoss), which clusters the representation of benign features into a compact space and ensures that the representation of malicious features is as distant from the benign space as possible."}, {"title": "4.1. Residual Latent Feature Learning", "content": "In CP scenarios, the dynamic environment causes noisy, non-stationary data distributions. Directly detecting malicious agents can be challenging due to this noise, as object detectors' feature maps often mix foreground and background information.\nTo tackle this challenge, we propose to learn residual latent features instead of directly learning the features of benign or malicious agents. By focusing on the differences between the collaborators' feature maps and the ego agent's feature maps, the model can better distinguish between benign and malicious agents. This mechanism is also inspired by the idea that the collaborators' intermediate feature maps will achieve a consensus rather than conflict with the ego CAV's intermediate feature maps.\nSpecifically, given the collaborators' intermediate feature maps $\\{F_{j\\rightarrow i}\\}_{j\\neq i, j\\in x_n}$ and the ego CAV's intermediate feature maps $F_i$, the residual feature is obtained as: $F_{\\text{res}} = F_i - F_{ji}$\nThen, we can leverage the residual latent features to detect malicious agents by modeling the detection problem as a binary classification task. A binary classifier $f_{\\text{cls}}(x; \\theta)$ is trained on the residual latent features to distinguish between benign (labeled 0) and malicious (labeled 1) agents. The model is optimized using the cross-entropy loss $L_{CE}$:\n$\\min_\\theta L_{CE}(f_{\\text{cls}}(F_{\\text{res}}^{ji}; \\theta), y_{ji})$ (4)\nwhere $y_{ji}$ is the ground truth label of residual feature $F_{\\text{res}}^{ji}$"}, {"title": "4.2. Dual-Centered Contrastive Loss", "content": "In practice, attackers can continuously design new attacks to manipulate the victim's intermediate feature maps. Therefore, we need a model that is resistant to unseen attacks, which means the model should cluster the representation of benign features into a more compact space and ensure that the representation of malicious features is as distant from the benign space as possible.\nTo tackle this challenge, we propose a Dual-Centered Contrastive Loss (DCCLoss), which is a contrastive learning-based objective function. It explicitly models the distribution relationship between benign and malicious features, enhancing the positive pairs (features of the same class) to their corresponding center closer, thereby enhancing the internal consistency of both benign and malicious features. Meanwhile, negative pairs (features from different classes) will be pushed away from each other's centers, ensuring maximal separation between benign and malicious features in the feature space. In this way, the robustness of the model against unseen attacks is improved.\nSpecifically, we first leverage the output of the penultimate fully connected layer $f_{cls}$ to obtain the one-dimensional vector $\\{V_i\\}_{i=0,1,...,N-1}$ of residual features $\\{F_{\\text{res}}^{\\text{F}\\$\\circledRi}\\}_{j\\neq i, j\\in \\mathcal{X}^N}$. Then, we introduce two feature centers in DCCLoss:\n1.  Benign feature center ($c_b$): represents the center of all benign features, ensuring a compact distribution of benign features.\n2.  Malicious feature center ($c_{mal}$): represents the center of all malicious features, ensuring maximal separation between malicious and benign feature distributions.\nThe benign feature center $c_b$ and the malicious feature center $c_{mal}$ are computed by averaging the vectors of benign and malicious features, respectively:\n\n$\\begin{aligned} \\qquad c_b &= \\frac{1}{N_b} \\sum_{V_i \\in \\{V_b\\}} V_i \\\\ c_{mal} &= \\frac{1}{N_{mal}} \\sum_{V_i \\in \\{V_{mal}\\}} V_i. (5) \\end{aligned}$\n\nwhere $N_b$ and $N_{mal}$ are the numbers of benign and malicious vectors, respectively, and $V_b$ and $V_{mal}$ are the sets of benign and malicious vectors, respectively.\nMoreover, denote $(V_m, V_n)$ as a pair of features, which is a positive pair if they are from the same class (both benign or malicious) and a negative pair otherwise. We have the DCCLoss of one feature pair $l(V_m, V_n)$ as:\n\n$l(V_m, V_n) = - \\log \\frac{\\exp \\left(\\frac{(V_m - c(m))^{\\top} (V_n - c(n))}{\\tau}\\right)}{\\sum_{o=1, o \\neq m}^{N} \\mathbb{I} \\exp \\left(\\frac{(V_m - c(m))^{\\top} (V_o - c(o))}{\\tau}\\right)} (6)$\n\nwhere $V_x - c(x) = \\begin{cases} V_x - c_b, & \\text{if } V_x \\in \\{V_b\\}, \\\\ V_x - c_{mal}, & \\text{if } V_x \\in \\{V_{mal}\\}, \\end{cases}$\n\nwhere $\\mathbb{I}(V_m, V_n)$ is an indicator function that returns one or zero for positive or negative pairs, respectively. $\\tau$ is a temperature parameter and $\\nu_{V_m V_n}$ denotes the cosine similarity, $\\nu_{V_m V_n} = \\frac{V_m \\cdot V_n}{\\|V_m\\| \\|V_n\\|}$. The final DCCLoss is the average of l of all positive pairs.\n\n$L_{DCCLoss} = \\sum_{m=1}^{N} \\sum_{n=m+1}^{N} (1 - \\mathbb{I}(V_m, V_n)) l(V_m, V_n)$ (7)\n$\\qquad \\qquad C(N, 2)$\n\nwhere $C(N, 2) = \\binom{N}{2} = \\frac{N!}{2!(N - 2)!}$. During training, we use the combination of cross entropy loss and Eq. 7 to optimize the model:\n\n$L = L_{CE} + \\alpha \\cdot L_{DCCLOSS}$ (8)\n\nwhere $\\alpha$ is a hyperparameter to balance the two losses.\nDiscussion of DCCLoss. In so doing, the first term $L_{CE}$ quantifies the difference between the true distribution and the predicted distribution from the model, thereby penalizing the confidence in wrong predictions. The second term $L_{DCCLOSS}$ contributes significantly to the learning process. Standard contrastive loss attempts to maximize the distance between negative pairs, which may cause the features of benign samples to gradually drift away from their center. However, DCCLoss calculates the distance using the feature center as a reference point, thus avoiding this issue. The optimization goal of DCCLoss is to maximize the angular distance between negative pairs to enhance feature discriminability while maintaining the compactness of benign sample features, keeping them as close to the feature center as possible. In other words, the introduction of dual-center modeling optimizes the distributional relationship between benign and malicious features, making them more separable and making the distributions of benign and malicious features more compact on its own, respectively. This helps resolve the distribution overlap problem and enhances the model's ability to detect unseen attacks."}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nDataset and Baselines. In our experiments, we consider two datasets: CP-GuardBench and V2X-Sim (Li et al., 2022). We designate CAV #1 as the ego CAV and randomly select adversarial collaborators from the remaining CAVs. Additionally, we use ROBOSAC (Li et al., 2023) and MADE (Zhao et al., 2024) as baselines, which are two state-of-the-art CP defense methods based on the hypothesize-and-verify paradigm.\nAttack Settings. We assess different CP defense methods targeted at five attacks: PGD attack (Madry et al., 2018), C&W attack (Carlini & Wagner, 2017), BIM attack (Kurakin et al., 2017), FGSM attack (Goodfellow et al., 2015), and GN attack. We set different perturbation sizes $\\Delta \\in \\{0.1, 0.25, 0.5, 0.75, 1.0\\}$. The number of malicious attackers varies in $\\{0,1,2\\}$ and all the attackers are randomly assigned from the collaborators, where 0 attacker indicates an upper-bound case. For PGD, BIM and C&W attacks, the number of iteration steps is 15 and the step size is 0.1.\nImplementation Details. The CP-Guard+ system is implemented using PyTorch, and we utilize the object detector described in Section 3.1. For each agent, the local LiDAR point cloud data is first encoded into 32 \u00d7 32 bird's eye view (BEV) feature maps with 256 channels prior to communication. For our CP-Guard+, we use ResNet-50 (He et al., 2016) as the backbone, and the training is performed for 50 epochs with batch size 10 and learning rate 1 \u00d7 10-3. Our experiments are conducted on a server with four RTX A5000 GPUs. For mixed contrastive training, we utilize the output of the fully connected layers preceding the final output layer in the backbone to form a one-dimensional feature vector for each agent, the dimension of which is 2048.\nEvaluation Metrics. We use a variety of metrics to evaluate the performance of our CP-Guard+ model. For malicious agent detection on our CP-GuardBench dataset, we consider Accuracy, True Positive Rate (TPR), False Positive Rate (FPR), Precision, and F1 Score. For CP defense on the V2X-Sim dataset, we use metrics including average precision (AP) at IoU=0.5 and IoU=0.7. Additionally, to assess the computation efficiency of different CP defense methods, we introduce the metric frames-per-second (FPS)."}, {"title": "5.2. Quantitative Results", "content": "Performance Evaluation of CP-Guard+. We evaluated CP-Guard+ on the CP-GuardBench dataset using various attack methods and perturbation budgets (\u0394), as detailed in Table 1. Metrics include Accuracy, True Positive Rate (TPR), False Positive Rate (FPR), Precision, and F1 Score. At \u0394 = 0.25, CP-Guard+ achieved over 98% accuracy for PGD, BIM, and C&W attacks. FGSM and GN attacks showed lower accuracy, around 91.64% and 90.95%, due to their weaker impact unless perturbations are large. With \u0394 = 0.5, the model maintained an average accuracy of 98.08% and a TPR of 97.07%. For \u0394 = 0.75 and \u0394 = 1.0, accuracy was 98.43% and 98.53%, respectively. The model consistently showed high TPR and low FPR, indicating robust detection of true positives and minimal false positives. Overall, CP-Guard+ demonstrated strong performance and resilience across various attacks and perturbation levels.\nPerformance Comparison with Other Defenses. We compare CP-Guard+ with MADE (Zhao et al., 2024) and ROBOSAC (Li et al., 2023) on the V2X-Sim dataset. Table 2 shows that without defense, AP@0.5/0.7 significantly drops. CP-Guard+ consistently achieves the highest scores, even as the number of malicious agents or perturbation level increases. For \u0394 = 0.25 and Nmal = 1, CP-Guard+ achieves 71.88% AP@0.5 and 69.92% AP@0.7, outperforming the no-defense case by over 186% and 196%, respectively. It also surpasses MADE and ROBOSAC by notable margins. At \u0394 = 0.5 and Nmal = 2, CP-Guard+ maintains superior performance, confirming its robustness and superiority.\nFPS Comparison. We compare the FPS performance of CP-Guard+ with MADE and ROBOSAC, as shown in Figure 4(a). The median FPS values for MADE, ROBOSAC, and CP-Guard+ are 56.86, 20.76, and 70.36, respectively. CP-Guard+ achieves a 23.74% higher FPS than MADE and a 238.92% increase over ROBOSAC, representing a significant improvement. These results highlight the high computational efficiency of our CP-Guard+.\nGeneralization of CP-Guard+. To evaluate our method's generalization ability, we conducted experiments using a leave-one-out strategy. In this approach, we iteratively excluded one type of attack from the training set, trained the model on the remaining attacks, and then tested its performance on the held-out attack type. The experimental results are presented in Table 3. From the table, we can see that our method achieves strong generalization ability on unseen attacks. Compared to the upper-bound case, our method experiences only a little decrease in overall performance. These findings underscore our method's robust capability to detect and handle unseen attack patterns."}, {"title": "5.3. Ablation Study and Visualization", "content": "The Effectiveness of DCCLoss. We assess the impact of DCCLoss on CP-Guard+ performance. Figure 5 shows the effectiveness of DCCLoss and its superiority compared with contrastive loss (CL) (Chen et al., 2020). Figure 5(b) shows that this training significantly boosts performance, including Accuracy, TPR, Precision, and F1 score. Figure 5(c) shows the comparison results between DCCLoss and CL, which demonstrate that our DCCLoss outperforms CL and shows its effectiveness in malicious agent detection. In addition, Figure 4(b) illustrates that DCCLoss increases the cosine distance between negative pairs and decreases it between positive pairs, enhancing the model's ability to distinguish between malicious and benign agents.\nVisualization. Figure 6 visualizes the CP system's results with and without the CP-Guard+ defense mechanism. The top row shows that without defense, malicious agents cause numerous false positives, compromising system performance and security. The bottom row demonstrates that CP-Guard+ effectively detects and eliminates malicious agents, reducing false positives and increasing the true positive rate, confirming its effectiveness."}, {"title": "6. Conclusion", "content": "In this paper, we have proposed a new paradigm for malicious agent detection in CP systems, which directly detects malicious agents at the feature level without generating multiple hypothetical results, significantly reducing system complexity and computation cost. We have also constructed a new benchmark, CP-GuardBench, for malicious agent detection in CP systems, which is the first benchmark in this field. Furthermore, we have developed CP-Guard+, a resilient framework for detecting malicious agents in CP systems, capable of identifying malicious agents at the feature level without the need to verify the final perception results, significantly reducing the computation cost and increasing frame rate. We hope the new benchmark and the proposed framework CP-Guard+ can contribute to the community and promote further development of secure and reliable CP systems."}]}