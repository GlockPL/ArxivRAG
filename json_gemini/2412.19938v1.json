{"title": "Towards Strong AI: Transformational Beliefs and Scientific Creativity", "authors": ["Samuel J. Eschker", "Chuanhai Liu"], "abstract": "Strong artificial intelligence (AI) is envisioned to possess general cognitive abilities and scientific creativity comparable to human intelligence, encompassing both knowledge acquisition and problem-solving. While remarkable progress has been made in weak AI, the realization of strong AI remains a topic of intense debate and critical examination. In this paper, we explore pivotal innovations in the history of astronomy and physics, focusing on the discovery of Neptune and the concept of scientific revolutions as perceived by philosophers of science. Building on these insights, we introduce a simple theoretical and statistical framework of weak beliefs, termed the Transformational Belief (TB) framework, designed as a foundation for modeling scientific creativity. Through selected illustrative examples in statistical science, we demonstrate the TB framework's potential as a promising foundation for understanding, analyzing, and even fostering creativity - paving the way toward the development of strong AI. We conclude with reflections on future research directions and potential advancements.", "sections": [{"title": "1 Introduction", "content": "This paper is the first in a series of three by the authors, exploring statistical reflections on strong AI. Strong AI, as envisioned here, would possess general cognitive abilities and scientific creativity akin to human intelligence, enabling it to approach knowledge acquisition and problem-solving. The other two papers in this series focus on individual cognition and scientific reasoning, respectively. This explains the title of the present article. Here, we consider scientific creativity from a statistical perspective. For a concrete discussion at the foundational level, we shall focus on scientific inquiry, where science is simply meant to gain knowledge from experience or experiments (ss, e.g., Newton, 1718; Martin and Liu, 2015a). As it is often the case that science inquiry is dynamic (Popper, 2005; Kuhn, 1970), a simple but proper statistical setting can be written as\n$(\\Omega_\\tau, D_\\tau, M_\\tau, \\Theta_\\tau)$\nwhere $\\tau$ indices the dynamic state such as sample size and time, $\\Omega_\\tau$ the world or environment of interest from which the observed data $D_\\tau$ were collected, and $M_\\tau$ the model with the space $\\Theta_\\tau$ of unknown parameters. Perhaps, it can be argued that the success of deep learning (DL) is largely due to its flexibility for a $\\tau$-dependent or dynamic approach to modeling data structures. A recent relevant discussion, with the focus on parameter estimation, is provided in Jiang and Liu (2024).\nAlthough somewhat special with respect to the complexity of the real world of scientific inquiry, the statistical setting (1) is deemed adequate to interpret the current logic foun-dations of weak AI. Here, this paper considers the creativity aspect of strong AI. While it has been playing a fundamental role in scientific inquiry from ancient times, creativity has appeared to be such an elusive concept that it is hitherto difficult to have a well-accepted definition. For example, in their review paper on computational creativity, Carnovalini and Rod\u00e0 (2020) noticed that researchers analyzed over 200 of definitions of creativity in literature. In the broader context of scientific discovery, philosophers of science have had unsettled debates for centuries on the closely related concept of scientific discovery, which can be viewed as the processes and products of scientific creativity. For instance, Schickore (2022) wrote:\nPhilosophical discussions of scientific discovery have been intricate and complex because the term \u201cdiscovery\u201d has been used in many different ways, both to refer to the outcome and to the procedure of inquiry. In the narrowest sense, the term \u201cdiscovery\u201d refers to the purported\u201ceureka moment\u201d of having a new insight.\nThe term of \u201ceureka moment\" or happy moment in the quote marks its importance in the ultimate definition of creativity consistent with our common sense."}, {"title": "2 Historical Discoveries in Natural Science", "content": "In this section, we develop intuitions for our definition of creativity by studying some exam-ples of great discoveries in astronomy and physics. Our exposition aims to provide sufficient details on the discovery of Neptune in Section 2.1 and to summarize our investigations on a list of other great discoveries in the history of astronomy and physics in Section 2.2. Section 2.2 also contains a brief review of the perspectives of philosophers of science on scientific discoveries and revolutions, which more or less agree with our observations."}, {"title": "2.1 The groundbreaking discovery of Neptune", "content": "A potential application of exploratory and transformational beliefs pertains to a scientist's reasoning process, through which they accept or reject their theories based on observed evidence. This uses probability theory and has been extensively debated by philosophers for over a century (see, e.g., Jaynes, 2003, p. 133, \u00a75.5). For a specific example, we consider the discovery of Neptune and in a chronological order, our exposition begins with the discovery of Uranus."}, {"title": "2.1.1 The discovery of Uranus", "content": "Aside from Earth, five planets \u2014 Mercury, Venus, Mars, Jupiter, and Saturn \u2014 are easily visible to the naked eye and have been known since ancient times. Uranus was the first planet discovered with the aid of a telescope by William Herschel on March 13, 1781, while conducting a systematic sweep of the contents of the night sky. Initially, he believed that he had found a comet because the object appeared to move relative to the stars. However, further observations by Herschel and other astronomers revealed that the object had a nearly circular orbit around the Sun at a distance about twice that of Saturn (see the Titius-Bode law (Gregory, 1715)), suggesting that it was a planet rather than a comet, which would have had a highly elliptical orbit.\nHerschel's finding extended the boundaries of the solar system and marked a significant advancement in astronomical research. For example, earlier star catalogs revealed that Uranus had been observed 20 times before its identification as a planet in 1781, dating back as early as 1690, but it was mistakenly identified as a star. Even more intriguingly, in 1821, French astronomer Alexis Bouvard compiled all available observations, spanning a period during which Uranus had traversed about one-third (32/84) of its orbit, and encountered a significant issue (Bouvard, 1821). Despite accounting for the gravitational"}, {"title": "2.1.2 The discovery of Neptune", "content": "The irregularities in the orbit of Uranus led Bouvard to hypothesize some perturbing body. The irregularities, both in the planet's ecliptic longitude and in its radius vector, could have been explained by several hypotheses, including:\nH1. the effect of the Sun's gravity at such a great distance might differ from Newton's description, i.e., the Newtonian theory was demolished;\nH2. the discrepancies might simply be observational error; or\nH3. perhaps Uranus was being pulled, or perturbed, by an undiscovered planet or multiple planets that are farther away from Uranus (Grosser, 1964).\nLike Bouvard, French astronomer Urbain Jean Joseph Leverrier (1811-1877) and English scholar John Couch Adams (1819-1892) from St John's College, Cambridge, likely regarded H3 as more plausible than H1, H2, and potentially many other hypotheses. This is evident from the immense computational efforts they invested in further pursuing their mathematical search of the hypothetical disturbing planet. Working within the framework of H3, Adams and Verrier independently reached a predicted position for the hypothesized perturber on the celestial sphere, using essentially the same perturbation theory techniques and the Titius-Bode law. On 24 September 1846, Johann Gottfried Galle and Heinrich Louis d'Arrest at the Berlin Observatory spotted a new planet that is very close to the predicted position by Verrier, who named this planet of the Solar system Neptune. This is a truly remarkable achievement for the epoch, as Airy (1846) wrote (p.121):\nIn the whole history of astronomy, I had almost said in the whole history of science, there is nothing comparable to this. The history of the discoveries of new planets in the latter part of the last century, and in the present century, offers nothing analogous to it.\nIn modern terms, the problem tackled by Adams and Verrier is an inverse problem. In a recent revisit to the problem, Rodr\u00edguez-Moris and Docobo (2024) recomputed the perturbations induced in the orbit of Uranus by Neptune, using the data from Solar System Dynamics at https://ssd.jpl.nasa.gov."}, {"title": "2.1.3 Statistical evidence for patterns of scientific discovery", "content": "It is remarkably worth noting that the process in the discovery of Uranus begins with an ab-normal phenomenon that contradicts the prediction principle: the observed is inconsistent with the expected or predicted, providing clear evidence that demands further investiga-tion. The same phenomenon occurred again with the discovery of Neptune. Perhaps more importantly, our observations suggest the possibility of statistical modeling of scientific discovery.\nThe above observations no doubt shed light on a meaningful definition of scientific creativity. Typically, creative innovations come next when investigators conduct a new scientific investigation to resolve the discovered anomaly. In the case of the discovery of Uranus, astronomers weakened their previous beliefs, and thus established new beliefs by remodeling the observed data. In the case of the discovery of Neptune, the beliefs were transformed, followed by remodeling the orbit data Uranus with missing values, the hypothetical planets."}, {"title": "2.2 More examples of great innovations in astronomy and physics", "content": "Far more historical examples are available than we have had space to exploit here. The observations of our studies of great discoveries in celestial mechanics and physics from a statistical perspective of their innovations are briefly summarized in Appendix A. Great discoveries all typically start with experiments and observations, build mathematical the-ories or statistical models, verify the new theories with experiments and new observations, and iterate such a process towards further verification, improvement, and discoveries.\nExtensive existing research on the science of creativity and discovery, primarily con-ducted by philosophers of science, has also been undertaken (see, e.g., Kuhn, 1970; Aleinikov, 2013; Schickore, 2022, and references therein). Their perceived general structure or pattern of such activities, particularly in natural science, exhibits similarities to our observations in this section. From a statistical perspective, we formulate in the next section a framework of scientific discovery in environments characterized by such a common process."}, {"title": "3 Transformational Beliefs: a General Framework", "content": "Although the focus here is on scientific creativity, our discussion cannot be independent of the scope of scientific discovery for problem-solving in scientific inquiry. It can arguably be said that one of the first principles of science is the principle that the observed data and the"}, {"title": "3.1 The prediction principle", "content": "predicted data must be consistent with each other. The general idea can be traced to Isaac Newton (1704, Newton (1718) and Schickore (2022)), as seen in his method of analysis:\n\u201cAs in Mathematicks, so in Natural Philosophy, the Investigation of difficult Things by the Method of Analysis, ought ever to precede the Method of Com-position. This Analysis consists in making Experiments and Observations, and in drawing general Conclusions from them by Induction, and admitting of no Objections against the Conclusions, but such as are taken from Experiments, or other certain Truths ... By this way of Analysis we may proceed from Com-pounds to Ingredients, and from Motions to the Forces producing them; and in general, from Effects to their Causes, and from particular Causes to more general ones, till the Argument end in the most general. This is the Method of Analysis\"\nWe refer to the fundamental underlying principle as the prediction principle . This principle was made clear in William Whewell's view, as Snyder (2023) summarized:\nOn Whewell's view, once a theory is invented by discoverers' induction, it must pass a variety of tests before it can be considered confirmed as an empirical truth. These tests are prediction, consilience, and coherence (see Whewell, 1858, p. 83-96). These are characterized by Whewell as, first, that \u201cour hypotheses ought to fortel [sic] phenomena which have not yet been observed\u201d (Whewell, 1858, p. 86); second, that they should \u201cexplain and determine cases of a kind differ-ent from those which were contemplated in the formation\u201d of those hypotheses (Whewell, 1858, p. 88); third that hypotheses must\u201cbecome more coherent\u201d over time (Whewell, 1858, p. 91).\nNotably, using modern inductive inference in terms of a sound logic of science (see Sec-tion 5), we can arguably perform Whewell's tests of significance against consilience and coherence based on the more fundamental concept of prediction.\nWith the prediction principle, here in this section we consider a narrow but concrete def-inition of scientific creativity that is summarized the three-step TB framework introduced in Section 1."}, {"title": "3.2 A philosophical perspective", "content": "The three-step idea here is similar to or, to some extent, can even be viewed as a modern logic of science-based renovation of that of scientific discovery of three steps of Whewell (1840); see the discussion in Schickore (2022): the happy thought or \u2018eureka moment', the articulation and development of that thought, and the testing and evaluation of it."}, {"title": "3.3 Abduction, reverse-engineering, and the TB framework", "content": "Our discussion of scientific creativity for strong AI is narrow and thus could be said to be an inquiry of statistical discovery. Recall that in general, problem-solving is an iterative process until certain conditions implementing the prediction principle are satisfied. Thus, in the context of statistical modeling of scientific creativity, the basic idea can be summarized as: at a simplest level, all your need is to consider reverse-engineering, re-sampling, and re-modeling; following Albert Einstein:\nIt can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience.\nThis is often quoted as \u2018Everything should be made as simple as possible, but not simpler', 'On the Method of Theoretical Physics', lecture delivered at Oxford, 10 June 1933; See Ratcliffe (2014)."}, {"title": "4 A Simple Illustration: Many Normal Means", "content": "Scientific discovery involves formulating and, when necessary, refining or rejecting hypothe-ses about the nature of the world. As we acquire evidence, we create models that we believe capture the structural mechanisms giving rise to our data. Over time, we may refine our parameter estimates for these models, and slowly the models will both fit our existing data better, and have higher predictive capacity. However, we will occasionally encounter sit-uations where new data appear to entirely discredit an existing model. Falling victim to the sunk cost fallacy, these new data may initially be rejected on the basis of their noncon-formity, and scientific progress may stall due to a hesitation to explore alternative theories when so much work has already been devoted to determining implications of the existing model. Nevertheless, statistical tools are available for formulating and testing whether an old model should be rejected in light of new data. This is particularly pertinent today. In the modern era of deep learning, foundation models are trained on massive datasets and then fine-tuned for specific use cases, but it is not often clear when a model should be fine-tuned as opposed to outright retrained. A special case, which we explore for the pur-pose of illustration, is the problem of many normal means, where we ask whether a normal mixture model with a given number of components and previously accepted set of means can reasonably concord with a new observation. This situation arises in repeated exper-iments, where the question becomes whether we are observing a more diverse population that previously thought. For example, if you were to take note of birds spotted in a prairie and you had four categories of known birds, you may at first mistakenly classify a new, but similar, bird into one of the existing categories. However, as measurement accuracy or sample size increases, it may become clear that the previous four categories are insufficient to describe the population.\nMore formally, the many-normal-means problem is about making inference on the un-known means \u03bc\u2081,\u2026\u2026, \u03bc\u03b7 from the sample Y1, ..., Yn with the model\n$\\mu_i \\sim P_0 \\quad \\text{and} \\quad Y_i | \\mu_i \\sim N(\\mu_i, 1) \\quad\\quad (Y_i \\in \\Upsilon, \\theta \\in \\Theta^{(n)} \\subseteq \\mathbb{R}^p)$\nfor i = 1, ..., n (see, e.g. Efron, 2016; Jiang and Liu, 2024). The inferential goal is to estimate \u03bci, to model Y\u00bf, to predict a new observation, or to produce uncertainty quantification on assertions of interest. As a further simplification of this problem for our illustration purpose, suppose that Po is a normal mixture model with an unknown number of components, K, each with the same known, small variance, \u03c3\u00b2. \u03a4\u03bf be clear, models with different K values"}, {"title": "4.1 Creation", "content": "In the creation step, we calculate an initial estimate of Kn. This estimate does not need to be optimal, because we can test against other hypotheses in the exploration step, but it should be reasonable in the sense that the model fits the data. One way to estimate the number of components is using penalized maximum likelihood, such as BIC (Schwarz, 1978) where we optimize\n$\\text{BIC} = -2 \\ell(\\theta|Y_1, ..., Y_n) + K \\log(n) = C(\\theta) - 2 \\log \\sum_{i=1}^{n} \\sum_{j=1}^{K} \\pi_j \\exp \\Big\\{ - \\frac{(Y_i - \\pi_j)^2}{2} \\Big\\} + K \\log(n)$"}, {"title": "4.2 Exploration and Evaluation", "content": "The evaluation step in scientific creativity is comprised of hypothesis testing and falsifica-tion. Given existing or new data, we check whether the model created during the creation step reasonable explains the data in the context of the alternatives articulated above. In"}, {"title": "5 The 260-Year Quest for a Unified Logic of Science", "content": "There have been too many creative works in statistics that would require a book-long space to discussion (see, e.g., Stigler, 2002; Bickel and Doksum, 2015; Gelman and Vehtari, 2021). In this section, we take a look at the quest for a unified logic of science, the most un-solved problem in statistics (Efron, 2012), from a exploratory and transformational beliefs perspective.\nIn Gigerenzer and Murray (2015), the authors argued that in the science of mind, theories are particularly likely to come from tools, and they are specially concerned with the emergence of the metaphor of the mind as an intuitive statistician. Here, tools, physical for changing environment or logical for reasoning, are certainly products of creativity. Their observations are particularly intriguing in the context of strong AI research. These include their recognized two scientific revolutions, cognitive and probabilistic, in the middle of the twentieth century. Our example here focuses on the latter and its more general form \u2014 statistical. Indeed, all is about reasoning with uncertainty and has a long history (Nickerson, 2004). Our brief discussion will focus on inventions for inductive inference that date back to the 260-year-old topic - Bayes (1763)."}, {"title": "5.1 The Bayesian invention", "content": "There have been debates on whether scientific tools such as logic, mathematics, and statis-tics belong to science. This is particularly relevant because the proposed TB framework we consider here is based on empirical science. Our simple solution is to consider the content for which the statistical method is designed. To be specific, consider the scientific problem of Bayes (1763) from an experiment consisting of n independent and identically distributed"}, {"title": "5.2 The fiducial inspiration", "content": "Philosophically, the primary foundation of frequentism lies in the interpretation of proba-bility as a measure of long-run frequency of events in repeated trials. This perspective was principally developed (created) by Ronald Fisher, as well as Jerzy Neyman and Egon Pear-son. Ronald Fisher made significant contributions by introducing the concept of significance testing, which evaluates how surprising a statistic is with respect to a null hypothesis.\nNeyman and Pearson expanded on Fisher's ideas to address scenarios involving multiple competing hypotheses. They proposed that the likelihood ratio, comparing probabilities under different hypotheses, could be used to maximize the differentiation between these hypotheses. Their work introduced the formal framework of Type I and Type II errors, with Type I errors representing false positives (rejecting a true null hypothesis) and Type II"}, {"title": "5.3 The Dempster-Shafer discovery", "content": "As (the creation of) both a successor fiducial and a generalization of Bayes (see Dempster (2008)), the Dempster-Shafer theory builds upon Dempster's discovery of the need of using upper and lower probabilities for inference and Shafer's development of a broader framework Shafer (1976) to define and manipulate belief functions. Its dual notions of belief and plausibility offer a nuanced way to assess confidence in propositions when full probabilistic information is unavailable.\nHere, from the TB perspective, consider the running example of inference with a bino-mial count. A natural way of extending Fisher's fiducial approach to continuous examples to the discrete cases would lead to considering the set-valued mapping (5):\n$\\Theta_x(u) = \\{ \\theta : \\text{Beta}_{x, n-x+1}(1-u) < \\theta < \\text{Beta}_{x+1, n-x}(1-u) \\}.$\nUsing a predictive interval for the unobserved auxiliary variable U, for example,\n$U_\\alpha = \\Big[ \\frac{\\alpha}{2}, 1 - \\frac{\\alpha}{2} \\Big], \\quad \\alpha \\in (0, 1)$,\nwe can construct a $(1 - \\alpha)100\\%$ frequentist confidence interval\n$\\Theta_\\alpha(u) = \\bigcup_{u \\in U_\\alpha} \\Theta_x(u) = [\\text{Beta}_{x, n-x+1}(\\alpha/2), \\text{Beta}_{x+1, n-x}(1-\\alpha/2)]$\nfor \u03b8, which corresponds to the method of Pearson (1920). Interestingly, this supports the perception that frequentist ideas are \u201cin the air\u201d when R. A. Fisher became what we now consider as frequentist.\nIn addition to the discovery of the necessity of using set-valued inverse mapping and, thereby, lower-and-upper or imprecise probabilities, other innovations of the Dempster-Shafer theory include combining information, Dempster's rule of combination, and the mathematical theory of evidence (Shafer, 1976). From the TB perspective, everything seems to have come together to form a satisfactory logic of science, except for the frequency evaluation that the majority of scientists apparently considered important logically by the nature of science. Viewed as TB-evaluation, this is discussed in depth in Martin et al. (2010) and Zhang and Liu (2011), which eventually lead to the work discussed next in Section 5.4. For an extensive review of the Dempster-Shafer theory and more discussion of its frequency properties, see Yager and Liu (2008), Liu and Martin (2015), Den\u0153ux (2016), and Den\u0153ux and Li (2018)."}, {"title": "5.4 The inferential models framework", "content": "Creating a fully satisfactory logic of science is probably still an unsolved problem. This was evidently in statistics and science near the turn of this millennium. For example, John"}, {"title": "6 Concluding Remarks", "content": "Currently, statistics is comprised of a multitude of disparate estimation and hypothesis testing techniques, and when a hypothesis is rejected, it's often unclear how to incorporate the knowledge of that rejection into the new estimation problem. However, in science, the formulation of hypotheses and their testing does not occur in a vacuum, and more plausible alternatives are often required to gain broad traction when rejecting an existing theory. In this work, we introduce an overarching framework that ties together hypothesis testing and estimation for the purposes of making scientific discoveries. This framework provides a direction to reconcile differences in foundational approaches to statistics, such as frequentism, Bayesianism, and inferential models, by placing them in their proper context within the process of creativity. While creation's natural statistical analogue is modeling and estimation and evaluation's is hypothesis testing, there are not yet well established methods for exploration. In our current era of weak-AI development, we have the ability to estimate arbitrary functions and incorporate data of various modalities into prediction machines, but these are all constrained in their capacity by their formulation. Strong AI will need to encompass all three steps of scientific discovery, and improvements in the creation and exploration steps will be necessary to get there.\nTechnically, we proposed a very simple statistical framework for understanding and analyzing creativity (see, e.g., Wiggins, 2006, for more elaborated systems on general cre-ativity). This framework is inspired by a detailed study of significant discoveries in celestial mechanics and physics and is illustrated through both a straightforward artificial exam-"}, {"title": "Appendices: Supplementary Materials", "content": null}, {"title": "A More examples of great innovations in astronomy and physics", "content": "Far more historical examples are available than we have had space to exploit here. In this section, we briefly summarize the observations of our studies of great discoveries in celestial mechanics and physics from a statistical perspective of their innovations. The first is the invention of the geocentric model that hypothesizes that the Sun, Moon, stars, and planets all orbit Earth. It was the predominant description of the cosmos in many ancient European civilizations, such as those of Aristotle (384-322 BC) in classical Greece and Ptolemy in Roman Egypt, as well as during the Islamic Golden Age. Claudius Ptolemy (c. 100 c. 170 AD), an Alexandrian mathematician, astronomer, astrologer, geographer, and music theorist, thought the solar system looked like this and standardized geocentrism. From a statistical perspective of the underlying creativity, we note that this model was built on two observations (Kuhn, 1992, pp. 5-20 and Wikipedia, 10/23/2024):\nFirst, from anywhere on Earth, the Sun appears to revolve around Earth once per day. While the Moon and the planets have their own motions, they also appear to revolve around Earth about once a day. The stars appeared to be fixed on a celestial sphere rotating once each day about an axis through the geographic poles of Earth. Second, the Earth seems to be unmoving from the perspective of an earthbound observer; it feels solid, stable, and stationary.\nThe geocentric model dominated for centuries until the discovery of the heliocentric sys-tem, known as the Heliocentric Hypothesis, of the Greek astronomer and mathematician Aristarchus of Samos (c.310 \u2013 \u0441. 230 \u0412\u0421). The heliocentric model places all of the then-known planets in their correct order around the Sun. With the invention of the telescope in 1609, observations made by Galileo Galilei led him to the thought that while this obser-vation was incompatible with the Ptolemaic system, it was a natural consequence of the heliocentric system. Continued observations and analysis led to variations of heliocentric system, where planets orbit in perfect circles.\nJohannes Kepler (1571 \u2013 1630) analyzed Tycho Brahe's famously accurate observations and afterwards constructed his three laws in 1609 and 1619: the orbit of a planet is an ellipse with the Sun at one of the two foci; a line segment joining a planet and the Sun sweeps out equal areas during equal intervals of time; and the square of a planet's orbital period is proportional to the cube of the length of the semi-major axis of its orbit. The"}, {"title": "B The implementation of the Gibbs sampler and the EM algorithm for the mixture of normals", "content": "With \u03bc\u03bf = 0 and \u03c3\u00b2 = 104, the steps for a Gibbs sampler with a Dirichlet prior are\n$\\sum_{i=1}^{K} \\pi_i = 1$ and $\\phi_i ^{(1)}$\n1. Initialize $\\pi_i ^{(1)}$,..., $\\pi_i ^{(K)}$ and $\\phi_i ^{(1)}$,..., $\\phi_i ^{(K)}$ such that \\\\\nall i \u2208 1, 2, ..., K \u2212 1.\n$\\Z_i ^{(t)} \\sim P(Z_i = k | \\overline{\\pi} ^{(t)}, Y_i) \\propto \\pi_k ^{(t)} \\exp \\Big( - \\frac{(Y_i - \\phi_k ^{(t)})^2}{2} \\Big)$\n2. Sample the augmented variable\nfor i = 1, ..., \u03b7.\n3. Using $n_k ^{(t)} = \\sum_{i=1}^{n} I(Z_i ^{(t)} = k)$ for k = 1,..., K, sample\n$(\\pi_1 ^{(t+1)}, ..., \\pi_K ^{(t+1)}) \\sim Dirichlet(1 + n_1 ^{(t)}, ..., 1 + n_k ^{(t)})$\n4. Let \\phi_k = $\\frac{1}{n_k}\\sum_{i=1}^nY_iI(Z_i ^{(t)} = k)$, and sample the posterior means\n$\\phi_i ^{(t+1)} \\sim N \\Big(\\frac{\\frac{\\mu_0}{\\sigma_0 ^2} + n_k ^{(t)} \\phi_k}{\\frac{1}{\\sigma_0 ^2} + n_k}, \\Big(\\frac{1}{\\sigma_0 ^2} + n_k\\Big) ^{-1} \\Big)$\nand go to step 2 to repeat.\nIf instead, we wish to calculate the MLE for the initial estimate, for brevity, we present that case when Kn = 2, where the kth expectation step is\n$w_i ^{(k)} = \\frac{\\pi_i ^{(k-1)} \\exp (-\\frac{(Y_i - \\phi_1 ^{(k)})^2}{2})}{\\pi_i ^{(k-1)} \\exp (-\\frac{(Y_i - \\phi_1 ^{(k)})^2}{2}) + (1-\\pi_i ^{(k-1)}) \\exp (-\\frac{(Y_i - \\phi_2 ^{(k)})^2}{2})}$\nand the kth maximization step is\n$\\sum_{i=1}^{n} w_i$\n$\\sum_{i=1}^{n}w_i$\n$\\pi_1 ^{(k)} = \\frac{i=1}{n}$, $\\phi_1 ^{(k)} = \\frac{i=1 w_i Y_i}{n}$ and $\\phi_2 ^{(k)} = \\frac{i=1 (1-w_i) Y_i}{\nBy choosing some reasonable initialization values, such as \\pi_1 ^{(0)} = 0.5, $\\phi_1 ^{(0)} = \\overline{Y} - 1$, and\n$\\phi_2 ^{(0)} = \\overline{Y} - 1$, and iterating until the $\\text{||} \\overrightarrow{\\pi} ^{(k)} - \\overrightarrow{\\pi} ^{(k-1)} \\text{||} < 10^6$, we obtain an approximate value for the MLE of the model parameters. The analogous setup where Kn > 2 has a separate wi calculated for each component to estimate a larger set of \u03c0\u012f values. Then, the di are calculated similarly to above for each of the components."}, {"title": "CA computational TB-evaluation with ChatGPT via a chain-of-thought and verification approach", "content": "We conducted a simple experiment of computational creativity to create a generalized logic of science with ChatGPT (OpenAI, 2024), a large language model. We use the chain-of-thought mechanism (Wei et al., 2022; Dhuliawala et al., 2023) in place of TB-evaluation."}]}