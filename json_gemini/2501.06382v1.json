{"title": "Dynamics of \"Spontaneous\" Topic Changes in Next Token Prediction with Self-Attention", "authors": ["Mumin Jia", "Jairo Diaz-Rodriguez"], "abstract": "Human cognition can spontaneously shift conversation topics, often triggered by emotional or contextual signals. In contrast, self-attention- based language models depend on structured statistical cues from input tokens for next-token prediction, lacking this spontaneity. Motivated by this distinction, we investigate the factors that influence the next-token prediction to change the topic of the input sequence. We define concepts of topic continuity, ambiguous sequences, and change of topic, based on defining a topic as a set of token priority graphs (TPGs). Using a simplified single-layer self-attention architecture, we derive analytical characteriza- tions of topic changes. Specifically, we demonstrate that (1) the model maintains the priority order of tokens related to the input topic, (2) a topic change occurs only if lower-priority tokens outnumber all higher-priority tokens of the input topic, and (3) unlike human cognition, longer context lengths and overlapping topics reduce the likelihood of spontaneous redi- rection. These insights highlight differences between human cognition and self-attention-based models in navigating topic changes and underscore the challenges in designing conversational AI capable of handling \"spontaneous\" conversations more naturally. To our knowledge, this is the first work to address these questions in such close relation to human conversation and thought.", "sections": [{"title": "1 Introduction", "content": "In human cognition, topic changes often occur sporadically, with little apparent structure or forethought: an spontaneous shift in focus during a conversation, a sudden leap between ideas when brainstorming, or an unexpected redirection in storytelling. These seemingly chaotic shifts are intrinsic to the human way of processing and communicating information (Christoff and Fox, 2018; Mills et al., 2020). For instance, consider a casual conversation where someone is discussing their favorite book, and then they suddenly mention an unrelated memory of a family gathering. This abrupt change may be due to an emotional connection, such as recalling reading that book during a family vacation, where sensory details like the scent of the ocean or the warmth of the sun trigger a vivid memory. The hippocampus helps link these sensory cues to specific memories, resulting in an unexpected yet meaningful shift (Lisman et al., 2017). The prefrontal cortex then integrates these memories, emotions, and contextual information, allowing for a spontaneous redirection of conversation (Miller and Cohen, 2001).\nSelf-attention mechanisms in language models rely on contextual signals in the input to perform topic shifts, with next-token prediction guided by the relevance of preceding tokens (Vaswani et al., 2017). Unlike humans, who transition fluidly and intuitively between topics, often driven by emotion or creativity, transformers follow a structured, statistical approach. They remain on topic unless explicit cues signal a change, reflecting a key distinction between the somehow rigid processing of language models and the dynamic transitions of human cognition (Brown et al., 2020; Clark et al., 2019; Devlin et al., 2019; Bisk et al., 2020).\nIt is also important to differentiate between topic changes and hallucinations. Hallucinations involve generating incorrect or fabricated information without clear contextual basis, often resulting in false or illogical content (Ji et al., 2023; Maynez et al., 2020). In contrast to topic changes, hallucinations are not deliberate or contextually driven shifts in discourse. More interestingly, some hallucinations may still appear to maintain topic consistency by staying loosely connected to the previous topic, yet they fail to introduce a meaningful or valid change. Our focus is on understanding when and why self-attention models predict next tokens from a different topic of the input, examining the conditions under which an input sequence from one topic leads to a prediction in another. Recent advancements in the related field have substantially deepened our understanding of self-attention mechanisms. The Transformer model, introduced by Vaswani et al. (2017), revolutionized natural language processing by using self- attention to determine the importance of each token in relation to others. This innovation enables Transformers to focus on relevant context when predicting the next token, significantly enhancing capabilities natural language understanding and generation (Brown et al., 2020; Devlin et al., 2019). Efforts to improve next token prediction have prioritized optimizing the efficiency of the self-attention mechanism, achieving empirical success across tasks such as machine translation, question answering, and text summarization. Additionally, connections between self-attention and support vector machines (SVM) have offered valuable insights into optimization strategies, and mechanisms within the next token prediction (Ataee Tarzanagh et al., 2023; Tarzanagh et al., 2023; Li et al., 2024b). These advancements underscore the transformative role of attention-based architectures in token prediction tasks. A recent study by Li et al. (2023) highlights that in mixed topic inputs, transformers achieve higher pairwise attention between same-topic words compared to different-topic words. However, our approach aligns more closely with human cognitive processes, where the input is typically centered on a single topic, ensuring that discussions or conversations remain within a consistent context or theme.\nDespite these advancements, our understanding of how LLMs manage topic changes in comparison to human cognition remains limited. Human cognition seamlessly navigates such transitions through spontaneous thought and contex- tual awareness (Christoff and Fox, 2018). Investigating this distinction could provide valuable insights into the strengths and limitations of language models in mimicking human-like conversation. Bridging this gap is essential for enhancing language models, particularly in their ability to adapt to and manage dynamic topic changes-a process closely aligned with the flow of spontaneous thought in human cognition. Specifically, we address the following research questions:\n\u2022 What underlying mechanism drives topic changes during next token predic- tion in self-attention architectures?\n\u2022 Which factors shape the dynamics of topic changes in next token prediction?\nTo answer the above questions, it is essential first to explore how self-attention allocates attention to tokens for next token prediction with respect to the input topic. Understanding how attention weights are allocated offers key insights into the prioritization of tokens by self-attention mechanisms, particularly when models are trained on datasets spanning multiple topics. This fundamental understanding will pave the way for uncovering the role of attention patterns in driving \"spontaneous\" topic changes. To the best of our knowledge, this is the first work to explore these questions in direct relation with human conversation and thought."}, {"title": "Summary of findings", "content": "For simplicity, we elaborate our results using a two-topic scenario, but it is straightforward to extend them to multiple topics. Imagine an oracle that is an expert on Topic A, capable of following any conversation within that topic while staying true to its context. Now, suppose the oracle gains knowledge of Topic B and is following a conversation about Topic A. Will the oracle's responses remain firmly within Topic A, or will the influence of the knowledge of Topic B cause the conversation to drift? This analogy encapsulates the problem we address: understanding when and why attention models might preserve a topic or change to another \"spontaneously\". Specifically, we make the following contributions:\n1. Preservation of input topic priorities. Using a controlled sandbox, we demonstrate in Theorem 3 that attention models trained on mixed-topic datasets maintain the priorities of tokens associated with the original topic of an input sequence (Topic A in our analogy).\n2. Changing topics triggered by token frequency. In Theorem 4, we show that only if a lower-priority token appears more frequently than all"}, {"title": "2 Problem setup", "content": "Next topic prediction with self-attention model.\nIn line with the approach presented by Ataee Tarzanagh et al. (2023) and Li et al. (2024b), we frame the next-token prediction task as a multi-class"}, {"title": "3 Defining Topics", "content": "In order to answer our research questions regarding the dynamics of topic changes we need to define the concept of a topic. In the previous settings, a dataset DSET generates TPGs {G(k)}K_1, but, conversely, an existing set of TPGs can generate DSET, akin to generating data from a particular topic, which leads to the following definitions.\nDefinition 2 (topic). A topic T is a set of TPGs {G(k)}K_1. Given topic T defined by TPGs {G(k)}K1, input sequence X belongs to T if \u2200x \u2208 X, x \u2208 G(x). A sequence (X, y) is within T if X belongs to T and \u2200x \u2208 X, (y \u21d2 x) \u2208 G(x).\nWe can define a topic as a set of TPGs because it inherently captures the relationships and contextual dependencies between tokens, offering a structured representation of a topic's semantics. This means that we can use a topic T to generate DSET by sampling sequences (X, y) within T. Given the finite number of edges, a DSET can be generated from T such that it can reconstruct the exact TPGs {G(k)}_1 that define T, following the construction method in Li et al. (2024b). This leads to the following reasonable assumption:\nAssumption 4. A DSET generated from any topic T defined by {G(k)}K=1 exactly reconstructs back the TPGs {G(k)}K_1.\nThis assumption enables the application of the results from Li et al. (2024b), with the concepts of topics and TPGs being used interchangeably.\nDefinition 3 (topic continuity). Given an input sequence X that belongs to T, a weight matrix W is said to keep topic T for the input sequence X if \u0177w \u2208 G(k) (X).\nThis definition establishes the criteria for a next predicted token to preserve the topic of the input sequence. Notice that if we have two topics, Ta and Tb, we can generate two different datasets DSETa and DSET. It is clear that Wal trained only with DSETa will always keep topic Ta. But we could also obtain Wab with a dataset combining DSETa and DSET, as training sets. The central question is whether Wab keeps topic Ta, given an input sequence X that belongs to Ta, or if it instead predicts tokens that prompt a topic change. This can be understood through an analogy: consider an oracle only proficient in Topic A, capable of following conversations in the context of Topic A. If this oracle gains knowledge of Topic B, will it continue to follow the conversation within Topic A's context when the conversation is within Topic A, or will its response begin to exhibit a shift to Topic B?"}, {"title": "4 Attention within mixed topics", "content": "Let's first understand the way in which attention models assign priority to tokens within mixed-topic setting. For simplicity, we elaborate our results using a two-topic scenario, but it is straightforward to extend all of our theoretical results on multiple topics. Notice the self-attention embedding output is a linear combination of X given by S(XW). The embeddings in X corresponding to the highest entries in the vector S(XW) will receive higher priority in their importance to predict the next token, therefore we can hypothesize that models in which S(XW) are ordered in a similar way will predict similar next tokens. This idea leads to our first main result which considers this situation within a mixed topics setting:\nTheorem 3. Consider datasets DSETa and DSET\u044c from topics Ta and T\u044c, respectively. Let DSETab be the union of DSETa and DSET. Suppose Assump- tions 1, 2, 3 and 4 hold. Set loss function as l(u) = -log(u). Starting Algo-GD from any initial point with constant size n and if Wsvm \u2260 0 and Wsum \u2260 0; for a given sequence X that belongs to Ta, we have that Wab preserves the attention priority of Ta on input X. This is \u2200i, j\u2208 [T]:\n\u2022 if [S(XWax)] = [S(XWax)]; then [S(XWabx)] = [S(XWabx)]j\n\u2022 if [S(XWax)] > [S(XWax)]; then [S(XWabx)] \u2265 [S(XWabx)]j\n\u2022 if [S(XWax)] < [S(XWax)]; then [S(XWabx)]i \u2264 [S(XWabx)]j\nThis implies that for an input sequence X, a model trained in a mixed-topic setting will maintain the priority of the topic to which X belongs. Consequently, the attention will be allocated in the same order as if the model had been trained exclusively on the original topic of X. The input sequence X = [e5, \u04351, \u0435\u0437, \u04354]T, as shown in Figure 2 (top), is generated from Ta. The predicted next token \u0177wab is e5 and the highest probability SCC in mixed-topics is G (X) = {e5}. Since \u0177wat belongs to Gat (X), Wab for input sequence X is considered as topic continuity, based on the Definition 3.\nThe only assumption about X on Theorem 3 is that it belongs to Ta. However, if X belongs to Ta and Tb, the priority will be preserved within both topics. Additionally, strict equality in the attention priority is maintained, but strict inequalities may not be when combining both datasets. In such cases, the union of their TPGs can form new SCCs, disrupting strict priority. As illustrated in Figure 1, we present G(4) and G(4) as the TPGs corresponding to last input token e4 for Ta and T\u266d, respectively. In Ga4), the token priority is e5 > e3 > e\u2081 = e2 > e4. Once combining G(4) and G(4), the priority order in the mixed-topics is e5 > e3 > e\u2081 = e2 e4, based on G(4). The strict equality e1 = e2 from Ga4) is"}, {"title": "5 Explaining topic shifts", "content": "The formation of new SCCs when combining datasets suggests that the highest priority SCC for some input sequences may increase in size in this new setting. This also suggests that topic shifts may arise from ambiguity within an input sequence rather than a straightforward change in topic. In our analogy on the oracle, gaining knowledge of both Topic A and Topic B might cause a conversation to be naturally followed within Topic A or also outside Topic A. Thus, we introduce the following definition to characterize this phenomenon:\nDefinition 4 (ambiguous sequence). Given DSET, and DSET, generated from two different topics Ta and T. Denote Tab as the combined topic defined by a combination of DSETa and DSET. A sequence X that belongs to Ta is ambiguous in Tab with respect to Ta if Wat does not keep topic Ta for X, but G()(X) CG) (X).\nDefinition 4 defines an ambiguous sequence as one where the highest-probability next-token predictions include tokens from both within and outside the input topic, reflecting natural ambiguity from overlapping topics. For instance, the input sequence X = [e1, e4, \u04351, \u04354]T in Figure 2 (middle) belongs to both Ta and T. G4 (X) is {e1}, as depicted in G(4) from Figure 1 (left) and G(X) is {e1, e4}, as shown in G4) Gab from Figure 1 (right). G(4) (X) is the subset of G(4) G(X), though war & G4) (X). We can argue that the next token predicted from an ambiguous sequence cannot be considered as a topic change, as it lacks the clear trigger phenomenon observed in human cognition. To address this, we propose a formal definition for a topic change:\nDefinition 5 (change of topic). Given DSETa and DSET, generated from two different topics Ta and Tb, and a sequence X that belongs to Ta. The weight matrix Wab changes topic Ta for sequence X if Wab does not keep topic Ta for X and X is not ambiguous in Tab with respect to Ta.\nFollowing the above definition, Wab for the input sequence X = [e5, e4, e4, e4], from Figure 2 (bottom), can be considered as change of topic.\nBuilding on the formal definitions of topic continuity, ambiguous sequences, and topic changes, we now present a necessary condition for a sequence to induce a topic change. This is achieved by introducing our final definition, grounded in the highest-priority SCC as determined by the order in the attention layer. Now, our second key result.\nDefinition 6 (highest priority SCC). Consider a sequence X that belongs to T. We define \u0121() (X) \u2208 G(2) as the highest priority SCC for X in G(x) such that \u2200x \u2208 G() (X) and xj \u2208 G() we have (xi \u21d2 xj) \u2208 G(z) or (xi = xj) \u2208 G(1).\nTheorem 4. Under the same settings and assumptions on datasets and training in Theorem 3, let X be a sequence that belongs to Ta. If Wab changes topic Ta for X then \u2203x; &G() (X) such that \u2200xi \u2208 Ga) (X), the number of times xj appears in X is greater than the number of times xi appears in X."}, {"title": "6 Discussion", "content": "Our findings on a simple single-layer attention model provide valuable clues about the fundamental differences in conversational dynamics between humans and machines. Unlike humans, who naturally shift topics through curiosity or contextual association, the model adheres to predictive patterns, mirroring LLMs' tendency to stay focused on a single thread. This suggests that spontaneous topic shifts, a hallmark of human conversation, may require mechanisms beyond predictive pattern recognition. These insights raise philosophical questions about the nature of human-like interaction and highlight the need for adaptability and creativity in advancing conversational AI."}, {"title": "A Technical Proofs", "content": ""}, {"title": "A.1 Proofs of lemmas", "content": "Proof of Lemma 2\nLet a = S(XW).\nCfw(X) = C (XTS(XW))\n= C (XTa) (3)\n= C (4)\n= (5)\n= (6)\nLet ki be the number of times token x\u2081 appears in X. Then,\n[Cf\u0175(X)]x\u2081 = kiai.\nFrom Assumption 3 we have that\n[Cfw(X)]x\u2081 = [Cfw(X)] x\u2081 \u21d4 Ai = aj  (7)\n\u21d4 (Xi = xj) \u2208 G(x) or Xi = Xj.  (8)\nIf xi \u2260 xj then xi and xj are in the same SCC.\nLemma 6. For an input sequence X that belongs to T and i, j \u2208 [T],\n\u2022 [S(XWx)] = [S(X\u0174x)]; \u21d4 (x\u2081 = xj) \u2208 G(x) or i = j.\n\u2022 [S(XWx)] < [S(X\u0174x)]; \u21d4 (xj \u21d2 xi) \u2208 G(x).\n\u2022 [S(XWx)] > [S(X\u0174x)]; \u21d4 (x\u2081 \u21d2 xj) \u2208 G(x).\nProof. Since X belongs to T, \u2200x \u2208 X we have x \u2208 G(\u314d), therefore from the construction of TPGs by Li et al. (2024b), for every xi, xj \u2208 X we have one of the these relationships: (xi \u21d2 Xj), (xj \u21d2 Xi), (Xi = xj) or xi = xj. From the constraints in Algo-GD:\n- [S(XWx)] = [S(X\u0174x)]; \u21d4 (xi \u2013 xj)\u0174x = 0 \u21d4 (xj < xz) \u2208 G() or i = j.\n\u2022 [S(XWx)] > [S(X\u0174x)]; \u21d4 (xi - xj)\u0174x > 1 \u21d4 (xj \u21d2 xi) \u2208 G(x).\n- [S(XWx)]; < [S(X\u0174x)]; \u21d4 (x \u2212 xj)\u0174x < 1 \u21d4 (x\u00bf \u21d2 xj) \u2208 G().\nLemma 7. For an input sequence X that belongs to T,\n\u0121(#) (X) = {xi | [S(XWx)]; = ||S(XWx)||\u221e}.\nProof. Let G = {xi | [S(X\u0174x)]; = ||S(X\u0174x)||\u00a3}. From Lemma 6 \u2200xi, xj \u2208 G, (XiXj) \u2208 G(x). Therefore all elements in G belong to the same SCC. Also from Lemma 6, \u2200xi \u2208 G,xj \u00a3 G we have (xi \u21d2 xj) \u2208 G(). This means that every element in G has the highest priority among tokens in X concluding our proof."}, {"title": "A.2 Proof of Theorem 3", "content": "From construction, \u2200k \u2208 [K], G(k) Gk) CGk This means that \u2200xi, xj \u2208 X, we have:\n\u2022 if (xi = xj) \u2208 G\u00ae) then (xi =xj) \u2208 G\n\u2022 if (xj \u2192 xi) \u2208 G) then (xj\u21d2 x\u2081) \u2208 G or (xixj) \u2208 G\n\u2022 if (xi \u21d2 xj) \u2208 Ga) then (xi \u21d2 xj) \u2208 or (xixj) EG \u2208 Gab\nCombining with Lemma 6:\n\u2022 [S(XWax)] = [S(X\u0174ax)]; (xi = xj) \u2208 Ga) or i = j, then (xi =xj) \u2208 G) or i = j \u2194 [S(X\u0174abx)]i = [S(X\u0174abx)];\n\u2022 [S(XWax)]; < [S(X\u0174ax)]; \u21d4 (xj \u2192 xi) \u2208 G then (xj \u2192 xi) \u2208 Ga or (xixj) \u2208 G [S(XWabx)]i \u2264 [S(X\u0174abx)]j\n\u2022 [S(XWax)] > [S(X\u0174ax)]; \u21d4 (xi \u21d2 xj) \u2208 Gaz) then (xi \u21d2 xj) \u2208 G or (xixj) \u2208G [S(XWabx)]i \u2265 [S(XWabx)]j"}, {"title": "A.3 Proof of Theorem 4", "content": "Let a = S(XWax) and b = S(X\u0174ab\u0101). Without loss of generality, suppose a is in decreasing order a\u2081 > \u2026 > \u03b1\u03c4. From Theorem 3, we also have b\u2081 \u2265\u2026 \u2265 by. Let ki be the number of times token xi appears in X. Following an analogous procedure as in Lemma 2 we get\n (9)\n (10)\nWe will proof the contrapositive: If \u2203x \u2208 \u0121) (X) such that ki \u2265 kj for all j\u2208 [K], then there is no change of topic, so Wab keeps topic Ta for input sequence X, or X is ambiguous in Tab with respect to \u03a4\u03b1.\nFrom Lemma 7, if xi \u2208 \u0120) (X), we have ai \u2265 aj for all j\u2208 [K]. Suppose \u2203xi\u2208 G) (X) such that ki \u2265 kj for all j\u2208 [K], we have that kiai \u2265 kjaj for all j\u2208 [K] then xi \u2208 G) (X). Analogously since bi \u2265 bj, xi \u2208 G(X). If \u039e\u03b1\u03b9 \u2208 G) (X) with x\u03b9 \u2260 xi then k\u2081\u03b1\u03b9 \u2265 kjaj for all j\u2208 [K], then k\u0131ai = kiai. Therefore from Assumption 3 and Lemma 7, (x1 = xi) \u2208 G(). Analogously (x1 = xi) \u2208 G). This means that if \u2203xi \u2208 G) (X) such that ki > kj for all j\u2208 [K], then G) (X) CG (X). Then Wab keeps topic Ta for input sequence X, or X is ambiguous in Tab with respect to Ta."}, {"title": "A.4 Proof of Theorem 5", "content": "1. This is a direct consequence from the law of large numbers. If T \u2192 \u221e the proportion of each token will match the probability. Since p > maxx\u00a2\u00a2\u00ae) (x) P (x = x), then the probability that \u2203x'; & G (X) such that \u2200x \u2208 G) (X), the number of times x appears in X' is greater than the number of times x appears in X' will go to zero, and therefore the probability of change topics will do it also.\n2. Without loss of generality suppose (\u00ae) (X) = {x1, x2,\u2026,1}. Clearly if we prove the result assuming \u2200x \u2208 G\u00af) (X), p = P (x = x), we will also have it for the more general case p = minx\u2208\u0121(\u00ae) (x) P (x = x).\nLet X = [1,12,1 x1] be a random sequences generated as described in the theorem, where the size of G() (X) is l. Let ki,t be the number of times xi is selected in X\u00a6 . Let A\u2081 = max1<i<l ki,l and B\u2081 = max\u0131+1<i<K ki,l\u2022 Let P(1) = P(B\u2081 > A\u2081). We want to prove P(l + 1) < P(l). We construct a coupling between X and X1+1 by performing T independent trials. For each trial i we generate a uniform random variable Ui in [0,1] and we choose tokens in X and X1+1 in this way:\n\u2022 If U \u2264 pl both the selected tokens x'11 and x1,1+1 are in {x1, x2,\u00b7\u00b7\u00b7, \u03b9}.\nil\n\u2022 If pl < Ui < p(1+1), we select x1 = x1+1 if Ui < pl + q or x\u2081\u2081,\u2081 = xl+2 otherwise, and we select x1,1+1 = x1+1; where q is the probability of choosing x1+1 in X\u00ed. Since p > q, there is an interval where xi,1 = X1+2 but xi,l+1 = X1+1.\n\u2022 If U\u00bf > p(l + 1), then both the selected tokens x1 and x1+1 are in {X1+2,X2,\u00b7\u00b7\u00b7,X1}. Notice that the probability of choosing xi in X1+1 for i \u2265 1 + 2 decreases because p is constant.\nFrom the previous coupling we have that ki,1 = ki,1+1 for 1 \u2264 i \u2264 1, k\u0131+1,1 \u2264 ki+1,1+1 for i = 1 + 1, and ki,1 \u2265 ki,l+1 for i \u2265 1 + 2. This means that A1+1 = max(A\u0131, k\u0131+1,1+1) > Al and Bi+1 = max\u0131+2<i<Kk ki,l+1 \u2264 B\u0131. Therefore P(l + 1) = P(B1+1 > Al+1) < P(B\u03b9 > \u0391\u03b9) = P(l)."}, {"title": "B Detailed simulation studies", "content": ""}, {"title": "B.1 Simulation process", "content": "Theoretical TPGs generation. For each token ek, L edges are randomly selected to construct the theoretical TPG Gtheor forek, ensuring that ek is involved, as either a source or destination node. Based on these selected edges, we add additional edges from ek to all other tokens included in Ledges, thereby ensuring that all tokens in Gtheor can be reached by ek. Thus, we obtain the theoretical TPGs {Gatheor} K-1 for Topic A. This process is repeated to generate another group of theoretical TPGs {9(theor}K-1 for the Topic B. Let G and G(k) Gb theor combine for each k, we obtain the theoretical TPGs for topics combinations {Gab,theor}k=1\nTraining Dataset Generation. Generate training datasets DSETa and DSET, based on {Gaktheor}K=1 }=1 and {Gotheor}=1, respectively. For each in- put sequence in DSET, the sequence length Ttrain is 4, which means X = [X1 X2 XTtrain] \u2208 RTtrain\u00d7d with xi from E = [e1, e2, ...ek]T. ek is ran- domly selected as the last token and other tokens (other input tokens and the next predicted token) are chosen based on Gtheor. Specifically, the next token eTtrain+1 is determined by sampling with the weighted probability in Gtheor, where the weight for each token corresponds to the number of outcoming edges. Given Assumption 2, we randomly choose the position of the next token in the input sequence. Then, the remaining input tokens are randomly selected from tokens connected by incoming edges from ek (i.e., ek \u2192 ei) and placed in the random position within the input sequence. This process is repeated n times to generate training data for each topic respectively. Empirical TPGs {Gakempir}=1 and {9empir}=1 are derived from the training datasets DSETa and DSET. According to Assumption 4, the empirical TPGs {G(k) empir Sk=1 are"}, {"title": "B.2 Experiments results", "content": "In this section, we provide empirical evidence to support our findings outlined in Theorem 3, Theorem 4, and Theorem 5, offering detailed insights into the behavior of self-attention mechanisms for changing topics."}, {"title": "B.2.1 Simulation in Section 4", "content": "For Figure 3 we used K = 10, L = 4, d = 16, and Ttest = Ttrain = 4, we also showed point-wise confidence intervals of rank similarities on all 5000 instances"}, {"title": "B.2.2 Simulation in Section 5", "content": "For Figure 4 (left), we predict next tokens for 5000 test sequences from Ta with Ttest = {4,8,16, 24, 32, 64, 128, 256, 512}, while fixing L = 4, d = 16, Ttrain = 4, and K = 10. The proportion of each scenario with varying T is illustrated in Table 1. For Figure 4 (right), we predict next tokens for 5000 test sequences (the sequence length is Ttest = 20) using models trained with L = {4, 6, 8, 10, 12, 14, 16, 18}, d = 16, K = 10, and Ttrain 4. The proportion of each scenario with varying L is illustrated in Table 2."}, {"title": "B.2.3 Additional experiments", "content": "Building upon the convergence experiments in Li et al. (2024b), our work demon- strates that the correlation coefficients are"}]}