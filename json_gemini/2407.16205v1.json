{"title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models", "authors": ["Shi Lin", "Rongchang Li", "Xun Wang", "Changting Lin", "Wenpeng Xing", "Meng Han"], "abstract": "The rapid development of Large Language Models (LLMs) has brought remarkable generative capabilities across diverse tasks. However, despite the impressive achievements, these models still have numerous security vulnerabilities, particularly when faced with jailbreak attacks. Therefore, by investigating jailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us in developing more robust defense mechanisms to fortify their security. In this paper, we further explore the boundary of jailbreak attacks on LLMs and propose Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes advantage of LLMs' growing analyzing and reasoning capability and reveals their underlying vulnerabilities when facing analysis-based tasks. We conduct a detailed evaluation of ABJ across various open-source and closed-source LLMs, which achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE) on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and efficiency. Our research highlights the importance of prioritizing and enhancing the safety of LLMs to mitigate the risks of misuse.", "sections": [{"title": "Introduction", "content": "With the ongoing advancement of Large Language Models (LLMs) like ChatGPT (OpenAI 2022), GPT-4 (Achiam et al. 2023), Claude (Anthropic 2023), Llama2 (Touvron et al. 2023) and Gemini (Team et al. 2023), numerous tasks such as math (Wei et al. 2022), machine translation (Peng et al. 2023), programming (Zhang et al. 2023), text generation (Yuan et al. 2022) and reasoning (Qin et al. 2023; Zhong et al. 2023) can be effectively addressed by these models. However, the impressive capabilities of LLMs have sparked concerns about their potential misuse, particularly in generating harmful responses or illegal outputs when faced with carefully crafted malicious jailbreak attack prompts. Despite efforts to align LLMs with human laws and regulations through methods such as Supervised Fine-tuning (SFT) (Wei et al. 2021; Chung et al. 2024), Reinforcement Learning from Human Feedback (RLHF) (Christiano et al. 2017; Ouyang et al. 2022), LLMs remain vulnerable to elaborately manipulated jailbreak attack prompts (Carlini et al. 2024; Perez and Ribeiro 2022; Shen et al. 2023; Li et al. 2023a), where adversaries manipulate and transform the original user prompt to conceal their malicious intentions and circumvent LLMs' safety protocols. These findings underscore the urgent need for further research into LLM vulnerabilities in jailbreak attacks to develop enhanced security measures.\nResearchers have extensively explored jailbreak attacks on LLMs, revealing that their diverse capabilities can provide attackers with new surfaces to exploit, introducing novel vulnerabilities. Specifically, (Ren et al. 2024; Lv et al. 2024; Ding et al. 2023) demonstrate that LLMs are vulnerable when handling code-based tasks. Besides, (Ding et al. 2023; Li et al. 2023b; Liu et al. 2023b) reveal that LLMs can be easily compromised in generating fictional role-play"}, {"title": "Related Work", "content": "Safety-aligned LLMs\nSafety-aligned large language models (LLMs) such as GPT-4 and Claude-3, embody a commitment to adhering to human ethical and legal preferences (Ziegler et al. 2019; Solaiman and Dennison 2021; Korbak et al. 2023; Rafailov et al. 2024), with interventions occurring at both the data and training levels. Initiatives like meticulous pre-training data filtering (Xu et al. 2020; Wang et al. 2022; Welbl et al. 2021) are pivotal, employing heuristic and automated classifiers to sift through vast datasets, ensuring the foundational training material aligns with ethical guidelines. The training process itself leverages sophisticated methodologies, including Supervised Fine-Tuning (SFT) (Wu et al. 2021) and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022), to refine the models' output per human values. Despite these efforts, research has been intensifying on aligning LLMs more effectively and efficiently (Zheng et al. 2024; Xu et al. 2024; Ji et al. 2024; Zhang et al. 2023). For instance, (Bai et al. 2022) introduces Constitutional AI, a method that encodes desirable AI behavior in a simple and transparent form, enabling more precise control with minimal human labeling. (Sun et al. 2024) develop SELF-ALIGN, a novel approach that leverages principle-driven reasoning and the generative capabilities of LLMs to achieve self-alignment of AI agents with minimal human supervision. (Dong et al. 2023) propose RAFT, an alignment framework that fine-tunes LLMs using samples efficiently ranked by reward functions. In this study, we validate the effectiveness of our approach on both open-source and closed-source LLMs and reveal that utilizing LLMs to perform analyzing-based tasks enables the evasion of safety alignment, which provides a valuable testing ground for the efficacy of these alignment methods.\nJailbreak Attacks on LLMS\nDespite concerted efforts to align LLMs with human values, jailbreak attacks (Deng et al. 2023; Lapid, Langberg, and Sipper 2023; Jin et al. 2023; Deng et al. 2024; Jones et al. 2023), which prompt LLMs to produce harmful content, still pose a significant risk to LLMs. Currently, jailbreak attack methods such as GCG (Zou et al. 2023), AutoDAN (Liu et al. 2023a), GPTFuzzer (Yu, Lin, and Xing 2023), PAIR (Chao et al. 2023) successfully induce LLMs to output harmful content by introducing perturbation and linguistic variation to the original malicious input. While these methods have proven effective, they neglect to analyze and explore the inherent and suspected vulnerability of LLMs, which leads to the problems of low attack success rate and attack efficiency, falling into the trap of blind attack.\nTo further understand the vulnerability of safety-trained LLMs to jailbreak attacks, (Wei, Haghtalab, and Steinhardt 2024) propose two failure modes as principles to guide the design of jailbreak attack methods: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. Based"}, {"title": "Methodology: Analyzing-based Jailbreak", "content": "In this section, we elaborate in detail on Analyzing-based Jailbreak(ABJ). This novel jailbreak attack method evaluates the safety of LLMs in handling analyzing-based tasks. ABJ consists of two main steps: data preparation and data analysis. Specifically, we first generate customized data pertinent to the original malicious input. Subsequently, we guide the target LLM in analyzing the prepared data and generate potentially harmful or inappropriate outputs. The entire process is autonomously executed by LLMs without requiring additional training or optimization. Figure 2 outlines ABJ, while Algorithm 1 provides the specific implementation details. We will introduce the step-by-step process of ABJ in the following sections."}, {"title": "Formulation", "content": "We formulate the jailbreak attack as follows: given a target model $LLM_{target}$ to be evaluated and an initial malicious input $X$, the objective of the jailbreak attack is to identify a specific modification strategy $S$ within a bounded policy space. The strategy $S$ aims to modify the initial malicious input $X$ to maximize the likelihood of the response from the target model $LLM_{target}$ being classified as malicious by a designated harmfulness evaluator $M_{eval}$. This can be represented as:\n$S^* = \\underset{S}{\\operatorname{argmax}} M_{eval}(LLM_{target}(S(X)))$ \nwhere $S^*$ is the strategy that maximizes the harmfulness score assigned by $M_{eval}$ to the response of the modified prompt $S(X)$. Notably, a higher harmfulness score indicates a greater likelihood of a successful jailbreak attack.\nIn this work, we focus on leveraging the inherent analyzing and reasoning capabilities of LLMs to enhance the effectiveness of jailbreak attacks in behavior prediction scenarios during the inference stage while simultaneously preserving the helpfulness of input, thereby circumventing the detection of malicious intentions by LLMs' defense mechanisms."}, {"title": "Design of Analyzing-based Jailbreak", "content": "Step I: Data Preparation. The first step of ABJ involves generating customized data related to the original malicious input. To this end, we consider addressing two pivotal issues: (1) what type of data should be generated, and (2) how to effectively generate the required data. To resolve the first issue, we thoroughly examined the workflow underlying data analysis tasks. In this work, we primarily focus on leveraging data analysis for behavior prediction tasks. For instance,"}, {"title": "Experiment", "content": "In this section, we present extensive evaluation and analysis across closed-source and open-source LLMs to demonstrate the effectiveness of ABJ.\nExperimental Setup\nDatasets. We leverage the Harmful Behaviors dataset from AdvBench (Zou et al. 2023), which comprises 520 carefully crafted prompts designed to evaluate the safety performance of LLMs. This dataset is meticulously assembled to cover a wide range of prohibited scenarios, which allows us to conduct a comprehensive vulnerability assessment of LLMs.\nTarget Models. To conduct a comprehensive assessment of the security of LLMs, we evaluate both open-source and closed-source LLMs, including GPT-3.5-turbo-0125, GPT-4-turbo-0409, Claude-3-haiku-0307, Llama-3-8B-Instruct, Qwen-2-7B-Chat, GLM-4-9B-Chat. Each of these models adheres rigorously to established safety alignments, guaranteeing that our work proceeds within a framework of secure and ethical AI. The hyperparameters are in the default settings.\nMetrics. We employ two metrics to evaluate jailbreak attacks described as follows:\n\u2022 Attack Success Rate (ASR): ASR is the percentage of unsafe responses in all responses generated by the target model. We also introduce ASR-Ensemble (ASR-E). We use one jailbreak attack prompt to attack the target model three times. The attack is considered successful if at least one attempt works. A higher ASR and ASR-E indicates better attack effectiveness.\n\u2022 Attack Efficiency (AE): AE is the number of accesses to the target model during a successful attack. A lower AE indicates better attack efficiency.\nJudgement. Following evaluation protocol from previous studies (Chiang and Lee 2023; Liu et al. 2023c) that LLMs can serve as reliable evaluators, we instruct GPT-4-turbo-0409 as a robust safety evaluator to judge whether a response is harmful (see Table 5 for more details). Additionally, considering the potential limitations of a single LLM-based evaluator in accurately discerning the intent behind lengthy and intricate responses, we manually reviewed the content initially deemed harmless in preliminary assessments to enhance judgment accuracy. Combining these two approaches can balance judgment efficiency and accuracy.\nBaselines. We compare ABJ with five representative baseline methods described as follows:\n\u2022 Direct Attack (DA): The attacker initiates Direct Attack by directly delivering harmful query to LLMs.\n\u2022 DeepInception: DeepInception (Li et al. 2023b) leverages the role-playing capability of LLMs to create a deeply-nested scenario, thereby inducing harmful contents in the target LLMs.\n\u2022 Prompt Automatic Iterative Refinement (PAIR): PAIR (Chao et al. 2023) is an optimization-driven jailbreak attack method that iteratively enhances the prompt to target LLMs to provoke malicious outputs.\n\u2022 Greedy Coordinate Gradient (GCG): GCG (Zou et al. 2023) crafts adversarial examples via greedy and gradient-based discrete optimization, requiring full access of LLM's weights and architecture to search for token sequences that bypass the safety defenses of LLMs.\n\u2022 AutoDAN: AutoDAN (Liu et al. 2023a) utilizes genetic algorithms to iteratively optimize adversarial examples, which can be automated to generate stealthy jailbreak prompts using a hierarchical genetic algorithm.\nSafeguards. We consider four safeguards against jailbreak attacks described as follows:\n\u2022 OpenAI Moderation Endpoint API: OpenAI Moderation Endpoint API (OpenAI 2023) is an authorized content moderation tool provided by OpenAI. It utilizes a multi-label classifier to categorize responses generated by language models into 13 specific categories. Any response that breaches these categories is flagged as violating OpenAI usage policy (OpenAI 2024).\n\u2022 Perplexity Filter (PPL): Perplexity Filter (Jain et al. 2023) is created to identify incomprehensible attack prompts. It establishes a threshold and employs another language model to compute the perplexity of the entire prompt or its segmented portions. Prompts surpassing this threshold are subsequently removed from consideration.\n\u2022 SmoothLLM: SmoothLLM (Robey et al. 2023) has two steps, perturbation step and aggregation step. In the perturbation step, random perturb prompts are passed as input to the LLM. In the aggregation step, it obtains a collection of perturbed prompts and aggregates the corresponding predictions.\n\u2022 In-context Defense (ICD): ICD (Wei, Wang, and Wang 2023) enhances model robustness by providing demon-"}, {"title": "Experimental Results", "content": "Our method achieves a high attack success rate and attack efficiency against all target LLMs. The results presented in Table 1 demonstrate that ABJ has a high attack success rate across open-source and closed-source LLMs while attaining remarkable attack efficiency. Notably, ABJ achieves over 90% ASR and 95% ASR-E on GPT-3.5-turbo-0125 and GPT-4-turbo-0409, while the AE is around 1, showcasing its effectiveness and efficiency compared to other baselines. Moreover, ABJ maintains a high ASR on Llama-3-8B-Instruct and Claude-3-haiku-0307, which are considered two of the currently secure LLMs with strong safety capabilities, exceeding 35% and 60%, respectively. The above experimental results demonstrate that LLMs' analyzing and reasoning capabilities can be easily exploited by attackers, becoming even more serious security vulnerabilities for LLMs to overcome. We provide more demonstrations in Figure 4 in the Appendix.\nOur method achieves a high attack success rate on different categories of harmful prompts. To conduct a more detailed analysis of the safety performance of target LLMs when confronted with different categories of harmful prompts, we adopt the classification approach from a previous study (Ding et al. 2023) and divide the Harmful Behaviors dataset into 7 distinct categories using GPT-4-turbo-0409 (The classification prompt can be seen in Table 5). As shown in Table 2 and Figure 3, our experimental evaluations across these categories reveal the vulnerability of target LLMs to different types of harmful prompts. Notably, the ASR and ASR-E on Malware and Economic Harm prompts are relatively higher than other categories, indicating that LLMs are more vulnerable to these attacks. In contrast, LLMs exhibit enhanced robustness when confronted with Hate Speech and Physical Harm prompts, with comparatively lower ASR and ASR-E for these categories.\nOur method maintains effectiveness when facing existing defenses against jailbreak attacks. The results in Table 3 demonstrate that ABJ can successfully maintain its effectiveness when facing existing defense methods on all target models. Specifically, OpenAI Moderation failed to detect any harmful prompts, which we attribute to the weakness of its base model. Furthermore, the performance of the PPL Filter is also far from satisfactory, indicating that the jailbreak"}, {"title": "Ablation Study", "content": "In this section, we further explore the effectiveness of ABJ and conduct a series of ablation experiments.\nImpact of the mismatch from the target topic. In the first step of ABJ, we prepare customized data related to the original malicious input to induce the target LLMs to make harmful outputs. However, is ABJ still effective with unrelated data? To answer this question, we investigate the feasibility of ABJ in scenarios where the prepared data deviates from the topic of the target query. Specifically, we collect the character and feature data generated during the experiments in the previous section and use a random selection method to prepare data that does not match the topic of the original malicious input. As shown in Table 4, under these settings, ABJ"}, {"title": "Conclusion and Future Work", "content": "In this paper, we reveal the underlying vulnerabilities of safety-trained LLMs while dealing with analyzing-based tasks and propose a new approach to jailbreak LLMs, named Analyzing-based Jailbreak (ABJ), which leverages the increasing analyzing and reasoning capabilities of LLMs and successfully bypasses the defense of state-of-the-art LLMs, such as GPT-4 and Claude-3. We conduct comprehensive experiments on ABJ and demonstrate its effectiveness, efficiency, and robustness in different settings. These experiments highlight the necessity of developing a more comprehensive safety alignment to LLMs. To address this issue, a promising direction is to implement safety alignment techniques (e.g., SFT, RLHF, and DPO) to match the diverse capabilities of LLMs while avoiding compromising their performance. We hope this work can contribute to a broader understanding of the security challenges of LLMs and en-"}]}