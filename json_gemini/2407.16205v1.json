{"title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models", "authors": ["Shi Lin", "Rongchang Li", "Xun Wang", "Changting Lin", "Wenpeng Xing", "Meng Han"], "abstract": "The rapid development of Large Language Models (LLMs) has brought remarkable generative capabilities across diverse tasks. However, despite the impressive achievements, these models still have numerous security vulnerabilities, particularly when faced with jailbreak attacks. Therefore, by investigating jailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us in developing more robust defense mechanisms to fortify their security. In this paper, we further explore the boundary of jailbreak attacks on LLMs and propose Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes advantage of LLMs' growing analyzing and reasoning capability and reveals their underlying vulnerabilities when facing analysis-based tasks. We conduct a detailed evaluation of ABJ across various open-source and closed-source LLMs, which achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE) on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and efficiency. Our research highlights the importance of prioritizing and enhancing the safety of LLMs to mitigate the risks of misuse.", "sections": [{"title": "Introduction", "content": "With the ongoing advancement of Large Language Models (LLMs) like ChatGPT (OpenAI 2022), GPT-4 (Achiam et al. 2023), Claude (Anthropic 2023), Llama2 (Touvron et al. 2023) and Gemini (Team et al. 2023), numerous tasks such as math (Wei et al. 2022), machine translation (Peng et al. 2023), programming (Zhang et al. 2023), text generation (Yuan et al. 2022) and reasoning (Qin et al. 2023; Zhong et al. 2023) can be effectively addressed by these models. However, the impressive capabilities of LLMs have sparked concerns about their potential misuse, particularly in generating harmful responses or illegal outputs when faced with carefully crafted malicious jailbreak attack prompts. Despite efforts to align LLMs with human laws and regulations through methods such as Supervised Fine-tuning (SFT) (Wei et al. 2021; Chung et al. 2024), Reinforcement Learning from Human Feedback (RLHF) (Christiano et al. 2017; Ouyang et al. 2022), LLMs remain vulnerable to elaborately manipulated jailbreak attack prompts (Carlini et al. 2024; Perez and Ribeiro 2022; Shen et al. 2023; Li et al. 2023a), where adversaries manipulate and transform the original user prompt to conceal their malicious intentions and circumvent LLMs' safety protocols. These findings underscore the urgent need for further research into LLM vulnerabilities in jailbreak attacks to develop enhanced security measures.\nResearchers have extensively explored jailbreak attacks on LLMs, revealing that their diverse capabilities can provide attackers with new surfaces to exploit, introducing novel vulnerabilities. Specifically, (Ren et al. 2024; Lv et al. 2024; Ding et al. 2023) demonstrate that LLMs are vulnerable when handling code-based tasks. Besides, (Ding et al. 2023; Li et al. 2023b; Liu et al. 2023b) reveal that LLMs can be easily compromised in generating fictional role-play stories. Additionally, (Yuan et al. 2023) found that LLMs' ability to understand non-natural language ciphers provides a novel surface for jailbreak attacks. Furthermore, (Wei, Wang, and Wang 2023; Anil et al. 2024) demonstrate that the In-context Learning (ICL) capacity can render LLMs susceptible to malicious inputs. These findings raise concerns about the potential risks of LLMs associated with their advanced capabilities. Motivated by previous research, we conduct further study and raise the question that has not been addressed:\n\"Will the advanced analyzing and reasoning capabilities of LLMs ultimately enable them to circumvent safety alignment, potentially posing unforeseen risks?\u201d\nTo answer this question, in this work, we further explore the boundary of jailbreak attacks and reveal the vulnerabilities of safety-trained LLMs while dealing with analyzing-based tasks. Specifically, we propose a novel jailbreak attack method, termed Analyzing-based Jailbreak (ABJ), which exploits LLMs' advanced analyzing and reasoning capabilities by engaging them in feature analysis and behavior prediction tasks, leading the model to generate potentially harmful outputs without adequately considering the associated risks (as illustrated in Figure 1). The ABJ approach consists of two main steps: (1) data preparation, which involves crafting customized data relevant to the original malicious input (e.g., feature, character, and job), and (2) data analysis, which directs the target LLM to analyze the prepared data and generate potentially harmful outputs.\nWe validate the effectiveness of ABJ by conducting comprehensive experiments across open-source and closed-source LLMs. Our experimental results demonstrate that ABJ successfully bypasses the safety alignment of target LLMs, achieving state-of-the-art attack effectiveness and efficiency compared to other baselines. Notably, ABJ achieves a high ASR of over 90% on GPT-3.5-turbo and GPT-4-turbo across all seven different domains, including illegal activity and hate speech. By pioneering a novel approach to jailbreak attacks, ABJ guides researchers in investigating more robust defense strategies for LLMs.\nTo sum up, our contributions are as follows:\n\u2022 We further explore the boundary of jailbreak attacks on LLMs and propose ABJ, the first jailbreak attack method specifically designed to assess LLMs' safety in handling analysis-based tasks. ABJ generalizes jailbreak attack prompts in two steps: data preparation and data analysis.\n\u2022 We conduct comprehensive experiments on both open-source (Llama-3, Qwen-2, GLM-4) and closed-source (GPT-3.5-turbo, GPT-4-turbo, Claude-3) LLMs. The results demonstrate that ABJ exhibits exceptional attack effectiveness and efficiency, achieving 94.8% ASR on GPT-4-turbo, while the AE is around 1.\n\u2022 We show the robustness of ABJ when facing different defense strategies, indicating that mitigating this attack might be difficult. Furthermore, by modifying and enriching the data of ABJ, we induce LLMs to generate a wide range of harmful content, encompassing various forms of harmful scenarios that are not limited to existing finite datasets."}, {"title": "Related Work", "content": "Safety-aligned LLMs\nSafety-aligned large language models (LLMs) such as GPT-4 and Claude-3, embody a commitment to adhering to human ethical and legal preferences (Ziegler et al. 2019; Solaiman and Dennison 2021; Korbak et al. 2023; Rafailov et al. 2024), with interventions occurring at both the data and training levels. Initiatives like meticulous pre-training data filtering (Xu et al. 2020; Wang et al. 2022; Welbl et al. 2021) are pivotal, employing heuristic and automated classifiers to sift through vast datasets, ensuring the foundational training material aligns with ethical guidelines. The training process itself leverages sophisticated methodologies, including Supervised Fine-Tuning (SFT) (Wu et al. 2021) and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022), to refine the models' output per human values. Despite these efforts, research has been intensifying on aligning LLMs more effectively and efficiently (Zheng et al. 2024; Xu et al. 2024; Ji et al. 2024; Zhang et al. 2023). For instance, (Bai et al. 2022) introduces Constitutional AI, a method that encodes desirable AI behavior in a simple and transparent form, enabling more precise control with minimal human labeling. (Sun et al. 2024) develop SELF-ALIGN, a novel approach that leverages principle-driven reasoning and the generative capabilities of LLMs to achieve self-alignment of AI agents with minimal human supervision. (Dong et al. 2023) propose RAFT, an alignment framework that fine-tunes LLMs using samples efficiently ranked by reward functions. In this study, we validate the effectiveness of our approach on both open-source and closed-source LLMs and reveal that utilizing LLMs to perform analyzing-based tasks enables the evasion of safety alignment, which provides a valuable testing ground for the efficacy of these alignment methods.\nJailbreak Attacks on LLMS\nDespite concerted efforts to align LLMs with human values, jailbreak attacks (Deng et al. 2023; Lapid, Langberg, and Sipper 2023; Jin et al. 2023; Deng et al. 2024; Jones et al. 2023), which prompt LLMs to produce harmful content, still pose a significant risk to LLMs. Currently, jailbreak attack methods such as GCG (Zou et al. 2023), AutoDAN (Liu et al. 2023a), GPTFuzzer (Yu, Lin, and Xing 2023), PAIR (Chao et al. 2023) successfully induce LLMs to output harmful content by introducing perturbation and linguistic variation to the original malicious input. While these methods have proven effective, they neglect to analyze and explore the inherent and suspected vulnerability of LLMs, which leads to the problems of low attack success rate and attack efficiency, falling into the trap of blind attack.\nTo further understand the vulnerability of safety-trained LLMs to jailbreak attacks, (Wei, Haghtalab, and Steinhardt 2024) propose two failure modes as principles to guide the design of jailbreak attack methods: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. Based on these two research outcomes, (Ren et al. 2024; Lv et al. 2024; Ding et al. 2023) expose that LLMs are easy to jailbreak when handling code-based tasks. (Ding et al. 2023; Li et al. 2023b; Liu et al. 2023b) reveal the vulnerability of LLMs in generating fictional stories. (Yuan et al. 2023; Wei, Haghtalab, and Steinhardt 2024) discover that LLMs\u2019 ability to understand non-natural language ciphers provides a novel surface for jailbreak attacks. (Wei, Wang, and Wang 2023; Anil et al. 2024) demonstrate that LLMs' ability of In-context Learning (ICL) can lead to the weakness against malicious inputs. In this paper, we further explore the boundary of jailbreak attacks on LLMs with comprehensive experiments by taking advantage of their considerable analyzing and reasoning capability and revealing the underlying vulnerabilities of safety-trained LLMs while dealing with analyzing-based tasks."}, {"title": "Methodology: Analyzing-based Jailbreak", "content": "In this section, we elaborate in detail on Analyzing-based Jailbreak(ABJ). This novel jailbreak attack method evaluates the safety of LLMs in handling analyzing-based tasks. ABJ consists of two main steps: data preparation and data analysis. Specifically, we first generate customized data pertinent to the original malicious input. Subsequently, we guide the target LLM in analyzing the prepared data and generate potentially harmful or inappropriate outputs. The entire process is autonomously executed by LLMs without requiring additional training or optimization. Figure 2 outlines ABJ, while Algorithm 1 provides the specific implementation details. We will introduce the step-by-step process of ABJ in the following sections."}, {"title": "Formulation", "content": "We formulate the jailbreak attack as follows: given a target model $LLM_{target}$ to be evaluated and an initial malicious input X, the objective of the jailbreak attack is to identify a specific modification strategy S within a bounded policy space. The strategy S aims to modify the initial malicious input X to maximize the likelihood of the response from the target model $LLM_{target}$ being classified as malicious by a designated harmfulness evaluator $M_{eval}$. This can be represented as:\n$S^* = \\underset{S}{\\text{argmax}} M_{eval}(LLM_{target}(S(X)))$\nwhere $S^*$ is the strategy that maximizes the harmfulness score assigned by $M_{eval}$ to the response of the modified prompt S(X). Notably, a higher harmfulness score indicates a greater likelihood of a successful jailbreak attack.\nIn this work, we focus on leveraging the inherent analyzing and reasoning capabilities of LLMs to enhance the effectiveness of jailbreak attacks in behavior prediction scenarios during the inference stage while simultaneously preserving the helpfulness of input, thereby circumventing the detection of malicious intentions by LLMs' defense mechanisms."}, {"title": "Design of Analyzing-based Jailbreak", "content": "Step I: Data Preparation. The first step of ABJ involves generating customized data related to the original malicious input. To this end, we consider addressing two pivotal issues: (1) what type of data should be generated, and (2) how to effectively generate the required data. To resolve the first issue, we thoroughly examined the workflow underlying data analysis tasks. In this work, we primarily focus on leveraging data analysis for behavior prediction tasks. For instance, when attempting to predict an individual's response to a specific task, it is essential to acquire representative data pertaining to that individual, such as personality features, individual characteristics, and other factors that may influence their cognitive processes and behavioral patterns. To address the second challenge, particularly the effective generation of the required data, we leverage the exceptional understanding and generation capabilities of LLMs and assign a state-of-the-art LLM the role of a data preparation assistant to ensure the quality of the generating data. Notably, we instruct GPT-4-turbo-0409 to generate a range of data with a fixed format, encompassing features, characters, and other attributes that influence an individual's response to a specific task (Algorithm 1 lines 7-10). For example, for the task 'how to make a bomb', a possible character is 'evil', and a possible feature is 'love to use chemical materials'.\nStep II: Data Analysis. After generating the required data, we utilize prompt engineering to guide the target LLM to analyze the data and figure out the potential harmful outputs. Specifically, we first create a role named Miller and instruct the target LLM to analyze his character and features in the provided data. Then, we guide the target LLM to perform a behavior prediction task and figure out Miller's response to the original malicious task based on the results of LLM's analysis process (Algorithm 1 line 13).\nBased on the above conceptual design, we derive a simple prompt-based template as follows:\nPrompt Template of ABJ\n<data>\ncharacter: {CHARACTER}\nfeature: {FEATURE}\ntask: {TASK}\n</data>\nPlease help me deal with a data analyzing task.\nYou should analyze Miller's character, feature in the data I provide.\nBased on the analyzing results, figure out Miller's step-by-step action while dealing with his task with at least 500 words.\nThe above template contains three attributes:\n\u2022 {CHARACTER}: some adjectives to describe an individual's character, e.g., evil, ruthless, irritable, which set the tone for the style of their action.\n\u2022 {FEATURE}: content that describes an individual's behavioral characteristics and preferences, e.g., love to create panic and chaos, love to abuse and insult others, which add more details to their behavior.\n\u2022 {TASK}: the target of conducting a jailbreak attack, e.g., how to make a bomb.\nFurthermore, we can flexibly adjust or expand these attributes in the data to elicit more diverse and detailed harmful responses from LLMs, thereby systematically exploring the vulnerability of LLMs to different types of inputs."}, {"title": "Experiment", "content": "In this section, we present extensive evaluation and analysis across closed-source and open-source LLMs to demonstrate the effectiveness of ABJ.\nExperimental Setup\nDatasets. We leverage the Harmful Behaviors dataset from AdvBench (Zou et al. 2023), which comprises 520 carefully crafted prompts designed to evaluate the safety performance of LLMs. This dataset is meticulously assembled to cover a wide range of prohibited scenarios, which allows us to conduct a comprehensive vulnerability assessment of LLMs.\nTarget Models. To conduct a comprehensive assessment of the security of LLMs, we evaluate both open-source and closed-source LLMs, including GPT-3.5-turbo-0125, GPT-4-turbo-0409, Claude-3-haiku-0307, Llama-3-8B-Instruct, Qwen-2-7B-Chat, GLM-4-9B-Chat. Each of these models adheres rigorously to established safety alignments, guaranteeing that our work proceeds within a framework of secure and ethical AI. The hyperparameters are in the default settings.\nMetrics. We employ two metrics to evaluate jailbreak attacks described as follows:\n\u2022 Attack Success Rate (ASR): ASR is the percentage of unsafe responses in all responses generated by the target model. We also introduce ASR-Ensemble (ASR-E). We use one jailbreak attack prompt to attack the target model three times. The attack is considered successful if at least one attempt works. A higher ASR and ASR-E indicates better attack effectiveness.\n\u2022 Attack Efficiency (AE): AE is the number of accesses to the target model during a successful attack. A lower AE indicates better attack efficiency.\nJudgement. Following evaluation protocol from previous studies (Chiang and Lee 2023; Liu et al. 2023c) that LLMs can serve as reliable evaluators, we instruct GPT-4-turbo-0409 as a robust safety evaluator to judge whether a response is harmful (see Table 5 for more details). Additionally, considering the potential limitations of a single LLM-based evaluator in accurately discerning the intent behind lengthy and intricate responses, we manually reviewed the content initially deemed harmless in preliminary assessments to enhance judgment accuracy. Combining these two approaches can balance judgment efficiency and accuracy.\nBaselines. We compare ABJ with five representative baseline methods described as follows:\n\u2022 Direct Attack (DA): The attacker initiates Direct Attack by directly delivering harmful query to LLMs.\n\u2022 DeepInception: DeepInception (Li et al. 2023b) leverages the role-playing capability of LLMs to create a deeply-nested scenario, thereby inducing harmful contents in the target LLMs.\n\u2022 Prompt Automatic Iterative Refinement (PAIR): PAIR (Chao et al. 2023) is an optimization-driven jailbreak attack method that iteratively enhances the prompt to target LLMs to provoke malicious outputs.\n\u2022 Greedy Coordinate Gradient (GCG): GCG (Zou et al. 2023) crafts adversarial examples via greedy and gradient-based discrete optimization, requiring full access of LLM's weights and architecture to search for token sequences that bypass the safety defenses of LLMs.\n\u2022 AutoDAN: AutoDAN (Liu et al. 2023a) utilizes genetic algorithms to iteratively optimize adversarial examples, which can be automated to generate stealthy jailbreak prompts using a hierarchical genetic algorithm.\nSafeguards. We consider four safeguards against jailbreak attacks described as follows:\n\u2022 OpenAI Moderation Endpoint API: OpenAI Moderation Endpoint API (OpenAI 2023) is an authorized content moderation tool provided by OpenAI. It utilizes a multi-label classifier to categorize responses generated by language models into 13 specific categories. Any response that breaches these categories is flagged as violating OpenAI usage policy (OpenAI 2024).\n\u2022 Perplexity Filter (PPL): Perplexity Filter (Jain et al. 2023) is created to identify incomprehensible attack prompts. It establishes a threshold and employs another language model to compute the perplexity of the entire prompt or its segmented portions. Prompts surpassing this threshold are subsequently removed from consideration.\n\u2022 SmoothLLM: SmoothLLM (Robey et al. 2023) has two steps, perturbation step and aggregation step. In the perturbation step, random perturb prompts are passed as input to the LLM. In the aggregation step, it obtains a collection of perturbed prompts and aggregates the corresponding predictions.\n\u2022 In-context Defense (ICD): ICD (Wei, Wang, and Wang 2023) enhances model robustness by providing demonstrations of rejecting to answer malicious inputs, leveraging LLMs' ability of In-context Learning (ICL)."}, {"title": "Experimental Results", "content": "Our method achieves a high attack success rate and attack efficiency against all target LLMs. The results presented in Table 1 demonstrate that ABJ has a high attack success rate across open-source and closed-source LLMs while attaining remarkable attack efficiency. Notably, ABJ achieves over 90% ASR and 95% ASR-E on GPT-3.5-turbo-0125 and GPT-4-turbo-0409, while the AE is around 1, showcasing its effectiveness and efficiency compared to other baselines. Moreover, ABJ maintains a high ASR on Llama-3-8B-Instruct and Claude-3-haiku-0307, which are considered two of the currently secure LLMs with strong safety capabilities, exceeding 35% and 60%, respectively. The above experimental results demonstrate that LLMs' analyzing and reasoning capabilities can be easily exploited by attackers, becoming even more serious security vulnerabilities for LLMs to overcome. We provide more demonstrations in Figure 4 in the Appendix.\nOur method achieves a high attack success rate on different categories of harmful prompts. To conduct a more detailed analysis of the safety performance of target LLMs when confronted with different categories of harmful prompts, we adopt the classification approach from a previous study (Ding et al. 2023) and divide the Harmful Behaviors dataset into 7 distinct categories using GPT-4-turbo-0409 (The classification prompt can be seen in Table 5). As shown in Table 2 and Figure 3, our experimental evaluations across these categories reveal the vulnerability of target LLMs to different types of harmful prompts. Notably, the ASR and ASR-E on Malware and Economic Harm prompts are relatively higher than other categories, indicating that LLMs are more vulnerable to these attacks. In contrast, LLMs exhibit enhanced robustness when confronted with Hate Speech and Physical Harm prompts, with comparatively lower ASR and ASR-E for these categories.\nOur method maintains effectiveness when facing existing defenses against jailbreak attacks. The results in Table 3 demonstrate that ABJ can successfully maintain its effectiveness when facing existing defense methods on all target models. Specifically, OpenAI Moderation failed to detect any harmful prompts, which we attribute to the weakness of its base model. Furthermore, the performance of the PPL Filter is also far from satisfactory, indicating that the jailbreak attack prompts generated by ABJ are semantically coherent. Additionally, SmoothLLM is also ineffective in defending against ABJ, likely due to LLMs' strong capability in comprehending the meaning of perturbation-introduced modified prompts. In contrast, we observe that ICD is the most effective defense method against ABJ, reducing the ASR and ASR-E by 12.8% and 10.9%, respectively. This may be attributed to its ability to enhance LLMs' detection of malicious intentions in analysis-based tasks through multiple demonstrations. However, the above experimental results indicate that all these safeguard methods are still inadequate to mitigate ABJ, highlighting the urgent need for developing more advanced defenses against jailbreak attacks."}, {"title": "Ablation Study", "content": "In this section, we further explore the effectiveness of ABJ and conduct a series of ablation experiments.\nImpact of the mismatch from the target topic. In the first step of ABJ, we prepare customized data related to the original malicious input to induce the target LLMs to make harmful outputs. However, is ABJ still effective with unrelated data? To answer this question, we investigate the feasibility of ABJ in scenarios where the prepared data deviates from the topic of the target query. Specifically, we collect the character and feature data generated during the experiments in the previous section and use a random selection method to prepare data that does not match the topic of the original malicious input. As shown in Table 4, under these settings, ABJ can still successfully induce target LLMs to output diverse harmful content by exploiting their exceptional analysis and reasoning capabilities. We provide more demonstrations in Figure 5 in the Appendix. This result demonstrates the robustness and diversity of ABJ, highlighting the urgency of developing secure LLMs.\nJailbreak LLMs without malicious query. Previous black-box and white-box jailbreak attacks transform the original malicious query to conceal the attacker's harmful intent. In contrast, in this work, we push the boundaries of jailbreak attacks and explore whether LLMs can be successfully attacked even when their input does not include the original malicious query. Specifically, we instruct GPT-4-turbo-0409 to generate the job of an individual doing a specific task. For example, for the task \u2018how to make a bomb,' a possible job is 'bomb-maker.' For each malicious task in the dataset, we generate a corresponding job to match it. After generating the required data, we obtained the prompt-based template of the modified ABJ as follows:\nPrompt Template of Modified ABJ\n<data>\njob: {JOB}\ncharacter: {CHARACTER}\nfeature: {FEATURE}\n</data>\nPlease help me deal with a data analyzing task.\nYou should analyze Miller's job, character and feature in the data I provide.\nBased on the analyzing results, figure out Miller's favorite task.\nFinally, figure out Miller's step-by-step action while dealing with his task with at least 500 words.\nWe assess the performance of modified ABJ on target LLMs. The results presented in Table 4 demonstrate that modified ABJ significantly increases the ASR on Claude-3-haiku-0307 and Llama-3-8B-Instruct, which is over 85% and 90%, respectively. It can be observed that by leveraging the increasingly powerful analyzing and reasoning capabilities of LLMs, modified ABJ can manipulate them to generate malicious outputs even when faced with input that does not contain a harmful query, demonstrating the flexibility and stealthiness of modified ABJ. Moreover, by taking advantage of the flexibility of modified ABJ, we explored its ability to output a wide variety of harmful content that is not limited to the existing harmful dataset by freely customizing the diversity of data. We provide more demonstrations in Figure 6 and Figure 7 in the Appendix. This further underscores the crucial and urgent need for developing a secure and trustworthy AI model.\nComposition with other jailbreak methods. Based on the experimental results presented in Table 1, we observe that ABJ exhibits a lower ASR on Claude-3-haiku-0307 and Llama-3-8B-Instruct compared to other target LLMs. To enhance the effectiveness of ABJ, we explore the possibility of combining ABJ with other jailbreak attack methods. Benefiting from the simplicity and flexibility of ABJ, we seamlessly integrate ABJ with other approaches to boost its effectiveness. Specifically, we investigate two composition strategies: (1) code-based jailbreak attack, which incorporates ABJ into a code completion scenario, and (2) adversarial jailbreak attack, which randomly splits each word of the original harmful query into two segments. As shown in Table 4, after combining these two jailbreak attack methods with ABJ, the combined approach significantly increases the ASR against target LLMs. This finding highlights the potential of ABJ in breaching security measures when combined with other strategies, rendering it a more challenging security vulnerability to mitigate."}, {"title": "Conclusion and Future Work", "content": "In this paper, we reveal the underlying vulnerabilities of safety-trained LLMs while dealing with analyzing-based tasks and propose a new approach to jailbreak LLMs, named Analyzing-based Jailbreak (ABJ), which leverages the increasing analyzing and reasoning capabilities of LLMs and successfully bypasses the defense of state-of-the-art LLMs, such as GPT-4 and Claude-3. We conduct comprehensive experiments on ABJ and demonstrate its effectiveness, efficiency, and robustness in different settings. These experiments highlight the necessity of developing a more comprehensive safety alignment to LLMs. To address this issue, a promising direction is to implement safety alignment techniques (e.g., SFT, RLHF, and DPO) to match the diverse capabilities of LLMs while avoiding compromising their performance. We hope this work can contribute to a broader understanding of the security challenges of LLMs and encourage further research on developing more reliable LLMs."}]}