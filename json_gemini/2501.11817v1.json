{"title": "Toward Effective Digraph Representation Learning: A Magnetic Adaptive Propagation based Approach", "authors": ["Xunkai Li", "Daohan Su", "Zhengyu Wu", "Guang Zeng", "Hongchao Qin", "Rong-Hua Li", "Guoren Wang"], "abstract": "The q-parameterized magnetic Laplacian serves as the foundation of directed graph (digraph) convolution, enabling this kind of digraph neural network (MagDG) to encode node features and structural insights by complex-domain message passing. As a generalization of undirected methods, MagDG shows superior capability in modeling intricate web-scale topology. Despite the great success achieved by existing MagDGs, limitations still exist: (1) Hand-crafted q: The performance of MagDGs depends on selecting an appropriate q-parameter to construct suitable graph propagation equations in the complex domain. This parameter tuning, driven by downstream tasks, limits model flexibility and significantly increases manual effort. (2) Coarse Message Passing: Most approaches treat all nodes with the same complex-domain propagation and aggregation rules, neglecting their unique digraph contexts. This oversight results in sub-optimal performance. To address the above issues, we propose two key techniques: (1) MAP is crafted to be a plug-and-play complex-domain propagation optimization strategy in the context of digraph learning, enabling seamless integration into any MagDG to improve predictions while enjoying high running efficiency. (2) MAP++ is a new digraph learning framework, further incorporating a learnable mechanism to achieve adaptively edge-wise propagation and node-wise aggregation in the complex domain for better performance. Extensive experiments on 12 datasets demonstrate that MAP enjoys flexibility for it can be incorporated with any MagDG, and scalability as it can deal with web-scale digraphs. MAP++ achieves SOTA predictive performance on 4 different downstream tasks.", "sections": [{"title": "1 Introduction", "content": "As high-order structured data, the directed graph (digraph) offers a new perspective to model intricate web-scale information by capturing node relationships. Its exceptional representational capacity at the data level has driven advancements in graph mining at the model level, drawing significant attention in recent years [53, 57]. Notably, although existing undirected GNNs can achieve satisfactory performance, the loss of directed information undeniably limits their potential, especially when addressing topological heterophily challenges (i.e., whether connected nodes have similar features or same labels) [45, 54, 59]. Therefore, researchers have increasingly focused on utilizing digraphs for modeling complex web scenarios, including recommendation [64, 72] and social networks [2, 55]. Based on this, web mining problems can be translated into node- [35, 61, 70], link- [29, 44, 71], and graph-level [37, 42, 60] tasks.\nTo achieve effective digraph learning, a promising approach is q-parameterized magnetic Laplacian $L_m$, which forms the foundation of digraph convolution from a spectral perspective to simultaneously encode node features and structural insights by message passing in the complex domain. Specifically, it is an adaptation of the standard Laplacian by incorporating complex-valued weights to account for the influence of a magnetic field on edges, which is particularly beneficial for investigating network properties when the edges are formulated as the asymmetry topology (e.g., digraphs) [5, 8]. Notably, the weights of $L_m$ denoted as $\\exp(i\\Theta_{uv}^q)$, where $\\Theta_{uv}^q$ represents the magnetic potential or phase linked to the directed edge $e_{uv}$ and q determines the strength of direction, reflecting the integration of the magnetic vector potential along the edge from node u to v. Intuitively, it can also be viewed as the spatial phase angle between connected nodes in the complex domain, describing the direction and granularity of spatial message passing.\nBuilding upon this concept, digraph neural networks based on the q-parameterized magnetic Laplacian (MagDGs) implicitly execute eigen-decomposition during convolution [20, 35, 39, 67, 70, 75]. This approach captures crucial structural insights (i.e., key properties of the digraph, such as connectivity) under the influence of the magnetic field, guiding optimal node encoding principles within the directed topology. Despite recent remarkable efforts in designing MagDGs, inherent limitations still exist:\n(1) Limited Understanding of q-parameterized Magnetic Laplacian in Digraph Learning. Intuitively, q determines the strength of direction for each edge in the digraph, manifested in the spatial phase angle between every connected node in the complex domain. For its direct impact on propagation and message (i.e., propagated results) aggregation, selecting an appropriate q is crucial. However, related studies have primarily concentrated on spectral graph theory, providing guidance on q selection from a strictly topological perspective and evaluating these principles in graph signal processing [14], community detection [12], and clustering [11]. Despite their effectiveness, directly applying these methods in digraph"}, {"title": "2 Preliminaries", "content": "We consider a digraph $G = (V, E)$ with $|V| = n$ nodes, $|E| = m$ edges. Each node has a feature vector of size $f$ and a one-hot label of size $c$, the feature and label matrix are represented as $X \\in R^{n \\times f}$ and $Y \\in R^{n \\times c}$. $G$ can be described by an asymmetrical adjacency matrix $A(u, v)$. Downstream tasks include node-level and link-level.\nNode-level Classification. Suppose $V_l$ is the labeled set, the semi-supervised node classification paradigm aims to predict the labels for nodes in the unlabeled set $V_u$ with the supervision of $V_l$.\nLink-level Prediction. (1) Existence: predict if $(u, v) \\in E$ exists in the edge sets; (2) Direction: predict the edge direction of pairs of nodes u, v for which either $(u, v) \\in E$ or $(v, u) \\in E$; (3) Three-class link classification: classify an edge $(u, v) \\in E$, $(v, u) \\in E$, or $(u, v), (v, u) \\notin E$. For convenience, we call it Link-C.\nData-centric Plug-and-play MAP: This approach encodes spatial phase angles in a weight-free manner by considering the characteristics of digraph data from both topological and feature perspectives. It optimizes existing MagDGs by replacing their predefined rigid graph propagation equations (i.e., Hand-crafted q).\nModel-centric MAP++: Building on MAP, this method introduces additional learnable parameters to enable adaptive edge-wise graph propagation and node-wise message aggregation. The learnable modules from the above two perspectives can be selectively applied based on the computational capabilities, offering flexibility."}, {"title": "2.2 Directed Graph Neural Networks", "content": "Prevalent Message Passing. In undirected scenarios, prevalent approaches [13, 19, 26, 36, 63, 66] adhere to strict symmetric message passing. This strategy entails the design of graph Propagation and the subsequent message Aggregation, facilitating the establishment of relationships among a node and its neighbors. For the current node u, the l-th W-parameterized aggregator is denoted as:\n$H_u^{(l)} = Agg(W^{(l)}, Prop(H_v^{(l-1)}, \\forall v \\in N(u)))$, (1)\nwhere $H^{(0)} = X$, $N(u)$ denotes the one-hop neighbors of u. To obtain node embeddings in digraphs, it's crucial to consider the direction of edges. Hence, the current node u initially employs learnable weights separably for its out-neighbors $(u \\rightarrow v)$ and in-neighbors $(v \\rightarrow u)$ to obtain multi-level aggregated representations followed by the Combination after directed message passing:\n$H_{out}^{(l)} = Agg(W_{out}^{(l)}, Prop(H_v^{(l-1)}, \\forall (u,v) \\in E))$,\n$H_{in}^{(l)} = Agg(W_{in}^{(l)}, Prop(H_v^{(l-1)}, \\forall (v, u) \\in E))$, (2)\n$H_u^{(l)} = Comb(W_c^{(l)}, H_u^{(l-1)}, H_{out}^{(l)}, H_{in}^{(l)})$.\nBuilding upon this concept, DGCN [62] and DiGCN [61] incorporate neighbor proximity to increase the receptive field (RF) of each node. DIMPA [21] increases the node RF by aggregating more neighbors during the graph propagation. NSTE [30] is motivated by the 1-WL graph isomorphism test to design the message aggregation. ADPA [59] explores appropriate directed patterns to conduct graph propagation. Despite their effectiveness, these methods inevitably introduce additional trainable weights and heavily rely on well-designed neural architectures that hinder their deployment."}, {"title": "The q-parameterized magnetic Laplacian driven MagDGs.", "content": "To address these issues, recent studies employ the q-parameterized magnetic Laplacian to define complex-domain message passing, explicitly modeling both the presence and direction of edges through real and imaginary components. Specifically, magnetic Laplacian is a complex-valued Hermitian matrix that encodes the asymmetric nature of a digraph via the q-parameterized complex part of its entries. This introduces a complex phase, influenced by a magnetic field, to the edge weights, extending the conventional graph Laplacian into the complex domain to more effectively capture asymmetry. The above q-parameterized magnetic Laplacian is formally defined as:\n$A_m(u, v) := \\frac{1}{2} (A(u, v) + A(v, u))$,\n$\\Theta^{(q)}(u, v) := 2 \\pi q (A(u, v) - A(v, u)), q \\geq 0$,\n$L_m = D_m - A_m \\exp(i \\Theta^{(q)})$, (3)\nwhere $D_m$ is the degree matrix of $A_m$, q determines the strength of direction. The real part in $L_m$(u, v) indicates the presence and the imaginary part indicates the direction. Since we only consider unsigned digraphs, there exists $cos \\Theta^{(q)} \\geq 0$. Moreover, due to the periodicity of the $sin \\Theta^{(q)}$, $\\Theta^{(q)} \\in [-\\pi/2, \\pi/2]$, we have $q\\in [0, 1/4]$. When setting q = 0, directed information becomes negligible. For q = 1/4, we have $A_m^{(q)}(u, v) = -A_m^{(q)}(v, u)$ whenever there is an edge from u to v only. Based on this, we can formally define the magnetic graph operator (MGO) with self-loop ($A_m = A_m + I$) to form the foundation of digraph convolution as follows:\nMGO := $\\hat{A_m} = (D_m^{-1/2}A_mD_m^{-1/2} \\odot \\exp(i\\Theta^{(q)}))$. (4)\nThis MGO enables graph propagation in the complex domain, elegantly encoding deep structural insights concealed in digraphs with asymmetric topology. Subsequently, we can instantiate the trainable message aggregation based on the propagated results. The above $W_c$-parameterized complex-domain message passing (proposed by MagNet [70]) can be formally defined as:\n$\\tilde{H}^{(l-1)} = Complex(H^{(l-1)}) := {Real(H^{(l-1)}), Imag(H^{(l-1)})}$,\n$H^{(l)}_u = Agg(W_c^{(l)}, Prop(\\tilde{H}^{(l-1)}, {\\tilde{H}_v^{(l-1)}, \\forall (u, v), (v, u) \\in E}))$. (5)\nBased on this foundation, MSGNN [20] extends this complex domain pipeline to directed signed graphs by varying the range of q. MGC [67] adopts a truncated version of PageRank named Linear-Rank to construct a filter bank to improve the graph propagation. Framelet-Mag [39] employs Framelet-based filtering to decompose the magnetic Laplacian into components of different scales and frequencies for better predictive performance. LightDiC [35] optimizes the MagDG framework by decoupling graph propagation and message aggregation for scalability in large-scale scenarios."}, {"title": "3 Empirical Investigation", "content": "As mentioned in Sec. 1 and Sec. 2.2, despite the remarkable efforts of existing MagDGs in improving complex-domain graph propagation, two limitations still exist. To address them, we provide comprehensive empirical analysis in terms of: (1) Illustrations: We clarify the node semantic context driven by directed topology and visualize the naive graph propagation in existing MagDGs."}, {"title": "4 Magnetic Adaptive Propagation", "content": "Motivated by the above key insights, in this paper, we propose two technologies: MAP and MAP++, offering a plug-and-play solution for existing MagDGs and a new MagDG framework, respectively. The core of our methods is the thorough integration of directed topology and node features, aimed at circulating the most appropriate magnetic field potential to directed edges. In other words, we strive to ensure the quality of complex domain message passing by adaptive edge-wise graph propagation and node-wise message aggregation. Specifically, MAP first identifies the topological context of directed edges by quantifying the comprehensive centrality of start and end nodes, highlighting the direction of frequently activated edges (motivated by Key insight 1). Subsequently, MAP quantifies the correlation between connected nodes in a weight-free manner throughout the edge projection in the complex plane. This process highlights the direction of edges linked by dissimilar nodes (motivated by Key insight 2). Building upon this foundation, MAP++ further introduces a learnable mechanism to achieve adaptive spatial phase angle encoding and weighted message aggregation to improve performance. The complete algorithm description and complexity analysis can be found in Appendix A.1."}, {"title": "4.1 Topology-related Uncertainty Encoding", "content": "Drawing from the empirical study, we conclude that frequently activated directed edges generate intricate information flows that compromise the uniqueness of node representations. Based on this, we provide a more generalized and thorough perspective: these intricate information flows driven by frequently activated directed edges introduce additional topological uncertainty to node representations, significantly disturbing their prediction, as evidenced by Fig. 1 (d).\nAs Key insight 1 highlighted, the directed information introduced by increased q can be construed as supplementary encoding of topological uncertainty, thereby regulating graph propagation to avoid node confusion. In other words, this directed information enhances the capacity to discern complex information flows, enabling fine-grained graph propagation, and thereby improving node discrimination. Consequently, we aim to understand this topology-related uncertainty. It first identifies frequently activated directed edges through connected nodes and then applies fine-grained encoding to their magnetic field potentials for personalized propagation.\nIn a highly connected digraph, nodes frequently interact with their neighbors. By employing random walks [50], we can capture these interactions and introduce Shannon entropy to measure node centrality [34] from a global perspective. Meanwhile, by adopting cluster connectivity, we can further offer a description of node centrality from a local perspective, which closely correlates with neighbor connectivity. The above processes are defined as:\nGlobal := $GC(v) = \\frac{d_{in}}{m}log\\frac{d_{in}}{m} + \\frac{d_{out}}{m} log\\frac{d_{out}}{m}$,\nLocal := $LC(v) = \\frac{m_v}{d_{in} d_{out}}$, (6)\nwhere $d_{in}$ and $d_{out}$ are the in and out-degrees in the digraph. $m_v$ is the triple motifs of node v. Notably, in contrast to directed structural entropy defined by the previous work [34], we address the limitation of only walking in the forward direction by incorporating reverse walking. This modification is motivated by the non-strongly connected nature of most digraphs, where the proportion of complete walk paths declines sharply. This decline suggests that most"}, {"title": "4.2 Feature-related Correlation Encoding", "content": "At this point, we have achieved topology-related uncertainly encoding for frequently activated directed edges. However, node features equally play a pivotal role in digraph learning. Therefore, we aim to fully leverage the correlation of connected nodes to further fine-tune the magnetic field potentials on directed edges. Motivated by Key insight 2, we conclude the following principles: (1) A smaller q for connected nodes with high feature similarity, disregarding directed information to mitigate knowledge dilution. (2) A larger q for connected nodes with low feature similarity, emphasizing directed information to enhance knowledge discernibly. These principles enable the current node to acquire more beneficial knowledge.\nAccording to Eq. (3), the complex plane is established by the q-parameterized magnetic Laplacian. Each directed edge is depicted as a vector within this complex plane and its projection on the x-axis(real part) is edge existence, while the projection on the y-axis(imaginary part) is edge direction. For connected nodes u and v, dissimilar features lead to a larger q with a greater angle for euv, indicating shorter projection along the x-axis and longer projection along the y-axis. This emphasizes euv direction during graph propagation and aligns with the previously mentioned principles. Consequently, we can directly leverage the correlation of features between connected nodes to encode the magnetic field potential of the corresponding directed edge, where node embeddings Z are obtained from W-parameterized backbone MagDG. The above process can be formally defined as:\n$q_{uv}^{feat} = Norm \\big(a * arccos (\\frac{Z_uZ_v}{||Z_u || * ||Z_v||})\\big)$, Norm(x) = $\\frac{2x}{\\pi}$. (8)"}, {"title": "4.3 MAP Framework", "content": "Now, we have achieved fine-grained magnetic field potential encoding for directed edges, considering both topological and feature perspectives. This is reflected in the adaptive spatial phase angles of connected nodes in the complex domain. To pursue scalability, we"}, {"title": "4.4 MAP++ Framework", "content": "Despite the progress made by MAP, the weight-free method often encounters limited improvement. Furthermore, most MagDGs directly stack linear layers to implement message passing, resulting in strict dependencies between the current and the previous layer. This coupled architecture can only support shallow MagDGs with limited RFs and toy-size datasets, as deeper ones would suffer from the over-smoothing problem, out-of-memory (OOM) error, and out-of-time (OOT) error, especially in web-scale sparse digraphs. To break the above limitations, we propose MAP++ as follows:\nStep 1: Edge-wise Graph Propagation. Based on the MAP, we first utilize a lightweight neural architecture Edge-Mag(\u00b7) parameterized by Wedge to further encode magnetic field potentials for each directed edge. In this strategy, we aim to enable iterative optimization through the training, which is formally defined as:\n$^*q = q^o Edge-Mag (Norm (GC_{u+v} q^{feat}||LC_{u+v} q^{feat}))$, (10)\nwhere  denotes the element-wise matrix multiplication. Notably, this approach is only for small- and medium-scale datasets due to scalability. To increase the RF of nodes, we conduct K-step complex-domain graph propagation, correspondingly getting a list of propagated features (i.e., messages) under different steps as follows:\n$X^{(K)} = \\hat{A}^{*K} X^{(0)} \\rightarrow [\\sum^{(0)}, \\sum^{(1)},..., \\sum^{(K)}], \\sum^{(0)} = X$. (11)\nDue to the learnable $\\hat{A}^K$, gradients flow towards propagated features. Thus far, we have achieved edge-wise graph propagation by integrating adaptive magnetic field potential during training.\nStep 2: Node-wise Message Aggregation. Recent studies [13, 58, 68] have highlighted that the optimal RF varies for each node, influenced by the intricate semantic context. This insight is especially critical for digraphs in the complex domain, where multi-level structural encoding in Eq.(11) often provides valuable prompts within the coupling of real and imaginary components. Therefore, we advocate explicitly learning the importance and relevance of multi-granularity knowledge within different RF in a node-adaptive manner to boost predictions. This process can be defined as follows:\n$H = \\bigoplus_{i=0}^K \\sum_i$,\n$\\mathbf{E}^{(i)} = MLP (Complex (\\mathbf{x}^{(0)} || ... || Complex (\\mathbf{x}^{(K)}))$, (12)\nwhere $\\delta$ is the non-linear activation function. This mechanism is designed to construct a personalized multi-granularity representation fusion for each node, facilitating the weighted message aggregation. As the training progresses, the MAP++ gradually accentuates the importance of neighborhood regions in the complex domain that contribute more significantly to the target nodes."}, {"title": "5 Theoretical Analysis", "content": "Now, we have achieved adaptive magnetic field potential modeling for directed edges. To further investigate the effectiveness of our approach and ensure theoretical interpretability, we build upon insights from related studies [22, 56] by extending the angular synchronization framework to graph attribute synchronization problem, which incorporates node features and directed topology.\nGraph Attribute Synchronization. The conventional angular synchronization problem aims to estimate a set of unknown angles $\\theta_1,..., \\theta_n$ from m noisy measurements of their pairwise offsets [56]. The noise associated with these measurements is uniformly distributed over the interval [0, 2\u03c0). Based on this, we have:\nDEFINITION 1. In the graph $G = {V,E}$, each node $u \\in V$ is associated with an angle $\\theta_u$. Given noisy measurements of angle offsets $d_{ij}$, the angular synchronization problem aims to estimate the angles $\\theta_1,..., \\theta_n$. The distribution of $d$ is divided into two categories: reliable (good) edges $E_{good}$ and unreliable (bad) edges $E_{bad}$\n$d_{ij} = \\theta_i - \\theta_j$ for $(i, j) \\in E_{good}$,\n$d_{ij} \\sim Uniform ([0, 2\\pi))$ for $(i, j) \\in E_{bad}$. (13)\nBased on this, the adaptive phase matrix in MAP functions as a weighted adjacency matrix, reflecting the presence of edges and capturing the offsets, analogous to d. By treating it as a noisy node feature offset matrix, we can generate attribute $w_u$ for each node u based on the node features and directed topology and have:\nDEFINITION 2. The graph attribute synchronization problem aims to estimate a set of unknown attributes $w_1,..., w_n$ based on their noisy adaptive complex-domain offsets $\\Theta^{(q^*)}$, which are defined as:\n$w_u - w_v := 2\\pi q^*_{uv} (A_{uv} - A_{vu})$. (14)\nThis formulation demonstrates how the attributes $w_u$ can be inferred from the topology and phase information by leveraging the feature-related relationships between nodes. For the numerous zero values in the matrix $\\Theta^{(q^*)}$, we treat them as noisy data.\nSpectral Analysis in MAP. According to the related studies [9, 10, 56], solving the above graph attribute synchronization problem typically involves constructing a Hermitian matrix. We first investigate the MAP encoding process (see Sec. 4.1-4.2) and have:\nTHEOREM 1. The adaptive phase matrix $\\Theta^{(q^*)}$ encoding by MAP is skew-symmetric, and H is Hermitian, where H = exp $(i\\Theta^{(q^*)})$.\nBased on this, we define the optimization objective as:\n$w = arg \\underset{w}{max} \\sum_{i,j=1}^n S e^{-iw_i} H_{ij} e^{iw_j}$. (15)\nThis formulation effectively captures complex-domain offsets from topology and feature perspectives. However, it remains a non-convex problem, making it difficult to solve in practice. Here, we introduce the relaxation: let $z_i = e^{iw_i}$ and impose the constraint $\\sum_{i=1}^n |z_i|^2 = n$. This leads to the following optimization objective:\n$z = arg \\underset{z}{max} z^*Hz$. (16)\nObviously, the maximizer z is given by z = $v_1$, where v\u2081 is the normalized top eigenvector satisfying $Hv_1 = \\lambda_1v_1$ and $||v_1||_2 = n$, where \u03bb\u2081 is the largest eigenvalue of H. Thus, the estimated attributes can be defined as: $e^{i\\hat{w}_i} = v_1 (i)/|v_1 (i)|$."}, {"title": "6 Experiments", "content": "In this section, we aim to offer a comprehensive evaluation and address the following questions to verify the effectiveness of our proposed MAP and MAP++: Q1: As a hot-and-plug strategy, what is the impact of MAP on the existing MagDGs? Q2: How does MAP++ perform as a new digraph learning model? Q3: If MAP and MAP++ are effective, what contributes to their performance? Q4: What is the running efficiency of them? Q5: How robust is MAP and MAP++ when dealing with sparse scenarios? To maximize the usage for the constraint space, we will introduce datasets, baselines, and experiment settings in Appendix A.6-A.9."}, {"title": "6.1 Performance Comparison", "content": "A Hot-and-plug Optimization Module. To answer Q1, we present the performance enhancement facilitated by MAP in Table 1 and Table 2. We observe that MAP significantly benefits all methods. This is attributed to its adaptive encoding of magnetic field potentials for directed edges, thereby customizing propagation rules. Notably, due to the different numerical ranges of the metrics, the improvements at the node level are more pronounced. Meanwhile, the coupling architectures and the additional computational overhead result in scalability issues for MagNet and Framelet, leading to OOM errors when dealing with the billion-level dataset. Although MGC decouples the graph propagation, its advantages require multiple propagations to fully manifest, leading to incomplete training within 12 hours and resulting in OOT errors. For detailed algorithmic complexity analysis, please refer to Appendix A.1."}, {"title": "6.2 Ablation Study", "content": "The Key Design of MAP and MAP++. To answer Q3, we present experimental results in Table 4, evaluating the effectiveness of (1) Topology-related uncertainty and Feature-related correlation encoding in Sec. 4.3; (2) Edge-wise graph propagation and node-wise message aggregation in Sec. 4.4. We draw the following conclusions: (1) Local structural encoding models neighbor in a fine-grained manner, reducing the prediction variance of MagNet from 0.48 to 0.32 on the Tolokers. (2) Global structural encoding enhances performance upper bounds by regulating propagation granularity comprehensively. (3) Feature correlation is directly relevant to downstream tasks, thereby crucial for performance improvement. Specifically, it boosts LightDiC's accuracy from 65.21 to 67.25 in CiteSeer. (4) Based upon these concepts, MAP++ introduces parameterized propagation kernels and attention-based message aggregation to further optimize predictions, leading to significant improvements."}, {"title": "6.3 Efficiency Comparison", "content": "Convergence Improvement. To answer Q4, we present the experimental results in Fig. 4, where we observe that MAP significantly aids existing MagDGs in achieving faster and more stable convergence, along with higher accuracy. For instance, in WikiCS, MAP assists MGC in achieving rapid convergence around the 20th epoch, saving nearly half of the training cost. Notably, due to the sparse node features in Tolokers and the intricate topology in large-scale arXiv, all methods inevitably suffer from over-fitting issues and slow convergence. However, integrating MAP significantly enhances the training efficiency of all baselines and mitigates these issues."}, {"title": "6.4 Performance under Sparse Scenarios", "content": "To answer Q5, we present experimental results in Fig. 6. For feature sparsity, we introduce partial missing features for unlabeled nodes. Consequently, methods relying solely on node quantity, such as D-HYPR, suffer performance degradation. Conversely, DiGCN, MGC, and MAP++ demonstrate resilience, as their high-order propagation partially compensates for the missing features. Regarding edge sparsity, since all baselines rely on topology to obtain high-quality node embeddings, they all face severe degradation. However, we observe that MAP++ outperforms others due to its fine-grained message passing. As for the label sparsity, we observe a similar trend to the feature sparsity. These findings collectively underscore the robustness enhancements achieved by MAP++ over baselines."}, {"title": "7 Conclusion", "content": "In recent years, MagDGs have stood out for edge direction modeling through the complex domain, inheriting insights from undirected graph learning. However, the extension of the q-parameterized magnetic Laplacian to digraph learning remains under-explored. To emphasize such a research gap, we provide valuable empirical studies and theoretical analysis to obtain the q-parameterized criteria for digraph learning. Based on this, we introduce two key techniques: MAP and MAP++. The achieved SOTA performance, coupled with flexibility and scalability, serves as compelling evidence of the practicality of our approach. A promising direction involves tailoring complex-domain graph propagation. Furthermore, an in-depth analysis of magnetic potential modeling from the perspective of topological dynamics shows great potential."}, {"title": "A.2 The Proof of Theorem 1", "content": "PROOF. To prove the skew-symmetry of $\\Theta^{(q^*)}$ and the Hermitian property of exp $(i\\Theta^{(q^*)})$, we begin by analyzing the relationships established in Eq. (7)-(9).\nFrom Eq. (7), we observe that $q_{uv}^{topo} = q_{vu}^{topo}$, indicating that the topological contribution to the parameter q between nodes u and v is symmetric. Similarly, from Eq. (8), we find that $q_{uv}^{feat} = q_{vu}^{feat}$, confirming that the feature-based contribution to q is also symmetric. Therefore, by combining these two components in Eq. (9), we conclude that $q^* = q^*$, meaning that the overall parameter q* is symmetric with respect to nodes u and v.\nNext, using this symmetry, we examine the matrix $\\Theta^{(q^*)}$ , which encodes the phase differences between nodes in the complex domain. Specifically, we have:$\\Theta^{(q^*)} (u,v) = -\\Theta^{(q^*)} (v, u)$. This relationship confirms that $\\Theta^{(q^*)}$ is skew-symmetric, meaning that $\\Theta (-\\Theta^{(q^*)})$. Here, for any real skew-symmetric matrix A, the matrix exp(iA) is Hermitian. $\\Theta^{(q^*)}$ is skew-symmetric, it follows that exp $(i\\Theta^{(q^*)})$ is a Hermitian matrix. This property is crucial in ensuring that the matrix captures the directed dependencies between nodes in a way that preserves the necessary mathematical structure for subsequent analysis. In summary, the symmetry of q* leads to the skew-symmetry of $\\Theta^{(q^*)}$, and as a result, exp $(i\\Theta^{(q^*)})$ is Hermitian, confirming the desired properties."}, {"title": "A.3 The Proof of Theorem 2", "content": "PROOF. We suppose the proportion of noisy offsets in is 1 \u2013 p. Let z be the normalized vector defined as $z_i = \\frac{e^{iw_i}}{\\sqrt{n}}$. With a probability of p, the edge {i, j} is good and $H_{ij} = e^{i(w_i-w_j)}$. On the other hand, with a probability of 1 - p, the edge is bad. The matrix H can be decomposed as $H = npzz^* + R$, where R is a noise matrix."}, {"title": "A.4 The Proof of Theorem 3", "content": "PROOF. Since is noise-free, we define K as an n \u00d7 n matrix where $K_{ij} = 1$ for all i, j. As the K is symmetric, it possesses a complete set of real eigenvalues $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n$, along with corresponding real orthonormal eigenvectors $\\psi_1, \\cdots, \\psi_n$. We can express K in terms of its eigenvalues and eigenvectors as follows:\n$K = \\sum_{l=1}^n \\lambda_l \\psi_l \\psi_l^*$. (23)\nNext, let Z be an n \u00d7 n diagonal matrix with diagonal elements $Z_{ii} = e^{iw_i}$. It is evident that Z is a unitary matrix, satisfying ZZ* = I. We then construct the Hermitian matrix B by conjugating K with Z:\n$B = ZKZ^*$. (24)\nThe eigenvalues of B remain the same as those of K, namely $\\lambda_1, \\lambda_2, ..., \\lambda_n$. The corresponding eigenvectors ${\\phi_l}_{l=1}^n$ of B, satisfying B$\\phi_l = \\lambda_l \\phi_l$, are given by\n$\\phi_l = Z\\psi_l$, l = 1, . . ., n. (25)\nNext, we observe the entries of B:\n$B_{ij} = e^{i(w_i-w_j)}$. (26)\nAccording to the Perron-Frobenius theorem [24], since K is a non-negative matrix, the components of the top eigenvector $v_1$ associated with the largest eigenvalue $\\lambda_1$ are all positive:\n$\\psi_1 (i) > 0, \\forall i = 1, 2, . . ., n$. (27)\nConsequently, we examine the complex phases of the coordinates of the top eigenvector $\\phi_1 = Z\\psi_1$. Thus, the complex phases of the coordinates of $1 are identical to the true attributes:\n$e^{iw_i} = \\frac{\\phi_1 (i)}{|\\phi_1(i)|}$. (28)"}, {"title": "A.5 Our Approach and GNNSync", "content": "The attribute synchronization problem we propose can also be addressed by GNNSync [22]. It reframes the synchronization problem as a theoretically grounded digraph learning task, where angles are estimated by designing a specific GNN architecture to extract graph embeddings and leveraging newly introduced loss functions. This method has demonstrated superior performance in high-noise environments. Notably, our proposed MAP framework can further enhance the attribute synchronization process when integrated with GNNSync in the following two significant ways.\nFirstly, MAP can act as an encoder within the GNNSync framework, generating higher-quality node embeddings compared to DIMPA used in the original implementation. By more effectively encoding both node features and topology, MAP improves the overall learning capability of the model. Secondly, the adaptive phase matrix introduced by MAP enables personalized encoding of directed edges, capturing critical directed information. This personalized encoding allows the generated node attributes to more accurately reflect the underlying characteristics of each node, ultimately improving the performance of the synchronization task. Through these enhancements, the MAP framework positions itself as a powerful tool for advancing the capabilities of GNNSync and other similar methods in digraph learning and attribute synchronization."}, {"title": "A.7 Compared Baselines", "content": "The baselines we employ are as follows: (1) Directed prevalent message passing-based approaches: DGCN [62", "61": "DIMPA [21", "30": "Dir-GNN [54", "59": 2, "MagDGs": "MagNet [70", "MGC[67": "Framelet-Mag (Framelet) [40", "35": ".", "networks": "GCN [27", "63": "GCNII [7", "4": "OptBasisGNN [18", "6": "NAG), GAMLP [69", "74": "and HoloNet [28"}]}