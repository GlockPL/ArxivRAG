{"title": "Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization", "authors": ["Ankan Mullick", "Sombit Bose", "Rounak Saha", "Ayan Kumar Bhowmick", "Aditya Vempaty", "Pawan Goyal", "Niloy Ganguly", "Prasenjit Dey", "Ravi Kokku"], "abstract": "The ever-increasing volume of digital information necessitates efficient methods for users to extract key insights from lengthy documents. Aspect-based summarization offers a targeted approach, generating summaries focused on specific aspects within a document. Despite advancements in aspect-based summarization research, there is a continuous quest for improved model performance. Given that large language models (LLMs) have demonstrated the potential to revolutionize diverse tasks within natural language processing, particularly in the problem of summarization, this paper explores the potential of fine-tuning LLMs for the aspect-based summarization task. We evaluate the impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect based summary dataset. We hypothesize that this approach will enable these models to effectively identify and extract aspect-related information, leading to superior quality aspect-based summaries compared to the state-of-the-art. We establish a comprehensive evaluation framework to compare the performance of fine-tuned LLMs against competing aspect-based summarization methods and vanilla counterparts of the fine-tuned LLMs. Our work contributes to the field of aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs for generating high-quality aspect-based summaries. Furthermore, it opens doors for further exploration of using LLMs for targeted information extraction tasks across various NLP domains.", "sections": [{"title": "1 Introduction", "content": "The ever-growing volume of information in various digital formats presents a significant challenge for users who need to efficiently extract key insights from large documents. Automatic text summarization has emerged as a valuable tool to address this challenge, providing concise and informative representations of textual documents - El-Kassas et al. (2021); Gambhir & Gupta (2017); Tas &\nKiyani (2007). While traditional summarization techniques aim to capture the overall gist of a\ndocument, aspect-based summarization offers a more focused approach.\nAspect-based summarization goes beyond generic summarization by targeting specific aspects or\ntopics within a document - Frermann & Klementiev (2019); Coavoux et al. (2019); Mukherjee et al.\n(2020). This targeted approach is particularly valuable for large documents, such as research papers,\nproduct reviews, or news articles, where specific information about certain aspects might be crucial\nfor the reader. Aspect-based summarization allows users to delve deeper into a document by quickly\nproviding summaries that cater to their specific information needs. For instance, consider a researcher\nreviewing a medical study. Using aspect-based summarization, they can prioritize summaries that\nhighlight the methodology and results sections. Conversely, a customer reading online reviews for a\nnew phone might prioritize summaries emphasizing aspects like battery life or camera performance -\nLi et al. (2020); Kunneman et al. (2018).\nHence, effective generation of aspect-based summaries presents a unique challenge. Unlike generic\nsummarization, which focuses on capturing the overall gist of a document, aspect-based summariza-"}, {"title": "2 Related Work", "content": "In this section, we perform a survey of the state-of-the-art literature on summarization and discuss\nthe literature on different types of summarization as follows:"}, {"title": "2.1 Generic Summarization", "content": "We focus on brief literature survey on generic summarization, which encompasses a broad approach\nto summarizing text without focusing on specific aspects, queries, or goals, using abstractive or\nextractive approaches. Among abstractive approaches, Chopra et al. (2016) introduced an abstractive\nsummarization model using attentive recurrent neural networks and discuss the challenges of gener-\nating coherent and informative summaries while avoiding redundancy. Based on pointer-generator\nnetwork framework, See et al. (2017) presents a model that combines extractive and abstractive\ntechniques for summarization by effectively incorporating source information into the generated sum-\nmaries. On the other hand, among purely extractive approaches, earlier researchers used graph-based\napproaches like TextRank (Mihalcea & Tarau (2004)) and LexRank (Erkan & Radev (2004))."}, {"title": "2.2 Aspect-based Summarization", "content": "Hayashi et al. (2021) employed a method for aspect-based summarization focusing on multiple\ndomains while (Coavoux et al. (2019)) focused on aspect-based multi-document abstractive summa-\nrization with an unsupervised approach. Few works have also explored domain-specific aspect-based\nsummarization such as (Mukherjee et al. (2020)) that focus on data from tourist review domain and\n(Akhtar et al. (2017)) that focus on dataset of hotel reviews. (Tang et al. (2016)) developed a deep\nmemory network for aspect-level sentiment classification, emphasizing the extraction of aspects\nwithin a document and these are relevant for aspect-based summarization. Again, (Wang et al. (2016))\nproposed an attention-based LSTM model which helps identify and emphasize important aspects and\nthese are used for aspect-based summarization."}, {"title": "2.3 Use of LLMs for summary evaluation", "content": "LLMs are recently emerging as alternatives to traditional metrics and human evaluation for evaluating\nNLP tasks. Recent work has explored LLM-based NLG evaluation methods (Gao et al. (2024)) while\n(Chan et al. (2023)) assessed the quality of generated responses from different models on open-ended\nquestions. (Zhou et al. (2023)) have proposed guidelines for LLM use for evaluations while few\nworks have proposed techniques to improve LLM evaluation performance [Hasanbeig et al. (2023);\nLiu et al. (2023)] and (Huang et al. (2023)) have also investigated the explainability of LLMs in\nevaluation contexts.\nIn this paper, our focus is on analysing the impact of fine-tuning open-source foundation LLMs on the\nperformance of the aspect-based summarization task and determine the type of LLMs that can help to\ngenerate high quality aspect-based summaries either using the pre-trained version or after fine-tuning\non relevant datasets. We also use LLMs (GPT4) to evaluate summaries on different conventions."}, {"title": "3 Dataset", "content": "We leverage the publicly available benchmark dataset, Open Aspect-based Summarization (OA-\nSUM) Yang et al. (2022), for both fine-tuning open-source foundation LLMs and evaluating their\nperformance. OASUM offers a rich collection of over 3.6 million document-aspect-summary triplets,\nfeaturing diverse aspects across various domains\n. There are 1M unique aspects in the entire dataset.\nThe average token count for the documents and aspect-based summaries are 1612 and 40 respectively.\nData Preprocessing and Variations: To facilitate targeted training and analysis, we prepared several\nvariations of the OASUM dataset:\n1. Domain-Wise Split: We selected 16 aspects from four popular domains (Healthcare, Music,\nEducation, Life & Career) resulting in a domain-specific dataset of 14,279 training instances.\n2. High-Frequency Aspects: We created the variation OASUM-Hi by choosing the top-50 most\nfrequent aspects (based on document count) and randomly selecting 1,000 documents for each. This\ndataset investigates the impact of fine-tuning on well-represented aspects.\n3. Low-Frequency Aspects: In contrast, the variation OASUM-Lo focuses on the long tail of\nthe dataset. We selected the 50 least frequent aspects (1 4 document occurrences) with 1,000\ndocuments each. This explores fine-tuning performance on less common aspects (aka long-tails)."}, {"title": "4 Proposed Framework", "content": "In this section, we detail our framework for fine-tuning open-source foundation LLMs on the OASUM\ndataset to obtain corresponding fine-tuned domain-specific LLMs specialized for the downstream\ntask of aspect-based summarization. We describe the fine-tuning process, the LLM architectures\nemployed, and the baseline models used for comparison."}, {"title": "4.1 Model architecture for fine-tuning LLMS", "content": "Our training process consists of employing different open-source foundation LLMs for fine-tuning on\nthe training set of OASUM dataset described above. Specifically, we leverage supervised fine-tuning\n(Zhang et al. (2023)) on the OASUM training dataset to transform pre-trained foundation LLMs\ninto domain-specific models suited to perform aspect-based summarization. This involves utilizing\nprompt-completion pairs to guide the pre-trained models towards generating aspect-based summaries.\nEach training instance comprises a document paired with an instruction to generate a summary based\non a specific aspect. The corresponding completion is the relevant aspect-based summary.\nTo enhance the fine-tuning process, we incorporate advanced techniques like Quantized Low-Rank\nAdaptation (QLORA) Dettmers et al. (2023) and PEFT (Parameter-Efficient Fine-Tuning) Fu et al.\n(2023) to optimize training efficiency. Following fine-tuning, these models (referred to as \"*FT\")\nacquire the ability to generate aspect-based summaries for corresponding documents based on the\nspecified aspect within the prompt. Following is a summary of the open-source foundation LLMs we\nfine-tuned on OASUM:\n1. Llama2: We use two different versions of Llama2 - vanilla: with sizes of 7b, 13b and 70b (Touvron\net al. (2023)) and fine-tuned: using models Llama2-7b and 13b. We have referred the Llama2-7b and\nLlama2-13b fine-tuned version as Lm7b-FT and Lm13b-FT.\n2. Mistral: We fine-tuned the Mistral-7b decoder-only Transformer model (Jiang et al. (2023)) from\nMistral AI, obtaining Mistral-7b-FT (abbreviated as Mis7b-Va for vanilla and Mis7b-FT for finetune).\n3. Gemma: We use Gemma which is a family of lightweight, state-of-the-art open models (Team\net al. (2024)) developed by Google DeepMind from the same technology used to create the Gemini\nmodels. Specifically, we finetune the Gemma-2b version to obtain the finetuned version referred to as\nGemma-FT.\n4. Aya: We use the Aya Model (\u00dcst\u00fcn et al. (2024)), a massively multilingual 13 billion parameter\nlanguage model capable of following instructions in 101 languages that is developed by Cohere and\nfine-tune the pre-trained version to obtain the fine-tuned model referred to as Aya-FT.\nFor performance comparison, we also include the vanilla pre-trained versions of each LLM (referred\nto as \"*VA\"). These include Llama2-7b-VA (Lm7b-VA), Llama2-13b-VA (Lm13b-VA), Llama2-70b-\nVA (Lm70b-VA), Mistral-7b-VA, Gemma-VA, and Aya-VA."}, {"title": "4.2 Baseline models", "content": "We use the following state-of-the-art competing baselines for comparing the performance of aspect-\nbased summarization task against the fine-tuned LLMs and their vanilla counterparts:\n1. LongFormer: The Longformer (Beltagy et al. (2020)) is a transformer-based model designed\nto handle long documents efficiently using an attention pattern that effectively combines local and\nglobal information, enabling to handle long inputs. We use Longformer-base (LED-ba) and large\n(LED-La) model with 149 million and 439 million parameters.\n2. T5 (Text-to-Text Transfer Transformer): This model Raffel et al. (2020) leverages transfer\nlearning for summarization tasks by converting them into a text-to-text format. We fine-tune the\nT5-3b version (T5-FT) with 3 billion parameters to generate aspect-based summaries.\n3. Flan T5: Flan-T5 Chung et al. (2022) instruction fine-tuned approach highlights the benefits of\nfine-tuning across various models, prompting setups, and evaluation tasks. We finetune the Flan T5\nXL model (Fl-T5-FT).\n4. BART (Bidirectional and Autoregressive Transformer): This denoising autoencoder Lewis\net al. (2019) is used for pre-training sequence-to-sequence models. We employ the instruction-\nprompted BART-large model with 406 million parameters, pre-trained on English and fine-tuned for\nsummarization on the CNN Daily Mail news dataset.\n5. Pegasus: We utilize the instruction-tuned Pegasus model Zhang et al. (2020) with 571 million\nparameters for generating aspect-based summaries.\n6. Falcon: The Falcon 7b-instruction-tuned model Penedo et al. (2023) is used for generating\naspect-based summaries.\n7. TLDR: We apply state-of-the-art approach \u2018TLDR-CATTS-XSUM' (TLDR) Cachola et al. (2020)\nfor extreme summarization to obtain crisp summary of the document."}, {"title": "5 Experimental evaluation and Results", "content": "In this section, we evaluate the performance of our different fine-tuned LLM models in terms of the\nquality of the generated aspect-based summaries for documents in the OASUM domain wise test set\nand compare against their vanilla counterparts as well as the competing baseline models."}, {"title": "5.1 Evaluation metrics and experimental settings", "content": "Our evaluation relies on two different approaches:\n1. Traditional: Here we check the comptenece of different models with traditional evaluation metrics\nlike (i) Rouge 1 (R1), Rouge 2 (R2) and Rouge L (RL) (Lin (2004)), (ii) Meteor (Mt) (Banerjee &\nLavie (2005)), (iii) Bleu (Bl) Papineni et al. (2002)), and (iv) BERTScore F1 (BeF1) (Zhang et al.\n(2019)) to assess the quality of generated summaries.\n2. GPT-4 Critique: Here, we use the GPT-4 LLM as a critique Valmeekam et al. (2023); Sun\net al. (2024) to evaluate the quality of the model generated aspect-based summaries against the gold\nstandard aspect-based summaries in the test set of the OASUM dataset variations from different\ndimensions. Specifically, we provide suitable critique based prompts to GPT-4 where we evaluate the\nsummaries based on a set of five predefined criterias (termed as GPT-4 criteria) defined below:\na. Relevance (Re): The extent to which the generated summary is relevant to the specific aspect-based\nsummary of the document.\nb. Coverage (Cv): The extent to which the generated aspect-based summary correctly covers all the\nimportant key points described in the gold standard aspect-based summary of the document.\nc. Impurity (Im): The extent to which the aspect-based summary does not contain information specific\nto any other aspect.\nd. Rating (Ra): Scores how well the summary captures the target aspect with the score reflecting\nif the summary is good, average or bad. A good summary is clear, concise, accurate, and engaging."}, {"title": "5.2 Results and discussion", "content": "In this section, we analyze the results presented in Table 3 as well as Figure 1 based on values of\ntraditional metrics and GPT-4 criteria respectively to understand how different models perform and\ngain insights into the effectiveness of fine-tuning LLMs for aspect-based summarization."}, {"title": "5.2.1 How effective is fine-tuning LLMs for aspect-based summarization based on traditional\nevaluation metrics?", "content": "In Figure 1, we can see comparison between vanilla and fine-tuned LLMs based on values for\ntraditional metrics like ROUGE and BERTScore. Here, we can observe a significant performance\nboost for fine-tuned LLMs (particularly Llama2-7b-FT, Llama2-13b-FT, Mistral-7b-FT) compared to\ntheir vanilla counterparts (Llama2-7b-VA, Llama2-13b-VA, Mistral-7b-VA) across all metrics. This\nindicates that fine-tuning successfully tailors these models to the task of aspect-based summarization,\nenabling them to generate summaries that better match the gold-standard summaries in terms of\nn-gram overlap and semantic similarity.\nAmong the fine-tuned LLMs, Llama2-13b-FT consistently achieves the highest scores across all\ntraditional metrics compared to competing baseline models (as seen from Table 3), suggesting that its\nlarger parameter size provides an advantage in capturing the nuances of aspect-based information.\nInterestingly, among the latest released LLMs, Aya-VA demonstrates an expected performance gain\nupon fine-tuning, suggesting its potential suitability for aspect-based summarization tasks. However,\nGemma-VA degrades in BeF1 score, highlighting the importance of model architecture and suitability\nfor aspect-based summarization task beyond parameter size. In summary, all models might NOT gain\nperformance upon finetuning."}, {"title": "5.2.2 How does fine-tuning LLMs impact the quality of summaries based on GPT-4 critiquing?", "content": "Table 3 and Fig. 1 also unveil a deeper perspective on summary quality through the lens of GPT-4\ncritiquing. Here, we evaluate summaries based on five criteria: relevance, key point coverage, aspect-"}, {"title": "5.2.3 Which LLMs achieve the best performance on fine-tuning?", "content": "By combining the insights from both traditional metrics and GPT-4 critiquing results, Llama2-13b-FT\nemerges as the clear winner for generating aspect-based summaries, consistently demonstrating\nsuperior performance in terms of similarity, key point coverage, relevance, and overall quality. Its\nlarger parameter size appears to be instrumental in achieving this level of performance for the\naspect-based summarization task, along with its superior architecture.\nThese findings significantly strengthen the case for fine-tuning LLMs for aspect-based summarization,\nfor most of the base models. Fine-tuning not only improves the similarity of generated summaries to\nthe gold standard but also enhances their ability to capture the essence of the target aspect and deliver\nclear, concise information. While parameter size plays a role, model architecture also plays a crucial\npart, as evidenced by Gemma-VA's limitations with fine-tuning not improving its performance and\nthe marginal improvement of Aya-FT over its vanilla counterpart."}, {"title": "5.2.4 How robust is the fine-tuned LLM for variations in dataset and domains for aspect-based\nsummarization?", "content": "To answer this question, we pick our best perfoming fine-tuned model from the results in the previous\nsection, the Llama2-13b-FT, and evaluate it on variations in the dataset.\nDifferent Types of OASUM Data: To check the effectiveness of oir fine-tuned models, we\nexperiment on different types of OASUM data: OASUM-Hi, OASUM-Lo and OASUM-Ra as shown\nin Table 4. By employing multiple dataset variations, we aim to achieve a comprehensive evaluation\nof fine-tuned LLMs for aspect-based summarization, taking into account various data characteristics\nand potential shortcomings in existing summaries. As expected, evaluation outcomes are best for\nOASUM-Hi and least for OASUM-Lo since number of aspects for OASUM-Hi is much lesser than\nOASUM-LO. OASUM-Ra exhibit results better than OASUM-Lo due to presence of lesser aspects than\nOASUM-Lo. Llama2-13b-FT performs best for almost all scenrios across different evaluation metrics.\nEvaluations for different Domains: In Table 5, we show five different traditional metric and GPT4\ncritique scores for the best performing Llama2-13b Finetuned model of OASUM data for 4 different\ndomains - Healthcare, Education, Life and Career and Music. It shows consistent performance of\nLlama2-13b Finetuned model for different domains.\nDifferent Evaluation Parameter Settings We evaluate outcomes of various models with different\nparameter setting during GPT4 critique - max-new-token and temperature. Best results are obtained\nwhen max-new-token size is 80 (as shown in Fig. 2) and GPT4 critique's temperature is 0.0.\nVarying Training Size Dataset: To understand the effect of training data size on the performance,\nwe vary the OASUM Domain-Wise Split training data for the Llama2-13b model - taking 10%, 40%\nand 70% of the initial training data, and finetune the Llama2-13b model with same parameter and\nhyper-parameter settings and the five criterias of GPT4-Critique outcome (in %) are shown in Fig\n2. We see that with increasing the dataset size, the performance of Llama2-13b improves in terms\nof different GPT4 critique metrics: Relevance (Re), Coverage (Cv), Impurity (Im), Rating (Ra) and\nGoodness (Gd). Even at 40% of the dataset, the model is able to achieve a decent performance. It"}, {"title": "6 Conclusion", "content": "In this paper, we addressed the ever-growing challenge of efficiently extracting key insights from\nvoluminous documents in the digital age. We explored the potential of fine-tuning large language\nmodels (LLMs) to enhance the performance of aspect-based summarization task. Our work centered\naround fine-tuning open-source foundation LLMs, including Llama2, Gemma, Mistral, and Aya,\non aspect-based summarization datasets. We hypothesized that this approach would enable these\nmodels to excel at identifying and extracting information relevant to user-specified aspects within a\ndocument, ultimately leading to superior quality aspect-based summaries.\nThrough a comprehensive evaluation framework, we compared the performance of fine-tuned LLMs\nagainst state-of-the-art aspect-based summarization methods and vanilla counterparts of the fine-tuned\nLLMs, and demonstrated significant improvement in quality of generated summaries as a result of\nfine-tuning. Our findings not only contribute towards the advancement of aspect-based summarization\ntechniques but also hold significant implications for the broader field of NLP. By demonstrating\nthe effectiveness of fine-tuning LLMs for targeted information extraction tasks like aspect-based\nsummarization, we open doors for further exploration and potential applications in various NLP\ndomains requiring focused information retrieval and summarization, ultimately empowering users to\nnavigate the ever-expanding sea of information with greater efficiency and precision."}, {"title": "Limitations", "content": "Our datasets are not multilingual and multimodal. We plan to capture aspects involving multimodal\ncontent, such as images or videos, limiting their comprehensiveness. LLMs may face challenges in\nadapting to domain-specific jargon, resulting in less informative summaries for aspects containing\nspecialized terminology. So, we need to explore how to correct these - which we aim to do as a part\nof future work."}, {"title": "Ethics Statement", "content": "Our work does not reveal any personal sensitive information and we use publicly available bench-\nmarked datasets and models in different contexts."}, {"title": "Appendix", "content": "We use prompting in two stages - finetune-inference and critique. There are two kinds of prompts -\nsystem prompt and user prompt."}, {"title": "A.1 Finetune and Inference prompt", "content": "You are an AI assistant who is to generate the summary of a textual document specific to a\ncertain aspect.\nSummarize the textual document given below from the perspective of aspect:"}, {"title": "A.2 Critique", "content": "You are an AI assistant who is to evaluate the summary of a textual document specific to\na certain aspect. You need to return a score between 0 and 1 reflecting the quality of the generated\nsummary based on some criteria.\nYou are given a textual document and the corresponding summary of the document generated\nfrom the respective of an aspect {aspect} predicted by a language model as follows.\nEvaluate the above aspect based summary for the document in terms of each of the following criteria\nand return only a score between 0 and 1 without any explanation:"}]}