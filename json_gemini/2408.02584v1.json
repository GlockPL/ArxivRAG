{"title": "Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization", "authors": ["Ankan Mullick", "Sombit Bose", "Rounak Saha", "Ayan Kumar Bhowmick", "Aditya Vempaty", "Pawan Goyal", "Niloy Ganguly", "Prasenjit Dey", "Ravi Kokku"], "abstract": "The ever-increasing volume of digital information necessitates efficient methods for users to extract key insights from lengthy documents. Aspect-based summarization offers a targeted approach, generating summaries focused on specific aspects within a document. Despite advancements in aspect-based summarization research, there is a continuous quest for improved model performance. Given that large language models (LLMs) have demonstrated the potential to revolutionize diverse tasks within natural language processing, particularly in the problem of summarization, this paper explores the potential of fine-tuning LLMs for the aspect-based summarization task. We evaluate the impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect based summary dataset. We hypothesize that this approach will enable these models to effectively identify and extract aspect-related information, leading to superior quality aspect-based summaries compared to the state-of-the-art. We establish a comprehensive evaluation framework to compare the performance of fine-tuned LLMs against competing aspect-based summarization methods and vanilla counterparts of the fine-tuned LLMs. Our work contributes to the field of aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs for generating high-quality aspect-based summaries. Furthermore, it opens doors for further exploration of using LLMs for targeted information extraction tasks across various NLP domains.", "sections": [{"title": "Introduction", "content": "The ever-growing volume of information in various digital formats presents a significant challenge for users who need to efficiently extract key insights from large documents. Automatic text summarization has emerged as a valuable tool to address this challenge, providing concise and informative representations of textual documents - El-Kassas et al. (2021); Gambhir & Gupta (2017); Tas & Kiyani (2007). While traditional summarization techniques aim to capture the overall gist of a document, aspect-based summarization offers a more focused approach.\nAspect-based summarization goes beyond generic summarization by targeting specific aspects or topics within a document - Frermann & Klementiev (2019); Coavoux et al. (2019); Mukherjee et al. (2020). This targeted approach is particularly valuable for large documents, such as research papers, product reviews, or news articles, where specific information about certain aspects might be crucial for the reader. Aspect-based summarization allows users to delve deeper into a document by quickly providing summaries that cater to their specific information needs. For instance, consider a researcher reviewing a medical study. Using aspect-based summarization, they can prioritize summaries that highlight the methodology and results sections. Conversely, a customer reading online reviews for a new phone might prioritize summaries emphasizing aspects like battery life or camera performance - Li et al. (2020); Kunneman et al. (2018).\nHence, effective generation of aspect-based summaries presents a unique challenge. Unlike generic summarization, which focuses on capturing the overall gist of a document, aspect-based summariza-"}, {"title": "Related Work", "content": "In this section, we perform a survey of the state-of-the-art literature on summarization and discuss the literature on different types of summarization as follows:"}, {"title": "Generic Summarization", "content": "We focus on brief literature survey on generic summarization, which encompasses a broad approach to summarizing text without focusing on specific aspects, queries, or goals, using abstractive or extractive approaches. Among abstractive approaches, Chopra et al. (2016) introduced an abstractive summarization model using attentive recurrent neural networks and discuss the challenges of generating coherent and informative summaries while avoiding redundancy. Based on pointer-generator network framework, See et al. (2017) presents a model that combines extractive and abstractive techniques for summarization by effectively incorporating source information into the generated summaries. On the other hand, among purely extractive approaches, earlier researchers used graph-based approaches like TextRank (Mihalcea & Tarau (2004)) and LexRank (Erkan & Radev (2004))."}, {"title": "Aspect-based Summarization", "content": "Hayashi et al. (2021) employed a method for aspect-based summarization focusing on multiple domains while (Coavoux et al. (2019)) focused on aspect-based multi-document abstractive summarization with an unsupervised approach. Few works have also explored domain-specific aspect-based summarization such as (Mukherjee et al. (2020)) that focus on data from tourist review domain and (Akhtar et al. (2017)) that focus on dataset of hotel reviews. (Tang et al. (2016)) developed a deep memory network for aspect-level sentiment classification, emphasizing the extraction of aspects within a document and these are relevant for aspect-based summarization. Again, (Wang et al. (2016)) proposed an attention-based LSTM model which helps identify and emphasize important aspects and these are used for aspect-based summarization."}, {"title": "Use of LLMs for summary evaluation", "content": "LLMs are recently emerging as alternatives to traditional metrics and human evaluation for evaluating NLP tasks. Recent work has explored LLM-based NLG evaluation methods (Gao et al. (2024)) while (Chan et al. (2023)) assessed the quality of generated responses from different models on open-ended questions. (Zhou et al. (2023)) have proposed guidelines for LLM use for evaluations while few works have proposed techniques to improve LLM evaluation performance [Hasanbeig et al. (2023); Liu et al. (2023)] and (Huang et al. (2023)) have also investigated the explainability of LLMs in evaluation contexts.\nIn this paper, our focus is on analysing the impact of fine-tuning open-source foundation LLMs on the performance of the aspect-based summarization task and determine the type of LLMs that can help to generate high quality aspect-based summaries either using the pre-trained version or after fine-tuning on relevant datasets. We also use LLMs (GPT4) to evaluate summaries on different conventions."}, {"title": "Dataset", "content": "We leverage the publicly available benchmark dataset, Open Aspect-based Summarization (OA-SUM) Yang et al. (2022), for both fine-tuning open-source foundation LLMs and evaluating their performance. OASUM offers a rich collection of over 3.6 million document-aspect-summary triplets, featuring diverse aspects across various domains\u00b9. There are 1M unique aspects in the entire dataset. The average token count for the documents and aspect-based summaries are 1612 and 40 respectively.\nData Preprocessing and Variations: To facilitate targeted training and analysis, we prepared several variations of the OASUM dataset:\n1.  Domain-Wise Split: We selected 16 aspects from four popular domains (Healthcare, Music, Education, Life & Career) resulting in a domain-specific dataset of 14,279 training instances.\n2.  High-Frequency Aspects: We created the variation OASUM-Hi by choosing the top-50 most frequent aspects (based on document count) and randomly selecting 1,000 documents for each. This dataset investigates the impact of fine-tuning on well-represented aspects.\n3.  Low-Frequency Aspects: In contrast, the variation OASUM-Lo focuses on the long tail of the dataset. We selected the 50 least frequent aspects (1 < 4 document occurrences) with 1,000 documents each. This explores fine-tuning performance on less common aspects (aka long-tails)."}, {"title": "Proposed Framework", "content": "In this section, we detail our framework for fine-tuning open-source foundation LLMs on the OASUM dataset to obtain corresponding fine-tuned domain-specific LLMs specialized for the downstream task of aspect-based summarization. We describe the fine-tuning process, the LLM architectures employed, and the baseline models used for comparison."}, {"title": "Model architecture for fine-tuning LLMS", "content": "Our training process consists of employing different open-source foundation LLMs for fine-tuning on the training set of OASUM dataset described above. Specifically, we leverage supervised fine-tuning (Zhang et al. (2023)) on the OASUM training dataset to transform pre-trained foundation LLMs into domain-specific models suited to perform aspect-based summarization. This involves utilizing prompt-completion pairs to guide the pre-trained models towards generating aspect-based summaries. Each training instance comprises a document paired with an instruction to generate a summary based on a specific aspect. The corresponding completion is the relevant aspect-based summary.\nTo enhance the fine-tuning process, we incorporate advanced techniques like Quantized Low-Rank Adaptation (QLORA) Dettmers et al. (2023) and PEFT (Parameter-Efficient Fine-Tuning) Fu et al. (2023) to optimize training efficiency. Following fine-tuning, these models (referred to as \"*FT\") acquire the ability to generate aspect-based summaries for corresponding documents based on the specified aspect within the prompt. Following is a summary of the open-source foundation LLMs we fine-tuned on OASUM:\n1. Llama2: We use two different versions of Llama2 - vanilla: with sizes of 7b, 13b and 70b (Touvron et al. (2023)) and fine-tuned: using models Llama2-7b and 13b. We have referred the Llama2-7b and Llama2-13b fine-tuned version as Lm7b-FT and Lm13b-FT.\n2. Mistral: We fine-tuned the Mistral-7b decoder-only Transformer model (Jiang et al. (2023)) from Mistral AI, obtaining Mistral-7b-FT (abbreviated as Mis7b-Va for vanilla and Mis7b-FT for finetune).\n3. Gemma: We use Gemma which is a family of lightweight, state-of-the-art open models (Team et al. (2024)) developed by Google DeepMind from the same technology used to create the Gemini models. Specifically, we finetune the Gemma-2b version to obtain the finetuned version referred to as Gemma-FT.\n4. Aya: We use the Aya Model (\u00dcst\u00fcn et al. (2024)), a massively multilingual 13 billion parameter language model capable of following instructions in 101 languages that is developed by Cohere and fine-tune the pre-trained version to obtain the fine-tuned model referred to as Aya-FT.\nFor performance comparison, we also include the vanilla pre-trained versions of each LLM (referred to as \"*VA\"). These include Llama2-7b-VA (Lm7b-VA), Llama2-13b-VA (Lm13b-VA), Llama2-70b-VA (Lm70b-VA), Mistral-7b-VA, Gemma-VA, and Aya-VA."}, {"title": "Baseline models", "content": "We use the following state-of-the-art competing baselines for comparing the performance of aspect-based summarization task against the fine-tuned LLMs and their vanilla counterparts:\n1. LongFormer: The Longformer (Beltagy et al. (2020)) is a transformer-based model designed to handle long documents efficiently using an attention pattern that effectively combines local and global information, enabling to handle long inputs. We use Longformer-base (LED-ba) and large (LED-La) model with 149 million and 439 million parameters.\n2. T5 (Text-to-Text Transfer Transformer): This model Raffel et al. (2020) leverages transfer learning for summarization tasks by converting them into a text-to-text format. We fine-tune the T5-3b version (T5-FT) with 3 billion parameters to generate aspect-based summaries.\n3. Flan T5: Flan-T5 Chung et al. (2022) instruction fine-tuned approach highlights the benefits of fine-tuning across various models, prompting setups, and evaluation tasks. We finetune the Flan T5 XL model (Fl-T5-FT).\n4. BART (Bidirectional and Autoregressive Transformer): This denoising autoencoder Lewis et al. (2019) is used for pre-training sequence-to-sequence models. We employ the instruction-prompted BART-large model with 406 million parameters, pre-trained on English and fine-tuned for summarization on the CNN Daily Mail news dataset.\n5. Pegasus: We utilize the instruction-tuned Pegasus model Zhang et al. (2020) with 571 million parameters for generating aspect-based summaries.\n6. Falcon: The Falcon 7b-instruction-tuned model Penedo et al. (2023) is used for generating aspect-based summaries.\n7. TLDR: We apply state-of-the-art approach \u2018TLDR-CATTS-XSUM' (TLDR) Cachola et al. (2020) for extreme summarization to obtain crisp summary of the document."}, {"title": "Experimental evaluation and Results", "content": "In this section, we evaluate the performance of our different fine-tuned LLM models in terms of the quality of the generated aspect-based summaries for documents in the OASUM domain wise test set and compare against their vanilla counterparts as well as the competing baseline models."}, {"title": "Evaluation metrics and experimental settings", "content": "Our evaluation relies on two different approaches:\n1. Traditional: Here we check the comptenece of different models with traditional evaluation metrics like (i) Rouge 1 (R1), Rouge 2 (R2) and Rouge L (RL) (Lin (2004)), (ii) Meteor (Mt) (Banerjee &\nLavie (2005)), (iii) Bleu (Bl) Papineni et al. (2002)), and (iv) BERTScore F1 (BeF1) (Zhang et al. (2019)) to assess the quality of generated summaries.\n2. GPT-4 Critique: Here, we use the GPT-4 LLM as a critique Valmeekam et al. (2023); Sun et al. (2024) to evaluate the quality of the model generated aspect-based summaries against the gold standard aspect-based summaries in the test set of the OASUM dataset variations from different dimensions. Specifically, we provide suitable critique based prompts to GPT-4 where we evaluate the summaries based on a set of five predefined criterias (termed as GPT-4 criteria) defined below:\na.  Relevance (Re): The extent to which the generated summary is relevant to the specific aspect-based summary of the document.\nb.  Coverage (Cv): The extent to which the generated aspect-based summary correctly covers all the important key points described in the gold standard aspect-based summary of the document.\nc.  Impurity (Im): The extent to which the aspect-based summary does not contain information specific to any other aspect.\nd.  Rating (Ra): Scores how well the summary captures the target aspect with the score reflecting if the summary is good, average or bad. A good summary is clear, concise, accurate, and engaging.\ne.  Goodness (Gd): Extending from 4, we manually verify the goodness of the summary.\nThis combined evaluation strategy allows us to assess performance from both a similarity and quality perspective, leveraging established metrics and leveraging the capabilities of GPT-4 for in-depth analysis.\nExperimental Settings: We use 80GB A100 GPU, 210MHz clock cycle and 6 epochs for all experiments (Details are in Appendix). We have used NLTK, Spacy, openai(version=0.28), hugging-face_hub, torch and transformers python packages for all experiments\u00b3."}, {"title": "Results and discussion", "content": "In this section, we analyze the results presented in Table 3 as well as Figure 1 based on values of traditional metrics and GPT-4 criteria respectively to understand how different models perform and gain insights into the effectiveness of fine-tuning LLMs for aspect-based summarization."}, {"title": "How effective is fine-tuning LLMs for aspect-based summarization based on traditional evaluation metrics?", "content": "In Figure 1, we can see comparison between vanilla and fine-tuned LLMs based on values for traditional metrics like ROUGE and BERTScore. Here, we can observe a significant performance boost for fine-tuned LLMs (particularly Llama2-7b-FT, Llama2-13b-FT, Mistral-7b-FT) compared to their vanilla counterparts (Llama2-7b-VA, Llama2-13b-VA, Mistral-7b-VA) across all metrics. This indicates that fine-tuning successfully tailors these models to the task of aspect-based summarization, enabling them to generate summaries that better match the gold-standard summaries in terms of n-gram overlap and semantic similarity.\nAmong the fine-tuned LLMs, Llama2-13b-FT consistently achieves the highest scores across all traditional metrics compared to competing baseline models (as seen from Table 3), suggesting that its larger parameter size provides an advantage in capturing the nuances of aspect-based information. Interestingly, among the latest released LLMs, Aya-VA demonstrates an expected performance gain upon fine-tuning, suggesting its potential suitability for aspect-based summarization tasks. However, Gemma-VA degrades in BeF1 score, highlighting the importance of model architecture and suitability for aspect-based summarization task beyond parameter size. In summary, all models might NOT gain performance upon finetuning."}, {"title": "How does fine-tuning LLMs impact the quality of summaries based on GPT-4 critiquing?", "content": "Table 3 and Fig. 1 also unveil a deeper perspective on summary quality through the lens of GPT-4 critiquing. Here, we evaluate summaries based on five criteria: relevance, key point coverage, aspect-"}, {"title": "Which LLMs achieve the best performance on fine-tuning?", "content": "By combining the insights from both traditional metrics and GPT-4 critiquing results, Llama2-13b-FT emerges as the clear winner for generating aspect-based summaries, consistently demonstrating superior performance in terms of similarity, key point coverage, relevance, and overall quality. Its larger parameter size appears to be instrumental in achieving this level of performance for the aspect-based summarization task, along with its superior architecture.\nThese findings significantly strengthen the case for fine-tuning LLMs for aspect-based summarization, for most of the base models. Fine-tuning not only improves the similarity of generated summaries to the gold standard but also enhances their ability to capture the essence of the target aspect and deliver clear, concise information. While parameter size plays a role, model architecture also plays a crucial part, as evidenced by Gemma-VA's limitations with fine-tuning not improving its performance and the marginal improvement of Aya-FT over its vanilla counterpart."}, {"title": "How robust is the fine-tuned LLM for variations in dataset and domains for aspect-based summarization?", "content": "To answer this question, we pick our best perfoming fine-tuned model from the results in the previous section, the Llama2-13b-FT, and evaluate it on variations in the dataset."}, {"title": "Different Types of OASUM Data:", "content": "To check the effectiveness of oir fine-tuned models, we experiment on different types of OASUM data: OASUM-Hi, OASUM-Lo and OASUM-Ra as shown in Table 4. By employing multiple dataset variations, we aim to achieve a comprehensive evaluation of fine-tuned LLMs for aspect-based summarization, taking into account various data characteristics and potential shortcomings in existing summaries. As expected, evaluation outcomes are best for OASUM-Hi and least for OASUM-Lo since number of aspects for OASUM-Hi is much lesser than OASUM-LO. OASUM-Ra exhibit results better than OASUM-Lo due to presence of lesser aspects than OASUM-Lo. Llama2-13b-FT performs best for almost all scenrios across different evaluation metrics."}, {"title": "Evaluations for different Domains:", "content": "In Table 5, we show five different traditional metric and GPT4 critique scores for the best performing Llama2-13b Finetuned model of OASUM data for 4 different domains - Healthcare, Education, Life and Career and Music. It shows consistent performance of Llama2-13b Finetuned model for different domains."}, {"title": "Different Evaluation Parameter Settings", "content": "We evaluate outcomes of various models with different parameter setting during GPT4 critique - max-new-token and temperature. Best results are obtained when max-new-token size is 80 (as shown in Fig. 2) and GPT4 critique's temperature is 0.0."}, {"title": "Varying Training Size Dataset:", "content": "To understand the effect of training data size on the performance, we vary the OASUM Domain-Wise Split training data for the Llama2-13b model - taking 10%, 40% and 70% of the initial training data, and finetune the Llama2-13b model with same parameter and hyper-parameter settings and the five criterias of GPT4-Critique outcome (in %) are shown in Fig 2. We see that with increasing the dataset size, the performance of Llama2-13b improves in terms of different GPT4 critique metrics: Relevance (Re), Coverage (Cv), Impurity (Im), Rating (Ra) and Goodness (Gd). Even at 40% of the dataset, the model is able to achieve a decent performance. It"}, {"title": "Conclusion", "content": "In this paper, we addressed the ever-growing challenge of efficiently extracting key insights from voluminous documents in the digital age. We explored the potential of fine-tuning large language models (LLMs) to enhance the performance of aspect-based summarization task. Our work centered around fine-tuning open-source foundation LLMs, including Llama2, Gemma, Mistral, and Aya, on aspect-based summarization datasets. We hypothesized that this approach would enable these models to excel at identifying and extracting information relevant to user-specified aspects within a document, ultimately leading to superior quality aspect-based summaries.\nThrough a comprehensive evaluation framework, we compared the performance of fine-tuned LLMs against state-of-the-art aspect-based summarization methods and vanilla counterparts of the fine-tuned LLMs, and demonstrated significant improvement in quality of generated summaries as a result of fine-tuning. Our findings not only contribute towards the advancement of aspect-based summarization techniques but also hold significant implications for the broader field of NLP. By demonstrating the effectiveness of fine-tuning LLMs for targeted information extraction tasks like aspect-based summarization, we open doors for further exploration and potential applications in various NLP domains requiring focused information retrieval and summarization, ultimately empowering users to navigate the ever-expanding sea of information with greater efficiency and precision."}, {"title": "Limitations", "content": "Our datasets are not multilingual and multimodal. We plan to capture aspects involving multimodal content, such as images or videos, limiting their comprehensiveness. LLMs may face challenges in adapting to domain-specific jargon, resulting in less informative summaries for aspects containing specialized terminology. So, we need to explore how to correct these - which we aim to do as a part of future work."}, {"title": "Ethics Statement", "content": "Our work does not reveal any personal sensitive information and we use publicly available bench-marked datasets and models in different contexts."}, {"title": "Appendix", "content": "We use prompting in two stages - finetune-inference and critique. There are two kinds of prompts - system prompt and user prompt."}, {"title": "Finetune and Inference prompt", "content": "system: You are an AI assistant who is to generate the summary of a textual document specific to a certain aspect.\nuser prompt - Summarize the textual document given below from the perspective of aspect:"}, {"title": "Critique", "content": "system: You are an AI assistant who is to evaluate the summary of a textual document specific to a certain aspect. You need to return a score between 0 and 1 reflecting the quality of the generated summary based on some criteria.\nuser: You are given a textual document and the corresponding summary of the document generated from the respective of an aspect {aspect} predicted by a language model as follows."}]}