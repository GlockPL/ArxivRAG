{"title": "VROPE: Rotary Position Embedding for Video Large Language Models", "authors": ["Zikang Liu", "Longteng Guo", "Yepeng Tang", "Junxian Cai", "Kai Ma", "Xi Chen", "Jing Liu"], "abstract": "Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VROPE), a novel positional encoding method tailored for Video-LLMs. Our approach restructures positional indices to preserve spatial coherence and ensure a smooth transition between video and text tokens. Additionally, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Extensive experiments on Vicuna and Qwen2 across different model scales demonstrate that VROPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have achieved remarkable progress (Touvron et al., 2023; Bai et al., 2023). Building on the success of LLMs, Video Large Language Models (Video-LLMs) (Maaz et al., 2023; Li et al., 2024d; Jin et al., 2024) have emerged as a powerful paradigm for video-language understanding. These models typically integrate LLMs with pre-trained vision encoders, enabling the joint modeling of video and textual information. However, a fundamental challenge in Video-LLMs lies in effectively modeling positional relationships within video sequences.\nIn LLMs, positional encoding plays a crucial role in enabling models to capture order-dependent patterns, as self-attention mechanisms themselves are inherently permutation-invariant. Among various positional encoding schemes, Rotary Position Embedding (RoPE) (Su et al., 2024) has gained widespread adoption due to its ability to encode relative position relationships. RoPE enables efficient long-range dependencies, making it highly effective in text-based models. However, when applied directly to video data, vanilla RoPE\u2014where video tokens are treated as a simple sequence akin to text\u2014fails to account for the complex spatiotemporal structure inherent in video frames, leading to suboptimal representations. Despite its critical role, an effective video-specific positional encoding strategy remains an open challenge.\nTo optimally encode positional relationships in Video-LLMs, we identify three key properties that an ideal video positional encoding method should satisfy:\n(1) Spatiotemporal Structure Modeling. Unlike text, where positional relationships are strictly one-dimensional, video frames exhibit both spatial (width, height) and temporal (frame index) dimensions. An effective encoding must reflect this inherent structure to facilitate accurate modeling of spatiotemporal dependencies. Recent approaches (Wang et al., 2024) have attempted to extend ROPE for video structure. These methods, which we refer to as ROPE-3D, split the feature channels into 3 parts to separately encode frame, width, and height positions.\n(2) Positional Unbiasedness. A critical yet often overlooked aspect of positional encoding is its impact on attention distribution. As illustrated in Figure 1 (a), RoPE, by design, applies a long-term decay over increasing positional indices, inadvertently introducing a bias that amplifies attention toward later tokens. This issue persists in RoPE-3D, where spatial positions within video frames are unevenly weighted, causing attention to be disproportionately focused on certain areas\u2014typically the bottom-right regions of frames\u2014while suppressing others, which is shown in Figure 1 (a). Such biases distort spatial contextual modeling and can lead to suboptimal video comprehension. An effective video positional encoding should mitigate these biases to ensure uniform attention across the entire frame.\n(3) Seamless Video-Text Transition. For effective multimodal understanding, an ideal positional encoding should ensure a seamless transition between video and text tokens. However, as demonstrated in Figure 1 (b), RoPE-3D introduces a discontinuity when transitioning from video to text tokens, as the positional indices of text tokens are arbitrarily offset by the maximum position index of the video sequence (determined by the largest of frame count, width, and height, which often vary significantly). This artificial \u201cjump\u201d in the positional encoding space disrupts the smooth flow of information between modalities, making it harder for the model to establish meaningful cross-modal dependencies.\nBased on the above principles, we propose Video Rotary Position Embedding (VROPE), a novel positional encoding method specifically designed for Video-LLMs. Our approach consists of two key components to satisfy those principles. (1) Cross-Modal Continuity Rotation: We introduce a spatial rotation transformation that redistributes RoPE-3D's positional indices while preserving local spatial structures. This transformation not only maintains spatial coherence within video frames but also ensures a smooth transition between video and text tokens, mitigating discontinuities in the positional encoding space. (2) Symmetric Bias Mitigation: To counteract the attention bias present in RoPE-based encodings, we design a symmetric positional representation that encodes each spatial coordinate with both positive and negative components. By distributing attention more uniformly across spatial locations, this method prevents positional distortions and improves overall video understanding.\nOverall, VROPE effectively enhances Video-LLMs by preserving spatiotemporal structure, mitigating attention bias, and ensuring smooth video-text transitions. We conduct extensive experiments on Vicuna (Chiang et al., 2023) and Qwen2 (Yang et al., 2024) across different model scales (1.5B and 7B parameters). Our results demonstrate significant performance improvements over RoPE, ROPE-3D, and other positional encoding variants on multiple video benchmarks, covering general video understanding, temporal reasoning, long video comprehension, and video retrieval. These findings establish VROPE as a robust and efficient positional encoding method tailored for Video-LLMs."}, {"title": "2 Related Work", "content": "Recent advancements in Video-LLMs have significantly enhanced video processing by integrating multiple modalities and employing instruction fine-tuning. Notable innovations include Video-ChatGPT (Maaz et al., 2023), which introduced video instruction tuning for text generation, and VideoChat (Li et al., 2023) and VideoChat2 (Li et al., 2024b), which improved modality alignment via cross-attention and multi-stage bootstrapping etc. Other models, such as Chat-UniVi (Jin et al., 2024) and LLaMA-VID (Li et al., 2024d), focus on efficient video representations through techniques like token compression and dual-token methods that separate context and content. Additionally, PLLaVA (Xu et al., 2024) explores the use of image-pretrained LLaVA models for video tasks, utilizing simple spatial pooling techniques."}, {"title": "2.2 Multimodal Position Embedding", "content": "Most Video-LLMs inherit the default design from LLMs by using Rotary Position Embedding (RoPE) (Su et al., 2024) for positional encoding. ROPE encodes relative distance information as absolute position embeddings, offering key advantages like no additional training parameters and improved performance in various tasks (Su et al., 2024). It is widely used in modern LLMs due to its ability to extrapolate context length, extending a model's window size without the need for expensive retraining. However, RoPE's 1D design, effective for text, overlooks the spatiotemporal structure of video data, limiting its suitability for Video-LLMs. To address this, several approaches have adapted ROPE for video (Su, 2024; Wang et al., 2024). For instance, RoPE-2D (Agrawal et al., 2024; Wang et al., 2024) extends the encoding to capture spatial relationships in video frames, while RoPE-3D (Wang et al., 2024) divides the channel dimension into three groups to better represent the spatiotemporal dimensions.\nHowever, these approaches still face issues like Positional Attention Bias and Cross-Modal Positional Discontinuity, which are discussed in Section 3. Our VROPE method addresses these limitations, offering more accurate and robust positional encoding tailored for Video-LLMs."}, {"title": "3 Motivation", "content": "Rotary Positional Embedding (RoPE) is a widely adopted method in LLMs that encodes absolute positional information while preserving relative positional relationships. This property makes ROPE particularly effective for self-attention mechanisms, as it allows models to capture the relative distance between tokens in a computationally efficient manner. Given a token embedding x at position index m, ROPE applies a complex-valued rotation operation, formulated as:\nRoPE(x,m) = xe^{\u0456\u0442\u03b8}"}, {"title": "3.1 Preliminary: Rotary Position Embedding", "content": "where i is the imaginary unit, and the frequency encoding vector \u03b8 is defined as:\n\u03b8_j = based^{-2j}"}, {"title": "3.2 ROPE for Video-LLMs", "content": "In Video-LLMs, video frames are typically processed by vision encoders (e.g., ViTs (Alexey, 2020) or CNNs (He et al., 2016)) and transformed into a sequence of visual tokens. These visual tokens are then concatenated with text tokens and fed into an LLM backbone. The overall architecture of typical Video-LLMs is denoted in Figure 2.\nIn most existing approaches, video tokens are treated as a simple 1D sequence, with position indices assigned in an increasing order, similar to text. However, this naive approach, referred to as ROPE, overlooks the inherent spatiotemporal structure of video data. Flattening video frames this way disrupts spatiotemporal structure and leads to inefficient token usage. Unlike text, video tokens carry less dense semantic information, and their excessive sequence length can weaken contextual dependencies, making long-range understanding harder."}, {"title": "3.3 ROPE-3D for Video-LLMs", "content": "Recent approaches, such as M-RoPE in Qwen2-VL(Wang et al., 2024), have proposed ROPE-3D as an extension of ROPE for video structure preserving. RoPE-3D intuitively partitions the feature dimensions to separately encode spatial (width, height) and temporal (frame index) positions. Given a video token with coordinates (w, h, t), RoPE-3D computes:\nROPE-3D_j(x, w, h, t) =\n{\nROPE_j(x, w), j \u2208 D_w\nROPE_j(x, h), j\u2208 D_h\nROPE_j(x, t), j\u2208 D_t"}, {"title": "3.4 Problem Analysis", "content": "While ROPE-3D introduces a promising design by partitioning the feature dimensions to encode spatial (width, height) and temporal (frame index) positions separately, two critical issues persist when handling video-text data.\nAs is demonstrated in Figure 3 (a), RoPE naturally applies a long-term decay over increasing positional indices, which amplify attention toward later positions. Unfortunately, we find that this issue persists in ROPE-3D, where the decay leads to an uneven distribution of focus across spatial positions in video frames.\nROPE-3D introduces separate positional encodings for spatial (width, height) and temporal (frame index) dimensions. However, when video tokens are concatenated with subsequent text tokens, their positional indices do not follow a smooth transition. Instead, text tokens inherit positional indices that are arbitrarily offset by the maximum position value across spatial (W, H) and temporal dimensions T, i.e., max(W, H,T). This results in an artificial \"jump\u201d in the positional encoding space when transitioning from video to text tokens. The discontinuity creates an abrupt and non-uniform gap between the final video token and the subsequent text token. The magnitude of this gap depends on video dimensions rather than being a fixed offset, making it inconsistent across different video-text samples. Such a discrepancy can degrade the model's ability to establish seamless contextual dependencies across modalities. This issue is particularly problematic in long videos, as the increasing frame count T exacerbates the positional gap."}, {"title": "4 Method: VROPE", "content": "In this section, we introduce Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Our approach addresses the inherent limitations of ROPE-3D, including spatiotemporal attention bias and cross-modal positional discontinuity, by leveraging a combination of Cross-Modal Continuity Rotation and Symmetric Bias Mitigation. The overall framework of VROPE is illustrated in Figure 4."}, {"title": "4.1 Cross-Modal Continuity Rotation", "content": "To mitigate the cross-modal positional discontinuity introduced when concatenating video and text tokens, we propose a spatial rotation transformation that redistributes positional indices while preserving the inherent spatial structure. Specifically, we transform the width and height coordinates into a rotated space as follows:\nU = \n{\nu = \n{w+h\nw-h+H-1"}, {"title": "4.2 Symmetric Bias Mitigation", "content": "While Cross-Modal Continuity Rotation alleviates discontinuity, the issue of spatial attention bias persists. To address this, we introduce Symmetric Bias Mitigation, which ensures a more uniform attention distribution across spatial dimensions.\nAs is shown in Figure 4 (b), instead of using scalar positional values, we represent each spatial position u, v as a symmetric vector to capture bidirectional positional dependencies, i.e., u = (u+, u\u2212) and v = (v+, v\u2212). To achieve this, we evenly partition the feature dimensions into four equal parts, assigning each part to a different positional encoding term. For a video of size (W, H, T) with an initial position index pstart, its first frame's position indices are computed as follows:\nu+\nu^{-=u+-u+0+p_{start"}, {"title": "4.3 Discussion", "content": "In addition to preserving the video structure, ensuring a continuous transition between video and text tokens, and mitigating spatial attention bias, VROPE also exhibits the following characteristics.\nImplicit Temporal Modeling. While VROPE does not directly encode frame indices, it implicitly captures temporal cues by arranging frame tokens in an ascending sequence and applying symmetric positional transformations. The temporal correlations among the frames are implicitly captured through the opposite direction design, ensuring the model differentiate temporal progressions.\nDimensional Adaptability. A key advantage of VROPE is its ability to degenerate into lower-dimensional embeddings without altering its fundamental structure. Unlike methods that assign separate feature channels for each coordinate, VROPE employs linear combinations of the original coordinates, allowing any dimension set to 1 to seamlessly adapt into lower-dimension form. For instance, when H = 1, the encoded positions simplify to (w, -w, w, -w), effectively reducing to a 1D form\u2014unlike previous methods that rely on separate encodings, such as (w, 0). This property is particularly beneficial for adapting pre-trained model's positional encodings from images (2D) or videos (3D) to data of varying dimensions without disrupting the original encoding scheme. Consequently, models can transfer more effectively across modalities while preserving consistent positional behavior."}, {"title": "5 Experiments", "content": "We apply our proposed VROPE to Video-LLM architectures with three widely used LLM backbones: Vicuna-7B, Qwen2-1.5B, and Qwen2-7B, the resulting models are denoted as Video-Vicuna-7B, Video-Qwen2-1.5B, and Video-Qwen2-7B. For the vision encoder, we leverage Eva-CLIP (Sun et al., 2023), and connect the Vision Encoder to the LLM using a Multi-Layer Perceptron (MLP) (Tolstikhin et al., 2021). For video input, the frames are tokenized using a 2 \u00d7 2 pooling kernel with a stride of 2, i.e., each frame has 64 tokens as input. Training follows a two-stage paradigm: in the pre-training stage, only the MLP connector is trained, while in the instruction-tuning stage, both the MLP and LLM backbones are fine-tuned, with the Vision Encoder frozen throughout. During pre-training, we use a batch size of 256 and a learning rate of 1e-3, while for instruction-tuning, we reduce the batch size to 128 and set the learning rate to 2e-5. A warm-up ratio of 0.03 is used, followed by cosine learning rate decay after the linear warm-up phase. The training was conducted on 8 Nvidia A800 GPUs.\nFor Vicuna-7B, we pre-train the model on the LLaVA-558K dataset (Liu et al., 2024a) with WebVid samples (Bain et al., 2021) and fine-tune it on the LLaVA-mix665K (Liu et al., 2024a) dataset augmented with VideoChat-GPT data (Maaz et al., 2023). For the Qwen2 LLM series, we pre-train the models on a randomly sampled 1M caption dataset, which includes LLaVA-558K, WebVid, DenseFusion-1M (Li et al., 2024c), VALOR (Liu et al., 2024b), and CC3M (Changpinyo et al., 2021). The models are then fine-tuned on a combination of LLaVA-mix665K, VideoChatGPT, OneVision (Li et al., 2024a), and LLaVA-Video-178K (Zhang et al., 2024). We use a 224 \u00d7 224 resolution for both image and video inputs."}, {"title": "5.1 Experimental Setup", "content": "We evaluate the performance of RoPE, ROPE-3D, and our proposed VROPE across six video understanding benchmarks, with the number of input frames fixed to 16. As shown in Table 1, VROPE consistently outperforms both RoPE and ROPE-3D, achieving the highest average scores across all tasks and backbones.\nFor instance, in the Video-Vicuna-7B row, VROPE achieves an average score of 44.48, surpassing ROPE by 1.13 points. Similarly, when evaluated with larger backbones like Qwen2-1.5B and Qwen2-7B, VROPE demonstrates consistent improvements across all benchmarks. Notably, it outperforms RoPE and RoPE-3D by significant margins on tasks such as Video-MME (a 3.4-point increase for Qwen2-1.5B) and MLVU (a 2.94-point increase for Qwen2-7B).\nThese results highlight the superior adaptability of VROPE across different LLM types and parameter sizes. Importantly, VROPE introduces no new learnable parameters and does not increase computational complexity, making it a cost-free performance enhancement for Video-LLMs."}, {"title": "5.2 Main Results", "content": "We compare our method with RoPE (Su et al., 2024) and RoPE-3D (Wang et al., 2024) on the long video retrieval task to evaluate the model's generalization ability with longer video inputs. Following the setup in Video-NIAH (Zhao et al., 2024), we conduct Video Needle-In-A-Haystack (V-NIAH) experiments, where a target \"needle\" frame is inserted into a sequence of background frames, with the total frame count varying between 256 and 1216.\nAs shown in Figure 5, the retrieval accuracy of ROPE drops significantly when the number of input frames exceeds 832. While ROPE-3D demonstrates better frame extrapolation capabilities compared to ROPE, VROPE outperforms both approaches by a considerable margin. The quantitative results, presented in Table 3, further evidence this finding. Specifically, VROPE achieves an accuracy that is 32.19 points higher than RoPE and 14.22 points higher than RoPE-3D when the number of input frames increases to 1024-1216. Notably, these results are obtained even though the input frame count in this range is dozens of times greater than the maximum number seen during training. This demonstrates the exceptional extrapolation ability of VROPE."}, {"title": "5.3 Results on Long Video Retrieval", "content": "We conduct experiments to assess the impact of three key properties: Spatiotemporal Structure Modeling (S.S.M.), Positional Unbiasedness (P.U.), and Seamless Video-Text Transition (S.V.T.), as discussed in Section 1. The results, summarized in Table 2, highlight the importance of these properties.\nWe first compare RoPE-2D (Agrawal et al., 2024) and RoPE-3D (Wang et al., 2024) with the baseline RoPE (Su et al., 2024) method. RoPE-2D encodes only the spatial coordinates (w, h) of each frame, arranged sequentially. While it resolves the cross-modal positional discontinuity of RoPE-3D, it still suffers from positional bias. Both RoPE-2D and RoPE-3D show improvements over RoPE, demonstrating the benefits of spatiotemporal structure modeling.\nNext, we evaluate two additional variants, RoPE-Share and RoPE-Compact, to further ablate the impact of S.S.M. and P.U. RoPE-Share uses identical positional embeddings within each frame, arranged sequentially. While it resolves positional bias and ensures continuity, it neglects the spatial structure of the frames, leading to a performance drop compared to ROPE. ROPE-Compact is a ex-tention of ROPE-3D that addresses positional discontinuity by encoding subsequent text tokens with (W + 1, H + 1, T + 1), but it deviates from text compatibility requirements, which slightly limits its performance. In contrast, our proposed method (VROPE) incorporates all three properties, achieving a 1.99-point improvement over the RoPE baseline, surpassing all other variants.\nWe conduct ablation experiments to evaluate the individual contributions of the Cross-Modal Continuity Rotation and Symmetric Bias Mitigation components. The results, presented in Table 4, reveal that when applied separately, each method produces mixed effects. Specifically, Cross-Modal Continuity Rotation improves performance on Video-MME, indicating its effectiveness in enhancing smooth translation for general video understanding. Symmetric Bias Mitigation shows a significant gain on LongVideoBench, indicating its effectiveness in reducing bias in long video tasks. When combined in VROPE, the two components work synergistically, resulting in more consistent performance."}, {"title": "5.4 Ablation Studies", "content": "In conclusion, we propose VROPE, a dedicated positional encoding strategy for Video-LLMs that balances spatiotemporal structure, mitigates attention bias, and ensures a smooth transition between video and text tokens. Extensive experiments on different model scales validate its superior performance in video understanding, temporal reasoning, and retrieval tasks. We believe VROPE can serve as a useful building block for future Video-LLMs, enabling better video-language understanding."}, {"title": "Ablation on VROPE Components", "content": "While VROPE demonstrates strong performance, there are some limitations. Due to computational resource constraints, our experiments were limited to models with 1.5B and 7B parameters. Larger-scale models could potentially yield further performance gains. Additionally, although VROPE is adaptable across different dimensions, its extension to other modalities (e.g., audio, 3D point clouds, Electroencephalography (EEG)) and higher-dimensional data (e.g., 4D spatiotemporal or medical imaging data) remains an area for future research and validation."}, {"title": "6 Conclusion", "content": "Implementation Details.\nTraining Data.\nEvaluation Benchmarks.\nWe evaluate the performance of RoPE, ROPE-3D, and our proposed VROPE across six video understanding benchmarks, with the number of input frames fixed to 16. As shown in Table 1, VROPE consistently outperforms both RoPE and ROPE-3D, achieving the highest average scores across all tasks and backbones.\nFor instance, in the Video-Vicuna-7B row, VROPE achieves an average score of 44.48, surpassing ROPE by 1.13 points. Similarly, when evaluated with larger backbones like Qwen2-1.5B and Qwen2-7B, VROPE demonstrates consistent improvements across all benchmarks. Notably, it outperforms RoPE and ROPE-3D by significant margins on tasks such as Video-MME (a 3.4-point increase for Qwen2-1.5B) and MLVU (a 2.94-point increase for Qwen2-7B).\nThese results highlight the superior adaptability of VROPE across different LLM types and parameter sizes. Importantly, VROPE introduces no new learnable parameters and does not increase computational complexity, making it a cost-free performance enhancement for Video-LLMs."}, {"title": "7 Limitations", "content": "We compare our method with RoPE (Su et al., 2024) and RoPE-3D (Wang et al., 2024) on the long video retrieval task to evaluate the model's generalization ability with longer video inputs. Following the setup in Video-NIAH (Zhao et al., 2024), we conduct Video Needle-In-A-Haystack (V-NIAH) experiments, where a target \"needle\" frame is inserted into a sequence of background frames, with the total frame count varying between 256 and 1216.\nAs shown in Figure 5, the retrieval accuracy of ROPE drops significantly when the number of input frames exceeds 832. While ROPE-3D demonstrates better frame extrapolation capabilities compared to ROPE, VROPE outperforms both approaches by a considerable margin. The quantitative results, presented in Table 3, further evidence this finding. Specifically, VROPE achieves an accuracy that is 32.19 points higher than RoPE and 14.22 points higher than RoPE-3D when the number of input frames increases to 1024-1216. Notably, these results are obtained even though the input frame count in this range is dozens of times greater than the maximum number seen during training. This demonstrates the exceptional extrapolation ability of VROPE."}]}