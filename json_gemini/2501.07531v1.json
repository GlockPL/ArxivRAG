{"title": "Evaluating Agent-based Program Repair at Google", "authors": ["Pat Rondon", "Renyao Wei", "Jose Cambronero", "J \u00b4 urgen Cito", "Aaron Sun", "Siddhant Sanyam", "Michele Tufano", "Satish Chandra"], "abstract": "Agent-based program repair offers to automatically resolve complex bugs end-to-end by combining the planning, tool use, and code generation abilities of modern LLMs. Recent work has explored the use of agent-based repair approaches on the popular open-source SWE-Bench [1], a collection of bugs from highly-rated GitHub Python projects. In addition, various agentic approaches such as SWE-Agent [2] have been proposed to solve bugs in this benchmark.\nThis paper explores the viability of using an agentic approach to address bugs in an enterprise context. To investigate this, we curate an evaluation set of 178 bugs drawn from Google\u2019s issue tracking system. This dataset spans both human-reported (78) and machine-reported bugs (100).\nTo establish a repair performance baseline on this benchmark, we implement Passerine, an agent similar in spirit to SWE Agent that can work within Google\u2019s development environment. We show that with 20 trajectory samples and Gemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e., plausible) for 73% of machine-reported and 25.6% of human reported bugs in our evaluation set. After manual examination, we found that 43% of machine-reported bugs and 17.9% of human-reported bugs have at least one patch that is semantically equivalent to the ground-truth patch.\nThese results establish a baseline on an industrially relevant benchmark, which as we show, contains bugs drawn from a different distribution\u2014in terms of language diversity, size, and spread of changes, etc.\u2014compared to those in the popular SWE Bench dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Automated program repair (APR) has a long history in the programming languages and software engineering research communities. In the traditional setup, the APR system is given a bug-reproducing test suite and is tasked with fixing the bug. With the rise in machine learning methods, APR systems have increasingly relied on models (initially statistical and eventually deep learning-based) to perform critical APR tasks such as fault localization, patch generation, patch ranking and eventually end-to-end repair. More recently, work such as SWE-Agent [2], AutoCodeRover [3], RepairAgent [4], CodeR [5], AutoDev [6], OpenDevin [7], and others have shown that, when incorporated into an agent-based system, LLMs can be used to perform end-to-end software engineering tasks in complex environments. Specifically, agentic repair systems can start from a bug description and autonomously generate bug-reproduction tests, localize faults, make candidate edits, validate these patches, and then submit a solution.\nWhile such autonomous workflows have generated excitement in the APR community, systems in this space have been designed and evaluated using open-source bugs found in the GitHub ecosystem. In particular, SWE-Bench [1], a collection of 2,294 Python bugs/fixes from popular GitHub repositories, and SWE-Bench-Lite, a subset of 300 bugs from SWE-Bench, have become the de-facto evaluation benchmarks for APR.\nIt is not yet clear whether systems that perform well on SWE-Bench can achieve similar success when applied in the broader software industry, where we often encounter diverse collection of bugs which span a wide array of projects. Such conditions present both an opportunity and a challenge for APR systems. Given the enormous cost and effort to maintain code in enterprise environments, if agentic APR systems can perform as well in enterprise settings as on SWE-Bench, they hold the promise for substantial impact in industry.\nTo investigate the viability of agentic repair we first had to curate a benchmark set. Handling randomly-chosen bugs from Google\u2019s internal issue tracking system (GITS) would have been a non-starter for assessing agentic APR performance, due to various reasons, some of which are similar to those also encountered by the authors of SWE-Bench in their context: for example, each bug should have an easily-executable test. Importantly, a randomly-chosen subset of bugs, while useful for population comparisons, provides little signal for agent-level design and improvements. Moreover, including bugs that go beyond the current, but not future, limitations of a basic agent (e.g., screenshots) would further cloud the informativeness of any failures.\nConsequently, we curated an evaluation set, GITS-Eval, of 178 bugs from Google\u2019s internal issue tracking system (GITS). This benchmark consists of 78 bugs reported by human developers and 100 machine-reported bugs, comprising 50 bugs reported by a suite of automated sanitizers (SAN), and 50 bugs reported by an automated test order dependency analyzer (TOD). These bugs reflect different projects and programming languages, while remaining tractable for an APR system; the criteria for filtering are discussed in Section II.\nTo place GITS-Eval into context, we also studied the differences between SWE-Bench open source bugs and GITS. We sampled 2,000 bugs from GITS with filtering that reflects similar principles to those underlying SWE-Bench and found that GITS bugs exhibit different distributional characteristics from SWE-Bench open source bugs. In particular, we have observed differences in language diversity, size and spread of changes, and the presence of code terms in the bug description as a proxy for localization difficulty. Specifically, we want to note that, due to these differences, the performance of an agent"}, {"title": "II. COLLECTING A GITS EVALUATION SET", "content": "While SWE-Agent evaluated their system on SWE-Bench, a collection of open-source repository bugs and their associated fixes, an agent in the Google internal environment would face bugs of a different nature as a result of the environment\u2019s idiosyncrasies. To mention a few differences, Google uses a multilingual monorepo [10], with projects often spanning different portions of the repository, designed to build and run with Google-specific infrastructure (e.g., Bazel [11]), and with varying levels of domain-specific knowledge required to successfully develop in them. These differences have deep implications for not only writing, running, and testing code, but even for how the agent itself interacts with the Google internal environment (e.g. logging, which is sensitive business data and must be stored accordingly).\nGITS, Google\u2019s internal issue tracking system, houses a vast and diverse collection of bugs spanning a wide array of projects. This presents both an opportunity and a challenge for automated program repair (APR) systems. Opportunistically, randomly sampling bugs from GITS may seem appealing, as in principle the full database and constant stream of bugs could benefit from automated repair. However, properly selecting sensible bugs is challenging, as ad-hoc random sampling of bugs does not allow for repeatable progress measurement nor does it provide an informative signal for the potential of APR.\nTo effectively leverage this resource, we employ a multi stage filtering funnel to curate a focused set of actionable bugs. This funnel ensures that the bugs presented to Passerine are both relevant to its capabilities and representative of real-world challenges within Google\u2019s codebase. The filtering process comprises four distinct phases, as shown in Figure 1."}, {"title": "A. Phase 0: Fixed Bugs Population", "content": "This initial phase casts the widest net, aiming to define the broadest possible scope of relevant bugs. We apply minimal filtering criteria (Table I), primarily to ensure accessibility and a clear association between bug reports and their cor responding fixes. This results in a large and diverse initial population of fixed bugs within a specific timeframe, serving as the foundation for further refinement."}, {"title": "B. Phase 1: Bugs Where We Can Determine if a Fix is Plausible", "content": "Phase 1 refines the selection by focusing on bugs that agentic APR could conceptually address, even if not currently supported by the agent\u2019s implementation, and for which we can assess whether the agent generated a plausible fix as part of the evaluation. This necessitates identifying bugs with testable code changes and verifiable fixes. A key aspect of this phase is the separation of human-reported bugs from machine-reported bugs. This distinction allows for tailored filtering criteria based on the nature of the bug report and the availability of information regarding the fix (Table II).\nWe focus on two types of machine-reported bugs (Figure 2): those surfaced by the SAN and TOD systems. SAN performs a variety of sanitizer-based analyses, including memory and thread-related sanitizer reporting, which capture errors such as out-of-bounds accesses, uninitialized values, data races, and more. Meanwhile, TOD automatically identifies test order dependence."}, {"title": "C. Phase 2: Automated Curation", "content": "Phase 2 (Table III) shifts the focus to practical considera tions for evaluating our agent\u2019s current capabilities. Automated filters are applied to exclude bugs that would pose challenges for evaluation, such as those with long-running integration tests or requiring multi-modal understanding (e.g., screenshots in bug descriptions).\nWe limit the size of the patch in this phase to be less than 150 lines of code. This number represents the 90th percentile of bug fix patch sizes on a broad set of internal patches, ensuring that we are addressing a wide range of potential bugs."}, {"title": "D. Phase 3: Heuristic Curation", "content": "To begin phase 3, we sample bugs from our phase 2 population and curate them incrementally. Specifically, we execute any associated test with the bug and ensure that there are appropriate failures before the ground-truth patch, which are then resolved after the ground-truth patch is applied. In addition, to ensure consistent reproducibility, we remove bugs that exhibit flaky behavior.\nFinally, we introduce a layer of human expertise to ensure the quality and relevance of the benchmark set. We conduct manual reviews against a rubric to identify and filter out bugs that are not suited to automatic, execution-based evaluation, such as those relying on \u201cmagic constants\u201d that cannot cur rently be easily captured by deterministic automated filters. We define \u201cmagic constants\u201d as either literal values (typically strings) or newly introduced code literals (such as method, class, or enum names) that appear in the updated test case but are not present in the original source code. Furthermore, these constants are not readily derivable from the bug report or any other existing code element. This characteristic implies that the successful execution of the bug confirmation test hinges on the precise value of this symbol or constant. However, a valid fix for the underlying bug may not necessitate the exact naming or literal value present in the ground-truth test. To mitigate the potential for errors in this labeling process, at least one of the authors manually labeled each data point as containing a \u201cmagic constant change\u201d or not, based on the definition provided above. In cases where the initial annotator was uncertain, a second author reviewed the data point to resolve any ambiguity. A data point was only labeled as containing a \u201dmagic constant change\u201d when both annotators concurred. This negotiated agreement approach helped ensure the reliability of our manual labeling process.\nFor the future, we plan to explore automating these heuris tics (e.g., through use of few-shot learning given our currently manually labeled set of examples).\nThis multi-stage filtering process ensures that the resulting GITS evaluation set (GITS-Eval) is both representative of real world bugs within Google\u2019s codebase and suitable for evalu ating the capabilities and potential of agent-based APR. Our final evaluation set comprises 178 bugs: 78 human-reported bugs, and 100 machine-reported bugs, of which 50 come from automated sanitizers (SAN) and 50 from an automated test order dependency analyzer (TOD)."}, {"title": "E. GITS vs SWE-Bench Bugs", "content": "Understanding the nature of our evaluation set is crucial for interpreting the results of our analysis. To provide context, we compare the distribution of GITS bugs with those in the GitHub-derived SWE-Bench dataset, highlighting important differences in their distributions. While SWE-Bench does not reflect the entirety of GitHub, it is a widely-used benchmark which sets the standard for evaluating automatic program repair systems. Thus, to create a comparable test set for GITS, we draw a random sample of 2,000 bug-fixing patches from Phase 1 of our bug filtering phases which comprise both human- and machine-reported bugs for which we can determine a plausible fix. We compare SWE-Bench to Phase 1 bugs, rather than later manually-curated bugs, to capture intrinsic differences in bug distribution without confounding this comparison as a result of Passerine\u2019s current limitations (e.g. no multimedia, no flaky tests) or infrastructure challenges (e.g. long running tests).This is roughly in line with the methodology used for curation of the SWE-Bench dataset.\nWe will compare distribution differences along dimensions that correlate with localization difficulty and editing difficulty to highlight the unique challenges Google internal bugs need to address.\n1) Localization: Before a candidate patch can be generated, a fault must be localized to identify what program statements are the root cause for the bug and so should be modified to produce a valid fix. While the difficulty of localization can be influenced by many factors, we consider two aspects: searchability and spatial distribution of changes.\nCode search typically plays a substantial role in localization, as it allows a user to navigate large and potentially unfamiliar codebases [12]. Often, code search starts by identifying terms that are relevant to the bug and that may appear in the underlying codebase. To emulate this process, we consider the frequency of terms in a bug issue description that are likely to be codebase symbols, such as class names. We extract these terms using a simple regex-based heuristic, where any term that matches either snake_case or CamelCase identifiers is considered a code term. Figure 3a shows an empirical cumulative distribution function (ECDF) of the number of possible code terms in the associated bug descriptions. We find that GITS bugs have fewer possible code terms in their descriptions. Only 18% of GITS bugs have at least 2 possible code terms, compared to approximately 60% in SWE-Bench.\nNext, we consider the extent to which fixes are spatially related. We first compare the number of different files modified by a patch, as well as the number of hunks resulting from the segmentation of those changes [13]. Finally, we also consider patch spread, defined as the number of lines of separation between continuous hunks [14], as a measure of how much a patch is dispersed throughout a file or across multiple files. We compute the number of unmodified lines between consecutive changes for each affected file and sum these to yield the patch spread. A higher patch spread suggests a greater degree of interleaving, where modified lines are scattered throughout the file. Conversely, a lower spread indicates that modifications are clustered together in contiguous blocks.\nFigures 3b and 3c show that GITS patches modify more files (up to twice as many), and that these modifications result in much larger hunk counts, respectively. When we measure patch spread (Figure 3c), we again find that patches are more widely separated within files than those in SWE-Bench.\n2) Editing: Once a fault location is identified, the associated statements must be edited to produce a fix. Intuitively, larger edits (meaning more lines of code) can pose a chal lenge as they introduce more opportunities for mistakes. We measured the number of lines changed in ground-truth patches. As shown in Figure 3e, almost all patches from SWE-Bench are under 100 lines while only approximately 40% of GITS patches are under 100 lines.\nThis complexity is further compounded by the need to con sider the specific syntax and semantics of different program ming languages. To better understand the language distribution we identified the top 5 most frequent file extensions in our sample of GITS patches (SWE-Bench only considers Python files): Java, C++, TypeScript, Kotlin, and Python.\nGITS vs SWE-Bench. Our analysis shows that GITS bugs present unique challenges compared to SWE-Bench, par ticularly in terms of localization and code modification. GITS and SWE-Bench differ slightly in the amount of code related terms in their descriptions. Additionally, patches for GITS bugs differ in nature when it comes to changes across files, the number of modified lines, and dispersion of the modifications (i.e., patch spread)."}, {"title": "F. GITS-Eval vs. SWE-Bench-Lite", "content": "We previously introduced GITS-Eval, a more tractable, curated subset of 178 bugs, which we use for evaluation of our agentic repair system. In an open source context, most state-of-the-art agent-based APR approaches are evaluated on SWE-Bench-Lite, a more practical and self-contained bench mark derived from SWE-Bench. GITS-Eval is the analogous dataset in our context. Recognizing the potential for variations between human-reported and machine-reported bugs, we have categorized the bugs within GITS-Eval accordingly.\nFigure 4 compares SWE-Bench-Lite, human, and machine reported bugs in GITS-Eval. We find that machine-reported bugs in GITS-Eval are comparable to bugs in SWE-Bench Lite. Meanwhile, human-reported GITS-Eval bugs display in creased complexity compared to SWE-Bench-Lite, resembling the differences observed in our earlier comparison of GITS and SWE-Bench."}, {"title": "III. PASSERINE: AN AGENT-BASED REPAIR SYSTEM", "content": "To establish a repair performance floor for agent-based APR on GITS-Eval, we developed an APR agent, Passerine, inspired by SWE-Agent. Like SWE-Agent, Passerine uses a ReAct [15] loop to iteratively produce \u201cthoughts,\u201d run commands against the current state of the workspace, and observe the commands\u2019 results; the output of the agent is a modified workspace in which, if the agent is successful, the bug is fixed, as well as a trace of the agent\u2019s execution, for human interpretation and debugging. Similarly to SWE Agent, we expose commands to the agent that are suited both to the APR task and designed with the ergonomics of the \u201cagent-computer interface\u201d in mind; these commands include both workalikes of the SWE-Agent commands (file viewing, file editing, and terminating the agent), as well as commands that provide functionality specific to Google\u2019s development environment (compiling and running tests, searching the index of Google\u2019s monorepo).\nUnlike SWE-Agent, we find that we do not need to ex pose a full Linux command line to the agent, e.g., via containerization. Google\u2019s test-running infrastructure already provides sufficient containerization [10]. Our experience, as well as prior work in agent-based APR like RepairAgent [4] and AutoCodeRover [3], shows that a small, APR-focused command set is sufficient for bug-fixing.\nPasserine is intentionally simple and minimal, avoiding complex architecture or explicitly dividing the APR process into discrete phases like localization, editing, testing, and so forth. This simple approach is validated by the effectiveness of agents without prespecified control flow like SWE-Agent [2], CodeAct [16], and OpenDevin [7]. We show, through the first evaluation of a simple SWE-Agent-like APR system in an industrial setting, that, in spite of its minimalism, our agent is capable of addressing many real-world bugs."}, {"title": "A. Agent Design", "content": "Passerine is a fully \u201cdynamic\u201d agent in the sense that it has no prespecified control flow. Every step in Passerine corresponds to a ReAct [15] style step, depicted in Figure 5, where the agent issues an LLM prompt asking for the next command to perform and obtains back a response in the form of \u201cthought\u201d and \u201caction.\u201d The thought is a natural language fragment reasoning about the state of the agent and describing the next step. The action is a code fragment consisting of a Unix-like tool name, associated arguments for its call, and, for some commands (e.g., text editing), multiline input text. The agent framework then parses that command, executes it, and replies to the agent with any observable outcomes.\nLLM Prompting and History: When querying the LLM to obtain a new step, the agent includes information on the current state of the environment. Currently, Passerine includes the entire history of the agent. This has the benefit of simplicity and is facilitated by longer-context models. However, as we describe in Section VI-A, we can still encounter agent runs that exceed LLM context limits and thus we could benefit from more-sophisticated history management strategies.\nAgent Termination: Passerine can call a finish [success|failure] command to terminate, along with the agent\u2019s judgment of the outcome. To avoid non-termination, we also enforce a maximum step limit.\nIsolation: Because the agent makes stateful changes to the en vironment, we rely on Google\u2019s codebase containerization [10] to enforce isolation between different agent executions. We describe more of the infrastructure associated with running the agent in Section III-C."}, {"title": "B. Commands", "content": "Passerine takes a minimalist approach by providing a set of only 5 commands (and 1 additional alias commonly referenced in bug descriptions) that the agent can use, which are designed to interact with Google internal APIs. A direct benefit of using a limited command set, restricted only to a few operations that can be implemented on top of already-containerized APIs, is that Passerine does not require a full virtual machine for isolation, improving scalability.\nCommands in our design have a uniform input and output interface. For inputs, commands accept zero or more positional arguments, followed by arbitrary text fragments after a newline (e.g. to support editing). Every command output consists of an exit code and output text. The output text is the observable behavior that is exposed to the agent in its history.\nWe now briefly describe each command shown on the right hand side of Figure 5.\n\u2022 cat: Takes a single file in the workspace and lists its contents using internal APIs. Every line is prefixed with a line number.\n\u2022 code search: Takes one or more terms and concatenates these to issue a query for the Google internal Code Search API [12].\n\u2022 edit: Takes a filename, starting and ending lines, and the replacement text with which to substitute the identified line region. When the edit is applied, the command\u2019s output text corresponds to the modified content (along with up to 3 context lines around it), where lines have been prefixed with the resulting line numbering post-edit. Edits are done through an internal API, which interacts with the isolated codebase container.\n\u2022 bazel: Takes a test suite target, along with arbitrary parameters for test suite execution, and issues a dis tributed build/execution [17] for the associated target using Google\u2019s internal build tool Bazel [18]. The frame work then parses the build report, extracts outcomes and relevant log excerpts, and surfaces these as an observable output for the agent.\n\u2022 finish: Takes as argument \u201csuccess\u201d or \u201cfailure\u201d and terminates agent execution.\nAll commands are implemented as simple Python functions, which easily support incorporating command validation and logging logic. We present each command to Passerine in its initial prompt, providing a high-level description of the behavior, along with fixed basic examples showing how each command can be used."}, {"title": "C. Evaluation Framework for Historic Bugs", "content": "As part of developing Passerine, we also implement a frame work which allows us to evaluate different agent configurations on Google infrastructure and analyze their performance, all in the context of historical bugs that have already fixed.\nSetup: Before evaluating an agent, the framework sets up the environment by 1) loading the appropriate bug information and 2) checking out the repository state (also known as a changelist, which is the snapshot that undergoes code review1 ) prior to the associated ground-truth fix. Then the framework reproduces the bug in this environment, to confirm that the setup correctly exposes the issue as expected. To do so, our framework allows benchmark repair tasks to specify the expected test targets to run and any files that are relevant to that state, and may need to be copied over from the ground-truth patch. In practice, for machine-reported bugs, our framework automatically extracts bug-reproducing tests from the associated issue content using regular expressions. Note that, for TOD tests, the associated test specifies a single ordering of the test cases which exhibits the order dependency. For human-reported bugs, our framework has an offline pass that uses Google tooling to identify test targets that depend on the test files modified by the patch, considers these as candidate bug reproducing tests, and prunes the set down to"}, {"title": "IV. EVALUATING AGENT-GENERATED REPAIRS", "content": "We evaluate Passerine on GITS-Eval considering dimen sions of overall patch correctness and trajectory dynamics.\nPatch Correctness Consistent with prior APR work, we distinguish between plausible and valid patches. In our work, a plausible patch is defined as patch that enables successful execution of the bug-reproducing test suite associated with the repair task. A valid patch [19]\u2013[21], in turn, is defined as patch that implements a fix that is semantically equivalent to the one in the ground-truth patch [22], [23]. In cases where ground-truth patches include additional actions like adding or modifying tests, we focus solely on whether the plausible patch addresses the bug-fixing logic. (Generating regression tests is beyond the scope of this evaluation.) We determine this correctness through manual analysis performed by three of the authors. During the process, one reviewer analyzed each patch, consulting with the other two authors for assessment of complex cases. In a practical deployment of Passerine, patches could additionally be tested using Google\u2019s internal continuous integration platform, which runs extensive project specific tests. However, for our evaluation we focus on manual grading of validity, as project-specific tests, like all testing, are generally insufficient for establishing or rejecting equivalence. Note that we do not evaluate the textual similarity of the generated patches to the ground truths.\nTrajectory Dynamics We analyze and compare Passerine\u2019s repair trajectories across several dimensions:\n\u2022 Trajectory Strategies: We investigate the types and se quences of commands that Passerine employs during the repair process. This helps us understand how the agent approaches different bug types and whether there are distinct patterns in its actions.\n\u2022 Localization: We examine how effectively Passerine pin points the correct file(s) to modify by measuring the filesystem distance between the files modified by Passer ine and the actual location of the ground truth bug fix.\n\u2022 Trajectory Smells: We analyze the presence of potentially suboptimal or unusual patterns within the trajectories, which we refer to as \u201ctrajectory smells,\u201d inspired by code smells [24]. We consider four specific \u201csmells\u201d: (i) NO_TEST_SMELL: trajectory does not include any test execution commands (i.e., agent never confirmed the bug nor tested the patch); (ii) NO_OP_CAT_SMELL: trajec tory contains instances where a file is re-read without any intervening modifications to that file (i.e., agent made an unnecessary read); (iii) CONSECUTIVE_SEARCH: trajectory contains at least three code search commands in a sequence (i.e., agent is repeatedly searching); (iv) CONSECUTIVE_EDITS: trajectory contains at least 3 edits to the same file in a sequence (i.e., agent is repeat edly editing the same file);"}, {"title": "V. RESULTS", "content": "We present the results on our evaluation set of 178 GITS Eval bugs, comprising 50 TOD bugs, 50 SAN bugs, and 78 human reported bugs. These bugs are drawn from the Phase III bugs (see Section II) and reflect real Google bugs that are not explicitly ruled to be outside of the scope of Passerine capabilities (e.g., require multimedia) .\nFor all our experiments, we use Gemini 1.5 Pro (gemini 1.5-pro-001) [25] as the LLM underlying Passerine. We use Gemini out-of-the-box without any additional fine tuning. LLM calls are performed with temperature = 0.2, and top p = 0.95, and we sample the most-likely completion. Like many generate-and-validate APR approaches [26], we perform sampling: the agent is run for 20 (independent) trajectories, where each trajectory proceeds for a maximum of 25 steps. Code search results are limited to 5 file matches, where each file match can contain several matching code snippets. As discussed in Section III-B, Passerine uses the internal code search, build/test, and filesystem tools available within Google."}, {"title": "A. Patch Correctness", "content": "A critical measure of Passerine performance is its ability to produce both plausible and valid patches. We summarize our findings in Table IV. We find that Passerine can effectively generate at least one valid patch for a substantial fraction of both machine-reported and human-reported bugs. LLM cost has not yet been optimized; we leave this to future work.\nFigure 7 provides further detail on patch plausibility and validity as a function of samples. We observe that the gap between plausibility and validity varies by bug type, as a result of nuances in their testing behavior. TOD bugs, which have the largest gap, are judged to be plausible if the test order dependence is removed under the original reproduction circumstances, which specify a single ordering of tests that exhibit test order dependency; however, this criterion may be too lenient, and we may filter more solutions by trying additional test orderings. For human bugs we find that the opposite is true: the gap between plausible and valid patch rates is small. We observe that for these cases the plausibility criterion\u2014the test contained in the ground-truth commit\u2019s test file(s), which the agent does not have access to\u2014is a strict criterion."}, {"title": "Agent-based APR for Google bugs.", "content": "Our experiments show that Passerine can tackle bugs in Google\u2019s enterprise-scale setting, producing plausible and valid patches for both human-reported and machine-reported bugs."}, {"title": "B. Observations", "content": "We now share three key observations from our experiments.\nAgent strategies As described in Section III, we do not provide any high-level guidance (or restriction) on what com mands Passerine can issue or in what order. As a result of this freedom, we observe that Passerine can adopt different strategies for different kinds of bug reports.\nFigure 8 shows the frequency of commands by step index in a trajectory, grouped by bug type. We find that early steps in human-reported bugs are typically dominated by localization style operations like code_search and cat. In contrast, for machine-reported bugs, which, as shown in Figure 2, include bug reproduction information and guidance on the kind of error, we find that Passerine often starts by running bazel (the test building and running command). Later steps in machine reported bugs switch to include more edit commands. Thus, the agent is effectively able to enter a de facto localization when most needed (for human-filed bugs), without explicitly being constrained to do so, while it can also take other actions (running tests, for machine-filed bugs) if they might yield more useful information.\nAdditionally, we note that for SAN bugs, the first agent step very often includes an invalid, but unnecessary, command (not in our API) as a result of the content in the bug description, which assumes access to all standard Google tooling compared to our restricted set. However, Passerine adapts quickly and recovers in step 2, where there are no further such commands.\nAgent strategy. Despite the lack of high-level strategy guid ance (e.g. there is no state machine restricting commands or per-bug-type prompt in our implementation), Passerine adapts its strategy across bug types. This same flexibility allows it to recover from incorrect commands.\nDesigning agent-informative bug reports To characterize Passerine\u2019s progress on cases where the system does not produce a plausible patch, we consider whether the agent at least identifies the correct files to edit. Specifically, we compute the file-system distance between the files edited by the agent and the ground-truth correct file locations, where file-system distance is defined as the distance between two nodes (files) in an n-ary tree (file system) .\nFigure 9 shows that 53.8% of machine-reported bug tra jectories without a plausible patch edited the correct file, compared to only 3.5% of human-reported bug trajectories without a plausible patch. So while Passerine cannot produce a plausible patch, it does a better job at file-level localization for machine-reported bugs, which have rich bug reports (see Figure 2) that include description, reproduction information, and test expectations.\nThe fact that an agent like Passerine can exploit this information in bug reports, combined with the expectation that agents will become an increasingly-common part of developer workflows, leads us to highlight implications for researchers and practitioners designing bug reporting platforms. Incor porating nudges for human reporters to enrich reports with the type of information found in machine-reported bugs (e.g. bug reproduction guidance) can be a powerful design tool to increase the effectiveness of agent-based APR, allowing developers to focus their time on bugs that are truly out of reach for current agent approaches.\nBug reporting. Passerine can produce higher fix rates (or if it fails, can at least localize the fault to the right file more often) when given richer bug reports. As agent-based repair continues to gain traction, the community should consider what nudges can be provided to human bug reporters to increase the richness of their issues and put them within reach of existing agent capabilities.\nTrajectory analysis Performing granular inspection of Passer ine trajectories revealed interesting opportunities for additional optimization. One such opportunity consists of identifying (and pruning) degenerate agent trajectories, which display clear misbehaviors. Table V shows the incidence of two basic properties that signal such degenerate behavior. Trajectories without testing (NO_TEST_SMELL) suggest the agent does not know where to start or how to confirm fixes \u2013 this is observed in human bugs. Interestingly, we also find that more subtle behaviors, such reading a file that is already in the agent\u2019s context (NO_OP_CAT_SMELL), correlates with failures. Furthermore, we note that some trajectory dynamics can reflect bug type and difficulty. Table V shows that human bugs are more likely to perform repeated consecutive searches (CONSECUTIVE_SEARCH), likely due to the lack of informa tion contributing to localization challenges as discussed previ ously. Both SAN and TOD trajectories, in contrast, are easier to localize and the agent is more likely to perform repeated edits on the same file (CONSECUTIVE_EDIT). As might be expected, both of these smells correlate with bug difficulty and so we observe higher incidence in failing trajectories compared to trajectories that generate a plausible patch.\nTrajectory analysis. Analyzing agent trajectories in detail, such as identifying repeated misbehaviors, can expose oppor tunities for optimizations, some of which, with little effort, could lower reviewer burden."}, {"title": "VI. DISCUSSION", "content": "We now discuss ongoing challenges, possible mitigations, and some deployment considerations, all staying within the general framework that we established for Passerine."}, {"title": "A. Challenges and Possible Mitigations", "content": "Context size: Due to verbose outputs (e.g. test logs", "reproduction": "Machine-reported bugs", "tools": "When we inspect agent responses that result in a tool usage failure", "print(Please provide more context about the issue with [...] What type of artifacts are missing?": "Similarly", "i should try to access it": "which we do not support). We plan to extend Passerine\u2019s command set based on this observed behavior.\nFault localization: We found that for human-reported bugs, fault localization can be challenging \u2013 as reflected by low rates of successful file-level localization"}]}