{"title": "Enhancing Environmental Monitoring through Multispectral Imaging: The WasteMS Dataset for Semantic Segmentation of Lakeside Waste", "authors": ["Qinfeng Zhu", "Ningxin Weng", "Lei Fan", "Yuanzhi Cai"], "abstract": "Environmental monitoring of lakeside green areas is crucial for environmental protection. Compared to manual inspections, computer vision technologies offer a more efficient solution when deployed on-site. Multispectral imaging provides diverse information about objects under different spectrums, aiding in the differentiation between waste and lakeside lawn environments. This study introduces WasteMS, the first multispectral dataset established for the semantic segmentation of lakeside waste. WasteMS includes a diverse range of waste types in lawn environments, captured under various lighting conditions. We implemented a rigorous annotation process to label waste in images. Representative semantic segmentation frameworks were used to evaluate segmentation accuracy using WasteMS. Challenges encountered when using WasteMS for segmenting waste on lakeside lawns were discussed. The WasteMS dataset is available at https://github.com/zhuqinfeng1999/WasteMS.", "sections": [{"title": "1 Introduction", "content": "Compared to Red-Green-Blue (RGB) imaging methods, multispectral imaging captures reflections from the real world in multiple specific bands, providing richer information than visible light alone [1]. Multispectral imaging is vital for remote sensing, agriculture, and environmental monitoring, as the infrared band offers feature perception capabilities that visible light cannot [2]. By analyzing specific infrared bands, multispectral imaging can reveal material composition, vegetation health, and other environmental parameters. Thus, in complex outdoor environments with varying lighting conditions, multispectral information may enhance the perception capabilities of a model in remote sensing tasks. As such, this approach has been widely adopted in remote sensing. Currently, most remote sensing satellites are equipped with both RGB and multispectral or hyperspectral imaging cameras, and numerous studies have demonstrated the importance of multispectral imaging analysis [3]."}, {"title": "2 The WasteMS Dataset", "content": "The WasteMS dataset consists of a total of 117 nine-channel multispectral images with a resolution of 682\u00d7682 pixels, along with their semantic segmentation annotations. We divided the dataset into training, validation, and test sets in an approximate ratio of 7:1:2 for standard deep learning training and evaluation of semantic segmentation. The"}, {"title": "2.1 Data Collection", "content": "We used the CMS4 multispectral camera for data collection, which covers a wavelength range that includes the visible red-light region as well as a rich near-infrared region. This wavelength range is well-suited for environmental monitoring and vegetation analysis [23]. Specifically, the CMS4 multispectral camera can capture information across 9 bands, including 8 narrowband color filters and one black-and-white filter. The central wavelengths ($\\lambda$) of the 8 bands are 653nm, 695nm, 731nm, 772nm, 809nm, 851nm, 886nm, and 929nm, with maximum transmittance ($T_{\\text{max}}$) ranging from 50% to 60%. Additionally, the camera includes a neutral density filter (Band 9), which operates over the spectral range of 500 to 1000nm, with an average transmittance ($T_{\\text{mean}}$) of 12%. Transmittance is an important factor in multispectral cameras, as it determines the amount of light at specific wavelengths. Higher transmittance means capturing more light. To clearly demonstrate the characteristics of the multispectral data we collected, the relationship between transmittance and the nine bands is illustrated in the Fig. 2. Each macro-pixel of the camera consists of a 3\u00d73 pixel matrix, representing the 9 bands, resulting in an original data resolution of 2048\u00d72048 pixels. After preprocessing, the 9 channels data can be acquired with a resolution of 682\u00d7682 pixels."}, {"title": "2.2 Annotation", "content": "The annotation was carried out by multiple trained persons. During annotation, we fused the information from the 9 channels to create pseudo-color images and used annotation software Label Studio to annotate these pseudo-color images, as shown in Figure 3(a). To ensure the accuracy of the annotation outlines, we first used the Segment Anything Model (SAM) [24] to assist with the annotation. By inputting rectangular prompts, SAM's ViT pretrained model would infer and generate masks. In the second step, we manually adjusted these masks at the pixel level to ensure the precision of the mask edges."}, {"title": "3 Benchmarks", "content": ""}, {"title": "3.1 Representative Baselines", "content": "We used several representative semantic segmentation methods to benchmark the WasteMS dataset. These methods adopt an encoder-decoder architecture [25], where the encoder benefits from pre-training on large image classification datasets like ImageNet [26, 27], enhancing its feature extraction capabilities.\nFor the encoder, we employed ResNet [28], ConvNeXt [29], and Swin Transformer[30]. ResNet introduces residual learning into the network to avoid vanishing and exploding gradient problems, allowing deeper networks to be effectively trained. ConvNeXt also utilizes the concept of residual connections but makes more significant internal structural improvements, such as using larger convolutional kernels, simplified normalization, and activation processes. This gives it a more powerful ability to handle image details, making it particularly effective in fine-grained semantic segmentation tasks. Swin Transformer, built on the hierarchical Vision Transformer, introduces shifted windows, which divides the image into multiple small patches and applies self-attention within these patches individually. This approach improves processing efficiency while capturing global dependencies across windows, making it better suited for handling large variations in waste scales at lakesides.\nFor the decoder, we employed Fully Convolutional Networks (FCN) [31], Pyramid Scene Parsing Network (PSPNet) [32], DeepLabV3+ [33, 34], and UperNet [35]. FCN is a pioneer in semantic segmentation tasks, replacing traditional fully connected layers with convolutional layers, allowing the network to accept input images of any size and"}, {"title": "3.2 Experimental Settings", "content": "When combining encoders and decoders, we used the combinations of Swin Transformer with UperNet, ConvNeXt with UperNet, ResNet with DeepLabV3+, ResNet with PSPNet, ResNet with UperNet, and ResNet with FCN as baselines to benchmark WasteMS. The experimental settings for these combinations are shown in Table 1.\nAmong these, the combinations of ConvNeXt with UperNet and Swin Transformer with UperNet used the AdamW optimizer with a 1500-iteration warm-up strategy, while the other methods used the SGD optimizer without warm-up. The learning rates were chosen based on extensive experimentation to find the optimal values. Given the limited size of the WasteMS dataset, data augmentation was employed to effectively enhance generalization ability [36]. The augmentation strategies included random resize, random crop, random flip, and random rotate. All experiments were conducted using two RTX 4090D GPUs."}, {"title": "3.3 Benchmarking Results", "content": "Table 2 presents the performance of various representative semantic segmentation frameworks on the WasteMS test set, including fine-tuning results after pre-training on ImageNet1k and fully supervised training results. We used the Intersection over Union (IoU) metric to evaluate the segmentation results. Additionally, we calculated the number of parameters (Parms) and the computational load (FLOPs) for each semantic segmentation network using a single 512\u00d7512 resolution, nine-channel multispectral image."}, {"title": "4 Challenges and Outlook", "content": ""}, {"title": "4.1 Small Targets", "content": "In WasteMS, there is a significant variation in the scale of waste, with many small targets [37] present. During annotation, we referenced high-resolution camera images to ensure that very small objects, such as cigarette butts and small pieces of paper, were annotated, even though they constitute a very small proportion in the multispectral images. Although existing semantic segmentation networks can effectively segment larger waste items, such as plastic bottles and cans, they still face challenges with small objects like cigarette butts and small pieces of paper, often resulting in false negatives. In the future, it is important to develop networks that can effectively perceive these small objects, with a focus on more efficient fusion and feature extraction of multispectral information."}, {"title": "4.2 Pre-training Generalization", "content": "Pre-training and fine-tuning is an effective paradigm in semantic segmentation tasks [38], widely applied in remote sensing, medical imaging, and autonomous driving. However, as shown in Table 2, the improvement in performance on WasteMS from"}, {"title": "4.3 Channel Selection", "content": "WasteMS has 9 data channels, and an excessive number of channels may lead to data redundancy, increasing computational load and affecting segmentation accuracy. Thus, in future multispectral semantic segmentation networks, it is meaningful to explore channel selection strategies [39]. This can also improve the interpretability of multispectral data understanding, clearly quantifying the importance of each channel."}, {"title": "4.4 Limited Samples", "content": "Due to the limited scenarios of lakeside waste and the high cost of data collection and annotation, the number of WasteMS data samples is limited. Therefore, exploring data augmentation strategies for multispectral images in the future is worthwhile. Image generation technologies based on Generative Adversarial Networks (GANs) [40] and diffusion models [41] have been widely applied in recent years and have shown good results in augmenting image [42] and point cloud data [36]. However, research on data augmentation for multispectral images based on generative models is still limited and deserves further exploration. Additionally, developing more effective small-sample learning networks for multispectral data is a future research direction."}, {"title": "5 Conclusion", "content": "In this study, we introduced WasteMS, the first multispectral dataset tailored for semantic segmentation of lakeside waste, aiming at enhancing environmental monitoring through the utilization of multispectral imaging. The dataset encompasses a wide range of waste types under various lighting conditions, and has been carefully annotated to ensure high annotation accuracy. We conducted extensive testing using representative semantic segmentation networks, providing benchmark performance. We discussed in detail the challenges presented by the WasteMS dataset, including small objects, pre-training generalization, channel selection, and small sample size, and proposed future work to address these challenges."}]}