{"title": "Pruning Foundation Models for High Accuracy without Retraining", "authors": ["Pu Zhao", "Fei Sun", "Xuan Shen", "Pinrui Yu", "Zhenglun Kong", "Yanzhi Wang", "Xue Lin"], "abstract": "Despite the superior performance, it is challenging to deploy foundation models or large language models (LLMs) due to their massive parameters and computations. While pruning is a promising technique to reduce model size and accelerate the inference, the traditional pruning techniques can hardly be applied for LLMs as they need to finetune the model on the full dataset with multiple epochs consuming massive data and hardware resources. To deal with this problem, post-training pruning methods are proposed to prune LLMs in one-shot without retraining. However, their accuracy after pruning may suffer from certain performance degradation due to the lack of retraining with massive data. To address this issue, in this paper, we first formulate the post-training problem for layer-wise LLM compression to simultaneously prune multiple weights in LLMs. Next, we provide an optimal solution for this problem and design our post-training pruning algorithm for both unstructured and semi-structured sparsity. Our extensive experiments demonstrate the superior performance of the proposed methods in comparison to SOTA baselines across various LLM families including transformer-based LLMs and Mamba-based LLMs.", "sections": [{"title": "Introduction", "content": "Foundation models or large language models (LLMs) have achieved remarkable performance on a variety of tasks. However, it is challenging to deploy LLMs in practical applications due to their massive parameters and computations. To facilitate LLM deployment in practice, various model compression techniques targeting LLMs including pruning (Hubara et al., 2021b; Frantar and Alistarh, 2023) and quantization (Dettmers et al., 2022; Frantar et al., 2022; Yao et al., 2022; Xiao et al., 2023) have been proposed to reduce memory and computation costs.\n\nThe traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar and Alistarh, 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023; Zhang et al., 2024), achieving accuracy close to SparseGPT.\n\nHowever, the performance of SparseGPT is still sub-optimal as it relies on the solution of Single Removal Problem (SRP) (Singh and Alistarh, 2020; Frantar et al., 2021) to address the pruning of multiple weights, which is essentially a Multiple Removal Problem (MRP). In particular, the SRP provides the optimal solution to prune one weight at a time and modify all other weights to compensate the pruned single weight and minimize the loss. However, as the optimal solution is unaware of all previous pruned weights and requires to modify all other weights including previous pruned ones (making them unpruned again), it is at odds with the MRP for multiple pruned weights. Thus, the optimal solution in SRP can not be directly applied to solve MRP. To successfully incorporate the SRP solution, a series of approximation methods are adopted in SparseGPT, at the cost of certain performance degradation, as detailed in Sec. 2.3.\n\nDifferent from the SRP-based SparseGPT, we directly formulate the MRP for layer-wise LLM"}, {"title": "Background", "content": "2.1 Post-Training Pruning\n\nThe post-training pruning problem can be typically formulated as the following,\n\\begin{equation} \n\\begin{aligned} \n\\min_{\\delta \\omega} L'(\\mathbf{w} + \\delta \\omega) - L'(\\mathbf{w}), \\\\\ns.t. (\\mathbf{w} + \\delta \\omega) \\odot \\mathbf{M} = 0, \n\\end{aligned} \n\\tag{1} \n\\end{equation}\n\nwhere $\\mathbf{w}$ is the original model weights, $\\delta \\omega$ is the modifications of model weights and $\\mathbf{M}$ is the binary pruning mask on model weights with 1 denotes pruning. $L'(\\mathbf{w})$ is the typical training loss. The problem minimizes the difference of the loss before and after pruning, by optimizing the unpruned weights, with the constraint that the pruned weights following the mask should be zero.\n\n2.2 Single Removal Problem\n\nThe single removal problem (SRP) is investigated in many works (Singh and Alistarh, 2020; Frantar"}, {"title": "Limitations of SRP", "content": "Following SRP, SparseGPT (Frantar and Alistarh, 2023) applies the SRP solution for each linear layer in LLMs with $H = 2\\mathbf{x}\\mathbf{x}^T$. However, the assumption with zero Jacobian does not hold in this layerwise pruning setting with a local quadratic loss $L'(\\mathbf{w}) = ||\\mathbf{w}\\mathbf{x}||^2$, since we can directly obtain the Jacobian $\\nabla L' = 2\\mathbf{w}\\mathbf{x}\\mathbf{x}^T$ which is non-zero. The assumption may be unreasonable here.\n\n2.3.2 Sequential Weight Freezing\n\nAnother difficulty with the SRP solution is its unawareness of all other pruned weights during pruning. Specifically, when compensating the loss of a single pruned weight, its optimal solution requires to modify all other weights including all previous pruned weights, making them unpruned again and violating the pre-defined sparsity requirement.\n\nThus, the SRP solution is not able to directly prune multiple weights. To address this problem, SparseGPT applies a series of techniques such as Optimal Partial Updates and Hessian Synchronization. The key idea is to sequentially prune weights in the same row, and freeze/fix all weights (including pruned and unpruned weights) previous to the current pruned weight, so that the previous pruned weights are kept zero. The drawback is that all previous unpruned weights are also frozen without further updating, leading to sub-optimal achievements with potential performance degradation."}, {"title": "Multiple Removal Problem", "content": "3.1 Notations\n\nFor linear layers, the forward computation can be represented as $\\mathbf{w}\\mathbf{x}$, where $\\mathbf{w} \\in \\mathbb{R}^{n \\times m}$ is the weights and $\\mathbf{x} \\in \\mathbb{R}^{m \\times B}$ (B is the token number in each batch) is the layer input. $[\\mathbf{A}]_{q,p}$ denotes the weight in the $q^{th}$ row and the $p^{th}$ column of the 2D matrix $\\mathbf{A}$. $[\\mathbf{A}]_{q,:}$ denotes the $q^{th}$ row of $\\mathbf{A}$, and $[\\mathbf{A}]_{:,p}$ represents the $p^{th}$ column of $\\mathbf{A}$.\n\nTo make the problem tractable, the pruned weights are distributed in $k$ rows ($k < n$), and their row indexes are denoted by $q_i, \\forall i \\in \\{1,...,k\\}$. In the $q_i^{th}$ row, there are $k_i$ pruned elements, and their column indexes are denoted by $p_{ij}, \\forall j\\in \\{1,...,k_i\\}$. Since different rows $q_i$ have different numbers and distributions of pruned locations, we use the representation $p_{ij}$ rather than $p_j$. Thus, the pruned locations/indexes in the weight matrix $\\mathbf{w}$ can be expressed as $(q_i, p_{ij}), \\forall i \\in \\{1,...,k\\},\\forall j \\in \\{1,...,k_i\\}$, or $(q_i, p_{i1}), (q_i, p_{i2}), ..., (q_i, p_{ik_i}), \\forall i \\in \\{1, ...,k\\}$.\n\n$\\mathbf{e}_s$ is a one-hot vector with the $s^{th}$ element as one and all others as zero. So $\\mathbf{e}_{q_i} \\mathbf{e}_{p_{ij}} \\mathbf{w}$ means the weight in the $q^{th}$ row and the $p^{th}$ column of $\\mathbf{w}$, with $\\mathbf{e}_{q_i} \\in \\mathbb{R}^{n \\times 1}$ and $\\mathbf{e}_{p_{ij}} \\in \\mathbb{R}^{m \\times 1}$.\n\n$Tr(\\cdot)$ represents the trace function.\n\nMotivation with MRP\n\nTo address the limitations of the SRP, we try to formulate and solve the MRP, which prunes multiple weights simultaneously. Our MRP is specifically formulated for the layer-wise LLM pruning without any assumptions. Furthermore, since our MRP prunes multiple weights at the same time, each pruned weight is aware of all other pruned weights and thus there is no need to freeze any weights, which effectively addresses the limitations of SRP.\n\nMRP Formulation\n\nIn LLMs, the linear layers in transformer (Vaswani et al., 2017) or Mamba (Gu and Dao, 2023) blocks are the main cost of computations and parameters. To reduce the overhead of pruning LLMs, following SparseGPT, we adopt the layer-wise compression strategy to sequentially load and prune one single block instead of the whole model. The significantly reduced memory cost makes it feasible to use only one single GPU for all computations.\n\nFor each linear layer, we try to minimize the difference of the linear outputs (measured by l2 norm) before and after pruning, i.e., $||(\\mathbf{w} + \\delta \\mathbf{w})\\mathbf{x} - \\mathbf{w}\\mathbf{x}|| = ||\\delta \\mathbf{w} \\mathbf{x}||_2$. To make the problem tractable, as discussed in Sec. 3.1, the pruned locations/indexes in the weight matrix $\\mathbf{w}$ can be expressed as $(q_i, p_{i1}), (q_i, p_{i2}), ..., (q_i, p_{ik_i}), \\forall i \\in \\{1,...,k\\}$. The pruned weights $(q_i, p_{ij})$ are set to zero, i.e. $[\\mathbf{w} + \\delta \\mathbf{w}]_{q_i,p_{ij}} = 0$. To minimize the loss incurred by pruning, the other unpruned weights are updated for the compensation of pruned weights. Our MRP is formulated as the following,\n\\begin{equation} \n\\begin{aligned} \n\\min_{\\delta \\mathbf{w}} L(\\delta \\mathbf{w}) = ||\\delta \\mathbf{w} \\mathbf{x}||_2^2, \\\\\ns.t. \\mathbf{e}_{q_i}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{i1}} + [\\mathbf{w}]_{q_i,p_{i1}} = 0, \\\\\n\\mathbf{e}_{q_i}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{i2}} + [\\mathbf{w}]_{q_i,p_{i2}} = 0, \\\\\n... \\\\\n\\mathbf{e}_{q_i}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{ik_i}} + [\\mathbf{w}]_{q_i,p_{ik_i}} = 0, \\\\\n\\forall i \\in \\{1, ..., k\\}, \n\\end{aligned} \n\\tag{3} \n\\end{equation}\nwhere $\\mathbf{e}_{q_i}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{ij}}$ denotes the weight in the $q^{th}$ row and the $p^{th}$ column of $\\delta \\mathbf{w}$.\n\nIt can be transformed to vector representation,\n\\begin{equation} \n\\begin{aligned} \n\\min_{\\delta \\mathbf{w}} L(\\delta \\mathbf{w}) = ||\\delta \\mathbf{w} \\mathbf{x}||_2^2, \\\\\ns.t. \\mathbf{e}_{q_1}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{\\text{\\_}q_1}} + \\mathbf{w}_{q_1} = 0, \\\\\n\\mathbf{e}_{q_2}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{\\text{\\_}q_2}} + \\mathbf{w}_{q_2} = 0, \\\\\n... \\\\\n\\mathbf{e}_{q_k}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{\\text{\\_}q_k}} + \\mathbf{w}_{q_k} = 0, \n\\end{aligned} \n\\tag{4} \n\\end{equation}\nwhere $\\mathbf{e}_{p_{\\text{\\_}q_i}} \\in \\mathbb{R}^{m \\times k_i}$ with $[\\mathbf{e}_{p_{\\text{\\_}q_i}}]_{:,j} = \\mathbf{e}_{p_{ij}}$, and $\\mathbf{w}_{q_i} = [[\\mathbf{w}]_{q_i,p_{i1}}, [\\mathbf{w}]_{q_i,p_{i2}}, ..., [\\mathbf{w}]_{q_i,p_{ik_i}}] \\in \\mathbb{R}^{1 \\times k_i}$. In MRP, multiple weights are pruned simultaneously. $\\mathbf{e}_{p_{\\text{\\_}q_i}}$ is a collection of all pruned column indexes in the $q^{th}$ row, and $\\mathbf{w}_{q_i}$ is a collection of all pruned weight values in the $q^{th}$ row.\n\nComparison with SRP\n\nOur problem formulation is different from the SRP (Singh and Alistarh, 2020) in several ways.\n\nRelax the zero Jacobian assumption. Different from SRP with the zero Jacobian assumption which does not hold for the layer-wise LLM pruning, our formulation directly optimize the difference of outputs before and after pruning, without any assumptions or approximations.\n\nFurthermore, we provide an explanation for why SRP can still achieve good performance with the unreasonable zero Jacobian assumption. Specifically, since $H = 2\\mathbf{x}\\mathbf{x}^T$ for linear layers with the quadratic loss $L'(\\mathbf{w}) = ||\\mathbf{w}\\mathbf{x}||$, we have $\\delta \\mathbf{w}H \\delta \\mathbf{w}^T = \\delta \\mathbf{w}\\mathbf{x}\\mathbf{x}^T \\delta \\mathbf{w}^T = ||\\delta \\mathbf{w} \\mathbf{x}||^2$, which means that the optimization objective of SRP is well aligned with that of our proposed MRP. That"}, {"title": "Methodology", "content": "We first derive our optimal solution for the MRP and then discuss the algorithm design.\n\nOptimal Solution\n\nThe Lagrange function of Problem (4) is\n\\begin{equation} \n\\begin{aligned} \n\\mathcal{L}(\\delta \\mathbf{w}, \\lambda) = ||\\delta \\mathbf{w} \\mathbf{x}||^2 + (\\mathbf{e}_{q_1}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{\\text{\\_}q_1}} + \\mathbf{w}_{q_1})\\lambda_1 \\\\\n+ (\\mathbf{e}_{q_2}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{\\text{\\_}q_2}} + \\mathbf{w}_{q_2})\\lambda_2 + ... \\\\\n+ (\\mathbf{e}_{q_k}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{\\text{\\_}q_k}} + \\mathbf{w}_{q_k})\\lambda_k, \\\\\n= Tr(\\mathbf{x}^T \\delta \\mathbf{w}^T \\delta \\mathbf{w} \\mathbf{x}) + \\sum_{i} (\\mathbf{e}_{q_i}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{\\text{\\_}q_i}} + \\mathbf{w}_{q_i})\\lambda_i, \n\\end{aligned} \n\\tag{5} \n\\end{equation}\n\nwhere $\\lambda_i \\in \\mathbb{R}^{k_i \\times 1}$ denotes the Lagrange multiplier corresponding to the constraint for the $q^{th}$ row in Problem (4). $\\lambda_i = [\\lambda_{i1}, \\lambda_{i2}, ..., \\lambda_{ik_i}]$ and each $\\lambda_{ij}$ corresponds to the constraint $\\mathbf{e}_{q_i}^T \\delta \\mathbf{w} \\mathbf{e}_{p_{i}} + [\\mathbf{w}]_{q_i,p_{ij}} = 0$ in Problem (3). Unlike the SRP with a scalar $\\delta \\omega \\mathbf{x}$, in our problem, $\\delta \\mathbf{w} \\mathbf{x}$ is a matrix, requiring the trace function $Tr(\\cdot)$.\n\nThe gradients with reference to $\\delta \\mathbf{w}$ should be 0.\n\\begin{equation} \n\\begin{aligned} \n\\frac{\\delta \\mathcal{L}(\\delta \\mathbf{w}, \\lambda)}{\\delta (\\delta \\mathbf{w})} = 2 \\delta \\mathbf{w} \\mathbf{x} \\mathbf{x}^T + \\sum_{i} \\mathbf{e}_{q_i} \\lambda_i \\mathbf{e}_{p_{\\text{\\_}q_i}}^T = 0. \n\\end{aligned} \n\\tag{6} \n\\end{equation}\n\nWe can obtain $\\delta \\mathbf{w}$ as below,\n\\begin{equation} \n\\begin{aligned} \n\\delta \\mathbf{w} = - (\\sum_{i} \\mathbf{e}_{q_i} \\lambda_i \\mathbf{e}_{p_{\\text{\\_}q_i}}^T) (2 \\mathbf{x} \\mathbf{x}^T)^{-1}. \n\\end{aligned} \n\\tag{7} \n\\end{equation}\nBy applying Equation (7) in Equation (5), we have the following,\n\\begin{equation} \n\\begin{aligned} \ng(\\lambda) = \\frac{1}{2} \\sum_{i} \\lambda_i^T \\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}} \\lambda_i + \\sum_{i} \\mathbf{w}_{q_i} \\lambda_i. \n\\end{aligned} \n\\tag{8} \n\\end{equation}\nNote that $\\mathbf{e}_{q_i}^T \\mathbf{e}_{q_i} = 1$ and $\\mathbf{e}_{q_i}^T \\mathbf{e}_{q_s} = 0$, for $i \\neq s$. Besides, we can switch the position of $\\mathbf{x}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}} \\lambda_i$ and $\\lambda_i^T \\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{x}$ in the trace function.\n\nThe gradients with reference to $\\lambda$ should be 0.\n\\begin{equation} \n\\begin{aligned} \n\\frac{\\delta g(\\lambda)}{\\delta \\lambda_i} = - \\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}} \\lambda_i + \\mathbf{w}_{q_i}^T = 0, \\forall i. \n\\end{aligned} \n\\tag{9} \n\\end{equation}\nWe can obtain the optimal $\\lambda$ as below,\n\\begin{equation} \n\\begin{aligned} \n\\lambda_i = [\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}]^{-1} \\mathbf{w}_{q_i}, \\forall i. \n\\end{aligned} \n\\tag{10} \n\\end{equation}\nThe optimal $\\delta \\mathbf{w}$ can be derived as below,\n\\begin{equation} \n\\begin{aligned} \n\\delta \\mathbf{w}^* = -(\\sum_i \\mathbf{e}_{q_i} \\mathbf{w}_{q_i} [\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}]^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}^T) (2 \\mathbf{x} \\mathbf{x}^T)^{-1}. \n\\end{aligned} \n\\tag{11} \n\\end{equation}\nThe minimal loss/error corresponding to the optimal $\\delta \\mathbf{w}$ can be obtained by\n\\begin{equation} \n\\begin{aligned} \nL^* = \\frac{1}{2} \\sum_i \\lambda_i^T \\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}} \\lambda_i \\\\\n= \\mathbf{w}_{q_i}^T [\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}]^{-1} \\mathbf{w}_{q_i}. \n\\end{aligned} \n\\tag{12} \n\\end{equation}\nRemark 4.1. Dampening for the inverse. If $2\\mathbf{x} \\mathbf{x}^T$ is not full rank with difficulties for the inversion $(2 \\mathbf{x} \\mathbf{x}^T)^{-1}$, the dampening technique is adopted to compute $(2 \\mathbf{x} \\mathbf{x}^T + \\gamma I)^{-1}$ instead, with $\\gamma$ as the dampening ratio.\n\nRemark 4.2. Separate row computation. For the optimal perturbation in Equation (11), since $\\mathbf{e}_{q_i}$ is a one-hot vector, $\\mathbf{e}_{q_i} \\times \\lambda$ only has non-zero values in the $q^{th}$ row with all zeros for all other rows. Thus, in Equation (11), each term with the index $i$ in the sum just computes the $q^{th}$ row in the outputs and the computation of the $q^{th}$ row does not affect the $q^{th}$ row, $\\forall s \\neq i$. Specifically, we have the following,\n\\begin{equation} \n\\begin{aligned} \n[\\delta \\mathbf{w}^*]_{q_i,:} = -\\mathbf{w}_{q_i} [\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}]^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \n\\end{aligned} \n\\tag{13} \n\\end{equation}\nRemark 4.3. Full interactions between pruned weights. For our optimal perturbation in Equation (11) and optimal loss in Equation (12), our solution is not the simple sum of multiple SRP solutions. Our solution not only depends on the multiple pruned weights, but also takes the interactions of the multiple removals (denoted by $[\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}]^{-1}$) into considerations, which are unavailable in SRP without the information of other multiple removals."}, {"title": "Algorithm Design", "content": "Our pruning algorithm is shown in Algorithm 1. We need to address two key problems: the pruning mask and optimal compensation. For each problem, we have two choices, including Solution M from our MRP and its simplified version, Solution S.\n\nPruning Mask\n\nIn the algorithm, we need to select the pruned locations and determine the pruning mask.\n\nSolution M. It is too complex to follow Equation (12) to find out the pruning mask with the minimal pruning loss. Specifically, it needs to select k weights from all weights for each combination, leading to too many combinations. It also needs to compute and sort the losses of all combinations to find out the minimal loss. Thus, for unstructured pruning, we do not implement Solution M.\n\nFor semi-structured pruning with N:M sparsity, we implement our Solution M based on our optimal loss in Equation (12). Specifically, in N:M sparsity, we split the weights into groups with M weights in each group, and then select N weights to be pruned in each group. For example, in 2:4 sparsity, there are 2 pruned weights every 4 weights. Thus, in each group with 4 weights, we use Equation (12) to select 2 weights to be pruned with the minimal loss. In particular, there are 6 combinations to select 2 elements from 4. We compute the loss with Equation (12) for each combination and find out the minimal loss with its corresponding 2 elements, which are determined to be pruned. By doing this for each group, we can determine the pruning mask for the whole matrix.\n\nNote that it is still a simplified version of Equation (12), since each group is computed separately without interactions from other groups. Ideally, Equation (12) needs to consider all groups together, which is unaffordable. For example, if there are G groups with 6 combinations in each group for 2:4 sparsity, there are totally $6^G$ combinations. So we just consider the combinations within each group, without connections between groups.\n\nSolution S. To reduce the complexity and make the problem tractable, we can assume that $\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}$ in Equation (12) is a diagonal matrix with all zeros for off-diagonal elements. It means that we ignore the interactions between multiple pruned locations and each pruned weight does not affect other pruned weights. Thus, Equation (12) can be transformed to the following,\n\\begin{equation} \n\\begin{aligned} \n\\hat{L}^* = \\sum_{i,j} \\frac{[\\mathbf{w}]_{i,j}^2}{[\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}]_{j,j}}. \n\\end{aligned} \n\\tag{14} \n\\end{equation}\nWe follow Equation (14) to compute the potential pruning loss for each single weight (indexed by (i, j)). Then we sort the losses of all weights and find out the K weights with smaller losses as the pruned weights. It is similar to the mask searching in SparseGPT (Frantar and Alistarh, 2023).\n\nOptimal Compensation\n\nWith the pruning mask, we need to update other unpruned weights to compensate the pruning loss.\n\nSolution M. To achieve the best performance, we directly follow Equation (11) to compute the modifications of other unpruned weights. In Equation (11), we do not need to exactly compute multiple matrix multiplications such as $\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1}$ and $\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}$, since they just select certain rows or columns in a matrix. Besides, the complexity of the inversion $[\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}]^{-1}$ is smaller than $(2 \\mathbf{x} \\mathbf{x}^T)^{-1}$ with a reduced dimension.\n\nSolution S. Similar to the pruning mask, we can reduce the complexity of Equation (11) by assuming that $\\mathbf{e}_{p_{\\text{\\_}q_i}}^T (2 \\mathbf{x} \\mathbf{x}^T)^{-1} \\mathbf{e}_{p_{\\text{\\_}q_i}}$ in Equation (11) is a diagonal matrix with all zeros for off-diagonal elements. It ignores the interactions between multiple pruned weights, and the solution is similar to that in SparseGPT (Frantar and Alistarh, 2023). For simplicity, we directly follow SparseGPT for Solution S of optimal compensation."}, {"title": "Accurate Pruning Algorithms", "content": "We design our post-training pruning algorithms for both unstructured and semi-structured sparsity.\n\nUnstructured Post-Training Pruning\n\nTo align with SparseGPT for a fair comparison, we adopt the block pruning setting. The weight matrix is split into blocks with a number of S columns (block-size) in each block. All blocks share the same pruning rate a to keep overall pruning rate. In Algorithm 1, for all blocks, based on how to solve the pruning mask and optimal compensation, we have two combinations, SS and SM. The first S (or M) denotes using Solution S (or M) for pruning mask, and the second S (or M) represents Solution S (or M) for optimal compensation. We do not implement Solution M for pruning mask due to its huge complexity. SS is just SparseGPT.\n\nFor the number of columns S in a block, S = 1 leads to too many blocks with high complexity. A typical S value is 128, 512, and 2048. S = all means that all columns are in the same block.\n\nSemi-Structured Post-Training Pruning\n\nSimilarly, in semi-structured pruning, we can use Solution S or M for pruning mask and optimal compensation, leading to 4 combinations: SS, SM, MS, and MM. The first S (or M) denotes using Solution S (or M) for pruning mask, and the second S (or M) is for optimal compensation."}, {"title": "Comparison with the SRP-based Solution", "content": "As discussed in Sec. 2.3, the SRP-based method such as SparseGPT needs to freeze (fix) all weights previous to the current pruned weight, incurring certain performance degradation without further updating the frozen unpruned weights. Different from SparseGPT, our MRP solution updates all unpruned weights, resulting in better performance. Furthermore, ours is not a simple sum of multiple SRP solutions. Instead, it depends on not only the pruned weights, but also the interactions between them, as shown in Equation (11) and (12)."}, {"title": "Experimental Results", "content": "Our implementation for the proposed method is based on PyTorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2019). We sequentially prune the linear layers of the blocks in LLMs, which only loads one single block each time with significantly less memory cost (Yao et al., 2022; Frantar and"}, {"title": "Results on Transformer-based LLMs", "content": "The results on transformer-based LLMs under various block-size settings are presented in Table 1. More results for OPT and BLOOM models are shown in Appendix A and B. We compare the perplexity for unstructured pruning (50% sparsity) and semi-structured pruning (2:4 sparsity). As discussed in Section 4.2, in unstructured pruning, we only have SS and SM since the complexity to address pruning mask with Solution M is too high. In semi-structured pruning, we have 4 combinations, SS, SM, MS, and MM. SS corresponds to the original SparseGPT, and other methods are based on our proposed optimal solution.\n\nAs demonstrated in Table 1, our method can achieve a lower perplexity for various models on different datasets under the same setting. Specifically, for unstructured sparsity, our perplexity is lower than SparseGPT. For 2:4 sparsity, all of our methods achieve lower perplexity than SparseGPT. MM typically performs the best since it adopts more advanced techniques following our optimal solution. However, it has the highest complexity. We notice that the performance of SM is very close to (or even better than) that of MM, with lower complexity. Thus, we suggest to use SM with a limited computation budget. In the following, we mainly show the results of SM.\n\nMore results for other models are demonstrated in Table A1, A2 and A3 with similar observations. We also demonstrate the comparison with other baselines under different sparsity in Table 2 and Appendix C. Ours can achieve better perplexity."}, {"title": "Results for Mamba-based LLMS", "content": "The results for Mamba models (Gu and Dao, 2023) are demonstrated in Table 3. Similarly, our method can achieve a better perplexity than other baselines for various Mamba models under the same setting."}, {"title": "Zero-Shot Evaluation", "content": "The zero-shot results for Mamba model are demonstrated in Table 3. Our method with a better perplexity can achieve a higher average accuracy on zero-shot datasets than other baselines.\n\nIn Table 3, we can observe that with 50% sparsity, the accuracy for LAMBADA under Magnitude pruning is very low, such as 2.37% for Mamba370M, while it is a bit higher on other datasets"}, {"title": "Ablation Study", "content": "We ablate different values of the dampening ratios and the number of calibration data. We test our method on the LLaMA2-7B model. As shown in Appendix D and Figure A1, by using a smaller dampening ratio or more calibration data, our performance can be better. But to make a fair comparison"}]}