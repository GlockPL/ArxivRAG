{"title": "OPEN-SOURCE MOLECULAR PROCESSING PIPELINE FOR GENERATING MOLECULES", "authors": ["Shreyas V", "Jose Siguenza", "Karan Bania", "Bharath Ramsundar"], "abstract": "Generative models for molecules have shown considerable promise for use in computational chemistry, but remain difficult to use for non-experts. For this reason, we introduce open-source infrastructure for easily building generative molecular models into the widely used DeepChem [Ramsundar et al., 2019] library with the aim of creating a robust and reusable molecular generation pipeline. In particular, we add high quality PyTorch [Paszke et al., 2019] implementations of the Molecular Generative Adversarial Networks (MolGAN) [Cao and Kipf, 2022] and Normalizing Flows [Papamakarios et al., 2021]. Our implementations show strong performance comparable with past work [Kuznetsov and Polykovskiy, 2021, Cao and Kipf, 2022].", "sections": [{"title": "1 Introduction", "content": "The discovery of new molecules and materials is crucial for addressing challenges in chemistry, such as treating diseases and tackling climate change [Liu et al., 2023, Sanchez and Aspuru-Guzik, 2018]. Traditional methods rely on human expertise and are time-consuming and costly, limiting the exploration of the vast chemical space [Polishchuk et al., 2013]. Generative models offer a promising solution using deep learning to design molecules based on desired properties, rapidly identifying diverse and optimized molecules for specific applications. These models vary in their approaches and have seen rapid development, with benchmarks now in place to evaluate their performance in terms of distribution learning and chemical diversity. Although these models are publicly available, practitioners require extensive Python and machine learning knowledge to reap their benefits. Thus, we introduce open-source molecular generative model infrastructure into DeepChem Ramsundar et al. [2019], a widely used open-source library for molecular machine learning.\nAmong these models, Molecular Generative Adversarial Networks (MolGAN) [Cao and Kipf, 2022] and Normalizing Flow models [Papamakarios et al., 2021] offer two complementary approaches to the computational generation of molecular structures. These models, though available openly, require the practitioners to have a deep understanding of machine learning and Python, which presents significant challenges and hinders their practical adoption\nMolGAN adapts the generative adversarial network [Goodfellow et al., 2014] framework to molecular contexts, utilizing an adversarial training mechanism to facilitate the generation of novel molecular configurations by approximating the distribution of empirical molecular data. Normalizing Flow models use an alternative strategy through exact likelihood models, employing diffeo-morphisms to convert simple distributional forms into complex molecular distributions.\nWe contribute implementations of MolGAN and Normalizing flow models in PyTorch to DeepChem and build example pipelines for training generative models. This adaptation broadens the accessibility of MolGAN and Normalizing Flows to the computational chemistry community. Our pipelines require little previous knowledge in machine learning or Python to generate molecules. On the other hand, the implementations are well-documented for advanced users who want to tinker around and make modifications as per their needs."}, {"title": "2 Methods and Background", "content": "2.1 Deepchem and Generative Molecular Models\nDeepChem [Ramsundar et al., 2019] is a highly versatile open-source Python library designed specifically for scientific machine learning and deep learning on molecular and quantum datasets [Ramsundar et al., 2021]. Its comprehensive framework enables the solution of complex scientific problems in diverse areas such as drug discovery, energy calculations, and biotech [Wu et al., 2018]. DeepChem's design philosophy is centered around a systematic approach whereby scientific calculations are broken down into workflows built from underlying primitives. This approach enables the DeepChem library to apply to a wide range of applications. Notably, DeepChem has facilitated large-scale benchmarking for molecular machine learning via the MoleculeNet benchmark suite [Wu et al., 2018], protein-ligand interaction modelling [Gomes et al., 2017], generative modeling of molecules [Frey et al., 2023], and other cutting-edge scientific endeavors.\nDeepChem had older TensorFlow implementations of MolGANs and Normalizing Flows. However, DeepChem has been migrating to PyTorch following the adoption of PyTorch into the Linux Foundation core for long-term stability. In this work, we migrate the older code to PyTorch and integrate the generative framework more tightly with DeepChem's ecosystem. The implementation allows users to experiment with the vast library of existing layers that can be plugged into the models to make new and creative models.\n2.2 Representation of Molecules\nThe strength of neural networks lies in their ability to take in a complex input representation and transform it into a latent representation needed to solve a particular task. In this way, the choice of input representation plays a key role in governing how the model learns information about the molecule. Input representations often fall into one of two categories: (1) one-dimensional (e.g., string-based representations), (2) two-dimensional (e.g., molecular graphs).\n2.2.1 One-dimensional representations\nThe most common one-dimensional representation of molecules is SMILES (Simplified Molecular Input Line Entry System) [Weininger, 1988], which transforms a molecule into a sequence of characters based on predefined atom ordering rules. This representation enables the use of neural network architectures developed for language processing. For instance, previous work [G\u00f3mez-Bombarelli et al., 2018, Olivecrona et al., 2017] used recurrent neural networks as generative models to create SMILES strings. However, these methods often produce invalid SMILES that cannot be converted to molecular structures due to their disregard for SMILES grammar. Kusner et al. [2017] and Dai et al. [2018] attempted to address this by incorporating syntactic constraints into the recurrent neural network, yet they still frequently generated invalid SMILES strings. To overcome this limitation, Krenn et al. [2020] introduced SELFIES (Self-Referencing Embedded Strings), an improved string representation. With SELFIES, a recurrent neural network can generate molecules with 100% validity, though validity here pertains to valency rules and does not guarantee molecular stability.\n2.2.2 Molecules as Graphs\nMolecules can alternatively be represented as graphs. Graphical representations are powerful in that they directly capture the connectivity between atoms, while in one-dimensional representations, this information must inferred by the model. For MolGAN, each molecule can be represented as an undirected Graph G with a set of edges E and nodes V. Each atom $v_i \\in V$ is associated with a D-dimensional one-hot vector $x_i$. Each edge $(v_i, v_j) \\in E$ is also associated with a bond type $y \\in \\{1, . . ., Y\\}$. Thus, we have a representation of a graph as two objects: a $X = [x_1,...,x_n]^T \\in \\mathbb{R}^{N\\times D}$ and an adjacency tensor $A \\in \\mathbb{R}^{N\\times N\\times Y}$. $A_{i,j} \\in \\mathbb{R}^Y$ is a one-hot vector indicating the type of edge between i and j.\n2.3 MolGAN\nMolecular Generative Adversarial Network (MolGAN) represents a novel approach in the Generative Space for molecules by employing the GAN framework, which allows for an implicit, likelihood-free generative model which overcomes various problems (like graph matching [Simonovsky and Komodakis, 2018] and node ordering heuristics) with previous models. MolGAN performs similarly to current SMILES-based approaches as well, albeit it is more susceptible to model collapse. We have incorporated the following loss used by the paper\n$L(x^{(i)}, G_{\\theta}(z^{(i)}; \\phi) = -D_{\\phi}(x^{(i)}) + D_{\\phi}(G_{\\theta}(z^{(i)})) +\\alpha(||\\nabla_{\\hat{x}^{(i)}} D_{\\phi}(\\hat{x}^{(i)} || - 1)^2$"}, {"title": "2.4 Normalizing Flows", "content": "Normalizing Flows constitute a generative model that uses invertible transformations to model a probability distribution. This methodology enables the computation of likelihoods and the generation of samples by transforming simple base distributions (e.g., gaussian) into more complex ones through a sequence of invertible (& differentiable) transformations. The precision and control afforded by Normalizing Flows make them particularly advantageous for generating molecules with highly specific attributes. Normalizing flows also take advantage of equivariance (through invariance) in the Graph's probability distribution, thus providing a complementary strength to adversarial approaches such as MolGAN in the generative modeling landscape.\n2.5 Standardizing Molecule Generation Workflows\nWe aim to standardize (\u201cdeepchemize\u201d [Ramsundar et al., 2021]) the molecule generation by MolGAN and Normalizing Flow model by implementing the models using the workflow structure implemented by other DeepChem models. With this standardization, we aim to experimenting with new generative models simpler and enable better molecule generation capabilities.\nBasicMolGANModel (the MolGAN implementation in DeepChem), can be used with different discriminators and generators, thus providing a flexible framework for molecule generation. This flexibility of the BasicMolGANModel can be used to incorporate more complex architectures of generators and discriminators to achieve better global density information to approximate a more accurate exchange-correlation functional. We allow for a \u201cplug-and-play\" style of training, inference, and generation with respect to the MPNN used, optimizer, number of MLP layers in the generator, and bond/node types. This can be used in a creatively to combine the strengths of multiple architectures.\nNormalizingFlowModel (the Normalizing Flow implementation in DeepChem) can be used with different flow layers with customized bijective transformations. This allows end users to incorporate more complex transformations and modeling capabilities into the framework. The interoperable nature of DeepChem's layers enables further innovation. Overall, these abstractions simplify and ease usage and experimentation to achieve state-of-the-art molecule generation."}, {"title": "3 Implementation", "content": "The Generative models were split and implemented as the following major components: Layers, Base Model, and Molecule Generation Pipeline. We briefly also describe the DeepChem Datasets used.\n3.1 Datasets\nOur work utilizes three publicly available datasets: QM7 [Rupp et al., 2012], BBBP [Martins et al., 2012], Lipophilicity [Hersey, 2015], PPB [Wang et al., 2005] and QM9 [Ramakrishnan et al., 2014]. QM7 is a subset of GDB-13 (a database of nearly 1 billion stable and synthetically accessible organic molecules) containing up to 7 heavy atoms C, N, O, and S. The blood-brain barrier penetration (BBBP) dataset encompasses approximately 2,000 molecules with binary labels on their permeability properties. The lipophilicity dataset, curated from ChEMBL database, contains 4200 compounds. The PPB dataset is from the Maximum Unbiased Validation (MUV) group, it is a benchmark dataset selected from PubChem BioAssay by applying a refined nearest neighbor analysis. It contains around eleven thousand compounds. QM9 is a subset of the GDB-17 database (a database of nearly 166 billion stable and synthetically accessible organic molecules), comprising 134 thousand stable organic molecules with up to 9 heavy atoms. All the mentioned datasets are downloaded through MoleculeNet [Wu et al., 2018].\n3.2 Layers\nMolGAN: The first layer in the Discriminator of a MolGAN model is an encoder layer, which consists of multiple convolutional layers followed by an aggregation layer. We provide an architecture that can be varied and overridden easily based on the end user's requirements.\nNormalizing Flows: Layers in Normalizing Flows are used to perform forward and backward computation. DeepChem Normalizing Flows pipeline supports linear layers, which offer low computational costs compared with other layers (e.g. planar, autoregressive). Linear transformations exhibit relationships through molecular data dimensions.\n$g(x) = Wx + b$\nWhere $W \\in \\mathbb{R}^{D\\times D}$ and $b \\in \\mathbb{R}^D$ are parameters. So, if W is an invertible matrix, the function is invertible.\n3.3 Base Model\nThe Base Model has three distinct components, which can be modified individually to enable flexibility.\nGenerator - Our implementation borrows from the original MolGAN [Cao and Kipf, 2022] implementation, and by default comprises of an MLP [Murtagh, 1991] with [embedding dimension, 128, 512] units to model the probability distribution (implicitly) over a graph (from which edges and nodes are sampled). So overall, for any\n$z \\sim N(0, I)$\nthe generator $G_{\\theta}(z)$ outputs two continuous and dense objects,\n$X\\in\\mathbb{R}^{N \\times D}, A\\in\\mathbb{R}^{N \\times N\\times D}$\nOver which we perform categorical sampling using the Gumbel Softmax trick [Jang et al., 2017] to obtain $\\bar{X}$ and $\\bar{A}$ as -\n$\\bar{X} = X + \\text{Gumbel}(\\mu = 0, \\beta = 1)$ (2)\n$\\bar{A} = A_{ijy} + \\text{Gumbel}(\\mu = 0, \\beta = 1)$ (3)\nThis method can be overridden to use a straight-through estimator [Paulus et al., 2020] or can also allow for $\\bar{X} = X$ and $\\bar{A} = A$.\nDiscriminator - The discriminator is tasked with scoring the graph produced. This involves a series of"}, {"title": "4 Experimental Results", "content": "4.1 Experimental Setup\nWe report results averaged across five runs of 1000 molecule generations, matching Cao and Kipf [2022].\nFor MolGAN evaluation, we allowed four edge types (single, double, triple, no-bond, and aromatic), five node types (C, N, O, F, and PAD), and a maximum of nine vertices per graph; this is borrowed from the original implementation along with a learning rate of 10-3. However, we use an embedding dimension of 100, no dropout [Srivastava et al., 2014], and a batch size of 1024 check-pointing every 20 epochs.\nFor Normalizing Flows, we used a learning rate of 10-4, weight decay of 10-4, two layers of flows with Masked Affine Flows [Dinh et al., 2017] along with an ActNorm layer [Kingma and Dhariwal, 2018]. We used a Multivariate Normal Distribution from PyTorch [Paszke et al., 2019] to build our flow model. The model was trained on 100 epochs with Adam optimizer [Kingma and Ba, 2017]. All experiments used Python 3.10.\n4.2 Results"}, {"title": "5 Conclusion & Discussion", "content": "In this work, we improve DeepChem's generative modeling tools and provide a more standardized and scalable implementation that makes generative molecular methods more accessible to scientists. Standard Machine Learning practices are built within DeepChem (e.g., checkpointing, validation, logging, etc.), which would otherwise need some form of human expertise. Benchmarks show comparable performance with existing implementations, and the tight"}, {"title": "Impact Statement", "content": "This paper makes Generative Molecular modeling accessible to a broad spectrum of people, and while this is intended, it is not tough to modify these models to make them generate toxic and harmful molecules. However, the synthesis of novel molecules is an area that requires a lot of human expertise, and in general, is not easy to do. In a future where synthesis methods are also equally accessible, these models can be potentially dangerous, but at the present time, the positive outcomes outweigh the negative ones."}, {"title": "A MolGAN Pipeline", "content": "Training -"}, {"title": "B NormalizngFlows Pipeline", "content": "Training -"}]}