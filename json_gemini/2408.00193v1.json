{"title": "Resilience and Security of Deep Neural Networks Against Intentional and Unintentional Perturbations: Survey and Research Challenges", "authors": ["SAZZAD SAYYED", "MILIN ZHANG", "SHAHRIAR RIFAT", "ANANTHRAM SWAMI", "MICHAEL DE LUCIA", "FRANCESCO RESTUCCIA"], "abstract": "In order to deploy deep neural networks (DNNs) in high-stakes scenarios, it is imperative that DNNs provide inference robust to external perturbations \u2013 both intentional and unintentional. Although the resilience of DNNs to intentional and unintentional perturbations has been widely investigated, a unified vision of these inherently intertwined problem domains is still missing. In this work, we fill this gap by providing a survey of the state of the art and highlighting the similarities of the proposed approaches. We also analyze the research challenges that need to be addressed to deploy resilient and secure DNNs. As there has not been any such survey connecting the resilience of DNNs to intentional and unintentional perturbations, we believe this work can help advance the frontier in both domains by enabling the exchange of ideas between the two communities.", "sections": [{"title": "INTRODUCTION", "content": "Thanks to their ability of performing critical tasks such as object detection [56], language translation [4], image classification [15], and efficient pose estimation [106], DNNs have become essential in our everyday life [14]. For a comprehensive survey on DNNs, we refer the reader to [130]. Breakthroughs in the form of large language models such as GPT-4 [124], universal segmentation models such as Segment Anything [74] and diffusion models such as Stable Diffusion [134] have advanced the frontier of artificial intelligence (AI) and captured the interest of ordinary citizens in using Al in their day-to-day lives.\nThe unprecedented benefits of DNNs come with their own set of social and ethical challenges, mainly in the form of privacy, safety and security issues [39]. For example, it has been shown that a DNN is extremely sensitive to intentional perturbations where changing a few pixels in the input can lead to misclassifications [11, 49, 115, 146]. In addition,"}, {"title": "1.1 Motivation and Novel Contributions", "content": "Although intentional and unintentional perturbations share several critical aspects, a unified vision has so far been elusive. Since its inception in 2014 [150], the study of resilient DNN design has been fragmented into separate domains. Some approaches have been studying intentional perturbations \u2013 also known as adversarial machine learning - while some approaches have been proposed to guarantee robustness against Out-of-Distribution (OOD) samples. Since a DNN needs to be resilient to both types of perturbation, it is important to study the connection between these domains. [136] discusses the relation between OOD detection and anomaly detection, open-set recognition, and novel set recognition. The survey in [2] focuses on the detection of adversarial inputs and bench-marking some of the detection approaches. To the best of our knowledge, literature lacks surveys focusing on the connection between the detection of adversarial input and OOD input. As such, in this paper we discuss the literature from the perspective of resilience of DNNs encompassing detection of both adversarial and OOD. Specifically, the main contributions of this paper can be summarized as follows:\n\u2022 We categorize and discuss the seminal, significant, and recent work in the domain of OOD detection (i.e., unintentional interference) and adversarial sample detection (i.e., intentional interference);\n\u2022 We investigate the commonalities among intentional and unintentional perturbation detection and the corre- sponding defense strategies, while remarking the strengths and weaknesses of these approaches. We believe these two communities can benefit from this study as they can discover common approaches, similarities in these two fields, and adopt new perspective from the other community.\n\u2022 We conclude the paper by pointing out some open questions and research directions regarding ensuring the resilience of DNNs in real-world inference systems."}, {"title": "2 VULNERABILITIES OF DEEP NEURAL NETWORKS: BACKGROUND AND TAXONOMY", "content": "We define the term \"vulnerability\" as any action that causes the DNN to perform in not its intended manner as compared to when the action is absent. Under the scope of this survey, we consider actions where external perturbations are introduced to the input samples during inference. In this section, we provide background on different approaches to introduce these perturbations intentionally. Next, we illustrate different scenarios that occur when external perturbations are added to the input samples in a natural fashion."}, {"title": "2.1 Intentional Perturbation: Adversarial Attack", "content": "We consider a target DNN f(x) = y, where x \u2208 X and y \u2208 Y represent input and output samples respectively, with X, Y respectively representing input and output space. The objective of an adversary is to find a perturbation \u03b4 that misleads the target DNN. Based on their intent or objectives, adversarial attacks can be categorized as untargeted and targeted. The former aim at causing an incorrect classification without specifying a particular target class, i.e.,\n$f(x + \\delta) \\neq y, \\delta \\leq \\epsilon$.\n(1)"}, {"title": "2.2 Unintentional Perturbation: Determining Out-of-Distribution Samples", "content": "Figure 2 shows a taxonomy of existing work in unintentional perturbation. To study the resilience of DNN to unintentional perturbation, existing work makes assumption regarding the nature of the perturbation. If the perturbation has no resemblance to the distribution of the training data, it is extremely challenging to adapt the DNN without additional knowledge and/or labeled samples. In this case, perturbation detection can be considered as a viable option."}, {"title": "Definition of Covariate Shift", "content": "The covariate shift is a change in the distribution D(X) which preserves their label distribution D(Y). In other words, Ds (Y|X) = DT (Y|X). Notice that the target feature distribution DT (X) is assumed to be known. Since the covariate shift does not change the semantic content of the input, it is also named non- semantic shift. Direct perturbation, either intentional or unintentional, may cause such shift. Among the unintentional perturbations, we have common corruptions such as Gaussian blur, Gaussian noise, motion blur, defocus blur, frost, fog and rain, among others [59]. This also includes shift in input distribution due to a change in geographical location, operating hardware (e.g., different camera or sensors) or viewpoint [58]. Due to the change in the distribution D(X), the distribution D(X, Y) changes. This brings the problem of dataset shift [128, 131]. Additionally, we can consider intentionally perturbed inputs a.k.a. adversarial inputs [18, 105, 115] as covariate shifts, since the perturbation only changes the input distribution. This highlights the connection between unintentional and intentional perturbations."}, {"title": "Definition of Semantic Shift", "content": "Any kind of shift that changes the semantic content and as a result the marginal distribution of label D(Y) of the dataset is a semantic shift or label shift. A semantic shift affects both the image space and the label space as the distribution of input Dx is shifted from the source distribution and new labels are introduced in the label space. The detection of such shifts can be considered as encompassing the tasks of novel class recognition and open set recognition. In the open set recognition problem [157], the DNN is presented with sample from classes which were not present in training data representing a shift in the label distribution. One-class novel class recognition can be thought of as an extreme version of open set recognition where the DNN is presented with a single class during training and is required to detect new incoming classes during testing or inference."}, {"title": "Definition of Prior Shift", "content": "Prior shift refers to a scenario where the marginal distribution of labels in source and target domain are different, i.e., Ds (Y) \u2260 DT (Y). On the other hand, the class conditional distribution of data given labels are assumed to be same, i.e., Ds(X|Y) = D\u315c(X|Y). Prior shift might affect domain adaptation processes, also known as Test Time Adaptation (TTA) in literature [48, 180]. Although the marginal label distribution Ds (Y) is uniform in most of the cases, in TTA the data is observed as a small batch of samples at a particular time. Hence, we might observe dominance of certain labels based on the current scenario. This is a very common example of prior shift that might happen which is also termed as 'correlated label distribution' in TTA literature."}, {"title": "Definition on Generalized Label Shift", "content": "Generalized label shift is a more challenging data drift that was first introduced in [151]. It occurs when both covariate shift Ds(X) \u2260 DT(X) and prior shift Ds (Y) \u2260 DT (Y) occurs simultaneously. Moreover, it is assumed that there exists some feature representation Z = g(X) for which the conditional distributions based on labels both on source and target domain are equal, i.e., Ds(Z|Y) = DT (Z|Y). This situation might arise when a DNN continuously experiences different data corruptions while also being adapted with a skewed label distribution."}, {"title": "3 A UNIFIED VISION FOR INTENTIONAL AND UNINTENTIONAL PERTURBATION", "content": "There are two main approaches to ensure resilience of DNNs. The first is by training the DNN to be inherently resilient. Examples include adversarial training [72, 73, 185] to avoid intentional perturbations, and training with different augmentation schemes [58, 62] for unintentional perturbations. The second way is to augment the DNN with another binary classifier that differentiates between perturbed and unperturbed samples. Ideally, the auxiliary classifier would be able to differentiate between unperturbed and perturbed inputs for both intentional and unintentional perturbation. In reality, the detection of intentional and unintentional perturbation have developed as two separate domains. In the following, we highlight the connection among the methods for detection of intentional and unintentional perturbations. As such, we have categorized the approaches from the perspective of input space, latent (i.e., model) space, and logit (i.e., output) space."}, {"title": "4 DETECTION OF INTENTIONAL PERTURBATION", "content": "Based on the classification in Section 3, we describe existing work in perturbation detection as well as the related defense strategies. The detection of intentional perturbation - also known as adversarial perturbations in literature - is mainly based on the hypothesis that adversarial samples reside in a manifold different than that of the unperturbed inputs [42, 145]. The key objective behind adversarial sample detection is to accurately detect and minimize the influence of adversarial samples on DNNs. The general framework for this detection is: (a) characterizing the adversarial perturbation; (b) formulating a score function based on the characterization; and (c) deciding on a threshold to accept or reject a sample as adversarially perturbed. In this section, we first review some seminal work in this field and then provide key insights into recent work. We summarize all the reviewed work in Table 4.1."}, {"title": "4.1 Use of Intermediate Representations", "content": "One of the earliest approaches utilize the features from intermediate layers of a DNN to detect adversarial samples. The authors of [109] train an adversary detector that receives inputs from the intermediate feature representations of a classifier. This detector aims to distinguish between samples from the original dataset and adversarial examples. The author considers two different scenarios: (i) a static scenario, where adversaries have access to the classification network and its gradients only; and (ii) a dynamic scenario, where adversaries have access to both the classification and the proposed detector network along with its gradients. The adversary detector network is trained in a supervised manner using training images and their corresponding adversarially perturbed images. The authors showed that a static detector cannot perform well against a dynamic adversary.\nTo tackle dynamic adversaries, the author proposes a dynamic adversary training method inspired by the approach in [49], where for each mini-batch the adversarial samples are computed on the fly. For each mini-batch, a dynamic adversary modifies a data point with a parameter \u03c3 selected randomly from the range [0, 1], as it depends on the gradient of the detector which evolves over time. By training the detector, one implicitly makes it resilient to dynamic adversaries for various values of \u03c3. Although it shows excellent results, it does not show robustness against random noise. One key takeaway from this work is that supervised training with specific attack strategy is unlikely to hold against adaptive attackers and a robust characterization of adversarial perturbation is required to be utilized by the detector so that it can generalize across different attack mechanisms instead of overfitting to a single type of attack.\nThe detector sub-network proposed in [109] remains vulnerable to the adversarial samples that are not used during the detector training process. To address this issue, [102] proposes a new approach called SafetyNet that relies on the hypothesis that adversarial attacks work by producing different patterns of activation in late-stage Rectified Linear Units (ReLUs) to those produced by natural examples. As a result, it focuses on discrete codes produced by the quantization of each ReLU function at later stages of the classification network. SafetyNet consists of the original classifier along with an adversary detector that examines the internal state of the later layers in the original classifier."}, {"title": "4.2 Manifold-based detectors", "content": "In [42], the key intuition is that the adversarial samples lie on a different manifold than unperturbed samples. The authors argue that if after perturbation the data x is transformed into x*, it can leave the manifold cx in favor of sub-manifold cx* in one of the following three ways illustrated in Figure 4a:\n\u2022 x* is distant from the submanifold of cx* but closer to the classification boundary between cx and cx*;\n\u2022 x* lies closer to cx* submanifold but is still outside. On the other hand, x* is distant from the classification boundary that separates the classes cx and cx*. As shown in Figure 4b, here one of the submanifolds has a pocket.\n\u2022 x* is close to the submanifold cx*, but is still outside. In addition, x* is close to the classification boundary which separates the classes cx and cx*.\nThe authors estimate the manifold with Kernel Density (KD) estimation. They do so with the output of the last hidden layer based on the hypothesis presented in [45], which states that the deeper layers of a DNN offer more linear and 'unwrapped' manifolds compared to the input space. Given an input point x and s set of training points X\u2081 having label 1, the Kernel Density Estimator (KDE) f can be obtained as f = \u03a3x\u2081\u2208X\u2081 k(xi, x), where k(., .) is the kernel function. The latter offers an indication of the distance between x and the submanifold for l. For the point x, if the last hidden layer activation map is \u03c6(x), then the density estimate with predicted class I is k(x, X\u2081) = \u2211x;\u2208x\u2081 ko(\u03c6(x), \u03c6(xi)) where \u03c3 is a tunable bandwidth. While this approach exhibits effective performance in the detection of adversarial samples"}, {"title": "4.3 Local Intrinsic Dimensionality (LID)", "content": "The objective of LID [104] is to characterize the specific regions where adversarial examples may be located. Specifically, this work shows that the KD approach adopted in [42] - which was based on the assumption that the adversarial subspaces are low probability regions - fails to detect some forms of adversarial attack. As an alternative, the authors propose LID to characterize the adversarial subspace. LID represents the dimension of the data submanifold local to the data point x under consideration. In connection to the classical expansion models, treating the probability mass as a proxy for volume may provide information about the dimensional structure of the data. LID considers the Cumulative Distance Function (CDF) of the number of data points encountered F(d), where d is a realization of the random variable D, i.e., the distance from data point x to other data points. The LID of x at a distance d can be defined as:\n$LIDF(d) = \\lim_{\\epsilon \\to 0} \\frac{ln(F((1+\\epsilon) d)/F(d))}{ln(1 + \\epsilon)} = \\frac{d. F' (d)}{F(d)}$.\n(7)\nwhere D is a positive random variable and the CDF F(d) of D is continuously differentiable at distance d > 0. The local dimension at x in turn is defined as LIDF = limd\u2192inf LIDF (d). The LIDF quantifies how quickly the CDF F(d) grows with the distance d. It can be approximated by considering the distances between a point x and its k nearest neighbors within the dataset. The work in [104] hypothesizes that for the estimation of LID of the adversarial samples, the nearest neighbors drawn should come not only from the manifold of the adversarial samples but also from the manifold of the normal samples, as the adversarial submanifold lies close to the data manifold. This will increase the dimension of the adversarial submanifold leading to higher value of LID. This approach exhibits better generalization across different attacks than KD. The authors train logistic regression model with LID feature to discriminate between perturbed and unperturbed samples. The major drawback is that it fails against stronger attacks, which indicates that the characterization of the adversarial manifold with LID is not universal for all adversarial perturbation."}, {"title": "4.4 Interpretability-based Approaches", "content": "Conversely from adopting adversarial sample training [42, 109], the work in [152] examines the adversarial samples from the DNN interpretability point of view. Specifically, a novel adversarial sample detection named Attack Meets Interpretability (AmI) has been proposed for face recognition. In this work, the main hypothesis is that adversarial samples utilize complex features extracted by the DNN that are imperceptible to humans. As such, AmI initially extracts a set of neurons called attribute witness, which are entangled with the face attributes. They substitute parts of the face from one image onto different images and look for unchanged neurons. This is named the Attribute Preservation step. Another step is to substitute the same parts from different images onto a single image - generating versions of the image only part-substituted. The neurons which change in this case are likely to be attributed to that part, which is the Attribute Substitution step. The common neurons obtained from attribute substitution and attribute preservation steps results in attribute witness neurons. The authors then construct an attribute-steered model by increasing the values of the witness neuron and decreasing the values of non-witness neurons. For a given test input, the inconsistency observed between the two models indicates that the input is adversarial. The major drawbacks of this approach are the"}, {"title": "4.5 Statistical Approaches", "content": "The work in this category falls attempts to extract statistical information from different layers of the model to detect adversarial samples. The work in [52] proposes a statistical detection method based on Maximum Mean Discrepancy (MMD). The authors hypothesize that only a limited number of samples is needed to observe a measurable difference between normal and adversarial samples using a statistical test. They use a two-sample hypothesis test on the distribution of MMD values to detect the difference between the normal samples and the adversarial samples. To detect single examples, they augment their classifier with an outlier class and train using the adversarial samples. The work in [83] proposed a Gaussian Discriminant Analysis (GDA)-based approach where the model features from different layers of the DNN as class conditional multivariate Gaussian and calculate the confidence score for a sample as the Mahalanobis Distance between the sample and the closest class conditional distribution. The authors extract such score from all the layers and integrate them using weighted averaging. Both [52, 83] utilize adversarial samples either to train their detector or find appropriate weights. These approaches are also model and attack specific and require separate detectors for different attack approaches. This severely limits their applicability.\nThe work in [153] approaches the detection of adversarial patches from a statistical perspective in the input space. The authors postulate that adversarial patches should contain a statistically higher amount of information, from an information theory perspective, compared to any random neighborhood from a natural image distribution. This leads to the proposal of Jedi, which detects adversarial patches in images based on entropy thresholds. They use a 50-pixel \u00d7 50-pixel kernel and compute the entropy threshold dynamically based on the entropy distribution of the dataset and the image under consideration. The extracted high-entropy patch is passed through a sparse autoencoder for improved localization. Finally, they use coherence transport-based image inpainting [13] which aims at defusing the patch. They obtain high robust accuracy with respect to the baseline methods."}, {"title": "4.6 Influence-based approaches", "content": "The work in [25] proposes a novel adversarial sample detection strategy by using an \"influence function\" [75]. This approach can be employed by any pre-trained DNN. The key intuition is that the training samples have close correspondence to the DNN classification. When this relationship is disrupted, it strongly suggests the presence of an adversarial input. As such, the influence function measures the impact of training data in the decision-making of the DNN. The influence of a training image x on the loss of a specific test image xtest can be measured as lup,loss (x, xtest) =\n \u2212\u2207\u03b8L(xtest, \u03b8)H\u22121\u2207\u03b8L(x, \u03b8) where L is the loss function and H is the Hessian of the machine learning model. The work also applies k-nearest neighbor (k-NN) classifier at the embedding space of the DNN as the resemblance of the nearest neighbor in the embedding space also dictates the decision of the DNN. The combination of the influence function and k-NN classifier"}, {"title": "4.7 Other Notable Approaches", "content": "Input Space Approaches: [188] introduce a diffusion based perturbation method and derive Expected Perturbation Score (EPS). They show that the distribution of EPS is different for normal and perturbed images. The difference in perturbation is measured using MMD. This approach achieves an AUC of 1, and its performance does not degrade for unseen attacks. This is because EPS models the distribution of the input data itself instead of modeling the feature space.\nOne drawback of the EPS score is that it cannot differentiate between adversarial perturbation and noise perturbation.\nThe work in [173] proposes ML-Leave-One-Out (ML-LOO) feature attribution based detection of adversarial samples. The authors observe that feature attribution or mapping of importance of input features to the final prediction behaves differently for unperturbed and adversarially perturbed images. Adversarial perturbation disperses the feature attribution scores with significant deviation from the normal samples. Equipped with this observation, the authors use simple statistics to characterize the deviation of the adversarial samples as it progress through the DNN and aggregate the statistics with a logistic regression model to differentiate between adversarial and normal samples. Although the authors utilize the leave-one-out feature attribution, their approach is generic to any feature attribution method. The work in [184] trains a binary classifier using the saliency data by concatenating the saliency map to the raw image along the channel, while [171] proposes to squeeze input features using bit-depth reduction and spatial smoothing (both local and non-local variants). The work compares the output probability distributions on the original input and the feature-squeezed inputs using L\u2081 distance which changes significantly for adversarial inputs while normal samples show no change. This is because feature squeezing removes unimportant non-robust features improving robustness.\nModel Space Approaches: The work in [159] proposed to modify the DNN and has implemented a self-contained toolkit named mMutant that integrates mutation testing and statistical hypothesis testing on DNNs. The key observation is that the sensitivity of the mutation on DNN is more acute for adversarial samples compared to unperturbed samples. If the DNN is slightly altered, there is a greater chance that the mutated DNN will alter the label of the adversarial sample than the unperturbed sample. The empirical investigation confirms this inherent sensitivity of the adversarial samples against a group of DNN mutants in terms of label change rate (LCR). However, with the increase of mutation rate, the distance of LCR between adversarial and normal samples decreases. This approach is also prone to the generation of some false positives during adversarial detection. Shumailov et al. [142] proposed a mechanism called Certifiable Taboo Trap (CTT), which incorporated the Taboo Trap detection, as well as numerical bound propagation. It prioritizes on finding the overexcited neurons being driven by adversarial perturbations outside of a predetermined range. The incorporation of numerical bound propagation on CTT certifies the detection bounds on activation values"}, {"title": "Output Space Approaches", "content": "Prior work has attempted to characterize adversarial examples in the output space to facilitate their detection. I-defender [193] modeled the distribution of the output of the linear layers and shows that the distributions are different for the normal and adversarially perturbed images. The authors used a mixture of Gaussian models to approximate the Intrinsic Hidden State Distribution (IHSD) for each class. If the class conditional probability is lower than a threshold for a sample, it is detected as adversarially perturbed. However, this approach performs poorly under moderate to strong attacks. Roth et al. [135] proposed a statistical metric for the detection of adversarial samples based on expected value of perturbed log-odds. They showed that the robustness properties of perturbed log-odds statistics are different for natural and adversarial samples. The idea is that geometrically optimal adversarial manipulations are embedded into a cone-like structure they call \"Adversarial Cone\". They also reported an intriguing finding, i.e., that adversarial samples are much closer to the ground truth unperturbed class than any other class. Based on these observations, they proposed the maximum expected deviation of the perturbed log-odds from its expected value as an indicator of an adversarial sample. Chyou et al. [23] proposed an unsupervised adversarial sample detection method without any extra model. They proposed new training losses to improve detection accuracy. The main idea is to remove unnecessary features for false outputs and strengthen the true outputs. This is achieved by forcing all the false raw outputs in a mini-batch to have a uniform distribution during training. By doing so, false outputs become adversarially robust and only true outputs can be attacked. Any attack on the true output changes the raw false output values triggering an adversarial detection. The proposed training loss keeps the accuracy of the original classification task almost the same, around 86% when using ResNet18 architecture and CIFAR10. Although the authors used adversarial examples to determine their threshold, they show that the estimated threshold for their binary detector generalizes for other stronger attacks. The achieved accuracy on CIFAR10 dataset for Resnet architecture falls short of the reported accuracy of >90% [55] although this can be related to the training strategy."}, {"title": "5 DETECTION OF OOD SAMPLES", "content": "Existing approaches for OOD detection mostly operate in the output space. Indeed, it has been shown that working with activations of earlier layers does not provide much improvement in detector performance [68, 148]. In this section, we first provide the discussion of seminal work in OOD detection, and then focus on the state of the art approaches. Table 2 provides a summary of the surveyed work regarding OOD detection."}, {"title": "5.1 Input Space Approaches", "content": "Among the few works in this space, Li et al. [86] forced the classifier to implicitly learn the ID data distribution instead of just learning features for classification. They applied a preprocessing step of masking and then learn to reconstruct the original image from the masked image. This calibrates the DNN for ID and OOD samples. Gao et al. [44] utilized the diffusion model to learn the distribution of ID data. They learned a Denoising Diffusion Implicit Model (DDIM) conditioned to the semantic labels, while during inference time, they inverted the image to obtain the latent representation using the DDIM and them reconstruct it from the latents."}, {"title": "5.2 Output Space Approaches", "content": "ODIN [93] is among the earliest work in the domain of OOD detection in the output (or logit) space. It adopted the baseline [60] where the authors utilize the softmax probability score to distinguish between ID and OOD samples and improve it by incorporating temperature scaling and input pre-processing steps. ODIN [93] shows that by manipulating the temperature parameter T \u2208 R+, it is possible to increase the separation between the ID and OOD examples. The resulting score function S\u1ef9(x; T) is given by Equation 8:\n$S_i (x; T) = \\frac{exp(f_i (x)/T)}{\\sum_{j=1}^{j=N} exp(f_j (x)/T)}$\n$S_{\\hat{y}}(x; T) = \\max_{i} S_i (x; T)$\n(8)\nHere, fi (x) denote the logit value corresponding to i-th class for DNN f. The authors complemented the temperature scaling by perturbing the input image before feeding it into the DNN. The perturbation procedure was inspired by [49], which adds the perturbation to decrease the softmax score but here the perturbation boosts the softmax score prediction. The perturbation follows x = x \u2013 esign(\u2212\u2207xlogS\u1ef9 (x; T)).\nThe authors explained the effect of temperature scaling using U\u2081 = 1-N \u2211i\u2260\u0177[f\u0177(x)-fi(x)] and U2 = 1-N \u2211i\u2260\u0177[f\u0177(x)\u2212 fi(x)]2. They showed that ID data contains some classes that are similar to each other, resulting in a higher value of U2 even when U\u2081 value for ID and OOD data is the same. By taking the Taylor approximation, the softmax function can be expressed as Sy = N-(U\u2081-)/T )/T. This suggests that for very high values of T, the softmax score is dominated by the term U\u2081 compensating the negative effect of U2 on the detection performance. They argue that this makes the ID and OOD data more separable. The authors argue from the Taylor expansion of the log softmax of the perturbed input x that ID images have a larger norm of the gradient of the score function compared to the OOD images which results in a higher score function values after perturbation.\n$\\log S_{\\hat{y}} (x; T) = \\log S_{\\hat{y}} (x; T) + \\epsilon||\\nabla_x \\log S_{\\hat{y}}(x; T)||_1 + O(\\epsilon)$\n(9)\nThe key observations are (i) DNNs produce outputs with larger variance for ID examples; and (ii) DNNs have larger value of the gradients of the log-softmax score when applied to ID images.\nGeneralized ODIN [65] improved [93] without utilizing any OOD samples. The authors point out the limitation of the softmax classifier, as it is an approximation of the indicator function it gives a categorical distribution rather"}, {"title": "5.3 Approaches based on Energy Functions", "content": "The work [96] provides a solution to OOD detection problem in the logit space. It connects the posterior probability in Gibb's distribution and the posterior probability of the softmax layer and proposes the equivalent of Helmholtz energy function for the softmax function as the score function for discriminating between ID and OOD samples. The key observation is the similarity between Gibb's density function in Equation 10 and the softmax function in Equation 11. By connecting these two equations, the energy for a specific sample is defined by E(x, y) = \u2212fy(x):\n$p(y/x) = \\frac{e^{-E(x,y)/T}}{\\sum_{y,} e^{-E(x,y,)/T}}$\n(10)\n$p(y/x) = \\frac{e^{f_y(x)/T}}{\\sum_{i=1}^K e^{f_i (x)/T}} = \\frac{e^{f_y(x)/T}}{e^{E(x)/T}}$\n(11)\nThis also leads to the equivalent of free-energy E(x) for DNN as E(x; f) = \u2212T \u03a3=K efi (x)/T. The authors show that the Energy function is affinely related to the logarithm of the probability of the input p(x). On the other hand, the logarithm of the softmax score or the softmax score itself is not related to log p(x) linearly and depends on the maximum logit value. The authors also proposed an energy-based regularizer for training the DNN, shown in Equation 12 and Equation 13, and demonstrated that it improves the performance of the detector.\n$mino \\mathbb{E}_{(x,y)~D_{train}} [-logF_y(x)] + \\lambda L_{Energy}$\n(12)\n$L_{Energy} = \\mathbb{E}_{(x,y)~D_{train}} (max(0, E(x_{in}) \u2013 min))^2 + \\mathbb{E}_{(x,y)~D_{train}} (max(0, min E(x_{out})))^2$\n(13)\nDespite having the advantage of being easy in nature, the energy-based OOD detection method requires access to the OOD distribution either to determine the threshold or to train the network with regularization."}, {"title": "5.4 Approaches based on Activation Shaping", "content": "These approaches assume that the features of a DNN affect the output differently for ID and OOD samples. As such, they rely on some predefined transformation of the activation values of intermediate layers of DNN to differentiate between ID and OOD samples. ReAct [148] is a rectification operation on the activation in model space to facilitate the detection of OOD samples by making ID and OOD samples more separable. The activations from the penultimate layer h(x) of the DNN are truncated using ReAct operation given by h(x) = ReAct(h(x; c)), where ReAct(x; c) = min(x, c).\nThe rectified activations are used to obtain the model output as fReAct (x; \u03b8) = WTh(x) + b. These outputs can be used with any score function for OOD detection. The detection performance depends on the threshold value c. Through empirical study, the authors showed that setting the value of c to the 90th percentile of the activations works best for ReAct. The authors theoretically showed that ReAct suppresses the activations more for the OOD samples than ID samples. This translates to a larger reduction in the output value for the OOD samples making them separable from the ID samples. When batch-normalization statistics calculated for ID data are applied to OOD data, significantly different activation patterns emerge as shown by the authors in Figure 7. The approach is also shown to work for DNN architectures using normalization techniques other than batch-normalization (e.g. weight normalization, group normalization)."}, {"title": "5.5 Approaches based on Mitigating Overconfidence in Prediction", "content": "LogitNorm [164] observes that the norm of the logit vector increases as the training progresses. As up-scaling the logits increases the maximum softmax probability, to normalize the logit values to address the issue. Their key idea is to decouple the influence of the norm of the output from the training objective and optimization step. They decomposed"}, {"title": "5.6 Approaches based on Training with Regularization", "content": "Some prior work assumes access to OOD data - also referred to as surrogate OOD data - that can be used at training time. Specifically, these approaches propose a regularization term to be added to the training loss that helps improve detection of OOD samples. The key issue with surrogate OOD data is that the OOD distribution may not be fully characterized. As a result, these methods perform poorly on unseen OOD distributions. Outlier Exposure (OE) [61", "160": "proposes an improvement over OE by making it distribution- agnostic. DOE is composed of OOD generation using model perturbation and worst OOD regret (WOR) based training. The perturbation is done with a matrix multiplying the DNN parameters, which leads to features resembling ones sampled from the OOD distribution. The WOR score measures the worst performance of the OOD detector and helps discover the hardest OOD samples. The hardest OOD samples are simulated by perturbing the model parameters.\nDiversified"}]}