{"title": "OMEGA: Efficient Occlusion-Aware Navigation for Air-Ground Robots in Dynamic Environments via State Space Model", "authors": ["Junming Wang", "Dong Huang", "Xiuxian Guan", "Zekai Sun", "Tianxiang Shen", "Fangming Liu", "Heming Cui"], "abstract": "Air-ground robots (AGRS) are widely used in surveillance and disaster response due to their exceptional mobility and versatility (i.e., flying and driving). Current AGR navigation systems perform well in static occlusion-prone environments (e.g., indoors) by using 3D semantic occupancy networks to predict occlusions for complete local mapping and then computing Euclidean Signed Distance Field (ESDF) for path planning. However, these systems face challenges in dynamic scenes (e.g., crowds) due to limitations in perception networks' low prediction accuracy and path planners' high computation overhead.\nIn this paper, we propose OMEGA, which contains OccMamba with an Efficient AGR-planner to address the above-mentioned problems. OccMamba adopts a novel architecture that separates semantic and occupancy prediction into independent branches, incorporating the mamba module to efficiently extract semantic and geometric features in 3D environments. This ensures the network can learn long-distance dependencies and improve prediction accuracy. Features are then combined within the Bird's Eye View (BEV) space to minimise computational overhead during feature fusion. The resulting semantic occupancy map is integrated into the local map, providing occlusion awareness of the dynamic environment. Our AGR-Planner utilizes this local map and employs Kinodynamic A* search and gradient-based trajectory optimization for ESDF-free and energy-efficient planning. Experiments demonstrate that OccMamba outperforms the state-of-the-art 3D semantic occupancy network with 25.0% mIoU. End-to-end navigation experiments in dynamic scenes verify OMEGA's efficiency, achieving a 96% average planning success rate.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, air-ground robots (AGRs) have attracted significant attention from both academia and industry due to their versatile navigation capabilities in the ground and aerial domains. To enhance AGR navigation in occlusion-prone environments, existing works employ sensors, such as depth cameras, to collect point clouds. These point clouds are then processed by 3D semantic occupancy networks, which predict occluded areas and generate a complete local map. Based on the local map, an Euclidean Signed Distance Field (ESDF) map is constructed for hybrid aerial-ground path planning."}, {"title": "II. RELATED WORK", "content": "Numerous researchers have explored various aerial-ground robot configurations, such as incorporating passive wheels, cylindrical cages or multi-limb onto drones. Recently, Fan et al. address ground-aerial motion planning. Their approach initially employs the A* algorithm to search for a geometric path as guidance, favouring ground paths by adding extra energy costs to aerial paths. Zhang et al. proposed a path planner and controller capable of path searching, but it relies on an ESDF map. The intensive computation and limited perception of occluded areas lead to a low success rate in path planning and increased energy consumption. Wang et al. proposed AGRNav, the first AGR navigation system with occlusion perception capability. Although it performs well in static environments, its simple perception network structure design and the defects of the path planner make it difficult to operate in complex and changing dynamic environments. Our work aims to explore new AGR navigation systems for efficient navigation in dynamic environments."}, {"title": "B. 3D Semantic Occupancy Prediction", "content": "3D semantic occupancy prediction is crucial for interpreting occluded environments, as it discerns the spatial layout beyond visual obstructions by merging geometry with semantic clues. This process enables autonomous systems to anticipate hidden areas, crucial for safe navigation and decision-making. Research on 3D semantic occupancy prediction can be summa-rized into three main streams: Camera-based approaches cap-italize on visual data, with pioneering works like MonoScene by Cao et al. exploiting RGB inputs to infer indoor and outdoor occupancy. Another notable work by Li et al. is VoxFormer, a transformer-based semantic occupancy frame-work capable of generating complete 3D volume semantics using only 2D images. LiDAR-based approaches like S3CNet by Cheng et al. , JS3C-Net by Yan et al. , and SSA-SC by Yang et al. , which adeptly handle the vastness and variability of outdoor scenes via point clouds. Fusion-based approaches aim to amalgamate the contextual richness of camera imagery with the spatial accuracy of LiDAR data. The Openoccupancy benchmark by Wang et al. is a testament to this synergy, providing a platform to assess the performance of integrated sensor approaches."}, {"title": "C. State Space Models (SSMs) and Mamba", "content": "State-Space Models (SSMs), including the S4 model, have proven effective in sequence modelling, particularly for capturing long-range dependencies, outperforming traditional CNNs and Transformers. The Mamba model builds on this by introducing data-dependent SSM layers, offering"}, {"title": "III. 3D SEMANTIC OCCUPANCY NETWORK OF OMEGA", "content": "OccMamba features three branches: semantic, geometric, and BEV fusion. The semantic and geometric branches are supervised by multi-level auxiliary losses, which are removed during inference. Multi-scale features generated by these two branches are fused in the BEV space to alleviate the computational overhead caused by dense feature fusion."}, {"title": "A. OccMamba Network Structure", "content": "Semantic Branch: The semantic branch consists of a vox-elization layer and three encoder Sem-Mamba Blocks with the same structures. The input point cloud $P \\in \\mathbb{R}^{N\\times3}$ is converted to a multi-scale voxel representation at a voxel resolution $s$ by the voxelization layer. These voxels are then aggregated by maximum pooling to obtain a unified feature vector for each voxel. The vectors from different scales are merged to form the final voxel feature $V_{fm}$, with a size of $L\\times W\\times H$, where $f_m$ represents the voxel index. The semantic features {S1, S2, S3} are projected into the bird's-eye view (BEV) space by assigning a unique BEV index to each voxel based on its $f_m$ value. Features sharing the same BEV index are aggregated using max pooling, resulting in sparse BEV features. These sparse features are then densified using the densification function of Spconv [26] to produce dense BEV features {BEVsem,1, BEVsem,2, BEVsem,3}.\nSem-Mamba Block: Mamba, a selective state-space model, has recently outperformed CNN- and Transformer-based approaches in various vision tasks due to its efficient long-distance sequence modelling and linear-time complexity. These characteristics make Mamba promising for improving 3D semantic occupancy prediction accuracy while maintaining fast reasoning in dynamic environments. Inspired by mamba's success, we introduce the Sem-Mamba block as the semantic branch encoder in our network to enable efficient semantic representation learning. Specifically, state space models introduce hidden states $h(t) \\in \\mathbb{R}^{N}$ to map inputs $x(t) \\in \\mathbb{R}^{L}$ to outputs $y(t)$, with the continuous state space dynamics governed by:\n$$h'(t) = Ah(t) + Bx(t), y(t) = Ch(t)  (1)$$\nUsing a time scale parameter $\\Delta$, the Mamba model discretizes the continuous parameters, yielding the discretized state space equations:\n$$\\overline{A} = exp(\\Delta A), \\overline{B} = (\\Delta A)^{-1}(exp \\Delta (A) - I) \\cdot \\Delta B  (2)$$\n$$h(t) = \\overline{A}h_{t-1}+ \\overline{B}x_{t}, Y_t = Ch_t (3)$$\nThe global convolution kernel $K \\in \\mathbb{R}^{L}$ is used to calculate the output $y$:\n$$K = (CB, CAB, ...,CA^{M-1}B), y = x * K (4)$$\nIn each Sem-Mamba Block, dense BEV features serve as the input $x$ to the mamba module (Fig. 3). By applying discretized SSM dynamics and global convolution kernels, the mamba block effectively processes the BEV features, resulting in an enhanced feature representation with richer long-distance dependencies. This enhanced representation im-proves the performance of semantic occupancy prediction by capturing and strengthening the spatial relationships between semantic elements. Meanwhile, mamba block's parallel feature learning properties ensure real-time reasoning in dynamic environments."}, {"title": "Geometry Branch and Geo-Mamba Block:", "content": "The geometric branch (Fig. 3) begins with an input layer using a 7 \u00d7 7 \u00d7 7 kernel and comprises three Geo-Mamba blocks as the encoder. Each Geo-Mamba block maintains a consistent architecture, combining a residual block with a mamba module and a BEV projection module. The residual block first processes the voxels $V \\in \\mathbb{R}^{1\\times L\\times W\\times H}$ obtained from the point cloud, and its output features serve as the input $x$ to the mamba module. The mamba module generates multi-scale dense geometric features {G1, G2, G3}, which enrich the captured geometric details. By leveraging the mamba module's ability to capture long-distance dependencies with linear complexity, the geometric branch can effectively process and refine the geometric infor-mation within the voxels. Subsequently, the dense 3D features are aligned along the z axis, and 2D convolutions are applied"}, {"title": "B. Optimization", "content": "Based above of our methods above, there are four main terms in our loss function. The semantic loss $L_{sem}$ is the sum of the lovasz loss [28] and cross-entropy loss [29] at each stage of the semantic branch:\n$$L_{sem} = \\sum_{i=1}^{3}(L_{cross,i} + L_{lovasz,i}) (5)$$\nThe training loss $L_{com}$ for this branch is calculated as follows:\n$$L_{com} = \\frac{1}{3}\\sum_{i=1}^{3}(L_{binary_cross,i} + L_{lovasz,i})  (6)$$\nwhere i denotes the i th stage of the completion branch and $L_{binary_cross}$ indicates the binary cross-entropy loss. The BEV loss $L_{bev}$ is :\n$$L_{bev} = 3 \\times (L_{cross} + L_{lovasz})  (7)$$\nWe train the whole network end-to-end. The overall objective function is:\n$$L_{total} = L_{bev} + L_{sem} + L_{com} (8)$$"}, {"title": "IV. AGR MOTION PLANNER", "content": "We introduce AGR-Planner, a novel gradient-based local planner tailored for AGRs built upon EGO-Swarm [14]. It features a Kinodynamic A* algorithm for efficient pathfind-ing and a gradient-based method for trajectory optimization, streamlining the planning process."}, {"title": "A. Kinodynamic Hybrid A* Path Searching", "content": "Our AGR-Planner begins by generating a preliminary \"ini-tial trajectory\" \u03b9 (see Fig. 4a), which initially disregards obstacles by incorporating random coordinate points, anchored by the start and end locations. Subsequently, for any \"col-lision trajectory segment\" found within obstacles, we em-ploy the kinodynamic A* algorithm to create a \"guidance trajectory segment\u201d \u03c4. This segment is defined using motion primitives rather than straight lines for edges during the search, incorporating additional energy consumption metrics for flying (Fig. 4a). This approach encourages the planning of ground trajectories, resorting to aerial navigation only when confronting significant obstacles, thus optimizing energy efficiency."}, {"title": "B. Gradient-Based B-spline Trajectory Optimization", "content": "B-spline Trajectory Formulation: In trajectory optimization (Fig. 4b), the trajectory is parameterized by a uniform B-spline curve \u0398, which is uniquely determined by its degree \u0440\u044c, Ne control points {$Q_1, Q_2, Q_3, ..., Q_N$}, and a knot vector {$t_1, t_2, t_3, ..., t_{M-1},t_m$}, where $Q_i \\in \\mathbb{R}^3$, $t_m \\in \\mathbb{R}$, $M = N + p_b$. Following the matrix representation of the [4] the value of a B-spline can be evaluated as:\n$$\\Theta(u) = [1, u, ..., u^{p_b}] \\cdot M_{p_b+1} \\cdot [Q_{i-p_b}, Q_{i-p_b+1}, ..., Q_{i}]^T (9)$$\nwhere $M_{p_b+1}$ is a constant matrix depends only on $p_b$. And $u = (t - t_i)/(t_{i+1} - t_i)$, for $t \\in [t_i,t_{i+1})$. In particular, in ground mode, we assume that AGR is driving on flat ground so that the vertical motion can be omitted and we only need to consider the control points in the two-dimensional horizontal plane, denoted as $Q_{ground} = {Q_{t0}, Q_{t1}, ..., Q_{tM}}$,"}, {"title": "Collision Avoidance Force Estimation:", "content": "Inspired by [14], for each control point on the collision trajectory segment, vector v (i.e., a safe direction pointing from inside to outside of that obstacle) is generated from \u03b9to\u03c4and pis defined at the obstacle surface (in Fig. 4a). With generated {$p, v$} pairs, the planner maximizes $D_{ij}$ and returns an optimized trajectory. The obstacle distance $D_{ij}$ if $i^{th}$ control point $Q_i$ to $j^{th}$ obstacle is defined as:\n$$D_{ij} = (Q_i - P_{ij}) \\times V_{ij} (11)$$\nBecause the guide path 7 is energy-saving, the generated path is also energy efficient (in Fig. 4a). To discover multiple viable paths that navigate through different local minima in a dynamic environment, we apply the method described in [14]. This method generates distance fields in various directions by reversing vector $v_1$ to obtain $v_2 = -v_1$. Following this, a search process identifies a new anchor point $p_2$ on the surface of the obstacle along $v_2$, as shown in Fig. 4a. This approach enables us to evaluate alternative trajectories and select the most cost-effective path for navigation."}, {"title": "Air-Ground Hybrid Trajectory Optimization:", "content": "Based on the special properties of AGR bimodal, we first adopt the follow-ing cost terms designed by Zhou et al. [14]:\n$$min J_{AGR} = \\sum_{\\phi} \\lambda_{\\phi} J_{\\phi} (12)$$\nwhere $ = {s, c, d, t}$ and the subscripted $\u03bb$ indicates the cor-responding weights. In addition, $J_s$ is the smoothness penalty, $J_c$ is for collision, $J_d$ is for dynamically feasibility, and $J_t$ is for terminal progress. \u03bbs, \u03bbc, \u03bbd, \u03bbt are weights for each cost terms. Meanwhile, $J_s$ and $J_t$ belongs to minimum error which minimize the total error between a linear transformation of decision variables L(Q) and a desired value D. $J_c$ and $J_d$ belongs to soft barrier constraint which penalize decision vari-ables exceeding a specific threshold \u00c7. Subsequently, based on our observations, AGR also faces non-holonomic constraints when driving on the ground, which means that the ground velocity vector of AGR must be aligned with its yaw angle. Additionally, AGR needs to deal with curvature limitations that arise due to minimizing tracking errors during sharp turns. Therefore, a cost for curvature needs to be added, and Jn can be formulated as:\n$$J_n = \\sum_{i=1}^{M-1} F_n(Q_{ti}) (13)$$\nwhere $F_n(Q_{ti})$ is a differentiable cost function with $C_{max}$ specifying the curvature threshold:\n$$F_n(Q_{ti}) =\\begin{cases}(C_i-C_{max})^2, C_i > C_{max}\\\\0, C_i C_{max}\\end{cases}(14)$$\nwhere $C_i = \\frac{\\Delta Q_{ti}}{\\Delta p_i}$ is the curvature at $Q_{ti}$, and the $\\Delta B_i = tan^{-1} \\frac{\\Delta y_{ti+1}}{\\Delta x_{ti+1}} - tan^{-1} \\frac{\\Delta y_{ti}}{\\Delta x_{ti}}$. In general, the overall objective function is formulated as follows:\n$$min J_{AGR} = \\lambda_s J_s + \\lambda_c J_c + \\lambda_d J_d + \\lambda_t J_t + \\lambda_n J_n  (15)$$\nThe optimization problem is addressed with the NLopt. Meanwhile, the same methods as in [4] are used for trajectory tracking and control, as well as additional mode switching."}, {"title": "V. EVALUATION", "content": "We evaluate OccMamba on the SemanticKITTI benchmark and integrate the best pre-trained model with AGR-Planner to establish our comprehensive OMEGA system. We then assess OMEGA's autonomous navigation efficiency for AGRS in simulated and real-world dynamic environments, focusing on metrics like planning success rate, average movement and planning time, and energy consumption. Finally, ablation experiments verify the navigation efficiency improvements brought by OMEGA's two key components."}, {"title": "A. Evaluation setup", "content": "3D Semantic Occupancy Network: We trained OccMamba using the outdoor SemanticKITTI dataset on a single NVIDIA 3090 GPU. This dataset [33] focuses on semantic occu-pancy prediction with LiDAR point clouds and camera im-ages. Specifically, the ground truth semantic occupancy is represented as the [256,256,32] voxel grids. Each voxel is [0.2m, 0.2m, 0.2m] large. OccMamba was trained for 80 epochs with a batch size of 6, using the AdamW optimizer at an initial learning rate of 0.001, and input point cloud augmentation by random flipping along the x y axis.\nSimulation Experiment: We conducted our simulation experi-ments on a laptop running Ubuntu 20.04 with an NVIDIA RTX 4060 GPU. The experiments took place in a 20m \u00d7 20m \u00d7 5m simulated environment, and a total of 200 trials were per-formed. The first set of 100 trials featured an environment pop-ulated with 80 walls and 20 rings, while the second set of 100 trials included 80 moving cylinders and 20 stationary rings. To generate a diverse range of occlusions and unknown areas, the positions of the obstacles were varied in each trial (Fig. 6). The objective was for the AGR equipped with OMEGA to navigate through these environments while avoiding collisions.\nReal-world Experiment: In our real-world experiments, OMEGA was operational on a customed AGR platform, illus-trated in Fig. 2. The platform employed a RealSense D435i for point cloud acquisition, a T265 camera for visual-inertial odometry, and a Jetson Xavier NX for real-time onboard computation. OccMamba was optimized with TensorRT for efficient offline inference, guaranteeing smooth and respon-sive navigation. Our evaluation is around the following key questions:\nRQ1: How does OccMamba's efficiency compare to base-line models? (\u00a7 V-B)\nRQ2: How well does OMEGA navigate in dynamic simula-tion environments? (\u00a7 V-C)\nRQ3: How effectively does OMEGA navigate in five real-world dynamic scenarios? (\u00a7 V-D)"}, {"title": "B. OccMamba Comparison against the state-of-the-art.", "content": "Quantitative Results: OccMamba has achieved state-of-the-art performance on the SemanticKITTI hidden test dataset, boasting a completion IoU of 59.9% and a mIoU of 25.0% (Table II). While recent methods (e.g., M-CONet [21] and Co-Occ [32]) have shown commendable performance in semantic occupancy prediction tasks, OccMamba outperforms the latter by a notable margin of 5.8% in IoU. This leap in accuracy is achieved solely by utilising point cloud data, without the need for other modalities inputs, simplifying integration and deployment in real-world robotic applications. In addition, OccMamba's efficiency parallels its performance; it integrates a linear-time mamba block within a compact, multi-branch network architecture and employs BEV feature fusion, culmi-nating in a lean framework, weighing only 23.8MB (Table III). Meanwhile, OccMamba operates on a modest 3.5GB of GPU memory per training batch and impresses with an inference speed of 22.1 FPS, markedly outpacing existing methods like Monoscene [5] and TPVFormer [30] by a factor of 20.\nQualitative Results: In the visualizations on the Se-manticKITTI validation set (Fig. 3), OccMamba demonstrates excellent semantic occupancy predictions in occlusion areas, particularly for complex or moving categories such as \u201cveg-etation\u201d, \u201cterrain\u201d, \u201cperson\u201d, and \u201cbicycle\u201d, aligning with the quantitative results in Table II. These reliable predictions are essential for subsequent path planning.\nAblation Study: The ablation experiments on the Se-manticKITTI validation set (Table IV) underscore the sig-nificance of the Sem-Mamba and Geo-Mamba blocks in our network architecture. Separating semantic and geometric feature processing promotes focused representation learning"}, {"title": "C. Simulated Air-Ground Robot Navigation", "content": "We conducted a comparative analysis of our OMEGA sys-tem, TABV [2] and AGRNav [1]. Through 200 trials with varied obstacle placements, we evaluated the average moving time, planning time, and success rate (i.e., collision-free) of each system (Fig. 6c). In dynamic environments, our system OMEGA demonstrates an average planning success rate of 98% (Fig. 6c). This achievement stems not solely from OccMamba's real-time and precise mapping of occluded areas, which facilitates complete local map acquisition for planning, but also from the innovative design of our AGR-Planner, which plans multiple candidate trajectories without the need for an ESDF map. When benchmarked against two ESDF-reliant navigation baselines, our AGR-Planner reduces planning time by a substantial 89.33% (Fig. 6c). This synergy between OccMamba's detailed environmental perception and AGR-Planner's efficient path computation guarantees swift and reliable navigation for AGRs in complex, dynamically changing, and visually obstructed scenarios."}, {"title": "D. Real-world Air-Ground Robot Navigation", "content": "We demonstrated OMEGA's superior efficiency and energy conservation in 5 dynamic scenarios with a velocity cap of 1.5 m/s. Each navigation method was tested 10 times per scenario for reliable performance comparison. We achieved a notable 96% average success rate (Fig. 7a) in 5 dynamic scenarios with OMEGA, exhibiting lower average energy consumption rela-tive to competitors (Fig. 7c). Specifically, in scenarios B and C, OMEGA recorded energy reductions of 22.1% and 17.28%, respectively, against AGRNav. These improvements are largely due to our OccMamba module's swift computational ability to predict obstacle distributions in non-visible areas, enabling the AGR system to avoid potential impediments. When integrated with the AGR-Planner, this foresight allows for the rapid generation of multiple candidate paths, optimizing for both energy efficiency and reduced traversal time (Fig. 7b)."}, {"title": "VI. CONCLUSION", "content": "In this letter, we introduce OMEGA, an advanced system for air-ground robot (AGR) navigation in dynamic environ-"}]}