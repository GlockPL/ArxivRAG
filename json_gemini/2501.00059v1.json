{"title": "Large Language Models for Mathematical Analysis", "authors": ["Ziye Chen", "Hao Qi"], "abstract": "Mathematical problem-solving is a key field in artificial intelligence (AI) and a critical benchmark for evaluating the capabilities of large language models (LLMs). While extensive research has focused on mathematical problem-solving, most existing work and datasets concentrate on computational tasks, leaving gaps in areas like mathematical analysis, which demands rigorous proofs and formal reasoning. We developed the DEMI-MathAnalysis dataset, comprising proof-based problems from mathematical analysis topics such as Sequences and Limits, Infinite Series, and Convex Functions. We also designed a guiding framework to rigorously enhance LLMs' ability to solve these problems. Through fine-tuning LLMs on this dataset and employing our framework, we observed significant improvements in their capability to generate logical, complete, and elegant proofs. This work addresses critical gaps in mathematical reasoning and contributes to advancing trustworthy AI capable of handling formalized mathematical language. The code is publicly accessible at LLMs for Mathematical Analysis.", "sections": [{"title": "1 Introduction", "content": "Mathematical analysis, with its emphasis on rigorous proofs and formal methods like the \u03b5-\u03b4 definition of limits, poses a significant challenge for artificial intelligence (AI). Large language models (LLMs) have shown remarkable advancements in solving computational problems across various domains, yet they often struggle with the formal rigor and reasoning required for mathematical analysis. Generating correct and structured solutions in this field remains a challenging task, as LLMs frequently rely on uncritical, computational shortcuts or fail to produce logically sound proofs. Existing mathematical datasets for fine-tuning and benchmarking LLMs primarily focus on com-putational tasks in areas like algebra, calculus, and geometry, while deliberately avoiding proof-based problems. This limitation hinders the development of LLMs capable of solving problems that require precise reasoning and formal processes.\nTo address this gap, we developed the DEMI-MathAnalysis dataset, a specialized corpus of proof-based problems in real analysis sourced from Problems in Mathematical Analysis (Demidovich, 1964) and Problems and Solutions in Real Analysis (Hata, 2007). This dataset includes diverse topics such as Sequences and Limits, Infinite Series, and Convex Functions. Additionally, we designed a guiding framework to improve LLMs' ability to generate rigorous, clear, and logically sound solutions.\nBy fine-tuning models like Llama 3.2 (Meta, 2024) and Qwen2 (Alibaba, 2024) on this dataset and applying the framework, our work advanced the field of AI-facilitated reasoning and contributed to building trustworthy AI capable of handling complex, formalized mathematical languages."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Mathematics Benchmarks for AI", "content": "The rapid development of AI has prompted the continuous introduction of mathematics benchmarks. GSM8K (Cobbe et al., 2021) is a dataset of grade school math word questions, while MATH (Hendrycks et al., 2021) contains challenging competition problems. MathVerse (Zhang et al., 2024b) collected multi-subject problems with diagrams, and GeoEval (Zhang et al., 2024a) facilitated a deeper investigation into the performance of LLMs in solving geometry problems. TAL-SCQ5K \u00b9 consists of multiple-choice questions in both English and Chinese. We collect and present the distribution of mainstream dataset topics in Figure 1."}, {"title": "2.2 LLMs for Mathematics", "content": "As extensive research progresses, LLMs have demonstrated proficiency in solving mathematical problems. Trinh et al., 2024 introduced Alpha-Geometry, a theorem prover for Euclidean plane geometry that approached the performance of an average International Mathematical Olympiad gold medallist using a neuro-symbolic system. Studies by Wang et al., 2023 show that LLMs with over 100B parameters are capable of addressing intricate tasks by employing a chain-of-thought (CoT) (Wei et al., 2023) when given a limited set of reasoning examples. And CoT frameworks have been used to enhance mathematical performance, incorporating other tools (He-Yueya et al., 2023). Researchers in OpenAI claimed that their newest model (OpenAI, 2024b) placed among the top 500 students in the U.S. in a qualifier for the USA Math Olympiad. However, some of the excellent performance on these benchmarks may result from memorizing the data from these benchmarks (Xu et al., 2024).\nIn the studies above, the impressive performance of LLMs is often shown by solving problems from fields like geometry or algebra since they can quickly be evaluated by checking the result. However, the importance of the solution process is ignored. We focus on the analyzing process to unlock the more comprehensive language potential of state-of-art models."}, {"title": "3 Motivation", "content": "LLMs rely on extensive datasets to develop their reasoning and problem-solving capabilities. However, most existing mathematical datasets for fine-tuning and benchmarking LLMs focus on computational tasks in domains such as algebra, number theory, calculus, geometry, statistics, and linear algebra. These datasets almost exclusively contain questions that require numerical or symbolic computation while deliberately avoiding proof-based problems. This leaves a significant gap in the ability of LLMs to handle rigorous reasoning and formal proofs, especially in mathematical analysis.\nMathematical analysis requires a deep understanding of formal processes, such as e-\u03b4 proofs and precise logical reasoning. Current LLMs often fail to produce solutions that adhere to these rigorous standards, as they are trained primarily on datasets that emphasize computation over formal derivation. To address this limitation, we developed the DEMI-MathAnalysis dataset, which focuses on proof-based problems. By incorporating problems that emphasize rigor and logical progression, this dataset aims to equip LLMs with the tools needed to handle formalized mathematical reasoning. This work is motivated by the goal of enhancing LLMs' performance in domains that demand rigorous use of languages, ultimately contributing to the development of more trustworthy and capable AI systems. Specifically, this study aims to reach the following research objectives:\n\u2022 RQ1: How can we develop a dataset for pre-training and benchmarking LLMs on mathematical analysis?\n\u2022 RQ2: How can we create a framework to improve LLMs' ability to solve mathematical analysis problems?\n\u2022 RQ3: How can we effectively evaluate the solutions generated by LLMs to ensure correctness and rigor?"}, {"title": "4 Dataset", "content": "Given that there has been no benchmark for evaluating mathematical proofs, the DEMI-MathAnalysis dataset is designed to enhance the ability of LLMs to solve mathematical analysis problems using rigorous and formally restricted methods, particularly the 6-8 technique. The dataset is divided into two parts: one for pretraining and one for benchmarking. Files can be founded at DEMI-MathAnalysis."}, {"title": "4.1 Dataset Creation", "content": "DEMI-MathAnalysis is built from problems in renowned texts. It covers topics like Sequences and Limits, Infinite Series, Continuous Functions, Differentiation, Integration, Improper Integrals, Series of Functions, Approximation by Polynomials, and Convex Functions. We ensure that topics of questions are distributed as evenly as possible and present the statistics in Figure 2."}, {"title": "4.2 Dataset Structure", "content": "Each entry in the DEMI-MathAnalysis dataset consists of four components:\n1. Number: A serial identifier linked to the original problem in the source material, enabling easy cross-referencing.\n2. Problem Type: A classification of the problem by its mathematical domain, aiding targeted training and evaluation.\n3. Problem: The problem statement, formatted in LaTeX to ensure clarity and precision for both humans and LLMs.\n4. Solution: A comprehensive, step-by-step solution written with formal mathematical rigor, highlighting key reasoning steps and adhering to standardized notation.\nThis dataset serves as a foundation for fine-tuning LMMs in the following section. It bridges gaps in existing mathematical datasets that focus primarily on computational tasks, ensuring that models are exposed to diverse problem types and guided toward generating logically sound, clear, and complete solutions."}, {"title": "5 Guiding Framework", "content": "Based on the dataset, we proposed a framework for mathematical analysis that integrates functional components to guide LLMs in solving problems in a human-like way. By combining problem classification, knowledge retrieval, and solution generation, the framework introduces an adaptable pipeline for addressing reasoning and formatting complexities."}, {"title": "5.1 Components", "content": "As shown in Figure 4, the framework consists of the following key components:\n1. Problem Identification: The input problem is first analyzed and classified into a specific category. This classification is performed by a lightweight LLM classifier trained on metadata from the DEMI-MathAnalysis dataset. Accurate classification ensures that the subsequent steps are tailored to the problem's mathematical domain, aligning the solution process with its requirements.\n2. Prompt Construction: Once the problem is classified, a detailed prompt is constructed to guide the LLM in generating a solution. The prompt includes:\n\u2022 The full problem statement to provide complete context.\n\u2022 The problem type, as determined by the classifier, to help the model focus on the appropriate reasoning approach.\n\u2022 Supplementary knowledge retrieved dynamically from the Knowledge Base to provide relevant mathematical context, such as theorems, definitions, or key properties.\nThis process ensures that the LLM receives all necessary information in a structured format, optimizing its ability to process and solve the problem.\n3. Knowledge Base Integration: The Knowledge Base is a curated repository of mathematical concepts, rules, and formal methods specific to mathematical analysis. It includes:\n\u2022 Key definitions, such as the e-d definition of limits.\n\u2022 Theorems and properties, such as those related to series convergence or convexity.\n\u2022 Problem-specific heuristics, such as step-by-step methods for proving continuity or differentiability.\nDuring prompt construction, relevant entries from the Knowledge Base are retrieved and incorporated into the prompt. This step ensures that the LLM is equipped with the required mathematical context, reducing reliance on general knowledge and increasing the rigor of the solution.\n4. Solution Generation: The Problem Solver module, powered by a fine-tuned LLM, uses the constructed prompt to generate a detailed solution. The solution generation process emphasizes:\n\u2022 Logical rigor: Ensuring each step logically follows from the previous one.\n\u2022 Completeness: Addressing all aspects of the problem and avoiding gaps in reasoning.\n\u2022 Clarity: Presenting the solution in a well-organized and comprehensible manner.\nThe Problem Solver incorporates formal reasoning techniques, such as \u03b5-\u03b4 proofs, series approximations, and convexity arguments, to produce solutions that adhere to the rigorous standards of mathematical analysis."}, {"title": "5.2 Features and Benefits", "content": "This framework bridges the gap between computational problem-solving and rigorous proof-based reasoning by equipping LLMs with the tools and methodologies needed to tackle formalized mathematical problems. It introduces several innovations to enhance LLMs' problem-solving ability, which requires powerful reasoning abilities:\n\u2022 Dynamic Prompt Adaptation: Prompts are dynamically tailored based on the problem type and retrieved knowledge, ensuring relevance and context-specific guidance.\n\u2022 Fine-Tuned Models: The framework fine-tunes mainstream models on the DEMI-MathAnalysis dataset, optimizing them for proof-based thinking.\n\u2022 Formal Reasoning Integration: The framework explicitly incorporates formal methods such as e-\u03b4 proofs and theorems on series convergence into the solution process."}, {"title": "6 Experiment and Results", "content": "To evaluate the effectiveness of our framework and provide a baseline of the DEMI-MathAnalysis dataset, we conducted extensive experiments using multiple language models. Our goal was to measure the improvements in solving proof-based problems in mathematical analysis, focusing on logical rigor, completeness, and clarity."}, {"title": "6.1 Experiment Setup", "content": "We applied our framework to two relatively small models (Llama-3.2-3B and Qwen-2.5) and tested the state-of-the-art OpenAI 01-preview model. The prompts for fine-tuning and inference are listed in Appendix C. With the help of Unsloth (Han and Han, 2023), we fine-tuned the models faster with less memory cost. The hyper-parameter settings can be found in Appendix B."}, {"title": "6.2 Evaluation Setup", "content": "For evaluation, we utilized GPT-40 (OpenAI, 2024a) as an expert. The evaluation was based on five key indicators, with a total score of 10 points:\n\u2022 Correctness: Logical rigor and adherence to problem requirements.\n\u2022 Completeness: Full justification of all steps and handling of assumptions.\n\u2022 Clarity: Structured presentation and consistency in mathematical notation.\n\u2022 Relevance: Use of appropriate methods and avoidance of irrelevant details.\n\u2022 Insight: Understanding of concepts and elegance of the solution."}, {"title": "6.3 Results and Discussion", "content": "The results demonstrate significant improvements in the models' ability to handle formal mathematical reasoning when fine-tuned with the dataset and the framework. The evaluation revealed significant differences between baseline models, fine-tuned models, and models utilizing the framework. Both baseline models, Llama-3.2-3B-Instruct and Qwen-2.5-Math-7B-bnb-4bit, failed to handle the rigorous proof-based problems in the DEMI-MathAnalysis dataset, achieving an average score of 0. This result emphasizes the complexity of the dataset and the need for specialized fine-tuning. Fine-tuning alone brought substantial improvements, with Llama-3.2 achieving an average score of 33.5% and Qwen-2.5 reaching 37.6%.\nIncorporating the proposed framework further enhanced performance. For Llama-3.2, the framework increased the score to 40.8%, demonstrating its capability to guide models in generating more rigorous and logically sound solutions. Similarly, Qwen-2.5's performance improved to 38.6%, showcasing the framework's adaptability across different models.\nComparatively, the OpenAI 01-preview model achieved the highest score of 41.5%, underscoring its state-of-the-art capabilities. However, the results show that fine-tuning smaller models with the DEMI-MathAnalysis dataset and applying the framework allows these models to approach the performance of much larger systems.\nThese findings validate the effectiveness of our idea in enhancing LLMs' ability to tackle formal mathematical reasoning. The significant improvements observed after fine-tuning and framework integration demonstrate that even smaller models have the potential to achieve robust performance on proof-based problems when guided by structured methodologies."}, {"title": "7 Conclusion", "content": "This work addresses the challenges of solving mathematical analysis problems using LLMs by introducing the DEMI-MathAnalysis dataset and a novel framework designed to enhance their reasoning capabilities. The dataset fills a critical gap by focusing on rigorous, proof-based problems that are often absent in existing mathematical datasets. Complementing the dataset, the framework integrates problem classification, knowledge retrieval, and solution generation to guide LLMs toward producing logical, complete, and rigorous solutions.\nEvaluation results demonstrate significant performance improvements, particularly for fine-tuned models leveraging the framework. These results validate the effectiveness of the dataset and the framework in advancing LLMs' ability to tackle proof-based reasoning tasks, bridging the gap between computational problem-solving and formal mathematical reasoning. However, one shortcoming comes that these results dominated by LLMs may fluctuate within a certain range. Without considering labor-intensive activities, a future task is to develop a more robust proof evaluation system. One can convert the outputs into Lean (Moura and Ullrich, 2021), a language that automates proofs, or design more detailed prompts.\nFuture work also involves expanding the dataset to include a broader range of mathematical topics and refining the framework for improved generalization and adaptability. By addressing these areas, we aim further to contribute to developing trustworthy and versatile AI systems."}, {"title": "A Current answer to a specific Problem from GPT-40", "content": "If we input the following problem:\nProve that if $f(x)$ is a continuous function, then the function $F(x) = |f(x)|$ is also continuous."}, {"title": "B Training settings", "content": "per_device_train_batch_size = 2,\ngradient_accumulation_steps = 4,\nwarmup_steps = 5,\nmax_steps = 300,\nlearning_rate = 2e-4,\noptim = \"adamw_8bit\",\nweight_decay = 0.01,\nlr_scheduler_type = \"linear\",\nseed = 3407."}, {"title": "C Prompt used for fine-tuning and inference", "content": ""}, {"title": "C.1 Classifier", "content": "As a mathematical assistant, You need to analyze the problem\nto find out what type of problem it belongs to in Real Analysis.\nProvide the Problem_Type and the Knowledges which may\nbe used to solve this problem.\n### Problem:\n{}\n### Problem_Type:\n{}\n### Knowledge:\n{}"}, {"title": "C.2 Solver", "content": "As a mathematical assistant, solve the following problem.\nProvide a detailed, step-by-step solution using rigorous\nmathematical reasoning. If the problem requires the use of\nthe $\\epsilon$-$\\delta$ method (e.g., when proving limits or\ncontinuity), ensure that you apply it appropriately. Use\nprecise mathematical language and notation throughout your\nsolution.\n### Problem_Type:\n{}\n### Problem:\n{}\n### Knowledge:\n{}\n### Solution:\n{}"}, {"title": "D Prompt used for evaluation", "content": "You are a mathematical expert in Real Analysis. You need to\nevaluate the process of a proof of a real analysis problem.\nPlease follow the steps to give a score to the solution:\nHere are 5 key indicators to measure when evaluating a proof\nsolution in Real Analysis:\n1. Correctness\nLogical Rigor: Does the solution logically follow from the given\npremises and known mathematical principles?\nAdherence to Problem Requirements: Does the solution directly\naddress the question and utilize the given conditions\nappropriately?\nAccuracy: Are all mathematical statements, derivations, and\nconclusions valid?"}]}