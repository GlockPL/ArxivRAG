{"title": "Variational Graph Contrastive Learning", "authors": ["Shifeng Xie", "Jhony H. Giraldo"], "abstract": "Graph representation learning (GRL) is a fundamental task in machine learning, aiming to encode high-dimensional graph-structured data into low-dimensional vectors. Self-supervised learning (SSL) methods are widely used in GRL because they can avoid expensive human annotation. In this work, we propose a novel Subgraph Gaussian Embedding Contrast (SGEC) method. Our approach introduces a subgraph Gaussian embedding module, which adaptively maps subgraphs to a structured Gaussian space, ensuring the preservation of graph characteristics while controlling the distribution of generated subgraphs. We employ optimal transport distances, including Wasserstein and Gromov-Wasserstein distances, to effectively measure the similarity between subgraphs, enhancing the robustness of the contrastive learning process. Extensive experiments across multiple benchmarks demonstrate that SGEC outperforms or presents competitive performance against state-of-the-art approaches. Our findings provide insights into the design of SSL methods for GRL, emphasizing the importance of the distribution of the generated contrastive pairs\u00b9.", "sections": [{"title": "Introduction", "content": "Graph representation learning (GRL) aims to effectively encode high-dimensional sparse graph- structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining [Ju et al., 2024].\nSelf-supervised learning (SSL) offers a promising approach to GRL by mitigating the need for exten- sive human annotation [Jaiswal et al., 2020]. Particularly, contrastive learning is a prominent approach in SSL that leverages the similarities and differences between data samples or data embeddings to learn representations. In this context, positive sample pairs are typically two augmented views of the same data point that should be close in the representation space, while negative sample pairs are original and augmented views of different data samples [Chen et al., 2020]. Current graph-based contrastive learning methods primarily generate positive and negative sample pairs through perturba- tions [Zhu et al., 2020a]2. However, t-SNE visualizations of current graph-based contrastive learning methods, like GCA [Zhu et al., 2021] and GSC [Han et al., 2022], reveal uneven node distributions within the same graph, with sharp boundaries and erroneous node clusters as illustrated in Figure 1. We observe that those characteristics of previous models penalize their performance in GRL tasks.\nIn this paper, we propose the Subgraph Gaussian Embedding Contrast (SGEC) model. In our method, a subgraph Gaussian embedding (SGE) module is proposed to generate the features of the subgraphs. The SGE maps input subgraphs to a structured Gaussian space, where the features of the output subgraphs tend towards a Gaussian distribution by using the Kullback\u2013Leibler (KL) divergence. SGE is a learnable mapping that controls the distribution of the embeddings. The"}, {"title": "Subgraph Gaussian Embedding Contrast (SGEC) Method", "content": "Mathematical Notation. Consider an undirected graph G = (V, E) with vertex set V and edge set E. The feature matrix X = [x1,...,x\u014a]\u315c \u2208 RN\u00d7C contains node features x\u2081 \u2208 RC, where N is the number of nodes and C is the feature dimension. The adjacency matrix A \u2208 RNXN represents the graph topology, and D is the diagonal degree matrix. For some node i, let Hi = (Vi, Ei) be its induced breadth-first search (BFS) subgraph with k nodes, adjacency matrix A\u00b2 \u2208 Rkxk, and feature matrix Xi \u2208 Rk\u00d7C. Our method embeds this subgraph into a structured space, yielding the embedded subgraph H\u00b2 = (Vi, \u0190) with adjacency matrix A\u00b2 and feature matrix X\u00b2. We provide the definitions of KL divergence, Wasserstein, and Gromov-Wasserstein distances in Appendix B.\nOverview of the Proposed Method. Figure 2 shows an overview of our methodology, where our process begins with an encoder of the input graph. Subsequently, we obtain subgraphs utilizing BFS-based sampling. The node representations and topology information within these subgraphs are adaptively embedded into a latent space tending towards a Gaussian distribution, which is enforced by the KL divergence regularization. Finally, we use the Wasserstein and the Gromov-Wasserstein distance to measure dissimilarities in the subgraphs for contrastive learning.\nGraph Encoder. We begin by employing a graph encoder to preprocess the graph data [Kingma and Welling, 2014, Han et al., 2022]. The graph encoder comprises some graph convolutions layers and activation functions \u03c3(\u00b7). We use two graph convolution layers [Kipf and Welling, 2017] in SGEC. Further details on the implementation of these layers are provided in the Appendix B.3.\nSubgraph Gaussian Embedding (SGE). The SGE module comprises GraphSAGE [Liu et al., 2020] and GAT (graph attention) [Veli\u010dkovi\u0107 et al., 2018] layers. Appendix B.4 provides further details about GraphSAGE. We first randomly sample a set of nodes S and get their BFS-induced subgraphs, where |S is a hyperparameter of SGEC. We then apply the combination of GraphSAGE and GAT layers on these subgraphs. The latent means and variances are encoded by separate GAT networks as follows:\n$\\mu_{i} = GATConv_{\\mu} (h_{i}), \\quad \\log \\sigma_{i}^{2} = GATConv_{\\sigma} (h_{i}), \\quad \\tilde{x}_{i} = \\mu_{i} + \\exp(\\frac{\\log(\\sigma_{i}^{2})}{2}) \\odot \\epsilon,$"}, {"title": "Regularization in the Subgraph Gaussian Embedding", "content": "In our approach, we introduce a regu- larization constraint to the SGE module to guide the embedded subgraph node features towards a Gaussian distribution. For the motivation of introducing the Gaussian regularization, please see the Appendix C. This regularization is implemented using the Kullback-Leibler divergence, denoted as KL(q(\u00b7)||p(\u00b7)), between the embedding distribution q(\u00b7) and a predefined Gaussian prior. The prior p(X) is taken as a product of independent Gaussian distributions for each latent variable \u017e\u012f, given by p(xi) = N(xi|0, I). The expression for the KL divergence then simplifies to:\n$\\mathrm{KL}\\left(q(X | X, A) \\| p(x)\\right)=\\frac{1}{|P|} \\sum_{i \\in P} \\sum_{j=1}^{C}\\left(\\frac{\\mu_{i j}^{2}}{2 \\sigma^{2}}+\\frac{\\sigma_{i j}^{2}}{2 \\sigma^{2}}-\\frac{1}{2}-\\log \\frac{\\sigma_{i j}}{\\sigma}\\right),$\nwhere \u03bc and \u03c3\u00b2 are the latent mean and latent variance of the ith embedded node, respectively, and P is the set of nodes in the |S| induced subgraphs. \u0108 is the dimension of the latent features."}, {"title": "Optimal Transport Contrastive Loss", "content": "Our contrastive loss function integrates the Wasserstein and Gromov-Wasserstein distances into the InfoNCE loss formulation [van den Oord et al., 2018], addressing the complexities of graph-based data. The Wasserstein distance, W(X\u00b2, X\u00b2), captures feature distribution representation within subgraphs, whereas the Gromov-Wasserstein distance, GW(A\u00b2, X\u00b2, A\u00b2, X\u00b2), captures structural discrepancies, providing a topology-aware similarity mea- sure. The complete contrastive loss Lcontrast is the sum of Lw and LGw provided as follows:\n$\\mathcal{L}_{W}=\\alpha\\left(-\\sum_{i \\in S} \\log \\frac{\\exp \\left(-W\\left(X_{i}, X_{i}\\right) / \\tau\\right)}{\\sum_{j \\in S, j \\neq i}\\left(\\exp \\left(-W\\left(X_{i}, X_{j}\\right) / \\tau\\right)+\\exp \\left(-W\\left(X_{i}, \\tilde{X}_{i}\\right) / \\tau\\right)\\right)}\\right)$,\n$\\mathcal{L}_{G W}=(1-\\alpha)\\left(-\\sum_{i \\in S} \\log \\frac{\\exp \\left(-G W\\left(\\mathcal{A}_{i}, X_{i}, \\mathcal{A}_{i}, X_{i}\\right) / \\tau\\right)}{\\sum_{j \\in S, j \\neq i}\\left(\\exp \\left(-G W\\left(\\mathcal{A}_{i}, X_{j}, \\mathcal{A}_{j}, \\tilde{X}_{j}\\right) / \\tau\\right)+\\exp \\left(-G W\\left(\\mathcal{A}_{i}, X_{i}, \\mathcal{A}_{j}, \\tilde{X}_{i}\\right) / \\tau\\right)\\right)}\\right)$,\nwhere a is a hyperparameter that balances the emphasis on feature distribution and structural fidelity, and 7 is a temperature hyperparameter. The final loss L of our model incorporates both contrastive and regularization components balanced by a hyperparameter \u03b2 as follows: L = Lcontrast + \u03b2KL(q(X | X, A)||p(X))."}, {"title": "Experimental Evaluation", "content": "Comparison of Classification Performance. We compare our model against state-of-the-art SSL for node classification methods: GREET [Liu et al., 2023], GRACE [Zhu et al., 2020a], GSC [Han et al., 2022], and MUSE [Yuan et al., 2023]. Additionally, we include classic SSL baselines for a comprehensive comparison: DGI [Velickovi \u02c7 c et al., 2019], GCA [Zhu et al., 2021], and GraphMAE \u00b4 [Hou et al., 2022]. We test SGEC on standard benchmark datasets for node classification Cora, Citeseer, Pubmed, Coauthor, Squirrel, Chameleon, Cornell, and Texas [Giraldo et al., 2023]. Please see the Appendix D for more information about the datasets and experimental setup. The evaluation results for these eight datasets are summarized in Table 1. We observe that SGEC achieves the highest accuracies on the Squirrel, Cornell, and Texas datasets, outperforming existing state-of-the-art SSL methods. While not always leading on other datasets, SGEC consistently demonstrates competitive performance, highlighting its robustness and effectiveness in GRL."}, {"title": "Conclusion", "content": "We introduced SGEC, which embeds subgraphs into a Gaussian space to enhance self-supervised graph representation learning. By controlling embedding distributions and utilizing optimal transport distances, SGEC achieves superior or competitive performance. Our results emphasize the importance of distribution control in generating contrastive pairs.\nFuture work will involve integrating spectral-based contrastive learning methods [Chen et al., 2024] to further validate and enhance our proposed approach. Additionally, we intend to explore the applicability of our framework to other modalities, extending beyond graph data. This would involve validating our underlying ideas in different contexts, potentially broadening the impact and utility of our approach in various domains of representation learning."}, {"title": "Optimal Transport Distance", "content": "The Wasserstein distance [R\u00fcschendorf, 1985], commonly used in optimal transport theory, serves as a robust metric for comparing probability distributions defined over metric spaces. For subgraphs Hi and H\u1ec9, their corresponding feature matrices are denoted as Xi \u2208 Rk\u00b2\u00d7C and Xi \u2208 Rk\u00b9\u00d7C, where ki and ki represent the number of nodes in the respective subgraphs, and C is the feature dimension. The Wasserstein distance between the feature distributions of these subgraphs is defined as:\n$W(X^{i}, \\tilde{X}^{i^{\\prime}}) =\\min _{T \\in \\pi(u, v)} \\sum_{p=1}^{k_{i}} \\sum_{q=1}^{k_{i^{\\prime}}} T_{p q} c(X_{p}^{i}, \\tilde{X}_{q}^{i^{\\prime}}),$\nwhere \u03a4 \u0395 \u03c0(u, v) represents the set of all possible transport plans with marginals u and v, which are the node feature distributions in subgraphs H\u00b2 and H\u00b9, respectively. Tpq is the transportation plan between nodes p and 1q, and c(X, X) represents the cost function, often defined as exp (()), where (,) denotes the cosine similarity between node features, and 7 is a temperature parameter. This distance captures the minimal cost of transforming one subgraph's node feature distribution into another.\nSimilarly, the Gromov-Wasserstein distance [Arya et al., 2024] extends this idea to compare graph- structured data, where the internal distances between nodes are taken into account. For two subgraphs H\u00b2 and H\u00b9 with adjacency matrices A\u00b2 \u2208 Rk\u00b9\u00d7k\u00b2 and Aj\u2208 Rki\u00d7ki, and feature matrices X\u00b2 and X, the Gromov-Wasserstein distance is defined as:\n$\\mathrm{GW}\\left(A^{i}, A^{i^{\\prime}}, X^{i}, \\tilde{X}^{i^{\\prime}}\\right)=\\min _{T \\in \\pi(u, v)} \\sum_{p, \\hat{p}, q, \\hat{q}} T_{p q} \\mid d_{A^{i}}\\left(X_{p}^{i}, X_{\\hat{p}}^{i}\\right)-d_{\\mathcal{A}^{i^{\\prime}}}\\left(\\tilde{X}_{q}^{i^{\\prime}}, \\tilde{X}_{\\hat{q}}^{i^{\\prime}}\\right) \\mid,$\nwhere da (X, X) and das (X, X) represent the shortest path distances between node pairs (p, p) in subgraph H\u00b2, and (q, q) in subgraph H\u00b9, respectively. The matrix T \u2208 \u03c0(u, v) is the optimal transport plan that matches node pairs from the two subgraphs. This metric assesses the discrepancy between the internal node pair distances of the two subgraphs, aligning their intrinsic geometric structures."}, {"title": "The Kullback-Leibler (KL) Divergence", "content": "The Kullback-Leibler (KL) divergence is an asymmetry measure between two probability distributions [Van Erven and Harremos, 2014], P and Q. It quantifies the amount of information lost when Qis used to approximate P. In mathematical terms, the KL divergence is defined as:\n$D_{K L}(P \\| Q)=\\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)},$\nwhere P(x) and Q(x) are the probability masses of P and Q at each point x in the sample space X."}, {"title": "Graph Convolutional Network", "content": "The graph encoder uses two graph convolution layers, which are mathematically represented as follows:\n$H_{1}=\\sigma\\left(\\left(D^{-\\frac{1}{2}}(A+I) D^{-\\frac{1}{2}}\\right) X W_{1}\\right), \\quad H_{2}=\\sigma\\left(\\left(D^{-\\frac{1}{2}}(A+I) D^{-\\frac{1}{2}}\\right) H_{1} W_{2}\\right).$"}, {"title": "GraphSAGE Layer", "content": "For GraphSAGE, we present expressions:\n$x_{v}^{(l+1)}=\\sigma\\left(W^{(l)} \\cdot \\operatorname{MEAN}\\left(\\left\\{x_{v}^{(l)}\\right\\} \\cup\\left\\{x_{u}^{(l)} | u \\in \\mathcal{N}(v)\\right\\}\\right)+b^{(l)}\\right),$\nwhere x is the feature vector of node v at layer l, W(1) is the weight matrix at layer l, N(v) denotes the set of neighbors of node v."}]}