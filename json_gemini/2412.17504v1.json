{"title": "An Evaluation Framework for Product Images Background Inpainting based on Human Feedback and Product Consistency", "authors": ["Yuqi Liang", "Jun Luo", "Xiaoxi Guo", "Jianqi Bi"], "abstract": "In product advertising applications, the automated inpainting of backgrounds utilizing AI techniques in product images has emerged as a significant task. However, the techniques still suffer from issues such as inappropriate background and inconsistent product in generated product images, and existing approaches for evaluating the quality of generated product images are mostly inconsistent with human feedback causing the evaluation for this task to depend on manual annotation. To relieve the issues above, this paper proposes Human Feedback and Product Consistency (HFPC), which can automatically assess the generated product images based on two modules. Firstly, to solve inappropriate backgrounds, human feedback on 44,000 automated inpainting product images is collected to train a reward model based on multi-modal features extracted from BLIP and comparative learning. Secondly, to filter generated product images containing inconsistent products, a fine-tuned segmentation model is employed to segment the product of the original and generated product images and then compare the differences between the above two. Extensive experiments have demonstrated that HFPC can effectively evaluate the quality of generated product images and significantly reduce the expense of manual annotation. Moreover, HFPC achieves state-of-the-art(96.4% in precision) in comparison to other open-source visual-quality-assessment models. Dataset and code are available at: https://github.com/created-Bi/background_inpainting_products_dataset/.", "sections": [{"title": "Introduction", "content": "AI-based background inpainting techniques (Chen et al. 2024; Verma et al. 2024; Phutke and Murala 2023; Feng et al. 2021; Bhargavi, Sindwani, and Gholami 2023; Alam, Sokhandan, and Goodman 2024) have become crucial in improving the homogeneity of products and backgrounds in the visual needs of markets, notably in e-commerce and advertising, as well as sectors prioritizing high-quality images for profitability. (Wang et al. 2012; Hao et al. 2021; He, Wang, and Wu 2021).\nCompared to the rapid development of AI-based background inpainting techniques, the core challenge shifts to the evaluation of images generated by these techniques (Zhang et al. 2023; Qureshi et al. 2017; Voronin et al. 2015). As illustrated in Fig. 1, generated product images often suffer from the following problems: inappropriate background and product inconsistency. Currently, in real-world scenarios, a necessary way to filter these low-quality generated product images is human annotation, which is expensive and demanding.\nThe existing methods used for image quality assessment tend to evaluate generated images in isolation, without considering the need to preserve the integrity of the original product, position it appropriately, or ensure its harmony with the background. Common automatic evaluation metrics, such as the referenced metrics Fr\u00e9chet Inception Distance(FID) (Yu, Zhang, and Deng 2021) and Cumulative Maximum Mean Discrepancy(CMMD) (Jayasumana et al. 2024), focus on image distribution but usually fail to capture nuances in individual images. Reference-free metrics such as CLIP-IQA (Wang, Chan, and Loy 2023) merely assess the appearance and aesthetics of the images. More recent approaches exert to collecting human preferences and train reward models to predict the preferences for improving the generative models(Kirstain et al. 2023). However, there is a gap between the definition of an excellent image in these methods and that in the background inpainting field(i.e., human feedback).\nTo relieve these issues, a novel evaluation framework named Human Feedback and Product Consistency(HFPC) was proposed to evaluate the quality of background inpainting product images. Firstly, to solve inappropriate backgrounds, human feedback on 44,000 automated inpainting product images is collected to train an image-referenced reward model. Specifically, in the training process, two sets of image pairs such as the (original, good) and (original, bad) generated images were constructed. Secondly, to filter generated product images that contain inconsistent products, a fine-tuned segmentation model is used to segment the products of the original and generated product images(Liu et al. 2023; Xiong et al. 2024) and then compare the differences between the above two. By visually comparing the differences between the segmentation masks, whether the product is preserved in its entirety can be determined.\nExperimentally, HFPC achieves state-of-the-art (SOTA) performance on our HFPC-44k dataset compared to existing image-quality evaluation models. In addition, our product consistency evaluator effectively identifies issues such as product loss during image transformation.\nThe main contributions of this paper are as follows:\n\u2022 Introduce the HFPC-44k dataset, identifying two major issues in AI-based background inpainting images: inappropriate backgrounds or inconsistent products.\n\u2022 Propose HFPC, a novel framework specifically designed for the automatic quality assessment of AI-based background inpainting images.\n\u2022 Extensive experiments demonstrate that HFPC achieves high accuracy in filtering out the generated product images with inappropriate backgrounds or inconsistent products, significantly reducing the expense of manual annotation."}, {"title": "Related Work", "content": "AI-based Background Inpainting for product Images\nAI-based background inpainting of product images involves several key techniques, including target segmentation, background retrieval, and image fusion(Chen et al. 2024; Verma et al. 2024; Phutke and Murala 2023; Feng et al. 2021; Bhargavi, Sindwani, and Gholami 2023; Alam, Sokhandan, and Goodman 2024). First, to obtain the products, target segmentation of the original image is required. Target segmentation not only detects the product but also generates its accurate mask. Next, a suitable background is retrieved in the background gallery. The background gallery can either be collected manually or generated by Generative Adversarial Networks (GANs), e.g. StyleGAN(Karras, Laine, and Aila 2019) can generate high-quality and realistic background images. Finally, the product and background are fused using image fusion techniques. The Deep Image Blending(DIB) method(Zhang, Wen, and Shi 2020) optimizes the image fusion results by combining the loss of Poisson fusion, as well as the loss of style and content computed from the deep network, and iteratively updating the pixels using the L-BFGS(Moritz, Nishihara, and Jordan 2016) solver to achieve smoothing in the gradient domain and consistency of texture within the image blending region.\nImage-to-image Evaluation\nIn the field of automatic image generation evaluation, predominant metrics such as the Inception Score (IS) (Barratt and Sharma 2018), FID (Yu, Zhang, and Deng 2021), and CMMD (Jayasumana et al. 2024) assess image quality by comparing statistical properties between original and generated images. However, these reference-based metrics often fail to capture detailed discrepancies at the individual image level. Alternatively, reference-free metrics like CLIP-IQA (Wang, Chan, and Loy 2023), leveraging the visual-linguistic capabilities of the pretrained CLIP model (Radford et al. 2021), aim to directly evaluate image quality perception and abstract representation. Despite their advances, these metrics struggle, particularly in evaluations involving product images, where they inadequately reflect the appropriateness between products and their backgrounds and fail to accurately detect morphological changes and in products. This indicates a significant challenge in using current metrics for the assessment of AI-based background inpainting product images.\nText-to-image Reward Models\nRecently, there have been several new studies in the area of using human preference training reward models to evaluate text-generated image models. Xu et al. (2024) created a human preference dataset by asking users to rank a series of images and score their quality. Based on this dataset, they trained a human preference learning reward model called ImageReward and proposed a new method of using the ImageReward model to optimize a diffusion model through reward feedback learning (ReFL). In addition, Kirstain et al. (2023) constructed a web application, Pick-a-Pic, which collects human preferences by allowing users to pick the better one from a pair of generated images, and this application has collected more than half a million images from different T2I models (e.g., Stable Diffusion 2.1, Dreamlike Photoreal 2.05, and Stable Diffusion XL) (Podell et al. 2023; Wu, Huang, and Wei 2024; Gupta et al. 2024), and they used these human preference data to train a CLIP-based scoring function, called PickScore, for predicting human preferences. In addition, Wu et al. (2023) also collected a large dataset of images generated by human choice and used this data set to train a classifier that produces a Human Preference Score (HPS), and demonstrated that image generation quality can be significantly improved by tuning the Stable Diffusion model. More recently, Liang et al. collected a rich feedback dataset containing artifactual regions, misaligned regions, misaligned keywords, and four fine-grained scores, and trained a multimodal model(RAHF)(Liang et al. 2024) to automatically predict these feedbacks."}, {"title": "Method", "content": "Overall Framework\nAs shown in Fig. 2, this paper introduces the HFPC architecture, designed to automatically evaluate the quality of AI-based background inpainting product images. The HFPC architecture comprises two parallel modules. Module 1, a multimodal BLIP-based(Li et al. 2022) reward model, processes both the original and generated images to output a score reflecting the appropriateness of the background. Module 2, the product consistency assessment model, utilizes a fine-tuned segmentation model to segment products from the original and generated images. These segmented products are then compared using a product consistency evaluator. An image is deemed acceptable if it meets predefined thresholds for both background appropriateness and product consistency.\nCollection of HFPC-44k Dataset\nHFPC-44k comprises 44,000 pairs of original images and AI-based background inpainting product images, each annotated with corresponding human feedback. Let $D$ represent the entire dataset, where each element $d \\in D$ consists of three components: : the original image $I$, a set of generated images $G = \\{G_1,\\ldots,\\ldots, G_i,\\ldots,\\ldots,G_K\\}$, and corresponding label set $L = \\{L_1,\\ldots, L_i,\\ldots,\\ldots, L_K\\}$. Here, $L_i = 1$ denotes that the human review of i-th generated image is passed and $L_i = 0$ means that the review is not passed.\nImage-Referenced Reward Model\nInspired by the success of the reward model in the textual field, an Image-Referenced Reward Model(IRRM) applying the image encoder from BLIP, which is shown in Fig. 3 was proposed. Firstly, IRRM receives an original image and two high-quality and low-quality AI-based background inpainting images. These images will be fed into the image encoder of BLIP to extract the respective image features. Subsequently, an attention module(Vaswani 2017) is employed to enhance the perception of the original image features by comparative learning (Lei et al. 2016). Namely, IRRM combines the information of the original and generated product images by focusing on the key difference in features between high-quality and low-quality images. The attention mechanism not only improves IRRM's capacity to recognize low-quality images but also improves IRRM's interpretability. Secondly, the fused features are passed to a multi-layer perceptron(MLP) to obtain two categories of output. One of them is a high-quality image score and the other is low-quality. Finally, a triplet loss function is applied during the model training process to carefully optimize the model performance and ensure that the model can accurately differentiate between high-quality and low-quality images.\nProduct Consistency Assessment Module\nThis module evaluates the consistency of products in original and generated images. Our method comprises three stages, as depicted in Figure 4.\nFirst, both the original and generated images are inputted into a fine-tuned EfficientSAM model(Xiong et al. 2024). This model aims to obtain segmentation masks for all products in the images. To provide EfficientSAM with high-quality bounding box prompts, the Grounding Dino was integrated with EfficientSAM. The Grounding Dino model processes the input images using a text prompt \"product.\" and then produces specific detection boxes for the products. Leveraging the detection boxes provided by Grounding Dino, the EfficientSAM model then generates segmentation masks for all products. Moreover, more than 500 product images are manually annotated to fine-tune the segmentation model at the pixel level. The text prompt \"products\" were uniformly used during training to ensure consistent cueing. Let the m masks generated from the original image is denoted as $M_{ori} = \\{M_1,\\ldots, M_i,\\ldots,\\ldots,M_m\\}$. n masks generated from the generated image are denoted as $N_{gen} = \\{N_1,\\ldots, N_j,\\ldots,\\ldots,N_n\\}$.\nGiven that the product's masks in $M_{ori}$ and $N_{gen}$ are unordered, it is necessary to match corresponding products between the original and generated images to accurately assess consistency. The matching is based on the shape and positional order of the products, under the assumption that the same products will exhibit similar shapes and positional arrangements in both images. For each mask $M_i$ in $M_{ori}$, the Intersection over Union (IoU) and positional distance with each mask $N_j$ in $N_{gen}$ was calculated, matching $M_i$ with the mask $N_{match} \\in N_{gen}$ that has the highest IoU and smallest positional distance. This process results in a set of matched product pairs $M_N = \\{(M_1, N_{match}),\\ldots (M_i, N_{match}),\\ldots,\\ldots,(M_m, N_{match})\\}$.\nFor each matched pair $(M_i, N_i)$, consistency by computing the average pixel difference within the mask region of the products was then assessed. The difference serves as a metric for visual dissimilarity between the products (i.e., product inconsistency).\nModel Optimization of Reward Model\nFor the reward model, it aims to evaluate the appropriateness of the background. The model is required to ensure that the high-quality images gain a higher score than the low-quality images. To achieve this goal, a composite loss function is designed to supervise the learning process.\nLet $x_i$ be the original image, $y_i = [y_{i,1}, y_{i,0}]$ is the generated image obtained from the original image $x_i$ after AI-based background inpainting, where $y_{i,1}$ is the generated image that the human review passes, and $y_{i,0}$ is the generated image that the human review fails, and the corresponding target label is $t_i = [1,0]$, where 1 means pass and 0 means fail. The $r_i = [r_{i,1}, r_{i,0}]$ denotes the model's predicted scores for the generated image $y_i$, where $r_{i,1}$ and $r_{i,0}$ represent the scores of the high-quality and low-quality images, respectively. Our overall loss function is defined as follows,\n$L_{total} = L_{rank} + L_{class}$ (1)\nwhere $L_{rank}$ is the ranking loss and $L_{class}$ is the classification loss.\nThe ranking loss uses cross-entropy to encourage positive sample pairs to score higher than negative sample pairs,\n$L_{rank} = \\frac{1}{N} \\sum_{i=1}^N L_{CE} (r_i, t_i)$ (2)\nwhere $L_{CE}$ denotes the cross-entropy loss function (Mao, Mohri, and Zhong 2023).\nThe classification loss is designed to ensure that the model prediction scores $\\sigma(r_{i,1})$ greater than 0.5 for high-quality images and less than 0.5 for low-quality images. Classification loss is defined as follows,\n$L_{class} = \\frac{1}{N} \\sum_{i=1}^N (L_{BCE}(\\sigma(r_{i,1}),1) + L_{BCE}(\\sigma(r_{i,0}), 0))$ (3)\nwhere $\\sigma(\\cdot)$ denotes the sigmoid function and $L_{BCE}$ denotes the binary cross-entropy loss. The relatively comprehensive loss function can simultaneously optimize the model's ranking ability and classification accuracy, thereby improving the overall performance of image quality assessment."}, {"title": "Experiment", "content": "In this section, how to expand the HFPC-44k dataset will be introduced. Considering the imbalance of different categories of images in real business scenarios (e.g., the number of photos in the apparel category is larger than that in the electrical appliances category), the training data is expanded accordingly. Moreover, the significant advantages of our approach are demonstrated in evaluating background inappropriateness and product consistency through extensive experiments.\nExpansion of Training Data\nHFPC-44k contains 44,244 pairs of original images of products and images generated by AI-based background inpainting. Each data is labeled with the generation time, the original image URL, the generated image URL, and the manual annotation tag (labeled as \"passed\" or \"failed\"). 35,000 pairs of images from this dataset were extracted for the model training process and the remaining 8,372 pairs of images were used in the test process. Among the 35,000 training samples, 80% are split to build the training dataset, and 20% are for the validation set.\nIt can be noticed that the number of images of certain commodity categories appeared to be extremely unbalanced. After extracting the features of all the original images of the commodities in the training set using a BLIP encoder, clustering by KMeans was performed to check the data distribution(Krishna and Murty 1999). Finally, the most suitable number of clustering categories (i.e., number of clusters) is defined as 25. Furthermore, as shown in Fig. 5, the number of product images corresponding to the blue points representing the shoes category is much higher than the red points representing the pots and pans category. The imbalance of data categories may cause the model to be biased in rating the product images of certain categories. Thus, a balanced method was applied to the training dataset. Specifically, a few numbers of product images are repeated using a random sampling of products in the same category. Specific measures include data expansion for categories with a low number of images. As shown in Fig. 6, the blue and orange lines represent the number of product images in each category before and after data expansion, respectively. In this way, the model can be accessed to a balanced number of different categories of product images during the training process.\nEvaluation Protocol\nOur HFPC model was trained based on 28,000 training samples and hyperparameters are tuned by performance on 7,000 validation samples. Finally, the performance of the model was validated on 8,000 test samples.\nIn the main background coordination scoring task, the Pearson linear correlation coefficient (PLCC) and the Spearman rank correlation coefficient (SRCC) (Sedgwick 2014) were reported. These two metrics are commonly used to evaluate scoring systems, measuring the accuracy and consistency of model performance in terms of linear and rank correlation, respectively (Talebi and Milanfar 2018; Zhai and Min 2020; Yang et al. 2022).\nFor the task of assessing the overall quality of images generated by AI-based background inpainting, including the assessment of background appropriateness and product consistency, three key metrics were considered as 1)low-quality image filtering accuracy rate ($P_b$), 2)low-quality image recall rate ($R_b$) and 3) high-quality image recall rate ($R_g$). The calculation process of each metric is as follows,\n1. Precision rate for low-quality image filtering $P_b$,\n$P_b = \\frac{N_{filtered, low-quality}}{N_{filtered}}$ (4)\n, where $N_{filtered, low-quality}$ is the number of low-quality images actually filtered out and $N_{filtered}$ is the total number of images filtered out.\n2. Recall rate for low-quality generated images $R_b$,\n$R_b = \\frac{N_{filtered, low-quality}}{N_{original, low-quality}}$ (5)\nwhere $N_{filtered, low-quality}$ is the total number of filtered low-quality generated images and $N_{original, low-quality}$ is the total number of original low-quality generated images.\n3. Recall rate for high-quality generated images $R_g$,\n$R_g= \\frac{N_{filtered, high-quality}}{N_{original, high-quality}}$ (6)\n, where $N_{filtered, high-quality}$ is the total number of filtered high-quality generated images and $N_{original, high-quality}$ is the total number of original high-quality generated images.\nMeasuring the above four key metrics can comprehensively provide strong support to evaluate the performance of the HFPC, even for further optimization."}, {"title": "Overall Results", "content": "The results of each method evaluation are shown in Table 1. From the table, it can be seen that our designed HFPC framework performs the best in all indicators. In addition, the P-R and ROC curves of each method are plotted in Fig. 7. The two metrics can demonstrate the performance of each approach in the case of pass and fail. Based on these curves, the evaluation effects of different modules can be more intuitively compared, leading to further proof of the superiority of the HFPC framework in image quality evaluation. In addition, the effects of the reward model and the product consistency assessment module in HFPC were verified from the ablation study, respectively. As shown in Table1, a single reward model already shows a performance very consistent with human preference, based on which the product consistency assessment module can further filter out the existence of product inconsistency images.\nAblation Study\nTo investigate the effect of each module on the performance of the model, we conducted addictional ablation experiments, including fusion of original and generated images via attention mechanisms, and classification and ranking losses. Particularly, we compared ImageReward-a ranking model without original image and with only ranking loss. As shown in Table2, introducing original image data via attention mechanisms significantly improves model performance, and classification loss further enhances performance."}, {"title": "Visual Analysis", "content": "Visualization of results of HFPC A range of results output by the HFPC are listed. Fig. 8 reveals that our method can accurately recognize product consistency and inappropriate background. It is worth noting that the Product Consistency Assessment module can still find low-quality generated images (i.e., fail) when the Reward-Model score is high.\nVisualization of Attention Mechanisms for the Reward Model Our Reward Model enhances the understanding of the relationship between the original image and the generated image through an attention mechanism. This mechanism enables the model to focus on the most critical information in the image, especially the part where the products interact with the background. Thus, the attention map(Gao et al. 2021) was visualized in Fig. 9 to value the model's attention in image processing."}, {"title": "Inference Time and GPU Overhead", "content": "Inference performance tests were conducted on the P100 machine (16GB), evaluating the GPU usage and inference time of both the Reward Model and Product Consistency Assessment module with batch size set to 1. Table 3 summarizes the results:"}, {"title": "Conclusion", "content": "The HFPC framework offers a novel and effective solution for the automatic quality assessment of AI-generated product images, particularly addressing the challenges of inappropriate backgrounds and product inconsistencies that are often overlooked by existing metrics. By integrating human feedback into a reward model and employing advanced segmentation techniques for precise product analysis, HFPC not only aligns more closely with human feedback but also considerably reduces the reliance on labor-intensive manual annotations. The demonstrated superiority of HFPC over other leading visual-quality-assessment models on the HFPC-44k dataset underscores its potential as a transformative tool in the realm of e-commerce and advertising.\nIn the future, we will further extend the HFPC-44k dataset to include larger-scale human annotations. In addition to static data, we will introduce real-time online feedback data to train the model to incrementally accommodate real-time user preference changes (Wu et al. 2019; Castro et al. 2018). In addition, our proposed HFPC framework is not only capable of filtering low-quality generated images but also utilizes reinforcement learning to improve the quality of the generative models (Bai et al. 2022; Knox and Stone 2011; Griffith et al. 2013)."}]}