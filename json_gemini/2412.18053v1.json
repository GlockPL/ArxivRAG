{"title": "Neuron Empirical Gradient: Connecting Neurons' Linear Controllability and Representational Capacity", "authors": ["Xin Zhao", "Zehui Jiang", "Naoki Yoshinaga"], "abstract": "Although neurons in the feed-forward layers of pre-trained language models (PLMs) can store factual knowledge, most prior analyses remain qualitative, leaving the quantitative relationship among knowledge representation, neuron activations, and model output poorly understood. In this study, by performing neuron-wise interventions using factual probing datasets, we first reveal the linear relationship between neuron activations and output token probabilities. We refer to the gradient of this linear relationship as \"neuron empirical gradients,\" and propose NeurGrad, an efficient method for their calculation to facilitate quantitative neuron analysis. We next investigate whether neuron empirical gradients in PLMs encode general task knowledge by probing skill neurons. To this end, we introduce MCEval8k, a multi-choice knowledge evaluation benchmark spanning six genres and 22 tasks. Our experiments confirm that neuron empirical gradients effectively capture knowledge, while skill neurons exhibit efficiency, generality, inclusivity, and interdependency. These findings link knowledge to PLM outputs via neuron empirical gradients, shedding light on how PLMs store knowledge. The code and dataset are released\u00b9.", "sections": [{"title": "Introduction", "content": "Although Transformer (Vaswani et al., 2017)-based language models (LMs) benefit from large-scale pre-training, the pre-trained LMs (PLMs) suffer from hallucination, where models generate incorrect knowledge. This issue makes it important to understand the mechanism by which PLMs store knowledge within their parameters (Dai et al., 2022; Niu et al., 2024; Wang et al., 2024a, 2022).\nIn Transformer-based LMs, feed-forward (FF) layers serve as key-value memory (Geva et al., 2021), with neurons possessing the ability to retrieve knowledge. Previous work reveals that spe-"}, {"title": "Settings", "content": "Models. To make the analysis result general, we experiment with two types of LMs, masked and causal LMs, with varied sizes and learning strategies. For masked LMs, we use three BERT (Vaswani et al., 2017; Devlin et al., 2019) models: BERTbase, BERTlarge, and BERTwwm. We construct masked prompts and let the model predict the masked token. For causal LMs, we examine three instruction-tuned LLMs of Llama2 family (Touvron et al., 2023), with sizes of 7B, 13B, and 70B. Following Zhao et al. (2024), we instruct them to generate single-token answers. See \u00a7 B for model details.\nDataset. We utilize a multi-prompt knowledge probing dataset, MyriadLAMA\u00b2 (Zhao et al., 2024), for neuron intervention. MyriadLAMA offers diverse prompts per fact, reducing the influence of specific linguistic expressions on probing results. We focus on single-token probing, where the target answer is represented by a single token. For each PLM, we randomly sample 1000 prompts from MyriadLAMA, where the model correctly predicts the target token. Due to differences in tokenizers, the probing prompts may vary across PLMs.\nNeuron-wise intervention. We conduct neuron-wise intervention to analyze how activation shift affects model outputs. Specifically, we alter the neuron activations within a range of [-10, 10] with a step size of 0.2 to observe the resulting changes in target token output probabilities. Since observing the effect of a single neuron on one token for one prompt requires 100 inference runs and is costly, we only perform the neuron-wise intervention on specific neurons selected by either random sampling and choosing the top-k neurons with the highest absolute computational gradients.3"}, {"title": "Results and Analysis", "content": "From experimental results, we reveal the numerical relationship between neuron activation shifts and output probabilities in PLMs.\nCorrelation vs. shift range. We first calculate the Pearson correlation between the shift ranges and the output probability of the correct tokens, considering only the absolute values to examine their linear relationship we call neuron linearity. The correlations are averaged over 10 prompts, each with 1000 neurons, for each activation shift size.4\nFigure 2 depicts averaged correlations for the two neuron selection methods. The top-gradient neurons demonstrate high correlations across the PLMs and shift range, which is higher than the randomly sampled neurons. This suggests that the neuron linearity holds for top-gradient neurons (possibly, knowledge neurons).5 Meanwhile, for top-gradient neurons, when setting the activation shift range to \u00b12, the correlations in all models are above 0.95, which we consider the threshold for indicating the linear relationship. Our subsequent analysis all uses the top-gradient neurons within a shift range of \u00b12 by default.\nNeuron linearity. We then present a quantitative analysis of the prevalence of neuron linearity and the generality of these neurons across different prompts and Transformer layers. Specifically, we report the ratio of neurons exhibiting"}, {"title": "Neuron Empirical Gradient", "content": "We quantify how important a neuron is in influencing the target token's probability by the gradient of the linear relationship we term neuron empirical gradient. Empirical gradient provides a quantified measurement of neurons' controllability: the magnitude indicates the intensity of control, while the polarity decides the controlling direction over model outputs. To calculate the neuron empirical gradient, we fit a zero-intercept linear regression between activation shifts and output probability changes acquired through neuron intervention. The regression coefficient is identified as the neuron empirical gradient, which requires extensive inferences for a specific neuron, prompt, and token.\nTo address this issue, we propose NeurGrad, inspired by the observation that computational gradients approximate empirical gradient magnitudes but fail to accurately capture neuron polarity, which is negatively correlated with activation signs.\n$G_E = G_C \\times -\\text{sign}(A)$     (1)\nwhere $G_E$, $A$, $G_C$, and $\\text{sign}(A)$ represents the estimated empirical gradient, activation, computational gradient, and sign of A8 (1 for A > 0 and -1 for A < 0), respectively."}, {"title": "Understanding Neuron Controllability", "content": "The efficiency and precision of NeurGrad allow us to estimate empirical gradients of all neurons in LLMs. In this section, we leverage NeurGrad-estimated gradients to the following questions."}, {"title": "Skill Neuron Probing using NeurGard", "content": "We have demonstrated that neurons could linearly influence output probability on factual probing tasks, showcasing their potential to manipulate model outputs. Building on this, we propose to investigate whether empirical gradients can effectively encode diverse language skills through the skill neuron probing (Wang et al., 2022). Skill neuron probing aims to locate neurons that encode the skill to solve language tasks. While previous studies explore the effectiveness of using neuron activations to identify skill neurons (Wang et al., 2022; Song et al., 2024), the representational capacity of empirical gradients is still underexplored."}, {"title": "Task Definition", "content": "We formulate the skill neuron probing task as follows. A dataset conveying specific language skills D consists of language sequence pairs, including knowledge inquiries $Q = \\{q_1, ..., q_{|Q|}\\}$ and answer sequences $A = \\{a_1, ..., a_{|T|}\\}$, where arbitrary $a_i$ belongs to the answer candidate set $A_{cands}$. For example, in the sentiment classification task, Q is the documents set, and A is the ground-truth sentiment labels. We then build classifiers that take behaviors of arbitrary neuron subset $N_s \\subseteq N$ as features to indicate the correct answer sequences $a_i$ for the knowledge inquiry $p_i$. N refers to all the neurons.10\nOur skill neuron prober aims to find N that can achieve optimal accuracy over the target dataset D.\n$N^* = \\text{arg max}_{N_s \\subset N} Acc(f (N_s), D)$  (2)\n$Acc(f(N_s), D) = \\frac{1}{|D|} \\sum_{i=1}^{|D|} 1[f(N_s, p_i) = a_i]$.     (3)\nHere, $f(N_s, p_i)$ is the output of the classifier F using the neuron subset $N_s$ for the prompt $p_i$. 1[X = Y] is an indicator function that equals 1 if X matches Y, and 0 otherwise."}, {"title": "Evaluation Benchmark: MCEval8K", "content": "As skill neuron probing requires a fixed target token, it faces high computational costs due to the infinite possibility of answer sequences. We thus create a multi-choice language skill evaluation benchmark, MCEval8K, that forces PLMs to generate a"}, {"title": "Gradient-based Skill Neuron Prober", "content": "For each task dataset D, we split it into: training set Dtrain to train the classifiers, validation set Dvalid to decide hyperparameters, and test set Dtest for evaluation, with the ratio of 6:1:1. We train three probers with different designs for comparison.\nPolarity-based majority vote (Polar-prober) adopts a simple majority-vote classifier, taking each neuron in N as one voter. A polarity-based classifier leverages the polarity of neurons (positive or negative) as features for classification. Given Dtrain = {(qi, ai)} and any neuron nk \u2208 N, we identify the polarity as feature xnk for each (qi, ai) pair. For each nk, we calculate the ratio of being positive and negative across all |Dtrain| examples and the dominant polarity is identified as their global polarity \u0161k. Neurons with more consistent polarity are ranked higher.\nTo make prediction of qi, we measure all polarities of $x^{q_i,a_j}_{nk}$, where aj \u2208 Acands, nj \u2208 N*.\nThe prediction of each pi is made as follows:\n$f(N, p_i) = \\text{arg max}_{a_j \\in A_{cands}} \\sum_{n_k \\in N^*} 1[x^{q_i, a_j}_{nk} = \\bar{x}_{n_k}]$     (4)\nWe identify the optimal size of N* with Dvalid.\nMagnitude-based majority vote (Magn-prober) utilizes gradient magnitudes as features for a majority-vote classifier. During training, for a specific pi and nk, we compare the gradients between a \u2208 Acands. Neurons that consistently exhibit the largest or smallest gradients for the ground truth ai compared to other candidates are used as skill indicators. We record each neuron's preference for being either the largest or smallest. Neurons exhibiting more consistent behavior are assigned higher importance and identified as skill neurons. During inference, similar to Eq. 4, the prediction is made by selecting aj that satisfies the majority of nk \u2208 N*. This prober is designed to compare against the polarity-based prober, aiming to investigate the differences between using polarity and gradient magnitude as feature sources."}, {"title": "Experiment Setup", "content": "Dataset & Prompt settings Since our probing method restricts the output sequence length to 1, we carefully craft instructions and options for all datasets in MCEval8K through human effort. We evaluate both zero-shot and few-shot settings, ensuring in few-shot experiments that all candidate tokens appear once in the demonstrations to prevent majority label bias (Zhao et al., 2021). See \u00a7 F for the designed instructions for all tasks.\nProber settings During validation, we select the optimal neuron size for majority-vote probers from 2n (0<=n<=13). For the random-forest prober, we report accuracy using scikit-learn's default settings, where the optimal subset of features is selected automatically: 100 trees with no depth limitation. See \u00a7 D.1 for detailed prober settings.\nModel We perform skill neuron probing on Llama2-7B using three probers, all datasets in MCEval8K, and the full training set (6000) per task. For Llama2-70B, due to high cost, we probe one dataset per genre\u2014NER, Agnews, PAWS, CSQA, HaluEval, and mLAMA\u2014using 1,024 training examples and only train major-vote probers."}, {"title": "Result and Analysis", "content": "Skill neuron-based classifier accuracy is compared to two baselines: random guessing (Rand), and"}, {"title": "Representation & Acquisition Efficiency", "content": "Representational efficiency: By finding the optimal neuron size on the validation set, we observe that skill-neuron prober can achieve high accuracy with a few neurons. We summarize optimal neuron sizes for all tasks with Magn-prober in Table 4. Most tasks achieved optimal accuracy within 256 neurons, demonstrating the efficiency of empirical gradients in representing language skills. Notably, factuality tasks, such as MyriadLAMA, CSQA, and mLAMA, engage a larger number of neurons, suggesting that handling facts requires more diverse neurons, reflecting the complexity of factual understanding tasks.\nAcquisition efficiency: We report the accuracy of skill-neuron probers with different training examples in Figure 6. While adding training examples can consistently increase the probers' accuracy, the earnings slow down after 128, indicating the efficiency of acquiring skill neurons with limited data."}, {"title": "Generality Across Diverse Contexts", "content": "We investigate how skill neurons change when we provide different contexts, including instructions, demonstrations, and options for the same task. Given context X, we first acquire the skill"}, {"title": "Are Neurons Exclusive in Skill Representations?", "content": "We investigate whether skill neurons exclusively represent specific skills or can be substituted by different neuron sets. We thus build Magn-probers using various neuron sets. Specifically, we select 64 consecutive neurons from the ranked list, ordered by their importance as skill indicators (\u00a7 5.1).12\nFigure 7 depicts the accuracies across six tasks, The result suggests that skill neurons are broadly"}, {"title": "Do skill neurons depend on each other?", "content": "The majority-vote probers assume independence between neurons, while the Tree-prober considers their interdependencies by building hierarchical classifiers, which advantage over the major-vote prober in Figure 5 suggests that language skills can be better represented when considering the inter-neurons dependency. To see how important interdependency is in representing language skills, we train Tree-probers with varying hyperparameters, including the number of trees and depths per tree.13 We report the resulting accuracies on PAWS, CSQA, and HaluEval in Figure 8. Their different shapes indicate that the interdependency levels required for different language skills are different. Some tasks (PAWS) prefer deep layers, while some (CSQA) prefer more trees, and some (HaluEval) require a balance between depths and trees."}, {"title": "Related Work", "content": "Mechanistic interpretability and knowledge attribution methods. Existing studies built the understanding of connections between knowledge and diverse modules in Transformers, such as attention heads (Clark et al., 2019; Olsson et al., 2022; Oymak et al., 2023), neurons in FF layers (Geva et al., 2021, 2022; Dai et al., 2022; Wang et al., 2024b), and the circuits within the models (Meng"}, {"title": "Conclusions", "content": "Our study uncovers a linear relationship between individual neurons and model outputs through neuron intervention experiments. We quantify this linearity by \"neuron empirical gradients\" and propose NeurGrad, an efficient and effective method for estimating these gradients. We demonstrate empirical gradients' utility in representing language skills through skill neuron probing experiments. Our analyses reveal key properties of skill neurons\u2014efficiency, generality, inclusivity, and interdependency. To our knowledge, this is the first study to establish a quantitative link between a model's internal representation and its output through gradients, laying a foundation for PLM output control via neuron-level adjustment."}, {"title": "Limitations", "content": "Our research establishes a framework for measuring neurons' influence on model output and demonstrates the effectiveness of empirical gradients in representing language skills, linking language skill representation to model output through neuron-level empirical gradients. However, the potential for achieving skill-level model output adjustment by tuning neuron values remains unexplored. Directly adjusting neuron values could offer a more efficient alternative to traditional weight-level tuning methods. This approach may enable dynamic behavior modification without altering the underlying parameters of LLMs, potentially reducing computational costs and enabling more flexible model adaptation.\nFurthermore, our discussion on neuron linearity and empirical gradient measurements is currently confined to single-token probing with factual prompts. In the future, we plan to expand our experiments to include prompts from diverse domains and investigate neuron attribution methods for multi-token contexts, aiming to support broader applications in generative language tasks."}, {"title": "Neuron Linearity Analysis", "content": "To understand this divergence between randomly sampled and top-gradient neurons, we measure the percentage of neurons exceeding specific gradient magnitudes across all neurons in PLMs. As shown in Figure 9, Llama2's gradients are five orders of magnitude smaller than BERT's, typically ranging from 10-5 to 10\u20138. Given that gradients are in 16-bit floats with about 5.96 \u00d7 10-8 precision and small gradient magnitudes on Llama2, random noise may overshadow true gradients in correlation calculation on Llama2 with randomly sampled neurons. This explains why correlations increase even with larger shift ranges for randomly sampled neurons (the left-hand side of Figure 2): the increased number of data points likely reduces the impact of noise on the results. We then focus on correlations using neurons with top gradient magnitudes to mitigate the random noise effect.\nTo reduce the impact of noise on the correlation, we select neurons with high absolute gradient values. We use the gradient computed from the computational graph through network back-propagation (hereafter, \u201ccomputational gradient\u201d). Specifically, we measure the correlations from the 1,000 neurons with the highest absolute computational gradients (Figure 2, right). The right-hand side of Figure 2 indicates that activation shifts tend to show stronger correlations with output tokens at smaller shift ranges, consistent across six models."}, {"title": "Generality of Neuron Linearity", "content": "In this section, we provide additional evidence to verify that linearity is a general property for neu-"}]}