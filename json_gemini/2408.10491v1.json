{"title": "Achieving the Tightest Relaxation of Sigmoids for Formal Verification", "authors": ["Samuel Chevalier", "Duncan Starkenburg", "Krishnamurthy (Dj) Dvijotham"], "abstract": "In the field of formal verification, Neural Networks (NNs) are typically reformulated into equivalent mathematical programs which are optimized over. To overcome the inherent non-convexity of these reformulations, convex relaxations of nonlinear activation functions are typically utilized. Common relaxations (i.e., static linear cuts) of \"S-shaped\" activation functions, however, can be overly loose, slowing down the overall verification process. In this paper, we derive tuneable hyperplanes which upper and lower bound the sigmoid activation function. When tuned in the dual space, these affine bounds smoothly rotate around the nonlinear manifold of the sigmoid activation function. This approach, termed a-sig, allows us to tractably incorporate the tightest possible, element-wise convex relaxation of the sigmoid activation function into a formal verification framework. We embed these relaxations inside of large verification tasks and compare their performance to LiRPA and a-CROWN, a state-of-the-art verification duo.", "sections": [{"title": "Introduction", "content": "Formal verification has an ever-widening spectrum of important uses, including mathematical proof validation (Trinh et al. 2024), adversarially robust classification (Zhang et al. 2022a), data-driven controller reachability analysis (Everett 2021), performance guarantees for surrogate models of the electric power grid (Chevalier and Chatzivasileiadis 2024), and more (Urban and Min\u00e9 2021). The formal verification of Neural Networks (NNs), in particular, has seen a flurry of recent research activity. Pushed by the international Verification of Neural Networks Competition (VNN-Comp), NN verification technologies have scaled rapidly in recent years (Brix et al. 2023b,a). Competitors have exploited, and synergistically spurred, the development of highly successful verification algorithms, e.g., \u03b1, \u03b2-CROWN (Wang et al. 2021; Lyu et al. 2019), Multi-Neuron Guided Branch-and-Bound (Ferrari et al. 2022), DeepPoly (Singh et al. 2019), etc. The winningest methods emerging from VNN-Comp serve as the leading bellwethers for state-of-the-art within the NN verification community.\nDespite these advances, verification technologies cannot yet scale to Large Language Model (LLM) sized systems (Sun et al. 2024). Nonlinear, non-ReLU activation functions present one of the key computational obstacles which prevents scaling. While these activation functions can be attacked with spatial Branch-&-Bound (B&B) approaches (Shi et al. 2024), authors in (Wu et al. 2023) note that \"existing verifiers cannot tightly approximate S-shaped activations.\" The sigmoid activation is one such S-shaped activation function which is challenging to deal with. Given its close relationship to the ubiquitous softmax function (Wei et al. 2023), which is embedded in modern transformer layers (Ildiz et al. 2024), efficient verification over the sigmoid activation function would help boost verification speeds and generally help extend verification technology applicability.\nOur contributions. Given the ongoing computational challenge of verifying over NNs containing sigmoid activation fucntions, our contributions follow:\n1. We derive an explicit mapping between the linear slope and y-intercept point of a tangent line which tightly bounds a sigmoid. This differentiable, tunable mapping is embedded into a verification framework to yield the tightest possible element-wise relaxation of the sigmoid.\n2. We propose a \u201cbackward\u201d NN evaluation routine which dynamically detects if a sigmoid should be upper or lower bounded at each step of a gradient-based solve.\n3. To ensure feasible projection in the dual space, we design a sequential quadratic program which efficiently pre-computes maximum slope bounds of all tunable slopes.\nMany verification algorithms exploit element-wise convex relaxation of nonlinear activation functions, resulting in convex solution spaces (Salman et al. 2020). The stacking up of tightening dual variables within a dualized problem reformulation, as in \u03b1, \u03b2-CROWN, can lead to a formally nonconvex mathematical program. Similarly, the verification formulation which we present in this paper is nonconvex, but it globally lower bounds the true verification solution. Authors in (Bunel et al. 2020) have shown that spurious local minima in nonconvex problems containing \u201cstaged convexity\" are very rare, and they even design perturbation-based approaches to avoid them. In this paper, we use a gradient-based approach to solve a formally nonconvex problem, but solutions smoothly converge to what appears to be a global maximum."}, {"title": "Related Works", "content": "To iteratively tighten relaxed ReLU-based NNs, the architects of \u03b1-CROWN (Xu et al. 2021) use the \"optimizable\" linear relaxation demonstrated in Fig. 1. In this approach, the parameter \u03b1 is tuned to achieve a maximally tight lower bound on the verification problem. Authors in (Salman et al. 2020) showed a variables to be equivalent to the dual variables of an associated linear program (LP)-relaxed verification problem. (Wang et al. 2021) applied the \u03b1-CROWN approach within a B&B context, introducing new split-constraint dual variables \u03b2 to be optimized over in the dual space. More recently, (Shi et al. 2024) developed a general framework (GenBaB) to perform B&B over a wide class of nonlinear activations, including sigmoid, tanh, sine, GeLU, and bilinear functions. The authors utilize pre-optimized branching points in order to choose linear cuts which statically bound portions of the activation functions. Critically, they also incorporate optimizable linear relaxations of the bilinear, sine, and GeLU activation functions using \u03b1-like tunable parameters.\nVerification over \"S-shaped\" activation functions is considered in (Wu et al. 2023), where the authors use sequential identification of counterexamples in the relaxed search space to iteratively tighten the activation relaxations. Authors in (Zhang et al. 2022b) propose a verification routine which incorporates the provably tightest linear approximation of sigmoid-based NNs. Notably, the approach uses static (i.e., non-tunable) linear approximations, improving upon other works whose sigmoid relaxations minimize relaxation areas (Henriksen and Lomuscio 2020) or use parallel upper and lower bounding lines (Wu and Zhang 2021). While existing approaches have designed advanced cut selection procedures for sigmoid activation functions, and even embedded these procedures within B&B, there exists no explicitly optimizable, maximally tight linear relaxation strategy for sigmoid activation functions."}, {"title": "Formal Verification Framework", "content": "Let $x \\in \\mathbb{R}^{n_1}$ be the input to an $L$-layer NN mapping $NN(\\cdot) : \\mathbb{R}^{n_1} \\rightarrow \\mathbb{R}^{n_L}$. A scalar verification metric function, $m(\\cdot) : \\mathbb{R}^{n_L} \\rightarrow \\mathbb{R}^1$, wraps around the NN to generate the verification function $f(x) = m(NN(x))$. This metric is defined such that the NN's performance is verified if $f(x) \\geq 0$, $\\forall x \\in C$, can be proved:\n$\\gamma = \\min_{x \\in C} f(x)$.\nIn this NN, $(\\iota) \\, W^{(i)} x^{(i)} + b^{(i)}$ is the $i^{th}$ layer linear transformation, with $x^{(1)} = x$ as the input, and $x^{(i+1)} = \\sigma(\\iota^{(i)})$ is the associated nonlinear activation. In this paper, $\\sigma(\\cdot)$ exclusively represents the sigmoid activation function:\n$\\sigma(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}$,\nwhere the gradient of the sigmoid is $\\sigma' = \\sigma(1 - \\sigma)$. As in (Wang et al. 2021), the region $C$ can be described as an $l_p$ norm ball constraint on the input via $C = \\{ x \\mid ||x - x_o||_p \\leq \\epsilon \\}$. To solve (1) to global optimality (or, at least, to prove $\\gamma > 0$), many recent approaches have utilized (i) convex relaxation of activation functions coupled with (ii) spatial (Shi et al. 2024) and discrete (Wang et al. 2021) Branch-and-Bound (B&B) strategies. B&B iteratively toggles activation function statuses, yielding tighter and tighter solutions, in pursuit of the problem's true lower bound. In this paper, we exclusively consider the so-called root node relaxation of (1), i.e., the first, and generally loosest, B&B relaxation, where all activation functions are simultaneously relaxed. We denote the root node relaxation of $f(x)$ as $f_r(x)$, where\n$\\min_{x \\in C} f(x) \\geq \\min_{x \\in C} f_r(x)$.\nis guaranteed, assuming valid relaxations are applied. While $f_r(x)$ is an easier problem to solve, (Salman et al. 2020) showed that verification over relaxed NNs can face a \"convex relaxation barrier\" problem; essentially, even tight relaxations are sometimes not strong enough to yield conclusive verification results. In this paper, we seek to find the tightest possible relaxation of the sigmoid activation function."}, {"title": "Sigmoid Activation Function Relaxation", "content": "In order to convexly bound the $ReLU(x) = max(x, 0)$ activation function, (Xu et al. 2021) famously replaced the tight \"triangle\" LP-relaxation of an unstable neuron with a tunable lower bound, $ax$, as illustrated in Fig. 1. This $a$ was then iteratively maximized over in the dual space to achieve the tightest lower bound of the relaxed NN. At each gradient step, the value of $a$ was feasibly clipped to $0 \\leq a \\leq 1$, such that $ax < ReLU(x)$ was always maintained.\nIn the same spirit, we seek to bound the sigmoid activation function $\\sigma(x)$ with tunable affine expressions via\n$\\alpha_l x + \\beta_l \\leq \\sigma(x) \\leq \\alpha_u x + \\beta_u$,\nwhere $\\alpha_l, \\beta_l, \\alpha_u$, and $\\beta_u$ are maximized over in the dual space to find the tightest lower bound on the NN relaxation. Analogous to the $0 < a < 1$ bound from (Xu et al. 2021), however, we must ensure that the numerical values of $\\alpha_l, \\beta_l$, and $\\alpha_u, \\beta_u$ always correspond to, respectively, valid lower and upper bounds on the sigmoid activation function.\nDefinition 1. In this paper,\n*   $\\alpha_l x + \\beta_l$ is called an \u201caffine lower bound\u201d, while\n*   $\\alpha_u x + B_u$ is called an \u201caffine upper bound\u201d."}, {"title": "", "content": "In order to derive the slope $\\alpha$ and intercept $\\beta$ terms which yield maximally tight convex relaxation of the sigmoid function, we consider the point at which the associated rotating bound $\\alpha x + \\beta$ intersects with the sigmoid at some tangent point (the upper $u$ and lower $l$ subscripts are dropped for notational convenience). We encode the intersection (5a) and tangent (5b) point relations via\n$\\begin{aligned} \\alpha x + \\beta & = \\sigma(x) \\\\ \\alpha & = \\sigma'(x), \\end{aligned}$ \nwhere $\\sigma'(x)$ is the gradient of the sigmoid. The system of (5) represents two equations and three unknowns. In order to maximize over the $\\alpha$ and $\\beta$ variables independent of the primal variable $x$, it is advantageous to eliminate $x$ entirely. Interestingly, the solution for $\\beta$, written strictly in terms of $\\alpha$, has a closed form solution $\\beta = h(\\alpha)$ (see appendix for derivation). This solution represents a main result from this paper, and it is given by\n$\\beta = \\frac{\\pm \\alpha \\cdot \\cosh^{-1}(\\frac{1}{2 \\alpha} - 1)}{1 + e^{\\pm \\cosh^{-1}(\\frac{1}{2 \\alpha} - 1)}}$.\nIn (6), the $\\pm$ terms are negative for the upper bounds, and positive for the lower bounds, as stated in the appendix. Associated affine bounds are plotted in Fig. 2. As depicted, these bounds are capable of yielding the tightest possible convex relaxation of the sigmoid activation function. Denoting (6) by $\\beta = h(\\alpha)$, we define a set $S$ of valid $\\alpha$, $\\beta$ values:\n$S = \\{ \\alpha, \\beta \\mid \\beta = h(\\alpha), \\, a \\leq \\alpha \\leq \\bar{a} \\}$.\nThe minimum and maximum allowable slope values, $a$ and $\\bar{a}$, are predetermined for every sigmoid activation function. Nominally, $a > 0$, since $\\inf(\\sigma'(x)) = 0$, and $\\bar{a} \\leq 1/4$, since $\\sup(\\sigma'(x)) = 0.25$. However, tighter slopes generally exist, based on the sigmoid's minimum and maximum input bounds $\\underline{x}$ and $\\bar{x}$. Fig. 3 illustrates a typical situation, where there are distinct slope bounds. In this figure, the minimum slopes (dashed lines) are simply computed as the gradients at the minimum and maximum inputs:\n$\\begin{aligned} \\alpha_l & = \\sigma'(\\underline{x}) \\\\ \\alpha_u & = \\sigma'(\\bar{x}). \\end{aligned}$\nThe maximum bounding slopes $\\bar{\\alpha}_l$ and $\\bar{\\alpha}_u$, however, are computed as the lines which intersect the sigmoid at two points: the bounded input anchor points ($\\underline{x}$ and $\\bar{x}$), and a corresponding tangent point. The parallelized calculation of these slopes is a pre-processing step involving sequential quadratic formulate iterations, and the associated procedures are discussed in the appendix.\nFig. 4 illustrates an alternative situation, where the maximum and minimum affine upper bound slopes are equal\u00b9: $\\bar{\\alpha}_u = \\alpha_u$. This occurs when the tangent slope at $\\bar{x}$ can be"}, {"title": "", "content": "\u201craised up\u201d such that the corresponding affine bound eventually intersects with $\\bar{x}$. Defining $\\Delta x = \\bar{x} - \\underline{x}$, we have two possibilities:\n$\\begin{aligned} \\text{if } \\underline{x} - \\sigma'(\\underline{x}) \\Delta x & \\leq \\bar{x}, \\text{ then } \\bar{\\alpha}_u = \\alpha_u \\\\\\ \\text{if } \\underline{x} - \\sigma'(\\underline{x}) \\Delta x & > \\bar{x}, \\text{ then } \\bar{\\alpha}_u > \\alpha_u. \\end{aligned}$\nSimilarly, in the lower affine bound case,\n$\\begin{aligned} \\text{if } \\underline{x} + \\sigma'(\\underline{x}) \\Delta x & > \\bar{x}, \\text{ then } \\bar{\\alpha}_l = \\alpha_l \\\\\\ \\text{if } \\underline{x} + \\sigma'(\\underline{x}) \\Delta x & < \\bar{x}, \\text{ then } \\bar{\\alpha}_l > \\alpha_l. \\end{aligned}$\nIn either case, equal slopes can be directly computed as\n$\\alpha = \\bar{\\alpha}_l = \\bar{\\alpha}_u = \\frac{\\sigma(\\bar{x}) - \\sigma(\\underline{x})}{\\bar{x} - \\underline{x}}$.\nBackward Bound Propagation\nTo efficiently minimize $f_r(x)$, as in (3), we utilize the backward bound propagation procedure employed in, e.g., (Wang et al. 2021). In applying this procedure, however, we utilize a sequential backward evaluation step in order to dynamically detect if a sigmoid function should be upper, or lower, bounded by the affine bound in (6). Consider, for example, the simple problem\n$\\min_{x \\in C} c \\sigma(x) \\geq \\min_{x \\in C} c \\cdot (\\alpha x + \\beta)$,\nwhere $c$ is some cost vector. To achieve minimum cost,"}, {"title": "", "content": "*   if $c_i \\geq 0$, then $\\alpha_l x + \\beta_l$ should lower bound the sigmoid;\n*   if $c_i \\leq 0$, then $\\alpha_u x + B_u$ should upper bound the sigmoid.\nThis procedure is sequentially applied as we move backward through the NN layers. The coefficients in front of each layer, however, will be a function of the numerical $\\alpha$ values, which will be changing at each gradient step during the verification solve. In the appendix, we define a NN mapping (20), and then we sequentially move backward through this mapping in (21), inferring the sign of the coefficients in front of each affine bound term."}, {"title": "Dual Verification", "content": "Using the affine relaxed version of the NN mapping in (20), we may start with input $x$ to sequentially replace all intermediate primal variables:\n$\\begin{aligned} f_r(x) & = c(...d(\\alpha^{(i)} (W^{(i)} (... x ...)+b^{(i)})+\\beta^{(i)})...) \\\\ & \\equiv g_1(\\alpha, \\beta)x + g_2(\\alpha, \\beta). \\end{aligned}$\nThe associated minimization problem over this relaxed verification problem is given by\n$\\begin{aligned} \\min_{x \\in C} f(x) & = \\min_{||x||_p \\leq \\epsilon} g_1(\\alpha, \\beta)x + g_2(\\alpha, \\beta) \\\\ & = - ||g_1(\\alpha, \\beta)||_q \\epsilon + g_2(\\alpha, \\beta), \\end{aligned}$\nwhere the dual norm (Wang et al. 2021; Chevalier, Murzakhanov, and Chatzivasileiadis 2024) has been used to transform the $p$ norm constraint into a $q$ norm objective term. Any valid (i.e., feasible) set of $\\alpha, \\beta$ parameters will yield a valid lower bound for the relaxed verification problem. To achieve the tightest lower bound, we may maximize $f_r(\\alpha, \\beta)$ over the feasible set $S$ of $\\alpha, \\beta$:\n$\\gamma = \\max_{\\{\\alpha,\\beta\\} \\in S} - ||g_1(\\alpha, \\beta)||_q \\epsilon + g_2(\\alpha, \\beta)$.\nWhile $\\alpha, \\beta$ are not dual variables in the traditional sense, they are responsible for actively constraining the primal space, so we we refer to (16) as a dual problem.\nWhile (16) can be solved via projected gradient ascent, we may alternatively use $\\beta = h(\\alpha)$ in order to eliminate the"}, {"title": "", "content": "$\\begin{aligned} & \\text{Compute } t = f_r(\\alpha, h(\\alpha)) \\\\ & \\textbf{for } \\text{NN Layers } i = L, L - 1, ...1, \\textbf{do} \\\\ & \\text{Compute layer sign vector } s^{(i)} \\text{ in (21)} \\\\ & \\text{Embed } s^{(i)} \\text{ inside of } \\beta \\text{ via (18)} \\\\ & \\textbf{end for} \\\\ & \\text{Back-propagate objective } f_r \\text{ of (17) w.r.t. } \\alpha \\\\ & \\text{Take a gradient ascent step with, e.g., Adam} \\\\ & \\text{Clip all } \\alpha \\text{ values between } \\underline{a} \\text{ and } \\bar{a} \\\\ & \\textbf{end while} \\\\ & \\textbf{return } \\text{tight lower bound } \\gamma = f_r(\\alpha, \\beta) \\end{aligned}$\n$\\begin{aligned} \\frac{1}{1 + \\exp \\Big( s_j^{(i)} \\cdot \\cosh^{-1} \\big( \\frac{1}{2 \\alpha_j^{(i)}} - 1 \\big) \\Big)} \\end{aligned}$\n$\\begin{aligned} & s_j^{(i)} \\alpha^{(i)} \\cdot \\cosh^{-1} \\bigg( \\frac{1}{2 \\alpha_j^{(i)}} - 1 \\bigg), \\, \\forall i, j \\in \\pm 1. \\end{aligned}$\n$\\gamma = \\max_{\\{\\alpha\\}} - ||g_1(\\alpha, h(\\alpha))^T ||_q + g_2(\\alpha, h(\\alpha))$.\nWe use a projected gradient routine in order to solve (17) and enforce $\\underline{a} \\leq \\alpha \\leq \\bar{a}$. Sigmoid functions may need to be upper bounded via (29), and then lower bounded via (30), as numerical values of $\\alpha$ evolve. To overcome this challenge, at each step of our numerical routine, we reverse propagate compute the sign vector $s^{(i)}$, from (21), for each layer. Since this vector tells us if the sigmoid function should be upper or lower bounded, we embed corresponding elements of this vector inside of (6), replacing the $\\pm$ terms. At the $j^{th}$ activation function of each $i^{th}$ NN layer, the corresponding slope and sign elements are feasibly related via\nWith this parameterization embedded inside of (17), we backpropagate through the objective function and take a gradient step with $\\alpha$. Next, we clip all values of $\\alpha$ to remain between $\\underline{a}$ and $\\bar{a}$, depending on whether the corresponding $\\alpha$ is acting like an upper or lower affine bound. The full gradient-based verification routine for a sigmoid-based NN is given in Alg 1, which we refer to as a-sig. The bounds $\\underline{x}, \\bar{x}$ for each activation function are treated as inputs."}, {"title": "Test Results", "content": "In order to test the effectiveness of a-sig and Alg. 1, we optimized over a range of randomly generated sigmoid-based NNs in two separate experiments. All NNs consisted of four dense layers with sigmoid activation functions, followed by"}, {"title": "", "content": "a dense linear layer. We considered NNs containing 5, 10, 50, 100, 500, and 1000 neurons per layer. For each NN size, we generated and verified over 5 independent NN instantiations. The posed verification problem sought to minimize the sum of all outputs (i.e., the $c$ vector from (20a) was set to the vector of all ones), and we assumed an allowable infinity norm perturbation of $||x||_{\\infty} \\leq 1$ (i.e., $-1 < \\underline{x} < \\bar{x} < 1$). To generate initial loose activation function bounds for all NN layers, we applied vanilla interval bound propagation via (39), as reviewed in the appendix. We note that tighter bounds could potentially be achieved via IBP + backward mode LiRPA (Xu et al. 2020), but we chose to utilize weaker IBP-based bounds to initialize a-sig in order to highlight its effectiveness without strong initial activation function bounds.\nIn each test, we took 300 projected gradient steps in pursuit of solving (17). We then benchmarked our results against \u03b1-CROWN + auto-LiRPA (Xu et al. 2021, 2020). In order to fairly compare, we increased the default \u03b1-CROWN iteration count by 3-fold, to 300 iterations. To avoid an early exit from CROWN (i.e., due to a successfully proved bound), we set the VNN-LIB (Ferrari et al. 2022) verification metric to an arbitrarily high value $\\Gamma$ (i.e., prove $f(x) \\geq \\Gamma, \\, \\forall x$). In order to compare the bounds proved via our \u03b1-sig in Alg. 1 vs \u03b1-CROWN, we defined $\\Tau$:\n$\\Tau = 100 \\times \\frac{\\gamma - \\text{CROWN}^{\\text{bound}}}{|\\text{CROWN}^{\\text{bound}}|}$,\nwhere $\\Tau$ represents the percent improvement or decline of our bound relative to \u03b1-CROWN (positive means \u03b1-sig provides a tighter bound than \u03b1-CROWN, while negative $\\Tau$ means the bound in looser). \u03b1-sig was built in Julia, and all code is provided as supplementary material.\nExperiment 1: Varying weight distributions. Normally distributed weight and bias parameters tend to yield NNs whose sigmoid activation functions are always stuck on or off. In order to avoid this, we we initialized all NN weights and biases via $W_i \\sim \\mathcal{N}(0, (2.5/j)^2)$ and $b_i \\sim \\mathcal{N}(0, 0.25^2)$, where $j \\in \\{ 1, 2, 3, 4, 5 \\}$ represents the model index; thus, in this experiment, the weight parameter variances for each NN model progressively shrank (for a given model size). Results associated with this test are given in Table 1: in this table, the values $\\Tau_1$ through $\\Tau_5$ represent bound comparisons, \u00e0 la (19), across five independently generated models (five for each NN size). Clearly, \u03b1-sig tended to provide marginally tighter bounds than \u03b1-CROWN. Across all six NN sizes, the bound progressions for \u03b1-sig are illustrated in Fig. 5. In this figure, initial CROWN and fully optimized \u03b1-CROWN bounds are superimposed for reference.\nIn the upper-right portion of Table 1, \u03b1-CROWN outperformed \u03b1-sig. The reason why is apparent and interesting: in our experiments, \u03b1-sig was initialized with fairly loose IBP-based primal bounds $\\underline{x}, \\bar{x}$ (see input to Alg. 1). As NN models shrink in size and weight parameter variances drop, auto-LiRPA's proclivity for primal bound solving seems to overtake \u03b1-sig's optimal tightening of the sigmoid activation function. The benefit of \u03b1-sig, however, is also in its speed. As demonstrated in Table 2, \u03b1-sig can be up to two orders of magnitude faster than \u03b1-CROWN, while still yielding"}, {"title": "Discussion and Conclusion", "content": "Verifying over NNs containing S-shaped activation functions is inherently challenging. In this paper, we presented an explicit, differentiable mapping between the slope and $y$-intercept of an affine expression which tangentially bounds a sigmoid function. By optimizing over this bound's parameters in the dual space, our proposed convex relaxation of the sigmoid is maximally tight (i.e., a tighter element-wise relaxation of the sigmoid activation function does not exist). As explored in the test results section, however, our ability to fully exploit this tightness hinges on having good primal bounds $\\underline{x}, \\bar{x}$ for all activation functions. For example, given"}, {"title": "Neural Network Mapping and Relaxation", "content": "The NN mapping, whose output is transformed by a verification metric $c^\\top$, is stated below. At each sigmoid activation function, we also state the corresponding affine relaxation, where $d(\\alpha)$ diagonalizes an $\\alpha$ vector into a diagonal matrix:\n$\\begin{aligned} CNN(x) & = c^\\top (W^{(L)}x^{(L)} +b^{(L)}) \\\\ x^{(L)} & = \\sigma(\\iota^{(L-1)}) \\\\ x^{(L)} & : \\iota^{(L-1)} (W^{(L-1)}x^{(L-1)} + \\beta^{(L-1)} \\\\ \\iota^{(L-1)} & = W^{(L-1)}x^{(L-1)} + b^{(L-1)} \\\\ \\& \\vdots \\\\ x^{(3)} & = \\sigma(\\iota^{(2)}) \\\\ x^{(3)} & : \\iota^{(2)} (W^{(2)}x^{(2)} + \\beta^{(2)} \\\\ \\iota^{(2)} & = W^{(2)}x^{(2)} + b^{(2)} \\\\ x^{(2)} & = \\sigma(\\iota^{(1)}) \\\\ x^{(2)} & : \\iota^{(1)} (W^{(1)}x^{(1)} + \\beta^{(1)} \\\\ \\iota^{(1)} & = W^{(1)}x^{(1)} + b^{(1)} \\\\ x^{(1)} & = x. \\end{aligned}$\nWe may now relax the sigmoid activation functions, replacing them with their affine bounds. To determine if each scalar $\\alpha, \\beta$ pair are chosen to be an upper bound or a lower bound, we need to determine the coefficient in front of the corresponding activation function. To do this, we move backward through the NN, defining vectors $s^{(i)}$ of sign values corresponding to the signs of the entries in the argument:\n$\\begin{aligned} s^{(L)} & = \\text{sign}(c^\\top W^{(L)}) \\\\ s^{(L-1)} & = \\text{sign}(c^\\top W^{(L)} d(\\alpha^{(L-1)}) W^{(L-1)}) \\\\ & \\vdots \\\\ s^{(2)} & = \\text{sign}(c^\\top W^{(L)} d(\\alpha^{(L-1)}) W^{(L-1)} ... \\\\ & \\quad d(\\alpha^{(2)}) W^{(2)}) \\\\ s^{(1)} & = \\text{sign}(c^\\top W^{(L)} d(\\alpha^{(L-1)}) W^{(L-1)} ... \\\\ & \\quad d(\\alpha^{(2)}) W^{(2)} d(\\alpha^{(1)}) W^{(1)}). \\end{aligned}$"}, {"title": "Slope and Intercept Relation Derivation", "content": "Starting from (5b), the sigmoid derivative can be written via $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$. Setting this equal to the upper affine bound slope, we have\n$\\alpha = \\frac{e^{-x}}{(1 + e^{-x})^2}$,\n$\\alpha = \\frac{1}{1 + (e^{-x} + e^{x} + 1)}$.\nMultiplying through by the expanded right-side denominator, we may solve for the primal variable $x$ explicitly:\n$\\alpha (e^{x} + e^{-x} + 2) = 1$\n$\\alpha (e^{-x} + e^{x} + 2) = 1$\n$2\\alpha (\\cosh(x) + 1) = 1$\n$x = \\pm \\cosh^{-1} (\\frac{1}{2\\alpha} - 1)$.\nSince $\\cosh(x) = \\cosh(-x)$, the solution for $x$ is non-unique, which is an inherent consequence of the symmetry of the sigmoid function, where exactly two points in the sigmoid manifold will map to the same slope. Notably, however, the affine upper bound will map to positive solutions"}, {"title": "", "content": "of $x$, i.e., in the region where the sigmoid function is concave, and the affine lower bound will map to negative values of $x$, i.e., in the region where the sigmoid function is convex:\n$\\begin{aligned} \\underline{x} & = \\cosh^{-1} (\\frac{1}{2\\alpha} - 1) \\\\ \\bar{x} & = - \\cosh^{-1} (\\frac{1}{2\\alpha} - 1). \\end{aligned}$\nReorganizing the intersection equation (5a), such that $\\beta = \\frac{1}{1 + e^{-x}} - \\alpha x$, we may plug the primal solutions (27)-(28) in for $x$. This yields explicit expressions for the intercept points $(\\beta_u, \\beta_l)$ as functions of the slopes $(\\alpha_u, \\alpha_l)$:\n$\\begin{aligned} \\beta_u & = \\frac{1}{1 + e^{+ \\cosh^{-1} (\\frac{1}{2 \\alpha_u} - 1)}} - \\alpha_u \\cosh^{-1} (\\frac{1}{2 \\alpha_u} - 1) \\\\ \\beta_l & = \\frac{1}{1 + e^{- \\cosh^{-1} (\\frac{1}{2 \\alpha_l} - 1)}} + \\alpha_l \\cosh^{-1} (\\frac{1}{2 \\alpha_l} - 1). \\end{aligned}$"}, {"title": "Computing Maximum Slope Limits", "content": "In order to compute the maximum slope values $\\bar{\\alpha"}, "l$ and $\\bar{\\alpha}_u$, as depicted in Fig. 3, we use a sequential numerical routine which iteratively solves a quadratic expansion of the associated problem. Consider the following system of equations, with unknown variables $\\alpha, \\beta$, and $\\hat{x}$ (intercept point):\n$\\begin{aligned} y & = \\alpha \\underline{x} + \\beta & \\quad \\text{anchor point} \\\\ \\sigma(\\hat{x}) & = \\alpha \\hat{x} + \\beta & \\quad \\text{sigmoid intersection at } \\hat{x} \\\\ \\sigma'(\\hat{x}) & = \\alpha & \\quad \\text{match sigmoid slope at } \\hat{x}, \\end{aligned}$\nwhere the known \u201canchor point\u201d $(\\underline{x}, y)$ is depicted in Fig. 6. Despite its similarity to (5), this system does not have a closed-form solution (i.e., it will result in a single nonlinear equation, similar to (6), but with $\\beta$ replaced by an expression for $\\alpha$). In order to efficiently solve this system of equations, we perform a single quadratic expansion of the sigmoid activation function (Agarwal et al. 2022):\n$\\sigma(x) \\approx \\tilde{\\sigma}(x) = \\sigma_o + \\sigma'(\\underline{x}) (x - x_o) + \\frac{1}{2} \\sigma''(\\underline{x}) (x - x_o)^2$,\nwhere $\\sigma_o \\equiv \\sigma(x)|_{x_o}$, etc., is used for notational simplicity. Collecting like powers of $\\hat{x}$, the quadratic expansion yields\n$\\begin{aligned} \\tilde{\\sigma}(x) & = \\sigma_o + \\sigma' - \\sigma' \\underline{x} + \\frac{1}{2} \\sigma'' (\\hat{x}^2 + \\underline{x}^2 - 2 \\hat{x} \\underline{x}) \\\\ & = \\sigma_o + \\sigma' - \\sigma' \\underline{x} + \\frac{1}{2} \\sigma'' (\\hat{x}^2 + \\underline{x}^2 - 2 \\hat{x} \\underline{x}) \\\\ & = (\\sigma_o - \\sigma' \\underline{x} + \\frac{1}{"]}