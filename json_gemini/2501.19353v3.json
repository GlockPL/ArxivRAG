{"title": "Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SCICAP Challenge 2023", "authors": ["Ting-Yao 'Edward' Hsu", "Yi-Li Hsu", "Shaurya Rohatgi", "Chieh-Yang Huang", "Ho Yin Sam Ng", "Ryan Rossi", "Sungchul Kim", "Tong Yu", "Lun-Wei Ku", "Clyde Lee Giles", "Ting-Hao \u2018Kenneth\u201d Huang"], "abstract": "Since the SCICAP dataset's launch in 2021, the research community has made significant progress in generating captions for scientific figures in scholarly articles. In 2023, the first SCICAP Challenge took place, inviting global teams to use an expanded SCICAP dataset to develop models for captioning diverse figure types across various academic fields. At the same time, text generation models advanced quickly, with many powerful pre-trained large multimodal models (LMMs) emerging that showed impressive capabilities in various vision-and-language tasks. This paper presents an overview of the first SCICAP Challenge and details the performance of various models on its data, capturing a snapshot of the field's state. We found that professional editors overwhelmingly preferred figure captions generated by GPT-4V over those from all other models and even the original captions written by authors. Following this key finding, we conducted detailed analyses to answer this question: Have advanced LMMs solved the task of generating captions for scientific figures?", "sections": [{"title": "Introduction and Background", "content": "Scientists use figures like bar charts, pie charts, or scatter plots to convey key findings in scholarly articles. However, the texts accompanying these figures, the figure captions, are often overlooked by the authors and do not receive the needed attention when being composed. Even though many studies have shown the role of captions in enhancing readers' comprehension and recall of the messages conveyed by figures (Nugent, 1983; Large et al., 1995; Bransford, 1979; Hegarty and Just, 1993), poorly-written captions are, unfortunately, common (Huang et al., 2023).\nIn response, Hsu et al. (2021) launched SCICAP, a large-scale collection of 133,543 single-panel line charts and their captions extracted from arXiv papers, aiming to fuel the creation of new models that generate high-quality captions for scientific figures. Over the past three years, SCICAP has advanced researchers' understanding of scientific figure captions and driven the progress of technologies generating them: SCICAP confirmed that many low-quality captions exist in scholarly articles, as over half of the figure captions in arXiv cs. CL papers were rated \u201cunhelpful\u201d by NLP Ph.D. students (Huang et al., 2023); SCICAP revealed that producing figure captions in scholarly articles is a generative task heavily reliant on the texts within the articles (Yang et al., 2023; Li and Tajbakhsh, 2023; Horawalavithana et al., 2023)- this task relies so much on the paper content that it can be more effectively tackled through text summarization, summarizing all paragraphs mentioning the figures (e.g., \"Figure 3 shows...\"), rather than as a vision-to-language task (Huang et al., 2023); SCICAP elevated the quality of generated captions to a level where, in instances where author-created captions were poorly crafted, readers found the generated captions more helpful (Huang et al., 2023), thereby offering practical assistance in caption writing (Hsu et al., 2024). In 2023, the first SCICAP Challenge took place. With an expanded SCICAP of 476,389 single-panel figures from 8 domains and 5 figure types, the challenge invited teams worldwide to develop caption-generation models for diverse figures.\nMeanwhile, in the past two years, the landscape of text generation has rapidly evolved: ChatGPT (GPT-3.5) was initially released to the public in November 2022; its more powerful successor, GPT-4, was released in March 2023; and GPT-4V, the successor to GPT-4, which can take images and texts as input, became publicly available in September 2023. Numerous open-source pre-trained Large Language Models (LLMs) and Large Multimodal Models (LMMs), such as OPT (Zhang et al., 2022), LLaMA-2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), Gemma (Team et al., 2024), LLaVA (Liu et al., 2023), BLIP-2 (Li et al., 2023), mPLUG-Owl2 (Ye et al., 2023) and MiniGPT-4 (Zhu et al., 2023), have been made available. All these models have shown impressive progress in understanding charts and tables within documents (Yue et al., 2023). Now is a good time to take a step back and critically assess the collective advancements made in generating high-quality, useful captions for scientific figures. Specifically, we aim to answer this question: Have the impressive large multimodal models solved the challenge of generating good captions for scientific figures?\nThis paper first overviews the 2023's SCICAP Challenge, including its data, procedure, winning teams and their models, which represents the status quo of the scientific figure caption generation task (Section 3). Next, we ran GPT-4V\u2014unavailable at the time of the challenge\u2014and other open LMMs, such as UniChart (Masry et al., 2023), on SCICAP Challenge's data and, through automatic and human evaluations, compared their performances to all models that participated in the challenge (Section 4). The automatic evaluation revealed that the linguistic features of captions differ by domain and figure type, resulting in a model's scores varying across these categories. More interestingly, in the human evaluation, three professional editors with expertise in technical academic writing unanimously preferred captions generated by GPT-4V over those from all other models, including the original captions written by the authors. Driven by this key finding, we investigated why editors strongly prefer GPT-4V (Section 6.3), comparing their views with those of Ph.D. and undergrad students (Section 5), to directly address whether LMMs have solved scientific figure caption generation. We concluded by identifying unresolved challenges and suggesting future research directions (Section 6)."}, {"title": "Related Work", "content": "In addition to the works directly related to SCICAP (Section 1), the complex nature of figure narrating-which demands an understanding of both vision and text, along with intricate domain-specific contexts-has made it an intellectually intriguing challenge, inspiring various parallel projects focused on text generation for figures. FigJAM leveraged metadata and a combined static and dynamic dictionary to generate better caption units (Qian et al., 2021). FigCAP developed innovative attention mechanisms and employed reinforcement learning for sequence-level training to enhance caption generation performance (Chen et al., 2020). Large-scale chart-to-text benchmark datasets have also been created to facilitate data-to-text techniques for chart summarization (Kantharaj et al., 2022; Obeid and Hoque, 2020). Tarsi et al. (2024) presented two datasets for scientific multimodal learning: SciOL, a large corpus covering various sciences, and MuLMS-Img, a high-quality annotated materials science dataset. Xia et al. (2024) introduced ChartX, a comprehensive evaluation set with 18 chart types, 7 tasks, 22 topics, and developed ChartVLM, a model tailored for multi-modal tasks requiring interpretable patterns, like chart reasoning. Unichart developed specialized pretraining tasks for charts, focusing on visual element extraction and reasoning skills (Masry et al., 2023). ChartSumm introduced a comprehensive benchmark dataset with 84,363 charts, metadata, and descriptions across various topics and types for creating concise and detailed summaries (Rahman et al., 2023). While inspiring, most efforts did not aim to produce real-world captions from scholarly articles. To show this, we included UniChart's output in our study."}, {"title": "The First SCICAP Challenge (2023)", "content": "The first SCICAP Challenge was held at the 5th Workshop on Closing the Loop Between Vision and Language during ICCV 2023. The organizers expanded the SCICAP dataset to produce a challenge-specific version which was divided into test, validation, and training sets, containing 476,389 single-panel arXiv figures. Teams had three months to develop solutions and submit model outputs for all test set figures. This section details the challenge."}, {"title": "Challenge Dataset", "content": "The SCICAP challenge dataset comprises 476,389 single-panel figures from arXiv papers\u00b9 in 8 domains: (i) Computer Science (cs), (ii) Economics (econ), (iii) Electrical Engineering and Systems Science (eess), (iv) Mathematics (math), (v) Physics (physics), (vi) Quantitative Biology (q-bio), (vii) Quantitative Finance (q-fin), and (viii) Statistics (stat). It covers 5 figure types, as denoted by the figure type classifier, FigureSeer (Siegel et al., 2016): (i) Node Diagram, (ii) Equation, (iii) Graph Plot, (iv) Scatterplot, and (v) Bar Chart. This dataset, with 476,389 figures, is more than 3 times the size of the original SCICAP dataset, which contains 133,543 figures. The figures were randomly sampled from arXiv papers published between 2010 and 2020, with a maximum of four figures selected per paper. While the organizers had access to the full collection of over 440,000 arXiv papers from this period, they intentionally limited the dataset size to encourage participation for teams with limited computational resources. Appendix A details the dataset preparation process.\nConcerns about the quality of arXiv papers are understandable. However, Huang et al. (2023) manually verified 399 figures from the original SCICAP test set and found that 81.2% (324/399) originated from papers published at academic conferences, with 51.9% (207/399) appearing in the ACL Anthology, IEEE, or ACM.\u00b2 These findings suggest that the dataset is both representative and of reasonable quality."}, {"title": "Challenge Procedure", "content": "The SciCap Challenge was hosted on EvalAI (Yadav et al., 2019). It had two phases: the Test Phase and the Challenge Phase. During the Test Phase-which began on May 29, 2023, and lasted approximately 2.5 months each team used the provided training set, validation set, and public test set to build and test their models. Each team can upload their predictions for the public test"}, {"title": "In-Depth Evaluation of Models' Performances", "content": "We conducted a series of analyses on each model's performance, including comparisons with advanced LMMs that were unavailable during the challenge. Study 1 (Section 4.1) and Study 2 (Section 4.2) assessed performance using the challenge's hidden test set, with Study 1 focusing on automatic evaluation results and Study 2 on human evaluation results. To verify the generalizability of our findings, Study 3 (Section 4.4) repeated these evaluations on newer arXiv papers."}, {"title": "Models Included in the Evaluation", "content": "This analysis compared the models of the two winning teams with three other sets of models:\nText Summarization Model: Pegasus. Prior studies showed that generating captions for scientific figures in scholarly articles can be effectively tackled by treating it as a text summarization task, i.e., summarizing all paragraphs mentioning the target figure into its caption (Huang et al., 2023). We fine-tuned Pegasus (Zhang et al., 2020), a well-known text summarization model, with the SciCap Challenge dataset's training set in our performance comparison.\nLarge Multimodal Model (LMM): GPT-4V.\u2075 We evaluated GPT-4V for figure captioning in two settings: (i) using figure-mentioning paragraphs and images as input, and (ii) using only figure images. We used GPT-4V through OpenAI's API. Appendix B describes the details.\nOpen LMM: UniChart (Masry et al., 2023). We fine-tuned UniChart using the SCICAP Challenge data. The training and inference details are provided in Appendix C."}, {"title": "Why Exclude Text-Only LMMs?", "content": "In our main study, we evaluated only LMMs that can take images as input, excluding text-only conditions. This decision followed our observation that combining text and images results in better captions. A human evaluation confirmed this performance difference: We randomly sampled 200 figures from the arXiv Challenge Dataset (Section 3) and generated two captions for each figure using GPT-4V: one caption based solely on the textual context (i.e., paragraphs referencing the figure), and another that also incorporated the figure images. The same three experts who performed human evaluation in Study 2 (Section 4.3) ranked these captions. Experts ranked image+text captions higher than text-only captions in 82.5%, 66%, and 56% of cases, respectively. This indicates that captions with images (mean=1.32; lower scores denote better performance) significantly outperformed text-only captions (mean=1.68)."}, {"title": "Study 1: Automatic Evaluation Results", "content": "We evaluated our models using BLEU-4, ROUGE-1, ROUGE-2, and ROUGE-L metrics (Papineni et al., 2002; Lin, 2004), calculating ROUGE scores with the rouge-score tool on all-lowercase, stemmed text (google research, 2022). Following Huang et al. (2023), we also used normalized ROUGE scores because ROUGE can be affected by text length, with longer texts typically scoring higher (Sun et al., 2019). The normalization factor was computed on the hidden test set.  and  show model performance across domains and figure types.\nModels maintained consistent performance rankings across categories. Figure 2 and Figure 3 showed the ROUGE-2-Normalized scores for each model across various domains and figure types. The Leaderboard Winner, NJUST, consistently scored the highest in every category, followed by PEGASUS in second place and USTC in third. Scores for both LLMs and LMMs were consistently lower. Model score rankings were consistent across categories, but score ranges varied. The domains of cs and eess saw the highest scores, and Node Diagrams emerged as the figure type with the highest scores.\nWe speculate that differences in automatic scores across categories stem from the linguistic characteristics of captions rather than the amount of training data available. physics, the domain with the lowest scores in Figure 2, was the most common, and graph plots, the most frequent figure type, also scored low in Figure 3."}, {"title": "Study 2: Human Evaluation Results By Professional Editors", "content": "Expert Judges. One of the primary goals of the SCICAP dataset and challenge is to produce captions that help human readers. To assess this, we recruited three professional editors through UpWork (upwork.com), specializing in academic articles in the technical field and all native American English speakers. Their qualifications include: one with over ten years of editing experience and a Ph.D. in Comparative Literature, and two from STEM fields-one in Theoretical Astrophysics and another in Neuroscience-both with a significant track record in editing, proofreading, and publishing academic papers. They spent between 30 to 60 minutes evaluating 10 figures, with their rates ranging from $50 to $60 per hour."}, {"title": "Varying Length Constraints", "content": "Three Varying Length Constraints. These editors conducted a three-part human evaluation. Caption length is known to be a key factor influencing human judgments of quality, as longer captions are often perceived as more informative by readers (Hartley, 2003; Gelman et al., 2002). To assess human-perceived model performance under varying length constraints, we conducted human evaluations for captions generated under three different settings, ranging from no length constraints to the strictest length constraints:\n1.  Generation with no length constraints (all arXiv domains, Figure 4A): For the first part, we selected 100 figures from the hidden test set and asked each editor to rank 6 captions for each figure: 1 author-written and 5 machine-generated by models used in Section 4.2 (including the two teams' outputs), excluding GPT-4V with image-only input due to its poor quality. They used a drag-and-drop interface (Figure 9 in Appendix D), similar to that of Hsu et al., to rank the captions based on the criterion: \"When I read the paper, the caption can help me understand the message that the figure tries to convey.\" This represented the least length-constrained setting. For additional context, when no length constraints were specified in prompts, the average length of GPT-4V's captions (34.9 words) was shorter than that of human-written captions (42.9 words).\n2.  Generation with 25-word length constraints (cs Papers, Figure 4B): The second part followed the same procedure as the first, but focused specifically on 100 figures from cs domain papers. We selected the cs domain because the average length of human-written captions in cs arXiv papers is 25.52 words, shorter than GPT-4V's average caption length (33.6 words) when no length constraints are given. This choice helped mitigate the potential effect of longer human-written captions in the first condition. To align GPT-4V's captions with the average length in the cs domain, we modified the prompt to limit caption length to 25 words. Consequently, the average length of GPT-4V's captions was 25.79 words, closely matching the 25-word target.\n3.  Generation with length no longer than human captions (all arXiv domains, Figure 4C): In the third part, we followed the same procedure as in the first condition to sample 100 figures from papers of all arXiv domains. We modified GPT-4V's prompt to restrict caption length to be no longer than the corresponding human-written captions from arXiv papers, establishing the strictest length constraints among all three settings. In this setting, generated captions were expected to always be shorter or match the length of human-written captions.\nGPT-4V captions were preferred over author-written ones and all other models. The ranking results are shown in Figure 4. Captions generated by GPT-4V, using both paragraphs and images, were overwhelmingly preferred by all experts in every setting, surpassing even author-written captions. Although author-written captions ranked slightly lower than GPT-4V, they still outperformed all other models, remaining reliable when considering the proportion of outputs ranked in the top two. Among the remaining models, the text-only summarization model, Pegasus, consistently outperformed Unichart and the two team models. Unichart, which was designed for chart understanding and reasoning rather than text generation, received the lowest scores in human evaluation. It is worth emphasizing that extensive evidence shows that automatic evaluation metrics, like BLEU, do not align well with human judgment (Dhingra et al., 2019). Therefore, in this paper, we place greater emphasis on human evaluation over automatic evaluation results. We also analyzed the experts' free-text feedback to understand their rankings, as detailed in Section 6.3.\nLength constraints did not affect GPT-4V's superiority. The human evaluation results show that, although human readers tend to favor longer captions, imposing stricter length constraints did not diminish the strong preference for GPT-4's captions. Each expert displayed individual preferences, but overall, they favored GPT-4's captions in most cases. Notably, GPT-4 roughly adhered to length constraints: In Condition 1 (all arXiv domains), the average lengths were 41.69 for human-written captions and 33.6 for GPT-4. In Condition 2 (cs domain only), the averages were 25.52 for humans and 25.79 for GPT-4. In Condition 3 (all arXiv domains with human-written length constrained), the human average was 39.68, while GPT-4's average was 44.59."}, {"title": "Additional Study: Do Paper Readers Agree with the Editors' Judgement?", "content": "Study 2 (Section 4.3) shows that professional editors' perspective toward machine-generated captions, raising an intriguing question: Do paper readers evaluate caption quality in the same way as professional editors? This distinction is important because prior studies have often relied on general readers, such as graduate students, rather than professional editors for human evaluations (Petsiuk et al., 2022; Kasai et al., 2022). Understanding any systematic differences between readers and editors is essential to inform the broader research community.\nTo answer this question, we obtained the data for the study conducted by Hsu et al. (2023), where Ph.D. students in the relevant fields ranked figure captions generated by different models, and undergraduate students rated the captions' helpfulness. The models Hsu et al. (2023) used differed from ours but similarly employed a fine-tuned text summarization model, Pegasus, as a baseline. Their study included three versions of captions produced by Pegasus: one fine-tuned with the entire training set of the SCICAP dataset [Pegasus (Paragraph+OCR)], one fine-tuned using only captions exceeding 30 words in length [Pegasus (Paragraph+OCR+Better)], and the other fine-tuned using only the figure image's OCR [Pegasus (OCR)]. They also included author-written captions in their comparison, providing a basis for comparing their results with ours. Interestingly, their study focused on three cs subdomains of arXiv: natural language processing (NLP, cs. CL), human-computer interaction (HCI, cs.HC), and computer vision (CV, cs. CV). This section describes the insight gained from comparing their results, as shown in Figure 6 and 7, with ours.\nStudents agreed with editors that the basic text-summarization model did not outperform paper authors. Ph.D. students (Figure 6) and undergraduate students (Figure 7) both preferred author-written captions to those generated by the Pegasus (Paragraph+OCR) model, which was fine-tuned on the entire dataset rather than just long captions. These preferences aligned with the editors' judgments (Section 4.3), where human-authored captions were chosen more often over Pegasus-generated ones, even though our text-summarization model was fine-tuned on a much larger dataset than Hsu et al. (2023)'s.\nUser groups do not always agree with each other. While the overall trend in quality assessment among models was consistent between Ph.D."}, {"title": "Discussion", "content": "Is the Problem Solved?\nOur human evaluations show that GPT-4V often generates captions that are seen as better than those written by authors (Section 4.3). However, this does not fully solve the challenge of generating captions for scientific figures. We have reached an important milestone, but there are new challenges ahead:\n\u2022  First, improving caption quality further. Being better than human-written captions does not necessarily mean the captions are of high quality; it might simply mean that the human-written captions are bad.\n\u2022  Second, evaluation is still hard. Despite GPT-4V's poor automatic scores (Section 4.2), it outperformed other models in evaluations by editors; efforts to use a higher-quality subset for better evaluation did not significantly differ in scores from the full test set (Table 1). These highlight the need for more research into evaluation methods, particularly as the quality of machine-generated content surpasses that of human-generated content. Future research should focus on detail, coherence, and factual accuracy. Developing a reliable automatic evaluation method could also enhance the quality of datasets used to develop new models.\n\u2022  Finally, personalization for different users. Different user groups have different information needs. Another challenge is to create models that can customize captions to meet the diverse preferences of various users."}, {"title": "Generalizability of Caption Generation Models", "content": "Our findings suggest caption generation models are versatile across domains and figure types. Although there are some variations and domain-specific details, large multimodal models pre-trained on extensive data can likely generate usable captions for many domains and figure types. We also tested several hypotheses to explain the variations in automatic scores across domains, such as the presence of non-English characters in captions or length of captions, but found no conclusive evidence to support them."}, {"title": "What Makes GPT-4's Captions Better?", "content": "To understand why GPT-4's generated captions were often rated higher than author-written captions, we manually analyzed the comments provided by three experts during the ranking process. Although experts were instructed to provide comments, we did not strictly enforce this for every figure. In total, we collected 1,123 comments corresponding to 500 figures. The first author of this paper manually coded these comments using open coding practices to identify the reasons behind GPT-4V's superior ratings. Two key themes emerged: (i) providing sufficient details and (ii) highlighting the figure's takeaway messages. Among the 1,123 comments analyzed, 277 (24.7%) indicated that captions were rated higher or lower based on the presence or absence of rich details, while 177 (15.8%) noted that including or omitting takeaway messages influenced their ratings. These findings align with prior research showing that detailed captions with clear takeaway messages receive higher ratings (Hsu et al., 2023). Because humans tend to prefer longer captions, we further analyzed the 207 comments from Study 2's C condition, where GPT-4's captions were restricted to the same length as the authors'. Even under this constraint, the two key factors remained prominent: 49 (23.7%) comments mentioned details, and 50 (24.2%) comments cited takeaway messages.\nWe also analyzed the experts' comments to identify the most common errors in AI-generated captions, particularly those from models other than GPT-4. These errors fell into three main categories: The first category is incorrect visual information, where visual details in captions, such as color-number relationships, are inaccurately described. The second category involves linguistic errors, including typos, missing punctuation, or minor grammatical mistakes. The final category is factual errors unrelated to visuals, such as incorrect numerical values. In addition to outright errors, we also examined cases where captions, though not strictly incorrect, were problematic. One common issue was incomplete captions, where truncation resulted in phrases like \"Delivery ratio vs.\u201d or \u201cPlot of F2 versus ab.\u201d Another problem was repetitive or overly generic captions, where models produced identical or indistinct descriptions, such as \u201cIoT Hub Workflow,\u201d reflecting a lack of specificity."}, {"title": "Conclusion and Future Work", "content": "This paper presents the first SCICAP Challenge results, highlighting GPT-4V's superiority in generating scientific figure captions, as professional editors overwhelmingly preferred figure captions generated by GPT-4V over those from all other models and even the original captions written by authors. Despite this achievement, we acknowledge that while an important milestone has been reached in caption generation, challenges such as improving quality, evaluation, and customization remain unresolved."}, {"title": "Limitations", "content": "This work has several limitations. First, our human evaluation involved only a small number of experts-three professional editors. While this scale is not uncommon in NLP research due to the high cost of expert evaluation, it may not sufficiently capture the diverse perspectives and interpretations of a broader group of users or experts. Second, the comparison made in Section 5 between our results and those from Hsu et al. (2023) was not fully direct due to differences in data and models, which we made clear in the paper. While we believe the comparison is valid at a high level, it may not be broadly generalizable. Third, our analysis spans all domains present on arXiv, but arXiv does not encompass all potential academic domains, such as biology and medicine, which are primarily published on PubMed. The characteristics of figures and captions in these areas may differ significantly from those in arXiv, potentially limiting the applicability of our conclusions outside of arXiv without additional verification. Finally, our best-performing model, GPT-4V, is proprietary, with no public access to its architecture or training data. This restriction, common in NLP research, means our findings may inherit limitations related to the closed nature of the model, including potential data contamination issues, and lacks transparency for in-depth analysis."}, {"title": "Ethics Statement", "content": "We recognize that employing LLMs or LMMs to produce texts for end users inherently carries risks, including the dissemination of inaccurate or misleading information. In scholarly contexts, particularly when using generated captions to enhance figure comprehension, such inaccuracies could mislead readers. We have ensured to inform participants of these risks during our user studies."}]}