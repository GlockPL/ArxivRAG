{"title": "Speed and Conversational Large Language Models (LLMs): Not All Is About Tokens per Second", "authors": ["Javier Conde", "Miguel Gonz\u00e1lez", "Pedro Reviriego", "Zhen Gao", "Shanshan Liu", "Fabrizio Lombardi"], "abstract": "The speed of open-weights Large Language Models and its dependency on the task at hand, when run on GPUs is studied to present a comparative analysis of the speed of the most popular open LLMs. The results suggest that existing token-based speed metrics do not necessarily correlate with the time needed to complete different tasks.", "sections": [{"title": "THE OPEN-WEIGTHS LLM ECOSYSTEM", "content": "The availability of powerful open LLMs that can be modified, integrated with other applications and run locally, has spurred an ecosystem with hundreds of LLMs of different sizes, parameter formats, languages supported, and customization for a given task. These LLMs are readily available, and several tools and libraries have been developed to ease the execution of the models.\n\nA common feature of most open-weights LLMs is that they can be run on a single computing unit, typically a GPU. This enables their use on commodity hardware such as computers with a GPU, or on single GPU instances, on the cloud. The main limitation to run on a single unit is the memory, low-end GPUs are typically equipped with a few GB of memory while high-end GPUs have tens of GB of memory. Each parameter of a LLM requires 2-4 bytes of memory when using traditional formats such as half and single precision floating-point representations. Therefore, even when using half precision floating point, a 7 billion parameter model such as Mistral 7B requires approximately 14 GB of memory. This makes running larger models such as LlaMa-70B rather challenging, because 140 GB of memory is required, a memory size that is not available even in high-end GPUs. To address this issue, the open-source community has proposed alternative formats for the parameters that require fewer bits, typically 8 or 4 and even close to 1 bit. These formats enable for example running LlaMa-70B with 4 bits per parameter on a GPU that has only 40 GB of memory.\nTo evaluate the speed of executing LLMs, the first step is to select a subset of models. Evaluating all open models is unfeasible and most of them are fine-tuned versions of other models, so we expect them to have similar speed as the base model. In our investigation we focus on five models from four companies with similar sizes, around 7 billion parameters. In this case we can use the same format, 16-bit floating-point, for all of them so that comparisons are fair. The first two are LLaMa-2 and LLaMa-3 models from Meta with sizes 7B and 8B; a model from Mistral with 7B, another from 01.Al with 6B, and the last one from Google with 7B parameters (the models were taken from Huggingface, their exact names are \"Llama-2-7b-chat-hf\", \"Meta-Llama-3-8B-Instruct\", \"Mistral-7B-Instruct-v0.1\", \"Yi-6B-Chat\" and \"gemma-7b-it\", respectively). The models selected for evaluation are summarized in Table 1. This selection covers a wide range of models of a similar size that can be run on a single GPU; they are models that have been widely used as base to derive fine-tuned models. Therefore, the results obtained can be to some extent extrapolated to the derivative models."}, {"title": "EVALUATING LLM SPEED", "content": "The speed of LLMs is typically measured by the number of tokens generated per second, or the time needed to generate a given number of tokens, for example 2568. These metrics are easy to compute but they have some important limitations when we want to compare LLMs. The first limitation is that for two LLMs that have different tokenizers, it is not a fair comparison because the number of tokens for the same text are different. A second limitation is that even if two LLMs use the same tokenizer, then they may produce a different number of output tokens for the same task and thus, measuring the tokens per second does not fully capture the speed seen by the user. These limitations boil down to the fact that as users, we want to measure the speed of the model when performing a task, not the number of tokens generated."}, {"title": "CONCLUSION", "content": "As LLMs are widely used, their speed and energy dissipation have become a key issue. Existing benchmarks for assessing the speed of LLMs focus on the time needed to generate several tokens, or the number of tokens generated per second. However, the critical figures of merit are the time and energy needed to complete a given task. In this paper, we have studied the performance of several widely used open-weights LLMs when performing three simple tasks. The results show that the time needed to complete each task does not necessarily correlate to the number of tokens that the LLM can generate per second. This is due to the difference in the tokenizers, but more importantly on the lengths of the texts generated by each model for the same task. It also shows the complexity of evaluating LLM speed in realistic applications. To address this challenge, an alternative approach is to develop task-oriented benchmarks that are representative of LLM used cases. Such benchmarks could be more informative as to the relative speed of LLMs when performing a given task."}, {"title": "ACKNOWLEDGMENTS", "content": "This work was supported by the Agencia Estatal de Investigaci\u00f3n (AEI) (doi:10.13039/501100011033) under Grant FUN4DATE (PID2022-136684OB-C22), by the European Commission through the Chips Act Joint Undertaking project SMARTY (Grant no. 101140087) and by NVIDIA with a donation of GPUs."}]}