{"title": "MIXING IT UP: THE COCKTAIL EFFECT OF MULTI-TASK FINE-TUNING ON LLM PERFORMANCE - A CASE STUDY IN FINANCE", "authors": ["Gil Shenderovitz", "Meni Brief", "Oded Ovadia", "Noga Ben Yoash", "Rachel Lemberg", "Eitam Sheetrit"], "abstract": "The application of large language models (LLMs) in domain-specific contexts, including finance, has expanded rapidly. Domain-specific LLMs are typically evaluated based on their performance in various downstream tasks relevant to the domain. In this work, we present a detailed analysis of fine-tuning LLMs for such tasks. Somewhat counterintuitively, we find that in domain-specific cases, fine-tuning exclusively on the target task is not always the most effective strategy. Instead, multi-task fine-tuning - where models are trained on a cocktail of related tasks - can significantly enhance performance. We demonstrate how this approach enables a small model, such as Phi-3-Mini, to achieve state-of-the-art results, even surpassing the much larger GPT-4-0 model on financial benchmarks. Our study involves a large-scale experiment, training over 200 models using several widely adopted LLMs as baselines, and empirically confirms the benefits of multi-task fine-tuning. Additionally, we explore the use of general instruction data as a form of regularization, suggesting that it helps minimize performance degradation. We also investigate the inclusion of mathematical data, finding improvements in numerical reasoning that transfer effectively to financial tasks. Finally, we note that while fine-tuning for downstream tasks leads to targeted improvements in task performance, it does not necessarily result in broader gains in domain knowledge or complex domain reasoning abilities.", "sections": [{"title": "INTRODUCTION", "content": "Recently, the application of large language models (LLMs) in domain-specific contexts has seen rapid growth, particularly in fields such as medicine (Singhal et al., 2023; Wu et al., 2024), law (Huang et al., 2023), and finance (Cheng et al., 2023; Wu et al., 2023). As LLMs are increas-ingly adopted across various domains, accurate evaluation of their domain-specific capabilities has become more necessary. While many benchmarks exist to evaluate LLM performance, they are typically designed for general purposes and not specifically for domain-specific evaluations.\nA common method for assessing LLM performance within a domain is through downstream tasks (Yang et al., 2024; Gu et al., 2021; Xie et al., 2024b). Such benchmarks emphasize well-defined, highly specific tasks that seek to reflect real-world applications within the target domain. These tasks are frequently framed as standard natural language processing (NLP) problems, such as text classification, summarization, causal reasoning, arithmetic reasoning, and more. While each test"}, {"title": "MULTI-TASK FINE-TUNING", "content": "Given a set of downstream tasks that have been selected to assess a model's capabilities in a target domain, the challenge becomes finding the optimal way to fine-tune the model across these tasks to maximize performance. In multi-task learning, the goal is to assess whether there exist synergies among the tasks, allowing for leveraging shared information to enhance individual task performance."}, {"title": "BACKGROUND", "content": "Multi-task training is not a new concept. The efficiency of this approach has been demonstrated for general domains in the past (Aribandi et al., 2021; Aghajanyan et al., 2021). More recent work has shown success with instruction tuning specifically (Wang et al., 2023b; Yue et al., 2023), as well as showing the impact of additional datasets. On the other hand, the exact interactions between tasks are still understudied, especially in the domain-specific case, and more specifically for finance. Past approaches to domain-specific adaptation, such as Cheng et al. (2023), used broader domain data, removing the ability to observe the interactions between the tasks themselves. While Wang et al. (2023a) use a task oriented approach in finance, there is no measurement on the task level, or experimentation around adding general data."}, {"title": "PROBLEM FORMULATION", "content": "Let $M$ be a pre-trained language model, and let $D = \\{D_1, D_2,..., D_n\\}$ represent a set of $n$ datasets used for fine-tuning. The set $D$ is partitioned into two subsets: domain-specific datasets $D_{domain} = \\{D_1,...,D_k\\}$, which correspond to tasks $T_1,...,T_k$, and general datasets $D_{gen} = \\{D_{k+1},..., D_n\\}$, which are not directly evaluated in the test tasks. Our goal is to determine what is the optimal combination of datasets for fine-tuning $M$ to maximize performance on a domain-specific task.\nThe task-level objective for multi-task fine-tuning can be formalized as:\n$D = arg max (\u0415\u0442 (MD))$"}, {"title": "METHODOLOGY", "content": "To investigate these questions, we employ a systematic empirical approach by fine-tuning the model on different combinations of datasets. We use an incremental approach for fine-tuning the model, starting from single-dataset fine-tuning to more complex mixtures. This methodology allows us to isolate the impact of individual datasets as well as explore the interactions between datasets when fine-tuned together. All fine-tuning steps use the base model M, and a standard uniform shuffling of Di. An overview of our approach for n training datasets is shown in Fig. 2.\nBefore fine-tuning, we evaluate the 'vanilla' model in its pre-trained state. This step establishes the baseline for all further comparisons, allowing us to quantify the relative changes in performance when fine-tuning.\nAfter the initial fine-tuning stage, we use a single dataset at a time. We use this step primarily to understand the performance of standard single task finetuning. Additionally, this step enables us to identify the number of samples required from each dataset for stable convergence of the training loss (in less than three epochs).\nTo explore the interactions between datasets, we fine-tune the model on pairs of datasets. By training on two datasets simultaneously, we aim to investigate the degree of influence each dataset has on improving or impairing the model's performance on another.\nNext, to fully understand the impact each dataset has, we remove a single dataset at a time, and use all other datasets for training. This step is crucial for understanding exactly how much a specific dataset influences the overall results when added to a cocktail.\nFinally, we fine-tune the model on the entire set of datasets simultaneously, completing the study."}, {"title": "DATASETS", "content": "As part of our study we selected a variety of datasets for training and evaluation. These datasets represent central downstream NLP tasks from the financial domain, covering central benchmarks from previous works (Wu et al., 2023; Cheng et al., 2023; Wang et al., 2023a). These tasks include named entity recognition (NER), sentiment analysis, numerical reasoning, and other domain-specific challenges. The datasets are categorized into two: training and evaluation datasets. The training set includes two general datasets, as well as the training split of seven financial tasks. The evaluation set includes the test split of the seven tasks and additional datasets aimed at testing broader financial reasoning abilities. Descriptions of the datasets are below, a summary of their key properties can be found in Table 1, and an example from each dataset can be found in Appendix E."}, {"title": "CORE FINANCIAL DATASETS", "content": "The following datasets are used both for fine-tuning and for evaluation:"}, {"title": "GENERAL TRAINING DATASETS", "content": "Besides the financial datasets discussed earlier, we also use two non-financial training datasets. The rationale for incorporating the first dataset is the proven benefit of instruction tuning in gen-eral (Longpre et al., 2023). Additionally, since finance-related tasks often involve mathematical reasoning, we include mathematical training data to improve the model's performance in this area. Neither of these datasets are incorporated during evaluation. The datasets are as follows:"}, {"title": "ADDITIONAL EVALUATION DATASETS", "content": "In addition to the core datasets outlined in Section 3.1, we also use FinanceBench (Islam et al., 2023) and MMLU-Pro (Wang et al., 2024) for evaluation. The FinanceBench dataset includes pairs of real-world questions about publicly traded companies, and information extracted from financial documents for answering the questions. This dataset aims to represent real-world professional use cases. MMLU-Pro contains multiple choice questions about various domains, requiring reasoning and knowledge for answering. Each question includes 10 options, reducing the probability of guessing correctly. We use only the business and economics subsets, as they are most applicable for finance."}, {"title": "EVALUATION AND RESULTS", "content": null}, {"title": "EXPERIMENT SETUP", "content": "To verify that there were no biases in the results towards a particular model, we selected three of the currently top performing small models, namely Phi-3-Small\u00b3 (Abdin et al., 2024), Llama-3.1-8B-Instruct (Dubey et al., 2024), and Mistral-7B-Instruct-v0.35 (Jiang et al., 2023). Additionally, to further demonstrate the effectiveness of multi-task fine-tuning, we chose a top performing miniature model, Phi-3-Mini (Abdin et al., 2024). We opted for the instruct versions of each model.\nAll experiments were conducted using a single machine with 2 Nvidia H100 GPUs. All experiments were done using full fine-tuning of all weights in the model. We experimented with various learning rates, ranging from 3e-6 to 3e-5. We used three epochs for the smaller runs ((1), (2)), and two epochs for the rest. The longest single fine-tuning experiment took under three hours to run."}, {"title": "METRICS", "content": "To properly interpret our results, we aggregate the experiments and present three main metrics for each model and downstream task: single-task fine-tuning (FT), multi-task fine-tuning, and baseline scores.\nFor single-task fine-tuning, we evaluate the model on the test split of a specific task after being trained exclusively on the training split of that task. Using the notation from Section 2.2, the single-task score for the i-th dataset is defined as:\nSingle-task Score := $E_{T_i} (D_i)$\nFor multi-task fine-tuning, we consider all multi-task experiments where one of the training datasets is the relevant dataset for the target task, combined with other datasets. The multi-task score is computed as:\nMulti-task Score := max $(E_{T_i} (M_{D_i})) = E_{T_i} (M_{D})$\nThe baseline score represents the performance of the pre-trained model on the test split of the down-stream task, without any fine-tuning. It is defined as:\nBaseline Score := $E_{T_i} (M)$\nNumerical Evaluations: FinQA and ConvFinQA require evaluating numerical exact match (EM) for scoring. To prevent issues stemming from rounding errors, or scale representations, we used a heuristic relaxation of exact match. We say that x is numerically same to y if for some small $\u20ac$, $y \u00b1 e = x^n$, n \u2208 {10^{-6},10^{-3}, 10^{-2}, 100, 102, 103, 106}. While not exhaustive, these are very common scales in finance (millions vs thousands vs billions, dollars vs cents, basis points, etc.).\nClassification: To evaluate classification tasks we use standard accuracy scores.\nOpen-End Evaluation: Unlike the other datasets, FinanceBench contains open-end question. To properly score model responses, we used LLM-as-a-Judge (Zheng et al., 2023) for evaluation. Specifically, we used GPT-4-o as the LLM, and use the prompt in Appendix A. We consider only a strict match as correct (i.e. a score of 2), and normalize by dividing by two."}, {"title": "MAIN RESULTS", "content": "The Cocktail Effect: In Table 2, we present a comparison for the three LLMs using the metrics discussed above. A visualization of these results is provided in Fig. 3. It is clear that fine-tuning,"}, {"title": "Domain Generalization", "content": "With the exception of Llama on FinQA, all the downstream tasks improve significantly with multi-task finetuning, across all models. Table 3 shows that this trend does not necessarily implicate that the models have improved in the general finance domain. While there may be some improvement in FinanceBench, there is no clear improvement in the other two tasks, and possibly even a regression. This finding raises a strong concern regarding the use of these downstream tasks, or many of the other commonly used benchmarks, as proxies for successful domain adaptation."}, {"title": "Data Regularization Hypothesis", "content": "We provide a further analysis of the data by examining the effect of the two non-financial datasets: Open-Orca and Orca-Math. In Fig. 4 we present a summary of all fine-tuning experiments. We compute the average score of each fine-tuned model across the seven core tasks described in Section 3.1. For visualization purposes, we normalize the results for each model separately to be between 0.15 and 0.85. There is a clear distinction between models that used the non-financial datasets, and models that relied purely on the downstream tasks."}, {"title": "RELATED WORK", "content": "Domain-specific LLMs: Recent advances in LLMs have led to many attempts at creating mod-els tailored to specific domains. These models aim to outperform general-purpose ones by having deeper knowledge of the domain, being more effective at solving tasks relevant to that domain, or adopting a more appropriate style. Several methods have been suggested for training these models. One approach is to pre-train a language model entirely on domain-specific data, as seen in (Wu et al., 2023; Singhal et al., 2023). Another common approach is to take pre-trained LLMs and fine-tune them for specific downstream tasks (Xie et al., 2023; Wang et al., 2023a; Cheng et al., 2024; Jiang et al., 2024; Cheng et al., 2023) in a domain adaptation process.\nDomain Adaptation of LLMs: Various techniques have been developed to transform a general language model into a domain-specific one. One option is continual pre-training (CPT) (Gururangan et al., 2020), where a pre-trained LLM undergoes further training on raw data that contains relevant domain-specific knowledge, enhancing the model's understanding of that domain. Another method involves supervised fine-tuning (SFT), where the model is trained on a large set of domain-specific instructions (Wei et al., 2021). Some approaches focus on specific tasks within the domain, fine-tuning the model with instruction datasets tailored to those particular tasks (Wang et al., 2023a). Additionally, a hybrid approach has been proposed, where CPT is performed first, followed by domain-specific instruction tuning to refine the model's capabilities (Bhatia et al., 2024; Wu et al., 2024; Xie et al., 2024b).\nFinance Benchmarks: With the increasing adoption of LLMs, several benchmarks have been proposed to evaluate model performance in the financial domain. Recently, efforts have been made to combine existing tests and datasets into more comprehensive evaluation frameworks. For instance, FinBen (Xie et al., 2024a), PIXIU (Xie et al., 2024b), and BBT-Fin (Lu et al., 2023) aggregate a variety of common tasks to provide a broad analysis of general financial skills. Other benchmarks focus on more specialized scenarios. For example, FinEval (Zhang et al., 2023) was developed to assess LLM financial knowledge based on academic textbooks, while SuperCLUE-Fin (Xu et al., 2024) aims to replicate real-world financial tasks through a detailed breakdown of subtasks. Another example is FinDABench (Liu et al., 2024), which places a strong emphasis on financial analysis and reasoning rather than pure knowledge evaluation."}, {"title": "CONCLUSIONS", "content": "In this work, we demonstrated the potential of multi-task fine-tuning as a robust approach to optimiz-ing the performance of LLMs on downstream tasks. Through extensive experimentation involving over 200 models, we showed that combining training data from multiple related financial tasks cre-ates a \"cocktail effect\", yielding significant performance gains, and even allowing smaller models such as Phi-3-Mini to surpass larger counterparts like GPT-4-0 on targeted benchmarks. Our findings highlight the advantages of a training approach that leverages synergies between tasks.\nFurthermore, our exploration of integrating general instruction-following and mathematical datasets demonstrated promising results, combining what may be a regularization effect, with an enhance-ment of numerical reasoning abilities. Nevertheless, we observed that while multi-task fine-tuning significantly boosts specific task performance, it does not necessarily translate into improved over-all domain knowledge. This suggests that while multi-task fine-tuning is effective for task-specific improvements, broader gains in domain competency may require more sophisticated strategies.\nOverall, our results provide strong empirical evidence for the benefits of multi-task fine-tuning in domain-specific model adaptation. This approach not only optimizes task performance but also underscores the importance of thoughtful dataset selection and the value of leveraging cross-task learning. Future work may benefit from exploring hybrid approaches that combine multi-task learn-ing with targeted domain adaptation, aiming to bridge the gap between task-specific proficiency and more comprehensive domain understanding."}, {"title": "LLM AS A JUDGE PROMPT", "content": "We used the following prompt:\n<Instruction >\nPlease act as an impartial judge and evaluate the quality of the response provided\nby an Al assistant to the user question displayed below. You will be given a ref-erence answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mis-takes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 0 to 2 by strictly following this format: [[rating]],\nfor example: The rating is: [[1]], or: My rating is [[0]].\nNote! The answers have to answer the question correctly, but they do not have to be identical, or equally detailed, or equally helpful! You are only measuring equality of correctness, not completeness. Be forgiving of rounding errors, as long as they are not essential, as well as over/under explaining.\nYou should provide a 0 rating when the answers does not match the reference.\nYou should provide a 1 rating when the answer is partially correct.\nYou should provide a 2 rating when the answer is correct.\nFor example, if the reference answer is \"It cost $5B annually\" and the assistant\nanswer is \"It cost $5 billion per year\", the rating should be 2.\nIf the assistant answer is \"It cost $5\", the rating should be 1.\nIf the assistant answer is \"It cost $4 million per month\", the rating should be 0.\nFor example, if the reference answer is a list of most major locations on Earth and the assistant replies concisely 'Globally', the rating should be 2.\nIf the assistant replies 'A variety of places worldwide', the rating should be 1.\nIf the assistant replies 'In Europe', the rating should be 0.\nFor example, if the question is \"What was his salary?\" and the reference answer is \"We can see that by adding the various components in table 3, we get that\n3K + 7.5K equals a total salary of 10.5K annually\", and the assistant's answer is\n\"10,500\", the rating should be 2.\nIf the assistant's answer is \"10.5K. This salary reflects and excellent compensation\ngiven the low cost of living in the area\", the rating should still be 2.\nIf the assistant's answer is \"the answer can be found in table 3 by adding 3K +7.5K\", the rating should be 1.\nIf the assistant's answer is \"7.5K\", the rating should be 0.\n</Instruction >\n<Question\n{question}\n</Question >\n<Reference Answer >\n{ref_answer}\n</Reference Answer >\n<Assistant's Answer >\n{answer}\n</Assistant's Answer >"}, {"title": "DATASET EXAMPLES", "content": "DATASET: HEADLINE\nInstruction:\nAssess if the news headline touches on price in the past. Options: Yes, No\nInput:\napril gold down 20 cents to settle at $1,116.10/oz.\nNo\nDATASET: FPB\nInstruction:\nYou are given a financial document. Your task is to infer its sentiment. Answer using one of thefollowing labels: ['Negative', 'Neutral', 'Positive'], and include nothing else. You must answer witha single word, and no additional context.\nInput:\nUnder the terms of the agreement, Bunge will acquire Raisio's Keiju, Makuisa and Pyszny Duetbrands and manufacturing plants in Finland and Poland.\nOutput:\nneutral\nDATASET: FINNERCLS\nInstruction:\nWhat is the entity type of '40 William St' in the input sentence. Options: person, location, organization\nInput:\nThis LOAN AND SECURITY AGREEMENT dated January 27, 1999, between SILICON VALLEYBANK (\"Bank\"), a California-chartered bank with its principal place of business at 3003 TasmanDrive, Santa Clara, California 95054 with a loan production office located at 40 William St., Ste.location\nDATASET: FINQA\nInstruction:\nPlease answer the given financial question based on the context.\nInput:\nInterest rate to a variable interest rate based on the three-month LIBOR plus 2.05% (2.34% as of"}]}