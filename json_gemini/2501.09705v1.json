{"title": "Practical Continual Forgetting for Pre-trained Vision Models", "authors": ["Hongbo Zhao", "Fei Zhu", "Bolin Ni", "Feng Zhu", "Gaofeng Meng", "Zhaoxiang Zhang"], "abstract": "For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios, erasure requests originate at any time from both users and model owners, and these requests usually form a sequence. Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify three key challenges. (i) For unwanted knowledge, efficient and effective deleting is crucial. (ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal. (iii) In real-world scenarios, the training samples may be scarce or partially missing during the process of forgetting. To address them, we first propose Group Sparse LoRA (GS-LORA). Specifically, towards (i), we introduce LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others. To further extend GS-LORA to more practical scenarios, we incorporate prototype information as additional supervision and introduce a more practical approach, GS-LoRA++. For each forgotten class, we move the logits away from its original prototype. For the remaining classes, we pull the logits closer to their respective prototypes. We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that our method manages to forget specific classes with minimal impact on other classes.", "sections": [{"title": "INTRODUCTION", "content": "As pre-trained models become larger nowadays, more training data are required. These data are usually collected through various ways such as the Internet, books, publicly available datasets, and manual labeling. Within the vast amount of data, there is often erroneous or privacy-sensitive information and pre-trained models may learn from it. For instance, the ImageNet Roulette project [1], [2] shows models tend to be biased toward racist, misogynistic, and cruel etc. Furthermore, with increased public awareness of privacy protection and updated privacy regulations [3], [4], individuals are now demanding the removal of any privacy-related information immediately. Therefore, practical model erasing techniques are required upon receiving a deletion request. In real-world scenarios, these requests usually originate at any time from both users and model owners and naturally form a sequence. Under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We identify this novel problem as continual forgetting. This task holds significance in upholding privacy and reducing unwanted model bias, such as gender and racial discrimination, in real-world applications.\nA related research topic is machine unlearning, which refers to the process of removing or erasing knowledge or patterns that a model has learned during training. Prior attempts mostly focused on typical machine learning algorithms [6]\u2013[9], e.g., linear/logistic regression [6], [9], and thus have a limited scope of application. Recent studies that explored unlearning techniques for deep learning models are either computationally heavy and only effective on small-scale problems [10]\u2013[12], or require specific designs in the pre-training process [7], [13], which are impractical. These approaches lack the ability to proceed with numerous everyday requests and thus are not capable of practical continual forgetting. In this work, we identify three key requirements in the design of practical continual forgetting algorithms. (i) Efficient and effective deleting for unwanted knowledge is crucial. Especially for continual forgetting scenarios, lightweight and fast modifications are more important to achieve deleting information promptly. (ii) Should have minimal impact on remaining knowledge, i.e., catastrophic forgetting should be mitigated. (iii) Being robust in practical scenarios where the forgotten and remaining data may be rare (few-shot) or some remaining data is missing.\nTo this end, we first propose Group Sparse LoRA (GS-LORA). Specifically, to facilitate the forgetting of unwanted knowledge, we propose modifying the FFN modules within Transformer [14] blocks, following Geva et al. [15], who found that the Feed-Forward Network (FFN) modules in Transformer [14] blocks store substantial amounts of knowledge. On the other hand, to realize efficient forgetting, we introduce Low-Rank Adaptation (LoRA) [16] to fine-tune the FFN modules inspired by parameter-efficient fine-tuning techniques [16]\u2013[18]. To mitigate catastrophic forgetting on remaining knowledge [19], we propose a group sparse regularizer to enforce a sparse and accurate modification of FFN modules, as fine-tuning fewer parameters has been shown to be effective [20]\u2013[23] in alleviating catastrophic forgetting. This is akin to conducting minimally invasive surgery on a model instead of a major surgery.\nTo unleash the full potential of GS-LoRA, we further introduce prototype information and propose GS-LoRA++. Specifically, GS-LoRA++ is designed to address more realistic and practical forgetting scenarios, where the samples for forgetting are rare due to the difficulty in acquiring the forgotten data. This makes the forgetting process more challenging and the model may not be able to fully forget the selected information. Adding prototype guidance can effectively mitigate this issue and lead to satisfactory performance. For each forgotten class, we shift the logits away from the original prototype, while for the remaining classes, we pull the logits closer to their respective prototypes."}, {"title": "RELATED WORK", "content": "In this section, we provide a brief introduction to the related literature, including continual learning, machine unlearning, parameter-efficient fine-tuning and few-shot learning."}, {"title": "Continual Learning", "content": "Continual learning aims to enable models to acquire new knowledge without forgetting previously learned information [19]. It is a learning paradigm that is particularly applicable in dynamic and changing scenarios. Researchers have mainly designed four strategies to achieve this goal, including rehearsal-based methods [25]\u2013[33], regularization-based methods [19], [34]\u2013[38], structure-based methods [20], [23], [39]\u2013[43] and prompt-based methods [22], [44]\u2013[47]. Rehearsal-based methods mitigate catastrophic forgetting by directly preserving samples from previous tasks [26], [28], [48] or training an additional generative model to mimic past data [25], [30]. Regularization-based methods incorporate a regularization loss during the learning process of new tasks in order to limit the modification of important weights [19], [34]\u2013[36], [49] or employ the previous model as a teacher to supervise the current one [26], [50], [51]. Structure-based methods either freeze specific architectures [20], [21] to mitigate oblivion, or introduce newly designed structure growth modules [23], [36], [39] to address new tasks. Prompt-base methods [22], [44]\u2013[47] use prompts to manage task-invariant and task-specific knowledge while maintaining model plasticity explicitly. These strategies for continual learning are frequently combined together to improve performance [26], [28], [52]\u2013[54].\nOur proposed method falls into the category of structure-based methods. However, our problem differs from continual learning as we aim to continuously delete, rather than add new knowledge to the model."}, {"title": "Machine Unlearning", "content": "Machine unlearning involves retraining or modifying machine learning models to diminish or eradicate the influence of previously acquired patterns or biases, aiming to enhance the models\u2019 fairness and safety [7], [55]\u2013[59]. A lot of studies design unlearning algorithms on typical machine learning algorithms [6], [8], [9], [58], [60], [61], like SVM [8], linear model [6], random forests [61], etc. As a result, the applicability of these algorithms is constrained. Initial work on forgetting in deep learning either slices the data and trains a series of submodels to isolate the effect of specific data points on the model [7], [13], [57] (exact unlearning) or calculates influence functions to approximate the impact of a data item on the parameters of models [10]\u2013[12], [62]\u2013[66] (approximate unlearning). However, these methods deteriorate when applied to larger datasets and models, and the computational cost is exceedingly high.\nOur problem focuses on the practical continual forgetting of a pre-trained model. One previous work [57] studies the continual exact unlearning by adding a class-specific synthetic signal in the pre-training stage. It should be noted that specific designs should not be performed in the pre-training process, which is not common in deep learning applications. Practical continual forgetting requires algorithms to continually forget for any on-the-shelf pre-trained models."}, {"title": "Parameter-Efficient Fine-Tuning", "content": "Training large models by self-supervised learning and then fine-tuning them on downstream tasks has become a new paradigm of deep learning [67]\u2013[75]. Parameter-efficient fine-tuning (PEFT) techniques [16]\u2013[18], [76]\u2013[79] are proposed to optimize a limited number of parameters, as fully fine-tuning increasing large models [67], [68], [80], [81] becomes less practical for various downstream tasks.\nRecent studies focus on three different types of PEFT methods, categorized based on the origin of trainable parameters. These methods include addition-based approaches [17], [18], [82], which incorporate supplementary trainable neural modules or parameters; freezing-based techniques [83], [84], which solely train a limited number of parameters from the original model; and parameter-factorization-based methods [16], [85], [86], which utilize matrix low-rank factorization to update the model. All these methods are designed to improve the performance of downstream tasks, while our method modifies pre-trained models with the help of PEFT."}, {"title": "Few-shot Learning", "content": "Few-shot learning [87]\u2013[94] aims to let a model learn to make accurate predictions by training on a very small number of labeled examples. To achieve this goal, researchers have designed generative model-based approaches [94]\u2013[99] and discriminative model-based approaches [89], [90], [100]\u2013[103]. Generative model-based approaches usually introduce some latent variables to describe the relationship between the input and output. Most of these methods assume some specific distribution of the latent variables and use some typical machine learning methods like Bayesian estimation [94], [99], neural statistician [104], etc. Discriminative model-based methods utilize metric learning [103], [105]\u2013[107] and meta-learning strategies [87], [89], [90], [101], [108], [109]. Metric learning methods create a continuous representation for a given data sample, such as a vector embedding. They make inferences by learning a function that evaluates a distance metric, quantifying the similarity between this representation and the comparison of other samples or classes. Meta-learning methods use prototypical network [90], [101], matching network [110] or relation network [89], etc. to learn the ability to transfer across different tasks. Another alternative solution to the problem is to generate or augment additional training samples [102], [111]\u2013[114]. These methods can be combined with generative model-based methods or discriminative model-based methods to improve the performance further."}, {"title": "PROBLEM FORMULATION", "content": ""}, {"title": "Single-step Forgetting", "content": "We propose a new problem termed continual forgetting, which involves the selective removal of specific knowledge from a pre-trained model while preserving the performance of the rest. In this subsection, we first consider the simplest situation where there is only one task that needs to be forgotten, and extend it to a continual form in Sec. 3.2.\nLet M be a model pre-trained on the dataset D, we denote the mapping relationship of the model as $f_M : X_D \\rightarrow Y_D$, where $X_D$ and $Y_D$ represent the input set and output set, respectively. Our objective is to selectively discard certain knowledge in the model while retaining the rest. Let $D_f$ and $D_r$ represent datasets containing knowledge to be forgotten and retained. Given that $|D_r|$ is typically large in practical scenarios and the retraining process is time-consuming, we require $|D_r| + |D_f| < |D|$ for a fast editing. Before forgetting, model M performs well on both $D_f$ and $D_r$, i.e.,\n$f_M: X_{D_r} \\rightarrow Y_{D_r}, f_M: X_{D_f} \\rightarrow Y_{D_f}$\nThe forgetting algorithm F modifies the model to obtain $M' = F(M, D_f, D_r)$ and a new mapping relationship $f_{M'}$ satisfying\n$f_{M'}: X_{D_r} \\rightarrow Y_{D_r}, f_{M'} \\nrightarrow X_{D_f} \\rightarrow Y_{D_f}$,\nHere, $f_{M'} \\nrightarrow$ means the mapping relationship no longer holds."}, {"title": "Continual Forgetting", "content": "Now, we extend the problem to a continual paradigm where the model is required to sequentially forget specific knowledge. Let $D_r = \\{D_{r_i}\\}$ and $D_f = \\{D_{f_i}\\}$ for $i = 1,2,...,T$ represent two sequences of datasets, where T is the number of forgotten tasks, $D_{f_i/r_i} = \\{(x_{f/r}^i, y_{f/r}^i)_{n_{f/r}}^i\\}$ is the forgotten or retained dataset of the i-th task, $x_{f/r}^i \\in X_{f/r_i}$ is an input and $y_{f/r}^i \\in Y_{f/r_i}$ is the corresponding label. The forgetting algorithm F handles erase requests sequentially, starting from M, and generates a sequence of models $M_{f_1}, M_{f_2},\u2026, M_{f_i},\u2026\u2026, M_{f_T}$, where $M_{f_i}$ represents the modified model after the i-th forgetting task. After processing task $T_i$, model $M_{f_i}$ performs poorly on $D_{f_i}$ but maintains the original performance in the remaining part, i.e., the corresponding mapping relationship $f_{M_i}$ holds\n$f_{M_i} \\nrightarrow X_{D_{f_i}} \\rightarrow Y_{D_{f_i}}, f_{M_i}: X_{D_{r_i}} \\rightarrow Y_{D_{r_i}}$\nwhere i = 1,2,..., t."}, {"title": "Practical Forgetting", "content": "In the real-world scenario, the forgotten and retained data may contain few samples or be partially missing because of the storage. Therefore, we consider two more practical and difficult settings from the data level, i.e., few-shot scenarios and missing class scenarios. For simplicity, we only present the formulation in single-step forgetting as the task ID does not affect the structure of the formulation."}, {"title": "Few-shot scenario.", "content": "In Sec. 3.1 and Sec. 3.2, we require $|D_r|+|D_f| < |D|$. Furthermore, we request a stricter setting where each class in $D_f$ and $D_r$ has a limited number K, e.g., 4 of samples, i.e.,\n$D_{f/r} = \\{(x_{f/r}^i, y_{f/r}^i)\\}_{i=1}^K$."}, {"title": "Missing class scenario.", "content": "In this setting, some classes in the remaining dataset have no training samples, i.e.,\n$D_f = \\{(x_i, y_i)\\}_{i=1}^{n_f}$\n$D_{ra} = \\{(x_i, y_i)\\}_{i=1}^{n_{ra}}, y \\in C_{available}$,\n$D_{rm} = \\empty$,\n$y \\in C_{missing}$\nwhere $D_{ra}$ is the available remaining dataset, $D_{rm}$ is the missing remaining dataset. $n_{f/r}$ are the total number of samples in forgotten/available remaining dataset. The categories $C_{available}$ and $C_{missing}$ constitute the entire categories $C_r$ of the remaining dataset $D_r$."}, {"title": "METHOD", "content": "Preliminary: LoRA. Hu et al. [16] argue that the weight matrix in the pre-trained model has a very low intrinsic rank and utilizes a low-rank decomposition to implement parameter updates. For a weight matrix $W \\in R^{d \\times k}$, it is updated following $W = W + \\Delta W = W + BA$, where $B \\in R^{d \\times r}$ and $A \\in R^{r \\times k}$ are low rank matrices and $r << min\\{d, k\\}$ is the rank of matrix B and A. Only matrices with low ranks, i.e., B and A are trainable, while the matrix W remains frozen during training. LoRA can be added to the linear projection matrices in Multi-Head Attention modules or the Feed-Forward Network (FFN) modules in Transformer [14] blocks.\nOverview. Considering three key challenges in Sec. 1 and the optimization goal in Eq. (3), we propose enhanced Group Sparse LoRA (GS-LoRA++) with selective forgetting, knowledge retention and prototype regularization to achieve practical continual forgetting."}, {"title": "Low-rank Tuning with Group Sparsity", "content": "LORA Based Model Tuning. Following the findings of Geva et al. [15], FFN layers in the Transformer blocks store a substantial amount of knowledge, necessitating modification of the FFN modules to achieve knowledge erasure. Although directly modifying these layers is theoretically feasible, it is inefficient due to the large number of parameters in the FFN layers. To reduce the learnable parameters, we incorporate a set of LORA modules to the FFN in each Transformer block and only make these LoRA modules trainable.\nSuppose x is the input of the l-th FFN module, the mathematical form can be expressed as:\nFFN$^{(l)}$ (x) = max $(0, xW_1^{(l)} + b_1^{(l)}) W_2^{(l)} + b_2^{(l)}$,\nwhere $W_1^{(l)}, W_2^{(l)}, b_1^{(l)}, b_2^{(l)}$ are the weights and biases of two fully connected layers from the pre-trained model, respectively. We use LoRA to only fine-tune the weights of FFN modules:\n$W_1^{(l)} = [W_{1_t}^{(l)} - \\sum_{i=1}^{t-1} B_1^{(l)} A_1^{(l)}]$\n$W_2^{(l)} = [W_{2_t}^{(l)} - \\sum_{i=1}^{t-1} B_2^{(l)} A_2^{(l)}]$\n$B_1^{(l)} = \\begin{bmatrix} B_{1_t}^{(l)} \\\\ O \\end{bmatrix} A_1^{(l)} = \\begin{bmatrix} A_{1_t}^{(l)} \\\\ O \\end{bmatrix}$\n$B_2^{(l)} = \\begin{bmatrix} O \\\\ B_{2_t}^{(l)} \\end{bmatrix} A_2^{(l)} = \\begin{bmatrix} O \\\\ A_{2_t}^{(l)} \\end{bmatrix}$,\nwhere $W_{1_t}^{(l)}$ and $W_{2_t}^{(l)}$ denote the weights of the l-th FFN modules after task $T_t$, and $B_1^{(l)}, A_1^{(l)}, B_2^{(l)}, A_2^{(l)}$ for i = 1,2,..., t refer to the corresponding LoRA matrices in task $T_t$. O is the zero matrix. Note that the output FFN layers are frozen to ensure forgetting occurs in the backbone and is difficult to recover. Sec. 6 provides detailed discussion.\nGroup Sparsity Selection. To mitigate catastrophic forgetting and achieve precise modifications automatically, we introduce a group sparsity selection strategy that enables the selection of fewer Transformer blocks. Although there are many ways to conduct a selection like routers [115], [116], meta learning [117], neural architecture search [118], [119], we utilize group Lasso, known for its simplicity and effectiveness in selecting parameters for specific groups [48], [120]\u2013[122] while setting others to zero. Suppose LoRA matrices added to the l-th Transformer block in task $T_t$ are $B_1^{(l)}, A_1^{(l)}, B_2^{(l)}, A_2^{(l)}$. We regard the LoRA weights in one Transformer block as a group. Therefore, the group sparse loss in the l-th group can be written as:\n$\\mathcal{L}_{gs}^l = ||B_1^{(l)}||_F + ||A_1^{(l)}||_F$.\nHere, $||\\cdot||_F$ is the Frobenius norm of the LoRA matrices and t denotes task Tt. Then, the group sparse loss on a set of weights can be represented as:\n$\\mathcal{L}_{structure} = \\sum_{l=1}^G \\mathcal{L}_{gs}^l$,\nwhere G is the number of groups, $\\mathcal{L}_{gs}^l$ is the group sparse loss of the l-th group."}, {"title": "Selective Forgetting and Knowledge Retain", "content": "Selective Forgetting. In each task $T_i$ for i = 1,2,..., T, the model needs to forget the knowledge stored in data $D_{f_i} = (X_{f_i}, Y_{f_i})$. To achieve forgetting, the optimization goal is $\\arg \\max_W \\mathcal{L} (f_{M_{t-1}}(X_{f_t}), Y_{f_t})$, where W is the parameter; $\\mathcal{L}$ is the original loss function; $f_{M_{t-1}}$ is the mapping function obtained at the end of task t \u2013 1. An intuitive idea is to perform gradient ascend, i.e., $\\mathcal{L}_{forget} = -\\mathcal{L} (f_{M_{t-1}} (X_{f_t}), Y_{f_t})$. Nevertheless, simply adding a minus sign to the original loss leads to an exploding unbounded loss that is challenging to optimize. Therefore, we employ a ReLU function to introduce a lower bound following Du et al. [123],\n$\\mathcal{L}_{forget} = ReLU (BND - \\mathcal{L} (f_{M_{t-1}} (X_{f_t}), Y_{f_t})),$\nwhere BND is a hyperparameter that determines the bound.\nKnowledge Retention. Besides forgetting selected knowledge, it is crucial for the model to maintain performance on the rest. Catastrophic forgetting on remaining classes [19] still exists. To mitigate this issue, we employ a small rehearsal buffer $D_{r_i} = (X_{r_i}, Y_{r_i})$ which satisfies $|D_{r_i}|+|D_{f_i}| < |D|$ to alleviate this undesirable forgetting and maintain efficient training. The knowledge retention loss can be written as:\n$\\mathcal{L}_{retain} = \\mathcal{L} (f_{M_{t-1}} (X_{r_t}), Y_{r_t})$.\nCombining Eqs. (10) and (11), we get the data loss\n$\\mathcal{L}_{data} = \\mathcal{L}_{retain} + \\beta \\mathcal{L}_{forget},$\nwhere $\\beta$ is a hyperparameter."}, {"title": "Prototype Regularization", "content": "In practical scenarios, the training data for selective forgetting and knowledge retain may be rare, i.e., the forgetting task is under a few-shot setting. This is due to the limited information provided by the one-hot labels. To solve this problem, we introduce prototype regularization to extract more information from the prototype. We calculate the prototype $P_c$ [37], [90] of a certain class c before forgetting as follows:\n$P_c = \\frac{1}{N_c} \\sum_{(x_i,Y_i) \\in S_c} hori (X_i),$\nwhere $N_c$ is the number of samples from class c, $S_c$ is the set of class c, $hori(\\cdot)$ denotes pre-softmax responses of the pre-trained model, i.e., logits."}, {"title": "Final Learning Objective", "content": "The final optimization goal can be expressed as follows:\n$\\mathcal{L}_{total} = \\mathcal{L}_{data} + \\mathcal{L}_{pro} + \\alpha \\mathcal{L}_{strcuture}$"}, {"title": "DISCUSSION", "content": "Real Forgetting or Deceptive Forgetting? When we want to forget some specific classes, the naive solution is to mask their output FFN weights directly, which we refer to as \"head forgetting\" for simplicity. However, this trivial solution is deceptive forgetting and easy to be recovered. It\u2019s like a kid who knows the answer and deliberately does not say it. Real forgetting should occur at backbone and is difficult to be recovered."}]}