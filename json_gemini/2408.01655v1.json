{"title": "Stimulating Imagination: Towards General-purpose Object Rearrangement", "authors": ["Jianyang Wu", "Jie Gu", "Xiaokang Ma", "Chu Tang", "Jingmin Chen"], "abstract": "General-purpose object placement is a fundamental capability of an intelligent generalist robot, i.e., being capable of rearranging objects following human instructions even in novel environments. To achieve this, we break the rearrangement down into three parts, including object localization, goal imagination and robot control, and propose a framework named SPORT. SPORT leverages pre-trained large vision models for broad semantic reasoning about objects, and learns a diffusion-based 3D pose estimator to ensure physically-realistic results. Only object types (to be moved or reference) are communicated between these two parts, which brings two benefits. One is that we can fully leverage the powerful ability of open-set object localization and recognition since no specific fine-tuning is needed for robotic scenarios. Furthermore, the diffusion-based estimator only need to \"imagine\" the poses of the moving and reference objects after the placement, while no necessity for their semantic information. Thus the training burden is greatly reduced and no massive training is required. The training data for goal pose estimation is collected in simulation and annotated with GPT-4. A set of simulation and real-world experiments demonstrate the potential of our approach to accomplish general-purpose object rearrangement, placing various objects following precise instructions.", "sections": [{"title": "1 Introduction", "content": "General-purpose object placement is a fundamental capability of an intelligent generalist robot. Much like humans, the robot must be capable of reasoning and recognizing target objects (even though it has never encountered before), and then constructing the rearrangement following human instructions. For example, if an instruction \"put the spicy potato chips on the plate\u201d is given, the ability of semantic understanding is required, i.e., reasoning about \"spicy potato chips\u201d even this phrase may be outside of the training distribution. Furthermore, the rearrangement should be physically-realistic by fully considering the physical structures, geometries and constraints.\nThe great progresses of generative models provide researchers an insight of solving this challenging problem. Some of them introduce powerful models pre-trained on vision [3, 2, 7, 40], for initializing robotic policies or enhancing semantic understanding. Despite benefiting from the large-scale pre-training, it remains doubts about the generalization ability, since the amount of robotic manipulation fine-tuning data are far less than that encountered in a person's experience (also less than pre-training). Another bottleneck is that these approaches do not look specifically at 3D spatial understanding. They assume that the underlying states of world can be characterized by images from certain angles."}, {"title": "2 Related work", "content": "2D pose estimation. Traditionally, the object rearrangement task is divided into object recognition and pose estimation tasks. The advancement of vision transformers has greatly improved object recognition performance, making robots more semantically aware. Neural networks are also used to generate object poses from 2D images and relational predicates as inputs [27, 39, 26, 41, 25]. However, two challenges remain: first, the set of relational predicates is limited; second, these methods struggle to handle collisions in 2D settings.\n3D scene generation. Vision generation models have endowed AI with visual imagination capabilities. Some research has attempted to apply diffusion models to arrangement tasks, thereby generalizing placement instructions. For instance, DALL-E-BOT [17] generates a goal image from text and matches it with an observed image to determine new object positions. However, it uses diffusion models directly, resulting in generated images that often differ from observed ones. DreamReal [16] employs a sampling strategy to sample candidate object positions in 3D space and uses Vision Language Models to score each candidate. Working in 3D space can make the generated scenes more physically realistic, avoiding collisions or placing objects in mid-air. StructFormer [24] and StructDiffusion [23] take a more direct approach, using transformers and diffusion models to edit observed 3D point clouds, enabling the execution of abstract instructions such as \"set the dining table.\" However, they exhibit weak referential capabilities for objects.\nImitation Learning. Another research direction involves generating robotic actions directly. For example, Transporter [42] uses ResNet to generate robotic actions instead of object poses, while CLIP-PORT [34] combines CLIP [30] and Transporter to enhance object recognition capabilities. With the development of Multimodal Large Models, some works address robotic manipulation problems more end-to-end. Examples include Diffusion Policy [8], VIMA [15], and RT2 [3], which attempt to solve general robotic manipulation tasks by using language prompts and visual observations to generate visuo-motor actions directly. However, these imitation approaches usually require large amounts of robotic tele-operation or simulation data to achieve generalization capabilities."}, {"title": "3 Semantic-aware and physically-realistic object rearrangement", "content": "We introduce SPORT, a Semantic-aware and Physically-realistic Object RerrangemenT method. The pipeline of SPORT is illustrated in Figure 2."}, {"title": "3.1 Preliminaries and problem formulation", "content": "Given a single view of a scene captured by RGB-D sensors, we wish for the robot to rearrange this scene to satisfy the natural language instruction. Let $I_{rgb}$ and $I_d$ be the captured RGB and depth images, respectively. Denote the $N$ objects in the scene as $O = \\{o_1, o_2, ..., o_N \\}$, and the language instruction as $W$. Typically, the robot would be required to treat some object as the reference, and then move another one to a certain position relative to the reference."}, {"title": "3.2 SPORT for object rearrangement", "content": "Our key insight is that decoupling the components of rearrangement helps improve generalization. First, being capable of reasoning about novel objects and scenarios is necessary, even they may not be present in the robotic training data. A large language model (LLM) enhanced segment anything model (trained on Internet-scale data) is introduced to enable this. Specifically, the object types $T = \\{T_1, T_2,...,T_N\\}$ are inferred from $W$. Without loss of generality, we use subscripts $r$ and $m$ to represent the reference and moving objects, i.e., $O_r$ and $O_m$ respectively. The remaining objects, namely $O \\backslash \\{O_r, O_m\\}$, are irrelevant ones for the given command. The vision segmentation model takes $I_{rgb}$ and object descriptions as inputs and output segmentation masks of objects, denoted by $M = \\{M_1, M_2, ..., M_N \\}$. According to the $M$ and $I_d$, the partial-view point clouds of objects can be derived, denoted by $P = \\{P_1, P_2, ..., P_N \\}$.\nThen the robot should act like a human that can \u201cimagine\" the 3D goal poses of objects after the rearrangement. A diffusion-based model is developed to achieve this, which takes $P$, $W$ and $T$ as inputs and estimate the pose $x_m$ (only the object to be moved requires the pose estimation). One should note that the diffusion-based model only accesses to the object types (which object requires to be moved and which object is the reference) while without knowing what they are. \"Moving a to the left of b\" and \"moving c to the left of d\" are nearly equivalent to the pose estimation in our setting, because the final relative positions of objects are the same in these two commands. Though without semantic information, the physical validity of the rearrangement can still be ensured by understanding point cloud data. Benefiting from this, a well-generalized goal pose estimator can be trained without massive data. 3"}, {"title": "Instruction parsing", "content": "An LLM is utilized to understand and parse the natural-language command $W$. We still use the \"put the spicy potato chips on the plate\" as an instance. We prompt LLM, e.g., GPT [28, 5] or LLaMA [36, 37], to extract object types and corresponding descriptions: \"spicy potato chips\" to be moved and \"plate\" to be the reference, respectively."}, {"title": "Object reasoning and segmentation", "content": "An open-set segmentation model is then needed. We employ LISA [20] in this work though other similar models could also be used. LISA combines a multi-modal LLM (LLaVA [22]) with the segmentation decoder (SAM [18]), showing powerful capacity of complex semantic reasoning that requires world knowledge. For example, given an image containing two bags of potato chips, it can segment the spicy one according to the common knowledge \"spicy snacks are usually packaged in red\".\nWe do not fine-tune the vision model on robotic data. The reason is that the scale of robotic data is smaller than that of web data for pre-training large vision models [3, 40]. We can fully leverage the existing high-capacity of complex reasoning and semantic understanding, while fine-tuning may hurt the generalization. Furthermore, without the heavy work of fine-tuning, we can cost-effectively and flexibly use stronger models with the development of the community."}, {"title": "Pose estimation", "content": "We parameterize the 6-DoF pose as $(t, R) \\in SE(3)$ (Special Euclidean Group). The goal pose estimator is based on a diffusion model, which is the basis of the recent remarkable AIGC approaches [31, 44]. It consists of several modules, i.e., a general-purpose text encoder, learnable type embeddings, a point cloud encoder and a vanilla transformer [38] as the backbone. We only use certain object (namely the moving one) to train the model, since the positions of the other objects remain unchanged after the rearrangement. The underlying idea behind this is similar to inpainting.\nText encoder. We deploy BERT [10] as the text encoder along with its tokenizer, as it can understand general-purpose instructions in natural language form. Unlike previous works that may be limited to the tokens in their customized vocabulary [23], BERT is capable of broadly understanding various instructions and can well capture the information within the instructions. Though more powerful text encoder could be more helpful [32], we choose BERT as a trade-off between the need for strong semantic understanding and resource overhead.\nType embedding. A set of learnable embeddings is introduced to indicate the token types, mainly the roles of corresponding objects in the rearrangement process, i.e., $T$. Four types are considered: the"}, {"title": "Object encoder", "content": "The object representations consist of two types of features. One encodes geometric and spatial information, and the other one encodes pose information at last time-step of diffusion model. For the former, we use a vanilla Point Cloud Transformer (PCT) model [13], given segmented partial-view point clouds of objects $P$. The mean position of the original point cloud is subtracted to ensure it does not retain any original pose information. For the latter, we use a multi-layer perceptron (MLP) to encode $(t, R)$. Apart from moving objects, the poses of other objects remain consistent with their initial pose. Finally, these two types of features are concatenated.\nDiffusion. A language-conditioned diffusion model is used to estimate the goal poses of objects. At each time-step, six types of embeddings (specifically the text, type, object, position, time and an extra token containing camera viewpoint information) are combined and fed to the backbone. The model then predict the poses at the current time-step, specifically the $t \\in R^3$ and two vectors $a, b \\in R^3$ to construct the rotation matrix $R \\in SO(3)$. The position and time embeddings follow standard design, indicating the token positions in sequences and the time-step in diffusion, respectively. Padding is used to maintain a consistent number of input tokens. Note that the introduced extra token is essential, with which the model can effectively accomplish the instructions even when the camera viewpoint changes in real scenarios.\nThough all objects are included in model inputs, only the moving one gets involved in iterative pose estimations. It is because the diffusion model needs to know all object information to achieve relative positional movement and avoid collisions (thus all objects are required in the input), while only the pose of the moving object would change. Accordingly, we only add noise to the moving object pose during model training. The training objective can be formulated as\n$\\arg \\min_\\theta \\sum_{i=1}^N I_{i=m} E_{\\epsilon, t} [|| \\epsilon - \\epsilon_\\theta(x_i, t) ||_1]$,  (1)\nwhere $\\epsilon$ is sampled from a standard normal distribution, $x_i$ is the pose estimation of i-th object in $O$ at t-timestep, and $I_{i=m}$ is an indicator checking whether i-th object is the one to be moved."}, {"title": "3.3 GPT-assisted object rearrangement data generation", "content": "The available amount of public 3D object rearrangement data is limited, especially the data containing low-level rearrangement instructions. In this work, we develop an automatic pipeline for generating high-quality rearrangement-instruction pairs. Each instance comprises an initial scene, a goal scene after the rearrangement and a corresponding instruction. A total of 40,000 stable and collision-free instances are generated in the PyBullet physics simulator [9], rendered by OpenGL [1]. The simulated objects are randomly selected from the popular ShapeNetSem [6] (specifically ShapeNetSem [33]) dataset. We collect various 581 objects from 30 categories to ensure diversity.\nThe entire generation pipeline consists of three steps: (1) pre-processing metadata to obtain well-constructed and realistic simulated objects; (2) randomly selecting the reference, moving and irrelevant objects, namely $O_r$, $O_m$ and $O \\backslash \\{O_r, O_m\\}$, loading them to PyBullet to obtain the initial and goal scenes, then filtering out physically-unrealistic ones; (3) using GPT-4 to generate rearrangement language instructions corresponding to the transition from the initial scene to the goal scene, based on the object and scene information. The difficulty lies in the time-consuming and cumbersome data collection process, as well as the limited capability of precise (fine-grained) spatial understanding and reasoning in existing models, even GPT-4.\nPre-processing. The metadata in ShapeNetSem needs to be pre-processed by scaling and translation, because the object models may have unrealistic sizes and the centroids of objects may not be aligned with the origin of the point-cloud coordinate system. The scaling factors are obtained with GPT-4, e.g., asking GPT-4 about the typical size of a cellphone and accordingly scaling the object model.\nScene generation. We categorize the data generation into multiple scenarios according to the relative spatial relationship between the moving and reference objects, such as left, right, front, behind, on, between, etc. We randomly select the object set $O$ and the scenario to generate scenes. For the initial scene, we place all the objects with random positions in PyBullet, wait for them to settle into stability and record their poses. For the goal scene, we replace the reference and irrelevant objects with the recorded poses and then load $O_m$. Its pose is randomly sampled within a region determined by the $O_r$ and the scenario. For example, in the coordinate system with $O_r$ as the origin, \"left\" refers to the region $\\{(x, y) \\in R^2 | x/\\sqrt{x^2 + y^2} < -d, |y|/\\sqrt{x^2 + y^2} < \\delta \\}$, where $d$ is a hyper-parameter. The physical validity is verified in two aspects: the stability is measured by the angular and linear velocities in the physics engine, the collision is determined by checking whether the positions of $O \\backslash \\{O_m\\}$ has any displacement.\nInstruction generation. Inspired by previous works [22, 21], we use GPT4 to generate the instruction in natural language form, given spatial coordinates and object information (e.g., RGB value and size) of scenes. We observe that it is essential to provide detailed descriptions of spatial and object information, otherwise GPT may not be able to understand the spatial transition or determine whether the placement is reasonable.\nAn interesting observation is that we have tried an end-to-end instruction generation approach by directly prompting GPT4 the rendered RGB images of the initial and final scenes. But the generated language instructions are of low accuracy. We attribute this to two main reasons: (1) the absence of lifelike qualities in the simulated objects impedes GPT4's ability to recognize them accurately, (2) deducing fine-grained 3D spatial relationships from RGB images, which usually requires considering occlusion and perspective effects, is challenging for existing LLMs [43] (even the powerful GPT4). We will continue to explore this topic in future work."}, {"title": "4 Experiments", "content": "The goal of the experiments is to evaluate the efficacy of SPORT in the object rearrangement task, especially in the ability of generalization, 3D spatial reasoning and precise instruction following. To this end, we need to answer the following questions:\n1. Does SPORT excel at the task, even in a new environment, given a precise instruction, given unseen objects with various attributes, requiring physically-realistic results?\n2. Can SPORT trained with simulation data seamlessly transfer to real-world scenarios, even in zero-shot and requiring more complex reasoning?\nThe experiments are then conducted both in simulation and real-world environments. The experimental details and results are reported in the following two sections."}, {"title": "4.1 Simulation experiments", "content": "Setup To fully validate the generalization ability of SPORT, we conduct a cross-dataset evaluation. Unlike the objects in training data (from ShapeNetSem), the simulated objects for testing are collected from Google Scanned Objects [11]. A total of 77 object models from 37 novel categories are randomly selected. We use PyBullet [9] as the physics simulator and OpenGL [1] as the appearance render. For each testing sample, the involved objects are randomly sampled. The scene and the rearrangement instruction are generated following the pipeline in subsection 3.3.\nWe use the success rate as the evaluation metric as in previous works [23]. There are three aspects to consider: given a command, the model should be able to recognize the involved objects, place them to correct positions, and the rearrangement is physically-realistic. We systematically measure whether the placed positions of objects satisfy the command, with similar rules for assessing spatial relationships in subsection 3.3. For example, given the command \"put $O_m$ to the left of $O_r$\u201d, in the coordinate system with $O_r$ as the origin, the coordinates (x, y) of $O_m$ should satisfy $x/\\sqrt{x^2 + y^2} < -\\delta', |y|/\\sqrt{x^2 + y^2} < \\delta'$. As for the assessment of physical validity, we continuously place objects in simulation based on the estimated poses, checking the collision and stability. A rearrangement is considered as correct only when all these aspects are satisfactory.\nA single diffusion model is trained for all scenarios of spatial relationships. We strive to ensure that the data amount of each scenario is balanced. Adam optimizer is used with a learning rate of le-4. The batch size is set to be 256. The training is performed for 200 epochs, which takes 4 hours on a single A100 GPU. During the inference phase, we use 200 steps for the denoising process of the diffusion model.\nQuantitative evaluation\nThe results are listed in Table 1. Whether a testing sample is classified as correct depends on three aspects: the objects are correctly recognized, the generated poses of objects satisfy the instruction, and the rearrangement is physically-realistic. \u201cPose accuracy\u201d refers to the accuracy of considering the first two aspects, ", "Overall Success\" is the overall success rate considering all three aspects. Six scenarios (corresponding to six spatial relationships) are conducted for evaluation, namely left, right, front, behind, on and between.\nAs shown in Table 1, SPORT achieves an overall success rate of 46.19% on the simulation testing set. The result is acceptable due to the challenging experimental setting: the testing and training data are collected from different datasets (the objects are totally different), and the final states of objects should stable and collision-free. However, we want to explore more, especially given the observation that Physical Realism achieve higher accuracy than Pose Accuracy, which is quite unusual.\nWe notice that LISA fails a lot on images rendered in simulation. On simulation-rendered images, the completeness at the edges of the object segmentation masks produced by LISA are not sufficient. As a result, the quality of the resultant 3D point clouds is often poor. We attribute this to the substantial domain differences between the simulation images and the real-world images used in LISA's training set, resulting in limited model generalization. However, LISA can indeed localize target objects even requiring complex reasoning in real-world scenarios. We have conducted related experiments, please refer to subsection 4.2 for the details.\nAccording to the above observation, we want to know the performance of SPORT if the required objects can be successfully obtained (since LISA can achieve this on real-world images). The results are listed in the second row, by using object masks directly from simulation data. One can see that SPORT achieves convincing performance, 87.8% on Pose Accuracy and 69.49% on Overall Success.\nFinally, one can see that our approach demonstrates a certain degree of effectiveness in generating physically-realistic poses. The success rate of Physical Realism achieves 76.40%, assessed by using the Pybullet simulator. Despite the progress, this particular ability indeed needs further enhancement. Its accuracy exhibits a disparity relative to Pose Accuracy. We leave the exploration as further work.\"\n  },\n  {\n   \"title\": \"Ablation study\",\n   \"content\": \"In this part, we perform ablation studies to synthetically analyze the proposed SPORT.\nEstimating Poses of Reference and Irrelevant Objects. In our approach, the poses of the reference and irrelevant objects are fixed when training the diffusion model. We conduct an experiment to explore the impact of such a design, i.e., comparing the performances of fixing poses versus not fixing them. As shown in Table 2, fixing the poses significantly aids the model in learning relative positional relationships, effectively improving the success rate, from 65.59% to 69.49%. It helps the model identify the reference points and possible collisions from the beginning of training, focusing on the learning of replacing the target object according to the command.\nMaking the instruction encoder trainable. We train the instruction encoder in the 3D goal estimator to study its effect on the performance. The performance comparison of training versus not training the encoder is listed in Table 2. One can observe that freezing the text encoder achieves better results. This is really an interest observation, since normally end-to-end tuning is a standard procedure. We speculate that the reason is that pre-trained BERT is already good enough for text understanding, while the amount of our collected data is insufficient for training all the modules in diffusion model. Similar observations can be found in [45].\nTraining on different scales of data. To investigate the impact of different scales of training data on model performance, we design a series of comparative experiments. In these experiments, the model is trained with 10%, 25%, and 50% of the entire dataset, while the network architectures and other training configurations stay the same. The results, as presented in Table 3, indicate a clear trend: as the volume of training data increases, the model's success rate correspondingly improves, but the rate of improvement diminishes with larger data volumes. The diminishing return suggests that further increasing the training data volume yields only marginal benefits. That is, further expansion of the training dataset is unlikely to provide significant additional benefits for our task. This observation supports the conclusion that the goal estimator does not require massive data for training.\"\n  },\n  {\n   \"title\": \"4.2 Real-world experiments\",\n   \"content\": \"Setup We collect several real-world scenes, captured by an Intel RealSense D435i RGB-D camera. Each scene includes several common objects placed on a table. The objects include various fruits, food, potato chips, tableware, etc. Challenging evaluations can be conducted that require complex spatial reasoning involving multiple yet similar objects.\nCompetitors\nSuSIE [2] leverages a diffusion model to \\\"edit\\\" the image of current scene to generate the intermediate subgoal image based on instructions. A low-level policy then executes the actions to reach the subgoal. Object rearrangement can be done by alternating this loop. The key is to implement the diffusion model with InstructPix2Pix [4], a powerful image-editing model pre-trained on Internet-scale data.\nAVDC [19] uses an image diffusion model to synthesize a video of imagined execution of the rearrangement process. The underlying assumption is that the video generation model is a \u201cworld model\\\" being capable of predicting the future. We utilize the public AVDC model trained on Bridge dataset [12] as the competitor, which is a real-world video dataset.\nPerformance comparison\nFigure 3 illustrates the comparisons. One can observe that SPORT performs significantly better than the competitors. It can \u201cimagine": "he object positions strictly following the command, performing the replacement directly in 3D space. AVDC produces fuzzy and less-realistic videos, and often fails to generate correct goal images. Actually we cannot fully reproduce the results of SuSIE, since we do not have the authors' robot setup. Nevertheless, we follow the code of SuSIE to produce goal images by repeating the image generation several times. Some issues can be observed to some extent, e.g., hallucination and loss of details. Moreover, both of these methods would hallucinate a robot or a human arm, originated from training data, which we believe may affect generalization."}, {"title": "5 Conclusions", "content": "In this work, we demonstrate the potential for achieving general-purpose object rearrangement by combining a pre-trained large vision model with a diffusion-based 3D pose estimation model. Given an instruction in natural language format, an LLM is used to identify the objects to be moved and to be reference. Given RGB-D images, we utilize an LLM-enhanced image segmentation model to segment required objects and then obtain their 3D point clouds. Based on these results, a diffusion-based 3D pose estimation model can follow precise low-level instructions to achieve physically-realistic position predictions. By establishing a GPT-assisted pipeline from a 3D perspective, a high-quality dataset for the object rearrangement task is generated. The results from both simulation and real-world experiments demonstrate the effectiveness of our approach. The model trained with simulation data can seamlessly transfer to real-world scenarios, achieving promising performance.\nThis project is a work still in progress, and several directions can be explored: (1) More realistic data. More realistic object models sampled from diverse environments and scenarios may probably benefit the model learning. Real-to-sim methods are worth trying. (2) Further improve physical realism. More strategies could be developed, e.g., encoding gravitational field, with which the model can simulate and predict a greater variety of real-world physical laws. This may be a big step to the \"world model\". (3) More powerful models. More recent diffusion models with high-capacity could be utilized to better estimate the object poses."}]}