{"title": "Stimulating Imagination: Towards General-purpose Object Rearrangement", "authors": ["Jianyang Wu", "Jie Gu", "Xiaokang Ma", "Chu Tang", "Jingmin Chen"], "abstract": "General-purpose object placement is a fundamental capability of an intelligent generalist robot, i.e., being capable of rearranging objects following human instructions even in novel environments. To achieve this, we break the rearrangement down into three parts, including object localization, goal imagination and robot control, and propose a framework named SPORT. SPORT leverages pre-trained large vision models for broad semantic reasoning about objects, and learns a diffusion-based 3D pose estimator to ensure physically-realistic results. Only object types (to be moved or reference) are communicated between these two parts, which brings two benefits. One is that we can fully leverage the powerful ability of open-set object localization and recognition since no specific fine-tuning is needed for robotic scenarios. Furthermore, the diffusion-based estimator only need to \"imagine\" the poses of the moving and reference objects after the placement, while no necessity for their semantic information. Thus the training burden is greatly reduced and no massive training is required. The training data for goal pose estimation is collected in simulation and annotated with GPT-4. A set of simulation and real-world experiments demonstrate the potential of our approach to accomplish general-purpose object rearrangement, placing various objects following precise instructions.", "sections": [{"title": "1 Introduction", "content": "General-purpose object placement is a fundamental capability of an intelligent generalist robot. Much like humans, the robot must be capable of reasoning and recognizing target objects (even though it has never encountered before), and then constructing the rearrangement following human instructions. For example, if an instruction \"put the spicy potato chips on the plate\u201d is given, the ability of semantic understanding is required, i.e., reasoning about \"spicy potato chips\u201d even this phrase may be outside of the training distribution. Furthermore, the rearrangement should be physically-realistic by fully considering the physical structures, geometries and constraints.\nThe great progresses of generative models provide researchers an insight of solving this challenging problem. Some of them introduce powerful models pre-trained on vision [3, 2, 7, 40], for initializing robotic policies or enhancing semantic understanding. Despite benefiting from the large-scale pre-training, it remains doubts about the generalization ability, since the amount of robotic manipulation fine-tuning data are far less than that encountered in a person's experience (also less than pre-training). Another bottleneck is that these approaches do not look specifically at 3D spatial understanding. They assume that the underlying states of world can be characterized by images from certain angles."}, {"title": "2 Related work", "content": "2D pose estimation. Traditionally, the object rearrangement task is divided into object recognition and pose estimation tasks. The advancement of vision transformers has greatly improved object"}, {"title": "3 Semantic-aware and physically-realistic object rearrangement", "content": "We introduce SPORT, a Semantic-aware and Physically-realistic Object RerrangemenT method. The pipeline of SPORT is illustrated in Figure 2."}, {"title": "3.1 Preliminaries and problem formulation", "content": "Given a single view of a scene captured by RGB-D sensors, we wish for the robot to rearrange this scene to satisfy the natural language instruction. Let Irgb and Id be the captured RGB and depth images, respectively. Denote the N objects in the scene as O = {01, 02, . . ., ON }, and the language instruction as W. Typically, the robot would be required to treat some object as the reference, and then move another one to a certain position relative to the reference."}, {"title": "3.2 SPORT for object rearrangement", "content": "We describe the details of the presented SPORT framework in this subsection.\nInstruction parsing An LLM is utilized to understand and parse the natural-language command W. We still use the \"put the spicy potato chips on the plate\" as an instance. We prompt LLM, e.g., GPT [28, 5] or LLaMA [36, 37], to extract object types and corresponding descriptions: \"spicy potato chips\" to be moved and \"plate\" to be the reference, respectively.\nObject reasoning and segmentation An open-set segmentation model is then needed. We employ LISA [20] in this work though other similar models could also be used. LISA combines a multi-modal LLM (LLaVA [22]) with the segmentation decoder (SAM [18]), showing powerful capacity of complex semantic reasoning that requires world knowledge. For example, given an image containing two bags of potato chips, it can segment the spicy one according to the common knowledge \"spicy snacks are usually packaged in red\".\nWe do not fine-tune the vision model on robotic data. The reason is that the scale of robotic data is smaller than that of web data for pre-training large vision models [3, 40]. We can fully leverage the existing high-capacity of complex reasoning and semantic understanding, while fine-tuning may hurt the generalization. Furthermore, without the heavy work of fine-tuning, we can cost-effectively and flexibly use stronger models with the development of the community.\nPose estimation We parameterize the 6-DoF pose as (t, R) \u2208 SE(3) (Special Euclidean Group). The goal pose estimator is based on a diffusion model, which is the basis of the recent remarkable AIGC approaches [31, 44]. It consists of several modules, i.e., a general-purpose text encoder, learnable type embeddings, a point cloud encoder and a vanilla transformer [38] as the backbone. We only use certain object (namely the moving one) to train the model, since the positions of the other objects remain unchanged after the rearrangement. The underlying idea behind this is similar to inpainting.\nText encoder. We deploy BERT [10] as the text encoder along with its tokenizer, as it can understand general-purpose instructions in natural language form. Unlike previous works that may be limited to the tokens in their customized vocabulary [23], BERT is capable of broadly understanding various instructions and can well capture the information within the instructions. Though more powerful text encoder could be more helpful [32], we choose BERT as a trade-off between the need for strong semantic understanding and resource overhead.\nType embedding. A set of learnable embeddings is introduced to indicate the token types, mainly the roles of corresponding objects in the rearrangement process, i.e., T. Four types are considered: the"}, {"title": "Diffusion", "content": "A language-conditioned diffusion model is used to estimate the goal poses of objects. At each time-step, six types of embeddings (specifically the text, type, object, position, time and an extra token containing camera viewpoint information) are combined and fed to the backbone. The model then predict the poses at the current time-step, specifically the t \u2208 R\u00b3 and two vectors a, b \u2208 R\u00b3 to construct the rotation matrix R \u2208 SO(3). The position and time embeddings follow standard design, indicating the token positions in sequences and the time-step in diffusion, respectively. Padding is used to maintain a consistent number of input tokens. Note that the introduced extra token is essential, with which the model can effectively accomplish the instructions even when the camera viewpoint changes in real scenarios.\nThough all objects are included in model inputs, only the moving one gets involved in iterative pose estimations. It is because the diffusion model needs to know all object information to achieve relative positional movement and avoid collisions (thus all objects are required in the input), while only the pose of the moving object would change. Accordingly, we only add noise to the moving object pose during model training. The training objective can be formulated as\narg min\u03b8 \u2211Ni=1Ii=mEe,t [||\u20ac \u2013 6\u03b8 (xi,t)||1], (1)\nwhere e is sampled from a standard normal distribution, xi is the pose estimation of i-th object in O at t-timestep, and Ii=m is an indicator checking whether i-th object is the one to be moved."}, {"title": "3.3 GPT-assisted object rearrangement data generation", "content": "The available amount of public 3D object rearrangement data is limited, especially the data containing low-level rearrangement instructions. In this work, we develop an automatic pipeline for generating high-quality rearrangement-instruction pairs. Each instance comprises an initial scene, a goal scene after the rearrangement and a corresponding instruction. A total of 40,000 stable and collision-free instances are generated in the PyBullet physics simulator [9], rendered by OpenGL [1]. The simulated objects are randomly selected from the popular ShapeNetSem [6] (specifically ShapeNetSem [33]) dataset. We collect various 581 objects from 30 categories to ensure diversity.\nThe entire generation pipeline consists of three steps: (1) pre-processing metadata to obtain well-constructed and realistic simulated objects; (2) randomly selecting the reference, moving and irrelevant objects, namely Or, Om and O \\ {Or, Om}, loading them to PyBullet to obtain the initial and goal scenes, then filtering out physically-unrealistic ones; (3) using GPT-4 to generate rearrangement language instructions corresponding to the transition from the initial scene to the goal scene, based on the object and scene information. The difficulty lies in the time-consuming and cumbersome data collection process, as well as the limited capability of precise (fine-grained) spatial understanding and reasoning in existing models, even GPT-4.\nPre-processing. The metadata in ShapeNetSem needs to be pre-processed by scaling and translation, because the object models may have unrealistic sizes and the centroids of objects may not be aligned with the origin of the point-cloud coordinate system. The scaling factors are obtained with GPT-4, e.g., asking GPT-4 about the typical size of a cellphone and accordingly scaling the object model.\nScene generation. We categorize the data generation into multiple scenarios according to the relative spatial relationship between the moving and reference objects, such as left, right, front, behind, on, between, etc. We randomly select the object set O and the scenario to generate scenes. For the initial scene, we place all the objects with random positions in PyBullet, wait for them to settle into stability and record their poses. For the goal scene, we replace the reference and irrelevant objects with the"}, {"title": "4 Experiments", "content": "The goal of the experiments is to evaluate the efficacy of SPORT in the object rearrangement task, especially in the ability of generalization, 3D spatial reasoning and precise instruction following. To this end, we need to answer the following questions:\n1. Does SPORT excel at the task, even in a new environment, given a precise instruction, given unseen objects with various attributes, requiring physically-realistic results?\n2. Can SPORT trained with simulation data seamlessly transfer to real-world scenarios, even in zero-shot and requiring more complex reasoning?\nThe experiments are then conducted both in simulation and real-world environments. The experimental details and results are reported in the following two sections."}, {"title": "4.1 Simulation experiments", "content": "Setup To fully validate the generalization ability of SPORT, we conduct a cross-dataset evaluation. Unlike the objects in training data (from ShapeNetSem), the simulated objects for testing are collected from Google Scanned Objects [11]. A total of 77 object models from 37 novel categories are randomly selected. We use PyBullet [9] as the physics simulator and OpenGL [1] as the appearance render. For each testing sample, the involved objects are randomly sampled. The scene and the rearrangement instruction are generated following the pipeline in subsection 3.3.\nWe use the success rate as the evaluation metric as in previous works [23]. There are three aspects to consider: given a command, the model should be able to recognize the involved objects, place them to correct positions, and the rearrangement is physically-realistic. We systematically measure"}, {"title": "Quantitative evaluation", "content": "The results are listed in Table 1. Whether a testing sample is classified as correct depends on three aspects: the objects are correctly recognized, the generated poses of objects satisfy the instruction, and the rearrangement is physically-realistic. \u201cPose accuracy\u201d refers to the accuracy of considering the first two aspects, \"Physical Realism\u201d focuses on the last one, and \u201cOverall Success\" is the overall success rate considering all three aspects. Six scenarios (corresponding to six spatial relationships) are conducted for evaluation, namely left, right, front, behind, on and between.\nAs shown in Table 1, SPORT achieves an overall success rate of 46.19% on the simulation testing set. The result is acceptable due to the challenging experimental setting: the testing and training data are collected from different datasets (the objects are totally different), and the final states of objects should stable and collision-free. However, we want to explore more, especially given the observation that Physical Realism achieve higher accuracy than Pose Accuracy, which is quite unusual.\nWe notice that LISA fails a lot on images rendered in simulation. On simulation-rendered images, the completeness at the edges of the object segmentation masks produced by LISA are not sufficient. As a result, the quality of the resultant 3D point clouds is often poor. We attribute this to the substantial domain differences between the simulation images and the real-world images used in LISA's training set, resulting in limited model generalization. However, LISA can indeed localize target objects even requiring complex reasoning in real-world scenarios. We have conducted related experiments, please refer to subsection 4.2 for the details.\nAccording to the above observation, we want to know the performance of SPORT if the required objects can be successfully obtained (since LISA can achieve this on real-world images). The results are listed in the second row, by using object masks directly from simulation data. One can see that SPORT achieves convincing performance, 87.8% on Pose Accuracy and 69.49% on Overall Success.\nFinally, one can see that our approach demonstrates a certain degree of effectiveness in generating physically-realistic poses. The success rate of Physical Realism achieves 76.40%, assessed by using the Pybullet simulator. Despite the progress, this particular ability indeed needs further enhancement. Its accuracy exhibits a disparity relative to Pose Accuracy. We leave the exploration as further work."}, {"title": "Ablation study", "content": "In this part, we perform ablation studies to synthetically analyze the proposed SPORT.\nEstimating Poses of Reference and Irrelevant Objects. In our approach, the poses of the reference and irrelevant objects are fixed when training the diffusion model. We conduct an experiment to explore the impact of such a design, i.e., comparing the performances of fixing poses versus not"}, {"title": "4.2 Real-world experiments", "content": "Setup We collect several real-world scenes, captured by an Intel RealSense D435i RGB-D camera. Each scene includes several common objects placed on a table. The objects include various fruits, food, potato chips, tableware, etc. Challenging evaluations can be conducted that require complex spatial reasoning involving multiple yet similar objects.\nCompetitors\nSuSIE [2] leverages a diffusion model to \"edit\" the image of current scene to generate the intermediate subgoal image based on instructions. A low-level policy then executes the actions to reach the subgoal. Object rearrangement can be done by alternating this loop. The key is to implement the diffusion model with InstructPix2Pix [4], a powerful image-editing model pre-trained on Internet-scale data.\nAVDC [19] uses an image diffusion model to synthesize a video of imagined execution of the rearrangement process. The underlying assumption is that the video generation model is a \u201cworld model\" being capable of predicting the future. We utilize the public AVDC model trained on Bridge dataset [12] as the competitor, which is a real-world video dataset.\nPerformance comparison\nFigure 3 illustrates the comparisons. One can observe that SPORT performs significantly better than the competitors. It can \u201cimagine\u201d the object positions strictly following the command, performing the replacement directly in 3D space. AVDC produces fuzzy and less-realistic videos, and often fails to generate correct goal images. Actually we cannot fully reproduce the results of SuSIE, since we do not have the authors' robot setup. Nevertheless, we follow the code of SuSIE to produce goal images by repeating the image generation several times. Some issues can be observed to some extent, e.g., hallucination and loss of details. Moreover, both of these methods would hallucinate a robot or a human arm, originated from training data, which we believe may affect generalization."}, {"title": "5 Conclusions", "content": "In this work, we demonstrate the potential for achieving general-purpose object rearrangement by combining a pre-trained large vision model with a diffusion-based 3D pose estimation model. Given an instruction in natural language format, an LLM is used to identify the objects to be moved and to be reference. Given RGB-D images, we utilize an LLM-enhanced image segmentation model to segment required objects and then obtain their 3D point clouds. Based on these results, a diffusion-based 3D pose estimation model can follow precise low-level instructions to achieve physically-realistic position predictions. By establishing a GPT-assisted pipeline from a 3D perspective, a high-quality dataset for the object rearrangement task is generated. The results from both simulation and real-world experiments demonstrate the effectiveness of our approach. The model trained with simulation data can seamlessly transfer to real-world scenarios, achieving promising performance.\nThis project is a work still in progress, and several directions can be explored: (1) More realistic data. More realistic object models sampled from diverse environments and scenarios may probably benefit the model learning. Real-to-sim methods are worth trying. (2) Further improve physical realism. More strategies could be developed, e.g., encoding gravitational field, with which the model can simulate and predict a greater variety of real-world physical laws. This may be a big step to the \"world model\". (3) More powerful models. More recent diffusion models with high-capacity could be utilized to better estimate the object poses."}]}