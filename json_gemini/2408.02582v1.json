{"title": "CLUSTERING AND MINING ACCENTED SPEECH FOR INCLUSIVE AND FAIR SPEECH RECOGNITION", "authors": ["Jaeyoung Kim", "Han Lu", "Soheil Khorram", "Anshuman Tripathi", "Qian Zhang", "Hasim Sak"], "abstract": "Modern automatic speech recognition (ASR) systems are typically trained on more than tens of thousands hours of speech data, which is one of the main factors for their great success. However, the distribution of such data is typically biased towards common accents or typical speech patterns. As a result, those systems often poorly perform on atypical accented speech. In this paper, we present accent clustering and mining schemes for fair speech recognition systems which can perform equally well on under-represented accented speech. For accent recognition, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering. Three schemes can significantly improve the accent recognition model especially for unbalanced and small accented speech. Fine-tuning ASR on the mined Indian accent speech using the proposed supervised or unsupervised clustering schemes showed 10.0% and 5.3% relative improvements compared to fine-tuning on the randomly sampled speech, respectively.", "sections": [{"title": "1. INTRODUCTION", "content": "Recent automatic speech recognition (ASR) systems have shown great success on diverse acoustic and linguistic conditions due to huge increase of model parameters and speech data. Typically, Modern ASR systems with several hundreds of thousands parameters can be easily trained on tens of thousands hours of speech data. The trained ASR systems will work as expected as long as target domains are well matched to training data. However, the distribution of speech data is typically focused on standard canonical speech patterns. As a result, ASR systems trained on such data distribution often perform poorly on the unseen or atypical accented speech.\nThe main challenge for recognizing accented speech is the lack of sufficient training data for these accents. It is very expensive and time-consuming to manually collect accented speech data especially when the target accent is rare and not often spoken. Therefore, well performing accent recognition systems would be important for systematically clustering and mining accented speech.\nThere has been extensive studies on joint learning of speech and accent recognition with multi-task method [1, 2, 3, 4, 5, 6]. For the most of the prior works, accent recognition is used as an auxiliary task to assist ASR system to be aware of accents in input audio speech. However there has been few research on investigating accent recognition models [7, 8]. The most of works were evaluated as a part of challenges or use their well-defined accented datasets which are not usually available now. Accented speech is rare and typically severely imbalanced between different accents. Furthermore, accent labels are not stable and sometimes severely corrupted.\nIn this paper, we investigate an accent recognition model when we are given severely imbalanced accented datasets and accent labels are not reliable. We applied three schemes to address and resolve issues discussed above:\nPre-Training: A pre-trained model can avoid learning the spurious features such as speaker identity by learning hidden speech representation from huge speech dataset. Both pre-training based on supervised and unsupervised training showed significant improvement on accent recognition.\nDistributionally Robust Optimization (DRO): Group DRO minimizes the empirical risk of the worst-performing group, instead of minimizing the average empirical risk, which can avoid overfitting groups with smaller datasets. Group DRO effectively reduced accuracy variance between accents.\nUnsupervised Clustering: Unsupervised clustering is used to recognize unseen accents during the model training stage. K-means algorithm is applied to only update centroids' locations by fixing the weights for the trained model.\nAn ASR model based on Transformer-Transducer is fine-tuned on the mined Indian accents from the proposed supervised and unsupervised accent models. They showed 10.0% and 5.3% relative WER improvements on the Indian accent compared to randomly sampled accents, respectively."}, {"title": "2. ACCENT RECOGNITION", "content": "Figure 1 shows the proposed accent recognition model. The model consists of convolutional layers and Transformer block. The convolutional layers sub-sample log-mel spectrograms with strides of 4. The strided convolution output is fed to the Transformer block and its output is pooled across time for the accent representation. There are several pooling methods. Among them, average and max-imum pooling were evaluated, which showed no significant differ-ence in accent prediction. In this paper, average-pooling was used.\nFigure 1 presents two stages of model training: supervised cross entropy (CE) with distributionally robust optimization (DRO) and unsupervised re-clustering by online K-means algorithm. DRO can effectively deal with data imbalance between different accents, which will be explained at section 2.2 in detail. The unsupervised re-clustering is useful when supervised accent labels are noisy. Online K means algorithm is used for computing new centroids for accent re-clustering."}, {"title": "2.1. Pre-Training", "content": "One of the challenges for training an accent recognition model is to prevent the model from correlating features other than speech accents to the predictions. For example, the model can utilize gender, speaker tone, channel characteristics or background noise to distin-guish accents especially when accent data is not large enough. Ta-ble 1 summarizes the number of utterances between 8 English ac-cents provided by Mozilla Common Voice data. Among all the ac-cents, Asian or Irish accents are much smaller than US accent and hence a recognition model can be susceptible to correlate based on spurious features such as speaker identity.\nA pre-trained model can avoid learning the biases by learning hidden speech representation on huge supervised or unsupervised speech data. For supervised pre-training, we utilize RNN-T ASR model trained on 400K supervised Youtube data. The audio encoder block in Youtube RNN-T model is used for initializing an accent recognition model. The audio encoder block has the same structure as the accent recognition model in Figure 1:2 convolutional layers and 20 Transformer layers. For a self-supervised model, wav2vec training is applied on one million unsupervised Youtube speech data. One difference on wav2vec model is that the softmax output for ac-cent recognition is derived from 10th layer of wav2vec model. Typi-cally, self-supervised models trained on input audio feature have bet-ter feature abstraction in the middle of layers because layers closer to output should reconstruct raw audio features. Two pre-trained models are evaluated at section 3.2. Both of them showed huge im-provements over a randomly initialized model."}, {"title": "2.2. Distributionally Robust Optimization (DRO)", "content": "Group DRO [9] was first introduced on computer vision and natural language processing applications that tries to make models robust to different group shifts at test time. It achieves that by minimizing the empirical risk of the worst-performing group, instead of mini-mizing the average empirical risk (i.e. ERM) at training time. Such optimization avoids learning correlations between input features and output targets that hold on average, but do not generalize to broader use cases. In practice, to train DRO efficiently, [9] proposed an online algorithm that tracks the loss for each training group to form a distribution over different groups. Such distribution is used to scale the losses from different groups to optimize the model.\nPublicly available accented speech data is usually imbalanced between different accent groups due to the difficulty of collecting rare accented speech. Table 1 showed US accent is the most com-mon and constitutes almost the half of total utterances. Even the sec-ond largest accent group (i.e. England) is only around one third of US accent. A neural network model trained to minimize the average empirical risk on a highly imbalanced training dataset tends to work better on larger accent groups (e.g. US) but often perform poorly on the smaller rare accent groups. A widely used technique to alleviate such performance gap due to class imbalance is to sample equally from each training class to have higher exposure to rare classes. Al-though this is effective in some cases, this tends to overfit easily and does not generalize well for most of the accent groups. On the con-trary, DRO scales the loss of worst-performing accent group more in addition to sampling each accent group equally. This prevents the model from further optimizing on smaller groups even when the corresponding losses are already small. DRO showed the smallest prediction variance between different accents without hurting mean prediction accuracy at Table 2."}, {"title": "2.3. Unsupervised Clustering", "content": "Figure 3 shows TSNE plots for different testing accent embeddings extracted from the pooling layer output of the trained accent recog-nition model. Each plot at Figure 3 depicts different clustering on the same set accent embeddings. For example, Figure 3 (a) shows accent groups by ground-truth labels and Figure 3 (b) presents clus-tering by the model's softmax predictions. There are a couple of things to note for the accent recognition model. First, since Cana-dian and US accents are close together, they are treated as the same accent during model training. Second, the accent recognition model was trained without Indian accents in order to test the generalization capability for the unseen accent clustering.\nThere are several observations on the ground-truth TSNE plots. First, TSNE plot shows nicely separated embedding groups. Especially, the Indian accents are clustered well from other accents, which suggests the trained accent model generalized well to unseen accents. Second, some accents such as US, Asian and Irish are mixed on the center region without clear boundaries. It is because the accent labels are collected based on speakers' location and therefore sometimes they do not match with the speakers' true accents: noisy accent labels. For example, we sampled US accent utterances which overlap with Indian accent embedding location in Figure 3 (a). We listened to each sampled utterance and confirmed all of them are closer to Indian accents than US ones.\nFigure 3 (b) shows accent groups predicted by the recognition model. The model clearly predicted well for accents seen during training but it did not correctly recognize Indian accents. The model cannot predict Indian accent because its softmax output only contains accents seen from training data. However, as explained before, the model showed good Indian accent clustering in the TNSE plot.\nWe propose the unsupervised accent clustering based on K-means algorithm. The main idea is that K-means algorithm only updates centroids' location and does not update the trained accent recognition model. The number of K-means centroids can be set flexibly large enough to cover unseen accents. Figure 3 (c) showed K-means clustering when the number of centroids are 6. The Indian accents are correctly recognized compared to the ground-truth plot.\nHere we describe the details of the online K-means algorithm we used. We followed K-means++ algorithm for centroid initialization:\n1. Randomly select data point x \u2208 X as the first centroid $c_1$.\n2. Each new centroid $c_i$ is sampled based on the distance probability: $ \\frac{D(x)^2}{\\sum_x D(x)^2}$ where D(x) is the distance to the closest centroid from data x.\n3. Repeat 2 until initializing all of centroids.\nCentroids are updated as follows:\nCompute new centroids: $\\hat{c_i} = \\frac{\\sum_{x \\in C_i} x}{|C_i|}$ .\nEMA updates: $c = \\alpha c_{-1} + (1 - \\alpha)\\hat{c_i}$\nwhere $C_i$ is the set of data points belonging to the $c_i$ centroid, $|C_i|$ is the cardinality of the set, EMA is exponentially moving average and \u03b1 is an EMA update weight."}, {"title": "3. EXPERIMENT AND RESULTS", "content": "We used three separate datasets: Mozilla Common Voice [10], Youtube supervised (YT-L) and Youtube unsupervised (YT-U) datasets and LibriSpeech. The Mozilla Common Voice is a speech dataset collected from voluntary user submissions based on vari-ous public domain sources. The dataset contains various English accents based on different countries. The total hours for training data with accent labels are around 700 hours and the total number of English accents are 14. However, due to dataset imbalance between accents, some accents only have a few hours of training data. In this paper, we merged accent groups to have a total of 8 accents as in Table 1. Youtube is a set of large audio datasets of anonymized Youtube Videos collected in accordance with Google Privacy and AI principles [11, 12]. Youtube supervised (YT-L) is roughly 350k hours of segmented, weekly-labeled audio, combined with 1000 hours of manually labeled data. Youtube unsupervised (YT-U) is a 900k hours of unlabeled audio dataset. It is first randomly collected from 3M hours Youtube videos of interviews and lectures, and then filtering non-speech parts later. Finally, LibriSpeech [13] corpus is a read speech data based on audio books and consists of 1000 hours of training and test sets."}, {"title": "3.2. Result on Accent Recognition", "content": "The accent recognition model has 20 Transformer layers based on bidirectional transformer-XL [14] and 2 2d-convolutional layers with total of stride 4. There are total of 5 different settings: Base, YT_NODRO, YT_DRO, YT_EQ and W2V_DRO. Base model is trained on Mozilla Common voice dataset with random initializa-tion. YT_NODRO is initialized from a RNN-T ASR model trained on YT-L. YT_DRO applied DRO optimization to YT_NODRO, YT_EQ is to sample unbalanced accents equally, and W2V_DRO has an wav2vec2.0 initialization [15] trained on YT-U dataset.\nFigure 2 compares accent prediction accuracies between dif-ferent settings. For the BASE model, most of accent predictions show poor performance. Especially, Irish or Scottish accent predic-tions showed 0% accuracies due to the lack of accented data. The YT_NODRO initialized from Youtube RNN-T model showed sig-nificant improvements on almost every accent predictions. How-ever, the issue on the model is that prediction performance is signif-icantly different between accents. For example, US, Canadian and Indian accents are around 90% accuracies but Asian, Australian and England are less than 50%. Furthermore, Mozilla accent labels are weekly-annotated using indirect meta information such as residence location. Too high accuracies on the test set especially for US or Canadian accents can be the indication of overfitting to speaker in-formation, channel condition or background noise other than accent information. Distributionally robust optimization (DRO) reduced accuracy variance between accents for YT_DRO. There are degra-dation for high performing accents such as US and Canadian but the rest of accent accuracies uniformly improved. Similarly, DRO model based on wav2vec initialization showed reduced variance be-tween accents. Table 2 shows accuracy mean and variance over all accent groups. Mean accuracies are similarly good for both DRO and the equal sampling scheme (YT_EQ). However, DRO showed smaller standard deviation than EQ."}, {"title": "3.3. Fine-Tuning ASR on the Mined Accented Speech", "content": "In this section, an ASR model based on Transformer-Transducer (T-T) [16, 17, 18] is fine-tuned on the accent speech mined by the pro-posed accent recognition model. The audio encoder consists of two strided convolutions followed by 20 full-attention Transformer lay-ers. The label encoder has a couple of streaming Transformer layers. 80-dimentional log-mel filterbanks are acoustic features to the au-dio encoder and SpecAugment and variational noise are added to the model input and parameters for robust training.\nThe BASE model is initially trained on LibriSpeech, which is then fine-tuned on the different mined accented speech as in Ta-ble 3. For Random YT-L, fixed size of YT-L speech data is ran-domly sampled. For No Indian YT-L, Indian accents were removed from random samples from YT-L. For Indian YT-L, Indian accents are mined by the accent model training with Indian accents. Lastly, for K-Means Indian YT-L, Indian accents are mined by the model trained without Indian accents. For a fair fine-tuning environment, all the fine-tuning setups have similar number of utterances. Since fine-tuning only on the specific accents frequently degrades other accents, US accented data from Mozilla Common Voice was mixed with mined speech data.\nIn Table 3 fine-tuning on the mined Indian accents (Indian YT-L) showed significant improvements on the Indian accent recogni-tion: 10.0% and 15.4% relative improvement over the random YT-L and random YT-L without the Indian accents. The unsupervised Indian accent clustering (K-Means Indian YT-L) also improved the Indian accent recognition: 5.3% and 11.1% relative improvements over Random YT-L and No Indian YT-L although the gain decreased compared to the supervised clustering. The WER gap between them would be because the Indian accent recognition for the unsupervised clustering is lower than supervised one, therefore, mined dataset would contain less Indian utterances.\nEvaluation on the US accents for all the fine-tuned ASR models showed similar WER. We did not see any degradation from fine-tuning on the mined Indian accent when small US accents were trained together."}, {"title": "4. CONCLUSION", "content": "In this paper, we showed the new framework for improving accent recognition models. Pre-training and group DRO schemes were ap-plied to overcome unbalanced and small accented speech. We also evaluated supervised and unsupervised clustering of accented speech and their application to fine-tuning ASR models. We have shown that the proposed accent recognition model can generalize well on the unseen accented speech by fine-tuning ASR on the mined ac-cented speech. As a future work, we will investigate how to reduce the gap between supervised and unsupervised clustering to improve its generalization on the unseen accent recognition."}]}