{"title": "Giving AI Personalities Leads to More Human-Like Reasoning", "authors": ["Animesh Nighojkar", "Bekhzodbek Moydinboyev", "My Duong", "John Licato"], "abstract": "In computational cognitive modeling, capturing the full spectrum of human judgment and\ndecision-making processes, beyond just optimal behaviors, is a significant challenge. This\nstudy explores whether Large Language Models (LLMs) can emulate the breadth of hu-\nman reasoning by predicting both intuitive, fast System 1 and deliberate, slow System 2\nprocesses. Unlike traditional AI research focused on optimizing accuracy, this paper inves-\ntigates the potential of AI to mimic diverse reasoning behaviors across a human population,\naddressing what we call the full reasoning spectrum problem. We designed reasoning tasks\nusing a novel generalization of the Natural Language Inference (NLI) format to evaluate\nLLMs' ability to replicate human reasoning. The questions were crafted to elicit both Sys-\ntem 1 and System 2 responses. Human responses were collected through crowd-sourcing\nand the entire distribution was analyzed and modeled, rather than just the majority of\nthe answers. We used personality-based prompting inspired by the Big Five personality\nmodel to elicit AI responses reflecting specific personality traits, capturing the diversity\nof human reasoning, and exploring how personality traits influence LLM outputs. Com-\nbined with genetic algorithms to optimize the weighting of these prompts, this method\nwas tested alongside traditional machine learning models. The results show that LLMs\ncan mimic human response distributions, with open-source models like Llama and Mis-\ntral unexpectedly outperforming proprietary GPT models. Personality-based prompting,\nespecially when optimized with genetic algorithms, significantly enhanced LLMs' ability\nto predict human response distributions, suggesting that capturing suboptimal, naturalis-\ntic reasoning may require modeling techniques incorporating diverse reasoning styles and\npsychological profiles. The study concludes that personality-based prompting combined\nwith genetic algorithms is promising for enhancing AI's human-ness in reasoning, propos-\ning a new methodology for studying and applying human reasoning by acknowledging and\nleveraging the vast differences in individual reasoning styles at a granular level.", "sections": [{"title": "1 Introduction", "content": "Capturing the nuanced, often imperfect, and highly diverse reasoning processes of humans\npresents significant challenges. Traditional uses of AI for this purpose have focused primarily\non optimizing accuracy and efficiency, but this approach often neglects the complexity of\nhuman cognition, where decisions can be significantly influenced by intuition, emotion,\nand prior experiences. The central issue is that current AI models struggle to capture\nthe full spectrum of human reasoning, particularly in problem spaces where decisions are\nnot simply correct or incorrect but rather involve a plurality of diverse thought processes.\nAddressing this limitation is crucial for advancing AI systems that can genuinely understand\nand interact with humans on a deeper, more intuitive level, making it a critical area of\nresearch that extends beyond technical optimization.\nUnderstanding and predicting human reasoning is as fascinating now as it has been for\nthousands of years (Aristotle, 2013; Wason, 1966). Dual Process Theory, one of the most\ninfluential theories in contemporary cognitive science (Kahneman, 2011; Stanovich & West,\n2000), distinguishes between two types of cognitive processes: System 1 and System 2.\nSystem 1 operates automatically, rapidly, and effortlessly, guiding intuitive and habitual\ndecisions without conscious control. In contrast, System 2 is slower, more deliberate, and\nresponsible for managing mental activities that require focused attention, such as com-\nplex calculations and deliberate decision-making. These dual processes are fundamental to\nunderstanding how humans navigate decisions, both trivial and significant, and provide a\nframework for assessing whether AI can genuinely replicate human-like reasoning.\nDual process theories have been given a second look in the age of the large language\nmodel (LLM), which have become a cornerstone in AI-driven reasoning due to their tremen-\ndous success on a variety of tasks. Some researchers have suggested\u2014or even demonstrated\nthrough experimental paradigms that LLMs with lower complexity or simpler prompts\ntend to engage in System 1 reasoning, while more complex architectures or prompts en-\ncourage System 2-like reasoning (Hagendorff et al., 2023; Kojima et al., 2022; Saha et al.,\n2024; Weston & Sukhbaatar, 2023; Yu et al., 2024). However, many such claims operate\nunder the assumption that System 1 is inferior and should be avoided, focusing instead on\nmaking AI reasoning more like human System 2. The use of datasets such as the Cognitive\nReflections Test (CRT) (Frederick, 2005), designed to provoke System 1 into giving the\nwrong answer while System 2 provides the correct one, is a reflection of this assumption.\nThis focus on accuracy neglects the reality that human reasoning is rarely so clear-cut.\nGiven these complexities, how well can our best AI approaches capture human reason-\ning? AI reasoning encompasses a broad range of approaches that have been described at\ntimes as efficient (Maclure, 2021), logical (Hagedorn et al., 2020), mechanistic, or capable\nof making sense of complex scenarios (Zollman et al., 2023). However, it has also been\ncriticized for being biased and opaque (O'Neil, 2017). Numerous studies have aimed to\nreduce bias in Al reasoning and promote fairness (Ma et al., 2023), yet human reasoning\nis naturally biased and flawed (Ariely, 2010; Gilovich et al., 2002; Stanovich et al., 2018).\nEliminating these flaws can therefore make AI reasoning less human-like (Chemero, 2023;"}, {"title": "2 Background", "content": "2.1 System 1 vs System 2 Reasoning in LLMs\nDual Process Theory distinguishes between two types of cognitive processes: System 1,\nwhich is fast, intuitive, and automatic, and System 2, which is slower, more deliberate,\nand rational (J. S. B. T. Evans & Stanovich, 2013). This framework is highly influential\non current theories of human reasoning, judgment, and decision-making. For instance,\nclassic problems like the Bat and Ball Problem (Frederick, 2005)"}, {"title": "2.2 The Natural Language Inference (NLI) Format", "content": "In our study, we utilize an enhanced version of the Natural Language Inference (NLI)\ntask, building upon prior foundational work (Bowman et al., 2015). NLI, a successor to\nRecognizing Textual Entailment (RTE) (Dagan et al., 2006), is centered around evaluating\nwhether a hypothesis h can logically follow from a given premise p. For illustration, consider\nthe premise p = \"A crowd gathers as three blue cars begin a race.\" Possible relationships\nbetween this premise and a hypothesis h could be:\n1. Entailment: h = \"A race is taking place.\" must be true if p is true.\n2. Contradiction: h = \"There are no cars racing.\" cannot be true if p is true.\n3. Neutral: h = \"Three men are competing in a race.\" is neither necessarily true nor\nnecessarily false given p.\nThe most common goal of NLI datasets is to study how a set of human reasoners nat-\nurally reasons about the inferential relationships in the NLI items. LLMs are traditionally\ntrained and benchmarked on NLI datasets to enhance their naturalistic reasoning capabili-\nties (Bowman et al., 2015; Nie, Williams, et al., 2020; Williams et al., 2018, 2022).\nWhile typically trifurcated into distinct categories, this specific categorization can suffer\nfrom problems of underspecification and a lack of granularity (Nighojkar et al., 2023). Ad-\ndressing these complexities, researchers have proposed various modifications to the standard\ncategorization, including:\n\u2022 Introducing an \"entailment strength\" parameter, reflecting either model confidence\nor perceived likelihood, though such measures have historically grappled with issues\nlike ambiguous evaluation standards and inconsistent annotator judgments (Chen et\nal., 2020; Meissner et al., 2021).\n\u2022 Examining the variability in annotator perspectives, considering whether a 'neutral'\njudgment indicates balanced reasons for both agreement and disagreement or a com-\nplete absence of decisive evidence (Pavlick & Kwiatkowski, 2019; S. Zhang et al., 2021;\nX. F. Zhang & Marneffe, 2021; Zhou et al., 2022).\n\u2022 Differentiating between various degrees of entailment, such as \"absolutely entails\" ver-\nsus \"likely entails,\" which echoes ongoing research into probabilistic reasoning (Kah-\nneman, 2011).\nTo refine these categories further, we propose a structured NLI framework with six\ndetailed categories, broadening the inferential spectrum from absolute contradiction to def-\nnite entailment. These categories are: (A) Absolutely must be false, (B) Is more likely to be"}, {"title": "3 Data Collection", "content": "3.1 Survey Design\nOur study utilizes a dual-phase survey design, hosted on Qualtrics, with 60 participants\nrecruited via Prolific. The survey consists of 27 questions in the first phase, which are\nrepeated in the second phase, following the design of the two-response paradigm (Bago &\nDe Neys, 2017). In this paradigm, participants are asked to provide two separate responses\nto the same question or task. The first response is typically their initial, instinctive answer,\nwhile the second response is given after some deliberation or additional information is\nprovided. This method allows us to analyze the differences between intuitive and reflective\nthinking, or System 1 and System 2, shedding light on cognitive processes like reasoning,\ndecision-making, and how people change their minds when given more time or information.\nIn the first phase, aimed at eliciting System 1 reasoning, participants are presented with NLI\nquestions while simultaneously solving a puzzle as a distraction. Each question is divided\ninto two parts: the premise and the conclusion, with six response options (A-F) available,\nas detailed in Section 2.2. The exact instructions provided to the participants are in\nParticipants read the premise then read the\nconclusion before selecting one of the six options Note that participants never\nhave access to the premise and the conclusion together in this phase and need to rely on\nwhatever they retain from the premise when answering the question about the conclusion\non Screen 3. The survey begins with an example question to familiarize participants with\nthe task. To reduce fatigue, one-minute breaks are incorporated after each question (a set\nof all three screens).\nIn the first phase, a time limit is imposed to encourage rapid reasoning. The initial time\nallocated per question is determined using the following formula:\nbase time = 0.0787 \u00d7 sentence length + 0.0016 \u00d7 perplexity + 6.3276\nThese coefficients are derived from ordinary least squares regression (Zdaniuk, 2014), based\non data collected in-house. We recorded the time each participant (the co-authors and other\nvolunteering students) took to read a question and trained the regression model to predict\nthe 90th percentile reading time based on the sentence length and its perplexity (Jelinek\net al., 1977). Additionally, the reading time is adaptively adjusted based on participant\npacing, either increasing or decreasing the time allotted depending on their speed, with a"}, {"title": "3.2 Item Types", "content": "In Section 1, we introduced the full reasoning spectrum problem (where existing datasets\nand methods fail to capture the whole spectrum of human reasoning patterns) and the data\ncontamination problem (the possibility that publicly available datasets may be included in\nthe training data of large language models, or LLMs). In Section 2.2, we also explained\nour decision to use NLI as our preferred format. Commonly available NLI datasets do not\nattempt to capture annotations that distinguish between System 1 and System 2 reasoning\nin humans. Additionally, it is highly likely that existing public NLI datasets have already\nbeen used to train LLMs. Consequently, these datasets suffer from both the full reasoning\nspectrum problem and the data contamination problem. To address these issues, we created\nour own dataset of NLI questions using OpenAI's GPT-4 model (specifically GPT-4-0314)\nto generate all the items. We provided the model with multiple prompts and used different\nOpenAI API calls to generate the premises and conclusions separately, ensuring that the\nmodel never produced both simultaneously. Finally, we made two additional API calls\nto rephrase the premises and conclusions individually. All our prompts are in Appendix\nB along with details about our data generation pipeline. We generate different types of\nitems to prevent monotonicity in the dataset and to prevent any pattern recognition for the\nhumans doing our survey and the models we use to predict the human responses. These\nitem types are explained in more detail below.\nStereoNLI StereoNLI connects NLI with stereotypes, building on previous research show-\ning that human System 1 reasoning is often biased and influenced by stereotypes (Geeraert,\n2013; Kahneman, 2011). We utilized the StereoSet dataset (Nadeem et al., 2021) to select\nseed words for generating StereoNLI questions. StereoSet comprises 17,000 sentences that\nassess stereotype bias in language models concerning gender, race, religion, and profession.\nEach sentence is labeled by multiple human annotators as either 'anti-stereotype', 'stereo-\ntype', 'unrelated', or 'related'. From StereoSet, we randomly chose nouns associated with\ngender, race, religion, and profession, then prompted GPT-4-0314 to generate a name for a\nhypothetical person called 'X'. We asked GPT-4 to write a paragraph with three sentences\nreflecting common assumptions about 'X' based on the given traits. This paragraph be-\ncame our premise. Excluding gender due to its limited variety in the dataset, we randomly\nselected one of the three traits and prompted GPT-4 to generate an assumption about 'X'\nwith a truth value. For each scenario, we created three instances\u2014contradiction, neutral,"}, {"title": "Fallacy", "content": "Fallacies of argumentation are arguments whose logical structures that have very\nlittle to no deductive validity, despite being commonly used in informal reasoning.\nPrevious cognitive psychology research (Boissin et al., 2022) has examined how System 1 can\nsometimes engage in fallacious reasoning. In our dataset, we focus on three specific types of\nfallacies. The first, post hoc ergo propter hoc, occurs when someone mistakenly believes that\nbecause one event follows another, the first event caused the second. The second, slippery\nslope, posits that an action will set off a chain of events leading to an undesirable outcome\nwithout establishing or quantifying the relevant contingencies. This fallacy, also known as\n\"the domino effect,\" often implies a long series of intermediate events connecting a seem-\ningly harmless start to an undesirable end, assuming uncertain or unlikely consequences.\nThe third fallacy, straw person, involves misrepresenting an opponent's argument to make\nit easier to refute. We created templates to generate premise-conclusion pairs for each type\nof fallacy. For example, in the case of post hoc ergo propter hoc, the premise follows the\nstructure \"X happened right before Y,\" with the conclusion stating \u201cX caused Y.\u201d GPT-\n4 fills in the details for X and Y, and we then rephrase the premises and conclusions to\nproduce the final items."}, {"title": "Syllogism and Stereo Syllogism", "content": "Syllogisms, a core component of traditional logic used\nin philosophical reasoning, consist of a major premise (a general statement), a minor premise\n(a specific statement), and a conclusion. The structure of a syllogism, or its figure, depends\non the positioning of the middle term (M), subject (S), and predicate (P). The complex\nstructure of syllogisms has led many researchers to examine its effects on eliciting System 1\nand System 2 reasoning For our dataset, we selected singular nouns as seed words to guide the GPT-4\nmodel in generating the major and minor premises while preserving the order of M, P, and\nS. The four primary syllogistic figures are: (1) Premise: M is P. S is M. Conclusion: S is P.\n(2) Premise: P is M. S is M. Conclusion: S is P. (3) Premise: Mis P. Mis S. Conclusion:\nS is P. (4) Premise: P is M. M is S. Conclusion: S is P. Finally, we rephrased the premises\nand conclusions to produce the final items. We also created a variant of syllogism questions\nusing seed words from StereoSet, referred to as Stereo Syllogism."}, {"title": "Guilt", "content": "Drawing on studies that demonstrate dual-process effects when participants are\nasked to assess the guilt of a suspect in a hypothetical scenario (Bergius et al., 2020; Kassin\net al., 2013; Peer & Gamliel, 2013; Rachlinski et al., 2015; Wistrich et al., 2015), we designed\nquestions using the following template: First, we prompted GPT to generate a few sentences\ndescribing the details of a crime for which the perpetrator has not been identified. We then\nasked GPT-4 to list several features likely to be true of the culprit. This enabled us to\ncreate two forms of questions: the entailment form (E Guilt), which included the crime\ndescription, a suspect characterized by features that made them appear likely to be guilty,"}, {"title": "Primacy and Recency", "content": "The first and last pieces of information presented can dispro-\nportionately influence the reader's perception due to what is known as the serial position\neffect (Murdock, 1962). To incorporate questions leveraging this effect, we began by man-\nually writing several sentences that simply stated an individual's name and a characteristic\n(e.g., \"Simon is a professor\u201d). We then asked GPT-4 to generate five sentences about that\nindividual that would likely be true if the characteristic were accurate (likely-true). We\nselected two sentences that best aligned with our archetypal conceptions of the characteris-\ntic. Next, we asked GPT-4 to generate five sentences about that individual that were likely\nnot true (likely-false) and selected three of these. We then created two forms of questions:\nP_Primacy/Recency questions had premises starting and ending with sentences from the\nlikely-true category, with three likely-false sentences in the middle, and the original sen-\ntence describing the individual and characteristic as the conclusion. N Primacy/Recen\u0441\u0443\nquestions had the same premise, but the conclusion was the original sentence in a negated\nform (e.g., \"Simon is not a professor\" instead of \"Simon is a professor\")."}, {"title": "3.3 Dataset Characteristics", "content": "We gathered 45 items across all item types (10 StereoNLI, 7 Fallacy, 8 Syllogism/Stereo-\nSyllogism, 10 Guilt, and 10 Primacy/Recency), each annotated 30 times by humans during\nthe first phase (System 1) and an additional 30 times by the same individuals in the second\nphase (System 2). Figure 7 provides a quantitative comparison of voting preferences be-\ntween System 1 and System 2, highlighting how the distribution of votes across six options\nchanged from System 1 to System 2. Notably, option E exhibited the most significant shift,\ndecreasing by 11.07 percentage points."}, {"title": "Inclusion and Ethics Statement", "content": "Our study was designed and conducted with careful\nconsideration for ethical research practices and inclusivity. We recruited 60 participants via\nProlific, aiming for a diverse sample. All participants provided informed consent and were\nclearly instructed about the study's nature. Data privacy and protection measures were\nimplemented to ensure participant anonymity and confidentiality. We incorporated breaks\nto reduce fatigue and set time limits to manage cognitive load, considering participant\nwell-being. The study design included various item types to prevent monotonicity and\nreduce potential biases. We used AI models (GPT-4) to generate survey items, addressing\npotential data contamination issues and creating a novel dataset. We have been transparent\nabout our methodology, including the use of AI-generated content and both proprietary and\nopen-source language models in our experiments. By adhering to these ethical principles\nand inclusive practices, we aim to contribute to the field of AI and cognitive science research\nin a responsible and equitable manner."}, {"title": "4 Modeling Experiments", "content": "The overarching aim of our modeling experiments is to predict how human votes are dis-\ntributed across each item. We begin by predicting the mode of the distribution (majority\nvote, as discussed in Section 4.1), then proceed to estimate the variance among the votes\n(Section 4.2), and ultimately aim to predict the entire distribution of human votes (Section\n4.3). None of these tasks can be considered inherently easier than the others due to the\nsignificant variability in human reasoning, as highlighted in Section 3.3. Given our devel-\nopment of the 6-way scheme, which includes six labels as detailed in Section 3, we also\ncompare the performance of LLMs on the more conventional NLI scheme: the 3-way clas-\nsification (contradiction, neutral, and entailment). It is important to note that we collect\nhuman responses solely for the 6-way scheme and subsequently map these responses to the\n3-way scheme by categorizing A and B as contradiction, C and Das neutral, and E and F\nas entailment. All LLMs are tested separately on both the 3-way and 6-way schemes. We\nutilize both proprietary LLMs (GPT-3.5, GPT-4 , GPT-40-mini )\nand open-source LLMs (Gemma 2 27b Llama 3.1 8b, Llama 3.1 70b, Llama\n3.1 405b, Mistral 7b with direct performance optimization to compare their performance against traditional (non-transformer-based)\nmachine learning algorithms.\nTo predict the entire vote distribution using LLMs, we must prompt them in a way that\nallows the recreation of a vote distribution rather than producing a single vote per item.\nWe do not fine-tune the LLMs because we do not have enough data to tune millions of\nparameters that we would need to tune even with techniques such as parameter-efficient\nfine-tuning (Ding et al., 2023). All the LLMs we use are chatbot models, which include\na system prompt that influences the LLM's responses and a user prompt (both described\nbelow) that contains the specific query. We maintain consistency in the user prompt by\nlimiting it to just the premise and hypothesis (we call these s1 and s2 in our prompts, not"}, {"title": "4.1 Are human responses predictable?", "content": "The task of predicting the mode (also referred to as the gold label) of human responses\ncan be approached as a classification problem, since all categories are different from one\nanother. While it can also be framed as a regression problem, this approach presents several"}, {"title": "4.2 Is the variance in human responses predictable?", "content": "After determining that we can predict the human gold label on both System 1 and System\n2 with accuracy exceeding the majority baseline, we proceed to predict the variance among\nall human responses for a given item. This variance is calculated by mapping the human\nresponses onto a scale from 1 to 6, as described in Section 4.1. It is important to note that,"}, {"title": "4.3 Can AI mimic the entire human response distribution?", "content": "Given the contrasting results obtained from our initial two experiments, further investiga-\ntion is required to evaluate the ability of LLMs to emulate human reasoning. Additionally,\nwhile maxima and variance provide insight into a distribution, they represent only two of\nits characteristics. To more comprehensively evaluate the similarity between the response\ndistributions of LLMs and humans, we now focus on comparing the entire response distribu-\ntion. The Wasserstein Distance (Dobrushin, 1970), also known as Earth Mover's Distance\n(EMD), serves as a metric for quantifying the difference between probability distributions\nacross a specified metric space in this instance, the set of labels in a 3-way or 6-way clas-\nsification scheme. Conceptually, if each distribution is visualized as a unit mass of earth,\nthe EMD reflects the minimal cost required to transform one distribution into the other,\nconsidering both the amount of earth that needs to be moved and the mean distance it\nmust be moved. This makes EMD particularly suitable for our case, as it is sensitive to the\nordinal nature of the metric.\nSince EMD requires a probability distribution, we transform each set of human responses\ninto a vector of size k (three or six for the 3-way or 6-way scheme), where each entry\nrepresents the frequency of the corresponding label. We then normalize this vector to\ncreate a probability distribution. Notably, EMD has an unbounded range, so we convert\nit into a similarity measure ranging from 0 to 1, which we term Earth Mover's Similarity\n(EMS). Our similarity function, defined as:\n$EMS(D_1, D_2) = 100^{EMD(D_1, D_2)}$\ntakes the two normalized probability distributions D\u2081 and D\u2082 as inputs. While e is often\nused as the base for exponents, we chose a base of 100 in this study to better distinguish\nvariations, as using e would compress the values of interest into a narrow range.\nIn our comparison, we previously evaluated LLMs against traditional ML algorithms\ntrained on our dataset. However, these algorithms do not generate a distribution of votes. In\nthis experiment, we assign weights to each of the ten personality prompts and use a genetic\nalgorithm to identify the optimal set of weights for each of the five folds (as in Experiments\n4.1 and 4.2). For each fold, we again conduct a grid search to find the best hyperparameters\n(parent selection type and crossover type) for training the genetic algorithm, while fixing\nthe number of generations at 8, the population per generation at 256, and the number\nof mating parents at 128. These parameters were chosen to balance computational costs"}, {"title": "5 Conclusion", "content": "This study advances the field of AI reasoning by shifting the focus from merely predicting a\nsingle answer chosen by the majority of humans to modeling the full distribution of human\nreasoning responses. Predicting this distribution a challenge we call the \"full spectrum\nreasoning problem\" is not just a more comprehensive approach; it is essential for devel-\noping AI systems that can more accurately reflect the diversity and complexity of human\nthought. This is particularly important in applications where understanding the range of"}, {"title": "Appendix B. Prompts for Dataset Creation", "content": "Here we outline all our prompts used to create items in our dataset for transparency. In\nthe OpenAI API, a system prompt sets the initial instructions and context for guiding the\nmodel's behavior, while a human prompt represents the user's input or query during the\nconversation. All premises and conclusions are rephrased using the system prompt: \"You\nare a writing assistant for a linguist named Steve. Steve comes to you because you are\ngood at writing sentences that are understandable and grammatically correct. Steve will\ngive you a sentence. You need to rephrase the sentence such that it is easier to understand.\nPreserving all information is not necessary, but preferred. Just write the sentence and\nnothing else before or after it.\" and the human prompt: \"Sentence: {sentence}\"."}]}