{"title": "SELF-EVOLVING MULTI-AGENT COLLABORATION NETWORKS FOR SOFTWARE DEVELOPMENT", "authors": ["Yue Hu", "Yuzhu Cai", "Yaxin Du", "Xinyu Zhu", "Xiangrui Liu", "Zijie Yu", "Yuchen Hou", "Shuo Tang", "Siheng Chen"], "abstract": "LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development. To address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network. To extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose rSDE-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness. Our experiments show that: i) The automatic requirement-aware evaluation in rSDE-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark. ii) EvoMAC outperforms previous SOTA methods on both the software-level rSDE-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities. The benchmark can be downloaded at https://yuzhu-cai.github.io/rSDE-Bench/.", "sections": [{"title": "1 INTRODUCTION", "content": "Automatic software development focuses on generating code from natural language requirements. Code is a universal problem-solving tool, and this automation presents significant potential to provide substantial benefits across all areas of our lives Li et al. (2022a). Recently, the industry has introduced several large language model (LLM)-driven coding assistants, including Microsoft's Copilot Microsoft (2023), Amazon's CodeWhisperer Amazon (2022), and Google's Codey Google (2023). These coding assistants significantly advance human efficiency and yield considerable commercial benefits. Despite the initial success of LLMs in assisting with line-level coding, they struggle to tackle more complex coding tasks. This limitation stems from the restricted reasoning abilities of single LLMs and their lack of capacity for long-context understanding Wang et al. (2024a); Li et al. (2024a); Wang et al. (2024b).\nTo handle function-level coding tasks, numerous multiple language agent collaboration (MAC) systems have been proposed Li et al. (2023); Hong et al. (2023); Chan et al. (2024); Islam et al. (2024); Yang et al. (2024b); Li et al. (2022b); Osika (2023). These MAC systems function as LLM-driven agentic workflow. They follow human-designed standardized operating procedures to divide the complex coding tasks into simpler subtasks within the workflow, allowing each agent to conquer specific subtasks. These MAC systems significantly advance coding capabilities from line-level to function-level tasks. However, current MAC systems rely on heuristic designs. These human-crafted static systems have two inherent limitations: i) their performance is confined to human initialization. Given the diversity of real-world coding tasks, human design cannot fully address the specific needs of each task; and ii) they lack the flexibility to adapt to new tasks. This rigidity necessitates that researchers and developers manually decompose tasks and create prompts. The complexity of this process inhibits effective human optimization for adapting to new challenges."}, {"title": "2 RELATED WORKS", "content": "LLM-based multi-agent collaboration. LLM-driven multi-agent collaboration (MAC) systems Xu et al. (2023); Hua et al. (2023); Ziems et al. (2024); Wu et al. (2023); Hong et al. (2023); Chan et al. (2024); Mandi et al. (2024a) enable multiple agents to share information and collaboratively complete the overall task. These MAC systems function as agentic workflows. They have demonstrated enhanced problem-solving capabilities in various domains, such as mathematics Islam et al. (2024),"}, {"title": "3 EvoMAC: SELF-EVOLVING MULTI-AGENT COLLABORATION NETWORK", "content": "This section presents EvoMAC, a novel self-evolving multi-agent collaboration network and its application to software development. The key feature of EvoMAC is its ability to iteratively adapt both agents and their connections during test-time for each task, mimicking the back-propagation process, a core algorithm in neural network training. We first formulate a general self-evolving paradigm in Sec. 3.1 and then describe its application to software development in Sec. 3.2."}, {"title": "3.1 A GENERAL SELF-EVOLVING PARADIGM VIA TEXTUAL BACKPROPAGATION", "content": "Multi-agent collaboration network. A multi-agent collaboration (MAC) network is a computational graph representing agentic workflows, where multiple agents empowered by LLMs interact as interconnected nodes to coordinate and share information for complex task-solving. The intuition behind to divide the complex task into more specific and manageable subtasks for each agent, allowing the overall task to be gradually conquered through the agentic workflow. Mathematically, we represent a MAC network with N autonomous agents as a directed acyclic graph A = (V, E), where V = {v_i}_{i=1}^N is the set of N nodes, and E = {e_{i,j}}_{i,j \\in [1,...,N],i\\neq j} is the set of directed edges with no circles. The i-th node v_i represents the i-th autonomous agent with the prompt p_i, which specifies its subtask. The edge e_{i,j} represents the task dependency between the i-th agent and the j-th agent, indicating that the j-th agent's subtask should be executed after the i-th agent's subtask in the agentic workflow. The overall graph topology specifies the agentic workflow. Analogy to traditional neural networks, agents function similarly to neurons, with agent prompts serving as neurons' weights and the agentic workflow as the layers and connections.\nThe feed-forward pass of MAC network is the execution of the agentic workflow. In this process, each agent is given two inputs: the initial task requirement and the output from the previous agent. Using these, each agent produces an output that fulfills its specific subtask. Eventually, the last agent's generation constitutes the final output, integrating all completed subtasks. Note that the initial task requirement is input to each agent as context, providing supplementary details to aid in the implementation of each subtask.\nRecently, various MAC networks have been designed using human expertise to assign fixed agent prompts and workflows Hong et al. (2023); Chan et al. (2024), resembling untrained neural networks. However, these designs solely rely on human priors and lack adaptability, causing limited performance improvement over a single agent. To overcome this, inspired by neural network training, we propose a self-evolving paradigm for multi-agent collaboration networks, enabling both agents and their connections to dynamically evolve during test-time for each given task.\nOptimization problem. Here we consider a general generation task. During test-time, given a task, the MAC network performs a feed-forward pass to generate the final output without knowing its"}, {"title": "3.2 SELF-EVOLUTION FOR SOFTWARE DEVELOPMENT", "content": "In this section, we apply the self-evolving paradigm to the task of software development. The overall architecture of the proposed self-evolving multi-agent collaboration network for software development is illustrated in Fig. 1. Given a coding task, the coding team, corresponding to the MAC network A_g, generates all the codes through its forward-pass; the testing team, associated with the MAC network A_t, is responsible for creating the target proxy; that is, unit tests of the coding task; and the objective environment tool is realized through the compiler. The identified bugs during execution form the textual environmental feedback. The updating team, consisting of two collaborative agents, manages the textual backpropagation. By continuously cycling through feed-forward, feedback collection, and textual backpropagation processes, the coding team is iteratively refined to more closely align with the test cases. The detailed implementation of agents can refer to Sec. 9 in the Appendix."}, {"title": "4\nRSDE-BENCH: REQUIREMENT-ORIENTED SOFTWARE DEVELOPMENT\nENGINEERING BENCHMARK", "content": "This section introduces rSDE-Bench, a requirement-oriented benchmark designed to assess the ability of models to handle software-level coding tasks. Each coding task involves multiple detailed software requirements. These requirements specify each functionality and constraint of the software, item by item, serving as measurable benchmarks for assessing the software's effectiveness. As shown in Fig. 3, unlike previous instruction-oriented approaches Qian et al. (2023); Hong et al. (2023) which rely on brief instructions as input, rSDE-Bench uses comprehensive software requirements as input, complemented by unit test cases to automatically evaluate the correctness. This benchmark provides software-level coding tasks and automatic evaluation, aligning more closely with real-world software development practices."}, {"title": "4.1 BENCHMARK CONSTRUCTION", "content": "rSDE-Bench involves two typical real-world software types: game and website. They can reflect different coding capacities demanded in realistic software development. Game often requires handling dynamic interactions, real-time state changes, and user-driven operations, focusing on elements like logic execution, initialization, and game state transitions. Website emphasizes static and dynamic content management, user interaction through forms and buttons, and ensuring page elements are displayed and functional. rSDE-Bench involves diverse requirements, each paired with a test case. Specifically, rSDE-Bench provides 53 unique coding tasks and 616 test cases. For details on the"}, {"title": "4.2 AUTOMATIC EVALUATION", "content": "rSDE-Bench supports automatic evaluation of requirement correctness. It achieves this by pairing a specifically designed black-box test case with each requirement. The test case can directly verify whether the generated code achieved the requirement. Its evaluation metric is the accuracy, which quantifies the proportion of correctly passed test cases. It is similar to the pass@1 metric in HumanEval Chen et al. (2021), which evaluates the pass ratio of correctly achieved functions against the total functions via unit test verification. It is a fully automated evaluation process, eliminating the need for human involvement while still providing accurate and reliable assessments.\nPrevious benchmarks for software code generation mainly rely on two evaluation methods. One method is human evaluation Hong et al. (2023), which is time-consuming and not scalable for large datasets. The other method is indirect evaluations Qian et al. (2023), which defines metrics like consistency, completeness, and quality. Consistency measures how closely the generated software aligns with the original requirement description by comparing the cosine similarity between the two. Completeness is determined by detecting the presence of placeholder (such as pass or TODO), which results in a binary value of 0 or 1. Quality is then calculated as the product of several factors: consistency, completeness, and executability. As illustrated in Fig. 3, they could not measure the correctness of the generated code in fulfilling requirements. In contrast, rSDE-Bench's test cases-based evaluation is more rigorous and precise. These test cases can accurately verify the correctness of generated code in fulfilling the requirements. rSDE-Bench promises reliable and scalable automatic evaluation. In the experiments, we have validated the significant advantages of the proposed automatic evaluation over the previous metrics, including consistency and quality; see Fig. 4."}, {"title": "4.3 FEATURES", "content": "Challenging and diverse software requirements. rSDE-Bench features long-context software requirements (averaging 507/1011 words for game and website tasks, respectively), unlike instruction-oriented benchmarks Chen et al. (2021); Austin et al. (2021); Jimenez et al. (2023) that rely on brief prompts. These detailed requirements better reflect real-world lengthy and complex software development challenges.\nRequirement-aware precise and efficient evaluation. rSDE-Bench employs detailed software requirements and automated unit tests to precisely measure how well generated software meets its objectives. Generated codes are evaluated based on pass rates from running specific test cases, offering an accurate and efficient process. In contrast, instruction-oriented benchmarks rely on brief"}, {"title": "5 EXPERIMENTS", "content": "5.1\nEXPERIMENTAL SETUP\nBaselines. To validate the effectiveness of our EvoMAC, we conducted comparisons against both single-agent and multi-agent baselines. The single-agent baselines involve three prominent large models: GPT-40-Mini (gpt-40-mini), Claude-3.5-Sonnet (claude-3-5-sonnet-20240620), and Gemini (gemini-1.5-flash). For multi-agent baselines, we included five state-of-the-art (SOTA) methods: MetaGPT Hong et al. (2023), Autogen Wu et al. (2023), Mapcoder Islam et al. (2024), Agentverse Chen et al. (2023), and ChatDev Qian et al. (2023). To ensure a fair comparison, all multi-agent baselines, including our EvoMAC, are powered by the efficient and powerful GPT-40-Mini model. Additionally, to demonstrate the adaptability and robustness of our EvoMAC, we developed two EvoMAC variants using GPT-40-Mini and Claude-3.5-Sonnet.\nDatasets. Our experiments cover both the proposed rSDE-Bench and the standard coding benchmark HumanEval Chen et al. (2021). HumanEval comprises 164 Python function completion problems, where the task is to generate code from a single function description."}, {"title": "5.2 EFFECTIVENESS OF RSDE-BENCH'S EVALUATION AND EVOMAC", "content": "rSDE-Bench's automatic evaluation metric (accuracy) is highly aligned with human evaluation. Our primary goal is to validate the effectiveness of the proposed automatic evaluation in rSDE-Bench by comparing it with two existing evaluation metrics: consistency and quality, both from SRDD Qian et al. (2023). For a fair comparison, our golden standard is human evaluation, conducted by two expert code engineers who manually verify the fulfillment of requirements by interacting with the developed software. This process is tedious, taking around four hours per expert to evaluate the entire benchmark. The effectiveness of an evaluation metric depends on how closely it aligns with human evaluation."}, {"title": "5.3 EFFECTIVENESS OF EVOLVING", "content": "EvoMAC continuously improves with the evolving times. Fig. 6 shows that as evolving iterations increase, performance consistently improves across all five dataset settings, covering two difficulty levels, two software types, and both requirement-oriented and function complement benchmarks. This highlights the effectiveness, generalizability, and robustness of the self-evolving approach, encouraging EvoMAC to evolve whenever possible.\nEvoMAC indistinguishably improves with different driving LLM. From Fig. 6, we see that: i) both EvoMAC variants continuously improve with evolving iterations, demonstrating the robustness of the self-evolving design; ii) the two curves do not intersect, indicating that the EvoMAC variant powered by a more powerful single model consistently outperforms the other, highlighting the advantage of using a stronger model. Success builds on success."}, {"title": "5.4 ABLATION STUDY", "content": "To assess the effectiveness of each component, Tab. 2 details an ablation study featuring seven EvoMAC variants.\nEffectiveness of objective environment feedback. Environment feedback, such as code execution logs, is essential for software development. Variant f) omits this tool, instead using an LLM-driven agent to critique the code. Comparing Variant g) with Variant f) shows a notable performance drop: Website tasks decrease by 12.67% and 14.97%, and Game tasks by 21.74% and 18.28% for Basic and"}, {"title": "5.5 CASE STUDY", "content": "Fig. 8 presents the generated code by a single agent, GPT-40-Mini, multi-agent systems, ChatDev, and our EvoMAC before and after evolving (iteration=0/1). We see that: i) EvoMAC after evolving can correct issues from previous iterations and successfully fulfill the task requirements; ii) multi-agent systems tend to better comprehend the task requirements and produce more well-structured code. More generated software can refer to Sec. 10 in the Appendix."}, {"title": "6\nCONCLUSION", "content": "We propose EvoMAC, a novel self-evolving paradigm for MAC networks. EvoMAC iteratively adapts agents and their connections during the testing phase of each task. It achieves this with a novel textual back-propagation algorithm. EvoMAC can push coding capabilities beyond function-level tasks and into more complex, software-level development. Furthermore, we propose rSDE-Bench, a novel requirement-oriented software development benchmark. rSDE-Bench features both complex and diverse software requirements, as well as the automatic evaluation of requirement correctness. Comprehensive experiments validate that the automatic requirement-aware evaluation in rSDE-Bench aligns closely with human evaluation. EvoMAC outperforms previous SOTAs in both software-level rSDE-Bench and function-level HumanEval benchmarks.\nFuture works. In the future, we plan to introduce a reward model to enhance the self-evolving paradigm's ability to learn from feedback and extend the rSDE-Bench to more software types."}, {"title": "8 ALGORITHM", "content": "In this section, we present the algorithm of EvoMAC in Alg. 1. For more details, please refer to Section 3."}, {"title": "9 PROMPTS", "content": "In this section, we present the agents' prompts in EvoMAC, including coding organizer (Tab. 6), coding agent (Tab. 7), testing organizer (Tab. 8), testing agent (Tab. 9), gradient agent (Tab. 10), and update agent (Tab. 11)."}, {"title": "10 SOFTWARE PRESENTATION", "content": "In this section, we show some games and websites written by EvoMAC. Fig. 12 and Fig. 13 present the games and websites respectively. We see that: i) EvoMAC outputs games with well-written GUI"}]}