{"title": "Large Language Models for Interpretable Mental Health Diagnosis", "authors": ["Brian Hyeongseok Kim", "Chao Wang"], "abstract": "We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP). Having a CDSS is important because of the high complexity of diagnostic manuals used by mental health professionals and the danger of diagnostic errors. Our CDSS is a software tool that uses an LLM to translate diagnostic manuals to a logic program and solves the program using an off-the-shelf CLP engine to query a patient's diagnosis based on the encoded rules and provided data. By giving domain experts the opportunity to inspect the LLM-generated logic program, and making modifications when needed, our CDSS ensures that the diagnosis is not only accurate but also interpretable. We experimentally compare it with two baseline approaches of using LLMs: diagnosing patients using the LLM-only approach, and using the LLM-generated logic program but without expert inspection. The results show that, while LLMs are extremely useful in generating candidate logic programs, these programs still require expert inspection and modification to guarantee faithfulness to the official diagnostic manuals. Additionally, ethical concerns arise from the direct use of patient data in LLMs, underscoring the need for a safer hybrid approach like our proposed method.", "sections": [{"title": "1 Introduction", "content": "Mental disorders impose a significant burden on the affected individuals and their communities (Gorvin and Brown 2012). Accurate diagnosis is a critical first step toward improving patient outcomes and fostering societal well-being. In clinical settings, the diagnostic process relies on matching a patient's symptoms with the mental health diagnostic rules outlined in official manuals such as DSM-5-TR (American Psychiatric Association 2022) and ICD-11 CDDR (World Health Organization 2024). These manuals, consisting of more than 1,000 pages of natural language descriptions, serve as authoritative references for not only mental health professionals but also insurance companies. However, their complexity poses a significant challenge, not only exacerbating the workload of already overburdened mental health professionals but also increasing the risk of diagnostic errors (American Psychological Association 2023). This underscores the pressing need for developing a robust clinical decision support systems (CDSS), a software tool that can verify the diagnosis made manually. Yet, such tools remain underdeveloped, particularly those that address issues of reliability and interpretability.\nRecent advancements in large language models (LLMs) suggest their potential in many applications (Friha et al. 2024) including clinical settings (Ullah et al. 2024). Thanks to their excellent processing and understanding of natural language, they can generate diagnostic suggestions based on medical literature and patient data. However, their adoption in clinical settings still faces challenges. For example, LLMs are prone to issues like hallucinations (Huang et al. 2023; Bai et al. 2024), lack of explainability (Zhao et al. 2023), lack of consistency (Moore, Deshpande, and Yang 2024), and limited proficiency in complex reasoning (Huang and Chang 2023). To date, no existing approach effectively combines LLMs with mechanisms that guarantee accuracy and interpretability in the context of mental health diagnosis, thus leaving a critical gap.\nTo fill the gap, we propose a method that combines LLMs with constraint logic programming (CLP), leading to a practical tool for assisting clinicians in making mental health diagnosis. Specifically, our method leverages LLMs to translate natural language descriptions of mental health diagnostic criteria from manuals such as DSM-5-TR and ICD-11 CDDR to logic rules, thus reducing the cognitive burden on domain experts. Simultaneously, we use an off-the-shelf CLP engine for solving the logic rules to ensure that the diagnostic output is verifiably correct, while enhancing interpretability through the explicitly defined rules and objec-"}, {"title": "2 Background", "content": "In this section, we review the background information of psychological diagnosis and constraint logic programming."}, {"title": "2.1 Psychological Diagnosis", "content": "Psychological diagnosis is the process by which clinicians assess if a patient's symptoms meet the criteria for specific disorders as outlined in the diagnostic manuals such as DSM-5-TR and ICD-11 CDDR. These authoritative manuals are widely adopted, underscoring their global relevance and importance. As an example, consider the following ICD-11 CDDR diagnostic criteria for schizophrenia:\nAt least two of the following symptoms must be present (by the individual\u2019s report or through observation by the clinician or other informants) most of the time for a period of 1 month or more. At least one of the qualifying symptoms should be from items (a) to (d) below:\n[List of symptoms from (a) to (g)... (omitted for brevity)].\nClinical decision support systems (CDSS) are a specific type of DSS (Keen and Scott Morton 1978) where patient data and medical knowledge are integrated to the software tool, to assist clinicians with decision-making. CDSS can address various scenarios such as offering diagnostic support, identifying drug interactions, and predicting treatment outcomes (Berner 2007). The focus of this work is offering diagnostic support in the context of mental disorders. Note that CDSS is merely a support system, as the name implies; it aims at helping clinical professionals in decision-making instead of replacing their decision-making role entirely."}, {"title": "2.2 Constraint Logic Programming", "content": "Constraint logic programming (CLP) is a paradigm that focuses on expressing logical rules of desired computations as opposed to implementing these computations. It is well-suited for applications where accuracy and transparency are critical, as it focuses more on what should be computed rather than how to compute it, thus enabling easier verification of correctness and logical soundness. In our work, we use Datalog as the CLP language, and solve Datalog programs using Souffl\u00e9, a state-of-the-art Datalog engine (Jordan, Scholz, and Suboti\u0107 2016; Scholz et al. 2016).\nListing 1 shows an example logic program that codifies the rules that infer the Path relation from the Edge relation. It starts with declarations of the two relations (Lines 1-2). Then, it specifies the input and the output (Lines 3-4). Finally, it defines the rules for inferring Path from Edge (Lines 5-6). Specifically, Line 5 means that Path(x,y) holds if Edge(x,y) holds, and Line 6 means that Path (x,y) holds if both Path(x,z) and Edge (z,y) hold. The comma (,) in Line 6 denotes logical AND, whereas a semicolon (;) denotes logical OR.\nGiven a set of facts, e.g., Edge from 1 to 2 and Edge from 2 to 3, the program in Listing 1 computes all entries of the Path relation: from 1 to 2, from 2 to 3, and from 1 to 3. This is how the program can answer queries, e.g., whether Path (1,3) holds. Similarly, we want to use Datalog to express ICD-11 CDDR diagnostic rules, and then answer queries for individual patients. This leads to a verifiably correct and explainable CDSS for mental disorders."}, {"title": "3 Methodology", "content": "In this section, we present our Datalog encoding of diagnostic rules and LLM-based translation of text to rules."}, {"title": "3.1 Datalog Encoding of the Diagnosis", "content": "We focus on ICD-11 CDDR diagnostic rules, but this can be done similarly for DSM-5-TR.\nThe Diagnostic Rules Listing 2 shows a Datalog program with rules that connect a patient's symptoms and past conditions to a mental disorder. Lines 4-6 specify the input and output relations. For brevity, we omit the declarations of intermediate relations, but they also require the decl keyword with variable types, similar to Lines 1-3. The program first extracts the patient's name from Observed and add it to a relation called AllPatients (Line 7), and then identifies which symptoms are core (must be present) or qualifying (can be present) according to the diagnostic criteria. Given a set of symptoms A, B, C, and D, for example, Symptoms A and B may be considered core whereas Symptoms C and D may be considered qualifying, and they must have been observed for more than 2 weeks (Lines 8-9).\nThe program has rules that count the number of symptoms in each category, which requires an aggregate function called count (Lines 10 and 12). If Core or Qual relations do not exist, the count is set to 0 (Lines 11 and 13). Once we have the counts for the symptoms, we add them up (Line 14). Finally, we decide if a patient should be given the Diagnosis of \"DisorderD\" (Line 15). Here, the diagnosis requires at least 1 core symptom and at least 2 symptoms in total (i.e., one core and one qualifying, or two core symptoms), and at least one occurrence of \"ConditionC\" in prior history.\nPatient Information The Datalog program in Listing 2 requires facts that describe the patient as input. These facts are expressed using the Observed and History relations. Observed indicates that Patient is experiencing Symptom for the duration of Week (Line 1). History indicates that Patient has a history of Condition for the Count\u00b9 number of times (Line 2).\nConsider the follwing example of \"PatientA\", who has been observed with \"SymptomA\u201d and \u201cSymptomB\" for 3.5 weeks, and has a prior history of \"ConditionC\" two times. The corresponding input facts are as follows:\n\u2022 Observed (\u201cPatientA\u201d, \u201cSymptomA", "SymptomB\", 3.5)\n\u2022 History(\u201cPatientA": "ConditionC\"", "DisorderD\\\" as shown in Line 15 of Listing 2.\"\n    },\n    {\n      \"title\": \"3.2 LLM-Based Translation of Manuals to Rules\",\n      \"content\": \"We prompt LLMs to translate the text-based diagnostic criteria from the ICD-11 CDDR manual into candidate logic programs in Datalog, similar to the program shown in Listing 2. Then, we assess whether the LLM-generated logic program can diagnose a given patient correctly. In-context learning (ICL) (Brown et al. 2020; Dong et al. 2022) allows LLMs to perform tasks better without explicitly updating the model parameters. As part of ICL, we provide an example of diagnostic criteria text from ICD-11 CDDR and its corresponding Datalog program, such that the models can learn from the demonstrated task. Our one-shot prompt template is as follows:\nSystem: You are an expert at translating mental health diagnostic criteria into a Datalog program in Souffl\u00e9. The patient data is given as input to the program as Observed and History relations. The patient diagnosis is returned as output from the program as Diagnosis relation. Explain the relations.\"\n    },\n    {\n      \"title\": \"4 Evaluation\",\n      \"content\": \"We use ICD-11 CDDR diagnostic manual (World Health Organization 2024) and focus on four mood disorders: Bipolar I (BPD1), Bipolar II (BPD2), Single Episode Depressive Disorder (SEDD), and Recurrent Depressive Disorder (RDD). From natural language descriptions of the diagnostic criteria, our method generates the candidate Datalog program as described in Section 3.1. Then, we manually inspect the LLM-generated Datalog program and correct errors to ensure that the finalized Datalog program accurately encodes the diagnostic rules.\nWe also manually validated the diagnosis results of the Datalog program. Given a dataset of 30 patients, the finalized Datalog program identified 9 patients with BPD1, 8 with BPD2, 5 with SEDD, and 4 with RDD. Four patients remained undiagnosed, as they did not meet the criteria for any of the considered mood disorders. We validated the program's results by cross-checking the patient data against the diagnostic criteria specified in the manual. Details of all 30 patients can be found in Appendix B.\"\n    },\n    {\n      \"title\": \"4.1 The Experimental Setup\",\n      \"content\": \"Given the time and effort required for manual rule translation and diagnosis validation, we aim to assess whether state-of-the-art LLMs can automate this process without compromising accuracy. Our main research questions are:\nRQ1. How accurate are the diagnostic outputs generated by the LLM-translated programs?\nRQ2. To what extent can LLMs accurately interpret and translate diagnostic criteria from text into Datalog?\nRQ3. How much additional human effort is required to correct errors in the LLM-translated programs?\nRQ4. How effective are LLMs in diagnosing a patient when given their data directly?\nWe used 3 state-of-the-art LLMs. GPT stands for GPT-4o on OpenAI, released in May 2024 (OpenAI 2024). Gemini stands for GEMINI-1.5-FLASH on Google Cloud, released in May 2024 (Gemini Team, Google 2024). Llama stands for LLAMA-3.2 on Meta AI, released in September 2024 (Llama Team, AI @ Meta 2024). These LLMs were accessed between October and November 2024.\"\n    },\n    {\n      \"title\": \"4.2 Two Baseline Approaches\",\n      \"content\": \"To evaluate our method, we developed two groups of baselines for comparison. The first group of baselines (LLM-only) involves directly providing a diagnosis given the patient data. This is analogous to using LLMs as an external consultant that can either validate or challenge a clinician's diagnosis (Wang et al. 2024). For this task, we use the following prompt template:\nSystem: You are an expert at diagnosing patients according to the ICD-11 CDDR. The considered disorders are [List of mood disorders": ".", "relations.\nTask": "Please output the diagnosis for the following patients. Patients with no clear diagnosis should be indicated as such. Include patient data.\nThe second group of baselines (LLM + Datalog) is running the LLM-translated Datalog programs without expert intervention. This approach is comparable to testing LLM-generated code in imperative programming (Jiang et al. 2024). These baselines follow the same prompt structure described in Section 3.2. Our method extends these baselines by incorporating expert corrections to address syntactic and semantic errors in the LLM-generated code. We refer to these expert-corrected programs as Our CDSS."}, {"title": "4.3 Experimental Results", "content": "Table 1 compares the performance of our method against the baselines across 10 patients. The remainder of this section will discuss the results for the first 10 patients. Results for all 30 patients can be found in Appendix A.\nColumns 1-2 list patient numbers and their disorders based on our manually written Datalog program, validated against the ICD-11 CDDR criteria. Columns 3-5 (LLM-only) show diagnoses directly provided by the LLMs, while Columns 6-8 (LLM + Datalog) show diagnoses from LLM-generated Datalog programs without expert intervention. Column 9 (Our CDSS) shows diagnoses from an expert-corrected LLM-generated program. Green cells indicate correct diagnoses, yellow cells indicate partial correctness (correct diagnosis with additional incorrect ones), and the final row summarizes correct diagnoses per method.\nAnswer to RQ1 To address RQ1 on the accuracy of LLM-translated programs, we look at LLM + Datalog columns for the as-is versions and the Our CDSS column for the expert-corrected version. Among the as-is programs, GPT performs the best with 7 correct diagnoses out of 10, followed by Gemini with 2 correct and 2 partially correct, and Llama with 3 correct. This pattern holds across all the patients, where GPT achieves 22 correct diagnoses out of 30, while Gemini has 8 correct and 4 partially correct, and Llama only 9 correct. We extend the most accurate program generated by GPT and implement logical changes to align with the ICD-11 CDDR criteria for the mood disorders. The expert-reviewed program, shown in Column 9, produces 10 correct diagnoses out of 10 (30 out of 30).\nAnswer to RQ2 To address RQ2 on the performance of LLMs in translating diagnostic criteria into Datalog programs, we take a closer look at the programs that correspond to Columns 6-8.\nAlthough GPT-generated program achieves the greatest number of correct diagnoses, it relies solely on History in its final diagnostic rules, despite constructing intermediate rules to identify mood episodes based on Observed. This issue arises from the diagnostic text's phrasing, which specifies a \"history of certain mood episodes as a requirement for a particular mood disorder. While clinicians would intuitively consider current symptoms for diagnosis, GPT interprets the text literally, lacking the nuanced understanding needed for accurate clinical interpretation.\nThe Gemini-generated code presents the opposite issue, where it only considers current symptoms, ignoring the patient's history. This leads to missed diagnoses such as Patient 1, where the current symptoms suggest no diagnosis (denoted '-' under Column 7), despite the patient's history indicating BPD2. This may stem from using a Datalog program for schizophrenia in the prompt, which doesn't incorporate History, which is specific to mood disorders. Additionally, Gemini-generated program frequently diagnoses conflicting disorders (e.g., \u201cBPD1, BPD2\" for Patients 3 and 5), which contradicts the diagnostic criteria that require mutually exclusive conditions. BPD1 requires a mixed or manic episode; BPD2 explicitly requires the absence of such episodes.\nThe Llama-generated code often misdiagnoses patients as BPD1 (e.g., Patients 2, 4, 6, 8, and 9). This stems from a lack of intermediary logic to properly identify mood episodes. The generated program counts associated symptoms without distinguishing core or qualifying symptoms and uses an arbitrary threshold that doesn't align with the ICD-11 criteria, leading to frequent BPD1 diagnoses, as it has the lowest threshold.\nOverall, the inconsistency across models suggests that while LLM-generated code shows promise, it is not yet reliable for direct clinical use. Models could benefit from more text-to-rule translation examples for ICL to generate more accurate programs. Additionally, breaking down the task into smaller steps through multi-turn conversation (Zheng et al. 2024) or Chain-of-Thought (CoT) prompting (Wei et al. 2022) could enhance their logical reasoning. Models could be fine-tuned for this task by experts with reinforcement learning from human feedback (RLHF) (Ouyang et al. 2022). Finally, using LLMs optimized for code generation (e.g., GitHub Copilot and Amazon Q Developer?) could improve the performance.\nAnswer to RQ3. As discussed, LLM-generated programs do not guarantee that the encoded logic accurately replicates the diagnostic criteria of ICD-11 CDDR. To address this, our method proposes a pipeline where LLMs generate candidate Datalog programs, which are then reviewed and refined by experts. This approach aims to balance the efficiency of AI with the crucial need for diagnostic accuracy. In this context, it is important to address RQ3, which examines the additional human effort required to accurately represent the logic of the diagnostic criteria.\nListing 3 highlights some of the changes made to the GPT-generated code, addressing two major issues. First, the original definition of MixedEpisode (Line 2) created a cyclic dependency by requiring both ManicEpisode and DepressiveEpisode for its definition. However, according to the ICD-11 CDDR manual, a mixed episode should be defined independently of these episodes and based on specific symptom thresholds. Furthermore, the logic expressed in the program contradicted clinical guidelines, since depressive or manic episodes should not apply if the symptoms qualify better as a mixed episode. The revised code resolves this by incorporating the absence of a mixed episode as part of criteria for other mood episodes (Line 1) and redefining MixedEpisode to directly evaluate symptom counts and core criteria (Line 2). This approach eliminates the cyclic dependency and ensures compliance with clinical guidelines. In order to achieve this, we manually added several missing intermediate relations and rules to accurately identify and count core and qualifying symptoms for different mood episodes.\nSecond, the original logic for diagnosing disorders relied solely on History (Lines 3-4). As discussed, this approach neglected the possibility of diagnosing based on present mood episodes. The corrected code addresses this by checking for the presence of the current mood episodes based on Observed symptoms (Line 5). Overall, these corrections ensure that the diagnosis logic is more comprehensive and aligns with clinical practice.\nThe final corrected version of the GPT-generated code passes for all 10 (30) patients. In total, 57 lines were added and 10 removed from the initial 107 lines of code (LoC), resulting in a final 154 LoC. The first set of corrections-addressing cyclic dependencies and clinical inconsistencies-required the addition of 47 LoC and removal of 6, reflecting changes that demanded significant domain expertise. In contrast, modifying the diagnosis logic to incorporate present mood episodes added 10 LoC and removed 4, which were relatively straightforward adjustments.\nThese statistics highlight the varying levels of effort"}, {"title": "Answer to RQ4", "content": "To answer RQ4, which evaluates the effectiveness of LLMs in diagnosing patients directly, we revisit Table 1 under the LLM-only columns. Among the tested models, GPT leads with 9 correct diagnoses out of 10, followed by Gemini with 8 and Llama with 7. This trend extends across all patients, where GPT leads with 22 correct diagnoses out of 30, followed by Gemini and Llama with 19 each.\nDirectly using LLMs for diagnosis generally results in higher accuracy than relying on LLM-generated candidate programs. However, the variability in model performance highlights significant challenges. LLMs inherently rely on probabilistic predictions rather than logical proofs, making it difficult to guarantee consistency and accuracy required in medical contexts. Furthermore, their complex architectures make them hard to interpret. Unlike LLM-generated logic programs, which offer transparent reasoning steps, the direct diagnoses provided by LLMs remain opaque, even when correct. Finally, there are always ethical implications and privacy concerns of providing real patient data to LLMs, which complicates their direct application in healthcare.\nInstead, our proposed method of combining LLMs with constraint logic programming offers a promising alternative. LLMs can be leveraged to generate interpretable logical rules that determine if a patient meets specific diagnostic criteria. This approach reduces ethical concerns by avoiding direct input of sensitive patient data into LLMs and takes advantage of the increasing availability of code-generative LLMs. Moreover, logic programs are inherently transparent and interpretable, as their rules can be manually verified for alignment with diagnostic standards of the manuals."}, {"title": "5 Related Work", "content": "To the best of our knowledge, our proposed method is the first method for combining LLMs and constraint logic programming to provide a clinical decision support system (CDSS) in the context of mental health diagnosis. Other recent studies have explored the use of LLMs in mental health contexts, including developing chat-based counselors (Liu et al. 2023), analyzing emotions (Yang et al. 2023), and predicting mental states from online text (Xu et al. 2024). However, these works do not extend to creating diagnostic tools or integrating logic-based reasoning to support clinical decision-making.\nBeyond clinical applications, there is ongoing research on LLMs for logical reasoning, such as translating text into specifications for Boolean satisfiability (SAT) solvers (Ye et al. 2023) or evaluating their reasoning capabilities in mathematical and strategic domains (Imani, Du, and Shrivastava 2023; Zhang et al. 2024). While these efforts demonstrate LLMs' potential for logic-based tasks, they do not address the use of logic programming languages like Datalog in clinical settings.\nPrior work on using logic in CDSS take an ontological approach of structured knowledge representations for diagnosis (Casado-Lumbreras et al. 2012), or apply satisfiability modulo theory (SMT) solvers and theorem provers to detect conflicts in medical treatments (Bowles et al. 2019). While these works address relevant clinical needs, they precede the advent of modern LLMs and do not incorporate logic programming. Our work bridges these gaps by leveraging LLMs to generate interpretable logic programs for mental health diagnosis, offering a unique combination of efficient AI techniques and explainable logic-based computation to assist clinicians."}, {"title": "6 Conclusion", "content": "We have presented a method that explores the novel application of constraint logic programming for clinical decision support systems (CDSS) in mental health diagnosis and critically evaluates the role of LLMs in this domain. Our evaluation demonstrates that while LLMs show promise in diagnostic tasks, they face significant limitations when used directly, including challenges in consistency, interpretability, and ethical considerations related to patient data. By leveraging LLMs for code generation and combining their capabilities with expert inspection, we propose a safer and more reliable approach that ensures diagnostic logic aligns with clinical criteria. Our findings underscore the importance of systems that balance the efficiency of LLMs with the rigor of expert validation. Future work will explore domain-specific fine-tuning of LLMs, evaluate the approach on real-world datasets, and extend the Datalog encoding to address more nuanced diagnostic criteria and specifiers."}, {"title": "Ethical Statement", "content": "The proposed CDSS aims at helping clinical professionals in decision-making. It is not meant to replace or refute the diagnoses provided by qualified clinicians. All evaluations and decisions regarding diagnoses must be conducted in accordance with the expertise of trained professionals. The hypothetical data and modeling used in this study are intended for proof of concept and are not meant to substitute for real patient data, which may be more complex."}, {"title": "C.1 Translating ICD-11 CDDR Manual to Datalog Rules", "content": "System: You are an expert at translating mental health diagnostic criteria into Souffl\u00e9 Datalog code. Translate the given criterion into a.dl program using Souffl\u00e9 syntax as follows. The patient information is given as input to the program as Observed and History relations. The patient diagnosis is returned as output from the program as Diagnosis relation.\n\u2022 .decl Observed(Patient:symbol, Symptom:symbol, Week:float) describes that Patient has experienced Symptom for Week number of weeks.\n\u2022 .decl History(Patient:symbol, Condition:symbol, Count:number) describes that Patient has experienced Condition for Count number of times.\n\u2022 .decl Diagnosis(Patient:symbol, Disorder:symbol) describes that Patient has been diagnosed with Disorder.\nFor context, here is an example of Scizophrenia criterion translated into Souffl\u00e9 .dl code.\n\u2022 Scizhophrenia criterion: [Scizhophrenia criterion from ICD-11 CDDR].\n\u2022 Relevant symptom names for Observed relation: [Symptom names]\n\u2022 Souffl\u00e9.dl code: [Manually crafted Datalog program for Schizophrenia]\nUser: Now, translate the following criteria into Souffle .dl code for Bipolar I, Bipolar II, Single Episode Depressive Disorder, and Recurrent Depressive Disorder.\n\u2022 Mood Episode criterion: [Depressive, Manic, Mixed, and Hypomanic Episode criteria from ICD-11 CDDR].\n\u2022 Mood Disorder criterion: [Bipolar I, Bipolar II, Single Episode Depressive Disorder, and Recurrent Depressive Disorder criteria from ICD-11 CDDR].\n\u2022 Relevant symptom names for Observed relation: [Symptom names]\n\u2022 Relevant condition names for History relation: [Condition names]"}, {"title": "C.2 Generating Diagnosis by LLM-only Approach", "content": "System: You are an expert at diagnosing patients according to the ICD-11 Clinical Descriptions and Diagnostic Requirements (CDDR). The patient data are represented by a list of current symptoms denoted as Observed and a list of history denoted as History. Observed matches the patient with the symptom and the number of weeks it has been observed. History matches the patient with the condition and the number of times it existed. No record for a patient means that there is no related data for them. The considered disorders are: Bipolar I, Bipolar II, Single Episode Depressive Disorder, and Recurrent Depressive Disorder.\nUser: For brevity, please output only the diagnosis for the following patients. Patients with no clear diagnosis should be indicated as such.\n\u2022 Observed: [Observed Data]\n\u2022 History: [History Data]"}]}