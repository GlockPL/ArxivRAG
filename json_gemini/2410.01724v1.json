{"title": "AUTO-DEMO PROMPTING: LEVERAGING GENERATED OUTPUTS AS DEMONSTRATIONS FOR ENHANCED BATCH PROMPTING", "authors": ["Longyu Feng", "Mengze Hong", "Chen Jason Zhang"], "abstract": "Batch prompting is a common technique in large language models (LLMs) used to process multiple inputs simultaneously, aiming to improve computational efficiency. However, as batch sizes increase, performance degradation often occurs due to the model's difficulty in handling lengthy context inputs. Existing methods that attempt to mitigate these issues rely solely on batch data arrangement and majority voting rather than improving the design of the batch prompt itself. In this paper, we address these limitations by proposing \u201cAuto-Demo Prompting,\u201d a novel approach that leverages the question-output pairs from earlier questions within a batch as demonstrations for subsequent answer inference. We provide a formal theoretical analysis of how Auto-Demo Prompting functions within the autoregressive generation process of LLMs, illustrating how it utilizes prior outputs to optimize the model's internal representations. Our method effectively bridges the gap between batch prompting and few-shot prompting, enhancing performance with only a slight compromise in token usage. Experimental results across five NLP tasks demonstrate its effectiveness in mitigating performance degradation and occasionally outperforming single prompts. Furthermore, it opens new avenues for applying few-shot learning techniques, such as demonstration selection, within batch prompting, making it a robust solution for real-world applications.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs), such as GPT (Brown et al., 2020), and PaLM (Chowdhery et al., 2023), have demonstrated an extraordinary ability to perform in-context learning (ICL), where they utilize provided examples or contextual information to adapt and solve a wide range of downstream tasks. This capability enables LLMs to generalize from few-shot or even zero-shot examples without requiring task-specific fine-tuning, significantly enhancing their versatility across diverse applications (Song et al., 2023). The success of ICL in these models highlights their potential as powerful tools for natural language processing and as adaptable frameworks for learning in dynamic, dataconstrained environments, offering broader implications for machine learning and AI research.\nRecently, the batch prompting method has attracted growing research interest (Cheng et al., 2023; Lin et al., 2024; Fan et al., 2024), where models are presented with a set of homogeneous questions - queries that share similar structure or content - within a single prompt. The main objective of batch prompting is to enhance interaction efficiency with LLMs by reducing computational costs, especially by minimizing the number of tokens processed. By grouping similar questions, this technique streamlines the model's handling of multiple tasks, optimizing resource usage while ensuring consistent performance across repeated or related queries. As illustrated in Figure 1 (a), the model processes multiple sentences with similar structures in a single batch, identifying and correcting grammatical errors for each input sentence. This demonstrates how batch prompting reduces token usage when working with structurally similar tasks. Moreover, advancements in hardware and algorithmic design have further expanded the capacity of LLMs to retain and process longer input contexts, enabling more effective batch prompting (Munkhdalai et al., 2024; Chen et al., 2023; Ainslie et al., 2023).\nWhile batch prompting provides significant advantages in terms of efficiency, it also presents a fundamental challenge, as handling lengthy context inputs is particularly difficult for LLMs based on transformers. Transformer models experience significant performance degradation when accessing relevant information from the middle of long contexts, leading to a decline in overall effectiveness (Liu et al., 2024). This issue becomes more pronounced in tasks that require LLMs to process long contextual inputs. The root of the challenge lies in the quadratic complexity of the self-attention mechanism within transformers, where computational costs increase dramatically with input length. As batch prompting is applied, the combined input length grows, exacerbating this issue and further degrading model performance. Therefore, developing a more effective batch prompting strategy is crucial for mitigating these performance limitations and unlocking the potential of large-scale applications in in-context learning, particularly for tasks involving long sequences of input data.\nFrom the internal representations of LLMs, we observe that batch prompting shares similarities with few-shot prompting, a technique that has proven crucial in enhancing the performance of LLMs by providing a few example demonstrations (Brown et al., 2020). However, an intriguing contradiction emerges: while few-shot prompting typically boosts performance, batch prompting often results in performance degradation. It was found that batch prompting can sometimes outperform single prompts in tasks with smaller batch sizes (Cheng et al., 2023), yet this line of investigation remains underexplored. This raises a key question: Could we bridge the gap between batch prompting and few-shot prompting to leverage the benefits of both?\nInterestingly, previous research has found that even when incorrect labels are present in few-shot demonstrations during in-context learning, the decrease in accuracy is generally minimal, typically between 0% and 5% (Min et al., 2022). This suggests that using model-generated answers as demonstrations in few-shot prompts is a viable approach, regardless of the potential for model hallucination or limitations that may lead to incorrect answers.\nIn this paper, we tackle the performance degradation problem from the perspective of connecting few-shot prompting with batch prompting. A key insight emerges: the outputs generated during earlier iterations of autoregressive LLMs can, with proper design, be automatically recognized as demonstrations for subsequent text generation, without needing to explicitly include them in the prompt. Building on this, we propose \u201cAuto-Demo Prompting,\u201d a novel batch prompting technique that instructs the model to repeat each question before answering it. The model then au-"}, {"title": "AUTO-DEMO PROMPTING", "content": "Prompt engineering is essential for unlocking the capabilities of large language models (Marvin et al., 2023). Typically, a standard prompt consists of a task description and a single data point, leading to the development of various prompting techniques to enhance LLM performance on downstream tasks (Sahoo et al., 2024; Besta et al., 2024; Wang et al., 2023; Wei et al., 2022). However, relying on single prompts limits inference to just one data point, making this approach less efficient for large-scale datasets or real-world applications. In contrast, Batch Prompting allows for the processing of multiple data points in a single inference. Cheng et al. (2023) suggests that while batch prompting performs well with smaller batch sizes, there is a tendency for performance to decline as batch sizes grow. Additionally, Lin et al. (2024) noted that varying the order of batch data can yield different results, proposing the integration of these outcomes through majority voting to enhance overall performance.\nUnlike previous works, Auto-Demo Prompting aims to maximize the potential of batch prompting and enhance its performance during single inference of large language models. This method guides LLMs to generate a question-answer pair for each input question in the batch, rather than simply providing an answer. By innovating the design of batch prompts with a new output control format, Auto-Demo Prompting ensures that LLMs maintain a consistent output structure across all questions. A key factor in this process is the autoregressive generation mechanism of the decoder in LLMs, which facilitates coherent and contextually relevant output generation."}, {"title": "AUTOREGRESSIVE GENERATION PROCESS IN LARGE LANGUAGE MODEL", "content": "Large Language Models can be classified into three categories: encoder-only, encoder-decoder, and decoder-only. The mainstream LLMs are primarily decoder-only and encoder-decoder models. The GPT series developed by OpenAI, including notable examples such as GPT-3, GPT-4, and GPT-40, consists entirely of decoder-only models and marks significant milestones in the evolution of"}, {"title": "AUTOREGRESSIVE GENERATION OF DEMONSTRATIONS", "content": "As illustrated in Figure 2, Auto-Demo Prompting creatss a loop where demonstrations are generated in an autoregressive manner during the inference of LLMs. This technique guides LLMs to produce question-answer pairs that align with the format of demonstrations used in context learning. Due to the autoregressive nature of token generation, the previously generated output is appended to the current input for the next step, serving as a demonstration for all subsequent questions in the batch. When the batch size is N, there will be {1, 2, 3, ..., N \u2013 1} demonstrations formed in the process.\nSuppose the batch size of Auto-Demo Prompting is denoted by N. The batch of questions can be represented as \\(Q = \\{q_1,q_2,...,q_N\\}\\), and the corresponding answers are denoted by \\(A = \\{a_1,a_2,...,a_n\\}\\). Let F denote the inference of the LLM. To illustrate the relationships among current batch prompting, Auto-Demo Prompting, and few-shot demonstrations in in-context learning, we compare the formulations of the LLM inference process in these prompting approaches."}, {"title": "Method 1 Few-Shot Prompting", "content": "Formulation: \\(F(a_n|Q_{1:n-1}, \\{(q_i, a_i)\\}_{i=1}^{n-1}) \\forall n \\in \\{0,1,..., N\\}\\)\nExample:\n1: \\(F(a_1/q_1)\\)\n2: \\(F(a_2|(q_1,a_1))\\)\n3:\n4: \\(F(a_n|(q_1,a_1), (q_2, a_2),..., (q_{n-1}, a_{n-1}))\\)"}, {"title": "Method 2 Batch Prompting (BP)", "content": "Formulation: \\(F(a_n|BP + Q_{1:n}, \\{a_i\\}_{i=1}^{n-1}) \\forall n \\in \\{0,1,..., N\\}\\)\nExample:\n1: \\(F(a_1 BP +q_1)\\)\n2: \\(F(a_2 BP+q_1,q_2, a_1)\\)\n3:\n4: \\(F(a_n BP+q_1q_2... q_n, a_1a_2...a_{n-1})\\)"}, {"title": "Method 3 Auto-Demo Prompting (ADP)", "content": "Formulation: \\(F((q_n, a_n)|ADP + Q_{1:n-1}, \\{(q_i, a_i)\\}_{i=1}^{n-1}) \\forall n \\in \\{0,1,...,N\\}\\)\nExample:\n1: \\(F((q_1,a_1) ADP +q_1)\\)\n2: \\(F((q_2, a_2)|ADP+q_1,q_2, (q_1, a_1))\\)\n3:\n4: \\(F((q_n, a_n)|ADP + q_1q_2... q_{n-1}, (q_1, a_1), (q_2, a_2), ..., (q_{n-1}, A_{n-1}))\\)"}, {"title": "BATCH DATA SELECTION", "content": "In the proposed Auto-Demo Prompting framework, batch data selection a conventional technique for improving batch prompting performance - can be viewed as demonstration selection. As illustrated in Method 3, the batched data (i.e., questions) is also present within the question-answer pairs generated by the LLM, which serve as demonstrations for subsequent questions. Therefore, we propose the following hypothesis: the selection of batched data in Auto-Demo Prompting achieves a similar effect to the selection of demonstrations in few-shot prompting."}, {"title": "Batch Data Selection \u2248 Demonstration Selection", "content": "Based on this hypothesis, demonstration selection methods can be effectively adapted for batch data selection in Auto-Demo Prompting. Current approaches to demonstration selection are diverse, utilizing criteria such as similarity, mutual information, perplexity, and diversity (Yang et al., 2023). Notably, demonstration selection based on text similarity has proven effective for text pair classification and multiple-choice tasks (Peng et al., 2024; Su et al., 2024).\nInspired by demonstration selection method, we designed the \"Batch Data Selection with Retrieval\u201d algorithm to identify similar questions within a single batch. As detailed in Algorithm 1, this algorithm features a data retrieval loop aimed at gathering similar data into one batch. For each batch with size N, a target data point is randomly selected. Subsequently, the N 1 most similar data points are identified using an embedding model to calculate the pairwise similarity."}, {"title": "EXPERIMENTS", "content": "We conducted comparative experiments between Auto-Demo Prompting and conventional batch prompting across various NLP tasks, including question answering (BoolQ), mathematical reasoning (GSM8K, SVAMP), textual entailment (RTE), and paraphrase detection (Quora Question Pairs, QQP). Specifically, the batch data selection experiments were focused on the RTE and QQP datasets to assess the effectiveness of our approach."}, {"title": "EXPERIMENTAL SETTINGS", "content": "We evaluate Auto-Demo Prompt with varying batch sizes over GPT-40-mini and GPT-40. The prompts used for the different datasets are provided in Appendix A.\nEmbedding Model for Batch Data Selection: The embedding model utilized is \"iic/nlp_corom_sentence-embedding_english-base,\" available on the ModelScope platform. This model is a dual-tower text representation architecture that employs the CoROM model as its foundation for the pre-trained language model. The training data is derived from the official open-source MS MARCO Passage Ranking dataset (Bajaj et al., 2018).\nParameters: All experiments were conducted in an in-context learning setting, without any model training or fine-tuning. The temperature for model inference was consistently set to 0 to ensure uniformity in responses. Batch sizes were set to 1/16/32 for the gpt-40-mini, which has a maximum output token limit of 16k, and to 8 or 16 for the gpt-40, with a maximum output token limit of 8k."}, {"title": "RESULTS AND DISCUSSIONS", "content": "As shown in Figure 3, the proposed method, highlighted in red, consistently outperforms Batch Prompt in most experiments, allowing LLMs to generate longer outputs without compromising performance. This highlights the beneficial impact of Auto-Demo Prompting, where the generated question-answer pairs enhance the accuracy of subsequent questions within the same batch. Notably, when applied with GPT-40 on the GSM8K dataset, Auto-Demo Prompt achieved an accuracy of 95.7% with a batch size of 16, surpassing the 95.3% accuracy of the single prompt (batch size = 1). A similar trend was observed with the SVAMP dataset, demonstrating the effectiveness of Auto-Demo Prompting in improving the performance of conventional batch prompts.\nOur findings align with previous research that highlights few-shot prompting as an effective strategy for enhancing model accuracy, particularly in datasets that necessitate sophisticated reasoning steps (Wei et al., 2022). The proposed method demonstrated significant effectiveness on the BoolQ dataset, which consists of naturally occurring yes/no questions that often require complex reasoning. In contrast, the effectiveness of Auto-Demo Prompt is less pronounced in simpler datasets like"}, {"title": "BATCH DATA SELECTION EXPERIMENTS", "content": "To validate the hypothesis stated in Equation 2, we conducted experiments on the RTE and QQP datasets to evaluate the effectiveness of selecting similar questions for batch data selection within the Auto-Demo Prompting framework. As shown in Figure 4, the 'Batch Data Selection with Retrieval' algorithm consistently improves accuracy compared to randomly selected batched data. The results of these experiments closely align with the effectiveness of few-shot demonstration selection based on similarity, as both approaches lead to significant performance improvements.\nSurprisingly, the results indicate that experiments utilizing larger batch sizes significantly outperform those employing a single prompt (i.e., batch size 1). For the RTE dataset, the Auto-Demo Prompting approach with a batch size of 48 using the gpt-40-mini model achieved an accuracy of 89.5%, surpassing the 88.8% accuracy obtained with a single prompt. Similarly, the gpt-4o model with a batch size of 16 achieved an accuracy of 90.3%, exceeding the 88.4% accuracy of the single prompt approach. In the QQP dataset, the batch size of 32 with the gpt-40 model yielded an accuracy of 87.3%, which is 2.0% higher than the result from the single prompt. These findings suggest that the proposed method not only addresses the limitation of performance degradation associated with larger batch sizes, but further enhances overall performance by leveraging previous questions as demonstrations in the autoregressive generation process. This improvement aligns closely with the effects of many-shot learning and underscores the importance of incorporating multiple examples in the prompting process."}, {"title": "DISCUSSION", "content": "The results of our experiments underscore the significant potential of Auto-Demo Prompting. These findings corroborate our earlier observations regarding equations 1 and 2, demonstrating that Auto-Demo Prompting consistently enhances Batch Prompting across most experimental scenarios. It is particularly noteworthy the performance improvement observed when batch data selection is applied. By grouping similar data within each batch, we found that Auto-Demo Prompting with larger batch sizes outperformed configurations utilizing a batch size of one, which is the standard prompt used in mainstream LLM applications. This advancement signifies a promising avenue for achieving both enhanced performance and increased efficiency.\nFurthermore, recent research by (Agarwal et al., 2024) demonstrates that many-shot demonstrations can outperform few-shot demonstrations, a trend observed across various domains (Jiang et al., 2024; Moayedpour et al., 2024). As the input and output length limits of LLMs continue to expand, the adoption of a more effective batch data selection method is likely to enhance these performance gains, further solidifying the benefits of our approach. Given the diverse and numerous demonstration selection methods available for in-context learning, the optimal choice can vary across different datasets. This variability encourages future research to evaluate their effectiveness and to develop novel approaches that are better suited for the Auto-Demo Prompting."}, {"title": "RELATED WORK", "content": "Batch prompting has emerged as a significant area of research in LLMs to facilitate efficient batch processing of many data points simultaneously. Cheng et al. (2023) first introduced the concept of batch prompting to reduce computational costs, with experiments focusing on batch sizes of fewer than six demonstrating comparable performance to standard prompting. Building on this, Lin et al. (2024) proposed \"BatchPrompt,\u201d which highlights the variability in results caused by different data orders and introduces a self-consistency method to address these discrepancies. With its cost-saving benefits, batch prompting has been applied in various NLP applications; for instance, Fan et al. (2024) proposed a framework utilizing a covering-based demonstration selection strategy for entity resolution, effectively balancing matching accuracy and computational cost, while Zhang et al. (2023) explored the impact of batch size on several data preprocessing tasks."}, {"title": "DEMONSTRATION SELECTION IN IN-CONTEXT LEARNING", "content": "To improve the performance of in-context learning, previous studies have explored the optimization of the selection and arrangement of few-shot demonstrations (Rubin et al., 2022; Zhang et al., 2022; Wu et al., 2023; Fu et al., 2023; Zhou et al., 2023). It was found that the choice of demonstrations is both data-dependent and model-dependent, leading to the proposal of a selection method that considers both factors (Peng et al., 2024). Empirical research by (Min et al., 2022) has demonstrated that preserving the structured format of demonstrations, which consists of text-label pairs, is"}, {"title": "CONCLUSION", "content": "In this paper, we introduce the Auto-Demo Prompt, a novel batch prompting method aimed at improving the performance of batch prompting. We provide a comprehensive overview of its operational mechanism and clarify the key concepts that establish the relationship between few-shot demonstrations in in-context learning and batch prompting. Importantly, we show that 'Batch Data Selection' can be conceptualized as 'Demonstration Selection' within the framework of Auto-Demo Prompting, facilitating the transfer of insights from few-shot learning research to our approach. Our extensive experiments validate the effectiveness of Auto-Demo Prompting, underscoring its significance as the input and output lengths of LLMs increase. As these lengths expand, the importance of efficient and high-performing batch prompting methods, such as Auto-Demo Prompting combined with Batch Data Selection, will become increasingly pronounced. Further research is encouraged to investigate demonstration selection in greater detail, as well as to explore the potential synergies between Auto-Demo Prompting and other emerging techniques such as prompt learning and explainable AI. By delving deeper into these areas, we can uncover new insights that may enhance the adaptability and effectiveness of LLMs, ultimately paving the way for advancements in their applications toward achieving artificial general intelligence (AGI)."}]}