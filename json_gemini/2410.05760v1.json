{"title": "TRAINING-FREE DIFFUSION MODEL ALIGNMENT WITH SAMPLING DEMONS", "authors": ["Po-Hung Yeh", "Kuang-Huei Lee", "Jun-Cheng Chen"], "abstract": "Aligning diffusion models with user preferences has been a key challenge. Existing methods for aligning diffusion models either require retraining or are limited to differentiable reward functions. To address these limitations, we propose a stochastic optimization approach, dubbed Demon, to guide the denoising process at inference time without backpropagation through reward functions or model retraining. Our approach works by controlling noise distribution in denoising steps to concentrate density on regions corresponding to high rewards through stochastic optimization. We provide comprehensive theoretical and empirical evidence to support and validate our approach, including experiments that use non-differentiable sources of rewards such as Visual-Language Model (VLM) APIs and human judgements. To the best of our knowledge, the proposed approach is the first inference-time, backpropagation-free preference alignment method for diffusion models. Our method can be easily integrated with existing diffusion models without further training. Our experiments show that the proposed approach significantly improves the average aesthetics scores for text-to-image generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models have been the state-of-the-art for image generation (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021; Karras et al., 2022; Saharia et al., 2022; Rombach et al., 2022), but, commonly, the end users' preferences and intention diverge from the data distribution on which the model was trained. Aligning diffusion models with diverse user preferences is an ongoing and critical area of research.\nOne approach to aligning diffusion models with user preferences is to fine-tune using reinforcement learning (RL) to optimize the models based on rewards signals that reflect the user preferences (Black et al., 2023; Fan et al., 2023). However, retraining the model every time when the preference changes is computationally expensive and time-consuming.\nAn alternative approach is to guide the denoising process using a differentiable reward function. This can be done through classifier guidance at inference time (Dhariwal & Nichol, 2021; Wallace et al., 2023b; Bansal et al., 2024) or backpropagation at training time (Prabhudesai et al., 2024; Clark et al., 2024; Xu et al., 2023). These methods are generally less resource-demanding and more efficient. While these methods are generally more efficient, they require the reward function to be differentiable. This limits the types of reward sources that can be used, as it excludes the non-differentiable sources like third-party Visual-Language Model (VLM) APIs and human judgements.\nTo address these limitations, we propose Demon, a novel stochastic optimization approach for preference optimization of diffusion models at inference time. Demon is a metaphor from Maxwell's Demon, an imaginary manipulator of natural thermodynamic processes. The core ideas are: (1) Quality of noises that seed different possible backward steps in a discretized reverse-time Stochastic Differential Equation (SDE) can be evaluated given a reward source; (2) Such evaluation enables us to synthesize \"optimal\u201d noises that theoretically and empirically improve the final reward of the generated image through stochastic optimization. Specifically, we leverage Probability Flow Ordinary Differential Equation (PF-ODE) (Song et al., 2021) or Consistency Model (CM) (Song et al., 2023; Luo et al., 2023) to help us efficiently evaluate the possible backward steps, seeded with different Gaussian noises."}, {"title": "2 RELATED WORK", "content": "Diffusion Model. Diffusion models for data generation were first proposed by Sohl-Dickstein et al. (2015), further developed for high-fidelity image generation by Ho et al. (2020), and generalized by Song et al. (2021) through the lens of SDEs. Karras et al. (2022) comprehensively studied the design space of Diffusion SDEs. In this work, we base many of the derivations on theirs. Furthermore, we focus on evaluating our method in the text-to-image generation setting (Rombach et al., 2022; Ho & Salimans, 2021; Podell et al., 2024)\nHuman Preference Alignment. Aligning models with human preferences has been studied with several approaches:reinforcement learning-based policy optimization (Fan et al., 2023; Yang et al., 2024; Black et al., 2023); training with reward backpropagation (Clark et al., 2024; Xu et al., 2023); backpropagation through the reward model and the diffusion chain (Prabhudesai et al., 2024; Wallace et al., 2023b; Bansal et al., 2024). Many metrics and benchmarks for evaluating alignment has also been proposed, including those by Xu et al. (2023); Kirstain et al. (2023); LAION (2023); Wu et al. (2023), and we use these either as optimization objectives or evaluation of the generated image. In Table 1, we further provide detailed comparisons of the proposed Demon approach with relevant existing methods in the literature from different aspects."}, {"title": "3 PRELIMINARY", "content": "Score-Based Diffusion Model. We base our derivation on EDM (Karras et al., 2022). With a sampling schedule $\\sigma_t = t$, we can write the reverse-time SDE sampling towards the diffusion marginal distribution as follows.\ndxt = [-tx log p (xt, t) \u2013 \u03b2t\u00b2\u2207x, log p(xt, t)] dt + \u221a2\u03b2t dwt, (1)\nf(xt,t)\ng\u03b2(t)\nwhere p(xt,t) = p(x0, 0) \u25ca N (0, t\u00b2In) and \u2297 denotes the convolution operation. x0 is a clean sample, $x_0 \\sim P_{data}$, and $x_t$ is a noisy sample at time t. \u03b2 expresses the relative rate at which existing noise is injected with new noise. In EDM, \u03b2 is a function of t, but in our study, we set \u03b2 to a constant for all t for simplicity. Essentially, f\u03b2(x, t) corresponds to drift and g\u03b2(t) corresponds to diffusion. As common in diffusion models, since p(xt, t) \u2248 N(0, t2IN) for a large enough t, we sample xtmax ~ N(0, tmax In) as the initial sample.\nA comprehensive list of the notations and conventions used in this paper is provided at Appendix A."}, {"title": "4 REWARD-GUIDED DENOISING WITH DEMONS", "content": "In this section, we describe how Demon works in two steps: Section 4.1 explains the process of scoring Gaussian noises in reverse-time SDE with a reward function; Section 4.2 further explains how the noise scoring allows us to guide the denoising process to align with the reward function, which is what we refer to as Demon."}, {"title": "4.1 SCORING NOISES IN REVERSE-TIME SDE", "content": "Let xo be the clean image corresponds to a xt at time step t, say:\nX = Xt + \u222btf(xu, u) du + gp(u) dwu, (2)\nwhere Equation (2) is denoted as x0 |\u03b2 xt, shorthanded as x0 | xt. For an arbitrary reward function r e.g. aesthetics score, we define the reward estimate of xt at time step t as\nrs(xt,t) := Exoxt [r(x0)]. (3)\nThis can be estimated with a Monte Carlo estimator by averaging over the reward of several SDE samples, but it requires many sample evaluations for high accuracy. To address this weakness, we introduce an alternative estimator for r\u03b2(xt, t) based on PF-ODE Song et al. (2021).\nAs shown in Song et al. (2021); Karras et al. (2022), the reversed-time SDE reduces to PF-ODE when \u03b2 = 0. For each t, a diffeomorphic relationship exists between a noisy sample xt and a clean sample xo generated by PF-ODE."}, {"title": "4.2 DEMONS FOR REWARD-GUIDED DENOISING", "content": "Let's first revisit reverse-time SDE. Following Karras et al. (2022), an SDE numerical evaluation of xt-\u25b3 sampled from xt can be seeded by noise z via a step of Heun's 2nd order method (Ascher & Petzold, 1998) as follows:"}, {"title": "4.2.1 \u03a4\u0391NH DEMON", "content": "Intuitively, we may consider up-weighting the good noises that improve the reward and down-weighting the bad noises that harm the reward, compared to the average reward \u00b5. Tanh Demon assigns positive weights to the good noises and negative weights to the bad noises with the tanh function, based on the reward estimates of the noises (Section 4.1):\n t \nz* = VN normalized(\u2211kanh (Ok) where tanhtanh(oc()-)\nk=1\n(11)\nHere we can estimate with , T is the temperature parameter to tanh, which can be adaptively tuned (as shown in Table 8).\nIn the following, we demonstrate that synthesizing z* with Equation (11) in every backward step, which nudges the sample towards the data distribution, leads to reward improvement of the final clean sample x0 with theoretical guarantee."}, {"title": "4.2.2 BOLTZMANN DEMON", "content": "Another intuitive approach is to estimate the candidate with maximum reward. We propose the Boltzmann demon, which assign noise weights as follows.\nboltz exp (roc()/T)\nbboltz (k)(12)\nk=1exp (roc(x)/T)\nThe theoretical guarantee of improvement in r\u03b2 in expectation is provided in Lemma 3 of Appendix, assuming r\u03b2 = r o c. This method is equivalent to the single-step cross entropy approach (De Boer et al., 2005). Although, empirically we find that Tanh Demon outperforms Boltzmann demon, adjusting \u0442 in Boltzmann demon provides control over deviation from the original SDE distribution, as demonstrated in Lemma 4 (Appendix)."}, {"title": "4.2.3 \u0421\u043eMPUTATIONAL CONSIDERATIONS", "content": "Let's first consider a Demon sampling trajectory Xt1 > Xt2 > \u2026 > Xtr \u2248 0 for a fixed number T. Each Demon's trajectory requires O(K \u00b7T) evaluations of c, and each evaluation comes with one reward estimation. The compute time is mainly influenced by the implementation of roc. We discuss two aspects of roc-the temporal cost and the fidelity-which are vital to the algorithm's time complexity and reward performance, respectively."}, {"title": "5 EXPERIMENTS", "content": "In this section, we present both quantitative and qualitative evaluations of our methods. Due to the page limit, we include the details of the implementation and experimental settings in Appendix H and the subjective results in Appendix F.2.\nBaseline Comparison. For the performance comparisons between our method and other baselines, we use the LAION (2023) aesthetics scores (Aes) as the evaluation metric, and the scores are evaluated on a set of various prompts for generating animal images, which were selected from a subset of 22 common animals in ImageNet-1K (Deng et al., 2009). We use 20-step Heun's ODE for reward estimate for our methods and Best-of-N (SD v1.4). In Figure 4, we can observe that the proposed Tanh Demon sampling method, in most cases, outperforms other baseline methods, including our Boltzmann Demon sampling method, Best-of-N, and DOODL (Wallace et al., 2023b), the state-of-the-art inference-time method. It is worth noting that, although given more number of reward queries, the performance of DOODL eventually surpasses Tanh, if we consider the same amount of execution time, Tanh is still consistently better. This is because of the computational cost associated with DOODL's backpropagation through the diffusion model chain. In addition, we empirically observe more reward hacking with DOODL (based on backpropagation) compared to our method (see Table 3), though the underlying reason isn't as clear. For further comparison on PickScore (Kirstain et al., 2023), please refer to Appendix E.1."}, {"title": "6 CONCLUSION", "content": "This work addresses the challenge of better aligning pre-trained diffusion models without training or backpropagation. We first demonstrate how to estimate noisy samples' rewards based on clean samples using PF-ODE. Additionally, we introduce a novel inference-time sampling method, based on stochastic optimization, to guide the denoising process with any reward sources, including non-differentiable reward sources that includes VLMs and interactive human judgements. Theoretical"}, {"title": "A.1 \u039d\u039f\u03a4ATIONS", "content": ""}, {"title": "A.2 CONVENTIONS", "content": ""}, {"title": "B GUIDELINE ON PARAMETER SETTING", "content": "We explore the optimal setting for parameter 7 with respect to the Boltzmann Demon and the Tanh Demon. For the Tanh Demon, the most effective T is neither \u221e nor 0. We recommend setting T to the standard deviation of the estimations {(rc)(x)}=1, rendering it an adaptive parameter that is robust to scaling. For the Boltzmann Demon, optimal performance is achieved by setting T to 0, as demonstrated in Table 8.\nWe also conduct an ablation study on the remaining parameters K and \u03b2. The base configuration is K = 16, \u03b2 = 0.1, with an adaptive temperature T for the Tanh Demon. We set T = 32 for the ablation study of \u1e9e and T = 64 for K.\nWe found a large \u1e9e makes the sampling unstable, given the number of steps T is fixed. Predictably, sampling with a \u1e9e close to 0 is reduced to ODE. From our theoretical result Lemma 1, the design methodology, and empirical results, the guidelines Table 9 can assist users in setting parameters. We"}, {"title": "C PSEUDOCODES", "content": "As an aid, we provide pseudocodes for the design of Demons Algorithm 2, Algorithm 3:\nAlgorithm 2 Tanh Demon with Adaptive Temperature\n1: Input: A list of ODE reward estimate [Rk]\n2: Output: Noise Weights [bk]\n3: K\u2190 length([Rk])\n4: \u03bc\u21901Rk\n5: T\u2190\u221a1(Rk\u03bc)\u00b2\n6: fork= 1 to K do\n7: bk tanh()\n8: end for\n9: Return [bk]\nAlgorithm 3 Boltzmann Demon with Fixed Temperature \u0442\n1: Input: A list of ODE reward estimate [Rk]\n2: Output: Noise Weights [bk]\n3: K\u2190 length([Rk])\n4: Z\u2190\u2211k=1exp (R)\n5: fork= 1 to K do\n6: bk\u2190exp()\n7: end for\n8: Return [bk]"}, {"title": "D MATHEMATICS", "content": "D.1 ERROR COMPREHENSION FOR REWARD ESTIMATE APPROXIMATION\nIn this section, we present the theoretical analysis and proof better to understand the error in our reward estimate approximation.\nD.1.1 ERROR TERM AS AN IT\u00d4 INTEGRAL\nLemma 1. Let the reward estimate function, h(xt, t) = (roc)(xt, t), be shorthanded as h. We have:\nrs(xt,t) \u2013 (roc) (xt, t) = Exo|xt\u222bVtVxhdJp (xu, u) \u2013 Bu\u00b2\u2207\u00b2h du, (13)\nwhere xo is sampled from Equation (2) and\ndJp(xu, u) = -\u03b2u\u00b2\u2207x log p(xu, u) du + \u221a2\u1e9eu dwu,\nis the Langevin diffusion SDE term, and \u22072h is the Laplacian of h.\nProof. We aim to prove:"}, {"title": "D.1.2 DISCUSSION", "content": "We interpret the error terms of the reward estimates approximation as follows:\n\u2022 The estimate becomes more accurate as \u03b2 decreases, satisfying the intuition that SDE trajectories will reduce to the ODE trajectory as \u03b2 \u2192 0.\n\u2022 If \u2207x\u2081h | \u2207 xu log p (xu, u), the term \u2207x\u2081h \u00b7 dJp(xu, u) cancels out in expectation.\n\u2022 If \u22072h = 0 and the previous condition holds, then r o c = r\u03b2.\nFor estimation purposes, we make the following assumptions to facilitate understanding and derivation of Equation (5):\nlog p(xt, t)\u2248 (24)\nc(xt, t)\u2248 Ctxt (25)\nVxrx (26)\nwhere Ct is a time-dependent constant and r is scale-invariant.\n\u2022 Equation (24) is derived from the assumption that p(xt) \u2248 N(0, t2I).\n\u2022 Equation (25) stems from image preprocessing algorithms, such as those used in Stable Diffusion, which normalize the image distribution. This normalization implies that images in the dataset are often scaled to lie on a sphere. Therefore, we can reasonably assume that a randomly generated xt is close to an image in the dataset in direction.\n\u2022 Equation (26) is based on the intuition that minor changes in brightness do not significantly affect the semantic interpretation of an image. Besides, many training algorithms incorporate scaling as part of data augmentation, which aligns with the assumption that the gradient of Vxr is orthogonal to x."}, {"title": "D.1.3 ILLUSTRATION OF MISMATCH", "content": "For better understanding, we provide an example that r\u03b2 and roc don't meet. We adopt assumptions in Appendix D.1.2 to illustrate the intuition, and suppose xt is a noisy sample at time t such that c(xt) is a sharp local maxima of r, where \u22072r < 0 near c(xt). Suppose further that \u03b2 is small enough such that the generated xo is near c(xt). In this case, r\u03b2(xt) \u2013 (roc)(xt) < 0 as r\u03b2(xt) = Exoxt [r(x0)] < (roc) (xt) by intuition.\nWe can also verify r\u03b2(xt) \u2013 (r oc)(xt) < 0 using Equation (15). Under the assumptions in Appendix D.1.2, we can write:\nrs(xt) - (roc)(xt) \u2248 Exo|xt\u221a2\u1e9etCtVxr. dwt \u2013 Bu\u00b2\u2207\u00b2h du (31)\n= Exoxt Bu\u00b2C\u00b2\u2207\u00b2r du] (32)\n< 0. (33)\nNote that the value of \u22072r is taken at c(xt), fluctuating with SDE."}, {"title": "D.2 MARTINGALE PROPERTY OF REWARD ESTIMATES", "content": "A martingale is a sequence of random variables that maintains a certain property over time Billingsley (2017): the expected future value, given all past values, is equal to the current value; for a fixed SDE, the current reward estimate is the expected value of the reward estimates at the next time step:\nFact 1. For any time step < 0 such that t > t - \u25b3 > 0:\nr\u1e9e(xt) = Ext-xt [r\u1e9e(x-1)]. (34)\nIntuitively speaking, this idea stems from the principles of conditional probability, which tell us that our current prediction of the final score should be the same as the average of all possible future predictions.\nProof. This result follows directly from the foundational theorem of expectation. Let G be the \u03c3-algebra generated by xt-\u25b3 and F be the o-algebra generated by xt. Note that G is a refinement of F.\nFor an integrable random variable r(x0), we have:\nE [r(xo) | F] = E [E [r(xo) | G] | F]"}, {"title": "D.3 TANH DEMON", "content": "We provide the theoretical idea behind the development of the algorithm. To start with", "follows": "nr\u03b2(x)-r\u03b2(xt) = g(t)\u221ax+r\u03b2\u2022z(k) \u221aA+o(\u25b3)", "SDE": "ndxt = f\u03b2 dt + g\u03b2 dwt", "is": "ndr\u03b2 = 9Bxr\u03b2. dwt. (41)\nProof. We begin by introducing a change of variables. Let s = tmax - t", "write": "ndx = -f\u03b2 ds + g\u03b2 d\u1ff6\u03c2", "r\u1e9e(xs,s)": "ndr\u00df=(43)d\u1ff6\u03c2.\nWe aim to prove the Kolmogorov backward equation:\nBfxr\u03b2 = 0. (44)\nTo do so", "tmax": "nr\u03b2(xtmax) - r\u03b2(xt) = \u222bds\u2032 (45)"}, {"title": "D.4 BOLTZMANN DEMON", "content": "Recall that\nXt-:= Xt + \u222bf(xu, u) du + 9\u03b2(u) dwu (65)\nxt-\u25b3 := xt \u2013 fs(xt,t)\u25b3+ gs(t)z\u221aA (66)\nXt\u2212 := Xt[f(x,t) ,t) + fs(xt-\u25b3, t \u2013 \u2206)] \u25b3 + = [gs(t) + gs(t \u2212 \u2206)] z\u221aA (67)\nWe first present the theoretical analysis and proof for the reward estimate error of the proposed Boltzmann Demon as follows."}, {"title": "D.5 HIGH DIMENSIONAL GAUSSIAN ON SPHERE", "content": "The original statement is more general in the textbook, but we provide specific proof for Gaussian.\nLemma 5. (Vershynin, 2020, Chap. 3) Let z be independent and identically distributed (i.i.d.) instances of a standard isotropic Gaussian N(0, IN) in a high-dimensional space N. With a high probability (e.g., 0.9999), it holds that\n||z|| = \u221aN + O(1) (88)\nProof. Consider the norm ||z||2, where z is an instance of a standard isotropic Gaussian N (0, IN) in N dimensions. The distribution of ||z||2 follows a Chi-squared distribution with N degrees of freedom. The mean and variance of this distribution are N and 2N, respectively.\nApplying a central limit theorem argument, we approximate the distribution of ||z||2 by a normal distribution when N is large, giving:\n||z||2 = N + CVN (89)"}, {"title": "E COMPARE ON PICKSCORE", "content": "E.1 PICKSCORE COMPARISONS.\nSince PickScore Kirstain et al. (2023) is trained specifically on generated images, we believe it is a more reliable measure and objective than the aesthetics score. To emphasize the strength of our method, we show how the median PickScore reward function improves across 20 different prompts using our Tanh Demon, as shown in Figure 7a.\nOur approach utilizes 1440 reward queries per sample and achieves a PickScore of 0.253, outperforming other methods alongside reduced computation time (180 minutes for our method vs. 240 minutes for resampling methods due to shortened ODE trajectories). Specifically, we compare our method to:\n\u2022 SDXL/SDXL-DPO Wallace et al. (2023a): A state-of-the-art method for direct preference optimization in diffusion models, which achieves a PickScore of 0.226, while the baseline SDXL reaches 0.222.\n\u2022 Diffusion-DPO(1440x): A variant that selects the highest quality median PickScore from 1440 samples among 20 prompts, achieving a PickScore of 0.246.\n\u2022 SDXL(1440x): Similar to the above, but without preference optimization, achieving a PickScore of 0.243.\nAdditionally, resampling an ODE from xtmax is crucial in applications where the distribution xtmax | xo plays a key role, such as in SDEdit Meng et al. (2022). Resampling methods fail to address such applications, highlighting the advantage of our approach."}, {"title": "E.2 QUALITATIVE RESULTS", "content": "In this section, we demonstrate the quantitative and qualitative results of PickScore in SDXL with our Tanh Demon."}, {"title": "F.1.1 GENERATION ON STABLE DIFFUSION XL", "content": ""}, {"title": "F.1.2 GENERATION ON STABLE DIFFUSION V1.4", "content": "Here, we provide more qualitative results as a continuation of Table 4."}, {"title": "F.2 SUBJECTIVE TEST OVERVIEW", "content": "We surveyed with 101 participants via Google Forms, as shown in Figure 9. Participants evaluated different image generation methods based on:\n\u2022 Subjective Preference: Visual aesthetics and image quality.\n\u2022 Semantic Alignment: Correspondence between generated images and text prompts.\nEach participant ranked images across four sections, with rankings aggregated using the following formula:\nexp(-(rankij - 1)) (93)\nwhere:\n\u2022 M = 4 (number of sections),\n\u2022 L = 101 (participants),\n\u2022 rankij is the ranking by participant j for method i."}, {"title": "F.2.1 SURVEY STRUCTURE", "content": "The subjective test comprised four sections: two comparing methods (DOODL, Baseline (SD or SDXL), Ensemble) based on subjective preference and prompt alignment, each with 3 sets containing one image per method; and two comparing methods applied to different objectives (Baseline, Ensemble, IR, Pick, HPSv2, Aes) also based on preference and prompt alignment, each with 3 sets containing six images per set."}, {"title": "F.2.2 RESULTS OVERVIEW", "content": "Methods Comparison Figure 9a shows that DOODL slightly outperforms the Baseline in aesthetic preference and prompt alignment. The Ensemble method significantly surpasses both, indicating superior visual quality and semantic accuracy."}, {"title": "F.2.3 ANALYSIS", "content": "We compared DOODL, Baseline, and Ensemble based on aesthetics and prompt alignment. DOODL marginally improves over the Baseline in both criteria, while the Ensemble method consistently outperforms both DOODL and Baseline, excelling in image quality and semantic accuracy. The Ensemble method demonstrates significant enhancements, particularly in tasks requiring visual refinement.\nEvaluating different objectives (IR, Pick, HPSv2, Aes) against Baseline and Ensemble revealed that almost all objectives surpass the Baseline in both preference and prompt alignment. However, Aes, an objective without explicit text guidance, shows weaker prompt alignment. Among the objectives, HPSv2 achieves the best performance on both criteria.\nThe Ensemble method provides the most substantial improvements in visual aesthetics and semantic alignment among method comparisons. Among the factors of the Ensemble method, HPSv2 outperforms other objectives, even the Ensemble method, highlighting its effectiveness in aligning preference for a real human."}, {"title": "G MORE DETAILS OF VLM AS DEMON", "content": "In this section, we provide more details of experiments and quantitative results of utilizing VLM during generation."}, {"title": "G.1 EXPERIMENTS SETTINGS", "content": "We provide the prompt template we used in Table 5 to VLMs. The prompt is fixed as \u201cA mysterious, glowing object discovered in an unexpected place, sparking curiosity and wonder. The setting changes based on the viewer's background, transforming the object's significance and the surrounding environment to match the realms of education, history, literature, design, science, and imagination.\" for all experiments in the VLM generation. At each step, the VLM is given a fixed prompt with different scenarios and asked to choose one of the images from c(xt) and c(z) that best matches the given scenario."}, {"title": "G.2 QUANTITATIVE MEASUREMENT OF EFFECTIVENESS", "content": "For VLMs as reward functions, we use Pickscore Kirstain et al. (2023), which is trained from CLIP Radford et al. (2021), to evaluate the effectiveness of VLM in aligning designed scenarios during image generation. For each scenario, we create a corresponding prompt that partially describes the scenario: \"For education\" for Teacher, \"For entertainment\" for Artist, \"For research\" for Researcher, and \"For Journalism\" for Journalist. Then, we assess the PickScore between the prompt and the scenario. The results are presented in Table 15, Table 16, Table 17 and Table 18, where the highest score for each prompt is highlighted in bold. Our observations indicate that 14 out of our VLM-generated 16 images demonstrate better PickScore alignment with the corresponding prompt than PF-ODE. Given that all images are generated using the same prompt and initial noisy sample in the same table, these results demonstrate the effectiveness of our approach employing VLM in aligning the scenarios."}, {"title": "H GENERAL IMPLEMENTATION DETAILS", "content": "In this section, we show the details of the implementation and experimental settings of the proposed approach as follows."}, {"title": "H.1 ADAPTING STABLE DIFFUSION TO EDM FRAMEWORK", "content": "In this paper, we tailor the existing text-to-image Stable Diffusion v1.4/v1.5/XL v1.0 (SDXL) (i.e., we use fp16 SD v1.4/SDXL v1.0 for generation.) to the SDE formulation proposed in EDM Karras et al. (2022) by Karras et al. for image generation since its reparameterized time domain, t \u2208 [tmin, tmax], improves numerical stability and sample quality during image generation. We realize the modification through the equation, V\u00e6 log p(x, t) = (D(x, t) \u2013 x) /t\u00b2, where the function D(x, t) = x \u2013 tF(s(t)\u00e6, u(t)) derived from the original model F. In addition, s(t) and u(t) represent the scaling schedule and the original temporal domain of the reparameterized temporal domain t, respectively."}, {"title": "H.2 NUMERICAL METHODS FOR IMAGE GENERATION", "content": "Moreover, for image sampling with ODE/SDE, our approach follows Karras et al. (2022), adopting Heun's method and time intervals determined by ti = (ax + (n-tax)), setting p = 7,T > 20 and ln tmax \u2248 2.7, lntmin \u2248 -6.2. The classifier-free guidance parameter is set to 2 throughout this paper. Across all temporal steps t of image generation, we keep K and \u03b2 constant. We have found that when t is less than 0.11, i.i.d. samples from SDE all appear similar to human perception. For the remaining evaluations, we will directly use ODE. As a result, the actual number of samples will be slightly smaller than K. T."}, {"title": "H.3 SIMPLIFICATIONS IN DIFFUSION PROCESS MODIFICATION", "content": "It is worth noting that in our work, since our main focus is on the modification of the diffusion process, without loss of generality, we omit the VAEs (Kingma & Welling (2014)) of Stable Diffusion models, the prompt c, and \u03b7 of classifier-free guidance (CFG) Ho & Salimans (2021) in our formulation for simplicity (i.e., using p(x) to denote the unnormalized p(x)p(c | x)\" for conciseness).\""}, {"title": "H.4 BATCH SIZE AND MEMORY CONSTRAINTS", "content": "When we generate many SDE samples, the batch size for solving ODE/SDE is 8 for both Stable Diffusion v1.4, v1.5, and SDXL models. However, due to memory limitations on the RTX 3090, the batch size for evaluating the VAE in SDXL is restricted to 1. This memory bottleneck prevents any further acceleration from using larger batch sizes, as it limits the parallelization during VAE evaluation."}, {"title": "H.5 EXPERIMENTAL SETUP AND HYPERPARAMETERS", "content": "We present the detailed hyperparameter settings of different experiments as follows:\nBaseline Comparison. The hyperparameters for generation are set to \u03b2 = 0.1, \u039a = 16, \u03b7 = 2 and 7 adaptive for Tanh, 10-5 for Boltzmann.\nThe parameter of DOODL optimized on aesthetics score is set as their demo provided.\nReward Estimate Approximation Comparison. We use SD v1.5 and its distilled CM. The CFG parameter is ignored in CM(set to 1).\nGeneration with Various Reward Functions. We use Tanh Demon for sampling with adaptive temperature. The hyperparameters for generation are set to \u03b2 = 0.05, K = 16, T = 64 as shown in Table 4, Table 13, Table 11, and Table 12.\nFor reward scaling in the ensemble setting, the PickScore was multiplied by 98.86, and HPSv2 was multiplied by 40.\nThe interaction step of DOODL is used as suggested by their implementation, 50 iteration for Aes and 100 iteration for Pick."}, {"title": "I LIMITATIONS", "content": "We present the theoretical result in Equation (5), which demonstrates that r\u00b0 c \u2248 rs. This result relies on the assumption that the reward function r is near harmonic near the ODE sample ourput, as detailed in Appendix D.1.2.\nIn practice, implementing ro c faces challenges related to time complexity and accuracy bottlenecks, thoroughly discussed in Section 4."}, {"title": "J FUTURE WORK", "content": "The only difference between Tanh-C and Tanh Demon lies in how ro c is implemented. Analysis of the data in Table 2 and Figure 4 indicates that Tanh-C's reward performance"}]}