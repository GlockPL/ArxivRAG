{"title": "Physics-Guided Foundation Model for Scientific Discovery: An Application to Aquatic Science", "authors": ["Runlong Yu", "Chonghao Qiu", "Robert Ladwig", "Paul Hanson", "Yiqun Xie", "Xiaowei Jia"], "abstract": "Physics-guided machine learning (PGML) has become a prevalent approach in studying scientific systems due to its ability to integrate scientific theories for enhancing machine learning (ML) models. However, most PGML approaches are tailored to isolated and relatively simple tasks, which limits their applicability to complex systems involving multiple interacting processes and numerous influencing features. In this paper, we propose a Physics-Guided Foundation Model (PGFM) that combines pre-trained ML models and physics-based models and leverages their complementary strengths to improve the modeling of multiple coupled processes. To effectively conduct pre-training, we construct a simulated environmental system that encompasses a wide range of influencing features and various simulated variables generated by physics-based models. The model is pre-trained in this system to adaptively select important feature interactions guided by multi-task objectives. We then fine-tune the model for each specific task using true observations, while maintaining consistency with established physical theories, such as the principles of mass and energy conservation. We demonstrate the effectiveness of this methodology in modeling water temperature and dissolved oxygen dynamics in real-world lakes. The proposed PGFM is also broadly applicable to a range of scientific fields where physics-based models are being used.", "sections": [{"title": "Introduction", "content": "Physics-based models of dynamical systems are often used to study scientific systems. For instance, scientists in aquatic science build physics-based models to simulate different water quality variables such as water temperature and dissolved oxygen (DO) concentrations, which are vital for assessing ecosystem health and water security. Similar models are also applied in agriculture (Jia et al. 2019a), geology (Reichstein et al. 2019), climate science (Faghmous and Kumar 2014), and bio-medicine (Yazdani et al. 2020). Despite their widespread use, physics-based models face limitations due to the simplified representations of complex physical processes and the challenges of selecting appropriate parameters (Jia et al. 2019b). Additional complexity arises when coupling these models to perform multiple tasks due to the dependencies amongst physical processes, e.g., DO concentrations are highly dependent on water temperature profiles.\nWith advances in data collection driven by improved sensor technologies, there is a growing interest in using machine learning (ML) to extract complex data patterns for scientific problems (Willard et al. 2022; He et al. 2023; Wang et al. 2024; Xu et al. 2024). However, most ML approaches are only tested for isolated and simple tasks while still being limited in applicability to complex real-world systems with interacting and non-stationary processes. Recently, foundation models have shown great promise in tasks like vision and natural language processing by pre-training over large datasets (Bommasani et al. 2021; Zhou et al. 2023; Ye et al. 2024). These models also offer tremendous opportunities for scientific modeling due to their ability to harness large and heterogeneous data and adapt to diverse downstream tasks.\nHowever, direct application of existing foundation models to scientific problems often leads to serious false discoveries due to several major challenges: 1. Data requirements: Advanced ML models can often outperform traditional empirical models (e.g., regression), but these models require extensive training data, which is often scarce in real scientific applications. 2. Effectiveness of pre-training: Unsupervised pre-training has been shown to significantly boost the performance of many existing foundation models and mitigate the need for large data in downstream tasks. However, traditional pre-training tasks are not well aligned with scientific modeling tasks and thus can be less effective. The datasets used to pre-train existing models also have little overlap with target scientific data. 3. Physical consistency and generalizability: With the absence of physical knowledge, existing foundation models can only learn statistical patterns from available data. Although the model could perform well in similar training data distribution, the patterns extracted by ML models may significantly violate some established physical relationships (e.g., mass and energy conservation). Consequently, these models can struggle to generalize to unseen scenarios. 4. Learning multiple tasks: Most existing foundation models for scientific problems still focus on single prediction tasks (Li et al. 2024; Xie et al. 2024) while largely ignoring the dependencies among multiple physical variables. This restricts their utility in complex systems characterized by multiple tasks and numerous influencing features.\nTo address these challenges, in this paper, we propose a Physics-Guided Foundation Model (PGFM) framework. Instead of relying on complex model architecture, PGFM"}, {"title": "Problem Formulation", "content": "The objective of this work is to predict daily water temperature profiles and DO concentrations at different depth layers in lake systems. As illustrated in Figure 1, thermal expansion properties of water facilitate stratification, creating a stable vertical density gradient. This results in distinct layers. Stratification inhibits vertical mixing, limiting the transfer of nutrients and oxygen between layers and reducing connectivity between the lower bottom and the atmosphere, thereby creating barriers to oxygen replenishment (Read et al. 2011). To reflect summer changes in DO concentrations, our study focuses on stratified lakes with a vertical density difference exceeding 0.05 kg/m\u00b3 between surface and bottom layers, average water temperatures above 4\u00b0C, and a thermocline.\nMore specifically, we aim to predict water temperature for multiple depth layers with an interval of 0.5 m. In contrast, we analyze the DO dynamics in two distinct depth layers of the water column: a well-mixed upper layer (epilimnion), and a cooler, nutrient-rich but light-limited deep layer (hypolimnion). From fall to spring, when the water column is typically completely mixed, our model aims to predict the total DO concentration throughout the lake.\nFor each lake, we have access to its phenological features $x_{d,t}$ at each depth $d$ and time-step $t$. These features span a broad range of $m$ diverse fields, governing the dynamics of lake temperature and DO concentration, represented as $x_{d,t} = \\{x_t^1,...,x_t^m\\}$. They include morphometric and geographic details such as lake area, depth, and shape; flux-related data like ecosystem and sedimentation fluxes; weather factors comprising wind speed and temperature; a range of trophic states from dystrophic to eutrophic; and diverse land use proportions extending from forests to wetlands. In addition to these input features, we observed water temperatures and DO concentrations on certain days and in certain depth layers. We use $T_t^d$ to represent the temperature at depth $d$ and time step $t$. During summer, we distinguish DO concentrations in the epilimnion, denoted as $DO_{epi}^t$, from those in the hypolimnion, denoted as $DO_{hyp}^t$. The thermocline, denoted by $t_c$, determines which layer each measurement pertains to: $DO_{epi}^t$ if the depth $d < t_c$, and $DO_{hyp}^t$ if $d > t_c$. From fall to spring, the recorded observations reflect the total DO concentration, denoted as $DO_{total}^t$"}, {"title": "Physics-Guided Foundation Model", "content": "In this section, we introduce the Physics-Guided Foundation Model (PGFM) framework, as illustrated in Figure 2. The PGFM framework consists of two primary learning stages: (1) the pre-training stage and (2) the fine-tuning stage."}, {"title": "Pre-training Stage", "content": "The success of existing foundation models is contingent upon effective pre-training overabundant representative data samples. Conventional pre-training tasks, e.g., masked token prediction, are not well aligned with scientific modeling tasks. This misalignment arises from the complex relationships between the input feature space (e.g., environmental conditions) and the target variable space (e.g., properties of lake systems). This challenge is further compounded by limited and less representative data. A key innovation of this work is pre-training in a physics-based simulated environmental system. This enables the development of a robust and generalizable model by leveraging comprehensive data that are well-aligned with established physical principles. The pre-training objective is to extract features and their interactions that are generalizable to various downstream tasks.\nIn the following, we outline the simulated environmental system used in the pre-training stage, and then discuss the evolution-based feature selection that facilitates the learning and adaptation of the proposed foundation model."}, {"title": "Simulated environmental system", "content": "We first construct a simulated environmental system. It encompasses a wide range of data features and various simulated labels generated by physics-based models. For simulating lake temperatures, we employ the general lake model (GLM) (Hipsey, Bruce et al. 2019), a widely used physics-based model that captures various heat energy fluxes affecting water temperature in lakes. These include the heating of the water surface from terrestrial long-wave radiation ($R_{LW}$) and incoming short-wave radiation ($R_{SW}$). The lake loses heat mainly through the outward fluxes of back radiation ($R_{LWout}$), sensible heat fluxes ($H$), and latent evaporative heat fluxes ($E$), as illustrated in Fig. 1. For short-wave radiation ($R_{SW}$) and long-wave radiation ($R_{LW}$), a portion of the energy is reflected by the lake surface. For DO concentration dynamics, an advanced physics-based model is adopted, as detailed in (Ladwig et al. 2022). The model simulates several key DO mass fluxes, including fluxes from the atmospheric exchange ($F_{ATM}$), net ecosystem production ($F_{NEP}$), and oxygen consumption by sediment ($F_{SED}$). Additionally, during the summer, it accounts for DO entrainment fluxes from or into the other layer driven by turbulent flow ($F_{ENT}$), as depicted in Fig. 1. Turbulent forces drive entrainment fluxes that either shallow or deepen the thermocline, affecting the transport of DO into either the epilimnion or the hypolimnion."}, {"title": "Evolution-based feature selection", "content": "We train the foundation model to perform evolution-based feature selection in"}, {"title": "Physics-Guided Foundation Model", "content": "the simulated environmental system using a heuristic algorithm. This training process is conceived as an evolutionary search, akin to how organisms strive to evolve better traits for higher survival rates. In this analogy, we liken feature interactions to genomes and foundation models to organisms, where traits inherited via genes drive evolutionary success.\nTo facilitate this process, we use an embedding layer to convert input phenological features into a series of multi-field feature embeddings $f_{d,t} = [f_{d,t}^1,\u2026\u2026, f_{d,t}^m]$, where $f_{d,t}^i = embed(x_{d,t}^i)$. Using these embeddings, the aim of evolution-based feature selection can be formally described as identifying the most informative feature interactions to improve the prediction of target objectives, as $H : M(a f_{d,t},\u03b2\u00b7g(f_{d,t})) \u2192 \\{T, DO\\}$, where $g$ denotes the set of operations to interact on feature pairs, and $g(f_{d,t})$ denotes the set of interactions. The algorithm $H$ is designed to minimize the mean squared error loss $L_{FM}(M)$ for the outputs of the foundation model $M$. The smaller the loss, the better the fitness of $M$, reflecting a closer alignment between the predicted labels from the foundation model and the simulated labels from physics-based models. Here the simulated labels could include multiple variables involved in the aquatic systems, thus the loss $L_{FM}(M)$ can be a multi-task objective.\nTo explicitly capture interactions amongst influencing features in the system, we introduce operations as the basic units of feature interaction. In particular, operations convert two individual features into interactions. Reflecting the diversity of genetic base pairs, we extend the operation set with four types of operations: $g = \\{\\oplus, \u2297, \u2295, \u7530\\}$, which have been widely utilized in prior research (Khawar et al. 2020; Song et al. 2020; Liu et al. 2020a; Yu et al. 2023). As depicted in Fig 2, these operations encompass element-wise sum ($+$), element-wise product ($\u2297$), and more complex forms like concatenation with a feed-forward layer ($\u2295$) and element-wise product with a feed-forward layer ($\u7530$).\nMotivated by the goal of enhancing model fitness through the preservation of beneficial genetic information, we aim to discern and prioritize important features and their interactions via a parameterized method. The idea is to introduce a set of relevance parameters to strengthen relevant feature interactions while diminishing or mutating those that contribute less. In this context, we define relevance parameters for features $f_{d,t}$ and interactions $g(f_{d,t})$ as $a = \\{\u03b1_i|1 < i < m\\}$ and $\u03b2 = \\{\u03b2_{i,j}|1 < i < j < m\\}$, respectively. Here, $g(f_{d,t})$ denotes the interaction of applying any operations from $g$ to a pair of features. The predictive response of our model at time step $t$ is formulated as: $M(a f_{d,t},\u03b2\u00b7g(f_{d,t}))$. Note that $M$ is agnostic of specific ML-based models. In this work, we opt for long short-term memory (LSTM) networks (1997), chosen for their proven effectiveness in capturing temporal dependencies in hydrology, as demonstrated in several studies (Jia et al. 2021; Hanson et al. 2020; Chen et al. 2023). We also test other advanced models (e.g., Transformer) in the experiments. We use a regularized dual averaging (RDA) optimizer to learn the relevance parameters $a$ and $\u03b2$ (Xiao 2009), with the aim to distinguish between relevant and irrelevant feature interactions. When the absolute value of the cumulative gradient average value in a certain"}, {"title": "Physics-Guided Foundation Model", "content": "position in $a$ or $\u03b2$ is less than a threshold, the weight of that position in relevance parameters will be set to 0, resulting in the sparsity of the relevance (Xiao 2009; Liu et al. 2020b).\nMutation and crossover serve as key mechanisms of our evolution process. The mutation mechanism primarily aims at mutating the operations associated with irrelevant interactions into alternative operations, thus generating a new model (the offspring). For example, for an interaction $g_k(f_{d,t}^i, f_{d,t}^j)$, mutation is triggered with a probability $\u03c3$ after every \u03c4 steps if the relevance parameter $\u03b2_{i,j}$ drops below a threshold $\u03bb$. In other words, to regenerate a new interaction, the operation $g_k$ of the interaction $g_k(f_{d,t}^i, f_{d,t}^j)$ mutates into another operation $g_i$, which is randomly selected from the operation set as $g_i = \\{g|g \u2208 g,g \u2260 g_k\\}$. The new interaction $g_i(f_{d,t}^i, f_{d,t}^j)$ replaces the irrelevant interaction $g_k(f_{d,t}^i, f_{d,t}^j)$, and its corresponding relevance $\u03b2_{i,j}$ is reset. Consequently, the parent model $M$ evolves into its offspring $M'$. When the relevance of interactions is low (indicated by a lighter color), these are targeted for mutation, meaning that the operations of the interactions change into the other operations.\nFor a population-based search with a population size of $n$ ($n > 1$), a crossover mechanism is used across multiple parent models to generate the offspring model. Consider $n$ random models as a population $P$. For a model $M_v \u2208 P$, we denote the relevance of features and interactions as $\u03b1^{M_v}$ and $\u03b2^{M_v}$, respectively. Different models in the population may have various operations for the same feature pair $(f_i, f_j)$, represented as $g_{ij}^{M_v} = \\{g_1^{M_v},..., g_l^{M_v}\\}$. We select the operation with the highest relevance for the offspring model, given as $g_{ij}^{M'} = arg max_{g \u2208 g_{ij}^{M_v}} \u03b2_{i,j}^{M_v}$. If the relevance of interactions of a parent is small (shown as lighter color), the operations should be selected from the other parents whose relevance of the interactions is large. Meanwhile, interactions of the offspring inherit their relevance from respective parents."}, {"title": "Physics-Guided Foundation Model", "content": "Instantiation of (n+1)-PGFM pre-training. We present an instantiation to illustrate the steps for pre-training foundation models. In line with the canonical nomenclature used in evolution strategies, we refer to this as the (n+1)-PGFM.\nInitially, (n+1)-PGFM creates a population of $n$ random models. For every $\u03c4$ iterations, the crossover mechanism generates an offspring from parent models, and mutation is applied to ensure diversity within the population. New parents are selected from both the parents and offspring, with offspring only advancing to the next generation's parent pool if its fitness meets or exceeds that of the least fit current parent, given as $M = arg max_{M_i \u2208 P} L_{FM}(M_i)$. Additionally, the 1/5 successful rule is employed to adapt the search regions for the population, that is, if previous iterations fail to improve the model significantly, it suggests that the model may be approaching a local optimum. In such cases, reducing the mutation probability can help exploit the promising region near the optimum more effectively. Finally, the algorithm culminates by delivering the best foundation"}, {"title": "Physics-Guided Foundation Model", "content": "model in $P$, given as $M^* = arg min_{M_i \u2208 P} L_{FM}(M_i)$.\nDiscussion and remark. We progressively evolve foundation models within the simulated environmental system, selecting important feature interactions that align with multi-task objectives. This approach offers two significant advantages. Firstly, it effectively mitigates the issue of limited observed labels in real-world environments. Secondly, by enabling the model to learn from extensive labels rooted in universal physical laws and diverse environments, the feature interactions identified by the foundation model demonstrate broad generality. This strategy addresses the constraints of traditional physics-guided machine learning, which typically focuses on isolated and simple scenarios."}, {"title": "Fine-tuning Stage", "content": "The fine-tuning process leverages the features and their interactions selected through the pre-training stage, refining them to capture the dynamics of specific target variables in a real system. It utilizes the observations of target variables as references while also regularizing the model with physical laws that govern the underlying processes.\nSpecifically, we utilize standard ML training loss $L_{ML}$ that measures the difference between observed labels and predicted labels. Besides, we introduce the physical loss $L_{PHY}$, which measures the degree of violation of established physical laws such as energy or mass conservation. The fine-tuning loss function is formulated as: $L_{FT} = L_{ML} + \u03bb_{PHY}L_{PHY}$, where the hyper-parameter $\u03bb_{PHY}$ adjusts the balance between the standard ML loss and the physics-based penalties. It is noteworthy to mention that the computation of $L_{ML}$ relies on the sparsely available observations, and thus can only be defined on certain dates and depth layers when observations are available. In contrast, the physical loss does not require observed variables but only needs to check whether the predictions are consistent with known physical relationships. Hence, the physical loss can be applied to all the data points and thus contribute to learning better continuous dynamics. In the following, we detail the implementation of physical loss functions designed for modeling lake water temperatures and DO concentrations."}, {"title": "Energy conservation loss", "content": "Fig. 1 illustrates the major incoming and outgoing heat fluxes for a lake system. The impact on the balance between these fluxes results in changes to the lake's total thermal energy ($U_t$). Specifically, the relationship between the lake's thermal energy $U_t$ and these energy fluxes should satisfy $\u2206U_t = R_{SW}(1 \u2013 \u03b1_{SW}) + R_{LWin}(1 - \u03b1_{LW}) \u2013 R_{LWout} \u2013 E \u2013 H$, where $\u2206U_t = U_{t+1} - U_t$, $\u03b1_{SW}$ represents the short-wave albedo (the proportion of short-wave radiation reflected by the lake surface), and $\u03b1_{LW}$ represents the long-wave albedo. We denote the net gain of heat on the right side of the equation as $F_E$.\nIn this context, we define the physical loss based on energy conservation, as $L_{PHY} = \u2211 ReLU (\u2206U_t - F_E \u2013 TEC)$, where $\u2206U_t$ is computed directly as $U_{t+1} - U_t$, and $U_t$ is estimated as the volume-averaged water temperature predicted over different depth layers. The hyper-parameter TEC represents a tolerance threshold for the violation of energy"}, {"title": "Physics-Guided Foundation Model", "content": "conservation. This is introduced to account for potential impacts from minor factors not included in estimating the heat fluxes or from observational errors in meteorological data. All the heat fluxes can be estimated from the input drivers."}, {"title": "DO mass conservation loss", "content": "Referring back to Fig. 1, we categorize the fluxes caused by atmospheric exchange ($F_{ATM}$), net ecosystem production ($F_{NEP}$), and mineralization through sediment oxygen demand ($F_{SED}$), among other factors, as exogenous fluxes ($F_{EXO}$). In the well-mixed conditions from fall to spring, assuming that diurnal variations in total lake volume are negligible, we can model the DO dynamics as $DO_{total}^t = DO_{total}^{t-1} + F_{EXO} \u00d7 \u2206t$. During the stratified conditions typical of summer, the dynamics becomes more complex. It becomes necessary to account for daily volume changes in both the epilimnion and hypolimnion, as well as the entrainment fluxes between layers caused by turbulent flow ($F_{ENT}$). The modeling is adjusted accordingly: $DO_{epi}^t = (DO_{epi}^{t-1} \u00d7 V_{epi}^{t-1} + F_{EXO,epi} \u00d7 \u2206t) \u00d7 V_{epi}^t / V_{epi}^{t-1} \u2013 F_{ENT,epi}$ and $DO_{hyp}^t = (DO_{hyp}^{t-1} \u00d7 V_{hyp}^{t-1} + F_{EXO,hyp} \u00d7 \u2206t) \u00d7 V_{hyp}^t / V_{hyp}^{t-1} + F_{ENT,hyp}$, where $V_{epi}^t$ and $V_{hyp}^t$ represent the volumes of the epilimnion and hypolimnion, respectively. In the context of predicting DO concentrations, we define the physical loss as $L_{PHY} = \u2211 ReLU (|DO_t \u2013 DO_t| - TMC)$, with TMC set as a tolerance threshold for the mass conservation loss.\nRecognizing the interdependence between these tasks, particularly how temperature fluctuations influence oxygen solubility and biochemical reactions, we substitute the temperature from the simulated environmental system in the input variable x with the predicted lake temperature $T_t$ for predicting DO concentrations. This change enhances the accuracy and relevance of the model's predictions. Additionally, exogenous flux ($F_{EXO}$) and lake volume ($V_t$) are included in the input variables x. The entrainment fluxes ($F_{ENT}$) are calculated based on the predicted DO concentration ($DO_t$) and fluctuations of the thermocline ($t_c$)."}, {"title": "Discussion and remark", "content": "The integration of the physical loss $L_{PHY}$, based on energy conservation and mass conservation, offers several benefits. By aligning the machine"}, {"title": "Physical consistency analysis", "content": "model with established physical principles, this approach effectively narrows the search space, enhancing model performance, particularly in scenarios with sparse data and out-of-sample conditions. Moreover, the computation of $L_{PHY}$ does not require observed values and thus can be implemented on large unlabeled data points."}, {"title": "Experimental Evaluation", "content": "Data preparation. We evaluate the proposed PGFM framework for predicting water temperature and DO concentration using a comprehensive dataset covering 41 years"}, {"title": "Physical consistency analysis", "content": "tance. Each feature is also visually represented by uniformly colored bars. Interactions deemed irrelevant, with their relevance parameters reduced to 0, are omitted, leaving their corresponding genes depicted in white \u201c\u22121\u201d. One observation from the analysis is that water temperature and DO concentration are predominantly influenced by features such as water volume, weather conditions, and air temperature, with relatively minor effects from local land use factors."}, {"title": "Conclusion", "content": "This paper proposed a Physics-Guided Foundation Model (PGFM) for scientific discovery. PGFM leverages a wide range of influencing features and various simulated variables generated by physics-based models for pre-training, enabling it to learn from extensive labels rooted in universal physical laws and diverse environments. We applied the PGFM framework specifically to aquatic science, where we developed physical loss functions based on principles of energy and mass conservation and incorporated them into the fine-tuning stage. In the future, we encourage the adaptation of this idea to other scientific fields to explore its potential."}]}