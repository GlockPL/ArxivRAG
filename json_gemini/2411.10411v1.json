{"title": "Repurposing Stable Diffusion Attention for Training-Free Unsupervised Interactive Segmentation", "authors": ["Markus Karmann", "Onay Urfalioglu"], "abstract": "Recent progress in interactive point prompt based Image Segmentation allows to significantly reduce the manual effort to obtain high quality semantic labels. State-of-the-art unsupervised methods use self-supervised pre-trained models to obtain pseudo-labels which are used in training a prompt-based segmentation model. In this paper, we propose a novel unsupervised and training-free approach based solely on the self-attention of Stable Diffusion. We interpret the self-attention tensor as a Markov transition operator, which enables us to iteratively construct a Markov chain. Pixel-wise counting of the required number of iterations along the Markov chain to reach a relative probability threshold yields a Markov-iteration-map, which we simply call a Markov-map. Compared to the raw attention maps, we show that our proposed Markov-map has less noise, sharper semantic boundaries and more uniform values within semantically similar regions. We integrate the Markov-map in a simple yet effective truncated nearest neighbor framework to obtain interactive point prompt based segmentation. Despite being training-free, we experimentally show that our approach yields excellent results in terms of Number of Clicks (NoC), even outperforming state-of-the-art training based unsupervised methods in most of the datasets.", "sections": [{"title": "1. Introduction", "content": "The goal of point prompt based interactive image segmentation is to obtain a high-quality segmentation from limited user interaction in the form of clicking. Prompt based image segmentation gained popularity lately due to large supervised foundation models [15, 28]. In this paper, we focus on unsupervised methods, where no segmentation labels are used at all in the design and/or training of the models. Most recent approaches rely on self supervised backbones like ViT [6], trained either by DINO [2] or MAE [12] based self-supervised techniques. On the other hand, Stable Diffusion"}, {"title": "2. Related Work", "content": "Point prompt based interactive image segmentation has been approached from multiple perspectives. In this paper, we distinguish between supervised and unsupervised methods."}, {"title": "2.1. Supervised Methods", "content": "In [44] a click map and click sampling strategies are used in combination with FCN [24], creating the foundation for many follow-up methods.\nIn [20] the first click is emphasized, while in [13, 35], backpropagation refinement schemes (BRS) are proposed as a refinement step to correct mislabeled pixels. Diffusing prediction is introduced in [4], which propagates labeled representations from clicks to conditioned destinations and in [10], where interactive information of user clicks with edge-guided flow is utilized. Iterative click sampling [36] proposes a simple feed-forward model for click-based interactive segmentation that employs the segmentation masks from previous steps. SimpleClick [23] utilizes ViT, pre-trained as a masked autoencoder (MAE), for feature generation. Gaussian process posterior [46] formulates the Image Segmentation task as a Gaussian process-based pixel-wise binary classification model to fully and explicitly utilize and propagate the click information. In [5, 21] the authors propose to segment on local crops to improve efficiency. In [22] click prompt learning with Optimal Transport is proposed, which leverages optimal transport theory to capture diverse user intentions with multiple click prompts. CFR-ICL [37] introduces three components to improve the interactive segmentation, namely Cascade-Forward Refinement, Iterative Click Loss and diverse augmentation, respectively.\nAlthough the supervised approaches achieve good performance and efficiency, they require large-scale pixel-level annotations to train, which are expensive and laborious to obtain. While many supervised methods are tested on additional domains like medical images, it is unclear if there are other domains where the trained models would have a domain gap."}, {"title": "2.2. Unsupervised methods", "content": "Classical, unsupervised methods not based on Deep Learning like GraphCut [31], Random Walk [7], Geodesic Matting [1], GSC [8] and ESC [8] have been proposed. However, recently Deep Learning based approaches show great potential. Especially, methods utilizing self-supervised learning achieve impressive results. Such methods rely on pre-trained models (e.g., DenseCL [39], DINO [2]) to extract segments from their features. In [33] some heuristics are proposed to choose pixels belonging to the same object according to their feature similarity. [42] introduces normalized cuts [32] on the affinity graph constructed by pixel-level representations from DINO to divide the foreground and background of an image. In [9] a segmentation head is trained by distilling the feature correspondences from DINO. [26] adopts spectral decomposition on the affinity graph to discover meaningful parts in an image. FreeSOLO [40] designs pseudo instance mask generation based on multi-scale feature correspondences from densely pre-trained models and trains an instance segmentation model with these pseudo masks.\nRecently, several papers used Stable Diffusion (SD) [29] for various kinds of applications targeting unsupervised Se-"}, {"title": "3. Method", "content": "In this section, we introduce our Markov-Map Nearest Neighbor (M2N2) framework in full detail. As shown in Fig. 2, M2N2 is composed of three main stages. In the first stage, we obtain an attention tensor of the input image by aggregating SD's self-attentions. The second stage extracts and enhances the semantic information of the attention tensor by creating a Markov-map for each prompt point. The semantically rich Markov-maps are then utilized in a truncated nearest neighbor algorithm to obtain a training-free unsupervised segmentation.\nIn the following subsections, we first formulate the problem in the context of a nearest neighbor algorithm in Sec. 3.1, followed by an explanation of the acquisition of the attention tensor in Sec. 3.2 and extraction of the Markov-maps including the flood fill approach in Sec. 3.3. Finally, we introduce the full M2N2 algorithm Sec. 3.4."}, {"title": "3.1. Truncated Nearest Neighbor", "content": "In point prompt based segmentation we are given an image \\(I \\in R^{H\\times W\\times 3}\\) of width W and height H and a set of labeled prompt points \\(D = \\{(x_1, y_1), (x_2,y_2), ..., (x_N,y_N)\\}\\) where \\(x_i \\in R^2\\) are the 2D spatial coordinates of each prompt point in image pixel space and the labels \\(y_i \\in \\{0,1\\}\\) denote whether a point \\(x_i\\) belongs to the background \\(y_i = 0\\) or foreground \\(y_i = 1\\). To perform k-NN segmentation with k = 1, we assign each query pixel \\(x_q\\) of our output segmen-"}, {"title": "3.2. Attention Aggregation", "content": "In this paper, we use the pre-trained SD 2. Given an image I, we perform a single denoising step by computing a forward pass through the denoising U-Net and extract the multi-head self-attentions Sj of each transformer block j. Each tensor Sj is of the shape \\(N_{heads} \\times h \\times w \\times h \\times w\\). \\(N_{heads}\\) denotes the number of attention heads and h and w are the height and width of the attention maps respectively. Each attention map Sj[n, k, l, :, :] of the tensor Sj is a probability distribution, meaning the sum of each map's elements is equal to 1. We define the aggregated tensor \\(A \\in R^{h \\times w \\times h \\times w}\\) as follows:"}, {"title": "3.3. From Attention Tensor to Markov-Maps", "content": "First, we flatten the attention tensor \\(A \\in R^{h\\times w \\times h \\times w}\\) to obtain a matrix \\(A \\in R^{(h\\cdot w)\\times (h\\cdot w)}\\). Since A is a right stochas-"}, {"title": "3.4. M2N2: Markov-Map Nearest Neighbor", "content": "Using the one-hot encoded coordinates of each prompt point \\(x_i\\) as start states, we generate a Markov-map \\(M_{x_i}\\) for each prompt point \\(x_i\\). This allows us to construct the following distance function for the truncated nearest neighbor:"}, {"title": "4. Experiments", "content": "Datasets. We evaluate the performance of our approach on 4 public datasets:\nEvaluation Metrics. Following previous work [18, 23], we evaluate our approach by simulating user interaction, in which we place the next click in the center of the largest\nComparison with Previous Work. Tab. 3 compares our approach M2N2 with previous supervised and unsupervised approaches. To the best of our knowledge, M2N2 is the first unsupervised interactive point prompt based segmentation framework utilizing a pre-trained model without requiring any additional training. All other methods either are not based on any deep learning, e.g., GrabCut and related ones, or require the generation of pseudo-labels to train an interactive model, e.g., MIS. Our method surpasses the previous state-of-the-art unsupervised method MIS, which is trained on pseudo labels, on both metrics in three out of four test datasets. We observe the largest improvement on the DAVIS dataset, where we reduced the NoC85 by 1.73 and the NoC90 by 1.72 clicks. We achieve second best results in SBD. A possible explanation for this is that all deep-learning models listed in Tab. 3 are trained on the training set of SBD and therefore might have an advantage on this dataset.\nBaselines. We provide two additional baselines in Tab. 3 which use the same framework as M2N2 but without Markov-maps. Attention Truncated Nearest Neighbor (Attention-NN) uses attention maps as a semantic distance measure. KL-Divergence Truncated Nearest Neighbor (KL-NN) utilizes a symmetric KL-Divergence between the attention map of the prompt point and all attention maps in the attention tensor as distance function. Finally, we also provide a version of M2N2 without flood fill. We observe that the combination of Markov-maps with Flood fill achieves the best results."}, {"title": "4.1. Implementation Details", "content": "We use the SD2 implementation and weights provided by the Hugging Face transformers package. We don't add noise to the encoded image latent to keep the results deterministic and perform the single denoising step with empty text prompts. Due to the large memory requirements of the attention tensors, we run SD on 16-bit floating-point precision and convert it to 32-bit floating-point for the attention aggregation and further processes in our framework. We implement JBU as described in [16] with two modifications. We change the low-resolution solution sampling from sparse to dense sampling for smoother results and ex-"}, {"title": "4.2. Ablation Study", "content": "We perform extensive ablation studies on the hyperparameters of our segmentation algorithm to demonstrate the impact of each component of M2N2.\nAttention Aggregation. Our experiments in Fig. 4 show that higher attention tensor resolution improves NoC significantly for most of our datasets, performing best at a resolution of 128 \u00d7 128 \u00d7 128 \u00d7 128. This is surprising since it requires an input image size of 1024 \u00d7 1024 which is beyond SD2's training resolution of 768 \u00d7 768. We also evaluate the NoC for the SD time step, which is required for the single denoising step. Time steps greater than 200 increase the NoC which we assume is due to the distribution shift caused by not adding noise to the encoded latent. For the aggregation of attention tensors, we evaluate each attention block individually in Tab. 1 and observe that the attention tensors of up0 and up1 individually achieve significantly better NoC than the other layers. Aggregating up0 and up1 results in the best NoC.\nMarkov-maps. Lowering the temperature \\(T\\) of the aggregated tensor A gradually improves the NoC in Fig. 4 up to \\(T = 0.5\\). Smaller values of \\(T < 0.5\\) reduce the entropy and therefore require more iterations in the Markov chain, exceeding the maximum number of iterations and causing numerical instabilities. Different settings of the relative probability threshold \\(\\tau\\) prove to have a relatively low impact on the NoC.\nTruncated Nearest Neighbor. Experiments in Tab. 2 show the contribution of each score function \\(s_{i, .}(\\lambda)\\) to the NoC of the total score function \\(s_i(\\lambda)\\). In the second row we observe that the function \\(s_{i, prior}\\) has the lowest impact on the NoC. Removing \\(s_{i, edge}\\) on the other hand increases NoC90 to 9.99 and is therefore the most important score function."}, {"title": "4.3. Qualitative Results of M2N2", "content": "Fig. 3 compares the semantic maps of our proposed Markov-maps with the other baselines. We observe that Markov-maps are less noisy, have clearer semantic boundaries, and have more uniform values within semantically similar regions. Additionally, we notice that Markov-maps nicely reflect a semantic hierarchy due to the segment size ambiguity of a single prompt point. For example, in the first row and right-most column, the two overlapping flowers have three clearly separated hierarchy levels. Each hierarchical level can be selected by setting a corresponding threshold \u03bb. The first level contains the right flower's ovary, the second contains both flowers together and the final level covers everything. The two overlapping flowers also show the strengths and weaknesses of our flood fill approach. It enables instance segmentation of the right flower's ovary by suppressing the local minimum of the left flower's ovary. On the other hand, flood fill would fail to segment the individual instances of the flowers, because they are overlapping. Please note that M2N2 can still segment overlapping instances by utilizing multiple prompt points.\nLooking at segmentation examples of the DAVIS dataset in Fig. 5, we observe a similar issue in the NoC90 = 10 case, where a single dancer is in front of a semantically similar crowd. An additional disadvantage of flood fill is that obstructions can result in splitting instances into multiple areas. As an example, the segmentation of the rhinoceros requires two prompt points, instead of one, as the obstructing tree splits the instance into two separate regions. Due to the limited attention resolution, M2N2 faces challenges in segmenting thin and fine structures, as shown in the right-most column. In general, we observe that M2N2 generates consistent segments with sharp semantic boundaries with-"}, {"title": "5. Conclusion", "content": "We proposed M2N2, a novel method for unsupervised training-free point prompt based interactive segmentation. By interpreting an aggregated self-attention tensor of Stable Diffusion 2 as a Markov transition operator, we generated semantically rich Markov-maps. We showed that Markov-maps have less noise, clearer semantic boundaries, and more uniform values for semantically similar regions. By combining Markov-maps with truncated nearest neigh-"}]}