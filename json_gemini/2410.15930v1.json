{"title": "Centrality-aware Product Retrieval and Ranking", "authors": ["Hadeel Saadany", "Swapnil Bhosale", "Samarth Agrawal", "Diptesh Kanojia", "Constantin Or\u0103san", "Zhe Wu"], "abstract": "This paper addresses the challenge of improving user experience on e-commerce platforms by enhancing product ranking relevant to users' search queries. Ambiguity and complexity of user queries often lead to a mismatch between the user's intent and retrieved product titles or documents. Recent approaches have proposed the use of Transformer-based models, which need millions of annotated query-title pairs during the pre-training stage, and this data often does not take user intent into account. To tackle this, we curate samples from existing datasets at eBay, manually annotated with buyer-centric relevance scores and centrality scores, which reflect how well the product title matches the user's intent. We introduce a User-intent Centrality Optimization (UCO) approach for existing models, which optimises for the user intent in semantic product search. To that end, we propose a dual-loss based optimisation to handle hard negatives, i.e., product titles that are semantically relevant but do not reflect the user's intent. Our contributions include curating challenging evaluation sets and implementing UCO, resulting in significant product ranking efficiency improvements observed for different evaluation metrics. Our work aims to ensure that the most buyer-centric titles for a query are ranked higher, thereby, enhancing the user experience on e-commerce platforms.", "sections": [{"title": "Introduction", "content": "Achieving a user-focused experience on e-commerce platforms (eBay, Walmart, Amazon, Etsy, JD) is enabled by ranking products relevant to the user's intent expressed via the search query. However, user queries often do not fully reflect the underlying intent behind the search terms used within the query. For example, ambiguous queries like 'iphone 13', or \u2018i5 pc 1tb 16gb 8gb gpu' can lead to many variants. To aggravate the challenge further, user queries can consist of lexical terms with alphanumeric characters, which do not reveal a semantic match within existing product titles. Information Retrieval (IR) systems depend upon semantic similarity/distance between words or phrases used in the search query and the product title. Therefore, ranking the product titles based on only lexical or only semantic query-title match can be a particularly challenging problem, as detailed in the examples below:\nAmbiguous Queries Some queries can be ambiguous and do not clearly reflect the user's intention. From the same example above, for a query like \u2018iPhone 13', the user is most likely looking to buy the base variant or to check out other device variants. However, this intent is not clear from the query, and the system can even rank \u2018iPhone 13 cover' among the top retrieved products. Hence, a major challenge faced by search systems is to retrieve titles that are likely to be relevant to the user intent at high ranks, and push down negative titles such as 'iPhone 13 cover' which have semantic proximity to positive titles within the embedding space of the computational model but may not reflect user's underlying objective.\nRepetition Similar to the example above, the repetition of the exact string of words from a user's query, such as 'iPhone 13', in both relevant and irrelevant titles often renders embeddings-based similarity approaches futile as the proximity of positive and negative titles in the embedding space may not be reflective of their relevancy. In such cases, human annotation towards user intent for a query-title pair is needed to establish a clear ranking among products retrieved by the model.\nAlphanumeric Queries Queries such as \u2018S2716DG\u2019 consist of alphanumeric characters where a letter or number can signify important detail for the product/model. For example, based on the naming convention of PC monitors, a single letter defines the type of panel in the product. In this case, the Dell S2716DG is a 27-inch monitor with a TN panel, and changing the last letter to P would refer to a monitor with an IPS panel. Similarly, product colour or a specific spare part can be identified from such queries. Unless the product title contains this alphanumeric sequence of characters, the semantic similarity between the query and a non-intended product can be high, thus misleading the system.\nIn this paper, we investigate the challenges listed above and take a two-step approach to improve product retrieval and ranking. We curate samples from existing internal datasets at eBay consisting of user search queries paired with retrieved product titles on their platform. These datasets are human-annotated based on detailed guidelines to produce two buyer-centric relevance annotations. First, a widely used relevance ranking schema where query-title pairs are provided a ranked class from among Bad (1), Fair (2), Good (3), Excellent (4) and Perfect (5), where 'perfect' reflects an exact query-title pair match, i.e., the annotator is very confident that the user found precisely what they were looking for, while 'bad' reflects no match between the product and the need expressed in the query (Jiang et al., 2019; Kang et al., 2016). Second, query-title pairs are annotated with a binary centrality score, obtained from majority voting over multiple human annotations, i.e., indicating whether the item reflects the need expressed in the query. The difference between centrality and relevance scoring is that the latter detects whether an item is an outlier, a surprising addition to the recall set, or the item centrally matches the expectations. Figures 1 and 2 show two examples of the centrality annotation for the same query, \"Thomas Sabo charm\". Figure 1 shows a product central to the query since, based on purchase data, this query typically reflects the user's need for a charm (a small ornament worn on a necklace or bracelet). On the other hand, the product in Figure 2 is not central to the user's intent as it is a Thomas Sabo charm attached to a bracelet; the user intent is a charm, not a bracelet. Although both titles are semantically related to the query, based on the degree of specificity expressed in the query, the product in Figure 2 becomes less central to the user's intent and gets annotated with 0 as its centrality score whereas the product in Figure 1 receives 1. We use an internal human-annotated dataset for this task. Henceforth, we refer to it as Internal Graded Relevance or IGR dataset.\nWe extract challenging evaluation sets from the IGR dataset based on the challenges discussed above. Our objective is to increase the retrieval and ranking efficiency of product search by training a model for query-title pairs that integrates the user intent in the similarity algorithm. Given the search query, we propose using a user-intent centrality optimisation (UCO) step for existing models which cater to the ranking of relevant products. Further, we propose utilising a dual-loss based optimisation to address the query-title pairs which constitute hard negatives, i.e., query-title pairs where the product title is semantically relevant to the user's query but is annotated as non-central to the user intent, or has Bad or only Fair annotated relevancy. We hypothesise that there is an unwanted semantic proximity of such negative titles to their search queries in the model embeddings space. To improve search, we optimise the existing ranking model with our dual-loss-based optimisation approach, ensuring that the retrieval algorithm should have the most \u201ctypical\u201d titles for a query ranked highly than other titles which may be relevant but are not typical. Our contributions are 1) curating challenging evaluation sets that cater to this problem and 2) user-intent centrality optimisation (UCO), which results in a stark improvement on all the evaluation sets."}, {"title": "Related Work", "content": "Our work is based on a two-step approach to improve product ranking given a search query for retrieving items. Existing literature on traditional candidate retrieval research focused on learning query rewrites (Bai et al., 2018; Guo et al., 2008) as an indirect approach to bridge the vocabulary gap between queries and documents/titles. Some approaches, including latent semantic indexing with matrix factorization (Deerwester et al., 1990), and with probabilistic models (Hoffman, 1990), and semantic hashing with an auto-encoder (Salakhutdinov and Hinton, 2009), have been proposed. Most of these are unsupervised models based on word co-occurrence in documents/product titles.\nModern IR systems deploy semantic retrieval models as bi-encoders (Muennighoff, 2022) or Siamese networks (Chiang and Chen, 2021) comprising two encoders. Most existing studies focus on designing or pre-training encoders with different representation learning approaches (Gao et al., 2011; Salakhutdinov and Hinton, 2009; Yih et al., 2011; Huang et al., 2020; Liu et al., 2020). Representative works, namely, the Deep Semantic Similarity Model (DSSM) (Huang et al., 2013), and CDSSM (Shen et al., 2014b), are some of the earliest methods which utilise a deep neural network (DNN) using clickthrough data. Subsequently, CNNs (Gao et al., 2014; Shen et al., 2014a,b; Severyn and Moschitti, 2015) and RNNs (Palangi et al., 2014, 2016) have been utilised for semantic retrieval. Recently, new models, including DRRM (Guo et al., 2016) and Duet (Mitra et al., 2017) were developed to include traditional IR lexical matching (e.g., exact matching, term importance) within semantic retrieval performed by DNNs. However, (Mitra et al., 2018) argues that most works proposed in this direction focus on the ranking stage, where the optimisation objectives differ from candidate title retrieval. To further improve the performance of semantic retrieval, Transformer-based Pre-trained Language models (PTLMs) like BERT (Devlin et al., 2018) and ERNIE (Zhang et al., 2019) have been leveraged (Fuchs et al., 2020; Wang et al., 2024; Liu et al., 2021). Using larger pre-trained models, semantic retrieval has observed a significant performance improvement and generalisation for retrieval but without a specific focus on ambiguous or alphanumeric queries, which is what we essentially address in this paper.\nFurther, interaction-based approaches (Moe, 2003; Long et al., 2012; Gu et al., 2020; Yates et al., 2021; Zou et al., 2020; Dai et al., 2023) have also been widely used for IR systems, which further go into semantic matching to model for query-document/title interaction using DNNs (Lu and Li, 2013; Mitra et al., 2017; Wan et al., 2016; Zhao et al., 2020; Kabir et al., 2022). Most of these approaches focus on user personalisation needs, and often rely on hand-crafted rules. Often, such approaches cannot cache the document embeddings offline for faster retrieval, and may be inefficient for retrieval (Liu et al., 2021). (Su et al., 2018) use the results of an online survey and search logs from a commercial product search engine to show that product search falls into categories like Target Finding, Decision Making and Exploration. (Yao et al., 2021) propose Personal Word-embeddings for Personalized Search (PEPS) which uses as additional layer trained on user embeddings and personal logs.\nWhile personalised embeddings and interaction-based approaches improve ranking performance for ambiguous user queries, our work focuses on dealing with similar challenges using a different approach infusing centrality-awareness. To be considered an impactful solution for the challenges at hand, we believe that product ranking approaches can be more generalised compared to personalised embeddings, improving the base retrieval with a focus on user intent. Our approach utilises two existing loss functions that cater to the task and optimise the retrieval model, which can be used at both stages, retrieval and ranking."}, {"title": "Methodology", "content": ""}, {"title": "Baseline Model: eBERT", "content": "For training our system, we employ the in-house multilingual eBERT model. eBERT is trained on item/product data from eBay and general domain (Wikipedia and RefinedWeb) text. The item data used to train this model consists of approximately 3 billion item titles. We also test another eBERT variant, eBERT-siam, which is fine-tuned to generate similar embeddings for item titles using a Siamese network. This model is designed specifically for tasks related to similarity search on query and product titles. Both models are used offline to perform experiments and are optimised with UCO to note performance changes for retrieval and ranking."}, {"title": "User-intent Centrality Optimization (UCO)", "content": "We perform UCO as an optimisation step to overcome the problem of top-ranked, hard negative query-title pairs that are semantically relevant but not central to user intent. Thus, we fine-tune the baseline model with a supervised binary classification task on product centrality. Then, based on our hypothesis for transfer learning capabilities, we employ the knowledge learned from the domain information of centrality optimisation as an inductive bias to boost the ranking capability of a retrieval model, thereby, optimising the ranking task for our challenging evaluation sets. We employ dual-loss optimisation, as explained in the next section."}, {"title": "Dual-Loss Based Optimisation", "content": "Multiple Negative Ranking Loss (MNRL) (Henderson et al., 2017) is the first loss function we employ. MNRL quantifies the difference between positive and negative samples for a query. MNRL is used to create a clear distinction between relevant (positive) and irrelevant (negative) data points, achieved by minimising the distance between the query and positive samples while maximising it for multiple negative samples. Multiple negatives provide more context, enabling the optimisation to discriminate between varying degrees of irrelevance. Mathematically, it can be represented as follows:\nMNRL = $\\sum_{p=1}^{P} \\sum_{n=1}^{N} max(0, f(q, p_i) - f(q, n_j) + margin)$ (1)\nwhere $P$ is the number of positive titles, $N$ is the number of negative titles, $q$ is the query, $f$ is our similarity function, which is cosine similarity, and $margin$ is a hyperparameter defining the optimum distance between positive and negative titles defined by the centrality of the user-intent. The MNRL minimises the distance between (q, pi) while it simultaneously maximises the distance (q, nj) for all P and N titles.\nOnline Contrastive Loss (OCL) is a variant of Contrastive Loss (CL) (Carlsson et al., 2020). OCL attends to negative pairs that have a lower distance than the positive pairs with the largest distance, as well as, the positive pairs that have a higher distance than the lowest distance of negative pairs, i.e., the hard cases in a batch, and computes the loss only for these cases. It selects hard positive (positives that are far apart) and hard negative pairs (negatives that are close), and backpropagates only for such pairs. OCL can be represented as follows:\nOCL = Y * D + (1 \u2212 Y) * max(margin \u2013 D,0)$^{2}$ (2)\nwhere Y is our centrality score between the query and title, it will be 1 if the title is central to the user intent and 0 if it is not. The D variable is the function that returns the distance between the query and title embeddings, which is the cosine similarity in our case. The max function takes the largest value of 0 and the margin minus the distance. The negative samples (centrality = 0) should have a distance of at least the margin value which we empirically set during training. This means that if we define some radius/margin, all the central titles should fall inside this margin, and all the non-central ones should fall outside.\nMNRL primarily reduces the distance between positive pairs out of a large set of possible candidates and hence works particularly well when the dataset has a significant number of positives, which caters to the dataset skew in our case. However, MNRL does not push dissimilar pairs away. Therefore, we combine both losses for better optimisation (see below for ablation results)."}, {"title": "Experiment Setup", "content": ""}, {"title": "Dataset Curation", "content": "We preprocess all query-title pairs from the IGR dataset by filtering out non-English pairs to ensure linguistic consistency and relevance. Once preprocessed, we select queries that have both the corresponding positive titles (relevancy > 3) and negative titles (relevancy < 3) from the IGR dataset. This selection forms our initial split, referred to as Common Queries (CQ). We observed a notable imbalance towards positive query-title pairs in CQ, stemming from the inherent nature of e-commerce product listings and the data collection strategy highlighted in Section 1, which emphasises capturing relevant matches. To address this imbalance and ensure a fair comparison, we introduce a balanced version of CQ, where the number of positive and negative query-product title pairs is approximately equal, referred to as CQ-balanced.\nUpon examining the query-title pairs, as also discussed in Section 1, we found that often, the exact string of a query appears in both positive and negative product titles. We isolate these query-title pairs to form our third split, named CQ-common-str (see Figure 4). This task necessitates considering both, user centrality and semantic connections between the query and product titles. We conduct a correlation test, and observe that Pearson, Kendall and Spearman correlations between the graded relevance score and the binary centrality score are 0.78, 0.73 and 0.77, respectively, validating our assumption that both types of scores are highly correlated and hence the ranked results are expected to conform with the overall pattern of the dataset.\nLastly, to facilitate the evaluation of our proposed methodology specifically on alphanumeric query-title pairs, we create a separate split containing only queries and titles with alphanumeric characters, referred as CQ-alphanum. For each evaluation split, all the positive and negative titles constitute the retrieval corpus, while we create distinct development and test query sets in an 80:20 ratio. Table 1 shows the number of entries in the corpus and query sets for each split. The development query set assists in selecting the best-performing UCO model (i.e., during optimisation on user-intent centrality), while the unseen test query set validates the ranking capability of UCO."}, {"title": "Implementation Details", "content": "We optimise both the encoder backbones on the centrality score classification-train split for a maximum of 10 epochs. During training, we run two sequential evaluators on both the centrality scores and the retrieval ranking in the curated IGR datasets. First, an evaluator that will compute the embeddings for both query and title and use them to calculate the cosine similarity. If the similarity is above a threshold, we have a central title. Second, given a query and the corpus of all titles, the evaluator finds the most relevant product title to the query (top 3, 5 and 10 titles). During optimisation, we save the checkpoint that performs best on the second evaluator. For all experiments, we use a batch size of 32, with the Adam optimiser and 2e \u2013 05 as the learning rate, and 0.01 as weight decay. Optimising one encoder backbone using the above parameters takes 30 hours on a single NVIDIA V100 GPU. For evaluation, we use cosine similarity as scoring function.\nEvaluation Metrics We use different existing evaluation metrics to measure the overall model performance. Precision@k measures the proportion of relevant products in the top-k recommendations (considering their relevance), while Recall@k measures the proportion of relevant products that were retrieved among all relevant products (irrespective of their rank). NDCG (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) measures the ranking quality by comparing the recommended items' order against an ideal ranking. As a result, NDCG considers both the relevance and rank of the recommended products. Mean Reciprocal Rank (MRR) evaluates the average rank of the first relevant item across all queries. A high MRR is an indication of being able to provide users with relevant products ranked as high as possible."}, {"title": "Results and Discussion", "content": "Considering various aspects like retrieval and ranking quality, we analyse model performance using a diverse set of metrics (explained in \u00a74.1). We also perform an ablation test on the eBERT model to identify the contribution of both loss functions, MNRL and OCL, and discuss the qualitative analysis below. Table 2 displays the results for each of the evaluation splits, CQ, CQ-balanced, CQ-common-str and CQ-alphanum. Across each split, a consistent pattern emerges: the incorporation of UCO leads to a substantial improvement in product retrieval performance across all metrics. This improvement is evident regardless of whether the backbone encoder employed is eBERT or eBERT-siam. This highlights UCO's capability to enhance an existing model's embedding space, enabling it to capture semantic relationships between user queries and product titles attuned to the user intent, thus retrieving products with high user centrality. It is evident that BERT, a publicly available model, was unable to capture query-title relations given it was not pre-trained on internal data. Even with internal models, the results without UCO show the challenge posed by these evaluation splits curated for this work. For alphanumeric queries, the NDCG performance improvement ranges from 7% points for the base model to 47% points, including the model fine-tuned with the Siamese approach, demonstrating the efficacy of UCO. For query-titles with common strings, it ranges from 8% to 58% points. We also see similar improvements in all metrics, for the other two evaluation sets.\nLoss Ablation We conducted a quick ablation test over the CQ evaluation split. For this test, we fine-tuned the eBERT and eBERT-siam models using individual loss functions and their combination, which is our finalised approach. From Table 3, it is clear that the combination of both loss functions helps improve performance for both models. We evaluate this using both NDCG and MRR evaluation metrics. When employed individually, MNRL seems to outperform OCL in both metrics. Overall, dual-loss based optimisation emerges as a clear winning strategy.\nQualitative Analysis We discuss the performance improvement shown by UCO with two examples in Figures 5 and 6, shown in the Appendix below. We use the eBERT-siamese model to rank retrieved products with and without UCO optimisation."}, {"title": "Conclusion and Future Work", "content": "This work addresses product search queries that represent an important challenge for e-commerce platforms. The main challenge occurs when the retrieved titles are semantically relevant, but not central to the user-intent as is reflected by the specificity of the query. The challenge is even greater with ambiguous queries where the same query string is present in both relevant and irrelevant titles as well as when queries are alphanumeric. We address the semantic complexity of these challenging query-title pairs by fine-tuning existing internal models with a user-intent centrality optimisation (UCO) step to infuse information about the typicality of query-title pairs. The retrieval model performance showed significant improvement with several hard example datasets with a dual-loss based optimisation approach, which pays attention to negative pairs that have a lower distance than the positive pairs with the largest distance. The dual-loss based optimisation helps in separating the irrelevant pairs of queries and titles while keeping the distance smaller for relevant query-title pairs. The improvement in ranking performance demonstrated by our approach helps identify and categorise what users intend to find online when they search the platform.\nIn future, we aim to restructure queries in our hard-negative pairs to be less ambiguous. Leveraging GenAI-based prompt engineering and explainability using approaches like chain-of-thought, we can investigate titles that indicate typical queries, aligning them closer to the user intent, and moving towards explainable product retrieval."}]}