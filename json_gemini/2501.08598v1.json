{"title": "LlamaRestTest: Effective REST API Testing with Small Language Models", "authors": ["MYEONGSOO KIM", "SAURABH SINHA", "ALESSANDRO ORSO"], "abstract": "Modern web services rely heavily on REST APIs, typically documented using the OpenAPI specification. The widespread adoption of this standard has resulted in the development of many black-box testing tools that generate tests based on these specifications. Recent advancements in Natural Language Processing (NLP), particularly with Large Language Models (LLMs), have enhanced REST API testing by extracting actionable rules and generating input values from the human-readable portions of the specification. However, these advancements overlook the potential of continuously refining the identified rules and test inputs based on server responses. To address this limitation, we present LlamaRestTest, a novel approach that employs two custom LLMs to generate realistic test inputs and uncover parameter dependencies during the testing process by incorporating server responses. These LLMs are created by fine-tuning the Llama3-8b model, using mined datasets of REST API example values and inter-parameter dependencies. We evaluated LlamaRestTest on 12 real-world services (including popular services such as Spotify), comparing it against RESTGPT, a GPT-powered specification-enhancement tool, as well as several state-of-the-art REST API testing tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results demonstrate that fine-tuning enables smaller LLMs to outperform much larger models in detecting actionable rules and generating inputs for REST API testing. We also evaluated different tool configurations, ranging from the base Llama3-8B model to fine-tuned versions tailored for REST API testing, and explored multiple quantization techniques, including 2-bit, 4-bit, and 8-bit integer formats, to improve efficiency and performance. Our study shows that small language models can perform similar to, or better than, large language models in REST API testing, striking a balance between effectiveness and efficiency. Furthermore, LlamaRestTest outperforms state-of-the-art REST API testing tools in code coverage achieved and internal server errors identified, even when those tools utilize enhanced specifications generated by RESTGPT. Finally, through an ablation study, we show that each of the novel components of LlamaRestTest contributes to its overall performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Web services often rely on Representational State Transfer (REST) APIs for efficient communication and data exchange. REST, a paradigm established by Roy Fielding [22], emphasizes scalability and flexibility, enabling seamless interactions between diverse software systems through standard web protocols [46]. REST APIs are pivotal in modern software architecture, facilitating operations over the Internet without requiring client and server to know details about each other beyond the shared understanding communicated through the API itself [48].\nPlatforms like APIs Guru [6] and Rapid API [47] highlight the prevalence of REST APIs by hosting a vast collection of REST API specifications and emphasizing the critical role of clear, standardized documentation. REST APIs are typically documented using structured languages such as OpenAPI [45], RAML [42], and API Blueprint [4]. These specifications provide essential API details, such as available operations, data formats, and response types, offering a structured way of describing REST APIs, thus enhancing comprehensibility and compatibility with software tools.\nThis structured approach to documentation sparked the development of many automated black-box testing tools for REST APIs (e.g., [8, 16, 26, 29, 30, 33, 37, 39, 54, 58]).\nHowever, despite advancements in REST API testing tools, significant challenges remain due to constraints and inter-parameter dependencies among API parameters [35, 38]. Current REST API testing tools employ different techniques, such as breadth-first search (BFS), depth-first search (DFS), combinatorial testing, evolutionary algorithms, random-based approaches, and Q-Learning, to address these challenges. Among these tools, NLP2REST [32] employs Natural Language Processing (NLP) to extract actionable REST API rules and converts them into machine-readable formats for REST API testing tools. Similarly, ARTE [2] generates realistic parameter inputs by extracting key information from parameter names and descriptions and querying knowledge bases such as DBPedia [13] for input values. RESTGPT [34] uses Large Language Models (LLMs) to perform both actionable rule extraction and test input generation. Although these tools achieve considerable success in enhancing API specifications and improving testing effectiveness, they lack the ability to dynamically interact with testing tools during execution. This limitation prevents them from refining test inputs and rules based on real-time server feedback, leaving both critical inter-parameter dependencies and semantically valid value generation under-explored.\nTo address these limitations, we present LlamaRestTest, the first black-box testing approach that leverages the human-like reasoning capabilities of LLMs [44, 56] to take into account dynamic server feedback during testing. The core novelty of LlamaRestTest lies in its two components, LlamaREST-IPD and LlamaREST-EX, which are LLMs created by fine-tuning the Llama3-8b model [41] for the tasks of identifying inter-parameter dependencies and generating input values for operation parameters, respectively. LlamaREST-IPD decides which parameters to select for each API, whereas LlamaREST-EX determines what values to assign to each parameter. LlamaREST-IPD is fine-tuned using a database used in a study by Martin-Lopez [38], which includes a range of parameters and natural-language descriptions of their dependencies. We parse these parameters and their associated actionable inter-parameter dependency rules using RESTGPT [34] and manually validate the rules to create the fine-tuning database. LlamaREST-EX is fine-tuned on a dataset of example values mined from over four thousand API specifications, containing more than 1.8 million parameters, from APIs-guru [6]. We collected the parameters which contain example values, and through fine-tuning, we enhance LlamaREST-EX's ability to generate realistic testing inputs for REST APIs. Both these models are then quantized to reduce their sizes, resulting in smaller, faster, streamlined model versions. We integrated these LLMs with the ARAT-RL framework [33], which is a reinforcement learning based REST API testing tool. When ARAT-RL struggles to process an API operation, LlamaREST-IPD and LlamaREST-EX utilize both parameter descriptions and server"}, {"title": "2 BACKGROUND AND MOTIVATING EXAMPLE", "content": "2.1 REST APIs and OpenAPI Specification\nREST APIs are web APIs that adhere to RESTful principles, enabling data exchange between clients and servers using protocols like HTTP [12]. Key concepts of REST include statelessness, where each request is independent; cacheability, allowing response storage; and a uniform interface that simplifies client-server interactions [22, 48]. Clients interact with web resources via HTTP requests, performing operations such as creating, reading, updating, or deleting data using methods like POST, GET, PUT, and DELETE. Each HTTP method paired with a resource endpoint defines an operation, with requests containing headers, payloads, and metadata. Responses include headers, bodies, and status codes indicating success (2xx), client errors (4xx), or internal server errors (500), forming the foundation of RESTful communication.\nThe OpenAPI Specification (OAS) [45], which evolved from Swagger in version 2 to the OpenAPI Specification in version 3, is a crucial standard in the realm of RESTful API design and documentation. As an industry-standard format, OAS delineates the structure, functionalities, and anticipated behaviors of APIs in a standardized and human-accessible way. Figure 1 presents a portion of the Ohsome API's OpenAPI Specification. This example highlights the structured approach of OAS in defining API operations. For instance, it specifies a GET request at the /elements/area endpoint, aimed at calculating the area of OpenStreetMap (OSM) elements. This operation contains multiple query parameters, such as bboxes, bcircles, and bpolys, each requiring specific coordinate formats and types. Parameters like filter, format, and time are also included, offering additional"}, {"title": "2.2 REST API Testing and Tools for Enhancing OpenAPI Specifications (OAS)", "content": "REST API testing is a critical practice for ensuring the proper functioning of web services [51]. Leveraging REST API documents such as OAS, numerous black-box REST API testing tools have emerged [8, 16, 26, 29, 30, 33, 37, 39, 54, 58]. These tools aim to detect issues like internal server errors (e.g., HTTP 500 status code) by systematically exploring API endpoints and their parameters. They primarily rely on the machine-readable parts of API specifications [32, 35]. However, these tools often fall short in complex scenarios, particularly when handling specific data formats or inter-parameter dependencies. For instance, when testing operations in the Ohsome API, they may generate random string values for parameters such as bboxes, bcircles, and bpolys, failing to account for their specific constraints.\nAssistant tools like NLP2REST [32] and RESTGPT [34] enhance REST API documents with natural language descriptions, offering better support for REST API testing tools. For example, RESTGPT reads all parameter descriptions in OAS and augments them with machine-readable rules. The key rules include type/format, constraints, example values, and inter-parameter dependencies. Inter-parameter dependencies, as studied by Martin-Lopez et al., refer to the relationships between API parameters where the value of one parameter depends on the value of another, ensuring that valid and contextually appropriate combinations of parameters are generated for accurate API testing [38]. Among these, we particularly focus on example values and inter-parameter dependencies because the type/format and constraints rules are ultimately aimed at generating valid parameters for REST API testing tools."}, {"title": "2.3 Large Language Models", "content": "2.3.1 Overview. Large Language Models (LLMs), such as the Generative Pre-trained Transformer (GPT) series, are at the forefront of advancements in natural language processing (NLP) [25]. A language model is essentially a type of artificial intelligence that understands, interprets, and generates text in a manner similar to how humans use language [44]. These models are trained on extensive collections of written text, enabling them to learn a wide range of linguistic patterns and styles. The GPT series, including well-known models like GPT-3, exemplifies these advanced LLMs [14]. Trained on diverse and vast datasets, they have demonstrated remarkable ability in producing text that is strikingly human-like, making them valuable for a variety of applications. This has expanded the horizons of NLP, leading to its application in challenging areas ranging from education to customer service [10, 17]. GPT-3 and its successors have shown the effectiveness of LLMs, particularly in their versatility to handle a wide array of language-related tasks [44].\n2.3.2 Quantization. Quantization is a technique used to reduce the computational resources required while minimizing the loss in model performance [28]. It involves converting the standard floating-point representations of a model's weights and activations into lower-precision formats, such as 8-bit or 4-bit, or even lower [19, 36]. The primary advantage of quantization is the significant reduction in memory usage and computational demand [1].\n2.3.3 Fine-Tuning. Fine-tuning is a technique for adapting pre-trained LLMs for specific tasks, allowing them to perform better on specialized datasets by adjusting their learned representations. A notable advancement in this area is the introduction of Lora (Low-rank Adaptation of Large Language Models), which modifies only a small set of parameters during training, specifically the attention mechanism's low-rank components. This approach minimizes the computational overhead and reduces the number of parameters updated, facilitating efficient fine-tuning of"}, {"title": "2.4 Motivating Example", "content": "REST API operations often present significant challenges for traditional testing tools, particularly when managing complex parameter formats and inter-parameter dependencies. The Ohsome API, which calculates the area of OpenStreetMap (OSM) elements (see Figure 1), illustrates this complexity. Consider the /elements/area operation in the Ohsome API. According to the OpenAPI specification (see Figure 1), this operation requires one of three mutually exclusive boundary parameters: bboxes, bcircles, or bpolys, each with specific and intricate formatting rules. For instance, bboxes requires WGS84 coordinates in the format \u201clon1,lat1,lon2,lat2\u201d, whereas bcircles requires coordinates along with a radius in meters. The complexity is further compounded by the filter parameter, which requires valid OSM tags, the format parameter, which controls the output format, and the time parameter. Current state-of-the-art testing tools often fail in these scenarios by generating invalid API requests, without considering the parameter value format and the inter-parameter dependencies.\nFor example, these tools might generate requests that include both bboxes and bcircles, violating the mutual exclusivity requirement, and produce incorrectly formatted coordinates. Although REST API testing assistant tools like RESTGPT enhance the specification by providing more detailed descriptions-such as the correct WGS84 coordinate format\u2014they still struggle with understanding and enforcing inter-parameter dependencies. For instance, RESTGPT fails to recognize the mutual exclusivity of bboxes, bcircles, and bpolys, and it does not provide valid values for the filter parameter, which must adhere to the OSM tagging schema (e.g., \u201camenity=school\u201d or \"building=residential\u201d). These issues reduce test coverage and effectiveness.\nLlamaRestTest addresses these challenges with an adaptive two-stage approach involving LlamaREST-EX and LlamaREST-IPD, both fine-tuned for REST API testing purposes. Initially, LlamaRestTest generates inputs randomly. If the request fails, LlamaRestTest uses LlamaREST-IPD and LlamaREST-EX to identify valid parameter sets and values. For example, if the initial request does not specify any boundary parameters, the response may return an error message such as: \u201cThe query did not specify any parameter. Please remember: You need to define one of the boundary parameters (bboxes, bcircles, bpolys).\u201d Similarly, if a request contains more than two boundary parameters, the server would respond \u201cThe query should not have more than one boundary parameters (bboxes, bcircles, bpolys).\u201d\nLlamaREST-IPD processes the operation's parameter descriptions along with the server's response to identify inter-parameter dependencies. In this case, it detects from the server message that only one boundary parameter can be defined at a time. Using this feedback, LlamaREST-IPD refines the next request, ensuring that only one valid boundary parameter (e.g., bboxes) is included. LlamaREST-EX then enhances this process by generating semantically valid values for the parameters, such as correctly formatted bboxes values (\u201clon1,lat1,lon2,lat2\") or appropriate OSM tags for the filter parameter. By combining the adaptive learning from LlamaREST-IPD with the input-generation capabilities of LlamaREST-EX, LlamaRestTest can effectively create valid parameter sets that comply with the API's constraints and formatting rules. In addition to this, we also explore the benefit of efficiency through quantization."}, {"title": "3 OUR APPROACH", "content": "Figure 2 presents an overview of LlamaRestTest. LlamaRestTest is designed to address the challenges of REST API testing, particularly in handling complex inter-parameter dependencies and generating semantically valid inputs. In this section, we discuss the key features of LlamaRestTest, including model selection, database construction, fine-tuning, and quantization, which together enable adaptive, efficient, and effective testing. These components work in synergy to dynamically generate and refine API requests based on server feedback, improving both test coverage and fault detection.\n3.1 Base Model Selection\nLlamaRestTest uses language models to identify Inter-Parameter Dependencies (IPDs) and generate example values for API parameters. Assistant tools, such as RESTGPT, which leverages OpenAI's ChatGPT-3.5 Turbo model to enhance OpenAPI specifications with rules for REST API testing, perform similar tasks, including IPD identification and example value generation. However, the cost-approximately $10 per specification-poses a significant scalability challenge, especially when considering the additional expenses associated with dynamic server interactions beyond static specifications. Moreover, RESTGPT focuses solely on static specifications, whereas our approach seeks to leverage server response messages dynamically. Given the potential exposure of sensitive information in REST API testing scenarios, we also prioritized using open-source models that can be hosted on user's computing resource.\nOur goal was to employ powerful yet lightweight language models to reduce costs, improve speed, and ensure compatibility with various computing environments, including CPU-based systems. However, we encountered challenges with LLMs, such as Llama3-8B, the smallest in the Llama3 family, which still requires considerable GPU resources [20]. Additionally, Llama3-8b lacked the capability to generate valid IPD rules and example values tailored for REST APIs when tested using prompts from RESTGPT, as shown by our results (Table 3 and Table 4).\nTo address these issues, we applied fine-tuning to enhance the model's capabilities for REST API testing and employed quantization techniques to make the model more lightweight. Another reason for selecting Llama3-8B as our baseline model is its architecture's support for various quantization options, including popular 2-bit, 4-bit, and 8-bit integer formats [23, 52, 60]. Llama3 is also open-source and offers a GPT-Generated Unified Format (GGUF) transformation option-a binary format designed for fast model loading and saving.\n3.2 Database Construction\nFigure 2 illustrates how we constructed the fine-tuning databases. We systematically created two databases: the Inter-Parameter Dependency (IPD) database and the Example (EX) database. The goal of this process was to collect parameter names, descriptions, and associated rules for the"}, {"title": "3.3 Fine-Tuning and Quantization", "content": "3.3.1 Training Dataset Preparation. Datasets for IPD and EX were preprocessed and reformatted into a custom tokenized format suitable for fine-tuning. For each parameter, a structured format with special tokens was used to provide context and guide the model's training. For LlamaREST-IPD, the input sequences follow this structure:\n<s>[INST] Find Inter-Parameter Dependency for the parameter below\nname: travelMode\ndescription: If startTime is present, travelMode must be 'driving'. [/INST]\nIF startTime THEN travelMode == 'driving';</s>\nThis format includes the parameter's name and description, followed by the dependency rule in a structured manner, enclosed with special tokens like \u2018<s>\u02bb and \u2018[INST]\u2018. Here, \u2018<s>\u2018 marks the start of the prompt, \u2018[INST]\u02bb indicates the start of the instruction, \u2018[/INST]\u02bb marks the end of the instruction, and \u2018</s>\u02bb denotes the end of the prompt. The text between \u2018<s>[INST]\u02bb and \u2018[/INST]\u02bb is the instruction, which provides the context, and the text after \u2018[/INST]\u02bb presents the expected rule. This setup allows the model to learn both the natural language description and the corresponding logic for inter-parameter dependencies.\nFor LlamaREST-EX, the data is formatted to extract and list valid example values for each parameter. An example of this structure is:\n<s>[INST] Find example values for the parameter below in a list format\nname: Content-Type\ndescription: The content type. [/INST]\n['application/x-www-form-urlencoded', 'application/json', ...]</s>\nThis tokenization ensures that the model understands the task of generating example values for each parameter in list format. The input includes the parameter name and description, while the output provides a list of valid example values. We experimented with different tokenization schemes and ultimately chose Llama2 tokens, as they are shorter and more efficient than Llama3 tokens. Since Llama3's agentic token support was not required for REST API testing tasks, the simpler Llama2 tokens were a better fit. The structured format and custom tokens facilitate efficient parsing of inter-parameter dependencies and example values during training.\n3.3.2 Parameter-Efficient Fine-Tuning. Fine-tuning plays a pivotal role in enhancing the performance of LlamaREST-IPD and LlamaREST-EX by allowing the models to adapt to specific tasks, such as inter-parameter dependency (IPD) detection and example value extraction. Fine-tuning can be approached in two ways: Full Parameter Fine-Tuning (FFT) and Parameter-Efficient Fine-Tuning (PEFT). While FFT requires large datasets and extensive computational resources, PEFT is designed to work efficiently with smaller datasets, making it a better fit for our task.\nSince our IPD dataset contains only 551 parameters, it is insufficient for FFT, as prior research has shown that fewer than 1,000 examples do not yield significant performance improvements with FFT [57]. To address this, we employed PEFT, which optimizes the model by fine-tuning only a subset of its parameters. This selective updating process allows it to efficiently adapt to new tasks using smaller datasets and fewer computational resources.\nThe two most commonly used PEFT methods are LoRA (Low-Rank Adaptation)[27] and QLoRA[18]. Both approaches allow fine-tuning with significantly reduced computational requirements. LoRA focuses on injecting trainable low-rank matrices into transformer layers, while QLoRA quantizes model weights during training, further reducing memory usage and computational costs. Given the limitations of our compute environment, which includes a Google Colab machine with a 40GB A100 GPU, we opted for QLoRA. QLoRA's quantization compresses the model, allowing it to efficiently fit within our memory constraints while still performing the necessary updates.\nWe based our fine-tuning configuration on the recommended settings from Meta's official guidelines [40], making necessary adjustments to accommodate our hardware. Specifically, we reduced the batch size to 8 to fit within the 40GB VRAM of the A100 GPU. The fine-tuning process leverages a structured dataset specifically designed for inter-parameter dependency (IPD) detection"}, {"title": "3.4 REST API Testing", "content": "Figure 2 illustrates the LlamaRestTest framework, which performs adaptive REST API testing by integrating the ARAT-RL (Adaptive REST API Testing with Reinforcement Learning) approach [33] with fine-tuned models, LlamaREST-IPD and LlamaREST-EX. The process begins with the Specification Parser, which extracts key elements from the OpenAPI specification, including available operations and required parameters. Based on this, ARAT-RL initializes a Q-Table that prioritizes operations and parameters by assigning higher values to more frequently used resources, indicating their relative importance.\nOnce initialized, ARAT-RL applies an e-greedy strategy to balance exploration (testing less-explored operations) and exploitation (focusing on previously promising combinations). The Operation Selector chooses an API operation based on its Q-value, followed by the Parameter Selector, which assigns probabilities to parameters, with more frequently used parameters being favored. The Value Generator produces parameter values, typically chosen randomly but in conformance with the expected types.\nWhen repeated failures occur (e.g., multiple 4xx responses), LlamaREST-IPD and LlamaREST-EX are activated. The LlamaREST-IPD model uses the same prompt employed during fine-tuning:\n<s>[INST] Find Inter-Parameter Dependency for the parameter below\nname: {parameter name}\ndescription: {parameter description and server response message} [/INST]\nThis prompt directs the model to analyze parameter descriptions and server response messages, identifying dependencies, such as when a parameter is required only if another is present.\nSimilarly, LlamaREST-EX generates example values for parameters using the following prompt:\n<s>[INST] Find example values for the parameter below in a list format"}, {"title": "4 EVALUATION", "content": "In this section, we present the results of empirical studies conducted to assess LlamaRestTest, focusing on answering the following research questions:\n(1) RQ1: How do LlamaREST-EX and LlamaREST-IPD compare with the state-of-the-art REST API testing assistant tool, RESTGPT, in terms of identifying inter-parameter dependencies and generating semantically valid testing inputs for REST API testing?\n(2) RQ2: How do fine-tuning and quantization affect REST API testing in terms of achieved code coverage and operation coverage?\n(3) RQ3: How does LlamaRestTest compare with state-of-the-art REST API testing tools in terms of achieved code coverage and operation coverage?\n(4) RQ4: In terms of error detection, how does LlamaRestTest perform in triggering 500 (Internal Server Error) responses compared to state-of-the-art REST API testing tools?\n(5) RQ5: How do parameter description, server response messages, LlamaREST-IPD, and LlamaREST-EX contribute to the overall performance of LlamaRestTest in terms of code coverage and operation coverage?\n4.1 Experiment Setup\nOur experimental framework was set up on an M1 MacBook Pro running Sonoma 14.4.1 with 64GB of memory. We installed all the services from our benchmark suite, along with the REST API testing tools we intended to evaluate. To prevent any dependency conflicts, we restored the database for each service before running each test. Each service and tool was hosted with default settings and tested sequentially to avoid interference. We continuously monitored CPU and memory usage to ensure that resource limitations did not impact the performance of the testing tools. Each tool was tested for one hour, a standard duration in the field [24, 33, 35, 37, 63], to provide a fair comparison. To minimize the effect of randomness on the results, we repeated each test 10 times and averaged the outcomes except for the ablation study (RQ5 used the 3 runs setting).\nFor a thorough evaluation, we used the same REST API testing tools and services employed by ARAT-RL [33], the most recent REST API testing tool available. ARAT-RL also features the"}, {"title": "4.2 RQ1: Comparison with RESTGPT", "content": "To assess the effectiveness of LlamaREST-IPD in identifying inter-parameter dependency (IPD) rules and LlamaREST-EX in generating semantically valid testing inputs, we compared both models with RESTGPT, a state-of-the-art tool for enhancing specifications. The comparison utilized the RESTGPT dataset, which includes parameter names, descriptions, inter-parameter dependency rules, and the corresponding values generated by RESTGPT. Additionally, we evaluated Llama3-8B, the base model for both LlamaREST-IPD and LlamaREST-EX, along with various quantization variants (2-bit, 4-bit, and 8-bit).\nTable 3 presents seven services from the RESTGPT benchmark, alongside the precision of semantically valid value generation for LlamaREST-EX, RESTGPT, and Llama3-8B. A recent study proposed evaluating value generation based on semantic validity, a method also adopted by REST-GPT: a semantically valid input must be coherent with the API domain [2]. For instance, \"Berlin\" is a valid input for the parameter addressLocality in DHL's API, whereas \"dog\" is not. For RESTGPT, we used the generated values from its artifact, while for Llama3-8B, we applied the same prompts used by RESTGPT. Precision was determined by two external developers independently assessing whether the generated values were semantically valid, with any discrepancies resolved through discussion until a consensus was reached.\nLlama3-8B, while competitive on some APIs, generally underperforms with an average accuracy of just 24.94%. It struggles significantly on APIs like Spotify (0%) and Ohsome (2.78%), underscoring its limitations in generating semantically valid values. However, fine-tuning the model, as demonstrated with LlamaREST-EX, leads to significant performance improvements.\nLlamaREST-EX, especially in its FT version, shows substantial gains over Llama3-8B, achieving an average accuracy of 72.44%. For example, on Ohsome, the FT version of LlamaREST-EX reaches 92.16%, outperforming RESTGPT's 76.83%, and on Genome-Nexus, it scores 57.02%, surpassing REST-GPT's 36.25%. Overall, LlamaREST-EX with fine-tuning outperforms RESTGPT, which achieved 68.82%, compared to LlamaREST-EX's 72.44%. Even the quantized versions of LlamaREST-EX (4-bit and 8-bit) show reasonable performance, with 60.21% and 66.12% precisions, respectively.\nTable 4 presents the available inter-parameter dependency rules identified through parameter descriptions for three services among the seven services in Table 3. Llama3-8B demonstrates limited capability in detecting IPD rules, identifying only 2 rules across all services. LlamaREST-IPD, particularly in its fine-tuned (FT) version, significantly outperforms both Llama3-8B and RESTGPT. The FT variant of LlamaREST-IPD identifies 12 correct IPD rules, compared to RESTGPT's 9, highlighting its ability to capture more complex relationships. Even the quantized versions of LlamaREST-IPD (4-bit and 8-bit) demonstrate consistent improvements in performance, while the 2-bit version identified only 1 correct IPD rule.\nFine-tuning and quantization of LlamaREST models significantly improve the accuracy of semantically valid input generation and inter-parameter dependency rule detection, outperforming both the base model and state-of-the-art RESTGPT."}, {"title": "4.3 RQ2: Impact of Fine-Tuning and Quantization in REST API Testing", "content": "To evaluate the impact of fine-tuning and various quantization levels on REST API testing, we assessed LlamaRestTest under five configurations: the vanilla Llama3-8B model, a fine-tuned version, and quantized versions at 8-bit, 4-bit, and 2-bit. The evaluation was conducted across 12 services,"}, {"title": "4.4 RQ3: Comparison with State-of-the-art REST API Testing Tools", "content": "Using the same services from Section 4.3, we measured operation coverage and code coverage, including branch, line, and method coverage, to compare the effectiveness of REST API testing with state-of-the-art tools. Figure 4 presents the code coverage achieved by MoRest, RESTler, ARAT-RL, EvoMaster, and LlamaRestTest across various open-source REST APIs in our benchmark, showing the percentages of branch, line, and method coverage. In this experiment, the testing tools used enhanced version of specification generated by RESTGPT."}, {"title": "4.5 RQ4: Fault-Detection Capability", "content": "Table 2 presents the fault-detection capability of five REST API testing tools-MoRest, RESTler, ARAT-RL, EvoMaster, and LlamaRestTest-measured by the number of faults detected across all services in our benchmark. We used the 500 internal server error counting script provided by the ARAT-RL artifact, which automatically counts the 500 errors in the server log and discards duplicate entries. The sum of faults detected by each tool illustrates their overall effectiveness in identifying bugs across a range of open-source services.\nLlamaRestTest demonstrated the highest fault-detection capability, with a total score of 204. This represents a 45.7% improvement over MoRest (140), a 56.9% improvement over RESTler (130), a 27.5% improvement over ARAT-RL (160), and a 56.9% improvement over EvoMaster (130). The results clearly show that LlamaRestTest outperforms all other tools, detecting more faults across diverse services.\nThe most notable detected errors were in the Ohsome and Spotify cases. For Ohsome, LlamaRestTest was the only tool to detect faults, identifying 36 bugs where all other tools failed. Similarly, LlamaRestTest was the only tool to find faults in Spotify, detecting 8 errors where all others found none. Both cases involve processing complex scenarios. For example, the GET /playlists/playlist_id/tracks operation in Spotify's API requires specific knowledge of how Spotify generates playlist_id. Spotify generates IDs for playlists that are typically 22 characters long, with constraints on the permitted characters and patterns. While many tools fail to generate valid IDs, LlamaREST-EX uses its LLM to accurately retrieve and generate valid Spotify playlist IDs. As a result, LlamaRestTest is able to successfully query the GET /playlists/playlist_id/tracks operation, triggering a chain of interactions across the service. For instance, after retrieving the International Standard Recording Code (ISRC) from the playlist's tracks, the mutation process"}, {"title": "4.6 RQ5: Ablation Study", "content": "To assess the contribution of each key component in LlamaRestTest, we conducted an ablation study by systematically removing specific features and measuring the resulting impact on method, branch, and line coverage across three runs. Table 7 presents the coverage achieved by LlamaRestTest with and without these components, allowing us to evaluate how much each feature contributes to the overall performance.\nThe full LlamaRestTest system achieved method coverage of 55.8%, branch coverage of 28.3%, and line coverage of 55.3%. Removing the parameter description feature resulted in a noticeable drop, with method coverage decreasing by 10.6%, branch coverage by 4.9%, and line coverage by 8.1%. This suggests that including parameter descriptions improves test case generation by providing better information to generate valid inputs. Interestingly, excluding the server response feature caused an even more significant drop, particularly in branch coverage, which decreased by 15.5%. Method and line coverage also fell by 10.9% and 11.4%, respectively. While previous studies in this area have focused primarily on enhancing specifications, these results highlight the critical role of server feedback in guiding test case generation and increasing coverage by detecting dynamic behaviors during runtime.\nThe removal of the LlamaREST-IPD (Inter-Parameter Dependency) module had the most substantial impact on performance, reducing method coverage by 16.5%, branch coverage by 21.9%, and line coverage by 15.2%. This suggests that modeling inter-parameter dependencies is crucial for generating more effective test cases that cover complex interactions between parameters. Lastly, excluding LlamaREST-EX resulted in moderate reductions in coverage, with method, branch, and line coverage decreasing by 6.6%, 14.7%, and 5.4%, respectively. While LlamaREST-EX still contributes meaningfully to overall test effectiveness, its impact appears to be less critical than that of the other components.\nThe ablation study shows that server response handling and inter-parameter dependency modeling are the most critical components of LlamaRestTest, with their removal causing the largest drops in coverage, while all components contribute to the overall performance."}, {"title": "4.7 Threats to Validity", "content": "A key concern in our study is the generalizability of the results, as we tested a limited range of RESTful services, including FDIC, Language-Tool, OhSome, OMDb, REST-Countries, Genome-Nexus, OCVN, Spotify, and YouTube-Mock API. These services may not represent all possible types of REST APIs, and different types of services could yield varying outcomes. Additionally, we relied"}, {"title": "5 RELATED WORK", "content": "Automated REST API Testing: Automated testing for REST APIs has seen various strategies. White-box and black-box approaches", "7": ".", "8": ".", "16": ".", "37": "focuses on model-based testing to simulate user interactions and test case generation", "58": "employs combinatorial testing techniques to systematically explore parameter value combinations and their effects on API behavior. ARAT-RL introduces a reinforcement learning approach", "33": ".", "39": ".", "61": ".", "Generation": "This domain includes tools like ucsCNL and"}]}