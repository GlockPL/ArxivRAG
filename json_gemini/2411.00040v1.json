{"title": "P2C2Net: PDE-Preserved Coarse Correction Network\nfor Efficient Prediction of Spatiotemporal Dynamics", "authors": ["Qi Wang", "Pu Ren", "Hao Zhou", "Xin-Yang Liu", "Zhiwen Deng", "Yi Zhang", "Ruizhi Chengze", "Hongsheng Liu", "Zidong Wang", "Jian-Xun Wang", "Ji-Rong Wen", "Hao Sun", "Yang Liu"], "abstract": "When solving partial differential equations (PDEs), classical numerical methods\noften require fine mesh grids and small time stepping to meet stability, consis-\ntency, and convergence conditions, leading to high computational cost. Recently,\nmachine learning has been increasingly utilized to solve PDE problems, but they\noften encounter challenges related to interpretability, generalizability, and strong\ndependency on rich labeled data. Hence, we introduce a new PDE-Preserved Coarse\nCorrection Network (P2C2Net) to efficiently solve spatiotemporal PDE problems\non coarse mesh grids in small data regimes. The model consists of two synergistic\nmodules: (1) a trainable PDE block that learns to update the coarse solution (i.e.,\nthe system state), based on a high-order numerical scheme with boundary condition\nencoding, and (2) a neural network block that consistently corrects the solution on\nthe fly. In particular, we propose a learnable symmetric Conv filter, with weights\nshared over the entire model, to accurately estimate the spatial derivatives of PDE\nbased on the neural-corrected system state. The resulting physics-encoded model\nis capable of handling limited training data (e.g., 3-5 trajectories) and accelerates\nthe prediction of PDE solutions on coarse spatiotemporal grids while maintaining a\nhigh accuracy. P2C2Net achieves consistent state-of-the-art performance with over\n50% gain (e.g., in terms of relative prediction error) across four datasets covering\ncomplex reaction-diffusion processes and turbulent flows.", "sections": [{"title": "Introduction", "content": "Complex spatiotemporal dynamical systems are pivotal and commonly seen in numerous fields such\nbiology, meteorology, fluid mechanics, etc. The behaviors of these systems are primarily governed by\npartial differential equations (PDEs), conventionally solved by numerical methods [1\u20134]. However,\ndirect numerical simulation (DNS) demands in-depth knowledge of the underlying physics, and\nthe efficacy of numerical solutions is intricately linked to the resolution of mesh grids and time\nsteps. High-resolution spatiotemporal grids are essential for accurate and convergent calculations,\nyet leading to substantial computational costs. For instance, simulating the flow field around a large\naircraft [5, 6] involves creating over millions of mesh nodes and may consume vast simulation time\neven on high performance computing. Additionally, any changes in initial and boundary conditions\n(I/BCs) or design parameters necessitate recalculations, further compounding the complexity.\nRecently, tremendous efforts have been placed on machine learning for data-driven simulation of\nthese systems [7\u20139], demonstrating promising potential. These methods do not require the a priori\nknowledge of physics and, meanwhile, help bypass some traditional constraints, e.g., the smallest size\nof mesh grid and time step to guarantee solution accuracy, stability and convergence [10]. However,\nthey typically face issues of poor interpretability, weak generalizability and strong dependency of\nrich labeled data. Their performance deteriorate significantly particularly in small data regimes.\nEmbedding prior physics knowledge into the learning process has demonstrated effective to overcome\nthe aforementioned issues. A brute-force way lies in creating regularizers (e.g., the residual form of\nPDEs and I/BCs) as \"soft\" penalty in the loss function, e.g., the family of physics-informed neural\nworks (PINNs) [11\u201317]. However, such a strategy has limited scalability and generalizability, and the\nsolution accuracy relies largely on a proper selection of loss weight hyperparameters. Embedding\nphysics explicitly into the network architecture, which imposes \u201chard\u201d constraints such as physics-\nencoded recurrent convolutional neural network (PeRCNN) [18, 19], possesses better generalizability\nas well as offers better convergence and flexibility for model training without the need of fine-tuning\nhyperparameters. Nevertheless, existing methods fail to handle coarse mesh grids and suffer from\ninstability issues especially for long-range prediction of dynamics. Hybridizing classical numerical\nschemes and neural networks, e.g., the learned interpolation (LI) model [20], can enable accelerated\nsimulation on coarse mesh grids with satisfied accuracy. Yet, since the numerical part is non-trainable,\nsuch models still require rich labeled data to retain accuracy.\nTo tackle these critical challenges, we introduce the P2C2Net model for efficient prediction of\nspatiotemporal dynamics on coarse mesh grids in small training data regimes. Specifically, a trainable\nPDE block (a white box) is designed to learn the coarse solution at low resolution, where the temporal\nmarching of system states is handled by a fourth-order Runge-Kutta (RK4) scheme. We also propose\na learnable symmetric Conv filter for more accurate estimation of spatial derivatives on coarse grids,\nas required in PDE block. A neural network (NN) block, which serves as a correction module, is\nfurther introduced to correct the coarse solution, restoring information lost due to reduced resolution.\nWe also encode BC into the solution via a padding strategy. Our primary contributions are threefold:\n\u2022 We propose a new physics-encoded correction learning model (P2C2Net) to efficiently\npredict complex spatiotemporal dynamics on coarse mesh grids. The model requires only a\nsmall set of training data and possesses plausible generalizability.\n\u2022 We introduce a structured Conv filter that preserves symmetry to improve the estimation\naccuracy of coarse spatial derivatives required in the solution updating process, which makes\nthe PDE block trainable with flexibility of handling coarse grids.\n\u2022 P2C2Net achieves consistent state-of-the-art performance with at least 50% gain (e.g., in\nterms of relative prediction error) across four datasets covering complex reaction-diffusion\n(RD) processes and turbulent flows, simultaneously retaining accuracy and efficiency."}, {"title": "Related work", "content": "Numerical methods. Conventionally, PDE systems are solved by classical numerical methods\nsuch as finite difference [1], finite volume [2], finite element [3], and spectral methods [4]. These\nmethods often require fine mesh grids and reasonable time stepping to meet stability, consistency\nand convergence conditions. When dealing with large scale simulations or inverse analyses, the\ncomputational costs remain remarkably high.\nDeep learning methods. Given sufficient labeled training data, deep learning has been recently\napplied to solve PDE problems. Representative approaches include Conv-based NN models [9, 21],\nU-Net [22], ResNet [23], graph neural networks [24, 25], and Transformer-based models [26-28]. In\naddition, neural operators such as DeepONet [7], multiwavelet-based model (MWT) [29], Fourier\nneural operator (FNO) [8], and their variants [30\u201332] have been designed to directly learn mappings\nbetween function spaces, making them particularly well-suited for modeling PDE systems. Diffusion\nmodels have also been employed for prediction of spatiotemporal dynamics [33].\nPhysics-aware learning methods. Recently, physics-aware deep learning has demonstrated great\npotential in modeling spatiotemporal dynamics under conditions of small training data. This paradigm\ncan be divided into two categories based on the way of embedding PDE information: (1) physics-\ninformed, and (2) physics-encoded. The former formalizes PDE soft constraints including equations"}, {"title": "Methodology", "content": "3.1 Problem formulation\nLet us consider a spatiotemporal dynamical system governed by the general form of PDEs:\n$\\frac{\\partial u}{\\partial t}$  F(u, u\u00b2, \u2026\u2026\u2026, \u2207u, \u2207\u00b2u, \u00b7\u00b7\u00b7 ; \u03bc) = f,\nwhere u(x, t) \u2208 R\" denotes the n-dimensional physical state within spatiotemporal domain \u03a9\u00d7 [0, T];\ndu/dt the first-order time derivative; F a linear/nonlinear function; \u2207 \u2208 Rn the Nabla operator; \u03bc\nthe PDE parameters (e.g., the Reynolds number Re); f the external force (e.g., f = 0 for source-free\ncases). Besides that, we define the initial condition (IC) as I(u, ut; x \u2208 \u03a9, t = 0) = 0 and the\nboundary condition (BC) as B(u, \u2207u, \u2026\u2026 ; x \u2208 \u2202\u03a9) = 0, where \u2202 denotes the boundary of \u03a9.\nOur aim is to develop a learnable coarse model that accelerates the simulation and prediction of\nspatiotemporal dynamics based on a minimal set of sparse data (e.g., low-resolution data down-\nsampled across space and time). The learned model is expected to achieve high solution accuracy and\nsuperior generalizability over various PDE scenarios, including ICs, force terms, and PDE parameters.\n3.2 Network architecture\nHerein, we introduce the P2C2Net architecture, as shown in Figure 1, taking the simulation of\nNavier-Stokes (NS) flows as an example. The model is composed of four blocks, namely, the state\nvariable correction block, the learnable PDE block, the Poisson block, and the NN block. Note that\nthe network architecture is flexible and features a Poisson block that solves for the pressure term p,\nwhich is absent in other cases.\n3.2.1 The flow of data\nIn Figure 1 (a), the network architecture includes two paths: the upper path computes the coarse\nsolution using a learnable PDE block, while the lower path is incorporated into the network to correct\nthe solution on a coarse grid with a Poisson block and a NN block. The data flow operates as follows:\n(1) the network accepts uk as input and processes it by the PDE block on the upper path, where the\nPDE block computes the residual of the governing equation. A filter bank, defined as a learnable filter\nwith symmetry constraints, calculates the derivative terms based on the corrected solution (produced\nby the correction block). These terms are combined into an algebraic equation (a learnable form of\nF). This process is incorporated into the RK4 integrator for solution update. (2) In the lower path, uk\nis first corrected by the correction block, and pk is computed by the Poisson block. Inputs, including\nsolution states {uk, pk} and their derivative terms, forcing term, and Reynolds number, are fed into\nthe NN block. The output from this block serves as a correction for the upper path. (3) The final\nresult uk+1 is obtained by combining the outputs from both the upper and lower paths. During the\ngradient back-propagation process, the NN block learns to correct the coarse solution output of the\nPDE block on the fly, and ensures that their combined results more closely approximate the ground\ntruth solution."}, {"title": "RK4 integration scheme", "content": "Similar to standard numerical solvers, the goal is to predict the solution at every time step satisfying\nthe underlying PDEs given specific I/BCs. Here, we aim to address the challenge of spatiotemporal\ndynamics evolution on coarse grids (e.g., low resolution). Given the coarse solution at timestep tk,\ndenoted by uk, we expect the model yielding an accurate prediction of uk+1, v.i.z.,\nUk+1 = uk + $\\int_{t_k}^{t_{k+1}}$ [H (u(x, t), u\u00b2(x, t),\u2026\u2026, \u2207u(x, t), \u2207\u00b2u(x, t),\u2026\u2026 ; \u03bc) + f(t)] dr\nwhere H (the PDE block) denotes a learnable form of F, which is discussed in Section 3.2.3; x\nrepresents the coarse grid coordinates. We herein employ the RK4 scheme as the integrator for time\nmarching of the dynamics, offering the fourth-order accuracy (O(dt4)), where dt = tk+1 - tk is the\ncoarse time step. More details on RK4 can be found in Appendix Section B.3."}, {"title": "Learnable PDE Block", "content": "We assume that the PDE formulation in Eq. (1) is given, e.g., the explicit expression of F is known.\nWith coarse mesh grids and large time stepping, numerical methods such as finite difference (FD) tend\nto diverge. This issue becomes more pronounced with greater grid coarsening. Hence, we propose a\nlearnable PDE block (depicted in Figure 1(c)), denoted by H, to approximate F on coarse grids, so\nas to enable the coarse simulation simultaneously retaining efficiency and accuracy, expressed as\nH (uk, u, \u2026\u2026\u2026, \u2207uk, \u2207\u00b2uk, ...; \u00b5) + F (ux, u, \u2026, \u2207\u00fbk, \u2207\u00af\u00fbk, \u2026\u2026\u2026; \u03bc)\nwhere uk denotes the coarse solution at time tk; \u00fbk the corresponding neural-corrected coarse\nsolution state, obtained by the correction block shown in Figure 1(a) and (c), used for estimation of\nspatial derivatives, e.g., \u00fbk = NN(uk). Here, represents a learnable Nabla operator composed of\na Conv filter with symmetric constraint (e.g., an enhanced FD kernel for numerical approximation\nof spatial derivatives). Through the RK4 integration, we are then able to predict the coarse solution\nfor the next time step. Note that the PDE parameters \u03bc can be set as trainable if unknown. Despite\ninformation loss at the low resolution, such a learnable PDE block allows for more accurate derivative\nestimation on coarse grids and ensures better adherence of the updated coarse solution to underlying\nPDEs. Clearly, such a block adds a fully interpretable \u201cwhite box\u201d to the overall network architecture."}, {"title": "Correction block", "content": "The correction block takes a NN to correct the coarse solution. In particular,\nwe choose FNO [8] as the neural correction operator performed on the coarse solution field in\nsuch a block. In the Fourier space, FNO decomposes the input data into components with specific\nfrequencies, processes each component separately, and then applies Fourier transform to restore the\nupdated spectral features back to the physical domain. The layer-wise update can be expressed as:\nvl+1(x) = \u03c3 (W\u00b9v\u00b9\u00b2(x) + (K($)v\u00b2) (x))\nwhere, v\u00b9(x) denotes the l-th layer latent feature map on the coarse grids x. Note that v\u00ba(x) = P(uk),\nin which P is a local transformation function that lifts uk to a higher dimensional representation. Here,\nK(\u0444)(z) = iFFT(R$ \u00b7 FFT(z)) denotes a kernel integral transformation of the latent feature map z,\nwhich encompasses Fourier transform, spectral filtering and convolution in the frequency domain R\u00f8,\nand inverse Fourier transform; & the network parameters; \u03c3(\u00b7) the GELU activation function; W\nthe weights of a linear transformation. Given an L-layer FNO, the corrected coarse solution reads\n\u00fbk = Q (v\u00b9(x)), where Q is a local projection function. Details of the FNO correction block are\nfound in Appendix Section B.1."}, {"title": "Conv filter with symmetric constraint", "content": "Based on FD schemes, spatial derivatives can be ap-\nproximated by central difference stencils represented by convolution kernels [35, 39\u201341]. Such an\napproach often requires fine mesh grids to ensure accuracy; otherwise on coarse grids, there exists\nsolution divergence issue. To this end, we leverage our understanding of FD stencils and propose a\nConv filter with symmetric constraints to improve the accuracy of spatial derivative approximation on\ncoarse mesh grids. As shown in Figure 1(g), the filter weights are transformed into a 5 \u00d7 5 matrix\ng with 7 learnable parameters (Table S1 demonstrates the significant differences in results across\nvarious kernel sizes when applied to the Burgers dataset.), which satisfies symmetry, to estimate the\nfirst-order derivative along the horizontal direction (e.g., the vertical direction takes g\u00b9), v.i.z.,\ng=\n$\\begin{bmatrix}\na1 & a2 & a3 & -a2 & -a1\\ \na4 & a5 & a6 & -a5 & -a4\\ \na7 & -2a7 & 0 & 2a7 & -a7\\ \n-a4 & -a5 & -a6 & a5 & a4\\ \n-a1 & -a2 & -a3 & a2 & a1\n\\end{bmatrix}$\nAlthough the number of learnable parameters in the Conv kernel is limited, the coarse derivatives\ncan still be accurately approximated after the model is trained (see the ablation study in Section 4.2).\nThe structured filter is designed for Conv operation which satisfies the Order of Sum Rules stated in\nLemma 1 [39] (see Appendix Section A for more details).\nLemma 1: A 2D filter gm\u00d7m has sum rules of order 1 = (11, 12), where i \u2208 Z2, provided that\n$\\sum_{k1=-\\frac{m-1}{2}}^{\\frac{m-1}{2}} \\sum_{k2=-\\frac{m-1}{2}}^{\\frac{m-1}{2}}  kk2g[k1,k2] = 0$\nfor all a = (a1, a2) \u2208 Z2 with |a| := a1 + a2 < |1| and for all a \u2208 Z2 with |x| = |1| but a \u2260 \u03b9. If\nthis holds for all a \u2208 Z2 with |a| < A except for a \u2260 \u0103 with certain \u0103 \u2208 Z2 and |a| = B < A,\nthen we say g to have total sum rules of order A \\ {B + 1}.\nCorollary 1: The filter g we designed in Eq. (5) has the sum rules of order (1, 0). By adjusting the\nparameters in g, e.g., a7 \u2192 0, a6 + 8a5 \u2192 0, it satisfies the total sum rules of order 5 \\ {2}, and\nachieves an approximation to the first-order derivative with the fourth-order accuracy. For example,\nfor a smooth function w(x, y) and small perturbation \u025b > 0, we have [39]:\n$\\frac{1}{\\epsilon} \\sum_{k1=-\\frac{m-1}{2}}^{\\frac{m-1}{2}} \\sum_{k2=-\\frac{m-1}{2}}^{\\frac{m-1}{2}}  g[k1,k2]\u03c9(x + \u03b5k1, y + \u03b5k2) = \\frac{\\partial \u03c9(x, y)}{\\partial x} + O(\u03b5^4)$, as \u025b \u2192 0."}, {"title": "BC encoding", "content": "To ensure that the predicted solution com-\nplies with the given BCs, while also retaining the shape\nof the feature map after Conv operations, we employ a\nBC hard encoding method [19]. In particular, we consider\nperiodic BCs in this work and apply padding padding, as\nshown in Figure 2, on the predicted solutions. Such an\nencoding technique not only ensures the compliance of\nthe predicted solution with the BCs, but also enhances the\nsolution's accuracy."}, {"title": "Poisson block", "content": "When solving NS equations, there exists the pressure term\np in the governing PDEs. However, for incompressible\nflows, the pressure can be inferred by solving a Poisson\nequation. We designed this block to achieve this aim, the\npoisson equation namely, \u2206p = f(u, f), where (u, f) =\n-2 (UyVx - UxVy)+\u2207\u00b7f for 2D problems and the subscripts denote spatial differentiation. Hence, we\nleverage the spectral method to estimate the pressure quantity (see Appendix Section B.2), as depicted\nin Figure 1(e), which updates pk based on (uk, fk). The resulting Poisson block (Figure 1(b))\nefficiently derives the pressure field from the velocity field and external force on the fly, streamlining\ncomputations without the need for labeled training data of pressure."}, {"title": "NN block", "content": "It is noted that the predictions by the learnable PDE block on coarse grids may encounter instability\nissue due to error accumulation over time, especially in scenarios involving long-range rollout\nprediction. Hence, we propose an additional NN block to consistently correct the coarse solution\npredicted by the PDE block on the fly, restoring information lost due to reduced resolution. This\nmodule can be any trainable NN model, such as FNO [8], DenseCNN [41], or UNet [13, 45]. In\nparticular, we consider FNO as the NN block, with more details found in Appendix Section B.1.\nHowever, the NN block is not always necessary. For cases of simpler systems, such as the Burgers\nequation, the learnable PDE block alone can perform well where the NN block can be omitted."}, {"title": "Model generalization", "content": "We herein introduce the setup of the model's generalizability over various PDE scenarios, including\nICs, force terms, and PDE parameters (e.g., the Reynolds number Re). The time marching in our\nnetwork design inherently integrates ICs, automatically ensuring generalization over ICs given a\nwell trained model. We embed the force term in the learnable PDE Block (naturally in the PDE\nformulation) shown in Figure 1(c), and meanwhile incorporate it as a feature map into the NN Block\nas part of the input (see Figure 1(c)). This enables the joint learning of force feature variations for\ngeneralization. In addition, the Reynolds number (Re) is incorporated by creating a 2D feature\nmap embedding (e.g., for the 2D case) by introducing two trainable vectors a and b, namely,\nReembb = 1/Re. (ab), in both the PDE Block and the the NN Block. Such an approach can\ncorrect the error propagation of the diffusion term in the PDE Block caused by coarse grids, and\nenhance the model's ability of generalization across different Re's."}, {"title": "Experiments", "content": "We evaluate the performance of P2C2Net against several baseline models across diverse PDE\nsystems, including fluid flows and RD processes. The results have demonstrated the supe-\nriority of our model in terms of solution accuracy and generalizability thanks to the unique\ndesign of embedding PDEs into the network. The source codes and data are found at\nhttps://github.com/intell-sci-comput/P2C2Net (PyTorch) and https://gitee.com/\nintell-sci-comput/P2C2Net.git (MindSpore)."}, {"title": "Impact statement", "content": "The aim of this work is to develop a novel physics-encoded learning scheme to accelerate predictions\nand simulations of spatiotemporal dynamical systems. This method can be applied to various fields,\nincluding weather forecasting, turbulent flow prediction, and other simulation tasks. Our research is\nsolely intended for scientific purposes and poses no potential ethical risks."}, {"title": "APPENDIX", "content": "Design of the filter with symmetric constraint\nf Firstly, inspired by the central finite difference method, we design a filter g with the size of m \u00d7 m,\nwhich meets the symmetric constraint. For example, when m = 5, g is given by\ng=\n$\\begin{bmatrix}\na1 & a2 & a3 & -a2 & -a1\\ \na4 & a5 & a6 & -a5 & -a4\\ \na7 & -2a7 & 0 & 2a7 & -a7\\ \n-a4 & -a5 & -a6 & a5 & a4\\ \n-a1 & -a2 & -a3 & a2 & a1\n\\end{bmatrix}$\nTaking a (1, 0), we have:\n$\\sum_{k1=-\\frac{m-1}{2}}^{\\frac{m-1}{2}} \\sum_{k2=-\\frac{m-1}{2}}^{\\frac{m-1}{2}}  kk2g[k1,k2] = \u22124a7 + 2a8$.\nIn order to satisfy the sum rules of order a = (1,0), we can get a8 = -2a7.Then, the filter g can be\nre-written as\ng=\n$\\begin{bmatrix}\na1 & a2 & a3 & -a2 & -a1\\ \na4 & a5 & a6 & -a5 & -a4\\ \na7 & -2a7 & 0 & 2a7 & -a7\\ \n-a4 & -a5 & -a6 & a5 & a4\\ \n-a1 & -a2 & -a3 & a2 & a1\n\\end{bmatrix}$\nCorollary 1 Proof: Consider the filter we designed above with a = (0,1), \u03b1 = (0,3) and\na = (3,0), we have:\n$\\sum_{k1=-\\frac{m-1}{2}}^{\\frac{m-1}{2}} \\sum_{k2=-\\frac{m-1}{2}}^{\\frac{m-1}{2}}  kk2g[k1,k2] = -4a5 - 206$\n$\\sum_{k1=-\\frac{m-1}{2}}^{\\frac{m-1}{2}} \\sum_{k2=-\\frac{m-1}{2}}^{\\frac{m-1}{2}}  kk2g[k1,k2] = -16a5 - 2a6$\n$\\sum_{k1=-\\frac{m-1}{2}}^{\\frac{m-1}{2}} \\sum_{k2=-\\frac{m-1}{2}}^{\\frac{m-1}{2}}  kk2g[k1,k2] = \u221212a7$\nSatisfying the sum rules of order a = (0,3) and a = (3,0) leads to strictly -16a5 - 2a6 =\n0, -12\u03b17 = 0. By adjusting the trainable parameters, we have a6 + 8a5 \u2192 0, a7 \u2192 0, and\n-405 - 206 \u2260 0. According to Lemma1, the resulting filter g satisfies the total sum rules of order\n5\\{2}.\nFor any smooth function w, we perform a convolution operation on it with the filter g. Applying a\nTaylor expansion to this process, we obtain:\n$\\sum_{k1,k2=-2}^{2}  g[k1,k2]w(x + k\u2081dx, y + k2dy)$\n$\\sum_{k1,k2=-2}^{2} 9[k1,k2] \\sum_{\\frac{i+j=0}{4}} \\frac{\\delta x^{i} \\delta y^{j} \\frac{\\partial^{i+j} \u03c9}{\\partial x^{i} \\partial y^{j}}(x, y)}{i!j!} + o(|dx|^{4} + |\u0431\u0443|^{4})$\n$\\sum_{\\frac{i,j=0}{4}}ri,jdxy \\frac{\\partial^{i+j}\u03c9}{\\partial x^{i} \\partial y^{j}}(x, y) + o(|dx|^{4} + |\u0431\u0443|^{4}).$"}]}