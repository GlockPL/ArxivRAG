{"title": "Implementing Systemic Thinking for Automatic Schema Matching: An Agent-Based Modeling Approach", "authors": ["Hicham Assoudi", "Hakim Lounis"], "abstract": "Several approaches are proposed to deal with the problem of the Automatic Schema Matching (ASM). The challenges and difficulties caused by the complexity and uncertainty characterizing both the process and the outcome of Schema Matching motivated us to investigate how bio-inspired emerging paradigm can help with understanding, managing, and ultimately overcoming those challenges. In this paper, we explain how we approached Automatic Schema Matching as a systemic and Complex Adaptive System (CAS) and how we modeled it using the approach of Agent-Based Modeling and Simulation (ABMS). This effort gives birth to a tool (prototype) for schema matching called Reflex-SMAS. A set of experiments demonstrates the viability of our approach on two main aspects: (i) effectiveness (increasing the quality of the found matchings) and (ii) efficiency (reducing the effort required for this efficiency). Our approach represents a significant paradigm-shift, in the field of Automatic Schema Matching.", "sections": [{"title": "I. INTRODUCTION", "content": "Schema Matching is an important task for many applications, such as data integration, data warehousing and e-commerce. Schema matching process aims at finding a pairing of elements (or groups of elements) from the source schema and elements of the target schema such that pairs are likely to be semantically related [2] [3].\nSchema matching existing approaches rely largely on human interactions, either for the matching results validation, during the post-matching phase, or for the matching process optimization, during the pre-matching phase. Although this human involvement in the automatic matching process could be considered as acceptable in a lot of matching scenarios, nevertheless it should be kept to a minimum, or even avoided, when dealing with high dynamic environments (i.e., semantic Web, Web services composition, agents communication, etc.) [1]. Thus, the existing approaches are not suited for all the matching contexts due to their intrinsic limitations. We can summarize those limitations as follows:\n\u2022 Lack of autonomy to the extent that the user involvement is still needed for the results validation and analysis, but also for matching process configuration and optimization (tuning) to improve the matching result quality and then reduce uncertainty.\n\u2022 Lack of adaptation in sense that the optimization task of the matching tool must be repeated and adapted manually, for every new matching scenario.\nWe were motivated to investigate other prospects not yet applied on Schema Matching. We try to answer the following general question: \u201cHow can we, with the help of a generic approach, better manage complexity and uncertainty inherent to the automatic matching process in general, and in the context of dynamic environments (minimal involvement of the human expert)?\"\nMore specifically, we asked the following questions:\n(i) How can we model the complexity of the matching process to help reduce uncertainty?\n(ii) How can we provide the matching process of autonomy and adaptation properties with the aim to make the matching process able to adapt to each matching scenario (self-optimize)?\n(iii) What would be the theoretical orientation that may be adequate to respond to the above questions?\nIn our work, we have investigated the use of the theory of CAS emanating from systemic thinking, to seek, far from the beaten path, innovative responses to the challenges faced by classical approaches for automatic schema matching, (e.g., complexity, uncertainty). The central idea of our work is to consider the process of matching as a CAS and to model it using the approach of ABMS. The aim being the exploitation of the intrinsic properties of the agent-based"}, {"title": "II. CURRENT APPROACHES OF SCHEMA MATCHING", "content": "Many algorithms and approaches were proposed to deal with the problem of schema matching and mapping [1] [4]-\n[15]. Although the existing schema matching tools comprise a significant step towards fulfilling the vision of automated schema matching, it has become obvious that the user must accept a degree of imperfection in this process. A prime reason for this is the enormous ambiguity and heterogeneity of schema element names (descriptions). Thus, it could be unrealistic to expect a matching process to identify the correct matchings for any possible element in a schema [16][17].\nA comprehensive literature review, of the existing matching tools and approaches, allowed us to identify the most important factors affecting, in our opinion, the schema matching process. Moreover, some causal relationships, between those different factors, participating to the schema matching difficulties and challenges, were identified. As shown in Figure 1, the factors influencing the Schema Matching are:\n\u2022 Heterogeneity: in general, the task of matching involves semantics (understanding the context) to have complete certainty about the quality of the result. The main challenge in all cases of automatic matching is to decide the right match. This is a very difficult task mainly because of the heterogeneity of the data.\n\u2022 Uncertainty: the cause for this uncertainty lies mainly in the ambiguity and heterogeneity, both syntactic, and semantic, which often characterize the Schema Elements to match.\n\u2022 Optimization: the uncertainty about the matching results implies the optimization of the process to improve the matching quality, and the testing of different combinations (e.g., different Similarity Measures, Aggregate Functions, and Matching Selection Strategies). Each step of the matching process involves choosing between multiple strategies, which leads to a combinatorial explosion (complexity).\n\u2022 Complexity: the matching process optimization generates complexity because of the search space (combinatorial explosion). In addition, changing matching scenarios exacerbates this complexity to the extent that the result of the optimization often becomes obsolete with changing scenarios.\nOne of the commonalities between all existing approaches is the thinking behind these approaches, namely reductionism (as opposed to holism). The reductionist thinking is a very common and efficient thinking approach. It is at the basis of the almost totality of previous schema matching approaches, and then, on their characteristics that are, in our view, the root causes preventing the automatic matching schemes to cope fully with the challenges and difficulties.\nReductionism, as opposed to systemic (holism), is a philosophical concept that refers both to the way of thinking solutions as well as to their modeling methodology. Reductionism advocates reducing system complexity or phenomenon to their basic elements which would then be easier to understand and study [18]. This reductionist approach, despite its high efficacy in several areas, shows, however, its limits within certain contexts. In fact, for explaining certain phenomena or solving certain problems, the approach consisting of reducing or abstracting the reality"}, {"title": "III. SCHEMA MATCHING AS A SYSTEMIC APPROACH", "content": "As part of our research we investigated the use of the theory of CAS (systemic thinking), to try to find an innovative response to challenges (i.e. complexity, uncertainty) that the conventional approaches for schema matching are still facing.\nWe think that the CAS could bring us the adaptation capability to the realm of schema matching tools (self-configuration and self-optimization), which should relieve the user from the complexity and effort resulting from configuring and optimizing the automatic schema matching systems.\nOur conceptual model for schema matching, based on the theory of complexity, sees the schema matching process as a complex adaptive system.\nAs illustrated in Figure 3, in this model, each schema element of the schemas to match (source or target schema) is modeled as an autonomous agent, belonging to a population (source or target schema population). Each agent behaviors and interaction, at the micro level, with the other agents in the opposite population and with its environment, brings out at the macro level, a self-organized system that represents the global solution to matching problem (i.e., relationships between schemas elements). In other words, the resolution of the matching problem goes through individual effort deployed by each agent, locally, throughout the simulation to find the best match in the opposite population.\nWe think that many intrinsic properties of our model, derived from the ABMS modeling approach, can contribute efficiently to the increase of the matching quality and thus the decrease of the matching uncertainty. These properties are:\n\u2022 Emergence: the emergence of the macro solution (schema matching) comes from local behaviors, rules and interactions between agents (micro solutions).\n\u2022 Self-organization: the cooperation of source and target schema elements (represented as agents) helps to reach a consensus about their best matching.\n\u2022 Stochasticity (randomness): the randomness within the model, gives the ability to perform statistical analysis on the outcome of multiple simulations (meta-simulation) for the same matching scenario.\nBriefly, our idea is to model the Schema Matching process as interactions, within a self-organized environment, between agents called \"Schema Attribute Agent\". In the rest of the paper, we are going to refer to the \"Schema Element Agent\" simply as agent. Each schema element is modeled as an agent belonging to one of two populations: source or target schema group. Furthermore, the schema matching process is modeled as the interaction between the two populations of agents.\nIn our model, the internal architecture of the agents is Rule-based (reflexive agent). The agents have as a main goal to find the best matching agent within the other group of agents. The foundation of the rules governing the agent's behaviors is stochasticity (randomness). In fact, a certain degree of randomness is present in each step executed by each agent during the simulation.\nThe main random elements influencing the simulation are as follows:\n\u2022 Similarity Calculation based on similarity measures selected randomly from a similarity measures list.\n\u2022 Similarity Scores aggregation based on aggregation functions selected randomly from an aggregation function list (MAX, AVERAGE, WEIGHTED).\n\u2022 Similarity score validation based on generated random threshold value (within interval)\nAs opposed to deterministic solutions for schema matching (all the existing matching solutions), the nondeterministic and stochastic nature of our agent-based simulation increase the confidence in the quality of the matching results. Even though the agent's behaviors are based on randomness (e.g., during the similarity calculation), our model can often produce the right matchings at the end of each simulation run.\nFigure 4 illustrates the internal states of each agent. It allows representing the transitions between the internal states, during the perception-decision-action cycle of the agent. In the context of our operational model, the agent during the perception phase, perceives its environment by interrogating it, by performing similarity calculations (which can be considered as an act of recognition) or by capturing certain events. The result of this phase will be a set of percepts, allowing the agent to identify the agents of the other group, available for matching. The capture of events, coming from the environment, is another action of perception: for instance, the event that is triggered when the agent is chosen by another one as a matching candidate. During the decision phase, the agent from the results of the perception phase, reasons, deliberates and decides on the action to be selected. The decisions, involving the choice of actions, are the following: (i) the decision concerning the convergence of similarities and the selection of a candidate matching, (ii) the decision concerning the reset of the beliefs concerning the candidate matching, and (iii) the decision on consensual matching. During the action phase, the agent executes the actions selected during the previous phase. The current iteration of the simulation ends with this phase.\nThe behavior of the agent is driven by the goal of finding a consensual match. The consensus-selection approach is a naive approach, consisting of waiting for a consensus that must coincide for both agents (which may imply a longer duration for the simulation)."}, {"title": "IV. EMPIRICAL EVALUATION", "content": "The validation of agent-based simulation models is a topic that is becoming increasingly important in the literature on the field of ABMS. Three types of validation could be identified [22]: (i) Empirical Validation, (ii) Predictive Validation, and (iii) Structural Validation.\nAs we will see in detail, the empirical validation is the type of validation that we have adopted for the evaluation of our Agent-based Simulation Model for Schema Matching (i.e. prototype Reflex-SMAS).\nFirst, we will start with the description of the methodology used as our validation approach, and then we continue by providing a summarized view of our validation results.\nWe are seeking, through this empirical evaluation, to validate the following aspects of our prototype Reflex-SMAS:\n\u2022 That our solution is, indeed, an effective and efficient automatic schema matching system, capable of autonomously changing behaviors and evolving over time, to adapt, and to self-organize and thus make the solution for any matching scenario to emerge.\n\u2022 That our solution is easy to understand, and therefore, could display a high degree of maintainability (e.g., adding new matchers).\nThe proof strategy consists on conducting experiments and then collecting and analyzing data from these experiments. Thus, the validation approach that we have adopted is considered as a hybrid validation approach combining two validation approaches coming from two different fields, namely Schema Matching and ABMS. On one hand, from the field of Schema Matching, we are leveraging a popular evaluation method consisting of the comparison of results with those expected by the user [23]. On the other hand, from the field of ABMS, we are using the Empirical Validation [22] which is mainly based on the comparison among the results obtained from the model and what we can observe in the real system.\nThus, the strategy adopted for the validation of our prototype (implementing our multi-agent simulation model for schema matching) consists of:\n\u2022 Defining different synthetic matching scenarios (three matching scenarios namely \"Person\", \"Order\" and \"Travel\") with different sizes and different level of lexical heterogeneity, so we can evaluate the prototype matching performance in different situations (adaptation).\n\u2022 Conducting experiments, compiling results and evaluating the matching performance by comparing, for those three matching scenarios, the matching results (matchings) obtained from our prototype Reflex-SMAS with the results expected by the user.\nIn the first matching scenario \"Person\", we need to match two schemas with small size (i.e., six elements) showing a medium lexical heterogeneity level. The second matching scenario \"Order\" is composed of schemas with medium size with a high lexical heterogeneity level. The schemas in the last matching scenario \"Travel\" have a relatively big size with a low lexical heterogeneity level.\nIn order to assess the relevance and level of difficulty that can represent those synthetic matching scenarios (i.e., \"Person\", \"Order\" and \"Travel\"), we decided to evaluate them, first, using the well-known matching tool COMA [24]-[26]. Since, the COMA tool was not able to resolve all the all expected matches for those scenarios, we can say that the proposed synthetic matching scenarios, should be enough challenging scenarios for our validation (from their level of heterogeneity perspective).\nRegarding the experiments execution and results compilation, we have decided to run series of three meta-simulations for each scenario (each meta-simulation includes 10 simulations).\nThe final matching result is based on a statistical analysis of each meta-simulation outcome. In other word, the matching result relies on the calculation of the frequency of occurrence of a found match on the ten simulations composing the meta-simulation. Furthermore, executing for each scenario the meta-simulations three times is a choice that we made to help with the assessment of the experiment repeatability."}, {"title": "V. CONCLUSION AND FUTURE WORKS", "content": "Our prototype (Reflex-SMAS) empirical evaluation showed us clearly its capability of providing a high-quality result for different schema matching scenarios without any optimization or tuning from the end-user. The experiments results are very satisfactory. Thus, we can conclude that approaching the schema matching as a CAS and modeling it as ABMS is a viable and very promising approach that could greatly help to overcome the problems of uncertainty and complexity in the field of schema matching.\nOur approach represents a significant paradigm-shift, in the field of ASM. In fact, to the best of our knowledge, never the ASM problem has been addressed by adopting systemic thinking (holistic approach) or has been considered as a CAS and modeled using ABMS modeling approach.\nAs future work, we are planning to enhance the conceptual model of our prototype to tackle challenges, such as complex schema (n:m cardinalities) by exploiting other Similarity Measures, such as Structural Similarities (schemas structures). On the other hand, in order to open up new perspectives and to overcome the limits of purely reactive behavior, we are thinking on a \"conceptual\" evolution of the internal architecture of our agent, evolving it from a reactive agent to an agent of rational type. This evolution consists in the implementation of a decision-making model under uncertainty, at the level of the decision-making phase of the agent, giving it the ability to reason and to choose between conflicting actions. The rational agent we are aiming for, should have a memory, a partial representation of its environment and other agents (its perception), and a capacity for reasoning, allowing it to make a rational choice (to choose the action with the greatest utility) that can guarantee it to maximize its satisfaction (measure of performance). The result of this conceptual evolution could give rise to a new version of our prototype."}]}