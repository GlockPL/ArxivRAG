{"title": "Dyve: Thinking Fast and Slow for Dynamic Process Verification", "authors": ["Jianyuan Zhong", "Zeju Li", "Zhijian Xu", "Xiangyu Wen", "Qiang Xu"], "abstract": "We present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation (System 1) for straightforward steps and comprehensive analysis (System 2) for complex ones. Leveraging a novel step-wise consensus-filtered process supervision technique, combining Monte Carlo estimation, LLM-as-a-Judge, and specialized reasoning models, we curates high-quality supervision signals from noisy data for Dyve. Experimental results on ProcessBench and the MATH dataset confirm that Dyve significantly outperforms existing process-based verifiers and boosts performance in Best-of-N settings.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have significantly enhanced their reasoning capabilities by shifting from rapid, intuitive System 1 responses to more deliberate, extended System 2 thinking (Team et al., 2025; Arrieta et al., 2025; Guo et al., 2025). While enabling more complex problem-solving in math and scientific reasoning, this has also introduced new challenges in process verification, particularly in the reliable evaluation of incomplete reasoning traces.\nProcess-based verifiers (PRMs) are essential for detecting process errors. However, becuase human annotations for process supervision (Lightman et al., 2023b) are prohibitively expensive, researchers increasingly use Monte Carlo estimation methods (Wang et al., 2024; Luo et al., 2024) to annotate process labels, even though these labels are noisy and weak (Zhang et al., 2025). Moreover, most verifiers rely on a simplistic \"System 1\" binary yes/no prediction, which is insufficient for capturing complex process errors.\nRecently released reasoning LLMs, such as OpenAI O1 (Jaech et al., 2024) and DeepSeek R1 (Guo et al., 2025), show promise in detecting process errors through reinforcement learning. Their reasoning traces include metacognitive cues (e.g. \u2018hmm', 'wait, let's check') that hint at a rudimentary verification mechanism, a kind of 'aha' moment. However, since process verification was not the primary design goal, these abilities can be unreliable. Moreover, their reliance on a System 2-style self-correction process often leads to overthinking (Chen et al., 2025) and reduce efficiency.\nOur work introduces Dyve (Dynamic Process Verifier), a specialized reasoning language model that dynamically detects process errors using fast and slow thinking, inspired by Kahneman's Systems Theory (Kahneman, 2012). For reasoning traces from step 1 to t, Dyve adaptively applies either System 1, which supplies single-token confirmation for clearly correct steps, or System 2 for deeper analysis to complex ones. To support this adaptive mechanism, we introduce a novel stepwise consensus-filtered process supervision technique. Our method leverages Monte Carlo estimation to generate multiple rollouts per query, uses an LLM-as-a-Judge (Gu et al., 2024) to assess the full reasoning trace, and employs a reasoning LLM for step-by-step analysis to flag steps that require further verification. In doing so, we curate approximately 117K high-quality training examples from 1.2M noisy Monte Carlo rollouts, demonstrating that quality, not quantity, is key to effectively train an process-based verifier.\nExperimental results on ProcessBench (Zheng et al., 2024) show that Dyve significantly outperforms existing PRMs and other reasoning models in detecting process errors in complete or incomplete reasoning traces. Furthermore, when combined with a proposer language model, Dyve yields better performances under Best-of-N then other PRMs."}, {"title": "Related Work", "content": "Recent research (Setlur et al., 2024; Wang et al., 2024; Guan et al., 2025) shows that external reward models can improve LLM reasoning by selecting the best path from multiple candidates. Outcome Reward Models (ORMs) (Cobbe et al., 2021b; Yang et al., 2024) optimize for final outputs but overlook vital intermediate steps. Process Reward Models (PRMs) (Lightman et al., 2023a; Zhang et al., 2025; Wang et al., 2024) provide rapid binary validations for each step, yet struggle with a deeper analysis of incomplete traces. In contrast, Generative Verifiers (GenRMs) (Zhang et al., 2024) combine chain-of-thought reasoning with next-token predictions to verify and generate solutions, although at a high computational cost. To balance these trade-offs, our DyVe framework merges the strengths of PRMs and GenRMs using Kahneman's dual system theory.\nHigh-quality step-level supervision is crucial for training process verifiers, yet human annotations (e.g., PRM800k (Lightman et al., 2023b)) are prohibitively expensive. To avoid this, OmegaPRM (Luo et al., 2024) employs a divide-and-conquer Monte Carlo Tree Search (MCTS) to generate annotations, although our experiments show that these labels are often noisy and weak. To address this issue, we adopt consensus filtering with an LLM-as-a-Judge (Gu et al., 2024) to eliminate unreliable samples (Zhang et al., 2025), and further extend this approach with step-wise flagging, where a reasoning LLM conducts step-by-step analysis to identify steps that require System 2 verification."}, {"title": "Method", "content": ""}, {"title": "Overview", "content": "Dyve can assess the correctness of multi-step reasoning trace generated by a language model. Given a problem P and its reasoning steps {$1, $2,..., ST}, Dyve sequentially verifies each step:\nrt = Dyve(81:t; 0)\nwhere the response rt, varying from 1 to 8192 tokens based on System 1 or System 2 usage, is parsed by Parse(\u00b7) to yield a binary outcome. If"}, {"title": "Step-wise Consensus-Filtered Process Supervision", "content": "We introduce a novel step-wise consensus-filtered process supervision technique to enable adaptive verification within Dyve. The pipeline includes:\nQueries Collection We gather query-response pairs from datasets like GSM8k (Cobbe et al., 2021a) and MATH (Hendrycks et al., 2021a), totaling 15K queries.\nMonte Carlo Rollouts Generation Using OmegaPRM (Luo et al., 2024), we generate 20 roll-outs per query. We also gather open-souce PRM data from MathShepherd (Wang et al., 2024) and RLHFlow, excluding PRM800k (Lightman et al., 2023b) to prevent data leakage, yielding approximately 1.2 million positive and negative rollouts with noisy labels.\nConsensus Filtering with LLM-as-Judges We prompt DeepSeek V3 to verify the initial error steps identified by OmegaPRM. This filtering removes about 50% of noisy rollouts. We then create a dataset of 117K high-quality examples by re-balancing the number of positive and negative step labels."}, {"title": "Step-Level Analysis with Reasoning LLMs", "content": "A reasoning model performs step-by-step analysis on curated rollouts. Correct steps are marked with a \u201c+\u201d token, while uncertain steps undergo further detailed evaluation, ensuring alignment with high-quality reasoning traces."}, {"title": "Training", "content": "We train the deepseek-ai/DeepSeek-R1-Distill-Qwen-14B model using supervised fine-tuning on our curated dataset. This enables the model to learn rapid System 1 verification and comprehensive System 2 correction. The training objective minimizes the cross-entropy loss:\nL(0) = 1/{NT(i)} * \u2211{i=1}^{N}\u2211{t=1}^{T(i)} log p\u03b8(y_t^(i)|x^(i),y_{<t)^(i)}\nwhere \u03b8 indicates the model parameters, x(i) is the input query, and y(i) is the target label for the i-th example."}, {"title": "Experiments", "content": "To evaluate Dyve's capabilities, we conduct experiments in two main areas. First, we assess Dyve's ability to identify process errors. Second, we integrate Dyve with Proposer LLMs using a Best-of-N approach to evaluate its synergy within a reasoning framework. All experiments are conducted on 8 \u00d7 NVIDIA A800-SXM4-80GB GPUs. Interested Readers may refer to Appendix A.1 for detailed experimental setup."}, {"title": "Benchmarks", "content": "ProcessBench (Zheng et al., 2024) comprises four sets of test data derived from GSM8k (Cobbe et al., 2021a), MATH (Hendrycks et al., 2021b), OlympiadBench (He et al., 2024), and Omni-MATH (Gao et al., 2024). It includes 3,400 test cases, covering high-school to Olympiad-level math problems. Each case provides a step-by-step solution with error locations annotated by experts. Models are given 81:t, from the first to the last step, and must identify the earliest error or confirm that all steps are correct. For each ProcessBench subset, we calculate the accuracies for erroneous and correct samples and compute their harmonic mean as the F1 score.\nMATH-500 (Lightman et al., 2023b) evaluates Dyve's integration with a Proposer LLM. We measure performance using maj@kand rm@kmetrics as defined in (Yang et al., 2024) and apply a Best-of-N decoding strategy. Due to inconsistent results from different evaluation tools, we manually verified all reported outcomes."}, {"title": "Processbench", "content": "Results and Analysis Dyve achieves the highest F1 scores across all benchmark subsets, outperforming all baselines. Despite being trained primarily on high-school and college-level mathematics, its dual reasoning system generalizes effectively to Olympiad-level problems. In contrast, LLM-as-Judge with DeepSeek-R1-Distill-Qwen-14B shows weaker performance on OlympiadBench and OmniMATH, indicating less reliable process error detection.\nCamparison on Inference Time According to, the inference speed comparison in ProcesBench, highlights model efficiency. System-1 is the fastest, maintaining minimal latency. Dyve, slightly slower, balances speed and performance, excelling in complex datasets like OlympiadBench and OmniMATH. R1-14B has the longest inference times, suggesting a bottleneck for rapid processing. This analysis highlights Dyve's ability to deliver competitive performance with efficient inference times, making it well-suited for applications demanding both accuracy and speed.\nModel Choice and Step-wise Consensus Filtering The ablation study in illustrates the impact of model selection and step-wise consensus filtering in ProcessBench. For Llama-3.1-8B-Instruct, consensus filtering significantly improves performance, boosting scores from 35.6 to 49.3 on GSM8K and from 28.3 to 40.2 on MATH. Similarly, DS-R1-Distill-Qwen-14B sees substantial gains, with MATH scores increasing from 34.7 to 56.0 and OmniMATH from 11.2 to 37.7. Step-wise flagging further amplifies performance, achieving scores of 68.5 on GSM8K and 58.3 on MATH. These results underscore the effectiveness of these techniques and highlight the superior reasoning capabilities of the 14B model compared to the 7B Llama, validating our choice of DeepSeek-R1-Distill-Qwen-14B."}, {"title": "Integrating Dyve with Proposer LLMS", "content": "We integrate Dyve as a process verifier to assist Proposer LLMs (Qwen-Math-7B and Deepseek-R1-Distill-Qwen-14B) on MATH-500. For fairness, we compare three setups across Best-of-N (N = 1, 2, 4, 8) decoding settings: Dyve verification, System 1 only, and Majority Vote (no verification).\nResults and Analysis As shown in Figure 4, Dyve's combination of fast and slow verification outperforms both Majority Voting and System 1 verification when integrated with Best-of-N decoding. When the generation budget is N = 8, Dyve with DeepSeek-R1-Distill-Qwen-14B achieves 95.5% accuracy, while Dyve with Qwen2.5-MATH-7B-Instruct reaches 90.4%, outperforming both baselines. This demonstrates how our dual-system with fast and slow thinking, approach effectively guides Proposer LLMs to select more accurate reasoning paths, showcasing the synergy between the Dyve and proposer models."}, {"title": "Conclusion", "content": "Our study demonstrates Dyve's, with a dual reasoning approach, superior performance in mathematical reasoning verification. The consensus filtering and step-wise flagging significantly enhanced model accuracy and robustness. Ablation studies confirm the 14B model's advantages over smaller variants for complex reasoning tasks, establishing Dyve as an effective solution for precise and efficient error detection."}, {"title": "Broader Ethical Impact", "content": "Our method is centered on rigorous verification of Al reasoning, ensuring each step is systematically validated for enhanced reliability and transparency. By exclusively using publicly available datasets under their proper licenses, we adhere to responsible research practices. We believe that improving verification in AI reasoning not only boosts system robustness but also exemplifies ethical AI development."}, {"title": "Limitations", "content": "While Dyve demonstrates strong performance, it shares several limitations common to verification-based systems. Its effectiveness naturally depends on the complexity of the reasoning tasks, and more intricate multi-step problems may require further adaptation or deeper analysis. In addition, although our consensus-filtered process supervision considerably enhances signal quality, a modest level of noise remains inherent in any automated estimation process. Finally, the overall performance is influenced by the quality and diversity of the training data, suggesting that further efforts in data curation and filtering could yield even more robust results. These aspects offer promising directions for future research."}, {"title": "Training Details", "content": "Our model processes inputs with a maximum token length of 2048, ensuring robust contextual understanding. To further enhance efficiency, we employ Low-Rank Adaptation (LoRA) configured with a rank of 16, an alpha value of 16, and a dropout rate of 0.1. The training regimen spans three epochs, using a per-device batch size of 2 and leveraging gradient accumulation over 8 steps. The learning rate is set to 2 \u00d7 10-5 and a weight decay of 0.01 is applied. Training is executed with mixed precision (fp16), optimizing computational resources without sacrificing performance.\nInference During inference, our model leverages a multi-step reasoning process to evaluate each problem instance. The procedure begins by formulating a sequence of conversational prompts that encapsulate both the problem statement and its progressive steps. At each step, the Dyve model is queried via its custom chat interface, and the generated response is examined for specific response patterns \u2014 such as the presence of a \u201c+\u201d symbol signaling a correct evaluation. This iterative mechanism continues until a response fails to meet the designated correctness criteria, at which point the process halts. To ensure efficiency, the inference is executed concurrently using a pool of 32 parallel workers, processing various configurations from the ProcessBench dataset (including gsm8k, math, olympiadbench, and omnimath). For every evaluated problem, all intermediate responses (or generations) and the final step classification are recorded. These results are then systematically saved in JSON Lines format, facilitating subsequent analysis and serving as a robust foundation for further evaluation."}, {"title": "Efficient Estimation of MCTS", "content": "In this section, we detail our approach to efficiently utilize Monte Carlo Tree Search (MCTS) for sampling rollouts, which are crucial for training process-based verifiers.\nOverview\nOur method leverages MCTS to construct a state-action tree representing detailed reasoning paths for a given question. This approach allows us to collect Process-based Reward Model (PRM) training examples by exploring various reasoning paths and identifying errors efficiently."}, {"title": "State-Action Tree Construction", "content": "Each state s in the tree corresponds to a question and its preceding reasoning steps, with the root state being the question without any reasoning steps. An action a is a potential next step, and the state transition function is defined as s' = Concatenate(s, a). Each node s stores the visit count N(s), Monte Carlo estimation MC(s), and rollout value function Q(s, r).\nMCTS Process\nSelection We maintain a pool of rollouts with 0 < MC(s) < 1. During selection, a rollout is chosen based on tree statistics using a variant of the PUCT algorithm:\nU(s) = Cpuct * sqrt(\u2211N(si)) / (1 + N(s))\nThis strategy initially favors rollouts with low visit counts, gradually shifting preference towards those with high rollout values.\nBinary Search A binary search identifies the first error location in the selected rollout. Rollouts with 0 < MC(s) < 1 are added to the selection pool. The search process divides and examines rollouts to pinpoint errors, updating the tree with new states.\nMaintenance After binary search, update the statistics N(s), MC(s), and Q(s,r). Increment N(s) for the selected state-action pair and update MC(s) and Q(s,r) based on the binary search results.\nTree Construction Repeat the above process to construct the state-action tree. The process ends when the search count reaches a predetermined limit or no additional rollouts are available."}], "appendix": [{"title": "Detailed Experiment Setup", "content": ""}, {"title": "Finetuning Data Example", "content": ""}], "training": ["Training"]}