{"title": "IMPACT : In-Memory ComPuting Architecture Based on Y-Flash Technology for Coalesced Tsetlin Machine Inference", "authors": ["Omar Ghazal", "Wei Wang", "Shahar Kvatinsky", "Farhad Merchant", "Alex Yakovlev", "Rishad Shafik"], "abstract": "The increasing demand for processing large volumes of data for machine learning models has pushed data bandwidth requirements beyond the capability of traditional von Neumann architecture. In-memory computing (IMC) has recently emerged as a promising solution to address this gap by enabling distributed data storage and processing at the micro-architectural level, significantly reducing both latency and energy. In this paper, we present the IMPACT (In-Memory comPuting architecture based on Y-FlAsh technology for Coalesced Tsetlin machine inference), underpinned on a cutting-edge memory device, Y-Flash, fabricated on a 180 nm CMOS process. Y-Flash devices have recently been demonstrated for digital and analog memory applications, offering high yield, non-volatility, and low power consumption. The IMPACT leverages the Y-Flash array to implement the inference of a novel machine learning algorithm: coalesced Tsetlin machine (CoTM) based on propositional logic. CoTM utilizes Tsetlin automata (TA) to create Boolean feature selections stochastically across parallel clauses. The IMPACT is organized into two computational crossbars for storing the TA and weights. Through validation on the MNIST dataset, IMPACT achieved 96.3% accuracy. The IMPACT demonstrated improvements in energy efficiency, e.g., 2.23X over CNN-based ReRAM, 2.46X over Neuromorphic using NOR-Flash, and 2.06X over DNN-based PCM, suited for modern ML inference", "sections": [{"title": "1. Introduction", "content": "In machine learning (ML) systems, algorithm architectures are inherently driven by data during both the training and inference regimes. As such, they are confronted with challenges related to data storage, communication, processing, and the associated energy costs. These issues are exacerbated further by the growing demands of ML applications with large dimensionality of data. The traditional von Neumann computing architectures, which separate processing and memory units, do not scale well for such data-driven workloads. These architectures incur larger latency and increased energy consumption when fetching the data between the CPU and memory, thereby limiting the computational efficiency [1,2].\nIn-memory computing (IMC) offers a promising solution to address these challenges by integrating storage and processing of data. Compared to von Neumann architectures, this approach significantly reduces the data transfer overheads as well as offers natural parallelization between different processing units. Motivated by these advantages, over the years, several IMC approaches leveraging digital and analog memory technologies were proposed [1-4].\nTraditional digital-based complementary metal oxide semiconductor (CMOS) memories, including those based on static random access memory (SRAM) and dynamic random access memory (DRAM), have been investigated for IMC architectures [1,2]. These memory arrays can be reconfigured to perform bit-wise (dot multiplication) operations, making them suitable for parallel data processing. Utilizing digital memory arrays for IMC offers advantages such as established manufacturing processes, compatibility with existing digital logic, and high-density integration on a single chip [5]. However, the digital memory arrays have major limitations, including substantial static leakage, frequent logic transitions, and refreshing that cause an increase in both energy consumption and latency. Moreover, implementing complex arithmetic operations within memory arrays requires specialized hardware design and complex peripheral circuitry for computational tasks [1,2,6].\nTo address the above limitations, emerging non-volatile analog memory devices, notably memristors, have been explored. Examples include resistive RAM (ReRAM) [7], phase-change memory (PCM) [8], and magnetoresistive RAM (MRAM) [9]. Memristors can adjust its resistance based on the applied voltage/current; they can process and store information simultaneously, allowing for more efficient and parallel data processing across multiple storage units [10]. These devices can store information in a Boolean manner, similar to the two distinct binary states of digital memory, with the highest conductance state (HCS) or the lowest conductance state (LCS). Moreover, they can store information in an analog fashion, representing a continuous range of values, mimicking the behavior of synapses in the human brain [11]. These distinguishing characteristics make memristors well-suited for applications in neuromorphic computing and various machine learning algorithms, including novel approaches like the Tsetlin machine [12], where energy consumption and processing speed are core design objectives [13].\nDespite their potential, memristors have drawbacks that delay their widespread implementation in ML applications. One of the main challenges is their variability; memristors can exhibit inherent variations in their resistance states, leading to noise in computations and potentially impacting accuracy. Furthermore, memristors' endurance and retention properties are still active research areas, as these factors can affect the long-term reliability and stability of the devices [14,15].\nRecently, a novel non-volatile memristor device known as Y-Flash was introduced in [16-18]. These memristors are designed to be compatible with CMOS processes. The Y-Flash device features a floating gate isolated from its two sources, with the control gate merged with the drain. This unique design significantly reduces the device footprint while enabling program, erase, and readout operations in a two-terminal configuration. They offer the precision of digital memory and the continuous range of analog memory. In other words, the devices can operate in both digital modes (LCS and HCS) and can be fine-tuned within these states (1 n\u0391 \u2013 5 \u03bc\u0391), providing analog-like functionality. When organized in a crossbar array, the Y-Flash devices can eliminate sneak-path currents, thus removing the need for the selector device. This hybrid memristor shows"}, {"title": "2. Methods", "content": "This section introduces the two main principles for constructing the proposed architecture: IMPACT. Firstly, CoTM is an algorithm for classification tasks designed to improve efficiency and scalability through weighted clauses. Secondly, the Y-Flash device is used to create the crossbar array, which is known for its multi-conductance capability.\n(a) CoTM algorithm\nThe CoTM is an advancement of the traditional Tsetlin machine (TM) [12,21], an algorithm for machine learning based on propositional logic. In the TM, basic decision-makers called Tsetlin Automata (TA) learn to make binary decisions by receiving rewards and penalties during training. These TAs combine to form clauses. Each class has the same number of clauses that individually vote to either support or oppose its class. This structured approach allows TM to identify patterns in the input data. The CoTM improves the traditional TM by merging \"coalescing\" clauses. This approach allows the clauses to share TAs, thereby improving overall performance by reducing complexity and expediting the learning process with less computational overhead, as fewer resources are required to maintain and update the TAs. Below, we provide an overview of the algorithmic pipeline, including data preparation and TA states dynamics to form clauses. Following this, the clauses' logical expressions are shared across multiple classes, and finally, how these coalesced clauses are used collectively to classify input data into different classes.\nData preparation: is a crucial initial step in the CoTM framework. This process involves transforming raw data (Xi) into a structured format suitable for CoTM processing. Typically,"}, {"title": "2. Methods", "content": "Here, j can be {1, ..., n}, where n is the number of clauses in the CoTM, and it depends on the size of the classification problem. K is the number of Boolean literals. Therefore, the clause matrix size is (K x n).\nWeight Matrix is a two-dimensional array (W) in which each row represents a class, and each column represents a clause (see Figure 1c). The element in the matrix represents the weight of the associated clause's importance in predicting the corresponding class. During training, the clause weights are stochastically adjusted based on their success in predicting the class. Clauses that contribute effectively to accurate predictions may have their weights increased, while those leading to errors may have their weights decreased. This stochastic weight adjustment enables the CoTM to adapt to the data's characteristics and optimize its performance over time. Mathematically, the weight matrix W can be represented as:\nWhere:\n\u2022 $W_{ij}$ represents the weight associated with the contribution of clause j to class i.\n\u2022 i varies from 1 to m (number of classes).\n\u2022 j varies from 1 to n (number of clauses)."}, {"title": "3. IMPACT Architecture", "content": "This section presents the IMPACT architecture, which is based on the Y-Flash memristor crossbar. The overall architecture is shown in Figure 4. The IMPACT is designed to perform in-memory computing inference for the CoTM algorithm. To verify the robustness and reliability of IMPACT, two crossbars were assembled and tested using the MNIST dataset: the clause crossbar tile (see Figure 4a) and the class crossbar tile (see Figure 4b). The MNIST dataset consists of (28 \u00d7 28) grayscale images of the 10 handwritten digits. This results in (K = 2 \u00d7 28 \u00d7 28) Boolean literals and 10 classes. The clause crossbar tile should have rows equivalent to the number of Boolean literals K and columns equal to the number of clauses specified in the design to be 500. To ensure a robust and adaptable architecture, the size of the clause crossbar tile was set at (2048 \u00d7 500). The size of the class crossbar tile is determined by the number of clauses (rows) and classes (columns), so it is designed to be (500 \u00d7 10). The principles and operations of each tile are examined and supported with experimental measurements in the following subsections."}, {"title": "(a) Clause Crossbar Tile", "content": "The TA actions are mapped into the clause crossbar tile of the trained CoTM on the MNIST dataset as shown in Figure 4a. The array in this crossbar operates in Boolean mode by performing program/erase to an LCS < 1 nS or an HCS > 2.4 \u00b5S. Each TA's action is mapped to a single Y-Flash cell in the array, according to the mapping mechanism outlined in Table 2. When the TA's action is inclusion, indicated by logic \"1\" it is translated to an HCS in the corresponding Y-Flash cell. Conversely, when the action is exclusion, indicated by logic \"0\" it is translated to an LCS in the related cell in the crossbar array. The input Boolean literals are converted to voltage levels in such a way that the Boolean literal \"1\" and literal \"0\" are applied to the crossbar as a floating node \"Z\" or a reading voltage $V_R = 2 V$, respectively. This logic representation simulates the digital inverter gate behavior after the TA in the CoTM algorithm. Leveraging the multiplication in Ohm's law at the crosspoint between the programmed TA and the input literals mimics the OR gate in the algorithm, where the interaction between the TA action and the input literals is executed. This can be expressed as follows\n$I=TAX L \u00d7 V_R$\n\u2022 $I \\in (\\approx0, \\approx 5\\mu) A$ is the interaction output current.\n\u2022 $TA \\in (<1\\eta, > 2.4\\mu)S$.\n\u2022 L \u2208 (0, 1) used as a selector for the row multiplexer.\n\u2022 $V_R = 2V$.\nThis approach ensures that the CoTM digital logic operations are faithfully replicated in the clause crossbar tile, supporting accurate in-memory computation for the CoTM algorithm.\nThe CoTM algorithm performs clause computation using the logic AND gate (see Figure 1b). The clause output is assigned a logic \"0\" if at least one literal \u201c0\u201d interacted with an include action, regardless of the other TAs and their interacting literals. This principle is implemented in the IMPACT architecture through the clause computation tile. Each clause is defined by one column integrated with a Current Sense Amplifier (CSA) at the end of the column. Accordingly, the clause computation is governed by Kirchhoff's current law, which accumulates all interaction currents produced by the TAs associated with the clause during the reading cycle. Then the clause voltage is the potential drop across a resistor connected to the input of the CSA. Figure 5a displays the design of the CSA. It has two input terminals connected to the clause voltage and a reference voltage (Vref). Vref determines the voltage range that the input terminals of the CSA can tolerate while accurately sensing the clause state. The CSA has two outputs: the Boolean clause and its inverted version. A simple latch serves as the sensing element. The Sense Enable (SE) activates the CSA in the reading cycle and charges one of the outputs to the SE level, while the second output will be near the common voltage. SE also helps to reduce leakage power. The Discharge signal (Dis) balances the two CSA outputs by discharging them to a common voltage level, preparing the CSA for the next reading cycle. The CSA operational boundaries are designed to differentiate between the Boolean states of the clause when evaluating the clause analog voltage. These two boundaries are set based on the data in Table 2 and CoTM inference mechanism. According to Table 2, if at least one literal \u201c0\u201d interacts with TA that stores an include action, the produced"}, {"title": "(b) Class Crossbar Tile", "content": "The weights of a trained CoTM model for the MNIST dataset are merged into the class crossbar tile, as shown in Figure 4b. The array in this crossbar operates in a tunable analog mode, using varied pulse widths during programming to achieve lower conductance states and during erasing to achieve higher conductance states. Each weight (W) is assigned to a single Y-Flash cell in the array, with higher weights corresponding to higher conductance in the respective Y-Flash cell and lower weights corresponding to lower conductance. As shown in Figure 6, the CoTM produces"}, {"title": "where", "content": "Where:\n\u2022 ICS: The class weighted sum (current).\n\u2022 C: The Boolean clause \u2208 (0, 1) is used as a selector for the row multiplexer.\n\u2022 j: The jth class {1,...,m}."}, {"title": "5. Performance and Efficiency Evaluation of the IMPACT", "content": "For evaluating the performance and efficiency of the IMPACT architecture, we measured several key metrics: accuracy, energy consumption, area, and performance, as shown in Table 4. The MNIST dataset, comprising 10,000 test images, was used to assess the architecture, which employs Y-Flash devices at the core of its memory to implement the CoTM inferences.\nThe programming energy consumption/pulse was measured (5 V \u00d7 139 \u03bc\u0391 \u00d7 200 \u03bc\u03b5). However, the current can be less than this because it depends on the state of the conductance, and this current was measured without using compliance current to limit the current where it can be limited to 20 \u03bc\u0391 [16]. The erase pulse (8 V \u00d7 1 nA \u00d7 100 \u00b5s) the voltage applied to both transistors SI and SR in the two terminal configuration of Y-Flash. The reading energies in the clause crossbar tile were measured for HCS 0.05 p.J at a current of 5 \u00b5A, and for LCS (3.2 \u00d7 10-5) pJ using the average current sensed by the CSA (see Figure 5).\nMoreover, the operation in the IMPACT architecture is defined as reading one column in the crossbar array, with each column consisting of 2048 cells. To provide a concrete example, we"}, {"title": "where power efficiency directly impacts the functionality and scalability of the system.", "content": "Furthermore, C2C and D2D tests were conducted to evaluate the variability of the Y-Flash device, demonstrating its reliability for uniform weight mapping with minimal variations. Moreover, the energy per operation, 5.76 pJ, was measured with all cells in the column programmed to HCS, representing the upper bound of energy consumption and ensuring data independence in this calculation. We tested the IMPACT architecture on the MNIST dataset and achieved an accuracy of 96.3% through extensive validations. We demonstrated the performance of the inference accelerator at TOPS/mm\u00b2 = 0.17 and TOPS/W = 24.56.The results proved that IMPACT meets the demands of large-scale data processing and the corresponding need for an efficient edge computing architecture that can handle real-time inference and continuous model improvement. In future work, we will extend the evaluation of the IMPACT architecture to larger and more complex datasets, such as CIFAR-10 and ImageNet, to further validate its scalability and performance."}, {"title": "and ensures stable performance for reasonable-sized arrays (e.g., 2048 \u00d7 500, as employed in our MNIST experiments), see section 2(b). Further, the IMPACT design specification mitigates the memristor's inherent limitations with the help of CoTM's propositional logic. It is achieved by representing a literal '1' as a floating node and thus reduces the number of active nodes in a single column during the clause computation cycle. This approach improves energy efficiency and reduces the column resistance voltage drops, contributing to the stable performance of larger arrays.", "content": "The second aspect is the ability to distribute tasks across multiple arrays. The IMPACT architecture is organized in a hierarchical crossbar structure to achieve task distribution, see section 3(b), and 3(a). The first array serves intermediate inferences by performing the clause computations. The clauses are processed further at a higher-level crossbar to compute the classes for the final classification decision-making. Moreover, the Boolean nature of clause computation in the CoTM enhances the modularity of the IMPACT architecture, especially when dealing with larger datasets. In such cases, the number of literals exceeds the capacity of a single clause computation. However, the clause computation task can be distributed across multiple crossbars, where each crossbar processes a subset of the literals, generating partial Boolean clauses [20,22], shown in Figure 14a. These partial outputs are combined outside the arrays using digital AND gates to compute the related full Boolean clause. Similarly, the class computation layer can benefit from this modular approach as shown in Figure 14b. When the number of weights per class exceeds the capacity of a single array, the class computation can be split across multiple crossbars. Each crossbar stores and processes a subset of the weights, and the analog outputs are digitized using Analog-to-Digital Converters (ADCs). These digitized outputs are combined in the digital domain to form the final related class weight. Figure 14 uses the maximum unrolling parallelism for the two matrices, however, this is subject to resource availability. The P tiles can be computed in a combination of temporal and spatial subsets."}, {"title": "6. Conclusions", "content": "We introduced an in-memory computing inference architecture for the coalesced Tsetlin machine algorithm (CoTM), named IMPACT. It is constructed by two memory arrays: the clause crossbar tile and the class crossbar tile. The clause crossbar tile facilitates the storage and interaction of TA actions with corresponding input Boolean literals, thereby enabling the computation of Boolean clauses. The class crossbar tile stores weights and executes the weighted sum operations by integrating these weights with the computed Boolean clauses. The CoTM principle aims to learn patterns using simple propositional logical expressions to supervise decision-making. This highlights the structural simplicity of CoTM and ensures scalability and efficiency within the IMC hierarchical inference approach. The IMPACT integrates a Y-Flash memristor device manufactured using a 180 nm CMOS process. It has a fast read speed 5 ns, data retention > 10 years, and endurance of 105 cycles. Y-Flash is used in the clause crossbar tile to operate in Boolean mode by storing a HCS \u2248 2.5 \u00b5S to represent the include action and a LCS \u2248 1 nS for the exclude action. In the class crossbar tile, Y-Flash operates in analog tunable conductance mode. The entire conductance range (1 nS \u2013 2.5 \u00b5S) is uniformly segmented to target conductances equal to the number of the highest unsigned software weight."}, {"title": "where", "content": "Moreover, the energy per operation, 5.76 PJ, was measured with all cells in the column programmed to HCS, representing the upper bound of energy consumption and ensuring data independence in this calculation. We tested the IMPACT architecture on the MNIST dataset and achieved an accuracy of 96.3% through extensive validations. We demonstrated the performance of the inference accelerator at TOPS/mm\u00b2 = 0.17 and TOPS/W = 24.56.The results proved that IMPACT meets the demands of large-scale data processing and the corresponding need for an efficient edge computing architecture that can handle real-time inference and continuous model improvement. In future work, we will extend the evaluation of the IMPACT architecture to larger and more complex datasets, such as CIFAR-10 and ImageNet, to further validate its scalability and performance."}]}