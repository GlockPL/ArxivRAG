{"title": "Investigating Imperceptibility of Adversarial Attacks on Tabular Data: An Empirical Analysis", "authors": ["Zhipeng He", "Chun Ouyang", "Laith Alzubaidi", "Alistair Barros", "Catarina Moreira"], "abstract": "Adversarial attacks are a potential threat to machine learning models, as they can cause the model to make incorrect predictions by introducing imperceptible perturbations to the input data. While extensively studied in unstructured data like images, their application to structured data like tabular data presents unique challenges due to the heterogeneity and intricate feature interdependencies of tabular data. Imperceptibility in tabular data involves preserving data integrity while potentially causing misclassification, unlike imperceptibility in images, which deals with subtle changes in pixel values. This distinction underscores the need for tailored imperceptibility criteria for tabular data. However, there is currently a lack of standardised metrics for assessing adversarial attacks specifically targeted at tabular data.\nTo address this gap, we derive a set of properties for evaluating the imperceptibility of adversarial attacks on tabular data. These properties are defined to capture seven perspectives of perturbed data: proximity to the original input, sparsity of alterations, deviation to datapoints in the original dataset, sensitivity of altering sensitive features, immutability of perturbation, feasibility of perturbed values and intricate feature interdepencies among tabular features.\nFurthermore, this paper conducts both quantitative empirical evaluation and case-based qualitative examples analysis for seven imperceptibility properties. The evaluation reveals a trade-off between the success of adversarial attacks and imperceptibility, particularly concerning proximity, sensitivity, and deviation. Although no evaluated attacks can achieve optimal effectiveness and imperceptibility simultaneously, unbounded attacks, such as C&W and DeepFool, prove to be more promised for tabular data, emphasising their effectiveness in crafting imperceptible adversarial examples. The study also highlights the limitation of evaluated algorithms in controlling sparsity effectively. We suggest incorporating a sparsity metric in future attack algorithms to regulate the number of perturbed features. The insights gained from these empirical findings provide valuable knowledge that guides the development of imperceptible adversarial attack algorithms, proving essential for navigating the complex landscape of adversarial machine learning on tabular data.", "sections": [{"title": "1. Introduction", "content": "Adversarial attacks aim to generate perturbations on input data, known as adversarial examples, to deceive machine learning models into making incorrect predictions (Szegedy et al., 2014). As a result, adversarial examples can be used to examine the vulnerability and robustness of machine learning models. In addition to being effective in causing misclassification, adversarial examples are also expected to contain imperceptible perturbations, which can be described as \"indistinguishable to the human eye\" (Goodfellow et al., 2015), ensuring they remain undetectable to human observation.\nThe imperceptibility of adversarial examples varies significantly between unstructured data (such as images and text) and structured data (like tabular data) due to the inherent differences in their structures and features. Current adversarial attack algorithms primarily target unstructured data, especially images (Croce et al., 2020). Tabular data distinguishes itself as a prime example of structured data, playing a pivotal role in real-world applications like historical transaction trail in fintech, electronic medical records in healthcare management, and sensor readings in traffic monitoring and control. Compared to image data which is high-dimensional and homogeneous, tabular data is low-dimensional but diverse in feature types and ranges and characterised by heterogeneity and intricate feature interdependencies.\nIn image data, imperceptibility often refers to making changes to the pixel values that are subtle enough to go unnoticed by the human eye. Adversarial examples are crafted by perturbing pixel values strategically to induce misclassification by machine learning models while maintaining visual similarity to the original image. Consider the example of a specific adversarial attack to an image as depicted in Figure 1a: the perturbation applied to the image has resulted in misclassification but is challenging to detect by human observation.\nHowever, when applying the same attack algorithm to a tabular dataset as depicted in Figure 1b, it may effectively cause misclassification, whereas the perturbation-induced change can be easily noticed by humans. This suggests that the concept of imperceptibility must be approached differently for tabular data compared to images, as the characteristics of tabular data pose new requirements when addressing imperceptibility. These may include: 1) modification of a smaller number of features; 2) minimisation of the distance between the original data points and adversarial examples; and 3) prevention of the generated adversarial examples from being deemed outliers.\nRecent developments in adversarial machine learning have increasingly centered on tabular data, with a noticeable emphasis on the imperceptibility of adversarial attacks. Existing research has tended to focus on specific facets of imperceptibility, such as perturbing less significant features determined by feature importance (Ballet et al., 2019), integrating domain knowledge to safeguard sensitive or immutable features from attacks (Mathov et al., 2022; Chernikova and Oprea, 2022), managing imperceptibility using cost-constrained attacks (Kireev et al., 2022). A notable gap in the literature lies in the absence of a comprehensive definition for what constitutes imperceptibility specifically in the context of tabular data.\nTherefore, this paper aims to address the gap by providing a more thorough investigation of imperceptibility in adversarial attacks on tabular data. Unlike previous studies that focus on individual aspects of imperceptibility, our research adopts a holistic approach with multifaceted analysis to enhance the understanding of the challenges and opportunities associated with securing tabular data against adversarial attacks. Our contributions to the field are twofold.\nFirstly, we propose seven key properties that characterise imperceptible adversarial attacks for tabular data. These properties, namely proximity, sparsity, deviation, sensitivity, immutability, feasibility and feature interdependency, are derived through an examination of the unique characteristics and challenges inherent in tabular data. Secondly, we investigate all seven properties of imperceptibility, analyse the relation between adversarial attacks' effectiveness and imperceptibility, and draw insights from a comprehensive evaluation. This leads to the findings about limitations of current adversarial attack algorithms in achieving imperceptibility in attacks targeting tabular data. It also helps us identify potential research directions for improving adversarial attack algorithms specifically tailored for tabular data. Furthermore, the insights are essential for both researchers and practitioners, offering tangible guidance in the increasingly complex landscape of adversarial machine learning on tabular data.\nThe rest of this paper is structured as follows. Section 2 provides an overview of the background and reviews related work on adversarial attacks and their imperceptibility concerning tabular data. Section 3 proposes imperceptibility properties and metrics pertinent to adversarial attacks on tabular data. Section 4 specifies the experiment settings and analyses the evaluation results. Section 5 presents the insights drawn from the evaluation analysis and discusses limitations in current adversarial attack algorithms. Finally, Section 6 concludes the work and outlines potential avenues for future research."}, {"title": "2. Background and Related Work", "content": "2.1. Adversarial Attacks and Imperceptibility\nSeveral state-of-the-art adversarial attack methods have been proposed since the concept of adversarial examples was introduced by Szegedy et al. (2014). The fast gradient sign method (FGSM) (Goodfellow et al., 2015) generates adversarial examples by taking the gradient of the model's loss with respect to the input. The basic iterative method (BIM) (Kurakin et al., 2017) is similar to FGSM but aims to find smaller perturbations through iterations. The momentum iterative method (MIM) and projected gradient descent (PGD) are two variants of BIM. MIM (Dong et al., 2018) incorporates momentum to better handle noise and increase the transferability of adversarial examples. It keeps a running average of the gradients across iterations, allowing for smoother updates. PGD (Madry et al., 2017) iteratively projects the perturbed input onto an $l_p$-bounded space around the original input in order to generate more effective attacks targeting at defense mechanisms like gradient masking. This makes PGD more robust against gradient masking and stochastic gradient descent attacks. The Jacobian-based saliency map approach (JSMA) (Papernot et al., 2016) finds and perturbs the input features that are most important to the model's prediction using saliency map. The Carlini and Wagner (C&W) Attack (Carlini and Wagner, 2017) optimises a loss function that measures the difference between the original inputs and perturbed instances to craft adversarial examples. DeepFool (Moosavi-Dezfooli et al., 2016) generates adversarial examples by linearising the model's decision boundaries and applying minimal perturbation to move a given input across its nearest boundary.\nImperceptibility is an important property of adversarial perturbations. Existing work mainly focus on evaluating imperceptibility of adversarial examples for image data, where $l_p$ norm is commonly used to measure the distance between the original and perturbed images (Szegedy et al., 2014). For example, Luo et al. (2018) propose perturbation sensitivity for determining which region of pixels in images is prone to be perceived if being perturbed. Croce and Hein (Croce and Hein, 2019) use $l_0$ norm to limit the number of perturbed pixels in generating sparse and imperceptible adversarial examples. Furthermore, Sharif et al. (2018) suggest considering human perception and semantic information in evaluating the imperceptibility of images, highlighting the limitation of $l_p$ norm in measuring imperceptibility from human observation and semantic perspectives.\n2.2. State-of-the-art Adversarial Attacks on Tabular Data\nLowProFool (Ballet et al., 2019) uses a weighted $l_p$ norm to determine the set of features to perturb. This attack method relies on the absolute value of the Pearson's correlation coefficient for each numerical feature to determine their feature importance in order to calculate the weighted $l_p$ norm. As such, LowProFool is only applicable to numerical features, which restricts its applicability to a specific set of scenarios. Hashemi and Fathi (2020) utilise the similarity between adversarial and counterfactual"}, {"title": "3. Imperceptibility of Adversarial Attacks on Tabular data", "content": "3.1. Establishing Criteria for Imperceptibility\nSince the majority of adversarial attack methods have been developed for image data, their application to tabular data requires consideration of the unique characteristics of the latter. In addition to distance-based criteria, it is necessary to account for the specific nature of tabular data, when addressing the imperceptibility of adversarial attacks. Based on the review of literature in Section 2, we establish the following criteria in addressing imperceptibility of adversarial attacks on tabular data.\nMinimisation of feature perturbation. In principle, smallest changes are expected to help make adversarial examples imperceptible. More specifically, an adversarial example should be crafted as 'close' as possible to the original input data points and fewer features should be modified in the perturbation.\nPreservation of statistical data distribution. Adversarial attacks that are expected to be imperceptible should closely align with the input data distribution. This means that the perturbations should maintain the key characteristics of the data on which the model was trained. Adversarial examples that significantly deviate from the original statistical properties are more likely to be detected by their model.\nNarrow-guard feature perturbation. In tabular data, each feature typically exhibits a unique distribution. When perturbations are applied across features, the impact is more pronounced on features with narrower distributions compared to those with broader distributions, as features with narrower distributions are more sensitive to changes. Hence, to generate imperceptible adversarial attacks, it is important to apply perturbations that avoid altering features with narrow distribution or at least minimise their impact.\nPreservation of feature semantics. In tabular data, each feature typically has clearly defined semantics and valid practical value ranges, such as gender and age. However, adversarial attacks may introduce perturbations that alter feature semantics (e.g., changing gender from female to male) or extend feature values beyond valid practical value ranges (e.g., changing age from 20 to 120). Hence, ensuring imperceptibility in adversarial attacks on tabular data requires preserving feature semantics in alignment with its domain knowledge.\nPreservation of feature interdependencies. Feature interdependency in a tabular dataset refers to the relationships and correlations between different attributes or variables. Recognising feature inter-dependency is important because changes to one feature can affect the validity or interpretation of related features. For example, an individual's age and their age group (such as 'young adult' or 'senior') are inter-dependent, and changes to the age feature should correspond to changes in the age group feature to maintain consistency. Hence, ensuring imperceptibility in adversarial attacks on tabular data requires preserving feature inter-dependencies to maintain data integrity.\n3.2. Properties of Imperceptibility\nBased on the criteria established above, we propose a set of properties for assessing the imperceptibility of adversarial attacks. First of all, we provide a definition of adversarial attacks."}, {"title": "Definition 1 (Adversarial Attack).", "content": "Consider a dataset where each input data point a vector $x \\in X$ belongs to a class with label $y \\in Y$. Let $f(.)$ denote a machine learning classifier, an adversarial example $x^{adv}$ generated by an adversarial attack is a perturbed input similar to $x$ and misclassifies the label of $x$.\n$x^{adv} = x + \\delta$ subject to $f(x^{adv}) \\neq y$\nwhere $\\delta$ denotes input perturbation.\n3.2.1. Proximity\nGiven the criterion of minimisation of feature perturbation, a good adversarial example should introduce minimal changes, which can be quantified by maintaining the smallest possible distance from the original feature vector. We use the term 'proximity' to refer to relevant distance metrics. As is the case with most of the existing adversarial attack algorithms (Carlini and Wagner, 2017; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016), we employ $l_p$ norm to measure the perturbation distance.\nProximity Metrics. $l_p$ norm is a distance metric for measuring the magnitude of a vector in n-dimensional space. Three candidatures of $l_p$ norm can be used as proximity metrics for measuring the distance between $x$ and $x^{adv}$ in grid-like path distance ($l_1$ distance), straight-line distance ($l_2$ distance) and maximum feature difference ($l_\\infty$ distance).\n$l_p(x^{adv}, x) = ||x^{adv} - x||_p = \\begin{cases} (\\sum_{i=1}^{n} |x_i^{adv} - x_i|^p)^{1/p}, & p \\in \\{1, 2\\}\\\\\n\\sup_n |x_n^{adv} - x_n|, & p \\rightarrow \\infty \\end{cases}$"}, {"title": "3.2.2. Sparsity", "content": "Given the criterion of minimisation of feature perturbation, this property seeks to identify the minimum feature set required to create an adversarial example. Tabular data, unlike images, is relatively low-dimensional, and as such, the number of altered features in a perturbation would have a more obvious effect on its imperceptibility (Borisov et al., 2021). Therefore, an ideal adversarial example should change the model's prediction by altering as few features as possible.\nSparsity Metric. Straightforwardly, sparsity measures the number of altered features in an adversarial example $x^{adv}$ compared to the original input vector $x$.\n$Spa(x^{adv}, x) = l_0(x^{adv}, x) = \\sum_{i=1}^n 1(x_i^{adv} \\neq x_i)$"}, {"title": "3.2.3. Deviation", "content": "Existing studies (Lee et al., 2018; Nguyen et al., 2015) suggest that adversarial attacks are a special example of out-of-distribution being created with the intention to fool a model. Whilst adversarial examples are not deemed to be representative of the actual distribution of predictive models, a perturbed input should be as similar as possible to the majority of original inputs when addressing imperceptibility, thus addressing the criterion of preservation of statistical data distribution.\nDeviation Metric. We propose to use Mahalanobis distance (MD) (McLachlan, 1999) to measure the deviation between an adversarial perturbation and the distribution of the original data input variation. MD is a multi-dimensional generalisation of $l_2$ norm. This distance metric is commonly used for identifying outliers in multi-dimensional data, as well as for detecting correlations among different features. Given an input vector x, a perturbed vector $x^{adv}$ and the covariance matrix V.\n$MD(x^{adv}, x) = \\sqrt{(x^{adv} - x)^T V^{-1} (x^{adv} - x)}$"}, {"title": "3.2.4. Sensitivity", "content": "Luo et al. (2018) propose the notion of perturbation sensitivity for images by calculating the inverse of the standard deviation of the pixel region with the attack perturbation, and add perturbation sensitivity as a regularisation to the attack algorithm to generate imperceptible images. In our work, we adapt the concept of perturbation sensitivity as a metric to measure the degree to which the features with narrow distribution in tabular data are altered, addressing the criterion of narrow-guard feature perturbation.\nSensitivity Metric. Perturbation sensitivity is a normalised $l_1$ distance between x and $x^{adv}$ by the inverse of the standard deviation of all numerical features within the input dataset X. It is represented by:\n$\\begin{aligned}\nSDV(x_i) &= \\sqrt{\\frac{\\sum_{j=1}^{m}(x_{i,j} - \\bar{x_i})^2}{m}}\\\\\nSEN(x, x^{adv}) &= \\sum_{i=1}^n \\frac{|x_i^{adv} - x_i|}{SDV(x_i)}\n\\end{aligned}$"}, {"title": "4. Evaluation", "content": "4.1. Design of Experiments\nWe conduct a series of experiments to evaluate the imperceptibility of adversarial attack algorithms on tabular datasets, using the properties of imperceptibility proposed in the previous section. Our experiments follow the typical machine learning pipeline, comprising data preparation, model training, and model testing. We generate adversarial examples using adversarial attack algorithms and then assess these algorithms through the following experiments:\n\u2022 Experiment 1: Evaluating the effectiveness of applied adversarial attack algorithms, specifically how successful these algorithms are in misclassifying a trained machine learning model.\n\u2022 Experiment 2: Evaluating the imperceptibility of applied adversarial attack algorithms using quantitative properties, such as sparsity, proximity, deviation, and sensitivity.\n\u2022 Experiment 3: Analysing case-based examples to assess the qualitative properties, such as immutability, feasibility, and feature interdependency, for the imperceptibility of applied adversarial attack algorithms."}, {"title": "4.3. Evaluation of Attack Imperceptibility using Quantitative Properties", "content": "As mentioned before, an adversarial attack should achieve a success rate above a give threshold before its imperceptibility can be evaluated. We set a threshold of 30% for attack success rate. As shown in Table 2, applying the C&W attack to the LinearSVC model resulted in a success rate below the specified threshold across all five datasets. Therefore, it is excluded from the imperceptibility evaluation. Tables 4 to 8 present the results of imperceptibility of evaluation using four quantitative properties.\nSparsity. Sparsity metric quantifies the average number of modified features over all adversarial examples generated by an attack. Lower sparsity values suggest that the adversarial attacks possess better sparsity, implying that the modification made to the features are less perceptible. Since the number of features varies across datasets, sparsity can only be compared within the same dataset. The results in Table 4 suggest that all attack methods yield comparable sparsity results cross different models in each dataset.\nFurthermore, we investigate the potential relationship between sparsity value and feature types. In two numerical feature only datasets, we observe that the sparsity value is consistently close to the total number of numerical features in each corresponding dataset (refer to Figure 1). The results imply that five attack methods exhibit a tendency to perturb almost all features within two numerical datasets. When analysing mixed datasets, comparing the sparsity value solely with the number of numerical or categorical features is insufficient to identify the relationship between them. We then examine all original inputs and their corresponding adversarial examples in each combination of attack and models. We calculate the individual feature perturbation count for each combination, which record the total number of times being perturbed by each feature. The results are visualised as heatmaps in Figure 2, which indicate that all attacks tend to alter numerical features more frequently than categorical features in Adult, German and Compas datasets.\nOnly in Adult and German datasets, DeepFool and C&W attacks modify categorical features.\nProximity. Proximity measures the similarity of the perturbed example and original input. A smaller proximity metric value indicates that adversarial examples are closer to original inputs, which represents better imperceptibility. In our evaluation, we use $l_2$ and $l_\\infty$ norm as our two proximity metrics. The results of two metrics are shown in Table 5 ($l_2$) and Table 6 ($l_\\infty$) respectively.\nTable 5 suggests, in most datasets, C&W attack method yields the lowest Proximity $l_2$ values on both LR and MLP models, with the exception of German dataset and LR model. On the other hand, DeepFool achieves the lowest $l_2$ values using the LinearSVC model in all five datasets.\nTable 6 suggests the proximity $l_\\infty$ values for FGSM and PGD attacks are consistently 0.3 across all models"}, {"title": "5. Discussion and Limitations", "content": "5.1. A trade-off between imperceptibility and performance on adversarial attacks\nWe conduct experiments encompassing all four quantitative properties related to imperceptibility. All five adversarial attacks with different performance have similar sparsity levels, indicating that there is no correlation between attack success and sparsity levels. However, proximity, deviation, and sensitivity appear to display inconsistent behaviours in the different levels of attack success rate. To analyse the potential relationship between these three metrics and attack success rate, all adversarial example are divided into two groups: successful attack examples and unsuccessful attack examples. Box plots are then utilised to compare the distribution of each metric across these two groups of adversarial examples.\nAs depicted in Figure 4, the distributions of the two groups of adversarial examples reveal that successful attack instances typically exhibit larger values in $l_2$ distance compared to unsuccessful attack examples, particularly evident in unbounded attacks. However, in bounded attacks, where there's a constrained attack budget in place limiting the maximum proximity, the pattern is less distinct. The similar patterns are also identified in deviation (Figure 5) and sensitivity (Figure 6). The distributions of deviation and sensitivity for Deepfool, C&W attack and LowProFool shift from low to high when changing the attack success from false to true. For bounded attacks, although the limited attack budget still has an impact, trade-off patterns are observed in some experimental settings.\nBy incorporating the comparative analysis between attack success rate and three imperceptibility metrics, we identify there is a trade-off between effectiveness and imperceptibility for adversarial attacks on tabular data. It emphasises the intricate balance that must be maintained to develop both effective and imperceptible adversarial attacks.\n5.2. No one silver bullet for attacking tabular data, but unbounded attacks demonstrate greater promise\nAccording to our evaluation on the effectiveness and imperceptibility of five adversarial attack algorithms, none of them can simultaneously achieve the highest attack success rate and the highest level of imperceptibility. Despite their high success rates, bounded attacks like FGSM and PGD often struggle to maintain high levels of imperceptibility, especially when considering metrics such as deviation and sensitivity. Additionally, determining an appropriate attack budget poses a significant challenge for bounded attacks on tabular data, as it directly impacts the trade-off between attack success and imperceptibility. In theory, by systematically investigating different budget values and evaluating their impact on both success rate and imperceptibility metrics, researchers can identify the sweet spot that maximises attack effectiveness while minimising perceptibility. However, conducting such an exhaustive search for the best attack budgets for each dataset is impractical due to resource constraints and the vast diversity of datasets.\nOn the other hand, unbounded attacks demonstrate more promise in crafting imperceptible adversarial examples. C&W and DeepFool outperform FGSM and PGD in the evaluation of proximity, deviation"}, {"title": "6. Conclusion", "content": "In conclusion, this paper has made significant contributions in advancing our understanding of imperceptibility in adversarial machine learning on tabular data. The derivation of seven key imperceptibility properties, including proximity, sparsity, deviation, sensitivity, immutability, feasibility and feature interdependency, provides a comprehensive framework for characterising adversarial attacks in the context of tabular data. Furthermore, the practical insights derived from our empirical evaluation on these imperceptibility properties and metrics offer valuable guidance, such as the identified trade-offs between attack success and imperceptibility, the efficacy of optimisation-based attacks, and limitations in controlling sparsity, for developing robust and imperceptible adversarial attacks.\nWhile our investigation of adversarial attacks on tabular data has provided valuable insights, several limitations and directions for future research should be acknowledged. The use of one-hot encoding increases dataset dimensionality, prompting a need for careful consideration of distance metrics for both numerical and categorical features. Future research should expand to larger datasets, sophisticated models, and a wider array of attack strategies. Additionally, the absence of the ablation study signals a need for future investigations into the impact of feature removal on adversarial robustness and imperceptibility."}]}