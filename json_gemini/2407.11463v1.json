{"title": "Investigating Imperceptibility of Adversarial Attacks on Tabular Data: An Empirical Analysis", "authors": ["Zhipeng He", "Chun Ouyang", "Laith Alzubaidi", "Alistair Barros", "Catarina Moreira"], "abstract": "Adversarial attacks are a potential threat to machine learning models, as they can cause the model to make incorrect predictions by introducing imperceptible perturbations to the input data. While extensively studied in unstructured data like images, their application to structured data like tabular data presents unique challenges due to the heterogeneity and intricate feature interdependencies of tabular data. Imperceptibility in tabular data involves preserving data integrity while potentially causing misclassification, unlike imperceptibility in images, which deals with subtle changes in pixel values. This distinction underscores the need for tailored imperceptibility criteria for tabular data. However, there is currently a lack of standardised metrics for assessing adversarial attacks specifically targeted at tabular data.\nTo address this gap, we derive a set of properties for evaluating the imperceptibility of adversarial attacks on tabular data. These properties are defined to capture seven perspectives of perturbed data: proximity to the original input, sparsity of alterations, deviation to datapoints in the original dataset, sensitivity of altering sensitive features, immutability of perturbation, feasibility of perturbed values and intricate feature interdepencies among tabular features.\nFurthermore, this paper conducts both quantitative empirical evaluation and case-based qualitative examples analysis for seven imperceptibility properties. The evaluation reveals a trade-off between the success of adversarial attacks and imperceptibility, particularly concerning proximity, sensitivity, and deviation. Although no evaluated attacks can achieve optimal effectiveness and imperceptibility simultaneously, unbounded attacks, such as C&W and DeepFool, prove to be more promised for tabular data, emphasising their effectiveness in crafting imperceptible adversarial examples. The study also highlights the limitation of evaluated algorithms in controlling sparsity effectively. We suggest incorporating a sparsity metric in future attack algorithms to regulate the number of perturbed features. The insights gained from these empirical findings provide valuable knowledge that guides the development of imperceptible adversarial attack algorithms, proving essential for navigating the complex landscape of adversarial machine learning on tabular data.", "sections": [{"title": "1. Introduction", "content": "Adversarial attacks aim to generate perturbations on input data, known as adversarial examples, to deceive machine learning models into making incorrect predictions (Szegedy et al., 2014). As a result, adversarial examples can be used to examine the vulnerability and robustness of machine learning models. In addition to being effective in causing misclassification, adversarial examples are also expected to contain imperceptible perturbations, which can be described as \"indistinguishable to the human eye\" (Goodfellow et al., 2015), ensuring they remain undetectable to human observation.\nThe imperceptibility of adversarial examples varies significantly between unstructured data (such as images and text) and structured data (like tabular data) due to the inherent differences in their structures and features. Current adversarial attack algorithms primarily target unstructured data, especially images (Croce et al., 2020). Tabular data distinguishes itself as a prime example of structured data, playing a pivotal role in real-world applications like historical transaction trail in fintech, electronic medical records in healthcare management, and sensor readings in traffic monitoring and control. Compared to image data which is high-dimensional and homogeneous, tabular data is low-dimensional but diverse in feature types and ranges and characterised by heterogeneity and intricate feature interdependencies.\nIn image data, imperceptibility often refers to making changes to the pixel values that are subtle enough to go unnoticed by the human eye. Adversarial examples are crafted by perturbing pixel values strategically to induce misclassification by machine learning models while maintaining visual similarity to the original image. \nHowever, when applying the same attack algorithm to a tabular dataset, it may effectively cause misclassification, whereas the perturbation-induced change can be easily noticed by humans. This suggests that the concept of imperceptibility must be approached differently for tabular data compared to images, as the characteristics of tabular data pose new requirements when addressing imperceptibility. These may include: 1) modification of a smaller number of features; 2) minimisation of the distance between the original data points and adversarial examples; and 3) prevention of the generated adversarial examples from being deemed outliers.\nRecent developments in adversarial machine learning have increasingly centered on tabular data, with a noticeable emphasis on the imperceptibility of adversarial attacks. Existing research has tended to focus on specific facets of imperceptibility, such as perturbing less significant features determined by feature importance (Ballet et al., 2019), integrating domain knowledge to safeguard sensitive or immutable features from attacks (Mathov et al., 2022; Chernikova and Oprea, 2022), managing imperceptibility using cost-constrained attacks (Kireev et al., 2022). A notable gap in the literature lies in the absence of a comprehensive definition for what constitutes imperceptibility specifically in the context of tabular data.\nTherefore, this paper aims to address the gap by providing a more thorough investigation of imperceptibility in adversarial attacks on tabular data. Unlike previous studies that focus on individual aspects of imperceptibility, our research adopts a holistic approach with multifaceted analysis to enhance the understanding of the challenges and opportunities associated with securing tabular data against adversarial attacks. Our contributions to the field are twofold.\nFirstly, we propose seven key properties that characterise imperceptible adversarial attacks for tabular data. These properties, namely proximity, sparsity, deviation, sensitivity, immutability, feasibility and feature interdependency, are derived through an examination of the unique characteristics and challenges inherent in tabular data. Secondly, we investigate all seven properties of imperceptibility, analyse the relation between adversarial attacks' effectiveness and imperceptibility, and draw insights from a comprehensive evaluation. This leads to the findings about limitations of current adversarial attack algorithms in achieving imperceptibility in attacks targeting tabular data. It also helps us identify potential research directions for improving adversarial attack algorithms specifically tailored for tabular data. Furthermore, the insights are essential for both researchers and practitioners, offering tangible guidance in the increasingly complex landscape of adversarial machine learning on tabular data.\nThe rest of this paper is structured as follows. Section 2 provides an overview of the background and reviews related work on adversarial attacks and their imperceptibility concerning tabular data. Section 3 proposes imperceptibility properties and metrics pertinent to adversarial attacks on tabular data. Section 4 specifies the experiment settings and analyses the evaluation results. Section 5 presents the insights drawn from the evaluation analysis and discusses limitations in current adversarial attack algorithms. Finally, Section 6 concludes the work and outlines potential avenues for future research."}, {"title": "2. Background and Related Work", "content": "Several state-of-the-art adversarial attack methods have been proposed since the concept of adversarial examples was introduced by Szegedy et al. (2014). The fast gradient sign method (FGSM) (Goodfellow et al., 2015) generates adversarial examples by taking the gradient of the model's loss with respect to the input. The basic iterative method (BIM) (Kurakin et al., 2017) is similar to FGSM but aims to find smaller perturbations through iterations. The momentum iterative method (MIM) and projected gradient descent (PGD) are two variants of BIM. MIM (Dong et al., 2018) incorporates momentum to better handle noise and increase the transferability of adversarial examples. It keeps a running average of the gradients across iterations, allowing for smoother updates. PGD (Madry et al., 2017) iteratively projects the perturbed input onto an $l_p$-bounded space around the original input in order to generate more effective attacks targeting at defense mechanisms like gradient masking. This makes PGD more robust against gradient masking and stochastic gradient descent attacks. The Jacobian-based saliency map approach (JSMA) (Papernot et al., 2016) finds and perturbs the input features that are most important to the model's prediction using saliency map. The Carlini and Wagner (C&W) Attack (Carlini and Wagner, 2017) optimises a loss function that measures the difference between the original inputs and perturbed instances to craft adversarial examples. DeepFool (Moosavi-Dezfooli et al., 2016) generates adversarial examples by linearising the model's decision boundaries and applying minimal perturbation to move a given input across its nearest boundary.\nImperceptibility is an important property of adversarial perturbations. Existing work mainly focus on evaluating imperceptibility of adversarial examples for image data, where $l_p$ norm is commonly used to measure the distance between the original and perturbed images (Szegedy et al., 2014). For example, Luo et al. (2018) propose perturbation sensitivity for determining which region of pixels in images is prone to be perceived if being perturbed. Croce and Hein (Croce and Hein, 2019) use $l_0$ norm to limit the number of perturbed pixels in generating sparse and imperceptible adversarial examples. Furthermore, Sharif et al. (2018) suggest considering human perception and semantic information in evaluating the imperceptibility of images, highlighting the limitation of $l_p$ norm in measuring imperceptibility from human observation and semantic perspectives."}, {"title": "2.1. Adversarial Attacks and Imperceptibility", "content": null}, {"title": "2.2. State-of-the-art Adversarial Attacks on Tabular Data", "content": "LowProFool (Ballet et al., 2019) uses a weighted $l_p$ norm to determine the set of features to perturb. This attack method relies on the absolute value of the Pearson's correlation coefficient for each numerical feature to determine their feature importance in order to calculate the weighted $l_p$ norm. As such, LowProFool is only applicable to numerical features, which restricts its applicability to a specific set of scenarios. Hashemi and Fathi (2020) utilise the similarity between adversarial and counterfactual example generation methods to create adversarial examples. Mathov et al. (2022) introduce surrogate models that maintain the target model's properties to improve the effectiveness of adversarial example generation techniques for tabular data. Alongside traditional distance-based attacks, Kireev et al. (2022) propose a different approach by incorporating the concepts of cost and utility as constraints in the adversarial example generation process. This method relies on a greedy best-first graph search algorithm to effectively craft adversarial examples that meet the relevant cost and utility constraints. In addition to white-box attacks, Cartella et al. (2021) adapt three black-box attack techniques, namely the Boundary attack (Brendel et al., 2017), HopSkipJump attack (Chen et al., 2020), and ZOO (Chen et al., 2017), to target tabular data.\nIn the context of tabular data, human eyes tend to more readily detect perturbations in feature values compared to image data. Distance-based metrics alone often cannot fully encompass all the characteristics outlined in Section 1, and additional factors must be considered. For example, the number of features perturbed (or called perturbation size) can also serve as a useful indicator for measuring imperceptibility (Ballet et al., 2019). Recent studies have also emphasised semantic imperceptibility of adversarial attacks in the context of tabular data (Mathov et al., 2022; Chernikova and Oprea, 2022). These works leverage domain knowledge to enhance the efficacy of their algorithms, strategically choosing features to perturb. Mathov et al. (2022) propose rules for immutable features and consider data types for different features. They use an embedding function incorporating all these rules to maintain the validity of perturbed examples. Similarly, Chernikova and Oprea (2022) translate domain knowledge related to network traffic into constraints, ensuring that the generated adversarial attacks do not yield in invalidated adversarial examples. These approaches showcase a subtle integration of domain expertise, shedding light on the intricacies of semantic imperceptibility of adversarial attacks in the context of in tabular data.\nOur review indicates that current studies address the imperceptibility of adversarial attacks on tabular data from various angles and with specific methods. However, there is a lack of an overall framework that comprehensively defines imperceptibility in these attacks. This gap highlights the need to not only survey existing approaches but also to establish a unified understanding of imperceptibility in the context of tabular data. This will facilitate a deeper insight into adversarial attacks against tabular data and enable the development of improved adversarial training strategies suitable for such data."}, {"title": "3. Imperceptibility of Adversarial Attacks on Tabular data", "content": "Since the majority of adversarial attack methods have been developed for image data, their application to tabular data requires consideration of the unique characteristics of the latter. In addition to distance-based criteria, it is necessary to account for the specific nature of tabular data, when addressing the imperceptibility of adversarial attacks. Based on the review of literature in Section 2, we establish the following criteria in addressing imperceptibility of adversarial attacks on tabular data.\nMinimisation of feature perturbation. In principle, smallest changes are expected to help make adversarial examples imperceptible. More specifically, an adversarial example should be crafted as 'close' as possible to the original input data points and fewer features should be modified in the perturbation.\nPreservation of statistical data distribution. Adversarial attacks that are expected to be imperceptible should closely align with the input data distribution. This means that the perturbations should maintain the key characteristics of the data on which the model was trained. Adversarial examples that significantly deviate from the original statistical properties are more likely to be detected by the model.\nNarrow-guard feature perturbation. In tabular data, each feature typically exhibits a unique distribution. When perturbations are applied across features, the impact is more pronounced on features with narrower distributions compared to those with broader distributions, as features with narrower distributions are more sensitive to changes. Hence, to generate imperceptible adversarial attacks, it is important to apply perturbations that avoid altering features with narrow distribution or at least minimise their impact.\nPreservation of feature semantics. In tabular data, each feature typically has clearly defined semantics and valid practical value ranges, such as gender and age. However, adversarial attacks may introduce perturbations that alter feature semantics (e.g., changing gender from female to male) or extend feature values beyond valid practical value ranges (e.g., changing age from 20 to 120). Hence, ensuring imperceptibility in adversarial attacks on tabular data requires preserving feature semantics in alignment with its domain knowledge.\nPreservation of feature interdependencies. Feature interdependency in a tabular dataset refers to the relationships and correlations between different attributes or variables. Recognising feature inter-dependency is important because changes to one feature can affect the validity or interpretation of related features. For example, an individual's age and their age group (such as 'young adult' or 'senior') are inter-dependent, and changes to the age feature should correspond to changes in the age group feature to maintain consistency. Hence, ensuring imperceptibility in adversarial attacks on tabular data requires preserving feature inter-dependencies to maintain data integrity."}, {"title": "3.1. Establishing Criteria for Imperceptibility", "content": null}, {"title": "3.2. Properties of Imperceptibility", "content": "Based on the criteria established above, we propose a set of properties for assessing the imperceptibility of adversarial attacks. First of all, we provide a definition of adversarial attacks."}, {"title": "Definition 1 (Adversarial Attack).", "content": "Consider a dataset where each input data point a vector $x \\in X$ belongs to a class with label $y \\in Y$. Let $f(.)$ denote a machine learning classifier, an adversarial example $x^{adv}$ generated by an adversarial attack is a perturbed input similar to $x$ and misclassifies the label of $x$.\n$x^{adv} = x + \\delta$ subject to $f(x^{adv}) \\neq y$\nwhere $\\delta$ denotes input perturbation."}, {"title": "3.2.1. Proximity", "content": "Given the criterion of minimisation of feature perturbation, a good adversarial example should introduce minimal changes, which can be quantified by maintaining the smallest possible distance from the original feature vector. We use the term 'proximity' to refer to relevant distance metrics. As is the case with most of the existing adversarial attack algorithms (Carlini and Wagner, 2017; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016), we employ $l_p$ norm to measure the perturbation distance.\nProximity Metrics. $l_p$ norm is a distance metric for measuring the magnitude of a vector in n-dimensional space. Three candidatures of $l_p$ norm can be used as proximity metrics for measuring the distance between $x$ and $x^{adv}$ in grid-like path distance ($l_1$ distance), straight-line distance ($l_2$ distance) and maximum feature difference ($l_{\\infty}$ distance).\n$l_p(x^{adv}, x) = ||x^{adv} - x||_p =  \\begin{cases} (\\sum_{i=1}^{n}(x_i^{adv} - x_i)^p)^{1/p}, & p \\in {1,2} \\\\ sup_i|x_i^{adv} - x_i|, & p \\rightarrow \\infty  \\end{cases}$"}, {"title": "3.2.2. Sparsity", "content": "Given the criterion of minimisation of feature perturbation, this property seeks to identify the minimum feature set required to create an adversarial example. Tabular data, unlike images, is relatively low-dimensional, and as such, the number of altered features in a perturbation would have a more obvious effect on its imperceptibility (Borisov et al., 2021). Therefore, an ideal adversarial example should change the model's prediction by altering as few features as possible.\nSparsity Metric. Straightforwardly, sparsity measures the number of altered features in an adversarial example $x^{adv}$ compared to the original input vector $x$.\n$Spa(x^{adv}, x) = l_0(x^{adv}, x) = \\sum_{i=1}^{n} 1(x_i^{adv} - x_i)$"}, {"title": "3.2.3. Deviation", "content": "Existing studies (Lee et al., 2018; Nguyen et al., 2015) suggest that adversarial attacks are a special example of out-of-distribution being created with the intention to fool a model. Whilst adversarial examples are not deemed to be representative of the actual distribution of predictive models, a perturbed input should be as similar as possible to the majority of original inputs when addressing imperceptibility, thus addressing the criterion of preservation of statistical data distribution.\nDeviation Metric. We propose to use Mahalanobis distance (MD) (McLachlan, 1999) to measure the deviation between an adversarial perturbation and the distribution of the original data input variation. MD is a multi-dimensional generalisation of $l_2$ norm. This distance metric is commonly used for identifying outliers in multi-dimensional data, as well as for detecting correlations among different features. Given an input vector x, a perturbed vector $x^{adv}$ and the covariance matrix V.\n$MD(x^{adv}, x) = \\sqrt{(x^{adv} - x)V^{-1}(x^{adv} - x)^T}$"}, {"title": "3.2.4. Sensitivity", "content": "Luo et al. (2018) propose the notion of perturbation sensitivity for images by calculating the inverse of the standard deviation of the pixel region with the attack perturbation, and add perturbation sensitivity as a regularisation to the attack algorithm to generate imperceptible images. In our work, we adapt the concept of perturbation sensitivity as a metric to measure the degree to which the features with narrow distribution in tabular data are altered, addressing the criterion of narrow-guard feature perturbation.\nSensitivity Metric. Perturbation sensitivity is a normalised $l_1$ distance between x and $x^{adv}$ by the inverse of the standard deviation of all numerical features within the input dataset X. It is represented by:\n$SDV(x_i) = \\sqrt{\\frac{\\sum_{j=1}^{m}(x_{i,j} - \\bar{x}_i)^2}{m}}$\n$SEN(x, x^{adv}) = \\frac{\\sum_{i=1}^{n}|x_i^{adv} - x_i|}{\\sum_{i=1}^{n} SDV(x_i)}$\nwhere n is the number of numerical features, m is the number of all input vectors, and $\\bar{x}_i$ represents the average of the ith features within all datapoints."}, {"title": "3.2.5. Immutability", "content": "Given the criterion of preservation of feature semantics, when crafting an adversarial example, it should avoid perturbations that could introduce biases or ethical concerns. For instance, changing a critical feature like gender, age or ethnic group to influence loan approval would exhibit inherent bias and therefore should not be allowed. Hence, it is important to recognise immutable features in tabular data.\nImmutable features are fixed attributes within a dataset that are either inherently unchangeable or should not be altered due to ethical or practical considerations. Examples include personal identifiers, demographic information, and genetic data, which provide a stable foundation for analyses and ensure fairness by preventing biased alterations. Preserving the immutability of these features is crucial for maintaining data integrity and ethical compliance. Accordingly, we propose immutability as a qualitative property to evaluate whether an attack algorithm can generate adversarial examples that preserve immutable features in tabular data, which contributes to addressing the criterion of preservation of feature semantics.\nSome recent works on generating imperceptible adversarial attack for tabular data (Mathov et al., 2022) attempt to introduce constraints or masks to prevent the modification of immutable features by attack algorithms. Extracting these constraints requires a thorough understanding of the relevant domain knowledge and a clear grasp of the specific context of the tabular dataset. However, not all datasets provide the necessary level of detailed context. To this end, using case-based examples can be an effective way to analyse the immutability property in our approach."}, {"title": "3.2.6. Feasibility", "content": "Unlike image data, which always have the same fixed value range for pixels (commonly from 0 to 255), tabular data is heterogeneous and often consists of different feature types (e.g., categorical vs. numerical) and varying value ranges. When addressing the criterion of preservation of feature semantics, adversarial attacks should also prevent the introduction of perturbations that extend feature values beyond feasible practical ranges.\nWe employ feasibility as a qualitative property to assess whether a perturbed feature value aligns with semantic correctness. It often requires to leverage domain knowledge, if available, or common or practical knowledge relevant to the tabular dataset for checking the practical validity of a perturbed feature value. In existing research, feasibility is introduced through the identification of feasible counterfactual examples (Poyiadzi et al., 2020) and adversarial examples (Chernikova and Oprea, 2022).\nSimilar to the analysis of immutability, feasibility can also be evaluated using case-based examples in our approach. By examining these examples, we can determine if the perturbed feature values maintain their semantic integrity and remain within acceptable practical ranges for a given adversarial attack algorithm."}, {"title": "3.2.7. Feature Interdependency", "content": "Unlike image data, where pixel values are spatially arranged and often interpreted in a relatively straightforward manner by deep learning models, tabular data involves features that may have non-linear and context-specific interactions. Adversarial attacks on tabular data should navigate these intricate dependencies to produce meaningful and realistic perturbations. For instance, modifying a feature like 'income' in a financial dataset must be consistent with changes in related features such as 'age' and 'education level' to maintain data integrity and realism. However, simply altering 'income' independently of other correlated features could produce anomalies that can be easily detected.\nHence, we propose feature interdependency as a qualitative property to measures how well adversarial attack algorithms handle the intricate relationships and dependencies between features within a tabular dataset, thus addressing the criterion of preservation of feature interdependencies.\nWhile feature interdependency may be handled using methods such as feature selection or by imposing constraints that ensure synchronised changes among correlated features, these approaches require careful consideration of the underlying data structure, significant effort in managing data complexity, and a deep understanding of the domain-specific relationships between features. Our objective of evaluation is to leverage intrinsic properties of the data to assess whether adversarial attacks incorporate mechanisms to effectively manage interactions between features or attributes in a tabular dataset."}, {"title": "4. Evaluation", "content": "We conduct a series of experiments to evaluate the imperceptibility of adversarial attack algorithms on tabular datasets, using the properties of imperceptibility proposed in the previous section. Our experiments follow the typical machine learning pipeline, comprising data preparation, model training, and model testing. We generate adversarial examples using adversarial attack algorithms and then assess these algorithms through the following experiments:\n\u2022 Experiment 1: Evaluating the effectiveness of applied adversarial attack algorithms, specifically how successful these algorithms are in misclassifying a trained machine learning model.\n\u2022 Experiment 2: Evaluating the imperceptibility of applied adversarial attack algorithms using quantitative properties, such as sparsity, proximity, deviation, and sensitivity.\n\u2022 Experiment 3: Analysing case-based examples to assess the qualitative properties, such as immutability, feasibility, and feature interdependency, for the imperceptibility of applied adversarial attack algorithms.\nDatasets. We investigate adversarial attacks on five widely-adopted benchmark tabular datasets, namely Adult, German, Breast Cancer, and Diabetes, from UCI library, and COMPAS from ProPublica (Barenstein, 2019). Breast Cancer and Diabetes consist of only numerical features, while the other three contain both numerical and categorical features."}, {"title": "4.1. Design of Experiments", "content": null}, {"title": "4.2. Evaluation of Attack Effectiveness", "content": "To prepare for evaluating adversarial attacks, the initial phase involves training three distinct predictive models on five datasets. As shown in Table 2, subsequent testing confirms that each model achieves satisfactory accuracy, consistently exceeding the 70% threshold across all datasets. Comparing the models within the same dataset, they demonstrate similar performance levels. These findings validate the feasibility of applying adversarial attacks to these models, and it is viable to assess the effectiveness and imperceptibility of the same adversarial examples across the three models."}, {"title": "4.3. Evaluation of Attack Imperceptibility using Quantitative Properties", "content": "As mentioned before, an adversarial attack should achieve a success rate above a give threshold before its imperceptibility can be evaluated. We set a threshold of 30% for attack success rate. As shown in Table 2, applying the C&W attack to the LinearSVC model resulted in a success rate below the specified threshold across all five datasets. Therefore, it is excluded from the imperceptibility evaluation.\nSparsity. Sparsity metric quantifies the average number of modified features over all adversarial examples generated by an attack. Lower sparsity values suggest that the adversarial attacks possess better sparsity, implying that the modification made to the features are less perceptible. Since the number of features varies across datasets, sparsity can only be compared within the same dataset. The results suggest that all attack methods yield comparable sparsity results cross different models in each dataset.\nFurthermore, we investigate the potential relationship between sparsity value and feature types. In two numerical feature only datasets, we observe that the sparsity value is consistently close to the total number of numerical features in each corresponding dataset (refer to Figure 1). The results imply that five attack methods exhibit a tendency to perturb almost all features within two numerical datasets. When analysing mixed datasets, comparing the sparsity value solely with the number of numerical or categorical features is insufficient to identify the relationship between them. We then examine all original inputs and their corresponding adversarial examples in each combination of attack and models. We calculate the individual feature perturbation count for each combination, which record the total number of times being perturbed by each feature. The results are visualised as heatmaps in Figure 2, which indicate that all attacks tend to alter numerical features more frequently than categorical features in Adult, German and Compas datasets.\nProximity. Proximity measures the similarity of the perturbed example and original input. A smaller proximity metric value indicates that adversarial examples are closer to original inputs, which represents better imperceptibility. In our evaluation, we use $l_2$ and $l_{\\infty}$ norm as our two proximity metrics.\nTable 5 suggests, in most datasets, C&W attack method yields the lowest Proximity $l_2$ values on both LR and MLP models, with the exception of German dataset and LR model. On the other hand, DeepFool achieves the lowest $l_2$ values using the LinearSVC model in all five datasets.\nTable 6 suggests the proximity $l_{\\infty}$ values for FGSM and PGD attacks are consistently 0.3 across all models and datasets. This is expected since bounded attacks are constrained to generate perturbations within this budget. As for the other three unbounded attacks, the $l_{\\infty}$ norm values are influenced by different datasets, but no consistent patterns are observed. For instance, in Diabetes dataset, $l_{\\infty}$ values of DeepFool attack are less than 0.3, but for other datasets, they may exceed it. In addition, three implemented models also have a potential impact on $l_{\\infty}$ values, though results are inconsistent. For example, while DeepFool consistently achieves lower $l_{\\infty}$ values on LinearSVC compared to LR and MLP, LowProFool does not exhibit the same trend.\nDeviation. The deviation metric assesses how perturbed examples relate to the overall dataset distribution. When the deviation metric is notably high, adversarial attacks are more likely to produce examples outside of the dataset's distribution. Therefore, lower deviation values are preferable for crafting imperceptible attacks. In Table 7, the examination of the deviation metric reveals that C&W attack consistently results in the lowest deviation value, implying a higher degree of similarity to the original statistical distributions, especially in LR and MLP models. DeepFool, on the other hand, excels when applied to the LinearSVC model, achieving the best results in terms of deviation. The deviation level of LowProFool is lower than that of C&W and DeepFool attacks, but higher than two bounded attacks. In contrast, two bounded attacks, FGSM and PGD, consistently yield higher deviation values, suggesting that they tend to produce more outliers.\nSensitivity. Sensitivity quantifies how much narrow distribution features are altered by attack algorithms. Lower sensitivity values are preferable. No distinguishable patterns in sensitivity results have been observed crossing different attacks (see Table 8). In most combinations of models and datasets, C&W attack demonstrates relatively low values in sensitivity metric. DeepFool, LowProFool, FGSM, and PGD attacks consistently result in significantly higher sensitivity values. This indicates that these four attacks tend to introduce more significant perturbations on narrow distribution features than C&W."}, {"title": "4.4. Analysis of Imperceptibility using Qualitative Properties", "content": "In Section 3, we define three qualitative properties of attack imperceptibility, which are Immutability, feasibility and feature interdependency, and they often rely on domain-specific knowledge about the datasets. We analyse these qualitative properties using case-based examples.\nImmutability. We analyse immutability across three mixed datasets (Adult, COMPAS, and German). Using common knowledge, we can identify examples of immutable features in each of these datasets as shown below:\n\u2022 Adult: Sex, Race and Marital Status\n\u2022 COMPAS: Sex and Race\n\u2022 German: Personal Status/Sex and Foreign Worker Status\nWe examine each adversarial example from these datasets to check whether the corresponding feature values have been modified. We can therefore assess whether the identified immutable features are susceptible to manipulation. We observe that DeepFool attempts to perturb immutable features in both LR and MLP models for all three dataset, clearly deviating from the immutability requirement. For instance, in the Adult dataset, DeepFool generates a total of 6,513 adversarial examples with the MLP model. In these examples, the feature Race is modified in 648 instances, Sex in 345 instances, and Marital Status in 1,096 instances. Conversely, C&W, FGSM and PGD do not alter these immutable features. However, this does not inherently imply that three attacks adhere to the immutability requirement. Theoretically, these attacks do not take into account the perturbation of immutable features in their algorithm designs. Hence, they may not have capability to change these immutable features.\nHere, we use the combination of COMPAS dataset and LR model as example to explain how Deepfool perturbs immutable features. COMPAS dataset is well-known tabular dataset with racial bias for scoring criminal defendant's likelihood of recidivism (Barenstein, 2019), in which both feature race and feature sex are the immutable features. In our experiment, we classify each case as \"Medium-Low\" or \"High\" risk. We generate 1408 adversarial examples for DeepFool attack with LR model, and filtered cases with perturbation on immutable features. Two example cases (#285 and #501) are originally predicted as \"Medium-Low\". DeepFool attack successfully misclassified these cases' predictions, but notably, it also changed the individuals' ethnicity from Caucasian and Hispanic to Native American and African-American.\nTo understand the perturbations on immutable features, we analysed the feature weights (coefficients) of the LR model, notably, different values of feature race lead to different prediction contributions. As a result, DeepFool successfully perturbed the immutable feature race from categories with either positive or low negative impact (e.g., Caucasian and Hispanic) to categories with high negative impact (e.g., Native American and African-American).\nFeasibility. Property feasibility assesses if adversarial attack algorithms can perturb feature values within a reasonable range. However, since there's a shortage of background knowledge for all datasets, it's impractical to identify all feasible feature ranges for each dataset. To assess the feasibility of adversarial attacks, we turn to the diabetes dataset, which exclusively comprises numerical features used to predict diabetes based on diagnostic measurements. Even though the original dataset includes descriptions of each variable (Smith et al., 1988), it does not specify feasible feature ranges. However, drawing on common medical knowledge, we have identified suitable ranges for certain features.\nFrom this dataset, we've chosen two cases, labeled as #19 and #57, from the generated adversarial examples with LR models. We compare the original feature values with the adversarial examples generated by DeepFool, C&W attack, and LowProFool attack."}, {"title": "5. Discussion and Limitations", "content": "We conduct experiments encompassing all four quantitative properties related to imperceptibility. All five adversarial attacks with different performance have similar sparsity levels, indicating that there is no correlation between attack success and sparsity levels. However, proximity, deviation, and sensitivity appear to display inconsistent behaviours in the different levels of attack success rate. To analyse the potential relationship between these three metrics and attack success rate, all adversarial example are divided into two groups: successful attack examples and unsuccessful attack examples. Box plots are then utilised to compare the distribution of each metric across these two groups of adversarial examples.\nAs depicted, the distributions of the two groups of adversarial examples reveal that successful attack instances typically exhibit larger values in $l_2$ distance compared to unsuccessful attack examples, particularly evident in unbounded attacks. However, in bounded attacks, where there's a constrained attack budget in place limiting the maximum proximity, the pattern is less distinct. The similar patterns are also identified in deviation and sensitivity. The distributions of deviation and sensitivity for Deepfool, C&W attack and LowProFool shift from low to high when changing the attack success from false to true. For bounded attacks, although the limited attack budget still has an impact, trade-off patterns are observed in some experimental settings.\nBy incorporating the comparative analysis between attack success rate and three imperceptibility metrics, we identify there is a trade-off between effectiveness and imperceptibility for adversarial attacks on tabular data. It emphasises the intricate balance that must be maintained to develop both effective and imperceptible adversarial attacks."}, {"title": "5.1. A trade-off between imperceptibility and performance on adversarial attacks", "content": null}, {"title": "5.2. No one silver bullet for attacking tabular data, but unbounded attacks demonstrate greater promise", "content": "According to our evaluation on the effectiveness and imperceptibility of five adversarial attack algorithms, none of them can simultaneously achieve the highest attack success rate and the highest level of imperceptibility. Despite their high success rates, bounded attacks like FGSM and PGD often struggle to maintain high levels of imperceptibility, especially when considering metrics such as deviation and sensitivity. Additionally, determining an appropriate attack budget poses a significant challenge for bounded attacks on tabular data, as it directly impacts the trade-off between attack success and imperceptibility. In theory, by systematically investigating different budget values and evaluating their impact on both success rate and imperceptibility metrics, researchers can identify the sweet spot that maximises attack effectiveness while minimising perceptibility. However, conducting such an exhaustive search for the best attack budgets for each dataset is impractical due to resource constraints and the vast diversity of datasets.\\{\n        \"On the other hand, unbounded attacks demonstrate more promise in crafting imperceptible adversarial examples. C&W and DeepFool outperform FGSM and PGD in the evaluation of proximity, deviation and sensitivity. Nevertheless, further improvements are necessary for unbounded attacks to enhance their performance in attack success rate of attacking tabular dataset, especially for C&W attacks.\nMoreover, none of these algorithms take into account the requirements of feasibility, immutability, and feature interdependency in tabular data. Although auxiliary methods such as feature masks can be employed during the implementation of the attack algorithms, the design of these algorithms does not inherently consider these factors, making them theoretically unsuitable for crafting imperceptible adversarial examples for tabular data.\""}, {"title": "5.3. Considering sparsity in adversarial attack algorithm optimisation for tabular data", "content": "Our evaluation indicates that all attack methods target the alteration of all numerical features, akin to how image-based attacks perturb all pixels. But as discussed in Section 3, achieving imperceptibility in tabular data necessitates attacks to alter as few features as possible. Modifying fewer features is less likely to be noticeable to the human eye and can evade detection. Conversely, attacks that modify many features can significantly reduce the level of imperceptibility, thereby increasing the risk of detection. The $l_0$ norm is non-differentiable and, therefore, cannot serve as the sole optimisation metric (Croce and Hein, 2019). However, when incorporated sparsity as a part of attack algorithm optimisation, it could possibly govern the number of altered features."}, {"title": "5.4. Assumption on uniform feature importance", "content": "This research operates under the assumption that all features contribute equally to the predictive models, analogous to the notion that each pixel holds equal importance in images. However, in real-world tabular datasets, intricate inter-dependencies exist among various features. This observation suggests that the importance of different features is not uniform. Later in the research, there is a need for an extension into non-uniform adversarial attacks (Erdemir et al., 2021; Nandy et al., 2023). Addressing non-uniform adversarial challenges will contribute to a more comprehensive understanding of the robustness and generalisation capabilities of predictive models in practical applications."}, {"title": "5.5. Limitations", "content": "Side effects of one-hot encoding. Tabular data poses various challenges when encoding categorical variables, and the selection of an appropriate encoding method can have a substantial impact on model performance. The prevalent use of distance-based metrics in attack techniques for measuring perturbation size is a key consideration for selecting the one-hot encoding. While one-hot encoding is a common approach to handle categorical features, it concurrently amplifies the total number of features in a dataset. This expansion results from each unique category in a categorical feature being represented by a distinct binary feature, thereby introducing computational challenges, including the well-known curse of dimensionality (Borisov et al., 2021). This necessitates a careful consideration of how distance metrics are applied to both numerical and categorical features to ensure effective and meaningful perturbation measurements in the pursuit of imperceptibility. Exploring alternative distance metrics, such as Gower's distance (Gower, 1971) and other categorical feature similarity measures (Cost and Salzberg, 1993; Le and Ho, 2005), holds promise for advancing the design of attack methods tailored to tabular data.\nLimitations of parameter settings. The current experiment settings on evaluating the effectiveness and imperceptibility of adversarial attacks on tabular data faces several notable limitations. Primarily, the restricted size of the tabular datasets under investigation hinders the generalisability of findings to larger and more diverse datasets. Another limitation lies in the use of simplistic predictive models, predominantly simple neural networks, which may not fully capture the complexity of real-world tabular data scenarios. The research is further confined by a narrow spectrum of attack methods, which may omit vulnerabilities exploitable by alternative adversarial strategies. Additionally, the fixed attack settings, particularly the consistent setting of the bounded attack budget $\\epsilon$ to 0.3, may not accurately reflect the dynamic and variable nature of adversarial scenarios, potentially overlooking nuanced insights into the model's robustness. Varying the attack budget across the entire range from 0 to 1 could yield valuable insights into the imperceptibility of adversarial attacks on tabular data models. Overcoming these limitations is essential to enhance the reliability and applicability of the research findings in real-world settings."}, {"title": "6. Conclusion", "content": "In conclusion, this paper has made significant contributions in advancing our understanding of imperceptibility in adversarial machine learning on tabular data. The derivation of seven key imperceptibility properties, including proximity, sparsity, deviation, sensitivity, immutability, feasibility and feature inter-dependency, provides a comprehensive framework for characterising adversarial attacks in the context of tabular data. Furthermore, the practical insights derived from our empirical evaluation on these imperceptibility properties and metrics offer valuable guidance, such as the identified trade-offs between attack success and imperceptibility, the efficacy of optimisation-based attacks, and limitations in controlling sparsity, for developing robust and imperceptible adversarial attacks.\nWhile our investigation of adversarial attacks on tabular data has provided valuable insights, several limitations and directions for future research should be acknowledged. The use of one-hot encoding increases dataset dimensionality, prompting a need for careful consideration of distance metrics for both numerical and categorical features. Future research should expand to larger datasets, sophisticated models, and a wider array of attack strategies. Additionally, the absence of the ablation study signals a need for future investigations into the impact of feature removal on adversarial robustness and imperceptibility."}]}