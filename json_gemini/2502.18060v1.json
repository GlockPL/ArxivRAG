{"title": "Defining bias in AI-systems: Biased models are fair models", "authors": ["Chiara Lindloff", "Ingo Siegert"], "abstract": "The debate around bias in AI systems is central\nto discussions on algorithmic fairness. How-\never, the term \"bias\" often lacks a clear defini-\ntion, despite frequently being contrasted with\n\"fairness\" implying that an unbiased model\nis inherently fair. In this paper, we challenge\nthis assumption and argue that a precise con-\nceptualization of bias is necessary to effectively\naddress fairness concerns. Rather than viewing\nbias as inherently negative or unfair, we high-\nlight the importance of distinguishing between\nbias and discrimination. We further explore\nhow this shift in focus can foster a more con-\nstructive discourse within academic debates on\nfairness in AI systems.", "sections": [{"title": "1 Introduction", "content": "Bias mitigation is a key topic in current discus-\nsions surrounding AI systems (Gray et al., 2024).\nBut what exactly is bias? From a technical per-\nspective, bias describes input modulation in neural\nnetworks. However, beyond this technical defini-\ntion lies a socially relevant dimension that necessi-\ntates mitigation strategies. Bias is often understood\nas a systematic error in judgment, rooted in prej-\nudice, that perpetuates existing power structures\nsuch as racism or sexism\u2014making outcomes un-\nfair. Consequently, mitigation strategies, aimed at\npromoting fairness, typically focus on eliminating\nbias by equalizing treatment across groups (Bell,\n2015; Bender et al., 2021; Holdsworth, 2024; Kar-\ntal, 2022; Ntoutsi et al., 2020).\nRecent research highlights the complexity and\nimportance of bias mitigation. For example, Berke-\nley Haas School of Business provides practical\nstrategies for identifying and mitigating bias to pro-\nmote responsible and equitable use of AI (Smith\nand Rustagi, 2020). The National Institute of Stan-\ndards and Technology (NIST) emphasizes the need\nfor a socio-technical approach to address bias and\nhighlights the role of context in mitigation strate-\ngies (Schwartz et al., 2023). Additionally, a large-\nscale empirical study evaluates 17 different bias\nmitigation methods, providing insights into their\neffectiveness and impact on fairness (Chen et al.,\n2023). These examples illustrate the range of ap-\nproaches in current bias mitigation research-from\npractical frameworks to empirical evaluations.\nYet, what does it actually mean to say a model\nis biased? Despite its frequent juxtaposition with\n\"fairness\", the term remains diffuse and ambigu-\nously defined. This paper seeks to clarify what bias\ntruly is, why it requires mitigation, and how its\nconceptualization shapes the discourse on fairness\nin Al systems."}, {"title": "2 What 'Bias' Means in Neural Networks:\nA Look at Its History and Technical\nRole", "content": "The elementary building blocks of neural networks,\nartificial mechanical neurons, trace their origin\nto the founding fathers of the technology that\nnow builds our large language models (LLMs) \u2014\nthey may also be referred to as MCP neurons, or\nMcCulloch-Pitts-Neurons (McCulloch and Pitts,\n1943), which are simple binary neuron models that\nprocess inputs using a threshold value, account for\ninhibitory signals, and produce an output of either\n0 or 1 (Vijaychandra et al., 2019).\nThis concept was further developed by Frank\nRosenblatt, whose perceptron model,\npaved the way for modern machine learning al-\ngorithms by introducing adjustable weights and a\nlearning rule for updating these weights based on\nerrors (Rosenblatt, 1962). Although the perceptron\nrelied on a threshold for classification, the explicit\nconcept of a bias term to shift the decision bound-\nary was not yet introduced (Vijaychandra et al.,\n2019) Its adaptive capabilities laid the groundwork\nfor pretty much all modern algorithmic solutions,\nas its main goal is to make accurate classification.\nThe basic functionality allows for linearly separa-\nble classification according to whether or not the\nweighted inputs meet a certain threshold 0.\nIn 1960, Bernard Widrow built on Rosenblatt's\nperceptron and developed ADALINE (Widrow\net al., 1988), which is where the term 'bias' was\nfirst explicitly used to denote a constant input node\nthat adjusts the model's decision boundary indepen-\ndently of the input values,\nADALINE\n(Adaptive Linear Neuron) is a single-layer neural\nnetwork that uses a linear activation function and\napplies the Least Mean Squares (LMS) learning\nrule to minimize the error between predicted and\nactual outputs during training. The model was im-\nplemented in a physical device called Memistor,\nwhich simulated the learning process by adjust-\ning electrical conductance to represent changing\nweights.\nWhen multiple perceptrons are connected in lay-\ners, they form a multi-layer perceptron (MLP), ca-\npable of distinguishing non-linearly separable data.\nIn an MLP, each neuron is fully connected to all\nneurons of both the preceding and succeeding lay-\ners. Additionally, each neuron includes a \u201cbias\u201d\ninput, which provides a constant input to help shift\nthe activation function. The concept of \"bias\" in\nthis architectural context can be summarized as a\nconstant term added to the weighted sum of inputs\nbefore applying the activation function, allowing\nthe model to shift the decision boundary indepen-\ndently of the input values. This description reflects\nthe standard, widely accepted structure of modern\nmulti-layer perceptrons (MLPs) used in today's\nneural networks.\nIt is important to distinguish this architectural\nbias from the statistical concept of the bias-variance\ntradeoff (Belkin et al., 2019). The latter refers to\na model's tendency to either oversimplify patterns\n(high bias, underfitting) or overfit to noise (high\nvariance, overfitting). While both concepts relate\nto model performance, they address fundamentally\ndifferent aspects of learning.In modern deep learn-\ning architectures, such as those powering large lan-\nguage models (LLMs), bias terms remain a fun-\ndamental component. They ensure that networks\ncan learn complex representations by shifting acti-\nvation thresholds, improving convergence during\ntraining, and enhancing the model's ability to gen-\neralize across diverse inputs.\nEvidently, \"bias\u201d is a technical term in AI devel-\nopment, particularly in machine learning, where\nit refers to a constant input term that shifts the de-\ncision boundary of a model to aid learning and\nimprove convergence. This architectural defini-\ntion is distinct from the statistical concept in the\nbias-variance tradeoff and should not be confused\nwith societal concerns related to \"salient social cat-\negories\" (Bender et al., 2021) addressed in bias-\nmitigation strategies."}, {"title": "3 What 'Bias' Means in Everyday\nLanguage: Technical Term or Social\nConcept?", "content": "In current academic discourse, the term \"bias\",\nwhen used in discussions surrounding AI, is typ-\nically more associated with concepts like \"preju-\ndiced\" or \"bigoted\"; the term conjures associations\nwith discriminatory ideology such as racism, sex-\nism, or homophobia. Oftentimes, in discourse unre-\nlated to AI, this is what people are trying to describe\nwhen using the word bias. However, it is important\nto remember that the term can also be used to mean\nsomething more like \u201cpreference\". Especially in\neveryday social contexts, 'bias' can imply preju-\ndice, but it is often used more neutrally to express\na personal inclination, preference, or even strong\nliking (Newton and Ferenczi, 2024). If a loved"}, {"title": "4 Unbiased is not the same as fair", "content": "Defining bias and fairness as the two extremes a\nmodel might fall between is, ultimately, a false\ndichotomy. The juxtaposition suggests that if a\nmodel is fair, that means it's unbiased. Let us have\na look at this proposition for a moment and attempt\nto find an ex-negativo definition of what is meant\nby \"bias\" by firstly looking at what fairness means:\nFairness is about equitable treatment that\naccounts for different circumstances and\nneeds. While equality means providing\neveryone with the same resources or op-\nportunities, fairness considers individual\ncontexts and removes barriers that hinder\nparticipation or success.\n(The Oxford Review, 2024)\nBut what, then, makes a model fair? In order to\nanswer this question, we need to elucidate what is\nmeant when asking for such a model. Consider, for\nexample, sampling bias:\nTraining data [that] is not representative\nof the population it serves [leads] to poor\nperformance and biased predictions for\ncertain groups.\n(Ferrara, 2023)\nA common example is a facial recognition algo-\nrithm trained predominantly on white individuals,\nresulting in significantly lower accuracy for people\nof color-a clear instance of sampling bias causing\nharmful outcomes (Ferrara, 2023). We can take this\nto imply that an unbiased model would perform the\nsame on people of all ethnic backgrounds, i.e., treat\neveryone the same.\nLet us examine a different example for sampling\nbias: Consider a model built to make predictions\nabout children's growth and development in order\nto design a new curriculum. Suppose you could\nsurvey every child in the world to acquire train-\ning data. Hypothetically, this must lead to an un-\nbiased model. Using data from every child will\nmake sure that no one group is over- or underrepre-\nsented, i.e., the data is representative. Any number\nof factors like biological, genetic and environmen-\ntal conditions can affect developmental processes.\nSo the hypothetical training data will use data that\naccurately reflects these differences (and their per-\ncentages across the population) to create the best\ncurriculum.\nNow consider these three children:\n1. An able-bodied intersex child born to farmers\nin rural Asia.\n2. A physically disabled boy born to university-\neducated parents in suburban Africa.\n3. A selectively mute girl born to a blue-collar\nsingle parent in urban Europe.\nThese children will lead very different lives with\nvery different influences on their development and\nunique challenges.\nAlthough the model is unbiased, as it treats every\nchild equally, it applies a one-size-fits-all standard\nwithout considering individual needs. This raises\nthe question: Is treating everyone equally the same\nas treating everyone fairly? Is it fair to expect these\nchildren to learn the same things at the same time\nand with the same speed?\nIf a real-world application were possible, we\nassume that most people would likely consider this\nmodel biased. Specifically against children with\ne.g., disabilities or toward an imaginary norm of\nthe \"ideal child\u201d. But why? There is no sampling\nbias, no algorithmic bias, no representation bias\nand no measurement bias as defined in (Ferrara,\n2023). We think it's because the problem here is\nnot bias. The real problem here is discrimination."}, {"title": "5 discriminating between vs\nDiscrimination", "content": "Let us, for a moment, return to the beginnings\nof AI, specifically to the MCPs and Perceptrons\nmentioned at the start of the paper. Their chief\nobjective is to classify by distinguishing between\ncategories based on measurable criteria. This tech-\nnical form of discrimination-meaning differenti-\nation-is value-neutral. No ethical concerns arise\nbecause the criteria are relevant to the task. Thus,\nthis process does not lead to accusations of discrim-\nination.\nBut when AI is said to discriminate (which is,\nas shown above, what people actually try to ex-\npress when saying it is biased), discussion around\nmitigation strategies based on obvious sentiment\nand arguments against discriminatory ideology that\nemphasize equal treatment for people irrespective\nof categories like ethnic background is brought\nup. Why is that? Simple neural networks like the\nones mentioned previously aim to linearly sepa-\nrate data. If two things are linearly separable, that\nmeans it is possible to create a hyperplane in a two-\ndimensional vector space that has one category of\nthing on one side and another category of thing on\nthe other. What parameters decide the distribution\non either side? There are two dimensions (the axes)"}]}