{"title": "PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation", "authors": ["Qixuan Li", "Chao Wang", "Zongjin He", "Yan Peng"], "abstract": "Text-to-3D asset generation has achieved significant optimization under the supervision of 2D diffusion priors. However, when dealing with compositional scenes, existing methods encounter several challenges: 1). failure to ensure that composite scene layouts comply with physical laws; 2). difficulty in accurately capturing the assets and relationships described in complex scene descriptions; 3). limited autonomous asset generation capabilities among layout approaches leveraging large language models (LLMs). To avoid these compromises, we propose a novel framework for compositional scene generation, PhiP-G, which seamlessly integrates generation techniques with layout guidance based on a world model. Leveraging LLM-based agents, PhiP-G analyzes the complex scene description to generate a scene graph, and integrating a multimodal 2D generation agent and a 3D Gaussian generation method for targeted assets creation. For the stage of layout, PhiP-G employs a physical pool with adhesion capabilities and a visual supervision agent, forming a world model for layout prediction and planning. Extensive experiments demonstrate that PhiP-G significantly enhances the generation quality and physical rationality of the compositional scenes. Notably, PhiP-G attains state-of-the-art (SOTA) performance in CLIP scores, achieves parity with the leading methods in generation quality as measured by the T\u00b3Bench, and improves efficiency by 24x.", "sections": [{"title": "1. Introduction", "content": "Text-to-3D models (Zhu et al., 2024; Poole et al., 2023; Lin et al., 2023; Liu et al., 2024) are systems that convert natural language descriptions into 3D assets by integrating techniques like 2D-to-3D conversion, neural implicit representation, and 3D mesh generation, leveraging deep learning for cross-modal and diverse 3D content creation. There is a growing demand for high-quality 3D assets, particularly for training scenarios in autonomous driving (Mittal, 2020) and robotic navigation (Bermejo et al., 2021). In contrast, 3D content creation, especially for complex scenes, often requires substantial time and effort from domain experts, resulting in constrained production capacity. The advent of text-to-3D technologies offers a novel solution to this challenge, empowering non-expert users to create 3D assets through natural language. However, existing 3D generation methods typically prioritize improving the quality of individual asset, while paying insufficient attention to tasks like compositional scene generation.\nCompositional scene generation refers to the process of generating a finite number of 3D assets based on scene descriptions and arranging them in a physically plausible layout. Current mainstream text-to-3D models generally lack an understanding of complex semantics and guidance for the layout of scene-level 3D assets. As a result, generating the compositional 3D scene involving multiple objects frequently gives rise to disorganized layouts and inadequate physical consistency. (i.e., issue 1). Concurrently, a recent trend (Bai et al., 2024; Cohen-Bar et al., 2023; Po & Wetzstein, 2023) involves manually designed layouts to impose geometric constraints, capturing relationships among multiple objects in the scene, and using implicit neural radiance fields (NeRF) (Mildenhall et al., 2020) for generation. However, this approach struggles to meet all constraints in the layout, leading to blurry textures and geometric distortions (i.e., issue 2). In contrast, some recent 3D scene generation methods employ LLMs as agents to analyze textual descriptions and leverage the reasoning capabilities of LLMs for layout guidance (Kumaran et al., 2023; Yang et al., 2024). Nevertheless, these models often focus only on 3D asset layouts, requiring assets to be sourced from existing 3D assert libraries. Such limitations inherently constrains their 3D asset generation capabilities, significantly reducing the flexibility of the generation model (i.e., issue 3). From the above issues, 3D compositional scene generation emerges as a task that extends beyond merely stacking assets. This task requires models with exceptional single-asset generation capabilities, advanced semantic understanding, and physics-"}, {"title": "2. Related Work", "content": "3DGS for text-to-scene generation. Traditional text-to-3D methods primarily rely on generative approaches based on adversarial networks or variational autoencoders (Zhao et al., 2022; Ko et al., 2023; Ferreira et al., 2022; Kosiorek et al., 2021; Eguchi et al., 2022; Petrovich et al., 2021), utilizing 2D images or textual descriptions to infer and generate complex 3D shapes, which are often computationally expensive and slow to produce. The recently popular 3DGS (Kerbl et al., 2023) demonstrates a method for representing 3D spaces by optimizing 3D Gaussian spheres, enabling fast rendering and making it popular in 3D scene reconstruction. (Chung et al., 2023) generates 3D scenes through image inpainting with stable diffusion (Rombach et al., 2021), using reference images or text to expand different viewpoints. GALA3D (Zhou et al., 2024) utilizes object-level text-to-3D modeling, and MVDream (Shi et al., 2023) generates"}, {"title": "3. Method", "content": "As illustrated in Figure 2, our overall framework consists of two main components: 1). 3D assets generation, and 2). physical 3D scene layout. For the first component, a keyword extraction agent infers the scene graph and relationships from a complex scene description T, followed by the generation of 3D assets using a 2D image generation agent and a 3D Gaussian generation model (Section 3.2). For the second component, the framework incorporates a designed physical pool with a relationship classification agent and an iterative fine-tuning visual supervision agent as part of the world model, enabling a two-stage compositional scene layout (Section 3.3)."}, {"title": "3.1. Overview", "content": "As illustrated in Figure 2, our overall framework consists of two main components: 1). 3D assets generation, and 2). physical 3D scene layout. For the first component, a keyword extraction agent infers the scene graph and relationships from a complex scene description T, followed by the generation of 3D assets using a 2D image generation agent and a 3D Gaussian generation model (Section 3.2). For the second component, the framework incorporates a designed physical pool with a relationship classification agent and an iterative fine-tuning visual supervision agent as part of the world model, enabling a two-stage compositional scene layout (Section 3.3)."}, {"title": "3.2. Generation of High-Quality 3D Scene Assets", "content": "This section mainly introduces the process of generating 3D scene objects from scene descriptions. First, we explain how the scene descriptions are processed. Then, we describe the use of the treated text in generating 3D scene assets.\nScene description processing. When our brain receives a description of a complex scene, we instinctively think about the objects in the scene and their sizes and relationships. Analogously, when constructing a 3D scene from a"}, {"title": "3.3. Two-Stage Compositional 3D Scene Layout", "content": "This section primarily describes the two-stage compositional scene layout process based on the world model. In the first stage, we implement an initial scene layout through the design of simple and effective physical pool. In the second stage, we design a scene layout supervision agent"}, {"title": "4. Experiments", "content": "Quantitative analysis uses the CLIP metric. In Table 1, we use the text-image similarity metric CLIP for qualitative evaluation, analyzing the consistency and quality between text descriptions and 3D scenes. We also compare the performance of our method with current SOTA methods on the task of text-to-3D complex scene construction. To improve the reliability of the benchmark, we consider various 3D reconstruction and representation techniques when selecting advanced methods, including NeRF-driven approaches, voxel-based representation methods, and techniques based on 3D geometric structures. Our approach integrates agents based on the world model while utilizing 3D GS for generation. For a comprehensive demonstration of each model's generation capabilities across diverse 3D scenes, we test scene prompts containing 3, 4, 6, and 8 assets. Ultimately, ours achieves higher CLIP scores than other generation models in the text-to-3D compositional scene generation and layout task, demonstrating better semantic consistency between text and scenes.\nQuantitative analysis uses the T\u00b3Bench metric. To further evaluate the generation quality and semantic consistency of ours, we used the evaluation metrics provided by T\u00b3Bench (He et al., 2023), as shown in Table 2. The results of other methods in the table are derived from T\u00b3Bench, a comprehensive text-to-3D benchmark specifically designed for evaluating the qulaity of 3D generation. Its quality metrics combine multi-view text-image scoring and regional convo-"}, {"title": "4.1. Quantitative Comparison", "content": "Quantitative analysis uses the CLIP metric. In Table 1, we use the text-image similarity metric CLIP for qualitative evaluation, analyzing the consistency and quality between text descriptions and 3D scenes. We also compare the performance of our method with current SOTA methods on the task of text-to-3D complex scene construction. To improve the reliability of the benchmark, we consider various 3D reconstruction and representation techniques when selecting advanced methods, including NeRF-driven approaches, voxel-based representation methods, and techniques based on 3D geometric structures. Our approach integrates agents based on the world model while utilizing 3D GS for generation. For a comprehensive demonstration of each model's generation capabilities across diverse 3D scenes, we test scene prompts containing 3, 4, 6, and 8 assets. Ultimately, ours achieves higher CLIP scores than other generation models in the text-to-3D compositional scene generation and layout task, demonstrating better semantic consistency between text and scenes.\nQuantitative analysis uses the T\u00b3Bench metric. To further evaluate the generation quality and semantic consistency of ours, we used the evaluation metrics provided by T\u00b3Bench (He et al., 2023), as shown in Table 2. The results of other methods in the table are derived from T\u00b3Bench, a comprehensive text-to-3D benchmark specifically designed for evaluating the qulaity of 3D generation. Its quality metrics combine multi-view text-image scoring and regional convo-"}, {"title": "5. Conclusion", "content": "In this paper, we present PhiP-G, a novel text-to-3D compositional scene generation framework that combines advanced 3D Gaussian generation techniques with world model-based layout guidance. The framework excels in generating 3D scenes with strong textual consistency and physical coherence. Extensive experiments validate that PhiP-G outperforms existing methods in compositional scene generation, demonstrating superior semantic understanding and multi-object layout capabilities. Our future work will focus on: 1). Incorporating higher-quality 3D generation models into the framework as the 3D generation module; 2). Enhancing the integration of world models for more advanced complex scene generation."}, {"title": "A.1. Implementation details", "content": "We select DreamGaussian as the 3D Gaussian generation model, where the guidance scale is set to 100. The learning rates for opacity, position, and color are set to 5 \u00d7 10\u22122, 1.6 \u00d7 10-4, and 5 \u00d7 10-3, respectively. For texture and mesh extraction of 3D assets, we set the geometry learning rate to 1 \u00d7 10-4 and the texture learning rate to 2 \u00d7 10-1, which are used for geometric adjustments of the mesh and enhancing texture details. Regarding the agent design, we use GPT-4 as the core for the natural language analysis and reasoning agents AG-extractor and AG-classifier. For the text-to-2D generation agent AG-generater, DALL\u00b7E 3 is used as the core, while for the visual supervision agent AG-supervisor, we select GPT-40 (OpenAI, 2023) for its strong visual understanding capabilities. For capturing scene images used in visual supervision, cameras are placed along each axis, facing the origin, with the distance set to 8. Since our work primarily relies on the designed physical pool and various powerful agents for scene layout, training is not required. High-quality compositional scene generation can be completed in approximately 10 minutes on a 12G NVIDIA 4080 Laptop."}, {"title": "A.2. Physical Relationship Database within the Physical Pool", "content": "\u2022 Basic relationships: on, under, left, right, front, behind.\n\u2022 Vague relationships: far, near.\n\u2022 Alignment relationship: center-aligned.\n\u2022 Leaning relationship: leaning-on.\n\u2022 Rotation relationships: facing,rotation.\n\u2022 Special relationships: duplicate_x_alignment, duplicate_y_alignment,duplicate_facing.\nWe categorize object relationships into basic, vague, alignment, leaning, and rotational relationships, structuring asset positions in scene descriptions accordingly. To further aid the LLM in understanding overall scene adjustment requirements, we define three special relationships: duplicate_x_alignment refers to \"copy and align the entire scene along the x-axis\u201d, duplicate_y_alignment refers to \"copy and align the entire scene along the y-axis\", and duplicate_facing refers to \"copy and face each other\"."}, {"title": "A.3. Primary Agent Prompt", "content": "Here, we provide the example of agent prompt engineering, as shown in Figure 5 and Figure 6. We outline the core components of the critical agent prompt engineering process. For AG-extractor, the CoT design encompasses object extraction and image generation, size classification, relationship extraction, special inference, and output example. For AG-supervisor, along with input data configuration and evaluation metrics, it integrates reverse reasoning prompts to enhance its capabilities."}, {"title": "A.4. Agent Reasoning Demonstration", "content": "The Figure 7 presents examples of agent reasoning, illustrating the AG-extractor reasoning process for scene graph generation and the AG-supervisor process for scene evaluation and guidance. Leveraging the reflection mechanism mentioned earlier, uncertainty in agent reasoning and generation is significantly reduced, ensuring stable and consistent execution."}, {"title": "A.5. Ground Material Generation", "content": "We simulate three types of realistic ground: grass, sandy, and wood ground, using Blender's asset construction capabilities, as illustrated in Figure 8. First, we determine the ground's position and materials based on the generated scene. Then, a ground plane is created, followed by the generation of procedural texture nodes, with adjustments to texture density, roughness, and detail levels. Various nodes (such as texture coordinates, mapping, noise, and color gradients) are connected in sequence to produce the desired effect and apply it to the ground. For uneven ground, such as grass, a particle system is added, with particles configured in hair mode to generate grass. Randomness and clustering effects are introduced to control the clumping and roughness of the grass."}]}