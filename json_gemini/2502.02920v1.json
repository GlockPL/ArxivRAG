{"title": "Adaptive Budget Optimization for Multichannel Advertising Using Combinatorial Bandits", "authors": ["Briti Gangopadhyay", "Alberto Silvio Chiappa", "Zhao Wang", "Shingo Takamatsu"], "abstract": "Effective budget allocation is crucial for optimizing the performance of digital advertising campaigns. However, the development of practical budget allocation algorithms remain limited, primarily due to the lack of public datasets and comprehensive simulation environments capable of verifying the intricacies of real-world advertising. While multi-armed bandit (MAB) algorithms have been extensively studied, their efficacy diminishes in non-stationary environments where quick adaptation to changing market dynamics is essential. In this paper, we advance the field of budget allocation in digital advertising by introducing three key contributions. First, we develop a simulation environment designed to mimic multichannel advertising campaigns over extended time horizons, incorporating logged real-world data. Second, we propose an enhanced combinatorial bandit budget allocation strategy that leverages a saturating mean function and a targeted exploration mechanism with change-point detection. This approach dynamically adapts to changing market conditions, improving allocation efficiency by filtering target regions based on domain knowledge. Finally, we present both theoretical analysis and empirical results, demonstrating that our method consistently outperforms baseline strategies, achieving higher rewards and lower regret across multiple real-world campaigns.", "sections": [{"title": "1 INTRODUCTION", "content": "Digital advertising is a fast growing area of research, with the global market size approaching $700 billion in 2024 and projected to surpass $830 billion by 2026 [3]. In 2023, the average internet user spent around 6.5 hours daily engaging with content largely driven by advertisement. In the United States, digital advertising expenditure reached $189 billion in 2021 [29], showing a significant 35% year-over-year growth, driven in part by the COVID-19 pandemic. Despite economic challenges such as high inflation and rising interest rates, digital advertising continued to expand, reaching $225 billion in 2023 [30].\nAs the advertising sector continues to evolve, the number of sub-campaigns within a portfolio grows [16], driven by diversification across various formats (e.g., Search, Display, Video) and platforms (e.g., Google, Meta). To ensure profitability from delivering a diverse portfolio of campaigns, it is crucial to manage digital marketing budgets effectively (Fig 1). This resource allocation problem has attracted significant interest from the machine learning community [31] as logged data can be procured from different business campaigns. The presence of rich features in this data further fuels the development of automated decision-making systems, as learning algorithms are often better equipped to interpret multidimensional tabular data than human intuition.\nResearch on budget allocation algorithms remain limited, despite its importance for advertisers. A well-planned spending strategy is crucial, as campaigns with inadequate budgets may struggle to reach high-quality traffic. Effective budget allocation can significantly boost Return on Ad Spend (ROAS) by ensuring that ads are displayed where users are most likely to engage [25]. Multi-armed bandit strategies [2, 10, 27] have proven highly effective for budget allocation due to their simplicity, ease of analysis, and practical implementation in real-world systems. However, these algorithms often suffer from inefficient exploration and may struggle to adapt to the evolving behavior of campaigns over extended periods. Non-stationarity is a frequent issue in online advertising environments [21], where detecting changes and quickly adapting to them is critical."}, {"title": "2 RELATED WORK", "content": "Budget allocation across multiple ad campaigns [10, 13] has been extensively studied in industrial research by companies like Criteo [8], Netflix [23], and Lyft [20]. A common approach is to discretize the budget and model each sub-campaign as an arm in a multi-armed bandit problem. The optimal allocation is obtained by solving a combinatorial optimization problem [38] based on the expected reward of each arm. In previous literature, domain knowledge has been used to formulate parametric models of the arms, approximating the cost-to-reward function with a power law [18] or a sigmoid [16], followed by Thompson Sampling to handle uncertainty and induce exploration. However, these methods often overlook noise in the data, a critical factor in real-world deployments. In the presence of noise, parametric models can significantly deviate from the true reward function. A more flexible alternative is to model the reward function using Gaussian Process (GP) models [27, 28], which allow for greater adaptability. These algorithms typically use Upper Confidence Bound (UCB) or Thompson Sampling (TS) to guide exploration. However, unlike our approach, they do not incorporate domain knowledge to promote exploration, which can lead to higher regret. Additionally, these algorithms are mostly studied for budget allocation for a single day or month [28] which does not account for changing behaviours of the reward function, a characteristic often observed in campaigns running over many months.\nHandling non-stationarity in multi-armed bandits is a well-studied problem in the literature [4, 6, 32]. Common methods include passive approaches, such as sliding windows with UCB or TS sampling [36], or using discounted rewards [15]. Active methods, such as change point detection [5, 24], offer a more dynamic approach. Passive methods either discard older data points or assign them less weight. However, in long-running campaigns where non-stationarity changes occur infrequently, these approaches are less effective. For our algorithm, we adopt an active approach to better handle reward function shifts."}, {"title": "3 PROBLEM FORMULATION", "content": "We follow the standard formulation of the Automatic Budget Allocation (ABA) problem from the literature [27]. Consider an advertising campaign $A = {A_1, ..., A_N }$ with $N \\in \\mathbb{N}$, where each $A_j$ represents a sub-campaign in the portfolio. The campaigns run over a finite time horizon of $T \\in \\mathbb{N}$ days with a budget $B = {b_1,..., b_T }$, where $b_t \\in \\mathbb{R}^+$ denotes the maximum budget that can be spent at time $t \\in {1,..., T}$. For each day and sub-campaign $A_j$, the advertiser must allocate a budget $b_{j,t} \\in [b_\\text{l}, b_t]$, where $b_\\text{l} \\in \\mathbb{R}^+$ represents the minimum budget. After setting the budget, the platform determines the cost $x_{j,t}$, and the advertiser receives feedback in the form of rewards (such as clicks or conversions) from an unknown function $\\eta_{j,t}$. The goal of the advertiser is to determine the optimal budget allocation across all sub-campaigns to maximize the cumulative return on investment. Formally, the problem is formulated as the following constrained optimization problem:\n$\\begin{aligned}\n&\\underset{\\textbf{x}}{\\text{max}} & \\sum_{j=1}^{N} \\sum_{t=1}^{T} \\eta_{j,t}(x_{j,t}) \\\\\n& \\text{s.t.} & \\sum_{j=1}^{N} b_{j,t} \\leq b_t \\\\\n& & b_l \\leq b_{j,t} \\leq b_t \\forall j\n\\end{aligned}$  \\tag{1a} \\tag{1b} \\tag{1c} \\tag{1}\nHere, $x_{j,t}$ represents the cost spent on the sub-campaign $A_j$ at time t. The cost-to-reward relationship $\\eta_{j,t}$ is dynamic, often changing over time due to market fluctuations. In particular, we focus on settings where the reward function changes abruptly, modeled as a piece-wise constant function of time that shifts a finite number of times. Formally, in the non-stationary setting, a break-point $p \\in {1,..., T}$ is defined as a round where the expected reward with respect to budget set B of at least one sub-campaign undergoes a change, i.e.,"}, {"title": "$\\mathbb{E}[\\sum_{i=0}^{B} \\eta_{j,p-1}(b_i)] \\neq \\mathbb{E}[\\sum_{i=0}^{B} \\eta_{j,p}(b_i)]$ for some sub-campaign j.", "content": "Let $P = p_1,..., p_r$ denote the set of breakpoints, with $p_0 = 1$, partitioning the rounds into a set of phases $F_1,..., F_r$, where each phase is defined as:\n$F_\\phi = {t \\in {1, ...,T} | p_{\\phi-1} \\leq t < p_{\\phi}}$. \\tag{3}\nWithin each phase $F_\\phi$, the reward function for sub-campaign $A_j$ remains constant and is given by:\n$\\mu_{j,\\phi} = \\mathbb{E}[\\sum_{i=0}^{B} \\eta_{j,t}(b_i)]$ for $t \\in F_\\phi$.\nTo effectively detect abrupt changes in the reward functions, we follow two standard assumptions commonly used in non-stationary multi-armed bandit (MAB) settings [33]:\nAssumption 1$\\exists \\tau \\in \\mathbb{R}^+$, known to the learner, such that for each sub campaign $A_j$ whose expected reward changes between consecutive phases $\\phi$ and $\\phi + 1$, we have:\n$|\\mu_{j,\\phi} - \\mu_{j,\\phi+1}| \\geq \\tau$.\nThis lets the learner decide on a minimum possible magnitude of change such that the learner is able to detect it.\nAssumption 2 There exists a time period $T_p$, unknown to the learner, such that:\n$min_{ \\phi \\in {1,...,T} } (p_\\phi - p_{\\phi-1}) \\geq T_p$.\nThis prevents the breakpoints from being too close to one another.\nAssumption 3 Based on previous literature, the reward function at any phase $\\eta_j(x)$ exhibits the following properties [17, 19]:\n(1) $\\eta_j(x)$ is continuous and smooth to at least the second order.\n(2) $\\eta_j(x)$ is monotonically increasing with the cost (more spend always yields more clicks/conversions), i.e., $\\eta'_j(x) > 0$.\n(3) $\\eta_j(x)$ has a diminishing marginal impact, i.e., $\\eta''_j(x) < 0$."}, {"title": "4 PRELIMINARIES", "content": "In a combinatorial semi-bandit framework [7], the agent selects a subset of options, referred to as super-arms, from a finite set of available choices, known as arms. This selection is subject to combinatorial constraints, such as the knapsack constraint. In this work, the reward of each arm is modeled using Gaussian Process Regression, and the optimization is solved using a multi-choice knapsack algorithm. We briefly explain each of these concepts as follows:"}, {"title": "4.1 Gaussian Process Regression", "content": "Gaussian Process Regression (GPR) [34] is employed to model the relationship between budget allocation and resulting reward. GPR is a non-parametric, probabilistic method that provides both predictive mean and uncertainty estimates for a given set of inputs. Formally, a GP is defined as:\n$f(x) \\sim GP(\\mu(x), k(x, x'))$\nwhere f(x) represents the unknown function that relates the input variables x (e.g., budget) to the output variables (e.g., clicks). The mean function $\\mu(x)$ is typically assumed to be zero. The covariance or kernel function $k(x, x')$ encodes the correlation between any two input points.\nThe predictive mean $\\mu(x*)$ and variance $\\sigma^2(x*)$ at a test point $x*$ are given by:\n$\\mu(x) = k_*^T (K + \\sigma_n^2 I)^{-1} y$\\n$\\sigma^2(x*) = k(x*, x*) - k_*^T (K + \\sigma_n^2 I)^{-1} k_*$\nwhere $k_* = [k(x*, x_1), ..., k(x*, x_n)]$ is the vector of covariances between the test point x* and each training input $x_i$, and K is the covariance matrix computed over the training inputs, with entries $K_{ij} = k(x_i, x_j)$ and y is the observed mean. The term $\\sigma_n^2$ represents the variance of the noise in the observations.\nThe budget-to-reward relationship is modeled using the Radial Basis Function (RBF) kernel. The RBF kernel assumes a smooth and continuous relationship, defined as $K_{RBF}(x, x') = exp(-\\frac{||x-x'||^2}{2l^2})$, where $\\sigma_f^2$ is the signal variance and l is the length scale."}, {"title": "4.2 Multi Choice Knapsack", "content": "The optimization problem can be cast as a modified version of the knapsack problem from [22] called Multi Choice Knapsack (MCK). Given an estimated reward model of each sub-campaign and an evenly spaced discritization of the daily budget $B \\subset B$, the optimal reward for each sub-campaign can be identified through enumeration. The solution can be efficiently computed with a dynamic programming approach. The matrix M(j, b) with j \\in 1... N and b \\in B. For a particular $F_\\phi$, The matrix is iteratively filled: each element is initialized as M(j, b) = 0 for all j and b \\in B. For j = 1, the value is set:\n$M(1, b) = \\eta_1(b) \\forall b \\in B$\nThis equation represents the best budget allocation for the sub-campaign $A_1$ if it were the only sub-campaign to consider. For j > 1, each matrix entry is updated as follows:\n$M(j, b) = \\underset{b' \\in B, b' \\leq b}{max} (M(j - 1, b') + \\eta_j (b - b'))$\nThen the maximum value among all combinations is selected. At the end of the recursion, the optimal solution is found by evaluating the matrix cell corresponding to:\n$\\underset{b \\in B}{max} M(N, b)$\nTo retrieve the corresponding budget allocation, the matrix is traced back to store the partial assignments that maximize the total value. The time complexity of this algorithm is $O(NH^2)$, where N is the number of subcampaigns and H = |B| represents the cardinality of the budget set."}, {"title": "5 SIMULATION ENVIRONMENT", "content": "A major challenge in studying budget allocation algorithms for digital ads is the lack of open-source simulation environment capable of simulating logged offline data. Previous studies have either relied on synthetic data [10, 27], which fails to fully capture real-world dynamics, or on proprietary data that is not publicly available [19, 37], rendering research results difficult to reproduce. Available real world datasets like criterio dataset [11] do not provide structured campaign groups and are limited to a time horizon of 30 days.\nTo bridge this gap, we designed a simulation environment that mimics the behaviour of long running ad campaigns from logged data. The simulation environment and the logged data will be released publicly to facilitate reproducible research. The architecture of the simulation environment is depicted in Fig 2a.\nThe daily budget is set as per the total monthly cost consumed by all the campaigns of a campaign group divided by the number of days per month. In any realistic ad delivery platform the actual spent cost $x_{j,t}$ is in not equivalent to the allocated budget and depends on the platforms internal learning algorithms. For example the Google Ads platform provides the following guidelines [1] : 1) The spent amount can be lower or 2 times higher than the daily budget on any particular day. 2) The total spent budget is not more than 30.4 the average daily budget. We model this variability in daily budget spent using a truncated normal distribution:\n$x_{j,t} \\sim N(b_{j,t}, \\sigma^2) s.t 0 \\leq x_{j,t} \\leq 2 * b_{j,t}$ \\tag{4}\nThe cost variability is shown in Fig 2b. Following [19] we model the cost to reward function of each sub campaign as a power law function with noise.\n$\\eta_j(x_{j,t}) = a_c * x_j^{\\alpha_c} + \\epsilon$ \\tag{5}\nWhere $\\epsilon$ adds a small error in observation. The simulation environment updates the reward model every day with data points from the logged data of that day. The parameters $a_c$ and $\\alpha_c$ are estimated from data using curve fitting as shown in Fig 2a i). In order to model abrupt changes between the reward functions we maintain a power law model $a_f$ and $\\alpha_f$ for the next $T_p$ days from the current time point in simulation (Assuming a stationary period of length $T_p$) of data. If a change is detected, i.e., when $a_c$ and $a_f$ differ more than 20%, the current model is replaced with the future model on the onset of detected change as shown in Fig 2a ii). This allows the function to change at arbitrary points during the run of the campaign as would happen in a real campaign as depicted in Fig 2c."}, {"title": "6 AUTOMATIC BUDGET ALLOCATION ALGORI\u03a4\u0397\u039c", "content": "The ABA algorithm is summarised in Algo 1 which involves the following broad steps:\n(1) Estimation of reward function using GP\n(2) Predicting rewards for each arm of the bandit\n(3) Allocating budget using multi-choice knapsack\n(4) Change point detection.\nThe algorithm enhances the automatic budget allocation strategy to cater to practical considerations. In any multichannel advertising application exploration is expensive. This means we should be selective about spending budget in regions where we expect higher gains. First we observe that a zero-mean Gaussian Process Regressor as used in [28] obtains a pessimistic prior over the budget range as depicted in Fig 3 (i). This prior restricts effective exploration to higher ranges of budget where quality traffic might be present. To address this, we modify the mean of the GP model with a saturating mean function for each sub-campaign j as follows:\n$\\eta_j = \\begin{cases}\n\\eta_{jmax}, & \\text{if } b_{j,i} > b_{jmax} \\\\\n\\eta_{j}, & \\text{otherwise}\n\\end{cases}$ \\tag{6}\nWhere $\\eta_j$ is the GP estimate of $\\eta_j$ with time subscript removed for brevity and $b_{jmax}$ is the current budget level with highest reward value for campaign j and $i \\in B$. This allows the mean to saturate at the last estimated maximum observed reward for a campaign as shown in Fig 3 (ii). Next, we introduce a modified Upper Confidence Bound exploration strategy to enhance the performance of the combinatorial bandit approach. The modified exploration strategy is defined as follows:\n$\\hat{\\eta}_j(\\cdot) \\leftarrow \\hat{\\eta}_j(\\cdot) + {\\beta * (1 - \\theta_j) * \\sigma_j}|\\mathbb{I}_{b_{j,i}>b_{j,max}}$ \\tag{7}\nWhere $\\beta$ is the exploration factor for balancing exploration and exploitation. The proposed modified UCB promotes the following:\n*   $\\Theta_j$ represents the efficiency of arm j. For example, Cost per Click (CPC) can be used as $\\Theta_j$ when maximizing clicks where $\\Theta_j = \\frac{cpc_j}{\\underset{j}{\\text{cost}_{j,t}/max(\\Sigma_t\\text{click})}}$ is the normalized cost per click of sub campaign j. A lower cpc denotes higher efficiency of the sub-campaign. The inclusion of term 1-$\\Theta_{cpc_j}$ incentivizes the policy to perform aggressive explorations for efficient arms. This term can be replaced by any other metric of efficiency as per advertiser's objective. For example, the Cost per Acquisition (CPA) can be chosen as the exploration incentive during maximizing conversions.\n*   The term $\\mathbb{I}_{b_{j,i}>b_{j,max}}$ denotes and indicator function that checks whether a discritized budget level used by MCK is higher than the current observed budget level having highest predicted reward. The uncertainty based exploration is only targeted towards regions that contain more information than the current best knowledge as illustrated in Fig 3 (iv). Without this targeted exploration the algorithm may incur unnecessary regret by exploring lower budget levels as shown in Fig 3 (iii).\nFor the non stationary change detection we maintain two models. $M_j$ denotes the model which estimates the reward function for data points of phase $F_\\phi$ until break-point $P_{\\phi+1}$ is detected. $\\tilde{M_j}$ denotes the model using data points from current $windowlength$."}, {"title": "$\\mathbb{p}rediff = \\frac{1}{|B|}\\sum_{i=1}^{B} |\\tilde{M(b_i)} - M(b_i)|$ \\tag{8}", "content": "We then perform change point detection using a Mean Average Error test over the entire budget set to check if the predictions from the models have changed beyond a threshold $\\tau$.\nMAE is used due to its ease of implementation for practical usage. Any sophisticated change point detection strategy can be used in place of MAE. If a change is detected the data buffer is refreshed with the current $windowlength$ data denoting the start of a new phase $F_{\\phi+1}$."}, {"title": "We now theoretically analyze the regret bound of the proposed method and show that the regret bound reduces for the proposed UCB utility under Assumption 3.", "content": "LEMMA 6.1 (FROM [35]). Given the realization of a GP f(\u00b7), the estimates of the mean $\\hat{\\mu}_{t-1}(b)$ and variance $\\hat{\\sigma}^2_{t-1}(b)$ for the input b belonging to the input space B, for each $\\beta \\in \\mathbb{R}^+$ the following condition holds:\n$P(|f(b) - \\hat{\\mu}_{t-1}(b)| \\geq \\sqrt{\\beta} \\hat{\\sigma}_{t-1}(b)) \\leq e^{-\\frac{\\beta}{2}},$\nfor each b \\in B.\nPROPOSITION 6.2. Let us consider an ABA problem over T rounds where the function $\\tilde{\\eta}_j(b)$ is the realization of a GP, using TUCB-MAE algorithm with the following upper bound on the reward function $\\eta_j(b)$:\n$\\bar{\\mu}_{t-1}(b) := \\hat{\\eta}_{j,t-1}(b) + \\sqrt{\\Beta_{j,t}} \\hat{\\sigma}_{j,t-1}(b)$\nwhere b is a budget level,n denotes the round and j is the campaign, with probability at least 1 \u2013 $\\delta$, it holds:\n$R_T(U) = \\tilde{O} ( \\sqrt{T N \\sum_{j=1}^N \\gamma_T (\\tilde{\\eta}_j)} )$\nwhere the notation $\\tilde{O} (\\cdot)$ disregards the logarithmic factors.\nProof Sketch: It can be derived regret is lower bounded by $\\hat{\\sigma}_{j,t-1}(a)$ where a is the action with max $\\hat{\\sigma}_{j,t-1}$ for campaign j. Using Lemma 5.6 of [35], the information gain provided by the observations $n_{t-1} = (\\tilde{\\eta}_{j,1},..., \\tilde{\\eta}_{j,t-1})$ corresponding to the actions $(a_{j,1}, ..., a_{j,t-1})$ is:\n$IG(\\hat{\\mu}_{t-1}|\\tilde{\\eta}_j) = \\frac{1}{2} \\sum_{h=1}^t [\\tilde{\\eta}_j -  \\hat{\\mu}_{j,h}(a_{j,h})]^2 \\frac{1}{\\lambda}$\nand $\\hat{\\sigma}_{j,t-1}(a)$ can be bounded by:\n$\\hat{\\sigma}^2_j(a, b) \\leq \\frac{\\sigma_j^2}{\\lambda} log(\\frac{1}{\\delta})$+\n$log(1+\\frac{\\sigma_j^2}{\\lambda})$ \\tag{A.4}\nand regret can be derived as a lower bound of IG,\nwith $\\Beta_{j,t} = \\frac{\\pi^2 k_j}{6}, k_j = (1 - \\Theta_j)$. For every $\\delta \\in (0,1)$ the following holds with probability at least 1 \u2013 $\\delta$ (using Lemma 6.1),\n$R_T (U) \\leq 4T\\Beta_T ( \\frac{1}{\\lambda} log (1+\\frac{1}{\\lambda})) \\sum_{j=1}^N \\gamma_T(\\tilde{\\eta}_j)$\nwhere $\\lambda$ is the variance of the measurement noise of the reward function $\\tilde{\\eta}_j(\\cdot)$ and $\\gamma_T (\\tilde{\\eta}_j)$ is the total information gain .\nSince the regret is bounded by information gain, if we explore values of $b_{j,t} \\leq b_{j max,t}$, by monotonicity, we have:\n$\\eta_j(b_{j,t}) \\geq \\eta_j(b_{j max,t}) \\geq \\hat{\\eta}_j(b_{j,t})$.\nWhere $b_j$ is the budget level with maximum reward of arm j. This means that exploring in this region incurs unnecessary regret because we are not gaining new information about potentially better actions. By restricting exploration to values $b_{j,t} > b_{j max,t}$, the effective space of arms to explore is reduced. This reduces $\\gamma_T (\\tilde{\\eta}_j)$, which in"}, {"title": "We now theoretically analyze the regret bound of the proposed method and show that the regret bound reduces for the proposed UCB utility under Assumption 3.", "content": "LEMMA 6.1 (FROM [35]). Given the realization of a GP f(\u00b7), the estimates of the mean $\\hat{\\mu}_{t-1}(b)$ and variance $\\hat{\\sigma}^2_{t-1}(b)$ for the input b belonging to the input space B, for each $\\beta \\in \\mathbb{R}^+$ the following condition holds:\n$P(|f(b) - \\hat{\\mu}_{t-1}(b)| \\geq \\sqrt{\\beta} \\hat{\\sigma}_{t-1}(b)) \\leq e^{-\\frac{\\beta}{2}},$\nfor each b \\in B.\nPROPOSITION 6.2. Let us consider an ABA problem over T rounds where the function $\\tilde{\\eta}_j(b)$ is the realization of a GP, using TUCB-MAE algorithm with the following upper bound on the reward function $\\eta_j(b)$:\n$\\bar{\\mu}_{t-1}(b) := \\hat{\\eta}_{j,t-1}(b) + \\sqrt{\\Beta_{j,t}} \\hat{\\sigma}_{j,t-1}(b)$\nwhere b is a budget level,n denotes the round and j is the campaign, with probability at least 1 \u2013 $\\delta$, it holds:\n$R_T(U) = \\tilde{O} ( \\sqrt{T N \\sum_{j=1}^N \\gamma_T (\\tilde{\\eta}_j)} )$\nwhere the notation $\\tilde{O} (\\cdot)$ disregards the logarithmic factors.\nProof Sketch: It can be derived regret is lower bounded by $\\hat{\\sigma}_{j,t-1}(a)$ where a is the action with max $\\hat{\\sigma}_{j,t-1}$ for campaign j. Using Lemma 5.6 of [35], the information gain provided by the observations $n_{t-1} = (\\tilde{\\eta}_{j,1},..., \\tilde{\\eta}_{j,t-1})$ corresponding to the actions $(a_{j,1}, ..., a_{j,t-1})$ is:\n$IG(\\hat{\\mu}_{t-1}|\\tilde{\\eta}_j) = \\frac{1}{2} \\sum_{h=1}^t [\\tilde{\\eta}_j -  \\hat{\\mu}_{j,h}(a_{j,h})]^2 \\frac{1}{\\lambda}$\nand $\\hat{\\sigma}_{j,t-1}(a)$ can be bounded by:\n$\\hat{\\sigma}^2_j(a, b) \\leq \\frac{\\sigma_j^2}{\\lambda} log(\\frac{1}{\\delta})$+\n$log(1+\\frac{\\sigma_j^2}{\\lambda})$ \\tag{A.4}\nand regret can be derived as a lower bound of IG,\nwith $\\Beta_{j,t} = \\frac{\\pi^2 k_j}{6}, k_j = (1 - \\Theta_j)$. For every $\\delta \\in (0,1)$ the following holds with probability at least 1 \u2013 $\\delta$ (using Lemma 6.1),\n$R_T (U) \\leq 4T\\Beta_T ( \\frac{1}{\\lambda} log (1+\\frac{1}{\\lambda})) \\sum_{j=1}^N \\gamma_T(\\tilde{\\eta}_j)$\nwhere $\\lambda$ is the variance of the measurement noise of the reward function $\\tilde{\\eta}_j(\\cdot)$ and $\\gamma_T (\\tilde{\\eta}_j)$ is the total information gain .\nSince the regret is bounded by information gain, if we explore values of $b_{j,t} \\leq b_{j max,t}$, by monotonicity, we have:\n$\\eta_j(b_{j,t}) \\geq \\eta_j(b_{j max,t}) \\geq \\hat{\\eta}_j(b_{j,t})$.\nWhere $b_j$ is the budget level with maximum reward of arm j. This means that exploring in this region incurs unnecessary regret because we are not gaining new information about potentially better actions. By restricting exploration to values $b_{j,t} > b_{j max,t}$, the effective space of arms to explore is reduced. This reduces $\\gamma_T (\\tilde{\\eta}_j)$, which in"}, {"title": "turn reduces the regret bound. Specifically, if we denote the restricted exploration space by X, we have:\n$\\gamma_T(\\tilde{\\eta}_j, X^*) \\leq \\gamma_T (\\tilde{\\eta}_j)$.\nThus, under monotonocity assumption of \\tilde{\\eta}_j\n$R_T(U^*) = O ( \\sqrt{\\frac{T N}{\\lambda} \\sum_{j=1}^N \\gamma_T (\\tilde{\\eta}_j, X^*)} ) \\leq R_T (U)$\ndetailed proof is given in supplementary material.", "content": null}, {"title": "7 EMPIRICAL STUDIES", "content": "We perform empirical experiments on multiple real logged campaign data obtained from different platforms. We denote the different advertisement platforms as Platform A and Platform B. The hyper-parameter choices are reported in supplementary material.\nFor experimental analysis we choose $T_p$ = 20 assuming a stationary period of 20 days and $windowlength$ = 7 days. The budget discretization granularity is 500. We simulate these campaigns in the simulation environment allowing the experiments to be conducted for long running campaigns with changing behaviour due to market dynamics. The noise ($\\epsilon$) is sampled from a normal distribution N(0, 0.1). The proposed algorithm is compared against the following SOTA baselines:\n(1) UCB - MAE: Represents a combinatorial multi-arm bandit strategy with upper confidence bound for exploration and mean average error for change point detection. Represents the class of active approaches where the reward function is re-learned based on change point detection [33]. Comparison shows superiority of our proposed exploration utility.\n(2) UCB - NCPD: Is a combinatorial bandit strategy with UCB exploration and no change point detection depicting the importance of change point detection.\n(3) UCB - SW [15]: Represents a combinatorial bandit algorithm with UCB exploration and sliding window of fixed length (10 days) for non stationary adaption and same exploration parameter $\\beta$ as our algorithm.\n(4) TS-SW [14]: Represents a combinatorial bandit algorithm with thompson sampling exploration and sliding window of fixed length (10 days) for non stationary adaption.\n(5) UCB-DS [15]: A combinatorial bandit strategy with discounting past data using a factor 0.9 and UCB exploration strategy.\nWe report the results in Table 1 with respect to three metrics explained as follows:\nClicks: A higher number of clicks generally reflects increased user engagement, making it a key measure of effective budget allocation.\nRegret: We report the average cummulative regret compared to an oracle optimizer which has access to the parameters of the true reward function in the simulation environment.\nCost Per Click (CPC): The average cost per click for all the sub-campaigns in a campaign group. A lower CPC denotes higher ROAS and efficiency for advertisers."}, {"title": "7.1 REWARD TYPES", "content": "We consider two kinds of reward signals for budget allocation. The first choice is maximizing clicks which has been popularly used in pay per click advertisements [17, 28]. However, in businesses advertisers often aim at maximizing the number of conversions for campaigns which drives profitability. We observe the number of conversion per day is a very sparse signal often having a low value for many days which renders this signal inefficient to be estimated"}, {"title": "as a reward function and optimized directly. In order to optimize conversions we formulate $\\mathbb{p}seudoconversion$ defined as follows:\n$\\mathbb{p}seudoconversion = \\frac{\\sum_{t'=t-7}^{t} click_{t'} - conversion_{t'}}{\\sum_{t'=t-7}^{t} click_{t'}}$", "content": "$\\mathbb{p}seudoconversion$ calculates a weighted conversion rate based on the number of clicks for each day, scaled by the conversion rate over the past 7 days as depicted in Fig 6, capturing how effective ad campaigns are at driving conversion."}, {"title": "7.2 Ablation Studies", "content": "We perform ablation studies by studying the effect of different components of the proposed combinatorial bandit approach on AI Prediction Tool Campaigns with respect to clicks. TUCBMAENOSM represents a policy using Targeted UCB with CPC as efficiency but no saturating mean. TUCBMAENOCPC is a policy without efficiency incentive for exploration. NoTUCBMAEWithCPC is a policy"}, {"title": "without targeted UCB for higher budget range but with CPC incentive for exploration along with normal UCB and saturating mean.", "content": "The results are reported in Fig 7. It can be clearly interpreted from the ablation study the targeted UCB has the highest contribution to performance gain as NoTUCBMAEWithCPC has the lowest reward. Additionally we observe the efficiency incentive provides performance boost. Finally, the ablation study shows all three components contribute to the performance improvement of the algorithm."}]}