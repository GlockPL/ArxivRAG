{"title": "Unveiling the Decision-Making Process in Reinforcement Learning with Genetic Programming", "authors": ["Manuel Eberhardinger", "Florian Rupp", "Johannes Maucher", "Setareh Maghsudi"], "abstract": "Despite tremendous progress, machine learning and deep learning still suffer from incomprehensible predictions. Incomprehensibility, however, is not an option for the use of (deep) reinforcement learning in the real world, as unpredictable actions can seriously harm the involved individuals.\nIn this work, we propose a genetic programming framework to generate explanations for the decision-making process of already trained agents by imitating them with programs. Programs are interpretable and can be executed to generate explanations of why the agent chooses a particular action. Furthermore, we conduct an ablation study that investigates how extending the domain-specific language by using library learning alters the performance of the method. We compare our results with the previous state of the art for this problem and show that we are comparable in performance but require much less hardware resources and computation time.", "sections": [{"title": "1 Introduction", "content": "In recent years, machine learning and deep learning have made enormous progress both in research and in everyday use to support users in all kinds of tasks. While they are helpful in the short term, there is still no guarantee that the predictions or the generated content will be correct. Explainable artificial intelligence (XAI) is on the rise to alleviate some of the problems in understanding how a model generates a prediction. In the case of decision-making and reinforcement learning (RL), it is even more important to understand how an agent makes a decision."}, {"title": "2 Related work", "content": "Genetic programming [19] and especially program synthesis [35] have a long history in the computer science research community. The application of GP to improve the interpretability of various machine learning models has been studied before in [13]. Therefore, the authors introduce GPX, the Generic Programming Explainer, which generates a tree structure of the behavior of models like deep neural networks, random forests or support vector machines. Another approach to explain a learned RL policy was introduced in [34], which is based on a multi-objective GP algorithm. The authors achieve state of the art results when comparing the interpretable extracted policy in different control environments. In [22] a neurogenetic approach is implemented to directly learn interpretable reinforcement learning policies. The method was applied to common OpenAI Gym environments and was able to keep up with the leaderboards. Another approach, genetic programming for reinforcement learning (GPRL), is introduced by Hein et al. [15]. The method is able to extract compact algebraic equations or boolean logic terms from observed trajectories.\nHowever, GP is not the only method in the field of explainable reinforcement learning (XRL). In a recent extensive survey [25], XRL is categorized into three explainable facets: feature importance, learning process and Markov decision process, and policy-level. Among these categories, feature importance provides explanations for which features of the state affected the decision-making process of the agent. This category includes programmatic policies, where a policy is represented by a program, since programs directly use the state of the environment and thus the program can be traversed to generate an explanation [32,33,2,28]. Furthermore, other works within this category synthesize finite state machines to represent policies [16] or use decision tree-based models [29,3]. Our methodology fits into this category as we explain sub-trajectories of policies and our overarching goal in the future is to extract a program capable of representing the complete policy."}, {"title": "3 Background", "content": "In this section, we introduce the various definitions and ideas on which our work is based. We first provide a general overview of the domain-specific language and then give a detailed description of the framework proposed in [10]."}, {"title": "3.1 Program and Domain-specific Language", "content": "To represent programs, this work uses a typed domain-specific language (DSL), inspired by the Lisp programming language [24], represented as a uniformly distributed probabilistic grammar, i.e., each production rule has the same probability [23]. The DSL's core components consist of control flow production rules, the available actions the agents can use and modules designed to help the agent perceive its environment. Given our focus on grid environments, the agent's perception relies on modules that determine specific positions on the grid and compare them with possible objects of the environment such as walls, empty cells, or game-specific objects. Additionally, the control flow production rules consists of if-else statements and Boolean operators, enabling the synthesis of more complex conditions."}, {"title": "3.2 Program Synthesis with Library Learning for Reinforcement Learning", "content": "In [10] a method is introduced to learn generalizable and interpretable knowledge in grid-based RL environments. To achieve this, the authors adapt the state-of-the-art program synthesis system DreamCoder [11] for reinforcement learning. First, RL agents are trained for a given environment to collect data to imitate. This is equivalent to sampling of the state-action pairs from the learned policy in Figure 1. After the data has been collected from the trained agents, the program synthesis part begins. This part consists of an iterative procedure which is guided by a curriculum [4] that provides the program synthesizer with the input-output examples. The curriculum is able to adapt the sequence length of the sub-trajectories to make the task more challenging and is based on the intuition that shorter sequences are easier to imitate than longer ones. Shorter sequences contain less information, so the programs are shorter and easier to synthesize.\nThe iterative procedure consists of three possible components: the search for programs for the provided state-action pairs from the curriculum which can be done with a neural network or a symbolic approach; the generation of a training dataset and the training of the neural network; a library learning module that analyzes programs that can imitate at least one sub-trajectory to extend the DSL. In [10] three different kinds of synthesizers are studied, a language model (LM) fine-tuned on code [36], a brute-force enumerative search and a neural-guided enumerative search [11]. These program synthesizers are integrated into the iterative procedure that is guided by the curriculum. The necessary steps for each method are described briefly below."}, {"title": "3.3 Case Study: Drawbacks of previous Method", "content": "All proposed program synthesizers of [10] have several disadvantages compared to a GP algorithm for this task. Both enumerative search methods always start from scratch of the search process instead of exploiting the fact that the input-output examples are provided by the curriculum and thus share similar structures. GP takes advantage of this by making local changes and keeping the population of the last iteration as the starting population for the next iteration.\nThe greatest issue with an LM is the dataset generation. In [10], 50000 random programs are generated in each iteration and are executed in the RL environment. However, recent work has shown that 50000 random programs are far from sufficient to learn the semantics of the DSL [18]. Jin et al. showed that for a simplified DSL, without if-clauses and Boolean operators, the LM learns the semantics of a DSL, which controls an agent in a grid environment [18]. To achieve this, the authors generated one million programs for the training dataset, 20 times the size of the dataset to train CodeT5.\nIn addition, the library learning module of DreamCoder has a high memory and time requirement. These drawbacks have been mitigated by a new library learning algorithm called Stitch [5], which is about three orders of magnitude faster and requires about two orders of magnitude less memory. DreamCoder uses a deductive approach that is challenging to scale to longer programs because programs are refactored with rewrite rules, resulting in huge memory and time requirements. Stitch, on the other hand, synthesizes abstractions directly through a method called corpus-guided top-down synthesis, which uses a top-down search to constrain the search space and guide it to abstractions that cover"}, {"title": "4 Methodology", "content": "In this section, we present a genetic programming algorithm for explaining the decision-making process of RL agents. First, we give an overview of the general algorithm, including the mutation and crossover operators. Second, the integrated curriculum, bloat control and the library learning module are described."}, {"title": "4.1 Genetic Programming", "content": "Given that our DSL is based on Lisp, the proposed GP algorithm is tree-based and directly modifies the abstract syntax tree of Lisp [19]. Mutation: With a probability of Pmutation per node, we randomly choose nodes for mutation. We then select a random production rule for the same return type as the selected node and check whether the input parameter types match. If they match, we keep the child nodes of the tree, if not, we sample new subtrees for each child parameter. This mutation process is similar to sampling random programs for initializing the population, as only the return type is specified to generate a new tree."}, {"title": "Crossover:", "content": "We use a one-point crossover operator where we randomly select a node from a given tree of a program with a probability of Pcrossover for each node. We then scan the other tree for the same return type and randomly select one of those nodes. Afterwards, we link the subtree to the parent of the selected node from the other tree and do it for the remaining node vice versa."}, {"title": "Fitness:", "content": "Fitness is evaluated based on the number of correct state-action pairs that the algorithm imitates with an individual. We adapt the formula for calculating the accuracy for a set of programs from [10] to calculate the proportion of correctly solved tasks for a single program p, i.e. an individual in the population. Equation 1 calculates the fitness of an individual for the dataset D to imitate. ND is the size of D, i.e. the number of state-action pairs. A sub-trajectory from D is defined by T, which consists of state-action pairs (s, a). Equation 2 defines f (p, T) that evaluates a program p and returns 1 if the complete rollout T can be imitated and otherwise 0. EXEC(p, s) executes the program for the state s and returns an action a. Boolean values are mapped to 0 and 1 with the identity function 1.\nFitness = $\\frac{1}{1 + N_D - \\sum_{T \\in D} f(p, T)}$\n$f(p, \\tau) = 1 \\{ EXEC(p, s) == a, \\forall (s, a) \\in \\tau \\}$,\nis 0 after the first (p,s) where EXEC(p,s) != a"}, {"title": "Selection:", "content": "Tournament Selection [26] is used to select individuals for mutation and crossover. First, k individuals are selected randomly and then from this smaller population, the individual with the best fitness is selected."}, {"title": "Curriculum:", "content": "The curriculum is integrated into the GP algorithm and increases the sequence length of the state-action pairs every 10 generations or if at least 95% of the collected data is imitated. Afterwards, the library learning module is used to analyze all correct programs. The curriculum stops when no individual in the population is able to imitate a state-action pair."}, {"title": "Bloat Control:", "content": "Bloat control, which limits the excessive growth of the program trees in the population, and simplification of programs is an important aspect in GP to make programs interpretable and concise [17]. For this reason, we limit the growth of the program trees by integrating a bloat control into the fitness function. For this, we need an auxiliary function size, which returns the size of the input program, i.e. how many production rules the program has used. The following equation calculates the final fitness for an individual p:\nFitness =$\\frac{1}{1 + N_D - \\sum_{T \\in D} f(p, T) + w_b * size(p)}$\nwhere wb is the bloat weight, which penalizes bigger programs.\nIt is also possible to enforce the simplification of programs with a more restrictive type system in the DSL, resulting in more concise programs without excessive bloat. The types of the functions can be more restricted, for example by only allowing the comparison of the direction of the agent agentDirection"}, {"title": "4.2 Library Learning", "content": "In this step, the library learning module analyzes all programs, which could solve at least one task, and extract functions from these programs to extend the DSL. Before we add a new function to the DSL, we check whether the size of the function is below a predefined value. In our experiments, we used 10 to mitigate the problem that abstractions are too specific and the search gets stuck in local minima. This is similar to pruning of decision trees [12]. In addition, the number of extracted functions is limited, as otherwise too many production rules in the DSL could hinder the search [7]. To enhance the execution speed of the extracted functions, we implement a cache that stores the return value for each input parameter configuration."}, {"title": "5 Experiments", "content": "In this section, we evaluate the proposed algorithm. First, the domain is explained, then the execution time and accuracy are compared with the baseline methods. Afterwards, we conduct an ablation study on the influence of library learning for genetic programming."}, {"title": "5.1 Domain", "content": "In this work, we use the grid-world domain [6] and a maze-running task with a partial observation of 5x5 from the agent's point of view. We trained an agent to collect data with the default hyperparameters provided by [27]. Imitation learning is necessary due to the full size of the maze, the partial observation of the agent and the sparse rewards that are only given if the agent finds the exit of the maze. This makes it challenging to solve this problem directly with GP."}, {"title": "5.2 Evaluation", "content": "We compare the runtimes and accuracy of the proposed method to the program synthesizers from [10]: enumerative search, DreamCoder, CodeT5 and LibT5, CodeT5 with a library learning module."}, {"title": "Runtime Improvements", "content": "To show the runtime improvements of GP in the training phase, the time is measured until the threshold for increasing the sequence length is reached. We start by imitating state-action pairs with a sequence length of three and report the runtimes for each iteration until a sequence length of ten is reached, as the times can be extrapolated to the larger sequence lengths and different sequence lengths are reached at the end depending on the method. To ensure the fairest possible comparison, each method uses the same state-action sequences to imitate and similar CPU resources. For GP, enumerative search and DreamCoder, we use four CPU cores from the AMD EPYC processor family 23 model 1. For the language models, four CPU cores from the Intel(R) Core(TM) i7-6950X CPU @ 3.00GHz family 6 model 79 in combination with a 48GB NVIDIA GeForce RTX 2080 Ti GPU are used.\nFor this experiment the number of programs for DreamCoder's library learning module is restricted to 50, as otherwise it will run out of RAM. We exclude CodeT5 and the enumerative search from the table, except for the cumulative sum of all sequence lengths, since the enumerative search is also part of DreamCoder and CodeT5 has the same runtime as LibT5, but without the library learning module. To increase the sequence length, we use the identical curriculum strategy as described in [10]. This strategy extends the sequence length when at least 10% of the state-action pairs are imitated and halts if no sequences can be imitated, for two consecutive instances."}, {"title": "Accuracy", "content": "Figure 4 shows the accuracy of the various program synthesizers. The accuracy calculates the proportion of correctly imitated state-action pairs available for each sequence length [10]. For the genetic algorithm, the results are averaged over ten runs using different seeds. For shorter sequences up to 35, GP is in the middle between DreamCoder and the neural program synthesizers, but offers runtime improvements and lower hardware requirements. GP is the best method for longer sequences, as it can imitate sequences twice as long as DreamCoder. The symbolic-based approaches have a big margin over the LM-based program synthesizers."}, {"title": "Library Learning", "content": "To investigate the influence of learning a library of reusable functions in combination with GP, we plot the difference in accuracy for a genetic programming run with and without library learning in Figure 5. For shorter sequences, almost 10% better accuracy is achieved. For sequence lengths from 10 to 80, library learning is better in most cases. For sequences longer than 80, library learning is detrimental."}, {"title": "6 Discussion & Limitations", "content": "In our experiments, we have shown the benefits of using genetic programming for the task of imitating sub-trajectories for an agent solving a grid-based maze environment. There are, however, various aspects we need to discuss for the different experiments.\nWhen evaluating the program synthesizers, it is difficult to make a fair comparison as they are all based on different methods. The enumerative and neural-guided search have a timeout limit of 720 seconds and thus stop after the given amount of time. The neural program synthesizers are allowed to generate 100 programs for each testing task. GP has for every sequence length a maximum of 10 generations. Table 2 shows that the specified computation budget for LMs and DreamCoder is comparable. For our GP approach, we need much less computation time and at the same time improve the results for longer sequence lengths than all other methods. Even when considering the worst-case scenario and all 10 generations are required, GP takes 136.7 minutes (19.53 min per generation on average) to enhance the sequence length, yet it remains more than 70 minutes faster than the fastest among the alternative methods. This is one of the main advantages of using GP for this type of task and shows a significant reduction in hardware requirements, resulting in more sustainable computing compared to the other methods. Unlike LMs, no GPU is necessary, which significantly reduces the environmental impact [8]. Even though it is possible to achieve human-competitive programming with LMs [21], this is limited to large amounts of available training data, which is not possible in our case since the kind of programs we want to synthesize in a customized DSL are not available anywhere online. Other related problems of LM-based synthesizers were already discussed in section 3.3.\nThe GP-based methods are not as good as DreamCoder and the enumerative search for the shorter sequences up to a length of 35. In terms of the main goal of this research, a policy extraction algorithm, this is not a problem. It is even more important to imitate longer state-action sequences, since programs for longer sequences are more reliable as they explain more of the policy. For"}, {"title": "6.1 Complexity of Implementation", "content": "Another advantage of this work is the complexity of the implementation, which reduces the effort required for follow-up work and for other researchers who want to use this method for other problems. Our implementation only requires Python, whereas DreamCoder uses OCaml and Python in combination. DreamCoder requires 45050 lines of code for the Python files and 12099 lines of code for the OCaml files. In contrast, the method proposed in this paper requires only 1247 lines of code in total - only 2.18% of the DreamCoder implementation, so this implementation is much easier to understand and to customize for follow-up research."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we have presented a tree-based genetic programming method to explain the decision-making process of a reinforcement learning agent. To achieve this, a typed domain-specific language is used that allows to directly modify the abstract syntax tree in the mutation and crossover operators. In addition, we have integrated learning a library of functions, that represent high-level concepts the agent has learned, into the framework. We have shown improvements in runtime and achieved better accuracy at longer sequence lengths than the baseline methods. By using fewer hardware resources than the previous methods, we have also emphasized the sustainable computing aspects of genetic programming compared to neural program synthesis.\nA promising approach for future work is to reduce the complexity of the search problem by considering not just a single grid cell of the environment, but several. This can be done by learning high-level features that are represented as usable functions in the DSL [30]. In addition, the mutation operator can be improved by training a neural network to predict the probabilities of the production rules in the grammar to guide the mutations into the most promising direction."}]}