{"title": "Unveiling the Decision-Making Process in Reinforcement Learning with Genetic Programming", "authors": ["Manuel Eberhardinger", "Florian Rupp", "Johannes Maucher", "Setareh Maghsudi"], "abstract": "Despite tremendous progress, machine learning and deep learning still suffer from incomprehensible predictions. Incomprehensibility, however, is not an option for the use of (deep) reinforcement learning in the real world, as unpredictable actions can seriously harm the involved individuals.\nIn this work, we propose a genetic programming framework to generate explanations for the decision-making process of already trained agents by imitating them with programs. Programs are interpretable and can be executed to generate explanations of why the agent chooses a particular action. Furthermore, we conduct an ablation study that investigates how extending the domain-specific language by using library learning alters the performance of the method. We compare our results with the previous state of the art for this problem and show that we are comparable in performance but require much less hardware resources and computation time.", "sections": [{"title": "1 Introduction", "content": "In recent years, machine learning and deep learning have made enormous progress both in research and in everyday use to support users in all kinds of tasks. While they are helpful in the short term, there is still no guarantee that the predictions or the generated content will be correct. Explainable artificial intelligence (XAI) is on the rise to alleviate some of the problems in understanding how a model generates a prediction. In the case of decision-making and reinforcement learning (RL), it is even more important to understand how an agent makes a decision.\nCurrently, RL is hardly applicable in the real world [9], since unpredictable actions can cause accidents with potentially serious and long-lasting consequences for the involved individuals.\nCreating comprehensible explanations for the decision-making process of agents is still an underexplored problem. Recent work [10] showed that it is possible to generate post-hoc explanations for the decision-making process with program synthesis in combination with library learning. The objective of program synthesis is to find a program for a given specification, such as input-output examples or a natural language description [14]. Library learning benefits program synthesis by extracting functions from already synthesized programs, thus, generating short-cuts in the program search space. This leads to faster search and finding of more programs in the same amount of time [11]. In addition, using programs to control an agent has the benefit that the program can be verified before using it in a production system [20] or can be adapted to other scenarios by software engineers [31].\nThis work studies how genetic programming (GP) [19] can alleviate some of the drawbacks of the proposed method from [10], by first conducting a study, what the problems of the different kind of program synthesizers for these tasks are and how GP tackles those problems. Afterwards, we introduce a GP algorithm which can generate explanations for grid-based RL environments, and show the feasibility of the method for explaining decisions of a maze running agent. This algorithm combines a typed domain-specific language based on Lisp [24] in combination with library learning. Figure 1 shows an overview of the problem setting and approach. We train an agent to sample sub-trajectories, i.e. state-action pairs, from the learned policy. These examples are then imitated with the GP algorithm to generate explanations for the agent's decision-making process. This is possible by highlighting the positions on the state, which the agent checks in the program.\nOur contributions include:\nA genetic programming algorithm for explaining decisions of reinforcement learning agents.\nAblation studies on library learning for genetic programming."}, {"title": "2 Related work", "content": "Genetic programming [19] and especially program synthesis [35] have a long history in the computer science research community. The application of GP to improve the interpretability of various machine learning models has been studied before in [13]. Therefore, the authors introduce GPX, the Generic Programming Explainer, which generates a tree structure of the behavior of models like deep neural networks, random forests or support vector machines. Another approach to explain a learned RL policy was introduced in [34], which is based on a multi-objective GP algorithm. The authors achieve state of the art results when comparing the interpretable extracted policy in different control environments.\nIn [22] a neurogenetic approach is implemented to directly learn interpretable reinforcement learning policies. The method was applied to common OpenAI Gym environments and was able to keep up with the leaderboards. Another approach, genetic programming for reinforcement learning (GPRL), is introduced by Hein et al. [15]. The method is able to extract compact algebraic equations or boolean logic terms from observed trajectories.\nHowever, GP is not the only method in the field of explainable reinforcement learning (XRL). In a recent extensive survey [25], XRL is categorized into three explainable facets: feature importance, learning process and Markov decision process, and policy-level. Among these categories, feature importance provides explanations for which features of the state affected the decision-making process of the agent. This category includes programmatic policies, where a policy is represented by a program, since programs directly use the state of the environment and thus the program can be traversed to generate an explanation [32,33,2,28]. Furthermore, other works within this category synthesize finite state machines to represent policies [16] or use decision tree-based models [29,3]. Our methodology fits into this category as we explain sub-trajectories of policies and our overarching goal in the future is to extract a program capable of representing the complete policy."}, {"title": "3 Background", "content": "In this section, we introduce the various definitions and ideas on which our work is based. We first provide a general overview of the domain-specific language and then give a detailed description of the framework proposed in [10].", "sections": [{"title": "3.1 Program and Domain-specific Language", "content": "To represent programs, this work uses a typed domain-specific language (DSL), inspired by the Lisp programming language [24], represented as a uniformly distributed probabilistic grammar, i.e., each production rule has the same probability [23]. The DSL's core components consist of control flow production rules, the available actions the agents can use and modules designed to help the agent perceive its environment. Given our focus on grid environments, the agent's perception relies on modules that determine specific positions on the grid and compare them with possible objects of the environment such as walls, empty cells, or game-specific objects. Additionally, the control flow production rules consists of if-else statements and Boolean operators, enabling the synthesis of more complex conditions. The full DSL is shown in Table 1.\nAs we use a typed DSL, we explicitly denote types for every function or constant. Constants are indicated with a single type in the type column. For functions, multiple types are linked using an arrow symbol $\\rightarrow$. The last type is the return type of the function, the preceding types are the input parameters. To minimize the runtime of the type checks, we use a specific if-production-rule for each possible type in the implementation, i.e. we have a specific if-clause that only returns actions of the agent, another that only returns game objects and so on."}, {"title": "3.2 Program Synthesis with Library Learning for Reinforcement Learning", "content": "In [10] a method is introduced to learn generalizable and interpretable knowledge in grid-based RL environments. To achieve this, the authors adapt the state-of-the-art program synthesis system DreamCoder [11] for reinforcement learning. First, RL agents are trained for a given environment to collect data to imitate. This is equivalent to sampling of the state-action pairs from the learned policy in Figure 1. After the data has been collected from the trained agents, the program synthesis part begins. This part consists of an iterative procedure which is guided by a curriculum [4] that provides the program synthesizer with the input-output examples. The curriculum is able to adapt the sequence length of the sub-trajectories to make the task more challenging and is based on the intuition that shorter sequences are easier to imitate than longer ones. Shorter sequences contain less information, so the programs are shorter and easier to synthesize.\nThe iterative procedure consists of three possible components: the search for programs for the provided state-action pairs from the curriculum which can be done with a neural network or a symbolic approach; the generation of a training dataset and the training of the neural network; a library learning module that analyzes programs that can imitate at least one sub-trajectory to extend the DSL. In [10] three different kinds of synthesizers are studied, a language model (LM) fine-tuned on code [36], a brute-force enumerative search and a neural-guided enumerative search [11]. These program synthesizers are integrated into the iterative procedure that is guided by the curriculum. The necessary steps for each method are described briefly below.\nSteps 1. For the enumerative search and the neural-guided search theses steps are necessary:\n1. Perform a brute-force search that enumerates programs in decreasing order of the probability of being generated from the probabilistic grammar until a timeout is reached or a program is found. The enumerated programs are checked if they can solve any of the provided state-action pairs."}, {"title": "3.3 Case Study: Drawbacks of previous Method", "content": "All proposed program synthesizers of [10] have several disadvantages compared to a GP algorithm for this task. Both enumerative search methods always start from scratch of the search process instead of exploiting the fact that the input-output examples are provided by the curriculum and thus share similar structures. GP takes advantage of this by making local changes and keeping the population of the last iteration as the starting population for the next iteration.\nThe greatest issue with an LM is the dataset generation. In [10], 50000 random programs are generated in each iteration and are executed in the RL environment. However, recent work has shown that 50000 random programs are far from sufficient to learn the semantics of the DSL [18]. Jin et al. showed that for a simplified DSL, without if-clauses and Boolean operators, the LM learns the semantics of a DSL, which controls an agent in a grid environment [18]. To achieve this, the authors generated one million programs for the training dataset, 20 times the size of the dataset to train CodeT5.\nIn addition, the library learning module of DreamCoder has a high memory and time requirement. These drawbacks have been mitigated by a new library learning algorithm called Stitch [5], which is about three orders of magnitude faster and requires about two orders of magnitude less memory. DreamCoder uses a deductive approach that is challenging to scale to longer programs because programs are refactored with rewrite rules, resulting in huge memory and time requirements. Stitch, on the other hand, synthesizes abstractions directly through a method called corpus-guided top-down synthesis, which uses a top-down search to constrain the search space and guide it to abstractions that cover"}]}, {"title": "4 Methodology", "content": "In this section, we present a genetic programming algorithm for explaining the decision-making process of RL agents. First, we give an overview of the general algorithm, including the mutation and crossover operators. Second, the integrated curriculum, bloat control and the library learning module are described.", "sections": [{"title": "4.1 Genetic Programming", "content": "Given that our DSL is based on Lisp, the proposed GP algorithm is tree-based and directly modifies the abstract syntax tree of Lisp [19]. Figure 2 shows the corresponding tree for Listing 1. The various operators and the fitness function for the genetic algorithm are described below:\nInitialization of Population: We initialize the population P with random programs by sampling them from the uniformly distributed grammar. Since we use a typed DSL, the programs are sampled by specifying the return type of the program. We randomly select a production rule for the given return type and then sample subtrees for the input parameters of that production rule. This is repeated until all leaves of the tree are terminals, i.e., have no input parameters.\nMutation: With a probability of $P_{mutation}$ per node, we randomly choose nodes for mutation. We then select a random production rule for the same return type as the selected node and check whether the input parameter types match. If they match, we keep the child nodes of the tree, if not, we sample new subtrees for each child parameter. This mutation process is similar to sampling random programs for initializing the population, as only the return type is specified to generate a new tree.\nCrossover: We use a one-point crossover operator where we randomly select a node from a given tree of a program with a probability of $P_{crossover}$ for each node. We then scan the other tree for the same return type and randomly select one of those nodes. Afterwards, we link the subtree to the parent of the selected node from the other tree and do it for the remaining node vice versa.\nFitness: Fitness is evaluated based on the number of correct state-action pairs that the algorithm imitates with an individual. We adapt the formula for calculating the accuracy for a set of programs from [10] to calculate the proportion of correctly solved tasks for a single program p, i.e. an individual in the population. Equation 1 calculates the fitness of an individual for the dataset D to imitate. $N_D$ is the size of D, i.e. the number of state-action pairs. A sub-trajectory from D is defined by $\\tau$, which consists of state-action pairs (s, a). Equation 2 defines $f(p, \\tau)$ that evaluates a program p and returns 1 if the complete rollout $\\tau$ can be imitated and otherwise 0. EXEC(p, s) executes the program for the states and returns an action a. Boolean values are mapped to 0 and 1 with the identity function 1.\n$Fitness = \\frac{1}{1 + N_D - \\sum_{\\tau \\in D} f(p, \\tau)}$ (1)\n$f(p, \\tau) = 1 \\{ EXEC(p, s) == a, \\forall (s, a) \\in \\tau \\}$, (2)\nis 0 after the first (p,s) where EXEC(p,s) != a\nSelection: Tournament Selection [26] is used to select individuals for mutation and crossover. First, k individuals are selected randomly and then from this smaller population, the individual with the best fitness is selected.\nCurriculum: The curriculum is integrated into the GP algorithm and increases the sequence length of the state-action pairs every 10 generations or if at least 95% of the collected data is imitated. Afterwards, the library learning module is used to analyze all correct programs. The curriculum stops when no individual in the population is able to imitate a state-action pair.\nBloat Control: Bloat control, which limits the excessive growth of the program trees in the population, and simplification of programs is an important aspect in GP to make programs interpretable and concise [17]. For this reason, we limit the growth of the program trees by integrating a bloat control into the fitness function. For this, we need an auxiliary function size, which returns the size of the input program, i.e. how many production rules the program has used. The following equation calculates the final fitness for an individual p:\n$Fitness = \\frac{1}{1 + N_D - \\sum_{\\tau \\in D} f(p,\\tau) + w_b * size(p)}$ (3)\nwhere $w_b$ is the bloat weight, which penalizes bigger programs.\nIt is also possible to enforce the simplification of programs with a more restrictive type system in the DSL, resulting in more concise programs without excessive bloat. The types of the functions can be more restricted, for example by only allowing the comparison of the direction of the agent agentDirection"}, {"title": "4.2 Library Learning", "content": "In this step, the library learning module analyzes all programs, which could solve at least one task, and extract functions from these programs to extend the DSL. Before we add a new function to the DSL, we check whether the size of the function is below a predefined value. In our experiments, we used 10 to mitigate the problem that abstractions are too specific and the search gets stuck in local minima. This is similar to pruning of decision trees [12]. In addition, the number of extracted functions is limited, as otherwise too many production rules in the DSL could hinder the search [7]. To enhance the execution speed of the extracted functions, we implement a cache that stores the return value for each input parameter configuration."}]}, {"title": "5 Experiments", "content": "In this section, we evaluate the proposed algorithm. First, the domain is explained, then the execution time and accuracy are compared with the baseline methods. Afterwards, we conduct an ablation study on the influence of library learning for genetic programming.", "sections": [{"title": "5.1 Domain", "content": "In this work, we use the grid-world domain [6] and a maze-running task with a partial observation of 5x5 from the agent's point of view. We trained an agent to collect data with the default hyperparameters provided by [27]. Imitation learning is necessary due to the full size of the maze, the partial observation of the agent and the sparse rewards that are only given if the agent finds the exit of the maze. This makes it challenging to solve this problem directly with GP. Figure 3 shows the medium-sized perfect maze environment used to collect state-action pairs for the imitation learning task. The agent's task is to find the exit of the maze. The available actions are moving forward, turn left or turn right."}, {"title": "5.2 Evaluation", "content": "We compare the runtimes and accuracy of the proposed method to the program synthesizers from [10]: enumerative search, DreamCoder, CodeT5 and LibT5, CodeT5 with a library learning module.\nIn our experiments, we use the same hyperparameters for all genetic programming runs, except for the hyperparameter whether library learning is used. The population size is 1000, the tournament size is 100, $P_{mutation}$ = 0.5, $P_{crossover}$ = 0.5 and $w_b$ = 0.025. We also limit the maximum depth of sampling random programs from the grammar to six. The number of generations is limited by the curriculum and is therefore not explicitly specified.\nRuntime Improvements To show the runtime improvements of GP in the training phase, the time is measured until the threshold for increasing the sequence length is reached. We start by imitating state-action pairs with a sequence length of three and report the runtimes for each iteration until a sequence length of ten is reached, as the times can be extrapolated to the larger sequence lengths and different sequence lengths are reached at the end depending on the method. To ensure the fairest possible comparison, each method uses the same state-action sequences to imitate and similar CPU resources. For GP, enumerative search and DreamCoder, we use four CPU cores from the AMD EPYC processor family 23 model 1. For the language models, four CPU cores from the Intel(R) Core(TM) i7-6950X CPU @ 3.00GHz family 6 model 79 in combination with a 48GB NVIDIA GeForce RTX 2080 Ti GPU are used.\nFor this experiment the number of programs for DreamCoder's library learning module is restricted to 50, as otherwise it will run out of RAM. We exclude CodeT5 and the enumerative search from the table, except for the cumulative sum of all sequence lengths, since the enumerative search is also part of DreamCoder and CodeT5 has the same runtime as LibT5, but without the library learning module. To increase the sequence length, we use the identical curriculum strategy as described in [10]. This strategy extends the sequence length when at least 10% of the state-action pairs are imitated and halts if no sequences can be imitated, for two consecutive instances.\nTable 2 compares the different program synthesizers based on their runtimes. For GP there exists no phase 1 since we do not need to train a neural network; phase 2 is the runtime of the GP algorithm introduced in section 4. Phase 1 for DreamCoder refers to steps 1 point 1 and phase 2 to steps 1 point 2 in section 3.2. Phase 1 of the language model are the steps 2 point 1 and 2; phase 2 refers to steps 2 point 3 in section 3.2. The proposed method outperforms all other methods by a large margin. We hypothesize that the runtime drop in DreamCoder for sequence length four is due to the fact that the tasks are too simple after extracting a library in the previous iteration, resulting in fast search times since the search stops when all tasks are imitated.\nIn the last row of Table 2 the corresponding cumulative sum of the runtimes for each evaluated method is shown in minutes. GP outperforms all other methods by more than an order of magnitude.\nAccuracy Figure 4 shows the accuracy of the various program synthesizers. The accuracy calculates the proportion of correctly imitated state-action pairs available for each sequence length [10]. For the genetic algorithm, the results"}]}, {"title": "6 Discussion & Limitations", "content": "In our experiments, we have shown the benefits of using genetic programming for the task of imitating sub-trajectories for an agent solving a grid-based maze environment. There are, however, various aspects we need to discuss for the different experiments.\nWhen evaluating the program synthesizers, it is difficult to make a fair comparison as they are all based on different methods. The enumerative and neural-guided search have a timeout limit of 720 seconds and thus stop after the given amount of time. The neural program synthesizers are allowed to generate 100 programs for each testing task. GP has for every sequence length a maximum of 10 generations. Table 2 shows that the specified computation budget for LMs and DreamCoder is comparable. For our GP approach, we need much less computation time and at the same time improve the results for longer sequence lengths than all other methods. Even when considering the worst-case scenario and all 10 generations are required, GP takes 136.7 minutes (19.53 min per generation on average) to enhance the sequence length, yet it remains more than 70 minutes faster than the fastest among the alternative methods. This is one of the main advantages of using GP for this type of task and shows a significant reduction in hardware requirements, resulting in more sustainable computing compared to the other methods. Unlike LMs, no GPU is necessary, which significantly reduces the environmental impact [8]. Even though it is possible to achieve human-competitive programming with LMs [21], this is limited to large amounts of available training data, which is not possible in our case since the kind of programs we want to synthesize in a customized DSL are not available anywhere online. Other related problems of LM-based synthesizers were already discussed in section 3.3.\nThe GP-based methods are not as good as DreamCoder and the enumerative search for the shorter sequences up to a length of 35. In terms of the main goal of this research, a policy extraction algorithm, this is not a problem. It is even more important to imitate longer state-action sequences, since programs for longer sequences are more reliable as they explain more of the policy. For shorter programs, it is much easier to find a program with a false explanation, i.e. focusing the attention to incorrect grid cells that were not responsible for the decision-making process of the agent. With this in mind, we successfully mitigated the drawbacks of DreamCoder with GP described in section 3.3 by reusing the population from the previous iteration, resulting in significantly longer imitated sequence lengths. Nevertheless, more work is necessary for a complete policy extraction algorithm, but we think GP is a big step forward compared to the other program synthesizers to enable rapid experimentation for further research.\nWe were able to improve the integration of library learning in GP by limiting the number of extracted functions to five or adding only functions that are concise, i.e. functions that use less than ten production rules. We hypothesize that adding too many functions steers the search into local maxima by allowing shortcuts in the search space that are hard to escape. Adding too specific functions leads to the same problem. The drop in the difference in accuracy for sequence lengths above 80 (see Figure 5) is another indication that library learning steers the search into local maxima and the more functions are extracted, the earlier this occurs. If we do not limit the number of functions at all, GP with library learning does not reach a sequence length of 45. Another possible reason for this behavior is that functions added in the first iterations for short sequences are not useful for longer sequences. This is the case if the synthesized programs provide incorrect explanations. Consequently, the question arises how the assessment of whether the explanations are correct, can be conducted. A recent study discussed these and other problems related to XAI in general [1]. Future work is imperative to assess the trustworthiness of explanations for shorter sequences."}, {"title": "6.1 Complexity of Implementation", "content": "Another advantage of this work is the complexity of the implementation, which reduces the effort required for follow-up work and for other researchers who want to use this method for other problems. Our implementation only requires Python, whereas DreamCoder uses OCaml and Python in combination. DreamCoder requires 45050 lines of code for the Python files and 12099 lines of code for the OCaml files. In contrast, the method proposed in this paper requires only 1247 lines of code in total - only 2.18% of the DreamCoder implementation, so this implementation is much easier to understand and to customize for follow-up research."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we have presented a tree-based genetic programming method to explain the decision-making process of a reinforcement learning agent. To achieve this, a typed domain-specific language is used that allows to directly modify the abstract syntax tree in the mutation and crossover operators. In addition, we have integrated learning a library of functions, that represent high-level concepts the agent has learned, into the framework. We have shown improvements in runtime and achieved better accuracy at longer sequence lengths than the baseline methods. By using fewer hardware resources than the previous methods, we have also emphasized the sustainable computing aspects of genetic programming compared to neural program synthesis.\nA promising approach for future work is to reduce the complexity of the search problem by considering not just a single grid cell of the environment, but several. This can be done by learning high-level features that are represented as usable functions in the DSL [30]. In addition, the mutation operator can be improved by training a neural network to predict the probabilities of the production rules in the grammar to guide the mutations into the most promising direction."}]}