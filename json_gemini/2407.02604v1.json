{"title": "D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions", "authors": ["Hareem Nisar", "Syed Muhammad Anwar", "Zhifan Jiang", "Abhijeet Parida", "Vishwesh Nath", "Holger R. Roth", "Marius George Linguraru"], "abstract": "Large vision language models (VLMs) have progressed incredibly from research to applicability for general-purpose use cases. LLaVA-Med, a pioneering large language and vision assistant for biomedicine, can perform multi-modal biomedical image and data analysis to provide a natural language interface for radiologists. While it is highly generalizable and works with multi-modal data, it is currently limited by well-known challenges that exist in the large language model space. Hallucinations and imprecision in responses can lead to misdiagnosis which currently hinder the clinical adaptability of VLMs. To create precise, user-friendly models in healthcare, we propose D-Rax a domain-specific, conversational, radiologic assistance tool that can be used to gain insights about a particular radiologic image. In this study, we enhance the conversational analysis of chest X-ray (CXR) images to support radiological reporting, offering comprehensive insights from medical imaging and aiding in the formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the LLaVA-Med architecture on our curated enhanced instruction-following data, comprising of images, instructions, as well as disease diagnosis and demographic predictions derived from MIMIC-CXR imaging data, CXR-related visual question answer (VQA) pairs, and predictive outcomes from multiple expert AI models. We observe statistically significant improvement in responses when evaluated for both open and close-ended conversations. Leveraging the power of state-of-the-art diagnostic models combined with VLMS, D-Rax empowers clinicians to interact with medical images using natural language, which could potentially streamline their decision-making process, enhance diagnostic accuracy, and conserve their time.", "sections": [{"title": "1 Introduction", "content": "Burnout in radiology is on the rise globally leading to chronic job dissatisfaction and critical under-staffing [4]. Radiologists routinely spend extensive time meticulously analyzing medical images to identify pathologies and diagnose diseases, which is vital in guiding treatment decisions and ensuring appropriate patient care. The retrospective"}, {"title": "2 Related Work", "content": "The introduction of foundational large VLMs has flooded the gates for the design of complex multi-modal AI tools. Flamingo [1] is one of the earliest multi-modal VLMS that bridged the gap between image-only and text-only methods. It combines prompts and multi-line chains of thought to produce sensible outcomes. Another notable example is the Large Language and Vision Assistant (LLaVA) [20] model that leverages a multi-modal architecture capable of processing both visual and textual information. Both of these VLM frameworks closely follow the technicalities from the Contrastive Language-Image Pre-training (CLIP) [22] model, which is a technique to associate images with corresponding textual descriptions. Such VLMs are widely adopted in the computer vision industry and are a gateway to many advances in biomedicine.\nIn the realm of biomedical VLMs, BioMedClip [27] is an important founda-tion model, with vision-language processing capabilities, enabling several standard biomedical imaging tasks such as classification and visual question-answering. LLaVA-Med [18], a specialized version of LLaVA, is tailored for biomedical applications, including radiology, to enable clinicians to interact with medical images in a conversational language setting, thereby facilitating more efficient radiological workflows. OphGLM [5] combined expert model deductions with large language models (LLM) by generating a diagnostic report from retinal images. Most biomedical VLMs, however, are generalized and suffer from hallucinations, inaccurate diagnosis, and imprecise question answering. A domain-specialized tool in radiology can help overcome these challenges and provide accurate outcomes."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Data", "content": "Baseline Instruction-following Data The multi-modal nature of our task requires both vision and language information. In this study, we use the MIMIC-CXR and Medical-Diff-VQA datasets to generate a baseline instruction-following dataset for our experiments. MIMIC-CXR [7,14,15] is a large open-access dataset of 377,110 CXRS with structured labels on cardiopulmonary conditions derived from 227,827 free-text radiology reports. Medical-Diff-VQA [9,10,14] is a derivative of the MIMIC-CXR dataset containing 700,703 question-answer (QA) pairs derived from CXRs. The questions are divided into seven categories: abnormality, presence, view, location, level, type, and difference. Each category can hold either open-ended questions such as 'why, what, how', etc. with dynamic natural language answers or close-ended questions such as \u2018Is there' with binary answers like \u2018yes/no'. To limit the complexity of the evaluation, we did not focus on longitudinal changes, therefore the difference QAs were removed from the current evaluation. As a result, only a single image per patient was extracted to form the test set. Table 1 summarizes the data distribution for the baseline dataset.\nEnhanced Expert Instruction-following Data We enhanced the baseline dataset by incorporating MIMIC-CXR along with QA conversations and integrating expert model predictions using pre-trained models from the TorchXRayVision [3] model"}, {"title": "3.2 Domain Specific Radiologic Assistant Design", "content": "The original LLaVA-Med model was trained on 15 million figure-caption pairs from PubMed [27]. While this teaches the model the context of biomedical application, we argue that for the sensitive process of medical imaging diagnosis, it is beneficial to develop a domain-specific VLM. Therefore, we perform end-to-end instruction tuning by training our model with CXRs and VQA-derived instructions generated from the associated radiology reports. In the process, we generated novel and enhanced instruction-following data for CXRs by incorporating predictions from expert models (Figure 1).\nNetwork Architecture: The definition of the expert VLM model follows the net-work architecture proposed in [20]. We chose Llama2 [23] as our LLM due to the availability of the pre-trained checkpoints and particularly used the Llama2-7B model. The visual encoder was kept consistent as ViT-Large/14 which is a pre-trained CLIP model. For any given input image X, and a series of question and answer defined as $(X_i, X_1, X_2, ... X_T, X_T)$. First, $X_i$ is transformed into a set of visual features $Z_u$ by the CLIP model. For training with the instruction tuning data, the visual"}, {"title": "3.3 Experiments", "content": "For visual question answers related to radiology, LLaVA-Med was finetuned and evaluated on the VQA-RAD and SLAKE datasets. However, the data used covers multiple modalities and is relatively small in size, for instance: VAQ-RAD [16] has 315 radiology images and 3,515 QA pairs, and SLAKE [19] has 642 images and 7,000 QA"}, {"title": "3.4 Evaluation", "content": "For performance evaluation, two metrics were utilized: accuracy and token recall, depending on the type of questions evaluated. For close-ended questions, the task can be considered as a classification, and hence we used accuracy. For open-ended questions, token recall measures the ratio of tokens correctly generated by the trained model according to the ground truth. Evaluating VLMs, particularly for open-ended questions is still a difficult problem and some approaches try to use OpenAI's GPT-4 to evaluate the similarity between ground truth and predicted answers [18,20]. The inference of the finetuned model required 20G of GPU memory and could generate answers for 10,000 questions per hour on a single NVIDIA H100 80G GPU."}, {"title": "4 Results", "content": "Performance of Enhanced Instruction Figure 2 shows the qualitative evaluation of D-Rax by showing an example of conversations on a given CXR, as generated by VLMs trained on basic and expert-enhanced data. The results from quantitative evaluation (Table 3) indicates that the enhanced expert instruction training allows for statistically significant improvements in the model performance for abnormality and presence questions (both open and closed-ended). Meanwhile, for location, level, and type questions, where the expert model provides no explicit information, training on both basic and enhanced data mostly yields similar performance and even showcases improvements when using the LLaVA-Med-RAD model as the base. Intriguingly, in addressing the view questions, the expert model introduces different view information but does not affect the model's capacity to derive correct answers from images and questions. Overall, expert model-enhanced instruction training enables higher performance without impeding the pre-trained model's inherent ability to comprehend queries and images."}, {"title": "5 Discussion and Conclusion", "content": "Our goal for developing D-Rax, a domain-specific expert model-guided radiologic as-sistant, is to reduce the hallucinations and improve the precision observed in responses from VLMs. We achieve this goal by establishing a novel training paradigm incorporat-ing predictions from expert models. Hence, in our target application of CXR analysis, we embed expert predictions for disease, age, race, and view with the VQA instruc-tions generated from radiological reports. Our results validate our hypothesis that (1) domain-specific knowledge, such as the use of MIMIC-CXR and Medical-Diff-VQA for CXR analysis, extracted from clinical radiology reports introduces a human factor into the model resulting in reduced hallucinations and allowing the system to provide pre-cise information; and (2) addition of expert information from SOTA AI models gener-ates statistically significant improved outcomes, enhancing accuracy of answering both open and close-ended questions in a conversation. D-Rax has the potential to enable a natural flow of diagnostic reasoning, enhance communication among clinicians, provide clear and accessible information to patients, and ultimately improve clinical care."}]}