{"title": "Handwritten Code Recognition for Pen-and-Paper CS Education", "authors": ["Md Sazzad Islam", "Christopher D. Manning", "Moussa Koulako Bala Doumbouya", "Chris Piech"], "abstract": "Teaching Computer Science (CS) by having students write programs by hand on paper has key pedagogical advantages: It allows focused learning and requires careful thinking compared to the use of Integrated Development Environments (IDEs) with intelligent support tools or \"just trying things out\". The familiar environment of pens and paper also lessens the cognitive load of students with no prior experience with computers, for whom the mere basic usage of computers can be intimidating. Finally, this teaching approach opens learning opportunities to students with limited access to computers. However, a key obstacle is the current lack of teaching methods and support software for working with and running handwritten programs. Optical character recognition (OCR) of handwritten code is challenging: Minor OCR errors, perhaps due to varied handwriting styles, easily make code not run, and recognizing indentation is crucial for languages like Python but is difficult to do due to inconsistent horizontal spacing in handwriting. Our approach integrates two innovative methods. The first combines OCR with an indentation recognition module and a language model designed for post-OCR error correction without introducing hallucinations. This method, to our knowledge, surpasses all existing systems in handwritten code recognition. It reduces error from 30% in the state of the art to 5% with minimal hallucination of logical fixes to student programs. The second method leverages a multimodal language model to recognize handwritten programs in an end-to-end fashion. We hope this contribution can stimulate further pedagogical research and contribute to the goal of making CS education universally accessible. We release a dataset of handwritten programs and code to support future research 1.", "sections": [{"title": "1 INTRODUCTION", "content": "Handwriting-based computer programming teaching tools have the potential to increase the accessibility and effectiveness of elementary CS education programs. However, to date, there aren't any usable software tools to support this pedagogy.\nHandwriting-based elementary CS curricula, in contrast to those based on the use of integrated development environments (IDE), are advantageous in several ways. First, they reduce children's exposure to screens, which has been linked to adverse outcomes [21, 32]. Second, they offer a more familiar and less intimidating learning environment for students with no prior experience with computers and teachers with limited experience in computer science [7]. Third, they allow students to become more intimately familiar with code syntax as handwriting is more optimal for learning [30], particularly for young children [26]. Finally, they are less costly to implement and accessible to most schools and students.\nHowever, to be effective, such curricula require a handwritten code recognition and execution tool that allows the student to quickly execute and test their handwritten programs. Automated optical handwritten code recognition is challenging because of variations in individual student handwriting features, which include character shapes, line slants, and vertical spacings [31]. For programming languages in which indentations are lexemes, such as Python, variations in the horizontal width of indentation units pose additional recognition challenges.\nIn this work, we compare various approaches to addressing the above challenges. On the one hand, we employ modular systems that include distinct modules for optical character recognition, discrete indentation level recognition, and language-model-based post-correction. On the other hand, we employ a multimodal language model that performs handwritten code recognition in an end-to-end fashion. We discuss the successes and limitations of the approaches we tried, including tradeoffs between recognition fidelity and language model-induced hallucinations.\nThe main contributions of this paper are:\n(1) Provide two first-of-their-kind public benchmark datasets for handwritten student-code recognition and a methodology for measuring the correctness of a given recognition method.\n(2) Contribute two novel methods of indentation recognition for handwritten Python code.\n(3) Provide a novel methodology of incorporating OCR for handwritten student code with LLMs to achieve a 5% error rate, an improvement from the state-of-the-art 30% error."}, {"title": "1.1 Related Work", "content": "Over the years, several attempts have been made to address the challenges of making computer science education more accessible. Additionally, recognizing and digitizing handwritten texts, particularly code, has been a subject of study with its unique challenges.\nComputer Science Education without Computers. Several initiatives have explored the teaching of computer science principles without the need for physical computers. Approaches such as the popular CS Unplugged [4, 5] curriculum use off-computer activities to teach core concepts [17, 33]. This is especially the case for young children [28]. However, the lack of an execution environment remains a challenge.\nHandwriting Recognition and Digitization. Optical Character Recognition (OCR) systems have been worked on since as early as 1912 when Emanuel Goldberg presented a machine that could convert characters into telegraph code [14] and 1913 when Dr. Edmund Fournier d'Albe created the Optophone [11]. OCR became a well-studied area for computing in the 1980s [23, 24] where much of the focus was on identifying computer-printed characters. OCR for handwritten characters is an especially difficult OCR task [2, 22]. The introduction of the famous MNIST dataset [9] in 2013 became a popular application of deep learning algorithms which in turn led to a sharp rise in accuracy when recognizing single handwritten characters [16]. Since then OCR of handwriting has been applied to multiple languages [18] as well as ancient scripts [25]. However, until 2020 long form, OCR remained relatively inaccurate, unless an algorithm was given a large sample of a particular person's handwriting. Integrating Large Language Models (LLMs) [1, 6] with OCR for post-correction has created excitement and new advances in accuracy for OCR [19] especially using large multi-modal models such as GPT-4V [15, 29]. These models have been recently developed and to the best of our knowledge have not been incorporated into state-of-the-art available OCR tools. While they are promising for OCR in general, the use of LLMs needs intentional study in the context of student code, as LLMs may start solving student coding tasks while performing OCR.\nOCR for Student Code. While there has been a lot of work on OCR in general, and a small amount of work on OCR applied to recognizing computer printed code [20] there is a surprising lack of work on handwritten code, let alone student handwritten code. The problem was first introduced in 2012 in the paper CodeRunner [12]. Perhaps because of a lack of a public dataset, little progress has been made. A project, Handwritten Code Scanner from 2021 could not progress because of how surprisingly hard it was to get standard handwritten OCR packages to work for student code [34]."}, {"title": "2 THE STUDENT-CODE OCR CHALLENGE", "content": "The Student-Code Optical Character Recognition (OCR) challenge is to convert images of handwritten student code into a digital string representation. This process presents a significant challenge, particularly for languages like Python where indentation plays a crucial semantic role. The advent of Large Language Models (LLMs) for post-processing has ushered in an exciting era for OCR technology, enhancing its effectiveness and accuracy. However, the transcription of student code introduces distinct challenges. It is imperative that the OCR process avoids introducing or \"hallucinating\" logical corrections or fixes. This is true regardless of whether the OCR is conducted by teachers for assessment purposes or by students during their learning process.\nWe contribute two novel benchmark datasets as well as evaluation methodology. The Correct Student Dataset is created by 40 real students in, Code In Place [27], an international, free, online intro to Python course. The students wrote answers to a provided task. The Python coding tasks included grid-world, console, and graphics challenges from the introductory computer science course CS-106A, at Stanford. We augment the dataset with the Logical Error Dataset that has a range of mistakes that you would expect to see in an introduction to coding class. While some of the images in the Logical Error Dataset were from real students, the majority were written by the authors. We open-source both datasets. Using these two datasets we can then run both the Edit Distance Test to measure how accurately a student-code OCR algorithm captures what the student had written as well as the Logical Fix Hallucination Test to measure how many corrections the OCR method injects."}, {"title": "2.1 Measuring OCR Error", "content": "Understanding how closely the digitalization from a Student-Code OCR algorithm approximates the true gold label digitalization is crucial. For the 55 programs in both the Correct Student Dataset and the Logical Error Dataset combined, we measure the average Levenshtein distance, normalized by the length of the student programs. The Levenshtein distance quantifies the minimal number of single-character edits (i.e., insertions, deletions, or substitutions) required to change the predicted student program into the annotation of what they actually meant to write [35]. Lower values of the edit distance indicate that the algorithm's output is closer to what the student intended. By normalizing the Levenshtein distance by the number of characters in the test we have a metric that is more interpretable. It is approximately, the % of characters for which the algorithm made a mistake. Mathematically, the normalized Levenshtein distance ($L_{norm}$) is defined as follows:\n$L_{norm} = \\frac{L(str_1, str_2)}{|str_1|} \\times 100\\%$\nwhere: L(str1, str2) is the Levenshtein distance between the ground truth string str\u2081 and the OCR output string str2. |str1| denotes the length of the ground truth string. The OCR Error metric is the average Lnorm across all student handwritten programs."}, {"title": "2.2 Logical Fix Hallucination Test", "content": "One of the primary concerns in integrating AI systems that possess vast knowledge of coding is the potential risk of the system inadvertently providing answers to the students. Take, for example, a scenario wherein a student is attempting to write a solution to the Fibonacci sequence, a rather standard programming task. An AI system, especially one equipped with a language model, might unintentionally rectify conceptual errors in the student's code, leading to what we term as \"logical fix hallucination\". This not only risks undermining the student's learning process but also doesn't represent a good faith translation of the student's original intention. To safeguard against this and to accurately measure the extent of solution hallucination, a robust testing method is essential. In response to this need, we introduced the Logical Error Dataset. This dataset comprises a curated selection of typical student errors, thereby serving as a benchmark to determine whether the AI recognition algorithm tends to hallucinate fixes.\nThe Logical Error Dataset includes 11 programs. Each program has an image of the program handwritten, the correct digitalization, and the error description (see Table 4 for description). The dataset contains a spectrum of errors such as Fence Post Errors, Arithmetic Errors, Control Flow Errors, and Scope Errors. These errors focus on logical, or semantic, errors rather than syntactic errors (such as misspelling a variable or forgetting a colon). This is for a pedagogical reason: syntactic issues are often ones that a teacher may teach by showing the correction. Logical errors are ones where the teacher may want the student to find the solution on their own."}, {"title": "3 METHODS", "content": "We consider two methods for OCR of student code: An algorithm that uses LLMs as post-processing as well as a multimodal LLM.\nIn the former algorithm, we decompose the task into three phases. (1) In the first phase we apply an off-the-shelf OCR method, such as Azure or Google OCR. (2) In the second phase we apply an Indentation Recognition algorithm and (3) in the third and final phase we use an LLM for Post Correction."}, {"title": "3.1 Initial OCR", "content": "The initial phase involved digitizing the handwritten code, for which we employed four leading Optical Character Recognition (OCR) technologies: Google Cloud Vision, Microsoft Azure OCR, AWS Textract, and MathPix. These platforms established our baseline for accuracy assessment. As demonstrated in Table 2, the error rates were significantly high, underscoring the challenges of OCR in the given context. Minor typographical errors, inherent to the OCR process, rendered the codes non-executable, highlighting the critical need for precise transcription in coding applications.\nThis observation prompted further investigation into two key areas: recognition of indentation patterns, essential for understanding code structure, and the development of a post-correction mechanism to rectify OCR-induced errors. Given the comparative analysis, Microsoft Azure OCR was selected for subsequent phases due to its superior accuracy among the evaluated platforms."}, {"title": "3.2 Indentation Recognition", "content": "Indentation is a critical part of OCR for handwritten Python code. In Python indentation has semantic meaning. However, raw OCR results do not preserve the indentation structure intrinsic to the ground truth. The OCR data does includes bounding box coordinates for each line of text, providing spatial information that could be leveraged to deduce indentation levels. We utilized this spatial data through two different methods to ascertain the indentation level for each line of code.\nAbsolute Indentation Clustering. Indentation recognition in handwritten code can be formalized as a clustering challenge, predicated on the assumption that lines with similar indentations would align at comparable horizontal start points. The main task is to cluster the x-coordinate of the top-left point of each line's bounding box, as identified by OCR, to identify distinct indentation levels. This effectively transforms the problem into a straightforward one-dimensional clustering task. We used Meanshift clustering as it does not require a predefined number of clusters [8, 10]\nMeanshift has a bandwidth hyperparameter, which defines the range of influence of each cluster centroid. Across our datasets, no bandwidth worked well for all images. To address this variability, we devised an adaptive bandwidth estimation formula: Estimated Bandwidth = 1.5 x $\\frac{1}{N} \\sum_{i=1}^{N} h_i$, where hi represents the height of the ith bounding box in the OCR output, and N is the total number of bounding boxes.\nRelative Indentation Clustering. We hypothesize that when students write code, the horizontal spacing of each line is influenced by the spatial position of the immediately preceding lines. We propose a method that uses the relative difference between lines.\nIn our relative indentation approach, we translate the OCR bounding box outputs into \"deltas\" between each line's minimum \"x\" coordinate (See Figure 2, left side). So, for an image with n lines of code, we will have n - 1 deltas. We normalize the deltas using the image's width. We observe from visual inspection that using these deltas, it is much easier to differentiate between indentation and no indentation. See Figure 2. If we consolidate the deltas with positive values we notice that some positive delta values are large, corresponding to an indentation. Others are close to zero, corresponding to no indent.\nThe relative indentation method separates deltas into ones with positive and negative values. Among the lines with positive indentation, we classify the deltas as either being \"single indent\" or \"no indent\". Among lines with a negative delta, we search for the nearest ancestor. See Algorithm 1 for details.\nTo classify between indent and no-indent, we model the positive deltas using a Gaussian Mixture Model (GMM). The GMM has two Gaussians, one to represent the deltas for Indent and one to represent deltas for No-Indent:\n$D_{no-indent} \\sim N(\\mu_1, \\sigma_1^2)$\n$D_{indent} \\sim N(\\mu_2, \\sigma_2^2)$\nOur apriori assumption is that each Gaussian is equally likely. Formally, that is equivalent to setting the mixture parameter \u03c4 = 0.5. As such, to train our model we simply need to estimate the mean (\u03bc) and variance (\u03c3) of the two Gaussians. We estimate the four parameters using LOOCV on a subset of manually annotated images from our data. We fit the hyper-parameters of the GMM using Maximum Likelihood Estimation (MLE) from the labelled data. This produces the following estimates for the hyper-parameters:"}, {"title": "3.3 Language Model-Based Post-Correction", "content": "The tiniest of textual errors introduced by OCR can render a code un-executable. As the last part of our method, we use a language model to fix the errors in transcription introduced by the OCR. This module receives input directly from the Indentation Recognition Module, with the explicit aim of correcting only the typographical errors without altering the code's logical structure or indentation. A significant challenge in this phase is mitigating the \"hallucination\" effect commonly observed in large language models-unintended alterations such as indentation changes, unwarranted corrections of logical errors, or random modifications. Our objective was to minimize the edit distance, ensuring no deviation from the original text. We explored two distinct approaches.\nSimple Prompting Approach. The initial strategy involved straightforwardly feeding the output from the Indentation Recognition Module to the language model. This approach significantly reduces errors but is susceptible to a hallucination-based logical fix. The challenge was engineering a prompt that maintained a low error rate while minimizing hallucinations. We found it particularly important to include direct messaging such as: \"*VERY STRICT RULE* - Do not fix any logical, or numerical error of the original code. - Do not fix any indentation of the original code.\"\nChain-of-Thought (CoT) Prompting Approach. To further reduce hallucination, especially regarding logical corrections and indentation changes, we adopted a more nuanced, three-step Chain-of-Thought Prompting Approach. The first step involves prompting the language model to correct spelling errors potentially introduced by the OCR system. Subsequent steps assume that the model might inadvertently correct logical errors or alter indentation; hence, specific instructions are provided to revert any such changes. This method proved highly effective and eliminated all hallucinatory logical corrections in our dataset. However, we observed a slightly higher OCR error rate in this case compared to Simple Prompting."}, {"title": "3.4 Multi-Modal Handwritten OCR", "content": "Large multi-modal models offer robust text transcription directly from images, enabling an end-to-end transcription process that mitigates the need for intermediate steps like our three-stage approach. Leveraging GPT-4-Vision-Preview, we implemented an end-to-end OCR process that directly transcribes text from images without the need for segmented preprocessing steps like the aforementioned method. This method involves directly feeding images of handwritten code, with a prompt into the model. The model then applies its capabilities to recognize and transcribe text. The key to this approach is a carefully crafted prompt that guides the model to focus strictly on transcription while avoiding the introduction of errors typical of automated recognition systems. The specific prompt used, instructs the model to strictly adhere to the text as it appears in the image, emphasizing accuracy and fidelity to the source without attempting to correct or interpret the code logic or structure."}, {"title": "4 RESULTS", "content": "The off-the-shelf, state-of-the-art algorithms performed quite poorly on the OCR challenge for handwritten student code. Among the commercial solutions, Azure was the best performing with an average OCR Error (normalized Levenshtein distance) of 30.2 \u00b1 1.8. As an aside, while MathPix had a high error rate, that was mainly due to its representation of code as LaTeX. It was still useful when combined with LLM prompting, though not as accurate as Azure.\nBy employing our algorithms to correct indentation and by using LLM post-correction we were able to decrease OCR error significantly. The best-performing algorithms used our GMM-based relative indentation correction and LLM post-correction. The two methods of post-correction had different advantages. The \"Simple\" method achieved the lowest OCR error (5.3 \u00b1 0.9). While the Chain of Thought post-correction technique had a higher OCR error (8.5 \u00b1 1.0) it produced 0 logical fixes, compared \"Simple\" which fixed 9% of the errors in the Logical Error Dataset. See Table 2.\nMulti Modal Results: The large Multi-Modal GPT-4V(ision) achieved notable success, registering an average OCR Error of 6.0 \u00b1 0.8 while only introducing logical fixes in 5% of the Logical Error Dataset. These results are promising and appear to approach the Pareto frontier between the two post-correction methods.\nIndentation Algorithm Results: Both the absolute algorithm (Mean Shift) and the relative algorithm (GMM-based) improved OCR error rates. The relative indentation algorithm had the best results and qualitatively seemed to make very few indentation errors. This was especially important for \"Grid World\" programs [3] where the LLM would not have been able to infer the indentation level simply from the code. The Mean Shift was accurate but would make mistakes, especially on longer programs. Lines that, to a human observer, would belong to one indentation level might be situated closer on the x-axis to a denser cluster associated with a different indentation level. Such discrepancies highlighted the limitations of absolute indentation clustering in contexts where local data characteristics may offer a more intuitive guide to cluster membership than global data density. The four hyper-parameters for the Relative indentation algorithm were set using a subset of 16 images. Even though there are only four hyper-parameters, those values might have overfit the indentation statistics of those 16 images. To make sure that the indentation results are valid, we rerun all of the results on the 39 images that were not used to set the hyper-parameters. In this \"heldout set\" we observe that Relative indentation recognition has a similar success as on the full dataset. It performs just as well both before and after the post-correction (See Table 3 in the appendix for the full results). This gives us confidence that the four hyper-parameters did not allow the relative indentation algorithm to overfit the data."}, {"title": "4.1 Qualitative Analysis of Language Model Hallucinations, and Logical Fixes", "content": "The prompting techniques that we used did not induce substantial hallucinations of logical fixes. The best-performing algorithm in terms of OCR error (Azure + Relative + Simple) had a single non-indentation logical fix out of the 11 programs in the Logical Error Dataset. This fix can be seen in Figure 7.\nOn line 4 the student wrote if number / 2 != 0 but the LLM corrected it to be if number % 2 != 0. Note that the algorithm changed the incorrect division to be a mod \u2013 possibly giving away part of the answer to the student. For this particular example, the Chain of Thought prompting did not create the logical fix, however, it incorrectly translated the division as a 1 (carrying forward a mistake from Azure). For some learning environments, this type of logical fix may be more tolerable than others. There are many examples where the simple OCR algorithm managed to faithfully translate the student's code while still maintaining the logical errors. One example can be seen in Figure 5. In the identify leap year function the student included the incorrect logic if (Year % 4 == 0) or (Year % 100 == 0) or (year % 400 == 0). It should have been if (Year % 4 == 0) and (Year % 100 != 0) or (Year % 400 == 0). Even though the LLM certainly would be able to produce the correct code, it did not fix the student's mistake."}, {"title": "5 LIMITATIONS", "content": "5.1 More comprehensive evaluation datasets\nOne of the contributions of this paper is the first (to the best of our knowledge) public datasets of handwritten student codes. However, we hope that for future work we can substantially increase the size of our public dataset. 55 programs is enough to understand high-level differences between algorithms, now that future work will concentrate on more subtle improvements (from 5% error rate towards 0%) it will be important to increase the size of this dataset. Similarly, it will be important to fix the Logical Error Dataset to encompass a broader set of the sorts of logical errors that students could make while programming. It is surprisingly hard to get handwritten student code which is free to share. Now that we have a useful solution, we hope to be able to collect orders of magnitude more data (with student consent, of course)."}, {"title": "5.2 The Iterative Editing Challenge", "content": "In this paper, we focused on the context of having a single photo that needs to be digitized into code. However, in a natural learning environment, we imagine that students will also need a user interface that supports iterative work. A potential interaction with our OCR system could unfold as follows: a student writes code by hand, digitizes it for execution, and corrects a syntax error on the computer, but then needs to revisit the handwritten code for conceptual adjustments. This process raises a question: does the student rewrite the entire code? There's an undeniable need for a more sophisticated approach to such iterative editing. One default solution would be for students to maintain significant whitespace between lines, facilitating subsequent code insertions. Another solution is for students to take photos of different subsets of their code. If they need to edit a part of their code, they would only need to replace the corresponding photo."}, {"title": "5.3 Handling Crossed-Out Code", "content": "As shown in the example in Figure 1, our system is able to handle basic \"crossing out\" of code. We have observed that our system handles crossed-out codes with notable accuracy. However, crossed-out code can become arbitrarily hard. Students writing code, without concern for the OCR system could use annotations that our OCR system is not able to handle. One example of this would be the use of arrows to indicate that a block of code should be inserted somewhere in the codebase. One potential solution is to set expectations for students that they need to keep their code as clean as possible. However, we note that there is great promise that the multi-modal systems, such as GPT4V may be especially adept at handling these sorts of annotations."}, {"title": "5.4 Applicability to Larger Programs", "content": "The longest programs that we tested in this paper were on the order of 40 lines long. The scalability of our approach to longer programs remains a topic of inquiry. The threshold beyond which our method might be less effective or feasible for learners is yet to be established. For longer programs it seems reasonable to have the student take several photos of their code. While this approach complicates indentation recognition, achieving consistent indentation recognition across multiple photos appears to be a solvable issue."}, {"title": "5.5 Offline Mode", "content": "Our system has three distinct modules, which could be implemented in various ways: (1) OCR module, (2) Indentation Recognition Module, and (3) Language model used for post-correction. In the offline embodiment of our system, which we will explore in future work, the OCR module and the language model used for post-correction are executed on a local device, reducing the usage cost (see Appendix B), and removing the need for internet connectivity."}, {"title": "6 DISCUSSION", "content": "Our research into the digitization of handwritten code, utilizing a symbiotic approach that combines Optical Character Recognition (OCR) with Large Language Models (LLMs), has culminated in both noteworthy outcomes and a comprehensive understanding of the prevailing challenges in this domain. This initiative primarily aimed to augment computer science education in regions where computer accessibility is restricted."}, {"title": "6.1 CS Education with Limited Digital Resources and Minimal Distractions", "content": "Handwritten code recognition facilitates computer science education without the need for prolonged access to computers, which is particularly beneficial in two educational settings. In the first setting, students do not have personal computers, and their schools lack the funding to acquire and maintain computer labs. With our solution, a classroom could perform programming activities using only a single shared mobile device to scan and execute handwritten student code. In the second setting, to mitigate the adverse effects of prolonged screen exposure, teachers may intentionally limit young students' access to computers. Instead, they encourage activities performed on paper with pens. In this scenario, as in the first, students can meticulously craft their programs on paper and then use the classroom's shared code execution device. Handwritten code recognition thus provides a familiar, distraction-free way for students to engage with algorithms and programming. Additionally, this method fosters reflection, review, and precision, thereby promoting deliberate learning. These experiences instill qualities beyond mere coding skills. By embracing accessible methodologies, educators worldwide can prioritize comprehension and problem-solving skills, empowering students to achieve a deeper understanding of computer science concepts."}, {"title": "6.2 Application in Grading Handwritten Exams", "content": "In the current academic landscape, digital exams are susceptible to misconduct, especially with the advent of sophisticated LLMs like ChatGPT. Many educational institutions, from high schools to universities, resort to handwritten exams. These exams, while reducing cheating opportunities, are labor-intensive to grade. Additionally, the potential introduction of gender bias during grading-stemming from handwriting perception-is a concern. By digitizing and accurately recognizing text from these handwritten exams, automated unit tests can be employed. This not only expedites the grading process but also may diminish human-induced grading variance. However, to maintain academic integrity, it's paramount that the digitization process avoids any form of solution hallucination."}, {"title": "6.3 OCR of Grid Based Coding", "content": "In an early stage of our research, we considered whether we could develop a method that would make zero errors. One creative solution was to have students write their code on graph paper in a grid-based representation of their code. Grid-based coding has been proposed for accessible CS education in the context of low vision and blind programmers [13]. OCR of a handwritten grid should be substantially easier than handwritten code. Clearly, this introduces an extra level of work for the student. However, we hypothesized that this extra work could have deep pedagogical benefits. Learning to represent one's code in a binary representation would expose students to some valuable lessons. However, as we realized that OCR for students directly writing Python would be so accurate we did not fully explore this interesting direction."}, {"title": "7 CONCLUSION", "content": "We believe that the ability to digitize handwritten student code could have transformative potential for learning coding at scale. It will be especially useful for increasing accessibility to students who don't have or want constant access to a computer. In this paper, we introduce a novel methodology for indentation recognition as well as the first application of LLMs to the task of student code OCR. We contribute two novel datasets for student-code digitalization labelled with the indented code, and including deliberate errors. The tool we have developed is accurate enough to be used by students in a real learning environment. We plan to deploy this research in classrooms in Guinea where the project originated and Bangladesh, as well as in a massive online coding class."}, {"title": "A BENCHMARKS", "content": "This is the result when we filtered the 16 images we trained it on."}, {"title": "B COST CALCULATION BREAKDOWN", "content": "This section provides a breakdown of the cost calculations for the GPT-4(Vision) and our Azure + Relative Indentation + Simple. The cost analysis is based on the February 2024 pricing for the respective services:\n\u2022 Azure OCR: $0.001 per image\n\u2022 GPT-4-0613 Text Processing:\nInput: $0.03 per 1,000 tokens\nOutput: $0.06 per 1,000 tokens\n\u2022 GPT-4-Vision-Preview:\nImage: $0.00765 (over 768px by 768px)\nInput: $0.01 per 1,000 tokens\nOutput: $0.03 per 1,000 tokens\nOur pipeline integrates Azure OCR and GPT-4 text processing, with the costs broken down as follows:\n\u2022 Azure OCR Cost: $0.001 per image\n\u2022 GPT-4 Text Processing Cost: Calculated based on the average token count per image\nIn our dataset, the handwritten codes had an average of 320.2545 characters, which roughly equates to 80.0636 tokens (assuming an average of 4 characters per token).\nThe instructions comprised 381 characters, contributing to nearly 95.25 tokens. In contrast, the output codes held an average of 341.5455 characters, resulting in around 85.3863 tokens.\nThe cost for text processing per image is thus calculated as:\nPrice = $\\frac{(80.0636 + 95.25) \\times 0.03 + 85.3863 \\times 0.06}{1000} = $0.01038\nSumming the costs for Azure OCR and GPT-4 text processing yields our total pipeline cost:\nTotal Pipeline Cost = $0.001 + $0.01038 = $0.01138 per image\nThe GPT-4(Vision) model incurs a cost of $0.00765 per image for all images over 786x786 size, which is standalone and does not require additional text processing costs.\nThe instructional requirement for each image is quantified as 387 characters, which translates to roughly 96.75 tokens at an approximate ratio. The output codes held an average of 308.9636 characters, approximately resulting in around 77.2409 tokens.\nThe cost per image is thus calculated as:\nPrice = $\\frac{96.75 \\times 0.01 + 77.2409 \\times 0.03}{1000} + 0.00765 = $0.0111864\nComparing the costs:\n\u2022 Our Pipeline Cost: $0.01138 per image\n\u2022 GPT-4(Vision) Cost: $0.01094 per image"}, {"title": "C DETAILS OF LOGICAL ERROR DATASET"}, {"title": "D ADDITIONAL EXAMPLES"}]}