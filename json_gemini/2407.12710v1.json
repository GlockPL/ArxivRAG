{"title": "A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems", "authors": ["Mohammad-Amin Charusaie", "Samira Samadi"], "abstract": "Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using ad-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of baselines.", "sections": [{"title": "1 Introduction", "content": "Machine learning algorithms are increasingly used in diverse fields, including critical applications, such as medical diagnostics [68] and predicting optimal prognostics [60]. To address the sensitivity of such tasks, existing approaches suggest keeping the human expert in the loop and using the machine learning prediction as advice [32], or playing a supportive role by taking over the tasks on which machine learning is uncertain [36, 57, 4]. The abstention of the classifier in making decisions, and letting the human expert do so, is where the paradigm of learn-to-defer (L2D) started to exist.\n\nThe development of L2D algorithms has mainly revolved around optimizing the accuracy of the final system under such paradigm [57, 47]. Although they achieve better accuracy than either the machine learning algorithm or the human expert in isolation, these works provide inherently single-objective solutions to the L2D problem. In the critical tasks that are mentioned earlier, more often than not, we face a challenging multi-objective problem of ensuring the safety, algorithmic fairness, and practicality of the final solution. In such settings, we seek to limit the cost of incorrect decisions [43], algorithmic biases [11], or human expert intervention [54], while optimizing the accuracy of the system. Although the seminal paper that introduced the first L2D algorithm targeted an instance of such multi-objective problem [41], a general solution to such class of problems, besides specific examples [23, 54, 48, 49], has remained unknown to date.\n\nMulti-objective machine learning extends beyond the realm of L2D problems. A prime example that is extensively studied in various settings is ensuring algorithmic fairness [16] while optimizing accuracy. Recent advances in the algorithmic fairness literature have suggested the superiority of post-processing methodology for tackling this multi-objective problem [69, 12, 18, 72]. Post-processing algorithms operate in two steps: first, they find a calibrated estimation of a set of probability scores"}, {"title": "2 Related Works", "content": "Human and ML's collaboration in decision-making has been demonstrated to enhance the accuracy of final decisions compared to predictions that are made solely by humans or ML [34, 65]. This overperformance is due to the ability to estimate the accuracy and confidence of each agent on different regions of data and subsequently allocate instances between human and ML to optimize the overall accuracy [2]. Since the introduction of the L2D problem, the implementation of its optimal rule has been the focus of interest in this field [7, 47, 10, 48, 8, 40, 45, 42]. The multi-objective classification with abstention problems is studied for specific objectives in [41, 54, 45] via in-processing methods. The application of Neyman-Pearson lemma for learning problems with fairness criteria is recently introduced in [71].\n\nWe refer the reader to Appendix B for further discussion on related works."}, {"title": "3 Problem Setting", "content": "Assume that we are given input features $x_i \\in \\mathcal{X}$, corresponding labels $y_i \\in \\mathcal{Y} = \\{1, ..., L\\}$, and the human expert decision $m_i$ for such input, and assume that these are i.i.d. realizations of random variables $X, Y, M \\sim \\mu = \\mu_{XYM}$. Since there exists randomness in the human decision-making process, for the sake of generality, we treat $M$ as a random variable similar to $Y$ and do not assume that $m_i = m(x_i)$ for some function $m$. Further, assume that for the true label $y$ and a certain feature vector $x$, the cost of incorrect predictions is measured by a loss function $l_{AI}(y, h(x))$ for the classifier prediction $h(x)$, and a loss function $l_{H}(y, m)$ for human's prediction $m$. The question that we tackle in this paper is the following: What is an optimal classifier and otherwise an optimal way of deferring the decision to the human when there are constraints that limit the decision-making? The constraints above can be algorithmic fairness constraints (e.g., demographic parity, equality of opportunity, equalized odds), expert intervention constraints (e.g., when the human expert can classify up to $b$ proportion of the data), or spatial constraints to enforce deferral on certain inputs, or any combination thereof."}, {"title": "4 d-dimensional Generalization of Neyman-Pearson Lemma", "content": "The idea behind minimizing an expected error while keeping another expected error bounded is naturally related to the problem that is designed by Neyman and Pearson [52]. They consider two hypotheses $H_0, H_1$ as two distributions with density functions $g_0(x)$ and $g_1(x)$ for which a given point $x$ can be drawn. Then, they maximize the probability of correctly rejecting $H_0$, while bounding the probability of incorrectly rejecting $H_0$, i.e., for a test $T(x) \\in [0, 1]$ that rejects the null hypothesis when $T(x) = 1$, they solved the problem\n\n$\\displaystyle \\max_{T \\in [0,1]^\\mathcal{X}} E_{x \\sim g_1} [T(X)], \\text{ s.t. } E_{x \\sim g_0} [T(X)] \\leq \\alpha$. (5)\n\nThey concluded that thresholding the likelihood ratio is a solution to the above problem. Formally, they show that all optimal hypothesis tests take the value $T(x) = 1$ when $g_1(x)/g_0(x) > k$ and take the value $T(x) = 0$ when $g_1(x)/g_0(x) < k$, where $k$ is a scalar and dependent on $\\alpha$.\n\nIn this section, we aim to solve (3) as a generalization of Neyman-Pearson lemma for binary testing to the case of multi-hypothesis testing, in which correctly and incorrectly rejecting each hypothesis has a certain reward and loss. To clarify how the extension of this setting and the problem (3) are equivalent, assume the general case of $d$ hypotheses $H_0, ..., H_{d-1}$, each of which corresponding to $X$ being drawn from the density function $g_i(x)$ for $i \\in \\{0, ..., d-1\\}$. Further, assume that for each hypothesis $H_i$, in case of true positive, we receive the reward $r_i(x)$, and in case of false negative, we receive the loss $l_i(x)$. Assume that we aim to find a test $f : \\mathcal{X} \\rightarrow \\Delta_d$ that for each input $x \\in \\mathcal{X}$ rejects $d-1$ hypotheses, each hypothesis $H_i$ with probability $1 - f^i(x)$ and maximizes a sum of true positive rewards, and that keeps the sum of false negative losses under control. Then, this is equivalent to \\(\\displaystyle \\arg\\max_{f \\in \\mathcal{A}} \\sum_{i=0}^{d-1} E_{x \\sim g_i} [f^i(x)r_i(x)]\\) subjected to\n\n$\\displaystyle \\sum_{i=0}^{d-1} E_{x \\sim g_i} [(1 - f^i(x))l_i(x)] \\leq \\delta_1$ which in turns is equivalent to\n\n$\\displaystyle \\arg\\max_{f \\in \\mathcal{A}} E_{x \\sim g_0} [f^i(x) \\frac{r_i(x) g_i(x)}{g_0(x)}] \\text{ s.t. } E_{x \\sim g_0} [f^i(x) \\frac{l_i(x) g_i(x)}{g_0(x)}] \\leq \\delta_1. (6)$\n\nThis problem can be seen as instance of (3), when we set $\\psi_0(x) = [r_0(x) \\frac{g_0(x)}{g_0(x)}, ..., r_{d-1}(x) \\frac{g_{d-1}(x)}{g_0(x)}]$ and $\\psi_1(x) = [\\sum_{j \\neq 0} l_j(x) \\frac{g_j(x)}{g_0(x)}, ..., \\sum_{j \\neq d-1} l_j(x) \\frac{g_j(x)}{g_0(x)}]$. Similarly, we can show that for all $\\psi_0(x), \\psi_1(x)$ in (3) there exists a set of densities $g_1(x), ..., g_{d-1}(x)$ and rewards and losses such that (6) and (3) are equivalent. This can be done by setting $g_i = g_0$ and noting that the mapping from $l_i$'s and $r_i$'s into $\\mathcal{V}_0$ and $\\mathcal{V}_2$ is invertible.\n\nThe formulation of (3) can be seen as an extension of the setting in [66] when we move beyond type-k error bounds to a general set of constraints. That work achieves the optimal test by applying strong duality on the Lagrangian form of the constrained optimization problem. However, we avoided using this approach in proving our solution, since finding $f^*$, and not the optimal objective, is possible via strong duality only when we know apriori that the Lagrangian has a single saddle point (for more details and fallacy of such approach, see Section E). As another improvement to the duality method, we not only find a solution to (3), but also show that there is no other solution that works as well as ours.\n\nBefore we express our solution in the following theorem, we define an import notation as an extension of the argmax function that helps us articulate the optimal predictor. In fact, we define\n\n$\\displaystyle T_a = \\{ r : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\Delta_d \\big|  \\sum_{i:x_i=\\max\\{x_1,...,x_d\\}} (\\tau(x_1,x_2))(i) = 1\\}$ (7)\n\nthat is a set of functions that result in one-hot encoded argmax when there is a clear maximum, and otherwise, based on its second argument, results in a probability distribution on all components that achieved the maximum value.\n\n$\\bf{Theorem 4.1 (d-GNP).}$ For a set of functions $\\psi_i$ where $i \\in [0, m]$, assume that $(\\delta_1, ..., \\delta_m)$ is an interior point of the set $F = \\{(\\mathbb{E}[\\langle r(x), \\psi_1(x) \\rangle], ..., \\mathbb{E}[\\langle r(x), \\psi_m(x) \\rangle : f \\in \\Delta^{\\mathcal{X}} \\}$. Then, there"}, {"title": "5 Empirical d-GNP and its Statistical Generalization", "content": "In previous sections, we obtained the optimal solution to the constrained optimization problem (3) using d-GNP. In this section, we propose a plug-in method in Algorithm 1 and tackle the generalization"}, {"title": "6 Experiments", "content": "We implemented Algorithm 1, first for COMPAS dataset [24] in which the recidivism rate of 7214 criminal defendants is predicted. The human assessment is done in this dataset on 1000 cases by giving humans a description of the case and asking them whether the defendant would recidivate within two years of their most recent crime."}, {"title": "7 Conclusion", "content": "The d-GNP is a general framework that obtains the optimal solution to various constrained learning problems, including but not limited to multi-objective L2D problems. Using this post-processing framework, we can first estimate the scores related to our problem and then find a linear rule of these scores by fine-tuning for specific violation tolerances. This method reduces the computational complexity of in-processing methods while guaranteeing achieving a near-optimal solution in a large data regime."}]}