[{"title": "", "authors": ["Mohammad-Amin Charusaie", "Samira Samadi"], "abstract": "Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using ad-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of baselines.", "sections": [{"title": "Introduction", "content": "Machine learning algorithms are increasingly used in diverse fields, including critical applications, such as medical diagnostics [68] and predicting optimal prognostics [60]. To address the sensitivity of such tasks, existing approaches suggest keeping the human expert in the loop and using the machine learning prediction as advice [32], or playing a supportive role by taking over the tasks on which machine learning is uncertain [36, 57, 4]. The abstention of the classifier in making decisions, and letting the human expert do so, is where the paradigm of learn-to-defer (L2D) started to exist.\nThe development of L2D algorithms has mainly revolved around optimizing the accuracy of the final system under such paradigm [57, 47]. Although they achieve better accuracy than either the machine learning algorithm or the human expert in isolation, these works provide inherently single-objective solutions to the L2D problem. In the critical tasks that are mentioned earlier, more often than not, we face a challenging multi-objective problem of ensuring the safety, algorithmic fairness, and practicality of the final solution. In such settings, we seek to limit the cost of incorrect decisions [43], algorithmic biases [11], or human expert intervention [54], while optimizing the accuracy of the system. Although the seminal paper that introduced the first L2D algorithm targeted an instance of such multi-objective problem [41], a general solution to such class of problems, besides specific examples [23, 54, 48, 49], has remained unknown to date.\nMulti-objective machine learning extends beyond the realm of L2D problems. A prime example that is extensively studied in various settings is ensuring algorithmic fairness [16] while optimizing accuracy. Recent advances in the algorithmic fairness literature have suggested the superiority of post-processing methodology for tackling this multi-objective problem [69, 12, 18, 72]. Post-processing algorithms operate in two steps: first, they find a calibrated estimation of a set of probability scores"}, {"title": "", "content": "for each input via learning algorithms, and then they obtain the optimal predictor as a function of these scores. Similarly, in a recent set of works, optimal algorithms to reject the decision-making under a variety of secondary objectives are determined via post-processing algorithms [48, 49], which is in line with classical results such as Chow's rule [14] that is the simplest form of a post-processing method, thresholding the likelihood.\nInspired by the above works, in this paper, we fully characterize the solution to multi-objective L2D problems using a post-processing framework. In particular, we consider a deferral system together with a set of conditional performance measures {\u03a80,..., \u03a8m} that are functions of the system outcome \u0176, the target label Y, and the input X. The goal is to optimize the average value of \u03a80 over data distribution while keeping the average value of the rest of performance measures \u03a81,..., \u03a8m for all inputs under control. As an example, in binary classification, \u03a80 can be the 0 \u2013 1 deferral loss function, while \u03a8\u2081 can be the difference between positive prediction rates of \u0176 for all instances of X that belong to demographic group A = 0 or A = 1. The solution for which we aim optimizes the accuracy while assuring that the demographic parity measure between the two groups is bounded by a tolerance value \u03b4\u2081 \u2208 [0, 1].\nTo provide the optimal solution, we move beyond staged learning [10] methodology, in which the classifier h(x) is trained in the absence of human decision-makers, and then the optimal rejection function r(x) is obtained for that classifier to decide when the human expert should intervene (r(x) = 1). Instead, we jointly obtain the classifier and rejection function. The reason that we avoid this methodology is that firstly, objectives such as algorithmic fairness are not compositional, i.e., even if the classifier and the human are fair, due to the emergence of Yule's effect [59] the obtained deferral system is not necessarily fair (see Appendix A), and in fact abstention systems can deter the algorithmic biases [33]. Secondly, the feasibility of constraints is not guaranteed under staged learning methodology [70], e.g., there can be cases in which achieving a purely fair solution is impossible, while this occurs neither in vanilla classification [18] nor in our solution.\nThis paper shows that the joint learning of classifier and rejection function for finding the optimal multi-objective L2D solution boils down to a generalization of the fundamental Neyman-Pearson lemma [52]. This lemma is initially introduced in studying hypothesis testing problems and char-acterizes the most powerful test (i.e., the test with the highest true positive rate) while keeping the significance level (true negative rate) under control. As a natural extension to this paradigm, we con-sider a multi-hypothesis setting where for each true positive prediction and false negative prediction, we receive a reward and loss, respectively. Then, we show that the extension of Neyman-Pearson lemma to this setting provides us with a solution for our multi-objective L2D problem.\nIn summary, the contribution of this paper is as below:\n\u2022 In Section 3, we show that obtaining the optimal deterministic classifier and rejection function under a constraint is, in general, an NP-Hard problem, then\n\u2022 by introducing randomness, we rephrase the multi-objective L2D problem into a functional linear programming.\n\u2022 In Section 4, we show that such linear programming problem is an instance of d-dimensional generalized Neyman-Pearson (d-GNP) problem, then\n\u2022 we characterize the solution to d-GNP problem, and we particularly derive the corresponding parameters of the solution when the optimization is restricted by a single constraint.\n\u2022 In Section 5, we show that a post-processing algorithm that is based on d-GNP solution generalizesto\nconstraints and objective with the rate $O(\\sqrt{\\frac{\\log n}{n}}, \\sqrt{\\frac{\\log(1/\\epsilon)}{n}}, \\epsilon')$ and $O((\\frac{\\log n}{n})^{1/2\\gamma}, (\\frac{\\log(1/\\epsilon)}{n})^{1/2\\gamma}, \\epsilon')$, respectively, with probability at least $1-\\epsilon$ where n is the size of the set using which we fine-tune the algorithm, $\\epsilon'$ measures the accuracy of learned post-processing scores, and $\\gamma$ is a parameter that measures the sensitivity of the constraint to the change of the predictor. Then,\n\u2022 we show that the use of in-processing methods in L2D problem does not necessarily generalize to the unobserved data, and finally\n\u2022 we experiment our post-processing algorithm on two tabular datasets, and observe its performance compared to the baselines for ensuring demographic parity and equality of opportunity on final predictions.\nLastly, the d-GNP theorem has potential use cases beyond the L2D problem, particularly in vanilla classification problems under constraints. However, such applications are beyond the scope of this"}, {"title": "Related Works", "content": "Human and ML's collaboration in decision-making has been demonstrated to enhance the accuracy of final decisions compared to predictions that are made solely by humans or ML [34, 65]. This overperformance is due to the ability to estimate the accuracy and confidence of each agent on different regions of data and subsequently allocate instances between human and ML to optimize the overall accuracy [2]. Since the introduction of the L2D problem, the implementation of its optimal rule has been the focus of interest in this field [7, 47, 10, 48, 8, 40, 45, 42]. The multi-objective classification with abstention problems is studied for specific objectives in [41, 54, 45] via in-processing methods. The application of Neyman-Pearson lemma for learning problems with fairness criteria is recently introduced in [71].\nWe refer the reader to Appendix B for further discussion on related works."}, {"title": "Problem Setting", "content": "Assume that we are given input features $x_i \\in \\mathcal{X}$, corresponding labels $y_i \\in \\mathcal{Y} = \\{1, ..., L\\}$, and the human expert decision $m_i$ for such input, and assume that these are i.i.d. realizations of random variables $X, Y, M \\sim \\mu = \\mu_{XYM}$. Since there exists randomness in the human decision-making process, for the sake of generality, we treat $M$ as a random variable similar to $Y$ and do not assume that $m_i = m(x)$ for some function $m$. Further, assume that for the true label $y$ and a certain feature vector $x$, the cost of incorrect predictions is measured by a loss function $l_{AI}(y, h(x))$ for the classifier prediction $h(x)$, and a loss function $l_H(y, m)$ for human's prediction $m$. The question that we tackle in this paper is the following: What is an optimal classifier and otherwise an optimal way of deferring the decision to the human when there are constraints that limit the decision-making? The constraints above can be algorithmic fairness constraints (e.g., demographic parity, equality of opportunity, equalized odds), expert intervention constraints (e.g., when the human expert can classify up to b proportion of the data), or spatial constraints to enforce deferral on certain inputs, or any combination thereof."}, {"title": "", "content": "Let us put the above question in a formal optimization form. To that end, let $r(x) \\in \\{0, 1\\}$ be the rejection function\u00b9, i.e., when $r(x) = 0$ the classifier makes the decision for input $x$ and otherwise $x$ is deferred to the expert. We obtain the deferral loss on $x$ and given a label $y$ and the expert decision $m$ as\n$\\begin{equation}\nl_{def} (y, m, h(x), r(x)) = r(x)l_H(y, m) + (1 - r(x))l_{AI}(y, h(x)).\n\\end{equation}$\nTherefore, we can find the average deferral loss on distribution $\\mu$ as\n$\\begin{equation}\n\\mathcal{L}_{def} (h, r) := \\mathbb{E}_{X, Y, M \\sim \\mu}[l_{def} (Y, M, h(X), r(X))].\n\\end{equation}$\nWe aim to find a randomized algorithm $A$ that defines a probability distribution $\\mu_{\\alpha}$ on $\\mathcal{H} \\times \\mathcal{R}$ that solves the optimization problem\n$\\begin{aligned}\n&\\mu_{\\alpha} \\in \\underset{\\mu}{\\operatorname{argmin}} \\mathbb{E}_{(h, r) \\sim A} [\\mathcal{L}_{def} (h, r)], \\\\\n&\\text { s.t. } \\mathbb{E}_{X, Y, M \\sim \\mu} \\mathbb{E}_{(h, r) \\sim \\mu_{\\alpha}} [\\Psi_{i} (X, Y, M, h(X), r(X))] \\leq \\delta_{i}\n\\end{aligned}$\nwhere $\\Psi_{i}$ is a performance measure that induces the desired constraint in our optimization problem. We assume that $\\Psi_i$, similar to $l_{def}$, is an outcome-dependent function, i.e., if the deferral occurs, the outcome of the classifier does not change $\\Psi_i$, and otherwise, if deferral does not occur, the human decision does not change $\\Psi_i$. In other words, the value of the constraints can only be a function of input feature $x$ and of the deferral system prediction $\\hat{Y} = r(x)M + (1 - r(x))h(x)$. Here, $\\hat{Y}$ is the expert decision when deferral occurs, and is the classifier decision otherwise.\nTypes of constraints. Before we discuss our methodology to solve (2), it is beneficial to review the types of constraints with which we are concerned: (1) expert intervention budget that can be written in form of $\\Pr (r(X) = 1) \\leq \\delta$, limits the rejection function to defer up to $\\delta$ proportion of the instance, (2) demographic parity that is formulated as $|P(\\hat{Y} = 1|A = 0) - P(\\hat{Y} = 1|A = 0)| \\leq \\delta$, ensures that the proportion of positive predictions for the first demographic group ($A = 0$) is comparable"}, {"title": "", "content": "to that for the second demographic group (A = 1). (3) equality of opportunity that is defined as $|Pr(\\hat{Y} = 1|A = 1, Y = 1) - Pr(\\hat{Y}|A = 0, Y = 1)| < \\delta$ limits the differences between correct positive predictions among two demographic groups, (4) equalized odds that is similar to equality of opportunity but targets the differences of correct positive and negative predictions among two groups, i.e., $\\underset{y=0,1}{\\operatorname{max}} |Pr(\\hat{Y} = 1|A = 1, Y = y) - Pr(\\hat{Y} = 1|A = 0, Y = y)| \\leq \\delta$, (5) out-of-distribution (OOD) detection that is written as $P_{out} (r(X) = 0) \\leq \\delta$ limits the prediction of the classifier on points that are outside its training distribution and incentivizes deferral in such cases, (6) long-tail classification deals with high class imbalances. This method aims to minimize a balanced error of classifier prediction on instances where deferral does not occur. Achieving this objective as mentioned in [50] is equivalent to minimizing $\\sum_{i=1}^{K} \\frac{1}{C_i} \\Pr (Y \\neq h(X), r(X) = 0|Y \\in G_i)$ when the feasible set is $\\Pr (r(X) = 0, Y \\in G_i) = \\delta_i$, and where $\\{G_i\\}_{i=1}^{K}$ is a partition of classes, and finally (7) type-k error bounds that is a generalization of Type-I and Type-II errors, limits errors of a specific class k using $\\Pr (\\hat{Y} \\neq k|Y = k) < \\delta$.\nAll above constraints are expected values of outcome-dependent functions (see Appendix D for proof). To put it informally, if we change the classifier outcome after the rejection, such constraints do not vary.\nLinear Programming Equivalent to (2). The outcome-dependence property helps us to show that (see Appendix C) obtaining the optimal classifier and rejection function is equivalent to obtaining the solution of\n$\\begin{equation}\nf^* = [f_1, ..., f_d] \\in \\underset{f \\in \\Delta^d}{\\operatorname{argmax}} \\mathbb{E}[\\langle f(X), \\psi_0(X) \\rangle], \\text { s.t. } \\mathbb{E}[\\langle f(x), \\psi_i(x) \\rangle] \\leq \\delta_i, i \\in [1 : m]\n\\end{equation}$\nwhere $\\Delta^d$ is a simplex of d dimensions, $d = L + 1$, and $\\forall i : \\mathcal{X} \\rightarrow \\mathbb{R}^d$ is defined as\n$\\begin{equation}\n\\Psi_i(x) := \\mathbb{E}_{Y, M|X=x} \\begin{bmatrix}\n\\Psi_{i}(x, Y, M, 1, 0), ..., \\Psi_{i}(x, Y, M, 1, 0), \\Psi_{i}(x, Y, M, 0, 1)\n\\end{bmatrix}\n\\end{equation}$\nthat we name the embedding function\u00b2 corresponding to the performance measure $\\Psi_{i}$ for $i \\in [0 : m]$, where for simplifying the notation we define $\\Psi_0 = -l_{def}$. Furthermore, the optimal algorithm is obtained by predicting $h(x) = i$ with normalized probability of $\\frac{f^i(x)}{\\sum_{i=1}^{d} f^i(x)}$, where $\\sum_{i=1}^{d} f^i(x) \\neq 0$, and rejecting $r(x) = 1$ with probability $f^{d}(x)$. In case of $\\sum_{i=1}^{d} f^i(x) = 0$ the classifier is defined arbitrarily. A list of embedding functions for the mentioned constraints and objectives is provided in Table 1 (See Appendix D for derivations).\nHardness. We first derive the following negative result for the optimal deterministic predictor in (3). We use the similarity between (3) and 0-1 Knapsack problem (see [55, pp. 374]) to show that there are cases in which solving the former is equivalent to solving an NP-Hard problem. More particularly, if we assume that the distribution of $X$ contains finite atoms $x_1, ..., x_n$, each of which have probability of $\\Pr(X = x_i) = p_i$, and if we set $\\Psi_1(x_i) = [0, w_i]$ and $\\Psi_0(x_i) = [0, v_i]$ for $\\forall v_i, w_i \\in \\mathbb{R}+$, then (3) reduces in $\\underset{f^1}{\\operatorname{argmax}} \\sum_i f^1(x_i)v_i$ subjected to $f^1 : \\mathcal{X} \\rightarrow \\{0, 1\\}$ and $\\sum_i f^1(x_i)w_i \\leq \\delta_1$, which is the main form of the Knapsack problem. In the following theorem, we show that a similar result can be obtained if we choose $\\Psi_0$ and $\\Psi_1$ to be embedding functions corresponding to accuracy and expert intervention budget. All proofs of theorems can be found in the appendix.\nTheorem 3.1 (NP-Hardness of (2)). Let the human expert and the classifier induce 0 - 1 losses and assume $\\mathcal{X}$ to be finite. Finding an optimal deterministic classifier and rejection function for a bounded expert intervention budget is an NP-Hard problem.\nNote that the above finding is different from the complexity results for deferral problems in [46, Theorem 1] and [20, Theorem 1]. NP-hardness results in these settings are consequences of restricting the search to a specific space of models, i.e., the intersection of half-spaces and linear models on a subset of the data. However, in our theorem, the hardness arises due to a possibly complex data distribution and not because of the complex model space.\nThe above hardness theorem for deterministic predictors justifies our choice of using randomized algorithms to solve multi-objective L2D. In the next section, by finding a closed-form solution for the randomized algorithm, we show that such relaxation indeed simplifies the problem."}, {"title": "d-dimensional Generalization of Neyman-Pearson Lemma", "content": "The idea behind minimizing an expected error while keeping another expected error bounded is naturally related to the problem that is designed by Neyman and Pearson [52]. They consider two hypotheses $H_0, H_1$ as two distributions with density functions $g_0(x)$ and $g_1(x)$ for which a given point $x$ can be drawn. Then, they maximize the probability of correctly rejecting $H_0$, while bounding the probability of incorrectly rejecting $H_0$, i.e., for a test $T(x) \\in [0, 1]$ that rejects the null hypothesis when $T(x) = 1$, they solved the problem\n$\\begin{equation}\n\\underset{T \\in [0, 1]^{\\mathcal{X}}}{\\operatorname{max}} \\mathbb{E}_{X \\sim g_1} [T(X)], \\text { s.t. } \\mathbb{E}_{X \\sim g_0} [T(X)] \\leq \\alpha.\n\\end{equation}$\nThey concluded that thresholding the likelihood ratio is a solution to the above problem. Formally, they show that all optimal hypothesis tests take the value $T(x) = 1$ when $\\frac{g_1(x)}{g_0(x)} > k$ and take the value $T(x) = 0$ when $\\frac{g_1(x)}{g_0(x)} < k$, where $k$ is a scalar and dependent on $\\alpha$.\nMulti-hypothesis testing with rewards. In this section, we aim to solve (3) as a generalization of Neyman-Pearson lemma for binary testing to the case of multi-hypothesis testing, in which correctly and incorrectly rejecting each hypothesis has a certain reward and loss. To clarify how the extension of this setting and the problem (3) are equivalent, assume the general case of d hypotheses $H_0, ..., H_{d-1}$, each of which corresponding to $X$ being drawn from the density function $g_i(x)$ for $i \\in \\{0, ..., d-1\\}$. Further, assume that for each hypothesis $H_i$, in case of true positive, we receive the reward $r_i(x)$, and in case of false negative, we receive the loss $l_i(x)$. Assume that we aim to find a test $f: \\mathcal{X} \\rightarrow \\Delta_d$ that for each input $x \\in \\mathcal{X}$ rejects $d-1$ hypotheses, each hypothesis $H_i$ with probability $1 - f^i(x)$ and maximizes a sum of true positive rewards, and that keeps the sum of false negative losses under control. Then, this is equivalent to $\\underset{f \\in \\Delta_d}{\\operatorname{argmax}} - \\mathbb{E}_{X \\sim g_i} [f^i(x)r_i(x)]$\nsubjected to $\\sum_{i=0}^{d-1} \\mathbb{E}_{X \\sim g_i} [(1 - f^i(x))l_i(x)] \\leq \\delta_1$ which in turns is equivalent to\n$\\begin{equation}\n\\underset{f \\in \\Delta_d}{\\operatorname{argmax}} \\mathbb{E}_{X \\sim g_0} [f^i(x)\\frac{g_i(x)}{g_0(x)}r_i(x)] \\text { s.t. }  \\mathbb{E}_{X \\sim g_0} [f^i(x)\\frac{l_i(x)g_j(x)}{g_0(x)}] \\leq \\delta_i.\n\\end{equation}$\nThis problem can be seen as instance of (3), when we set $\\psi_0(x) = [r_0(x) \\frac{g_0(x)}{g_0(x)}, ..., r_{d-1}(x) \\frac{g_{d-1}(x)}{g_0(x)}]$ and $\\psi_1(x) = [\\sum_{j \\neq 0} l_j (x) \\frac{g_j(x)}{g_0(x)}, ..., \\sum_{j \\neq d-1} l_j (x) \\frac{g_j(x)}{g_0(x)}]$. Similarly, we can show that for all $\\psi_0(x), \\psi_1(x)$ in (3) there exists a set of densities $g_1(x), ..., g_{d-1}(x)$ and rewards and losses such that (6) and (3) are equivalent. This can be done by setting $g_i = g_0$ and noting that the mapping from $l_{i}s$ and $r_{i}s$ into $\\Psi_0$ and $\\Psi_2$ is invertible.\nThe formulation of (3) can be seen as an extension of the setting in [66] when we move beyond type-k error bounds to a general set of constraints. That work achieves the optimal test by applying strong duality on the Lagrangian form of the constrained optimization problem. However, we avoided using this approach in proving our solution, since finding $f^*$, and not the optimal objective, is possible via strong duality only when we know apriori that the Lagrangian has a single saddle point (for more details and fallacy of such approach, see Section E). As another improvement to the duality method, we not only find a solution to (3), but also show that there is no other solution that works as well as ours.\nBefore we express our solution in the following theorem, we define an import notation as an extension of the argmax function that helps us articulate the optimal predictor. In fact, we define\n$\\begin{equation}\n\\mathbb{T}_a = \\{r : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\Delta_d \\mid \\sum_{i:x_i = \\max\\{x_1, ..., x_d\\}} \\tau((x_1, ...), (i)) = 1\\}\n\\end{equation}$\nthat is a set of functions that result in one-hot encoded argmax when there is a clear maximum, and otherwise, based on its second argument, results in a probability distribution on all components that achieved the maximum value.\nTheorem 4.1 (d-GNP). For a set of functions $\\psi_{i}$ where $i \\in [0, m]$, assume that $(\\delta_1, ..., \\delta_m)$ is an interior point of the set $\\mathcal{F} = \\{(\\mathbb{E}[\\langle \\tau(x), \\psi_1(x) \\rangle], ..., \\mathbb{E}[\\langle \\tau(x), \\psi_m(x) \\rangle]) : f \\in \\Delta^d \\}$. Then, there"}, {"title": "", "content": "is a set of fixed values $k_1, ..., k_m$ and $\\tau \\in \\mathbb{T}_a$ such that the predictor\n$\\begin{equation}\nf^* (x) = \\tau(\\psi_0(x) - \\sum_{i=1}^{m}k_i\\psi_i(x), x),\n\\end{equation}$\nobtains the optimal solution of $\\underset{f \\in \\Delta^d}{\\operatorname{sup}} \\mathbb{E}[\\langle f(x), \\psi_0(x) \\rangle]$, subjected to the constraints being achieved tightly, i.e., when for $i \\in [1 : m]$ we have $\\mathbb{E}[\\langle f(x), \\psi_i(x) \\rangle] = \\delta_i$. If $k_1,...,k_m$ are further non-negative, then $f^*(x)$ is the optimal solution to (3). Moreover, all optimal solutions of (3) that tightly achieve the constraints are in form of (8) almost everywhere on $\\mathcal{X}$.\nExample 1 (L2D with Demographic Parity). In the setting that we have a deferral system and we aim for controlling demographic disparity under the tolerance \u03b4, we can set $\\psi_0(x) = [\\Pr(Y =\n0|x), \\Pr(Y = 1|x), \\Pr(Y = M|x)]$ and $\\psi_1(x) = s(A) [0, 1, \\Pr(M = 1|x)]$, using Table 1, where\n$\\begin{equation}\ns(A) := \\begin{cases} \\frac{I_{A=1}}{\\mathbb{P}r(Y=y, A=1)}\\\\ \\frac{I_{A=0}}{\\mathbb{P}r(Y=y, A=0)}\n\\end{cases}.\n\\end{equation}$\nTherefore, d-GNP, together with the discussion after (4) shows that the optimal classifier and rejection function are obtained as\n$\\begin{aligned}\nh(x) &= \\begin{cases}\n1 & Pr(Y = 1|x) > \\frac{1 + ks(A)}{2} \\\\\n0 & Pr(Y = 1|x) < \\frac{1 + ks(A)}{2}\n\\end{cases},\nr(x) &= \\begin{cases}\n1 & Pr(Y = M|x) - ks(A) \\Pr(M = 1|x) > \\lambda(A, x) \\\\\n0 & Pr(Y = M|x) - ks(A) \\Pr(M = 1|x) < \\lambda(A, x),\n\\end{cases}\n\\end{aligned}$\nfor a fixed value $k \\in \\mathbb{R}$, and where $\\lambda(A, x) := \\max\\{\\Pr(Y = 0|x), \\Pr(Y = 1|x) - ks(A)\\}$. The above identities imply that the optimal fair classifier for the deferral system thresholds the scores for different demographic groups using two thresholds $ks(0)$ and $ks(1)$. This is similar in form to the optimal fair classifier in vanilla classification problem [12, 18]. However, the rejection function does not merely threshold the scores for different groups, but adds an input-dependent threshold $ks(A) \\Pr(M = 1|x)$ to the unconstrained deferral system scores.\nIt is important to note that although we have a thresholding rule for the classifier, the thresholds are not necessarily the same as of isolated classifier under fairness criteria. Furthermore, the deferral rule is dependent on the thresholds that we use for the classifier. Therefore, we cannot train the classifier for a certain demographic parity and a rejection function in two independent stages. This further affirms the lack of compositionality of algorithmic fairness that we discussed earlier in the introduction of this paper.\nExample 2 (L2D with Equality of Opportunity). Here, similar to the previous example, we can obtain the embedding function for accuracy and equality of opportunity constraint as $\\psi_0(x) = [\\Pr(Y = 0|x), \\Pr(Y = 1|x), \\Pr(Y = M|x)]$ and $\\psi_1(x) = t(A,1) [0, \\Pr(Y = 1|x), \\Pr(M = 1, Y = 1|x)]$, respectively. Therefore, the characterization of optimal classifier and rejection function using d-GNP results in\n$\\begin{aligned}\nh(x) &= \\begin{cases}\n1 & (2-kt(A, 1)) \\Pr(Y = 1|x) > 1 \\\\\n0 & (2 - kt(A, 1)) \\Pr(Y = 1|x) < 1,\n\\end{cases}\nr(x) &= \\begin{cases}\n1 & \\Pr(Y = M|x) (1 - kt(A, 1) \\Pr(M = 1|Y = M, x)) > \\nu(A, x) \\\\\n0 & \\Pr(Y = M|x)(1 - kt(A, 1) \\Pr(M = 1|Y = M, x)) < \\nu(A, x),\n\\end{cases}\n\\end{aligned}$\nfor $k \\in \\mathbb{R}$ and where $\\nu(A, x) := \\max\\{\\Pr(Y = 0|x), (1 - kt(A, 1)) \\Pr(Y = 1|x)\\}$. Assuming 2-kt(A, 1) takes positive values for all choices of A, we conclude that the optimal classifier is to threshold positive scores differently for different demographic groups. However, the optimal deferral is a function of probability of positive prediction by human expert.\nExample 3 (Algorithmic Fairness for Multiclass Classification). In addition to addressing the L2D problem, the formulation of d-GNP in Theorem 4.1 allows for finding the optimal solution in vanilla classification. In fact, for an L-class classifier, if we aim to set constraints on demographic parity $| \\Pr(\\hat{Y} = 0|A = 0) - \\Pr(\\hat{Y} = 0|A = 1)| \\leq \\delta$ or equality of opportunity $| \\Pr(Y = 0|Y = 0, A ="}, {"title": "", "content": "0) - \\Pr(\\hat{Y"}, 0, "Y = 0, A = 1)| \\leq \\delta$ on Class 0, then we can follow similar steps as in Appendix D to find the embedding functions as\n$\\begin{aligned}\n\\psi_{DP} &= s(A) [1,0, ..., 0], \\\\\n\\psi_{EO} &= t(A, 0) [\\Pr(Y = 0|x), 0, ..., 0].\n\\end{aligned}$\nAs a result, since the accuracy embedding function is $\\psi_0(x) = [\\Pr(Y = 0|x), ..., \\Pr(Y = L|x)]$, then, by neglecting the effect of randomness, the optimal classifier under such constraints are as\n$\\begin{aligned}\nh_{DP}(x) &= \\operatorname{argmax} \\{ \\Pr(Y = 0|x) - ks(A), \\Pr(Y = 1|x), ..., \\Pr(Y = L|x)\\},\\\\nh_{EO}(x) &= \\operatorname{argmax} \\{ \\Pr(Y = 0|x) (1 - kt(A, 0)), \\Pr(Y = 1|x), ..., \\Pr(Y = L|x)\\}.\n\\end{aligned}$\nEquivalently, for demographic parity, the optimal classifier includes a shift on the score of Class 0 as a function of demographic group, and for equality of opportunity, the optimal classifier includes a multiplication of the score of Class 0 with a value that is a function of demographic group. It is easy to show that under condition of positivity of the multiplied value, these classifiers both reduce to thresholding rules in binary setting.\nNote that although Theorem 4.1 characterizes the optimal solution of (3), it leaves us uninformed regarding parameters $k_1,..., k_m$, and further does not give us the form of the optimal solution when $\\psi_0(x) - \\sum_{i=1}^{m} k_i\\Psi_i(x)$ has more than one maximizer. In the following theorem, we address these issues for the case that we have a single constraint.\nTheorem 4.2 (d-GNP with a single constraint). The optimal solution (8) of the optimization problem (3) with one constraint is equal to $f_{x,p}(x) = \\tau(\\psi_0(x) - k\\Psi_\\phi(x), x)$ where $\\tau$ is a member of $\\mathbb{T}_a$ such that if there is a non-singleton set $I$ of maximizers of a vector```json\n{\n  \"title\": \"A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems\",\n  \"authors\": [\n    \"Mohammad-Amin Charusaie\",\n    \"Samira Samadi\"\n  ],\n  \"abstract\":", "Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using ad-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of baselines.", "sections\": [\n    {\n      \"title\": \"Introduction", "content\": \"Machine learning algorithms are increasingly used in diverse fields, including critical applications, such as medical diagnostics [68] and predicting optimal prognostics [60]. To address the sensitivity of such tasks, existing approaches suggest keeping the human expert in the loop and using the machine learning prediction as advice [32], or playing a supportive role by taking over the tasks on which machine learning is uncertain [36, 57, 4]. The abstention of the classifier in making decisions, and letting the human expert do so, is where the paradigm of learn-to-defer (L2D) started to exist.\nThe development of L2D algorithms has mainly revolved around optimizing the accuracy of the final system under such paradigm [57, 47]. Although they achieve better accuracy than either the machine learning algorithm or the human expert in isolation, these works provide inherently single-objective solutions to the L2D problem. In the critical tasks that are mentioned earlier, more often than not, we face a challenging multi-objective problem of ensuring the safety, algorithmic fairness, and practicality of the final solution. In such settings, we seek to limit the cost of incorrect decisions [43], algorithmic biases [11], or human expert intervention [54], while optimizing the accuracy of the system. Although the seminal paper that introduced the first L2D algorithm targeted an instance of such multi-objective problem [41], a general solution to such class of problems, besides specific examples [23, 54, 48, 49], has remained unknown to date.\nMulti-objective machine learning extends beyond the realm of L2D problems. A prime example that is extensively studied in various settings is ensuring algorithmic fairness [16] while optimizing accuracy. Recent advances in the algorithmic fairness literature have suggested the superiority of post-processing methodology for tackling this multi-objective problem [69, 12, 18, 72]. Post-processing algorithms operate in two steps: first, they find a calibrated estimation of a set of probability scores"], "content": "for each input via learning algorithms, and then they obtain the optimal predictor as a function of these scores. Similarly, in a recent set of works, optimal algorithms to reject the decision-making under a variety of secondary objectives are determined via post-processing algorithms [48, 49], which is in line with classical results such as Chow's rule [14] that is the simplest form of a post-processing method, thresholding the likelihood.\nInspired by the above works, in this paper, we fully characterize the solution to multi-objective L2D problems using a post-processing framework. In particular, we consider a deferral system together with a set of conditional performance measures {\u03a80,..., \u03a8m} that are functions of the system outcome \u0176, the target label Y, and the input X. The goal is to optimize the average value of \u03a80 over data distribution while keeping the average value of the rest of performance measures \u03a81,..., \u03a8m for all inputs under control. As an example, in binary classification, \u03a80 can be the 0 \u2013 1 deferral loss function, while \u03a8\u2081 can be the difference between positive prediction rates of \u0176 for all instances of X that belong to demographic group A = 0 or A = 1. The solution for which we aim optimizes the accuracy while assuring that the demographic parity measure between the two groups is bounded by a tolerance value \u03b4\u2081 \u2208 [0, 1].\nTo provide the optimal solution, we move beyond staged learning [10] methodology, in which the classifier h(x) is trained in the absence of human decision-makers, and then the optimal rejection function r(x) is obtained for that classifier to decide when the human expert should intervene (r(x) = 1). Instead, we jointly obtain the classifier and rejection function. The reason that we avoid this methodology is that firstly, objectives such as algorithmic fairness are not compositional, i.e., even if the classifier and the human are fair, due to the emergence of Yule's effect [59] the obtained deferral system is not necessarily fair (see Appendix A), and in fact abstention systems can deter the algorithmic biases [33]. Secondly, the feasibility of constraints is not guaranteed under staged learning methodology [70], e.g., there can be cases in which achieving a purely fair solution is impossible, while this occurs neither in vanilla classification [18] nor in our solution.\nThis paper shows that the joint learning of classifier and rejection function for finding the optimal multi-objective L2D solution boils down to a generalization of the fundamental Neyman-Pearson lemma [52]. This lemma is initially introduced in studying hypothesis testing problems and char-acterizes the most powerful test (i.e., the test with the highest true positive rate) while keeping the significance level (true negative rate) under control. As a natural extension to this paradigm, we con-sider a multi-hypothesis setting where for each true positive prediction and false negative prediction, we receive a reward and loss, respectively. Then, we show that the extension of Neyman-Pearson lemma to this setting provides us with a solution for our multi-objective L2D problem.\nIn summary, the contribution of this paper is as below:\n\u2022 In Section 3, we show that obtaining the optimal deterministic classifier and rejection function under a constraint is, in general, an NP-Hard problem, then\n\u2022 by introducing randomness, we rephrase the multi-objective L2D problem into a functional linear programming.\n\u2022 In Section 4, we show that such linear programming problem is an instance of d-dimensional generalized Neyman-Pearson (d-GNP) problem, then\n\u2022 we characterize the solution to d-GNP problem, and we particularly derive the corresponding parameters of the solution when the optimization is restricted by a single constraint.\n\u2022 In Section 5, we show that a post-processing algorithm that is based on d-GNP solution generalizesto constraints and objective with the rate $O(\\sqrt{\\frac{\\log n}{n}}, \\sqrt{\\frac{\\log(1/\\epsilon)}{n}}, \\epsilon')$ and $O((\\frac{\\log n}{n})^{1/2\\gamma}, (\\frac{\\log(1/\\epsilon)}{n})^{1/2\\gamma}, \\epsilon')$, respectively, with probability at least $1-\\epsilon$ where n is the size of the set using which we fine-tune the algorithm, $\\epsilon'$ measures the accuracy of learned post-processing scores, and $\\gamma$ is a parameter that measures the sensitivity of the constraint to the change of the predictor. Then,\n\u2022 we show that the use of in-processing methods in L2D problem does not necessarily generalize to the unobserved data, and finally\n\u2022 we experiment our post-processing algorithm on two tabular datasets, and observe its performance compared to the baselines for ensuring demographic parity and equality of opportunity on final predictions.\nLastly, the d-GNP theorem has potential use cases beyond the L2D problem, particularly in vanilla classification problems under constraints. However, such applications are beyond the scope of this"}, {"title": "Related Works", "content": "Human and ML's collaboration in decision-making has been demonstrated to enhance the accuracy of final decisions compared to predictions that are made solely by humans or ML [34, 65]. This overperformance is due to the ability to estimate the accuracy and confidence of each agent on different regions of data and subsequently allocate instances between human and ML to optimize the overall accuracy [2]. Since the introduction of the L2D problem, the implementation of its optimal rule has been the focus of interest in this field [7, 47, 10, 48, 8, 40, 45, 42]. The multi-objective classification with abstention problems is studied for specific objectives in [41, 54, 45] via in-processing methods. The application of Neyman-Pearson lemma for learning problems with fairness criteria is recently introduced in [71].\nWe refer the reader to Appendix B for further discussion on related works."}, {"title": "Problem Setting", "content": "Assume that we are given input features $x_i \\in \\mathcal{X}$, corresponding labels $y_i \\in \\mathcal{Y} = \\{1, ..., L\\}$, and the human expert decision $m_i$ for such input, and assume that these are i.i.d. realizations of random variables $X, Y, M \\sim \\mu = \\mu_{XYM}$. Since there exists randomness in the human decision-making process, for the sake of generality, we treat $M$ as a random variable similar to $Y$ and do not assume that $m_i = m(x)$ for some function $m$. Further, assume that for the true label $y$ and a certain feature vector $x$, the cost of incorrect predictions is measured by a loss function $l_{AI}(y, h(x))$ for the classifier prediction $h(x)$, and a loss function $l_H(y, m)$ for human's prediction $m$. The question that we tackle in this paper is the following: What is an optimal classifier and otherwise an optimal way of deferring the decision to the human when there are constraints that limit the decision-making? The constraints above can be algorithmic fairness constraints (e.g., demographic parity, equality of opportunity, equalized odds), expert intervention constraints (e.g., when the human expert can classify up to b proportion of the data), or spatial constraints to enforce deferral on certain inputs, or any combination thereof."}, {"title": "", "content": "Let us put the above question in a formal optimization form. To that end, let $r(x) \\in \\{0, 1\\}$ be the rejection function\u00b9, i.e., when $r(x) = 0$ the classifier makes the decision for input $x$ and otherwise $x$ is deferred to the expert. We obtain the deferral loss on $x$ and given a label $y$ and the expert decision $m$ as\n$\\begin{equation}\nl_{def} (y, m, h(x), r(x)) = r(x)l_H(y, m) + (1 - r(x))l_{AI}(y, h(x)).\n\\end{equation}$\nTherefore, we can find the average deferral loss on distribution $\\mu$ as\n$\\begin{equation}\n\\mathcal{L}_{def} (h, r) := \\mathbb{E}_{X, Y, M \\sim \\mu}[l_{def} (Y, M, h(X), r(X))].\n\\end{equation}$\nWe aim to find a randomized algorithm $A$ that defines a probability distribution $\\mu_{\\alpha}$ on $\\mathcal{H} \\times \\mathcal{R}$ that solves the optimization problem\n$\\begin{aligned}\n&\\mu_{\\alpha} \\in \\underset{\\mu}{\\operatorname{argmin}} \\mathbb{E}_{(h, r) \\sim A} [\\mathcal{L}_{def} (h, r)], \\\\\n&\\text { s.t. } \\mathbb{E}_{X, Y, M \\sim \\mu} \\mathbb{E}_{(h, r) \\sim \\mu_{\\alpha}} [\\Psi_{i} (X, Y, M, h(X), r(X))] \\leq \\delta_{i}\n\\end{aligned}$\nwhere $\\Psi_{i}$ is a performance measure that induces the desired constraint in our optimization problem. We assume that $\\Psi_i$, similar to $l_{def}$, is an outcome-dependent function, i.e., if the deferral occurs, the outcome of the classifier does not change $\\Psi_i$, and otherwise, if deferral does not occur, the human decision does not change $\\Psi_i$. In other words, the value of the constraints can only be a function of input feature $x$ and of the deferral system prediction $\\hat{Y} = r(x)M + (1 - r(x))h(x)$. Here, $\\hat{Y}$ is the expert decision when deferral occurs, and is the classifier decision otherwise.\nTypes of constraints. Before we discuss our methodology to solve (2), it is beneficial to review the types of constraints with which we are concerned: (1) expert intervention budget that can be written in form of $\\Pr (r(X) = 1) \\leq \\delta$, limits the rejection function to defer up to $\\delta$ proportion of the instance, (2) demographic parity that is formulated as $|P(\\hat{Y} = 1|A = 0) - P(\\hat{Y} = 1|A = 0)| \\leq \\delta$, ensures that the proportion of positive predictions for the first demographic group ($A = 0$) is comparable"}, {"title": "", "content": "to that for the second demographic group (A = 1). (3) equality of opportunity that is defined as $|Pr(\\hat{Y} = 1|A = 1, Y = 1) - Pr(\\hat{Y}|A = 0, Y = 1)| < \\delta$ limits the differences between correct positive predictions among two demographic groups, (4) equalized odds that is similar to equality of opportunity but targets the differences of correct positive and negative predictions among two groups, i.e., $\\underset{y=0,1}{\\operatorname{max}} |Pr(\\hat{Y} = 1|A = 1, Y = y) - Pr(\\hat{Y} = 1|A = 0, Y = y)| \\leq \\delta$, (5) out-of-distribution (OOD) detection that is written as $P_{out} (r(X) = 0) \\leq \\delta$ limits the prediction of the classifier on points that are outside its training distribution and incentivizes deferral in such cases, (6) long-tail classification deals with high class imbalances. This method aims to minimize a balanced error of classifier prediction on instances where deferral does not occur. Achieving this objective as mentioned in [50] is equivalent to minimizing $\\sum_{i=1}^{K} \\frac{1}{C_i} \\Pr (Y \\neq h(X), r(X) = 0|Y \\in G_i)$ when the feasible set is $\\Pr (r(X) = 0, Y \\in G_i) = \\delta_i$, and where $\\{G_i\\}_{i=1}^{K}$ is a partition of classes, and finally (7) type-k error bounds that is a generalization of Type-I and Type-II errors, limits errors of a specific class k using $\\Pr (\\hat{Y} \\neq k|Y = k) < \\delta$.\nAll above constraints are expected values of outcome-dependent functions (see Appendix D for proof). To put it informally, if we change the classifier outcome after the rejection, such constraints do not vary.\nLinear Programming Equivalent to (2). The outcome-dependence property helps us to show that (see Appendix C) obtaining the optimal classifier and rejection function is equivalent to obtaining the solution of\n$\\begin{equation}\nf^* = [f_1, ..., f_d] \\in \\underset{f \\in \\Delta^d}{\\operatorname{argmax}} \\mathbb{E}[\\langle f(X), \\psi_0(X) \\rangle], \\text { s.t. } \\mathbb{E}[\\langle f(x), \\psi_i(x) \\rangle] \\leq \\delta_i, i \\in [1 : m]\n\\end{equation}$\nwhere $\\Delta^d$ is a simplex of d dimensions, $d = L + 1$, and $\\forall i : \\mathcal{X} \\rightarrow \\mathbb{R}^d$ is defined as\n$\\begin{equation}\n\\Psi_i(x) := \\mathbb{E}_{Y, M|X=x} \\begin{bmatrix}\n\\Psi_{i}(x, Y, M, 1, 0), ..., \\Psi_{i}(x, Y, M, 1, 0), \\Psi_{i}(x, Y, M, 0, 1)\n\\end{bmatrix}\n\\end{equation}$\nthat we name the embedding function\u00b2 corresponding to the performance measure $\\Psi_{i}$ for $i \\in [0 : m]$, where for simplifying the notation we define $\\Psi_0 = -l_{def}$. Furthermore, the optimal algorithm is obtained by predicting $h(x) = i$ with normalized probability of $\\frac{f^i(x)}{\\sum_{i=1}^{d} f^i(x)}$, where $\\sum_{i=1}^{d} f^i(x) \\neq 0$, and rejecting $r(x) = 1$ with probability $f^{d}(x)$. In case of $\\sum_{i=1}^{d} f^i(x) = 0$ the classifier is defined arbitrarily. A list of embedding functions for the mentioned constraints and objectives is provided in Table 1 (See Appendix D for derivations).\nHardness. We first derive the following negative result for the optimal deterministic predictor in (3). We use the similarity between (3) and 0-1 Knapsack problem (see [55, pp. 374]) to show that there are cases in which solving the former is equivalent to solving an NP-Hard problem. More particularly, if we assume that the distribution of $X$ contains finite atoms $x_1, ..., x_n$, each of which have probability of $\\Pr(X = x_i) = p_i$, and if we set $\\Psi_1(x_i) = [0, w_i]$ and $\\Psi_0(x_i) = [0, v_i]$ for $\\forall v_i, w_i \\in \\mathbb{R}+$, then (3) reduces in $\\underset{f^1}{\\operatorname{argmax}} \\sum_i f^1(x_i)v_i$ subjected to $f^1 : \\mathcal{X} \\rightarrow \\{0, 1\\}$ and $\\sum_i f^1(x_i)w_i \\leq \\delta_1$, which is the main form of the Knapsack problem. In the following theorem, we show that a similar result can be obtained if we choose $\\Psi_0$ and $\\Psi_1$ to be embedding functions corresponding to accuracy and expert intervention budget. All proofs of theorems can be found in the appendix.\nTheorem 3.1 (NP-Hardness of (2)). Let the human expert and the classifier induce 0 - 1 losses and assume $\\mathcal{X}$ to be finite. Finding an optimal deterministic classifier and rejection function for a bounded expert intervention budget is an NP-Hard problem.\nNote that the above finding is different from the complexity results for deferral problems in [46, Theorem 1] and [20, Theorem 1]. NP-hardness results in these settings are consequences of restricting the search to a specific space of models, i.e., the intersection of half-spaces and linear models on a subset of the data. However, in our theorem, the hardness arises due to a possibly complex data distribution and not because of the complex model space.\nThe above hardness theorem for deterministic predictors justifies our choice of using randomized algorithms to solve multi-objective L2D. In the next section, by finding a closed-form solution for the randomized algorithm, we show that such relaxation indeed simplifies the problem."}, {"title": "d-dimensional Generalization of Neyman-Pearson Lemma", "content": "The idea behind minimizing an expected error while keeping another expected error bounded is naturally related to the problem that is designed by Neyman and Pearson [52]. They consider two hypotheses $H_0, H_1$ as two distributions with density functions $g_0(x)$ and $g_1(x)$ for which a given point $x$ can be drawn. Then, they maximize the probability of correctly rejecting $H_0$, while bounding the probability of incorrectly rejecting $H_0$, i.e., for a test $T(x) \\in [0, 1]$ that rejects the null hypothesis when $T(x) = 1$, they solved the problem\n$\\begin{equation}\n\\underset{T \\in [0, 1]^{\\mathcal{X}}}{\\operatorname{max}} \\mathbb{E}_{X \\sim g_1} [T(X)], \\text { s.t. } \\mathbb{E}_{X \\sim g_0} [T(X)] \\leq \\alpha.\n\\end{equation}$\nThey concluded that thresholding the likelihood ratio is a solution to the above problem. Formally, they show that all optimal hypothesis tests take the value $T(x) = 1$ when $\\frac{g_1(x)}{g_0(x)} > k$ and take the value $T(x) = 0$ when $\\frac{g_1(x)}{g_0(x)} < k$, where $k$ is a scalar and dependent on $\\alpha$.\nMulti-hypothesis testing with rewards. In this section, we aim to solve (3) as a generalization of Neyman-Pearson lemma for binary testing to the case of multi-hypothesis testing, in which correctly and incorrectly rejecting each hypothesis has a certain reward and loss. To clarify how the extension of this setting and the problem (3) are equivalent, assume the general case of d hypotheses $H_0, ..., H_{d-1}$, each of which corresponding to $X$ being drawn from the density function $g_i(x)$ for $i \\in \\{0, ..., d-1\\}$. Further, assume that for each hypothesis $H_i$, in case of true positive, we receive the reward $r_i(x)$, and in case of false negative, we receive the loss $l_i(x)$. Assume that we aim to find a test $f: \\mathcal{X} \\rightarrow \\Delta_d$ that for each input $x \\in \\mathcal{X}$ rejects $d-1$ hypotheses, each hypothesis $H_i$ with probability $1 - f^i(x)$ and maximizes a sum of true positive rewards, and that keeps the sum of false negative losses under control. Then, this is equivalent to $\\underset{f \\in \\Delta_d}{\\operatorname{argmax}} - \\mathbb{E}_{X \\sim g_i} [f^i(x)r_i(x)]$\nsubjected to $\\sum_{i=0}^{d-1} \\mathbb{E}_{X \\sim g_i} [(1 - f^i(x))l_i(x)] \\leq \\delta_1$ which in turns is equivalent to\n$\\begin{equation}\n\\underset{f \\in \\Delta_d}{\\operatorname{argmax}} \\mathbb{E}_{X \\sim g_0} [f^i(x)\\frac{g_i(x)}{g_0(x)}r_i(x)] \\text { s.t. }  \\mathbb{E}_{X \\sim g_0} [f^i(x)\\frac{l_i(x)g_j(x)}{g_0(x)}] \\leq \\delta_i.\n\\end{equation}$\nThis problem can be seen as instance of (3), when we set $\\psi_0(x) = [r_0(x) \\frac{g_0(x)}{g_0(x)}, ..., r_{d-1}(x) \\frac{g_{d-1}(x)}{g_0(x)}]$ and $\\psi_1(x) = [\\sum_{j \\neq 0} l_j (x) \\frac{g_j(x)}{g_0(x)}, ..., \\sum_{j \\neq d-1} l_j (x) \\frac{g_j(x)}{g_0(x)}]$. Similarly, we can show that for all $\\psi_0(x), \\psi_1(x)$ in (3) there exists a set of densities $g_1(x), ..., g_{d-1}(x)$ and rewards and losses such that (6) and (3) are equivalent. This can be done by setting $g_i = g_0$ and noting that the mapping from $l_{i}s$ and $r_{i}s$ into $\\Psi_0$ and $\\Psi_2$ is invertible.\nThe formulation of (3) can be seen as an extension of the setting in [66] when we move beyond type-k error bounds to a general set of constraints. That work achieves the optimal test by applying strong duality on the Lagrangian form of the constrained optimization problem. However, we avoided using this approach in proving our solution, since finding $f^*$, and not the optimal objective, is possible via strong duality only when we know apriori that the Lagrangian has a single saddle point (for more details and fallacy of such approach, see Section E). As another improvement to the duality method, we not only find a solution to (3), but also show that there is no other solution that works as well as ours.\nBefore we express our solution in the following theorem, we define an import notation as an extension of the argmax function that helps us articulate the optimal predictor. In fact, we define\n$\\begin{equation}\n\\mathbb{T}_a = \\{r : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\Delta_d \\mid \\sum_{i:x_i = \\max\\{x_1, ..., x_d\\}} \\tau((x_1, ...), (i)) = 1\\}\n\\end{equation}$\nthat is a set of functions that result in one-hot encoded argmax when there is a clear maximum, and otherwise, based on its second argument, results in a probability distribution on all components that achieved the maximum value.\nTheorem 4.1 (d-GNP). For a set of functions $\\psi_{i}$ where $i \\in [0, m]$, assume that $(\\delta_1, ..., \\delta_m)$ is an interior point of the set $\\mathcal{F} = \\{(\\mathbb{E}[\\langle \\tau(x), \\psi_1(x) \\rangle], ..., \\mathbb{E}[\\langle \\tau(x), \\psi_m(x) \\rangle]) : f \\in \\Delta^d \\}$. Then, there"}, {"title": "", "content": "is a set of fixed values $k_1, ..., k_m$ and $\\tau \\in \\mathbb{T}_a$ such that the predictor\n$\\begin{equation}\nf^* (x) = \\tau(\\psi_0(x) - \\sum_{i=1}^{m}k_i\\psi_i(x), x),\n\\end{equation}$\nobtains the optimal solution of $\\underset{f \\in \\Delta^d}{\\operatorname{sup}} \\mathbb{E}[\\langle f(x), \\psi_0(x) \\rangle]$, subjected to the constraints being achieved tightly, i.e., when for $i \\in [1 : m]$ we have $\\mathbb{E}[\\langle f(x), \\psi_i(x) \\rangle] = \\delta_i$. If $k_1,...,k_m$ are further non-negative, then $f^*(x)$ is the optimal solution to (3). Moreover, all optimal solutions of (3) that tightly achieve the constraints are in form of (8) almost everywhere on $\\mathcal{X}$.\nExample 1 (L2D with Demographic Parity). In the setting that we have a deferral system and we aim for controlling demographic disparity under the tolerance \u03b4, we can set $\\psi_0(x) = [\\Pr(Y =\n0|x), \\Pr(Y = 1|x), \\Pr(Y = M|x)]$ and $\\psi_1(x) = s(A) [0, 1, \\Pr(M = 1|x)]$, using Table 1, where\n$\\begin{equation}\ns(A) := \\begin{cases} \\frac{I_{A=1}}{\\mathbb{P}r(Y=y, A=1)}\\\\ \\frac{I_{A=0}}{\\mathbb{P}r(Y=y, A=0)}\n\\end{cases}.\n\\end{equation}$\nTherefore, d-GNP, together with the discussion after (4) shows that the optimal classifier and rejection function are obtained as\n$\\begin{aligned}\nh(x) &= \\begin{cases}\n1 & Pr(Y = 1|x) > \\frac{1 + ks(A)}{2} \\\\\n0 & Pr(Y = 1|x) < \\frac{1 + ks(A)}{2}\n\\end{cases},\nr(x) &= \\begin{cases}\n1 & Pr(Y = M|x) - ks(A) \\Pr(M = 1|x) > \\lambda(A, x) \\\\\n0 & Pr(Y = M|x) - ks(A) \\Pr(M = 1|x) < \\lambda(A, x),\n\\end{cases}\n\\end{aligned}$\nfor a fixed value $k \\in \\mathbb{R}$, and where $\\lambda(A, x) := \\max\\{\\Pr(Y = 0|x), \\Pr(Y = 1|x) - ks(A)\\}$. The above identities imply that the optimal fair classifier for the deferral system thresholds the scores for different demographic groups using two thresholds $ks(0)$ and $ks(1)$. This is similar in form to the optimal fair classifier in vanilla classification problem [12, 18]. However, the rejection function does not merely threshold the scores for different groups, but adds an input-dependent threshold $ks(A) \\Pr(M = 1|x)$ to the unconstrained deferral system scores.\nIt is important to note that although we have a thresholding rule for the classifier, the thresholds are not necessarily the same as of isolated classifier under fairness criteria. Furthermore, the deferral rule is dependent on the thresholds that we use for the classifier. Therefore, we cannot train the classifier for a certain demographic parity and a rejection function in two independent stages. This further affirms the lack of compositionality of algorithmic fairness that we discussed earlier in the introduction of this paper.\nExample 2 (L2D with Equality of Opportunity). Here, similar to the previous example, we can obtain the embedding function for accuracy and equality of opportunity constraint as $\\psi_0(x) = [\\Pr(Y = 0|x), \\Pr(Y = 1|x), \\Pr(Y = M|x)]$ and $\\psi_1(x) = t(A,1) [0, \\Pr(Y = 1|x), \\Pr(M = 1, Y = 1|x)]$, respectively. Therefore, the characterization of optimal classifier and rejection function using d-GNP results in\n$\\begin{aligned}\nh(x) &= \\begin{cases}\n1 & (2-kt(A, 1)) \\Pr(Y = 1|x) > 1 \\\\\n0 & (2 - kt(A, 1)) \\Pr(Y = 1|x) < 1,\n\\end{cases}\nr(x) &= \\begin{cases}\n1 & \\Pr(Y = M|x) (1 - kt(A, 1) \\Pr(M = 1|Y = M, x)) > \\nu(A, x) \\\\\n0 & \\Pr(Y = M|x)(1 - kt(A, 1) \\Pr(M = 1|Y = M, x)) < \\nu(A, x),\n\\end{cases}\n\\end{aligned}$\nfor $k \\in \\mathbb{R}$ and where $\\nu(A, x) := \\max\\{\\Pr(Y = 0|x), (1 - kt(A, 1)) \\Pr(Y = 1|x)\\}$. Assuming 2-kt(A, 1) takes positive values for all choices of A, we conclude that the optimal classifier is to threshold positive scores differently for different demographic groups. However, the optimal deferral is a function of probability of positive prediction by human expert.\nExample 3 (Algorithmic Fairness for Multiclass Classification). In addition to addressing the L2D problem, the formulation of d-GNP in Theorem 4.1 allows for finding the optimal solution in vanilla classification. In fact, for an L-class classifier, if we aim to set constraints on demographic parity $| \\Pr(\\hat{Y} = 0|A = 0) - \\Pr(\\hat{Y} = 0|A = 1)| \\leq \\delta$ or equality of opportunity $| \\Pr(Y = 0|Y = 0, A ="}, {"title": "", "content": "0) - \\Pr(\\hat{Y} = 0|Y = 0, A = 1)| \\leq \\delta$ on Class 0, then we can follow similar steps as in Appendix D to find the embedding functions as\n$\\begin{aligned}\n\\psi_{DP} &= s(A) [1,0, ..., 0], \\\\\n\\psi_{EO} &= t(A, 0) [\\Pr(Y = 0|x), 0, ..., 0].\n\\end{aligned}$\nAs a result, since the accuracy embedding function is $\\psi_0(x) = [\\Pr(Y = 0|x), ..., \\Pr(Y = L|x)]$, then, by neglecting the effect of randomness, the optimal classifier under such constraints are as\n$\\begin{aligned}\nh_{DP}(x) &= \\operatorname{argmax} \\{ \\Pr(Y = 0|x) - ks(A), \\Pr(Y = 1|x), ..., \\Pr(Y = L|x)\\},\\\\nh_{EO}(x) &= \\operatorname{argmax} \\{ \\Pr(Y = 0|x) (1 - kt(A, 0)), \\Pr(Y = 1|x), ..., \\Pr(Y = L|x)\\}.\n\\end{aligned}$\nEquivalently, for demographic parity, the optimal classifier includes a shift on the score of Class 0 as a function of demographic group, and for equality of opportunity, the optimal classifier includes a multiplication of the score of Class 0 with a value that is a function of demographic group. It is easy to show that under condition of positivity of the multiplied value, these classifiers both reduce to thresholding rules in binary setting.\nNote that although Theorem 4.1 characterizes the optimal solution of (3), it leaves us uninformed regarding parameters $k_1,..., k_m$, and further does not give us the form of the optimal solution when $\\psi_0(x) - \\sum_{i=1}^{m} k_i\\Psi_i(x)$ has more than one maximizer. In the following theorem, we address these issues for the case that we have a single constraint.\nTheorem 4.2 (d-GNP with a single constraint). The optimal solution (8) of the optimization problem (3) with one constraint is equal to $f_{x,p}(x) = \\tau(\\psi_0(x) - k\\Psi_\\phi(x), x)$ where $\\tau$ is a member of $\\mathbb{T}_a$ such that if there is a non-singleton set $I$ of maximizers of a vector $"}]