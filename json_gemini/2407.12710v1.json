{"title": "A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems", "authors": ["Mohammad-Amin Charusaie", "Samira Samadi"], "abstract": "Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using ad-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of baselines.", "sections": [{"title": "1 Introduction", "content": "Machine learning algorithms are increasingly used in diverse fields, including critical applications, such as medical diagnostics [68] and predicting optimal prognostics [60]. To address the sensitivity of such tasks, existing approaches suggest keeping the human expert in the loop and using the machine learning prediction as advice [32], or playing a supportive role by taking over the tasks on which machine learning is uncertain [36, 57, 4]. The abstention of the classifier in making decisions, and letting the human expert do so, is where the paradigm of learn-to-defer (L2D) started to exist.\n\nThe development of L2D algorithms has mainly revolved around optimizing the accuracy of the final system under such paradigm [57, 47]. Although they achieve better accuracy than either the machine learning algorithm or the human expert in isolation, these works provide inherently single-objective solutions to the L2D problem. In the critical tasks that are mentioned earlier, more often than not, we face a challenging multi-objective problem of ensuring the safety, algorithmic fairness, and practicality of the final solution. In such settings, we seek to limit the cost of incorrect decisions [43], algorithmic biases [11], or human expert intervention [54], while optimizing the accuracy of the system. Although the seminal paper that introduced the first L2D algorithm targeted an instance of such multi-objective problem [41], a general solution to such class of problems, besides specific examples [23, 54, 48, 49], has remained unknown to date.\n\nMulti-objective machine learning extends beyond the realm of L2D problems. A prime example that is extensively studied in various settings is ensuring algorithmic fairness [16] while optimizing accuracy. Recent advances in the algorithmic fairness literature have suggested the superiority of post-processing methodology for tackling this multi-objective problem [69, 12, 18, 72]. Post-processing algorithms operate in two steps: first, they find a calibrated estimation of a set of probability scores"}, {"title": "2 Related Works", "content": "Human and ML's collaboration in decision-making has been demonstrated to enhance the accuracy of final decisions compared to predictions that are made solely by humans or ML [34, 65]. This overperformance is due to the ability to estimate the accuracy and confidence of each agent on different regions of data and subsequently allocate instances between human and ML to optimize the overall accuracy [2]. Since the introduction of the L2D problem, the implementation of its optimal rule has been the focus of interest in this field [7, 47, 10, 48, 8, 40, 45, 42]. The multi-objective classification with abstention problems is studied for specific objectives in [41, 54, 45] via in-processing methods. The application of Neyman-Pearson lemma for learning problems with fairness criteria is recently introduced in [71].\n\nWe refer the reader to Appendix B for further discussion on related works."}, {"title": "3 Problem Setting", "content": "Assume that we are given input features $x_i \\in \\mathcal{X}$, corresponding labels $y_i \\in \\mathcal{Y} = \\{1, ..., L\\}$, and the human expert decision $m_i$ for such input, and assume that these are i.i.d. realizations of random variables $X, Y, M \\sim \\mu = \\mu_{\\mathcal{X}YM}$. Since there exists randomness in the human decision-making process, for the sake of generality, we treat $M$ as a random variable similar to $Y$ and do not assume that $m_i = m(x_i)$ for some function $m$. Further, assume that for the true label $y$ and a certain feature vector $x$, the cost of incorrect predictions is measured by a loss function $l_{AI} (y, h(x))$ for the classifier prediction $h(x)$, and a loss function $l_H(y, m)$ for human's prediction $m$. The question that we tackle in this paper is the following: What is an optimal classifier and otherwise an optimal way of deferring the decision to the human when there are constraints that limit the decision-making? The constraints above can be algorithmic fairness constraints (e.g., demographic parity, equality of opportunity, equalized odds), expert intervention constraints (e.g., when the human expert can classify up to $b$ proportion of the data), or spatial constraints to enforce deferral on certain inputs, or any combination thereof."}, {"title": "4 d-dimensional Generalization of Neyman-Pearson Lemma", "content": "The idea behind minimizing an expected error while keeping another expected error bounded is naturally related to the problem that is designed by Neyman and Pearson [52]. They consider two hypotheses $H_0, H_1$ as two distributions with density functions $g_0(x)$ and $g_1(x)$ for which a given point $x$ can be drawn. Then, they maximize the probability of correctly rejecting $H_0$, while bounding the probability of incorrectly rejecting $H_0$, i.e., for a test $T(x) \\in [0, 1]$ that rejects the null hypothesis when $T(x) = 1$, they solved the problem\n\n$\\max_{T \\in [0, 1]^{*}} \\mathbb{E}_{X\\sim g_1} [T(X)], \\quad \\text{s.t.} \\quad \\mathbb{E}_{X\\sim g_0} [T(X)] \\leq \\alpha$.\n\nThey concluded that thresholding the likelihood ratio is a solution to the above problem. Formally, they show that all optimal hypothesis tests take the value $T(x) = 1$ when $g_1(x)/g_0(x) > k$ and take the value $T(x) = 0$ when $g_1(x)/g_0(x) < k$, where $k$ is a scalar and dependent on $\\alpha$.\n\nMulti-hypothesis testing with rewards. In this section, we aim to solve (3) as a generalization of Neyman-Pearson lemma for binary testing to the case of multi-hypothesis testing, in which correctly and incorrectly rejecting each hypothesis has a certain reward and loss. To clarify how the extension of this setting and the problem (3) are equivalent, assume the general case of $d$ hypotheses $H_0, ..., H_{d-1}$, each of which corresponding to $X$ being drawn from the density function $g_r(x)$ for $i \\in \\{0, ..., d- 1\\}$. Further, assume that for each hypothesis $H_i$, in case of true positive, we receive the reward $r_i(x)$, and in case of false negative, we receive the loss $l_i(x)$. Assume that we aim to find a test $f: \\mathcal{X} \\rightarrow \\Delta_d$ that for each input $x \\in \\mathcal{X}$ rejects $d - 1$ hypotheses, each hypothesis $H_i$ with probability $1 - f^i(x)$ and maximizes a sum of true positive rewards, and that keeps the sum of false negative losses under control. Then, this is equivalent to $\\text{argmax} -\\sum_{i=0}^{d-1} \\mathbb{E}_{x\\sim g_i} [f^i(x)r_i(x)]$\n\n$\\begin{aligned} &\\text{subjected to } \\sum_{i=0}^{d-1} \\mathbb{E}_{x\\sim g_i} [(1 - f^i(x))l_i(x)] \\leq \\delta_1 \\\\ & \\text{which in turns is equivalent to}\\\\ &\\text{argmax} \\mathbb{E}_{x\\sim g_0} [f^i(x)\\frac{g_i(x)}{g_j(x)} r_i(x) \\frac{g_i(x)}{g_0(x)}] \\text{ s.t. } \\mathbb{E}_{x\\sim g_0} [f^i(x) \\frac{g_j(x)}{g_0(x)} \\sum_{j \\neq i} l_j (x) \\frac{g_j(x)}{g_0(x)}] \\leq \\delta_1. \\end{aligned}$\n\nThis problem can be seen as instance of (3), when we set $\\psi_0(x) = [r_0(x) \\frac{g_0(x)}{g_0(x)}, ...,r_{d-1}(x) \\frac{g_{d-1}(x)}{g_0(x)}]$ and $\\psi_1(x) = [\\sum_{j\\neq 0}l_j(x) \\frac{g_j(x)}{g_0(x)},..., \\sum_{j\\neq d-1}l_j(x) \\frac{g_j(x)}{g_0(x)}]$. Similarly, we can show that for all $\\psi_0(x), \\psi_1(x)$ in (3) there exists a set of densities $g_1(x), ..., g_{d-1}(x)$ and rewards and losses such that (6) and (3) are equivalent. This can be done by setting $g_i = g_0$ and noting that the mapping from $l_i$s and $r_i$s into $\\Psi_0$ and $\\Psi_2$ is invertible.\n\nThe formulation of (3) can be seen as an extension of the setting in [66] when we move beyond type-k error bounds to a general set of constraints. That work achieves the optimal test by applying strong duality on the Lagrangian form of the constrained optimization problem. However, we avoided using this approach in proving our solution, since finding $f^*$, and not the optimal objective, is possible via strong duality only when we know apriori that the Lagrangian has a single saddle point (for more details and fallacy of such approach, see Section E). As another improvement to the duality method, we not only find a solution to (3), but also show that there is no other solution that works as well as ours.\n\nBefore we express our solution in the following theorem, we define an import notation as an extension of the argmax function that helps us articulate the optimal predictor. In fact, we define\n\n$T_a = \\{r : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\Delta_d | \\sum_{i:x_i=\\text{max}\\{x_1,...,x_d\\}} \\Gamma((x_1, x_2))(i) = 1\\}$\n\nthat is a set of functions that result in one-hot encoded argmax when there is a clear maximum, and otherwise, based on its second argument, results in a probability distribution on all components that achieved the maximum value.\n\nTheorem 4.1 (d-GNP). For a set of functions $\\psi_i$ where $i \\in [0, m]$, assume that $(\\delta_1, ..., \\delta_m)$ is an interior point of the set $\\mathcal{F} = \\{(\\mathbb{E}[\\langle r(x), \\psi_1(x) \\rangle], ..., \\mathbb{E}[\\langle r(x), \\psi_m(x) \\rangle]) : f \\in \\Delta_d ^\\mathcal{X}\\}$. Then, there"}, {"title": "5 Empirical d-GNP and its Statistical Generalization", "content": "In previous sections, we obtained the optimal solution to the constrained optimization problem (3) using d-GNP. In this section, we propose a plug-in method in Algorithm 1 and tackle the generalization"}, {"title": "6 Experiments", "content": "We implemented 4 Algorithm 1, first for COMPAS dataset [24] in which the recidivism rate of 7214 criminal defendants is predicted. The human assessment is done in this dataset on 1000 cases by giving humans a description of the case and asking them whether the defendant would recidivate within two years of their most recent crime.5 The demographic parity is assessed for two racial groups of white and non-white defendants. Figure 2 shows the average performance of d-GNP over"}, {"title": "7 Conclusion", "content": "The d-GNP is a general framework that obtains the optimal solution to various constrained learning problems, including but not limited to multi-objective L2D problems. Using this post-processing framework, we can first estimate the scores related to our problem and then find a linear rule of these scores by fine-tuning for specific violation tolerances. This method reduces the computational complexity of in-processing methods while guaranteeing achieving a near-optimal solution in a large data regime."}, {"title": "A Lack of Compositionality of Fairness Criteria", "content": "Here, we show an example of lack of compositionality of fairness criteria for learn-to-defer problems. This falls in line with [26], where the authors studied the effect of the operators such as 'OR' or \u2018AND'. Here, we show that a similar non-compositionality holds for the operator \u2018DEFER'. The following example is found based on the insight that a fair predictor is fair over all the space $\\mathcal{X}$, and if it could take a decision over only a subset of $\\mathcal{X}$ it will not necessarily be a fair predictor. This can be seen as a particular application of Yule's effect [59] which explains that vanishing correlation in a mixture of distributions does not necessarily concludes vanishing correlation on each of such distributions.\n\nLet us assume that the space $\\mathcal{X}$ contains only four points $x_1, x_2, x_3$, and $x_4$, and that the input takes these values with probability $Pr(X = x_1) = Pr(X = x_2) = Pr(X = x_3) = Pr(X = x_4) = \\frac{1}{4}$. The first two points $x_1, x_2$ are corresponded to the demographic group $A = 0$ and the last two points are corresponded to the demographic group $A = 1$. Further, assume that the conditional target probability is $Pr(Y = 1|x_1) = Pr(Y = 1|x_2) = Pr(Y = 1|x_3) = Pr(Y = 1|x_4) = 1$. Moreover, we consider the equality of opportunity as the measure of fairness. Now, assume that the classifier $h(.): \\mathcal{X} \\rightarrow \\{0, 1\\}$ is taking values $h(x_1) = 1, h(x_2) = 0, h(x_3) = 1$, and $h(x_4) = 0$ and the human decision maker predicts $M = 0$ conditioned on $x_1, M = 1$ conditioned on $x_2$, and $M = 1$ conditioned on $x_3$, and $M = 0$ conditioned on $x_4$. Therefore, both classifier and human expert have accuracy of $\\frac{3}{4}$ on the data.\n\nFollowing the above assumptions, we can find the fairness measure for classifier as\n\n$\\begin{aligned} &Pr(h(X) = 1|Y = 1, A = 0) - Pr(h(X) = 1|Y = 1, A = 1) \\\\ =&Pr(h(X) = 1|Y = 1, A = 0, X = x_1) Pr(X = x_1|Y = 1, A = 0) \\\\ &+ Pr(h(X) = 1|Y = 1, A = 0, X = x_2) Pr(X = x_2|Y = 1, A = 0) \\\\ &- Pr(h(X) = 1|Y = 1, A = 1, X = x_3) Pr(X = x_3|Y = 1, A = 1) \\\\ &- Pr(h(X) = 1|Y = 1, A = 1, X = x_4) Pr(X = x_4|Y = 1, A = 1) = \\frac{1}{1} + 0 - \\frac{1}{1} - 0 = 0,  \\end{aligned}$\n\nwhich means that the classifier is fully fair. We can derive a similar result for the human expert, i.e.,\n\n$Pr(M = 1|Y = 1, A = 0) - Pr(M = 1|Y = 1, A = 1) = 0$.\n\nNow that we established a fair classifier and a fair expert, we take the step to find an optimal deferral solution, i.e., a deferral system that minimizes the overall loss. We can observe that for $x_1$ the classifier is accurate, while for $x_2$ the human expert is accurate. Furthermore, for $x_3$ and $x_4$ they both are equally inaccurate. Therefore, an optimal solution is not to defer for $x_1$, and defer for $x_2$, and take an arbitrary decision for $x_3$ and $x_4$. Now, if we find the fairness measure of the resulting deferral"}, {"title": "B Extended Related Works", "content": "The deferral problem has been studied under a variety of conditions. Rejection learning [17, 3, 9, 13] or selective classification [27, 30, 29], assumes that a fixed cost is incurred to the overall loss, when ML decides not to make a prediction on an input. The first Bayes optimal rule for rejection learning was derived in [14]. Assuming that the accuracy of human, and consequently the cost of deferring to the human, can vary for different inputs, [47] obtained the Bayes optimal deferral rule. The deferral problem is further studied assuming that the number of available instances for deferral are bounded and a near-optimal classifier and deferral rule is required as a solution of empirical risk minimization [20, 21]. Most recently, the implementation of deferral rules using neural networks and surrogate losses is studied for binary and multi-class classification [7, 47, 10, 48, 8, 40, 45, 42]. A possible shift in human expert for L2D methods recently studied in [64]. The problem multi-objective L2D and rejection learning is mainly studied in an in-processing approach. A few instances of tackling such problems can be found in [54, 49, 50] and [70, 38] for L2D and rejection learning, respectively.\n\nNeyman-Pearson's fundamental lemma is introduced in [52] originally for binary hypothesis testing and later was generalized in [53] to give a close-form formulation for a variety of binary constrained optimization problems. Later, [19] found conditions for which Neyman and Pearson solution exists and is unique. The generalization of the empirical solution to Neyman-Pearson problem is studied in two lines of works: (i) the generalization of direct (in-processing) solutions to the optimization problem [62, 61, 58], and (ii) the generalization of plug-in methods [67] that first approximate the score functions and then use Neyman-Pearson lemma to approximate the predictor. The generalization of Neyman-Pearson lemma to multiclass setting is first empirically studied in [37] and under strong duality assumption is proved in [66]. Our lemma d-GNP extends these works in order to (i) be able to control a general set of constraints instead of Type-K errors, and (ii) be valid in absence of strong duality assumption. Further, the idea of using Neyman-Pearson lemma for controlling fairness criteria originally dates back to [72] (later as [71]). More recently, a similar post-processing method is introduced in [12] using cost-sensitive learning and strong duality technique. Although these works cover binary classification problem, in this paper we focus on solving multi-class classification problem, and particularly in a deferral system.\n\nLastly, this work differs from multi-class classification with complex performance metrics [51] in the sense that they consider constraints that are non-linear functions of confusion matrix, while ignoring the dependence on input $x$. In our setting, the constraints are linear in terms of confusion matrix when conditioned on the input, but the linear coefficients vary with the input."}, {"title": "C Rephrasing (2) into Linear Functional Programming", "content": "Here, we first characterize functions that are outcome-dependent. To that end, we define $I(x)$ as\n\n$I = [I_{r(x)=0,h(x)=1},...,I_{r(x)=0,h(x)=L}, I_{r(x)=1}].$\n\nThis function can retrieve the value of $r(x)$ and can retrieve the value of $h(x)$ only if $r(x) = 0$. In fact, we can obtain $r(x) = (I(x)) (L + 1)$ and $h(x) = i$ if $r(x) = 0$ and $(I(x))(i) = 1$. Therefore,"}]}