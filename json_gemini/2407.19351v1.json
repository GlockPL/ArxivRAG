{"title": "AccessShare: Co-designing Data Access and Sharing with Blind People", "authors": ["Rie Kamikubo", "Farnaz Zamiri Zeraati", "Kyungjun Lee", "Hernisa Kacorri"], "abstract": "Blind people are often called to contribute image data to datasets for Al innovation with the hope for future accessibility and inclusion. Yet, the visual inspection of the contributed images is inaccessible. To this day, we lack mechanisms for data inspection and control that are accessible to the blind community. To address this gap, we engage 10 blind participants in a scenario where they wear smartglasses and collect image data using an AI-infused application in their homes. We also engineer a design probe, a novel data access interface called AccessShare, and conduct a co-design study to discuss participants' needs, preferences, and ideas on consent, data inspection, and control. Our findings reveal the impact of interactive informed consent and the complementary role of data inspection systems such as AccessShare in facilitating communication between data stewards and blind data contributors. We discuss how key insights can guide future informed consent and data control to promote inclusive and responsible data practices in AI.", "sections": [{"title": "1 INTRODUCTION", "content": "Participatory data stewardship [53] has gained attention as a means to promote ethical and responsible data practices related to col-lection, sharing, and usage. However, the challenge of ensuring meaningful engagement of diverse stakeholders is pronounced at the intersection of accessibility and artificial intelligence (AI), fur-ther raising concerns about the limited agency and control that disabled people may have [96]. This is also the case when it comes to their data. With a focus on blind people and the context of com-puter vision, we see unique accessibility challenges, such as how to support inspection of image data that they have captured and that may contain unintended information [5] without relying on sight. Despite efforts such as 'disability-first dataset' creation [86, 92], we still lack mechanisms for providing blind data contributors with access and control over their image data.\nTo address this issue, our work aims to explore first what kind of data access and control blind people seek over their image data, as part of our broader investigation into participatory stewardship mechanisms. This is a pressing topic considering how blind people have been early adopters of computer vision technology, sending their photos over servers and assistive applications in exchange for privacy [5, 32]. Consequently, they are one of the most represented contributors of publicly-available accessibility datasets [58]; yet, there is no clear process or consensus among researchers on how to ethically share such datasets. More so, conversations around these data loom large, especially with the tensions involved around benefits and risks-e.g., large multi-modal models are trained on photos taken by sighted people with further widening performance gaps on photos taken by blind users [23, 71].\nIn this work, we engineered a larger cross-sectional study de-signed to foster the involvement of blind participants in challenging real-world data practices from collection to sharing. In this context, we first deployed a teachable object recognizer [57] in the homes of blind participants, where they captured photos and labels of objects (i.e., study data). These data capturing activities were conducted in real-world settings to bring potential value as AI datasets [72] but came with heightened privacy concerns. Also, the photos were taken via smartglasses that may capture a broader field of view with higher risks of including potentially identifiable information (PII). The observations and findings presented in this paper are sit-uated within this larger study, which includes a co-design session to explore blind people's needs and preferences regarding access and sharing of their study data.\nOur work encompasses the decisions that 10 blind participants made after the initial data collection and their interactions with their study data via a novel interface called AccessShare. It serves as a design probe [33] to brainstorm potential functionalities and features for an accessible data sharing system. Additionally, our approach facilitates an opportunity for blind participants to access their study data prior to arriving at their decision of whether or not to share them via a public AI dataset. We examined participants' decision-making processes situated in the larger study to under-stand how decisions were formed, communicated, and evolved over a participatory data stewardship framework.\nOur findings revolve around two axes capturing (i) blind partici-pants' data sharing decisions and (ii) requirements for accessible data sharing systems. In the first axis, we found that the majority (N = 7) made a decision to share their study data. A common ratio-nale, in addition to their motivation behind this decision, was the absence of concerning elements. Participants carefully employed strategies that e.g., avoided capturing personal information and people in the background. Yet, our manual inspection still revealed instances of PII in their photos. We also found that obtaining writ-ten data sharing consents via email after the data collection was not effective. All but two participants chose this route. Instead, par-ticipants opted for verbal consent during the co-design session. We observed that this is explained in part by a need for an interactive approach to consent. Many participants found the co-design session helpful for guiding their decisions e.g., for better understanding study data and exploring options such as partial sharing.\nFindings from the second axis surfaced the complementary na-ture of data access systems such as AccessShare, in supporting communication with sighted people (e.g., data stewards and family members). Yet, a deeper dive into participants' responses revealed a delicate balance between asking for sighted help and self-reliance when making their decision. While some expressed the need for more reliable descriptors to independently inspect the data, one participant opted for a joint decision making approach with a family member to both inspect the data and share agency with a bystander. The contrasting viewpoints on independence and interdependence prompted considerations for incorporating multiuser functionali-ties in data access systems. Overall, as a design probe, AccessShare was effective for ideating approaches to enhance data inspection and control and support communication with data stewards.\nOur contributions include: (1) empirical findings from a real-world participatory data stewarding process with blind participants, reflecting on decisions formed and how they were communicated; (2) implementation of AccessShare as a probe for co-designing accessible data sharing systems that facilitate control among blind data contributors; and (3) methods for obtaining consent through different means, offering insights into challenges and opportunities for meaningful informed consent for blind data contributors. By illustrating the active participation of blind people across our data stewarding process, this paper can inform future work in involving underrepresented communities in discussions around informed data sharing consents and meaningful data inspection and control."}, {"title": "2 BACKGROUND", "content": "We first outline the landscape of data ecosystems and policies in AI, with a focus on individuals' control over their data. We then review prior work on participation frameworks for increasing agency and discuss the value of human-centered research on AI datasets for accessibility. We build upon recent \"disability-first\" data collection efforts, which have highlighted the benefits of engaging blind peo-ple in contributing to datasets for AI innovation [56, 86, 92] and some of the motivational factors for data contributors [58]."}, {"title": "2.1 Prospects for Data Control in AI", "content": "Data in the digital age is a key input and source of value, often referred to as \"the new oil\" [48], a trend that continues with genera-tive AI. A counter-trend is the rising tension to address ethical and privacy concerns surrounding user-generated data [62, 84]. The majority of generative AI applications have been pre-trained on large collections of text data sourced from the internet, including user-generated content on websites and social media platforms [18]. Many of these applications also utilize the history of interactions with users to refine and update their models to improve the per-formance over time [2]. As data continuously fuels AI, discussions about policy and technical interventions to grant users better con-trol of their data are on the rise [30, 42].\nSince the launch of the European General Data Protection Reg-ulation (GDPR) in 2018, many users were promised rights related to data protection, including the right to access, erase, and request corrections on their data, and the right to restrict processing [98]. Yet, even when these rights are granted, practical challenges re-main, with users struggling to understand, use, and control their data [6, 14]. Additional challenges arise from the technical and systemic complexity of removing individual data from models once they have been trained [9, 19]. Tracking the origin and usage of individual data points throughout the AI lifecycle (e.g., training, validation, and deployment stages) can also be challenging, partly due to companies often lacking a full understanding of what has gone into their models [40]. While the recent EU's AI Act could fill in the gaps at scale, enforcing documentation, record-keeping, transparency, and human oversight [93], there are yet no clear mechanisms to allow users to have complete data control.\nOne crucial aspect of AI and data regulations is the power bal-ance among stakeholders [26]. Despite the regulatory interventions such as the GDPR, Delacroix and Lawrence [27] point to the power asymmetry between data subjects-who have knowingly or unknow-ingly provided their data to various entities-and data controllers, typically organizations that make decisions about what data to collect and how it is processed. With our work, we explore methods and tools that can address the current data gaps and facilitate the agency of individuals whose data are part of the Al ecosystem, with focus in the context of research in 'AI for Accessibility'."}, {"title": "2.2 Participation in Data Stewardship", "content": "Participatory data stewardship recognizes the importance of in-volving stakeholders, communities, and individuals in the decision-making processes related to data that affect them [53]. Ultimately, it aims to rebalance the asymmetries of power [60]. This approach is gaining some traction in Al research and development [13, 61, 85]."}, {"title": "2.3 Blind Contributors in Accessibility Datasets", "content": "The field of accessibility is no exception when it comes to the growing need for data, considering the potential to facilitate novel assistive technologies [15, 56, 74]. For instance, photos of every-day objects captured by blind users, the focus of this work, can contribute to building image recognition applications to access visual information [66, 88, 105]. There is a rich body of work re-lated to photos sourced from blind people for Al datasets in this space [38, 66, 72, 86, 88]. Carefully balancing the benefits and unique risks of accessibility datasets is critical in these efforts [58]. While datasets have more value when collected in the 'wild, capturing real-world scenarios (e.g., people's homes), this comes with height-ened ethical and privacy concerns [5, 37, 45].\nIn response, we see efforts toward conscientious data steward-ship practices, emphasizing a \u201cdisability-first\" approach [80, 86, 92]. To facilitate the involvement of blind people in the creation of a teachable object recognition dataset, Theodorou et al. [92] imple-mented an interface that supports communication between data stewards and data contributors during data collection; for instance, blind people are notified of data that fail to meet the validation criteria (e.g., quality and privacy) and asked to retake them. How-ever, this prompts unaddressed questions about data assessment and the role of contributors. Despite being most affected by the outcomes, blind people are informed rather than actively involved in validation and decision-making processes, partly due to data in-accessibility. For instance, the inspection and filtering of personally identifiable information are done by the data stewards [86, 92], not the blind contributors. Thus, privacy-preserving techniques that are accessible to blind people are attracting attention (e.g., [101, 103]).\nEnsuring ethical data practices remains a challenge, especially in engaging blind people to contribute visual content with their consent while respecting their privacy [86, 90]. We complement prior guidelines for disability-first datasets [86, 92] by addressing how blind people can access, inspect, and better control their data."}, {"title": "3 ACCESSSHARE: OUR DESIGN PROBE", "content": "To facilitate discussion and ideas for an accessible data-sharing sys-tem that can support data control, we develop a design probe [33]. It is called AccessShare, shown in Figure 1. It incorporates auto-matically generated descriptors in an HTML webpage that enables blind users to navigate, access, and inspect their data. The probe is closely tied to the data collection task, which in this work is training a teachable object recognizer [56]-users teach the model to recognize objects of their choice by providing photos along labels as training examples. They also take additional photos for testing the personalized model. As seen in previously shared datasets in this context (e.g., [66, 72, 88]), photos captured by blind people are organized into training and testing, accompanied by files containing object labels. In this section, we detail how the probe is designed for accessing and inspecting these data, which we later discuss with blind participants to brainstorm ideal functionalities and features."}, {"title": "3.1 Automatically Generated Descriptors", "content": "Facilitating a quick turnaround from data collection to data inspec-tion might be necessary to prevent potential data contributors from losing the context needed to reflect on the photos they collected. Thus, instead of manually describing the photos, in our probe we incorporate data descriptors that are automatically generated. We adopt an approach from Hong et al. [46], which introduces photo-and set-level descriptors for blind users to access fine-grained vi-sual differences across training photos. We find it suitable for our probe in facilitating quick inspection of otherwise similar photos. We repurpose the descriptors to generate summaries and alt-texts for individual photos, enabling blind users to inspect their data. As shown in Figure 1-1, photo-level descriptors are embedded as alt-text for individual photos in AccessShare.\nWe selected initial descriptors based on two primary factors to assist blind people in determining potential sharing [59]: i) ben-efits of sharing datasets to improve real-world recognition tasks and ii) associated risks. In terms of benefits, we considered char-acteristics that tend to be present in photos taken by blind par-ticipants such as ill-framed objects or blurriness [65, 72] that can help future models generalize better for photos with such 'image quality'. In terms of risks, we considered privacy concerns [37]. Specifically, the photo-level descriptors in AccessShare that capture 'image quality' indicate: the number of objects of interest detected in the photo as 'single object detected, \u2018more than one object detected' or 'no object detected', photo being 'blur'; whether the object is 'cropped'; and whether it appears\u02bbtoo small' within the frame. The privacy related photo-level descriptors include: \u2018hand presence' (e.g., for potential identification of tattoos and jewelry) and 'face pres-ence', as reflected in privacy as well as ethical concerns of blind people [37, 59]. As shown in Figure 1-2, set-level descriptors in AccessShare are merely an aggregate of the photo-level descrip-tors including summary information on the prevalence of those characteristics. Unlike Hong et al. [46], we do not include detailed set-level descriptors for variation; they were designed to help blind users assess their training strategies to improve their personalized models, a task beyond data inspection for sharing. Instead, the goal of the summary in AccessShare is to help blind users quickly grasp the contents of each set before interacting with individual photos."}, {"title": "3.2 Interface Design and Implementation", "content": "We extracted the descriptors using the publicly available code from Hong et al. [46] and stored them in a JSON format. We then used the Jinja web templating engine [83], to generate the accessible HTML interface with alt-text for photos derived from the descriptors. We designed a navigation menu reflecting the file structure of photos during the data collection, which follows a two-level hierarchy. The primary menu contains options for 'Training Data' and 'Testing Data'. Each opens a secondary menu that provides links to the spe-cific content related to the photos. We named these links according to the object names that users would add as labels during training (see Figure 1-3). One critical decision related to split in the file structure was that testing photos are assigned to objects based on the recognition output of a user's model, that is error prone; thus, they could be misclassified. While these recognition outputs in test scenarios are important for error analysis when shared [66], a thorough (manual) review is required to correct the labels of testing photos. Since we expect the same challenge in the real world, where only recognition labels are available in testing [47], we used the recognition labels to organize testing photos and explore potential improvements and concerns with blind people."}, {"title": "4 METHODS", "content": "We take a participatory approach to explore what kind of data access and control blind people seek over their data. The ultimate goal is to inform future data collection and sharing frameworks so that they can better align the decision-making process with the needs and expectations of their blind data contributors. As shown in Figure 2, our co-design session took place within a larger cross-sectional study that involved a series of activities engineered to capture a real-world data stewarding scenario. Specifically, we engaged 10 blind participants who were situated in the context of contributing to an AI dataset after evaluating an assistive technology-participants evaluated an object recognition app deployed on smartglasses in their homes and later decided whether they wanted the research team to share their study data via a public AI dataset. To help them decide whether to share or not, we implemented AccessShare, a design probe for inspecting one's study data and invited them to a follow up session where we could design together ways for blind data contributors to access and control their data.\nTo provide the context of the larger study, which spanned multi-ple days, we started with a 30-minute long Zoom call to capture par-ticipant demographics, attitudes, and experience with technology. A day or two later, participants joined the system evaluation from their homes and remotely connected with the researchers to per-form a series of data capturing activities (described in Section 4.2). Typically within a week after, participants joined a remote semi-structured interview to reflect on their motivations and concerns relating to sharing their study data. At the end of the interview, participants received an email from the research team, the data stewards, to indicate their decision on whether to share their study data (i.e., anonymized photos and labels) via a public AI dataset. The need to make this decision was communicated to participants early on and was included in their consent forms. Participants could also opt to join the follow-up co-design, the focus of this paper, typically conducted a few weeks later via Zoom. Some opted to confirm their decision on data sharing during this final session. We provide more details in Section 4.3 on our co-design that aims to uncover ways for providing data contributors with more control, followed by our analysis approach in Section 4.4. The ongoing analysis from the evaluation of the assistive technology is beyond this paper's scope. Findings from the semi-structured interview are available in [59]."}, {"title": "4.1 Recruitment and Participants", "content": "The study protocol was approved by our institution's review board (IRB #1822836-1). We recruited participants through an existing mailing list in our lab, by submitting a request form for participant solicitation to the National Federation of the Blind (NFB), and via word of mouth. The mailing list in our lab includes blind people who participated in our previous studies and consented to future contact. Members of the list and NFB contacts received our call for participation and consent form via email, and some shared it within their networks. Potential participants emailed their consent to a researcher, confirming they were 18 or older and blind, as required for the study. Participants consented to the multi-day study but were informed they could withdraw anytime with compensation.\nA total of 13 participants were invited to begin via a Zoom/phone call, where we collected demographic information, including age, gender, education, and occupation, as well as technology experi-ence relevant to the system evaluation. After the semi-structured interview, participants received an email with a link to their data via AccessShare to indicate whether to share their data or continue with the co-design; 10 participants opted to participate in the co-design. At the end of the study, participants were also given the option to specify the demographic information that they preferred not to be made available on publication.\nOf 10 participants who joined the co-design, seven were totally blind and three were legally blind. On average, blind participants were 53.46 years old (STD=14.94). Five self-identified as women and five as men. Participants were compensated $15/hour, with an average of 2.76 hours (STD=0.51) spent up to the system evaluation (including the opening questionnaires), 1.68 hours (STD=0.36) for the interview, and 1.09 hours (STD=0.13) for the co-design."}, {"title": "4.2 Data Collection in Evaluation", "content": "Data collection activities were conducted in our participants' homes, where they evaluated a teachable object recognition app deployed on smartglasses. We aimed to explore realistic concerns beyond the lab settings (e.g., privacy impacted by the environment [45]) and data contributions grounded in real-world applications. We asked participants to find a comfortable spot at home to set up the laptop for the Zoom call and interact with the stimuli objects while wearing smartglasses with an embedded camera. We then provided 2 practice objects to help learn how to take photos and provide labels (i.e., object names) using the smartglasses.\nParticipants used the touchpad on the smartglasses to navigate the menu and trigger the photo taking and labeling functions, all supported via text-to-speech. They also used voice commands to input, correct, and confirm the object label. Once familiar with the system, participants were asked to complete data capture activities that involved taking multiple photos per object for a total of 6 ob-jects and providing associated labels for training and evaluating a computer vision model. Half of these objects were stimuli pro-vided by the research team engineered to be visually distinct but nearly-identical by touch (i.e. different bags of snacks). They were fixed across all participants. The rest of the objects were up to the"}, {"title": "4.3 Data Access and Sharing in Co-design", "content": "The process for the co-design started right after the semi-structured interview, where blind participants received an email from the research team to indicate their decision on whether to share their study data. The email also attached a unique link to AccessShare, to access their own set of photos organized by labels collected during the evaluation study. They were encouraged to open it on their devices prior to making their decision and were invited to join the research team in a follow up session for designing together ways for blind data contributors to access and control their data.\nThe main goals of the co-design session are: to understand the data sharing decisions and the factors that shaped them; explore accessible ways to share study data with blind participants; and gain insights into meaningful descriptors for accessible data inspection. The co-design activities took place through a Zoom video call and consisted of two parts."}, {"title": "4.3.1 Part 1: Hands-on interface exploration.", "content": "We first asked our participants to open AccessShare on their devices to have an oppor-tunity to explore the interface with the researcher. Some chose to use two different devices, one for the call (e.g., smartphone) and the other to use AccessShare (e.g., laptop), referring to the accessibility challenges of navigating between two windows within the same device. We assigned 3 sequential mini-tasks during this exploration. First, they were tasked with finding a specific object label on the navigation menu. Then they would read a summary content for that object label. Last, they would describe the first three photos on the page after accessing their alt text."}, {"title": "4.3.2 Part 2: Discussion with guiding questions.", "content": "We then asked par-ticipants to describe and reflect on their experience accessing their data. We had a series of guiding questions to foster our discussion and brainstormed potential improvements and features. We list some of the example questions below, which can be described in four primary themes (see supplementary material):\nAlternative methods: What would be an ideal way for researchers to help you make a decision on whether they can share your study data with others or not? Should it be a link to your data? Should it be a phone call where they walk you through what is collected? Should it be just a question at the end of the study whether it is OK for them to share your data?\nDescriptors: Was any particular piece of information found on Ac-cessShare helpful to you in accessing what you wanted to know about your data before making a decision to allow for their sharing? What was missing from AccessShare that you wanted to know about your data before making a decision?\nTrade-offs: How much would you rely on the automatically-generated image descriptors for making your decision especially when you know that they can be erroneous? Would you still make a decision? Or would you ask for sighted help?\nPotential features and capabilities: Are there any AccessShare features that are making it efficient for you to make this decision? Are there any new features or capabilities you would add to it?\nAll study sessions were audio/video recorded via the recording feature in Zoom. We also intended to record AccessShare screen activities both to offer guidance during hands-on exploration and to collect observational data. However, this proved challenging in instances where participants accessed Zoom and AccessShare on separate devices. Instead, we relied on the audio from the screen reader to track some of these screen activities during the session."}, {"title": "4.4 Analysis", "content": "To gain a better understanding of how blind participants sought access and control over their study data and how their decisions were shaped in the process, we triangulated data from a variety of sources: recordings from the co-design session, offline communi-cation of participants' decisions around data sharing, and manual annotation of personally identifiable information (PII) in the photos."}, {"title": "4.4.1 Participant feedback during co-design.", "content": "To analyze the data obtained in our co-design session, all participant comments and responses were transcribed. We performed inductive thematic anal-ysis [16], which began with the researcher (R1), who facilitated all the co-design sessions, assigning initial descriptive (i.e., seman-tic) codes to the data and collating the codes into categories. The research team (including two other researchers, R2 and R3) then conducted ongoing data meetings to add interpretative codes/notes to the categorized data to broaden the analysis. For example, for a data excerpt with an initial code [preference toward phone call for real-time feedback] in the category of \u201cdecision making modality\u201d the team added [rationale: granular decision on data points] to the ex-cerpt. The process of adding interpretations was completed in two steps: i) the team discussed the sample of organized data (roughly 20% of all data excerpts) during the meetings, with R2 leading the discussion for an interpretative-level structure and ii) R3 followed the structure to add interpretations to the remaining data. Once done, R1 proceeded with generating and reviewing initial themes and sub-themes in relation to the descriptive codes and interpreta-tions. In synthesis, the (sub)themes were further refined through team discussions to conceptualize them as unifying concepts [17]."}, {"title": "4.4.2 Participant communication of decisions.", "content": "We report blind par-ticipants' decisions related to the sharing of their study data. We map these decisions to the analysis of PII in their study data for better contextualization of the rationale behind the decisions. To enrich the overall analysis of their decision making, we also pay attention to their method of consent (e.g., verbal/written) and its timing (e.g., before or after given access to AccessShare) while participating in our data stewarding process."}, {"title": "4.4.3 PII annotation of photos.", "content": "Two researchers reviewed blind participants' photos and manually annotated the areas with PII with a codebook that was informed from prior work [37, 92]. The codebook had two main categories: \u201cobject\u201d and \u201ctext\". Under the\""}, {"title": "5 FINDINGS ON TO SHARE OR NOT TO SHARE", "content": "We explore participants' decisions around the sharing of their data, why they arrived at those decisions, when they were formed, and how they were communicated. We also provide data on PII across participants' photos for better contextualization of the findings with a heatmap shown in Figure 3. We then expand on the preferred modalities for participants to form and communicate decisions."}, {"title": "5.1 What Were the Decisions, Their Rationale, and Connection to PII", "content": "As shown in Table 2, the majority (N = 7) of our participants made a decision to share their study data including photos of objects and labels. A common rationale for deciding to share was related to the absence of concerning elements in the photos. P3 and P4 came to this conclusion after spending at least an hour reviewing their study data with AccessShare, with P3 doing so independently and P4 with a sighted family member \u201cto make sure that something did not end up in there that I don't want shared with everybody\" (P4). Surprisingly, both P3 and P4 had many photos (87% and 52% respectively) in which pictures with recognizable faces (e.g., found in a bookshelf in the background) were visible, as shown in Figure 3. After discussing these PII with the researcher, only P3 changed their initial decision from 'share' to 'partial share,' omitting the photos with faces.\nWhen talking about their decision to share, others like P5 and P8 referred back to their privacy strategies employed in data collection, reflecting their preparation for data sharing. For example, P5 said, \"I purposefully, when I took the images, made sure that it was pointing towards the carpet or towards the wall. So I know that there's nobody in this photo, so I feel comfortable sharing.\" Indeed, these two partici-pants had the smallest occurrence of PII in their photos. Yet, the few"}, {"title": "5.2 When and How Decisions Were Formed and Communicated", "content": "As shown in Table 4, when and how participants communicated their decisions differed. Initially, some participants (P2, P8, P9) indi-cated their decision informally during the interview study, which occurred before they were given access to their study data. After the interview, only two participants (P2, P3) chose to provide written consent by responding to our email; we received their responses within a few days. P2 re-confirmed their decision to share, and P3 confirmed it for the first time after accessing their study data. The majority (N = 8) of the participants opted for verbal consent only which they conveyed while participating in the co-design session. Even those who (N = 3) who opted out of the co-design study alto-gether, did not respond back to the consent email. This indicates that although emails may appear to be an appealing approach for data stewards to obtain written consents, their effectiveness may be limited for blind data contributors, regardless of whether they are sent at the end of the study or between study sessions."}, {"title": "5.3 What Were the Decision-Making Modalities", "content": "When asked about their preferred decision-making approaches, par-ticipants expressed mixed perspectives on the modalities involved. The primary modality chosen by participants was typically a single question at the end of the study (P2, P3, P5, P10) or a phone call (P8, P9) allowing for verbal exchanges. Written communication like emails was the least preferred option among those wanting to convey granular decisions in real time (P4) and reduce the chance of misunderstandings (P5). Our participants' tendency to provide verbal consent as opposed to written consent could be reflecting this preference. Still, to authorize permission for sharing, one par-ticipant (P1) stressed the need for a written consent process via email or a form to communicate data storage and purpose, along with assurances regarding anonymity and privacy.\nAs we explored these preferred consent methods, participants expressed different opinions regarding the role of AccessShare. While some (P1, P3, P8) perceived data access as supplementary, described as a \u201cnice option to have\u201d (P8), some others (P4, P6, P7) considered it a complementary modality for decision making. This preference for data access as complementary (except for P7) seemed to tie closely with accessing sighted help: \u201cThe link to the data was helpful for me, because I had somebody go through the photos with me, just to make sure what we were looking at that was in the background\u201d (P4). On a different end, P7 preferred a system with enhanced image descriptors and mechanisms that can eliminate the need for sighted help, since \u201cit undermines the very point of a system like this if we need it [sighted help] in the first place. Otherwise, what's the point?\u201d\nWhen asked to expand on the potential use of AccessShare as a communication interface for decisions, participants elaborated on some design features that could reflect their preferences. There was a suggestion for implementing an approval/disapproval button (P4, P8) or a comment feature (P8) on individual photos to communicate partial sharing. Being concerned about transparency related to data reuse purposes once shared, P1 also suggested a way to directly interact with those requesting to access their study data, which they would need to provide \"purpose, the uses, who else would have access\" in order to grant permission."}, {"title": "6 FINDINGS ON ACCESSIBLE DATA SHARING", "content": "We explore participants' perspectives on designing an accessible data sharing application for blind people. We report their experience with AccessShare focused on the current state of the interface and descriptors, as well as their feedback on improvements from our ideation activities to further understand design opportunities."}, {"title": "6.1 What Was the Experience with AccessShare", "content": "Overall", "straightforward and streamlined": "obile interface. Two participants (P7", "[participant's name]'s dataset": "P7) or a short \u201chuman subject summary\u201d (P9) outlining the objects that they trained and tested. The majority of the participants (excluding P3 and P4) expressed spending little time on AccessShare", "P3": "The summaries were extremely helpful, or helpful than the descriptions of the photos. Well, because for example, the one we just looked at where it said, I think it said four photos, there was a face that (...) that immediately told me something's wrong. In other words, that was sufficient to tell me, yes, something's wrong. And by seeing it in the summary, I didn't really need to know where it shows each individual photo.", "It just says one object detected, multiple objects detected, hand detected. But it doesn't really tell me if it detected the object that I had intended it to be.": "ome participants (P3, P5, P6) were further confused by vague descriptions like 'more than one object detected' without spec-ifying the extra objects. This perspective mirrors a trend seen in prior work [69"}]}