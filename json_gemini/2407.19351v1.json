{"title": "AccessShare: Co-designing Data Access and Sharing with Blind People", "authors": ["Rie Kamikubo", "Farnaz Zamiri Zeraati", "Kyungjun Lee", "Hernisa Kacorri"], "abstract": "Blind people are often called to contribute image data to datasets for AI innovation with the hope for future accessibility and inclusion. Yet, the visual inspection of the contributed images is inaccessible. To this day, we lack mechanisms for data inspection and control that are accessible to the blind community. To address this gap, we engage 10 blind participants in a scenario where they wear smartglasses and collect image data using an AI-infused application in their homes. We also engineer a design probe, a novel data access interface called AccessShare, and conduct a co-design study to discuss participants' needs, preferences, and ideas on consent, data inspection, and control. Our findings reveal the impact of interactive informed consent and the complementary role of data inspection systems such as AccessShare in facilitating communication between data stewards and blind data contributors. We discuss how key insights can guide future informed consent and data control to promote inclusive and responsible data practices in AI.", "sections": [{"title": "1 INTRODUCTION", "content": "Participatory data stewardship [53] has gained attention as a means to promote ethical and responsible data practices related to collection, sharing, and usage. However, the challenge of ensuring meaningful engagement of diverse stakeholders is pronounced at the intersection of accessibility and artificial intelligence (AI), further raising concerns about the limited agency and control that disabled people may have [96]. This is also the case when it comes to their data. With a focus on blind people and the context of computer vision, we see unique accessibility challenges, such as how to support inspection of image data that they have captured and that may contain unintended information [5] without relying on sight. Despite efforts such as 'disability-first dataset' creation [86, 92], we still lack mechanisms for providing blind data contributors with access and control over their image data.\nTo address this issue, our work aims to explore first what kind of data access and control blind people seek over their image data, as part of our broader investigation into participatory stewardship mechanisms. This is a pressing topic considering how blind people have been early adopters of computer vision technology, sending their photos over servers and assistive applications in exchange for privacy [5, 32]. Consequently, they are one of the most represented contributors of publicly-available accessibility datasets [58]; yet, there is no clear process or consensus among researchers on how to ethically share such datasets. More so, conversations around these data loom large, especially with the tensions involved around benefits and risks-e.g., large multi-modal models are trained on photos taken by sighted people with further widening performance gaps on photos taken by blind users [23, 71].\nIn this work, we engineered a larger cross-sectional study designed to foster the involvement of blind participants in challenging real-world data practices from collection to sharing. In this context, we first deployed a teachable object recognizer [57] in the homes of blind participants, where they captured photos and labels of objects (i.e., study data). These data capturing activities were conducted in real-world settings to bring potential value as AI datasets [72] but came with heightened privacy concerns. Also, the photos were taken via smartglasses that may capture a broader field of view with higher risks of including potentially identifiable information (PII). The observations and findings presented in this paper are situated within this larger study, which includes a co-design session to explore blind people's needs and preferences regarding access and sharing of their study data.\nOur work encompasses the decisions that 10 blind participants made after the initial data collection and their interactions with their study data via a novel interface called AccessShare. It serves as a design probe [33] to brainstorm potential functionalities and features for an accessible data sharing system. Additionally, our approach facilitates an opportunity for blind participants to access their study data prior to arriving at their decision of whether or not to share them via a public AI dataset. We examined participants' decision-making processes situated in the larger study to understand how decisions were formed, communicated, and evolved over a participatory data stewardship framework.\nOur findings revolve around two axes capturing (i) blind participants' data sharing decisions and (ii) requirements for accessible data sharing systems. In the first axis, we found that the majority (N = 7) made a decision to share their study data. A common rationale, in addition to their motivation behind this decision, was the absence of concerning elements. Participants carefully employed strategies that e.g., avoided capturing personal information and people in the background. Yet, our manual inspection still revealed instances of PII in their photos. We also found that obtaining written data sharing consents via email after the data collection was not effective. All but two participants chose this route. Instead, participants opted for verbal consent during the co-design session. We observed that this is explained in part by a need for an interactive approach to consent. Many participants found the co-design session helpful for guiding their decisions e.g., for better understanding study data and exploring options such as partial sharing.\nFindings from the second axis surfaced the complementary nature of data access systems such as AccessShare, in supporting communication with sighted people (e.g., data stewards and family members). Yet, a deeper dive into participants' responses revealed a delicate balance between asking for sighted help and self-reliance when making their decision. While some expressed the need for more reliable descriptors to independently inspect the data, one participant opted for a joint decision making approach with a family member to both inspect the data and share agency with a bystander. The contrasting viewpoints on independence and interdependence prompted considerations for incorporating multiuser functionalities in data access systems. Overall, as a design probe, AccessShare was effective for ideating approaches to enhance data inspection and control and support communication with data stewards.\nOur contributions include: (1) empirical findings from a real-world participatory data stewarding process with blind participants, reflecting on decisions formed and how they were communicated; (2) implementation of AccessShare as a probe for co-designing accessible data sharing systems that facilitate control among blind data contributors; and (3) methods for obtaining consent through different means, offering insights into challenges and opportunities for meaningful informed consent for blind data contributors. By illustrating the active participation of blind people across our data stewarding process, this paper can inform future work in involving underrepresented communities in discussions around informed data sharing consents and meaningful data inspection and control."}, {"title": "2 BACKGROUND", "content": "We first outline the landscape of data ecosystems and policies in AI, with a focus on individuals' control over their data. We then review prior work on participation frameworks for increasing agency and discuss the value of human-centered research on AI datasets for accessibility. We build upon recent \"disability-first\" data collection efforts, which have highlighted the benefits of engaging blind people in contributing to datasets for AI innovation [56, 86, 92] and some of the motivational factors for data contributors [58].\n2.1 Prospects for Data Control in AI\nData in the digital age is a key input and source of value, often referred to as \"the new oil\" [48], a trend that continues with generative AI. A counter-trend is the rising tension to address ethical and privacy concerns surrounding user-generated data [62, 84]. The majority of generative AI applications have been pre-trained on large collections of text data sourced from the internet, including user-generated content on websites and social media platforms [18]. Many of these applications also utilize the history of interactions with users to refine and update their models to improve the performance over time [2]. As data continuously fuels AI, discussions about policy and technical interventions to grant users better control of their data are on the rise [30, 42].\nSince the launch of the European General Data Protection Regulation (GDPR) in 2018, many users were promised rights related to data protection, including the right to access, erase, and request corrections on their data, and the right to restrict processing [98]. Yet, even when these rights are granted, practical challenges remain, with users struggling to understand, use, and control their data [6, 14]. Additional challenges arise from the technical and systemic complexity of removing individual data from models once they have been trained [9, 19]. Tracking the origin and usage of individual data points throughout the AI lifecycle (e.g., training, validation, and deployment stages) can also be challenging, partly due to companies often lacking a full understanding of what has gone into their models [40]. While the recent EU's AI Act could fill in the gaps at scale, enforcing documentation, record-keeping, transparency, and human oversight [93], there are yet no clear mechanisms to allow users to have complete data control.\nOne crucial aspect of AI and data regulations is the power balance among stakeholders [26]. Despite the regulatory interventions such as the GDPR, Delacroix and Lawrence [27] point to the power asymmetry between data subjects-who have knowingly or unknowingly provided their data to various entities-and data controllers, typically organizations that make decisions about what data to collect and how it is processed. With our work, we explore methods and tools that can address the current data gaps and facilitate the agency of individuals whose data are part of the Al ecosystem, with focus in the context of research in 'AI for Accessibility'.\n2.2 Participation in Data Stewardship\nParticipatory data stewardship recognizes the importance of involving stakeholders, communities, and individuals in the decision-making processes related to data that affect them [53]. Ultimately, it aims to rebalance the asymmetries of power [60]. This approach is gaining some traction in Al research and development [13, 61, 85]."}, {"title": "2.3 Blind Contributors in Accessibility Datasets", "content": "The field of accessibility is no exception when it comes to the growing need for data, considering the potential to facilitate novel assistive technologies [15, 56, 74]. For instance, photos of everyday objects captured by blind users, the focus of this work, can contribute to building image recognition applications to access visual information [66, 88, 105]. There is a rich body of work related to photos sourced from blind people for Al datasets in this space [38, 66, 72, 86, 88]. Carefully balancing the benefits and unique risks of accessibility datasets is critical in these efforts [58]. While datasets have more value when collected in the 'wild', capturing real-world scenarios (e.g., people's homes), this comes with heightened ethical and privacy concerns [5, 37, 45].\nIn response, we see efforts toward conscientious data stewardship practices, emphasizing a \u201cdisability-first\" approach [80, 86, 92]. To facilitate the involvement of blind people in the creation of a teachable object recognition dataset, Theodorou et al. [92] implemented an interface that supports communication between data stewards and data contributors during data collection; for instance, blind people are notified of data that fail to meet the validation criteria (e.g., quality and privacy) and asked to retake them. However, this prompts unaddressed questions about data assessment and the role of contributors. Despite being most affected by the outcomes, blind people are informed rather than actively involved in validation and decision-making processes, partly due to data in-accessibility. For instance, the inspection and filtering of personally identifiable information are done by the data stewards [86, 92], not the blind contributors. Thus, privacy-preserving techniques that are accessible to blind people are attracting attention (e.g., [101, 103]).\nEnsuring ethical data practices remains a challenge, especially in engaging blind people to contribute visual content with their consent while respecting their privacy [86, 90]. We complement prior guidelines for disability-first datasets [86, 92] by addressing how blind people can access, inspect, and better control their data."}, {"title": "3 ACCESSSHARE: OUR DESIGN PROBE", "content": "To facilitate discussion and ideas for an accessible data-sharing system that can support data control, we develop a design probe [33]. It is called AccessShare. It incorporates automatically generated descriptors in an HTML webpage that enables blind users to navigate, access, and inspect their data. The probe is closely tied to the data collection task, which in this work is training a teachable object recognizer [56]-users teach the model to recognize objects of their choice by providing photos along labels as training examples. They also take additional photos for testing the personalized model. As seen in previously shared datasets in this context (e.g., [66, 72, 88]), photos captured by blind people are organized into training and testing, accompanied by files containing object labels. In this section, we detail how the probe is designed for accessing and inspecting these data, which we later discuss with blind participants to brainstorm ideal functionalities and features."}, {"title": "3.1 Automatically Generated Descriptors", "content": "Facilitating a quick turnaround from data collection to data inspection might be necessary to prevent potential data contributors from losing the context needed to reflect on the photos they collected. Thus, instead of manually describing the photos, in our probe we incorporate data descriptors that are automatically generated. We adopt an approach from Hong et al. [46], which introduces photo- and set-level descriptors for blind users to access fine-grained visual differences across training photos. We find it suitable for our probe in facilitating quick inspection of otherwise similar photos. We repurpose the descriptors to generate summaries and alt-texts for individual photos, enabling blind users to inspect their data. As shown in Figure 1-1, photo-level descriptors are embedded as alt-text for individual photos in AccessShare.\nWe selected initial descriptors based on two primary factors to assist blind people in determining potential sharing [59]: i) benefits of sharing datasets to improve real-world recognition tasks and ii) associated risks. In terms of benefits, we considered characteristics that tend to be present in photos taken by blind participants such as ill-framed objects or blurriness [65, 72] that can help future models generalize better for photos with such 'image quality'. In terms of risks, we considered privacy concerns [37]. Specifically, the photo-level descriptors in AccessShare that capture 'image quality' indicate: the number of objects of interest detected in the photo as 'single object detected, \u2018more than one object detected' or 'no object detected', photo being 'blur'; whether the object is 'cropped'; and whether it appears\u02bbtoo small' within the frame. The privacy related photo-level descriptors include: \u2018hand presence' (e.g., for potential identification of tattoos and jewelry) and 'face presence', as reflected in privacy as well as ethical concerns of blind people [37, 59]. As shown in Figure 1-2, set-level descriptors in AccessShare are merely an aggregate of the photo-level descriptors including summary information on the prevalence of those characteristics. Unlike Hong et al. [46], we do not include detailed set-level descriptors for variation; they were designed to help blind users assess their training strategies to improve their personalized models, a task beyond data inspection for sharing. Instead, the goal of the summary in AccessShare is to help blind users quickly grasp the contents of each set before interacting with individual photos."}, {"title": "3.2 Interface Design and Implementation", "content": "We extracted the descriptors using the publicly available code from Hong et al. [46] and stored them in a JSON format. We then used the Jinja web templating engine [83], to generate the accessible HTML interface with alt-text for photos derived from the descriptors. We designed a navigation menu reflecting the file structure of photos during the data collection, which follows a two-level hierarchy. The primary menu contains options for 'Training Data' and 'Testing Data'. Each opens a secondary menu that provides links to the specific content related to the photos. We named these links according to the object names that users would add as labels during training. One critical decision related to split in the file structure was that testing photos are assigned to objects based on the recognition output of a user's model, that is error prone; thus, they could be misclassified. While these recognition outputs in test scenarios are important for error analysis when shared [66], a thorough (manual) review is required to correct the labels of testing photos. Since we expect the same challenge in the real world, where only recognition labels are available in testing [47], we used the recognition labels to organize testing photos and explore potential improvements and concerns with blind people."}, {"title": "4 METHODS", "content": "We take a participatory approach to explore what kind of data access and control blind people seek over their data. The ultimate goal is to inform future data collection and sharing frameworks so that they can better align the decision-making process with the needs and expectations of their blind data contributors. As shown in Figure 2, our co-design session took place within a larger cross-sectional study that involved a series of activities engineered to capture a real-world data stewarding scenario. Specifically, we engaged 10 blind participants who were situated in the context of contributing to an AI dataset after evaluating an assistive technology-participants evaluated an object recognition app deployed on smartglasses in their homes and later decided whether they wanted the research team to share their study data via a public AI dataset. To help them decide whether to share or not, we implemented AccessShare, a design probe for inspecting one's study data and invited them to a follow up session where we could design together ways for blind data contributors to access and control their data.\nTo provide the context of the larger study, which spanned multiple days, we started with a 30-minute long Zoom call to capture participant demographics, attitudes, and experience with technology. A day or two later, participants joined the system evaluation from their homes and remotely connected with the researchers to perform a series of data capturing activities (described in Section 4.2). Typically within a week after, participants joined a remote semi-structured interview to reflect on their motivations and concerns relating to sharing their study data. At the end of the interview, participants received an email from the research team, the data stewards, to indicate their decision on whether to share their study data (i.e., anonymized photos and labels) via a public AI dataset. The need to make this decision was communicated to participants early on and was included in their consent forms. Participants could also opt to join the follow-up co-design, the focus of this paper, typically conducted a few weeks later via Zoom. Some opted to confirm their decision on data sharing during this final session. We provide more details in Section 4.3 on our co-design that aims to uncover ways for providing data contributors with more control, followed by our analysis approach in Section 4.4. The ongoing analysis from the evaluation of the assistive technology is beyond this paper's scope. Findings from the semi-structured interview are available in [59]."}, {"title": "4.1 Recruitment and Participants", "content": "The study protocol was approved by our institution's review board (IRB #1822836-1). We recruited participants through an existing mailing list in our lab, by submitting a request form for participant solicitation to the National Federation of the Blind (NFB), and via word of mouth. The mailing list in our lab includes blind people who participated in our previous studies and consented to future contact. Members of the list and NFB contacts received our call for participation and consent form via email, and some shared it within their networks. Potential participants emailed their consent to a researcher, confirming they were 18 or older and blind, as required for the study. Participants consented to the multi-day study but were informed they could withdraw anytime with compensation.\nA total of 13 participants were invited to begin via a Zoom/phone call, where we collected demographic information, including age, gender, education, and occupation, as well as technology experience relevant to the system evaluation. After the semi-structured interview, participants received an email with a link to their data via AccessShare to indicate whether to share their data or continue with the co-design; 10 participants opted to participate in the co-design. At the end of the study, participants were also given the option to specify the demographic information that they preferred not to be made available on publication.\nOf 10 participants who joined the co-design, seven were totally blind and three were legally blind. On average, blind participants were 53.46 years old (STD=14.94). Five self-identified as women and five as men. Participants were compensated $15/hour, with an average of 2.76 hours (STD=0.51) spent up to the system evaluation (including the opening questionnaires), 1.68 hours (STD=0.36) for the interview, and 1.09 hours (STD=0.13) for the co-design."}, {"title": "4.2 Data Collection in Evaluation", "content": "Data collection activities were conducted in our participants' homes, where they evaluated a teachable object recognition app deployed on smartglasses. We aimed to explore realistic concerns beyond the lab settings (e.g., privacy impacted by the environment [45]) and data contributions grounded in real-world applications. We asked participants to find a comfortable spot at home to set up the laptop for the Zoom call and interact with the stimuli objects while wearing smartglasses with an embedded camera. We then provided 2 practice objects to help learn how to take photos and provide labels (i.e., object names) using the smartglasses.\nParticipants used the touchpad on the smartglasses to navigate the menu and trigger the photo taking and labeling functions, all supported via text-to-speech. They also used voice commands to input, correct, and confirm the object label. Once familiar with the system, participants were asked to complete data capture activities that involved taking multiple photos per object for a total of 6 objects and providing associated labels for training and evaluating a computer vision model. Half of these objects were stimuli provided by the research team engineered to be visually distinct but nearly-identical by touch (i.e. different bags of snacks). They were fixed across all participants. The rest of the objects were up to the participant; they could choose anything in their home. Typically, participants opted for products that were similar to the stimuli, such as different canned goods,. Participants answered questions related to their experience during the data collection and at the end.\nEach participant generated on average 222 photos (STD=59.9) across the 6 object labels. Both the photos and the labels collected during these activities were referred to the participants as \"your study data\" throughout the communication with the research team; instead, demographic information was referred to as \u201cmetadata\u201d."}, {"title": "4.3 Data Access and Sharing in Co-design", "content": "The process for the co-design started right after the semi-structured interview, where blind participants received an email from the research team to indicate their decision on whether to share their study data. The email also attached a unique link to AccessShare, to access their own set of photos organized by labels collected during the evaluation study. They were encouraged to open it on their devices prior to making their decision and were invited to join the research team in a follow up session for designing together ways for blind data contributors to access and control their data.\nThe main goals of the co-design session are: to understand the data sharing decisions and the factors that shaped them; explore accessible ways to share study data with blind participants; and gain insights into meaningful descriptors for accessible data inspection. The co-design activities took place through a Zoom video call and consisted of two parts.\n4.3.1 Part 1: Hands-on interface exploration. We first asked our participants to open AccessShare on their devices to have an opportunity to explore the interface with the researcher. Some chose to use two different devices, one for the call (e.g., smartphone) and the other to use AccessShare (e.g., laptop), referring to the accessibility challenges of navigating between two windows within the same device. We assigned 3 sequential mini-tasks during this exploration. First, they were tasked with finding a specific object label on the navigation menu. Then they would read a summary content for that object label. Last, they would describe the first three photos on the page after accessing their alt text.\n4.3.2 Part 2: Discussion with guiding questions. We then asked participants to describe and reflect on their experience accessing their data. We had a series of guiding questions to foster our discussion and brainstormed potential improvements and features. We list some of the example questions below, which can be described in four primary themes:\nAlternative methods: What would be an ideal way for researchers to help you make a decision on whether they can share your study data with others or not? Should it be a link to your data? Should it be a phone call where they walk you through what is collected? Should it be just a question at the end of the study whether it is OK for them to share your data?\nDescriptors: Was any particular piece of information found on AccessShare helpful to you in accessing what you wanted to know about your data before making a decision to allow for their sharing? What was missing from AccessShare that you wanted to know about your data before making a decision?\nTrade-offs: How much would you rely on the automatically-generated image descriptors for making your decision especially when you know that they can be erroneous? Would you still make a decision? Or would you ask for sighted help?\nPotential features and capabilities: Are there any AccessShare features that are making it efficient for you to make this decision? Are there any new features or capabilities you would add to it?\nAll study sessions were audio/video recorded via the recording feature in Zoom. We also intended to record AccessShare screen activities both to offer guidance during hands-on exploration and to collect observational data. However, this proved challenging in instances where participants accessed Zoom and AccessShare on separate devices. Instead, we relied on the audio from the screen reader to track some of these screen activities during the session."}, {"title": "4.4 Analysis", "content": "To gain a better understanding of how blind participants sought access and control over their study data and how their decisions were shaped in the process, we triangulated data from a variety of sources: recordings from the co-design session, offline communication of participants' decisions around data sharing, and manual annotation of personally identifiable information (PII) in the photos.\n4.4.1 Participant feedback during co-design. To analyze the data obtained in our co-design session, all participant comments and responses were transcribed. We performed inductive thematic analysis [16], which began with the researcher (R1), who facilitated all the co-design sessions, assigning initial descriptive (i.e., semantic) codes to the data and collating the codes into categories. The research team (including two other researchers, R2 and R3) then conducted ongoing data meetings to add interpretative codes/notes to the categorized data to broaden the analysis. For example, for a data excerpt with an initial code [preference toward phone call for real-time feedback] in the category of \u201cdecision making modality\u201d the team added [rationale: granular decision on data points] to the excerpt. The process of adding interpretations was completed in two steps: i) the team discussed the sample of organized data (roughly 20% of all data excerpts) during the meetings, with R2 leading the discussion for an interpretative-level structure and ii) R3 followed the structure to add interpretations to the remaining data. Once done, R1 proceeded with generating and reviewing initial themes and sub-themes in relation to the descriptive codes and interpretations. In synthesis, the (sub)themes were further refined through team discussions to conceptualize them as unifying concepts [17].\n4.4.2 Participant communication of decisions. We report blind participants' decisions related to the sharing of their study data. We map these decisions to the analysis of PII in their study data for better contextualization of the rationale behind the decisions. To enrich the overall analysis of their decision making, we also pay attention to their method of consent (e.g., verbal/written) and its timing (e.g., before or after given access to AccessShare) while participating in our data stewarding process.\n4.4.3 PII annotation of photos. Two researchers reviewed blind participants' photos and manually annotated the areas with PII with a codebook that was informed from prior work [37, 92]. The codebook had two main categories: \u201cobject\u201d and \u201ctext\". Under the \"object", "text\" category covered various textual PII, such as [book], [business card], [clothing], [screen], [credit card], [letter], [license plate], [menu], [paper], [newspaper], [prescription], [receipt], [street sign], and [poster]. Both \"object\" and \"text\" categories included a [suspicious] annotation for cases where it was difficult to confirm private content due to poor image quality or complex scenes. Additionally, the \"other [name]\" category accounted for any PII instances that were not captured above.\nConsistent with prior work, we sought an annotation tool with the following features: (i) can annotate data offline to uphold privacy and data security; (ii) supports remote collaboration across annotators; and (iii) allows data export for further analysis. After a thorough exploration of the available annotation tools, we selected the VGG annotator [29] as our tool of choice due to its alignment with these requirements. Using VGG, two researchers from our team first annotated the participant photos individually (3060 photos each) with the order of photos being randomized.\nUsing Cohen's Kappa [73], we found a strong agreement between the researchers' annotations for the presence of \u201cobjects\u201d like [face] ($\\kappa$ = 0.89), [photo of face] ($\\kappa$ = 0.82), and [face reflection] ($\\kappa$ = 0.92). For annotations in the \"text\" category like [letter] and [clothing], the researchers achieved a moderate level of agreement ($\\kappa$ = 0.73 and 0.73, respectively). There were no agreements on the rest of the PII categories: \"text [screen]\u201d ($\\kappa$ = -0.05), \u201ctext [suspicious]\u201d ($\\kappa$ = -0.04), \"object [suspicious]\u201d ($\\kappa$ = 0.0), and \"other [flag]\u201d ($\\kappa$ = 0.0).\nTo finalize annotations, researchers resolved disagreements through discussion and reached consensus together. Disagreements on [face], [photo of face], and [face reflection] (N = 9, 221, and 19, respectively) occurred when faces were cropped or partially visible. Often this involved the researchers' faces being shown on the nearby laptop during the Zoom call. Consistent with prior work [37], researchers agreed to label partial faces as PII. Disagreements regarding [letter] and [clothing] (N = 5 and 5, respectively) were initially missed by one of the researchers. Additionally, a new category, \"other [flag]\" (N = 92) was introduced by one of the researchers. The rationale behind this addition stemmed from the recognition that a flag could potentially reveal someone's nationality, and some individuals might be hesitant to disclose such information [59]. Disagreements on [screen] (N = 466) were common and captured the study laptop placed nearby. When present in a photo, one researcher annotated only the PII inside the screen, such as [photo of a face]. However, the other annotated the entire screen. To mitigate the risk of overlooking any other potential PII content within the screen, in the final annotations, the entire screen was included. The most common disagreement arose in cases where it was difficult to confirm private content due to poor image quality or complex scenes annotated as \"object [suspicious]\" (N = 373) and \"text [suspicious]\u201d (N = 390). \u201cObject [suspicious]": "ostly involved photos containing blurred photos of faces. One of the researchers annotated these photos under the assumption that given the similarity among participants' photos, the presence of a face could be inferred from the context. Researchers resolved disagreements by annotating these instances as \"object [photo of face]", "Text [suspicious]": "ostly involved framed papers on the wall or brochures in the background. These instances were jointly annotated as \" text [miscellaneous paper]\"."}, {"title": "5 FINDINGS ON TO SHARE OR NOT TO SHARE", "content": "We explore participants' decisions around the sharing of their data, why they arrived at those decisions, when they were formed, and how they were communicated. We also provide data on PII across participants' photos for better contextualization of the findings. We then expand on the preferred modalities for participants to form and communicate decisions.\n5.1 What Were the Decisions, Their Rationale, and Connection to PII\nAs shown in Table 2, the majority (N = 7) of our participants made a decision to share their study data including photos of objects and labels. A common rationale for deciding to share was related to the absence of concerning elements in the photos. P3 and P4 came to this conclusion after spending at least an hour reviewing their study data with AccessShare, with P3 doing so independently and P4 with a sighted family member \u201cto make sure that something did not end up in there that I don't want shared with everybody\" (P4). Surprisingly, both P3 and P4 had many photos (87% and 52% respectively) in which pictures with recognizable faces (e.g., found in a bookshelf in the background) were visible, After discussing these PII with the researcher, only P3 changed their initial decision from 'share' to 'partial share,' omitting the photos with faces.\nWhen talking about their decision to share, others like P5 and P8 referred back to their privacy strategies employed in data collection, reflecting their preparation for data sharing. For example, P5 said, \"I purposefully, when I took the images, made sure that it was pointing towards the carpet or towards the wall. So I know that there's nobody in this photo, so I feel comfortable sharing.\" Indeed, these two participants had the smallest occurrence of PII in their photos. Yet, the few PII included could pose a high privacy risk. For instance, 4% of P5 photos included a letter capturing both the sender's and receiver's information that given a better image resolution would be directly visible. Similarly, 4% of P8 photos captured a fully visible face of a family member. Upon discussion of these PII with the researcher, P8 changed their initial decision from 'share' to 'partial share'.\nAs an overarching rationale behind data sharing, P5 and P8's cautious but not bulletproof data capturing strategy could also reflect blind participants' underlying motivations for joining the study. Their main goal could be to contribute to research, which they expected to achieve through data, as specifically expressed by P2, P9, and P10. For example, P9 said, \"I knew right away ...I don't want to be an inhibitor to say no, you can't look at my data that would be foolish. Why would I want to participate in the study? I'm here to help you. If you can't learn from my data, then what am I as a human subject?\" We suspect that by \"you\" P9 is referring to our research team in particular instead of sharing more broadly. Among these participants, P2 and P9 only included photo of faces captured from the study laptop screen including the Zoom setup in 7% and 9% of their photos, respectively. However, P10 had a greater number of PII, capturing their own face reflection from the mirror in 59% of their photos.\nLooking at those who decided not to share their data, we also found that the most common resistance stemmed from potentially concerning elements in the photos. During the co-design session, despite having the inclination to share initially, P6 ultimately opted out from sharing after discovering that their captured photos contained unintended information in the background. Indeed, all (100%) of P6's photos were flagged as including photo of face, which were family pictures captured in the background (i.e., framed on the wall). P7, who was initially inclined not to share, did not change their decision. Their rationale revolved around the automatically generated descriptors that were perceived to lack the reliability needed to identify concerning elements. P7 said, \u201cBased on the fact that there's not enough information for me to review it reliably in the sense of- the picture descriptions don't have enough...It's no, because I can't make an informed decision.\" Our manual inspection revealed that in the majority (79%) of the P7's photos, the study laptop screen was visible but a smaller portion (10%) included their face in the Zoom call as a small icon. Not surprisingly, the underlying face detector models used for the corresponding AccessShare descriptor failed to detect this small screen icon as a face in most photos where it was present. Making an informed decision was also a challenge for P1, whose photos included the faces in the Zoom call captured from the laptop screen (46%) and often their reflections in the glass table (34%). Yet, P1 found making an informed decision challenging for a different reason, stressing the lack of information regarding how the study data would be reused by others once shared.\nInterestingly"}]}