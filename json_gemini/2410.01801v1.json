{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images", "authors": ["CHENG ZHANG", "YUANHAO WANG", "FRANCISCO VICENTE CARRASCO", "CHENGLEI WU", "JINLONG YANG", "THABO BEELER", "FERNANDO DE LA TORRE"], "abstract": "We introduce FabricDiffusion, a method for transferring fabric textures from a single clothing image to 3D garments of arbitrary shapes. Existing approaches typically synthesize textures on the garment surface through 2D-to-3D texture mapping or depth-aware inpainting via generative models. Unfortunately, these methods often struggle to capture and preserve texture details, particularly due to challenging occlusions, distortions, or poses in the input image. Inspired by the observation that in the fashion industry, most garments are constructed by stitching sewing patterns with flat, repeatable textures, we cast the task of clothing texture transfer as extracting distortion-free, tileable texture materials that are subsequently mapped onto the UV space of the garment. Building upon this insight, we train a denoising diffusion model with a large-scale synthetic dataset to rectify distortions in the input texture image. This process yields a flat texture map that enables a tight coupling with existing Physically-Based Rendering (PBR) material generation pipelines, allowing for realistic relighting of the garment under various lighting conditions. We show that FabricDiffusion can transfer various features from a single clothing image including texture patterns, material properties, and detailed prints and logos. Extensive experiments demonstrate that our model significantly outperforms state-to-the-art methods on both synthetic data and real-world, in-the-wild clothing images while generalizing to unseen textures and garment shapes.", "sections": [{"title": "1 INTRODUCTION", "content": "There is an increasing interest to experience apparel in 3D for virtual try-on applications and e-commerce as well as an increasing demand for 3D clothing assets for games, virtual reality and augmented reality applications. While there is an abundance of 2D images of fashion items online, and recent generative AI algorithms democratize the creative generation of such images, the creation of high-quality 3D clothing assets remains a significant challenge. In this work we explore how to transfer the appearance of clothing items from 2D images onto 3D assets, as shown in Figure 1.\nExtracting the fabric material and prints from such imagery is a challenging task, since the clothing items in the images exhibit strong distortion and shading variation due to wrinkling and the underlying body shape, in addition to general illumination variation and occlusions. To overcome these challenges, we propose a generative approach capable of extracting high-quality physically-based fabric materials and prints from a single input image and transfer them to 3D garment meshes of arbitrary shapes. The result may be rendered using Physically Based Rendering (PBR) to realistically reproduce the garments, for example, in a game engine under novel environment illumination and cloth deformation.\nExisting methods for example-based 3D garments texturing primarily focus on direct texture synthesis onto 3D meshes using techniques such as 2D-to-3D texture mapping [Gao et al. 2024; Majithia et al. 2022; Mir et al. 2020] or multi-view depth-aware inpainting by distilling a pre-trained 2D generative model [Richardson et al. 2023; Yeh et al. 2024; Zeng 2023]. However, these approaches often lead to irregular and low-quality textures due to the inherent inaccuracies of 2D-to-3D registration and the stochastic nature of generative processes. Moreover, they struggle to faithfully represent texture details or disentangle garment distortions, resulting in significant degradation in texture continuity and quality.\nIn this work, we seek to overcome these limitations by drawing inspiration from the real-world garment creation process in the fashion industry [Korosteleva and Lee 2021; Liu et al. 2023]: most 3D garments are typically modeled from 2D sewing patterns with normalized\u00b9 and tileable texture maps. This allows us to approach the texturing process from a novel angle, where obtaining such texture maps enables more accurate and realistic garment rendering across various poses and environments. Interestingly, if we take the 3D mesh away from our task of texture transfer, there has been a long history of development in 2D exemplar-based texture map extraction and synthesis [Cazenavette et al. 2022; Diamanti et al. 2015; Efros and Freeman 2023; Efros and Leung 1999; Guarnera et al."}, {"title": "2 RELATED WORK", "content": "Our method built upon recent and seminar work on image-based 3D garment modeling, exemplar-based texture and material extraction, and diffusion-based image generation."}, {"title": "2.1 Image-based 3D Garment Modeling", "content": null}, {"title": "2.1.1 Image-to-mesh texture transfer.", "content": "Existing methods on 2D-to-3D texture transfer typically involve (1) learning a 2D-to-3D registration [Gao et al. 2024; Majithia et al. 2022; Mir et al. 2020] and"}, {"title": "2.1.2 Image-based sewing pattern generation.", "content": "We argue that a major cause of the quality gap observed in generated textures is not the capacity of the generation networks, but rather from a suboptimal choice of representations for the texture generation operating from the reference image to the 3D mesh. Unfortunately, there has been little progress in leveraging the idea of generating texture maps that can be used in the 2D UV space, despite the availability of sewing patterns for 3D garments as the sewing pattern can either be manually created by technical artists [Liu et al. 2023] or automatically reconstructed from reference images [Chen et al. 2022; Li et al. 2023; Liu et al. 2023]. Concurrently, DeepIron [Kwon and Lee 2024] is the only work that leverages the similar idea of transferring the texture using sewing pattern representation. Unlike our method, they aim to transfer entire garments without PBR texture maps and exhibits subpar performance in real-world scenarios for practical usages."}, {"title": "2.1.3 3D garment generation.", "content": "Recently, there has been growing interest in 3D garment generation using generative models. For instance, GarmentDreamer [Li et al. 2024] and WordRobe [Srivastava et al. 2024] are recent work that focus on text-based garment generation, whereas our approach transfers textures using image guidance. Another relevant work, Garment3DGen [Sarafianos et al. 2024], can reconstruct both textures and geometry from a single input image. However, unlike Garment3DGen, our work focuses on generating distortion-free texture and prints and has the additional capability of generating standard PBR materials."}, {"title": "2.2 Exemplar-based Texture and Material Extraction", "content": "The literature on exemplar-based texture and material extraction is vast. We focus on representative works that are related to ours."}, {"title": "2.2.1 Texture map extraction.", "content": "We recast the task of image-to-3D garment texture transfer as generating texture maps from reference clothing image patches. Hao et al. [2023] trained a diffusion model to rectify distortions and occlusions in natural texture images. However, it does not extract tileable texture patches or PBR materials for fabrics. More recently, Material Palette [Lopes et al. 2024] addressed a similar problem by using a diffusion-based generative model to extract PBR materials. Their approach relies on personalization methods such as textual inversion [Gal et al. 2022] to represent the exemplar patch without normalizing the patch into a canonical space, i.e., distortion-free with unified lighting."}, {"title": "2.2.2 Tileable texture synthesis.", "content": "Previous work have attempted to synthesize tileable textures with a variety of methods, such as by maximizing perceived texture stationary [Moritz et al. 2017], by using Guided Correspondence [Zhou et al. 2023a], by finding repeated patterns in images using pre-trained CNN features [Rodriguez-Pardo et al. 2019], by manipulating the latent space of pre-trained GANS [Rodriguez-Pardo and Garces 2022], or by modifying the noise sampling process of a diffusion model, i.e., rolled-diffusion [Vecchio et al. 2023]. We found that a simple circular padding strategy following [Zhou et al. 2022] performs well with our model architecture for addressing tileable texture generation."}, {"title": "2.2.3 BRDF material estimation.", "content": "A significant body of research exists on BRDF material estimation from a single image [Casas and Comino-Trinidad 2023; Deschaintre et al. 2018; Henzler et al. 2021; Vecchio and Deschaintre 2024; Vecchio et al. 2021, 2024]. Our model produces normalized texture maps in a canonical space, enabling compatibility with existing Bidirectional Reflective Distribution Function (BRDF) material estimation pipelines such as MatFusion [Sartor and Peers 2023], which can be integrated seamlessly with our output normalized textures. By fine-tuning the pre-trained MatFusion model with fabric PBR texture data and incorporate it into our pipeline, our model generates high-quality material maps for realistic 3D garment rendering."}, {"title": "2.3 Diffusion-based Image Generation", "content": "Our model architecture is inspired by the recent advancements in diffusion-based image generation models [Ho et al. 2020; Rombach et al. 2022; Sohl-Dickstein et al. 2015]. In this work, we fine-tune the pre-trained image generative model using carefully created synthetic data, enabling texture normalization, which includes distortion removal, lighting calibration, and shadow elimination."}, {"title": "3 METHOD", "content": "We propose FabricDiffusion to extract normalized, tileable texture images and materials from a real-world clothing image, and then apply them to the target 3D garment. The overall framework is illustrated in Figure 2. We first introduce the problem statement in Section 3.1, followed by procedures for constructing synthetic training examples in Section 3.2. In Section 3.3, we detail our specific approach of texture map generation. Finally, we describe PBR materials generation and garment rendering in Section 3.4."}, {"title": "3.1 Problem Statement", "content": "Given an input clothing image I and a captured texture region x, which may exhibit various distortions and illuminations due to occlusion and poses present in the input image, our goal is learn a mapping function g that takes the captured patch x and outputs the corresponding normalized texture map x, effectively correcting the distortions. The texture map x needs to retain the intrinsic properties of the original captured region, such as color, texture pattern, and material characteristics.\nAs mentioned in Section 1, we formulate the generation of normalized texture maps from a real-life clothing patch as a distribution mapping problem. Specifically, the mapping function g can be modeled by a generative process:\n$$x \\sim G_\\theta (x, \\epsilon), \\epsilon \\sim N(0, I)$$"}, {"title": "3.2 Synthetic Paired Training Data Construction", "content": "Collecting paired training examples with real clothing poses significant challenges. In contrast, we found that PBR textures \u2013 the fundamental unit for appearance modeling in 3D apparel creation \u2013 are much more accessible from public sources (see Section 4.1 for details on dataset collection). Given these observations, we propose to build synthetic environments for constructing distorted and flat rendered training pairs using the PBR material model [McAuley et al. 2012]. Figure 3 illustrates the overall pipeline."}, {"title": "3.2.1 Paired training examples construction.", "content": "For each material, we collect the ground-truth diffuse albedo ($k_d \\in \\mathbb{R}^3$), normal ($k_n \\in \\mathbb{R}^3$), roughness ($k_r \\in \\mathbb{R}^2$), and metallic ($k_m \\in \\mathbb{R}^2$) material maps. To create distorted rendered images that mimic real-world surface deformation and lighting, we map these material maps onto a raw garment mesh sampled from 22 common garment types. The PBR textures are tiled appropriately and illuminated using four environment maps with white lights to avoid color biases. During rendering, we capture frontal views of the garment and randomly crop patches from the rendered images to match the original fabric texture size. Separately, we render the same texture material on a plane mesh to create flat rendered images as ground-truths (image xo in Figure 3). For illumination, we use a fixed point light above the surface center and a fixed orthogonal camera for rendering. This method is highly beneficial as it provides supervision to align the distorted rendered images on the 3D garment to a canonical space of normalized, flat images with a unified lighting condition.\nIn fact, our flat image rendering and capturing approach may be reminiscent of the input format used in well-known SVBRDF material estimation methods [Sartor and Peers 2023; Zhou et al. 2023b, 2022; Zhou and Kalantari 2021], which require orthogonal close-up views of the materials and/or a flashing image as input. As will be described in Section 3.4, the output normalized textures from our method can be effectively integrated with SVBRDF material estimation models to generate high-quality PBR material maps."}, {"title": "3.2.2 Paired prints (e.g., logos) construction.", "content": "In additional to general textures, we aim to transfer clothing details by creating warped and flat pairs of print images. We map the print to a random location on the garment mesh and blend it with a uniformly colored background"}, {"title": "3.2.3 Scaling up training data with Pseudo-BRDF materials.", "content": "While the texture material maps are easier to acquire than real clothing, we raise the question: Do we really need a large amount of real BRDF material maps for paired training data construction, and what if we cannot obtain enough data?\nIn this work, we are able to collect a BRDF dataset comprises 3.8k assets in total (see Section 4.1 for details), covering a broad spectrum of fabric materials. However, the texture patterns in this dataset exhibit limited diversity because it is not large enough to model the appearance of fabric textures in our real life, given the vast range of colors, patterns, and materials. To address this, we augmented the dataset by gathering 100k textile color images featuring a wide array of patterns and designs, which are then used to generate pseudo-BRDF\u00b2 materials. Specifically, the color image served as the albedo map, while the roughness map was assigned a uniform value a sampled from the distribution N(0.708, 0.1932), with 0.708 and 0.193 representing the population mean and standard deviation of the mean roughness values of the real BRDF dataset, respectively. The metallic map was assigned a uniform value max(\u03b2, 0), where \u03b2 ~ U(-0.05, 0.05), and the normal map was kept flat.\nWe use a combination of real (3.8k) and pseudo-BRDF (100k) materials to create paired rendered images for training our texture generation model. During paired training examples construction, both real and pseudo-BRDF have x and xo (as illustrated in Figure 3), representing distorted and flat textures, respectively. Intuitively, the primary goal of our texture generator is to eliminate geometric distortions, and our generated pseudo rendered images, serve this purpose effectively."}, {"title": "3.3 Normalized Texture Generation via FabricDiffusion", "content": "Given the paired training images, we build a denoising diffusion model to learn the distribution mapping from the input capture to the normalized texture map. Next, we detail our training objective, model architecture and training, and the design for tileable texture generation and alpha-channel-enabled\u00b3 prints generation."}, {"title": "3.3.1 Training objective of conditional diffusion model.", "content": "Diffusion models [Ho et al. 2020; Sohl-Dickstein et al. 2015] are trained to capture the distribution of training images through a sequential Markov chains of adding random noise into clean images and denoising pure noise to clean images. We leverage Latent Diffusion Model (LDM) [Rombach et al. 2022] to improve the efficiency and quality of diffusion models by operating in the latent space of a pre-trained variational autoencoder [Kingma and Welling 2013] with encoder \u03b5 and decoder D. In our case, given the paired training data (x, xo), where x is the distorted patch and xo is the normalized texture, the feed-forward process is formulated by adding random Gaussian noise into the latent space of image x0:\n$$x_t = \\sqrt{\\gamma(t)}\\varepsilon(x_0) + \\sqrt{1 - \\gamma(t)}\\epsilon,$$\nwhere xt is a noisy latent of the original clean input x0, \u03f5 ~ N(0, 1), t\u2208 [0, 1], and \u03b3(t) is defined as a noise scheduler that monotonically descends from 1 to 0. By adding the distorted image x as the condition, the reverse process aims to denoise Gaussian noises back to clean images by iteratively predicting the added noises at each reverse step. We minimize the following latent diffusion objective:\n$$L(\\theta) = E_{\\varepsilon(x), \\epsilon \\sim N(0, 1), t} [||\\epsilon - \\epsilon_\\theta (x_t, t, \\varepsilon(x))||^2],$$\nwhere \u03b5\u03b8 denotes model parameterized by a neural network, xt is the noisy latent for each timestep t, and \u03b5(x) is the condition. Recalling Equation 1, the above formulation incorporates input-specific information (i.e., the captured patch x) into the training process for generating normalized textures. As will be shown in the experimental results in Section 4.2, this design is the key to producing faithful texture maps that differs from existing per-example optimization-based texture extraction approaches [Lopes et al. 2024; Richardson et al. 2023]."}, {"title": "3.3.2 Model architecture and training.", "content": "Any diffusion-based architecture for conditional image generation can realize Equation 3. Specifically, we use Stable Diffusion [Rombach et al. 2022], a popular open-source text-conditioned image generative model pre-trained on large-scale text and image pairs. To support image conditioning, we use additional input channels to the first convolutional layer, where the latent noise xt is concatenated with the conditioned image latent \u03b5(x). The model's initial weights come from the pre-trained Stable Diffusion v1.5, while the newly added channels are initialized to zero, speeding up training and convergence. We eliminate text conditioning, focusing solely on using a single image as the prompt. This approach addresses the challenge of generating normalized texture maps, which text prompts struggle to describe accurately [Deschaintre et al. 2023]."}, {"title": "3.3.3 Circular padding for seamless texture generation.", "content": "To ensure the generated texture maps are tileable, we employ a simple yet effective circular padding strategy inspired by TileGen [Zhou et al. 2022]. Unlike TileGen, which uses a StyleGAN-like architecture [Karras et al. 2020] and needs to replace both regular and transposed (e.g., upsampling or downsampling) convolutions, we only apply circular padding to all regular convolutional layers, thanks to the flexibility of diffusion models."}, {"title": "3.3.4 Transparent prints generation.", "content": "The vanilla Stable Diffusion model can only output RGB images, lacking the capability to generate layered or transparent images, which is in stark contrast to our demand for prints transfer. Instead of redesigning the existing generative model [Zhang and Agrawala 2024], we propose a simple and effective recipe to post-process the generated RGB print images for computing an additional alpha channel. We hypothesize that the alpha map for prints can be approximated as binary \u2013 either fully transparent or fully opaque. Based on this assumption, we assign a new RGB value for each pixel (i, j) as follows:\n$$RGB(i, j) = max[0, \\frac{x(i, j) - 0.1}{0.9}],$$\n$$A(i, j) = \\begin{cases}\n1 & \\text{if } x(i, j) \\geq 0.1, \\\\\nx(i, j)/0.1 & \\text{otherwise.}\n\\end{cases}$$\nThis approach assigns full opacity (alpha value of 1) to pixels where the initial value exceeds a certain threshold, and scales down the alpha value for other pixels, designating them as transparent background. As will be shown in Section 4.2 and Figure 5, our method can handle complex prints and logos and output RGBA print images that can be overlaid onto the fabric texture."}, {"title": "3.4 PBR Materials Generation and Garment Rendering", "content": "Our FabricDiffusion model is able to generate a normalized texture map that is tileable, flat, and under a unified lighting, ensuring compatibility with the SVBRDF material estimation method. The goal of this work is not to develop a new material estimation method but to demonstrate the compatibility of our approach with existing methods. MatFusion [Sartor and Peers 2023] is a state-of-the-art model trained on approximately 312k SVBRDF maps, most of which are non-fabric or non-clothing materials. We fine-tune this model using our dataset of real fabric BRDF materials. Specifically, we use our normalized textures as inputs, with the material maps (kd, kn, kr, km) as ground-truths for model fine-tuning.\nThe generated PBR material maps can be used for tiling in the garment sewing pattern. The remaining question is how to determine the scale for tiling? We consider two specific strategies: (1) Proportion-aware tiling. We use image segmentation to calculate the proportion of the caputured region relative to the segmented clothing, maintaining a similar ratio when tiling the generated texture onto the sewing pattern. (2) User-guided tiling. We emphasize that an end-to-end automatic tilling method may not be optimal, as user involvement is often necessary to resolve ambiguities and provide flexibility in fashion industries."}, {"title": "4 EXPERIMENTS", "content": "We validate FabricDiffusion with both synthetic data and real-world images across various scenarios. We begin by introducing the experimental setup in Section 4.1, followed by detailing the experimental results in Section 4.2. Finally, we conduct ablation studies and show several real-world applications in Section 4.3."}, {"title": "4.1 Setup", "content": "We detail the process of collecting BRDF texture, print, and garment datasets. (1) Fabric BRDF dataset. This dataset includes 3.8k real fabric materials and 100k pseudo-BRDF textures (RGB only). We reserved 200 real BRDF materials for testing the PBR generator and 800 pseudo-BRDF materials (combined with the 200 real materials) for testing the texture generator. (2) 3D garment dataset. We collected 22 3D garment meshes for training and 5 for testing. Using the method in Section 3.2, we created 220k flat and distorted rendered image pairs for training and 5k pairs for testing. (3) Logos and prints dataset. This dataset contains 7k prints and logos in PNG format. We generated pseudo-BRDF materials with specific roughness and metallic values and a flat normal map. Dark prints were converted to white if necessary. By compositing these onto 3D garments, we produced 82k warped print images."}, {"title": "4.1.2 Evaluation protocols and tasks.", "content": "We compare FabricDiffusion to state-of-the-art methods on two tasks: (1) Image-to-garment texture transfer. Our ultimate goal is to transfer the textures and prints from the reference image to the target garment. We evaluate FabricDiffusion and compare it to baseline methods using both synthetic and real-world test examples. (2) PBR materials extraction. We provide both quantitative and qualitative results on PBR materials estimation using our testing BRDF materials dataset."}, {"title": "4.1.3 Evaluation metrics.", "content": "We evaluate the quality of generated textures and garments using commonly used metrics: LPIPS [Zhang et al. 2018], SSIM [Wang et al. 2004], MS-SSIM [Wang et al. 2003], DISTS [Ding et al. 2020], and FLIP [Andersson et al. 2020]. To evaluate the tileability of the generated textures, we adopt the metric proposed by TexTile [Rodriguez-Pardo et al. 2024]. For the image-to-garment texture transfer task, we additionally report FID [Heusel et al. 2017] and CLIP-score in CLIP image feature space [Gal et al. 2022; Radford et al. 2021] to evaluate the visual similarity of the textured garment with the original input clothing."}, {"title": "4.1.4 Baseline methods.", "content": "We compare with state-of-the-art methods that support image-to-mesh texture transfer, including: (1) TEXTure [Richardson et al. 2023], the most representative method for texturing a 3D mesh based on a small set of sample images through per-subject optimization (i.e., textual inversion [Gal et al. 2022] for personalization). (2) Material Palette [Lopes et al. 2024], which focuses on texture extraction and PBR materials estimation from a single image using generative models. (3) MatFusion [Sartor and Peers 2023], for PBR materials estimation for general materials, not specifically fabric or clothing. We fine-tuned the pre-trained MatFusion model with our curated fabric BRDF training examples, resulting in improved performance."}, {"title": "4.2 Experimental Results", "content": null}, {"title": "4.2.1 FabricDiffusion on real-world clothing images.", "content": "We first show the results of our method on real-world images in Figure 4. Our method effectively transfers both texture patterns and material properties from various types of clothing to the target 3D garment. Notably, our method is capable of recovering challenging materials such as knit, translucent fabric, and leather. We attribute this success to our construction of paired training examples that seamlessly couples the PBR generator with the upstream texture generator. Since we focus on non-metallic fabrics, the metallic map is omitted in the visualizations in the section. Please be referred to Appendix for more details and results."}, {"title": "4.2.2 FabricDiffusion on detailed prints and logos.", "content": "In addition to texture patterns and material properties, our FabricDiffusion model can transfer detailed prints and logos. Figure 5 shows some examples. We highlight two key advantages of our design that benefit the recovery of prints and logos. First, our conditional generative model corrects geometry distortion caused by human pose or camera perspective. Second, as detailed in Section 3.3, our method can"}, {"title": "4.2.3 Image-to-garment texture transfer.", "content": "In Figure 6, we compare our method with Material Palette [Lopes et al. 2024] and TEXTure [Richardson et al. 2023] for image-to-garment texture transfer. We present the results on real-world clothing images featuring a variety of textures, ranging from micro to macro patterns and prints. Our observations indicate that FabricDiffusion not only recovers repetitive patterns, such as scattered stars or camouflage, but also maintains the regularity of structured patterns, like the plaid on a skirt. Please refer to Table 1 for quantitative results."}, {"title": "4.2.4 PBR materials extraction.", "content": "We compare our method to Material Palette [Lopes et al. 2024] and MatFusion [Sartor and Peers 2023] on PBR materials extraction. In Table 2, we present a comparison of pixel-level MSE and SSIM between the generated material maps and the ground-truths. Our FabricDiffusion material generator, fine-tuned from the base MatFusion model with additional fabric BRDF training examples, demonstrates superior performance. Additionally, Figure 7 shows visual comparisons between FabricDiffusion and Material Palette. While Material Palette [Lopes et al. 2024] struggles to accurately capture fabric materials, our FabricDiffusion model excels in recovering the physical properties for fabric textures, particularly in roughness and diffuse maps. We also evaluate different methods on the rendered images and show the results in Table 3. Particularly, we use render-aware metrics like FLIP [Andersson et al. 2020] and perceptual metrics like LPIPS and DISTS. FabricDiffusion consistently achieve better performance over other approaches."}, {"title": "4.3 Ablations, Analyses, and Applications", "content": null}, {"title": "4.3.1 Ablation on circular padding and tileability analysis.", "content": "We conduct an ablation study to evaluate the impact of circular padding using the TexTile metric [Rodriguez-Pardo et al. 2024], where higher values indicate better tileability. The results show that the MaterialPalette [Lopes et al. 2024] achieves a score of 0.54. Our method without circular padding scores 0.47, while with circular padding, our method improves significantly, reaching a score of 0.62."}, {"title": "4.3.2 Ablation on pseudo-BRDF data.", "content": "We compare the performance of using combined real-BRDF and pseudo-BRDF data versus using only real-BRDF data. The results, summarized in Table 4, demonstrate that the inclusion of pseudo-BRDF data alongside real-BRDF data improves performance across all metrics."}, {"title": "4.3.3 Effect of the capture location.", "content": "In Section 3.4, we explored how FabricDiffusion can be integrated into an end-to-end framework for 3D garment design. To assess whether the generated texture remains consistent with the input, Figure 8-(a) shows the results of varying the location of a fixed-size capture region. The results indicate that FabricDiffusion consistently produces similar texture patterns, regardless of the location of the captured region."}, {"title": "4.3.4 Effect of the capture scale.", "content": "In Figure 8-(b), we further study the effect of the size of the captured region. By varying the scale of the captured region, FabricDiffusion recovers the texture pattern from the input patch, demonstrating robustness to changes in resolution."}, {"title": "4.3.5 Multi-material texture transfer.", "content": "Since FabricDiffusion works on patches, it can be applied to multi-material garments as well as evidenced in Figure 10. This suggests that FabricDiffusion can serve as a basic building block for multi-material garment texture transfer."}, {"title": "4.3.6 Compatibility with AI-Generated Images.", "content": "We explore the possibility of enhancing FabricDiffusion with AI-generated images and demonstrate the results in Figure 9. In addition to real-life clothing, we use an advanced text-to-image model to create apparel images and the apply FabricDiffusion to transfer their textures to the target 3D garments. This opens up new creative possibilities for designers, allowing them to envision and materialize entirely novel textures and patterns through simple text descriptions."}, {"title": "5 DISCUSSION, LIMITATION, AND CONCLUSION", "content": "In this paper, we introduce FabricDiffusion, a new method for transferring fabric textures and prints from a single real-world clothing image onto 3D garments with arbitrary shapes. Our method, trained entirely using synthetic rendered images, is able to generate undistorted texture and prints from in-the-wild clothing images. While our method demonstrates strong generalization abilities with real photos and diverse texture patterns, it faces challenges with certain inputs, as shown in Figure 11. Specifically, FabricDiffusion may produce errors when reconstructing non-repetitive patterns and struggles to accurately capture fine details in complex prints or logos, especially since our focus is on prints with uniform backgrounds, moderate complexity, and moderate distortion. In the future, we plan to address these challenges by enhancing texture transfer for more complex scenarios and improving performance on difficult fabric categories, such as leather. Additionally, we plan to expand our method to handle a broader range of material maps, including transmittance, to further extend its applicability."}, {"title": "Supplementary Material", "content": "We provide details and results omitted in the main text.\n\u2022 Section A: Key advantages of FabricDiffusion.\n\u2022 Section B: Additional details on dataset construction.\n\u2022 Section C: Additional implementation details.\n\u2022 Section D: Additional results and analyses."}, {"title": "A KEY ADVANTAGES OF FABRICDIFFUSION", "content": "Normalized texture representation. Unlike existing image-to-3D texture transfer methods, FabricDiffusion generates normalized textures that can be used in the 2D UV space. We highlight two outputs: (1) High-quality, distortion-free, and tileable texture maps from a non-rigid garment surface. (2) Seamless integration with SVBRDF material estimation pipelines, which usually build upon the first output - standard close-up views of the materials as input.\nSim-to-real generalizability. The conditional diffusion model, trained entirely using synthetic rendering images, proves highly effective in generating normalized texture maps from real-world images. We attribute this success to: (1) Our model bridging the domain gap between real and rendered textures by conditioning on the real input texture. (2) Synthetic data offering controllable supervision and diverse geometric, illumination, and occlusion variations.\nData and computational efficiency. During training, our method of creating pseudo-BRDF material is effective in scaling up the training examples. During inference, our model performs feed-forward sampling from Gaussian noise, which takes approximately less than 5 seconds on a single NVIDIA A6000 GPU. In contrast, existing texture transfer methods often rely on costly per-example optimization."}, {"title": "B DETAILS ON DATASET CONSTRUCTION", "content": "Fabric BRDF and textile dataset. To curate textures and their BRDF materials, we use several public libraries (AmbientCG\u00b9, ShareTextures\u00b2, 3D Textures\u00b3) under the CC0 license and supplement them with additional assets purchased from artists. The real BRDF dataset we collected comprises 3.8k assets, encompassing a broad spectrum of fabric materials. The pseudo-BRDF dataset contain 100k fabric textures with only RGB color images. We reserved 200 materials"}, {"title": "C ADDITIONAL DETAILS OF OUR METHOD", "content": null}, {"title": "C.1 Details on physics-based rendering", "content": "During rendering", "equation": ""}]}