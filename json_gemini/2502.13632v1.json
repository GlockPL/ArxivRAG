{"title": "Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization", "authors": ["Or Raphael Bidusa", "Shaul Markovitch"], "abstract": "The opaque nature of Large Language Models (LLMs) has led to significant research efforts aimed at enhancing their interpretability, primarily through post-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck Models (CBMs), offer both interpretability and intervenability by incorporating explicit concept representations. However, these methods suffer from key limitations, including reliance on labeled concept datasets and significant architectural modifications that challenges re-integration into existing system pipelines. In this work, we introduce a new methodology for incorporating interpretability and intervenability into an existing model by integrating Concept Layers (CLs) into its architecture. Our approach projects the model's internal vector representations into a conceptual, explainable vector space before reconstructing and feeding them back into the model. Furthermore, we eliminate the need for a human-selected concept set by algorithmically searching an ontology for a set of concepts that can be either task-specific or task-agnostic. We evaluate CLs across multiple tasks, demonstrating that they maintain the original model's performance and agreement while enabling meaningful interventions. Additionally, we present a proof of concept showcasing an intervenability interface, allowing users to adjust model behavior dynamically, such as mitigating biases during inference.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) utilize large amounts of training data, learning rich, high-dimensional embeddings that pass through the network as intricate vector representations (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2023; Vaswani et al., 2023). While being highly effective, these intricate data representations pose a significant challenge in understanding and explaining the model's reasoning. The \"black box\" nature has raised concerns about the unchecked deployment of neural networks in areas that demand accountability and transparency such as in healthcare, finance, and legal systems (Golgoon et al., 2024; Chen et al., 2024; Mohammadi et al., 2025).\nInterpretability is the ability to understand and reason about the model's decisions. A popular approach for interpretability is post-hoc analysis, which attempts to explain an already-trained model without modifying its computational process (Ribeiro et al., 2016; Belinkov and Glass, 2019; Madsen et al., 2022). While widely used, post-hoc methods often fall short in capturing the true reasoning process of the model, operating externally, analyzing a model that remains inherently opaque, rather than being an inherent part of the model's decision-making pipeline (Laugel et al., 2019; Bordt et al., 2022; Wei et al., 2024).\nAn alternative in-hoc approach uses interpretable vector representations, where each dimension corresponds to a human-understandable concept, embedding interpretability directly into the model's structure rather than inferring it externally. This paradigm enables both interpretability and intervenability, as modifying the interpretable representation during inference allows for potentially mitigating undesirable behaviors. A notable example of this paradigm is the Concept Bottleneck Model (CBM) framework (Koh et al., 2020; Chauhan et al., 2023; Yuksekgonul et al., 2023; Oikarinen et al., 2023; Ismail et al., 2024), which first maps the input into an interpretable conceptual space, where each dimension represents the semantic relation to a specific concept. The model then uses this conceptual representation to make predictions. While CBMs have been particularly popular in computer vision, only a limited number of works have adapted them to NLP (Tan et al., 2023; Sun et al., 2024; Ludan et al., 2024).\nDespite their advantages, CBMs suffer from several drawbacks: Training data constraints arise as CBMs often require datasets with explicit concept labels (x, C, y) to train models to first predict concept representations before making final predictions; Selecting a set of concepts is challenging, often requiring domain experts or an external LLM, reintroducing trust issues in black-box models; Task specificity limits the applicability of selected concepts to other domains; Backward compatibility & architectural continuity pose significant challenges, as LLMs are already integrated into critical applications, and modifying their architecture disrupts existing pipelines, making full adoption impractical. Some works have addressed these issues, but none have tackled all of them.\nIn this work, we propose a method of enhancement that makes any language model both interpretable and intervenable, rather than building a new model based on on a given language model. Our method integrates Concept Layers (CLs) into the model, enabling conceptual projections and interventions at any layer of the original network. Our approach:\n\u2022 Maintains performance and agreement with the original model.\n\u2022 Does not rely on predefined (x, C, y) datasets for concept mapping and adds no additional learned parameters to the model.\n\u2022 Allows for task-agnostic or task-specific conceptualization, making it widely applicable.\n\u2022 Preserves architectural continuity, ensuring seamless integration with existing systems.\n\u2022 Facilitates automatic selection of concepts through ontology-based search.\nOur method achieves a structured, hierarchical interpretability framework without disrupting an existing one."}, {"title": "2 Conceptualizing Language Models", "content": "In this section, we describe our new methodology for enhancing a model's interpretability and intervenability via conceptualization. Given a model $f_\\theta$ and a concept set C, we first show how to construct a Concept Layer and integrate it into the model's architecture. In the next section, we introduce a novel, automatic, ontology-based method for generating such a concept set, tailored for the model."}, {"title": "2.1 Assumptions and Definitions", "content": "Consider a fully trained model $f_\\theta : \\mathcal{T} \\rightarrow \\mathcal{Y}$, parametrized by $\\theta$, that maps text from a textual space $\\mathcal{T}$ into a space $\\mathcal{Y}$. Define a model slice $(\\theta_{01}, h_{\\theta2})$ such that $f_\\theta = h_{\\theta_2} \\circ g_{\\theta_1}$, where $g_{\\theta_1}: \\mathcal{T} \\rightarrow \\mathcal{L}$ is a prefix of the model, parametrized by $\\theta_1$, and $h_{\\theta_2}: \\mathcal{L} \\rightarrow \\mathcal{Y}$ is a suffix of the model, parametrized by $\\theta_2$. The space $\\mathcal{L}$ of dim h is the model's internal latent representation, which we aim to project into an interpretable space. Denote the given concept set by $C = \\{c_1,..., c_n\\}$. We assume that we are given a textual semantic representation of each concept $\\tau : C \\rightarrow \\mathcal{T}$ (Simhi and Markovitch, 2023). It can be the concept's name, a definition, or a corpus of related texts. The normalized latent representation of a concept c is denoted by $\\hat{c} \\in \\mathcal{L}$. Formally,\n$$\\hat{c} \\coloneqq \\frac{g_{\\theta_1}(\\tau(c))}{\\|g_{\\theta_1}(\\tau(c))\\||}$$"}, {"title": "2.2 Defining the Concept Layer", "content": "A Concept Layer (CL) is a non-trainable module that integrates into the original model. The CL first projects vectors from the latent space $\\mathcal{L}$ into an interpretable conceptual space $\\mathcal{L}_c$, then reconstructs a representation in $\\mathcal{L}$ before passing it back to $h_{\\theta_2}$. $\\mathcal{L}_c$ is a conceptual space of dimension n, where the i-th element represents the semantic similarity to the concept $c_i$. Let $l = g_{\\theta_1}(t)$ be the latent representation of some text $t \\in \\mathcal{T}$ in $\\mathcal{L}$. Formally, CL uses two projections, $M_c : \\mathcal{L} \\rightarrow \\mathcal{L}_c$ and $M_c^+ : \\mathcal{L}_c \\rightarrow \\mathcal{L}$.\n$$M_c(l) \\coloneqq (\\hat{c_1}\\cdot l, ..., \\hat{c_n} \\cdot l)$$\nBy definition, $M_c(l)$ is a vector of the cosine similarities in the latent space $\\mathcal{L}$, between the input text and each concept in C, factored by the size of $l$. Therefore, in order to get the interpretable representation of a text during inference, all that is left to do is to store $M_c(l) / \\|l\\|$. Note that the projection is a linear operation and therefore $M_c$ can be simply defined as the matrix $M_c = (\\hat{C_1},..., \\hat{C_n})^T$. $M_c^+$ is the pseudo-inverse of $M_c$, calculated only once upon creating the CL.\nAs shown in fig. 1 a conceptualization of a model $f_\\theta$ is defined as the process of slicing the model and then integrating a CL in between the slices. This is followed by a short training phase to \"weld\" the CL to the model by adapting the parameters of $h_{\\theta_2}$, as described in the next subsection. The added projections are static and therefore no additional parameters are added to the network. Formally, the conceptualized model of $f_\\theta$ is defined by\n$$f_\\theta'(t) \\coloneqq h_{\\theta_2}(M_c^+M_c g_{\\theta_1}(t)), \\forall t \\in \\mathcal{T}$$"}, {"title": "2.3 Welding the CL", "content": "To keep $\\mathcal{L}_c$ interpretable we want to choose a relatively small number of concepts. n is usually smaller than the hidden dimension h, meaning $M_c$ is not a square matrix, and $l$ is projected into a lower-dimensional space. Furthermore, since the concepts themselves might not be independent, their representations in the latent space of the original model may also be correlated, potentially making $M_c^+$ not well-conditioned. The projection to a conceptual space and back, therefore, limits the expressiveness of the original latent representations by forcing them to align with interpretable concepts, limiting the degrees of freedom of the model. This can be seen as a structural regularization, similar to methods like low-rank approximations. (Sainath et al., 2013; Hu et al., 2021).\nLosing too much information, however, can be harmful to the model's performance. A short training phase is required in order to \"weld\" the CL to the model, adapting $h_{\\theta_2}$ to the loss of information. The welding is performed by training $h_{\\theta_2}$ for the task of feature-based distillation (Romero et al., 2015) with regard to the original model. This means that the loss is defined to be the distance between the vectors passing through both models at each stage of the forward pass, rather than just the final representation as in the original distillation method (Hinton et al., 2015). This, together with already being close to the original model, results in a fast welding phase on a much smaller dataset than the original model was trained on.\nNote that $g_{\\theta_1}$ must remain frozen during the welding phase, as training it would change the semantic meaning of the $M_c$ projection. The matrix $M_c$ was constructed by concatenating $\\{\\hat{C_i}\\}_{i=1}^n$ together where each $\\hat{c_i}$ was computed using $g_{\\theta_1}$ itself. Modifying $g_{\\theta_1}$ will prevent $M_c$ from correctly capturing the cosine-similarity between the input text and the concepts."}, {"title": "2.4 Multi-Layer Conceptualization", "content": "As was shown by post-hoc interpretability methods, different layers in deep neural networks capture different semantic ideas (Guan et al., 2019). Therefore, in order to enhance the interpretability of the model even more, the process of conceptualization can be repeated on a different layer, on an already conceptualized model. Such a process can extract more information about the similarity of an input text to different concepts, as learned by the original model. Furthermore, this process enables intervention at different stages of the forward computation, allowing adjustments to specific conceptual representations as they evolve within the model.\nLet $f_\\theta'$ be an already conceptualize model, $\\langle g_{\\theta_1'}, h_{\\theta_2'}\\rangle$ a slice of $f_\\theta'$, and $C'$, an additional concept set for the new CL. Note that the new projection matrix, $M_c'$, should be calculated with regard to the new prefix $g_{\\theta_1'}$, in order to preserve the cosine-similarity semantics of the new CL. An additional important note is that the new slicing should be in a deeper layer in the model than the previous slicing point. Failing to do so will result in disrupting the cosine-similarity semantics of the previous CL in the welding phase."}, {"title": "3 Concept Set Generation", "content": "We assume that we are given an ontology \u2014 an hierarchical set of human concepts. Our goal is to select a concept set of size n out of this ontology. Choosing the right concept set is a crucial step in the conceptualization process. It will define the pivotal ideas by which the input will be interpreted, compared, and projected to. It will determine the types of interventions that could be conducted and affect the output. Since the interpretable vectors are part of the internal architecture, it will also affect the model's performance and expressiveness."}, {"title": "3.1 Desired Properties", "content": "Given a contextual corpus of texts $\\mathcal{T}_{context}$ we want to search for a concept set C that will satisfy the following:\n\u2022 The concepts in C should capture the core ideas within $\\mathcal{T}_{context}$, ensuring that they represent the most significant elements of the corpus.\n\u2022 The concepts in C should differentiate between distinct ideas in $\\mathcal{T}_{context}$ enabling a clear separation between conceptual regions in the representation space.\n\u2022 The subspace of $\\mathcal{L}_c$, induced by the projected texts of $\\mathcal{T}_{context}$, should be expressive enough to preserve meaningful structure and maintain sufficient variance, ensuring that the conceptual space does not collapse into a limited subspace."}, {"title": "3.2 Task-Specific and Task-Agnostic Models", "content": "The corpus $\\mathcal{T}_{context}$ consists of samples from a text distribution. By basing the search on $\\mathcal{T}_{context}$, we introduce two distinct conceptualization processes. If $\\mathcal{T}_{context}$ is drawn from a distribution associated with a particular task (e.g., AG News), the resulting conceptualization is classified as task-specific, providing focused interpretability within a particular domain. Conversely, if $\\mathcal{T}_{context}$ is sampled from a generic distribution, preferably the original model's training set, it will lead to a task-agnostic conceptualization, preserving the model's versatility while maintaining a more general interpretability."}, {"title": "3.3 Ontology-Based Search", "content": "In our context, an ontology is a structured representation of the human knowledge that defines the relation between different concepts\u00b9. Let $G = (C^*, E)$ be our ontology graph, where $C^*$ is a set of concepts and E is the \"type-of\" relation, meaning that $(c, c') \\in E$ if $c'$ is \"type of\" c. Building on the idea of Simhi and Markovitch (2023), we select a concept set C via a search algorithm over the concept space $C^*$. We denote the set of the successors of a concept c by $Succ(c) = \\{c' \\in C^*|(c, c') \\in E\\}$."}, {"title": "3.4 Variance-Guided Algorithm", "content": "Our search algorithm maintains a concept set $C_f$, which is returned at the end. The algorithm will also maintain a priority queue of concepts open and a close set close to avoid expanding the same concept twice. It will start with an initial set of concepts as an initial guess, by default the root of the ontology, and in each step will decide which concept out of open should be expanded. The algorithm will also decide which of a concept's successors should be added to $C_f$ and to open itself, to possibly be expanded later. The priority queue will use a variance-based metric called Average Variance Gain (AVG). Let c be a concept. The variance of a corpus $\\mathcal{T}_{context}$ with respect to concept c, denoted by $V_T (c)$, is defined as the variance of the set of projected values:\n$$VT(c) \\coloneqq Var(\\{\\hat{c}\\cdot g_{\\theta_1} (t)|\\forall t \\in \\mathcal{T}_{context}\\} )$$\nThis measures how well the concept $\\hat{c}$ spans the semantic variability of $\\mathcal{T}_{context}$ in the latent space $\\mathcal{L}$. A higher variance indicates that $\\hat{c}$ differentiates between diverse meanings within the corpus. Let $s \\in Succ(c)$ be a successor of c. We will define the Variance Gain (VG) by,\n$$VG(c, s) \\coloneqq VT(s) - VT(c)$$\nThis measures the additional variance introduced by the successor concept compared to its parent. This measure was influenced by the information gain metric used in algorithms for creating decision trees (Quinlan, 1986) and serves as a criterion for evaluating how much the addition of a child concept increases the expressiveness of the future $\\mathcal{L}_c$. We define the Eligible Successors (ES), as the set of successors of c whose Variance Gain exceeds a given threshold thr, ensuring they contribute meaningfully to the conceptual space. Formally,\n$$ES(c, thr) \\coloneqq \\{s \\in Succ(c)|VS(c, s) > thr, s \\notin C_f \\}$$\nThis guarantees that only conceptually informative and previously unexplored successors are considered for expansion. Finally, the Average Variance Gain (AVG),\n$$AVG(c, thr) = \\frac{1}{|ES(c, thr)|} \\sum_{s\\in ES(c,thr)} VG(c, s)$$\nmeasures the overall informativeness of expanding a concept by averaging the Variance Gain across its eligible successors. Concepts will be selected from open based on their AVG score until $C_f$ reaches the desired size. If open is exhausted before $C_f$ reaches the target size, the threshold thr is reduced, and open is reinitialized with all concepts currently in $C_f$. This adjustment allows for a broader exploration of the conceptual space by including successors with lower Variance Gain, controlled by a threshold scheduler. We employ a linear scheduler, allowing thr to eventually become negative, enabling non-greedy expansions that may lead to more meaningful expansions in later iterations. The complete pseudocode is provided in Appendix algorithm 1."}, {"title": "4 Experiments", "content": "In this section, we evaluate our conceptualization method. First, we assess whether it preserves the original model's performance, ensuring that enhancement does not degrade accuracy while maintaining agreement with the original predictions. Second, we verify backward compatibility by testing the enhanced models in the same environment as the original. Finally, we provide a short proof-of-concept evaluation of intervenability. For our experiments, we used the all-MiniLM-L6-v2 sentence transformer \u00b2, referred to as \"the original model\". This model has 6 transformer layers with a hidden size of 384. We tested our method on three datasets: AG News, Yelp Polarity, and DBpedia-14 (Zhang et al., 2016).\nConceptualization was performed at two possible cuts: between the fifth and sixth layers (Single CL) or at both the fourth-to-fifth and fifth-to-sixth layers (Double CL). Concept selection was conducted via an ontology-based search for a set of 100 concepts. We evaluated eight models in total:\n\u2022 Two Task-Agnostic Models: Single CL and Double CL, trained with task-agnostic conceptualization.\n\u2022 Six Task-Specific Models: Each dataset had two variations: Single CL and Double CL, trained using dataset-specific conceptualization.\nFor the welding process, we used wikitext-103-v1 for task-agnostic models and combined it with the training set for task-specific models. Training was conducted on a single NVIDIA L40S GPU. Task-agnostic models trained faster (around 22 minutes per epoch) as they used only the general corpus, while task-specific models took longer (up to 30 minutes) due to the combined corpus. Models were trained for 15 epochs. Training hyperparameters: batch size = 32, learning rate = 3e-5, optimizer = AdamW, scheduler = linear with 500 warmup steps."}, {"title": "4.1 Model Recovery", "content": "Enhancing a model's interpretability is valuable, but it is crucial to ensure that its expressiveness, capabilities, and overall performance remain preserved. To evaluate whether conceptualization affects the model's effectiveness, we conducted a classification task using our datasets. For each of the nine models (the original model plus eight conceptualized variants), we trained a separate MLP classification head on the training set. The MLP was trained until convergence, monitored using a validation set."}, {"title": "4.1.1 Raw Performance", "content": "To assess performance, we evaluated each classifier on the test set, measuring accuracy, weighted F1-score, and loss. The accuracy results for all models are presented in table 1, while weighted F1-scores and losses are included in the appendix (tables 5 and 6).\nThe results indicate that conceptualization preserves model performance, with conceptualized models performing on par with or slightly better than the original model in most cases. Notably, the best-performing model for each dataset was a conceptualized variant rather than the original."}, {"title": "4.1.2 Agreement", "content": "Maintaining performance is essential, but agreement with the original model is equally critical. Two models may both achieve 90% accuracy, yet still disagree on 20% of the predictions if their errors occur on different samples. High agreement ensures that the conceptualized model retains behavioral consistency with the original model.\nThe agreement rates between the conceptualized and original models across datasets are reported in table 2. The results demonstrate a high level of agreement, confirming that conceptualization does not introduce drastic behavioral changes."}, {"title": "4.2 Backward Compatibility", "content": "Language models are deeply integrated into real-world applications, making backward compatibility a critical requirement. Enhancements should seamlessly integrate without disrupting existing components. In addition to maintaining the same output dimensionality, an enhanced model must ensure its outputs remain compatible with downstream modules.\nA good indicator of compatibility is whether a classifier head trained on the original model remains effective when transferred to a conceptualized model without retraining. To evaluate this, we applied an MLP classifier head, trained on the original model, to each conceptualized variant and measured accuracy and agreement (table 3). The corresponding F1-weighted scores and loss values are provided in the appendix (table 7, table 8). While a slight, expected drop in accuracy was observed, agreement with the original model remained consistently high. Most notably, the task-specific conceptualized models outperformed task-agnostic ones in both accuracy and agreement."}, {"title": "4.3 Interpretability and Intervenability", "content": ""}, {"title": "4.3.1 Interpretability", "content": "The conceptual vectors in $\\mathcal{L}_c$ are inherently interpretable as they represent cosine similarities between the input representation and each concept. If the original model learned rich semantic representations, the projection will reflect meaningful relationships. Since $g_{\\theta_1}$ remains unchanged during the welding phase, interpretability can be extracted by dividing $M_c(l)$ by $\\|l\\|$ and then sorting these values and selecting the top k concepts provides an interpretable explanation.\nTherefore, a key aspect of interpretability is the concept set itself\u2014and by extension, the method used to select it. Our variance-based heuristic determines which concepts are included in the projection space. While a human study is beyond this work's scope, we provide a list of selected concepts in the appendix (tables 9 to 11) for examination. In Yelp Polarity, the retrieved concepts align with the dataset's domain\u2014food, museums, arts, pubs, and nightlife\u2014with minimal noise. AG News shows similar alignment. In contrast, DBpedia, which classifies Wikipedia-derived categories, mirrors the Wikipedia category graph, making it unsuitable as an independent interpretability benchmark."}, {"title": "4.3.2 Interventability", "content": "Intervenability refers to modifying the model's decision-making by adjusting its conceptual representation. If each vector element corresponds to a distinct aspect, modifying it should produce a predictable, aspect-specific change in behavior. Chauhan et al. (2023) demonstrated this by enabling users to query and adjust individual concepts.\nWe provide a short proof of concept demonstrating model intervenability. We show how modifying conceptual activations influences model predictions and analyze specific cases.\nOur intervention interface is straightforward: it takes a list of pre-selected concepts and, during inference, attenuates the corresponding vector elements by multiplying them with a discount factor. We test this interface on a task-specific Concept Layer trained on the Yelp Polarity dataset.\nImagine a scenario where a Yelp Polarity-based classifier recommends attractions to a user. Suppose the user is unconcerned with cost and does not want price-related biases to affect recommendations. Since the dataset lacks explicit categories for why an attraction is classified as negative, we cannot directly filter results based on price. Instead, we use our intervention mechanism to reduce the influence of the \"Economy\" concept, ensuring that overpriced attractions are still considered in recommendations. Table 4 presents examples of reviews that were originally classified as negative (true negatives) but, after intervention, were reclassified as positive, demonstrating the model's ability to adjust predictions in a controlled, concept-driven manner."}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 Post-hoc Conceptualization of Embedding Spaces", "content": "Simhi and Markovitch (2023) proposed a post-hoc method for interpreting model embeddings by mapping them to a concept space, unlike our approach, which integrates conceptualization into the model. Their method introduces an automatic concept selection process using ontology-based search, similar in spirit to ours but with a different selection criterion. Instead of relying on learning from predefined datasets of concepts scores, they map model representations through dot product, avoiding the need for additional learned parameters."}, {"title": "5.2 Concept Bottleneck Models (CBMs)", "content": "The Concept Bottleneck Model (CBM) framework was introduced by Koh et al. (2020), proposing a structured approach where models first predict human-interpretable concepts before making final decisions. This allows for transparency and direct intervention at the concept level. CBMs have been widely explored, mainly in vision tasks. However, they require explicitly labeled concept datasets (x, C, y) and are inherently task-specific. Moreover, they define a new end-to-end model architecture rather than working with existing models, requiring full retraining."}, {"title": "5.3 Extending CBMs: Interactive and Label-Free Approaches", "content": "Interactive CBMs (ICBMs) (Chauhan et al., 2023) extend the CBM framework by introducing human feedback at inference time, allowing users to adjust concept activations before final predictions. This system enhances intervenability, as it enables real-time corrections to improve decision-making. Label-Free CBMs (LF-CBMs) (Oikarinen et al., 2023) focus on automating concept selection and labeling. Instead of requiring predefined concepts, LF-CBMs query an external LLM to generate concepts dynamically. Concept scores are then inferred using CLIP-based similarity. This approach removes the dependency on manually labeled datasets while still following the last-layer bottleneck structure."}, {"title": "5.4 Concept Bottlenecks in NLP", "content": "Recent works have explored adapting CBMs to natural language processing. Concept Bottleneck Large Language Models (CB-LLMs) Sun et al. (2024) introduced concept bottlenecks into LLMs, demonstrating their applicability in both text classification and text generation tasks. By enforcing conceptual constraints on latent representations, CB-LLMs enable interpretability while allowing for structured reasoning within LLM architectures. However, they rely on an external LLM for concept generation, remain task-specific, and following the last-layer bottleneck structure.\nLudan et al. (2024) introduced Text Bottleneck Models (TBMs), an interpretable text classification framework where a linear predictor is trained on concept labels generated by GPT-4. This approach relies on an external LLM to define and label the concept space, This approach depends entirely on GPT-4 for concept definition and labeling, making it reliant on external querying rather than internalizing a conceptual representation within the model.\nAnother approach, C3M (Tan et al., 2023), merges human-annotated concepts with concepts generated and labeled by ChatGPT to build a CBM on top of GPT-2 and BERT. By integrating human-defined and generated concepts, C3M provides a flexible way to incorporate structured reasoning in NLP tasks. However, it still requires predefined concept labels and relies on external models for generating part of the concept space."}, {"title": "5.5 Maintaining Model Structure Through Conceptual Mapping", "content": "Recent works have explored integrating concepts into existing models without enforcing a strict bottleneck while preserving their original structure. The framework suggested by Laguna et al. (2024) enables modifying model behavior through concept-based interventions without altering the underlying model. However, this framework still requires labeled (x, C, y) validation set for probing.\nAnyCBMs (Dominici et al., 2024) propose a post-hoc method to transform any pretrained model into a CBM-like system without requiring full retraining, By mapping internal model embeddings into a conceptual space. However, AnyCBMs rely on a validation set and use concepts generated by GPT-3, reintroducing dependencies on external models.\nBoth approaches preserve the structure of existing models, mapping internal representations into a conceptual space instead of enforcing an explicit concept bottleneck. However, they still require external supervision through labeled validation sets or predefined concept sets."}, {"title": "6 Conclusions", "content": "In this paper, we presented a novel methodology for enhancing a given LLM by incorporating conceptual layers into its architecture. We demonstrated that our approach introduces interpretability and intervenability without degrading the original model's performance.\nWe believe that our method will enable the development of techniques that leverage our intervention interface for understanding, debugging, and detecting biases in existing models. In future work, we plan to extend our experiments to more resource-intensive generative models."}, {"title": "7 Limitations", "content": "The welding phase introduces a necessary adaptation step where the model aligns with the conceptualized representation, requiring a short training process. This phase relies on distillation from the original model, which demands either direct access to its latent representations during training or precomputing them in advance. Both approaches can be challenging depending on system constraints, making this non-trivial step the primary computational cost of our method."}, {"title": "8 Ethical Considerations", "content": "The new methodology presented here has the potential to positively impact a wide range of ethical issues. For instance, our intervention interface can enable users of job application filtering systems to minimize the influence of political factors in decision-making by discounting politics-related concepts."}, {"title": "Appendix", "content": ""}]}