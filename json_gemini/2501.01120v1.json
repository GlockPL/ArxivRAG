{"title": "Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning", "authors": ["Jian Lang", "Zhangtao Cheng", "Ting Zhong", "Fan Zhou"], "abstract": "Multimodal learning with incomplete modality is practical and challenging. Recently, researchers have focused on enhancing the robustness of pre-trained MultiModal Transformers (MMTs) under missing modality conditions by applying learnable prompts. However, these prompt-based methods face several limitations: (1) incomplete modalities provide restricted modal cues for task-specific inference, (2) dummy imputation for missing content causes information loss and introduces noise, and (3) static prompts are instance-agnostic, offering limited knowledge for instances with various missing conditions. To address these issues, we propose RAGPT, a novel Retrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three modules: (I) the multi-channel retriever, which identifies similar instances through a within-modality retrieval strategy, (II) the missing modality generator, which recovers missing information using retrieved contexts, and (III) the context-aware prompter, which captures contextual knowledge from relevant instances and generates dynamic prompts to largely enhance the MMT's robustness. Extensive experiments conducted on three real-world datasets show that RAGPT consistently outperforms all competitive baselines in handling incomplete modality problems. The code of our work and prompt-based baselines is available at https://github.com/Jian-Lang/RAGPT.", "sections": [{"title": "Introduction", "content": "Multimodal learning has emerged as a critical paradigm in both research and industry, demonstrating broad application potential in areas such as healthcare assistance (Ghosh et al. 2024) and malicious content detection (Kiela et al. 2020). However, most successful methods typically assume that the completeness of all modalities is essential during both training and inference phases. In reality, factors such as malfunctioning sensors and privacy concerns often make it infeasible to collect complete modalities (Ma et al. 2021). As a result, the challenge of incomplete modalities significantly impacts the reliability, accuracy, and safety of multimodal models in practical applications (Woo et al. 2023; Cheng et al. 2024a).\nTo address this challenge, researchers have developed various robust multimodal methods that are broadly categorized into three groups: (1) Joint learning methods (Wang et al. 2023; Yao et al. 2024), (2) Cross-modal generation methods (Ma et al. 2021; Woo et al. 2023), and (3) Prompt-based methods (Lee et al. 2023; Jang, Wang, and Kim 2024). For joint learning methods, they heavily rely on the selection of similarity measures and require filling missing-modality inputs with masking values, resulting in the loss of critical information and the introduction of noise into the models (Wang et al. 2024). Cross-modal generation methods inevitably face modality heterogeneity issues and incur limited reconstruction quality. Recently, prompt-based methods have gained significant attention due to the rise of powerful pre-trained MultiModal Transformers (MMTs). These methods leverage prompt-tuning techniques to effectively transfer the capabilities of MMTs pre-trained on complete multimodal datasets to tasks involving missing modalities, achieving remarkable performance and making them a dominant trend in incomplete multimodal learning.\nHowever, for incomplete modalities, prompt-based methods typically use the available modalities as the only cue to fulfill task-specific objectives through prompt learning (see Fig. 1). Despite their progress, these methods often struggle in severe missing-modality scenarios due to several unresolved issues inherent in their design: (1) Remaining modalities typically provide restricted modal information, which fails to effectively address specific tasks when the missing modality contains crucial modal cues. (2) Modal incomplete inputs are often filled with dummy values (e.g., empty strings/pixels for texts/images), which may introduce noise, leading to degraded performance (Ma et al. 2022). (3) The prompt tokens are shared across any inputs and therefore are instance-agnostic. Thus, this static prompt-tuning is not well-suited for real multimodal instances, as instances with different types of missing modalities belong to distinct distributions. Additionally, static prompts typically provide limited knowledge for both missing- and full-modality instances. Therefore, these observations motivate us to design a universal prompt-tuning strategy to enhance the pre-trained MMT's robustness for incomplete modalities.\nTo address these issues, we draw inspiration from the human ability to learn through observation, which involves mastering skills by observing relevant subjects rather than attempting to memorize every subject (Hodges et al. 2007). As shown in Fig. 1, we leverage this cognitive principle to address the challenge of missing modalities. Our core idea is to retrieve relevant multimodal contents and utilize them as prompts to enhance the robustness of pre-trained MMT in both missing- and full-modality scenarios. Intuitively, for instances with missing modalities, appending multimodal content from similar instances can provide contextual knowledge relevant to the missing modality and improve task-specific predictions.\nTo this end, we propose RAGPT, a novel Retrieval-AuGmented dynamic Prompt Tuning framework to adaptively enhance the robustness of pre-trained MMT in both missing- and full-modality scenarios. Fundamentally, we reformulate incomplete modality learning in a principled retrieve-and-prompt manner and maintain a model-agnostic design that facilitates seamless integration with various prompt-based models. RAGPT includes three modules: multi-channel retriever, missing modality generator, and context-aware prompter. During retrieval, we propose a universal multi-channel retrieval strategy that disentangles multimodal representations into unimodal components, facilitating the retrieval of similar samples based on within-modality similarities for missing- and full-modality scenarios.\nNext, the missing modality generator comprises a learnable filter to approximate the missing information. Beyond traditional reconstruction techniques, which suffer from modality gaps during the cross-modal generation, this generator realizes intra-modal reconstruction by leveraging information from the retrieved samples that belong to the same modality as the missing one to recover the missing content. Moreover, this design enriches the missing-modality representation, ensuring alignment with the complete-modality input format of pre-trained MMTs during the pre-training phase. Finally, the context-aware prompter identifies the semantic correlations between the target and retrieved instances, producing dynamic multimodal prompts tailored to different inputs. These prompts facilitate the adaptive refinement of modality features in both missing- and full-modality scenarios, thereby enhancing the robustness of the pre-trained models. We insert these modules into the pre-trained MMTs to achieve a more accurate representation for both missing- and full-modality data. Following are our main contributions:\n\u2022 To our best knowledge, this is the first retrieval-augmented paradigm for incomplete modalities. We reveal that prior prompt-based methods suffer from issues related to dummy padding and static prompts, which drastically degrade performance in severe missing-modality cases.\n\u2022 To address these issues, we propose RAGPT, pioneering a retrieval-augmented dynamic prompt-tuning framework that bridges target and relevant instances, recovers missing modality, and generates dynamic prompts to enhance the MMT's robustness in diverse missing-modality situations.\n\u2022 We conduct extensive experiments on three real-world datasets to evaluate RAGPT in comparison with 9 competitive baselines and the results confirm RAGPT's effectiveness in addressing missing-modality issues."}, {"title": "Related Work", "content": "Missing-Modality in Multimodal Learning. Researchers have developed various robust methods for incomplete multimodal learning, which can be divided into three groups: (1) Joint learning methods (Zhao, Li, and Jin 2021; Wang et al. 2023; Yao et al. 2024) focus on distilling complex correlations from complete modalities to tackle missing-modality data. However, these methods require filling modality-incomplete inputs with masking values, which may cause unexpected behavior and introduce additional noise in the inference process. (2) Cross-modal generation methods (Lee et al. 2019; Yuan et al. 2021) primarily reconstruct the missing content by using remaining modalities. Researchers (Ma et al. 2021; Woo et al. 2023) directly employ VAE to generate the missing-modality based only on available modalities. Consequently, these methods inevitably face modality heterogeneity problems. (3) Prompt-based methods (Lee et al. 2023; Jang, Wang, and Kim 2024) represent a recent trend in this field, which introduces learnable prompts to help pre-trained MMTs address incomplete modalities.\nHowever, prompt-based methods are constrained by the dummy imputation and static prompting strategy, resulting in performance bottlenecks. In contrast, our RAGPT captures contextual knowledge from retrieved instances to recover the missing content and generate dynamic prompts to enhance the MMT's robustness for missing modalities.\nPrompt Learning. Prompt learning (Liu et al. 2023) utilizes a small number of learnable prompt parameters added to the input of pre-trained transformers, facilitating adjustments to the pre-trained models for alignment with downstream tasks. It has been successfully applied to various domains, such as visual identity (Khattak et al. 2023; Lee et al. 2023) and social network analysis (Zhou et al. 2021; Xu et al. 2021; Zhong et al. 2024; Cheng et al. 2024b, 2023). Following the success of prompt learning in NLP tasks (Li and Liang 2021), recent works have attempted to explore its application in multimodal learning (Zhou et al. 2022a). For instance, MaPLe (Khattak et al. 2023) introduces a soft prompt appended to the hidden representations of MMTs, resulting in significant improvements in few-shot image recognition. For incomplete multimodal learning, MAPs (Lee et al."}, {"title": "Methodology", "content": "Problem Definition: In this paper, we consider a multimodal dataset incorporating two modalities. Formally, we define D = {Df, Dm} to represent the multimodal dataset. Here, $D_f = \\{(x^i_t, x^i_v, y_i)\\}_{i=1}^{N_f}$ represents the modality-complete subset, where $y_i$ is the class label of i-th instance and $x$ denote two modalities (e.g., texts and images). $N_f$ is the total number of instances in the subset Df. Conversely, $D_m = \\{(x^i_t, --, y_i) \\lor (--, x^i_v, y_i)\\}$ is a modality-incomplete subset, where \u201c\u2013\u201d indicates a missing modality and $N_m$ is the number of missing-modality data in $D_m$. The objective of the task is to enhance model robustness in cases where modalities are missing during both training and testing phases.\nFig. 2 presents the key components and their relationships in RAGPT. The following sections delve into the specifics of each component and their respective implementations.\nMulti-Channel Retriever\nIn this section, we design a unified multi-channel retriever to identify similar modal content for queries within their respective modalities by using within-modality similarities.\nMemory Construction To store high-quality semantic information as prior knowledge, we define the memory B, which encodes multimodal instances using a collection of (image, text, label) triples.\nMulti-Channel Retrieval To adapt diverse missing- and full-modality scenarios, we develop a Multi-Channel Retriever (MCR) that effectively retrieves relevant instances through a unified retrieval architecture. Specifically, for the text-missing channel, the MCR employs the image representation as a query to identify top-K similar images and incorporates the associated texts to create multimodal instances. For complete modalities, the MCR utilizes both the image and text to search relevant texts and images, respectively, thereby creating multimodal instances.\nSpecifically, in the text-level branch, the MCR first tokenizes the text $x^i_t$ in the target instance T into n word tokens and then projects them into word embedding $W_i \\in \\mathbb{R}^{n \\times d_t}$, where $d_t$ is the dimension of word embedding. Next, the embedding $W_i$ is fed into a pre-trained textual encoder (e.g., CLIP textual encoder (Radford et al. 2021)) $\\Psi_t(\\cdot)$ to obtain text representation, represented as $E = \\Psi_t(W_i) \\in \\mathbb{R}^{d_t}$. Subsequently, the MCR utilizes the text query E to calculate similarity scores with the text representation $E^r$ from the memory B, enabling the identification of the top-K textually similar instances CR:\n$C_R = Top-K(\\frac{E^r E} {\\|E^r\\| \\|E\\|})$  (1)\nFor the vision content, the MCR first divides the image $x^i_v$ into m non-overlapping patches and then projects them into a sequence of patch tokens $V_i \\in \\mathbb{R}^{m \\times d_v}$. Next, these tokens $V_i$ are input into a pre-trained vision encoder (e.g., CLIP vision encoder (Radford et al. 2021)) $\\Psi_v(\\cdot)$ to obtain vision query $E \\in \\mathbb{R}^{d_v}$. Finally, the retrieval process for searching top-K vision content is the same as defined in Eq. 1. After retrieval, the top-K instances $C_R = \\{c_1, \\ldots, c_K\\}$ can be readily obtained. Each retrieved instance $c_k$ contains the (image, text, label) triplet. The retrieved top-K instances provide auxiliary context, guiding the recovery of missing content in the target instance and improving task-specific predictions.\nContext-Aware Prompter\nTo explicitly capture expressive contextual information and enhance robustness of pre-trained MMTs against missing-"}, {"title": "Context-Aware Prompter", "content": "modality issues, we design a Context-Aware Prompter (CAP) that constructs text-, vision-, and label-level dynamic prompts from the retrieved instances $C_R$. For text-level prompts, the CAP fuses the reference textual features in $C_R$ and aligns textual embedding in T through a simple network. Specifically, the CAP first tokenizes and projects the texts $x^i_t$ and $\\{x^k_t\\}_{k=1}^K$ into word embeddings $W_i \\in \\mathbb{R}^{n \\times d_t}$ and $W^R = \\{W^k_t\\}_{k=1}^K \\in \\mathbb{R}^{K \\times n \\times d_t}$. Subsequently, the word embedding $W_i$ is used as the query to interact with the retrieved text features $\\{W^k_t\\}_{k=1}^K$ via a cross-attention block to facilitate comprehension of context, thereby generating the text-level comprehensive representation $P_t \\in \\mathbb{R}^{n \\times d_t}$:\n$P_t = Att(f_Q(W_i), f_K(W^R_t), f_V(W^R_t))$,  (2)\n$Att(Q, K, V) = Softmax(\\frac{Q K^T}{\\sqrt{d}})V$,  (3)\nwhere $f_Q(\\cdot), f_K(\\cdot), f_V(\\cdot)$ denote the query, key, and value projection functions, respectively. For vision-level prompts, the CAP uses the same process to interact the vision patch tokens $V_i \\in \\mathbb{R}^{m \\times d_v}$, with the retrieved patch tokens $V^R \\in \\mathbb{R}^{K \\times m \\times d_v}$ to obtain the vision-level representation $P_v \\in \\mathbb{R}^{m \\times d_v}$. Then, the CAP employs an adaptive pooling strategy to obtain the final context-aware prompts $P_t \\in \\mathbb{R}^{l \\times d_t}$ and $P_v \\in \\mathbb{R}^{l \\times d_v}$, where $l$ is the prompt length. For label-level prompts, the CAP yields a label embedding matrix $P^l \\in \\mathbb{R}^{C \\times d}$ to encode C class labels, where $d$ is an adjustable dimension. Given retrieved labels, the CAP performs a look-up operation on embedding matrix $P^l$ and obtains each label embedding. Next, the CAP averages K label embeddings and generates label-level prompts $P^l \\in \\mathbb{R}^{d}$."}, {"title": "Knowledge-Augmented Prompt-Tuning", "content": "In this process, we first utilize the retrieved modal information to approximate the missing content through a missing modality generator. Next, we perform dynamic prompt-tuning on the pre-trained MMT (e.g., ViLT (Kim, Son, and Kim 2021)) to enhance task-specific inference.\nMissing Modality Generator Existing reconstruction methods (Ma et al. 2021) address missing-modality issues by recovering missing content through available modalities. However, these methods often overlook the modal heterogeneity issue and rely on complex generative structures. Based on these observations, we propose a Missing Modality Generator (MMG) that recovers the missing modality through an \"intra-modal reconstruction\". The MMG leverages retrieved content of the same modality as the missing one and incorporates a learnable filter layer to effectively approximate the missing modality in a simpler but effective manner. Specifically, given the text-missing instance Ti, the MMG employs a non-parametric strategy to average all text embeddings $W^R = \\{W^k_t\\}_{k=1}^K$ from retrieved instances $C_R$, thereby obtaining textual representation $\\widehat{W_t} \\in \\mathbb{R}^{n \\times d_t}$ to approximate the missing modality.\nConsidering potential noise in comprehensive textual representation $\\widehat{W_t}$, the MMG introduces a simple learnable filter block (i.e., MLP-based filter (Zhou et al. 2022b)) to efficiently refine textual features $\\widehat{W_t}$ by removing noise. Specifically, the MMG employs the Fast Fourier Transform (FFT) along the textual dimension. This operation transforms the text context representation $\\widehat{W_t}$ into the frequency domain:\n$Z_i = F(\\widehat{W_t}) \\in \\mathbb{C}^{n \\times d_t}$, (4)\nwhere F(.) denotes the one-dimensional FFT, and $Z_i$ is the spectrum of $\\widehat{W_t}$. The MMG then modulates the spectrum by element-wise multiplication with a learnable filter $W \\in \\mathbb{C}^{n \\times d_t}$:\n$\\widetilde{Z_i} = W \\odot Z_i$, (5)\nwhere $\\odot$ denotes the element-wise multiplication. Finally, the MMG utilizes the inverse FFT operation to the modulated spectrum $\\widetilde{Z_i}$ back into the time domain:\n$\\widehat{W_t} = F^{-1}(\\widetilde{Z_i}) \\in \\mathbb{R}^{n \\times d_t}$, (6)\nwhere $F^{-1}(.)$ is the inverse one-dimensional FFT, converting the complex tensor back to a real-valued tensor. To further stabilize training and enhance the embedding, the MMG incorporates a skip connection, layer normalization, and dropout:\n$\\widehat{W_t} = LayerNorm(\\widehat{W_t} + Dropout(\\widehat{W_t}))$. (7)\nFinally, the recovered representation $\\widehat{W_t}$ is used as the embedding for the missing modality and is subsequently fed into the pre-trained MMT. Additionally, the aforementioned process is applied to scenarios involving missing images to obtain the corresponding vision patch embedding $\\widehat{V_t}$.\nDynamic Prompt-Tuning Given a pre-trained MMT $f_{\\theta}$ with N consecutive Multi-head Self-Attention (MSA) layers, we denote the input representation of the b-th MSA layer as $h_b \\in \\mathbb{R}^{L \\times d}, b = 1, 2, \\ldots, N$ with input length L and embedding dimension d. For full-modality data, we utilize the embedding layer of the pre-trained model $f_e(\\cdot)$ to obtain the corresponding text embedding $E_t$ and image embedding $E_v$. In the case of missing-modality, we employ the generated word embedding $\\widehat{W_t}$ and vision patch embedding $\\widehat{V_t}$ to fill the corresponding missing modality. $h_1$ is the concatenation of text embedding $E_t$ and image embedding $E_v$. The context-aware prompts $P_t, P_v, and P^l$ are then attached to the embedding features along the sequence-length dimension to form the extended features $h_b = [P_t, P_v, P^l, h_b]$.\nThese extended features $h^b$ are fed into the MMT starting from the b-th layer and continue to propagate through the remaining layers. The final output $h_N$ represents comprehensive modal representation after the N-th layer. Rather than adding prompts at each MSA layer, which can result in considerable overhead, we selectively insert the prompts into the specific b-th layer.\nLabel-Augmented Prediction To further leverage the contextual information in label-level prompts, we design a label-augmented classifier by computing the similarity between the output representation of the MMT and the label matrix $P^l$. Specifically, for the final prediction, we feed the output representation $h_N$ into the pooler layer to obtain the representation $Z \\in \\mathbb{R}^{d \\times 1}$. Next, we calculate the probabilities $\\widehat{y} \\in \\mathbb{R}^{C \\times 1}$ for C classes: $\\widehat{y} = softmax(P^{l^T}*Z)$. During training, we freeze all parameters in the MMT and optimize the model using cross-entropy loss."}, {"title": "Experiments", "content": "Experimental Settings\nA summary of the experimental settings is provided in this section, which refers to datasets, baselines, evaluation metrics, setting of missing pattern, and implementation details.\nDatasets Following previous work (Lee et al. 2023; Jang, Wang, and Kim 2024), we evaluate our RAGPT on three downstream tasks. (1) MM-IMDb (Arevalo et al. 2017), primarily used for movie genre classification involving both image and text modalities. (2) Food101 (Wang et al. 2015), which focuses on image classification that incorporates both image and text. (3) HateMemes (Kiela et al. 2020), aimed to identify hate speech in memes using image and text modalities. Detailed statistics of datasets are presented in Table 1. The dataset splits are consistent with the original paper.\nBaselines We compare our RAGPT with 9 competitive baselines, which are classified into three categories: (1) Cross-modal generation methods: SMIL (Ma et al. 2021), TFR-Net (Yuan et al. 2021), and AcMAE (Woo et al. 2023). (2) Joint learning methods: IF-MMIN (Zuo et al. 2023), ShaSpec (Wang et al. 2023), DrFuse (Yao et al. 2024), and CorrKD (Li et al. 2024). (3) Prompt-based methods: MAPs (Lee et al. 2023) and MSPs (Jang, Wang, and Kim 2024).\nEvaluation Metrics Following prior works (Lee et al. 2023; Jang, Wang, and Kim 2024), we adopt appropriate dataset-specific metrics for evaluation: F1-Micro (F1-M) and F1-Sample (F1-S) for the MM-IMDb dataset, AUROC for the HateMemes dataset, and classification accuracy (ACC) for the Food101 dataset.\nSetting of Missing Pattern We define the missing rate $\u03b7\\%$ as the proportion of modality-incomplete data relative to the entire dataset. For each dataset, there are three possible cases of missing-modality: text missing, image missing, and both modalities missing. Text/image missing with a missing rate of $\u03b7\\%$ indicates that there are $\u03b7\\%$ instances consisting of texts/images and $(1-\u03b7)\\%$ instances that contain both modalities. Missing both modalities with a missing rate of $\u03b7\\%$ indicates that there are $\\frac{\u03b7}{2}\\%$ instances consisting solely of images, $\\frac{\u03b7}{2}\\%$ instances consisting solely of text, and $(1-\u03b7)\\%$ instances that are complete, containing both modalities.\nImplementation Details Following prior works (Lee et al. 2023; Jang, Wang, and Kim 2024), we utilize the pre-trained ViLT (Kim, Son, and Kim 2021) as our MMT backbone. The memory B for each dataset is constructed with the corresponding training set. The length l of context-aware prompts is set to 2, the number of retrieved instances K is chosen from \\{1, 3, 5, 7, 9\\}, and the prompt insertion layer b is set to 2. We utilize the AdamW optimizer (Loshchilov and Hutter 2017) with a learning rate of 1\u00d710-3 and total 20 epochs for optimizing the parameters. All experiments are conducted with an NVIDIA RTX 3090 GPU."}, {"title": "Overall Performance", "content": "To verify the superiority of RAGPT, we compare it with 9 competitive baselines on three datasets under a missing rate of $\u03b7\\% = 70\\%$. From these results, we have the following observations:\nFirst, our RAGPT consistently outperforms all strong baselines on three datasets under various modal conditions and metrics. Moreover, we retrain RAGPT and the best-performing baseline five times to calculate the p-value. Notably, RAGPT achieves improvements of 12.21% and 12.68% in the F1-M and F1-S metrics, respectively, on the MM-IMDb dataset with missing text. These results validate our design of exploiting expressive knowledge from retrieved instances to enhance both missing and complete modality data. Meanwhile, the missing modality generator and context-aware prompter distill expressive contextual information from retrieved instances to approximate missing content and generate dynamic prompts, respectively, thereby improving model robustness for incomplete modalities.\nSecond, cross-modal generation and joint learning methods demonstrate inferior performance, primarily due to the uncertainty introduced by random placeholders and the challenges of modality heterogeneity in reconstruction, which create significant performance bottlenecks. Moreover, prompt-based methods also exhibit limited effectiveness in missing-modality scenarios, as they rely on dummy imputations and static prompting strategies, further restricting their potential and resulting in performance stagnation."}, {"title": "Ablation Study", "content": "We conduct various ablation experiments to evaluate the impact of each component within RAGPT under a 70% text missing case and summarize the results in Table 3.\nEffect of Multi-Channel Retriever To analyze the impact of the retriever in RAGPT, we designed two variants: (1) CM Retriever: replacing the multi-channel retriever with cross-modal retriever, and (2) w/o Retriever: removing the retriever entirely. These results confirm the presence of the modal gap problem in cross-modal retrieval, which renders the retrieved instances irrelevant to the target images. Furthermore, this finding reinforces our design of the multi-channel retrieval that retrieves relevant instances by calculating within-modality similarities, thereby enhancing both missing and complete modality data.\nEffect of Missing Modality Generator To evaluate the impact of the missing modality generator, we designed variant models: (1) Padding: using random values to fill in the missing modality, and (2) w/o Filter: removing the filter block entirely. We observe that dummy padding results in a decline in performance. This finding supports our assertion that dummy padding contributes to performance bottlenecks in prompt-based methods. Additionally, the removal of the filter layer leads to a significant performance drop, underscoring the importance of the filter layer in RAGPT for effectively mitigating noise.\nEffect of Context-Aware Prompter To analyze the context-aware prompts, we design variants: (1) Static Prompt: re-"}, {"title": "Hyper-Parameter Analysis", "content": "Fig. 3(a) and 3(b) present the sensitivity analysis of RAGPT's hyper-parameters K on the MM-IMDb and HateMemes datasets. The results demonstrate that the performance of RAGPT is improved by retrieving relevant instances. However, incorporating a larger number of instances may result in a decline in performance due to the introduction of noise (i.e., irrelevant instances). Consequently, we adopt K = 3 under the image missing case on the MM-IMDb dataset and K = 5 under other scenarios."}, {"title": "Retrieval Quality Presentation", "content": "To further analyze the efficacy of our proposed multi-channel retriever, we randomly select two instances with incomplete modalities from the Food101 dataset. Fig. 4 visualizes the Top-2 similar retrieved instances, demonstrating a strong semantic correlation between the retrieved and target instances in both image and text modalities. The high quality of retrieval relevance indicates our multi-channel retriever's ability to effectively identify relevant modal information."}, {"title": "Model Generalizability", "content": "To investigate the model's generalizability, we design two experiments with varying missing rates in the training set"}, {"title": "Robustness to Different Missing Rates", "content": "We conduct an experiment to analyze the model's robustness to varying missing rates. Fig. 6 illustrates the results comparing RAGPT with four strong baselines (ShaSpec, DrFuse, MAPs, and MSPs) on the HateMemes dataset. We observe that the performance of all baselines deteriorates markedly as the missing rate increases. In contrast, RAGPT demonstrates only a slight performance decrease as the missing rate increases. This result highlights the valuable components of RAGPT for effectively mitigating the impact of missing data. Specifically, RAGPT leverages expressive knowledge from retrieved instances to approximate missing modalities through the missing modality generator. Additionally, RAGPT generates context-aware prompts that enhance the performance of the pre-trained MMTs."}, {"title": "Model Scalability", "content": "To further validate the RAGPT's scalability, we integrate key modules (multi-channel retriever, missing modality gen-"}, {"title": "Model Prediction Visualization", "content": "Fig. 8 illustrates the t-SNE (Van der Maaten and Hinton 2008) visualization of the embedding distributions for three genres (i.e., Sport, Film-Noir, and Western) in the MM-IMDb test set under a 90% text missing rate. We observe that while baseline MSPs learns distinguishable features, the learned features remain intertwined. In contrast, the representations of three genres learned by our RAGPT are more discriminative, exhibiting larger segregated areas among instances with different labels."}, {"title": "Conclusion", "content": "In this work, we proposed RAGPT, a novel retrieval-augmented dynamic prompt-tuning framework to address the missing-modality issue. This model-agnostic framework includes three key components: (1) the multi-channel retriever, (2) the missing modality generator, and (3) the context-aware prompter, to effectively inject valuable contextual knowledge into pre-trained MMT, thereby enhancing its robustness in the missing-modality scenario. Extensive experiments conducted on three real-world datasets demonstrate the superiority of RAGPT in tackling incomplete modality learning."}]}